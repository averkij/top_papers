{
    "paper_title": "NeKo: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts",
    "authors": [
        "Yen-Ting Lin",
        "Chao-Han Huck Yang",
        "Zhehuai Chen",
        "Piotr Zelasko",
        "Xuesong Yang",
        "Zih-Ching Chen",
        "Krishna C Puvvada",
        "Szu-Wei Fu",
        "Ke Hu",
        "Jun Wei Chiu",
        "Jagadeesh Balam",
        "Boris Ginsburg",
        "Yu-Chiang Frank Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an ``expert'' of speech-to-text, language-to-text and vision-to-text datasets by learning to route each dataset's tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative $5.0$% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with $15.5$% to $27.6$% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model."
        },
        {
            "title": "Start",
            "content": "NEKO: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts Yen-Ting Lin* Chao-Han Huck Yang Zhehuai Chen Piotr Zelasko Xuesong Yang Zih-Ching Chen Krishna Puvvada Szu-Wei Fu Ke Hu"
        },
        {
            "title": "Jun Wei Chiu",
            "content": "Jagadeesh Balam Boris Ginsburg Yu-Chiang Frank Wang NVIDIA corresponding authors: ytl@ieee.org, hucky@nvidia.com 4 2 0 2 8 ] . [ 1 5 4 9 5 0 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Construction of general-purpose postrecognition error corrector poses crucial question: how can we most effectively train model on large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in single model. Previous methods achieve this by having separate correction language models, resulting in significant increase in parameters. In this work, we present Mixture-of-Experts as solution, highlighting that MoEs are much more than scalability tool. We propose Multi-Task Correction MoE, where we train the experts to become an expert of speech-to-text, language-to-text and vision-to-text datasets by learning to route each datasets tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore new stateof-the-art performance by achieving an average relative 5.0% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with 15.5% to 27.6% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as multi-task model."
        },
        {
            "title": "Introduction",
            "content": "Human recognition (Biederman, 1987; Juang and Furui, 2000; Kanwisher et al., 1996) capabilities span multiple modalities, including speech recognition, visual patterns, and extensions to semantic and textual interpretations. These faculties, however, are not infallible and often incorporate misrecognition errors. Despite these imperfections, humans efficiently communicate using speech, language, or facial expressions. For instance, two non-native speakers (Lev-Ari, 2015; Valaki et al., 2004) can often achieve mutual understanding through this imperfect recognition *Work done at NVIDIA research as an intern. 1 Figure 1: Proposed NEKO, new form multi-task model to boost post-recognition results over speech, text, and visual inputs. NEKO could work for (i) post automatic speech recognition (ASR) correction, (ii) post speech translation (ST) and machine translation (MT) correction, and (iii) post optical character recognition (OCR) correction. NeKo discover new state-of-the-art results in (iv) zero-shot ASR correction and performs competitively as general-purpose (v) multi-task corrector. and subsequent interpretative processes, even when the conversation is marred by lexical inaccuracies and subdued accents. In other words, humans (as intelligent agents) exhibit robust capacity for generative understanding (Jiang et al., 2020; Cheng et al., 2021) that extends beyond initial recognition results. In neuroscience (Zatorre and Gandour, 2008), the inferior temporal gyrus and the temporal lobe are not confined to rudimentary perception but are also integral to the post-recognition processes that facilitate semantic understanding of language (Levinson and Evans, 2010), speech (Marshall et al., 2015), and visual patterns (Vink et al., 2020). This form of post-recognition correction, exemplified by the application of language modeling (LM) to initial recognition outputs, has been introduced to the field for both acoustic (automatic speech recognition, ASR) and visual (optical character recognition, OCR) modalities since the early explorations (Jelinek, 1976; Dixon and Silverman, 1975) of learning algorithms in 1970s. The most prevalent approaches to utilizing LMs for post-recognition boosting are predominantly ranking or retrieval-based. In these setups, the LM is tasked with ranking and scoring (Ljolje et al., 1999) the top n-best hypotheses generated by the first-pass recognition system. This process often incorporates discriminative modeling algorithms (Sukkar and Lee, 1996; Mangu et al., 2000) and representation embeddings, such as BERT (Salazar et al., 2020; Kenton and Toutanova, 2019), to minimize the word error rate (WER) during training (Prabhavalkar et al., 2018; Mangu et al., 2000). With the LMs scaling up to LLMs (Brown et al., 2020), recent efforts (Chan et al., 2023; Yang et al., 2023; CHEN et al., 2023; Hu et al., 2024a) have focused on exploring generative modeling for post-recognition correction. This generative error correction (GER) approach uses LLMs to conduct final recognition from given first-pass text-based predictions from recognition models, including ASR, image captioning (IC), and machine translation (MT). This cascaded two-agents text-to-text GER model has outperformed larger single multimodal and multi-task models in these tasks. Meanwhile, these GER solutions heavily depends on domain-specific fine-tuning processes (Chen et al., 2024a) that utilize parameterefficient components, which often suffers performance degradation from lack of generalizability across different datasets, domains, and tasks. In other words, how to design and further openly provide general-purpose post (every) recognition correction model is still one undiscovered and crucial topic within the research community. On the other hand, directly fine-tuning LLMs on mixture of diverse error correction datasets can lead to suboptimal performance (CHEN et al., 2023; Lange et al., 2022) due to differences in input modalities, output formats, error types, and domain characteristics. For example, ASR errors stem from phonetic similarities or acoustic ambiguities, while OCR errors involve visual or character-level confusions. Additionally, error distributions can vary widely across datasets, even within the same task. To characterize model generalization, mixtureof-experts (MoE) (Jiang et al., 2024a) has emerged as promising approach for multi-task learning, consisting of of set of expert networks and gating network that learns to route the input to the most appropriate expert(Sukhbaatar et al., 2024). This enables MoE models to learn more specialized and fine-grained representations compared to monolithic models. However, most MoE models are designed for general-purpose language modeling(Dai et al., 2024), with experts not explicitly assigned to specific tasks, but rather learn to specialize in different aspects of the input space through data-driven training, expect for recent vision work (Ye and Xu, 2023). Effectively leverage MoE for multi-task error correction, where the experts need to capture task-specific features while allowing knowledge sharing, remains an open question. In this work, we propose NEKO, geNErative multi-tasK error cOrrection approach that leverages pre-trained MoE model to drive diverse tasks and cross-domain knowledge, as shown in Figure 1. The key idea is to continuously pretrain MoE model on mixture of error correction datasets, with each expert specializing in specific domain. This task-oriented MoE fine-tuning approach enables the experts to capture task-specific features while allowing knowledge sharing through the router (Dai et al., 2024). NEKO captures the nuances of each task, benefiting from shared knowledge across experts. Evaluated on tasks such as ASR, ST, OCR, and unseen textual error correction (TEC), NEKO consistently outperforms baseline models, including Claude-Opus and GPT-3.5. It achieves state-of-theart WER reduction on the Hyporadise benchmark and large-scale Open ASR Leaderboard (Srivastav et al., 2023). NEKO also significant improves in OCR error correction. Further analysis confirms its robust multi-task capabilities. In summary, the main contributions of this work include: 1. We introduce NEKO, multi-task error correction LLM that leverages task-oriented mixtureof-experts for diverse post-recognition correction tasks. To the best of our knowledge, this is the first work that explores the use of MoE for multi-task error correction. 2. NEKO has been studied under new form of cross-modalities post-recognition correction evaluation, serving as strong open-source ASR, ST, OCR, and TEC baselines. Our results show that NEKO discovers new state-of-the-art performance in ASR as multi-task error correction model. 3. We discovered emergent abilities for cross-task correction from NEKO as first-of-its-kind 2 multi-task correction approach toward generalpurpose post-recognition LM designs. 4. The NEKO models, newly created source datasets, and training processes are scheduled to open source under the CC BY-SA 4.0 license to support reproducibility and to encourage future research."
        },
        {
            "title": "2 Related Work",
            "content": "Language Modeling and Generative Error Correction Neural correction LMs have been widely used for end-to-end (E2E) models for text error correction or normalization for both ASR (Jelinek, 1990; Irie et al., 2016; Zhang et al., 2019a,b; Guo et al., 2019) and OCR (Plamondon and Srihari, 2000; Sabir et al., 2017; Fogel et al., 2020). These models often use beam search to generate new estimates, and can usually handle text normalization and denormalization or spelling errors. To utilize the abundant textual data to improve ASR, neural LMs are integrated to E2E models using shallow fusion or rescoring over acoustic model likelihood confidence (Kannan et al., 2018; Salazar et al., 2020; Yang et al., 2021). In addition to using only textual hypotheses for correction, deliberation models (Hu et al., 2020; Wang et al., 2022; Hu et al., 2023a) utilizes both audio and text for generating new ASR hypotheses. While the aforementioned methods have drawn much attention in the past, recent advances focus on using pretrained textual LLMs to benefit E2E ASR (Song et al., 2023; Hu et al., 2023c), or similar to deliberation, employing both speech and text for speech understanding by prompting (Gong et al., 2023) and joint speech and language foundation models (SLMs) (Wang et al., 2023). On the other hand, text-to-text based GEC (CHEN et al., 2023; Yang et al., 2023; Radhakrishnan et al., 2023; Chan et al., 2023) have been recently introduced to ASR, ST, image captioning, and video summarization to set up superior performance 1. These GEC approaches take the text output as an input to large language models (LLMs), enabling them to perform zero-shot typo correction by generatively refining the final recognition results (CHEN et al., 2023; Yang et al., 2023; Radhakrishnan et al., Figure 2: The architecture of our proposed model, NEKO, which integrates MoE layers within Transformer architecture. During inference, we do not assume knowledge of the specific task an input belongs to and each token is routed to the top-2 experts solely based on their router probabilities. 2023; Chan et al., 2023). In this work, we aim to design text-to-text model as first step for providing open-source post-recognition correction to the communities. Mixture of Experts Mixture-of-experts (MoE) (Shazeer et al., 2017) is machine learning concept that employs multiple expert layers, each of which specializes in solving specific subtask. The experts then work together to solve the entire task at hand. Recently, MoE has been widely applied to large-scale distributed Deep Learning models by using cross-GPU layer that exchanges hidden features from different GPUs (Lepikhin et al., 2021; Fedus et al., 2022). The MoE approach is differentiated from existing scale-up approaches for DNNs, such as increasing the depth or width of DNNs, in terms of its high cost-efficiency. Specifically, adding more model parameters (experts) in MoE layers does not increase the computational cost per token at inference time. Thus, MoE has been studied for scaling the models to trillion-size parameters in NLP (Fedus et al., 2022). In speech processing, Gaur et al. (2021); Hu et al. (2023b); You et al. (2021) primarily showed benefits of MoE on speech recognition tasks. In this work, we further pursue this direction by modeling MoE on error correction and highlight the effectiveness and robustness of MoEs in learning from mixture of correction datasets."
        },
        {
            "title": "3 Method",
            "content": "1GEC have shown better results compared to similar scale of (i) multi-modal injection model and (ii) re-ranking based model, where we will leave these studies in the future upon having one multi-task correction LM as NeKo presented here. Please also"
        },
        {
            "title": "3.1 Mixture-of-Experts (MoE)",
            "content": "Our method, NEKO, is based on Transformer architecture (Vaswani et al., 2017) with modifications similar to those described in Jiang et al. (2023). The 3 key difference is that we replace the feedforward blocks with Mixture-of-Expert (MoE) layers. In MoE layer, each input token is assigned to subset of experts by gating network (router). The output of the MoE layer is the weighted sum of the outputs of the selected experts, where the weights are determined by the gating network. Formally, given expert networks {E0, E1, ..., En1}, the output of the MoE layer for an input token is: being able to generalize to new, potentially unseen tasks and domains during inference."
        },
        {
            "title": "3.3 Training Objective",
            "content": "We train NEKO on mixture of error correction datasets = {D1, D2, ..., Dm}, where each dataset Di corresponds to specific task Ti. The training objective is to minimize the negative loglikelihood of the target sequences: = n1 (cid:88) i=0 G(x)i Ei(x), (1) (cid:88) (cid:88) = i=1 (x,y)Di log p(yx, Ti), (4) where G(x)i is the weight assigned to the i-th expert by the gating network, and Ei(x) is the output of the i-th expert network for input x. The gating network G(x) is implemented as softmax over the top-K logits of linear layer: G(x) = Softmax(TopK(x Wg)), (2) where TopK(ℓ)i = ℓi if ℓi is among the top-K coordinates of logits ℓ Rn, and TopK(ℓ)i = otherwise. The number of experts used per token is hyperparameter that controls the computational cost."
        },
        {
            "title": "3.2 Task-Oriented Expert Assignment",
            "content": "The key idea of NEKO is to assign each expert to specific task during training. Given set of tasks = {T1, T2, ..., Tm}, we define mapping function : {1, 2, ..., n} that assigns each task to unique expert. During training, for an input token from task Ti, we deterministically route to the expert (Ti) in addition to the top-1 expert selected by the gating network. This ensures that each expert learns task-specific features while still allowing for knowledge sharing through the gating network. Formally, the output of the MoE layer for an input token from task Ti during training is: = G(x)f (Ti) Ef (Ti)(x) + G(x)top1 Etop1(x), (3) where top1 = arg maxj=f (Ti) G(x)j is the index of the top-1 expert selected by the gating network, excluding the task-specific expert (Ti). During inference, we do not assume knowledge of the specific task an input token belongs Instead, we route each token to the top-K to. experts selected by the gating network based on their predicted probabilities. This approach allows the model to leverage the task-specific knowledge learned by the experts during training while still where is the input sequence (e.g., ASR hypotheses, OCR output), is the target sequence (e.g., ground-truth transcription, corrected text), and p(yx, Ti) is the probability of the target sequence given the input sequence and the task prompt (Figure 3.) By jointly training on multiple error correction datasets with task-oriented expert assignment, NEKO learns to capture taskspecific features while allowing for knowledge sharing across tasks through the shared gating network and other model components."
        },
        {
            "title": "4.1 Training and Evaluation Datasets",
            "content": "ASR To assess the ability to handle diverse and noisy real-world speech, we use the Open ASR Leaderboard (Gandhi et al., 2022; Srivastav et al., 2023) for ASR evaluation, which comprises nine diverse datasets spanning various domains and speaking styles. These include LibriSpeech (Panayotov et al., 2015), Common Voice 9 (Ardila et al., 2020), VoxPopuli (Wang et al., 2021), TEDLIUM (Hernandez et al., 2018), GigaSpeech (Chen et al., 2021), SPGISpeech (ONeill et al., 2021), Earnings-22 (Del Rio et al., 2022), and AMI (Carletta, 2007; Renals et al., 2007), as one most representative benchmark due to its scale and data diversity. We include the training set of above 8 datasets for NeKo training. We use the word error rate as the evaluation metric for ASR. ST and MT For the speech translation error correction task, we use the subset of the HypoTranslate dataset (Hu et al., 2024b) for training and evaluation. This dataset includes translation results from FLEURS (Conneau et al., 2022), CoVoST-2 (Wang et al., 2020), and MuST-C (Di Gangi et al., 2019), covering range of languages such as Spanish, French, Italian, Japanese, Portuguese, Chinese, and 4 Persian. As an extra zero-shot textual correction setup, we evaluate NEKO on machine translation (MT) of WMT20 for Japanese and Chinese (Barrault et al., 2020a). We use the BLEU score (Papineni et al., 2002) as the evaluation metric for ST (with training and test) and MT (zero-shot). OCR For the optical character recognition (OCR) error correction task, we use the English portion of the Post-OCR Correction dataset (PleIAs, 2023), which contains newspaper texts from Chronicling America. The dataset includes original texts with varying numbers of OCR mistakes and their corresponding corrected versions. To evaluate our model, we take the first 1,000 characters of both the input text with OCR errors and the ground-truth corrected text. We use the WER as the evaluation metric. TEC For the textual error correction (TEC) task, we use subset of the CoEdIT dataset (Raheja et al., 2023), which contains diverse collection of 82K task-specific instructions for text editing. We select two editing tasks from CoEdIT that align with our error correction objectives: grammar correction and coherence improvement. These tasks focus on correcting grammatical errors and improving the overall coherence of the text, making them suitable for evaluating the effectiveness of our model in handling TEC-related editing instructions. We use the word error rate as the evaluation metric."
        },
        {
            "title": "Baselines",
            "content": "et al., 2023), ASR We compare against state-of-the-art ASR Distil-Whisper-V2-Large(Gandhi models, Whisper-V2-Large, et Whisper-V3-Large(Radford 2022), al., Canary(NVIDIA, 2024) without applying GEC method. end-to-end ASR-LLM, SALM (Chen et al., 2024b), is also compared. For all Cascaded ASR+GEC Methods, the task-specific system is the Canary model. This model transcribes the speech data and generate 5-best hypotheses for each utterance using temperature-based sampling (Ackley et al., 1985) with = 0.3. This allows us to capture diverse set of potential transcriptions for each utterance, which can then be fed into our error correction model. GenTranslate(Hu et al., 2024c), and cascaded approaches combining ASR and machine translation models (e.g., Whisper + NLLB (Costa-jussà et al., 2022)). These baselines cover both end-to-end speech translation models and pipeline approaches. We use SeamlessM4T-Large V2 as the task-specific system to decode -best hypotheses from input speech by beam search algorithm. We did this in two steps by first transcribing the speech and then translating the text, following (Hu et al., 2024c). LLMs then take the N-best hypotheses to produce final speech translation result. To investigate the generalization of our model, we also evaluate it in an alternative scenario: direct speech translation model, Canary, is used as the task-specific system to produce hypotheses. OCR and TEC We compare our proposed method against two baselines: (1) the input text without any correction (denoted as Baseline) and (2) Mixtral 8x7B model fine-tuned only on the respective dataset for each task (denoted as Mistral 8x7B Direct Finetune). This allows us to assess the effectiveness of our task-oriented expert assignment approach in handling OCR and TEC errors, as well as its ability to leverage knowledge from multiple tasks to improve performance on individual tasks compared to direct fine-tuning on single dataset."
        },
        {
            "title": "4.3 Post-recognition LLMs Setup",
            "content": "We implement NEKO using the Transformer architecture (Vaswani et al., 2017) and fine-tune both dense and MoE models for comparison. For dense models, we fine-tune Gemma 2B (Team et al., 2024) and Mistral 7B (Jiang et al., 2024b). For MoE models, we fine-tune Gemma 8x2B2 and Mixtral 8x7B without applying our task-oriented expert assignment. We explore the Branch-Train-Mix approach (Sukhbaatar et al., 2024), which involves branching from the Mistral 7B model to an 8x7B MoE model as one competing setup. To investigate the scalability of our method, we design NEKO to three different sizes of MoE models: Gemma 8x2B, Mixtral 8x7B, and Mixtral 8x22B. We further compared low-rank adaptation (LoRA(Hu et al., 2021)) with full fine-tuning (FFT) on 8x7B MoE setup. For MoE models, we use top-k routing as proposed in (Lepikhin et al., 2021) to balance the ST and MT For the speech and machine translation tasks, we compare against state-of-theart models SeamlessM4T (Barrault et al., 2023a), 2We made an up-cycled (Komatsuzaki et al., 2023) Gemma 8x2B MoE setup extended from single Gemma-2B (Team et al., 2024). 5 Table 1: Cross-domain ASR correction results in zero-shot and few-shot settings on the Hyporadise benchmark (CHEN et al., 2023). We compare NEKO against GPT-3.5 Turbo and Claude-Opus in 0and 5-shot settings. The baseline represents the WER of task-specific model Whisper-Large. The oracle results used in CHEN et al. (2023) (N-best and Compositional) provide an upper bound for the correction performance."
        },
        {
            "title": "Baseline",
            "content": "GPT-3.5 Turbo Claude-Opus 0-shot w/ NEKO"
        },
        {
            "title": "Oracle",
            "content": "0-shot 5-shot 0-shot 5-shot NEKO-FFT NEKO-BTX NEKO-MoE N-best Comp. WSJ-dev93 WSJ-eval92 ATIS CHiME4-bus CHiME4-caf CHiME4-ped CHiME4-str MCV-af MCV-au MCV-in MCV-sg 9.0 7.6 5.8 18.8 16.1 11.5 11.4 25.3 25.8 28.6 26.4 8.55.6% 7.33.9% 5.55.2% 7.714.4% 6.613.2% 5.013.8% 8.28.9% 7.07.9% 5.210.3% 7.417.8% 6.317.1% 4.719.0% 17.19.0% 15.716.5% 17.66.4% 16.213.8% 14.78.7% 13.714.9% 14.211.8% 13.218.0% 9.319.1% 10.95.2% 9.318.4% 10.94.4% 10.58.7% 10.57.9% 9.715.7% 9.714.9% 23.66.7% 24.91.6% 24.07.0% 25.12.7% 27.63.5% 25.012.6% 25.14.9% 26.5+0.4% 24.43.6% 24.64.7% 27.05.6% 25.91.9% 23.09.1% 23.49.3% 24.315.0% 24.57.2% 8.64.4% 7.42.6% 5.63.4% 17.75.9% 14.88.1% 11.04.3% 11.03.5% 25.01.2% 25.22.3% 27.82.8% 26.6+0.8% 7.516.7% 6.415.8% 4.817.2% 15.915.4% 13.416.8% 9.517.4% 9.417.5% 23.37.9% 23.78.1% 24.614.0% 24.76.4% 6.824.4% 5.823.7% 4.227.6% 14.522.9% 12.224.2% 8.625.2% 8.525.4% 21.017.0% 21.417.1% 22.222.4% 22.315.5% 6.5 5.5 3.5 16.8 13.3 8.5 9.0 23.6 24.9 27.1 25.5 5.3 4.7 2.4 10.7 9.1 5.5 6. 21.7 21.8 22.6 22.2 Table 2: ASR correction results on the Open ASR Leaderboard. We report the Word Error Rate (WER) for each dataset and the average across all 9 datasets. NEKO establishes new state-of-the-art performance on the leaderboard, outperforming both end-to-end ASR methods and cascaded ASR+GEC approaches. Lower WER indicates better performance. Model AMI Earnings22 Gigaspeech LS Clean LS Other SPGI Tedlium Voxp. MCV9 Average End-to-end ASR Methods Distil-Whisper-V2-Large (Gandhi et al., 2023) Whisper-V2-Large (Radford et al., 2022) Whisper-V3-Large Canary (NVIDIA, 2024) Bestow Speech LM (Chen et al., 2024c) Cascaded ASR+GEC Methods Gemma 2B (Team et al., 2024) FFT Gemma 8x2B FFT NEKO (Ours) Gemma 8x2B Mistral 7B (Jiang et al., 2023) FFT Mixtral 8x7B (Jiang et al., 2024b) FFT Mixtral 8x7B Lora Mistral 8x7B Branch-Train-Mix (Sukhbaatar et al., 2024) NEKO (Ours) Mixtral 8x7B NEKO (Ours) Mixtral 8x22B 14.65 16.82 16.01 14.00 12.58 13.20 13.10 13.00 13.07 12.91 12.96 13.13 12.55 12.61 12.12 12.02 11.30 12.25 12. 12.30 12.20 12.10 11.87 12.19 12.24 11.93 11.82 11.93 10.31 10.57 10.02 10.19 10.06 10.40 10.30 10.20 10.09 10.34 10.38 10.14 10.02 10.15 2.95 2.56 2.03 1.49 1. 1.60 1.50 1.40 1.48 1.54 1.55 1.49 1.49 1.52 6.39 5.16 3.91 2.49 3.07 2.60 2.50 2.40 2.46 2.55 2.56 2.47 2.47 2.51 3.28 3.77 2.95 2.06 2. 2.20 2.10 2.00 2.04 2.12 2.13 2.05 2.05 2.09 4.30 4.01 3.90 3.58 3.41 3.70 3.60 3.50 3.55 3.64 3.66 3.57 3.52 3.58 8.22 7.50 9.52 5.81 5. 6.00 5.90 5.80 5.75 5.89 5.92 5.78 5.76 5.82 12.60 10.11 9.67 7.75 6.97 7.50 7.40 7.30 7.29 7.43 7.47 7.33 7.25 7.33 8.31 8.06 7.70 6.67 6. 6.61 6.51 6.41 6.40 6.51 6.60 6.43 6.34 6.40 computational cost and model capacity. We use global batch size of 2 million tokens and apply sample packing (Raffel et al., 2020) to maximize the GPU utilization. We fine-tune the model for 3 epochs using the AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate of 1e 4 and weight decay of 0.01. We use cosine learning rate scheduler with warmup ratio of 0.1 and gradient clipping threshold of 1.0. For the expertdataset mapping, we randomly assign each dataset to one of the 8 experts in the Mixtral model. This random assignment serves as strong baseline and allows us to focus on the effectiveness of the taskoriented expert assignment approach. We leave the exploration of more advanced expert assignment strategies for future work. To efficiently train the large-scale model, we leverage DeepSpeed Zero (Rajbhandari et al., 2020) for memory optimization and Hugging Face Transformers (Wolf et al., 2020) for model implementation."
        },
        {
            "title": "Results",
            "content": "ASR We first evaluate the zero-shot ability of NEKO on unseen domain compared to two generalpurpose LLMs, including GPT-3.5 Turbo and Claude-Opus3. With task-specific recognition baseline of Whisper-V2-Large (third column) in Table 1, NEKO-MoE shows the best zero-shot ability with relative 22.3% average WER reduction. GPT-3.5 Turbo and Claude-Opus have relative 4.3% and 7.3% of zero-shot improvements, where NEKO consistently outperform their 5-shot ASR correction. Table 2 shows the WER scores on individual datasets and average performance on the Open ASR Leaderboard. We observe that the proposed 3Claude-Opus is one of the high-performance, generalpurpose LLMs evaluated in the LLM evaluation (Zheng et al., 2024) and performs competitively compared to GPT-4-Turbo and Gemini 1.5-Pro, offering unlimited API access. 6 Table 3: Speech translation results on FLEURS, CoVoST-2, and MuST-C EnX test sets in terms of BLEU score.We use bold to highlight surpassing SeamlessM4T baseline, and use underline to highlight the state-of-the-art performance. The baseline methods are introduced in 4.2, and all of their results are reproduced by ourselves. EnX Es Fr It FLEURS Ja Pt Zh Avg. Fa CoVoST-2 Ja Zh Avg. End-to-end ST Methods SeamlessM4T-Large (Barrault et al., 2023a) GenTranslate (Hu et al., 2024c) SeamlessM4T-Large-V2 (Barrault et al., 2023b) GenTranslate-V2 (Hu et al., 2024c) Cascaded ASR+MT Methods Whisper + NLLB-3.3b (Costa-jussà et al., 2022) SeamlessM4T-Large (ASR+MT) (Barrault et al., 2023a) SeamlessM4T-V2 (ASR+MT) (Barrault et al., 2023b) Cascaded ASR+GEC Methods GenTranslate GenTranslate-V2 NEKO-FT NEKO-BTX NEKO-MoE 23.8 25.4 23.8 25.5 25.1 24.6 24.7 26.8 27.0 26.9 27.2 28.5 41.6 43.1 42.6 44.0 41.3 44.6 44.1 45.0 44.3 44.2 44.5 46. 23.9 25.5 24.5 26.3 25.0 25.4 25.1 26.6 26.4 26.3 26.7 28.0 21.0 28.3 21.7 28.9 19.0 22.5 20.6 29.4 27.8 27.7 28.0 30. 40.8 42.4 43.0 44.5 41.5 41.9 43.6 43.1 44.5 44.4 44.7 46.3 28.6 34.3 29.5 34.9 23.5 31.2 30.6 36.8 36.1 36.0 36.3 38. 30.0 33.2 30.9 34.0 29.2 31.7 31.5 34.6 34.4 34.3 34.6 36.3 18.3 21.1 16.9 19.4 13.6 18.8 17.4 21.8 20.8 20.7 21.0 23. 24.0 29.1 23.5 29.0 19.0 24.0 23.8 30.5 29.7 29.6 29.9 32.6 34.1 42.8 34.6 43.6 32.0 35.1 35.4 43.3 43.5 43.4 43.8 46. 25.5 31.0 25.0 30.7 21.5 26.0 25.5 31.9 31.3 31.2 31.6 34.2 MuST-C It Zh Avg. 29.9 29.4 27.5 27. 29.9 30.8 27.8 31.0 28.3 28.2 28.5 32.8 16.2 18.5 15.6 18.1 13.5 17.7 14.5 19.6 16.9 16.8 17.1 21.5 26.8 27.3 25.1 25. 26.2 27.9 25.1 28.7 26.1 26.0 26.3 30.5 Es 34.2 33.9 32.1 32.2 35.3 35.1 33.0 35.5 33.2 33.1 33.4 37. NEKO improves the task-specific baseline Canary, with an average 5.0% WER reduction. Individually, we observe significant performance increase with NEKO on more challenging datasets, like AMI (conversational speech) and VoxPopuli (accented speech) due to experts learning dataset-specific features. While, Earnings22 shows slight performance drop possibly due to the reduced representation in the batch. Compared to other leading models on the leaderboard, NEKO establishes new state-of-the-art, outperforming speech-only foundational models like Whisper and Canary and end-to-end ASRLLM like SALM 4 (Chen et al., 2024b) across most datasets. On the AMI dataset, NEKO achieves WER of 12.58%, significantly lower than Whispers 16.82%. On VoxPopuli, NEKO obtains 5.84% WER, 1.66 point reduction from Whispers 7.5%. The strong performance of NEKO demonstrates the effectiveness of our speech-adapted MoE approach in handling diverse speech datasets and learning robust representations. ST and MT Table 3 presents the speech translation results on the FLEURS, CoVoST-2, and MuST-C datasets. For these experiments, we use SeamlessM4T-Large as the task-specific model to generate the initial speech translation hypotheses. NEKO is then applied to correct the outputs from SeamlessM4T-Large. Compared to the task-specific SeamlessM4T-Large model, NEKO achieves significant improvements, with an average 4We conduct one competitive end-to-end, multi-modal baseline based on Bestow (Chen et al., 2024c) with joint audio and text injection under cross-attention, which shows improved performance on the Open ASR benchmark. BLEU score increase of 5.4 points on the FLEURS dataset, 9.2 points on the CoVoST-2 dataset, and 5.4 points on the MuST-C dataset. These results demonstrate the effectiveness of NEKO in correcting errors made by the first-pass speech translation model. Moreover, NEKO outperforms other correction baselines, including the state-of-the-art GenTranslate model. On the FLEURS dataset, NEKO obtains an average BLEU score of 36.3, surpassing GenTranslate by 2.3 points. For CoVoST-2, NEKO achieves an average BLEU score of 34.2, outperforming GenTranslate by 3.2 points. On MuST-C, NEKO improves over GenTranslate by 3.2 BLEU points, reaching an average score of 30.5. Table 4: Machine translation BLEU scores on the WMT20 Japanese (Ja) and Chinese (Zh) test sets (Barrault et al., 2020b). NEKO is evaluated in zero-shot setting, while other models are fine-tuned on the respective language pairs. Higher BLEU scores indicate better translation quality. EnX WMT20 Ja WMT20 Zh Avg. ALMA-13b BigTranslate NLLB-3.3b SeamlessM4T-Large GenTranslate (fine-tuned) NEKO-MoE (zero-shot) 3.5 7.3 11.6 17.0 21.4 18.1 11.3 29.0 26.9 27.0 30.7 27.6 7.4 18.2 19.3 22.0 26.1 22. To further assess the generalization ability of NEKO , we evaluate it on the WMT20 machine translation benchmark for Japanese and Chinese in zero-shot setting. As shown in Table 4, NEKO achieves competitive performance compared to fine-tuned MT models, obtaining an average BLEU 7 score of 22.9. This result highlights the potential of NEKO to handle unseen translation tasks by leveraging the knowledge learned from speech translation datasets. OCR and TEC For the OCR task, NEKO achieves substantial error reduction, lowering the WER from 71.03% to 14.43%. This represents significant improvement over the baseline and demonstrates the models ability to correct OCR errors effectively. Compared to the MixtralMoE model fine-tuned directly on the OCR dataset, NEKO obtains 1.02% lower WER, highlighting the benefit of the task-oriented expert assignment approach. In the TEC task, NEKO showcases its versatility by improving the performance on both grammar correction and coherence improvement subtasks. For grammar correction, NEKO reduces the WER from 31.41% to 9.42%, outperforming the directly fine-tuned Mixtral-MoE model by 1.31%. On the coherence subtask, NEKO achieves WER of 9.71%, which is 0.46% higher than the directly fine-tuned model but still significant improvement over the baseline. Table 5: WER comparison of NEKO against the baseline and directly fine-tuned Mixtral-MoE model (8x7B) on grammar correction and coherence improvement tasks from the CoEdIT dataset (Raheja et al., 2023), and the OCR task using the PleIAs/Post-OCR-Correction dataset (PleIAs, 2023). Lower WER indicates better performance."
        },
        {
            "title": "Task",
            "content": "Grammar Correction Coherence Improv. OCR Baseline Mixtral-MoE-FFT NEKO-MoE 31.41 10.73 9.42 13.48 9.25 9.71 71.03 15.45 14.43 Table 6: WER comparison of NEKO against GPT-3.5Turbo, and Claude-Opus on the 5-shot IMDb typographical error correction dataset (Shah and de Melo, 2020). The baseline represents the WER between the corrupted text and the ground truth. Lower WER indicates better performance in correcting typographical errors."
        },
        {
            "title": "Model",
            "content": "Baseline (Corrupt vs Ground Truth) GPT-3.5-Turbo (5-shots) Claude-Opus (5-shots) NEKO-MoE (5-shots)"
        },
        {
            "title": "WER",
            "content": "18.35% 12.72% 8.18% 11.62% Emergent Unseen Task Zero-Shot Performance We investigate NEKOs generalization capabilities to unseen tasks using an additional synthetic typographical error correction dataset (Shah and de Melo, 2020). This dataset is derived from the IMDb test split, featured low noise levels (3.75% character error rate) with corruption applied using algorithms proposed in (Shah and de Melo, 2020). Our evaluation focused on zero-shot and five-shot learning scenarios to assess the adaptability of various models without and with minimal task-specific training. In the zero-shot scenario, where models were prompted to switch from an ASR task to typo correction without additional training, the challenge proved significant. The models, including the advanced Claude-Opus, yielded WERs above 30%. The predictions were markedly irrelevant to the ground truth, highlighting the difficulty of adapting to typo correction without specific finetuning. This finding prompts further investigation into efficient and effective training techniques for generalizing model capabilities across diverse linguistic tasks. In the five-shot scenario, all models improved against the corrupted baseline with Claude-Opus performing best. Notably, NEKO outperformed GPT-3.5-Turbo, indicating some affinity towards this task."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we proposed NEKO, multi-task GER approach that leverages task-oriented MoEs to handle diverse tasks. NEKO assigns each expert to specific dataset during training, enabling the experts to capture task-specific features while allowing knowledge sharing through the gating network. Our results show that task-oriented expert assignment is promising approach for multi-task learning in error correction and other natural language processing tasks. By aligning experts with datasets, NEKO can effectively capture the nuances and specificities of each task while benefiting from the shared knowledge learned by the gating network and other model components. Future work includes exploring more advanced expert assignment strategies, such as dynamically assigning experts based on the input characteristics. Investigating the interpretability of the learned expert representations and routing decisions would be important next step upon the open NEKO models."
        },
        {
            "title": "Limitation",
            "content": "We aim to provide transparent and comprehensive understanding of the current scope of NEKO, and pave the way for future research to further improve the NEKO model. Dataset Diversity and Size and Assumptions in Error Distribution This study addresses mixture of error correction tasks, including ASR, ST, OCR, and TEC, using representative task-specific datasets such as LibriSpeech for ASR, CoVoST for ST, ICDAR 2019 for OCR, and CoNLL-2014 for TEC. While these datasets are widely recognized benchmarks, they may not cover all possible error correction scenarios, particularly those involving more complex or less common error types found in real-world data. This setup assumes that the error distributions in the training datasets are representative of those in real-world applications. Consequently, the performance of NEKOmight be overestimated for certain types of data not covered by these benchmarks, affecting the generalizability of the results to more diverse and noisy realworld scenarios. Future research should include broader range of datasets, particularly those with more diverse and challenging error types, and investigate methods to dynamically adapt to varying error distributions, possibly through online learning (Yasunaga et al., 2021) or domain adaptation techniques (Khurana et al., 2021), to better evaluate the robustness and generalizability of the model. Ethical and Societal Considerations The study does not extensively address the ethical and societal implications of deploying NEKO in real-world applications. There could be unintended consequences, such as biases in error correction or misuse of the technology in sensitive applications. Future work should include thorough analysis of the ethical and societal impacts of the model, along with strategies to mitigate potential negative consequences. This could involve incorporating fairness and bias detection mechanisms (Liu et al., 2022) into the model to ensure responsible and ethical deployment. Boarder Impacts The NEKO models application of MoE for multi-domain and multi-task error correction has the potential to significantly enhance automated systems performance across various domains, such as healthcare, education and customer service. By improving standard mediums of communication such as speech recognition, translation and optical character recognition NEKO can facilitate more inclusive technologies, benefiting individuals with impairments or non-native speakers. Additionally, the economic benefits from reduced manual correction efforts and educational advantages from more accurate communication system can be substantial. The open-sourcing of NEKO under the CC BY-SA 4.0 license encourages collaboration and reproducibility with in the reserach community, fostering innovation and broader application. Future work should also consider optimizing the training process to minimize the environmental impact, promoting sustainable AI development practices. Task-Specific Fine-Tuning The NEKO model employs task-oriented MoE fine-tuning, where each expert is assigned to specific dataset. This approach may lead to overfitting to the specific characteristics of the training datasets even though knowledge could be shared. As result, the models performance might degrade when applied to new tasks or datasets that were not part of the training set, limiting its adaptability. Investigating more dynamic and adaptive fine-tuning strategies that can generalize better across unseen tasks and datasets would be beneficial. Techniques such as meta-learning or continual learning could be explored to enhance the models adaptability and robustness. Future Connections to In-Context and AutoAgent Learning with NEKO Integrating incontext learning (ICL) with NEKO could enable the model to adapt to various error correction tasks by conditioning on input examples without requiring explicit fine-tuning. This approach is particularly beneficial in scenarios where obtaining large labeled datasets for fine-tuning is impractical. By leveraging ICL, NEKO could adapt to diverse error types and use in-context examples to correct errors specific to new domains or applications, thereby improving its generalizability to real-world data. Furthermore, ICL would allow the model to dynamically adjust its error correction strategies based on the input context, enhancing its robustness to varying error distributions."
        },
        {
            "title": "References",
            "content": "David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. 1985. learning algorithm for boltzmann machines. Cogn. Sci., 9(1):147169. 9 Alëna Aksënova, Daan van Esch, James Flynn, and Pavel Golik. 2021. How might we create better benchmarks for speech recognition? In Proceedings of the 1st workshop on benchmarking: Past, present and future, pages 2234. Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. 2020. Common voice: massivelymultilingual speech corpus. In Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020, Marseille, France, May 11-16, 2020, pages 42184222. European Language Resources Association. Loïc Barrault, Magdalena Biesialska, Ondˇrej Bojar, Marta R. Costa-jussà, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubešic, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020a. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, pages 155, Online. Association for Computational Linguistics. Loïc Barrault, Magdalena Biesialska, Ondrej Bojar, Marta R. Costa-jussà, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubesic, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020b. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, WMT@EMNLP 2020, Online, November 19-20, 2020, pages 155. Association for Computational Linguistics. Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. 2023a. Seamlessm4t-massively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596. Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et al. 2023b. Seamless: Multilingual expressive and streaming speech translation. arXiv preprint arXiv:2312.05187. Irving Biederman. 1987. Recognition-by-components: theory of human image understanding. Psychological review, 94(2):115. Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. ArXiv, abs/2005.14165. Jean Carletta. 2007. Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus. Language Resources and Evaluation, 41(2):181190. David Chan, Austin Myers, Sudheendra Vijayanarasimhan, David Ross, and John Canny. 2023. Ic3: Image captioning by committee consensus. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8975 9003. William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. 2016. Listen, attend and spell: neural network for large vocabulary conversational speech recognition. In 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 49604964. IEEE. CHEN CHEN, Yuchen Hu, Chao-Han Huck Yang, Sabato Marco Siniscalchi, Pin-Yu Chen, and Ensiong Chng. 2023. Hyporadise: An open baseline for generative speech recognition with large language models. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Chen Chen, Ruizhe Li, Yuchen Hu, Sabato Marco Siniscalchi, Pin-Yu Chen, Ensiong Chng, and ChaoIts never too late: FusHan Huck Yang. 2024a. ing acoustic information into large language models for automatic speech recognition. arXiv preprint arXiv:2402.05457. Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Yujun Wang, Zhao You, and Zhiyong Yan. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. arXiv eprints, arXiv:2106.06909. Zhehuai Chen, He Huang, Andrei Andrusenko, Oleksii Hrinchuk, Krishna Puvvada, Jason Li, Subhankar Ghosh, Jagadeesh Balam, and Boris Ginsburg. 2024b. Salm: Speech-augmented language model with incontext learning for speech recognition and translation. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1352113525. IEEE. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Zhehuai Chen, He Huang, Oleksii Hrinchuk, Krishna Puvvada, Nithin Rao Koluguri, Piotr Zelasko, Jagadeesh Balam, and Boris Ginsburg. 2024c. Bestow: Efficient and streamable speech language model with 10 the best of two worlds in gpt and t5. arXiv preprint arXiv:2406.19954. Lauretta SP Cheng, Danielle Burgess, Natasha Vernooij, Cecilia Solís-Barroso, Ashley McDermott, and Savithry Namboodiripad. 2021. The problematic concept of native speaker in psycholinguistics: Replacing vague and harmful terminology with inclusive and accurate measures. Frontiers in psychology, 12:715843. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2022. FLEURS: few-shot learning evaluation of universal representations of speech. In IEEE Spoken Language Technology Workshop, SLT 2022, Doha, Qatar, January 9-12, 2023, pages 798805. IEEE. Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Y. Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. CoRR, abs/2207.04672. Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. 2024. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. CoRR, abs/2401.06066. Miguel Del Rio, Peter Ha, Quinten McNamara, Corey Miller, and Shipra Chandra. 2022. Earnings-22: Practical Benchmark for Accents in the Wild. arXiv e-prints, arXiv:2203.15591. Li Deng, Jinyu Li, Jui-Ting Huang, Kaisheng Yao, Dong Yu, Frank Seide, Michael Seltzer, Geoff Zweig, Xiaodong He, Jason Williams, et al. 2013. Recent advances in deep learning for speech research at microsoft. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 8604 8608. IEEE. Mattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019. MuST-C: Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 20122017, Minneapolis, Minnesota. Association for Computational Linguistics. Dixon and Silverman. 1975. description of parametrically controlled modular structure for speech processing. IEEE Transactions on Acoustics, Speech, and Signal Processing, 23(1):8791. Jun Du, Yan-Hui Tu, Lei Sun, Feng Ma, Hai-Kun Wang, Jia Pan, Cong Liu, Jing-Dong Chen, and Chin-Hui Lee. 2016. The ustc-iflytek system for chime-4 challenge. Proc. CHiME, 4(1):3638. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23:120:1120:39. Sharon Fogel, Hadar Averbuch-Elor, Sarel Cohen, Shai Mazor, and Roee Litman. 2020. Scrabblegan: Semisupervised varying length handwritten text generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 43244333. Sanchit Gandhi, Patrick von Platen, and Alexander M. Rush. 2022. ESB: benchmark for multiCoRR, domain end-to-end speech recognition. abs/2210.13352. Sanchit Gandhi, Patrick von Platen, and Alexander Rush. 2023. Distil-whisper: Robust knowledge distillation via large-scale pseudo labelling. arXiv preprint arXiv:2311.00430. Neeraj Gaur, Brian Farris, Parisa Haghani, Isabel Leal, Pedro J. Moreno, Manasa Prasad, Bhuvana Ramabhadran, and Yun Zhu. 2021. Mixture of informed experts for multilingual speech recognition. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021, pages 62346238. IEEE. Yuan Gong, Alexander Liu, Hongyin Luo, Leonid Karlinsky, and James Glass. 2023. Joint audio and In 2023 IEEE Automatic speech understanding. Speech Recognition and Understanding Workshop (ASRU), pages 18. IEEE. Jinxi Guo, Tara Sainath, and Ron Weiss. 2019. spelling correction model for end-to-end speech recognition. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 56515655. IEEE. Yanzhang He, Tara Sainath, Rohit Prabhavalkar, Ian McGraw, Raziel Alvarez, Ding Zhao, David Rybach, Anjuli Kannan, Yonghui Wu, Ruoming Pang, et al. 2019. Streaming end-to-end speech recognition for mobile devices. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 63816385. IEEE. François Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko, and Yannick Estève. 2018. TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation. In Speech and Computer, pages 198208. Springer International Publishing. 11 Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Ke Hu, Bo Li, and Tara Sainath. 2023a. Scaling up deliberation for multilingual asr. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 771776. IEEE. Ke Hu, Bo Li, Tara N. Sainath, Yu Zhang, and Françoise Beaufays. 2023b. Mixture-of-expert conformer for streaming multilingual ASR. CoRR, abs/2305.15663. Ke Hu, Tara Sainath, Bo Li, Nan Du, Yanping Huang, Andrew Dai, Yu Zhang, Rodrigo Cabrera, Zhifeng Chen, and Trevor Strohman. 2023c. Massively multilingual shallow fusion with large language models. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE. Ke Hu, Tara Sainath, Ruoming Pang, and Rohit Prabhavalkar. 2020. Deliberation model based two-pass In ICASSP 2020end-to-end speech recognition. 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7799 7803. IEEE. Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Chao Zhang, Pin-Yu Chen, and EnSiong Chng. 2024a. Large language models are efficient learners of noise-robust speech recognition. arXiv preprint arXiv:2401.10446. Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen, and Eng Siong Chng. 2024b. Gentranslate: Large language models are generative multilingual speech and machine translators. CoRR, abs/2402.06894. Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen, and Eng Siong Chng. 2024c. Gentranslate: Large language models are generative multilingual speech and machine translators. arXiv preprint arXiv:2402.06894. Kazuki Irie, Zoltán Tüske, Tamer Alkhouli, Ralf Schlüter, Hermann Ney, et al. 2016. Lstm, gru, highway and bit of attention: An empirical overview for language modeling in speech recognition. In Interspeech, pages 35193523. Fred Jelinek. 1990. Self-organized language modeling for speech recognition. Readings in speech recognition, pages 450506. Frederick Jelinek. 1976. Continuous speech recognition by statistical methods. Proceedings of the IEEE, 64(4):532556. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024a. Mixtral of experts. CoRR, abs/2401.04088. Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024b. Mixtral of experts. ArXiv, abs/2401.04088. Xiaoming Jiang, Kira Gossack-Keenan, and Marc Pell. 2020. To believe or not to believe? how voice and accent information in speech alter listener impressions of trust. Quarterly Journal of Experimental Psychology, 73(1):5579. Bing-Hwang Juang and Sadaoki Furui. 2000. Automatic recognition and understanding of spoken language-a first step toward natural humanmachine communication. Proceedings of the IEEE, 88(8):11421165. Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara Sainath, Zhijeng Chen, and Rohit Prabhavalkar. 2018. An analysis of incorporating an external language model into sequence-to-sequence model. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15828. IEEE. Nancy Kanwisher, Marvin Chun, Josh McDermott, and Patrick Ledden. 1996. Functional imaging of human visual recognition. Cognitive Brain Research, 5(1-2):5567. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1, page 2. Sameer Khurana, Niko Moritz, Takaaki Hori, and Jonathan Le Roux. 2021. Unsupervised domain adaptation for speech recognition via uncertainty driven self-training. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 65536557. IEEE. Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. 2023. Sparse upcycling: Training mixture-of-experts from 12 dense checkpoints. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. 2022. continual learning survey: Defying forgetting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):33663385. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. Gshard: Scaling giant models with conditional computation and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Shiri Lev-Ari. 2015. Comprehending non-native speakers: Theory and evidence for adjustment in manner of processing. Frontiers in psychology, 5:111794. Stephen Levinson and Nicholas Evans. 2010. Time for sea-change in linguistics: Response to comments on the myth of language universals. Lingua, 120(12):27332758. Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, and Soroush Vosoughi. 2022. Quantifying and alleviating political bias in language models. Artificial Intelligence, 304:103654. Andrej Ljolje, Fernando Pereira, and Michael Riley. 1999. Efficient general lattice generation and rescoring. In Sixth European Conference on Speech Communication and Technology. Ilya Loshchilov and Frank Hutter. 2019. Decoupled In 7th International weight decay regularization. Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000. Finding consensus in speech recognition: word error minimization and other applications of confusion networks. Computer Speech & Language, 14(4):373 400. Chloë Marshall, Anna Jones, Tanya Denmark, Kathryn Mason, Joanna Atkinson, Nicola Botting, and Gary Morgan. 2015. Deaf childrens non-verbal working memory is impacted by their language experience. Frontiers in psychology, 6:527. NVIDIA. 2024. New standard for speech recognition and translation from the nvidia nemo canary model. Accessed: 2024-05-20. Patrick K. ONeill, Vitaly Lavrukhin, Somshubra Majumdar, Vahid Noroozi, Yuekai Zhang, Oleksii Kuchaiev, Jagadeesh Balam, Yuliya Dovzhenko, Keenan Freyberg, Michael D. Shulman, Boris Ginsburg, Shinji Watanabe, and Georg Kucsko. 2021. SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition. In Proc. Interspeech 2021, pages 1434 1438. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An ASR corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311318. ACL. Réjean Plamondon and Sargur Srihari. 2000. Online and off-line handwriting recognition: comprehensive survey. IEEE Transactions on pattern analysis and machine intelligence, 22(1):6384. PleIAs. 2023. Post-ocr-correction. Rohit Prabhavalkar, Tara Sainath, Yonghui Wu, Patrick Nguyen, Zhifeng Chen, Chung-Cheng Chiu, and Anjuli Kannan. 2018. Minimum word error rate training for attention-based sequence-to-sequence models. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 48394843. IEEE. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust Speech Recognition via Large-Scale Weak Supervision. Technical report, OpenAI. Srijith Radhakrishnan, Chao-Han Yang, Sumeer Khan, Rohit Kumar, Narsis Kiani, David Gomez-Cabrero, and Jesper Tegnér. 2023. Whispering llama: cross-modal generative error correction framework for speech recognition. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1000710016. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67. Vipul Raheja, Dhruv Kumar, Ryan Koo, and Dongyeop Kang. 2023. Coedit: Text editing by task-specific instruction tuning. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, page 20. IEEE/ACM. 13 Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Speechbrain: Jianyuan Zhong, et al. 2021. arXiv preprint general-purpose speech toolkit. arXiv:2106.04624. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295. Steve Renals, Thomas Hain, and Herve Bourlard. 2007. Recognition and understanding of meetings the AMI and AMIDA projects. In 2007 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 238247. Ekraam Sabir, Stephen Rawls, and Prem Natarajan. 2017. Implicit language model in lstm for ocr. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 7, pages 2731. IEEE. Julian Salazar, Davis Liang, Toan Nguyen, and Katrin Kirchhoff. 2020. Masked language model scoring. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 26992712. Kshitij Shah and Gerard de Melo. 2020. Correcting the autocorrect: Context-aware typographical error correction via training data augmentation. In Proceedings of the 12th Language Resources and Evaluation Conference (LREC 2020), Paris, France. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Gan Song, Zelin Wu, Golan Pundak, Angad Chandorkar, Kandarp Joshi, Xavier Velez, Diamantino Caseiro, Ben Haynor, Weiran Wang, Nikhil Siddhartha, et al. 2023. Contextual spelling correction with large lanIn 2023 IEEE Automatic Speech guage models. Recognition and Understanding Workshop (ASRU), pages 18. IEEE. CE Valaki, Maestu, PG Simos, Zhang, Fernandez, CM Amo, TM Ortiz, and AC Papanicolaou. 2004. Cortical organization for receptive language functions in chinese, english, and spanish: crosslinguistic meg study. Neuropsychologia, 42(7):967 979. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Matthijs Vink, Thomas Edward Gladwin, Sanne Geeraerts, Pascal Pas, Dienke Bos, Marissa Hofstee, Sarah Durston, and Wilma Vollebergh. 2020. Towards an integrated account of the development of selfregulation from neurocognitive perspective: framework for current and future longitudinal multimodal investigations. Developmental Cognitive Neuroscience, 45:100829. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. 2021. VoxPopuli: Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 9931003, Online. Association for Computational Linguistics. Changhan Wang, Anne Wu, and Juan Miguel Pino. 2020. Covost 2: massively multilingual speech-to-text translation corpus. CoRR, abs/2007.10310. Vaibhav Srivastav, Somshubra Majumdar, Nithin Koluguri, Adel Moumen, Sanchit Gandhi, Hugging Face Team, Nvidia NeMo Team, and SpeechBrain Team. 2023. Open automatic speech recognition leaderboard. urlhttps://huggingface.co/spaces/huggingface.co/spaces/openasr-leaderboard/leaderboard. Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan Cao, Nanxin Chen, Yu Zhang, Hagen Soltau, Paul Rubenstein, et al. 2023. Slm: Bridge the thin gap between speech and text foundation models. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. IEEE. Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Rozière, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston, and Xian Li. 2024. Branch-train-mix: Mixing expert llms into mixture-of-experts LLM. CoRR, abs/2403.07816. Weiran Wang, Ke Hu, and Tara Sainath. 2022. Deliberation of streaming rnn-transducer by nonIn ICASSP 2022-2022 autoregressive decoding. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 74527456. IEEE. Rafid Sukkar and Chin-Hui Lee. 1996. Vocabulary independent discriminative utterance verification for nonkeyword rejection in subword based speech recognition. IEEE Transactions on Speech and Audio Processing, 4(6):420429. Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al. 2018. Espnet: End-to-end speech processing toolkit. arXiv preprint arXiv:1804.00015. 14 model for ctc-based end-to-end mandarin speech recognition. In Interspeech, pages 21802184. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36. Shinji Watanabe, Takaaki Hori, Suyoun Kim, John Hershey, and Tomoki Hayashi. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. IEEE Journal of Selected Topics in Signal Processing, 11(8):12401253. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Chao-Han Huck Yang, Yile Gu, Yi-Chieh Liu, Shalini Ghosh, Ivan Bulyko, and Andreas Stolcke. 2023. Generative speech recognition error correction with large language models and task-activating prompting. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. IEEE. Chao-Han Huck Yang, Linda Liu, Ankur Gandhe, Yile Gu, Anirudh Raju, Denis Filimonov, and Ivan Bulyko. 2021. Multi-task language modeling for improving In 2021 IEEE speech recognition of rare words. Automatic Speech Recognition and Understanding Workshop (ASRU), pages 10871093. IEEE. Michihiro Yasunaga, Jure Leskovec, and Percy Liang. 2021. Lm-critic: Language models for unsupervised grammatical error correction. arXiv preprint arXiv:2109.06822. Hanrong Ye and Dan Xu. 2023. Taskexpert: Dynamically assembling multi-task representations with memorial mixture-of-experts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2182821837. Zhao You, Shulin Feng, Dan Su, and Dong Yu. 2021. Speechmoe: Scaling to large acoustic models with dynamic routing mixture of experts. In Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021, pages 20772081. ISCA. Robert Zatorre and Jackson Gandour. 2008. Neural specializations for speech and pitch: moving beyond the dichotomies. Philosophical Transactions of the Royal Society B: Biological Sciences, 363(1493):10871104. Hao Zhang, Richard Sproat, Axel Ng, Felix Stahlberg, Xiaochang Peng, Kyle Gorman, and Brian Roark. 2019a. Neural models of text normalization for speech applications. Computational Linguistics, 45(2):293337. Shiliang Zhang, Ming Lei, and Zhijie Yan. 2019b. Investigation of transformer based spelling correction"
        },
        {
            "title": "A Appendix",
            "content": "Prompt Format We provide detailed correction example per [TASK] and actual prompt format of INPUT: used in the our experiments for qualitative studies as shown in Figure 3. For instance, each task will have specific task-activation prompt format, where ASR, ST, and MT would be based on the sampling or beam search results. On the other hand, OCR and TEC will use input texts for end-to-end mapping. Correction Examples We randomly select post-recognition example by NEKO . In Figure 4, long form ASR output has been selected and it remain the top 1-best correction with NEKO. or the ST and MT correction result in Figure 5 and in Figure 6, although the post-NEKO corrected output does not perfectly align with the ground truth, it boosts the general semantic meaning, as reviewed by native speakers. Meanwhile, the OCR and TEC correction results in Figures 7 and 8 demonstrate various types of corrections, such as pattern-wise character misrecognition and understanding-based coherence improvements. Additional Discussion on Human Recognition from Speech and Text Inputs Human recognition (e.g., speech, optical character, text translation) and has naturally evolved to excel at recognizing and understanding speech in wide range of real-world scenarios (He et al., 2019; Deng et al., 2013). However, the field of automatic speech recognition (ASR) has traditionally concentrated on training and evaluating models on specific datasets (Chan et al., 2016; Watanabe et al., 2017). These models have shown limited adaptability to new environments (Yang et al., 2021; Du et al., 2016; Hu et al., 2024a), leading to decreased accuracy and practicality in real-world settings. Recognizing the challenges posed by single dataset models and the availability of diverse datasets collected over time, unified models are being developed that merge information from multiple datasets into single framework (Barrault et al., 2023a). While Grammatical Error Correction (TEC) has been actively explored (Yang et al., 2023), ASR error correction is distinct due to the arbitrariness of spoken language (Aksënova et al., 2021), requiring efforts from both speech, NLP, and cognitive science communities as one human recognition example shown in Figure 9. Task-Oriented Inference for Mixture of Expert Models During inference, the Neko-model utilizes top-2 expert routing, instead of just top-1. Our pilot studies showed that top-1 routing indeed led to worse performance due to limited knowledge sharing. Using more than two experts (e.g., top-3 or higher) diverged from the training setup and increased inference costs (ranging from 23.5% to 75.5%) without significant gain (i.e., relative difference of less than 0.06%). Future Model Maintenance Plan and ASR Community For ASR tasks, we used Canary-v0, Whisperseires, and SeamlessM4T to decode textual hypotheses data. For Whisper, we included it as widely-used baseline, but our key comparisons are to other GEC methods also using Whisper (e.g. GenTranslate). Open eco-system, including ESPnet (Watanabe et al., 2018) and SpeechBrain (Ravanelli et al., 2021) models, are also our interests to be adapted as first-pass ASR in the open code base. This will provide more comprehensive evaluation across model types. In general, NeKos post-ASR correction improvements are consistent across datasets and first-pass models, suggesting the benefits generalize beyond model-specific (i.e., Canary, Whisper, or SeamlessM4T) strengths as the initial medical term correction results shown in Figure 10. Acknowledgment The authors would like to acknowledge the initial feedback from Taejin Park, Boyi Li, and Yejin Choi at NVIDIA and the discussion on medical speech recognition evaluation with Zhen Wan and Chenhui Chu from Kyoto University. 16 Figure 3: Example prompts of various correction tasks using Automatic Speech Recognition (ASR), Machine Translation (MT), Speech Translation (ST), Optical Character Recognition (OCR), and Textual Error Correction (TEC). 17 Figure 4: Examples of NEKO outputs for asr error correction task in SPGISpeech (ONeill et al., 2021). Figure 5: Examples of NEKO outputs for speech translation correction task in FLEURS (Conneau et al., 2022). 18 Figure 6: Examples of NEKO outputs for machine translation correction task in WMT20 (Barrault et al., 2020a). Figure 7: Examples of NEKO outputs for OCR correction task in PleIAs/Post-OCR-Correction. 19 Figure 8: Examples of NEKO outputs for textual error correction (TEC) tasks in CoEdIT (Raheja et al., 2023). Figure 9: Examples of (a) Human recognition given different input modalities, including audio, text, and visual patterns; (b) generative inference and correction (Marshall et al., 2015; Levinson and Evans, 2010) to understand the recognition results. 20 Figure 10: We provide medical post-ASR recognition correction on the Medical-ASR-EN dataset (https:// huggingface.co/datasets/jarvisx17/Medical-ASR-EN), where NeKo demonstrates the ability to (1) refine clinically related term errors and (2) correct grammar format."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}