{
    "paper_title": "FlatQuant: Flatness Matters for LLM Quantization",
    "authors": [
        "Yuxuan Sun",
        "Ruikang Liu",
        "Haoli Bai",
        "Han Bao",
        "Kang Zhao",
        "Yuening Li",
        "Jiaxin Hu",
        "Xianzhi Yu",
        "Lu Hou",
        "Chun Yuan",
        "Xin Jiang",
        "Wulong Liu",
        "Jun Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, quantization has been widely used for the compression and acceleration of large language models~(LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with the equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still remain steep and outspread. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach to enhance flatness of weights and activations. Our approach identifies optimal affine transformations tailored to each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead, we apply Kronecker decomposition to the transformation matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments show that FlatQuant sets up a new state-of-the-art quantization benchmark. For instance, it achieves less than $\\textbf{1}\\%$ accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by $\\textbf{7.5}\\%$. For inference latency, FlatQuant reduces the slowdown induced by pre-quantization transformation from 0.26x of QuaRot to merely $\\textbf{0.07x}$, bringing up to $\\textbf{2.3x}$ speedup for prefill and $\\textbf{1.7x}$ speedup for decoding, respectively. Code is available at: \\url{https://github.com/ruikangliu/FlatQuant}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 6 2 4 9 0 . 0 1 4 2 : r Preprint. Under review. FLATQUANT: FLATNESS MATTERS FOR LLM QUANTIZATION Yuxuan Sun1, Ruikang Liu2, Haoli Bai1, Han Bao1, Kang Zhao1, Yuening Li3, Jiaxin Hu1, Xianzhi Yu1, Lu Hou1, Chun Yuan2, Xin Jiang1, Wulong Liu1, Jun Yao1 1Huawei Noahs Ark Lab 2Shenzhen International Graduate School, Tsinghua University 3The Chinese University of Hong Kong {sunyuxuan8, baihaoli, baohan12, hujiaxin5, yuxianzhi, houlu3}@huawei.com {Jiang.Xin, liuwulong, yaojun97}@huawei.com liuruikang.cs@gmail.com zhaok14@tsinghua.org.cn yuening@link.cuhk.edu.hk yuanc@sz.tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Recently, quantization has been widely used for the compression and acceleration of large language models (LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with the equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still remain steep and outspread. In this paper, we propose FLATQUANT (Fast and Learnable Affine Transformation), new post-training quantization approach to enhance flatness of weights and activations. Our approach identifies optimal affine transformations tailored to each linear layer, calibrated in hours via lightweight objective. To reduce runtime overhead, we apply Kronecker decomposition to the transformation matrices, and fuse all operations in FLATQUANT into single kernel. Extensive experiments show that FLATQUANT sets up new state-of-theart quantization benchmark. For instance, it achieves less than 1% accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by 7.5%. For inference latency, FLATQUANT reduces the slowdown induced by prequantization transformation from 0.26x of QuaRot to merely 0.07x, bringing up to 2.3x speedup for prefill and 1.7x speedup for decoding, respectively. Code is available at: https://github.com/ruikangliu/FlatQuant."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent large language models (LLMs) have achieved remarkable success across wide range of tasks with an increasing number of parameters (Achiam et al., 2023; Jiang et al., 2023; Yang et al., 2024; Dubey et al., 2024). However, the growth of model size also incurs significant increase in computation and memory overhead. As result, reducing the computational and memory demands of LLMs has emerged as critical research direction, and quantization is one of the most effective solutions (Frantar et al., 2022; Lin et al., 2023; Dettmers et al., 2022; Xiao et al., 2023). Quantization decreases the memory footprint and accelerates the inference, by reducing the precision of model parameters and activations. Quantization error is commonly used metric to measure the performance of quantization methods (Nagel et al., 2020; Bai et al., 2020; Li et al., 2021). One key factor that affects the quantization error is the flatness of weights and activations. Intuitively, when the distribution of weights and activations is sharp and there exist multiple outspread values, quantizing them to the same quantized value usually incurs large quantization error (Chmiel et al., 2020; Li et al., 2024). Moreover, as LLMs generate outputs layer by layer, reduced quantization error also flattens the error landscape propagated across Transformer layers. Nevertheless, it is non-trivial to get flat distribution of weights and activations in LLMs. LLMs are known to have extreme outliers over activations (Dettmers et al., 2022; Xiao et al., 2023) and pivot Equal contribution. Corresponding author. 1 Preprint. Under review. tokens (Liu et al., 2024a; Sun et al., 2024). To alleviate this problem, various pre-quantization transformations are proposed to mitigate the impact of outliers (Xiao et al., 2023; Ashkboos et al., 2024; Liu et al., 2024b; Ma et al., 2024). However, we revisit these transformations and find them still sub-optimal in promoting flatness. For instance, per-channel scaling (Xiao et al., 2023; Shao et al., 2023) aims to balance the outliers between weights and activations, but it falls short of distributing outliers over the non-outlier channels. The recent Hadamard transformation (Ashkboos et al., 2024; Lin et al., 2024) attempts to solve this problem, while the individual characteristics of each linear layer are not considered. Moreover, the linear transformation introduced by these methods (Ma et al., 2024; Ashkboos et al., 2024; Liu et al., 2024b) inevitably introduces extra inference overhead that affects the overall speedup of quantization. In this work, we introduce new post-training quantization approach named FLATQUANT (Fast and Learnable Affine Transformation). Our approach is grounded in the principle of achieving flatter distribution of weights and activations, which is crucial for quantization. FLATQUANT aims to identify the optimal affine transformation for each linear layer, employing lightweight, block-wise training strategy over the calibration data. To minimize the inference overhead associated with affine transformations, FLATQUANT harnesses the efficiency of Kronecker decomposition, thus reducing both the memory and computational demands. The proposed approach is compatible with various quantization techniques such as learnable clipping, and can be applied to various quantization settings, e.g., weight-only quantization or KV cache quantization. Additionally, by observing that affine transformations in FLATQUANT are memory bound, we further fuse the affine transformations and quantization into single kernel, thereby minimizing the global memory access and kernel lunch overhead. Lastly, extensive experiments are conducted to assess FLATQUANT across various tasks, including language modeling and question answering, using LLaMA-2/3 models ranging from 7B to 70B parameters. The empirical results demonstrate that our proposed approach surpasses current state-of-the-art methods in terms of both accuracy and inference latency. The contributions of this work are summarized below: We highlight the significance of achieving flatness for LLM quantization, demonstrating that flat distributions of weights and activations facilitate quantization and reduce error propagation across Transformer layers. We introduce FLATQUANT, new post-training quantization method with fast and learnable affine transformations optimized for each linear layer. The approach is empirically demonstrated to enhance the flatness of both weights and activations in LLMs. Extensive experiments demonstrate that FLATQUANT sets new state-of-the-art results for quantization. To the best of our knowledge, we are the first to achieve 1% accuracy drop with simply round-to-nearest W4A4 quantization on the LLaMA-3-70B model. We have designed an efficient kernel that fuses affine transformation and quantization, reducing the additional latency caused by transformation from 0.26x slowdown with QuaRot to only 0.07x. This enhancement gives up to 2.3x speedup for prefill and 1.7x speedup for decoding compared to the FP16 baseline."
        },
        {
            "title": "2.1 PRELIMINARIES ON LLM QUANTIZATION",
            "content": "The inference of LLM typically has two stages: 1) the prefill stage, which creates key-value cache (KV Cache) layer by layer from the input sequence; and 2) the decoding stage, where the model autoregressively generates tokens based on previous KV cache. Quantization is common practice to reduce the model size and inference latency. It converts the full-precision weights Rmn or activations Rkn of linear layers (i.e., = XW), and optionally the KV cache to low-bit representations. For instance, b-bit weight quantization can be represented as follows: ˆW = Qb(W) = ΠΩ(b)(W/s), (1) where is the quantization step size, Π() is the projection function and Ω(b) = {0, 1, ..., 2b 1} is the set of b-bit integer points. For simplicity of notation, we denote Q() as the general quantization function in the rest of this paper. Quantizing weights enables memory time savings from weight 2 Preprint. Under review. (a) Wo of the 10th Transformer layer in LLaMA-3-8B. (b) Xo of the 10th Transformer layer in LLaMA-3-8B. (c) Wg of the 30th Transformer layer in LLaMA-3-70B. (d) Xg of the 30th Transformer layer in LLaMA-3-70B. Figure 1: Distributions of weights and inputs from LLaMA-3-8B and LLaMA-3-70B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. In Transformer layer, Wo and Xo denote the weight matrix and input of the output projection layer in the self-attention layer, respectively. Wg and Xg denote the weight and input of the gated linear layer of the feed-forward network, respectively. More visualizations can be found in Appendix D. (a) Per-channel Scaling. (b) Hadamard Transfrom. (c) FLATQUANT. (d) Stacked View. Figure 2: The mean squared error (MSE) of quantization across Transformer layers and input sequence in LLaMA-3-8B. Figure 2a-2c plot the MSE surface of each method, while Figure 2d overlays these surfaces by dividing each MSE with that of FLATQUANT. More details and visualizations can be found in Appendix D. loading from high-bandwidth memory (HBM) into the compute cores, and quantizing activations further reduces the computation, benefiting both the prefill and decoding stages of LLM inference. As recent works suggest (Xiao et al., 2023; Shao et al., 2023; Xi et al., 2023), LLMs exhibit persistent outliers in activations, posing significant challenges for quantization. Various works have been proposed to suppress these outliers to improve the quantized LLMs. Two most commonly used methods are per-channel scaling (Xiao et al., 2023; Lin et al., 2023; Wei et al., 2023) and Hadamard transformation or its variants (Xi et al., 2023; Ashkboos et al., 2024; Lin et al., 2024). Per-channel Scaling. The input activations of LLMs are often rich in outliers. To mitigate their impact on quantization, popular way is to apply channel-wise scaling over weights and activations (Xiao et al., 2023), i.e., = (Xdiag(c)1) (diag(c)W), where Rn is the channel-wise scaling factor. The scaling vector smooths the activations by jointly considering the magnitudes of input activations and weights, i.e. cj = max(Xjα)/ max(Wj1α). The scaled weights diag(c)W can be merged to eliminate the runtime computation. Additionally, Wei et al. (2023) introduces channel-wise shifting, i.e., (X z)diag(c)1, to further mitigate the impact of outliers, and Shao et al. (2023) treats both diag(c) and as learnable parameters. Hadamard Transformation. Recent works find Hadamard matrices {+1, 1}nn are particularly helpful in smoothing out outliers in activations (Xi et al., 2023; Ashkboos et al., 2024; Lin et al., 2024). In contrast to per-channel scaling which only adjusts the diagonal elements in the view of matrix multiplication, Hadamard transformation rotates the channels of both activations and weights, re-distributing the outliers among all channels to effectively eliminate them. Thanks 3 Preprint. Under review. to the orthogonality of Hadamard matrices (i.e., HH = I), the following equivalency holds: = XW = (XH)(HW). The transformed weight WH can be similarly pre-processed offline to reduce additional runtime overhead."
        },
        {
            "title": "2.2 THE FLATNESS FOR QUANTIZATION",
            "content": "We examine existing pre-quantization transformations with focus on their potential for flatness, critical factor for effective quantization. Intuitively, by removing outliers, these transformations are expected to produce flat weights and activations that are conducive to quantization. Additionally, the quantization error propagated through the network is also expected to be low and flat. However, our empirical results indicate that current methods are limited in achieving the desired flatness. This is in contrast to our proposed FLATQUANT, which will be introduced in Section 3. The Flatness of Weights and Activations. Flat tensors are intuitively easier to quantize after removing outliers, a.k.a tensors with low kurtosis (Chmiel et al., 2020; Li et al., 2024). Figure 1 displays the distributions of both the original and transformed weights and activations, sorted by the channel magnitudes in descending order. The flat weights and activations with horizontal envelopes are usually preferred by quantization. Compared with the original distributions, pre-quantization transformations can yield flatter activations (e.g., Figure 1b, 1d) but still with their limitations. Perchannel scaling flattens activations at the cost of steeper weight envelops (e.g., Figure 1a, 1c). While Hadamard transformation produces generally better flatness for both activations and weights than per-channel scaling, it still sometimes generates unsatisfactory weights and activations distributions (e.g., Figure 1a, 1b). In contrast, FLATQUANT consistently flattens both weights and activations. The Flatness of Quantization Error Landscape. The quantization error inevitably propagates, and it is insightful to show how pre-quantization transformations mitigate this issue. We plot the two-dimensional landscape of mean squared error (MSE) in Figure 2. First, it is observed that massive quantization errors occur at initial tokens, a.k.a. pivot tokens (Liu et al., 2024a), which contain massive outliers (Sun et al., 2024). Both per-channel scaling and Hadamard transformation are powerless to such errors (i.e., Figure 2a-2b). Instead, FLATQUANT shows much lower error at these pivot tokens from Figure 2c. Second, the quantization error increases layer-wisely, but is less evident along the input sequence. According to Figure 2d, FLATQUANT is the best in controlling the error propagation, followed by Hadamard transformation and lastly the per-channel scaling."
        },
        {
            "title": "3.1 FAST AND LEARNABLE AFFINE TRANSFORMATION",
            "content": "We begin with applying FLATQUANT for standard linear layer, and will discuss its integration with the Transformer architecture in Section 3.2. The overview of FLATQUANT is presented in Figure 3. primary objective of FLATQUANT is to find the best affine transformation for each linear layer to quantize. Ideally, given = XW, one can identify the optimal invertible matrix Rnn by = arg min Q(XP)Q(P1W)2 , (2) as studied in (Ma et al., 2024). The weights P1W can be pre-computed offline akin to (Ashkboos et al., 2024). However, unlike Hadamard matrices that can be reused for all layers, storing individual matrices for different linear layers is computationally expensive. In the forward pass, this approach doubles the computational cost and memory access for matrix multiplication. Additionally, it nearly doubles the model storage requirements. Thus, another key aspect of FLATQUANT is to identify fast alternative for the pre-quantization transformation. Kronecker Decomposition. We decompose the original Rnn into = P1 P2, where P1 Rn1n1 , P2 Rn2n2 are invertible matrices in smaller sizes, and = n1n2. Recall the vectorization trick of the Kronecker product, i.e., vec(V)(P1 P2) = vec(P 1 VP2) for some Rn1n2 , the matrix multiplication in Equation 2 can be re-written as Q(XP)Q(P1W) = Q(P (3) where Rkn1n2 and Rmn1n2 are reshaped from and accordingly, and denotes the reduction over the i-th axis. Note that both weights and activations are converted back 1 1 2 P2) Q(P1 1 1 2 (P1 2 )) , Preprint. Under review. Figure 3: The overall framework of FLATQUANT. (a): necessary notations of FLATQUANT; (b): the integration of FLATQUANT with conventional LLaMA layer, where merged parameters are grouped in red, online transformation and quantization functions in blue, and merged scaling vectors in green; (c): the exemplary view of FLATQUANT applied for the down-projection layer, where the scaling vector diag(c) over is merged to Wu in practice. n2 = n2 1+n2 2 to matrix before multiplication. Such decomposition can save the memory up to n/2 times, given n2 n. Moreover, the computation 2 , with the equality holds when n1 = n2 = that 2n1n2 n/2 times with the same optimal condition. In practice, we select 2 = arg min(n1+ saving is n2), s.t. n1n2 = and n1 n2. For instance, the optimal configuration is (n 2) = (64, 128) for = 8192. We find such strategy gives the best speedup without compromising performance, as will be detailed in Figure 5. 1, 1, Per-channel Scaling. To enhance the ability to balance outliers between the weights and activations, FLATQUANT explicitly introduces learnable scaling vector diag(c) Rn prior to the prequantization transformation, as illustrated in Figure 3 (c). Following (Xiao et al., 2023), the scaling vector can be merged pair-wisely to the preceding layer normalization or linear layers, thereby incurring no additional inference overhead. Learnable Clipping Thresholds. To further reduce the potential outlier after the above transformation, we combine learnable clipping thresholds αw, αa (0, 1) on both weight and activation for each linear layer, together with the KV cache. While previous studies (Jacob et al., 2018; Frantar et al., 2022; Ashkboos et al., 2024) demonstrate that grid search is valid to find reasonable clipping thresholds, we observe that learning the clipping thresholds yields better results. These parameters are layer-specific and can be jointly optimized with the linear transformation matrices and scaling vector diag(c).A sigmoid function is applied to constrain αw and αa within (0, 1). The Training Objective. We are now ready for the training objective of FLATQUANT. We follow post-training quantization and sequentially minimize the mean squared error (MSE) by quantization over small amount of calibration data (e.g., 128 randomly sampled sentences) for each Transformer block. The training objective for the l-th Transformer block is (cid:0)X; Θ(cid:1)(cid:13) 2 , (cid:13) (4) where Fl() and ˆFl() denote the original and the quantized Transformer block, Θ = {P, c, αa, αw} is abbreviated for all learnable parameters within the block. The transformation matrices within Transformer block will be explained in Section 3.2. To compute the matrix inversion in Equation 3 efficiently and accurately, we adopt the singular value decomposition together with automatic mixed precision. More details can be found in Appendix B.1. Note that we also experiment with training multiple Transformer blocks together but find similar performance at higher training costs. Finally, we remark that the sequential training with Equation 4 can effectively produce flat weights and activations in Figure 1, and reduce the error propagation along the Transformer blocks in Figure 2. (cid:0)X(cid:1) ˆFl min Θ (cid:13) (cid:13)Fl 3."
        },
        {
            "title": "INTEGRATION WITH THE TRANSFORMER ARCHITECTURE",
            "content": "We illustrate the integration of FLATQUANT with Transformer block based on an LLaMA-like architecture, as depicted in Figure 3. Following the conventional practices, we employ low-bit ma5 Preprint. Under review. trix multiplications for all linear layers, while keeping layer normalization layers, pre-quantization transformations, RoPE embeddings, and attention scores in FP16. Self-Attention. The self-attention module is equipped with four transformations {Pa, Po, Ph, Pv}. Specifically, Pa is applied to flatten the input activation for the query, key, and value projections, while Po smooths the input activation for the output projection. Ph and Pv are used to transform the key and value cache head by head, respectively. Note that we only decompose Pa and Po, but leave Ph and Pv in their original shape. This is because per-head quantization already facilitates cheap transformations, given that the head size is significantly smaller than the full hidden size. Moreover, we further fuse Po with Pv to reduce overhead, as inspired by QuaRot (Ashkboos et al., 2024). Our empirical results show this fusion does not result in additional loss of accuracy. Feed-forward Network. The feed-forward network (FFN) employs two transformation matrices, i.e., Pug and Pd. Pug is applied to flatten the input of the feed-forward network after layer normalization, while Pd flattens the input for the down projection layer. Both transformations are decomposed to minimize the inference overhead. Additionally, the per-channel scaling of Pd is merged into the weight of up projection layer, ensuring no additional computational overhead. Layer Normalization. Recall that QuaRot (Ashkboos et al., 2024) and SpinQuant (Liu et al., 2024b) modify the LayerNorm to RMSNorm and merge orthogonal transformations into preceding layers for efficiency. Nonetheless, the residual connection of the pre-norm architecture would constrain all Transformer blocks to share the same transformation after RMSNorm. Instead, FLATQUANT preserves the LayerNorm, and allows the use of fast and learnable affine transformations in Section 3.1 after LayerNorm for different layers, thereby enhancing the expressiveness."
        },
        {
            "title": "3.3 EFFICIENT KERNEL DESIGN",
            "content": "We design an efficient kernel for FLATQUANT that integrates both affine transformations and quan1 1 2 P2 tization into single operation. This design is motivated by two key factors. First, exhibits low computational intensity after Kronecker decomposition, making both prefill and decoding predominantly memory-bound. Second, the quantization is also known to be memory-bound. 1 1 X2 P2) into single kernel using OpenAI Triton (Tillet To address these issues, we fuse Q(P et al., 2019). Specifically, we load the entire P1 Rn1n1 and P2 Rn2n2 into SRAM. Each thread block slices tiling block Rn1n2 from X, performs the matrix multiplication P1 XP2, and quantizes the results on the fly. Throughout this process, all intermediate results are stored in SRAM before finally being written back to the global memory. This design thereby eliminates redundant memory accesses of intermediate results and reduces the kernel launch overhead. Further details of the kernel design are provided in the Appendix B.2. Finally, given the output above, we follow QuaRot (Ashkboos et al., 2024) to adopt the CUTLASS kernel for INT4 matrix multiplication, and FlashInfer (Ye, 2023) for KV cache quantization."
        },
        {
            "title": "4.1 SETTINGS",
            "content": "Evaluation and Baselines. We evaluate FLATQUANT on the LLaMA-2(7B/13B/70B) (Touvron et al., 2023) models and the LLaMA-3(8B/70B) (Dubey et al., 2024) models. Following previous works (Shao et al., 2023; Ashkboos et al., 2024), we report the perplexity (PPL) of language generation tasks on the WikiText2 (Merity et al., 2016) and C4 (Raffel et al., 2020) datasets. For commonsense reasoning tasks, we use six zero-shot evaluation tasks, including ARC-Challenge, ARC-Easy (Clark et al., 2018), HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), PIQA (Bisk et al., 2020), and WinoGrande (Sakaguchi et al., 2021). Additionally, we evaluate multiturn conversation ability on LLaMA-3.1-8B-Instruct using MT-Bench, with GPT-4o as the evaluator. For baselines, we compare FLATQUANT against popular INT4 post-training quantization methods, including SmoothQuant (Xiao et al., 2023), OmniQuant (Shao et al., 2023), AffineQuant (Ma et al., 2024), QUIK-4B (Ashkboos et al., 2023), and two recent state-of-the-art methods QuaRot (Ashkboos et al., 2024) and SpinQuant (Liu et al., 2024b). 6 Preprint. Under review."
        },
        {
            "title": "W Quantizer",
            "content": "WikiText-2 C4 FP"
        },
        {
            "title": "SmoothQuant\nOmniQuant\nAffineQuant\nQuaRot\nSpinQuant\nFLATQUANT",
            "content": "QUIK-4B QuaRot SpinQuant FLATQUANT -"
        },
        {
            "title": "GPTQ\nGPTQ\nGPTQ\nGPTQ",
            "content": "2-7B 2-13B 2-70B 5.47 4.88 83.12 14.74 12.69 8.56 6.14 5.79 8.87 6.10 5.96 5.78 35.88 12.28 11.45 6.10 5.44 5. 7.78 5.40 5.24 5.11 3.32 26.01 - - 4.14 3.82 3.55 6.91 3.79 3.70 3.54 3-8B 6. 210.19 - - 10.60 7.96 6.98 - 8.16 7.39 6.90 3-70B 2-7B 2-13B 2-70B 2.86 9.60 - - 55.44 7.58 3. - 6.60 6.21 3.77 7.26 6.73 77.27 21.40 15.76 11.86 9.19 7.79 - 8.32 8.28 7.86 43.19 16.24 13.97 8.67 8.11 7. - 7.54 7.48 7.11 5.71 34.61 - - 6.42 6.26 5.91 - 6.12 6.07 5.92 3-8B 9. 187.93 - - 17.19 13.45 11.13 - 13.38 12.19 11.21 3-70B 7.17 16.90 - - 79.48 15.39 7.86 - 12.87 12.82 7. Table 1: WikiText-2 and C4 perplexity of 4-bit weight & acitvation quantized LLaMA models. Implementation Details. We implement FLATQUANT based on Huggingface (Wolf, 2019) and PyTorch (Paszke et al., 2019). For optimization, we adopt the AdamW optimizer with an initial learning rate of 5e-3 and employ cosine annealing learning rate decay schedule. Specifically, the learning rate for clipping thresholds is 5e-2. FLATQUANT is trained for 15 epochs on calibration set comprising 128 sentences from WikiText-2, each sampled with 2048 tokens. The batch size is set to 4. The default calibration procedure costs approximately 26GB of GPU memory and about 0.9 hours for LLaMA-3-8B on single GPU. FLATQUANT is robust to initialization, and we employ random affine transformation matrices as the starting point. Further details about implementation and calibration time are provided in Appendix B.1. Quantization. We adopt per-channel and per-token symmetric quantization for weights and activations, respectively. For KV cache quantization, we utilize group-wise asymmetric quantization with group size of 128. This matches the head dimension of LLaMA, as suggested in previous studies (Zhao et al., 2024; Ashkboos et al., 2024), to effectively leverage the memory-bound characteristics of self-attention. By default, FLATQUANT employs round-to-nearest (RTN) as the weight quantizer. For fair comparison with QuaRot and SpinQuant, we also report weight quantization using GPTQ, which uses the same calibration data for both closed-form weight updates and training."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "Results on Language Generation Tasks. Table 1 presents the PPL results for FLATQUANT with and without the GPTQ weight quantizer on the WikiText-2 and C4 datasets. As can be seen, FLATQUANT with RTN weight quantizer consistently outperforms previous SOTA quantization methods across all major benchmarks. For the LLaMA-2-70B model, FLATQUANT achieves PPL score just 0.23 higher than the FP16 baseline, underscoring the effectiveness of our approach. For LLaMA-3-8B, FLATQUANT reduces the PPL from 7.39 (SpinQuant) to 6.98, narrowing the gap with the FP16 baseline to 0.84. Notably, FLATQUANT with RTN exhibits performance comparable to those with GPTQ but takes significantly less calibration time. This is particularly helpful in reducing the time consumption to deploy FLATQUANT in practice. These results highlight the efficacy of our proposed learnable transformations in enhancing flatness and mitigating the impact of outliers in both weights and activations, thereby establishing new SOTA in low-bit LLM quantization. Results on Zero-shot QA Tasks. We extend our evaluation to six zero-shot commonsense QA tasks, as shown in Table 2. For fair comparison, we reproduce QuaRot 1 and SpinQuant 2 with their official implementations and released checkpoints, evaluating all methods with the same version of lm-eval-harness framework (Gao et al., 2021). As can be seen, FLATQUANT significantly narrows the performance gap between quantized models and the FP16 baseline. Specifically, for larger models such as LLaMA-2-70B, the accuracy loss of FLATQUANT is only 0.43%, which is amazing with such low bit quantization setting. The recently released LLaMA-3 models have been shown to be more challenging for quantization (Huang et al., 2024). Nonetheless, FLATQUANT continues 1https://github.com/spcl/QuaRot 2https://github.com/facebookresearch/SpinQuant 7 Preprint. Under review. Model Method Quantizer ARC-C ARC-E HellaSwag LAMBADA PIQA Winogrande Avg 2-7B 2-13B 2-70B 3-8B 3-70B FP16 QuaRot SpinQuant FLATQUANT QuaRot SpinQuant FLATQUANT FP16 QuaRot SpinQuant FLATQUANT QuaRot SpinQuant FLATQUANT FP16 QuaRot SpinQuant FLATQUANT"
        },
        {
            "title": "QuaRot\nSpinQuant\nFLATQUANT",
            "content": "FP16 QuaRot SpinQuant FLATQUANT QuaRot SpinQuant FLATQUANT FP16 QuaRot SpinQuant FLATQUANT QuaRot SpinQuant FLATQUANT - RTN RTN RTN GPTQ GPTQ GPTQ - RTN RTN RTN GPTQ GPTQ GPTQ - RTN RTN RTN"
        },
        {
            "title": "GPTQ\nGPTQ\nGPTQ",
            "content": "- RTN RTN RTN GPTQ GPTQ GPTQ - RTN RTN RTN GPTQ GPTQ GPTQ 46.16 36.60 39.42 43.26 42.32 41.72 43.00 49.15 42.83 43.69 48.04 45.48 49.15 48. 57.17 52.22 55.03 56.14 55.46 55.38 56.40 53.50 38.65 45.73 50.00 45.73 47.27 50. 64.25 22.18 44.03 62.12 49.49 51.96 61.95 74.54 61.41 65.32 72.05 68.35 69.28 71. 77.44 69.95 72.43 76.64 73.27 77.19 76.94 81.02 76.60 79.17 80.30 79.76 79.04 80. 77.57 66.54 71.38 75.80 70.83 74.20 75.88 85.94 34.30 69.07 84.97 74.37 77.40 84. 75.98 65.07 71.45 73.64 72.53 72.90 73.31 79.39 73.54 75.52 77.59 76.03 76.86 77. 83.81 79.96 81.76 83.01 81.58 82.57 82.91 79.12 68.82 74.07 76.80 72.97 74.55 76. 84.93 32.15 74.57 83.95 77.22 77.29 83.87 73.92 48.06 66.16 72.04 65.40 71.28 72. 76.73 65.62 72.42 76.60 69.01 73.86 76.40 79.60 74.61 78.87 79.60 79.35 78.75 80. 75.51 57.20 67.67 72.91 62.70 70.29 73.20 79.37 13.35 63.34 78.73 71.69 71.90 77. 79.05 72.20 75.30 77.26 76.33 76.17 77.53 80.47 77.69 78.40 79.38 79.05 78.67 79. 82.70 81.12 81.45 82.75 81.83 82.37 82.92 80.74 71.82 76.66 79.16 75.35 77.37 79. 84.44 57.67 76.99 84.28 78.89 79.33 83.95 69.06 63.06 63.46 69.53 65.11 66.06 67. 72.14 67.88 68.90 70.24 70.64 69.85 70.56 77.98 76.32 74.27 77.90 76.09 78.22 76. 72.93 65.04 66.38 72.69 67.17 68.51 72.93 80.74 52.49 65.98 80.03 71.03 72.06 79. 69.79 57.73 63.52 67.96 65.01 66.23 67.47 72.55 66.25 68.56 71.42 68.91 70.93 71. 77.05 73.47 75.09 76.62 75.68 76.06 76.53 73.23 61.34 66.98 71.23 65.79 68.70 71. 79.95 35.36 65.66 79.01 70.45 71.66 78.58 Table 2: Zero-shot QA task results of 4-bit weight & activation quantized LLaMA models."
        },
        {
            "title": "Method",
            "content": "FP"
        },
        {
            "title": "STEM Humanities Avg",
            "content": "8.17 7.20 7.95 8.10 6.90 7.35 5.05 3.90 4. 7.00 5.30 7.20 6.10 4.05 4.80 8.67 6.70 7. 8.50 6.05 7.20 8.91 7.80 8.70 7.60 5.99 6. Table 3: MT-Bench results of 4-bit weight & activation quantized LLaMA-3.1-8B-Instruct model. to perform well, with an accuracy loss of 2.00% for LLaMA-3-8B and 0.94% for LLaMA-3-70B. Notably, while QuaRot with RTN largely lags behind QuaRot with GPTQ by an average accuracy gap over 4%, FLATQUANT with RTN can already obtain comparable results to GPTQ. Results on MT-Bench. We evaluate FLATQUANT on MT-Bench using the LLaMA-3.1-8BInstruct model in Table 3. While FLATQUANT trails behind the FP16 baseline in coding and STEM tasks, it consistently outperforms QuaRot with GPTQ across all categories, narrowing the gap between the quantized model and the FP16 baseline. Notably, for math problems, FLATQUANT matches the FP16 baselines score, exceeding QuaRot by 1.9 points. More evaluations are provided in Appendix C.1. 4."
        },
        {
            "title": "INFERENCE LATENCY",
            "content": "All experiments of inference latency below are conducted on the RTX3090 GPU. More details of the overall computational FLOPs, kernel profiling, and speedup gains are available in Appendix C.4. 8 Preprint. Under review. (a) Prefill Speedup. (b) Decoding Speedup. Figure 4: Prefill and decoding speedup of LLaMA-2-7B model across different batch sizes. We decode 256 tokens after the prefill on sequence length of 2048. Figure 5: Prefill speedup and WikiText2 PPL results of different decomposed matrix sizes on LLaMA-2-7B model. We decompose the hidden dimension 4096 into n1 n2 and range n1 from 1 to 2048, where n1 = 1 amounts to maintaining full-size transformation matrix. More details can be found in Appendix C.3. Figure 6: Prefill speedup of LLaMA-2-7B on sequence length of 2048 under batch size of 64 after applying different online transformations. We incorporate different online transformations sequentially to gauge their impact on the final speedup. Each point on the x-axis indicates adding new online transformation. End-to-end Speedup. Figure 4 shows the prefill and decoding speedup of FLATQUANT across different batch sizes, with 2048 and 256 tokens for prefill and decoding, respectively. With kernel fusion and INT4 tensor core, FLATQUANT can achieve up to 2.30x speedup for prefill and 1.76x speedup for decoding under the batch size of 64. Notably, FLATQUANT is apparently faster than QuaRot (Ashkboos et al., 2024) thanks to the Kronecker decomposition and efficient kernel design. Although there is still minor gap compared to the vanilla INT4 quantization, it significantly enhances accuracy and facilitates the deployment of INT4 LLMs in real-world applications. Kronecker Decomposition: Sizes and Perplexities. In Figure 5, we examine the impact of different decomposed matrix sizes in Equation 3 on model performance and speedup. As shown, the varying sizes of Kronecker decomposition significantly affect speedup, but have limited impact on the perplexity of generated text. The speedup peaks when P1 and P2 are of equal size (i.e., = 64), as predicted by our theoretical analysis in Section 3.1. When n2 exceeds n1 = n2 = 64, the speedup quickly decreases due to irregular memory access patterns for activations. These results further demonstrate FLATQUANTs effectiveness in minimizing inference overhead while maintaining quantization accuracy through matrix decomposition. Overhead of Each Online Transformation. We now investigate the impact of the five online transformations (i.e., {Pa, Po, Ph, Pug, Pd}) in FLATQUANT on the overall speedup, as shown in Figure 6. Even with five per-layer transformations, FLATQUANT results in minimal 0.07x end-to-end slowdown, significantly outperforming QuaRots 0.26x with just three Hadamard transformations. Specifically, FLATQUANTs Pd causes 0.04x slowdown due to large FFN intermediate 9 Preprint. Under review. LT PS LCT WikiText-2 C4 ARC-C ARC-E HellaSwag LAMBADA PIQA Winogrande Avg 1266.60 8.50 7.95 7.11 6.98 936.41 13.51 12.74 11.47 11.13 25.26 44.97 44.20 49.32 50.00 28.62 71.38 71.89 76.14 75. 27.04 73.17 74.21 76.30 76.80 1.26 67.05 68.72 72.17 72.91 51.80 76.88 77.15 78.89 79.16 51.93 67.48 66.30 71.51 72.69 30.99 66.82 67.08 70.72 71.23 Table 4: Ablation study of FLATQUANTs main components on LLaMA-3-8B. LLaMA-3-8B WikiText-2 PPL C4 PPL W4 A4 KV4 WikiText-2 PPL C4 PPL QA Acc W4A16 W3A16 W4A16 W3A16 FP 6.14 9.45 RTN GPTQ GPTQ-g128 AWQ QuIP FLATQUANT-RTN FLATQUANT-GPTQ 8.70 7.00 6.50 7.10 6.50 6.54 6.48 2.2E3 13.00 8.20 12.80 7.50 7.78 7.52 14.00 11.80 10.40 10.10 11.10 10.17 10. 5.6E3 45.90 13.70 16.80 11.30 12.64 12.91 Table 5: Weight-only quantization results on LLaMA-3-8B model. 6.14 6.56 6.49 6.23 6.98 9.45 10.25 10.13 9.61 11.13 73.23 72.92 72.20 73.43 71.23 Table 6: Extending the affine transformations trained under W4A4KV4 to different quantization settings on LLaMA-3-8B model. QA Acc is the average accuray of the six QA tasks in lm-eval-harness. sizes, compared with QuaRots 0.17x. Meanwhile, Po results in 0.01x slowdown, versus QuaRots 0.1x. The rest transformations (i.e., Pa and Pug) have an insignificant impact of less than 0.01x."
        },
        {
            "title": "4.4 DISCUSSIONS",
            "content": "Ablation Study. We conduct ablation studies for FLATQUANT focusing on its main components: 1) learnable transformation (LT); 2) per-channel scaling (PS); and 3) learnable clipping thresholds (LCT). Starting from RTN as baseline, we evaluate the impact of each component on perplexity and the average accuracy on zero-shot QA tasks, with LLaMA-3-8B model. As shown in Table 4, enabling LT significantly enhances the accuracy of the quantized model, reducing PPL from 1266.60 to 8.50 on WikiText-2. This shows LT is capable of adaptively flattening the distribution of weight and activation values. Additionally, incorporating PS and LCT further improves PPL by 0.55 and 0.84, respectively, demonstrating the necessity of each component to refine the model performance. Other Quantization Schemes. Although the main results above focus mostly on weightactivation quantization, FLATQUANT can be easily applied to other quantization schemes. The results of weight-only quantization against several state-of-the-art baselines are presented in Table 5. FLATQUANT again obtains leading accuracy compared with leading baselines. For additional results on KV cache quantization and extreme low-bit quantization, please refer to Appendix C.1. Train One and Get More. We demonstrate that the affine transformations learned from weightactivation quantization can be directly applied to other quantization settings, such as weight-only or KV cache quantization, with surprisingly strong performance. The associated results are presented in Table 6. For instance, the results labeled as W4 are comparable to those in Table 5 that are specifically trained for weight-only quantization. This significantly saves time when applying FLATQUANT to different quantization settings, as only one set of transformation matrices is saved. Due to space constraints, we provide additional discussions such as the impact of calibration data, and the effect of learnable clipping in Appendix C.2. More visualizations on the flatness of transformed weights and activations, and the quantization error landscapes are in Appendix D."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "In this study, we revisit the importance of flat weights and activations for effective quantization, and find existing solutions still produce steep outspread values after the pre-quantization transformation. Therefore, we introduce FLATQUANT, novel post-training quantization method with the purpose of identifying fast and learnable transformations for each linear layer, to promote the flatness of weights and activations. Extensive experiments demonstrate the superiority of FLATQUANT, e.g., 10 Preprint. Under review. with less than 1% accuracy drop for W4A4 quantization on the LLaMA-3-70B. Our efficient kernel fusion integrates the affine transformation and quantization, reducing the transformation overhead and bringing up to 2.3x and 1.7x speedup over FP16 inference at the prefill and decoding stages, respectively. We hope this work advances the practical application of low-bit quantization for LLMs."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end 4-bit inference on generative large language models. arXiv preprint arXiv:2310.09259, 2023. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36, 2024. Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri Weiser, et al. Robust quantization: One model to rule them all. Advances in neural information processing systems, 33:53085317, 2020. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35: 3031830332, 2022. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2022. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. framework for few-shot language model evaluation. Version v0. 0.1. Sept, 10:89, 2021. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good are low-bit quantized llama3 models? an empirical study. arXiv preprint arXiv:2404.14047, 2024. 11 Preprint. Under review. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 27042713, 2018. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language models. arXiv preprint arXiv:2402.18158, 2024. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024. Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact. arXiv preprint arXiv:2403.01241, 2024a. Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquantllm quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024b. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024c. Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, and Rongrong Ji. Affinequant: Affine transformation quantization for large language models. arXiv preprint arXiv:2403.12544, 2024. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2016. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pp. 71977206. PMLR, 2020. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pp. 15251534, 2016. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. 12 Preprint. Under review. Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023. Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024. Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pp. 1019, 2019. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:1740217414, 2022. Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Wolf. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. Training transformers with 4-bit integers. Advances in Neural Information Processing Systems, 36:4914649168, 2023. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: In International Accurate and efficient post-training quantization for large language models. Conference on Machine Learning, pp. 3808738099. PMLR, 2023. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. Zihao Ye. Flashinfer: Kernel library for llm serving. 2023. URL https://github.com/ flashinfer-ai/flashinfer. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, 2019. Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196209, 2024. 13 Preprint. Under review."
        },
        {
            "title": "A RELATED WORK",
            "content": "Quantization for Large Language Models. Quantization is crucial technique for reducing memory footprint and accelerating inference by employing fewer bits for storage and computation, especially for the application of LLMs. Unlike previous models, LLMs are shown to exhibit outliers in activation and massive outliers in pivot tokens (Wei et al., 2022; Dettmers et al., 2022; Liu et al., 2024a; Sun et al., 2024), which can severely degrade quantization accuracy. To eliminate the negative impact of outliers, pre-quantization transformations have been widely adopted in weight-activation quantization (Xiao et al., 2023; Wei et al., 2023; Shao et al., 2023; Ma et al., 2024; Ashkboos et al., 2024; Liu et al., 2024b) as well as in fully quantized training (Xi et al., 2023). Additionally, several weight-only quantization methods (Lin et al., 2023; Chee et al., 2024; Tseng et al., 2024) also incorporate pre-quantization transformations. Per-channel Scaling Transformation. SmoothQuant (Xiao et al., 2023) employs per-channel scaling to shift the challenge of quantization from activations to weights in weight-activation quantization. Building on this, Wei et al. (2023) additionally introduces channel-wise shifting, while OmniQuant (Shao et al., 2023) utilizes differentiable approach to learn optimal scaling and shifting parameters. However, the scaling-based methods can negatively impact weight quantization and struggle in low-bit settings, such as W4A4 quantization. Hadamard and Orthogonal Transformation. Recent research (Xi et al., 2023; Tseng et al., 2024; Ashkboos et al., 2024) has shown that the Hadamard transformation is effective in eliminating outliers and lowering quantization error by redistributing outliers across all channels through matrix multiplication. QuaRot (Ashkboos et al., 2024) is the first to apply Hadamard transformation in the LLM W4A4 PTQ setting, while SpinQuant (Liu et al., 2024b) exploits learnable orthogonal matrices with model-level loss to further alleviate outliers. Affine Transformation. Considering that per-channel scaling corresponds to the diagonal elements of the affine transformation matrix, AffineQuant (Ma et al., 2024) proposes learning the equivalent affine transformation. However, their approach focuses on learning full-size diagonally dominant matrices and employs gradual mask optimization method, which may hinder the full potential of affine transformation in reducing quantization loss. Moreover, due to the formidable overhead associated with full-sized matrix multiplication, AffineQuant can only apply affine transformation to small fraction of linear layers. In contrast, we employ fast and learnable affine transformations without these limitations, leading to substantial accuracy improvements and practical speedup. Pre-quantization Transformations in Other Quantization Tasks. Inspired by SmoothQuant, AWQ (Lin et al., 2023) introduces activation-aware per-channel scaling to reduce quantization errors in weight-only quantization. QUIP (Chee et al., 2024) and its extension, QUIP# (Tseng et al., 2024), leverage random rotation matrices or Hadamard transformations to enhance incoherence in weight-only quantization. In fully quantized training task, Xi et al. (2023) propose to utilize blockdiagonal transformation consisting of Hadamard matrices to reduce the quantization error."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "B.1 MATRIX INVERSION AND TRAINING COST critical aspect to implement FLATQUANT is the computation of the inverse affine transformation matrix P1. As discussed below, we use singular value decomposition (SVD) and automatic mixed precision to train FLATQUANT, enjoying both training stability and efficiency. Direct Inversion and FP32 Training. One straightforward approach is to use the inverse function provided by PyTorch. However, we find that the precision of this inverse function at FP16 is insufficient. Specifically, PP1 does not closely approximate I. The off-diagonal elements are on the order of 1 103, which negatively impacts FLATQUANTs performance during the early stages of training. Therefore, simple solution is to conduct training in FP32 without Automatic Mixed Precision (AMP) to maintain precision. However, this inevitably increases training time and more GPU memory consumption. 14 Preprint. Under review. SVD and AMP Training. To further reduce resource requirements during calibration, we propose to employ singular value decomposition for the affine transformation. For any real matrix P, we can decompose it as = UΣV, where and are orthogonal matrices, and Σ is diagonal matrix. This formulation allows us to easily compute P1 = VΣ1U, offering more computationally efficient method for obtaining the inverse. Notably, this approach reduces the off-diagonal elements of PP1 to the order of 1 106 at FP16 precision, enabling us to utilize AMP during calibration. With AMP, we can achieve 50% reduction in training time and memory usage while maintaining nearly lossless accuracy in most cases. For the orthogonal matrices and V, we employ the Cayley parameterization provided by PyTorch 3. Comparison of the Two Training Recipes. We compare the two training recipes in Table 7. As shown, FP32 training requires more than twice the time of AMP training and necessitates 1.28x more GPU memory under the same setting, while the performance remains relatively close. Thus, our default choice is the SVD approach combined with AMP training. However, we observe that in certain models or extremely low-bit scenarios, numerical errors may occur within the AMP framework. In such cases, full-precision training becomes necessary. Training Recipe WikiText-2 PPL C4 PPL QA Acc Memory"
        },
        {
            "title": "Time",
            "content": "FP"
        },
        {
            "title": "SVD",
            "content": "6.95 9.96 7.00 6.98 11.04 11. 11.17 11.13 71.35 71.24 70.57 71. 35384MiB 2.2 hours 35360MiB 2.2 hours 27624MiB 0.9 hours 27554MiB 0.9 hours Table 7: Comparison of different training recipes for FLATQUANT on the LLaMA-3-8B. Calibration Time. We further present the calibration time required by FLATQUANT for the LLaMA family in Table 8. Compared to SpinQuant (Liu et al., 2024b) and QAT methods, FLATQUANT requires significantly fewer computational resources and less training time, while delivering superior performance. For weight-only quantization, only transformations related to the linear weights are introduced, resulting in shorter calibration time compared to weight-activation quantization. Moreover, as discussed in Section 4.2, FLATQUANT does not need to be combined with GPTQ to achieve optimal performance, further reducing the calibration overhead."
        },
        {
            "title": "LLaMA",
            "content": "2-7B 2-13B 2-70B 3-8B 3-70B weight-activation weight-only 1.15 hours 0.67 hours 1.55 hours 1.01 hours 6.15 hours 5.00 hours 0.90 hours 0.70 hours 5.94 hours 4.89 hours Table 8: Calibration time for LLaMA models. The reported times correspond to training on 128 segments of 2048 tokens over 15 epochs with batch size of 4, using single GPU. B.2 MORE DISCUSSIONS ON KERNEL FUSION To avoid redundant memory access and improve computational efficiency, we attempt to fuse 1 1 2 P2) into single kernel, followed by the INT4 CUTLASS kernel to multiply Q(P the 4-bit quantized weights and activations. In most cases, the shared memory per thread block is sufficient to hold the source matrices P1, P2, X, and their intermediate results , as visualized in Figure 7a. Nonetheless, there are corner cases when the shared memory is insufficient to hold all necessary tensors (e.g., > 28762 with n1, n2 > 128 on the NVIDIA RTX 3090). We thus revise our design for the two cases, as shown in Figure 7b and Figure 7c, respectively. To distinguish these 3https://pytorch.org/docs/stable/generated/torch.nn.utils. parametrizations.orthogonal.html Preprint. Under review. (a) Default Design. (b) Corner Case 1. (c) Corner Case 2. Figure 7: The visualization of the kernel fusion in FLATQUANT based on the computation within thread block. The design holds mainly for (a), where both transformations and quantization are fused together. For completeness, we also revise the design for corner cases in (b) and (c), when the SRAM is not large enough to hold the intermediate results. scenarios more clearly, we have the following equations: Default Design: Corner Case 1: Corner Case 2: (n1 n1 + 2 n1 n2) 2 < (n2 n2 + 2 n1 n2) 2 < (tn1 n1 + n1 n2 + tn1 n2) 2 < (n2 n2 + 2 tn1 n2) 2 < (n1 bn1 + bn1 n2 + n1 n2) 2 < (n1 bn2 + bn2 n2 + n1 n2) 2 < (5) (6) (7) where is the shared memory size per thread block, tn1 is the tiling size of non-reduction dimension of P1, bn1 is the tiling size of reduction dimension of P1, bn2 is the tiling size of reduction dimension of P2 and 2 refers to two bytes to hold tensors in float16. Below we review the designs for the two corner cases respectively. Corner Case 1. When both and n1 are excessively large, it is suggested to prevent from loading the entire P1 and into SRAM. We manage this by tiling the non-reduction dimension of P1 into tn1 slices. This strategy enables us to integrate P1 XP2 into one kernel, with P1 representing slice of P1 on the non-reduction dimension. Subsequently, we invoke separate fused kernel for quantization, computing the quantization scale and scaling the input. Corner Case 2. When both and n2 are extremely large, P1, and P2 cannot be loaded into X, where each thread block slicing SRAM together. To handle this, we first compute the non-reduction dimension of P1 and with the tiling shape bn1. The output is written back to the global memory, and the SRAM memory is thus released. Next, we slice the non-reduction dimension of and P2 with tiling size bn2 , and compute the matrix multiplication, followed by quantizing the result on the fly. = 1 Kernel Profiling. We enumerate popular hidden sizes in the series of LLaMA models, and provide the detailed profiling results of FLATQUANTs online transformation with and without kernel fusion in Table 9. Note that the SRAM can hold all of these shapes with the default design on the NVIDIA RTX 3090. It can be found that kernel fusion achieves significant speedup across various hidden dimensions and batch sizes, e.g., 1.5x-3x prefill speedup and 1.2x-4x decoding speedup, respectively. We also selectively test the two corner cases with the hidden size of 28762, both of which bring considerably 2.3x speedup. 16 Preprint. Under review. Hidden Dimension Batch Size without Kernel Fusion with Kernel Fusion Speedup Prefill Time (ms) Decode Time (ms) Prefill Time (ms) Decode Time (ms) Prefill Decode 4096 5120 8192 11008 13824 1 2 4 8 16 32 64 1 2 4 8 16 32 64 1 2 4 8 16 32 64 1 2 4 8 16 32 64 1 2 4 8 16 32 64 1 2 4 8 16 32 0.1956 0.3809 0.7199 1.4019 2.7628 5.5101 10.9752 0.2519 0.4915 0.9073 1.7582 3.4748 6.9079 13.8619 0.3845 0.7393 1.4433 2.8529 5.6668 11.3183 22.6714 0.6154 1.2032 2.3654 4.7570 9.4536 18.9102 38.2700 0.7260 1.4203 2.8088 5.6228 11.2297 22.4302 45.4374 0.6932 1.3466 2.6557 5.2910 10.5185 20.9249 42. 0.0184 0.0195 0.0212 0.0236 0.0307 0.0317 0.0328 0.0195 0.0205 0.0225 0.0266 0.0338 0.0358 0.0379 0.0195 0.0205 0.0205 0.0215 0.0225 0.0246 0.0297 0.0215 0.0225 0.0223 0.0236 0.0256 0.0287 0.0379 0.0225 0.0236 0.0246 0.0247 0.0266 0.0319 0.0471 0.0215 0.0225 0.0236 0.0246 0.0257 0.0317 0. 0.0625 0.1116 0.2120 0.4188 0.8417 1.7091 3.4898 0.1321 0.2570 0.5161 1.0363 2.0480 4.1313 8.2033 0.1608 0.3092 0.6257 1.2411 2.4904 4.9418 9.8459 0.3830 0.7547 1.5032 2.9983 6.0099 12.0444 24.0000 0.4444 0.8653 1.7254 3.4273 6.8726 13.7216 27.4698 0.4178 0.8233 1.6507 3.2922 6.5966 13.0601 25. 0.0082 0.0072 0.0082 0.0082 0.0073 0.0082 0.0082 0.0113 0.0113 0.0113 0.0113 0.0121 0.0123 0.0123 0.0132 0.0132 0.0123 0.0133 0.0133 0.0133 0.0143 0.0173 0.0173 0.0164 0.0174 0.0184 0.0195 0.0248 0.0184 0.0184 0.0184 0.0195 0.0195 0.0205 0.0275 0.0184 0.0184 0.0184 0.0195 0.0195 0.0205 0. 3.13x 3.41x 3.40x 3.35x 3.28x 3.22x 3.14x 1.91x 1.91x 1.76x 1.70x 1.70x 1.67x 1.69x 2.39x 2.39x 2.31x 2.30x 2.28x 2.29x 2.30x 1.61x 1.59x 1.57x 1.59x 1.57x 1.57x 1.59x 1.63x 1.64x 1.63x 1.64x 1.63x 1.63x 1.65x 1.66x 1.64x 1.61x 1.61x 1.59x 1.60x 1.65x 2.25x 2.71x 2.59x 2.88x 4.20x 3.87x 4.00x 1.73x 1.82x 2.00x 2.36x 2.80x 2.92x 3.08x 1.48x 1.55x 1.67x 1.62x 1.69x 1.85x 2.07x 1.24x 1.30x 1.36x 1.35x 1.39x 1.47x 1.53x 1.22x 1.28x 1.33x 1.27x 1.37x 1.56x 1.72x 1.17x 1.22x 1.28x 1.26x 1.32x 1.55x 1.73x Table 9: Prefill and decoding speedup of kernel fusion across different hidden dimensions and batch sizes. The sequence length is 2048 for prefill and 1 for decoding. The default kernel design holds for all the above settings."
        },
        {
            "title": "C ADDITIONAL EXPERIMENTS",
            "content": "C.1 MORE EXPERIMENTAL RESULTS Results on LLaMA-3.1-8B-Instruct. Besides the results of MT-Bench, we present the PPL results and the performance on QA tasks for LLaMA-3.1-8B-Instruct model in Table 10. WikiText-2 C4 ARC-C ARC-E HellaSwag LAMBADA PIQA Winogrande"
        },
        {
            "title": "Avg",
            "content": "FP"
        },
        {
            "title": "QuaRot\nFLATQUANT",
            "content": "7.22 9.25 7.97 11.38 15.13 12.99 55.20 45.39 52. 79.67 73.15 79.25 79.20 73.45 76.68 73.14 66.41 70. 81.12 76.01 79.49 73.80 66.61 73.09 73.69 66.84 72. Table 10: Evaluation results of FLATQUANT on LLaMA-3.1-8B-Instruct. KV Cache Quantization. As introduced in Section 4.4, while our primary focus is on weightactivation quantization, FLATQUANT serves as general framework applicable to various quantization tasks. To further evaluate its versatility, we apply FLATQUANT to KV cache only quantization. In this setting, we retain high precision for the rest of the model (including weights and activations) and apply the group-wise asymmetric quantization (with group size of 128) to keys and values. 17 Preprint. Under review. bits bits WikiText-2 16 4 4 4 3 3 3 2 2 2 4 3 2 4 3 2 4 3 2 6.14 6.20 6.25 6.60 6.35 6.41 6.84 7.70 7.79 8.93 9.45 9.56 9.66 10.33 9.91 10.03 10.83 13.36 13.44 16.13 ARC-C ARC-E HellaSwag LAMBADA PIQA Winogrande"
        },
        {
            "title": "Avg",
            "content": "53.50 52.82 52.90 49.32 52.05 52.47 47.44 49.15 46.67 42.92 77.57 78.20 77.65 74.37 77.95 76.85 73.91 74.62 71.63 68.60 79.12 79.13 79.00 77.88 78.41 78.25 77.18 74.74 74.17 71. 75.51 75.32 75.10 72.77 73.94 74.02 70.37 63.65 63.05 55.58 80.74 80.47 80.79 79.22 79.71 79.98 78.73 77.58 77.48 75.30 72.93 72.77 73.48 72.69 73.48 72.61 71.19 68.67 68.51 64. 73.23 73.12 73.15 71.04 72.59 72.36 69.80 68.07 66.92 63.06 Table 11: Different bits for KV cache quantization on the LLaMA-3-8B model."
        },
        {
            "title": "FLATQUANT",
            "content": "K bits bits LLaMA-2-7B LLaMA-2-13B 16 16 4 3 2 4 3 2 4 3 4 3 2 5.47 5.51 5.68 9.23 5.50 5.61 6.66 4.88 4.91 5.02 7. 4.91 5.00 5.69 Table 12: WikiText-2 perplexity of LLaMA-2 models with different bits of KV cache quantization. Table 11 presents the results of KV cache quantization using various bit-widths on the LLaMA-38B model. Consistent with previous studies (Hooper et al., 2024; Liu et al., 2024c; Ashkboos et al., 2024), we observe that keys are more sensitive to quantization than values. Furthermore, Table 12 compares FLATQUANT with QuaRot for KV cache quantization on LLaMA-2-7B and LLaMA-213B models. As shown, FLATQUANT delivers superior performance in most cases, particularly for lower-bit (2-3 bits). When both keys and values are quantized to 2 bits, FLATQUANT outperforms QuaRot by 2.57 in perplexity for the 7B model. LLaMA3-8B WikiText-2 FP QuaRot-W4A4KV4 FLATQUANT-W4A4KV4 6.14 8.16 6.98 QuaRot-W3A3KV3 FLATQUANT-W3A3KV3 686.54 10.82 9.45 13.38 11.13 630.89 19.03 ARC-C ARC-E HellaSwag LAMBADA PIQA Winogrande Avg 53. 45.73 50.00 25.34 35.41 77.57 70.83 75.80 28.41 63.26 79. 72.97 76.80 28.07 65.30 75.51 62.70 72.91 0.78 52.49 80. 75.35 79.16 50.71 73.56 72.93 67.17 72.69 48.70 60.69 73. 65.79 71.23 30.33 58.45 Table 13: Extreme low bit quantization results on LLAMA-3-8B models. Extreme Low-bit Quantization. We quantize the LLM to extreme low-bit representations (e.g., INT3) to investigate the limitations of quantization. The results in Table 13 show that FLATQUANT still keeps most of the models abilities in the 3-bit setting, whereas QuaRot struggles under such extreme low-bit conditions. Nevertheless, 4-bit quantization remains better balance between inference resource efficiency and acceptable performance degradation for now. C.2 ADDITIONAL DISCUSSIONS Calibration Set. Since FLATQUANT employs gradient-based method to optimize transformations for increased flatness, one reasonable concern is whether FLATQUANT might overfit the calibration set. To assess its generalization ability, we conducted an ablation study using different calibration datasets: WikiText-2, C4, and Pile. As shown in Table 14, FLATQUANT maintains stable performance across all datasets. For example, when calibrated on different datasets, FLATQUANT exhibits similar performance on WikiText-2, with PPL ranging from 6.98 to 7.04. On the C4 dataset, results are equally consistent, with PPLs between 11.05 and 11.13. Furthermore, QA accuracy remains within narrow range (71.04% to 71.23%), suggesting that FLATQUANT generalizes well across different calibration datasets. This robustness is attributed to FLATQUANTs focus on learning 18 Preprint. Under review. an equivalent affine transformation with minimal quantization loss, rather than altering the models weights. Nevertheless, it is reasonable to assume that the diversity of calibration data can further enhance the performance of our method. Calibration set WikiText-2 C4 ARC-C ARC-E HellaSwag LAMBADA PIQA Winogrande"
        },
        {
            "title": "Avg",
            "content": "WikiText2 C4 Pile 6.98 7.04 7.04 11.13 11.05 11.08 50.00 50.34 51.11 75.80 75.38 77.36 76.80 76.74 76. 72.91 73.28 72.37 79.16 78.67 78.94 72.69 71.82 70.56 71.23 71.04 71.16 Table 14: Ablation study of FLATQUANTs calibration set on LLaMA-3-8B model. Effect of Clipping. Unlike weight clipping, which has been widely utilized in LLM quantization, activation clipping has been less explored. Although previous studies (Ashkboos et al., 2024; Liu et al., 2024b) show that activation clipping offers only modest benefits for quantization, our method demonstrates that LCT provides significant improvements. As shown in Table 15, applying our transformations prior to clipping allows for greater proportion of values to be clipped, resulting in better performance. In contrast, applying LCT before the transformation, similar to the approach used in RTN quantization, yields only limited improvements. This is consistent with prior findings (Dettmers et al., 2022) and is largely due to the presence of severe outliers in activation. We also report results using QuaRot-style clipping method, with 0.9 as the activation clipping threshold and 0.95 for the KV cache clipping threshold. In summary, the integration of transformations enhances the effectiveness of clipping, indicating that their combination significantly improves weight-activation quantization. LLaMA3-8B FP16 w/o LCT LCT before Transformation QuaRot-style Fixed Threshold LCT after Transformation WikiText-2 6.14 7.95 7.37 7.25 6. C4 9.45 12.74 11.86 11.62 11.13 ARC-C ARC-E HellaSwag LAMBADA PIQA Winogrande Avg 53. 44.20 48.72 48.21 50.00 77.57 71.89 76.18 75.29 75.80 79.12 74.21 75.11 75.66 76.80 75. 68.72 66.65 71.32 72.91 80.74 77.15 77.91 78.73 79.16 72.93 66.30 67.17 70.01 72.69 73. 67.08 68.62 69.87 71.23 Table 15: The effect of Learnable Clipping Thresholds. C.3 EXPERIMENT DETAILS OF FIGURE 5 In Figure 5, we present the prefill speedup and WikiText2 PPL results of different decomposed matrix sizes on LLaMA-2-7B model. We decompose the hidden dimension 4096 into n1 n2 and range n1 from 1 to 2048, where n1 = 1 amounts to maintaining full-size transformation matrix. The intermediate dimension 11008 is decomposed into 64 172 as done in FLATQUANT. For PPL evaluation, we only quantize the last Transformer block and learn the affine transformations within it. For speedup evaluation, we do not leverage the online transformation kernel in Section 3.3 and implement online transformations with naive matrix multiplication in PyTorch. C.4 ADDITIONAL ANALYSES OF INFERENCE LATENCY Baseline. We implement and report the latency results of INT4 quantization and QuaRot with QuaRots official code4. These baselines share the same quantization settings with FLATQUANT as described in Section 4.1 for fair comparison. End-to-end Speedup. We decode 256 tokens after the prefill on sequence length of 2048 and provide the prefill and decoding speedup of FLATQUANT in Figure 8 and Figure 9. FLATQUANT achieves prefill speedup of 2.30x and decoding speedup of 1.76x under the batch size of 64, with only 0.07x speedup loss compared to the naive INT4 quantization for both prefill and decoding. Note that when the batch size is smaller than 16, quantization overhead outweighs the benefits brought by KV cache memory reduction for the decoding stage, resulting in less than 1x speedup for both INT4 4https://github.com/spcl/QuaRot 19 Preprint. Under review. Figure 8: Prefill speedup of LLaMA-2-7B on sequence length of 2048. Figure 9: Decoding speedup on LLaMA-2-7B model. We decode 256 tokens after the prefill on sequence length of 2048. quantization and FLATQUANT. However, since the decoding speedup shows good scalability with the batch size, we can gain practical decoding speedup simply by employing large batch size. Total FLOPs of Online Transformations. (1) Self-Attention. The self-attention module has three online transformations, i.e., {Pa, Po, Ph}. Suppose the hidden dimension hd and intermediate dimension hi of LLM can be perfectly decomposed into hi, respectively, d/a, where is the batch size, then the total FLOPs of {Pa, Po, Ph} is 4bshd is the sequence length, and is the number of attention heads. (2) Feed-forward Network. The feedforward module has two online transformations, i.e., {Pug, Pd}. The total FLOPs of {Pug, Pd} is hi. In summary, the total FLOPs of the online transformations in Transformer 4bshd hi. In LLaMA-2-7B (i.e., hd = 4096, block amounts to 8bshd hi = 11008 and = 32), the FLOPs of online transformations only account for about 2.61% of those of the FP16 model when reaches 2048. hd + 2bshda + 4bsh2 hd +2bshda+4bsh2 d/a + 4bshi hd +4bshi hd and hd hi"
        },
        {
            "title": "D ADDITIONAL VISUALIZATIONS",
            "content": "D.1 MORE VISUALIZATIONS OF WEIGHT AND ACTIVATION DISTRIBUTIONS Experiment Details. We visualize the distribution of weights and activations after different transformations, including per-channel scaling in SmoothQuant (Xiao et al., 2023), Hadamard transformation in QuaRot (Ashkboos et al., 2024), and affine transformation in FLATQUANT. We compute the per-channel Frobenius norm to quantify the channel magnitude. We randomly sample from the C4 (Raffel et al., 2020) dataset to collect activation statistics. Visualizations on the LLaMA Models. We visualize the distribution envelopes of both original and transformed weights and activations on the LLaMA models in Figure 10-14. It can be observed that neither per-channel scaling nor Hadamard transformation can fully smooth out outlier channels to produce flatness, still leaving outlier channels, especially on activations. On the other hand, the 20 Preprint. Under review. affine transformation learned by FLATQUANT can effectively produce flatter distributions for both weights and activations which are easier to quantize. D.2 MORE VISUALIZATIONS OF QUANTIZATION ERROR LANDSCAPES Experiment Details. We randomly sample 128 samples from the C4 (Raffel et al., 2020) dataset and compute their average mean squared error for visualization. For per-channel scaling, we follow SmoothQuant (Xiao et al., 2023) and only perform per-channel scaling for the inputs of the self-attention and feed-forward modules. For the Hadamard transformation, we replace the affine transformation in FLATQUANT with fixed Hadamard transformation. The quantization settings are the same as those described in Section 4.1. Visualizations on the LLaMA Models. We visualize the quantization error landscapes of LLaMA models in Figure 2 and Figure 15-18. With the affine transformation to smooth outliers, FLATQUANT can effectively suppress the quantization errors at pivot tokens and ease the quantization error propagation, leading to flatter quantization error landscape compared with per-channel scaling and Hadamard transformation."
        },
        {
            "title": "E LIMITATIONS",
            "content": "In this study, we present FLATQUANT, but there are certain limitations to acknowledge. First, the full potential of 4-bit quantization has not been thoroughly explored. While we follow the previous studies to build the calibration set and demonstrate that FLATQUANT is robust across various data sources, the optimal selection of calibration sets remains an open question. Additionally, our focus has primarily been on the INT4 data type, and we have not examined the integration of FLATQUANT with newer data types, such as MXFP4, which may offer advantages over INT4. Addressing these aspects represents promising avenues for future research. (a) Wo of the 10th Transformer layer in LLaMA-2-7B. (b) Xo of the 10th Transformer layer in LLaMA-2-7B. (c) Wg of the 30th Transformer layer in LLaMA-2-7B. (d) Xg of the 30th Transformer layer in LLaMA-2-7B. Figure 10: Distributions of weights and inputs from LLaMA-2-7B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. 21 Preprint. Under review. (a) Wo of the 10th Transformer layer in LLaMA-2-13B. (b) Xo of the 10th Transformer layer in LLaMA-2-13B. (c) Wg of the 30th Transformer layer in LLaMA-2-13B. (d) Xg of the 30th Transformer layer in LLaMA-2-13B. Figure 11: Distributions of weights and inputs from LLaMA-2-13B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. (a) Wo of the 10th Transformer layer in LLaMA-2-70B. (b) Xo of the 10th Transformer layer in LLaMA-2-70B. (c) Wg of the 30th Transformer layer in LLaMA-2-70B. (d) Xg of the 30th Transformer layer in LLaMA-2-70B. Figure 12: Distributions of weights and inputs from LLaMA-2-70B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. (a) Wo of the 10th Transformer layer in LLaMA-3-8B. (b) Xo of the 10th Transformer layer in LLaMA-3-8B. (c) Wg of the 30th Transformer layer in LLaMA-3-8B. (d) Xg of the 30th Transformer layer in LLaMA-3-8B. Figure 13: Distributions of weights and inputs from LLaMA-3-8B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. (a) Wo of the 10th Transformer layer in LLaMA-3-70B. (b) Xo of the 10th Transformer layer in LLaMA-3-70B. (c) Wg of the 30th Transformer layer in LLaMA-3-70B. (d) Xg of the 30th Transformer layer in LLaMA-3-70B. Figure 14: Distributions of weights and inputs from LLaMA-3-70B, sorted by the channel magnitudes (i.e., the Frobenius norm) in descending order. 22 Preprint. Under review. (a) Per-channel Scaling. (b) Hadamard Transform. (c) FLATQUANT. (d) Stacked View. Figure 15: The mean squared error (MSE) of quantization across Transformer layers and input sequence in LLaMA-2-7B. Figure 15a-15c plot the MSE surface of each method, while Figure 15d overlays these surfaces by dividing each MSE with that of FLATQUANT. (a) Per-channel Scaling. (b) Hadamard Transform. (c) FLATQUANT. (d) Stacked View. Figure 16: The mean squared error (MSE) of quantization across Transformer layers and input sequence in LLaMA-2-13B. Figure 16a-16c plot the MSE surface of each method, while Figure 16d overlays these surfaces by dividing each MSE with that of FLATQUANT. (a) Per-channel Scaling. (b) Hadamard Transform. (c) FLATQUANT. (d) Stacked View. Figure 17: The mean squared error (MSE) of quantization across Transformer layers and input sequence in LLaMA-2-70B. Figure 17a-17c plot the MSE surface of each method, while Figure 17d overlays these surfaces by dividing each MSE with that of FLATQUANT. (a) Per-channel Scaling. (b) Hadamard Transform. (c) FLATQUANT. (d) Stacked View. Figure 18: The mean squared error (MSE) of quantization across Transformer layers and input sequence in LLaMA-3-70B. Figure 18a-18c plot the MSE surface of each method, while Figure 18d overlays these surfaces by dividing each MSE with that of FLATQUANT."
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab",
        "Shenzhen International Graduate School, Tsinghua University",
        "The Chinese University of Hong Kong"
    ]
}