{
    "paper_title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis",
    "authors": [
        "Tianbao Xie",
        "Jiaqi Deng",
        "Xiaochuan Li",
        "Junlin Yang",
        "Haoyuan Wu",
        "Jixuan Chen",
        "Wenjing Hu",
        "Xinyuan Wang",
        "Yuhui Xu",
        "Zekun Wang",
        "Yiheng Xu",
        "Junli Wang",
        "Doyen Sahoo",
        "Tao Yu",
        "Caiming Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 7 2 2 3 1 . 5 0 5 2 : r Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis Tianbao Xie Jiaqi Deng Xiaochuan Li Junlin Yang Haoyuan Wu Jixuan Chen Wenjing Hu Xinyuan Wang Yuhui Xu Zekun Wang Yiheng Xu Junli Wang Doyen Sahoo Tao Yu Caiming Xiong The University of Hong Kong sSalesforce AI Research"
        },
        {
            "title": "Abstract",
            "content": "Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWORLD-G, comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset JEDI, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on JEDI demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWORLD-G. Furthermore, we demonstrate that improved grounding with JEDI directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io."
        },
        {
            "title": "Introduction",
            "content": "Graphical user interface (GUI) grounding, the ability to accurately map natural language instructions to specific actions (including the positions of on-screen elements), is cornerstone for computer use agents to effectively interact with GUIs on devices such as mobile phones and desktop computers. It plays critical role, whether as an isolated component of human-machine interaction, facilitator of multi-model collaboration agents, or means to enhance end-to-end models. Achieving practical GUI grounding requires software commonsense (e.g., understanding the meaning of icons, the functions of components, and specific software knowledge), layout understanding (e.g., interpreting sidebar on one side or elements under panel) and fine-grained component manipulation (e.g., adjusting slider or selecting text on character level). Knowledge and grounding together enable comprehension and interaction. Additionally, rejecting infeasible instructions (e.g., mistaking Thunderbird for Firefox) is necessary to avoid unrecoverable states. Previous work around GUI grounding oversimplify these tasks as short referring expressions. Such descriptions are clear but leave gap with real-world requirements. As result, existing benchmarks like ScreenSpotv2 [10, 41] show saturation at early stages (90%) accuracy by recent approaches [32] together with the progress of vision-language models (VLMs) [see 8, 39, 5, i.a.], primarily focusing on simple instructions to locate referenced elements in screenshots. Current evaluation approaches either lack nuance in their assessment criteria or artificially inflate difficulty through unnatural Equal contribution. Corresponding authors. Work mainly done during TXs internship in Salesforce. Preprint. Figure 1: We have developed comprehensive benchmark comprising 564 examples that cover the diverse task types that previous work has overlooked. Additionally, we synthesize and release the largest computer use grounding dataset containing 4 million examples, and train models that achieve state-of-the-art performance on this dataset. conditions, such as ScreenSpot-Pros extreme resolutions that rarely occur in typical computing environments. Achieving practical grounding requires software context awareness and fine-grained manipulation capabilities for diverse GUI elements including dropdown menus, tabbed interfaces, scrollbars, and context-sensitive controls that have not been adequately measured or explored. On the data side, the primary capabilities of current grounding models arise from structured text and screenshot correspondences found on webpages(e.g., SeeClick [10], UGround [15], OmniParser [27], OS-Atlas [41], Aria-UI [44]). Alternatively, they rely on manually annotated data (e.g., Aguvis [43], UI-TARS [32]). The former can capture coarse-grained element understanding signals for webpage but lacks fine-grained operational capabilities for UI elements. The latter, due to high manual annotation costs, struggles to scale effectively. To better assist the community in addressing GUI grounding challenges, we start with benchmarks and data as shown in Figure 1. We develop the OSWORLD-G, comprising 564 finely annotated samples that systematically cover text matching, element recognition, layout understanding, fine-grained manipulation and infeasibility, with annotations for the element types required to solve each task. On the data side, we collect and synthesize the largest-scale open grounding dataset in the web and desktop domain through multi-perspective decoupling of tasks. Additionally, we train multi-scale models on this dataset to validate its effectiveness. Our evaluation on ScreenSpot-v2, ScreenSpot-Pro and OSWORLD-G demonstrates that our approach significantly outperforms existing models in aspect of grounding ability. Beyond standalone grounding performance, we show that improved grounding directly translates to enhanced agentic capabilities on complex tasks in OSWorld [42] and WindowsAgentArena [6] benchmarks. Through detailed ablation studies, we identify key factors that most significantly contribute to grounding performance, providing insights for future data collection and training efforts to enhance such abilities. Our case studies verify the effectiveness of our decomposition hypothesis, demonstrating that combining specialized data for different interface elements enables compositional generalization to novel interfaces."
        },
        {
            "title": "2 Approach",
            "content": "Task Definition Multimodal Agent is an AI system that visually perceives the GUI from the environment. At each step t, it receives visual observation Ot (e.g., pixel data RHW C) and executes an action at based on natural language instruction and its current observation (and potentially history). The agent learns policy π : (Ot, I, statet) at to generate the sequence of actions = {a1, . . . , an}, purely from visual perception without access to the GUIs underlying code or APIs. An action at consists of an action type (e.g., click, move_to, type) and action parameters that typically involve coordinates, represented as either point (x, y) or bounding box (x, y, w, h) to specify the target GUI element. GUI Grounding represents the core capability enabling the policy π to function effectively at each step t. Given potentially step-specific interpretation or sub-instruction It (derived explicitly or implicitly from I) and the current observation Ot, grounding is the process of mapping these inputs to the specific, executable action at. Achieving accurate grounding for each 2 (It, Ot) pair is fundamental objective in training the agent and key determinant of the policys success on the overall task."
        },
        {
            "title": "2.1.2 Data Types\nLeveraging the fine-grained element type\ntags, we categorize tasks into capabil-\nity dimensions that directly reflect core\ntext matching, el-\nmodel competencies:\nement recognition, layout understanding,\nfine-grained manipulation, and refusal han-\ndling, as presented in Table 1.",
            "content": "Table 1: Distribution of examples in the OSWORLD-G benchmark categorized by GUI grounding capabilities and their corresponding interface element types. Full table can be refer to Appendix A.1."
        },
        {
            "title": "Element Types",
            "content": "# of Examples"
        },
        {
            "title": "Element Recognition",
            "content": "Icon, Image, Button"
        },
        {
            "title": "Layout Understanding",
            "content": "Tab, Menu Bar, Dropdown Menu, Panel/Container, . . . Fine-grained Manipulation Slider, Stepper, Text Field, Input Box, Divider, Table, . . . 268 337"
        },
        {
            "title": "Refusal",
            "content": "Text Matching & Element Recognition Most cases in GUI grounding require simply text matching and element recognition as two fundamental capabilities. Text matching involves grounding actions according to explicit textual information provided in instructions (e.g., Select As Attachment). This requires matching the specified text to locate the appropriate screen region. Element recognition encompasses multiple aspects of visual understanding: identifying visual patterns such as icons or images (e.g., Click on Ellipse icon), and importantly, recognizing elements based on their implied functionality rather than explicit labels. For example, recognizing Save button by its floppy disk icon, Settings option by its gear icon, or Search function by its magnifying glass symbolall cases where the agent must associate visual elements with their functional purpose, even when no explicit text label is present. 54 Layout Understanding GUIs are typically designed with modular structures. Knowledge of layout hierarchy is critical to locate elements precisely, as visually similar elements may exist across different modules, and describing elements often requires referencing their position within the layout. For instance, instructions like Close the top notification bar require correct identification of the notification bar area, as multiple similar close buttons may appear throughout the interface. Other cases require identification of toolbars, panels, pop-up windows, and other common GUI modules. 2https://app.cvat.ai/ 3 Figure 2: Example of layout understanding case in OSWORLD-G. Fine-grained Manipulation Computer use agent tasks frequently involve text editing operations. Instructions such as Select the place between the world person and the number 1 require precise cursor placement between specific letters, which may occupy only small portion of the screen. Such actions demand the ability to perform operations with high precision within relatively small screen regions. Beyond text, this capability extends to interaction with compact components like sliders, steppers, table cell and other small elements. Figure 3: Example of fine-grained manipulation case in OSWORLD-G. Infeasible Certain tasks may arise from hallucinated or incorrect low-level user instructions or automated planning suggestions. An example could be an instruction like, Click to open the Firefox browser, when the shown screenshot does not contain Firefox icon or any visible reference to it. distinct subset of OSWORLD-G tasks with 54 examples explicitly highlights these infeasible scenarios. These tasks are valuable for evaluating systems ability to reject impossible instructions gracefully, preventing errors and ensuring safer, more robust interactions. 2."
        },
        {
            "title": "JEDI Data Construction",
            "content": "Figure 4: An overview of the synthetic data generation pipeline, demonstrating how screenshots and metadata are collected and synthesized, and subsequently converted into training data. To enable robust GUI grounding, we construct the worlds largest multimodal dataset tailored for computer-use grounding scenarios, containing 4 million newly synthesized examples. Our grounding data collection process centers on gathering pairs of screenshots and metadata (information such as filename, rendering code, element bounding box, etc.), which are then further transformed into training data which contains queries and corresponding answers for VLMs to learn from it. Previous methods in Figure 4 provides an overview of this pipeline."
        },
        {
            "title": "2.2.1 Icon\nIcons are essential visual elements in graphical user interfaces that convey functionality through\ncompact, recognizable imagery. To create a comprehensive collection of icons and corresponding\nmetadata for grounding, we employ three complementary data collection strategies.",
            "content": "GitHub Repositories and Specialized Icon Websites Many open-source software projects archive their design icons within GitHub repositories. To acquire varied collection, we systematically mine repositories containing the key term such as icon applying filtering criteria including star count, quantity of icon images, and temporal relevance. This yield icons representing various design paradigms such as flat design, fluent design, and skeuomorphism. To supplement our collection with production website icons, we implement targeted web crawling pipeline that identifies and extracts icon elements from popular websites across various categories, capturing both visual assets 4 and associated metadata including class names, aria labels, and contextual information. We leverage these icons by generating detailed descriptions through LLMs and creating training scenarios where models identify target icons based on textual descriptions. This comprehensive approach provides access to contemporary icons in their natural context, allowing us to capture emerging design patterns not yet available in open-source repositories. Reverse Engineering Software To address the gap in desktop software icons, we employ reverse engineering techniques using specialized tools like IconsExtract to extract icons directly from executable files, DLLs, and system libraries across Windows, macOS and Ubuntu. We target diverse range of software including Windows system applications and commonly used desktop applications. This method provide access to thousands of production-quality icons representing real-world software functionality."
        },
        {
            "title": "2.2.2 Component\nA component refers to a functional unit composed of icons, UI elements, and text, collectively\nenabling specific modes of computer-based interaction. Components serve as fundamental interaction\nunits essential for user engagement in digital environments.",
            "content": "Synthesis Process We collect screenshots and associated metadata primarily through code-andrendering pipeline. By leveraging mainstream production-level UI component libraries commonly used in front-end development (e.g., Material UI), we select components and use their example code as the base code. We then employ LLMs to synthesize functional cases for specific tasks (such as slider for air conditioning control) using the base code as context. We render these within React application to obtain visual screenshots and extract corresponding metadata, such as the element position tree, built-in component names, and coordinates. This approach allows us to generate diverse component examples with precise ground truth source code. Real-world Augmentation We observe that common interactive behaviors such as scrolling webpage, clicking cell in spreadsheet or resizing text box in slides are underrepresented in code-based libraries. To address this gap, we further source real-world screenshots from existing websites and applications. For these, we utilize HTML parsing and application-specific tools (e.g., python-pptx) to extract structured metadata. All the implementation details can be found in the Appendix A.2.3."
        },
        {
            "title": "2.2.3 Layout\nA layout refers to the spatial arrangement of UI elements and components within an application or\nacross the entire operating system. Layout understanding is crucial for tasks that require reasoning\nabout the overall structure of the screen, enabling agents to interpret and interact with complex,\nmulti-element interfaces at the application or system level.",
            "content": "Prototype Designs UI prototype platforms such as Figma 3 provide numerous website and application design templates, including authentic specifications for production applications like VSCode, Zoom, and Microsoft 365. These designs offer valuable ground truth information, as each element includes designer-specified bounding boxes, component types, and functional descriptions. By using the official APIs of these platforms, we exported the designs as high-quality images while preserving their structured metadata, including hierarchical relationships between elements and positional data. Real-World Application Screenshots To further improve scalability and diversity, we supplement our dataset with raw screenshots captured from real-world applications running on operating systems. We collect these screenshots by leveraging agent rollout data from OSWorld and WindowsAgentArena. Subsequently, we utilize the object detection model from OmniParser-v2 to generate bounding boxes for interface elements, thereby obtaining the necessary metadata."
        },
        {
            "title": "2.2.4 Data Processing\nAfter obtaining screenshots (icons, components, layouts) and metadata (filenames, paths, rendered\nsource code, UI designer annotations, etc.), we convert them into an image-text-to-text multimodal\nquestion-answering format, creating richer and more natural language-oriented data suitable for",
            "content": "3https://www.figma.com/ 5 VLM training. Our processing approach remain consistent across the different data types. We employ VisualSketchpad [18]-like prompting methodology with models such as GPT-4o and Claude to generate enriched annotations based on the original screenshots and metadata, describing both appearance and functionality. We construct two complementary training formats: (1) grounding format, where the model receives screenshot with instructions and must predict actions or relevant bounding boxes; and (2) description format, where the model receives screenshot with bounding boxes and must provide descriptive information. For screenshots with multiple potential query-answer pairs, we compress them into single conversation to improve training efficiency."
        },
        {
            "title": "3 Experiments",
            "content": "We first adapt previous benchmarks for testing our data effectiveness. We adapt different sizes of the latest Qwen2.5-VL [5] as our backbone model, set the maximum pixel limit to approximately 1080p. Model finetuning takes approximately 20 hours for the 3B model, and 30 hours for the 7B model, conducted using cluster of 128 CPU cores, 512GB memory, and 64 NVIDIA H100 GPUs."
        },
        {
            "title": "3.1 Grounding Ability",
            "content": "Table 2: Comparison of various planners and grounding methods on ScreenSpot-v2."
        },
        {
            "title": "Text",
            "content": "Icon/Widget Text Icon/Widget Text Icon/Widget - GPT-4o SeeClick 78.4 OS-Atlas-Base-7B 95.2 96.9 UI-TARS-7B 47.3 Operator 93.4 Qwen2.5-VL-3B 97.6 Qwen2.5-VL-7B OS-Atlas-Base-7B 96.2 95.5 OmniParser-v2 JEDI-3B JEDI-7B 96.6 96.9 50.7 75.8 89.1 41.5 73.5 87.2 83.4 74.6 81.5 87. 70.1 90.7 95.4 90.2 88.1 90.2 89.7 92.3 96.9 95.9 29.3 63.6 85.0 80.3 58.6 74.2 69.3 60.9 78.6 87. 55.2 90.6 93.6 92.8 88.0 93.2 94.0 88.0 88.5 94.4 32.5 77.3 85.2 84.3 71.4 81.3 79.8 59.6 83.7 84."
        },
        {
            "title": "Avg",
            "content": "55.1 85.1 91.6 70.5 80.9 88.8 87.1 80.7 88.6 91.7 We select several benchmarks for GUI grounding. The most commonly used benchmarks in the past include ScreenSpot-v2  (Table 2)  , ScreenSpot-Pro  (Table 3)  , which focuses on high-resolution and professional software charts, and OSWORLD-G  (Table 4)  , which we use to evaluate model performance on fine-grained and functional components. The results show that fine-tuning existing open-source models on our data achieves state-of-the-art performance, surpassing other dedicated computer use model such as Operator (unpublished data and model) and UI-TARS (unpublished data) with small model size. On OSWORLD-G, we observe that models generally achieve the highest accuracy on examples involving text matching, outperforming their abilities in element recognition and layout understanding, with the lowest performance observed in fine-grained manipulation tasks. Notably, although we included refusal data during training to encourage the model to reject instructions referring to elements not present on the screen, the model rarely produces refusal responses. Similarly, in all models except Gemini-2.5-Pro, especially those specifically trained for computer-use tasks, refusal predictions are consistently absent. 3.2 Agentic Ability We hope that the data and benchmark we provide will ultimately serve as critical signal in fostering the agentic capabilities required, rather than merely enhancing specific grounding abilities. We evaluate our approach on the computer use benchmarks in online environments, namely OSWorld [42] and WindowsAgentArena [6]. We employ GPT-4o as the planner model, which receives high-level instructions and, at each step, predicts the next low-level natural language instruction based on the 6 Table 3: Comparison of models on ScreenSpot-Pro."
        },
        {
            "title": "Office",
            "content": "OS"
        },
        {
            "title": "Avg",
            "content": "SeeClick [10] ShowUI-2B [24] Aria-UI [44] Operator [30] OS-Atlas-7B [41] UGround-V1-7B [15] UI-TARS-7B [32] Qwen2.5-VL-3B Qwen2.5-VL-7B JEDI-3B JEDI-7B"
        },
        {
            "title": "Text",
            "content": "0.6 16.9 16.2 50.0 33.1 - 58.4 38.3 51.9 61.0 42."
        },
        {
            "title": "Icon Avg",
            "content": "0.0 1.4 0.0 19.3 1.4 - 12.4 3.4 4.8 13.8 11.0 0.3 9.4 8.4 35.1 17.7 35.5 36.1 21.4 29.1 38.1 27.4 1.0 9.1 23.7 51.5 28.8 - 50.0 40.9 36.9 53.5 50. 0.0 0.0 2.1 23.1 2.8 - 9.1 4.9 8.4 8.4 11.9 0.6 5.3 14.7 39.6 17.9 27.8 32.8 25.8 24.9 34.6 34.0 2.5 2.5 7.6 16.8 12.2 - 20.8 22.3 17.8 27.4 38. 0.0 0.0 1.6 14.1 4.7 - 9.4 6.3 1.6 9.4 14.1 1.9 1.9 6.1 16.1 10.3 13.5 18.0 18.4 13.8 23.0 32.2 3.5 13.2 27.1 58.3 37.5 - 63.9 44.4 48.6 54.2 72. 0.0 7.3 6.4 24.5 7.3 - 31.8 10.0 8.2 18.2 25.5 2.0 10.6 18.1 43.7 24.4 38.8 50.0 29.5 31.1 38.6 52.4 1.1 15.3 20.3 60.5 33.9 - 63.3 48.0 53.7 64.4 75. 0.0 7.5 1.9 28.3 5.7 - 20.8 17.0 18.9 32.1 47.2 0.9 13.5 16.1 53.0 27.4 48.8 53.5 40.9 45.7 57.0 68.7 2.8 10.3 4.7 34.6 27.1 - 30.8 33.6 34.6 38.3 33. 0.0 2.2 0.0 30.3 4.5 - 16.9 4.5 7.9 9.0 16.9 1.5 6.6 2.6 32.7 16.8 26.1 24.5 20.4 22.4 25.0 26.0 1.8 10.8 17.1 45.0 28.1 - 47.8 37.8 39.9 49.8 52. 0.0 2.6 2.0 23.0 4.0 - 16.2 6.6 7.6 13.7 18.2 1.1 7.7 11.3 36.6 18.9 31.1 35.7 25.9 27.6 36.1 39.5 Table 4: Performance comparison of models on OSWORLD-G across multiple capability dimensions."
        },
        {
            "title": "Agent Model",
            "content": "Text Matching Element Recognition Layout Understanding Fine-grained Manipulation Refusal Overall OS-Atlas-7B UGround-V1-7B Aguvis-7B UI-TARS-7B Gemini-2.5-Pro Operator Qwen2.5-VL-3B Qwen2.5-VL-7B JEDI-3B JEDI-7B 44.1 51.3 55.9 60.2 59.8 51.3 41.4 45.6 67.4 65.9 29.4 40.3 41.2 51.8 45.5 42.4 28.8 32. 53.0 55.5 35.2 43.5 43.9 54.9 49.0 46.6 34.8 41.9 53.8 57.7 16.8 24.8 28.2 35.6 33.6 31.5 13.4 18.1 44.3 46.9 7.4 0.0 0.0 0.0 38.9 0.0 0.0 0. 7.4 7.4 27.7 36.4 38.7 47.5 45.2 40.6 27.3 31.4 50.9 54.1 current observation and action history. Our JEDI model then takes these low-level instructions and predicts the concrete actions to execute. To control for confounding variables, we do not introduce any specialized agent architecture or model scheduling [1]. Table 5: Success rate on the OSWorld and WindowsAgentArena benchmarks. The results are the average success rate of 4 runs with standard deviation. More detailed performance see A."
        },
        {
            "title": "OS SR WAA SR",
            "content": "GPT-4o Kimi-VL (15 steps) Aguvis-72B (15 steps) UI-TARS-72B (50 steps) Operator (15 steps) Operator (50 steps) Claude 3.7 Sonnet (50 steps) 5.0 8.2 10.3 22.7 19.7 32.6 26.0 9.4 10.4 - - - - - GPT-4o GPT-4o GPT-4o GPT-4o GPT-4o Aguvis-72B (15 steps) JEDI-3B (15 steps) JEDI-7B (15 steps) JEDI-7B (50 steps) JEDI-7B (100 steps) 17.0 22.4 0.33 22.7 0.51 25.0 1.30 27.0 1. - 29.1 0.57 30.2 1.17 32.8 0.86 33.7 0."
        },
        {
            "title": "Refined",
            "content": "63.7 63.8 57.1 57.5 51.8 59 54.1 50.9 47 45.2 46. ) % ( R c 65 55 50 45 UI-TARS-72B Gemini-2.5-pro Operator Qwen2.5-VL-32B JEDI-3B JEDI-7B Figure 5: Success rates of various models on the OSWORLD-G benchmark with original and refined instructions. The results demonstrate that, when using our model as the grounding component, simple agent with foundation models that are not specialized in computer use tasks can achieve stateof-the-art performance, surpassing previous approaches that used 72B-scale models for grounding, and matching the performance of specialized models. Additionally, our agent system exhibits similar trend to Operator, with performance improving as deployment scale increases. These findings suggest that, given the current reasoning capabilities of large language models, supplementing them with enhanced grounding abilityeither through additional data or external systemscan be starting point to build highly effective agentic systems. GUI grounding also requires knowledge and even reasoning. We aim to investigate the performance of pure grounding when almost no additional knowledge is required. To this end, we assume that the instruction recipient possesses minimal prior experience with GUI interactions, and we re-annotate the entire benchmark to minimize the background knowledge needed to understand each instruction. This is achieved by relying on easily identifiable universal features such as color and shape. For example, the instruction Open the filter function for search settings. is refined, based on the screenshot, to Click the button that includes an icon of funnel on the right of the search settings bar. We conduct experiments on several models and present the performance comparison before and after instruction refinement in Figure 5. First, we observe that model perfor7 mance generally improves after instruction refinement. This suggests that if we can supplement models with relevant interaction experience or provide more precise expressionseither manually or via upstream modelsgrounding performance can be enhanced. Second, after instruction refinement, our model achieves performance comparable to the largest state-of-the-art model, UI-TARS-72B. This indicates that, with appropriate data such as our JEDI dataset, smaller models are already sufficient in terms of pure grounding ability, and further advantages may lie in the supplementation of background knowledge."
        },
        {
            "title": "4.2 Performance as Data Scaling",
            "content": "ScreenSpot-v2 Performance OSWorld-G Performance 90 80 70 ) % ( R c 10% 20% 50% Training Data (%) 100% ) % ( R c 45 35 30 25 10% 20% 50% Training Data (%) 100% Figure 6: The effect of training data percentage on Qwen2.5-VL-3B model performance across different UI elements. Blue line: Icon; Orange line: Component; Green line: Layout; Red line: All. Left: ScreenSpot-v2 benchmark; Right: OSWORLD-G benchmark. We aim to investigate whether collecting data through our pipeline enables further performance improvements as the data scale increases. We sample data of icon, component, and layout at proportions of 10%, 20%, 50%, and 100%. For each data proportion, we train the models for the same number of steps, ensuring that all models are sufficiently trained to allow fair comparison of final performance under equal computational resources. The results are shown in Figure 6. First, we observe that as the data scale increases, model performance continues to improve, with no sign of saturation. This suggests that further scaling up the data using our proposed approach can yield additional gains. Second, we note that scaling up single data type (e.g., component) can lead to performance fluctuations. In contrast, scaling up mixed data types results in more stable improvements, indicating that combining data from multiple sources is beneficial."
        },
        {
            "title": "4.3 Case Study",
            "content": "We conducted detailed comparison of JEDI-7B and Qwen2.5-VL-7B-Instruct using OSWORLD-G. To illustrate the improvements of JEDI, we selected representative cases where their results differed, as shown in Figure 7. In each subfigure, the green square represents the click position of JEDI, which is the correct grounding action, while the red square indicates the erroneous click position of Qwen. In these examples, JEDI showcases exceptional fine-grained operational capabilities and comprehension skills in locating and matching information. As illustrated in the subfigure 7a, JEDI successfully identifies the target cell without an explicit location (like E19) by using information from both the timestamp and the table header. Similarly, by understanding the paragraph text and accurately identifying relative positions, JEDI effectively addresses the case presented in the 7b. Furthermore, as illustrated in the 7c, by learning from web page layouts, JEDI exhibits generalization to desktop environments, accurately locating the specified blank cell based on the positional description. Additionally, benefiting from training on extensive icon data, JEDI successfully associated the icon (a counter-clockwise arrow) with its corresponding function (rotate), as depicted in the 7d. Further analysis of additional examples can be found in Appendix A.5.1 and A.5.2."
        },
        {
            "title": "5 Related Work",
            "content": "Digital Agents Multimodal agents can be broadly categorized into digital and physical agents [34, 35, 14]. Existing digital agent research focuses on establishing environments for mobile and web interaction [34, 25, 29, 38, 45, 50, 33, 47, 20, 13], with subsequent works extending to real-world computer interaction scenarios [42, 6]. Recent advances include enhanced GUI understanding through visual encoding architectures [17, 5], reinforcement learning frameworks introduced to web/mobile operations [31, 4], agentic-frameworks [49, 16, 2, 44, 26] and joint visual-language modeling [41, 43, 32]. However, current methods face precision limitations on grounding due to 8 (a) Instruction: Click on the quantity of product in 12/12/14 16:35. (b) Instruction: Place the cursor before the capital in the paragraph about novel climaxes. (c) Instruction: Fill up the middle space of the second blank line in the visualized information form. (d) Instruction: Button to rotate the pdf. Figure 7: Qualitative comparison showing JEDIs enhanced fine-grained operation and GUI understanding compared to Qwen2.5-VL-7B-Instruct across four cases. The green square represents the click position of JEDI, while the red square indicates the click position of Qwen. homogeneous synthetic training data [10, 16, 44, 36], which overlook the systematic support for fine-grained component operations (e.g., slider adjustments, nested menu selections), finally limits the upper policy execution as well as further learning. Furthermore, the sources of data which could be beneficial for enhancing GUI interaction abilities are underexplored. GUI Grounding GUI grounding remains core challenge for digital agents executing actions in real world environment. Recent approaches have shifted from relying on textual information such as HTML/accessibility information to pure visual solutions [9, 49, 15, 27, 24, 46]. However, both existing training data and evaluation paradigms suffer from oversimplificationwhether through screenshot-text pairings or manual annotationsfailing to capture the complexity of natural language instructions and action execution, particularly in tasks requiring understanding of expressed intent rather than simple referencing, screen-level comprehension (such as identifying active windows), and fine-grained operations (like sliders and drag-and-drop), thus hindering meaningful assessment and advancement in these critical areas. We point out the problems by proposed benchmark and bridging these gaps through multiple aspects of synthetic data. The comparison with previous work is shown in Table 7."
        },
        {
            "title": "6 Conclusion",
            "content": "We highlight overlooked GUI grounding challenges such as fine-grained manipulation and layout understanding, introducing OSWORLD-G with 564 annotated samples for evaluation. We set up multiple pipelines to construct dataset containing 4 million examples to address these challenges. Our models trained on this dataset achieve competitive results on ScreenSpot-v2, ScreenSpot-Pro, and OSWORLD-G, while also boosting agent performance in OSWorld and WindowsAgentArena. These results demonstrate the effectiveness of addressing previously identified gaps in GUI grounding research."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Binyuan Hui, Weilu Xu, Dunjie Lu, Zhiyong Wu, Weiyun Wang, Eric Xing Wang, Yuhao Yang, Junlei Zhang, Victor Zhong for their helpful feedback on discussion around this work."
        },
        {
            "title": "7 Limitations",
            "content": "In this work, we mainly discuss the data synthesis methods while figuring out the essential factors. Screen capture data can be extracted from internet images and videos by neural networks, which can further expand the dataset. This approach can significantly expand the screenshot metadata, thus enlarging the grounding data. Due to resource restrictions, we leave this for further scaling through industrial efforts. On the other hand, based on our enhanced grounding model, we can construct human-like traversers that interact in the digital world with or without specific purposes, similar to how humans navigate digital environments. This approach can further collect interaction data to improve grounding capabilities and even enhance model knowledge. We also leave these explorations for future work."
        },
        {
            "title": "References",
            "content": "[1] Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164, 2024. [2] Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human, 2024. URL https://arxiv. org/abs/2410.08164. [3] Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, et al. Uibert: Learning generic multimodal representations for ui understanding. arXiv preprint arXiv:2107.13731, 2021. [4] Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:1246112495, 2025. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. [6] Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Fender C. Bucker, Lawrence Jang, and Zack Hui. Windows agent arena: Evaluating multi-modal os agents at scale. ArXiv, abs/2409.08264, 2024. URL https://api.semanticscholar.org/CorpusID: 272600411. [7] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024. [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024. URL https://arxiv.org/abs/2404.16821. [9] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. [10] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents, 2024. URL https://arxiv.org/abs/2401.10935. 11 [11] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pages 845854, 2017. [12] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023. [13] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, et al. Workarena: arXiv preprint How capable are web agents at solving common knowledge work tasks? arXiv:2403.07718, 2024. [14] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:1834318362, 2022. [15] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. [16] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents, 2024. URL https://arxiv.org/abs/2410.05243. [17] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023. [18] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. [19] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. arXiv preprint arXiv:2402.17553, 2024. [20] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. [21] Wei Li, Will Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents. In Neural Information Processing Systems, 2024. URL https://api.semanticscholar.org/CorpusID: 270285816. [22] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile ui action sequences. arXiv preprint arXiv:2005.03776, 2020. [23] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements. arXiv preprint arXiv:2010.04295, 2020. [24] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for generalist gui agent. In NeurIPS 2024 Workshop on Open-World Agents, 2024. [25] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802, 2018. 12 [26] Haowei Liu, Xi Zhang, Haiyang Xu, Yuyang Wanyan, Junyang Wang, Ming Yan, Ji Zhang, Chunfeng Yuan, Changsheng Xu, Weiming Hu, et al. Pc-agent: hierarchical multi-agent collaboration framework for complex task automation on pc. arXiv preprint arXiv:2502.14282, 2025. [27] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203, 2024. [28] Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. V. Jawahar. Docvqa: dataset for vqa on document images. 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 21992208, 2020. URL https://api.semanticscholar.org/CorpusID: 220280200. [29] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. [30] OpenAI. Computer-using agent: Introducing universal interface for ai to interact with the digital world. 2025. URL https://openai.com/index/computer-using-agent. [31] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. [32] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. Ui-tars: Pioneering automated gui interaction with native agents, 2025. URL https://arxiv.org/abs/2501.12326. [33] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: large-scale dataset for android device control. arXiv preprint arXiv:2307.10088, 2023. [34] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 31353144. PMLR, 2017. [35] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1074010749, 2020. [36] Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024. [37] SuperAGI. Guide. Hugging Face Datasets, 2023. URL https://huggingface.co/ datasets/SuperAGI/GUIDE. Apache 2.0 License. [38] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: reinforcement learning platform for android. arXiv preprint arXiv:2105.13231, 2021. [39] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution, 2024. URL https://arxiv.org/ abs/2409.12191. [40] Jason Wu, Siyan Wang, Siman Shen, Yi-Hao Peng, Jeffrey Nichols, and Jeffrey Bigham. Webui: dataset for enhancing visual ui understanding with web semantics. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 114, 2023. 13 [41] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. Os-atlas: foundation action model for generalist gui agents. ArXiv, abs/2410.23218, 2024. URL https://api.semanticscholar. org/CorpusID:273696039. [42] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024. URL https: //arxiv.org/abs/2404.07972. [43] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. [44] Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256, 2024. [45] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022. [46] Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, and Xiang Bai. Omniparser v2: Structured-points-of-thought for unified visual text parsing and its generality to multimodal large language models. arXiv preprint arXiv:2502.16161, 2025. [47] Danyang Zhang, Lu Chen, and Kai Yu. Mobile-env: universal platform for training and evaluation of mobile interaction. arXiv preprint arXiv:2305.08144, 2023. [48] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713, 2024. [49] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. [50] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 OSWORLD-G Statistics A.1.1 Data Types We categorize the examples into five categories that requires different grounding capabilities. And the classification can be refer to their corresponding element types in the Table 6. Table 6: Full table of distribution of examples in the OSWORLD-G benchmark categorized by GUI grounding capabilities and their corresponding interface element types."
        },
        {
            "title": "Element Recognition",
            "content": "Icon, Image, Button"
        },
        {
            "title": "Layout Understanding",
            "content": "Fine-grained Manipulation Tab, Banner/Notification, Accordion/Collapsible Panel, Pagination Control, Toolbar, Menu Bar, Dropdown Menu, List, Grid, Tree View, Dialog/Modal, Panel/- Container, Sidebar, Drawer Slider, Stepper, Divider, Toggle/Switch, Accordion/Collapsible Panel, Checkbox, Radio Button, Color Picker, Date Picker, Table, Text Field/Input Box, Search Bar, Text Filed, Input Box"
        },
        {
            "title": "Refusal",
            "content": "# of Examples 268 337 252 54 A.1.2 Comparison with Previous Work We show the comparison between OSWORLD-G and previous work in Table 7."
        },
        {
            "title": "Benchmarks",
            "content": "Table 7: Comparison between OSWORLD-G and previous benchmarks. Platforms # of Annotated UI-Types # of Examples Instruction Annotation Fine-grained Actions Refusal Cases ScreenSpot-v2 Mobile, Desktop, Web ScreenSpot Pro Desktop OmniAct Desktop, Web OSWORLD-G Desktop 1272 1581 9802 564 2 (Icon, Text) 2 (Icon, Text) 3 (Icon, Text, Color) 32 Visual Functional A.1.3 Data Examples We show examples of text matching type and element recognition type in Figure 2 (layout understanding), 3 (fine-grained manipulation), 8 (text matching, element recognition and refusal instruction). Layout Understanding Layout understanding tasks require models to comprehend the hierarchical structure of interface elements. In the example shown in Figure 2, closing the top notification bar requires recognizing that such bars typically appear at the top region of the editing area in Libreoffice Calc. Fine-grained Manipulation Fine-grained manipulation tasks demand high-precision actions within small or tightly packed screen regions. In the example in Figure 3, selecting the position between the word \"person\" and the number \"1\" requires the model to operate at character-level granularity. Text Matching Text matching tasks involve grounding actions based on explicit textual cues in the instruction. As shown in Figure 8a, choosing \"As Attachment\" requires the model to locate and match this phrase within the screenshot. Element Recognition Element recognition tasks require identifying visual patterns such as icons or images. In the example in Figure 8b, clicking on the ellipse icon involves recognizing the ellipse shape visually within the interface. 15 Refusal Instruction Refusal instruction tasks assess whether the model can recognize when an action is infeasible. In the example in Figure 8c, the instruction refers to \"Cindy Williams,\" who is not visible on the screen. Therefore, clicking on her email address is not possible, and the model is expected to refrain from taking action. (a) Text matching (b) Element recognition (c) Refusal instruction Figure 8: Examples in OSWORLD-G that require text matching and element recognition abilities. A."
        },
        {
            "title": "JEDI Statistics",
            "content": "A.2.1 Overview Table 8 provides an overview of the statistics for JEDI. To enhance the quality of our dataset, we made several improvements upon AGUVIS, where we name it AGUVIS++. First, we manually filter out low-quality annotations and samples unrelated to computer use scenarios. We then augment the dataset by incorporating OS-Atlas data. For synthetic data sources such as SeeClick and OS-Atlas, which inherently contain rendering artifacts and alignment issues, we employ UI-TARS 4 model for quality controlcomparing predicted outputs against ground truth values to ensure deviations remained within acceptable thresholds. In-house data is annotated by human workers. We ask them to use computers while recording timestamps of their actions and capturing screenshots from their screens as observations. These are later used with models like GPT to construct input instructions. Table 8: Data statistics of our dataset. The # Line indicates the compression of multiple queryanswer pairs to improve training efficiency."
        },
        {
            "title": "Data Source",
            "content": "# Image # Line # Turn"
        },
        {
            "title": "Sampling",
            "content": "JEDI Icon Captioning Icon Grounding Component Manipulation (Rule-based) Component Manipulation (Generated) Layout Captioning (App) Layout Grounding (App) Layout Captioning (OS) Layout Grounding (OS) JEDI Refusal Refusal Data (Various Sources) AGUVIS++ [43] SeeClick [9] WebUI [40] GUIEnv [7] GUIAct (web single) [7] Widget Captioning [23] RicoSCA [22] UI RefExp [3] RICO Icon [11] OmniACT [19] 267,102 202,399 29,303 60,085 5,117 5,117 2,901 2,901 403,584 202,419 40,653 529,749 17,721 25,133 14,351 26,190 251,837 202,419 40,653 1,192,687 366,774 916,539 258,334 774,"
        },
        {
            "title": "All\nAll\nAll\nAll\nAll\nAll\nAll\nAll",
            "content": "165,235 2,666,124 2,666,124 Random:5% 66,426 57,389 70,394 17,545 14,409 18,146 4,646 16,133 6,720 69,634 57,389 327,972 17,572 101,426 173,212 15,624 16,133 6,720 525,442 143,187 327,972 17,572 101,426 173,212 15,624 32,091 6,"
        },
        {
            "title": "Continued on next page",
            "content": "4https://huggingface.co/ByteDance-Seed/UI-TARS-72B-DPO 16 Table 8 continued from previous page Data Source DocVQA Grounding [28, 43] MM-Mind2Web [12] GUIAct (web multi) [7] AitZ [48] AndroidControl [21] Guide [37] OS-Atlas [41] # Image 9,756 7,351 13,262 12,002 54,678 12,422 303,472 # Line 34,060 7,351 65,740 11,914 54,678 12,422 303, # Turn 34,060 7,351 65,740 11,914 54,678 12,422 303,"
        },
        {
            "title": "Sampling\nAll\nAll\nAll\nAll\nAll\nAll\nAll",
            "content": "In-house Data Additional In-house Annotated and Augmented Data 1,392,009 1,392,016 1,486,"
        },
        {
            "title": "All",
            "content": "17 A.2."
        },
        {
            "title": "Icon Statistics",
            "content": "The Source Statistics of icon data in JEDI are detailed in Table 9. Table 9: Data sources for icon collection in the JEDI dataset. Due to the diverse and scattered nature of these sources, they are presented here collectively rather than being broken down in the overall dataset overview. Data Ubuntu 2204 Snap Store Windows XP Windows Vista Windows 7 Windows 10 Windows"
        },
        {
            "title": "Crawl",
            "content": "macOS Ventura macOS Sonoma macOS Sequoia macOS icon Collection Apple App store iOS App store Calculator"
        },
        {
            "title": "Crawl\nCrawl\nReverse engineering",
            "content": "Link https://github.com/ubuntu/yaru/tree/ master/icons https://snapcraft.io/store - - - - https://learn.microsoft.com/ en-us/windows/apps/design/style/ segoe-ui-symbol-font - https://github.com/microsoft/ fluentui-system-icons/tree/main/ assets https://apps.microsoft.com/apps?hl= en-gb&gl=US - - - https://macosicons.com/ https://github.com/microsoft/ calculator/tree/main/src/Calculator/ Assets https://github.com/audacity/ audacity/tree/master/au3/libraries/ lib-theme-resources https://fonts.google.com/icons https://github.com/microsoft/ vscode-icons https://github.com/LibreOffice/core/ tree/master/icon-themes https://github.com/primer/octicons/ tree/main/icons https://github.com/GNOME/gimp https://github.com/videolan/vlc - 18 A.2.3 Component Statistics The following Table 10 provides detailed list of the component libraries we use, along with the contribution of each component to the JEDI dataset. Table 10: Statistics of Material UI Components Component Type material (Total) no-ssr box textarea-autosize click-away-listener links floating-action-button bottom-navigation popper modal speed-dial accordion rating use-media-query dividers skeleton alert typography button-group radio-buttons steppers container badges cards progress icons image-list popover toggle-button checkboxes buttons selects backdrop menus transitions masonry text-fields portal dialogs breadcrumbs switches stack paper tooltips timeline chips transfer-list tabs snackbars app-bar table lists drawers grid-legacy pagination slider Conversations 385,493 321 560 445 764 886 689 6,709 3,258 1,699 3,974 1,840 9,409 7,285 2,318 2,103 7,290 511 2,474 3,020 5,252 625 2,991 3,881 4,448 4,663 2,389 658 7,919 8,447 4,545 5,122 214 15,498 1,794 7,932 3,964 134 9,478 3,693 7,050 2,371 5,993 5,648 7,893 13,440 2,100 52,425 6,891 17,474 11,536 17,377 15,942 5,979 12,497 27,843 19 Images 31,309 24 47 37 45 35 51 535 169 71 630 82 869 113 83 85 1,378 38 102 115 869 37 108 160 231 173 96 41 1,183 1,148 206 194 16 1,839 92 106 285 26 1,445 110 1,050 82 97 266 219 1,951 295 2,917 1,477 2,096 839 2,094 1,846 149 197 2,"
        },
        {
            "title": "Component Type\nautocomplete\navatars\ngrid",
            "content": "mantine (Total)"
        },
        {
            "title": "InputValidation\nDndTable\nButtonProgress\nActionToggle\nHeaderMenu\nAutocompleteLoading\nAuthenticationImage\nNavbarMinimalColored\nPasswordStrength\nHeaderTabs\nNavbarLinksGroup\nArticleCard\nHeroBullets\nInputWithButton\nFeaturesGrid\nCardsCarousel\nUsersRolesTable\nContainedInputs\nFeaturesImages\nNavbarMinimal\nHeroImageBackground\nTableSelection\nCardGradient\nHeroContentLeft\nButtonCopy\nFeaturesCards\nTableReviews\nUserCardImage\nStatsGrid\nNavbarSearch\nArticlesCardsGrid\nProgressCard\nNotFoundImage\nProgressCardColored\nUserInfoAction\nImageCheckboxes\nStatsCard\nImageActionBanner\nHeaderSearch\nCustomSwitch\nFaqSimple\nHeaderSimple\nForgotPasswordInput\nDndList\nArticleCardFooter\nCarouselCard\nCommentSimple\nStatsGroup\nStatsControls\nDoubleHeader\nTableOfContentsFloating\nFaqWithImage\nCardWithStats\nEmailBanner\nLeadGrid\nSubgrid\nSliderIcon\nUserButton",
            "content": "Table 10 continued from previous page Conversations 10,356 5,634 7,842 27,814 577 118 45 852 62 17 34 97 98 75 5,322 185 111 73 121 79 94 89 140 64 102 245 718 95 52 128 140 202 214 141 144 60 22 1,852 138 263 111 60 81 32 97 63 37 167 118 94 107 78 124 70 74 71 250 146 145 73 132 72 20 Images 322 154 174 762 14 6 6 17 3 3 3 4 4 3 56 7 4 4 4 4 4 6 5 3 7 6 15 6 5 4 3 7 7 5 6 4 3 28 8 9 5 4 4 3 4 4 4 6 4 4 5 3 5 5 4 4 8 6 7 4"
        },
        {
            "title": "NavbarSegmented\nNavbarSimple\nNothingFoundBackground\nFeaturesTitle\nHeroImageRight\nUsersStack\nFooterLinks\nNotFoundTitle\nContactUs\nButtonMenu\nGradientSegmentedControl\nArticleCardVertical\nNavbarSimpleColored\nCurrencyInput\nSliderLabel\nArticleCardImage\nFeaturesAsymmetrical\nFooterSocial\nHeaderMegaMenu\nStatsRingCard\nTableSort\nAuthenticationTitle\nTableScrollArea\nCommentHtml\nAuthenticationForm\nGetInTouch\nHeroTitle\nDropzoneButton\nServerOverload\nSliderMarks\nGetInTouchSimple\nSliderWhite\nStatsRing\nStatsSegments\nHeroText\nFloatingLabelInput\nCookiesBanner\nTaskCard\nForgotPassword\nInputTooltip\nTableOfContents\nCheckboxCard\nServerError\nFaqWithBg\nSplitButton\nLanguagePicker\nBadgeCard\nSwitchesCard\nFeaturesCard\nImageCard\nDoubleNavbar\nFaqWithHeader\nUserMenu\nUserInfoIcons\nNavbarNested\nSliderInput\nStatsGridIcons\nFooterSimple\nUsersTable\nSocialButtons\nSliderHover\nFooterCentered",
            "content": "Table 10 continued from previous page Conversations 54 103 116 181 132 240 208 19 398 261 102 99 99 43 196 48 76 157 91 74 108 77 92 147 195 305 57 24 124 32 109 64 138 149 117 19 48 1,383 49 40 119 15 35 74 76 100 38 1,339 91 115 110 151 136 103 167 113 186 137 2,927 325 84 58 21 Images 4 4 11 5 5 4 5 3 12 17 5 7 4 5 3 4 3 8 4 5 4 3 3 7 15 8 3 4 8 4 4 4 4 5 8 4 4 19 3 4 4 4 5 4 3 5 3 16 4 8 4 8 4 4 7 4 5 4 23 10"
        },
        {
            "title": "Component Type\nDndListHandle\nActionsGrid\nGridAsymmetrical",
            "content": "ant-design (Total) switch watermark skeleton divider tooltip rate auto-complete tour checkbox splitter time-picker collapse qr-code menu segmented flex notification alert list button timeline carousel modal drawer steps affix card progress mentions typography tree-select descriptions message transfer popover empty badge radio spin float-button image cascader popconfirm calendar form config-provider app statistic back-top breadcrumb input-number space avatar icon slider tabs upload anchor Table 10 continued from previous page Conversations 76 122 172 473,723 1,484 1,849 1,913 2,038 3,525 5,492 4,359 2,174 6,531 4,878 6,642 5,191 2,711 6,182 6,088 3,386 6,090 6,124 7,470 8,060 5,727 1,997 9,508 7,818 9,873 1,105 7,331 9,485 3,829 3,405 5,515 6,732 3,626 7,308 3,009 1,105 8,339 6,888 1,825 3,457 3,765 6,992 2,156 10,389 10,818 3,167 663 2,345 454 3,964 5,420 6,426 5,120 2,898 9,588 36,465 3,270 4,524 22 Images 2 4 2 16,837 94 83 99 98 194 135 203 114 255 254 276 225 160 215 238 113 208 199 190 329 163 120 379 275 338 66 345 274 190 203 248 236 156 193 163 89 292 239 126 215 217 372 153 141 679 143 35 96 29 130 286 240 150 135 231 609"
        },
        {
            "title": "Component Type",
            "content": "tag tree input select color-picker pagination layout dropdown grid date-picker table result chakra (Total) mark loader bleed aspect center skeleton fieldset locale list theme separator editable code float visually box segmented spinner simple link for hover blockquote flex alert accordion steps timeline stat switch radiomark text highlight drawer menu tooltip toggle collapsible button container checkmark badge close show field card empty textarea action Table 10 continued from previous page Conversations 5,118 16,354 7,012 15,055 6,342 9,673 6,604 9,035 13,957 20,445 44,891 744 330,074 288 488 420 1,045 964 1,097 834 424 1,758 787 1,751 1,684 1,630 1,669 1,725 2,337 2,405 1,921 2,256 1,910 1,057 1,231 1,770 1,653 2,702 3,855 3,763 2,784 3,082 3,285 680 1,250 2,075 13,309 5,395 11,407 846 325 5,358 672 624 2,590 446 636 1,850 3,155 987 2,550 673 Images 214 433 478 550 410 221 188 301 241 739 844 42 11,784 24 35 35 74 60 80 45 26 64 41 83 111 72 83 50 118 110 110 58 92 33 63 109 74 143 160 168 85 123 173 28 69 97 272 193 448 56 24 252 27 29 91 34 47 124 108 70"
        },
        {
            "title": "Component Type",
            "content": "image password toaster rating pin qr status group popover file prose tabs native em kbd portal dialog select tag clipboard grid table heading presence stack breadcrumb radio progress format pagination icon checkbox input avatar wrap number slider color data Table 10 continued from previous page Conversations 1,173 2,104 9,295 9,917 2,384 2,394 1,879 1,453 5,154 1,644 2,275 23,088 1,429 215 8,119 218 5,663 6,850 4,416 1,444 1,784 10,317 1,719 1,450 2,259 3,537 13,935 11,892 2,572 16,881 3,051 13,930 3,568 11,467 3,186 3,941 14,841 10,339 888 Images 83 68 231 195 147 141 60 66 200 168 90 397 67 23 197 26 220 232 150 93 45 210 86 77 86 116 496 359 134 254 139 464 224 403 56 206 302 560 29 A.2.4 Layout Statistics We collected layout data from two primary sources: the UI design community Figma and through rollouts across operating systems. Statistics can be found in the following Tables 11 12. The number of elements shown in the following table is not the exact number in the final dataset. These elements are filtered in later processing stage. Table 11: Statistics of OS Layout Data"
        },
        {
            "title": "Rollout Environments",
            "content": "OSWorld (Ubuntu) WindowsAgentArena (Windows)"
        },
        {
            "title": "Elements",
            "content": "2000 903 2903 183889 74445 258334 Table 12: Statistics of Layout Data Collected from Figma Commnuity Design Templates"
        },
        {
            "title": "Design Templates",
            "content": "[Freebie]-Home-Rent-App-UI-Design-(Community)"
        },
        {
            "title": "Images Elements",
            "content": ""
        },
        {
            "title": "Continued on next page",
            "content": ""
        },
        {
            "title": "Images Elements",
            "content": "Table 12 countined from previous page Github, &-more- &-moreBasecamp, Snapchat-&-more- (Variants)-macOS-Big-Sur-UI-Kit-for-Figma-(Community) 10-Real-Chat/Messaging-PagesFacebook, Reddit, (Community) 10-Real-Dashboard-PagesAirBnB, (Community) 10-Real-HomepagesAirBnb, Github, and-more-(Community) 10-Real-Notification-PagesAirBnB, Dropbox, Notion, (Community) 10-Real-Pricing-PagesBasecamp, Dribble, &-more-(Community) 10-Real-Search-Results-PagesGithub, Loom, Notion-&-more-(Community) 10-Real-Sign-Up-PagesCalendly, Dribbble, &-more-(Community) 10-Real-User-Settings-PagesCalendly, Github, Behance, &-more- (Community) 11-Real-Sign-In-PagesAirBnB, Calendly, &-more-(Community) 20-Modals, Popups, Alerts-(Community) AWS-Admin-Redesign-by-FluentUI-(Community) AWS-Amplify-UI-Kit-(Community) AWS-Platform-(Community) Ai-Design-Templates-(Community) AirbnbHome, Search, and-Listing-Pages-(Community) Airbnb-UI-Kit-(Community) Amazon-UI-Design-(Community) Android-UI-Kit-(Community) App-Clips-(Community) App-Store-TemplateSee-how-your-App-looks-like-in-App-Store- (Community) Apple Design Resources - macOS (Community) Apple-Mail-(Community) Apple-Mail-Design-(Community)-(Community) Apple-Maps-iOS-(Community) Apple-Messages-Templates-(Community) Apple-Pay-(Community) Apple-TV+-UI-Kit-(Community) Apple-Website-UI-2023-(apple.com)-(Community) Apple-Widgets-UI-Kit-(Community) Apple-and-Google-Play-store-UI-(Community) Apple-iCloud-Login-(Community) Apps-Paywalls-and-Subscription-Screens-(Community) Assets-Kit-UI-Mobile, Tablet-&-Desktop-(Community) Audiobooks-by-Booksbury-(Community) Betting-Mobile-app-(Community) Binance-Market-Trade-Dashboard-UI-Design-(Community) Booking.com-Mobile-App-RedesignUX/UI-Case-Study-(Community) Budddy-Chatbot-Freebie-(Community) CAPTCHA-UI-Kit-(Community) CAR-RENTAL-WEBSITE-(RESOONSIVE-DESIGN)-(Community) Calendar-Interactive-UI-Kit-(Community) Call-Center-Desktop-App-(Community) Car-Rent-Website-DesignPickolab-Studio-(Community) Car-Rental-Mobile-App-(Community) Casino-Web-Site-(Community) Chat-for-desktop/mobile--Free-to-use-(Community) ChatGPT-UI-Kit, AI-Chat-(Community) Cinema-4D-GUI-Redesign-(Community) CliconeCommerce-Marketplace-Website-Figma-Template-(Community) Club-Website-Design--WEB-UI-(Community) Code-block, Syntax-highlighting Coding-WebsiteUI-Kit-(Community) Coinbase-CloneWebsite-Prices-Page-(Community) Components-libraryLight-&-Dark-mode-(Community) Concept- -Mailbox-Design-(Community) 10 10 10 10 10 10 11 11 13 12 13 21 28 5 10 5 10 18 28 4 29 1 3 7 8 18 16 7 78 6 2 5 45 3 6 1 4 8 16 3 6 4 10 3 12 3 2 2 48 4 2 11 1 13 1 685 1269 2214 2162 1103 3554 1714 570 1006 557 135 1456 713 690 1071 905 49 3522 2364 77 1164 283 72 351 567 442 1290 2618 264 130 2 48 986 76 162 584 85 241 124 60 578 80 1232 83 3831 169 182 250 7178 66 20 416 448"
        },
        {
            "title": "Continued on next page",
            "content": ""
        },
        {
            "title": "Images Elements",
            "content": "Table 12 countined from previous page Coursera-UI-KIT-(Community) Crypto-App-Ui-Kit-(Community) Customer-onboarding-designs-&-componentsby-Bento-(Community) Dark-UI-Elements, Dropdowns-&-Calendar-(Community) DashboardOnline-Learning-Profile-(Community) Dashboard-UI-KitDashboard, Free-Admin-Dashboard-(Community) Data-table-design-components.-Free-UI-Kit-(Community) Dating-Mobile-App-(Community) Delivery-App-Ui-Kit-(Community) Desktop-Messaging-App-Concept-(Community) DeuploadDecentralized-Cloud-Storage-Landing-Pages-(Community) Discord-(Community) Discord-Redesign-(Community) Discord-UI-Mockup-(Community) Disney+-App-Redesign-(Community) DocketHub-(Community) Doordash-FREE-UI-KitBy-Marvilo-(Community) Dota-2-UI-Redesign-(Community) Duolingo-Pages-Collection-by-DesignDrops.io-(Community) Duolingo-WorkflowsOnboarding, Learning-a-language, Upgrading, &- Cancelling-(Community) E-StoreMobile/web-(Community) E-TutorLearning-Management-System-(Community) E-commerce-UIFigma-Ecommerce-UI-Kit-(Demo-Version)-(Community) E-commerce-Website-Template-(Freebie)-(Community) Ebay-New-Design-Concept-(Community) Ecommerce-Website-Design-(Community) Element-UI-Kit-2.15.7-(Community) EliteFood-Restaurant-&-Coffee-Free-Figma-Template-(Community) Email-Message-Modal-(Community) Embed-Media-Components-(Community) EonifyMobile-App-Authentication-Page-(Community) FREE-Gmail-Mockup-2024-template!-(Community) FREEBIES-Landingpage-LaslesVPN-(Community) Facebook-Page-Mockup-(2022)-(Community) Facebook-ReDesign-2023-(Community) Fantastical-Calendar-(Community) FigmaSharp-Toolkit: macOS-Big-Sur-2.0.0-(Community) Finance-Market-Trading-Terminal-(Community) Fitness-App-UI-Kit-for-Gym-Workout-App-Fitness-Tracker-Mobile-AppGym-Fitness-Mobile-App-UI-Kit-(Community) FiverrUI-RedesignedFreelance-Marketplace-Website-Design- (Community) Flight-Booking-App-UI-Kits-(Community) Food-Catering-Service-App-With-Landing-PageFigma-Freebies-- Doradesign-(Community) Food-Delivery-Website-&-App-Design-UI-Kit-(Community) Food-delivery-app-Ui-kit-(Community) FoodWagon-Food-Delivery-Landing-Template-by-ThemeWagon- (Community) Forms/Desktop-&-Mobile-(Community) Forum-Concept-for-Alem.school-(Community) Free-Fitness-App-Ui-Kit-(Community) Free-Instagram-UI-Mockups-2023-(Community) Free-Modal-Upload-Files-Kit-for-Web-and-MobileInclude-4-modes- (Community) Free-Trading-UI-Kit-(Community) Free-YouTube-Shorts-Mockups-(Community) Free-YouTube-Video-Player-Mockups-(Community) FreebiesApps-Tracking-Truck-Cargo-Courier-Delivery-(Community) FreebiesScooter-Renting-App-(Community) 0 61 10 4 3 6 13 42 54 1 45 2 16 11 2 10 5 12 13 145 15 69 160 9 1 1 42 16 2 6 7 4 1 1 7 1 5 14 87 3 20 17 18 1 12 6 48 12 27 43 18 6 2 4 0 2430 1315 254 201 2224 31059 722 2393 26 2188 116 3236 672 6 1121 569 2287 714 4974 1502 8475 6522 858 34 71 2078 769 165 38 91 98 28 53 449 655 573 766 1723 249 696 37 117 304 341 403 841 364 1150 725 634 506"
        },
        {
            "title": "Continued on next page",
            "content": ""
        },
        {
            "title": "Images Elements",
            "content": "Table 12 countined from previous page Full-Apple-Music-Classical-App-(Community) Full-E-Commerce-Website-UI-UX-Design-(Community) GitHub-UI-(Community) Github-UIFree-UI-Kit-(Recreated)-(Community) Gmail-UI-Mobile-Design-Template-2024!-(Community) Gmail-UI-Part-1: Inbox-(Community) Gmail-UI-Part-2: Reading-&-Composing-Emails-(Community) Google-Anlytics-Dashobard-(Community) Google-CalendarWeb-version-revamp-(Community) Google-Chrome-Browser-UI-Kit-2025-(Community) Google-Chrome-UI-Kit-2022-(Community) Google-Drive-Reverse-Engineer-(Community) Google-GeminiBuilt-with-Material-3-Design-Kit-(Community) Google-MapsBus-ticket-booking-(Community) Google-Maps-Parking-Prototype-Testing-(Community) Google-Meet-UI-(Community) Google-Scholar-re-designed-(Community) Google-Search-Result-Page-(SERP)-(Community) Google-SheetTemplate-(Unofficial)-(Community) Google-Sign-in-GISGoogle-Identity-Services-(Community) Google-Translate-Redesign-(Community) Google-Weather-App-Redesign-(Community) Google-search-(Community) Health-Fitness-Workout-App-(FREEBIEPrototype)-(Community) HealthRise-Health-Tech-Dashboard-(Community) Hero-GiveawayRedesigns-(Community) Hotel-booking-website-UI-(Community) HoteliqBooking-Hotel-App-Design-(Community) IKEA-/-eCommerce-Concept-Design-(Community) IMDb-Redesign-(Community) InTouchMessaging-App-UI-Kit-(Community) Instagram-UI-Screens-(Community) IntelliJ-Platform-UI-Kit-(Community) Invoice/Payment-ComponentsDipa-Inhouse-(Community) Job-Finder-App-UI-Kit-(Community) Job-Finder-Ui-App-Kit-(Community) JobpilotJob-Portal-Figma-UI-Template-(Community) LOGIFYWEB-LOGIN-UI-KIT-(Community) Leetcode-Homepage-(Community) Lenskart-RedesignedHiFi-Wireframes-(Community) LinkedIn-Business-Page-Mockup-(2024)-(Community) LinkedIn-Redesign-UI-Kit-(Community) Linkedin-Page-Mockup-(2022)-(Community) Linkedin-UI-Screens-(Community) Liquipedia-Web-Redesign-(Community) Live-Score-UI-KIT-(FREEBIES)-(Community) Login-&-Register-Web-UI-Kit-(Freebie)-(Community) Loom-UIFree-UI-Kit-(Recreated)-(Community) MEDDICALHospital-website-template-(Community) MacOS-file-upload-&-download-(Community) Map-Navigation-Mobile-App-UI-Kit-Template-(Community) Market-Stock-Exchange-(Community) Medical-Clinic-Booking-(Doctor-Appointment)-App-UI-Concept- (Community) Mercedes-Benz-App-(Community) Messager-Dashboard-design.-(Community) MetrowayTrain-Ticket-booking-website-(Community) Microsoft-365-UI-Kit-(Community) Microsoft-Excel-+-Word-2024-(Community) Mobile-Chat-Figma-UI-Kits--BRIX-Templates-(Community) Mobile-eCommerce-Clothing-Store-App-Design-(Community) 160 15 2 18 4 4 9 1 4 14 1 14 4 19 5 1 3 2 23 9 9 3 8 8 4 7 1 3 4 15 4 36 48 17 1 83 3 40 1 5 1 8 1 28 4 12 5 28 10 3 2 6 3 8 9 5 358 4 70 6 18706 2318 1101 6568 38 1940 2505 28 392 695 39 1620 646 34 68 2 33 167 3152 136 140 47 313 332 4513 1391 5 222 26 2293 87 441 4598 828 43 85 913 273 1 85 111 223 30 948 1606 349 170 5405 189 363 15 601 20 9 582 301 95741 1733"
        },
        {
            "title": "Continued on next page",
            "content": ""
        },
        {
            "title": "Images Elements",
            "content": "Table 12 countined from previous page Modern-Profile-UI-KitFreebies-UI-(Community) Money-transfer-Ui-App-Kit-(Community) Movie-App-Redesigned-HULU-(Community) Movie-Ticket-Booking-ApplicationCoursera-UX-Specialization- (Community) Movie-Ticket-Booking-Apps-(Community) MyCourses.ioCourse-Website--Course-Online--Course-details--Courselanding-page--Untitled-UI-(Community) Native-Web-ComponentsBrowser-Defaults-UI-Kit-(Community) Navigation-App-Design-(Waze-App-Redesign)-(Community) Neomorphism-music-player-for-desktop-(Community) Netflix-Home-Page-desktop-&-TV-(Community) Netflix-home-pageMobile-&-TV/Desktop-(Community) News-&-Blog-App-UI-Kit-By-Al-Ferdous-(Community) News-Website-UI-and-Presentation-for-Opportunists-(Community) Nike-UIFree-UI-Kit-(Recreated)-(Community) Nowted-A-Note-taking-App-(Community) Officevibe-UIFree-UI-Kit-(Recreated)-(Community) On-Demand-Medicine-Delivery-App-(My-Orders-Flow)-(Community) Onboarding-Appointment-booking-(Community) OnestClassified-Ads-Listing-Figma-Template-(Commnity) PDF-Viewer-(Community) PaperyNews-Magazine-Mobile-App-(Community) Parking-App-Design-UI--Figma-(Community) Patterns & Layouts UI Kit (Community) Payment-Page-(Desktop)-(Community) PictoPersonal-Portfolio-Free-Template-(Community) Pinterest-Redesign-(Community) Pinterest-UIFree-UI-Kit-(Recreated)-(Community) Plant-App-Freebies-(Community) Print-dialogFirefox-macOS-(Community) Quiz-Game-(Community) QuizGrad-webapp-(Community) Quora-Redesign-(Community) REISReal-State-Listing-Figma-Template-(Community) Real-Estate-App-UI-Kit-(Community) Recreating-Google-Drive-Using-Lexicon-(Community) Reddit-Design-System-(Community) Reddit-Material-Design-Redesign-(Community) RedesignChatGPT-(Community) Registration-Form-for-a-Medical-Laboratory--Medical-Analyzes- (Community) Restaurant-Booking-Uikit-(Community) Roommates-Apartments-Booking-(Community) Sass Plat form Layouts - Wireframe Kit (Community) ScheddoBookings-&-Reservations-UI/UXFreebie-(Community) Shell-TemplateWindows-11-(Community) ShopcartOnline-Ecommerce-website-(Community) ShoperyOrganic-eCommerce-Shop-Website-Figma-Template- (Community) Simple-Chat-Widget-for-Desktop-(Community) Siri-&-App-Shortcuts-(Community) Slack-Desktop-App-Clone-(Community) Slack-UIDesktop-(Community) Snow-Dashboard-UI-Kit-(Community) Soccer-Score-App-(Community) Social-Login-Auth-Modals-(Community) SportifySports-streaming-app-(Community) SpotifyMobile-UI-Kit-(Community) Spotify-Redesign-(Community) Spotify-UIFree-UI-Kit-(Recreated)-(Community) 4 55 23 22 5 50 12 2 6 1 2 6 2 18 6 18 9 1 44 1 21 30 108 2 1 5 4 13 2 12 6 2 3 79 2 16 14 1 5 20 2 11 7 87 1 4 57 5 1 6 5 5 34 22 32 10 6 236 400 40 49 3731 214 17 19 8 24 46 2 2769 362 3577 379 183 6139 18 1145 270 12275 184 202 363 716 69 72 206 132 10 572 3255 1002 762 142 53 502 122 54 1073 255 6881 67 6916 80 5077 238 118 2608 328 49 1832 468"
        },
        {
            "title": "Continued on next page",
            "content": ""
        },
        {
            "title": "Images Elements",
            "content": "Table 12 countined from previous page Spotify-UI-Design-(Search/Artist-Profile)-(Community) Starbucks-Redesign-Mobil-App-(Community) Steam-Redesign-(Community) Stock-Trading-AppUI-Concept-(Community) Stripe-Apps-UI-toolkit-(Community) Stripe-Connect-Embedded-ComponentsUI-Toolkit-(Community) Subscription-Paywall-Modal-(Community) Table-Booking-Restaurant-Application-(Web-+-Mobile-+-Admin-Panels)- (Community) Table-UI-3.0--Variants-Update-(Community) Tap-to-Pay-on-iPhone-(Community) TaskyTask-and-Time-Management-Dashboard-(Community) Taxi-Booking-App-(Community) Technical-Support-Applications-Page-(Community) Telegram-Design-System-(Community) Terminal-app-UI-(Community) Tesla-Mobile-App-Redesign-(Community) The-Unofficial-Spotify-Design-System-(Community) Ticketing-App-Freebies-(Community) TikTok-UI-Screens-(Community) Tinder-Mobile-App-(Community) TipKit-(Community) To-do-list-dashboard-(Freebie)-(Community) ToDoHQActivity-management-website-design-(Community) Todoist-Free-UI-KitBy-Marvilo-(Community) Todoist-for-macOS-app-concept-(Community) Tour-Guidetravel-agency/travel-booking-website-(Community) Travel-&-Hotel-Booking-Light-Mobile-App-(Community) Trello-Concept-(Community) Twitch-UIAutolayout-Interface-(Community) Twitch-UIFree-UI-Kit-(Recreated)-(Community) Twitter-UI-Clone-Design-(Community) Twitter-UI-Screens-(Community) Twitter-desktop-pages-(feed, sigup, login, profile)-(Community) UF-File-Manager-(Community) UI-DESIGN-FOR-MOCK-INTERVIEW-PLATFORM-(Company-side)- (Community) Uber-App-UIFree-UI-Kit-(Recreated)-(Community) Uber-Redesign-(Community) Ubuntu-Shiro-(Community) VPN-AppUI-Kit-(Community) Video-Player-For-Web-&-Mobile-(Community) Video-Streaming-WebsiteResponsive-web-app-prototype-(Community) Visual-Studio-Code-Toolkit-(Community) Wallet-(Community) WeChat-(Community) WeUI-kit(Wechat)-(Community) Web-Browser-Mockups-(Community) Web-Dashboard-UITask-&-Project-Management-(Community) Website-FAQ-Accordions-Figma-Template--BRIX-Templates-(Community) Website-Wireframes-UI-Kit--BRIX-Templates-(Community) WhatsApp-Pay-&-Split-(Community) Wikipedia-(Community) Windows-11-Chat-UI-Kit-(Community) Windows-File-Explorer--Egos-Take-(Community) Windows-Install-Redesigned-(Concept)-(Community) Windows-Outlook-Template-(Community) WordPress-Design-System-(Community) YouTube-Music-App-Redesign: (Community) YouTube-Redesign-(Community) Elevating-the-Music-Experience2 8 36 6 35 63 1 99 1 2 1 8 4 46 6 3 5 8 14 23 8 2 18 8 3 4 4 2 1 3 9 22 5 15 50 3 3 10 9 9 3 45 12 11 33 4 1 3 108 24 44 15 18 47 17 12 34 580 330 6855 302 2625 6002 56 2524 223 20 5 46 266 8535 26 185 854 549 100 192 67 86 329 868 1196 155 171 2 182 1670 992 315 175 280 528 306 19 606 200 186 50 8887 230 318 391 60 7 3 1800 294 11773 857 3384 22947 2269 5289"
        },
        {
            "title": "Continued on next page",
            "content": ""
        },
        {
            "title": "Images Elements",
            "content": "Table 12 countined from previous page YouTube-UI-Clone-Design-(Community) Zoom-Apps-UI-Overview-(Community) aeroSpeed-Bus-Booking-Application-UI-Kit-[User-+-Driver]-(Community) chat-app-UI-kit-(Community) eDexOnline-Course-E-Learning-Website-(Comunity) iBankBanking-&-E-Money-Management-App--FinPay--Digital-- Finance-Mobile-Banking-App-Ui-Kit-(Community) iMessage-Apps-and-Stickers-(Community) iOS-17-Apple-music-Now-Playing-interface-(Community) iOS-18-and-iPadOS-18-(Community) lark-(Community) macOS-Big-Sur-UI-Kit-(Community) macOS-Browser-UI-Kit-(Big-Sur-Update)-(Community) telegram-app-(Community) uiDesign-System-(Community) 10 19 14 5 2 89 22 4 91 28 20 6 15 17 1801 4507 330 7 447 2027 1761 4 7171 45169 3214 51"
        },
        {
            "title": "Total",
            "content": "5273 563721 30 A.3 JEDI Dataset Construction: Detailed Pipeline for Component A.3.1 Component Collection and Style Augmentation We begin by collecting example components from four mainstream UI libraries hosted on GitHub: Material UI, Ant Design, Mantine UI, and Chakra UI. From each repository, we extract example code snippets(in typescript) that showcase usage of individual components. To diversify these examples, we apply style augmentation using two LLMs: GPT-4o and Claude-3.5-Sonnet. For each original code snippet, we first ask the model to envision unique UI usage scenario. Based on the original code and the imagined context, it then generates stylistically augmented variant code. This process is repeated multiple times per example, each time with different context to promote diversity. Previously generated variants are included in the prompt to prevent redundancy across augmented examples. A.3.2 Rendering and Interaction Preparation Each augmented component is rendered on React application. Components are wrapped in container with randomized position to mitigate positional overfitting. Using Playwright, we programmatically open and interact with the rendered pages. We extract screenshots of the rendered component andelement tree information (positioning, hierarchy, etc.) using Playwrights evaluate method and custom JavaScript. These outputs are used to generate component-grounded actions via two distinct pipelines. Pipeline 1: Component-level Action Generation Step 1: Generate Action Intents We prompt GPT-4o with component name, component code and screenshot. GPT-4o returns list of action intents, each representing high-level user interaction. We use few-shot examples to guide this process. Step 2: Generate Action Details For each intent, we generate detailed interaction metadata using component name, component code, screenshot, element position tree and action intent. Each action detail includes: 1. Thought Process: The thinking process of generating an action detail 2. Action Space Type: None: No action space exists, Unique: Only one possible action exists (e.g., clicking button), Discrete: Limited/unlimited set of distinct possible actions (e.g., selecting from list of options), Continuous: Infinite possible actions within range (e.g., dragging slider to any position) 3. Action Description: Describe what the action does, which serves as the instantiation/implementation of the action intent. 4. Action Parameters: List of all parameter names for the action function(in action code) 5. Discrete Values: List of all possible parameter values for discrete action spaces (if applicable) 6. Continuous Intervals: List of interval for all possible parameter values for continuous action spaces (if applicable) 7. Action Code: function using PyAutoGUI to represent one action or kind of actions"
        },
        {
            "title": "Example",
            "content": "1 { 2 \" thought_process \" : \" The target element is slider , which provides continuous range of values from 0 to 100. The action involves setting specific value within this range by determining the corresponding position on the slider bar and simulating click at that position . The slider 's endpoints are identified , and linear interpolation will be used to calculate the appropriate position based on the desired value . \" , 31 \" action_s pace_type \" : \" continuous \" , \" action_desc \" : \" Set saturation to < saturation >% \" , \" action_params \" : [ \" saturation \" ] , \" ac io n_ dis re _v lu \" : null , \" i _ t o _ e l \" : { \" saturation \" : [ [ ] ] 0 . 0 , 1 0 0 . } , \" action_code \" : \" def action ( saturation ) : x_0 , y_0 = 600.5 , # Left endpoint 830 endpoint pyautogui . click (x , y_0 ) \" = x_0 + ( x_1 - x_0 ) * ( saturation / 100) x_1 , y_1 = 1064.5 , # Right 3 4 5 6 8 9 10 11 12 14 15 16 17 18 } ..., often involving pyautogui) into one We then convert the action code (e.g., def action(parameter): or more pieces of grounding datasuch as pyautogui.click(x, y)by sampling from the corresponding action space. If the action space is None, no sampling is needed. This conversion is guided using few-shot examples. An example of this process can be seen below."
        },
        {
            "title": "Example",
            "content": "Instruction: Set saturation to <saturation> Action code: def action(saturation): x_0, y_0 = 600.5, 830 x_1, y_1 = 1064.5, 830 = x_0 + (x_1 - x_0) * (saturation / 100) pyautogui.click(x, y_0) # Left endpoint of the saturation slider # Right endpoint of the saturation slider Sampled grounding data: # Set saturation to 24% pyautogui.click(711.86, 830) # Set saturation to 60% pyautogui.click(878.90, 830) ... Pipeline 2: Element-Level Action Generation Step 1: Element Extraction and Filtering We render each augmented component in browser and traverse the DOM tree to collect element nodes. Two filtering rules are applied: Duplicate boxes: Only one node is retained if multiple share the same bounding box. Abnormal sizes: Nodes with very small or very large bounding boxes are discarded. For each valid node, we collect position, text, visibility, interactivity, parent-child relationships, and metadata. Step 2: Multimodal Context Encoding To help GPT-4o understand each element, we provide element box, parent box, cropped screenshot(cropped screenshot with only the element region), context screenshot(cropped screenshot with element region and nearby surroundings, with the element highlighted in red bounding box) and full-page screenshot(full screenshot with the element highlighted in red bounding box) as input. And the model outputs include visual description(a detailed account of the elements appearance), position textual information(spatial relationship relative to the viewport and its parent), element functionality, UI type (e.g., button, slider) and possible actions at element center. To ensure quality, we also include visibility check and atomicity check, to check whether this element is single visible UI unit. 32 Step 3: Action Detail Generation For each possible action, we prompt GPT-4o with the action and relevant element informationincluding visual description, position, text content, functionality, and UI type. The model is asked to generate detailed action information, including the thought process, action description, action parameters, and action code. This is similar to the action detail in Pipeline 1, but limited to the unique action space. Step 4: Continuous Action Detection To identify elements like sliders that support continuous interactions, the model determines whether the element has continuous action space and generates the corresponding thought process, action description, action parameters, value range(action_continuous_interval), and action code. This step parallels the action detail in Pipeline 1, but focuses solely on the continuous action space. Step 5: Grounding Actions We convert each action code into one or more grounding samples, similar to that in Pipeline 1. A.3.3 Comparison of Pipelines Pipeline 1 is simpler. However, it may suffer from inaccurate bounding box targeting, limited action diversity and action vagueness. Pipeline 2 generates data with better localization and diversity. In practice, most of our dataset is generated using Pipeline 2. A.3.4 Post-Processing and Filtering To ensure data quality, we apply multiple filtering stages. 1. Visual Filter (via GPT-4o) Given: Cropped screenshot Marked screenshot (click position highlighted with green dot and circle) Full screenshot (element highlighted) GPT-4o filters out data that: 1. Shows visible errors (e.g., \"Compiled with problems\" or red overlays) 2. Targets an incorrect GUI element 3. Has incorrect click localization (e.g., not centered on button/text) 2. Instruction Filter (LLM-Based) Using GPT-4o-mini, we filter out ambiguous or low-quality instructions from Pipeline 1: 1. Unclear or vague semantics 2. Multiple interactive targets 3. References to non-visual identifiers like \"index 1\" 4. Multi-step or compound interactions 3. Instruction Filter (Rule-Based) We filter instructions with high error likelihood based on pattern rules: 1. Contains explicit coordinates (e.g., (x, y)): Instructions referencing raw screen coordinates are filtered out, as such positional references are not meaningful in vision-only context. 2. Mentions structural terms such as child, parent, path, or container: These terms imply hierarchical relationships derived from accessibility trees, which are not observable in visual input. 3. Mentions card component without spatial qualifiers such as in, within, or at: Such instructions typically refer to an entire composite element (e.g., card) rather than specific atomic component within it, resulting in ambiguous interaction targets. 4. Includes directional terms in combination with screen: Phrases like top-left of the screen are frequently found to be incorrect or misaligned with actual component layouts, likely due to LLM misinterpretation. 33 5. Refers to highlights or visual annotations (e.g., red dot, circle, highlight): These often result from the model misidentifying annotation markers (used to denote interaction points) as intrinsic parts of the interface. 6. Mentions textual UI elements (e.g., text, label, heading) in combination with interaction verbs (e.g., read, hover, click, interact): If the associated bounding box is visually simplebased on low color variance and edge densityit often indicates that the relevant text is located on the periphery of the box, while its center is visually empty, leading to inaccurate click localization. 7. Refers to sliders without specifying interaction values: Instructions such as interact with the slider without numerical targets are prone to ambiguity and do not provide sufficient grounding for generating actionable behavior. A.3.5 Real-world augmentation pipeline Office software, including document editors, presentation tools, and spreadsheets, is integral to daily work for many. Automating workflows in these applications can significantly boost productivity. However, gap exists between synthetic use cases and real-world scenarios, as synthetic datasets often lack sufficient office software-related cases. To bridge this, we propose targeted approach to designing and generating relevant data. Our methodology centers on creating two pools: resource pool and an action pool. The resource pool includes diverse set of office files, such as Excel spreadsheets, Word documents, and PowerPoint slides, sourced from the web, including online tutorials. The action pool enumerates common tasks performed in these applications, such as scrolling through document, clicking specific cells in spreadsheet, or auto-filling data in Excel. For each action, we manually analyze the associated structural components and develop code to extract relevant coordinate arrangements. For example, consider the action of \"scrolling document\" in Microsoft Word Online. The associated component is the scrollbar. We analyze the webpage structure to identify features that precisely locate the scrollbar, then use code to extract its coordinates, synthesizing data instance. Similarly, in Excel, for the action \"click the center of cell B3,\" we leverage the accessibility tree and HTML DOM structure to extract cell positions, generating precise instructions like \"click the center of cell B3,\" \"auto-fill from the bottom-right corner of cell A1,\" or \"select column D.\" These rule-based extraction methods ensure accurate component-level interactions across productivity applications. Additional actions and their components, including spreadsheet-specific tasks, are detailed in Table 13. Table 13: Actions and Associated Components in Office Software"
        },
        {
            "title": "Click\nClick\nClick\nClick\nClick",
            "content": "A.4 Additional Data Examples A.4."
        },
        {
            "title": "Cell\nEdge\nCell Corner\nColumn Header\nRow Header",
            "content": "In the icon description data, each data item includes an image of single icon as input and produces description that includes both its visual appearance and functionality as output. An example in JSON format is shown below. The corresponding image (Figure 9a) features an icon labeled krl_access, which likely represents access to commuter rail or train servicesuch as the KRL (Kereta Rel Listrik) system. 34 1 { 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 (a) Example of icon description data (b) Example of icon grounding data Figure 9: Examples of icon data \" conversations \" : [ { \" from \" : \" system \" , \" value \" : { \" text \" : \" You are GUI assistant that helps users understand and interact with interface elements . nWhen describing elements , focus on their visual appearance and functionality in clear , natural language . nFor each element , describe the visual appearance and functionality : n1 . What it looks like - including shape , color , symbols , and any text n2 . What it does when used - its purpose and effect n3 . When users might want to use it - common use cases and scenarios \" } } , { \" from \" : \" user \" , \" value \" : [ { \" image \" : \" g _ e _ o _ k u / cat egorized_ icons / Arcticons - Team / Arcticons / krl_access . png \" , \" width \" : 1 9 2 0 , \" height \" : 1 0 8 0 , \" resized_height \" : 1 0 9 2 , \" resized_width \" : 1 9 3 2 , \" seq_len \" : 2 6 9 3 \" text \" : \" Please describe what this interface element looks like and what it does . \" } , { } ] } , { \" from \" : \" assistant \" , \" value \" : [ { \" text \" : \" Visual Appearance : nThe icon is simple , white outline of train or subway car on black background . It features rectangular shape with rounded corners , two windows at the top , and front bumper at the bottom . The design is minimalistic and uses clean lines to represent the front view of train . nFunctionality : The icon labeled ' krl_access ' likely represents access to commuter rail or train service , such as KRL ( Kereta Rel Listrik ) system . It might be used in applications or websites to indicate train schedules , ticket purchasing , or station information . Clicking this icon could lead to more details about train services or access to train - related feature . \" } ] } ] , \" image_id \" : \" d4e5596b - c166 -4345 - a7d0 -947014505 f0a \" 31 32 33 34 35 }"
        },
        {
            "title": "Icon grounding data",
            "content": "In the icon grounding data, each data item includes an image containing multiple icons and description of the target icon as input, and produces grounding action (e.g., click) in the Qwen2.5-VL tool-use format as output. An example in JSON format is provided below. The image (Figure 9b) contains several icons, and the description corresponds to one commonly used icon which represents density or line spacing in user interfaces. 1 { 2 3 4 6 7 8 9 10 12 13 14 15 16 18 \" conversations \" : [ { \" from \" : \" system \" , \" value \" : { \" text \" : \" You are helpful assistant . # Tools nYou may call one or more functions to assist with the user query . nYou are provided with function signatures within < tools > </ tools > XML tags : < tools > { \" type \" : \" function \" , \" function \" : { \" name \" : \" computer_use \" , \" description \" : \" Use mouse and keyboard to interact with computer , and take screenshots . * This is an interface to desktop GUI . You do not have access to terminal or applications menu . You must click on desktop icons to start applications . * Some applications may take time to start or process actions , so you may need to wait and take successive screenshots to see the results of your actions . . . if you click on Firefox and window doesn 't open , try wait and taking another screenshot . * The screen 's resolution is 1 0 9 2 1 9 3 2 . * Whenever you intend to move the cursor to click on an element like an icon , you should consult screenshot to determine the coordinates of the element before moving the cursor . * If you tried clicking on program or link but it failed to load , even after waiting , try adjusting your cursor position so that the tip of the cursor visually falls on the element that you want to click . * ... \" } } , { \" from \" : \" user \" , \" value \" : [ { \" image \" : \" images_grounded / categorized_icons - / tabler / tabler - icons / baseline - density - medium . png \" , \" width \" : 1920 , \" height \" : 1080 , \" resized_height \" : 1092 , \" resized_width \" : 1932 , 36 \" seq_len \" : } , { \" text \" : \" Click on the icon with functionality : This icon is typically used to represent ' density ' or ' line spacing ' setting in user interfaces . It suggests adjusting the spacing between lines or items to medium level . Clicking this icon might change the display settings to medium density , balancing between compact and spacious layouts . \" } ] } , { \" from \" : \" assistant \" , \" value \" : [ { \" text \" : \" < tool_call > { \" name \" : \" computer_use \" , \" arguments \" : { \" action \" : \" left_click \" , \" coordinate \" : [ 7 0 9 , 4 7 1 ]}} </ tool_call > \" } ] } ] , \" image_id \" : \" cb 1 cb 4 ba - 6 1 ab - 4 caf - 8 8d - 5 7 ba 3 6 3 1 0 \" 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 36 } A.4.2 Component data (a) Example of rendered component grounding data (b) Example of doc grounding data (c) Example of slide grounding data (d) Example of sheet grounding data Figure 10: Examples of component data Component data from Code-and-rendering pipeline 37 In the component data from the Code-and-Rendering pipeline, each data item includes an image containing rendered UI component and corresponding user instruction as input, and produces grounding action (e.g., click) in the Qwen2.5-VL tool-use format as output. An example in JSON format is shown below. The associated image (Figure 10a) displays slider component, and the instruction reads \"...Read the text label displaying UI/UX Design Basics located in the lower third of the catalog interface, which is part of book listing...\" 1 { 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 \" conversations \" : [ { \" from \" : \" system \" , \" value \" : { \" text \" : \" You are helpful assistant . # Tools nYou may call one or more functions to assist with the user query . nYou are provided with function signatures within < tools > </ tools > XML tags : < tools > { \" type \" : \" function \" , \" function \" : { \" name \" : \" computer_use \" , \" description \" : \" Use mouse and keyboard to interact with computer , and take screenshots . * This is an interface to desktop GUI . You do not have access to terminal or applications menu . You must click on desktop icons to start applications . * Some applications may take time to start or process actions , so you may need to wait and take successive screenshots to see the results of your actions . . . if you click on Firefox and window doesn 't open , try wait and taking another screenshot . * The screen 's resolution is 1 0 9 2 1 9 3 2 . * Whenever you intend to move the cursor to click on an element like an icon , you should consult screenshot to determine the coordinates of the element before moving the cursor . * If you tried clicking on program or link but it failed to load , even after waiting , try adjusting your cursor position so that the tip of the cursor visually falls on the element that you want to click ... \" } } , { \" from \" : \" user \" , \" value \" : [ { \" image \" : \" data / chakra / slider / other_screenshot / - original / Ski llL evel Adj uster_ 1 7 4 2 0 0 5 8 3 7 . 4 7 5 9 6 9 . png \" , \" width \" : 1920 , \" height \" : 1080 , \" resized_height \" : 1092 , \" resized_width \" : 1932 , \" seq_len \" : 2693 \" text \" : \" Please generate the next move according to the UI screenshot and instruction . nInstruction : Set the skill level to 4 % on the slider . \" } , { } ] } , { \" from \" : \" assistant \" , \" value \" : [ 38 { } ] } \" text \" : \" < tool_call > { \" name \" : \" computer_use \" , \" arguments \" : { \" action \" : \" left_click \" , \" coordinate \" : [ 6 8 1 , 2 6 0 ]}} </ tool_call > \" ] , \" image_id \" : \" 5 6 8 8 9 3 0 - ec 6a - 4 5 7 4 -9 7 5 -a 1 8 ed 2 8 7 cc 0 \" 29 30 31 32 33 35 36 } Component data for real-world augmentation In the component data for real-world augmentation, each data item includes an image containing real-world screenshots from an existing website or application and corresponding user instruction as input, and produces grounding action (e.g., click) in the Qwen2.5-VL tool-use format as output. We provide one example for each of the three data sources: doc, slide, and sheet. The user instructions for these examples are as follows: Doc: Given the following text: \". Vestibulum a\", find this text in the document and click the space between the consecutive characters \"t\" and \"i\". Slide: Please generate the next move according to the UI screenshot and instruction. Instruction: Select the handle located at the top of the text box that contains the text \"Presentation title.\" Sheet: Navigate to the top-left corner of cell C19. Corresponding examples in JSON format are shown below. The associated UI screenshots are provided in Figures 10b, 10c, and 10d, respectively. Example of doc grounding data: 1 { 3 4 5 6 \" conversations \" : [ { \" from \" : \" system \" , \" value \" : { \" text \" : \" You are helpful assistant . # Tools nYou may call one or more functions to assist with the user query . nYou are provided with function signatures within < tools > </ tools > XML tags : < tools > { \" type \" : \" function \" , \" function \" : { \" name \" : \" computer_use \" , \" description \" : \" Use mouse and keyboard to interact with computer , and take screenshots . * This is an interface to desktop GUI . You do not have access to terminal or applications menu . You must click on desktop icons to start applications . * Some applications may take time to start or process actions , so you may need to wait and take successive screenshots to see the results of your actions . . . if you click on Firefox and window doesn 't open , try wait and taking another screenshot . * The screen 's resolution is 7 2 8 1 2 8 8 . * Whenever you intend to move the cursor to click on an element like an icon , you should consult screenshot to determine the coordinates of the element before moving the cursor . * If you tried clicking on program or link but it failed to load , even after waiting , try adjusting your cursor position so that the tip of the cursor visually falls on the element that you want to click ... \" 7 } } , { \" from \" : \" user \" , \" value \" : [ { \" image \" : \" AmHHgw - Nep 9 dv 1 3 9 5 gaoKsDxY_ 1 2 8 0 7 2 0 _SPACE_ 5 6 3 _ 5 . png \" , \" width \" : 1280 , \" height \" : 720 , \" resized_height \" : 728 , \" resized_width \" : 1288 , \" seq_len \" : 1198 } , { \" text \" : \" Given the following text : \" . Vestibulum \" , find the text in the document and click the space between the continuous character \" \" and \" \" in the text . \" } ] } , { \" from \" : \" assistant \" , \" value \" : [ { \" text \" : \" < tool_call > { \" name \" : \" computer_use \" , \" arguments \" : { \" action \" : \" left_click \" , \" coordinate \" : [ 6 8 8 , 6 3 0 ]}} </ tool_call > \" } ] } ] , \" image_id \" : \" 3 3 5 0 2 -d 5 4 1 -4 5 ea - be 2 3 -b 6 6 8 2 6 3 5 6 9 \" 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 32 33 34 35 } Example of slide grounding data: 1 { 2 4 5 6 \" conversations \" : [ { \" from \" : \" system \" , \" value \" : { \" text \" : \" You are helpful assistant . # Tools nYou may call one or more functions to assist with the user query . nYou are provided with function signatures within < tools > </ tools > XML tags : < tools > { \" type \" : \" function \" , \" function \" : { \" name \" : \" computer_use \" , \" description \" : \" Use mouse and keyboard to interact with computer , and take screenshots . * This is an interface to desktop GUI . You do not have access to terminal or applications menu . You must click on desktop icons to start applications . * Some applications may take time to start or process actions , so you may need to wait and take successive screenshots to see the results of your actions . . . if you click on Firefox and window doesn 't open , try wait and taking another screenshot . * The screen 's resolution is 7 2 8 1 2 8 8 . * Whenever you intend to move the cursor to click on an element like an icon , you should consult screenshot to determine the coordinates 40 of the element before moving the cursor . * If you tried clicking on program or link but it failed to load , even after waiting , try adjusting your cursor position so that the tip of the cursor visually falls on the element that you want to click ... \" } } , { \" from \" : \" user \" , \" value \" : [ { } , { \" image \" : \" slides_ 1 2 8 0 * 7 2 0 / slide_ 1 5 / original . png \" , \" width \" : 1280 , \" height \" : 720 , \" resized_height \" : 728 , \" resized_width \" : 1288 , \" seq_len \" : 1198 \" text \" : \" Please generate the next move according to the UI screenshot and instruction . nInstruction : Select the handle located at the top of the text box that contains the text \" Presentation title . \" \" } ] } , { \" from \" : \" assistant \" , \" value \" : [ { \" text \" : \" < tool_call > { \" name \" : \" computer_use \" , \" arguments \" : { \" action \" : \" left_click \" , \" coordinate \" : [ 4 6 7 , 2 0 8 ]}} </ tool_call > \" } ] } ] , \" image_id \" : \" 3 2 ebbae - dee 3 -4 fea - bbc 0 - ab 9 3 1 3 6 bedab \" 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 } Example of sheet grounding data: 1 { 2 3 4 6 \" conversations \" : [ { \" from \" : \" system \" , \" value \" : { \" text \" : \" You are helpful assistant . # Tools nYou may call one or more functions to assist with the user query . nYou are provided with function signatures within < tools > </ tools > XML tags : < tools > { \" type \" : \" function \" , \" function \" : { \" name \" : \" computer_use \" , \" description \" : \" Use mouse and keyboard to interact with computer , and take screenshots . * This is an interface to desktop GUI . You do not have access to terminal or applications menu . You must click on desktop icons to start applications . * Some applications may take time to start or process actions , so you may need to wait and take 41 successive screenshots to see the results of your actions . . . if you click on Firefox and window doesn 't open , try wait and taking another screenshot . * The screen 's resolution is 7 2 8 1 2 8 8 . * Whenever you intend to move the cursor to click on an element like an icon , you should consult screenshot to determine the coordinates of the element before moving the cursor . * If you tried clicking on program or link but it failed to load , even after waiting , try adjusting your cursor position so that the tip of the cursor visually falls on the element that you want to click ... \" } } , { \" from \" : \" user \" , \" value \" : [ { \" image \" : \" cel l_l ef t_t op _c or ne / images / 1 2 8 0 7 2 0 _cell_C 1 9 . png \" , \" width \" : 1280 , \" height \" : 720 , \" resized_height \" : 728 , \" resized_width \" : 1288 , \" seq_len \" : 1198 \" text \" : \" Navigate to top left corner of 1 9 \" } , { } ] } , { \" from \" : \" assistant \" , \" value \" : [ { \" text \" : \" < tool_call > { \" name \" : \" computer_use \" , \" arguments \" : { \" action \" : \" mouse_move \" , \" coordinate \" : [ 2 1 1 , 5 2 3 ]}} </ tool_call > \" } ] } ] , \" image_id \" : \" 1 5 9 8 7 cec - 7 4 4 -4 6 2 4 -b 5 3 9 -4 3 1 1 4 caac 8 \" 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 32 33 34 35 } A.4.3 Layout data (a) Example of icon description data (b) Example of icon grounding data Figure 11: Examples of component grounding data"
        },
        {
            "title": "Layout description data",
            "content": "In the layout description data, each item includes screenshot and the bounding box coordinates of target element as input, and generates description covering the elements visual composition, spatial context, user interaction, and type. An example in JSON format is provided below. The associated image (Figure 11a) shows product interface from shopping website, where the target element is an image carousel featuring thumbnail views of laptop. 1 { 2 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24 25 26 27 28 \" conversations \" : [ { \" from \" : \" system \" , \" value \" : { \" text \" : \" Working as UI navigation guide , you will analyze screenshot and bounding box coordinates . Your objective is to provide detailed , user - friendly description of the UI element within the specified region , helping users identify it without relying on coordinates . Address these key points : n1 . Visual Composition : Detail the element 's visual aspects , including its geometry , color scheme , and styling n2 . Spatial Context : Describe the element 's position in relation to surrounding interface components n3 . User Interaction : Explain the element 's functionality and how users should interact with it n4 . Element Type : Specify the category of UI component ( such as progress bar , tooltip ) nMaintain precision and clarity in your descriptions , avoiding vague references . Emphasize distinctive characteristics and contextual relationships that make the element easily identifiable . \" } } , { \" from \" : \" user \" , \" value \" : [ { \" image \" : \" ce0ff960 ... \" ( Truncated due to layout constraints ) , \" width \" : 1 9 2 0 , \" height \" : 1 0 8 0 , \" resized_height \" : 1 0 9 2 , \" resized_width \" : 1 9 3 2 , \" seq_len \" : 2 6 9 3 \" text \" : \" bounding box : =300 , =691 , =616 , =96. Generate pls . \" } , { } ] } , { \" from \" : \" assistant \" , \" value \" : [ { \" text \" : \" ## Visual Composition : The element consists of horizontal strip of thumbnail images , each depicting different views of laptop . The thumbnails are bordered by thin line and are evenly spaced . On either side of the strip , there are orange circular buttons with white arrows , indicating navigation options to scroll through more images . The 43 thumbnails include images of the laptop from various angles and box content view . ## Spatial Context : The element is located centrally below the main product image within pop - up product detail view . It is positioned above the product description and purchase options , acting as secondary image gallery for the product . ## User Interaction : The primary function of this element is to allow users to preview different images of the product . Users can click on the thumbnails to view larger version of the image in the main display area above . The navigation arrows enable scrolling through additional images . ## Element Type : Image carousel with navigation buttons . \" } ] } ] , \" image_id \" : \" e54778c0 -4 d45 -4997 - a382 -33244 ced2513 \" 30 31 32 34 35 }"
        },
        {
            "title": "Layout grounding data",
            "content": "In the layout grounding data, each data item includes screenshot and corresponding user description of an element as input, and produces bounding box of corresponding element as output. An example in JSON format is shown below. The associated image (Figure 11b) displays the layout of adding Microsoft account, and the description refers to the text input box that allows users to input their phone number, email, or Skype ID. 1 { 2 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 \" conversations \" : [ { \" from \" : \" system \" , \" value \" : { \" text \" : \" Operating as GUI element localization agent , your primary function is to process screenshots and element descriptions to identify the exact position of interface components . You work with coordinates (x , , width , height ) . \" } } , { \" from \" : \" user \" , \" value \" : [ { \" image \" : \" 06 c6786b ... \" ( Truncated due to layout constraints ) , \" width \" : 1 2 8 0 , \" height \" : 7 2 0 , \" resized_height \" : 7 2 8 , \" resized_width \" : 1 2 8 8 , \" seq_len \" : 1 1 9 8 \" text \" : \" The Text input box 's intended function : nThe primary function of this element is to allow users to input their phone number , email , or Skype ID as part of the Microsoft account sign - in process . Users are expected to click or tap on the box to activate it and then type their credentials . \" } , { } 44 ] } , { \" from \" : \" assistant \" , \" value \" : [ { \" text \" : \" 492 344 296 32 \" } ] } ] , \" image_id \" : \" db53ba9b -7416 -4 a01 - bb19 - e2a66d3a666d \" 23 25 26 27 28 29 31 32 33 34 35 } For demonstration purposes, we only show one round of dialogue in each \"conversations\" entry. In practice, to speed up training and save storage space, multiple rounds of dialogue related to single screenshot are consolidated into single JSON objects \"conversations\" list. A.5 Additional Case Studies In this section, we explain more logic behind the models actions and analyze the factors that contribute to their improvements or failures. A.5.1 Cases of improvement Case 12a Leveraging training on icon, component, and layout data, JEDI places greater emphasis on keywords like button and on the left. In contrast, Qwen concentrates more on literal text matching, resulting in clicking on Sheet1. Case 12b JEDI effectively identifies the correct icon with the specified function from numerous elements on the screen, showcasing its deep understanding of common icon functionalities. In contrast, traditional models often struggle to learn the association between icons and their functions when trained with coarse-grained data. Case 12c To execute this example correctly, models must thoroughly understand both the specific component (what constitutes horizontal scroll bar) and the overall layout (where the scroll bar is located). The Qwen model, however, interacted with an unrelated element. Case 12d We found that the base model, which has not been trained on components and layouts, may not accurately manage subpages such as pop-ups and message bars. In contrast, JEDI successfully identifies clickable text links. Case 12e This task involves having the model click on specific mathematical symbol. Although the Qwen model demonstrates strong mathematical skills, these abilities do not improve its GUI grounding capability without fine-tuning on decomposed GUI data. Case 12f The GUI for this task includes variety of elements and complex functions. However, JEDI successfully identified the area relevant to mode switching through precise text matching. 45 (a) Instruction: Add sheet by clicking the button on the left of \"Sheet1\". (b) Instruction: Add Decimal Place for the current cell. (c) Instruction: Drag the horizontal scroll bar to center the image in the viewing area. (d) Instruction: Check the privacy policy of delta.com. (e) Instruction: Click on the character of PI. (f) Instruction: Change the mode of this image. Figure 12: Additional cases demonstrate JEDIs improvement compared to Qwen2.5-VL-7B-Instruct. The green square represents the click position of JEDI, while the red square indicates the click position of Qwen. 46 A.5.2 Cases of failure (a) Instruction: Collapse the Properties panel by clicking on the right arrow. (b) Instruction: Align the text to the center. (c) Instruction: Accept the cookie preferences. (d) Instruction: Click on the letter \"o\" of the word \"for\" in the slide. Figure 13: Four OSWORLD-G failure cases for JEDI. JEDI still faces challenges in certain situations. We present selection of representative examples, with solutions to these challenges reserved for future work. Case 13a The task required the model to click the right-pointing arrow to close the right panel. Instead, JEDI clicked the button. Although this action was functionally correct, it did not strictly follow the requirement to click the right arrow. Therefore, we classify this as failure case. Case 13b The interface for this task shows the word center twice. One is displayed in white, indicating it is active, while the other is in gray, indicating it is inactive. JEDI misunderstood the color difference and mistakenly clicked the lower, inactive center option. Case 13c the cookie preferences option. We believe the wording in the instruction misled the model. In this task, JEDI was instructed to accept the cookie options. However, it mistakenly focused on Case 13d This task required clicking on letter in an artistic font, highlighting the models current limitations in handling grounding tasks involving artistic or stylized designs. This shortcoming may arise from the lack of art and design data in the current training corpus. A.6 Agentic Benchmark Results of JEDI 47 Table 14: Detailed performance of JEDI on OSWorld with four runs for each configuration TB"
        },
        {
            "title": "Calc",
            "content": "OS OS"
        },
        {
            "title": "Model",
            "content": "JEDI-3B JEDI-7B"
        },
        {
            "title": "15 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "50 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "100 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "15 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "50 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "100 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "Avg",
            "content": "21.95 22.76 22.37 22.36 32.50 22. 23.83 24.73 23.61 22.36 33.33 23. 24.43 25.19 23.66 22.74 34.44 24. 22.20 23.04 22.42 23.31 31.86 22. 26.06 26.27 23.87 23.87 35.56 25. 25.94 29.40 25.64 26.86 38.89 27. 39.13 39.13 43.48 39.13 6.38 6. 6.38 8.51 60.87 12.77 - - 47.83 43.48 45.45 39.13 6.38 8. 6.38 8.51 52.17 10.64 - - 38.10 40.91 8.51 8.51 45.45 10. 39.13 6.38 56.52 14.89 - - 43.48 8.51 43.48 10.64 34.78 10. 34.78 17.02 52.17 21.28 - - 30.43 17.02 47.83 19.15 39.13 14. 34.78 12.77 52.17 25.53 - - 39.13 14.89 52.17 12.77 43.48 6. 34.78 10.64 65.22 21.28 - - 8.57 12.83 12.77 10.70 14.89 - 10.70 12.77 10.64 12.77 12.77 - 13.11 14.95 17.47 13.11 21.28 - 10.70 10.70 10.70 8.51 10.70 - 8.59 10.71 14.95 10.70 14.89 - 16.30 14.95 14.95 10.71 21.28 - 26.08 17.38 17.38 34.77 43.48 - 26.08 39.12 43.47 21.73 56.52 - 26.08 39.12 30.42 21.73 43.48 - 30.42 30.42 21.73 43.47 43.48 - 30.42 43.47 43.47 34.77 56.52 - 34.77 43.47 36.35 39.12 60.87 - 29.41 20.00 29.41 33.33 17.65 33. 32.72 26.67 47.06 33.33 - - 29.41 33.33 23.53 40.00 29.41 33. 23.53 33.33 47.06 46.67 - - 23.53 46.67 29.41 40.00 23.53 20. 31.32 20.00 41.18 46.67 - - 35.29 26.67 17.65 33.33 29.41 33. 23.53 20.00 41.18 40.00 - - 40.40 46.67 38.96 40.00 17.65 33. 23.53 33.33 47.06 53.33 - - 29.41 26.67 29.41 46.67 20.72 53. 29.41 46.67 41.18 53.33 - - 26.09 28.06 25.94 21.54 34.78 - 32.61 30.43 26.09 23.91 36.96 - 32.40 34.58 25.88 32.40 39.13 - 23.71 28.05 25.88 28.05 39.13 - 34.58 25.88 32.40 25.88 39.13 - 32.40 36.75 28.05 32.40 45.65 - 56.52 60.87 56.52 60.87 78.26 - 52.17 47.83 47.83 56.52 78.26 - 43.48 39.13 56.52 56.52 69.57 - 56.52 65.22 65.22 60.87 82.61 - 60.87 56.52 56.52 69.57 86.96 - 60.87 73.91 73.91 78.26 95.65 - 50.00 53.85 53.85 50.00 73.08 - 53.85 64.00 53.85 53.85 76.92 - 68.00 56.00 53.85 46.15 73.08 - 56.00 50.00 42.31 50.00 65.38 - 61.54 46.15 46.15 53.85 65.38 - 46.15 57.69 46.15 53.85 65.38 - 11.83 9.68 11.39 8.60 15.05 - 10.48 9.78 10.75 9.68 15.05 - 9.68 12.62 9.78 11.55 18.28 - 5.38 7.53 10.75 7.61 11.83 - 8.60 10.75 5.38 8.60 13.98 - 12.90 11.83 11.68 11.56 17.2 - 39.13 39.13 43.48 39.13 60.87 - 47.83 43.48 45.45 39.13 52.17 - 38.10 40.91 45.45 39.13 56.52 - 43.48 43.48 34.78 34.78 52.17 - 30.43 47.83 39.13 34.78 52.17 - 39.13 52.17 43.48 34.78 65.22 - 11.13 11.13 11.11 14.55 19.66 - 11.99 16.54 15.38 13.46 20.51 - 13.82 17.12 17.27 12.09 23.08 - 13.70 14.55 12.84 18.80 21.37 - 16.27 20.54 20.54 16.26 27.35 - 19.39 19.68 15.54 16.27 29.06 - 25.64 53.06 29.37 57.14 25.55 55. 24.96 55.10 37.18 75.51 - - 32.05 53.06 30.77 56.25 28.21 51. 25.64 55.10 41.03 77.55 - - 33.21 56.25 34.49 47.92 24.24 55. 29.78 51.02 41.03 71.43 - - 24.00 56.25 23.68 57.14 26.20 53. 19.74 55.10 31.58 73.47 - - 38.17 61.22 31.45 51.02 29.36 51. 26.8 61.22 11.83 9.68 11.39 8. 15.05 - 10.48 9.78 10.75 9. 15.05 - 9.68 12.62 9.78 11. 18.28 - 5.38 7.53 10.75 7. 11.83 - 8.60 10.75 5.38 8. 43.59 75.51 13.98 - - - 30.65 53.06 37.06 65.31 31.32 59. 34.49 65.31 46.15 79.59 - - 12.90 11.83 11.68 11.56 17.2 - 48 Table 15: Detailed performance of JEDI on WindowsAgentArena with four runs for each configuration Configuration Overall Chrome File Explorer Notepad Edge OS Settings VLC VS Code Calculator Libre Calc Libre Writer Paint"
        },
        {
            "title": "Model",
            "content": "JEDI-3B JEDI-7B"
        },
        {
            "title": "15 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "50 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "100 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "15 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "50 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "100 Steps",
            "content": "Run 1 Run 2 Run 3 Run 4 Pass@"
        },
        {
            "title": "Avg",
            "content": "28.86 28.72 29.92 28.72 41.33 29. 32.05 32.48 32.05 28.72 44.00 31. 34.57 30.72 33.23 33.61 46.67 33. 30.00 29.38 31.90 29.38 42.67 30. 32.57 32.57 34.05 32.00 46.00 32. 33.90 34.67 33.46 32.67 47.33 33. 0.00 5.88 5.88 5.88 5.88 - 5.88 5.88 0.00 5.88 5.88 - 5.88 5.88 5.88 6.25 11.76 - 5.88 0.00 0.00 0.00 5.88 - 0.00 11.76 0.00 0.00 11.76 - 0.00 5.88 0.00 5.88 5.88 - 47.37 36.84 47.37 42.11 57.89 - 47.37 44.44 57.89 42.11 63.16 - 52.63 57.89 63.16 47.37 63.16 - 31.58 31.58 42.11 42.11 52.63 - 52.63 47.37 47.37 42.11 52.63 - 52.63 47.37 47.37 52.63 63.16 - 50.00 50.00 50.00 50.00 50.00 - 50.00 50.00 0.00 50.00 50.00 - 50.00 50.00 50.00 50.00 50.00 - 50.00 50.00 50.00 50.00 50.00 - 50.00 50.00 50.00 50.00 50.00 - 50.00 50.00 50.00 50.00 50.00 - 30.77 23.08 30.77 38.46 38.46 - 30.77 38.46 23.08 38.46 53.85 - 38.46 30.77 7.69 30.77 53.85 - 23.08 23.08 38.46 30.77 46.15 - 30.77 46.15 46.15 46.15 61.54 - 30.77 38.46 38.46 38.46 53.85 - 60.00 80.00 40.00 60.00 80.00 - 60.00 40.00 60.00 60.00 60.00 - 80.00 80.00 60.00 80.00 80.00 - 40.00 60.00 60.00 60.00 60.00 - 80.00 60.00 80.00 60.00 80.00 - 80.00 60.00 80.00 40.00 80.00 - 38.10 52.75 42.32 43.23 57.14 - 43.23 47.99 52.75 42.23 52.38 - 46.91 33.70 42.15 43.23 57.14 - 52.38 43.23 42.15 42.23 57.14 - 46.91 51.67 43.23 52.38 61.90 - 46.91 52.38 43.76 42.86 61.90 - 45.83 45.83 50.00 37.50 70.83 - 45.83 52.17 50.00 37.50 75.00 - 45.83 33.33 58.33 58.33 70.83 - 41.67 50.00 50.00 41.67 70.83 - 50.00 41.67 50.00 45.83 70.83 - 54.17 45.83 45.83 45.83 75.00 - 0.00 0.00 0.00 0.00 0.00 - 0.00 0.00 0.00 0.00 0.00 - 0.00 0.00 0.00 0.00 0.00 - 0.00 0.00 0.00 0.00 0.00 - 0.00 0.00 33.33 0.00 33.33 - 0.00 0.00 0.00 33.33 33.33 - 4.17 8.33 4.17 8.33 8.33 - 8.33 4.17 8.33 8.33 8.33 - 12.50 8.33 8.33 8.33 12.50 - 8.33 8.33 4.17 8.33 8.33 - 4.17 4.17 4.17 8.33 8.33 - 8.33 8.33 8.33 8.33 8.33 - 26.30 10.53 26.30 21.04 42.11 - 31.57 36.83 26.30 21.04 47.37 - 31.57 36.83 31.58 21.04 52.63 - 36.83 31.57 36.83 26.30 52.63 - 26.30 26.30 31.57 26.30 47.37 - 31.57 36.83 42.09 31.57 52.63 - 33.33 33.33 33.33 33.33 33.33 - 66.67 33.33 33.33 33.33 66.67 - 33.33 33.33 33.33 66.67 66.67 - 66.67 66.67 66.67 66.67 66.67 - 66.67 33.33 66.67 33.33 66.67 - 33.33 66.67 33.33 33.33 66.67 -"
        }
    ],
    "affiliations": [
        "Salesforce AI Research",
        "The University of Hong Kong"
    ]
}