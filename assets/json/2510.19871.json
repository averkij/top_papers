{
    "paper_title": "From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model",
    "authors": [
        "Yatai Ji",
        "Teng Wang",
        "Yuying Ge",
        "Zhiheng Liu",
        "Sidi Yang",
        "Ying Shan",
        "Ping Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Discrete diffusion models have emerged as a promising direction for vision-language tasks, offering bidirectional context modeling and theoretical parallelization. However, their practical application is severely hindered by a train-inference discrepancy, which leads to catastrophic error cascades: initial token errors during parallel decoding pollute the generation context, triggering a chain reaction of compounding errors and leading to syntactic errors and semantic hallucinations. To address this fundamental challenge, we reframe the generation process from passive denoising to active refining. We introduce ReDiff, a refining-enhanced diffusion framework that teaches the model to identify and correct its own errors. Our approach features a two-stage training process: first, we instill a foundational revision capability by training the model to revise synthetic errors; second, we implement a novel online self-correction loop where the model is explicitly trained to revise its own flawed drafts by learning from an expert's corrections. This mistake-driven learning endows the model with the crucial ability to revisit and refine its already generated output, effectively breaking the error cascade. Extensive experiments demonstrate that ReDiff significantly improves the coherence and factual accuracy of generated content, enabling stable and efficient parallel generation far superior to traditional denoising methods. Our codes and models are available at https://rediff-hku.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 1 7 8 9 1 . 0 1 5 2 : r FROM DENOISING TO REFINING: CORRECTIVE FRAMEWORK FOR VISION-LANGUAGE DIFFUSION MODEL Yatai Ji1, Teng Wang2, Yuying Ge2, Zhiheng Liu1, Sidi Yang1, Ying Shan2, Ping Luo1 1The University of Hong Kong, 2ARC Lab, Tencent PCG"
        },
        {
            "title": "ABSTRACT",
            "content": "Discrete diffusion models have emerged as promising direction for visionlanguage tasks, offering bidirectional context modeling and theoretical parallelization. However, their practical application is severely hindered by train-inference discrepancy, which leads to catastrophic error cascades: initial token errors during parallel decoding pollute the generation context, triggering chain reaction of compounding errors and leading to syntactic errors and semantic hallucinations. To address this fundamental challenge, we reframe the generation process from passive denoising to active refining. We introduce ReDiff, refining-enhanced diffusion framework that teaches the model to identify and correct its own errors. Our approach features two-stage training process: first, we instill foundational revision capability by training the model to revise synthetic errors; second, we implement novel online self-correction loop where the model is explicitly trained to revise its own flawed drafts by learning from an experts corrections. This mistake-driven learning endows the model with the crucial ability to revisit and refine its already generated output, effectively breaking the error cascade. Extensive experiments demonstrate that ReDiff significantly improves the coherence and factual accuracy of generated content, enabling stable and efficient parallel generation far superior to traditional denoising methods. Our codes and models are available at https://rediff-hku.github.io/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Discrete diffusion models have recently emerged as promising alternative to the dominant autoregressive (AR) paradigm for vision-language models (VLMs) (You et al., 2025; Yang et al., 2025; Li et al., 2025a; Wang et al., 2025a; Swerdlow et al., 2025; Li et al., 2025b; Yu et al., 2025). Unlike AR models, which generate text token-by-token in fixed unidirectional manner, diffusion models conceptualize generation as an iterative denoising process. This approach allows for bidirectional context modeling, granting them greater flexibility in controlling the generation process and theoretical potential for massive parallelization, promising significant gains in inference efficiency (Nie et al., 2025; Ye et al., 2025; Song et al., 2025; Wu et al., 2025). However, significant gap exists between the theoretical promise and the practical reality of these models. Existing discrete diffusion models (Nie et al., 2025; You et al., 2025; Li et al., 2025a) are often plagued by incoherent and hallucinated artifacts (e.g., formatting errors like sequential commas or visually misaligned text) when parallel generation, frequently defaulting to one-token-per-step decoding process. We argue that these shortcomings are symptoms of deeper, more fundamental problem: the error cascade driven by training-inference discrepancy. Models are trained exclusively on clean, ground-truth data but are required at inference to generate from their own noisy, intermediate outputs. In parallel decoding scenario, this discrepancy becomes catastrophic. As illustrated in Figure 1 (a), an error in few tokens instantly pollutes the context for all other tokens being generated in parallel, initiating cycle of compounding errors, which produces detailed yet entirely fabricated description of the input image. Corresponding author. 1 Figure 1: Comparison between standard vision-language diffusion models and our proposed refining-enhanced approach. (a) Mask-pred diffusion is trained for passive denoising (mask recovering under fixed context). An initial error, such as misidentifying the bus as trunk, triggers an error cascade. The model cannot correct this mistake and proceeds to hallucinate further details based on the flawed context (e.g., Two men are drinking), leading to factually incorrect description. (b) Refining-enhanced diffusion introduces paradigm of active refining, teaching the model not only to predict masked tokens but also to perform token refinement. Our ReDiff learns to self-correct through an online loop where its own flawed drafts are revised by an expert revisor. As result, the model can identify and correct its initial mistakes (revising trunk to bus, Two men to man), breaking the error cascade and generating factually grounded response. (c) Performance comparison between LLaDA-V (You et al., 2025) and ReDiff under different inference speeds. CLAIR and Coverage are detailed caption metrics on CapMAS (Lee et al., 2024), and CAPTURE is on DetailCaps-4870 (Dong et al., 2024). Our model delivers superior generation quality and achieves more stable results when using fewer inference steps. To break this vicious cycle, we propose paradigm shift: from passive denoising (mask recovering under fixed context) to active refining. We introduce corrective framework for vision-language diffusion models, called ReDiff, which systematically teaches the model to identify and correct its own errors during denoising. Unlike previous models that merely fill masked tokens, ReDiff actively refines the entire context to guide the generation process. Our approach consists of twostage training process. First, we instill foundational revision capability by training the model to correct synthetic errors, such as random token corruptions and injected hallucinations, moving beyond simple denoising to build general capacity for revision. Second, we introduce an online self-correction loop where the model is forced to confront and learn from its own mistakes. By capturing its flaweddrafts during training and learning to predict an experts revision, the model directly mitigates the training-inference discrepancy. This mistake-driven learning endows the model with crucial, previously absent capability: the ability to revisit and refine its own outputs, including previously unmasked tokens. By learning to self-correct, our model develops robustness to its own imperfections, effectively breaking the error cascade and enabling robust parallel generation. As shown in Figure 1 (b), our refinement-based model successfully identifies and revises an initial error, leading to more factually grounded and accurate generation. Our contributions are threefold: 1) We propose new perspective that reframes the generation process of diffusion models from passive denoising to active, iterative refining to address the core challenge of error cascades. 2) We design and implement two-stage training framework, featuring core online self-correction loop that enables the model to learn to fix its own intrinsic errors. 2 3) Extensive experiments demonstrate that our method significantly improves the coherence and factual accuracy of generated content, exhibiting stability far superior to traditional denoising methods, especially in challenging few-step parallel generation scenarios, thereby greatly enhancing inference efficiency."
        },
        {
            "title": "2.1 LARGE LANGUAGE DIFFUSION MODELS",
            "content": "Discrete diffusion models (Austin et al., 2021; Lou et al., 2024; Huang et al., 2025; Arriola et al., 2025; Sun et al., 2023; Sahoo et al., 2024) represent class of generative models tailored for discrete data like text. In contrast to image diffusion models, which corrupt data by adding Gaussian noise towards standard Gaussian prior, text diffusion models typically operate by replacing original tokens to degrade semantic content. Early approaches, such as D3PM (Austin et al., 2021), employed discrete Markov chains where transition matrix is progressively applied to the input, corrupting it towards uniform distribution (i.e., any token becomes any other with equal probability) or an absorbing state (e.g., [MASK] token). More recently, mask-and-pred diffusion models have demonstrated significant empirical success. For instance, LLaDA (Nie et al., 2025) achieves performance comparable to autoregressive large language models by generating sentences from fully masked sequence, progressively unmasking tokens with the highest confidence. Similarly, Dream (Ye et al., 2025) has shown strong results by initializing its parameters from pre-trained autoregressive model. Theoretically, discrete diffusion models offer advantages over traditional autoregressive models (Touvron et al., 2023; Team, 2025; Bi et al., 2024; vicuna, 2023; OpenAI, 2023). Their bidirectional context modeling enables flexible and controllable generation, while their inherent parallelism promises significant acceleration in sampling speed. However, this potential for parallel generation remains largely untapped. Current models often suffer from output degradationsuch as repetition and grammatical errorswhen attempting to predict multiple tokens per step. Our work directly addresses this by enhancing the stability of parallel decoding. This aligns with recent line of work exploring the correction of generated content. For example, SEED-Diffusion (Song et al., 2025) introduced an edit-based forward process for code generation, which adds edit-specific noise in the final 20% of steps to allow for revisions. Likewise, FUDOKI (Wang et al., 2025a), multimodal model based on discrete flow matching, progressively revises random sentence, where each word is uniformly sampled from the vocabulary, to the correct answer. Our method is distinct in that it treats revision not as another form of noise, but as high-level refinement process. Specifically, our framework trains the model to learn from and correct its own characteristic errors, moving beyond simple noise reversal. 2.2 LARGE VISION LANGUAGE MODELS Large vision language models (LVLMs) (Liu et al., 2023; Dai et al., 2023; Li et al., 2024; Bai et al., 2023; Ji et al., 2023; Wang et al., 2025c) have achieved remarkable success in vision understanding and have been applied to myriad of real-world scenarios (Ji et al., 2025; Zhang et al., 2023; Cheng et al., 2024). The dominant architecture connects pre-trained vision encoder (Radford et al., 2021; Tschannen et al., 2025) to an autoregressive language model via lightweight module like an MLP or Q-Former. These models first realize cross-model alignment with pre-training and then conduct visual instruction tuning to handle wide range of vision-centric tasks. Despite their success, persistent challenge in LVLMs is the phenomenon of hallucination (Bai et al., 2024), where the model generates text that is factually inconsistent with the visual input. In autoregressive models, this issue is exacerbated by error propagation; an incorrectly generated token can irreversibly misguide the subsequent generation path. Notably, current multimodal discrete diffusion models, such as LLaDA-V (You et al., 2025), LaViDa (Li et al., 2025a), and MMaDA (Yang et al., 2025), also adhere to this limitation, fixing tokens in place once they are unmasked. Our ReDiff, however, leverages the bidirectional attention mechanism inherent to the diffusion paradigm. This allows our model to revisit and optimize already-generated content, enabling progressive refinement process that directly mitigates hallucination. Figure 2: Overview of our proposed two-stage training framework for corrective refining. (a) We illustrate common failure modes in standard vision-language diffusion models, which are prone to generating syntactic errors (e.g., Domin bus bus) and factual hallucinations (e.g., woman). (b) In the foundational revision training stage, we instill general corrective capability by training base model (ReDiff-Base) to revise synthetic errors that are intentionally injected into ground-truth captions. (c) For the second stage, i.e., online self-correction learning, the model generates its own flawed drafts. These drafts, containing the models intrinsic errors, are then revised by an expert AI assistant. The resulting draft-refined pairs provide strong supervision, teaching our final model (ReDiff) to identify and correct its own characteristic mistakes, thus breaking the error cascade."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we introduce our refining-enhanced diffusion framework, ReDiff, designed to enhance the generation accuracy and stability of vision-language diffusion models. In contrast to traditional approaches that focus on recovering text from all [MASK] noise, our work emphasizes the high-level refinement of generated text. Guided by an expert model, our framework enables the model to learn from its own generation errors. This fosters self-correction capability during inference, allowing it to simultaneously unmask new tokens while refining previously generated ones, thereby mitigating the problem of error cascades in parallel generation. We will first present the preliminaries of discrete diffusion models in Section 3.1. We then introduce the first stage of our approach, foundational revision training, in Section 3.2 . Section 3.3 details the core of our framework, online self-correction learning. Section 3.4 details the inference process. 3.1 PRELIMINARIES OF DISCRETE DIFFUSION MODELS discrete diffusion model formalizes text generation through forward and reverse process. The forward process gradually corrupts clean text sequence x0 into noisy state xt over series of timesteps [0, 1]. In mask-pred models, this is achieved by replacing tokens with [MASK] token based on noise schedule γt, culminating in fully masked sequence as prior distribution. The forward process is formulated as: q(cid:0)xt[i] = (cid:12) (cid:12) x0[i](cid:1) = (cid:40)1 γt, γt, if = x0[i], if = M. (1) The reverse process aims to reverse this corruption. Starting from fully masked sequence, the model iteratively predicts the original tokens. At each step, it predicts probabilities for all masked positions, unmasks few high-confidence tokens, re-masks the rest, and feeds the updated sequence back into the model for the next iteration. The model, parametric mask predictor, is trained to predict all masked tokens (denoted by set M) simultaneously. The training objective is cross-entropy loss computed only on the masked tokens: LCE(θ) = Et,v,p0,r0,rt 1 Lr0(cid:88) i=1 1[ri = M] log pθ(ri 0v, p0, rt) , (2) where and p0 denote visual content and prompt, r0 is the correct response, is sampled uniformly, and rt is sampled from the forward process. key advantage of discrete diffusion models is their potential for parallel generation, where multiple tokens are unmasked in single step, significantly reducing the number of required iterations. However, existing model (Nie et al., 2025; Ye et al., 2025; You et al., 2025) treat already-unmasked tokens as fixed conditions for future predictions. If an incorrect token is generated, it can derail subsequent steps, leading to an error cascade. Yet, unlike the unidirectional attention in AR models, the bidirectional attention mechanism inherent to these models provides the architectural foundation for updating previously generated tokens, potential we exploit in our framework. 3.2 STAGE I: FOUNDATIONAL REVISION TRAINING Observations of existing vision-language diffusion models, especially in few-step generation scenarios, reveal two predominant error types: syntactic chaos (e.g., incoherence, repetition, grammatical errors) and semantic hallucinations (content that contradicts the visual input), as shown in Figure 2 (a). In this first training stage, we teach the model to correct these two types of errors, extending its capability from simple denoising to foundational text revision. We use two data construction ways. For syntactic errors, we corrupt the text from ground-truth image-text pairs by randomly replacing fraction of tokens with other tokens from the vocabulary, creating syntactically chaotic inputs. For hallucination errors, we leverage pairs of correct captions and human-corrputed captions with factual errors (e.g., incorrect objects, attributes, or counts), which directly provide examples of visually inconsistent text. As shown in Figure 2 (b), we task the model with restoring polluted response rt to its original, correct version r0. We first apply the standard masking process to r0 according to sampled noise level t. Then, on the remaining unmasked tokens, we inject the synthetic errors described above. This corrupted sequence serves as the models input. The model is trained to predict the entire original text r0. The loss is computed not only on the [MASK] tokens but also on the syntactically corrupted tokens (Lsyntax) and hallucinated tokens (Lhallucination). We also include loss on the uncorrupted tokens (Lclean) to encourage the model to preserve correct content. The loss of each type is calculated as follows: Ltype(θ) = Et,v,p0,r0,rt 1 1 Ntype Lr0(cid:88) i=1 1[ri type] log pθ(ri 0v, p0, rt) , (3) where type {mask, syntax, hallucination, clean}. Each loss component is normalized by the number of its corresponding tokens Ntype to balance their contributions. The total loss is: Lrevision = Lmask + Lsyntax + Lhallucination + Lclean. (4) After Stage I, we obtain ReDiff-Base, model equipped with the foundational capability to correct both syntactic errors and factual hallucinations. However, this stage has limitation: the errors are synthetic and may not reflect the characteristic mistakes the model itself is prone to making. 3.3 STAHE II: ONLINE SELF-CORRECTION LEARNING To teach the model to fix its own idiosyncratic errors, we introduce an online self-correction learning framework. The process, illustrated in Figure 2 (c), proceeds as follows: (1) Generating drafts: We use ReDiff-Base to generate response for an image, denoted as rdraft. We typically use decoding results of different generation steps to cover more mistakes. (2) Expert revision: The image I, the 5 Table 1: Performance comparison with state-of-the-art models on three detailed image caption benchmarks. The best scores of vision-language diffusion models are in bold. CapArena CapArena-Auto CapMAS CLAIR Coverage DetailCaps-4870 CAPTURE Factuality Model AR model LLaVA-1.5-7B (Liu et al., 2024) InternVL-2.5-7B (Chen et al., 2024) Qwen2.5-VL-7B Bai et al. (2023) Discrete diffusion model MMaDA (Yang et al., 2025) FUDOKI (Wang et al., 2025a) LaViDa (Li et al., 2025a) LLaDA-V (You et al., 2025) ReDiff (ours) 62.10 78.37 80.48 35.45 51.94 56.22 65.54 76. 34.30 52.57 57.32 14.33 39.18 44.18 49.22 55.07 52.80 78.69 82.73 57.98 46.04 53.57 61.06 63.29 -94.00 -29.83 -16.83 -97.00 -98.83 -90.00 -77.17 -51. 51.08 57.80 60.61 19.55 57.92 57.28 59.62 61.88 generated draft rdraft, and the ground truth are fed to powerful external expert model (e.g., o4mini). With carefully designed prompt, the expert model identifies and corrects both grammatical and hallucinatory errors in rdraft, producing refined version, rrefined. We specifically extract the pairs of erroneous and corrected segments. (3) Learning to refine: We form new training instance <I, rdraft, rrefined> and fine-tune our model on these data. Note that the training loss is computed only on the segments that the expert model identified and corrected. This targeted learning prevents the model from being penalized for other potential errors in the draft that the expert may have missed. The training loss is: Lrefine(θ) = Et,v,p0,rdraft,rrefined 1 Nmistake Lr0(cid:88) i=1 1[ri draft mistake] log pθ(ri refinedv, p0, rdraft) . (5) To maintain the foundational capabilities learned in the first stage, we mix in small amount of the Stage data during this phase. This entire cycle can be iterated: the refined model from one round can be used to generate new drafts for the next round of expert revision and fine-tuning, progressively enhancing its self-correction ability. The key advantage here is that the model learns from its own mistakes, which is more targeted and efficient way to improve its robustness and the stability of parallel generation. 3.4 INFERENCE PROCESS Our inference process differs from that of traditional discrete diffusion models by integrating refinement into each generation step. Specifically, the process starts with fully masked sequence. At each step, the model computes the output probability distribution over the entire vocabulary for all token positions. For masked positions, if the inference speed is tokens per step, we select the top-n most confident tokens and unmask them. For previously unmasked positions, we replace the existing tokens with the newly predicted ones. This allows for the simultaneous unmasking of new content and refining of existing content. As more context is generated, previously generated tokens are iteratively updated to be more coherent and factually accurate, effectively reducing the occurrence of syntactic chaos and hallucinations."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENT SETTINGS Training Setup. Our primary focus is on enhancing the generative capabilities of vision-language diffusion models. We select detailed image captioning as the representative task to validate our framework, although the methodology is generalizable to other generative tasks. Our model is built upon the existing LLaDA-V model, leveraging its foundational mask prediction capabilities while endowing it with the ability to refine. The training data comprises caption datasets from LLaVA1.5 (Liu et al., 2023), ShareGPT4V (Chen et al., 2023), and the ViCrit dataset (Wang et al., 2025b). When constructing hallucination revision data in Stage I, we leverage the existing hallucination 6 Table 2: Performance comparison of different inference steps on CapMas benchmark. Mask-pred training indicates training with the traditional mask-pred objective using identical datasets. Metrics Speed (token/step) Factuality 4 2 Coverage 4 CLAIR 4 2 1 8 8 1 1 LLada-V Mask-pred training ReDiff 65.54 74.53 76.74 66.20 73.57 76.81 63.40 69.23 75.85 44.47 46.38 67. 49.22 54.15 55.07 48.85 54.11 55.82 45.85 47.60 54.08 32.24 29.69 46.25 61.06 59.68 63.29 61.10 58.43 60. 60.69 59.66 60.87 64.97 67.79 65.14 dataset ViCrit, which contains pairs of correct and hallucinated descriptions. For Stage (foundational revision training), we use total of 260k image-text pairs, with random token replacement probability of 0.1 for creating syntactic chaos. For Stage II (online self-correction learning), we generate approximately 10k draft-refined caption pairs in each round. The drafts are generated with 128, 32, and 16 inference steps, and o4-mini serves as the expert model for revisions. The prompt for o4-mini is detailed in Appendix B. Our experiments revealed that single round of this online refinement training yielded the most significant improvements. Benchmarks and Evaluation Setup. We evaluate our model on three recent benchmarks for detailed image caption: CapMAS (Lee et al., 2024) uses three metrics evaluated by GPT-4o: CLAIR for overall caption quality, Coverage for the comprehensiveness of the description, and Factuality for the accuracy of the content. CapArena (Cheng et al., 2025) employs pairwise comparison methodology where the outputs of the test model are compared against those of three baseline models with GPT-4o. final score is calculated based on these win ratio. DetailCaps-4870 (Dong et al., 2024) uses the CAPTURE metric, which scores the generated caption by comparing its scene graph to that of the ground-truth description. We compare ReDiff against several vision-language diffusion models, including LLaDA-V, LaViDa, MMaDA, and FUDOKI. We also include results from some typical AR-based VLMs. At inference, the maximum generation length is 128. An inference process of 128 steps corresponds to speed of 1 token/step, while 32 steps correspond to 4 tokens/step. 4.2 MAIN RESULTS As shown in Table 1, our ReDiff achieves state-of-the-art results among all diffusion-based models across each metric. On CapMas, our models CLAIR score shows remarkable 11.2 point improvement over the LLaDA-V, reaching comparable level to InternVL-2.5. The Coverage and Factuality scores also increase by 5.85 and 2.23 points, respectively, indicating that our captions are not only richer in content but also more accurate. On CapArena, our model outperforms LLaDA-V by 25.67 points. Furthermore, we achieve CAPTURE score of 61.88, surpassing the powerful Qwen2.5-VL. These results demonstrate that our refining-enhanced diffusion method effectively improves fluency and mitigates hallucinations, leading to substantial enhancement in overall caption quality. In Tables 2 and 3, we compare models trained with the traditional mask-pred objective versus our refinement framework, using identical datasets and base model. Our model consistently outperforms the mask-trained baseline at every step count. Crucially, as the generation speed increases (i.e., fewer steps), our models performance degrades much more gracefully, demonstrating superior stability in parallel generation. For instance, on the CLAIR metric, as the speed increases from 1 token/step to 8 tokens/step, the mask-trained models score plummets from 74.53 to 46.38, whereas our models score only decreases from 76.74 to 67.44. Notably, our models performance at 4 tokens/step is higher than that of both LLaDA-V and the mask-trained baseline at 1 token/step. similar trend is observed for Coverage. The trend for Factuality is less pronounced, as the baselines score does not drop significantly at fewer steps. This is because the metric relies on extracting valid items for verification; as the baselines output becomes more chaotic, fewer items can be extracted, artificially stabilizing the correctness ratio. On both CapArena and CAPTURE, our model also demonstrates more robust parallel generation, with the CAPTURE score dropping by only 0.65 points when accelerating from 1 to 4 tokens/step. 4.3 ABLATION STUDIES Impact of Each Training Stage. In Table 4, we analyze the individual contributions of our two training stages. Both Stage (foundational revision) and Stage II (self-correction) independently 7 Table 3: Performance comparison of different inference steps on CapArena and CAPTURE metrics. Metrics Speed (token/step) CapArena-Auto 1 2 4 8 LLada-V mask training ReDiff -77.17 -56.00 -51.50 -84.00 -70.50 -56.83 -90.50 -90.33 -72.67 -99.00 -98.33 -91.67 1 59.62 59.98 61. CAPTURE 4 2 59.04 59.61 61.91 57.12 56.99 61.23 8 45.11 45.12 56.80 Table 4: Effect of each training stage in the refining-enhanced diffusion paradigm. Coverage Metrics CapArena-Auto 4 1 Speed (token/step) Factuality 4 1 CLAIR 4 1 1 LLada-V Base + Stage Base + Stage II Stage + Stage II 65.54 71.31 73.02 76.74 63.40 71.67 73.52 75.85 49.22 51.73 53.44 55.07 45.85 51.83 53.00 54.08 61.06 58.04 59.49 63. 60.69 55.22 57.40 60.87 -77.17 -69.17 -68.00 -51.50 -90.50 -73.17 -77.67 -72.67 improve the models performance and stability over the LLaDA-V baseline. Furthermore, the most significant gains are achieved when both stages are combined. Notably, Stage II alone provides more substantial boost than Stage I, confirming that teaching the model to learn from its own intrinsic errors is highly effective refinement strategy. After Stage training, the model exhibits stable parallel generation performance. For example, as the speed increases from 1 to 4 tokens/step, CLAIR improves from 71.31 to 71.67, and CapArena changes from -69.17 to -73.17. The combination of the two stages yields synergistic effect, with Stage providing foundational revision ability that is further amplified by Stage II, leading to large improvements in metrics like Factuality (+5.25) and CapArena (+17.67). Analysis of Foundational Revision Training. As shown in Table 5, we investigate different settings for the stage training. We find that revising syntactic errors primarily boosts overall quality (CLAIR) and Coverage, while also enhancing stability during parallel generation. Conversely, training on hallucination revision exhibits higher Factuality. Combining both error types allows our model to achieve the best overall performance. We also compare dynamic probability for random token replacement in the fourth line, where the dynamic rate is correlated with the noise level (using as replacement probability, when < 0.1). The results indicate that our fixed replacement rate yields better overall performance. Impact of Online Self-Correction Training Rounds. In Table 6, we examine the effect of iteration of the stage II training. The results show that while the first round of self-correction provides substantial performance boost over the ReDiff-Base model, subsequent rounds of training on newly generated data do not yield further significant improvements across most metrics. 4.4 QUALITATIVE ANALYSIS We provide qualitative examples to visually demonstrate how the refinement during inference produces more accurate and fluent results, thereby improving the stability of parallel generation. In Figure 3, we compare the parallel-generated captions from ReDiff and LLaDA-V. The baselines output suffers from token repetition (bus, the), grammatical errors, and hallucinations (e.g., misidentifying person on bus advertisement as woman). In contrast, our models output is Table 5: Effect of different settings in the foundational revision training stage. Metrics Speed (token/step) CLAIR 1 4 Coverage 4 1 Factuality 4 1 CapArena-Auto 4 LLada-V Revise hallucination Revise syntactic errors Dynamic revise ratio Ours (ReDiff-Base) 65.54 69.33 69.48 68.26 71.31 63.40 67.01 70.30 67.98 71.67 49.22 51.08 52.12 50.49 51.73 45.85 46.61 49.96 48.66 51.83 61.06 59.46 56.57 59.23 58.04 60.69 57.06 56.15 56.60 55.22 -77.17 -74.33 -69.67 -74.83 -69.17 -90.50 -87.67 -88.83 -82.50 -73.17 Table 6: Effect of online self-correction learning rounds. Metrics Speed (token/step) CLAIR 1 4 Coverage 4 1 Factuality 4 CapArena-Auto 1 4 ReDiff-Base Online training round 1 Online training round 2 71.31 76.74 76.10 71.67 75.85 74. 51.73 55.07 55.20 51.83 54.08 54.08 58.04 63.29 62.24 55.22 60.87 60.46 -69.17 -51.50 -56.17 -73.17 -72.67 -72. Figure 3: Cases comparison between LLaDA-V and our ReDiff under 4 tokens/step inference speed. ReDiff demonstrates superior fluency and accuracy in its generated captions. Figure 4: Refinement process of ReDiff at different inference step. Red tokens indicate the errors produced during generation, while green tokens mean the corresponding refined results. fluent, coherent, and factually grounded. In the second example, our model accurately describes all key elements in the scene, whereas the baselines output is chaotic and omits significant details. More comparison cases can be found in Appendix A. Figure 4 visualizes the token-level changes during 32-step generation process. It clearly shows the model simultaneously unmasking new tokens and refining previously generated ones. For instance, in the first example, the model refines the erroneous phrase rocks painted rocks into colorful painted rocks at step 20. At step 28, it corrects tall green plant to small green plant to better match the visual content. Figure 5 showcases comparison of inference with and without the refinement, showing that the refinement is critical for achieving high-quality outputs. If ReDiff inferences without the refinement, Figure 5: Generation results of ReDiff with or w/o refinement during inference. Figure 6: ReDiff can revise wrong input answers. errors tend to accumulate, such as repeated words or symbols and incoherent sentences, ultimately degrading the quality of the caption. This highlights the importance of the models refinement capability. Beyond correcting the models own errors during generation, ReDiff also demonstrates powerful, generalizable ability to revise disturbing inputs. As shown in Figure 6, we provide the model with an image and user-provided caption containing either syntactic chaos or factual hallucination. In both cases, our model successfully corrects the initial erroneous text and proceeds to generate coherent and accurate completion, highlighting the strong revision ability of ReDiff."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we addressed the critical challenge of error cascades that hampers the performance of vision-language diffusion models, particularly in efficient parallel generation scenarios. We proposed paradigm shift from passive denoising to active refining by introducing ReDiff, novel framework centered on mistake-driven, online self-correction loop. This approach teaches the model to learn from its own characteristic errors, endowing it with the ability to revisit and refine its generated output. Our extensive experiments validate that this method not only achieves state-ofthe-art performance but, more importantly, demonstrates far superior stability and factual accuracy in challenging few-step generation regimes where traditional denoising models catastrophically fail. By effectively breaking the error cascade, our work presents promising path toward developing more robust, efficient, and controllable generative systems."
        },
        {
            "title": "REFERENCES",
            "content": "Marianne Arriola, Aaron Gokaslan, Justin T. Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In ICLR. OpenReview.net, 2025. Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In NeurIPS, pp. 1798117993, 2021. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966, 2023. Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng arXiv preprint large language models: survey. Shou. Hallucination of multimodal arXiv:2404.18930, 2024. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. CoRR, abs/2311.12793, 2023. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In NeurIPS, 2024. Kanzhi Cheng, Wenpo Song, Jiaxin Fan, Zheng Ma, Qiushi Sun, Fangzhi Xu, Chenyang Yan, Nuo Chen, Jianbing Zhang, and Jiajun Chen. Caparena: Benchmarking and analyzing detailed image captioning in the llm era. arXiv preprint arXiv:2503.12329, 2025. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. In NeurIPS, 2023. Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang, Yuan Zhang, and Haoyuan Guo. Benchmarking and improving detail image caption. arXiv preprint arXiv:2405.19092, 2024. Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, and Guo-Jun Qi. Reinforcing the diffusion chain of lateral thought with diffusion language models. CoRR, abs/2505.10446, 2025. Yatai Ji, Rongcheng Tu, Jie Jiang, Weijie Kong, Chengfei Cai, Wenzhe Zhao, Hongfa Wang, Yujiu Yang, and Wei Liu. Seeing what you miss: Vision-language pre-training with semantic completion learning. In CVPR, pp. 67896798. IEEE, 2023. Yatai Ji, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, Xuefeng Xiao, Sidi Yang, Yujiu Yang, IDA-VLM: towards movie understanding via id-aware large vision-language and Ping Luo. model. In ICLR. OpenReview.net, 2025. Saehyung Lee, Seunghyun Yoon, Trung Bui, Jing Shi, and Sungroh Yoon. Toward robust hyperdetailed image captioning: multiagent approach and dual evaluation metrics for factuality and coverage. CoRR, abs/2412.15484, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326, 2024. 11 Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. Lavida: large diffusion language model for multimodal understanding. CoRR, abs/2505.16839, 2025a. Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding. In CVPR, pp. 27792790. Computer Vision Foundation / IEEE, 2025b. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In ICML. OpenReview.net, 2024. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai arXiv preprint Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv:2502.09992, 2025. OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524, 2024. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, Ge Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Yonghui Wu, and Hao Zhou. Seed diffusion: large-scale diffusion language model with high-speed inference. CoRR, abs/2508.02193, 2025. Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time In The Eleventh International Conference on Learning Representadiscrete diffusion models. tions, 2023. Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, and Katerina Fragkiadaki. Unified multimodal discrete diffusion. CoRR, abs/2503.20853, 2025. Qwen Team. Qwen3: Think deeper, act faster. 2025. URL https://qwenlm.github.io/ blog/qwen3/. https://qwenlm.github.io/blog/qwen3/. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. vicuna. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. https://vicuna.lmsys.org/, 2023. 12 Jin Wang, Yao Lai, Aoxue Li, Shifeng Zhang, Jiacheng Sun, Ning Kang, Chengyue Wu, Zhenguo Li, and Ping Luo. FUDOKI: discrete flow-based unified understanding and generation via kineticoptimal velocities. CoRR, abs/2505.20147, 2025a. Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, Kevin Lin, Linjie Li, Furong Huang, and Lijuan Wang. Vicrit: verifiable reinforcement learning proxy task for visual perception in vlms. CoRR, abs/2506.10128, 2025b. Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025c. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion LLM by enabling KV cache and parallel decoding. CoRR, abs/2505.22618, 2025. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. CoRR, abs/2505.15809, 2025. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. CoRR, abs/2505.16933, 2025. Runpeng Yu, Xinyin Ma, and Xinchao Wang. Dimple: Discrete diffusion multimodal large language model with parallel decoding. CoRR, abs/2505.16990, 2025. Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. CoRR, abs/2307.03601, 2023."
        },
        {
            "title": "A MORE VISUALIZATION",
            "content": "Figure 7 and Figure 8 show the generation results of ReDiff and LLaDA-V under different inference steps. In the 2 tokens/step scenario, LLaDA-V outputs great deal of hallucinated content, such as Goku and Vegeta in the first case, and mouse and keyboard in the second. This occurs because an initial hallucination can affect subsequent generation, leading to error cascades. In contrast, our ReDiff method produces captions that are consistent with the image content. In the cases of 8 tokens/step, the results of our model are more fluent and have less grammer errors. Figure 7: Cases comparison between LLaDA-V and our ReDiff under 2 tokens/step inference speed. Figure 8: Cases comparison between LLaDA-V and our ReDiff under 8 tokens/step inference speed."
        },
        {
            "title": "B PROMPT FOR STAGE II DATA CONSTRUCTION",
            "content": "In online self-correction learning loop, we use ReDiff-Base to generate caption drafts. Then the image, the draft, and ground truth caption are fed to o4-mini to detect and revise errors. The prompt for o4-mini is shown in Table 7."
        },
        {
            "title": "C ETHICS STATEMENT",
            "content": "All datasets and models used in this study are publicly available and open-source. No proprietary, private, or personally identifiable information was collected or used. The images employed are either natural scenes or normal human activities, without any violent, explicit, or otherwise harmful content. Therefore, the research meets relevant considerations regarding privacy, ethics and copyright. 14 Table 7: The prompt for o4-mini to revise models drafts. # ROLE: Hallucination detect and revise Assistant ## PERSONA: You are an AI assistant specialized in hallucination revision. You integrate information from image, question and ground truth answer, to analyze and judge whether the prediction from other models is right or not. If the prediction is wrong, you need to revise the hallucination and errors in the prediction. ## INPUT CONTEXT: You will receive the following: 1. **Image:** An image. 2. **Question:** users question about the image. 3. **Answer:** The right answer to the question. 4. **Prediction** The answer from our model. ## TASK: Your primary task is to judge if the prediction is right according to the image and ground truth answer. If the prediction is not right, detect hallucination and wrong parts, then revise them. * The prediction must be consistent to the image, detect all hallucination and errors. * For the words containing hallucination, you need to replace them with right words, which have same token number with original prediction. Make the original prediction correct with as few modifications as possible. * The answer may contain some chaos in grammar expression, such as repetition, incoherence, etc. You should also replace erroneous parts with fragments of identical token counts. ## GUIDELINES & CONSTRAINTS: 1. The prediction doesnt have to be identical to the reference answer; as long as it correctly answers the question, its acceptable. The GT (ground truth) serves merely as reference. Focus primarily on checking whether there are hallucination issues in the prediction that contradict the image content. 2. If the prediction is correct, output only right. 3. If the prediction contains hallucinations or errors, output JSON-formatted string containing multiple pairs of phrases. Each pair should consist of the original erroneous phrase segment and its corrected counterpart. 4. Modifications should be localized to the minimal necessary extent, typically targeting short multi-word segments. 5. For each pair, ensure the tokenized length of the original and modified segments remains identical. The semantics of replacement words must be inconsistent to the original segment. 6. The original segment should be unique within the prediction to facilitate error localization by users. ## OUTPUT FORMAT: 1. If the prediction is right, output only right. 2. If the prediction has errors, provide the output as single JSON object, which is list containing multiple dictionaries with the following keys: * org: (String) The hallucination or error segment in original prediction. * target: (String) The right segment to replace the wrong part in prediction. ## EXAMPLE: * **Image:** [Description: man in front of white trunk.] * **Question:** What might the man in the suit be doing? * **Answer:** The man dressed in business attire leaning on the white truck could be associated with the business related to the truck ... * **Prediction:**The man is leaning on pink trunk, and ... * **Expected Output:** json [ org: pink truck, target: white trunk ]"
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "The University of Hong Kong"
    ]
}