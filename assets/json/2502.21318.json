{
    "paper_title": "How far can we go with ImageNet for Text-to-Image generation?",
    "authors": [
        "L. Degeorge",
        "A. Ghosh",
        "N. Dufour",
        "D. Picard",
        "V. Kalogeiton"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent text-to-image (T2I) generation models have achieved remarkable results by training on billion-scale datasets, following a `bigger is better' paradigm that prioritizes data quantity over quality. We challenge this established paradigm by demonstrating that strategic data augmentation of small, well-curated datasets can match or outperform models trained on massive web-scraped collections. Using only ImageNet enhanced with well-designed text and image augmentations, we achieve a +2 overall score over SD-XL on GenEval and +5 on DPGBench while using just 1/10th the parameters and 1/1000th the training images. Our results suggest that strategic data augmentation, rather than massive datasets, could offer a more sustainable path forward for T2I generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 1 3 1 2 . 2 0 5 2 : r How far can we go with ImageNet for Text-to-Image generation? Lucas Degeorge * 1 2 3 Arijit Ghosh * 3 Nicolas Dufour 1 3 David Picard 3 Vicky Kalogeiton 1 1 LIX, Ecole Polytechnique, CNRS, IP Paris, France 2 AMIAD, Pole recherche 3 LIGM, Ecole Nationale des Ponts et Chaussees, IP Paris, Univ Gustave Eiffel, CNRS, France 0.7 0.6 0.5 Ours SDXL CAD Pixart-α SD2.1 106 0.4 108 109 1010 Train Dataset Size (log. scale) 0.8 Ours 0.7 0.6 SDXL Pixart-α SD 1.5 107 108 109 1010 Train Dataset Size (log. scale) o l e a e r l e r c G Figure 1. Results of our proposed text-to-image (T2I) generation method when trained solely on ImageNet. Left: Images generated with our 300M T2I model (CAD-I architecture) show good text understanding even for out-of-distribution prompts (e.g.: the pink elephant or the neon turtle). Right: Quantitative results on GenEval (top) and DPGBench (bottom). The size of the bubble represents the number of parameters. In both cases, we outperform models of 10 the parameters and models trained on 1000 the number of images."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Recent text-to-image (T2I) generation models have achieved remarkable results by training on billion-scale datasets, following bigger is better paradigm that prioritizes data quantity over quality. We challenge this established paradigm by demonstrating that strategic data augmentation of small, well-curated datasets can match or outperform models trained on massive webscraped collections. Using only ImageNet enhanced with well-designed text and image augmentations, we achieve +2 overall score over SD-XL on GenEval and +5 on DPGBench while using just 1/10th the parameters and 1/1000th the training images. Our results suggest that strategic data augmentation, rather than massive datasets, could offer more sustainable path forward for T2I generation. Code, models and dataset available here. The prevailing wisdom in text-to-image (T2I) generation holds that larger training datasets inevitably lead to better performance. This bigger is better paradigm has driven the field to billion-scale image-text paired datasets like LAION-5B (Schuhmann et al., 2022), DataComp12.8B (Gadre et al., 2023) or ALIGN-6.6B (Pham et al., 2023). While this massive scale is often justified as necessary to capture the full text-image distribution, in this work, we challenge this assumption and argue that data quantity overlooks fundamental questions of data efficiency and quality in model training. Our critique of the data-scaling paradigm comes from critical observations. While the community acknowledges dataset quality issues (Birhane et al., 2023) and employs extensive curation pipelines (Radenovic et al., 2023; Abbas et al., 2023), this approach still carries three fundamental flaws: first, the default paradigm still consists of collecting and curating massive web-scraped datasets, thus 1 How far can we go with ImageNet for Text-to-Image generation? leading to notable computational cost (Schuhmann et al., 2022). Second, current curation processes fail to eliminate societal biases, inappropriate content, copyrighted material, and privacy concerns, which ultimately manifest directly in the trained models. This raises serious questions about the underlying distributions learned by these models (Luccioni et al., 2024; Birhane et al., 2024). Third, the problem becomes particularly crucial for specialized applications, where creating aligned text-image pairs is prohibitively time and resource-intensive. These dataset challenges are particularly concerning given the remarkable progress of T2I models. Recent diffusionbased approaches, popularized by Rombach et al. (2022); Saharia et al. (2022); Ramesh et al. (2022), excel in generating high-fidelity images with photorealistic details, artistic sophistication, and complex compositional understanding (Chen et al., 2023; Betker et al., 2023; Podell et al., 2023). The field has rapidly evolved through several advances in architectures like transformed-based DiT (Peebles & Xie, 2023) or PixArt-α (Chen et al., 2023) and scaled-up models like Stable Diffusion (SD) (Podell et al., 2023); yet, these are all coupled with billion-scale datasets for training. Instead of tackling core data quality issues, the communitys response has been: to collect more data. This brute-force approach only magnifies the challenges of computational costs, curation complexity, and dataset bias. We propose radical shift: training text-to-image generation models with smaller, carefully curated datasets enhanced through strategic data augmentation. We leverage ImageNet (Russakovsky et al., 2015), well-known dataset whose biases and limitations are thoroughly studied. While ImageNet alone has never been used for T2I diffusion models due to its simple labels and object-centric nature, we overcome these limitations through two key contributions: (1) applying text augmentations by generating rich, descriptive captions with LLaVA (Liu et al., 2024b) to capture full scene complexity, and (2) applying image augmentations using CutMix (Yun et al., 2019) to create novel concept combinations absent from the original dataset. Using only ImageNet, we train several small diffusion models with different architectures (DiT (Peebles & Xie, 2023) and RiN (Jabri et al., 2023; Dufour et al., 2024a)). As shown in Figure 1, our approach achieves +2 point improvement on the GenEval benchmark and +5 point improvement on the DPGBench over Stable Diffusion-XL (Podell et al., 2023), despite using only 1/10-th the parameters and 1/1000-th the training images. This demonstrates that strategic data augmentation can match or exceed the performance of models trained on massive datasets while reducing computational costs. In summary, our contributions are: Albeit popular beliefs (Gokaslan et al., 2024), we show that high-quality T2I models can be trained on just 1.2M augmented image-text pairs, challenging the necessity of billion-scale datasets. We highlight the importance of simple data diversification techniques both in pixel space (via CutMix augmentation) and text space (via rich, long captioning). Our method results in state-of-the-art performances on GenEval (Ghosh et al., 2024) and DPGBench (Hu et al., 2024) for models with less than 1B parameters, with orders-of-magnitude reduction in training data (x1000). 2. Related Work Diffusion Models (Song et al., 2020b; Ho et al., 2020; Sohl-Dickstein et al., 2015) have demonstrated remarkable success across various domains (Huang et al., 2023; Courant et al., 2025; Dufour et al., 2024b). While image generation remains their most prominent application (Dhariwal & Nichol, 2021; Song et al., 2020b; Karras et al., 2022), text-to-image (T2I) synthesis (Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022) has emerged These models as particularly impactful use case. operate by learning to reverse gradual Gaussian noise corruption process. At extreme noise levels, the model effectively samples from standard normal distribution to produce realistic images. The core optimization objective is: min θ E(x0,c)pdata,ϵN (0,1) ϵ ϵθ(xt, c, t)2(cid:105) (cid:104) (1) where xt=(cid:112)γ(t)x0+(cid:112)1 γ(t)ϵ denotes the noised image at timestep t, x0 the original image, the corresponding condition (such as text), ϵ is standard normal noise, ϵθ the learned noise predictor, and γ(t) the variance schedule. Computational Efficiency Traditional diffusion models require substantial computational resources, with leading implementations consuming hundreds of thousands of GPU hours (Rombach et al., 2022). Recent advances have significantly improved training efficiency. (Wei et al., 2023; Yu et al., 2024) identified limitations in the diffusion losss representation learning capabilities, demonstrating that supplementary representation losses accelerate convergence. (Chen et al., 2023) achieved dramatic compute reduction by repurposing class-conditional models for text-to-image generation. (Dufour et al., 2024a) introduced architectural improvements and coherence-aware mechanisms, matching Stable Diffusions performance (Rombach et al., 2022) with 100x fewer GPU hours. Data Efficiency Early T2I models relied on billion-scale web-scraped datasets (Rombach et al., 2022), creating accessibility barriers due to storage requirements and reproducibility challenges from copyright restrictions. (Chen 2 How far can we go with ImageNet for Text-to-Image generation? et al., 2023) pioneered dataset reduction using 20M highquality images from recaptioned SAM data (Kirillov et al., 2023), though portions remain proprietary. Subsequent work explored CC12M (Changpinyo et al., 2021) and YFCC100Ms public subset (Thomee et al., 2016; Gokaslan et al., 2024), revealing overfitting below 10M samples. Our approach diverges by leveraging ImageNet (Russakovsky et al., 2015) reproducible, well-established benchmark with standardized metrics (Heusel et al., 2017). We transform this classification dataset into T2I training data through synthetic captions and image augmentations. Copy-Paste augmentation provides an object-aware data augmentation method by extracting source image objects and pasting them onto target images (Ghiasi et al., 2021). Prior work (Dvornik et al., 2018; Dwibedi et al., 2017; Fang et al., 2019; Ghiasi et al., 2021) demonstrated its effectiveness for instance segmentation and object detection, with some approaches focusing on paste location optimization (Dvornik et al., 2018; Fang et al., 2019; Ge et al., 2024) while others showing random placement suffices (Ghiasi et al., 2021). The merging of objects serves as regularization technique (Yun et al., 2019), which recent methods like UCC (Fan et al., 2022) and NeMo (Ha et al., 2024) build upon. Learning paradigms leverage contrastive learning (Wang et al., 2022) and teacher-student networks (Bai et al., 2023). While existing approaches use StableDiffusion to generate paste elements (Zhao et al., 2023; Lin et al., 2023; Ge et al., 2023; Rombach et al., 2022), our work instead leverages mixing training images to train T2I models. Synthetic captions Synthetic image captioning has benefited several tasks. For instance, visual question answering (Sharifzadeh et al., 2024) and visual representation learning (Tian et al., 2023) achieve state-of-the-art performances by enhancing the captioning output of Vision-Language Models (VLMs) (Lai et al., 2024; Sharifzadeh et al., 2024). Similarly, training with synthetic captions for text-to-image generation is becoming the defacto protocol for large diffusion models, such as DALL-E (Betker et al., 2023), Pixartα (Chen et al., 2023) and Stable Diffusion-3 (Esser et al., 2024). More recently, some approaches (Liu et al., 2024a; Li et al., 2024) extend this approach by training text-to-image (T2I) models on multi-level captions. Inspired by these, our method deploys the popular LLaVA captioner (Liu et al., 2024b) to augment existing textual captions and use them to train text-to-image generation models. 3. Method for text-pixel diversification We present systematic approach to train text-to-image diffusion models using ImageNet, demonstrating that strategic data augmentation can match the performance of models trained on billion-scale datasets. Our method exploits two 3 LLaVA Long captions CutMix e m i C e I a m N t i C LLaVA Long captions Long captions > τ and > ρ Combined Batch Denoising Model d e i Data Curation Model Training Figure 2. Pipeline of our Data Curation and Training process. Starting from ImageNet, we a) use LLaVa VLM to caption the images into long detailed caption (top branch left) and b) use several CutMix strategies to create new images combining several ImageNet concepts and caption them using LLaVa into long and detailed captions (bottom branch left). During training, we sample batches of normal and CutMix images and we select from each batch depending on the timestep at which the CutMix strategy is valid and probability of sampling CutMix images. key characteristics of ImageNet: its high curation quality and its object-centric nature, where main subjects are consistently positioned centrally. While ImageNet lacks the edge cases and concept combinations found in web-scraped datasets (e.g., abstract art, digital renderings, unusual object combinations), we show this limitation can be systematically addressed through structured augmentations. Our framework operates along two complementary axes: Text-space augmentation, which converts ImageNets class labels into semantically rich scene descriptions (Section 3.1). Pixel-space augmentation, where we introduce novel concept combinations through geometrically-controlled image mixing without compromising image coherence (Section 3.2). Each component is designed to preserve ImageNets inherent quality while addressing specific limitations. Figure 2 illustrates our complete pipeline. Additionally, Section 3.3 reports our training procedure with image augmentations. 3.1. Improvement in text diversity ImageNet is class-conditional dataset, initially used for classification and object-detection tasks. To overcome ImageNets limited class-conditional annotations, we implement two-stage text enrichment pipeline: Baseline Captioning (AIO). We establish baseline using the standard An image of <class-name> (AIO) format following (Radford et al., 2021). Limitations. The two main limitations of AIO captions for ImageNet are: First, AIO captions lack detailed descripHow far can we go with ImageNet for Text-to-Image generation? LLaVA w/o CM CM1/2 CM1/4 CM1/9 CM1/16 delicate white butterfly [...]. The flower, stunning shade of purple, [...]. The butterfly, positioned slightly to the left of the flowers center, [...]. On the left side, person is playing the trumpet on street. [...]. On the right side of the image, there are two penguins standing [...]. [...] silver sports car in the background. [...] Golden Retriever, is on the left side of the frame [...]. The sports car, positioned on the right, [...] [...] palace or manor house, [...] In front of the building is well-maintained garden [...] In the sky, there is single hot air balloon [...]. [...] husky dog resting in the snow. [...] Next to the dogs side, there is wine glass with red wine and few purple flowers [...] Figure 3. Long synthetic captions for (left) original and (right) CutMix pixed-augmented images. All captions, as generated by LLaVa, are highly diverse and add intricate details of compositionality, colors as well as concepts, which are not present in the original ImageNet dataset. For ease of visualization the captions are trimmed; full captions in Appendix B. tions, which constrains the diversity in the text-condition space. For example, caption an image of golden retriever mentions the class name but leaves out details and concepts. Second, ImageNet lacking any person class results in humans not being represented in the AIO text space. Long captions. We employ LLaVA (Liu et al., 2024b) to generate comprehensive captions that capture: (i) Scene composition and spatial relationships; (ii) Background elements and environmental context; (iii) Secondary objects and participants; (iv) Visual attributes (color, size, texture); and (v) Actions and interactions between elements. This enhancement addresses critical gaps in ImageNets annotations, particularly for images containing humans or multiple interacting elements that the original class labels miss entirely. Figure 3 shows examples of the richer captions generated by LLaVA. 3.2. Improvement in pixel space diversity For image augmentation, we introduce structured CutMix framework that systematically combines concepts while preserving object centrality. Our framework defines four precise augmentation patterns, each designed to maintain visual coherence while introducing novel concept combinations. These are briefly described below: 1. CM1/2 (Half-Mix): Scale: Both images maintain their original resolution. Position: Deterministic split along height or width. Coverage: Each concept occupies 50% of final image. Preservation: Both concepts maintain full resolution. 2. CM1/4 (Quarter-Mix): Scale: CutMix image resized to 50% side length. Position: Fixed placement at one of four corners. Coverage: 2nd concept occupies 25% of final image. Preservation: Base image center region remains intact. 3. CM1/9 (Ninth-Mix): Scale: CutMix image resized to 33.3% side length. Position: Fixed placement along image borders. Coverage: 2nd concept occupies 11.1% of final image. Preservation: Base image center, corners remain intact. 4. CM1/16 (Sixteenth-Mix): Scale: CutMix image resized to 25% side length. Position: Random placement not central 10% region. Coverage: 2nd concept occupies 6.25% of final image. Preservation: Base image center region remains intact. Each augmentation strategy generates 1,281,167 samples, matching ImageNets training set size. Figure 3 shows examples of the different structured augmentations. We also define CMall, which uniformly samples from all four patterns. The CMall variant combines equal proportions (25%) from each pattern to maintain the same total sample count. Post-augmentation, we apply LLaVA captioning to all generated images, ensuring semantic alignment between visual and textual representations. This produces detailed descriptions that accurately reflect the augmented content while maintaining natural language fluency. 3.3. Training with image augmentations Because our image augmentations have strong artifacts corresponding to the boundaries of the mixing, we have to prevent the model from learning those salient features and reproducing them. To that end, we propose to train on image augmentation only at timesteps where the noisy image xt is sufficiently noisy that the artifacts no longer matter. In practice, this corresponds to sampling either from the original image training set or from the augmented image training set AIA conditionally to t, compared to an additional hyperparameter τ deciding whether is sufficiently large for image augmentation. This extra condition leads to 4 How far can we go with ImageNet for Text-to-Image generation? Algorithm 1 Batch with image augmentation 4. Experiments Input: dataset A, AIA, augmentation time τ , augmentation probability batch size m, = {} for = 1 to do U(0, ) (x0, c) if > τ then ρ Bp if then (x0, c) AIA end if end if ϵ (0, 1) xt = (cid:112)γ(t)x0 + (cid:112)1 γ(t) = (xt, c, t) end for Return: replacing the original diffusion loss in Equation (1) with (cid:2)ϵ ϵθ(xt, c, t)2(cid:3) . (2) min θ tU (0,T ), ρB(τ,p)(t), (x,c)A(ρ), ϵN (0,1) In this novel loss, the timestep U(0, ) is still sampled uniformly. We introduce new random variable ρ that is sampled conditionally to t, where Bτ,p(t) denotes specific distribution that corresponds to: Bτ,p(t) = (cid:40) 0, if τ, Bp, else. (3) Here, Bp Bernoulli distribution of parameter p. The textimage pair (x0, c) is then sampled conditionally to ρ, where A(ρ) is distribution that uniformly samples from the original or the augmented datasets depending on ρ: A(ρ) = (cid:40) A, if ρ = 0, AIA, else. (4) The noise ϵ is sampled from the Normal distribution, as in the usual diffusion equation. Similarly, the noisy image xt is obtained by xt=(cid:112)γ(t)x0 + (cid:112)1 γ(t). This novel loss function is more involved than the regular diffusion training; yet, in practice, it is very easy to implement and can be done entirely during the mini-batch construction as described in Algorithm 1. 4.1. Experimental Setup Datasets. For training, we use the ImageNet dataset (Russakovsky et al., 2015). Each image is rescaled to resolution of 256 256. We adopt the framework of latent diffusion (Rombach et al., 2022), using pre-trained variational auto-encoder provided by (Rombach et al., 2022). We encode the text condition using the 5 text encoder model (Chung et al., 2024). We precompute both the image latent and the text embeddings for our dataset to make training efficient. We use two architectures for our experiments: DiT-I (our adaptation of DiT (Peebles & Xie, 2023) to handle text) and CAD-I (Dufour et al., 2024a). The suffix is added to indicate the model being trained only on ImageNet. Similar to prior works, for both the models, we maintain an expected moving average (EMA) model (Song et al., 2020b) of the online models. The results described in the following subsections are based on the EMA models. Details about the architecture can be found in Appendix A. The threshold τ that enables image augmentations (Section 3.3) is empirically chosen (see Appendix C). The ablation of the probability is discussed in Section 4.3.2. We always train with batch size of 1024. Evaluation We evaluate all our models using 250 steps of DDIM (Song et al., 2020a) using the following metrics: (1) FID We use the Frechet Inception Distance (FID) (Heusel et al., 2017) to evaluate the image quality w.r.t. two datasets: the 50k in-distribution ImageNet validation set and the 30k out-of-distribution MSCOCO captions validation set (Lin et al., 2014). For FID calculation, in addition to the standard Inception-v3 backbone (Szegedy et al., 2016), we employ the Dinov2 backbone (Oquab et al., 2024) to assess image quality. This dual approach ensures robust evaluation across both in-distribution and out-of-distribution datasets. (2) P,R,D,C To further evaluate the fidelity and diversity of our generated samples, we adopt the combined Precision and Recall (Kynkaanniemi et al., 2019), Density and Coverage (Naeem et al., 2020) metrics. All of them were calculated using Dinov2 backbone. (3) CLIPScore (CS) We also evaluate the alignment of text prompts with the generated images using CLIPScore (Hessel et al., 2021). As CLIPScore truncates the text input sequence at 77 tokens, we also evaluate the models using an updated version of CLIPScore using embeddings from jina-clip-v2 (Koukounas et al., 2024); this successfully accommodates text-image alignment evaluation on long text prompts. How far can we go with ImageNet for Text-to-Image generation? Model TA IA COCO 30k ImageNet Val 50k FID Inc. FID DINOv2 R C CS Jina-CS FID Inc. FID DINOv2 R C CS Jina-CS DiT-I CAD-I 252.22 34.19 36.46 46.35 46.93 49.41 2372.27 664.20 656. 858.43 655.37 646.51 0.24 0.57 0.57 0.52 0.66 0.66 0.05 0.35 0.36 0.18 0.42 0.41 0.15 0.46 0. 0.45 0.61 0.57 0.02 0.23 0.24 0.15 0.28 0.29 13.16 24.44 24.85 12.89 26.37 26.60 8.64 33.94 34. 14.06 35.72 36.51 23.13 8.22 8.52 84.77 6.16 6.62 354.84 112.55 114.54 904.50 91.53 91.72 0.66 0.75 0. 0.75 0.80 0.80 0.17 0.73 0.73 0.05 0.72 0.70 0.62 0.78 0.78 1.40 0.89 0.90 0.41 0.68 0. 0.10 0.76 0.76 25.71 9.06 9.22 8.94 8.69 8.53 32.93 38.77 38.75 20.55 38.01 38.17 Table 1. Image quality metrics for DiT-I L/2 and CAD-I models. Models are trained for 250k steps. Model SD v1.5 SD v2.1 PixArt-α SDXL SD3 SD v1.5 PixArt-α CAD SD3 DiT-I CAD-I Nb of params Training set size TA IA Overall One obj. Two obj. Count. Col. Pos. Col. attr. 0.9B 0.9B 0.6B 3.5B 2B 0.9B 0.6B 0.4B 2B 0.4B 0.4B 0.4B 0.3B 0.3B 0.3B 5B+ 5B+ 0.025B 5B+ 1B+ 5B+ 0.025B 0.020B 1B+ 0.001B 0.001B 0.001B 0.001B 0.001B 0.001B 0.43 0.50 0.48 0.55 0.62 0.13 0.48 0.50 0.49 0.07 0.25 0. 0.17 0.51 0.55 0.38 0.47 0.49 0.52 - 0.17 0.49 0.49 0.56 0.00 0.55 0.57 0.04 0.55 0.57 0.97 0.98 0.98 0.98 0. 0.48 0.96 0.95 0.88 0.33 0.71 0.65 0.82 0.95 0.94 0.97 0.98 0.99 0.99 - 0.63 0.99 0.92 0.95 0.00 0.95 0. 0.20 0.97 0.94 0.38 0.51 0.50 0.74 0.74 0.04 0.51 0.56 0.62 0.00 0.16 0.10 0.04 0.56 0.64 0.25 0.45 0.53 0.60 - 0.04 0.51 0.55 0.73 0.00 0.61 0.68 0.01 0.60 0.68 0.35 0.44 0.44 0.39 0.63 0.01 0.48 0.40 0.28 0.00 0.17 0. 0.03 0.37 0.38 0.32 0.46 0.46 0.43 - 0.04 0.47 0.36 0.28 0.00 0.36 0.36 0.00 0.42 0.40 0.76 0.85 0.80 0.85 0. 0.23 0.78 0.76 0.64 0.06 0.39 0.44 0.13 0.79 0.77 0.64 0.73 0.76 0.86 - 0.27 0.76 0.72 0.75 0.00 0.80 0. 0.02 0.74 0.70 0.04 0.07 0.08 0.15 0.34 0.00 0.07 0.11 0.22 0.00 0.05 0.03 0.25 0.14 0.21 0.04 0.10 0.09 0.11 - 0.01 0.09 0.20 0.32 0.00 0.28 0.28 0.00 0.26 0.35 0.06 0.17 0.07 0.23 0.36 0.00 0.08 0.22 0.31 0.00 0.04 0. 0.25 0.29 0.37 0.05 0.10 0.11 0.15 - 0.00 0.11 0.21 0.32 0.00 0.33 0.39 0.00 0.35 0.36 Table 2. Results on GenEval. Models are evaluated at 2562 resolution. means original GenEval prompts. means extended GenEval prompts. Gray color represents results reported with native resolution of 5122 or above. Bold indicates best, underline second best. Model Params Training set size Global Entity Attribute Relation Other Overall SDv1.5 Pixart-α CAD SDXL SD3-Medium Janus DiT-I (Ours) CAD-I (Ours) 0.9B 0.6B 0.4B 3.5B 2B 1.3B 0.4B 0.3B 5B+ 25M 20M 5B+ 1B+ 1B+ 1.2M 1.2M 74.63 74.97 84.50 83.27 87.90 82.33 76.29 80. 74.23 79.32 85.25 82.43 91.01 87.38 84.77 87.48 75.39 78.60 84.66 80.91 88.83 87.70 83.34 85.32 73.49 82.57 91.53 86.76 80.70 85.46 92.22 93. 67.81 76.96 74.8 80.41 88.68 86.41 70.80 78.00 63.18 71.11 77.55 74.65 84.08 79.68 75.99 79.94 Table 3. Results on DPG-Bench. We compare our models to the results reported in (Wu et al., 2024). bold for best, second best. (4) GenEval and DPGBench Finally, to evaluate the compositionality of the models, we use the GenEval (Ghosh et al., 2024) and DPG (Hu et al., 2024) benchmarks. In addition to using the default short text prompts provided by GenEval, we artificially extend these prompts using Llama-3.1 to approximate the distribution of long prompts used during training. 4.2. Main results Here, we analyze the impact of our augmentation strategies on T2I generation. We use two models (DiT-I and CADI) and train them solely on ImageNet using our proposed augmentation strategies discussed in 3.1 and 3.2. Below we report the results of our models and compare them against the state of the art. Quantitative results: Comparison to the state of the art on GenEval and DPG benchmarks One of the main purposes of our training augmentation strategy is to improve the diversity of concept combinations in the training set. As such, we test the composition ability of both our DiT-I and CAD-I models on the GenEval and DPGBench benchmarks and compare our performances to the ones of popular stateof-the-art models. GenEval Table 2 reports the results with and without using extended prompts ( denotes scores with extended prompts and without), noting that our method is tailored to extended prompts, given that short ones are out-of-distribution with respect to the training set. Compared to SD3, we observe that our models perform better on average (CAD-I 0.57, DiT-I 0.57) than SD3 (0.56) at resolution of 2562, when evaluated with the extended prompt . Our models also outperform SD1.5 (0.43), SD2.1 (0.50), SDXL (0.55) and PixArt-α (0.48), despite these models being evaluated at higher resolution. Resolution is crucial in this benchmark, as shown by the drop in performance of SD3 when evaluated at 5122 (0.62) compared to 2562 (0.49). Even without extended prompts , our CADI model successfully reaches the performance of SDXL at its full resolution, while having 10x fewer parameters and How far can we go with ImageNet for Text-to-Image generation? AIO TA TA+IA Figure 4. Qualitative comparison across models. From left to right: An image of {class-name} AIO, Text-Augmentation (TA), and Image-Augmentation with Text-Augmentation (TA + IA). The examples show generated images of (a) pirate ship sailing on steaming soup, (b) hedgehog and an hourglass, and (c) crab sculpted from yellow cheese. While text augmentation improves the models understanding, image augmentation leads to better text comprehension and higher image quality overall. being trained on only 0.1% of the data. DPGBench Table 3 reports the results on DPGBench, recent benchmark similar to Geneval but with more complex prompt. We observe similar trends as for GenEval: compared to the current leaderboard, we achieve an overall accuracy of 76% with DiT-I, which improves over SDXL by 1.3%. We reach an overall score of 79.94% for CAD-I, outperforming SDXL by +5% and PixArt-α by +8%. Impressively, our models reach accuracies comparable to that of Janus (Wu et al., 2024), 1.3B parameters VLM with generation capabilities. Notably, both our models are particularly good at relations, achieving state-of-the-art of 93.5% for CAD-I and 92.2% for DiT-I. Quantitative results: Image Quality We analyze the impact of our augmentations on image quality. We use DiTI and CAD-I and train them on ImageNet with either short captions An image of ..., or the long captions obtained from Llava. Table 1 reports the results when tested on the validation set of ImageNet (right part) and COCO (left part). For ImageNet, as point of reference, we remind the reader that models of this size (below 0.5B parameters) typically have an FID of 9 using the class-conditional setup (Peebles TA TA+IA 19 18 17 100 300 400 Training Steps in thousands of steps Figure 5. Training dynamics showing FID scores vs training steps. TA+IA (red) maintains better FID scores throughout training compared to TA only (blue), demonstrating improved resistance to overfitting. Lower FID scores indicate better image quality. & Xie, 2023). We observe that models trained with the short AIO captions fail to achieve this mark (23 for DiT-I and 85 for CAD-I), probably because the condition representation from the text encoder is more ambiguous than simple class label. In contrast, our augmentations allow us to reach lower FID (8.52 for DiT-I and 6.62 for CAD-I) and much better precision, recall, density, and coverage scores. For COCO, this trend is all the more dramatic, which is zero-shot task. Our augmented models are the only ones able to correctly follow the prompt as attested by the much improved CLIP score (DiT-I from 13.16 to 24.85; CAD-I from 12.89 to 26.60), while keeping similar image quality (slightly higher FID, but much lower FID using Dinov2 backbone). Qualitative results Comparison between our different model variants are shown in Figure 4. Using the An image of ... prompt format, the baseline model (AIO) struggles to generate coherent images when prompted with concepts outside of ImageNet classes. With text augmentation (TA), the model demonstrates improved concept understanding and composition abilities, though image quality remains limited. Combining text and image augmentations (TA + IA) leads to enhanced image quality and better prompt understanding. This improvement is particularly evident in the pirate ship scene: while the TA model generates ship awkwardly positioned with bowl of soup, the TA + IA model creates more natural composition with the pirate ship appropriately sailing in the bowl. Similarly, the hedgehog and hourglass example shows more refined details and aesthetically pleasing composition with TA + IA, whereas the TA model struggles to render recognizable hedgehog. 7 How far can we go with ImageNet for Text-to-Image generation? 4.3. Ablations 4.3.1. ABLATION ON CUTMIX SETTINGS First, we analyze the performances of the pixel augmentations for {CM1/2 , CM1/4 , CM1/9 , CM1/16 , CMall } settings. We fix the probability of using pixel-augmented image in the batch when > τ to = 0.5 and we measure both image quality and composition ability. Results are reported in Table 4. For image quality, all settings seem to perform similarly, with CM1/2 being the best at 6.13 FID and CMall being the worst at 6.81 FID. This indicates that all settings are able to avoid producing uncanny images that would disturb the training too much. For composition ability, CM1/16 is able to improve over the baseline on extended prompts, whereas CMall is able to improve over the baseline on original prompts. Overall, only CMall manages to keep closer performances between the original prompts and the extended ones. Since CMall is mixture of all other settings, it also has the most diverse training set and is thus harder to overfit. As such, we consider CMall for the best models. Model CutMix Settings FID GenEval 2 / - I - CM1/2 CM1/4 CM1/9 CM1/16 CMall CM1/2 CM1/4 CM1/9 CM1/16 CMall 8.74 8.40 8.68 8.31 8.41 6.13 6.41 6.63 6.42 6.81 0.37 0.29 0.21 0.25 0.29 0.46 0.49 0.51 0.47 0.53 0.53 0.57 0.53 0.54 0.57 0.55 0.53 0.51 0.56 0. Table 4. Ablation study on CutMix settings. The probability of sampling CutMix images used here is ρ = 0.5. Models are trained for 250k steps. FID is computed on the ImageNet val set with long prompts, using the Inception-v3 backbone. means original GenEval prompts. means extended GenEval prompts. 4.3.2. ABLATION ON CUTMIX PROBABILITY Next, we analyse the influence of the probability of using pixel augmented image in the batch, when the condition on is met. Results for {0.25, 0.5, 0.75, 1.0} are shown in Table 5, using CM1/4 pixel augmentations. As we can see in terms of image quality, the FID is slightly degraded by having too frequent pixel augmentation (p > 0.5). This can be explained by the fact that pixel-augmented images are only seen when > τ . As such, high value for creates distribution gap between the images seen for 8 Model ρ FID GenEval 2 / - I - C 0 0.25 0.5 0.75 1 0 0.25 0.5 0.75 1 8.22 8.52 8.40 8.73 8.34 6.16 5.99 6.41 6.71 6.07 0.25 0.33 0.29 0.35 0. 0.51 0.55 0.49 0.45 0.48 0.55 0.58 0.57 0.54 0.49 0.55 0.58 0.53 0.53 0.49 Table 5. Ablation study on probability ρ of sampling CutMix image during training. The CutMix setting is CM1/4. Models are trained for 250k steps. FID is computed on ImageNet val set with long prompts, using the Inception-v3 backbone. means original GenEval prompts. means extended GenEval prompts. > τ and the images seen for τ . Composition ability shows similar behavior with the GenEval overall score decreasing when increases for both the original and the extended prompts. As such, we consider 0.5 for the best models. 4.3.3. DETAILED GENEVAL RESULTS Table 2 reports the influence of text and image augmentations for both models. We observe that models trained without any augmentation perform very poorly on GenEval. Text augmentations allow for better prompt understanding, especially when using extended prompts. Pixel space augmentations greatly improve compositionality-related tasks like Two Objects and Count, leading to higher overall scores. 4.3.4. MITIGATING OVERFITTING Figure 5 shows that the TA+IA models can successfully maintain lower FID score than the TA models throughout training. While the FID of TA models starts increasing after 300k steps, indicating overfitting, the FID of TA+IA models continues to decrease. This demonstrates that image augmentation (IA) effectively mitigates overfitting, allowing the model to improve image quality using longer training. 5. Discussion In this work, we challenged the prevailing wisdom that billion-scale datasets are necessary for high-quality text-toimage generation. Through careful visual and textual augmentation techniques, we demonstrated that models trained on just 1.2M image-text pairs can match or exceed the performance of those trained on thousand-fold larger datasets. Our approach, combining rich generated captions with CutMix augmentations, enables efficient learning of complex How far can we go with ImageNet for Text-to-Image generation? visual concepts while dramatically reducing computational costs and environmental impact. rectional copy-paste for semi-supervised medical image segmentation. In CVPR, 2023. The implications of our work extend beyond just computational efficiency. By showing that smaller, well-curated datasets can achieve state-of-the-art results, we open new possibilities for specialized domain adaptation where largescale data collection is impractical. Our work also suggests path toward more controllable and ethical development of text-to-image models, as smaller datasets enable more thorough content verification and bias mitigation. Looking forward, we believe our results will encourage the community to reconsider the bigger is better paradigm. Future work could explore additional augmentation strategies, investigate the theoretical foundations of data efficiency, and develop even more compact architectures optimized for smaller datasets. Ultimately, we hope this work starts shift toward more sustainable and responsible development of text-to-image generation models. 6. Impact Statement Text-to-image generative models have raised concerns about their impact on society. non-exhaustive list of these concerns includes deep-fakes proliferation, racial and gender biases, copyright infringement, private data stealing through web-scraping, and environmental costs associated with training very large models on gigantic datasets. In this paper, we demonstrate that text-to-image generative models can be trained on smaller, well-curated datasets and still be competitive with models trained on web-scale datasets. This opens the door to better control over the data used for training such models and curating them to tackle bias, copyright or privacy issues. It also allows practitioners to significantly reduce the cost of training such models."
        },
        {
            "title": "Acknowledgment",
            "content": "This work was granted access to the HPC resources of IDRIS under the allocation 2025-AD011015436 and 2025AD011015594 made by GENCI, and by the SHARP ANR project ANR-23-PEIA-0008 funded in the context of the France 2030 program. The authors would like to thank Thibaut Loiseau, Yannis Siglidis, Yohann Perron, Louis Geist, Robin Courant and Sinisa Stekovic for their insightful comments, suggestions, and discussions."
        },
        {
            "title": "References",
            "content": "Abbas, A., Tirumala, K., Simig, D., Ganguli, S., and Morcos, A. S. Semdedup: Data-efficient learning at web-scale through semantic deduplication. ICLR, 2023. Bai, Y., Chen, D., Li, Q., Shen, W., and Wang, Y. BidiBetker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving image generation with better captions. OpenAI, 2023. Birhane, A., Prabhu, V., Han, S., Boddeti, V. N., and Luccioni, A. S. Into the laions den: Investigating hate in multimodal datasets. NeurIPS, 2023. Birhane, A., Han, S., Boddeti, V., Luccioni, S., et al. Into the laions den: Investigating hate in multimodal datasets. NeurIPS, 2024. Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic textto-image synthesis. ICLR, 2023. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. J. Machine Learning Research, 2024. Courant, R., Dufour, N., Wang, X., Christie, M., and Kalogeiton, V. Et the exceptional trajectories: Text-to-cameratrajectory generation with character awareness. In European Conference on Computer Vision, 2025. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. NeurIPS, 2021. Dufour, N., Besnier, V., Kalogeiton, V., and Picard, D. Dont drop your samples! coherence-aware training benefits conditional diffusion. In CVPR, 2024a. Dufour, N., Picard, D., Kalogeiton, V., and Landrieu, L. Around the world in 80 timesteps: generative approach to global visual geolocation. arXiv, 2024b. Dvornik, N., Mairal, J., and Schmid, C. Modeling visual context is key to augmenting object detection datasets. In ECCV, 2018. Dwibedi, D., Misra, I., and Hebert, M. Cut, paste and learn: Surprisingly easy synthesis for instance detection. In ICCV, 2017. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proc. ICML, 2024. 9 How far can we go with ImageNet for Text-to-Image generation? Fan, J., Gao, B., Jin, H., and Jiang, L. Ucc: Uncertainty guided cross-head co-training for semi-supervised semantic segmentation. In CVPR, 2022. Hu, X., Wang, R., Fang, Y., Fu, B., Cheng, P., and Yu, G. Ella: Equip diffusion models with llm for enhanced semantic alignment. arxiv, 2024. Fang, H.-S., Sun, J., Wang, R., Gou, M., Li, Y.-L., and Lu, C. Instaboost: Boosting instance segmentation via probability map guided copy-pasting. In ICCV, 2019. Gadre, S. Y., Ilharco, G., Fang, A., Hayase, J., Smyrnis, G., Nguyen, T., Marten, R., Wortsman, M., Ghosh, D., Zhang, J., Orgad, E., Entezari, R., Daras, G., Pratt, S., Ramanujan, V., Bitton, Y., Marathe, K., Mussmann, S., Vencu, R., Cherti, M., Krishna, R., Koh, P. W., Saukh, O., Ratner, A., Song, S., Hajishirzi, H., Farhadi, A., Beaumont, R., Oh, S., Dimakis, A., Jitsev, J., Carmon, Y., Shankar, V., and Schmidt, L. Datacomp: In search of the next generation of multimodal datasets. NeurIPS, 2023. Ge, Y., Xu, J., Zhao, B. N., Joshi, N., Itti, L., and Vineet, V. Beyond generation: Harnessing text to image models for object detection and segmentation. arXiv, 2023. Ge, Y., Yu, H.-X., Zhao, C., Guo, Y., Huang, X., Ren, L., Itti, L., and Wu, J. 3d copy-paste: Physically plausible object insertion for monocular 3d detection. NeurIPS, 2024. Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.-Y., Cubuk, E. D., Le, Q. V., and Zoph, B. Simple copy-paste is strong data augmentation method for instance segmentation. In CVPR, 2021. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. Gokaslan, A., Cooper, A. F., Collins, J., Seguin, L., Jacobson, A., Patel, M., Frankle, J., Stephenson, C., and Kuleshov, V. Commoncanvas: Open diffusion models trained on creative-commons images. In CVPR, 2024. Ha, S., Kim, C., Kim, D., Lee, J., Lee, S., and Lee, J. Finding nemo: Negative-mined mosaic augmentation for referring image segmentation. ECCV, 2024. Henry, A., Dachapally, P. R., Pawar, S., and Chen, Y. Querykey normalization for transformers. Proc. EMNLP, 2020. Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi, Y. Clipscore: reference-free evaluation metric for image captioning. arXiv, 2021. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. Huang, Q., Park, D. S., Wang, T., Denk, T. I., Ly, A., Chen, N., Zhang, Z., Zhang, Z., Yu, J., Frank, C., et al. Noise2music: Text-conditioned music generation with diffusion models. arXiv, 2023. Jabri, A., Fleet, D., and Chen, T. Scalable adaptive computation for iterative generation. Proc. ICML, 2023. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. NeurIPS, 2022. Kingma, D. P. Auto-encoding variational bayes. ICLR, 2014. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. In ICCV, 2023. Koukounas, A., Mastrapas, G., Wang, B., Akram, M. K., Eslami, S., Gunther, M., Mohr, I., Sturua, S., Martens, S., Wang, N., et al. jina-clip-v2: Multilingual multimodal embeddings for text and images. arXiv, 2024. Kynkaanniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T. Improved precision and recall metric for assessing generative models. NeurIPS, 2019. Lai, Z., Saveris, V., Chen, C., Chen, H.-Y., Zhang, H., Zhang, B., Tebar, J. L., Hu, W., Gan, Z., Grasch, P., Cao, M., and Yang, Y. Revisit large-scale image-caption data in pre-training multimodal foundation models. arxiv, 2024. Li, X., Tu, H., Hui, M., Wang, Z., Zhao, B., Xiao, J., Ren, S., Mei, J., Liu, Q., Zheng, H., Zhou, Y., and Xie, C. What if we recaption billions of web images with llama-3? arxiv, 2024. Lin, S., Wang, K., Zeng, X., and Zhao, R. Explore the power of synthetic data on few-shot object detection. In ICCV, 2023. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In ECCV, 2014. Liu, B., Akhgari, E., Visheratin, A., Kamko, A., Xu, L., Shrirao, S., Lambert, C., Souza, J., Doshi, S., and Li, D. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arxiv, 2024a. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probaLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction bilistic models. NeurIPS, 2020. tuning. NeurIPS, 2024b. 10 How far can we go with ImageNet for Text-to-Image generation? Luccioni, S., Akiki, C., Mitchell, M., and Jernite, Y. Stable bias: Evaluating societal representations in diffusion models. NeurIPS, 2024. Naeem, M. F., Oh, S. J., Uh, Y., Choi, Y., and Yoo, J. Reliable fidelity and diversity metrics for generative models. In Proc. ICML, 2020. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.-Y., Li, S.-W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research (TMLR), 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, 2023. Pham, H., Dai, Z., Ghiasi, G., Kawaguchi, K., Liu, H., Yu, A. W., Yu, J., Chen, Y.-T., Luong, M.-T., Wu, Y., Tan, M., and Le, Q. V. Combined scaling for zero-shot transfer learning. arXiv, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv, 2023. Radenovic, F., Dubey, A., Kadian, A., Mihaylov, T., Vandenhende, S., Patel, Y., Wen, Y., Ramanathan, V., and Mahajan, D. Filtering, distillation, and hard negatives for vision-language pre-training. CVPR, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In Proc. ICML, 2021. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022. Sharifzadeh, S., Kaplanis, C., Pathak, S., Kumaran, D., Ilic, A., Mitrovic, J., Blundell, C., and Banino, A. Synth2: Boosting visual-language models with synthetic captions and image embeddings. arxiv, 2024. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. Proc. ICML, 2015. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. ICLR, 2020a. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. ICLR, 2020b. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In CVPR, 2016. Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The new data in multimedia research. Communications of the ACM, 2016. Tian, Y., Fan, L., Chen, K., Katabi, D., Krishnan, D., and Isola, P. Learning vision from models rivals learning vision from data. CVPR, 2023. Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., and Jegou, H. Going deeper with image transformers. In ICCV, 2021. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. NeurIPS, 2017. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv, 2022. Wang, F., Wang, H., Wei, C., Yuille, A., and Shen, W. Cp 2: Copy-paste contrastive pretraining for semantic segmentation. In ECCV, 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. Wei, C., Mangalam, K., Huang, P.-Y., Li, Y., Fan, H., Xu, H., Wang, H., Xie, C., Yuille, A., and Feichtenhofer, C. Diffusion models as masked autoencoders. In ICCV, 2023. Wu, C., Chen, X., Wu, Z., Ma, Y., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., Ruan, C., et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv, 2024. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv, 2024. 11 How far can we go with ImageNet for Text-to-Image generation? Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019. Zhao, H., Sheng, D., Bao, J., Chen, D., Chen, D., Wen, F., Yuan, L., Liu, C., Zhou, W., Chu, Q., Zhang, W., and Yu, N. X-paste: Revisiting scalable copy-paste for instance segmentation using clip and stablediffusion. In Proc. ICML, 2023. 12 How far can we go with ImageNet for Text-to-Image generation? A. Implementation details + Pointwise Feedforward LayerNorm + Multi-head Cross-Attention LayerNorm + Multi-head Self-Attention LayerNorm Condition Block + Pointwise Feedforward LayerNorm + Multi-head Self-Attention LayerNorm Condition Block-2 Condition Block-1 Time Text Z-Norm Time Text Self Attention x2 shared across RIN-I blocks FLAN T5 XL Embeddings Time Cross Attention From Conditioning to Latents Cross Attention From Pixels to Latents Self Attention xN Latents Latents Cross Attention From Latents to Pixels Figure 6. Fundamental architecture blocks used in our experiments. Left: DiT-I block and Right: CAD-I block. In this work, we use both DiT (Peebles & Xie, 2023) and RIN (Jabri et al., 2023) architectures. To adapt DiT for textconditional setting, we replace AdaLN-Zero conditioning with cross-attention to input the text condition into the model, as in (Chen et al., 2023). Before feeding the text condition to the model, we refine it using two self-attention layers. Similar to (Esser et al., 2024), we add QK-Normalization (Henry et al., 2020) in each of the self-attention and cross-attention blocks to mitigate sudden growths of attention entropy and reduce training loss instability. We also add LayerScale (Touvron et al., 2021) to each of the residual blocks of DiT for further stability. Figure 6 details our DiT-I architecture. To adapt the RIN (Jabri et al., 2023) for the text-conditional setting, we used the off-the-shelf architecture from (Dufour et al., 2024a), an adaptation of the RIN architecture detailed in the Appendix of (Dufour et al., 2024a). Figure 6 details our CAD-I architecture. We use the framework of latent diffusion (Rombach et al., 2022). For encoding the images into the latent space, we use the pre-trained variational autoencoder (Kingma, 2014; Van Den Oord et al., 2017) provided by the authors of Stable Diffusion (Rombach et al., 2022). The checkpoint used is available on HuggingFace: https://huggingface. co/stabilityai/sd-vae-ft-ema. For text conditions, we encode the captions using the T5 text encoder. The checkpoint is available on HuggingFace: https://huggingface.co/google/flan-t5-xl. B. Captioning details Captioning efficiently with LLaVA To caption images, we use the checkpoint llama3-llava-next-8b-hf (available on HuggingFace: https://huggingface.co/llava-hf/llama3-llava-next-8b-hf) with the prompt Describe this image. LLaVA encodes images using dynamic resolution scheme. It processes both the entire image and four distinct patches as unique images and concatenates them. For 256x256 images, LLaVA uses around 2500 image tokens. To make the captioning process more efficient, we prune the image tokens, retaining only the tokens of the entire image and discarding patch-specific tokens. This optimization increased inference speed by factor of 2.7, without compromising performances. Examples of long captions generated by LLaVA are given in Figure 8. Captioning CutMix images We caption CutMix images from CM1/2 with similar settings used for captioning the original ImageNet images. However, to ensure that LLaVA does not describe both the base and the CutMix images independently, we use different prompt: Describe this image. Consider all the objects in the picture. Describe them, describe their position and their relation. Do not consider the image as composite of images. The image is single scene image. For settings CM1/4, CM1/9 and CM1/16, LLaVA tends to either ignore the smaller CutMix image or describe the image as composite of two images. To avoid this behaviour, we encode the image by using the entire image patch and add tokens 13 How far can we go with ImageNet for Text-to-Image generation? from the patch to which the CutMix image belongs. We use the following prompt: Describe this image. Consider all the objects in the picture. Describe them, describe their position and their relation. Do not consider the image as composite of images. The image is single scene image. Examples of long captions generated by LLaVA for CutMix images are given in Figure 8. C. Threshold τ for image augmentation (IA) The threshold τ that enables image augmentations (Section 3.3) is empirically chosen. Figure 7 shows the noise level for each timestep for the four image augmentation settings. We found that above timestep 400, the contents of the images are mostly lost. As result, the timestep threshold, τ , is set to 400 for our experiments. This helps in not mitigating the superimposition artifact to the model as it never sees such distribution of images and only sees the noisy counterparts. 0 100 Increasing Timesteps Figure 7. Noise levels across for the four image augmentation settings 14 Setting LLaVA w/o CM CM1/2 How far can we go with ImageNet for Text-to-Image generation? Long Caption In the heart of the verdant background, delicate white butterfly with hint of yellow around its edges is captured in mid-flight. two shades of cream and white, add soft contrast to the vibrant colors of the flower its hovering over. with yellow center that speaks of the suns rays it might have been basking in. Surrounding it is lush green foliage, adding depth to this tranquil scene. The butterfly, positioned slightly to the left of the flowers center, seems to be in the process of alighting or perhaps taking off, adding dynamic element to this otherwise serene tableau. The flower, stunning shade of purple, is adorned Its wings, blend of On the left side, there is person playing the trumpet on street. The individual is standing on the ground, wearing long-sleeved shirt and jeans. On the ground next to the player, there is trash can. of asphalt, with buildings in the background and clear sky overhead. right side of the image, there are two penguins standing on gravel surface, possibly road or rocky beach. contrast to the dynamic scene on the left side. black bodies, and white feathers are clearly visible. everyday moment of music played on the street, juxtaposed with the surreal and unexpected presence of the penguins. about the time of day, specific location, or any actions being performed by the penguins or the person. time rather than sequence of events. The penguins are frozen in place, displaying The image is static, presenting two moments captured in The image does not provide information The penguins yellow beaks, The image captures an The street is made On the CM1/4 The image shows large, muscular dog in the foreground and silver sports car The dog, appearing to be Golden Retriever, is on the left in the background. side of the frame, occupying significant portion of the space. hue with darker markings, and its ears are floppy. with snowflakes, suggesting that the photo was taken in snowy environment. sports car, positioned on the right, is two-door coupe with sleek curves and notable design, featuring the Mercedes-Benz logo on its front grille. has silver finish, and the photo captures it from perspective that shows the front and side profile. parking lot or driveway. while the car is positioned slightly towards the side, away from the viewers perspective. The car is parked on an asphalt surface, possibly The dog is facing the camera with direct gaze, The dog is heavily coated The car It has yellow The CM1/9 CM1/16 The image depicts picturesque outdoor scene featuring an ornate building, which appears to be palace or manor house, with classical architectural elements including symmetrical windows, central cupola, and multiple chimneys. In front of the building is well-maintained garden with pathways and neatly trimmed hedges or borders. few scattered clouds. bright orange and yellow pattern. height above the garden and the building, suggesting it might be part of leisure activity or special event. lighting, indicative of sunny day. In the sky, there is single hot air balloon with Above the garden, there is clear blue sky with The balloon is floating at considerable The image is photograph with natural The image is photograph featuring husky dog resting in the snow. The dog has light coat with darker markings around its face and ears, and it is lying on its side with its head up, looking directly at the camera. and its mouth is slightly open, showing teeth and pink tongue, which suggests the dog might be panting or in relaxed state. Next to the dogs side, there is wine glass with red wine and few purple flowers, which could be lilacs, The wine glass and flowers are positioned on the left side of the glass stem. set against blurred background that gives the impression of greenery. Its eyes are open Figure 8. Long captions generated by our synthetic LLaVA captioner. The captions generated are highly diverse and add in much more intricate details of compositionality, colors as well as concepts which are not present in the original ImageNet dataset. The captions generated for our augmented images are also highly coherent and explain the scene in much more realistic way. 15 How far can we go with ImageNet for Text-to-Image generation? D. Additional Qualitative Results D.1. Based on ImageNet validation set captions DiT-I CAD-I 16 How far can we go with ImageNet for Text-to-Image generation? D.2. Based on DPG bench captions DiT-I CAD-I 17 How far can we go with ImageNet for Text-to-Image generation? E. Teaser prompts 1. Hummingbird with Metallic Flower This is captivating image of hummingbird in mid-flight, its wings blur of motion as it approaches uniquely crafted flower. The flower is not natural bloom but an artistic creation of polished metal and vibrant rubies, its petals catching the light and reflecting spectrum of colors. The hummingbird, with its long, slender beak, is in the process of feeding from the flower, its delicate form contrasting with the solidity of the metal and stone. The background of the image is picturesque beach scene, with the ocean stretching out to the horizon and the sandy shore providing soft, natural counterpoint to the artificial beauty of the flower. The image conveys sense of wonder and the unexpected beauty that can arise from the juxtaposition of nature and art. 2. Dancing Turtle in Neonwave The image features turtle engaged in dance within vibrant neonwave environment. The turtle is depicted standing upright, its posture suggesting movement and rhythm. The surrounding environment is characterized by bright, glowing neon colors, creating dynamic and energetic atmosphere. The turtles shell and skin may reflect some of the neon light, adding to the visual spectacle. The overall scene evokes sense of fun and celebration, with the turtles dance and the neon wave backdrop combining to create unique and captivating image. The lighting is likely dynamic and colorful, enhancing the neonwave aesthetic. 3. Pink Elephant on Beach This is whimsical image of pink elephant enjoying day at the beach. The elephant, large and majestic creature, is colored bright, bubblegum pink, making it stand out against the natural surroundings. It is walking along the shoreline, its large feet leaving prints in the wet sand. The beach is typical tropical paradise, with fine white sand and clear turquoise water. The sun is shining brightly, casting warm glow over the scene. The image evokes sense of fun and lightheartedness, suggesting playful and carefree atmosphere. The contrast between the elephants unnatural pink color and the natural beauty of the beach creates surreal and captivating image. 4. Metallic Giant Ant The image features colossal ant, constructed entirely of polished metal, standing amidst desolate landscape. The ants metallic exoskeleton gleams under the harsh light, reflecting the barren surroundings. Its segmented body is intricately detailed, showcasing the rivets and joints that hold its metallic form together. The ants powerful legs, also crafted from metal, are firmly planted on the cracked earth, suggesting sense of strength and stability. Its antennae, long and delicate, reach out into the empty air, perhaps sensing the environment. The background is vast, empty plain, stretching out to the horizon under pale, cloud-strewn sky. The overall image evokes sense of science fiction or fantasy, where giant metallic creatures roam desolate worlds. 5. Two Corgis Portrait The image depicts two corgi dogs arranged side-by-side against backdrop that features warm color palette with subtle floral designs. Both dogs appear to be of the Pembroke Welsh Corgi breed, characterized by their short legs and long bodies. They have predominantly white coat with brown markings. The dog on the left is looking directly at the camera, while the dog on the right is not looking towards the camera, instead gazing slightly to the right. The dog on the right has its mouth slightly open, as if panting or cooling down, and its tongue is partially visible. Both dogs have calm demeanor, and there is no visible text on the image. The style of the photograph seems to be professional studio shot, likely taken for the purpose of showcasing the dogs. 6. Chimpanzee with Windmill The image captures playful moment with chimpanzee interacting with brightly colored paper windmill. The chimpanzee, with its expressive face and dexterous hands, is gently blowing on the windmill, causing its four blades to spin. The windmill is crafted from vibrant paper, featuring variety of colors that 18 How far can we go with ImageNet for Text-to-Image generation? contrast nicely with the chimpanzees dark fur. The background is soft, neutral tone, allowing the focus to remain on the chimpanzee and the windmill. The scene evokes sense of childlike wonder and innocent amusement, highlighting the chimpanzees intelligence and curiosity. The lighting is natural, illuminating the scene and adding depth to the image. 7. Hedgehog eating mushroom The image features small hedgehog in forest setting, focused on consuming mushroom. The hedgehog, with its spiky brown and white coat, is positioned with its head down, its tiny mouth engaged with the mushroom. The mushroom itself is classic toadstool shape, with red cap speckled with white dots. The forest floor around the hedgehog is covered in fallen leaves and other natural debris, suggesting an autumnal setting. The lighting is soft and diffuse, highlighting the textures of the hedgehogs quills and the mushrooms cap. The overall scene conveys sense of quiet woodland life and the hedgehogs natural foraging behavior. 8. Steampunk Robot in Forest Photography closeup portrait of an adorable rusty broken-down steampunk robot covered in budding vegetation, surrounded by tall grass, misty futuristic sci-fi forest environment. 9. Tropical Floral Arrangement This image captures close-up view of tropical floral arrangement, emphasizing the intricate details of the individual flowers and leaves. The arrangement features mix of vibrant tropical blooms, including fiery orange bird of paradise flowers with their distinctive beak-like shape, delicate pink and white ginger flowers with their spiralling petals, and clusters of tiny, fragrant jasmine blossoms. The flowers are interspersed with lush tropical foliage, such as broad, heart-shaped anthurium leaves and slender, cascading orchids. The lighting is soft and directional, highlighting the textures and colors of the flowers and leaves. The background is blurred, out-of-focus representation of tropical setting, perhaps beach or lush garden, further enhancing the sense of place. The overall image conveys feeling of warmth, abundance, and the exotic beauty of the tropics. 10. Otter with Rubber Duck This image features an otter and yellow rubber duck engaged in playful encounter. The otter, its fur rich, dark brown, is seen from slightly elevated angle, partially submerged in water. Its paws are actively engaged with the rubber duck, bright yellow object floating on the waters surface. The duck, classic representation of bath toy, has its typical orange beak and black dot eyes. The otters attention is fully focused on the duck, suggesting moment of playful exploration. The water around them is calm, with gentle ripples created by the otters movements. The background is slightly blurred depiction of natural environment, possibly rocky shoreline or pond. The lighting is soft and diffused, emphasizing the textures of the otters fur and the smooth surface of the rubber duck. The image captures moment of lighthearted interaction, conveying sense of the otters natural playfulness. 11. Scuba Diver at Coral Reef The image features scuba diver exploring vibrant coral reef. The diver, clad in full scuba gear including black wetsuit, bright yellow oxygen tank, and diving mask, is positioned near large, fan-shaped coral formation. The coral is mix of colors, including deep reds, vibrant oranges, and soft yellows, creating visually stunning underwater landscape. The divers fins are visible, suggesting gentle movement through the water. Sunlight filters down from the surface, illuminating the scene and highlighting the intricate details of the coral polyps. Small, brightly colored fish dart in and out of the coral branches, adding to the lively atmosphere of the reef. The water is clear and blue, offering excellent visibility of the underwater world. The overall scene evokes sense of wonder and tranquillity, showcasing the beauty of marine ecosystems. 12. Warhol-Style Parrot 19 How far can we go with ImageNet for Text-to-Image generation? The image depicts vibrant parrot, rendered in the iconic style of Andy Warhol. The parrots plumage is kaleidoscope of bold, contrasting colors, reminiscent of Warhols screen printing technique. The background is flat, solid color, perhaps bright pink or electric blue, further emphasizing the subject. The parrots pose is simple and direct, possibly perched on branch or presented against the stark backdrop. The overall composition evokes sense of pop art, with the repetition of color and form characteristic of Warhols work. The image captures the essence of Warhols fascination with celebrity and mass production, applying it to the natural beauty of parrot. 13. Dewy Cobweb The image showcases delicate cobweb, glistening with morning dew. The intricate structure of the web is highlighted by the tiny droplets, each one catching and reflecting the soft morning light. The dew clings to the fine strands, outlining the geometric pattern of the web against blurred background of early morning foliage. The overall effect is ethereal and fragile, capturing fleeting moment of natural beauty. The light catches the water droplets, creating tiny sparkling jewels along the silken threads. The scene evokes sense of peace and tranquillity, characteristic of quiet morning in nature. The focus is sharp on the web, emphasizing its delicate construction and the ephemeral nature of the dew. 14. Astronaut in Jungle The image features an astronaut standing amidst dense, vibrant jungle. The astronaut is clad in sleek, white spacesuit, complete with helmet that reflects the dappled sunlight filtering through the thick canopy. The suit shows subtle signs of wear and tear, perhaps hinting at long journey or unexpected landing. The visor of the helmet partially obscures the astronauts face, but hint of curiosity can be discerned. The jungle environment is lush, with towering trees draped in vines, and exotic flowers blooming in vibrant colors. The ground is covered in thick carpet of moss and ferns. The scene evokes sense of wonder and exploration, juxtaposing the advanced technology of the spacesuit with the raw, untamed beauty of the jungle. 15. Mexican Tacos The image captures close-up view of several delicious Mexican tacos, arranged on traditional ceramic plate. The tacos are filled with generous portion of seasoned ground beef, topped with shredded lettuce, diced tomatoes, crumbled cheese, and dollop of sour cream. The ground beef is cooked thoroughly, with rich brown color and visible bits of seasoning. The lettuce adds crisp green element to the tacos, while the diced tomatoes provide juicy burst of red. The crumbled cheese adds creamy texture and touch of white against the other ingredients. The dollop of sour cream adds smooth, tangy finish to the tacos. The traditional ceramic plate, with its colorful patterns and designs, complements the vibrant colors of the tacos. The background is slightly blurred, focusing attention on the details of the tacos. The lighting is bright and even, showcasing the textures and colors of the food. The overall image conveys the appetizing and authentic nature of Mexican street food. F. Qualitative results prompts Here we show the prompts used to make the Figure 4. Note that for AIO we use the short version of the prompt as it is closer to its train distribution: 1. pirate ship sailing on streaming soup The image showcases colossal, exquisitely crafted pirate ship, its presence commanding and largerthan-life, as it sails triumphantly across boundless sea of steaming soup. The ships hull, made of dark, polished wood, is adorned with intricate carvings of dragons and waves, while its three towering masts support vast, billowing sails that glow faintly in the warm, golden light radiating from the broth. The soup is vibrant, aromatic masterpiece, with swirls of rich broth, floating islands of noodles, and vibrant vegetables like carrots, bok choy, and mushrooms creating textured, immersive landscape. The ships deck is alive with detailropes coiled neatly, barrels stacked high, and crows nest peeking above the sails, all slightly damp from the soups rising steam. The bowl, an enormous, ornate vessel, is crafted from gleaming porcelain, its surface painted with delicate, hand-drawn scenes of mountains and rivers, adding layer of cultural richness 20 How far can we go with ImageNet for Text-to-Image generation? to the surreal composition. The scene is both absurd and breathtaking, blending the grandeur of seafaring adventure with the comforting, whimsical charm of bowl of soup, creating an image that is unforgettable and endlessly imaginative. 2. hedgehog and an hourglass The image features small, brown hedgehog with its characteristic spiky coat, standing near an hourglass in the middle of dense forest. The hourglass is made of clear glass, and fine grains of sand are visible as they fall from the top chamber to the bottom. The forest surrounding the hedgehog and the hourglass is lush and green, with tall trees and thick undergrowth. Sunlight filters through the leaves, creating dappled patterns on the forest floor. The scene evokes sense of tranquillity and the passage of time. The hedgehog appears to be observing the falling sand, perhaps contemplating the fleeting nature of time. 3. crab sculpted from yellow cheese quirky crab, entirely sculpted from various types of yellow cheese, sits proudly on white plate. Its body is smooth, golden wheel of cheese, round and rich in color, while soft, creamy cheese legs extend outward in neatly shaped segments, each one gently curled as if the crab is about to scuttle away. The claws are crafted from sharp, crumbly yellow cheese, carefully carved to resemble the pincers, with tiny bits of grated Parmesan scattered across to give it texture. The eyes are tiny olives, carefully set in place with delicate toothpicks, adding playful touch to the cheese creation. Surrounding the crab, fresh basil leaves are placed to resemble seaweed, completing the amusing and mouthwatering oceanic scene on the plate."
        }
    ],
    "affiliations": [
        "AMIAD, Pole recherche",
        "LIGM, Ecole Nationale des Ponts et Chaussees, IP Paris, Univ Gustave Eiffel, CNRS, France",
        "LIX, Ecole Polytechnique, CNRS, IP Paris, France"
    ]
}