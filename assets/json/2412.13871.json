{
    "paper_title": "LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer",
    "authors": [
        "Yipeng Zhang",
        "Yifan Liu",
        "Zonghao Guo",
        "Yidan Zhang",
        "Xuesong Yang",
        "Chi Chen",
        "Jun Song",
        "Bo Zheng",
        "Yuan Yao",
        "Zhiyuan Liu",
        "Tat-Seng Chua",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In multimodal large language models (MLLMs), vision transformers (ViTs) are widely employed for visual encoding. However, their performance in solving universal MLLM tasks is not satisfactory. We attribute it to a lack of information from diverse visual levels, impeding alignment with the various semantic granularity required for language generation. To address this issue, we present LLaVA-UHD v2, an advanced MLLM centered around a Hierarchical window transformer that enables capturing diverse visual granularity by constructing and integrating a high-resolution feature pyramid. As a vision-language projector, Hiwin transformer comprises two primary modules: (i) an inverse feature pyramid, constructed by a ViT-derived feature up-sampling process utilizing high-frequency details from an image pyramid, and (ii) hierarchical window attention, focusing on a set of key sampling features within cross-scale windows to condense multi-level feature maps. Extensive experiments demonstrate that LLaVA-UHD v2 achieves superior performance over existing MLLMs on popular benchmarks. Notably, our design brings an average boost of 3.7% across 14 benchmarks compared with the baseline method, 9.3% on DocVQA for instance. We make all the data, model checkpoint, and code publicly available to facilitate future research."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 1 1 7 8 3 1 . 2 1 4 2 : r LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer Yipeng Zhang 1 Yifan Liu 1 Zonghao Guo 1 Yidan Zhang 4,5 Xuesong Yang Chi Chen 1 Jun Song 3 Bo Zheng 3 Yuan Yao 2 Zhiyuan Liu 1 Tat-Seng Chua 2 Maosong Sun 1 1Tsinghua University 2National University of Singapore 3Alibaba Group 4University of Chinese Academy of Sciences 5Aerospace Information Research Institute, Chinese Academy of Sciences yipengzhang97@gmail.com guozonghao96@outlook.com https://github.com/thunlp/LLaVA-UHD"
        },
        {
            "title": "Abstract",
            "content": "In multimodal large language models (MLLMs), vision transformers (ViTs) are widely employed for visual encoding. However, their performance in solving universal MLLM tasks is not satisfactory. We attribute it to lack of information from diverse visual levels, impeding alignment with the various semantic granularity required for language generation. To address this issue, we present LLaVA-UHD v2, an advanced MLLM centered around Hierarchical window transformer that enables capturing diverse visual granularity by constructing and integrating highresolution feature pyramid. As vision-language projector, Hiwin transformer comprises two primary modules: (i) an inverse feature pyramid, constructed by ViT-derived feature up-sampling process utilizing high-frequency details from an image pyramid, and (ii) hierarchical window attention, focusing on set of key sampling features within cross-scale windows to condense multi-level feature maps. Extensive experiments demonstrate that LLaVA-UHD v2 achieves superior performance over existing MLLMs on popular benchmarks. Notably, our design brings an average boost of 3.7% across 14 benchmarks compared with the baseline method, 9.3% on DocVQA for instance. We make all the data, model checkpoint, and code publicly available to facilitate future research."
        },
        {
            "title": "Introduction",
            "content": "Embedding visual information into large language models (LLMs) [97, 21, 4, 2] has significantly enhanced their ability to tackle complex visual multimodal tasks, such as visual question answering [8, 38, 32], document analysis [78, 77, 66], and visual interaction [19, 27]. With the rise of transformers [98], the vision transformer (ViT) [24, 85] has emerged as standard approach for visual encoding in contemporary multimodal large language models (MLLMs) [23, 10, 99, 63, 62, 52, 4, 7, 103]. However, the ViT in an MLLM struggles with effectively handling all types of visual multimodal tasks, due to inadequate visual granularity captured by single-scale features, as illustrated in Fig. 1(a). For instance, visual grounding [107, 17, 110, 106] and optical character recognition [46, 49] demand Equal contribution. Corresponding authors. Figure 1: Comparison of LLaVA-UHD v2 with other MLLMs. (a) MLLMs typically align ViT features to language space using MLPs [63] or perceiver re-samplers [6, 52], lacking visual granularity. (b) Combining multiple visual encoders is non-universal and computationally intensive. (c) LLaVAUHD v2 employs the Hiwin transformer to build an inverse feature pyramid and compress it into visual tokens, providing various semantic granularity for language generation. fine-grained visual details, while image caption [84, 57] focuses on capturing high-level semantics and relations. Drawing on the advancements in computer vision, comprehensive coverage of fine-grained details and high-level semantics can be integrated effectively by feature pyramid [59, 87, 114]. However, this method presents two key challenges when applied to MLLMs: (1) Representation Inheritance. With its scalability, the ViT boasts general vision-language representations acquired through largescale image-text pre-training (e.g., CLIP [85] or SigLIP [109]), which is essential for continual learning in MLLMs. Modifying the ViT into hierarchical architecture such as Swin [67] for multi-scale representations renders the vision-language representations of existing pre-trained models non-heritable. (2) Compression Effectiveness. The quadratic computational cost of LLMs with respect to the number of visual tokens necessitates effective compression of the feature pyramid. Current techniques [91, 60, 71, 96] often resize features to fixed resolution, which distorts both the spatial shape and intrinsic semantics, as shown in Fig. 1(b). Moreover, down-sampling high-resolution features risks losing critical fine-grained details. To address these challenges, we present LLaVA-UHD v2, an advanced MLLM centered around novel Hierarchical window transformer, which enables capturing diverse visual granularities by constructing and integrating high-resolution feature pyramid, as shown in Fig. 1(c). The Hiwin transformer performs an expansion-then-compression procedure on spatial resolution of feature maps, which consists of two stages: (i) Constructing an inverse feature pyramid. We train parameterized feature up-sampler, the Joint Bilateral Upsampling (JBU) module proposed in FeatUp [26], on top of pre-trained ViT (i.e., CLIP-ViT [85]) to encode high-frequency information from the image into its feature map. This strategy can be applied to any ViT, inheriting its powerful feature representations, while constructing multi-resolution feature pyramid based on the ViT. (ii) Integrating features with hierarchical window attention. To condense the multi-level feature maps effectively, we propose utilizing set of hierarchical windows to capture semantics from local regions across different pyramid levels. set of learnable queries is restricted to attend solely to key sampled features within their respective windows. This attention mechanism performs effective compression on local dense features at the native resolution of each pyramid level, thereby enabling visual tokens to effectively capture both fine-grained details and high-level semantics. Extensive experiments demonstrate that LLaVA-UHD v2 dramatically outperforms the baseline method (LLaVA-UHD [31]) across 14 popular benchmarks, including document-centric visual question answering (e.g., +9.3% on DocVQA), visual grounding (e.g., average +5.7% on RefCOCOs [107]), and high-resolution image perception (e.g., +3.4% on HR-Bench [100]). Moreover, we 2 Figure 2: The overall architecture of proposed LLaVA-UHD v2, consisting of ViT, our hierarchical window transformer (Hiwin transformer), and an LLM. Hiwin transformers process sliced patches and the overview image by capturing inner multi-level representations and compressing them into spatially consistent tokens for better vision-language alignment. experimentally reveal that feature pyramids, regardless of the specific construction method (e.g., bilinear interpolation), can further enhance the visual perception capabilities of MLLMs, offering new insights for future research. Our contributions can be concluded as follows: We propose visual pyramid encoding method by pre-training an up-sampling module, providing enriched semantic granularity for language decoding. We propose hierarchical window attention, capable of integrating and compressing high-resolution feature pyramid into condensed set of visual tokens, thereby ensuring computational efficiency while preserving the critical details needed for language decoding. Trained efficiently on academic-scale data, LLaVA-UHD v2 achieves substantial improvements over the baseline method across 14 benchmarks."
        },
        {
            "title": "2.1 Feature Pyramid Representation",
            "content": "Image pyramid techniques are fundamental in image processing, facilitating multi-resolution analysis since the era of manual feature design, as seen in SIFTs scale-space keypoints [13, 70]. In deep learning, CNNs such as ResNet and VGG, inherently extract hierarchical features across scales [35, 93]. Innovations such as FPN and U-Net enhance semantic hierarchy for tasks like detection and segmentation [59, 87]. Recently, some transformer-based models [67, 111, 114] further advanced feature pyramid construction, capturing more comprehensive visual semantic granularity for visual representations. However, multimodal language models, often using CLIP-based ViTs, underutilize hierarchical features, suggesting research gap for integrating advanced feature pyramids into these models [111, 85]."
        },
        {
            "title": "2.2 Visual Encoding in MLLMs",
            "content": "CLIP-ViT, favored for its effective alignment of visual features with linguistic semantics through contrastive pre-training, is widely adopted in MLLMs [85, 64, 63, 10, 62, 7, 113, 55]. Emerging research explores alternative visual representations, primarily in three categories: (1) Fusing features from CLIP-based CNNs and ViTs. LLaVA-HR [73] integrates stage-wise CNN features [68] into ViTs layers, enhancing fine-grained perception. CogAgent [37] utilizes ViT features to query highresolution CNN features for detailed information during language decoding. Mini-Gemini [54] employs cross-attention-based post-fusion between CNN and ViT features. (2) Fusing features from visual experts trained with different vision pre-training tasks [60, 28, 71, 115, 96, 91, 102]. Candidate experts include DINO-v2 [83] by visual contrastive pre-training, SAM [47] by prompt segmentation pre-training, Pix2Struct [49] by document parsing pre-training, etc. Deepseek-VL [71], SPHINX-X [60, 28] and Eagle [91] down-sample output feature maps and concatenate them along the channel axis. Cambrian-1 [96] initializes embeddings to query local patches from visual experts. (3) Language models for visual encoding: Fuyu [11], Otter-HD [51], and SOLO [20] encode images directly with LLMs, bypassing dedicated visual encoders. However, these approaches, while effective, increase computational demand and hinder unified image-to-language design."
        },
        {
            "title": "2.3 Token Projection and Compression",
            "content": "MLPs [64, 63], perceiver resamplers [6, 10, 103] and Q-Formers [52, 22, 112] are basic projectors widely used in modern MLLMs. Recently, various new designs have emerged. (1) Spatial-preserving compression. Qwen2-VL [99] and MiniGPT-v2 [16] employ simple linear to merge tokens locally (e.g. 22). Honey-bee [15] introduces C-abstractor (i.e. CNN-based block), while Oryx [69] dynamically pools features and utilizes them to query the original ones. (2) Cross-layer feature compression. Token-Packer [53] compresses features across layers in ViT using cross-attention. MMFuser [14] uses final-layer features as queries to attend to early-layer features. (3) Semanticmerged compression. Chat-UniVi [40] and LLaVA-PruMerge [89] extending the token merging [12] strategy, merge features with similar semantics into single representation. However, these approaches depend heavily on feature padding, resizing, and reshaping, hindering their ability to compress features with arbitrary resolutions."
        },
        {
            "title": "3.1 Overview",
            "content": "The architecture of the proposed LLaVA-UHD v2 is illustrated in Fig. 2. It consists of three primary modules: visual encoder (ViT), vision-language projector (Hiwin transformer), and an LLM. We first divide and crop each input image into multiple slices with appropriate size and aspect ratio, leveraging the slicing strategy proposed in [31]. The image slices, along with an overview image, are then processed by CLIP-ViT [85] with interpolated positional embeddings, capable of processing images with arbitrary sizes and shapes. The resulting features are subsequently passed to the Hiwin transformer for vision-language projection, which is carried out with two stages: (i) constructing an inverse feature pyramid (detailed in Sec.3.2), and (ii) integrating the feature pyramid by hierarchical window attention (detailed in Sec.3.3). The core of the Hiwin transformer lies in enhancing each ViT-encoded feature into high-resolution feature pyramid encoding, thereby achieving enriched semantic granularity for each image slice. After this enhancement, visual tokens from different slices are reorganized into spatially consistent feature map relative to the original image, ensuring clarity in spatial relationships. Following by concatenating with the overview tokens, all the visual tokens provide visual context for language decoding. 3."
        },
        {
            "title": "Inverse Feature Pyramid",
            "content": "2l Preliminaries. In the era of hand-crafted visual representations [70, 13], gaussian convolution filter is widely employed to construct image pyramids {I 2l 3, = 0, 1...L 1}, where denotes the pyramid level, (H, ) the image height and width, and the total number of levels. Building upon this concept, deep convolutional neural networks (CNNs), such as ResNet and VGG, naturally yield bottom-up, hierarchical feature pyramid {F p2l C}, where represents the feature dimension and denotes the down-sampling ratio (e.g., = 8 in ResNet-50 [35]). In such pyramids, lower-level feature maps generally have higher resolution and capture more visual detail, whereas higher-level feature maps exhibit lower resolution but contain more abstract semantic information. However, ViTs splitting the image into coarse patches, only produce single-scale feature maps (i.e., 0 C, where for instance, = 14), as illustrated in Fig. 1(a). Lacking such feature pyramid, like that present in CNNs, hinders the performance of ViTs on MLLM tasks, which require both fine-grained visual details and high-level semantic information. Consequently, how to construct ViT-based feature pyramid with varying semantic granularity remains an unresolved challenge. p2l Guided feature up-sampling. Due to the lack of high-resolution features, up-sampling ViT features becomes the necessary strategy to inversely construct the feature pyramid. Two straightforward approaches can be employed:(1) plain bilinear interpolation and (2) deconvolution network. By doubling and quadrupling the resolution of resulting feature maps, ViT-based feature pyramid {F H2l C, = 0, 1, 2} is constructed. While effective, directly up-sampling deep features hardly introduces precise visual details, which are essential for tasks requiring fine-grained visual information. To address this limitation, we leverage the parameterized JBU module [26] to guide feature up-sampling with image priors, which helps introduce high-frequency visual details. 2l 4 Figure 3: Flowchart of the Joint Bilateral Upsampling (JBU) module, which leverages the image pyramid to guide feature up-sampling, integrating high-frequency information into the up-sampled feature maps. 2l Generally, the objective of JBU module is to learn (l 1) convolutional layers on the image pyramid {I H2l 3} to capture high-frequency patterns of local texture for guiding feature upsampling, in Fig. 3. Specifically, for each input image, its (l + 1)-th level ViT features is defined as l+1 = Conv(cid:0)Up(F l); Θl+1(I l+1)(cid:1), (1) where Up() denotes the up-sampling interpolation and Conv() represents the convolutional operation applied to feature maps using customized kernel weights Θl+1 learned from the corresponding image l+1. As result, each pixel value of the up-sampled feature maps is extracted as l+1[x, y] = 1 (cid:80) (x,y)U (cid:18) Up(F l)[x, y] Ddist Dsim (cid:0)Θl+1(I l+1[x, y]), Θl+1(I l+1[x, y])(cid:1) (cid:19) , (2) where [x, y] denotes the coordinate index of feature maps, the region of convolution neighborhood, Ddist is decay factor based on the distance between [x, y] and [x, y], and Dsim is similarity weight based on the attention of each image pixel using two-layer GeLU MLP Θl+1. Additional details can be found in supplement A.1. By iteratively performing Eq. 1, we progressively up-sample the feature maps and ultimately construct the feature pyramid {F l, = 0, 1, 2}. Learnable up-sampler optimizing. Following [26], we optimize the reconstruction loss between the highest-level feature maps and the lowest as = 0 Down(F 2; Ω)2 2, (3) where Down() is down-sampling network with trainable weights Ω, e.g., two convolution layers with stride of 2. Note that we omit some hyper-parameters in Eq. 3 for clarity. Please kindly refer to the supplement A.1 for details. However, when training the JBU module upon CLIP-ViT [85], we encounter the texture detail degradation of high-resolution feature maps. Specifically, features of CLIP-ViT scarcely exhibit enhanced detail representation at higher resolutions, compared to other ViTs like DINO-ViT [83]. We attribute this issue to the lack of inter-level feature reconstruction for language-aligned features during iterative up-sampling, resulting in an unstable transformation of such semantic-centric representations of the CLIP-ViT. To address this, we propose straightforward hierarchical supervision strategy, regulating each level in the pyramid. Eq. 3 is updated as follows. The complete form for Eq. 3 and Figure 4: The flowchart of hierarchical window attention. Feature maps from different levels of the feature pyramid are adaptively RoI-aligned into sampling features and then concatenated along the length axis to serve as the key for the learnable queries. additional details can be found in supplement A.1. = 1 2 2 (cid:88) l= 0 Down(F l; Ωl)2 2 (4)"
        },
        {
            "title": "3.3 Hierarchical Window Attention",
            "content": "The hierarchical nature of feature pyramids necessitates an effective approach for compressing features at varying resolutions while maintaining cross-level spatial alignment. Hierarchical window generation. Inspired by object detection [86, 114], we employ the RoIalign [36] to sample key features while preserving the spatial locality across feature maps at different levels, as illustrated in Fig. 4. Specifically, we begin by uniformly dividing the feature maps of each level into windows, whose widths and heights are float-point values ( ) rather than integers. Especially, windows that share the same anchor points across levels form set of hierarchical bounding boxes, represented by the top-left and bottom-right coordinates {Rl i,j R14, i, 0, 1, 2...N 1, = 0, 1, 2}, where indicates the feature level and (i, j) the 2D indices. To mitigate the size distortion of feature maps caused by the discrepancy between the aspect ratio of RoI-aligned feature maps and the original feature maps, we define pooling score to evaluate this discrepancy: , S(W, H, rw, rh) = (cid:12) (cid:12) (cid:12) (cid:12) log"
        },
        {
            "title": "W\nH",
            "content": "log rw rh (cid:12) (cid:12) (cid:12) (cid:12) , (5) w, where (rw, rh) denotes the width and height of pooled features. By maximizing the score S, we select the optimal grid size (r h) from set of predefined proposals {(3, 3), (2, 3), (3, 2), (2, 4), (4, 2)}. Subsequently, we perform RoI-align with the generated windows to sample the key feature maps for the following attention operation. Cross-window feature querying. To compress the multi-level feature pyramid {F l, = 0, 1, 2} of given image slice I, we initialize set of queries {Qi,j R1C, i, 0, 1, 2...N 1}, each of which corresponds to set of hierarchical windows {Rl i,j, = 0, 1, 2}, in Fig. 4. Given each query vector Qi,j, we prepare the key vector Kl )C of l-th level as xr i,j R(r i,j = RoI(F l, Rl Kl i,j) + ϕl, (6) where ϕl is level-wise positional embedding. The key vectors Kl length axis to form the final key vector Ki,j R(3r i,j are then concatenated along the )C for each query Qi,j. The corresponding xr 6 value vector Vi,j is obtained in the same manner, but without the level-wise positional embedding. Thus, the cross-attention can be performed as (7) i,j denotes the updated query, and φ, ϕ is the 2D spatial position embedding of query and i,j into feature map where key vector, respectively. In the end, we concatenate all the updated query RN C to represent the visual token of image I. i,j = CrossAttn(Qi,j + φi,j, Ki,j + ϕi,j, Vi,j),"
        },
        {
            "title": "3.4 Spatially-consistent Token Organization",
            "content": "Due to the varying partitions across different images, effectively organizing and conveying the structure of slices to the LLM is crucial for more accurate understanding of the image. Prior studies [31, 60, 103] employ spatial tokens (e.g., or ,) to denote the relative positioning of image slices. While such approaches provide cues regarding the global arrangement of each slice, they overlook the intrinsic spatial relationships among individual visual tokens. For instance, two horizontally adjacent tokens in the image, located in different slices, may become significantly separated in 1D arrangement. To address this, we leverage our Hiwin transformer, which preserves 2D spatial consistency with the original image, and amalgamate these 2D feature maps into large 2D feature map according to the slicing configuration, in Fig. 2. newline token is then inserted between rows, followed by flattening them into the 1D arrangement. Additionally, comma , is placed between the large feature maps and the features of the overview image to distinguish between different views of the image."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we present an empirical evaluation of LLaVA-UHD v2. We begin with comprehensive outline of the implementation details of our model, followed by comparative analysis of its performance across widely recognized benchmarks against competitive counterparts. Finally, we provide an in-depth ablation to further elucidate the capabilities and behavior of LLaVA-UHD v2. 4."
        },
        {
            "title": "Implementation Details",
            "content": "Model Setting. We adopt LLaVA-UHD [31] as the baseline method. Specifically, we employ CLIP-ViT-L/14-336 as the visual encoder, Vicuna-7B [21] as the language model, and our proposed Hiwin Transformer as the vision-language projector. We set the maximum slice number to 6 to cover range of aspect ratios and image resolutions. The number of learnable local queries is set as 144 (i.e.,N = 12). LLaVA-UHD v2 consists of three-stage training process, as outlined below. Stage 0: JBU module pre-training. Before employing the JBU module in MLLM, we pre-train it on the MS-COCO [58] dataset with global batch of 16 on 8A100. We leverage Adam optimizer with learning rate of 1e3 for 2000 steps. During this stage, the parameters of CLIP-ViT are frozen. Stage 1: MLLM pre-training. In this stage, the parameters of the visual encoder, JBU module, and LLM are frozen. We only fine-tune the parameters within the hierarchical window attention of the HiWin Transformer using LLaVA-Pretrain [63] dataset for 1 epoch. We employ the AdamW optimizer with learning rate of 1e3 and cosine learning rate schedule. The global batch size is set to 256. Note that during this stage, we only encode the overview image, excluding the slices, for efficiency. Stage 2: MLLM supervised fine-tuning. In this stage, we fine-tune all model parameters except for those in the JBU module. To manage training costs, we use dataset comprising 825k samples for analysis and ablation studies, including LLaVA-mix665k [63] and 160k samples from UReader [104]. For comparison with advanced MLLMs, we balance our data distribution and introduce the 858kmixed dataset, which is detailed in supplement A.3. The learning rate is set to 2e5 with batch size of 128."
        },
        {
            "title": "4.2 Experimental Setting",
            "content": "We present the experimental settings, detailing the benchmarks, evaluation metrics, and compared counterparts. 7 Table 1: Main performance on popular benchmarks. For fair comparison, we only report the method using 7B level LLM (e.g., Vicuna-7B). #Data denotes the volume of overall data during MLLM pre-training and supervised fine-tuning. MaxRes. is the maximum accessible resolution of MLLM. Avg.: average results of 13 benchmarks. VQAD: DocVQA. BenchOCR: OCR-Bench. VQAC: ChartQA. VQAT: TextVQA. SQA: Science-QA. MMMUv: MMMU-val. SEEDI: SEEDImage. MMEP: perception sub-set of MME. RWQA: RealWorldQA. BenchHR: HR-Bench. Method #Data MaxRes. #FLOPs. Avg. VQAD BenchOCR VQAC VQAT AI2D SQA MMMUv GQA SEEDI MMB MMEP RWQA BenchHR OCR & Chart Knowledge General Vision Spatial High Res. - - - 56.9 62. Qwen-VL [10] MiniGPT-v2 [16] - - 65.4 49.0 21.8 4.0T 1.45B 448448 4.0T 326M 448448 1.7T mPLUG-Owl2 [105] 401M 448448 20.3T 86M 8961120 8.0T 1.22M 336336 - 42.2T 1.01B 762762 - 15.3M 448448 21.3T - 1.22M 10241024 24.3T - 8.2T 51M 336336 2.6T 52.5M 336336 - 54.6T 59.4 61.9 3.0M 672672 28.0T 59.2 66.5 1.40B 8961344 44.4T 61.0 63.6 1.34M 672672 UReader [104] LLaVA-1.5 [63] SPHINX-2k [60] SPHINX-X [28] LLaVA-HR [73] VILA [56] Honey-bee [15] Mini-Gemini [54] Monkey [55] LLaVA-Next [62] - 56.3 - - - LLaVA-UHD v2 (ours) 1.42M 1008 17.5T 63.2 68.1 48.8 15.7 - - 31.8 - - - - - 47.7 51.4 53.2 53.9 66.3 - - 59.3 17.8 - 39.7 - - - 47.4 65.1 54.3 61.5 - 58.2 57.6 45.5 61.2 58.1 67.1 64.4 - 65.2 67.6 64.9 57.7 68. - - - - 68.7 - - 55.5 66.8 70.6 63.0 70.4 65.1 68.2 - - - - 68.2 69.6 62.6 69.4 67.0 70. 35.9 - - - 37.0 - - - - 35.3 36.8 38.9 35.8 57.5 60.3 56.1 - 62.0 63.1 56.2 64.2 62.3 - 64.5 60.7 64.2 65.4 - 57.8 - 65.8 71.6 68.8 64.2 61.1 64.5 66.9 64.3 70.2 61.8 - 64.5 - 66.5 65.9 57.9 - 68.9 70.1 65.8 59.8 67.4 74.4 - 72.5 - 75.3 73.6 63.0 77.7 76.7 79.2 77.3 73.6 76.0 64. 67.6 70.5 71.3 38.2 65.4 70.0 68. 74.7 49.3 - - - 54.8 - - - - - 51.1 51.6 57.8 58.2 30.5 - - - 36.1 - - - - - 50.1 38.0 47.9 51.5 Table 2: Ablation studies of modules in our proposed method. HFP is the abbreviation of highresolution feature pyramid. denotes the overall improvement compared to the baseline. REC reports the average accuracy of RefCOCO/g/+. Method LLaVA-UHD [31] + JBU module +HFP integration +Token organization Vision Spatial High Res. Average VQAD BenchOCR VQAC VQAT AI2D SQA MMMUv GQA SEEDI MMB MMEP RWQA REC BenchHR OCR & Chart Knowledge General 58.0 60.0 61.5 61.7 +3.7 56.7 60.2 65.0 66.0 +9. 40.9 50.4 51.3 50.1 +9.2 56.3 60.4 62.5 62.8 62.2 67.1 68.5 66.8 55.4 70.7 57.8 70.5 58.1 69.2 59.4 69.8 37.0 38.2 38.9 37. 63.8 64.0 64.6 64.0 65.6 66.7 67.4 67.4 64.8 65.6 65.5 66.1 70.0 71.2 73.0 73.6 54.4 51.9 55.5 56.9 68.3 72.3 73.3 74. +6.5 +4.6 +4.0 -0.9 +0.6 +0.2 +1. +1.3 +3.6 +2.5 +5.7 45.6 43.9 48.9 49.0 +3. Benchmarks. Extensive benchmarks are used to analyze the effectiveness of our modules. We categorize these benchmarks into the following groups: (1) General VQA benchmarks including MME [25], MMB [65], SEED-Image [50] and GQA [38]; (2) Knowledge-based VQA benchmarks including MMMU-val [108], Science-QA [72], AI2D [44]; (3) OCR-based VQA benchmarks including ChartQA [77], OCR-Bench [66], TextVQA [94] and DocVQA [78]; (4) Visual spatial understanding benchmarks such as RealWorldQA [1] and RefCOCOs [107] (used in ablation studies); (5) High-resolution image perception benchmarks like HR-Bench(4K) [100]. Evaluation Protocols. Beyond benchmark evaluations, we report additional metrics for comprehensive analysis, including: (1) overall volume of training data, (2) maximum supported image resolution for each method, and (3) computational cost of the entire MLLM at maximum resolution. Counterparts. We compare our model with the advanced MLLM counterparts. (1) General MLLMs like Qwen-VL [10], MiniGPT-v2 [16], Honey-bee [15], mPLUG-Owl2 [105], VILA [56] and LLaVA1.5 [63]. (2) High-resolution MLLMs including UReader [104], Monkey [55], LLaVA-HR [73] ,LLaVA-Next [62]. (3) Mixture of visual experts such as [73], SPHINX-series [60, 28] and MiniGemini [7]."
        },
        {
            "title": "4.3 Main Performance",
            "content": "Table 1 showcases comparative analysis of our proposed LLaVA-UHD v2 against state-of-the-art MLLMs across 13 widely recognized benchmarks. To facilitate comprehensive evaluation, we have selected 5 prominent open-source models, and assessed them on all 13 benchmarks, to serve as our reference points. (1) LLaVA-UHD v2 outperforms current counterparts. Our model exhibits significant enhancement in average performance, achieving an improvement of 2.2% compared to LLaVA-Next, among other models. Specifically, compared with general models (such as Qwen-VL and LLaVA-1.5) and high-resolution MLLMs (like Monkey and LLaVA-Next), LLaVA-UHD v2 8 Figure 5: Performance on different visual tasks with JBU module and vanilla bilinear interpolation. OCR denotes the optical character recognition, Seg the Linear probing semantic segmentation, and Cls the fine-grained classification on SUB-200. Table 3: Comparison of different methods for feature pyramid construction. ConvNext means we replace the CILP-ViT with CLIP-ConvNext [68] as visual encoder and directly use the feature maps from multiple stages as the final hierarchical feature pyramid. General Method LLaVA-UHD w. ConvNext w. DeConv. w. Bilinear w. JBU module Average MMEP GQA 63.8 70.0 62.7 68.2 64.2 71.2 64.5 72.0 64.6 73.0 58.6 59.7 61.7 62.0 63.0 Knowledge AI2D 55.4 55.6 57.4 57.8 58.3 OCR & Chart High Res. VQAC VQAT VQAD BenchHR 56.3 61.8 61.8 62.2 62.5 45.6 44.0 46.3 46.5 48.9 62.2 63.5 67.8 67.6 68.5 56.7 61.8 63.4 63.7 65.0 demonstrates consistent improvements across various tasks, including general VQA (e.g., 65.4% on GQA), ultra-high-resolution image perception (e.g., 51.5% on HR-Bench). Notably, LLaVA-UHD v2 surpasses OCR-centric models like UReader on DocVQA (68.1% vs. 65.4%) and outperforms those with multiple experts (such as SPHINX-2k and Mini-Gemini), achieving superior performance on general tasks like MMB (68.2%) and SEED (70.0%). These results highlight the value of rich semantics derived from multi-level visual granularity in enhancing both the understanding and perceptual capabilities of MLLMs. (2) LLaVA-UHD v2 indicates efficiency on data utilization and computation. Compared to LLaVA-Next and Mini-Gemini, both operating at the 672672 resolution, LLaVA-UHD v2 supports 1.5 times the resolution (i.e., 6721008) and achieves superior performance with less than 40% of the computational cost. Furthermore, in contrast to Honey-bee and VILA, which utilize 52.5M and 51M data samples respectively, LLaVA-UHD v2 attains comparable or superior performance using only 2.8% of the data, demonstrating the data efficiency of our model, making it highly suitable for low-cost exploratory research in the academic community."
        },
        {
            "title": "4.4 Analytical Study",
            "content": "We conduct analytical experiments on proposed modules to validate the effectiveness of LLaVA-UHD v2. Note that we freeze the CLIP-ViT during the supervised fine-tuning, which facilitates clearer verification of the effectiveness of each module introduced in the Hiwin transformer. Main module ablation. In Table 2, by replacing the low-resolution CLIP-ViT features with JBUenhanced high-resolution ones, 2% average improvement can be seen, especially, on tasks depending on visual details like OCR-Bench (+9.5%) and RefCOCOs (+4.0%). Employing the Hiwin transformer to integrate the feature pyramid constructed by the JBU module further improves average accuracy by 1.5%, especially 5.0% on ultra-high resolution perception (HR-Bench), demonstrating rich visual granularity facilitates more precise language generation. Additionally, spatially consistent token organization leads to further performance improvements in visual-spatial understanding, with 1.4% increase on RealWorldQA and 0.7% improvement on RefCOCOs. 9 Figure 6: Qualitative comparison of proposed LLaVA-UHD v2 and advanced MLLMs, including LLaVA-Next, Mini-Gemini, and GPT-4V on high-resolution complex perception tasks, which require the integration of both fine-grained visual information and high-level semantic contexts. Table 4: Comparison of different choice of grid sizes on performance and efficiency. Pyramid means the feature grids from different levels form region-level feature pyramid, e.g., [23] for level-0, [46] for level-1, [812] for leval-2. Fix represents all feature maps are pooled into 33 feature grid. We measure the training period on 8A100s, the latency on an A100 with 1008672 image, and the GPU memory on 8A100s with 1 image per GPU in supervised fine-tune phase. Efficiency General Method Period(h) Latency(s) Memory(G) Average MMEP GQA Pyramid 60.8 Fix [33] 63.9 Selective 64.6 62.4 64.6 65.3 1.26 0.62 0.54 69.0 73.8 73.0 62.4 26.9 27. 60.3 41.7 39.4 Knowledge AI2D 57.3 58.8 58.3 OCR & Chart VQAC VQAT VQAD 58.9 67.5 60.7 63.8 66.2 60.9 65.0 68.5 62.5 JBU module demonstrates effectiveness on both MLLM tasks and vision tasks. We investigate the generalizability of different feature up-sampling methods (i.e., bilinear interpolation, de-convolution [36], JBU module, and CNN [68]) across diverse tasks. (1) Results on MLLM tasks. Table. 3 clearly shows that feature pyramids, regardless of their construction method, can enhance performance across various MLLM tasks. Nonetheless, the feature pyramid constructed with our JBU module achieves an average performance gain of 1.0% over bilinear interpolation, indicating that the JBU module further enhances beneficial visual representations (e.g., high-frequency visual features). (2) Results on visual tasks. Beyond the VQA task in MLLM, we further select some fundamental visual tasks, including semantic segmentation [58], optical character recognition [80], and fine-grained classification [45], to compare the effect of bilinear interpolation and JBU module. The settings of training and testing are detailed in supplement A.2. As shown in Fig. 5, the JBU module outperforms the bilinear interpolation on OCR (+4.0%), semantic segmentation (+4.7%) and fine-grained classification (+3.8%), demonstrating that the JBU module captures more visual details, enabling precise semantic discrimination. The choice of grid sizes influences the performance of MLLMs. We explore the impact of RoIalign grid size on the efficiency and performance of LLaVA-UHD v2. Specifically, we RoI-align region-level feature pyramid from the inverse feature pyramid within window set, ensuring that higher-resolution feature maps retain finer-grained pooling grids, like [96]. However, this approach, rather than improving multi-scale feature integration, significantly degrades performance and inference efficiency, as demonstrated in Table 4. Compared to fixed grids, selecting proper pooling grid (defined in Eq.5), offers better performance and efficiency, thanks to more approximate aspect ratio to the original image."
        },
        {
            "title": "4.5 Visualization Analysis",
            "content": "Case study. In Fig. 6, we visualize the performance of well-known MLLMs on high-resolution, complex perception tasks. This kind of task requires MLLMs to well fuse both visual details and highlevel semantics to accurately identify fine-grained targets (e.g., OCR, colors) during the procedure of complex semantic perception (e.g., semantic relation and visual behavior). It is evident that 10 Figure 7: Activation response of specific textual tokens to different visual feature levels. Red circles highlight the obvious difference between levels. (Best viewed in color and zoomed-in) LLaVA-UHD v2 correctly recognizes the tree planter in the newspaper photo and associates it with the name within the dense image caption (Case A). We also can see that LLaVA-UHD v2 captures the player who raises his hands and reads the number 3 on his clothes (Case B). In contrast, LLaVA-Next overlooks the name information within dense texts (Case A) and hallucinates on the player number (Case B). Mini-Gemini fails to extract the true name (Case A) and also hallucinates (Case B). Additionally, GPT-4V shows limitations in referencing the information in the newspaper (Case A) and falsely recognizes number 24 due to wrong fine-grained action perception (Case B). Appendix B.2.2 provides further cases for more comprehensive comparison across various task scenarios. Semantic activation cross feature scale. In Fig. 7, we demonstrate the activation responses of specific textual prompts in the language model to the inverse feature pyramid. As shown, OCR-like textual tokens yield finer-grained and more accurate activations at higher feature levels, facilitating accurate scene text recognition (first row). For object-level semantics, higher feature levels enhance edge detail activations, enabling more precise semantic localization(second row). Collectively, the feature pyramid offers more exhaustive set of visual semantics with rich granularity, effectively supporting nuanced language decoding."
        },
        {
            "title": "5 Conclusion",
            "content": "The proposed hierarchical window transformer, central to our LLaVA-UHD v2, effectively addresses the limitations of conventional ViT-based MLLMs by capturing varying visual granularity essential for precise language generation. The Hiwin transformer adeptly constructs an inverse feature pyramid for enriched semantics representation, which is subsequently condensed into compact set of visual tokens through our hierarchical window attention. This process enhances nuanced visuallinguistic alignment as well as facilitates efficient visual prompting for the LLM. LLaVA-UHD v2 has demonstrated substantial gains over the baseline method across range of MLLM benchmarks, demonstrating its capability in MLLM tasks that demand both fine-grained and high-level semantics. Furthermore, the Hiwin transformer offers versatility, presenting potential adaptability across diverse ViT-based MLLM architectures."
        },
        {
            "title": "References",
            "content": "[1] RealWorldQA. https://huggingface.co/datasets/xai-org/RealworldQA. 2024-09-24. [2] Introducing ChatGPT. https://openai.com/blog/chatgpt. 2022. [3] LAION-GPT4v dataset. https://huggingface.co/datasets/laion/gpt4v-dataset, 2023. 11 [4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [5] Guillaume Alain. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016. [6] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. Flamingo: visual language model for few-shot learning. In NeurIPS, 2022. [7] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 1, 2023. [8] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In IEEE ICCV, pages 24252433, 2015. [9] Rowel Atienza. Vision transformer for fast and efficient scene text recognition, 2021. [10] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [11] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, , and Sagnak Tasırlar. Introducing our multimodal models. adept.ai/blog/fuyu-8b. 2023. [12] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. [13] Peter J. Burt and Edward H. Adelson. The laplacian pyramid as compact image code. IEEE Trans. Commun., 31(4):532540, 1983. [14] Yue Cao, Yangzhou Liu, Zhe Chen, Guangchen Shi, Wenhai Wang, Danhuai Zhao, and Tong Lu. Mmfuser: Multimodal multi-layer feature fuser for fine-grained vision-language understanding. arXiv preprint arXiv:2410.11829, 2024. [15] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced projector for multimodal llm. In IEEE CVPR, pages 1381713827, 2024. [16] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. MiniGPT-v2: large language model as unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023. [17] Jierun Chen, Fangyun Wei, Jinjing Zhao, Sizhe Song, Bohuai Wu, Zhuoxuan Peng, S-H Gary Chan, and Hongyang Zhang. Revisiting referring expression comprehension evaluation in the era of large multimodal models. arXiv preprint arXiv:2406.16866, 2024. [18] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv:2311.12793, 2023. [19] Peng Chen, Pi Bu, Jun Song, Yuan Gao, and Bo Zheng. Can vlms play action role-playing games? take black myth wukong as study case. arXiv preprint arXiv:2409.12889, 2024. [20] Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji. single transformer for scalable vision-language modeling. arXiv preprint arXiv:2407.06438, 2024. [21] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023. [22] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023. 12 [23] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k HD. arXiv preprint arXiv:2404.06512, 2024. [24] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. [25] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. MME: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [26] Stephanie Fu, Mark Hamilton, Laura Brandt, Axel Feldman, Zhoutong Zhang, and William Freeman. Featup: model-agnostic framework for features at any resolution. arXiv preprint arXiv:2403.10516, 2024. [27] Quentin Gallouédec, Edward Beeching, Clément Romac, and Emmanuel Dellandréa. Jack of all trades, master of some, multi-purpose transformer agent. arXiv preprint arXiv:2402.09844, 2024. [28] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024. [29] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In IEEE CVPR, pages 69046913, 2017. [30] Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In ICML, pages 369376, 2006. [31] Zonghao Guo, Ruyi Xu, Yuan Yao, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. LLaVA-UHD: an lmm perceiving any aspect ratio and high-resolution images. In ECCV, 2024. [32] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. VizWiz grand challenge: Answering visual questions from blind people. In IEEE CVPR, pages 36083617, 2018. [33] Mark Hamilton, Evan Shelhamer, and William Freeman. It is likely that your loss should be likelihood. arXiv preprint arXiv:2007.06059, 2020. [34] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Unsupervised semantic segmentation by distilling feature correspondences. In ICLR, 2022. [35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE CVPR, pages 770778, 2016. [36] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask R-CNN. In IEEE ICCV, pages 29612969, 2017. [37] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. CogAgent: visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023. [38] Drew Hudson and Christopher Manning. GQA: new dataset for real-world visual reasoning and compositional question answering. In IEEE CVPR, pages 67006709, 2019. [39] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Synthetic data and artificial neural networks for natural scene text recognition. In NeurIPS, 2014. [40] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In IEEE CVPR, pages 1370013710, 2024. [41] Kushal Kafle, Scott Cohen, Brian Price, and Christopher Kanan. DVQA: Understanding data visualizations via question answering. In IEEE CVPR, 2018. [42] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, pages 787798, 2014. [43] Aniruddha Kembhavi, Michael Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. arXiv:1603.07396, 2016. [44] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, pages 235251, 2016. [45] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016. [46] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In ECCV, pages 498517, 2022. [47] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloé Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross B. Girshick. Segment anything. arXiv preprint arXiv:2304.02643, 2023. [48] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, pages 3273, 2017. [49] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In ICML, pages 1889318912, 2023. [50] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [51] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu. OtterHD: highresolution multi-modality model. arXiv preprint arXiv:2311.04219, 2023. [52] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. ICML, 2023. [53] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392, 2024. [54] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. [55] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607, 2023. [56] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In IEEE CVPR, pages 2668926699, 2024. [57] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740755, 2014. [58] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740755, 2014. [59] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In IEEE CVPR, pages 21172125, 2017. [60] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao. SPHINX: the joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. [61] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv:2306.14565, 2023. 14 [62] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, ocr, and world knowledge. https://llava-vl.github.io/ blog/2024-01-30-llava-next/. 2024. [63] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. [64] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024. [65] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMBench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. [66] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. [67] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In IEEE ICCV, pages 1001210022, 2021. [68] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In IEEE CVPR, pages 1197611986, 2022. [69] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. [70] David G. Lowe. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis., 60(2): 91110, 2004. [71] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. [72] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. NeurIPS, 35:25072521, 2022. [73] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003, 2024. [74] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In IEEE CVPR, pages 1120, 2016. [75] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In IEEE CVPR, 2019. [76] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv:2203.10244, 2022. [77] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [78] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for VQA on document images. In WACV, pages 21992208, 2021. [79] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [80] Anand Mishra, Karteek Alahari, and CV Jawahar. Scene text recognition using higher order language priors. In BMVC, 2012. [81] A. Mishra, K. Alahari, and C. V. Jawahar. Scene text recognition using higher order language priors. In BMVC, 2012. [82] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. OCR-VQA: Visual question answering by reading text in images. In ICDAR, pages 947952, 2019. 15 [83] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [84] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In IEEE ICCV, pages 26412649, 2015. [85] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. [86] Shaoqing Ren. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015. [87] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, pages 234241, 2015. [88] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In ECCV, pages 146162, 2022. [89] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. [90] ShareGPT. https://sharegpt.com/, 2023. [91] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. [92] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In ECCV, pages 742758, 2020. [93] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [94] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In IEEE CVPR, pages 83178326, 2019. [95] Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. Document collection visual question answering. In ICDAR, 2021. [96] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. [97] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [98] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. [99] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [100] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. arXiv preprint arXiv:2408.15556, 2024. [101] Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo, Lianwen Jin, Chee Seng Chan, Anton van den Hengel, and Liangwei Wang. On the general value of evidence, and bilingual scene-text visual question answering. In IEEE CVPR, pages 1012610135, 2020. [102] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large vision-language models. arXiv preprint arXiv:2312.06109, 2023. 16 [103] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [104] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. UReader: Universal OCR-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126, 2023. [105] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257, 2023. [106] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. [107] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, pages 6985, 2016. [108] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In IEEE CVPR, pages 95569567, 2024. [109] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In IEEE ICCV, pages 1197511986, 2023. [110] Yufei Zhan, Yousong Zhu, Zhiyang Chen, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon: Spelling out all object locations at any granularity with large language models. In ECCV, pages 405422, 2024. [111] Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi Dai, Qixiang Ye, and Qi Tian. Hivit: simpler and more efficient design of hierarchical vision transformer. In ICLR, 2023. [112] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. [113] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [114] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020. [115] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024."
        },
        {
            "title": "A Implementation Details",
            "content": "We provide details of the JBU module, experimental setting on visual tasks, and the dataset composition in the supervised fine-tuning phase of MLLM. A."
        },
        {
            "title": "JBU module",
            "content": "Up-sampling kernel. As presented in Eq. 2 of the main paper, the kernel of the JBU module relies on two weights Ddist and Dsim. Spatial distance decay Ddist is defined as (cid:18) Ddist = exp (x, y) (x, y)2 2 2σ2 dist (cid:19) (8) to represent the Euclidean distance relations between adjacent pixels, where σdist denotes learnable width. And pixel similarity weight Dsim is determined as Dsim = softmax (x,y)U (cid:18) Θl+1(I l+1[x, y]) Θl+1(I l+1[x, y]) σ2 sim (cid:19) , (9) where σsim is learnable temperature factor to modulate the distribution of similarity scores and = 7 Learnable down-sampler. We detail the implementation of the learnable down-sampler defined in Eq. 4 of the main paper. Compared to simple convolutional layer with stride of 2, we apply an attention-based down-sampler following [26] in each feature level. Specifically, 11 convolution layer is first applied to the feature maps of (l + 1)-th level to extract saliency map, followed by combining it with modified fully-connected layer to normalize the features in the local neighborhood . We summarize the above operation as network () with trainable parameters Ωl+1. As result, the feature pixel at the location of [x, y] on down-sampled feature maps (of l-th level) is formally defined as Down(F l+1; Ωl+1)[x, y] = softmax (cid:0)f (F l+1[Vx,y]; Ωl+1)(cid:1) l+1[Vx,y], (10) where Vx,y denotes the local neighborhood in the high-resolution feature maps. Note that, before performing Eq. 10, we experimentally up-sample the l+1 to the size of the original image using bilinear interpolation. And we set Vx,y = 14, aligning with the patch size of CLIP-ViT, to simulate the feature extraction of ViT. Multi-view hierarchical reconstruction loss. The objective of the JBU module is to construct high-resolution features by observing multiple different views of low-resolution features like NeRF [79] that generates 3D scene representation from multi-view 2D images. Given the input image and l-th level of feature maps l, we detail the Eq. 4 of the main paper as = 1 2T (cid:80)2 l=1 (cid:80) tT (cid:18) 1 u2 0 (t (I)) Down (cid:0)t (cid:0)F l(cid:1) ; Ωl(cid:1)2 2 + log(u) (cid:19) , (11) where is the image jitter operation sampled from collection of transformation such as pads, zooms, crops, flips, and etc. We experimentally set = 2. The semantic uncertainty = (F 0(t(I))), as introduced by [33], is parameterized by linear network . This term evaluates the certainty of features for up-sampling operations, as such processes may risk losing critical semantic information, which is vital for preserving the integrity of the representation. A.2 Experimental setting on visual tasks After training the JBU module in Stage 0, we applied it on the top of the visual backbone to assess its effectiveness. Unless explicitly stated otherwise, both naive bilinear interpolation and the JBU module are employed to perform feature up-sampling on low-resolution (i.e., 2424) feature maps which are extracted from the second-to-last layer of CLIP-ViT [85]. 18 Data Size Response formatting prompts LLaVA [63] ShareGPT [90] 158K 40K VQAv2 [29] GQA [38] OKVQA [75] OCRVQA [82] DocVQA [95] ChartQA [76] A-OKVQA [88] DVQA [41] TextCaps [92] 83K 72K 9K 80K 15K 20K 66K 20K 22K ShareGPT4V [18] 55K AI2D [43] LAIONGPT4V [3] SythDog-EN [46] LRV-Instruct [61] RefCOCO [42, 74] VG [48] 3K 11K 40K 30K 48K 86K Total 858K Answer the question using single word or phrase. Answer directly with the options letter from the given choices. Provide one-sentence caption for the provided image. Provide short description for this region. (for Region Caption) Provide the bounding box coordinate of the region this sentence describes. (for Referring Expression Comprehension) Table 5: Detailed composition of our 858k-mixed dataset. Optical character recognition. We follow the experimental setting of [9], training on MJSynth [39] dataset and evaluating on IIIT5K [81] dataset. Specifically, we first up-sample the low-resolution feature maps to higher ones (i.e., 4848) by using bilinear interpolation and pre-trained JBU module. The resulting feature maps are then fed to sequence encoder and CTC head [30] to predict the class labels of characters in the images. We train the entire model with global batch of 96 on 8A100, using Adadelta optimizer with 5e2 learning rate, and cosine scheduler for 6800 steps. We use the character-level cross-entropy loss between the predicted labels and the ground truth for supervision. For evaluation, we report the sentence-level recognition accuracy of the test split of IIIT5K [81] in Fig.5 of the main paper. Linear probing semantic segmentation. We follow the experimental setting of previous research [26, 5, 34]. To be specific, on the COCOStuff dataset [58], we train linear projection upon frozen CLIP-ViT to directly predict the class category of each pixel. The input of the linear projection is the low-resolution feature maps. We train the linear projection with global batch of 1024 on 8A100, using Adam optimizer with 5e3 learning rate for 360 steps. We use the pixel-level cross-entropy loss between the predicted labels and the ground truth for supervision. During the linear probing phrase, we up-sample the low-resolution feature maps to high-resolution (i.e., 9696) one by using bilinear interpolation or pre-trained JBU module and then directly feed them into the linear projection to predict the pixel category. We report the segmentation accuracy of the validation split of COCOStuff in Fig.5 of the main paper. Fine-grained classification. We train and assess on SUB-200 [45], fine-grained bird classification dataset. During the training, the low-resolution feature maps are first up-sampled to high-resolution (9696) ones with bilinear interpolation and pre-trained JBU module. Then, classification head with two linear layers pools the feature maps into vector for image classification. Note that, the CLIP-ViT is frozen and only the parameters in the classification head are trainable. We set the global batch as 16 on 1A100, using Adam optimizer with 1e4 learning rate and cosine scheduler for 10 epochs. We use the image-level cross-entropy loss between the predicted categories and the ground truth for supervision. We report the image classification accuracy on the SUB-200 validation dataset in Fig.5 of the main paper. 19 Table 6: Comparison of different choices of feature level on performance and efficiency. HS.: hierarchical supervision. ESTVQA [101] is VQA benchmark focusing on scene text recognition. Efficiency Performance Level 0,2 0,1,2 0,1,2,3 0,1,2,3 (w/o HS.) Period(h) Memory(G) Average GQA SQA REC VQAC VQAT ESTVQA MMEP 71.0 63.9 69.5 71.5 72.1 63.8 70.2 71.8 71.4 64.4 69.3 72.6 72.0 63.6 69.8 67.1 40.6 40.8 41.6 39.9 63.4 63.7 63.8 62. 41.9 41.9 53.0 52.6 27.7 28.0 45.6 45.6 60.5 60.5 60.7 57.8 66.5 66.9 66.4 66.5 Figure 8: Qualitative comparison on high-resolution dense perception task which requires the capabilities of fine-grained details perception. A.3 Supervised fine-tuning dataset As illustrated in Table 5, we detail the proposed 858k-mixed dataset in the supervised fine-tuning phase of MLLM."
        },
        {
            "title": "B Analysis",
            "content": "We analyze the behaviors of LLaVA-UHD v2 through qualitative and quantitative experiments. B.1 Quantitative experiment Level choice. As shown in Table 6, the introduction of higher level (higher resolution) feature maps results in consistent enhancement of the average performance. However, incorporation of even higher resolution features, such as level-3 feature maps (i.e., 8 resolution than level-0), yields marginal benefits while substantially increasing the training cost. If we abandon the hierarchical supervision strategy, performance suffers, which we attribute to the detailed degeneration issue discussed in Sec.3.2 of the main paper. In this case, the JBU module encounters significant challenges in effectively integrating high-frequency information into CLIP-ViT features. 20 Figure 9: Qualitative comparison on high-resolution fine-grained perception task which requires robust fine-grained visual texture perception capabilities. Figure 10: Qualitative comparison on high-resolution spatial perception which necessitates the capabilities of high-level spatial contexts. B.2 Qualitative experiment B.2.1 Enhanced high-resolution features. As discussed in Sec.3.2 of the main paper, we performed qualitative visualization to assess the impact of hierarchical supervision, presented in Fig.11 and Fig.12, Note that all the high-resolution features are 8 resolution than CLIP-ViT features. While bilinear interpolation increases the nominal resolution of features, it fails to enhance the fidelity of image detail representation. In comparison, naive JBU module captures finer details but retains degree of blurriness. With hierarchical 21 Figure 11: PCA visualization of the up-sampled features by JBU module on nature scene. With hierarchical supervision, the high-resolution features (8) could clearly depict object boundary and text appearance. (Best viewed in color and zoomed in) supervision, the degradation of high-resolution features is markedly reduced, resulting in more accurate and refined representation of visual details. B.2.2 Case studies. We add more cases to analyze the behavior of our LLaVA-UHD v2. The capabilities are summarized as four aspects as follows. Dense Perception. In Fig. 8, we visualize the performance of well-known MLLMs on dense perception tasks. Dense perception tasks require models to possess highly robust fine-grained perception capabilities to distinguish object boundaries within large number of densely packed similar objects, thereby accurately locating the target and its boundaries to identify the target precisely. It is evident that LLaVA-UHD v2 and GPT-4V accurately identify the beginning time of the television program H20 X5 Mop (case A), the performing date of the show The 3 Mile (case B), the duration of whole workouts (case C), and the prize of stoli (case D), indicating highly robust fine-grained perception capabilities provided by our visual pyramid representation. In comparison, other models either fail to precisely locate the target (LLaVA-Next) or cannot distinguish the target from similar adjacent objects, limited in accurately completing dense OCR tasks (Mini-Gemini). Fine-grained Perception. In Fig. 9, we visualized the performance of well-known MLLM on fine-grained perception tasks. These tasks require models to have robust fine-grained perception capabilities to detect the textures of small or blurry targets, thereby accurately locating and identifying small targets. Figure 12: PCA visualization of the up-sampled features by JBU module on OCR scene. With hierarchical supervision, the high-resolution features (8) could clearly depict object boundary and text appearance. (Best viewed in color and zoomed in) Cases indicate that LLaVA-UHD v2 accurately identified the small green light, and the tiny number of duration time associated with green light, demonstrating that the introduction of high-frequency information in hierarchical features can handle small, blurry targets effectively. In contrast, other models, cant find the small green light, or fail to accurately perform OCR tasks due to the text being too small or blurry (e.g., GPT-4V, LLaVA-Next, Mini-Gemini). This capability is further demonstrated in cases A, and D, where both LLaVA-UHD v2 and GPT-4V accurately identified the tiny number on the basketball jersey (case A), the blurry number on the very end of the bus (case B), and the time on the phone (case D), while LLaVA-Next and Mini-Gemini exhibited limitations. Spatial Perception. In Fig. 10, we visualized the performance of well-known MLLM on spatial perception tasks. Spatial perception tasks require models to have robust high-semantic perception capabilities to discern the spatial relationships between different objects. It is evident that LLaVA-UHD v2 and GPT4V perceive the spatial relative positions between different objects, to accurately identify the number above the right tire on the car (case A), the number of the player in the middle (case B), the number of the coin on the middle right (case C), the lowest ml (case D). This accuracy is attributed to our high-resolution visual pyramid representation, which allows the perfect integration of features of varying semantics and spatial information across different levels. In contrast, other models, such as LLaVA-Next and Mini-Gemini, fail to accurately perceive these relative spatial positions."
        }
    ],
    "affiliations": [
        "Aerospace Information Research Institute, Chinese Academy of Sciences",
        "Alibaba Group",
        "National University of Singapore",
        "Tsinghua University",
        "University of Chinese Academy of Sciences"
    ]
}