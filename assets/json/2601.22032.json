{
    "paper_title": "Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving",
    "authors": [
        "Linhan Wang",
        "Zichong Yang",
        "Chen Bai",
        "Guoxiang Zhang",
        "Xiaotong Liu",
        "Xiaoyin Zheng",
        "Xiao-Xiao Long",
        "Chang-Tien Lu",
        "Cheng Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art."
        },
        {
            "title": "Start",
            "content": "Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving Linhan Wang * 1 Zichong Yang 2 Chen Bai 3 Guoxiang Zhang 3 Xiaotong Liu 3 Xiaoyin Zheng 3 Xiao-Xiao Long 4 Chang-Tien Lu 1 Cheng Lu 3 6 2 0 2 9 2 ] . [ 1 2 3 0 2 2 . 1 0 6 2 : r Abstract End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with simple transformerbased decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting new state-of-the-art. The code is available at https://github.com/linhanwang/Drive-JEPA. 1. Introduction End-to-end autonomous driving (Pomerleau, 1988; Chitta et al., 2022; Hu et al., 2023b) has emerged as promising paradigm that directly maps raw sensor observations to driving actions using unified neural model. By eliminating hand-designed intermediate representations used in * Work done during an internship at XPENG Motors. 1Virginia Tech 2Purdue University 3XPENG Motors 4Nanjing University. Preprint. January 30, 2026. 1 Figure 1. Comparison between end-to-end planners on both perception-free and perception-based settings. traditional modular pipelines, end-to-end approaches aim to reduce information loss and improve scalability by learning directly from large collections of human driving data. Recently, end-to-end autonomous driving increasingly seeks to leverage self-supervised video pretraining to learn transferable representations for planning. However, pretraining video world models for scene understanding has so far brought only limited improvements. Existing approaches in this direction largely fall into two categories. First, videogenerative methods, such as VaVAM (Bartoccioni et al., 2025) and Epona (Zhang et al., 2025), learn representations by reconstructing or generating videos and then transfer them to planning, but this pixel-level objective incurs heavy computation and may over-emphasize visual details that are irrelevant to decision making. Second, to reduce cost, latent world models predict compact feature dynamics (e.g., LAW (Li et al.) predicts feature +1 from feature , and World4Drive (Zheng et al., 2025) further introduces pretrained foundation models to enrich latent targets. However, these latent approaches are typically used as auxiliary objectives and have not demonstrated clear benefits from scaling up pretraining. Orthogonally, end-to-end driving faces supervision bottleneck: each scene typically provides only single human trajectory, despite inherently multimodal futures. Prior works address this by generating multimodal trajectories through either discrete or continuous formulations. Discrete approaches such as VAD v2 (Chen et al., 2024) and Hydra-MDP (Li et al., 2024a) cluster trajectories into fixed vocabulary and predict scores reflecting safety and comfort; however, their expressiveness is fundamentally limited by the coverage and quality of anchor trajectories, leading to poor generalization in out-of-vocabulary scenarios (Liao et al., 2025). Alternatively, diffusion-based methods, including DiffusionDrive (Liao et al., 2025) and GoalFlow (Xing et al., 2025), model multimodal trajectory distributions via iterative sampling, which has shown strong generative capability. Nevertheless, these approaches remain constrained by supervision from single human trajectories per scene, inherently limiting the diversity of learned behaviors. In this work, we propose Drive-JEPA, an end-to-end autonomous driving framework that addresses the above two bottlenecks in unified way. First, we adapt V-JEPA (Assran et al., 2023; 2025) to the driving domain to learn planning-aligned predictive representations from large-scale raw videos, improving transfer beyond prior world-model pretraining. Second, we introduce multimodal trajectory distillation that distills knowledge from simulators into proposal-centric planner, providing diverse supervision beyond single human trajectories and enabling safer multimodal decision making. Specifically, our framework consists of three components: Driving Video Pretraining, Multimodal Trajectory Distillation, and Momentum-aware Trajectory Selection. In the first module, we curate large-scale driving video dataset and pretrain ViT-based vision encoder using V-JEPA (Assran et al., 2023; 2025), which learns predictive representation by predicting future latent with effective mode collapse prevention. In the second module, the waypoint-anchored proposals generation leverages deformable attention (Xia et al., 2022; Guo et al., 2025) to aggregate BEV features (Li et al., 2024b) at trajectory waypoints and refine proposals iteratively. To increase diversity, proposals are supervised using both human trajectories and simulator-generated multimodal trajectories that satisfy safety and comfort constraints, enabling effective knowledge distillation from the simulator. Finally, the selection module assigns scores to all candidates by predicting collision risk, traffic-rule compliance, and comfort, and further incorporates momentum-aware penalty to reduce frame-to-frame trajectory distortion. We validate Drive-JEPA on NAVSIM v1 (Dauner et al., 2024), NAVSIM v2 (Cao et al., 2025a) , and Bench2Drive (Jia et al., 2024). Drive-JEPA achieves 93.3 PDMS on NAVSIM v1 and 87.8 EPDMS on NAVSIM v2, setting new state of the art. Notably, with only single front-view camera and lightweight transformer planner, our V-JEPApretrained model outperforms prior work by 3 PDMS in the perception-free setting, highlighting the effectiveness of V-JEPA pretraining for planning. On Bench2Drive, the Multimodal Trajectory Distillation consistently improves driving quality, demonstrating the benefit of diverse supervision for generating safe, multimodal trajectories. Our contributions can be summarized as follows: We introduce V-JEPA pretraining to end-to-end autonomous driving, boosting performance in both perception-based and perception-free settings. We propose novel multimodal trajectories supervision to distill simulator knowledge to proposal-centric framework, generating diverse multimodal trajectories. We design momentum-aware trajectory selection module, enhancing driving comfort. Our method achieves new state of the art on NAVSIM v1 and NAVSIM v2. In addition, our method achieves strong performance on NAVSIM even without relying on perception annotations. 2. Related Work 2.1. End-to-end autonomous driving Early works such as ALVINN (Pomerleau, 1988) and PilotNet (Bojarski et al., 2016) leverage large-scale human driving data to learn policies that map sensor observations directly to control actions. However, these models often lack interpretability and can degrade due to issues such as causal confusion. To mitigate this, recent studies incorporate intermediate representations and auxiliary supervision to improve robustness. Transfuser (Chitta et al., 2022) fuses LiDAR and camera features in the BEV space and strengthens BEV features with BEV segmentation and 3D detection supervision. Going further, UniAD (Hu et al., 2023b) unifies the full stack of driving tasksincluding tracking, mapping, and motion predictionwithin single framework jointly optimized with planning. VAD (Jiang et al., 2023) explores compact vectorized scene representations for efficiency, while SparseDrive (Sun et al., 2025) proposes query-centric sparse structure as BEV-free alternative. DriveTransformer (Jia et al.) further improves efficiency by using small set of learned queries to aggregate multi-view image features. Moreover, DiffusionDrive (Liao et al., 2025) and GoalFlow (Xing et al., 2025) investigate diffusion-based end-to-end trajectory generation. Despite these advances, end-to-end driving remains challenging due to two fundamental requirements: capturing the spatiotemporal structure of complex scenes and modeling the inherently multimodal nature of driving behaviors. 2.2. World Models for End-to-end driving Video world models in autonomous driving predict how scenes evolve under ego actions. Recent progress in controllable video generation (Gao et al., 2024; Hu et al., 2023a) has enabled action-conditioned world models, suggesting their potential as learned simulators. Motivated by the idea 2 that realistic generation implies strong dynamics understanding, several works transfer world-model knowledge to endto-end driving. VaVIM (Bartoccioni et al., 2025) trains causal auto-regressive video model and extends it to generate ego trajectories, while Epona (Zhang et al., 2025) proposes hybrid diffusionauto-regressive predictor with dual-stream diffusion decoder for joint video and trajectory synthesis. However, both approaches remain computationally heavy due to pixel-level reconstruction. To improve efficiency, latent world models predict future features instead of pixels. LAW (Li et al.) integrates latent dynamics learning into end-to-end driving and achieves strong perception-free performance, and World4Drive (Zheng et al., 2025) further leverages multimodal video foundation models as richer latent targets. Nonetheless, these latent approaches have not clearly demonstrated benefits from scaling to large-scale video pretraining and may suffer from representation collapse. To overcome this, we adopt V-JEPA (Assran et al., 2025) as an efficient latent world model with built-in collapse prevention, enabling scalable video pretraining for end-to-end driving. 2.3. MultiModal Trajectories Generation In planning tasks such as manipulation and autonomous driving, given scenario often offers multiple action options, requiring effective multimodal modeling. Recently, VADv2 (Chen et al., 2024) and Hydra-MDP (Li et al., 2024a) introduce large fixed vocabulary of trajectories by discretizing and clustering the continuous action space. For each scene, this vocabulary provides diverse multimodal choice space for driving behaviors, and planning is performed by selecting trajectories based on predicted scores. However, this paradigm is fundamentally limited by the coverage of the vocabulary and cannot generalize to out-of-vocabulary scenes. Other works introduce diffusion models to generate multimodal trajectories. DiffusionDrive (Liao et al., 2025) guides the diffusion process using small fixed vocabulary, while GoalFlow (Xing et al., 2025) uses goal points as guidance during diffusion-based generation. Compared with computationally expensive diffusion-based methods, proposal-centric approaches such as iPad (Guo et al., 2025) iteratively refine set of trajectory proposals using efficient deformable attention. Compared with Hydra-MDP, iPads proposals can be viewed as an online-generated continuous vocabulary; however, its candidates are supervised purely by single human trajectory per scene, which limits diversity. In contrast, our Multimodal Trajectory Distillation breaks this limitation by distilling knowledge from simulator-generated trajectories. 3. Method 3.1. Preliminary End-to-end Autonomous Driving In the task of end-toend autonomous driving, the objective is to estimate the , 2 , w2 , . . . , wM future trajectory of the ego vehicle in the form of waypoints. , . . . , Formally, let It = {I 1 } denote the set of surrounding multi-view images captured at time step t. The model is expected to predict sequence of waypoints Wt = {w1 t, ψi }, where each waypoint wi t) represents the predicted BEV position and heading angle of the ego vehicle at time step + i. Here, denotes the number of future positions to be predicted. In addition, the model also takes as input the ego status of the vehicle, which includes the driving command (e.g., left, forward, right), speed, and acceleration. = (xi t, yi V-JEPA V-JEPA (Bardes et al., 2024) learns predictive video representations by estimating the latent representation of target view from masked view x, where subset of spatiotemporal patches are randomly dropped. The method adopts meta-architecture with an encoder Eθ() that extracts video features and predictor Pϕ() that predicts the representations at masked locations. The encoder and predictor are optimized jointly with min θ,ϕ,y Pϕ(y, Eθ(x)) sg(Eθ(y))1 , (1) where is learnable mask token indicating the dropped patch locations. The target branch uses stop-gradient operator sg() and an exponential moving average encoder Eθ (with parameters θ) to stabilize training and avoid representation collapse. The loss is computed only on masked positions. Both Eθ() and Pϕ() are instantiated as Vision Transformers (ViTs) (Dosovitskiy et al., 2020). 3.2. Driving Video Pretraining To enhance the representation for planning through selfsupervised video pretraining, prior works explored pixelspace driving world models and latent world models. While the former faces expensive computation and the latter fails to scale, we propose to leverage V-JEPA in large-scale driving video pretraining. Table 1. Comparison with previous perception-free planners. LAW World4Drive Epona Ours Encoder size Data scale PDMS 21M 20h 83.8 21M 20h 85.1 1.1B 128h 86. 307M 208h 89.0 Driving Video Dataset Curation and Scaling We initialize the ViT encoder with parameters released by V-JEPA 2 (Assran et al., 2025). To bridge the domain gap, we curate large-scale driving video dataset from three publicly available datasets: CoVLA (Arai et al., 2025), DrivingDojo (Wang et al., 2024), and OpenScene (Sima et al., 2023). All videos are captured from front-view camera and processed into 8-frame clips at resolution of 512 256 and 2 Hz. 3 Figure 2. Overview of the Drive-JEPA architecture. Driving Video Pretraining learns ViT encoder from large-scale driving videos using the self-supervised V-JEPA objective. Given the pretrained features, Waypoint-anchored Proposal Generation efficiently produces multiple trajectory proposals, whose distribution is guided by Multimodal Trajectory Distillation. Finally, Momentum-aware Trajectory Selection picks the final trajectory by accounting for cross-frame comfort. We adopt the V-JEPA objective to train the ViT encoder in self-supervised manner on this curated dataset. As shown in Table 1, thanks to the efficiency of the latent prediction task and effective mode-collapse prevention, we successfully scale pretraining to 208 hours with lower computational cost than prior methods. Perception-free End-to-End Autonomous Driving Following prior world-model-based end-to-end driving works, we adopt perception-free setting for evaluation, where the model is supervised solely by human trajectories without relying on perception annotations (Li et al.). Given spatiotemporal features extracted by the ViT encoder from the front-view image, future waypoints are predicted using transformer decoder with learnable queries. Given the front-view inputs 1 t1, we extract spatiotemporal features using the pretrained ViT encoder and denote them as Ft RNf D. We introduce learnable query embeddings RM D, each corresponding to one future waypoint. The transformer-based decoder (Vaswani et al., 2017) attends to Ft via cross-attention to produce and 1 = TransformerDecoder(Q, Ft), which is then mapped to predicted waypoints ˆWt = MLP(H), t, ˆyi t, ˆψi = (ˆxi , . . . , ˆwM } and each ˆwi where ˆWt = { ˆw1 t) represents the BEV position and heading at time step + i. The network is trained end-to-end using MSE loss between ˆWt and the ground-truth trajectory Wt. Despite its simplicity, this setup significantly outperforms prior methods  (Table 1)  , highlighting the effectiveness of V-JEPA-based driving video pretraining. 3.3. Waypoint-anchored Proposals Generation Building upon the strong representations from Driving Video Pretraining, we design planner that follows proposal-selection paradigm. As mentioned before, fixed vocabulary can be seen as proposals but suffers from discretization error. Inspired by iPad (Guo et al., 2025), we instead generate proposals online. Given the visual features Ft RNf and ego status at time t, we project the ego status by linear layer into an ego feature et R1D. The proposal queries are initialized as Q0 RNpM by adding et to learnable positional embeddings, where Np is the number of waypoint-trajectory proposals and is the number of future waypoints. We iteratively refine the proposal queries Qℓ for iterations. At iteration ℓ, an MLP decodes Qℓ into waypoint-trajectory proposals Wℓ = { (n) n=1, with Wℓ RNpM 3 and each waypoint (x, y, ψ). Using these explicit waypoint locations as anchors, we refine the queries by exchanging information among proposals and aggregating features from ℓ }Np 4 Ft around each predicted waypoint via lift-splat BEV feature sampling (Philion & Fidler, 2020). We then update the queries with lightweight MLP: these multimodal trajectories as pseudo-teachers to guide the proposals, instead of single human trajectory. The final Ltraj is defined as: Qℓ+1 = MLP (cid:16) (cid:17) WADA(Qℓ, Wℓ, Ft) , where WADA denotes Waypoint-anchored Deformable Attention (Xia et al., 2022). Because the final trajectory for planning is selected from WL, its distribution is critical. Given human trajectory Wt and the intermediate proposals { Wℓ}L1 ℓ=0 , naive way to guide Wℓ is using the minimum-over-N loss (Gupta et al., 2018) with discounted supervision across iterations: Ltraj = L1 (cid:88) ℓ=0 λLℓ1 min n{1,...,Np} (cid:13) (cid:13)Wt (n) (cid:13) ℓ (cid:13) (cid:13) (cid:13)2 , where λ = 0.1 down-weights earlier iterations to encourage coarse-to-fine refinement. However, in autonomous driving, there are often multiple valid choices beyond the single human trajectory for scene. This naive guidance method limits the multimodality of the proposals. We present our solution in the next section. 3.4. Multimodal Trajectories Distillation To alleviate sparse supervision from single human trajectory per scene, we distill knowledge from rule-based simulators. HydraMDP (Li et al., 2024a) performs hydradistillation by learning scores over fixed vocabulary. Instead, we let the simulator provide multimodal trajectory targets to guide the proposal distribution. Concretely, we start by building trajectory vocabulary following VADv2 and HydraMDP, but use the vocabulary for different purpose. We gather all trajectories in the training dataset, which includes more than 100k trajectories. Then we use clustering method, k-means (Douze et al., 2024), to select trajectory centers. We select 8192 centers as the trajectory vocabulary, balancing coverage and computational cost. For each scene in the training dataset, we select high quality multimodal trajectories from the vocabulary using rule-based simulators. Following NAVSIM v2 (Cao et al., 2025b; Caesar et al., 2021), we calculate the EPDM score for all trajectories. We refer to the Appendix for the detailed definition. Specifically, we first run PID controller to convert 8 waypoints into denser 41-point trajectory. Then, at each timestep, we replay other road agents, traffic lights, etc., and compute collisions and other metrics. The rule-based simulator in NAVSIM v2 is designed only for evaluation. We further improve the vectorized computation efficiency to meet large-scale offline scoring needs. After obtaining scores for all trajectories in the vocabulary across all scenes in the training dataset, we select group of multimodal trajectories Pt = {P 1 } for each scene by ranking and thresholding. During training, we use , . . . , Npseudo λLℓ(cid:16) (cid:88) ℓ= minWt (n) ℓ 2 + minP (n) ℓ 2 (cid:17) . (cid:88) Pt (2) Here, the min operator takes the minimum over the proposal index {1, . . . , Np} at iteration ℓ. Figure 3. Birds eye view of proposals. As shown in Figure 3, without Multimodal Trajectories Distillation (MTD), the proposals exhibit clear mode collapse; with MTD, they become multimodal. }Np 3.5. Momentum-aware Trajectory Selection To select the best trajectory among the proposal set for planning, we train neural scorer to evaluate the final proposals { (n) n=1. Concretely, we apply max pooling over the waypoint dimension to the proposal queries QL RNpM to obtain pooled features QL RNpD, which are then fed into multi-layer perceptron (MLP) to produce proposal scores RNp1. The scorer is trained with binary cross-entropy (BCE) loss: Lscore = BCE(S, ˆS), }. , . . . , Npseudo where BCE(x, y) = log (1 y) log(1 x). The supervision ˆS is derived from simulator-based EPDMS evaluation and is also used to define candidate pseudo-targets Pt = {P 1 While Multimodal Trajectory Distillation improves proposal diversity, it can amplify temporal inconsistency, increasing discomfort due to larger variation across adjacent time frames. To mitigate this, we make the score momentumaware by incorporating comfort term. Let ˆWt1 denote the selected trajectory at the previous time frame. We compute distortion-based comfort score Sc RNp1 by comparing ˆWt1 with each current proposal in { (n) n=1, and recalibrate the learned score via }Np 7 + Sc 8 . where the weights are following NAVSIM v2. Finally, the selected trajectory is formalized as ˆWt = (n) , = arg max n{1,...,Np} Sn, where Sn denotes the recalibrated score of proposal (n) . 3.6. Losses In end-to-end driving tasks, it is important to add auxiliary tasks to enhance the models environment understanding capability, e.g., BEV map segmentation, 3D object detection, and tracking. However, these traditional dense understanding tasks are computationally intensive. Here we use the lightweight auxiliary tasks instead (Guo et al., 2025), which contain rich spatiotemporal signals and are compatible with the proposal-centric design. We use two auxiliary tasks: proposal-centric mapping and collision prediction. For the first task, our model predicts the on-road and on-route probabilities of the proposed waypoints in Wℓ, denoted as RNpM 2. The proposal-centric mapping loss is Lmap = BCE(R, ˆR). For proposal-centric collision prediction, we estimate the collision probability Av of waypoints in Wℓ using logreplay simulation. This task not only requires the model to detect surrounding objects, but also to understand their moving pattern. The proposal-centric collision loss is Lcolli = Av ˆAv + 0.1 BCE(Av, ˆAv). Our Drive-JEPA is end-to-end differentiable. The training loss is defined as: = Ltraj + wscoreLscore + wmapLmap + wcolliLcolli, where wscore = 1, wmap = 2 and wcolli = 1. 4. Experiments 4.1. Dataset and Metrics We evaluate our method on three benchmarks, including NAVSIM v1, NAVSIM v2 and Bench2Drive. NAVSIM v1 NAVSIM is real-world dataset based on OpenScene (Sima et al., 2023) and NuPlan (Caesar et al., 2021). It contains 103k and 12k diverse and challenging driving scenarios for model training (Navtrain) and evaluation (Navtest), and introduces simulation-based metrics to better review closed-loop planning capability through open-loop evaluation. During evaluation, the output trajectory is evaluated by simulator to get rule-based simulation metric scores, including No at-fault Collisions(NC), Drivable Area Compliance(DAC), Time to Collision with bounds(TTC), Ego Progress(EP) and Comfort(C). The final PDM Score(PDMS) is derived by aggregating these metrics: DM = DAC 5 (EP + C) + 2 12 (3) NAVSIM v2 Compared with NAVSIM v1, NAVSIM v2 strengthens driving-quality evaluation by extending PDMS to EPDMS with richer rule-compliance and comfort assessment. It adds Driving Direction Compliance (DDC), Traffic Light Compliance (TLC), and Lane Keeping (LK) to better capture traffic-rule adherence, and replaces the original comfort term with History Comfort (HC) and Extended Comfort (EC) to evaluate both shortand longer-horizon smoothness. Bench2Drive Bench2Drive (Jia et al., 2024) is closedloop evaluation benchmark based on CARLA (Dosovitskiy et al., 2017), designed to assess end-to-end autonomous driving systems in interactive urban scenarios. The evaluation includes 220 routes spanning 44 diverse, interactive scenarios. Official metrics include Driving Score (DS), Success Rate (SR), Efficiency and Comfortness, which collectively measure navigation performance, safety, and rule adherence. For detailed metric definitions, see Appendix A. 4.2. Implementation Details In the Driving Video Pretraining stage, we use 8 H800 GPUs and train for 50 epochs, which takes about 3 days. The Drive-JEPA planners are trained on two NVIDIA A30 GPUs for 20 epochs with total batch size of 64, using the Adam (Kingma, 2014) optimizer with learning rate of 1 104, while the ViT encoder uses learning rate of 1 105. We set Np = 32 proposals, which is efficient while achieving strong performance in our ablation studies. We use only the front-view camera, resized to 512 256. Despite using fewer input images than prior methods and no LiDAR, we outperform them. See Appendix for details. 4.3. Main Results Results on NAVSIM v1 As shown in Table 2, compared with previous methods, Drive-JEPA achieves the best PDMS with ResNet34. When using ViT/L, Drive-JEPA is only second to DriveSuprim (Yao et al., 2025), which uses advanced data augmentations. Notably, while maintaining high safe metrics, such as NC, DC and TTC, our method achieves the best Ego Progress, resulting in an assertive driving style. Perception-free End-to-end Autonomous Driving We also evaluated our method in perception-free setting, where we use simple decoder with the pretrained ViT encoder as described in 3.2. As shown in Table 2, our method surpasses previous methods by large margin, regardless of backbone size. The PDMS is even close to SOTA methods that rely on perception annotations, demonstrating the strength of V-JEPA pretraining. Results on NAVSIM v2 NAVSIM v2 has more sophisticated metrics than NAVSIM v1. Our method still outperformances all prior methods. While prior methods struggle with EC, our method performs quite well while achieving good results on safety metrics, traffic rule compliance and Ego Progress. Results on Bench2Drive Bench2Drive evaluates au6 Table 2. Quantitative comparisons on NAVSIM v1. We indicate the best and second best with bold and underlined respectively. The first block shows results for perception-free setting. Method Backbone Inputs NC DAC EP TTC PDMS LAW (Li et al.) World4Drive (Zheng et al., 2025) Epona (Zhang et al., 2025) Ours Transfuser (Chitta et al., 2022) HydraMDP (Li et al., 2024a) HydraMDP++ (Li et al., 2025) DiffusionDrive (Liao et al., 2025) GoalFLow (Xing et al., 2025) DriveDPO (Shang et al., 2025) DriveSuprim (Yao et al., 2025) iPad (Guo et al., 2025) Drive-JEPA(Ours) Hydra-MDP (Li et al., 2024a) iPad (Guo et al., 2025) DriveSuprim (Yao et al., 2025) Drive-JEPA(Ours) resnet34 resnet34 ViT/G ViT/L ResNet34 ResNet34 ResNet34 ResNet34 ResNet34 ResNet34 ResNet34 ResNet34 ResNet34 ViT/L ViT/L ViT/L ViT/L & & Camera Camera & & & & & & Camera Camera Camera & Camera Camera Camera 97.4 97.4 97.9 98. 97.7 98.3 97.6 98.2 98.4 98.5 97.8 98.4 98.2 98.4 99.2 98.6 99.1 93.3 94.3 95.1 96.2 92.8 96.0 96.0 96.2 98.3 98.1 97.3 97.9 98.0 97.7 97.4 98.6 98.2 78.8 79.9 80.4 82. 79.2 78.7 80.4 82.2 85.0 84.3 86.7 87.4 88.8 85.0 87.8 91.3 90.8 100 100 99.9 100 100 100 100 100 100 100 100 99.9 99.9 100 99.7 100 99.9 91.9 92.8 93.8 95. 92.8 94.6 93.1 94.7 94.6 94.8 93.6 94.9 94.2 94.5 96.3 95.5 95.9 83.8 85.1 86.2 89.0 84.0 86.5 86.6 88.1 90.3 90.0 89.9 91.1 91.5 89.9 91.7 93.5 93.3 Table 3. Quantitative comparisons on NAVSIM v2. Method Backbone NC DAC DDC TL EP TTC LK HC EC EPDMS ResNet34 Transfuser HydraMDP++ ResNet34 ResNet34 DriveSuprim iPad ResNet34 Drive-JEPA(Ours) ResNet34 HydraMDP++ iPad DriveSuprim Drive-JEPA(Ours) ViT/L ViT/L ViT/L ViT/L 96.9 97.2 97.5 98.7 98. 98.5 98.7 98.4 98.4 89.9 97.5 96.5 97.8 97.4 98.5 98.0 98.6 98.6 97.8 99.4 99.4 99.1 99.0 99.5 98.9 99.6 99.1 99.7 99.6 99.6 99.8 99. 99.7 99.8 99.8 99.8 87.1 83.1 88.4 84.0 83.5 87.4 86.6 90.5 88.4 95.4 96.5 96.6 98.0 98.0 97.9 98.3 97.8 97.8 92.7 94.4 95.5 96.0 96. 95.8 97.2 97.0 97.6 98.3 98.2 98.3 98.0 98.1 98.2 98.3 98.3 97.9 87.2 70.9 77.0 68.2 85.6 75.7 74.6 78.6 84.8 76.7 81.4 83.1 84.1 85. 85.6 85.8 87.1 87.8 Table 4. Quantitative comparisons on Bench2Drive. Method Effi. Comf. SR DS AD-MLP UniAD VAD TCP DriveDPO iPad DriveTransformer Ours 48.45 129.21 157.94 76.54 166.80 153.83 100.64 157.85 22.63 43.58 46.01 18.08 26.79 35.51 20.78 30.24 0.00 16.36 15.00 30.00 30.62 33.18 35.01 36.82 18.05 45.81 42.35 59.90 62.02 60.52 63.46 64. tonomous agents in close-loop simulation. Our method achieves the best Driving Score with very competitive Efficiency. Our method surpasses another proposal-centric method iPad by 4 in Driving Score, identifying the effectiveness of Multimodal Trajectories Distillation. 7 4.4. Ablation Studies Ablation studies on proposed modules We first conducted ablation studies on the proposed modules: M1: V-JEPA 2 checkpoints, M2: Driving Video Pretraining, M3: Multimodal Trajectories Distillation, and M4: Momentumaware Trajectory Selection. As shown in Table 5, replacing ResNet34 with the ViT released by V-JEPA 2 (M1) improves EPDMS. Our Driving Video Pretraining further boosts performance by reducing the domain gap (M2). After adding M3, the framework achieves better (Diversity) (Liao et al., 2025) and overall metrics, which is also supported by the validation score curve in Figure 5. However, the increased diversity results in worse EC. Finally, adding M4 not only largely boosts EC to 84.8, but also sets new best record on EPDMS. Ablation study on the number of Pseudo Teacher TrajecTable 5. Ablation study of the proposed modules on NAVSIM v2. M1 M2 M3 M4 NC DAC DDC TL EP TTC LK HC EC EPDMS (cid:37) (cid:37) (cid:37) (cid:37) 98.7 (cid:33) (cid:37) (cid:37) (cid:37) 98.7 (cid:37) (cid:33) (cid:37) (cid:37) 98.3 (cid:37) (cid:33) (cid:33) (cid:37) 98.5 (cid:37) (cid:33) (cid:33) (cid:33) 98.4 25% 84.1 21% 85.8+1.7 24% 86.1+2.0 40% 84.5+0.4 40% 87.8+3. 96.0 97.2 97.7 97.6 97.6 84.0 86.6 89.1 89.1 88.4 99.8 99.8 99.9 99.8 99.8 98.0 98.3 98.1 97.8 97.9 98.0 98.3 97.7 97.9 97.8 97.8 98.0 98.1 98.6 98. 99.1 98.9 99.1 99.1 99.1 68.2 74.6 69.7 47.9 84.8 Figure 4. Qualitative comparison of trajectories by different models in front-facing camera and birds eye view on different driving scenarios. Trajectories are shown for: Human Trajectory, Drive-JEPA, Transfuser. iPad, tories. As shown in Table 6, we tried Npseudo = 0, 1, 2, 4, 8 pseudo-teacher trajectories. While the correlation between Npseudo and EPDMS is not very strong, using pseudoteacher trajectories consistently performs better than using none (Npseudo = 0). Table 6. Ablation on number of pseudo-teacher trajectories. Npseudo 0 2 4 8 EPDMS 87.2 87. 87.7 87.8 87.5 Table 7. Comparison with mainstream vision pretraining methods. Vision Encoder Size PDMS Epona (Zhang et al., 2025) ViT/G ResNet34 ImageNet (Deng et al., 2009) DepthAnything (Yang et al., 2024) ViT/L ViT/L MAE (He et al., 2022) ViT/L Dinov2 (Oquab et al., 2023) ViT/L Sigclip (Zhai et al., 2023) ViT/L V-JEPA 2 (Assran et al., 2025) Ours ViT/L 86.2 76.0 - - 76.1 83.4 86.1 89.0 Ablation on Driving Video Pretraining. We use the same simple decoder with encoders pretrained by mainstream pretraining methods. As shown in Table 7, V-JEPA 2 performs the best among them. MAE and DepthAnything could not converge. This highlights the strength of the V-JEPA objective for video pretraining. In this work, we curated large driving video dataset. The ViT/L encoder trained on this dataset with the V-JEPA objective further boosts performance, surpassing the SOTA Epona by 3 PDMS. Figure 5. Multimodal Trajectory Distillation improves PDM score. 5. Conclusion We proposed Drive-JEPA, an end-to-end driving framework that combines V-JEPA video pretraining with multimodal trajectory distillation to mitigate imitation-learning modal collapse. Pretraining ViT encoder on large-scale driving videos yields strong planning representations, enabling simple decoder to achieve competitive perception-free performance. Distilling simulator-guided pseudo-teacher trajectories improves proposal diversity, and momentumaware selection further enhances temporal stability and comfort. Drive-JEPA achieves state-of-the-art results on NAVSIM v1/v2 and improves closed-loop performance on Bench2Drive. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Arai, H., Miwa, K., Sasaki, K., Watanabe, K., Yamaguchi, Y., Aoki, S., and Yamamoto, I. Covla: Comprehensive vision-language-action dataset for autonomous driving. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 19331943. IEEE, 2025. Assran, M., Duval, Q., Misra, I., Bojanowski, P., Vincent, P., Rabbat, M., LeCun, Y., and Ballas, N. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15619 15629, 2023. Assran, M., Bardes, A., Fan, D., Garrido, Q., Howes, R., Muckley, M., Rizvi, A., Roberts, C., Sinha, K., Zholus, A., et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. Bardes, A., Garrido, Q., Ponce, J., Chen, X., Rabbat, M., LeCun, Y., Assran, M., and Ballas, N. Revisiting feature prediction for learning visual representations from video. arXiv preprint arXiv:2404.08471, 2024. Bartoccioni, F., Ramzi, E., Besnier, V., Venkataramanan, S., Vu, T.-H., Xu, Y., Chambon, L., Gidaris, S., Odabas, S., Hurych, D., et al. Vavim and vavam: Autonomous driving through video generative modeling. arXiv preprint arXiv:2502.15672, 2025. Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller, U., Zhang, J., et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016. Caesar, H., Kabzan, J., Tan, K. S., Fong, W. K., Wolff, E., Lang, A., Fletcher, L., Beijbom, O., and Omari, S. nuplan: closed-loop ml-based planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.11810, 2021. Chen, S., Jiang, B., Gao, H., Liao, B., Xu, Q., Zhang, Q., Huang, C., Liu, W., and Wang, X. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning. arXiv preprint arXiv:2402.13243, 2024. Chitta, K., Prakash, A., Jaeger, B., Yu, Z., Renz, K., and Geiger, A. Transfuser: Imitation with transformer-based sensor fusion for autonomous driving. IEEE transactions on pattern analysis and machine intelligence, 45(11): 1287812895, 2022. Dauner, D., Hallgarten, M., Li, T., Weng, X., Huang, Z., Yang, Z., Li, H., Gilitschenski, I., Ivanovic, B., Pavone, M., et al. Navsim: Data-driven non-reactive autonomous vehicle simulation and benchmarking. Advances in Neural Information Processing Systems, 37:2870628719, 2024. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., and Koltun, V. Carla: An open urban driving simulator. In Conference on robot learning, pp. 116. PMLR, 2017. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazare, P.-E., Lomeli, M., Hosseini, L., and Jegou, H. The faiss library. 2024. Gao, S., Yang, J., Chen, L., Chitta, K., Qiu, Y., Geiger, A., Zhang, J., and Li, H. Vista: generalizable driving world model with high fidelity and versatile controllability. Advances in Neural Information Processing Systems, 37: 9156091596, 2024. Guo, K., Liu, H., Wu, X., Pan, J., and Lv, C. ipad: Iterative proposal-centric end-to-end autonomous driving. arXiv preprint arXiv:2505.15111, 2025. Cao, W., Hallgarten, M., Li, T., Dauner, D., Gu, X., Wang, C., Miron, Y., Aiello, M., Li, H., Gilitschenski, I., Ivanovic, B., Pavone, M., Geiger, A., and Chitta, K. Pseudo-simulation for autonomous driving. In Conference on Robot Learning (CoRL), 2025a. Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., and Alahi, A. Social gan: Socially acceptable trajectories with generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 22552264, 2018. Cao, W., Hallgarten, M., Li, T., Dauner, D., Gu, X., Wang, C., Miron, Y., Aiello, M., Li, H., Gilitschenski, I., et al. Pseudo-simulation for autonomous driving. arXiv preprint arXiv:2506.04218, 2025b. He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. 9 Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J., and Corrado, G. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023a. Hu, Y., Yang, J., Chen, L., Li, K., Sima, C., Zhu, X., Chai, S., Du, S., Lin, T., Wang, W., et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1785317862, 2023b. Jia, X., You, J., Zhang, Z., and Yan, J. Drivetransformer: Unified transformer for scalable end-to-end autonomous driving. In The Thirteenth International Conference on Learning Representations. Jia, X., Yang, Z., Li, Q., Zhang, Z., and Yan, J. Bench2drive: Towards multi-ability benchmarking of closed-loop endto-end autonomous driving. In NeurIPS 2024 Datasets and Benchmarks Track, 2024. Jiang, B., Chen, S., Xu, Q., Liao, B., Chen, J., Zhou, H., Zhang, Q., Liu, W., Huang, C., and Wang, X. Vad: Vectorized scene representation for efficient autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 83408350, 2023. Kingma, D. P. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Li, K., Li, Z., Lan, S., Xie, Y., Zhang, Z., Liu, J., Wu, Z., Yu, Z., and Alvarez, J. M. Hydra-mdp++: Advancing end-to-end driving via expert-guided hydra-distillation. arXiv preprint arXiv:2503.12820, 2025. Li, Y., Fan, L., He, J., Wang, Y., Chen, Y., Zhang, Z., and Tan, T. Enhancing end-to-end autonomous driving with latent world model. In The Thirteenth International Conference on Learning Representations. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Philion, J. and Fidler, S. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In European conference on computer vision, pp. 194 210. Springer, 2020. Pomerleau, D. A. Alvinn: An autonomous land vehicle in neural network. Advances in neural information processing systems, 1, 1988. Shang, S., Chen, Y., Wang, Y., Li, Y., and Zhang, Z. Drivedpo: Policy learning via safety dpo for end-to-end autonomous driving. arXiv preprint arXiv:2509.17940, 2025. Sima, C., Tong, W., Wang, T., Chen, L., Wu, S., Deng, H., Gu, Y., Lu, L., Luo, P., Lin, D., and Li, H. Scene as occupancy. 2023. Sun, W., Lin, X., Shi, Y., Zhang, C., Wu, H., and Zheng, S. Sparsedrive: End-to-end autonomous driving via sparse scene representation. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 87958801. IEEE, 2025. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang, Y., Cheng, K., He, J., Wang, Q., Dai, H., Chen, Y., Xia, F., and Zhang, Z.-X. Drivingdojo dataset: Advancing interactive and knowledge-enriched driving world model. Advances in Neural Information Processing Systems, 37: 1302013034, 2024. Li, Z., Li, K., Wang, S., Lan, S., Yu, Z., Ji, Y., Li, Z., Zhu, Z., Kautz, J., Wu, Z., et al. Hydra-mdp: End-to-end multimodal planning with multi-target hydra-distillation. arXiv preprint arXiv:2406.06978, 2024a. Xia, Z., Pan, X., Song, S., Li, L. E., and Huang, G. Vision transformer with deformable attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 47944803, 2022. Li, Z., Wang, W., Li, H., Xie, E., Sima, C., Lu, T., Yu, Q., and Dai, J. Bevformer: learning birds-eye-view representation from lidar-camera via spatiotemporal transformers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024b. Liao, B., Chen, S., Yin, H., Jiang, B., Wang, C., Yan, S., Zhang, X., Li, X., Zhang, Y., Zhang, Q., et al. Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1203712047, 2025. Xing, Z., Zhang, X., Hu, Y., Jiang, B., He, T., Zhang, Q., Long, X., and Yin, W. Goalflow: Goal-driven flow matching for multimodal trajectories generation in end-to-end autonomous driving. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 16021611, 2025. Yang, L., Kang, B., Huang, Z., Xu, X., Feng, J., and Zhao, H. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1037110381, 2024. 10 Yao, W., Li, Z., Lan, S., Wang, Z., Sun, X., Alvarez, J. M., and Wu, Z. Drivesuprim: Towards precise trajectory selection for end-to-end planning. arXiv preprint arXiv:2506.06659, 2025. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Zhang, K., Tang, Z., Hu, X., Pan, X., Guo, X., Liu, Y., Huang, J., Yuan, L., Zhang, Q., Long, X.-X., Cao, X., and Yin, W. Epona: Autoregressive diffusion world model for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. Zheng, Y., Yang, P., Xing, Z., Zhang, Q., Zheng, Y., Gao, Y., Li, P., Zhang, T., Xia, Z., Jia, P., et al. World4drive: Endto-end autonomous driving via intention-aware physical latent world model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 28632 28642, 2025. A. Metrics A.1. Extended Predictive Driver Model Score (EPDMS) NAVSIM v2 (Cao et al., 2025b) extends the PDMS metric from NAVSIM v1 to EPDMS, which is defined as: EP DM = DAC DDC LC 5 (EP + C) + 2 (LK + HC + EC) 16 (4) The subscores include: No at-fault Collision (NC), Drivable Area Compliance (DAC), Driving Direction Compliance (DDC), Traffic Light Compliance (TLC), Ego Progress (EP), Time to Collision (TTC), Lane Keeping (LK), History Comfort (HC), and Extended Comfort (EC). Among these, DDC, TLC, LK, HC, and EC are newly introduced in NAVSIM v2. We summarize them below; for full computation details, we refer readers to NAVSIM v2 (Cao et al., 2025b). Driving Direction Compliance (DDC). The ego vehicle must follow the legal direction of travel within lanes and avoid driving in oncoming lanes outside intersections. Traffic Light Compliance (TLC). This score evaluates whether the ego vehicle obeys traffic-light phases and enters intersections only under valid green signal. Lane Keeping (LK). This score measures whether the ego vehicle stays near the centerline of the current lane and avoids lingering between adjacent lanes, while discouraging hesitant half-commit lane-change probes. History Comfort (HC). To better assess ride comfort, we prepend the predicted trajectory with short segment of the human drivers recent motion using fixed padding length of 1.5 seconds. The resulting continuous trajectory is then evaluated using the same comfort metric adopted in the nuPlan framework (Caesar et al., 2021). Extended Comfort (EC). This score checks that the predicted motion remains smooth across consecutive time steps. A.2. Diversity (D) We report this metric in Table 5. Following DiffusionDrive (Liao et al., 2025), it is defined as: = 1 1 Np Np (cid:88) Area i=1 Area (cid:16) Wti (cid:83)Np (cid:16) Wti (cid:83)Np j=1 j=1 (cid:17) (cid:17) . Wtj Wtj To compute the IoU between trajectories, we rasterize each trajectory polyline into 2D occupancy mask by buffering the polyline with 2-meter width (i.e., 2 m-thick corridor) and projecting it onto grid. A.3. Metrics used in Bench2Drive We briefly describe the four metrics used in Bench2Drive; for full computation details, we refer readers to Bench2Drive (Jia et al., 2024). Success Rate (SR). The proportion of routes completed successfully within the allotted time and without traffic violations. Driving Score (DS). This score follows the official CARLA (Dosovitskiy et al., 2017) metric, combining route completion with penalties for infractions. Efficiency. CARLA includes check for excessively low speed by comparing the ego vehicles speed with nearby traffic. Comfort. This metric follows nuPlans (Caesar et al., 2021) smoothness (comfort) protocol, which evaluates longitudinal acceleration (min/max), the maximum absolute lateral acceleration, yaw rate, yaw acceleration, longitudinal jerk, and the maximum magnitude of the jerk vector. B. More details Threshold. In Section 3.4, we use threshold on simulated EPDMS to select trajectories from the vocabulary. The threshold is set to 0.95. In many scenes, more than Npseudo trajectories exceed this threshold; during training, we uniformly sample Npseudo trajectories at random from this high-quality subset. Resolution. As shown in Table 8, HydraMDP++, DriveSuprim, and GoalFlow follow Transfuser in using an input resolution of 1024 256, formed by stacking the front, left, and right camera images. iPad uses higher resolution (768 432) and four camera views (front, left, right, and back). Our setting uses only the front camera at 512 256. We include both It and It1, resulting in an input tensor of 2 512 256. 12 Table 8. Input image resolution."
        },
        {
            "title": "Method",
            "content": "Transfuser HydraMDP++ DriveSuprim GoalFlow iPad"
        },
        {
            "title": "Input image resolution",
            "content": "1024256 1024256 1024256 1024256 4768432 2512 C. More visualization Figure 6. Birds eye view of proposals. Without Mutimodal Trajectory Distillation (MTD), the proposals collapse into one mode. With MTD, the proposals show multimodal distribution."
        }
    ],
    "affiliations": [
        "Nanjing University of Aeronautics and Astronautics",
        "Purdue University",
        "Virginia Tech",
        "XPENG Motors"
    ]
}