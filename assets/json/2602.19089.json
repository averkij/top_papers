{
    "paper_title": "Ani3DHuman: Photorealistic 3D Human Animation with Self-guided Stochastic Sampling",
    "authors": [
        "Qi Sun",
        "Can Wang",
        "Jiaxiang Shang",
        "Yingchun Liu",
        "Jing Liao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current 3D human animation methods struggle to achieve photorealism: kinematics-based approaches lack non-rigid dynamics (e.g., clothing dynamics), while methods that leverage video diffusion priors can synthesize non-rigid motion but suffer from quality artifacts and identity loss. To overcome these limitations, we present Ani3DHuman, a framework that marries kinematics-based animation with video diffusion priors. We first introduce a layered motion representation that disentangles rigid motion from residual non-rigid motion. Rigid motion is generated by a kinematic method, which then produces a coarse rendering to guide the video diffusion model in generating video sequences that restore the residual non-rigid motion. However, this restoration task, based on diffusion sampling, is highly challenging, as the initial renderings are out-of-distribution, causing standard deterministic ODE samplers to fail. Therefore, we propose a novel self-guided stochastic sampling method, which effectively addresses the out-of-distribution problem by combining stochastic sampling (for photorealistic quality) with self-guidance (for identity fidelity). These restored videos provide high-quality supervision, enabling the optimization of the residual non-rigid motion field. Extensive experiments demonstrate that \\MethodName can generate photorealistic 3D human animation, outperforming existing methods. Code is available in https://github.com/qiisun/ani3dhuman."
        },
        {
            "title": "Start",
            "content": "ANI3DHUMAN: Photorealistic 3D Human Animation with Self-guided Stochastic Sampling Qi Sun1 Can Wang"
        },
        {
            "title": "Jiaxiang Shang Yingchun Liu",
            "content": "Jing Liao1* 1City University of Hong Kong 6 2 0 2 2 2 ] . [ 1 9 8 0 9 1 . 2 0 6 2 : r Figure 1. Given reference human image and target SMPL mesh sequence, our method synthesizes photorealistic 3D human animation. Unlike the previous state-of-the-art (SOTA) methods (e.g., LHM [58] (top-right)) that are limited to rigid motion, our ANI3DHUMAN (bottom) can further generate high-fidelity nonrigid dynamics, capturing the natural flow of the dress."
        },
        {
            "title": "Abstract",
            "content": "Current 3D human animation methods struggle to achieve photorealism: kinematics-based approaches lack non-rigid dynamics (e.g., clothing dynamics), while methods that leverage video diffusion priors can synthesize non-rigid motion but suffer from quality artifacts and identity loss. To overcome these limitations, we present ANI3DHUMAN, framework that marries kinematics-based animation with video diffusion priors. We first introduce layered motion representation that disentangles rigid motion from residual non-rigid motion. Rigid motion is generated by kinematic method, which then produces coarse rendering to guide the video diffusion model in generating video seCorresponding author. quences that restore the residual non-rigid motion. However, this restoration task, based on diffusion sampling, is highly challenging, as the initial renderings are out-ofdistribution, causing standard deterministic ODE samplers to fail. Therefore, we propose novel self-guided stochastic sampling method, which effectively addresses the outof-distribution problem by combining stochastic sampling (for photorealistic quality) with self-guidance (for identity fidelity). These restored videos provide high-quality supervision, enabling the optimization of the residual nonrigid motion field. Extensive experiments demonstrate that ANI3DHUMAN can generate photorealistic 3D human animation, outperforming existing methods. Code is available in https://github.com/qiisun/ani3dhuman. 1 1. Introduction The importance of 3D digital humans has been growing across various applications, including AR/VR [14], gaming [4], education [34], and healthcare [32]. This has motivated numerous research efforts aimed at automatically animating 3D humans [51, 58, 66]. In traditional 3D human animation, researchers drive the motion with either kinematics-based methods, such as skeletons [51, 66] and SMPL meshes [58], or physics-based methods [15, 91]. Kinematics-based methods offer controllable way to describe rigid human movement, but are challenging to model non-rigid deformations, such as soft body movements, clothing, and hair, which involve complex, flexible changes in shape and structure. Physics-based methods [15, 82, 91] focus on modeling the complex dynamic effects of clothing interacting with human bodies. While effective at generating natural and realistic non-rigid garment animations, these methods require high computational resources and involve significant complexity in specifying physical models and numerous physical parameters. Recent advancements in video diffusion models [3, 74] offer compelling alternative, inherently modeling both rigid and non-rigid dynamics without physical simulation. Using score distillation sampling [2, 56, 67] to distill motion suffers from unsatisfactory results like over-saturation. Another approach first uses diffusion model to generate videos, and then directly reconstructs 3D animation from them. However, this pipeline suffers from distinct failure modes originating from video generation: 1) When relying on multi-view diffusion models [13, 40, 83], the reconstruction quality is limited by the scarcity of 4D training data, which leads to low-quality video generation. 2) When using pose-driven 2D video models, such as PERSONA [66], the generated videos suffer from identity loss, as the model hallucinates different appearance for each video. To address these issues, we propose ANI3DHUMAN, framework that marries kinematics-based animation with 2D video diffusion priors. We first design layered motion representation that adopts mesh rigging as strong motion prior, augmented by deformation field [5, 79] for modeling non-rigid motion. In contrast to direct reconstruction methods [66, 83, 87], our key insight is to leverage kinematics as strong structural and identity prior. Specifically, we first render coarse video from our mesh-rigged animation, which provides powerful, view-consistent constraint on the humans identity and structure that previous methods lack. We then use pretrained video diffusion model to restore this rendering, tasking it to synthesize realistic nonrigid dynamics (e.g., clothing flow) onto the existing structure rather than inventing an identity. These restored videos provide high-fidelity photorealistic supervision to optimize the residual motion field. The restoration, however, is highly non-trivial. The iniFinally, tial renderings are unrealistic and thus out-of-distribution (OOD) for the pretrained video model. Framing this restoration as diffusion sampling task, we find that standard deterministic ODE sampling methods fail on this OOD data, producing unsatisfying results (detailed in Fig. 3). Therefore, our core technical contribution is self-guided stochastic sampling, novel restoration method designed for this task. Motivated by the robustness of stochastic sampling [26, 48] in correcting OOD samples, we develop stochastic counterpart for deterministic flow matching. To solve the identity loss that occurs during this high-noise restoration process, we further introduce self-guidance mechanism. Inspired by posterior sampling (DPS) [8], this guidance modifies the sampling process to ensure the posterior mean remains faithful to the input in preserved regions. to robustly use these high-fidelity restored videos for 4D optimization, we must account for the inherent inconsistency across multiple samples. We employ diagonal view-time sampling as an efficient strategy to provide coherent optimization signal by minimizing the number of generative trajectories, enabling sharp reconstruction. In summary, ANI3DHUMAN achieves photorealistic human animation results with the novel self-guided stochastic sampling algorithm, generates high-fidelity photorealistic non-rigid dynamics, capturing the natural flow of the dress (as shown in Fig. 1), significantly surpassing the stateof-the-art methods. We provide extensive analysis and ablations that validate the critical roles of both stochasticity (for photorealistic quality) and self-guidance (for identity fidelity) in our sampler. These experiments demonstrate that sampling method is an essential and effective technique for restoring OOD renderings into high-quality, identitypreserving videos for 4D supervision. 2. Related Work 2.1. Traditional 3D Human Animation Kinematics-based methods. Kinematics-based methods [61] efficiently driving character motion by controlling skeletal poses. Among these, Linear Blend Skinning (LBS) [35] is core and widely used technique, deforming the surface mesh through weighted average of bone transformations. This paradigm was significantly advanced by parametric models like SMPL [47, 54], which extends LBS with identity-driven shape variations and pose-dependent shape variations. Such models are pivotal for tasks like motion retargeting, adapting existing motions to new characters. This reliance on an explicit, mesh-driven structure continues in current human implicit field and 3DGS methods [24, 30, 46, 57, 58, 77], which typically achieve animation via their corresponding meshes. While the rendering quality of explicit meshes may lag behind modern video generation, this mesh-based approach provides significant 2 Figure 2. Pipeline overview. Our ANI3DHUMAN animates 3D Gaussian (reconstructed with LHM [58] from the reference image) with mesh sequence. Our layered motion combines mesh-rigged motion with residual field for non-rigid dynamics. coarse rendering from the rigid motion is restored to high-quality video using our self-guided stochastic sampling. This restored video then provides supervision to progressively optimize the residual motion field. (rigid) motion prior. We leverage this by incorporating the mesh-rigged motion into our layered motion. Physics-based animation. Another line of research [15, 70, 91] uses physics simulation to enhance visual realism, particularly for modeling the complex dynamic effects of clothing in interaction with human bodies. These methods often require modeling the garment as separate mesh to simulate its physical interactions with the body. For instance, PhysAvatar [91] adopts the Codimensional Incremental Potential Contact (C-IPC) solver [37], which uses log-barrier penalty function for its robustness in handling complicated bodycloth collisions. However, this pipeline demands heavy runtime cost and extensive preprocessing, including the creation of separate garment meshes and the meticulous tuning of numerous physical parameters (e.g., stiffness, damping). To avoid this modeling and simulation complexity, we use general motion field to represent the complex non-rigid deformation, and video diffusion prior for effective supervision. 2.2. Video Diffusion Prior for 3D Animation Score distillation sampling (SDS). Recent advances in video diffusion models [3, 6, 7, 16, 18, 21, 31, 74, 84, 86] have inspired research on distilling 4D dynamic scenes from pre-trained models. MAV3D [67] was an early text-todynamic object work using hexplane representation. Following methods leverage 3D Gaussian Splatting for highfidelity rendering [1, 38, 39, 41, 72, 78]. Methods like DG4D [62] and Disco4D [53] use single-view videos as supervision alongside SDS with 3D-aware image diffusion to enhance unseen views. However, SDS is known to suffer from lengthy optimization times [73], unstable training dynamics [19], and unsatisfactory generation quality, such as oversmoothing or over-saturation [49]. Photometric reconstruction with generated videos. Another line of research in 3D animation is photometric reconstruction from generated videos, optimizing generic 4D representation without cumbersome SDS objective. This strategy primarily follows two paths. The first employs multi-view video diffusion models [25, 36, 40, 65, 71, 75, 81, 83, 87, 89], which are fine-tuned to synchronize video generation across views. To address human animation specifically, CharacterShot [13] enhances imageto-video diffusion transformer [86] with 2D pose conditions and extends to multi-view setting. While conceptually sound, these models are fundamentally limited by the scarcity of high-quality 4D training data, and their generation quality lags significantly behind general 2D video models. The second strategy, used by PERSONA [66], employs pose-controlled 2D video diffusion [90] to create training data. It then uses these generated videos to jointly optimize canonical 3D Gaussians and pose-dependent non-rigid deformation field, finally adopting LBS for animation. While this leverages high-quality 2D priors, it suffers from severe identity loss and temporal shifts, as the model hallucinates different appearance for each generated video. In contrast to these direct reconstruction, our method restores initial mesh-rigged renderings, which provides strong structural and identity prior. And our self-guided stochastic sampling ensures both photorealistic quality and strong fidelity. 3. Preliminary: Flow Matching Building upon the success of denoising diffusion models [20, 69], Flow Matching (FM) [42, 45] generates samples by learning velocity field vθ that transports prior distribution p1 to the data distribution p0. Rectified Flow [45], notable variant, simplifies this by defining linear interpolation path: xt = (1 σt)x0 + σtx1, where x0 p(x) is data sample and x1 (0, 1) is noise sample. The corresponding target velocity field is constant, ut = x1 x0. The model vθ is trained to predict this 3 velocity with the objective: min θ Et,x0,x1 vθ(xt, t) (x1 x0)2 2. (1) The sampling process starts from x1 (0, 1) and integrates the learned field vθ backward using an ODE solver: dxt = vθ(xt, t)dt, solved from = 1 to = 0. (2) key property, analogous to the Tweedies formula [11], is the ability to predict the paths endpoints (x0 and x1) from any intermediate point xt. By rearranging the path definition and substituting vθ ut, we can derive estimators for both the posterior mean ( ˆx0, the predicted data) and the posterior noise ( ˆx1): ut = ut = xt x0 σt x1 xt 1 σt = ˆx0t = xt σtvθ(xt, t); (3) = ˆx1t = xt + (1 σt)vθ(xt, t). (4) standard deterministic (ODE) solver uses these two predictions to perform an update step (from to tnext) by reinterpolating on the linear trajectory: xtnext = (1 σtnext) ˆx0t + σtnext ˆx1t. (5) 4. Proposed Method We present the pipeline of our framework in Fig. 2. Given 3D human represented by 3DGS [27] (reconstructed from reference image) and mesh sequence represented by the SMPL parameter sequence {st}N t=1,our goal is to animate the 3D human by modeling both rigid and non-rigid motions, such as body pose and cloth deformations, and to render photorealistic video from any viewpoint. This is achieved by first defining layered motion representation (Sec. 4.1) and then proposing self-guided stochastic sampling approach (Sec. 4.2) to generate high-quality video supervision signals for progressive optimization (Sec. 4.3) to learn the motion field. 4.1. Layered Motion Representation Our human motion representation combines an explicit mesh-rigged motion with an implicit residual motion field. Mesh-rigged motion. We first model the rigid motion using mesh-rigged approach based on SMPL [47, 54]. SMPL describes articulated motion via sparse set of skeleton parameters {sτ }T τ =1. To drive our 3DGS sequence, we establish bijective correspondence between them and point cloud {pi}N i=1 on the surface of the canonical SMPLX mesh [54]. This mapping is based on spatial relations (e.g., Euclidean distance or SDF). At each time step τ , the skeleton parameters sτ determine the translation and rotation of each SMPL point pi. We then apply these exact Figure 3. Distribution mismatch in deterministic flow matching. Our degraded input (out-of-distribution, OOD) creates noisy latent xt that is off the marginal distribution pt(x). deterministic Flow-ODE (orange path) follows an incorrect trajectory as its velocity predictions are inaccurate for OOD samples, resulting in low-quality sample. This motivates our use of an SDE sampler, which can actively correct the path by driving the sample back toward the marginal distribution. transformations to the corresponding Gaussian Gi, thus animating the rigid motion of the 3D Gaussians. Residual motion field. Since the mesh-rigged motion field cannot capture non-rigid motions, we incorporate residual motion field to model the non-rigid motion. This implicit function is parameterized with Hexplane [5, 79], which first queries feature fp in the canonical Gaussian position p. Once the feature is obtained, lightweight decoder implemented with an MLP predicts the offset θ of the Gaussian parameters θ, such as position and rotation. 4.2. Video Re-rendering with Self-guided Stochastic"
        },
        {
            "title": "Flow Sampling",
            "content": "Our goal is to generate high-quality, identity-preserving video to provide effective supervision for the residual motion field. However, the initial video y, rendered by 3DGS from the mesh-rigged motion, is highly unrealistic, exhibiting artifacts such as missing (garment) regions and unstable fine-grained motion. Therefore, we propose selfguided stochastic sampling to restore this coarse input into the high-fidelity target x. Limitation of deterministic flow-ODE. We formulate the re-rendering as video-conditioned sampling problem. Following SDEdit [50], we first inject significant Gaussian noise into the input to level t: xt = σtϵ + (1 σt)y, ϵ (0, 1). (6) baseline approach would be to use deterministic ODE solver (Flow-ODE) [45] to reverse this process from to 0 (see Eq. (2)). However, this approach fails. As shown in Fig. 3, since the input is OOD, its noised version xt also lies off the marginal distribution pt(x) that the flow model was trained on. deterministic ODE trajectory has no mechanism to correct this error; it is off the rails and will follow an incorrect path, leading to low-quality results. Algorithm 1 Self-guided Stochastic Sampling (Practical) Require: Low-quality video y; Pre-trained flow-based model vθ; Preserved region M; Initial noise step t0, constant step size λ; Ensure: Desirable high-quality video 1: Sample ϵ (0, I) 2: xt = σt0ϵ + (1 σt0)y 3: for : t0 0 do 4: 5: 6: 7: Eq. (6) Sampling loop ˆx0t xt σtvθ(xt, t) Eq. (3) ˆx1t xt + (1 σt)vθ(xt, t) Eq. (4) ˆx0t ˆx0t λxtM (y ˆx0t)2 Eq. (10) Sample ϵ (0, I) 1 σt ˆx1t + ˆx1t xtnext (1 σtnext) ˆx0t + σtnext ˆx1t Eq. (8) Eq. (5) σtϵ 8: 9: 10: end for 11: Return = xtt=0 High-quality generation with stochastic sampling. Stochastic SDE sampling typically yields higher generation quality [26, 52, 68, 69] compared to ODE sampling. More importantly, EDM [26] proves that the stochastic process actively pulls samples toward the target marginal pt(x) at each step, correcting errors accumulated from the initial OOD state. Motivated by this, we propose reverse-time SDE analogous to the deterministic flow ODE in Eq. (2): dx = vt(xt, t)dt + g(t)dwt, (7) where g(t)dwt is the stochastic diffusion term that performs the correction. To implement this, we propose novel stochastic discretization: we add noise directly to the clean noise prediction ˆx1t before interpolation: ˆx1t (cid:112)γ(t)ϵ + (cid:112)1 γ(t) ˆx1t, (8) where γ(t) is set to σt empirically. This re-noising (Eq. 8) is our simple and effective implementation of the stochastic term g(t)dwt, providing the necessary path correction, which is essential for achieving high-quality results from our OOD inputs. Proof is detailed in the supplementary. Identity preserving with self-guidance. The entire rerendering process begins by injecting high level of noise (Eq. 6) into the input y. This high noise, while necessary for the stochastic sampler to reset and find the correct data manifold, simultaneously destroys or corrupts identitycritical information from the original input. Consequently, the unguided stochastic sampler, while producing highquality video, will fail to preserve the humans identity; it will hallucinate plausible but incorrect appearance (e.g., different face) that is consistent with the corrupted noisy latent xt. Therefore, to ensure fidelity to the original input y, we must explicitly guide the sampling process. Theoretically, this conditioning is achieved by modifying the SDEs 5 Figure 4. Diagonal view-time sampling. (a) Illustration of diagonal sampling in view-time matrix (Ntraj = 3). This method simultaneously evolves the camera view and time, distinct from fixed-time (bullet-time) or fixed-camera (independent-view) sampling. (b) An example trajectory shows the camera orbiting 360 as time progresses. drift term (Eq. 7) with the score of the posterior p(yxt): dx = [vt(xt, t)g(t)2xt log p(yxt)]dt+g(t)dwt. (9) However, the guidance term xt log p(yxt) is intractable. We therefore adopt the core insight from Diffusion Posterior Sampling (DPS) [8], which provides an elegant and practical approximation. DPS proves that this complex scorespace guidance (Eq. 9) can be effectively approximated by applying simple data-space L2 loss to the posterior mean ˆx0t. In each sampling step, we first compute the standard posterior mean ˆx0t (Eq. 3). Then, we apply guidance step to pull this prediction closer to our masked input y: ˆx0t ˆx0t λ(t)xtM (y ˆx0t)2, (10) where is binary mask for preserved regions (e.g., face, hands) and λ(t) is the step size. This gradient has simple closed-form solution (as derived in the supplementary), making this step computationally efficient. Finally, we combine our stochastic component (Eq. 8) and our new guided component (Eq. 10) into the standard update rule (Eq. 5) to compute the next sample xtnext , finally obtain x. This self-guided stochastic sampler thus achieves both high-quality generation (from the SDE) and strong identity preservation (from the guidance). The practical algorithm is summarized in Algorithm 1. Personalized diffusion prior. While general video diffusion models are powerful, their priors may not be optimized for human animation. Therefore, we finetune video model with two human-related conditions to provide personalized diffusion prior: (1) reference human image via an additional branch, providing strong prior for identity preservation (fidelity), and (2) 2D pose sequence for precise motion control. By pre-training specifically on human-centric data, this specialized prior offers more suitable foundation, enabling higher generation quality and photorealism compared to general-purpose video model. 4.3. Progressive 4D Optimization Diagonal view-time sampling. We use our high-quality restored videos to optimize the residual motion field. The Figure 5. Comparison with state-of-the-art methods. Our method (Ours) is the only one to simultaneously achieve high quality, identity preservation, and realistic non-rigid motion. Existing methods fail in key areas: Disco4D [53] and SV4D 2.0 [87] suffers from low quality (due to SDS and multi-view video diffusion); PERSONA loses identity (due to direct reconstruction from pose-driven video diffusion); and LHM [58] captures identity but fails to model clothing dynamics. (* self-implementation) primary challenge is the inter-trajectory inconsistency [44, 76, 80] of generative models. Standard aggregation of many independent-view [83] or bullet-time [63] trajectories accumulates conflicting signals, leading to blurred artifacts. We therefore propose diagonal view-time sampling (sampling and simultaneously, Fig. 4), as it captures spatio-temporal information using the minimum number of trajectories, thus minimizing exposure to inconsistency. Dataset update. To address the sparsity of this minimal set, we pair it with progressive dataset update strategy [17]. Every 5k iterations, we generate new trajectories based on the current state of the 4D model and add them to the training set. This generation-optimization cycle progressively densifies the supervision in consistent manner, ensuring high-fidelity 4D reconstruction. Optimization objective. We adopt the commonly-used photometric loss: L1 loss, dSSIM loss and LPIPS loss to supervise the 4D representation. In addition, to preserve the geometry of rigid part of motion, we design regularization by calculating depth difference between the original depth of the optimized depth within the preserved region: = LL1 + λ1LLPIPS + λ2Ldssim + λ3Lmask + λ4Lreg. (11) 5. Experiments 5.1. Experimental settings Implementation details. For fair comparison, we use LHM [58] to generate the canonical 3D human Gaussians from single-view image. The render resolution is set to H/W/T =832/480/81. We finetune the personalized poseconditioned video diffusion based Wan2.1-1.3B [74]. For key parameter in video re-rendering, we set noise injection rate t0 = 0.6, initial denoising step = 30, λ = 0.2 in self-guidance. Preserved mask is obtained with SAM2 [60]. All experiments are conducted on NVIDIA A6000 48G GPU. In progressive 4D optimization, we use AdamW [29] with constant learning of 1e-5 with 25k iterations. Baselines. We compare our method with several state-ofthe-art 3D animation methods: LHM [58], Disco4D [53], SV4D 2.0 [87], and PERSONA [66]. LHM reconstructs 6 Figure 6. Comparison on other video re-rendering methods. (a) original rendering x; (b-f) competitive sampling methods; (g) our results x. Only our self-guided stochastic sampling can generate sharp details while preserving the original identity well. Figure 7. Ablative experiment on self-guided stochastic sampling. We compare full sampling method (e) and the generation results of our model with set of ablations. (a) Original rendering with mesh-rigged animation; (b) replacing personalized diffusion prior with general diffusion prior introduces slight performance degradation and artifacts; (c) We observe that our method produces significant quality drop when removing stochastic sampling; (d) removing self-guided sampling greatly reduces the identity preservation. human Gaussian from single image, then animate the human Gaussian with mesh-rigged motion. Disco4D uses hybrid supervision of single-view video and 3D-aware image SDS to optimize the motion field. SV4D 2.0 first generates multi-view videos given single-view video then uses DynNeRF [12] for reconstruction. PERSONA learns pose-dependent non-rigid deformation (relative to canonical SMPL) from pose-conditioned video diffusion, then uses LBS to animate the Gaussian particles. Evaluation tasks, dataset, and metrics. To evaluate the performance quantitatively, we select 10 cases from ActorsHQ [23] dataset, and reconstruct from singleview image and extracted motion sequence. Then we use common pixel-wise reconstruction metrics, PNSR, SSIM, LPIPS, and CLIP-Image [59] to measure the similarity to the ground truth, and adopt FID/FVD to evaluate the general rendered image/video quality. For novel motion (no GT), we conduct user study to evaluate the quality in identity preservation, frame quality, motion realism, physical plauTable 1. Quantitative results with the state-of-the-art methods of human animation in ActorsHQ [23] dataset. Methods PSNR SSIM LPIPS CLIP-I FID FVD Disco4D [53] SV4D 2.0 [87] PERSONA [66] LHM [58] Ours 12.05 15.25 17.01 19.51 20.08 0.5590 0.7708 0.8219 0.8382 0.8312 0.5019 0.3773 0.2602 0.2169 0.2125 0.6439 0.7640 0.8779 0.9009 0.9160 613.9 364.9 199.1 124.1 105.3 622.1 478.7 367.0 339.9 295. Table 2. User study on human animation with novel motion. Methods (%) Disco4D [53] SV4D 2.0 [87] PERSONA [66] LHM [58] Ours Identity Preservation Frame Quality Motion Realism (Non-rigid) Physical Plausibility Overall Preference 0 0 20.6 39.2 40.1 1.47 2.94 30.9 29.4 35.3 4.41 16.2 14.7 29.4 35.3 0 4.34 19.1 14.7 61. 0 5.89 22.1 17.6 54.4 sibility in non-rigid part and overall preference. 5.2. Comparisons to the State-of-the-art Methods We compare our method and four the state-of-the-art methods in Fig. 5. In this comparison, we demonstrate 4 human with different motion sequence. Disco4D suffers from oversaturation and notable artifacts due to SDS. Directly recon7 structing from multi-view diffusion videos (SV4D) cannot achieve high quality. Although equipped with lots of preprocessing and regularization, PERSONA cannot preserve the original identity well. While LHM that applies meshrigged motion that achieves high identity preservation and precise body control, but it cannot model the realistic nonrigid garment motions. Only our method can generate photorealistic and physics-plausible non-rigid motion in various motions, such as clothing dynamics like fluttering and folds (also see Fig. 1). Quantitative results in Tab. 1 also support this, we have surpassed the state-of-the-art methods, with competitive reconstruction metrics and considerable 18.8 FID improvement. User preference also shows that our method shares the best scores among all terms. 5.3. Analysis of Self-guided Stochastic Sampling Comparison with other sampling methods. We compare our self-guided stochastic sampling with several competitive methods in Fig. 6, including vanilla SDEdit-FM [50], FlowEdit [33], MCS [76], HFS-SDEdit [64], and NCSDEdit [85]. For fair comparison, all methods use the same base video model, an initial noise level of t0 = 0.6, and 30 denoising steps. Vanilla SDEdit (b) fails to preserve the humans identity. To address this, following works in visual editing/restoration incorporate the input video to improve fidelity. For example, MCS [76] (f) updates the posterior mean ˆx0t with the weighted averaging of itself and the reference latents x. HFS-SDEdit [64] (d) matches the high-frequency component of posterior mean with that of the reference image x. However, these methods (b-f) are built on deterministic ODE sampling. As argued in Sec. 4.2, ODE sampling struggles with the out-of-distribution (OOD) nature of our input, resulting in low-quality outputs. In contrast, our method (g) provides an effective solution that simultaneously achieves high-quality results (via stochastic sampling) and strong fidelity (via self-guidance), fully leveraging the power of the video diffusion prior. Implementation details are in the supplementary. Component-wise validation. We also validate the components of our method in Fig. 7. The original rendering (a) suffers from severe blur and unrealistic artifacts on the garment, caused by the initial mesh-rigged motion. We ablate our two key contributions. First, in (c), we replace our stochastic sampler with its deterministic ODE counterpart. The resulting video quality is significantly worse, and the blurriness persists, which confirms our hypothesis that stochastic sampling is essential for correcting the OOD input and achieving high-quality restoration. Second, in (d), we remove the self-guidance term. While the video quality is high (due to stochastic sampling), the humans identity is lost. This demonstrates that our self-guidance is crucial for fidelity. Additionally, we replace our personalized diffusion prior with general one (b), which leads to slight drop in Figure 8. Ablation study on motion field. Our layered motion (right) captures intricate hand details, while the single-layer baseline (left) fails. Figure 9. Ablation study on sampling method. Baseline methods (left) suffer from significant floaters and spikes, while our diagonal sampling (right) reconstructs sharp details. realism. Our full method (e) is the only setting that successfully resolves the initial artifacts, generates high-quality video, and faithfully preserves the humans identity. 5.4. Analysis of Motion Field and Data Sampling We ablate our proposed methods: the motion representation and the view-time sampling strategy. Additional ablations on loss functions, progressive optimization, hyperparameter selection are detailed in the supplementary materials. Impact of layered motion representation. In Fig. 8, we compare our layered representation against single-layer motion field [44, 88] initialized with mesh-rigged motion. The baseline fails to model intricate transformations, such as human hands, whereas our layered approach captures these details effectively. Impact of diagonal view-time sampling. In Fig. 9, we compare our diagonal view-time sampling against bullettime sampling and independent view sampling, using an equal trajectory count (Ntraj = 3). The baseline methods, suffering from temporal and spatial sparsity, produce significant floaters and spikes. In contrast, our strategy yields sharp, artifact-free details. 6. Conclusion We presented ANI3DHUMAN, novel framework for photorealistic human animation that successfully captures complex non-rigid motion. First, we introduce layered motion representation composed of mesh-rigged motion and residual field. Second, to supervise this, we propose self-guided stochastic sampling, which is specifically designed to transform our low-quality, out-of-distribution initial renderings into high-fidelity, identity-preserving videos. 8 It achieves this by balancing stochasticity (for quality) with self-guidance (for fidelity). We also introduce diagonal view-time sampling to ensure coherent 4D optimization free from generative inconsistencies. Comparative experiments show that our framework surpasses state-of-the-art methods, achieving best perceptual results. Our extensive ablation studies validate the effectiveness of our core sampling algorithm, as well as the necessity of the layered motion representation and diagonal sampling. The key limitation lies in the lengthy sampling time of the video diffusion prior. valuable future avenue is incorporating few-step generation techniques [22] to reduce the overall time cost."
        },
        {
            "title": "References",
            "content": "[1] Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: In European Trajectory-conditioned text-to-4d generation. Conference on Computer Vision, pages 5372. Springer, 2024. 3 [2] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B. Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 2, 3 [4] Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei, Daxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, Chen Change Loy, and Ziwei Liu. Playing for 3d human recovery. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2 [5] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130141, 2023. 2, 4, [6] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. 3 [7] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. 3 [8] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2023. 2, 5, 13 [9] Xiao Cui, Yulei Qin, Wengang Zhou, Hongsheng Li, and Houqiang Li. Optimizing distributional geometry alignment with optimal transport for generative dataset distillation. arXiv preprint arXiv:2512.00308, 2025. 15 [10] Xiao Cui, Weicai Ye, Yifan Wang, Guofeng Zhang, Wengang Zhou, Tong He, and Houqiang Li. Streetsurfgs: Scalable urban street surface reconstruction with planar-based gaussian IEEE Transactions on Circuits and Systems for splatting. Video Technology, 2025. [11] Bradley Efron. Tweedies formula and selection bias. Journal of the American Statistical Association, 106(496):1602 1614, 2011. 4 [12] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In Proceedings of the IEEE International Conference on Computer Vision, 2021. 7 [13] Junyao Gao, Jiaxing Li, Wenran Liu, Yanhong Zeng, Fei Shen, Kai Chen, Yanan Sun, and Cairong Zhao. Charactershot: Controllable and consistent 4d character animation. arXiv preprint arXiv:2508.07409, 2025. 2, 3, 16 [14] Adelaıde Genay, Anatole Lecuyer, and Martin Hachet. Being an avatar for real: survey on virtual embodiment in augmented reality. IEEE Transactions on Visualization and Computer Graphics, 28(12):50715090, 2022. 2 [15] Artur Grigorev, Michael Black, and Otmar Hilliges. Hood: Hierarchical graphs for generalized modelling of clothing In Proceedings of the IEEE/CVF Conference dynamics. on Computer Vision and Pattern Recognition, pages 16965 16974, 2023. 2, 3 [16] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [17] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: EditIn Proceedings of the ing 3d scenes with instructions. IEEE/CVF International Conference on Computer Vision, 2023. 6 [18] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint 2211.13221, 2022. 3 [19] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. arXiv preprint arXiv:2304.07090, 2023. 3 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 3 [21] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Cogvideo: Large-scale pretraining for and Jie Tang. text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 3 [22] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, Self forcing: Bridging the trainand Eli Shechtman. test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 9 [23] Mustafa Isık, Martin Runz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, and Matthias Nießner. Humanrf: High-fidelity neural radiance fields for humans in motion. ACM Transactions on Graphics (TOG), 42(4):112, 2023. 7, 13, 20, 24, 9 [24] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Avatarcraft: Transforming text into neural human avatars with parameterized shape and pose control, 2023. 2 [25] Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. Animate3d: Animating any 3d arXiv preprint model with multi-view video diffusion. arXiv:2407.11398, 2024. 3 [26] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proc. NeurIPS, 2022. 2, 5, 14 [27] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4), 2023. 4, 14 [28] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision models. arXiv preprint arXiv:2408.12569, 2024. [29] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6 [30] Muhammed Kocabas, Rick Chang, James Gabriel, Oncel Tuzel, and Anurag Ranjan. Hugs: Human gaussian splats, 2023. 2 [31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3 [32] Matthew Korban and Xin Li. survey on applications of digital human avatars toward virtual co-presence. arXiv preprint arXiv:2201.04168, 2022. 2 [33] Vladimir Kulikov, Matan Kleiner, Inbar HubermanSpiegelglas, and Tomer Michaeli. Flowedit: Inversion-free arXiv text-based editing using pre-trained flow models. preprint arXiv:2412.08629, 2024. 8, 17 [34] Abdelaziz Lakhfif. Design and implementation of virtual 3d educational environment to improve deaf education. arXiv preprint arXiv:2006.00114, 2020. 2 [35] John Lewis, Matt Cordner, and Nickson Fong. Pose space deformation: unified approach to shape interpolation and skeleton-driven deformation. In SIGGRAPH, 2000. 2 [36] Bing Li, Cheng Zheng, Wenxuan Zhu, Jinjie Mai, Biao Zhang, Peter Wonka, and Bernard Ghanem. Vivid-zoo: Multi-view video generation with diffusion model, 2024. 3 [37] Minchen Li, Danny M. Kaufman, and Chenfanfu Jiang. Codimensional incremental potential contact. ACM Trans. Graph. (SIGGRAPH), 40(4), 2021. 3 [38] Xuan Li, Qianli Ma, Tsung-Yi Lin, Yongxin Chen, Chenfanfu Jiang, Ming-Yu Liu, and Donglai Xiang. Articulated kinematics distillation from video diffusion models. arXiv preprint arXiv:2504.01204, 2025. 3, 16 [39] Zhiqi Li, Yiming Chen, and Peidong Liu. Dreammesh4d: Video-to-4d generation with sparse-controlled gaussianIn Advances in Neural Informesh hybrid representation. mation Processing Systems (NeurIPS), 2024. 3 [40] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos Plataniotis, Yao Zhao, and Yunchao Wei. Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models. arXiv preprint arXiv:2405.16645, 2024. 2, [41] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your Gaussians: Text-to-4D with dynamic 3D gaussians and composed diffusion models. In CVPR, 2024. 3 [42] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [43] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023. 16 [44] Tianqi Liu, Zihao Huang, Zhaoxi Chen, Guangcong Wang, Shoukang Hu, liao Shen, Huiqiang Sun, Zhiguo Cao, Wei Li, and Ziwei Liu. Free4d: Tuning-free 4d scene generation with spatial-temporal consistency. arXiv preprint arXiv:2503.20785, 2025. 6, 8 [45] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3, 4 [46] Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, and Ziwei Liu. Humangaussian: Text-driven 3d human generation with gaussian splatting. arXiv preprint arXiv:2311.17061, 2023. [47] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multiperson linear model. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia), 34(6):248:1248:16, 2015. 2, 4 [48] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 2, 15 [49] David McAllister, Songwei Ge, Jia-Bin Huang, David W. Jacobs, Alexei A. Efros, Aleksander Holynski, and Angjoo Kanazawa. Rethinking score distillation as bridge between image distributions. In Advances in Neural Information Processing Systems, 2024. 3 [50] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. 4, 8, 17 [51] Gyeongsik Moon, Takaaki Shiratori, and Shunsuke Saito. Expressive whole-body 3d gaussian avatar. In ECCV, 2024. 2 [52] Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, and Chongxuan Li. The blessing of randomness: Sde beats ode in general diffusion-based image editing. arXiv preprint arXiv:2311.01410, 2023. [53] Hui En Pang, Shuai Liu, Zhongang Cai, Lei Yang, Tianwei Zhang, and Ziwei Liu. Disco4d: Disentangled 4d human generation and animation from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2633126344, 2025. 3, 6, 7, 16 10 [54] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019. 2, 4 [55] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 15 [56] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. 2 [57] Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, and Siyu Tang. 3dgs-avatar: Animatable avatars In Proceedings of via deformable 3d gaussian splatting. the IEEE/CVF conference on computer vision and pattern recognition, pages 50205030, 2024. [58] Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, and Liefeng Bo. Lhm: Large animatable human reconstruction model from single image in seconds. In ICCV, 2025. 1, 2, 3, 6, 7, 16 [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 7 [60] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 6 [61] Rim Rekik, Stefanie Wuhrer, Ludovic Hoyet, Katja Zibrek, and Anne-Hel`ene Olivier. survey on realistic virtual human animations: Definitions, features and evaluations. Computer Graphics Forum, 2022. 2 [62] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. 3, 16 [63] Denys Rozumnyi, Jonathon Luiten, Numair Khan, Johannes Schonberger, and Peter Kontschieder. Bulletgen: Improving 4d reconstruction with bullet-time generation. arXiv preprint arXiv:2506.18601, 2025. [64] Nuri Ryu, Jiyun Won, Jooeun Son, Minsu Gong, Joo-Haeng Lee, and Sunghyun Cho. Elevating 3d models: High-quality texture and geometry refinement from low-quality model. In ACM SIGGRAPH 2025 Conference Papers, New York, NY, USA, 2025. Association for Computing Machinery. 8, 17 [65] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, and Yebin Liu. Human4dit: 360-degree human video generation with 4d diffusion transformer. ACM Transactions on Graphics (TOG), 43(6), 2024. 3, 16 [66] Geonhee Sim and Gyeongsik Moon. PERSONA: Personalized whole-body 3D avatar with pose-driven deformations from single image. In ICCV, 2025. 2, 3, 6, 7, 16 [67] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4D dynamic scene generation. arXiv preprint arXiv:2301.11280, 2023. 2, 3 [68] Saurabh Singh and Ian Fischer. Stochastic sampling from deterministic flow models. arXiv preprint arXiv:2410.02217, 2024. 5 [69] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, 2021. 3, [70] Zhaoqi Su, Liangxiao Hu, Siyou Lin, Hongwen Zhang, Shengping Zhang, Justus Thies, and Yebin Liu. Caphy: Capturing physical properties for animatable human avatars. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1415014160, 2023. 3 [71] Qi Sun, Zhiyang Guo, Ziyu Wan, Jing Nathan Yan, Shengming Yin, Wengang Zhou, Jing Liao, and Houqiang Li. Eg4d: Explicit generation of 4d object without score distillation. In International Conference on Learning Representations, 2025. 3 [72] Qi Sun, Can Wang, Jiaxiang Shang, Wensen Feng, and Jing Liao. Animus3d: Text-driven 3d animation via motion score In Proceedings of the SIGGRAPH Asia 2025 distillation. Conference Papers, New York, NY, USA, 2025. Association for Computing Machinery. 3 [73] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 3 [74] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 6, 15 [75] Chaoyang Wang, Peiye Zhuang, Tuan Duc Ngo, Willi Menapace, Aliaksandr Siarohin, Michael Vasilkovsky, Ivan Skorokhodov, Sergey Tulyakov, Peter Wonka, and Hsin-Ying Lee. 4real-video: Learning generalizable photo-realistic 4d video diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1772317732, 2025. [76] Haiping Wang, Yuan Liu, Ziwei Liu, Wenping Wang, Zhen Dong, and Bisheng Yang. Vistadream: Sampling multiview consistent images for single-view scene reconstruction. arXiv preprint arXiv:2410.16892, 2024. 6, 8, 17, 18, 22 [77] Shaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu Tang. Arah: Animatable volume rendering of articulated human sdfs. In European Conference on Computer Vision, 2022. 2 [78] Thomas Wimmer, Michael Oechsle, Michael Niemeyer, and Federico Tombari. Gaussians-to-life: Text-driven animation 11 [90] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with In International Conferconfidence-aware pose guidance. ence on Machine Learning, 2025. 3, 16 [91] Yang Zheng, Qingqing Zhao, Guandao Yang, Wang Yifan, Donglai Xiang, Florian Dubost, Dmitry Lagun, Thabo Beeler, Federico Tombari, Leonidas Guibas, and Gordon Wetzstein. Physavatar: Learning the physics of dressed 3d In European Conference avatars from visual observations. on Computer Vision (ECCV), 2024. 2, 3, 16 of 3d gaussian splatting scenes. In International Conference on 3D Vision (3DV), 2025. [79] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20310 20320, 2024. 2, 4, 15 [80] Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, and Huan Ling. Difix3d+: Improving 3d reconstructions arXiv preprint arXiv: with single-step diffusion models. 2503.01774, 2025. 6 [81] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion modIn Proceedings of the Computer Vision and Pattern els. Recognition Conference, pages 2605726068, 2025. 3 [82] Donglai Xiang, Timur Bagautdinov, Tuur Stuyck, Fabian Prada, Javier Romero, Weipeng Xu, Shunsuke Saito, Jingfan Guo, Breannan Smith, Takaaki Shiratori, et al. Dressing avatars: Deep photorealistic appearance for physically simulated clothing. ACM Transactions on Graphics (TOG), 41 (6):115, 2022. 2 [83] Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d: Dynamic 3d content generation In Internawith multi-frame and multi-view consistency. tional Conference on Learning Representations, 2025. 2, 3, 6, 16 [84] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. 2023. 3 [85] Qinyu Yang, Haoxin Chen, Yong Zhang, Menghan Xia, Xiaodong Cun, Zhixun Su, and Ying Shan. Noise calibration: Plug-and-play content-preserving video enhancement using pre-trained video diffusion models. In European Conference on Computer Vision, pages 307326. Springer, 2024. 8, [86] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3 [87] Chun-Han Yao, Yiming Xie, Vikram Voleti, Huaizu Jiang, and Varun Jampani. SV4D2.0: Enhancing spatio-temporal consistency in multi-view video diffusion for high-quality 4d generation. arXiv preprint arXiv:2503.16396, 2025. 2, 3, 6, 7, 16 [88] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo Jeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via video diffusion models. Advances in Neural Information Processing Systems, 37:4525645280, 2024. 8 [89] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, 4diffusion: Multi-view arXiv preprint Yunhong Wang, and Yu Qiao. video diffusion model for 4d generation. arXiv:2405.20674, 2024."
        },
        {
            "title": "Contents",
            "content": "A. Supplementary Video B. Proof C. Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1. 3D Gaussian Splatting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2. 4D Gaussian Splatting . C.3. Video Diffusion Transformer Backbone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D. More Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1. Preserved Area Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2. Residual Field Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3. Personalized Video Diffusion . D.4. Baseline Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5. Details of Competitive Sampling Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. Ablation Studies (Extended) E.1. Quantitative Ablation . . E.2. More Results of Self-guided Stochastic Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3. Sensitivity Analysis of Initial Noise Strength . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4. More Ablations in 4D Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F. Results (Extended) . . . . F.1. Training Efficiency . . F.2. Discussion with Image-based Animation methods. F.3. Results in ActorsHQ dataset [23] F.4. Additional Qualitative Results . . . F.5. Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 13 14 . 14 . 15 . 15 15 . 15 . 15 . 15 . 16 . 17 17 . 17 . 18 . 18 . 19 . 19 . 20 . 20 . 20 . 21 . . . . . . . . . . . . . . . . . A. Supplementary Video To better demonstrate the efficacy of our framework and the visual quality of our results, we provide comprehensive supplementary video (overall length 247). We strongly recommend viewing the video to fully dynamic visual results. B. Proof Proposition B.1 (Error Bound of Gradient Approximation) Consider the score approximation xt log p(yxt) xt log p(yˆx0t) used in Eq.(10). Let be the measurement operator and ˆx0t = E[x0xt] be the posterior mean. Under the manifold constraint, the approximation error ϵ is upper bounded by: ϵ M2 Ex0p(x0xt)[x0 ˆx0t], (12) where is constant related to the Lipschitz property of the noise schedule. Proof: formula. The spectral norm M2 represents the maximum amplification factor of the measurement operator. Following the theoretical framework in DPS [8], the likelihood gradient can be decomposed via the Tweedies In our specific task, the operator is defined as binary mask {0, 1}n. The spectral norm of diagonal matrix (or masking operator) is given by its maximum singular value: M2 = max Mii = 1. 13 (13) Consequently, the error bound simplifies to ϵ E[x0 ˆx0t]. This term represents the uncertainty of the posterior estimation at time t. As the diffusion process approaches the clean data manifold (t 0), the posterior distribution p(x0xt) collapses to Dirac delta distribution δ(x0 ˆx0), leading to ϵ 0. This ensures that the approximate gradient converges to the true score direction in the final sampling stages. Proposition B.2 (SDE Correction Mechanism [26]) The continuous implicit Langevin diffusion dxt = 1 dwt actively corrects sampling errors by admitting the data marginal p(x) as its unique stationary distribution. 2 log p(x)dt + Proof: The time evolution of the probability density pt(x) is governed by the Fokker-Planck Equation (FPE): (cid:18) 1 2 ( log p)pt pt = pt. 1 + (cid:19) (14) We verify the stationarity by setting pt(x) = p(x). Using the identity ( log p)p = p, the drift term becomes 1 2 p, yielding pt 1 towards p(x), correcting deviations accumulated from prior steps. 2 (p) = = 0. Thus, the dynamics inherently drive any distribution 2 p. This exactly cancels the diffusion term Proposition B.3 (Equivalence of Stochastic Term.) Our proposed stochastic sampling step, which acts on the noise prediction component, acts as valid discretization of reverse-time SDE by introducing an explicit diffusion term to the standard Rectified Flow ODE. Proof: Recall that the standard deterministic (ODE) update in Rectified Flow is given by linear interpolation: 1 γ ˆx1t + xtnext = (1 tnext) ˆx0t + tnext ˆx1t. Our method introduces stochasticity by perturbing the target noise prediction ˆx1t. Specifically, we replace ˆx1t with ˆxstoch 1t = (15) γϵ, where ϵ (0, I) and γ is scheduling parameter. Substituting this into the update rule yields: (cid:16)(cid:112)1 γ ˆx1t + (cid:105) (cid:112)1 γ ˆx1t (cid:104) (1 tnext) ˆx0t + tnext = (1 tnext) ˆx0t + tnext xSDE tnext γϵ + = (cid:17) . (16) (17) (cid:124) (cid:123)(cid:122) Effective Drift (Deterministic) (cid:125) [tnext (cid:124) γϵ] (cid:125) (cid:123)(cid:122) Effective Diffusion (Stochastic) The resulting update equation takes the form of standard Euler-Maruyama discretization of an SDE (dx = (x, t)dt + g(t)dw). The first term represents the drift (the intended restoration path), while the second term represents the diffusion γ. This explicitly proves that our method injects the necessary stochas- (g(t)dw), with the noise magnitude scaled by tnext ticity to correct out-of-distribution (OOD) errors during sampling. Derivation of closed-form guidance. To enforce the identity constraint, we minimize the loss = (x ˆx0t)2 with respect to the noisy latent xt. Applying the chain rule yields xtL = ( ˆx0t ) ˆx0tL. Calculating the exact Jacobian xt ˆx0t requires computationally expensive backpropagation through the diffusion backbone. To achieve an efficient closedxt form solution, we follow standard Diffusion Posterior Sampling practice and approximate this Jacobian as scalar identity matrix (absorbing scaling factors into the step size λ(t)). Consequently, the gradient simplifies directly to the masked residual xtL (x ˆx0t), enabling fast, derivative-free guidance updates. C. Background C.1. 3D Gaussian Splatting 3D Gaussian Splatting (3D-GS) [27] is photorealistic 3D scene representation and real-time rendering technique [10]. Instead of using traditional polygons or volumetric grids, 3D-GS models scene as collection of millions of explicit, anisotropic 3D Gaussians. Each Gaussian is defined by several key properties: its 3D position (mean), shape (a 3D covariance matrix, allowing it to be sphere, needle, or flat disk), color (often represented by Spherical Harmonics to capture view-dependent effects), and opacity (alpha). The scene is created by optimizing these properties, typically starting from sparse point cloud generated by Structure-from-Motion (SfM). During this optimization, process of adaptive density control dynamically adds (clones) or removes (prunes) Gaussians to efficiently reconstruct fine details. To render new view, these 3D Gaussians are projected onto the 2D image plane, sorted by depth, and alpha-blended back-to-front in highly efficient rasterization process. 14 C.2. 4D Gaussian Splatting To extend 3D-GS to dynamic scenes, 4D Gaussian Splatting (4D-GS) [79] techniques model how Gaussians move and change over time. Instead of storing separate 3D-GS models for each frame, holistic 4D representation is learned. common strategy is to define set of canonical 3D Gaussians and then predict their deformation at any given timestamp. To efficiently encode this 4D space-time information, methods often employ decomposed neural voxel grid, drawing inspiration from HexPlane [5]. This approach factorizes the 4D space (x, y, z, t) into several lower-dimensional planes (e.g., xy, xz, yt). To find Gaussians deformation, its 4D coordinates are used to query features from these planes. The aggregated features are then passed through lightweight MLP to predict the transformation (such as translation or rotation), allowing the scene to be reconstructed at novel times. C.3. Video Diffusion Transformer Backbone Our framework leverages the Wan [74] architecture, state-of-the-art text-to-video model built upon the Diffusion Transformer [9, 48, 55] (DiT) paradigm. This architecture consists of three core components: 1) spatio-temporal VAE that compresses input videos from pixel space into compact latent space; 2) robust text encoder (e.g., umT5), selected for its multilingual capabilities and convergence properties, to encode text prompts; 3) The central Diffusion Transformer, which processes sequences of video latent tokens. Within the Transformer blocks, text conditions are injected via cross-attention to ensure semantic fidelity. Temporal information is embedded using shared MLP that predicts modulation parameters for each block, design that efficiently enhances performance with minimal parameter overhead. The model is trained using the Flow Matching framework, specifically Rectified Flows (RF), which provides stable and theoretically grounded generative process. RF models the transition from pure noise x0 to the real data latent x1 as linear interpolation (Ordinary Differential Equation). For time step [0, 1], the training input xt is defined as: xt = x1 + (1 t) x0. (18) The model is trained to predict the velocity field vt of this trajectory, where the ground truth velocity is simply vt = x1 x0. The training objective minimizes the mean squared error (MSE) between the predicted and ground truth velocity. Training follows multi-stage curriculum, progressing from low-resolution images to high-resolution joint image-video training. D. More Implementation Details D.1. Preserved Area Segmentation To ensure identity preservation, we define preserved area mask M. We utilize Grounded-DINO-SAM2 to segment the human region, denoted as Mhuman, and the garment region, Mgarment. The final preserved area is obtained by excluding the garment region from the human mask: = Mhuman Mgarment. (19) To align with the latent space of the Video VAE, we downsample the binary mask to match the latent dimensions (specifically, downsampling by factor of 8 spatially and 4 temporally). D.2. Residual Field Configuration For the non-rigid motion modeling, we employ multi-resolution HexPlane module. The base resolution R(i, j) is set to 64 and is progressively upsampled by factor of 2. The Gaussian deformation decoder is implemented as lightweight MLP using zero-initialization for the final layer weights, ensuring the deformation field starts as an identity mapping. D.3. Personalized Video Diffusion Control DiT via Channel-wise Concatenation. We inject dense spatiotemporal conditions (e.g., the control video) directly into the main branch via latent space augmentation. Unlike adapter-based methods that operate on intermediate features, we concatenate the encoded control latents with the noisy video latents along the channel dimension prior to the patch embedding layer. Formally, the input to the DiT becomes [x; y] RB(Cx+Cy)F HW . This strategy ensures that every spatial patch processed by the Transformer is explicitly conditioned on the corresponding local structural information. Reference Image Fusion. To achieve appearance transfer, we treat the reference image as visual prompt. The reference image is encoded into latents and passed through projection layer to match the embedding dimension of the DiT. These projected features are flattened and concatenated with the video tokens along the sequence dimension, effectively serving 15 Method Training Objective Disco4D [53] AKD [38] PhysAvatar [91] SV4D/SV4D 2.0 [83, 87] CharacterShot [13] Human4DiT [65] PERSONA [66] LHM [58] Ours MSE+SDS SDS MSE MSE MSE MSE MSE - MSE Single-view Image Input Skeleton-controllable Identity Preservation Non-rigid Motion High-quality Rendering Table 3. Difference among the other human (character) animation methods. - means there is no optimization process in the animation. as visual prefix. By integrating the reference signal into the input sequence, the DiT utilizes its global self-attention mechanism to attend to reference appearance details across all generated frames. These prefix tokens are masked out during the final video reconstruction. Training Details. To facilitate the personalized video generation, we implement the proposed Wan-Control framework based on the DiffSynth library. The model is fine-tuned on curated subset of the TikTok dataset (available via HuggingFace), comprising approximately 20,000 video clips. For high-fidelity motion guidance, we pre-process all video frames using DWPose to extract dense human pose annotations. The training process is conducted on cluster of 8 NVIDIA RTX A6000 GPUs for approximately 15,000 iterations. We utilize constant learning rate with batch size optimized for the GPU memory. For further architectural details and hyper-parameter configurations. We also find some good open-sourced alternative, such as https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-1.3B-Control, and https://huggingface.co/alibabapai/Wan2.2-Fun-A14B-Control, as our video backbone. D.4. Baseline Implementation We mainly classify our baselines in Tab. 3, selecting several representative methods with official implementation1 for comparison. Disco4D [53]. Due to the unavailability of key components in the official repository, we re-implemented the core algorithm within our own framework. Following the supervision strategy of DreamGaussian4D [62], this method combines Mean Squared Error (MSE) loss from single-view driving video with Score Distillation Sampling (SDS) guidance from Zero123 [43]. To ensure fair comparison, we generated the required driving video using our personalized Wan-based model, conditioned on the front-view skeleton rendering and the reference image. We adopted the SDS implementation directly from the DreamGaussian4D repository. SV4D 2.0 [87]. SV4D is multi-view video diffusion model fine-tuned on Stable Video Diffusion (SVD) using large-scale 4D dataset filtered from Objaverse. It takes single-view video as input and outputs synchronized multi-view videos. We utilized the same driving video generated for Disco4D as the input. However, we observe severe identity shift between the output and input videos. We attribute this to the domain gap, as SV4D is trained primarily on synthetic Objaverse objects rather than realistic human captures. PERSONA [66]. We utilize the official implementation of PERSONA. This method employs MimicMotion [90], posedriven video diffusion model, to generate synthetic video data which is then used to optimize canonical 3D Gaussian field and pose-dependent deformation field. The pipeline relies on an extensive set of off-the-shelf components, including Sapiens [28], DECA, and ResShift. Despite incorporating various regularization terms, such as geometry weighted optimization and multiple monocular normal/depth priors, we find that the method struggles to preserve the fine-grained identity of the subject during complex motions. LHM [58]. We use the official implementation of LHM as representative kinematics-based baseline. LHM effectively reconstructs human from single-view image with high-fidelity identity and efficient inference speed. However, as it relies purely on kinematics-based deformation to animate the 3D Gaussians, it fundamentally lacks the ability to model non-rigid dynamics such as clothing deformation. (Note: Our method builds upon this kinematics-based representation, using it as starting point to learn residual non-rigid motions via video diffusion priors.) 1For example, since Human4DiT/CharactorShot is not open-sourced, we choose SV4D as representative method of reconstructing from MV video. 16 D.5. Details of Competitive Sampling Methods We compare our approach against several representative methods capable of transforming low-quality source inputs into high-quality targets using off-the-shelf diffusion priors. Since some of algorithms are designed from DDPM, we implement them in the context of flow matching with their core ideas. Vanilla SDEdit [50]. SDEdit serves as the foundational baseline for image and video restoration. The method follows strictly stochastic process: it first perturbs the source input xsrc by adding Gaussian noise to reach an intermediate time step t0 (0, 1). This forward diffusion process effectively destroys high-frequency artifacts. Subsequently, the standard reverse ODE/SDE sampling is applied from t0 to = 0 to generate the restored output. While effective for minor denoising, it often faces trade-off between preserving identity (low t0) and removing significant artifacts (high t0). MCS [76]. Multiview Consistency Sampling (MCS) was originally proposed to balance fidelity and generation quality in 3D scene generation. The authors observe that while higher noise injection improves realism, it degrades the structural fidelity to the input. To mitigate this, MCS modifies the posterior mean during sampling to explicitly include signal from the input image. In our implementation, we adapt this to the Flow Matching framework. At each sampling step, we modify the predicted posterior mean ˆx0t to incorporate weighted component of the source input xsrc. This bias term forces the generation trajectory to remain structurally close to the input video, ensuring that the hallucinated details align with the original identity. HFS-SDEdit [64]. HFS-SDEdit aims to preserve structural details by explicitly fusing frequency components in the latent space. It operates on the hypothesis that the structural identity resides in high-frequency signals. During the reverse sampling process, the method replaces the high-frequency component of the current denoised latent xt with that of the noisy source input. The update rule is defined as: = LPF(xt) + HPF(xsrc,t), (20) where LPF and HPF denote Gaussian low-pass and high-pass filters, respectively, and xsrc,t is the noised version of input data corresponding to time t. This operation forces the solver to generate realistic low-frequency content (lighting, materials) while rigorously adhering to the edges and boundaries of the source input. NC-SDEdit [85]. We adapt the Noise Calibration (NC) strategy to our Flow Matching framework. While the original implementation calibrates the noise estimate ϵ, our adaptation operates directly on the estimated clean data (posterior) to ensure structural consistency. In each sampling step t, we first solve the flow equation to estimate the clean target ˆx0t from the current noisy latent xt and predicted velocity vt. We then calibrate this posterior by replacing its high-frequency components with those of the source reference xsrc: ˆx 0t = ˆx0t HPF( ˆx0t) + HPF(xsrc), (21) where HPF() extracts high-frequency details via Fourier transform. Finally, the solver (e.g., Euler step) computes the latent for the next timestep xtnext using this calibrated target ˆx 0t. This approach enforces strict structural alignment with the input video throughout the generation trajectory while allowing the low-frequency content to be refined by the diffusion prior. FlowEdit [33]. FlowEdit constructs mapping between source and target distributions by leveraging the reversibility of ODEs. It defines the editing direction based on the difference between source velocity (conditioned on source prompt) and target velocity (conditioned on target prompt). In our experiments, we utilize negative prompt (describing low-quality attributes) to match the source distribution and positive prompt for the target. However, we find that because FlowEdit relies on the models semantic understanding of the prompt to model the degradation, it often fails to correct the severe, non-semantic out-of-distribution (OOD) artifacts present in the coarse 3D renderings, as these specific artifacts are not easily described by text. E. Ablation Studies (Extended) E.1. Quantitative Ablation Table 4 provides comprehensive quantitative evaluation of each key component within our framework, including the stochastic sampling mechanism, the self-guidance strategy, and the personalized video diffusion module. According to the results, our full model configuration achieves the optimal balance between visual fidelity and identity preservation. Specifically, while the exclusion of self-guidance leads to marginal improvement in the Frechet Inception Distance, it incurs substantial degradation in identity consistency, as reflected by the significant drop in the CLIP-Identity score. This observation validates that self-guidance is indispensable for maintaining the subjects unique features throughout the generation 17 process. Furthermore, the integration of stochastic sampling and personalized diffusion proves essential for temporal coherence and motion realism, with the full model yielding the lowest Frechet Video Distance. Although individual modules may favor specific metrics, the synergistic effect of all components ensures that the model produces high-quality videos without compromising the structural or stylistic integrity of the personalized target. Table 4. Quantitative ablation study of the proposed components. The results demonstrate that the full model configuration achieves the most robust performance across all evaluation metrics."
        },
        {
            "title": "Metrics",
            "content": "Coarse Model w/o Stochastic w/o Self-Guidance w/o Personalized Full Model FID FVD CLIP-Identity 199.1 367.0 0.8847 187.4 349.7 0.8804 104.1 298.8 0.8220 125.3 301.4 0. 105.3 295.2 0.8838 E.2. More Results of Self-guided Stochastic Sampling Due to space constraints in the main manuscript, we provide additional qualitative comparisons to validate the efficacy of our core technical contribution: self-guided stochastic sampling. We evaluate our approach against two distinct baselines: 1) Direct Generation, which uses the pretrained video model directly (with reference image and 2D skeleton sequence); and 2) Standard ODE-based Restoration, where we employ MCS [76] as representative deterministic sampling method. Dynamic visualizations of these comparisons can be found in the Supplementary Video (00:40 - 01:32). As illustrated in Fig. 13, the challenges of this task are evident. The initial mesh-rigged animation (Input) exhibits significant artifacts, including unnatural garment dynamics and blurred edges, consistent with the limitations discussed in the main paper (e.g., Fig. 1). Direct Generation, while achieving high realism, suffers from severe identity loss and hallucinations; notably, the model generates extraneous accessories such as bag (Row 2) or watch (Row 3), rendering it unsuitable for faithful reconstruction. Furthermore, standard ODE-based sampling (MCS) fails to effectively correct the out-of-distribution nature of the coarse rendering, resulting in over-smoothed textures and persistent blurring along garment boundaries. In contrast, our self-guided stochastic sampling effectively bridges the gap between realism and fidelity. It restores photorealistic details and valid non-rigid dynamics while strictly preserving the original human identity. E.3. Sensitivity Analysis of Initial Noise Strength The initial noise strength, denoted as t0, serves as the critical hyperparameter in our self-guided stochastic sampling strategy. It governs the trade-off between the restoration capability and the fidelity to the initial coarse rendering. As illustrated in Fig. 10, we conduct comprehensive sensitivity analysis by varying t0 across the range [0.2, 0.8]. At lower noise levels (t0 {0.2, 0.4}), the sampling trajectory is too short to effectively correct the Out-of-Distribution (OOD) artifacts, resulting in outputs that retain the degradation of the source mesh-rigged animation. Conversely, at higher noise levels (t0 {0.6, 0.8}), our method demonstrates significant robustness. Unlike standard restoration methods where high noise often leads to identity loss, our self-guidance mechanism ensures that the subjects identity remains remarkably stable even at t0 = 0.8. Ultimately, we empirically select t0 = 0.6 as the default setting, as it strikes an optimal balance between generation quality, identity preservation, and sampling efficiency. E.4. More Ablations in 4D Optimization Adaptive densification. Adaptive densification and pruning are fundamental mechanisms in 3D Gaussian Splatting for capturing high-frequency details. We incorporate these strategies into our photorealistic 4D reconstruction pipeline. As demonstrated in Fig. 11a, relying solely on the deformation of the canonical geometry is insufficient to model complex texture dynamics (e.g., shifting wrinkles). Without densification, the model fails to allocate sufficient primitives to these dynamic regions, causing the clothing textures to appear significantly blurred. Mask loss regularization. Prior works, such as PERSONA, have established that geometric constraints are critical for fidelity. We validate this by ablating the mask loss during our 4D optimization. This regularization is particularly important in conjunction with our densification strategy. As shown in Fig. 11b, without the mask loss to constrain the generation of new primitives, floaters emerge in free space, and the boundary definition of the subject degrades compared to our full setting. Iterative dataset update. We compare our iterative dataset update strategy against standard single-stage optimization. As illustrated in Fig. 11c, single-stage optimization tends to result in over-smoothed textures, effectively averaging out high-frequency details due to inherent view and temporal inconsistencies in the initial supervision. In contrast, employing 18 Figure 10. Sensitivity analysis of the initial noise strength t0. We visualize restoration results across varying noise strengths. Low noise levels (t0 = 0.2, 0.4) fail to deviate sufficiently from the source, leaving artifacts from the coarse mesh rendering intact. Higher noise levels (t0 = 0.6, 0.8) effectively hallucinate plausible details and correct non-rigid dynamics. Notably, thanks to our self-guidance mechanism, the identity is preserved even at high noise strengths (t0 = 0.8), overcoming the traditional quality-fidelity trade-off. (a) Effect of Densification. (b) Effect of Mask Loss. (c) Effect of Dataset Update. Figure 11. Ablations on optimization strategies. (a) Adaptive densification is crucial for capturing high-frequency texture dynamics. (b) Mask loss regularization is essential to constrain the geometry. (c) Dataset update mitigates over-smoothing caused by inconsistent supervision, allowing the model to converge on sharp, clear details. the dataset update mechanism allows the optimization to reject inconsistent noise and converge towards high-fidelity results, significantly sharpening fine-grained features such as dress wrinkles. F. Results (Extended) F.1. Training Efficiency From single image, we adopt LHM to obtain the canonical 3D Gaussians, and generate the basic mesh-rigged animations with prepared SMPLX mesh sequences within 1 minute. During re-rendering and 4D optimization, we have 30k optimization iterations in total, and update our generated pseudo-ground truth per-5k iterations. Each video re-rerendering (sampling) step takes about 67s in average, and we simultaneously update each trajectory. The overall time cost is about 19 mins. In contrast, PERSONA needs more than 6 hours to create an animation (more than 4 hours for complex data preprocessing and longsequence video generation, and additional > 1 hour optimization). Figure 12. Comparison with image-based animation. F.2. Discussion with Image-based Animation methods. To further evaluate the effectiveness of our framework, we compare our method with state-of-the-art image-driven animation models, including Champ and Uni3C, as well as our backbone, Personalized Diffusion. As summarized in Table 5, while image-based methods such as Uni3C achieve competitive rendering quality in terms of Frechet Video Distance, they struggle to maintain high identity consistency, particularly in challenging side-view perspectives. This is reflected in their lower CLIP-Identity scores compared to our approach. Our method consistently outperforms these baselines by leveraging the robust identity priors of the personalized video diffusion model. Table 5. Quantitative comparison with state-of-the-art image-driven animation methods. Our method achieves superior balance between motion fidelity and identity preservation. Metrics Champ Uni3C Personalized Diffusion Ours FID FVD CLIP-Identity 196.3 467.2 0.7633 132.3 284.4 0.8357 138.5 330.6 0.8001 112.8 289.0 0. key advantage of our framework over video-based animation methods is the ability to distill the pose-controlled video diffusion model into 4D Gaussian Splatting (4DGS) representation. Traditional video diffusion models require timeconsuming iterative denoising process to generate each sequence. In contrast, once our distillation process is complete, the resulting 4D Gaussian representation allows for high-fidelity, real-time rendering of the personalized character in any viewpoint . This shift from generative inference to rasterization-based rendering significantly reduces the computational latency, making our approach highly suitable for interactive applications that require both personalized identity and responsive motion control. F.3. Results in ActorsHQ dataset [23] To further assess the generalization capability of our framework, we evaluate ANI3DHUMAN on the high-fidelity ActorsHQ dataset [23]. As shown in Fig. 15 and Fig. 16, our method successfully reconstructs and animates the subject using only single-view image as input. The results demonstrate that our approach effectively handles challenging articulation scenarios, such as high leg raises, while generating plausible non-rigid dynamics for loose clothing (e.g., skirts). We note that some systematic spatial misalignment between our rendering and the ground truth is observed; this is attributable to inaccuracies in the underlying SMPL parameters estimated from the raw video data, rather than limitation of the generation pipeline itself. Despite this, the method maintains strong identity preservation and temporal consistency. F.4. Additional Qualitative Results In Fig. 17, we present renderings using dynamic 360-degree camera trajectories across various subjects and complex motions. These results demonstrate that our framework generalizes effectively to diverse identities and actions, maintaining high visual fidelity and temporal consistency from all viewing angles. F.5. Limitations Although our framework achieves high-quality results in 3D human animation, it is subject to the inherent limitations of the underlying representation. Specifically, as we rely on 4D Gaussian Splatting (4DGS), the reconstruction is not strictly lossless. While the method achieves high quantitative metrics (PSNR 35 dB), the discrete nature of the primitives may still result in minor smoothing of extremely high-frequency texture details compared to the source video. 21 Figure 13. Visual comparison of sampling strategies. Cases I: Two girls are walking (Row2/4) and running (Row1/3). The Mesh-Rigged Animation (Input) exhibits unrealistic artifacts, such as unnatural cloth dynamics and blurry edges. Direct Generation suffers from severe identity shift, introducing hallucinations like bag (Row 2) or watch (Row 3). ODE Sampling (represented by MCS [76]) fails to recover high-frequency details, leaving garment edges blurry due to the OOD nature of the input. In contrast, Ours successfully restores highfidelity details and realistic motion while maintaining strict identity consistency. 22 Figure 14. Visual comparison of sampling strategies. Case II: Two girls are walking (Row1/3), running (Row2), dancing (Row4). 23 Figure 15. Qualitative evaluation on the ActorsHQ [23] dataset (I). We show single person with difference motions. The asterisk (*) denotes renderings at specific viewpoint (elevation 10, azimuth 0). Note that slight spatial misalignments between the generation and ground truth are due to inherent errors in the SMPL estimation derived from the source video. Despite relying on single-view input, our method faithfully preserves human identity and captures complex non-rigid deformations (e.g., dress dynamics), even during extreme poses such as high leg raises. 24 Figure 16. Human reconstruction results in ActorsHQ [23] dataset (II). We show different person with diverse motions. 25 Figure 17. Additional human animation results. We visualize diverse subjects performing various motions, rendered with dynamic 360-degree camera trajectories."
        }
    ],
    "affiliations": [
        "City University of Hong Kong"
    ]
}