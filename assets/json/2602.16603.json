{
    "paper_title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
    "authors": [
        "Chia-chi Hsieh",
        "Zan Zong",
        "Xinyang Chen",
        "Jianjiang Li",
        "Jidong Zhai",
        "Lijie Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge. In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs."
        },
        {
            "title": "Start",
            "content": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving Zan Zong University of Science and Technology Beijing Beijing, China zongzan@ustb.edu.cn Chia-chi Hsieh Tsinghua University Beijing, China xiejq24@mails.tsinghua.edu.cn Xinyang Chen Tsinghua University Beijing, China chenxinyang95@gmail.com 6 2 0 2 8 1 ] . [ 1 3 0 6 6 1 . 2 0 6 2 : r Jianjiang Li University of Science and Technology Beijing Beijing, China lijianjiang@ustb.edu.cn Jidong Zhai Tsinghua University Beijing, China zhaijidong@tsinghua.edu.cn Lijie Wen Tsinghua University Beijing, China wenlj@tsinghua.edu.cn Abstract The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where longrunning requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains key challenge. In this paper, we propose FlowPrefill, TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) OperatorLevel Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6 compared to state-of-the-art systems while satisfying heterogeneous SLOs. Corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference XX, Woodstock, NY 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX CCS Concepts Computing methodologies Distributed computing methodologies; Artificial intelligence. Keywords large language model, SLO-aware scheduling, disaggregated serving ACM Reference Format: Chia-chi Hsieh, Zan Zong, Xinyang Chen, Jianjiang Li, Jidong Zhai, and Lijie Wen. 2026. FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference XX). ACM, New York, NY, USA, 13 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nLarge language models (LLMs) [1, 7, 30, 51, 56] have demonstrated\nremarkable generative capabilities and are widely utilized in appli-\ncations, such as chatbots [5, 17, 37, 55] and code assistants [4, 15, 22].\nAs the demand for LLMs grows, service providers [23, 34, 50] must\nmeet diverse service level objectives (SLOs) across different users\nand tasks, making SLO attainment a key metric in LLM serving.",
            "content": "Time-to-First-Token (TTFT) is critical metric in LLM serving, as it governs the users perception of responsiveness and initiates the interactive loop. LLM inference typically consists of two phases with distinct computational profiles: the prefill phase, which processes the input prompt in parallel and is compute-bound; and the decode phase, which generates subsequent tokens autoregressively and is memory-bound. Due to this disparity, existing solutions explore the division of the two phases from both temporal and spatial aspects. In the temporal division, chunked prefill [2] splits long prompts into sequential segments to allow decode phases to interleave. However, this approach often fails to guarantee SLOs across different requests. In the spatial division, prefill-decode (PD) disaggregation [31, 36, 38, 41, 62] isolates prefill and decode workloads onto separate hardware to eliminate interference. However, by co-locating the prefill phases of all requests, PD disaggregation can exacerbate resource contention for long prompts. Conference XX, June 0305, 2018, Woodstock, NY Chia-chi Hsieh, Zan Zong, Xinyang Chen, Jianjiang Li, Jidong Zhai, and Lijie Wen Figure 1: Request length distributions and arrival patterns for different task types in QwenTrace [53]. We explore the possibility of scheduling requests with diverse SLO guarantees. For service providers, it is essential to support heterogeneous workloads [33, 41, 53]. To maximize service quality, providers must meet diverse latency requirements while maximizing goodput, defined as the maximum sustainable request rate under an SLO attainment goal (e.g., 90%). For example, real-world trace illustrated in Figure 1 contains multiple task types, each with distinct request length distributions, arrival patterns, and often different SLO requirements [8, 9, 18, 20, 62]. Task characteristics vary significantly: summarization [6] often involves long inputs with relaxed SLOs, whereas chatbot interactions [60] typically feature short inputs but demand strict latency. This heterogeneity exacerbates Head-of-Line (HoL) blocking: when long-context prefill monopolizes resources, incoming high-priority requests cannot be scheduled immediately, leading to queuing delays and TTFT SLO violations. Existing optimizations such as multi-level queues [39, 54] or output-length prediction [14, 24, 42, 44], approximate ShortestJob-First (SJF) scheduling to alleviate decode-induced HoL blocking. However, these approaches still cannot address HoL blocking caused by long prefill computations. Recent solutions like chunklevel scheduling [3, 59] introduce preemption at chunk boundaries, often pairing with Earliest-Deadline-First (EDF) policies to improve fairness and SLO adherence (Figure 2(a)). In addition, layer-level scheduling [27, 28] offers finer preemption granularity but incurs control-plane overhead proportional to model depth (Figure 2(b)). Ultimately, these fixed-size chunk approaches are trapped in dilemma: smaller chunks enhance responsiveness but incur higher kernel launch overheads and lower throughput; conversely, larger chunks optimize throughput but exacerbate HoL blocking. However, eliminating HoL blocking caused by long-input requests during the prefill phase remains challenging. In an ideal scenario, prefill execution would support fine-grained preemption triggered precisely on-demand, thereby eliminating redundant scheduling overheads. To enable efficient LLM serving with strict SLO guarantees, we must address several key challenges: Balancing preemption granularity and execution efficiency. Reducing blocking latency requires fine-grained preemption. However, existing approaches rely on fixed-granularity partitioning, Figure 2: Using different prefill granularities to meet the SLO of time-to-first-token. leading to fundamental trade-off: finer granularity increases execution overhead, while coarser granularity exacerbates blocking. An effective solution must therefore support adaptive preemption, enabling interruption at the most appropriate execution boundary without explicitly splitting requests or sacrificing efficiency. Achieving responsive scheduling without excessive overhead. Timely response to high-priority requests demands fast scheduling decisions. Yet tightly coupling scheduling with over-segmented execution granularities introduces significant control-plane overhead due to frequent checks. more desirable approach is to decouple scheduling from execution granularity, enabling timely preemption only when needed while avoiding continuous or unnecessary scheduling overhead. To address these challenges, we propose FlowPrefill, system designed to maximize goodput for online LLM serving. FlowPrefill is built upon two key ideas: operator-level preemption and eventdriven scheduling, as illustrated in Figure 2(c). Operator-level preemption enables adaptive runtime preemption at operator boundaries, allowing FlowPrefill to respond promptly to newly arrived high-priority requests, even during long-running prefill execution. Event-driven scheduling is triggered only on request arrival or completion, enabling SLO-aware prioritization without incurring scheduling overhead proportional to preemption granularity. Consequently, FlowPrefill is naturally applicable to PD disaggregation systems, where prefill is handled separately, effectively mitigating prefill-induced HoL blocking and maximizing system goodput under heterogeneous SLO requirements. In summary, our key contributions are: We reveal structural limitation where coupling execution granularity with scheduling frequency forces compromise between throughput and SLO achievement. We propose FlowPrefill, goodput-optimized system designed to decouple these two factors for efficient handling of heterogeneous inference requests. We introduce operator-level preemption, technique that exploits natural operator boundaries as preemption checks for interruption. This approach minimizes blocking time effectively while avoiding the excessive overhead of small fixed chunks. We propose an event-driven scheduling framework that separates scheduling decisions from execution granularity. By FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving Conference XX, June 0305, 2018, Woodstock, NY integrating Slack-aware EDF (S-EDF) and SLO-aware batching algorithms, our scheduler maximizes goodput and proactively mitigates SLO violations. We evaluate FlowPrefill using real-world production traces and state-of-the-art systems such as DistServe [62] and vLLM [26]. It achieves up to 5.6 higher goodput compared to baselines and can satisfy 3.1 tighter SLOs, validating its effectiveness in high-demand production environments."
        },
        {
            "title": "2.1 Transformer-based Generative Inference\nModern LLMs are built upon the Transformer [52] architecture,\nwhich consists of stacked layers containing Self-Attention mecha-\nnisms and Feed-Forward Networks (FFN). The inference execution\nis typically divided into two distinct phases: prefill and decode. First,\nin the prefill phase, the model processes the entire input sequence\n(or prompt) ğ‘‹ = {ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘› } in parallel to initialize the Key-\nValue (KV) cache and predict the first subsequent token. Following\nthis, the system transitions to the decoding phase, which operates\nautoregressively: the predicted token ğ‘¥ğ‘›+1 is appended to the se-\nquence, and the process repeats iteratively token-by-token until a\ntermination condition is met. For a single Transformer layer, the\ncore operation is Multi-Head Attention (MHA). The input hidden\nstates ğ‘‹ are projected into Query (ğ‘„), Key (ğ¾), and Value (ğ‘‰ ) ma-\ntrices using learned weight matrices ğ‘Šğ‘„,ğ‘Šğ¾,ğ‘Šğ‘‰ âˆˆ Rğ‘‘ Ã—ğ‘‘ :",
            "content": "ğ‘„ = ğ‘Šğ‘„ğ‘‹, ğ¾ = ğ‘Šğ¾ğ‘‹, ğ‘‰ = ğ‘Šğ‘‰ ğ‘‹ (1) the attention mechanism then computes weighted sum of the values based on the similarity between queries and keys. Formally, the output is calculated as: Attention(ğ‘„, ğ¾, ğ‘‰ ) = softmax (cid:19) ğ‘‰ (cid:18) ğ‘„ğ¾ğ‘‡ ğ‘‘ğ‘˜ (2) where ğ‘‘ğ‘˜ is the dimension of the key vectors. Following the attention block, the output passes through position-wise FFN and residual connections to produce the input for the subsequent layer. This computation is repeated across all ğ¿ layers of the model."
        },
        {
            "title": "2.2 HoL Problem in Disaggregated Serving\nLLM inference comprises two phases with contrasting resource\ndemands. The prefill phase processes the entire input in parallel\nusing Matrix-Matrix Multiplications (GEMMs). It is compute-bound\ndue to high arithmetic intensity, as model weights are reused across\nall input tokens. Conversely, the decode phase generates tokens\nautoregressively using Matrix-Vector Multiplications (GEMVs). This\nphase is memory-bound, as weights must be fetched from HBM\nfor each token, limiting performance by bandwidth rather than\ncomputation.",
            "content": "To mitigate resource contention between these phases, modern systems adopt PD disaggregation [62], which decouples prefill and decode tasks onto separate GPU instances. This architecture effectively eliminates the interference of compute-intensive prefill bursts on the latency-sensitive decode phase. However, disaggregation shifts contention entirely to the prefill instances. Since prefill is compute-bound, single long-context request can monopolize GPU for hundreds of milliseconds, causing severe HoL blocking for subsequent requests. Consequently, meeting diverse TTFT SLOs becomes challenging, as the scheduler struggles to process heavy workloads without stalling high-priority, short-latency requests."
        },
        {
            "title": "3.1 Preemption Granularity v.s. Efficiency\nTo mitigate the HoL blocking caused by long prefill requests, ex-\nisting systems employ fixed chunking strategies, such as Chunked\nPrefill [2] and Layered Prefill [27, 28]. However, our analysis reveals\nthat simply reducing execution granularity introduces a fundamen-\ntal conflict between responsiveness, computational efficiency, and\nscheduling overhead.\nEfficiency Degradation in Small Chunking. Chunked prefill\npartitions long inputs to enable scheduling at chunk boundaries. As\nillustrated in Figure 3, while reducing chunk size improves preemp-\ntion granularity, it comes at a steep cost to efficiency. Small chunks\nlead to a significant throughput collapse due to: (1) increased kernel\nlaunch overheads; (2) redundant memory accesses to the KV cache\n(since causal attention [43] requires reloading prior keys/values);\nand (3) under-utilization of device computation. Conversely, large\nchunks recover throughput but reintroduce substantial blocking\nlatency, failing to satisfy strict TTFT SLOs.\nCoupling of Scheduling Decision and Execution Granular-\nity. Layered prefill avoids the memory overhead of chunking by\nutilizing the natural boundaries of Transformer layers. However,\nthis approach tightly couples execution granularity with schedul-\ning frequency. If the system performs a scheduling check at every\nlayer to maximize responsiveness, it incurs dramatic control-plane\noverheads. In practice, high-priority requests do not arrive con-\ntinuously. Triggering complex scheduling logic at every potential\nboundary disrupts execution continuity and degrades overall sys-\ntem efficiencyâ€”especially when no preemption is actually required.\nFurthermore, for extremely long inputs, even a single layerâ€™s exe-\ncution can take hundreds of milliseconds, implying that layer-level\npreemption alone imposes an unavoidable granularity bound.",
            "content": "Conference XX, June 0305, 2018, Woodstock, NY Chia-chi Hsieh, Zan Zong, Xinyang Chen, Jianjiang Li, Jidong Zhai, and Lijie Wen Figure 3: Throughput and latency of chunked prefill for 32K-token input under different chunk sizes when serving an Llama3-8B. Alternating colors within each bar demarcate individual chunks. Takeaway-1: Employing fixed, excessively small execution units to minimize blocking degrades hardware efficiency due to redundant memory accesses and kernel launch overheads. Coupling scheduling decisions tightly with these execution boundaries introduces unnecessary control-plane overheads."
        },
        {
            "title": "3.2 Workload Asymmetry in Prefill Batching\nBeyond single-request execution, we observe a distinct asymmetry\nin how batching affects requests of different lengths.\nThroughput Bound for Short Requests. As shown in Figure 4(a),\na single short request (32â€“256 tokens) cannot fully saturate the\nGPUâ€™s massive parallelism. Consequently, batching multiple short\nrequests is essential. Throughput increases rapidly with batch size\nbefore saturating, while latency grows only modestly (Figure 4(b)).\nThis indicates that for short inputs, aggressive batching significantly\nboosts hardware utilization and system throughput while maintaining\nSLO compliance.\nIn contrast, long prefill\nLatency-Bound for Long Requests.\nrequests are compute- and memory-intensive enough to saturate\nthe GPU individually. For these requests, batching provides minimal\nthroughput gains but causes linear latency inflation due to the\naggregated computational load. As a result, long requests in large\nbatches are highly prone to TTFT SLO violations without yielding\nproportional system-level benefits.",
            "content": "Takeaway-2: It is critical for boosting the throughput of short requests with minimal latency cost, whereas for long requests, it offers negligible throughput gains while significantly increasing the risk of SLO violations."
        },
        {
            "title": "4 Overview\nFlowPrefill is an LLM serving system optimized for goodput. It di-\nrectly addresses the trade-off between responsiveness and through-\nput caused by the fixed setting of granularity, as discussed in Â§3.\nBy decoupling preemption from scheduling granularity, FlowPrefill\nenables timely preemption without sacrificing throughput, thereby",
            "content": "(a) Throughput (b) Normalized TTFT Figure 4: Throughput and normalized TTFT during the prefill phase for different input lengths when serving an Llama38B. mitigating severe HoL blocking during the prefill phase and improving online service quality. As shown in Figure 5, typical serving system consists of three major components: the Proxy, Prefill Instances, and Decode Instances. Our primary optimizations are concentrated in the prefill instances. Each prefill instance comprises three core modules: the Request Queue, the Execution Pool, and the Scheduler. We briefly describe each component below and defer detailed design to 5.1 and 5.2. Proxy. The Proxy acts as the central coordination component of the system. It receives online requests from the frontend, parses them, and sequentially dispatches each request to the prefill and decode instances for inference. Upon inference completion, the Proxy returns the generated outputs to users. When multiple instances are deployed, the Proxy distributes incoming requests using simple round-robin policy, without considering instance-level load imbalance, which is beyond the scope of this work. Prefill Instance. The core optimizations of FlowPrefill are implemented within the prefill instances to collectively address HoL blocking caused by long-running prefill computation under heterogeneous workloads. To this end, we improve both the scheduling and execution logic of the native inference framework, enabling efficient and safe preemptive scheduling driven by dynamic runtime demands. Each prefill instance consists of the following three components: Request Queue. Each prefill instance maintains dedicated Request Queue that tracks the logical states of all requests in the system. These states are used to support admission control and scheduling decisions. Incoming requests from the Proxy are enqueued into the Request Queue, while completed requests return their results to the Proxy and are subsequently removed. This design ensures that the Scheduler always operates on an up-to-date and consistent view of all requests. Execution Pool. The Execution Pool manages execution tasks from different request batches. At runtime, it executes at most one task and safely preserves the execution state of preempted tasks until they are resumed. It is responsible only for execution management in response to explicit commands issued by the Scheduler (e.g., preempt, submit, and resume) and does not make request selection or scheduling FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving Conference XX, June 0305, 2018, Woodstock, NY Figure 5: Overview of FlowPrefill. decisions. The detailed interaction between the Scheduler and the Execution Pool is described in 5.1. Scheduler. The Scheduler is responsible for request ordering and dispatching. To support dynamic preemption while minimizing scheduling overhead, scheduling is event-driven. An Event Monitor observes an event queue, where actions such as request arrivals and completions enqueue events. The Event Monitor consumes events sequentially, and each event triggers scheduling round. During each round, the Scheduler consults the request state maintained in the Request Queue and applies the slack calculator, priority indicator, and SLO-aware batching modules to select the highest-priority request batch. Based on this decision, it issues control commands to the Execution Pool, ensuring that the current execution task always corresponds to the highest-priority one. The detailed design is described in 5.2. Decode Instance. Decode instances reuse the default execution logic of the native inference framework and schedule decode requests using first-come-first-served (FCFS) policy. Since decoding optimization is not our primary objective, we concentrate our design and evaluation exclusively on the prefill phase."
        },
        {
            "title": "5 Method\n5.1 Operator-Level Preemption\nRecent approaches attempt to mitigate HoL blocking through chunk-\nor layer-level scheduling, where preemption is only possible at\nfixed execution boundaries. However, as discussed in Â§3.1, this\nfixed-granularity design introduces a fundamental trade-off: finer\ngranularity improves responsiveness but incurs substantial execu-\ntion and scheduling overhead, while coarser granularity preserves\nefficiency at the cost of increased blocking latency. To overcome\nthis limitation, we propose operator-level preemption, which en-\nables preemption immediately after the completion of an operator\nwithout request splitting. This design exploits the finest practical\nexecution boundary that preserves kernel semantics, enabling high\nresponsiveness without sacrificing throughput.",
            "content": "Figure 6: Execution logic is hierarchical, and preemption checks can be placed at different execution boundaries. Preemption checks at the chunk or layer level result in coarser preemption granularity, whereas FlowPrefill places them at core operator boundaries for the finest-grained preemption. (Blue circles denote preemption checks.) To avoid the expensive overhead of processor context-level preemption [11], FlowPrefill adopts cooperative preemption at operator boundaries. During execution, lightweight preemption checks are inserted between operators, allowing the mechanism to generalize across different models and deployment scenarios by flexibly selecting operator boundaries. In this work, which targets dense LLMs, we place preemption checks at the boundaries between core operators, including qkv_proj, attn, o_proj, gate_up_proj, down_proj. As illustrated in Figure 6, preemption checks are performed at every operator boundary to monitor preemption signals from the Scheduler. These operators constitute the smallest practical execution units in LLM architectures, establishing minimal lower bound on preemption latency. When preemption is requested, the execution runtime adaptively detects the nearest operator boundary and suspends execution upon its completion, enabling timely responses to dynamic preemption demands. Each preemption check Conference XX, June 0305, 2018, Woodstock, NY Chia-chi Hsieh, Zan Zong, Xinyang Chen, Jianjiang Li, Jidong Zhai, and Lijie Wen Algorithm 1 SLO-Aware Batching 1: Input: Highest-priority request ğ» , Candidate requests ğ¶, Batch token budget ğº 2: Function SLOawareBatching(ğ», ğ¶, ğº): 3: ğµ {ğ» } ğ‘‡ğ‘Ÿğ‘’ğ‘šğ‘ğ‘–ğ‘› ğ» .ğ‘‘ğ‘’ğ‘ğ‘‘ğ‘™ğ‘–ğ‘›ğ‘’ Timer.time() ğ‘ ğ» .ğ‘›ğ‘¢ğ‘š_ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘  for ğ‘Ÿ ğ¶ do ğ‘› ğ‘ + ğ‘Ÿ .ğ‘›ğ‘¢ğ‘š_ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘  ğ¿ predict_latency(ğ‘›) if ğ‘‡ğ‘Ÿğ‘’ğ‘šğ‘ğ‘–ğ‘› > ğ¿ and ğ‘› < ğº then ğµ ğµ {ğ‘Ÿ } ğ» .ğ‘›ğ‘¢ğ‘š_ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘  ğ‘› ğ‘ ğ‘› return ğ», ğµ 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: To reason about request urgency, we propose slack-aware earliest-deadline-first (S-EDF) scheduling policy. Each request is assigned priority defined as ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿğ‘–ğ‘¡ğ‘¦ = sgn(ğ‘ ğ‘™ğ‘ğ‘ğ‘˜) ğ‘‘ğ‘’ğ‘ğ‘‘ğ‘™ğ‘–ğ‘›ğ‘’ (3) where ğ‘ ğ‘™ğ‘ğ‘ğ‘˜ = ğ‘‘ğ‘’ğ‘ğ‘‘ğ‘™ğ‘–ğ‘›ğ‘’ ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡_ğ‘¡ğ‘–ğ‘šğ‘’ (cid:154)TTFT, and sgn() is signum function returning 1 for positive slack and -1 for negative slack. The ğ‘‘ğ‘’ğ‘ğ‘‘ğ‘™ğ‘–ğ‘›ğ‘’ is the request arrival time plus its TTFT SLO, (cid:154)TTFT is the predicted TTFT obtained from lightweight offlinefitted model. In other words, the request with the highest priority is the one with the earliest deadline and non-negative slack. By incorporating slack into the priority, S-EDF can proactively deprioritize requests that cannot meet their deadlines, thereby improving the stability of SLO adherence. As discussed in 3.2, existing batching policies do not account for heterogeneous latency requirements. To maximize goodput, we adopt an SLO-aware batching strategy (Algorithm 1), which batches the highest-priority request ğ» with compatible requests only if its remaining time ğ‘‡remain can accommodate the predicted batch latency ğ¿ and the batch token budget ğº is not exceeded. The batch token budget ensures that the benefits of batching short requests are properly captured. When request is admitted to the batch, the aggregate token count of ğ» is incrementally updated to guide subsequent admission decisions. By allocating controlled token budget to ğ» within latency constraints, this strategy minimizes the risk of SLO violations while maximizing system throughput. The complete scheduling procedure is shown in Algorithm 2. At each scheduling step, we first wait for an event (arrival or completion) to occur (line 4). After that, all requests ğ‘„ğ‘ğ‘™ğ‘™ are ranked by descending priority, and the highest-priority request ğ» is selected (lines 512). If ğ» is in the waiting queue ğ‘„ğ‘¤, indicating that it has not yet been assigned an execution task, we attempt SLO-aware batching (Algorithm 1) to batch it with compatible requests (lines 1315). Finally, we check whether the currently running execution ğ¸ should be preempted, moving it to the preemption queue ğ‘„ğ‘ if needed, and then either submits new execution task or resumes previously preempted task (lines 1626). By combining Figure 7: Cooperative preemption process. consists only of simple concurrency primitive operations, incurring negligible overhead. Once execution is instrumented with preemption checks, it continuously monitors whether the Scheduler has issued preemption signal. Upon detecting such signal, execution cooperatively performs preemption and safely suspends the running task. Figure 7 illustrates the cooperative preemption mechanism. When higherpriority request arrives, the Scheduler sets preemption signal to notify the Execution Pool and waits for an acknowledgment. Rather than interrupting execution immediately, the runtime performs preemption check after the current operator completes. If the signal is set, the runtime unsets it to indicate successful preemption and sends an acknowledgment to the Scheduler. It then suspends the current task and moves it to the preempted-task queue, where it awaits resumption by the Scheduler. If the signal is unset, execution proceeds without interruption. Upon receiving the acknowledgment, the Scheduler submits the higher-priority request for execution. By restricting preemption to operator boundaries, FlowPrefill avoids interrupting in-flight kernels while bounding preemption latency by the execution time of single operator. Tensor parallelism supporting. Serving of large-scale LLMs typically uses tensor parallelism, which runs multiple processes that synchronize via communications. When preemption is required, simply suspending each process is unsafe because faster processes may reach communication points while slower ones are paused, leading to deadlocks. To prevent this, FlowPrefill uses synchronized iteration counter among tensor parallel processes and only suspends them when they reach the same counter, ensuring safe preemption without disrupting communication."
        },
        {
            "title": "5.2 Event-Driven Scheduling\nOperator-level preemption provides the mechanism for safe and effi-\ncient interruption. However, supporting heterogeneous workloads\nwith diverse latency requirements further requires a principled\nscheduling policy that determines when to preempt and which re-\nquests to prioritize. To this end, we propose event-driven scheduling,\nwhich decouples scheduling decisions from execution granularity\nwhile enabling SLO-aware prioritization. Instead of continuously\npolling for preemption opportunities, scheduling is triggered only\nby two types of events: request arrival and completion. This design\nenables timely responses to high-priority requests while avoiding\nthe overhead of frequent scheduling checks.",
            "content": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving Conference XX, June 0305, 2018, Woodstock, NY Figure 8: Example of serving two requests with different priorities in FlowPrefill. Algorithm 2 Event-Driven Scheduling 1: Input: Batch token budget ğº 2: Initialize ğ‘„ğ‘¤ , ğ‘„ğ‘ , ğ¸ ğ‘›ğ‘¢ğ‘™ğ‘™ 3: while True then 4: Wait for request arrival or completion event 5: ğ‘Ÿğ‘¤ get_new_requests() ğ‘„ğ‘¤ ğ‘„ğ‘¤ ğ‘Ÿğ‘¤ ğ‘„ğ‘ğ‘™ğ‘™ ğ‘„ğ‘¤ ğ‘„ğ‘ {ğ¸} if ğ‘„ğ‘ğ‘™ğ‘™ = then continue Sort requests ğ‘„ğ‘ğ‘™ğ‘™ in descending order of ğ‘Ÿ .priority, yielding (ğ‘Ÿ1, ğ‘Ÿ2, . . .) with ğ‘Ÿ1 having the highest priority. Initialize ğ» ğ‘Ÿ1, ğµ if ğ» ğ‘„ğ‘¤ then ğ¶ ğ‘„ğ‘ğ‘™ğ‘™ ğ‘„ğ‘ {ğ» } ğ», ğµ SLOawareBatching(ğ», ğ¶, ğº) if ğ» ğ¸ then if ğ¸ null then preempt_execution(ğ¸) ğ‘„ğ‘ ğ‘„ğ‘ {ğ¸} if ğµ then submit_new_execution(ğ», ğµ) ğ‘„ğ‘¤ ğ‘„ğ‘¤ ğµ else resume_execution(ğ» ) ğ‘„ğ‘ ğ‘„ğ‘ {ğ» } ğ¸ ğ» 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: event-driven scheduling with operator-level preemption, FlowPrefill achieves responsive, SLO-aware scheduling while avoiding the overhead of fine-grained, continuously triggered scheduling. An illustrated example. As shown in Figure 8, we present simple example to illustrate the event-driven scheduling logic in FlowPrefill, covering all control commands including submit, preempt, and resume. The example involves two requests, request with low priority and request with high priority. Initially, there is no executing task and the set of preempted tasks is empty. When request arrives, it generates an event that triggers scheduling round, and since no execution is running, the Scheduler issues submit command to execute request as task A. When request arrives, it generates another event and triggers new scheduling round. Because request has higher priority, the Scheduler issues preempt command to suspend task and free the execution slot, followed by submit command to execute request as task B. After task completes, its completion generates an event that triggers another scheduling round. Since no higher-priority requests remain, request becomes the highest-priority request and the Scheduler issues resume command to continue its execution task. Finally, when task completes, it generates an event that triggers scheduling round with no runnable requests. Throughout this process, the Scheduler is responsible for issuing control commands, while the Execution Pool carries out the corresponding actions on the execution tasks of the relevant requests, as described in 4."
        },
        {
            "title": "6 Evaluation\n6.1 Experimental Setup\nImplementation. We implement FlowPrefill on vLLM-0.11.2 [26].\nThe Execution Pool uses ThreadPoolExecutor to manage per-\nrequest execution tasks, each running in its own thread with a\ndedicated CUDA stream and vLLM runner, synchronized via con-\ncurrency primitives.\nHardware. We conduct experiments on a server equipped with\neight NVIDIA A800-SXM4-80GB GPUs interconnected via 200 GB/s\nNVLink, two 52-core Intel Xeon Platinum 8470 CPUs, and 1 TB of\nhost memory.\nModels. We evaluate FlowPrefill on three models of varying sizesâ€“\nLlama3-8B, Qwen2.5-14B [49], Llama3-70B [10]â€“using one-, two-,\nand four-way tensor parallelism respectively. We also use a pop-\nular mixture-of-expert model Qwen3-30B-A3B [56] to show the\ngeneralization of our methods.\nWorkloads. The evaluation uses a real-world production trace,\nQwenTrace [53], which contains four task types with distinct prompt-\nlength distributions (Table 1) and timestamped request arrivals",
            "content": "Conference XX, June 0305, 2018, Woodstock, NY Chia-chi Hsieh, Zan Zong, Xinyang Chen, Jianjiang Li, Jidong Zhai, and Lijie Wen (a) Llama3-8B (b) Qwen2.5-14B (c) Llama3-70B Figure 9: End-to-end performance of three models under varying request rates and SLO requirements on QwenTrace. Table 1: Prompt length across diverse tasks in the evaluation trace."
        },
        {
            "title": "Mean",
            "content": "P99 Std. Ratio(%) Chatbot (Text) Image Understanding (Image) Web Search (Search) Summarization (File) 590 532 5976 6833 3040 2764 16635 652 510 3456 5186 68 8 20 4 Table 2: Settings of different TTFT SLO requirements."
        },
        {
            "title": "File",
            "content": "Llama3-8B Qwen2.5-14B Llama3-70B 0.25s 0.4s 1.0s 0.5s 0.8s 2.0s 4.0s 6.5s 15.0s 6.0s 9.0s 18.0s minimal SLO the system can meet, which are marked by vertical lines in the curve plots. Baselines. We evaluate the effectiveness of FlowPrefill on both PDdisaggregated and PD-colocated architectures. 1) Disaggregated Setting: We compare against the state-of-the-art system DistServe [62] under 1P1D (one prefill, one decode instance) setup. Since DistServe defaults to First-Come-First-Served (FCFS) policy, we augment its prefill stage with Chunked Prefill (CP) [2] and an EarliestDeadline-First (EDF) policy to enable SLO-aware scheduling. We evaluate two variants: DistServe-CP2K (chunk size 2048) and DistServeCP8K (chunk size 8192). We select 2048 as the primary baseline as it empirically yields the best performance, while 8192 serves as reference for coarse-grained preemption. 2) Co-located Setting: We compare FlowPrefill against vLLM-0.11.2 [26], which natively supports chunked prefill via temporal multiplexing. This comparison demonstrates the versatility of our method across different serving paradigms. (Figure 1). We preprocess the trace by converting all requests into single-turn queries. To reflect realistic multi-SLO requirements  (Table 2)  , we assign task-specific SLOs based on prior work [20, 62] and application semantics. Specifically, text requests have the strictest SLOs for interactive chat, image requests tolerate moderate latency, while search/file requests involve long inputs and thus receive much looser SLOs. As the trace contains only request lengths and does not include the actual prompt contents, we generate random token sequences that conform to the specified lengths for each request. Metrics. Since FlowPrefill performs preemption only during the prefill phase, we focus on TTFT SLO attainment as the primary metric. We define goodput as the maximum sustainable request rate at 90% SLO attainment. By varying the request rate and latency requirements, we evaluate both the maximum goodput and the"
        },
        {
            "title": "6.2 End-to-End Speedup\nWe compare the end-to-end performance of FlowPrefill with the\nbaselines on QwenTrace by varying the request rate and SLO re-\nquirements.\nSLO attainment vs. request rate. We evaluate FlowPrefill under\nvarying request-rate scales for all three models. As shown in the first\nrow of Figure 9, increasing the request rate raises system load and\nleads to a gradual decline in SLO attainment, as more requests fail to\nmeet their latency requirements. The vertical lines indicate the max-\nimum goodput. Across all models, FlowPrefill sustains 4.7Ã—â€“5.6Ã—\nhigher request rates than DistServe, and outperforms DistServe-\nCP2K and DistServe-CP8K by up to 2.0Ã— and 4.5Ã—, respectively. This\nimprovement primarily stems from FlowPrefillâ€™s ability to promptly\npreempt low-priority requests under heterogeneous SLOs. Upon",
            "content": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving Conference XX, June 0305, 2018, Woodstock, NY Figure 10: Performance comparison under three scheduling policies. Figure 11: Performance comparison under varying batch token budgets and no batching. the arrival of higher-priority requests, event-driven scheduling promptly identifies preemption opportunities, and operator-level preemption suspends ongoing long-input prefill executions with relaxed SLOs to free resources for latency-critical requests, enabling near non-blocking interruption. As result, latency-sensitive requests are processed without undue delay, which reduces HoL blocking during the prefill phase and prevents SLO violations. When the request rate continues to increase, all baselines reach their goodput limits. In contrast, FlowPrefill continues to admit additional requests while satisfying latency constraints. This is enabled by SLO-aware batching, which selects requests that can be batched efficiently without violating their deadlines, and by S-EDF, which proactively deprioritizes requests that cannot meet their deadlines under heavy load. Together, these mechanisms prevent sharp drops in SLO attainment and maintain higher overall performance under high load. DistServe performs poorly due to its lack of SLO awareness. DistServe-CP2K initially outperforms DistServe-CP8K because its finer-grained preemption better favors latency-sensitive requests. However, as the request rate increases, DistServe-CP2K approaches and may fall behind DistServe-CP8K, since smaller chunks incur higher splitting overhead. SLO attainment vs. SLO requirements. The second row of Figure 9 evaluates robustness under different latency requirements. We fix the request rate and linearly scale the latency targets in Table 2 using an SLO scale parameter. The vertical lines indicate the minimum SLO targets each system can support. Compared to DistServe-CP2K, FlowPrefill supports 1.52.3 tighter SLOs, and compared to DistServe-CP8K, 2.13.1 tighter SLOs. These gains result from FlowPrefills near non-blocking execution of latencysensitive requests, which improves SLO attainment and reduces both average and tail latencies. As the SLO scale increases, the SLO attainment of DistServe-CP8K gradually improves and can eventually surpass that of DistServe-CP2K, since larger chunk sizes reduce request fragmentation and tail latency under looser SLO constraints. Figure 12: Average preemption blocking time under operatorand layer-level preemption boundaries. Different preemption granularities are realized by placing preemption checks at different execution boundaries. that have already missed their deadlines are assigned the lowest priority. As shown in Figure 10, S-EDF significantly outperforms both baselines in terms of maximum goodput and minimum SLO target. This advantage arises because S-EDF proactively deprioritizes requests that are unlikely to meet their SLOs under high load, thereby preventing collapse of SLO attainment caused by accumulated queueing delays. In contrast, D-EDF lacks foresight into request feasibility and may waste resources on requests that are destined to violate their SLOs, whereas S-EDF leverages slack estimation to enable more robust scheduling. SLO-aware batching. batch token budget is imposed to limit batching size. As discussed in 3.2, exceeding certain token threshold yields little throughput gain while increasing SLO violation risk. Figure 11 compares SLO attainment and throughput under varying batch token budgets and no batching. The left figure shows that larger budgets increase SLO violation risk but still outperform no batching. The right figure shows that no batching yields the lowest throughput, while larger budgets improve throughput with diminishing returns, with little difference between 4K and 8K. These results suggest that selecting moderate batch token budget captures the throughput benefits of batching short requests while reducing SLO violation risk, consistent with the analysis above."
        },
        {
            "title": "6.4 Runtime Analysis\nPreemption blocking time. We define the preemption blocking\ntime as the interval between the moment a higher-priority request\nsends a preemption signal and the moment the new request can be\nsubmitted (i.e., when the ACK is received in Figure 7).",
            "content": "To quantify the benefits of our approach, we benchmark it against the layer-level preemption boundaries proposed in concurrent work Conference XX, June 0305, 2018, Woodstock, NY Chia-chi Hsieh, Zan Zong, Xinyang Chen, Jianjiang Li, Jidong Zhai, and Lijie Wen (a) Llama3-8B (b) Qwen2.5-14B (c) Llama3-70B Figure 13: Real and predicted TTFT for three models serving trace segment from QwenTrace at given request rate. Figure 14: Performance comparison under single-SLO workload. FlowPrefill matches baseline throughput, indicating negligible operator-level preemption (5.1) overhead but delivers better SLO attainment as request rates scale. [28]. While layer-level scheduling improves granularity, we observed that it remains coarse enough to incur significant delays. As illustrated in Figure 12, operator-level preemption reduces the average blocking time by 3.54.2 compared to layer-level preemption, with all observed latencies remaining below 4.5 ms. This minimal latency effectively achieves near non-blocking execution for high-priority requests, substantially mitigating HoL blocking during the prefill phase. While some deployment scenarios may not require such finegrained preemption, FlowPrefill allows preemption boundaries to be configured flexibly to accommodate different requirements. Importantly, even when using the finest-grained operator-level preemption, system performance is not adversely affected. Consequently, operator-level preemption is adopted as the default configuration in FlowPrefill, as it provides robust performance and generality for most production workloads. TTFT prediction. TTFT prediction is required for slack calculator and SLO-aware batching. Our prediction model fits polynomial to offline prefill profiles, with ğ‘¥ as token count and ğ‘¦ as predicted TTFT. To validate the model, we record the real TTFT of each request during evaluation. As shown in Figure 13, most requests have real TTFT close to their predictions, with few outliers, demonstrating accurate prediction even in online scenarios. This accuracy is enabled by the PD-disaggregated setting, where decode execution does not interfere and prefill computation scales nearly linearly with token count, allowing simple polynomial fit to suffice. Scheduling cost. We count the number of event-driven scheduling decisions during evaluation. Since each request generates at most two events and each event triggers scheduling round, the total number of scheduling decisions is approximately twice the number of requests and slightly lower when batching occurs. Not every scheduling round results in control commands. Consequently, the number of scheduling rounds that actually trigger submit, preempt, or resume actions is smaller than the total number of scheduling rounds."
        },
        {
            "title": "6.5 System compatibility\nAll experiments in this section use Llama3-8B, except for the MoE\nmodels. Experiments other than the single-SLO scenario use a trace\nsegment from QwenTrace.\nSingle-SLO scenario. We evaluate FlowPrefill under a single-SLO\nworkload by sampling 500 requests from ShareGPT [61] and gener-\nating arrivals with a Poisson process at varying rates. All requests",
            "content": "Figure 15: Performance of FlowPrefill with chunked prefill under different chunk sizes. share the chatbot SLO in Table 2 and have input lengths below 2K, resulting in limited preemption. We therefore compare FlowPrefill against DistServe-CP2K. To evaluate the robustness of FlowPrefill, we report not only SLO attainment but also throughput, which measures system efficiency regardless of whether SLOs are met. As shown in Figure 14, FlowPrefill maintains high SLO attainment while achieving throughput comparable to the baseline, indicating that it preserves responsiveness without sacrificing throughput. These results demonstrate that the preemption checks used in 5.1 do not introduce measurable efficiency loss. Unlike chunklevel or layer-level scheduling, which requires frequent scheduling interventions, operator-level preemption in FlowPrefill relies on lightweight preemption checks. This design enables fine-grained preemption without interrupting execution, thereby achieving efficient preemption with minimal overhead. Chunked prefill. As previously noted, chunked prefill is often employed to reduce HoL blocking in the prefill stage, but it creates trade-off between responsiveness and throughput that depends on the chosen chunk size. Using large chunks has minimal effect on throughput but is typically impractical because preemption can only occur at coarse boundaries. In contrast, FlowPrefill supports fine-grained, operator-level preemption, so the blocking time is limited by the maximum runtime of any single operator. For very long inputs, however, one operator can still introduce noticeable latency. In such cases, combining operator-level preemption with chunked prefill reduces individual operator runtimes and further tightens the upper bound on blocking time. Based on the above observations, we evaluate FlowPrefill combined with chunked prefill under varying chunk sizes. As shown in Figure 15, small chunk sizes (e.g., 2K tokens) provide finer preemption granularity but incur substantial overhead from frequent FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving Conference XX, June 0305, 2018, Woodstock, NY (a) TTFT (b) TBT Figure 16: Performance comparison under the PD-colocation setting across varying request rates, reporting TTFT and TBT SLO attainment. request splitting, degrading performance. Large chunk sizes (e.g., 16K tokens) reduce splitting overhead but lack sufficient preemption granularity, also hurting performance. An intermediate chunk size balances operator execution time and overall throughput. These results indicate that, when appropriately configured, combining FlowPrefill with chunked prefill can further improve performance. PD-Colocation. Although FlowPrefill is primarily designed for PD-disaggregated serving, it can also be adapted to PD-colocation setting. We adopt an intra-PD architecture [29, 46], where prefill and decode instances are colocated within the same device group. To avoid the complexity of cross-process management, we run on Python 3.14t [40], which disables the Global Interpreter Lock (GIL) and allows both instances to be managed within single main process while sharing the same KV-cache logic and memory across devices in the group. For the PD-colocated evaluation, we benchmark FlowPrefill against vLLM configured with chunked prefill and an empirically tuned chunk size of 2048 tokens (vLLM-CP2K). Since the colocated setup utilizes only half the number of GPUs compared to the PD-disaggregated configuration, we adjust the SLO constraints accordingly. Specifically, we relax the TTFT SLO to 3 the values in Table 2, and set the Time-Between-Tokens (TBT) SLO to 0.1 seconds for text/image requests and 0.2 seconds for search/file requests. Figure 16(a) demonstrates that FlowPrefill achieves substantially higher TTFT SLO attainment as request rates scale. We also observe positive effects on TBT SLO attainment (Figure 16(b)). Typically, rising request rates increase prefill demand, causing the interleaving of prefill and decode phases to degrade TBT performance. However, because FlowPrefills adaptive preemption expedites short prefills, it incidentally reduces the duration of interference, improving TBT SLO attainment by up to 1.6. Although FlowPrefill primarily targets PD-disaggregated serving, this evaluation demonstrates its robustness under PD-colocation. Mixture-of-Experts (MoE) models. To further validate the generality of our approach, we extend FlowPrefill to MoE models. In MoE architectures, attention computation uses the same operators as dense models, while the main difference lies in the FFN layer, which includes gated router and multiple expert FFNs. In practice, this introduces two additional fused operators, gate and experts. Following the FlowPrefill design, preemption checks are inserted at the boundaries of these operators to enable operator-level preemption. This extension is plug-and-play and can be applied to variety of model architectures. Figure 17: Performance comparison for serving Qwen3-30BA3B. We evaluate FlowPrefill on Qwen3-30B-A3B [56] with 2-way tensor parallelism. Figure 17 compares FlowPrefill against DistServeCP2K and DistServe-CP8K. The results are consistent with the end-to-end performance observed for dense models, with FlowPrefill achieving up to 1.6 higher goodput and up to 2.4 tighter SLO attainment. These results demonstrate that FlowPrefill is compatible with MoE models and remains effective under more complex model structures."
        },
        {
            "title": "7 Related Work\nAggregated LLM Serving Systems. Early LLM serving systems [12,\n26, 35, 57] optimize performance within a unified execution model.\nOrca [57] introduces continuous batching to enable per-iteration\nscheduling, while vLLM [26] improves memory efficiency through\nfine-grained KV-cache management with PagedAttention. SARATHI [2]\nmitigates HoL blocking by interleaving decode with chunked prefill\nexecution. More recent systems [19, 29, 46] adopt intra-PD architec-\ntures that dynamically adjust resources between prefill and decode\nto improve utilization. While these approaches partially alleviate\ninterference between colocated prefill and decode execution, they\nstruggle to provide consistently high service quality in latency-\nsensitive online serving scenarios.\nDisaggregated LLM Serving Systems. Prefillâ€“decode disaggre-\ngated architectures [13, 21, 31, 36, 38, 41, 47, 62] further improve ser-\nvice quality by decoupling the two phases. DistServe [62] jointly op-\ntimizes per-phase resource allocation to improve SLO-constrained\ngoodput, while Splitwise [38] focuses on cost-efficient placement\non heterogeneous resources. TetriInfer [21] stabilizes decode la-\ntency under mixed workloads, and Mooncake [41] enables efficient\ndisaggregation via a distributed KV-cache store. Although disaggre-\ngation removes direct prefillâ€“decode interference, existing systems\nremain inefficient under heterogeneous SLOsâ€”particularly dur-\ning the compute-intensive prefill phase, where severe HoL block-\ning persists. In contrast, FlowPrefill targets multi-SLO goodput\noptimization and mitigates prefill-induced HoL blocking through\noperator-level fine-grained preemption.\nLLM Scheduling. Existing LLM scheduling policy typically lever-\nages request characteristics, such as input length or SLO targets,\nto balance latency and throughput. Prior work [14, 44, 54] approxi-\nmates SJF scheduling using multi-level queues or length prediction.\nVTC [45] and QLM [39] emphasize fairness and SLO adherence\nacross tenants or models. Other systems [16, 48, 58] reduce SLO\nviolations via resource-aware dynamic scheduling, and several stud-\nies [8, 20, 25, 32] explicitly address multi-SLO prioritization. Recent",
            "content": "Conference XX, June 0305, 2018, Woodstock, NY Chia-chi Hsieh, Zan Zong, Xinyang Chen, Jianjiang Li, Jidong Zhai, and Lijie Wen approaches [3, 59] use chunked prefill with EDF-style policies to reduce HoL blocking from long requests. However, these methods primarily rely on prioritization and coarse-grained preemption. In contrast, FlowPrefill directly targets severe HoL blocking caused by long prefill execution and ensures service quality through finegrained preemption and efficient scheduling."
        },
        {
            "title": "8 Conclusion\nThis paper presented FlowPrefill, a goodput-oriented LLM serving\nsystem that mitigates prefill-induced HoL blocking under heteroge-\nneous SLO requirements. By decoupling preemption granularity\nfrom scheduling frequency through operator-level preemption and\nevent-driven scheduling, FlowPrefill achieves near non-blocking\nresponsiveness without sacrificing execution efficiency. Extensive\nevaluations on real-world production traces demonstrate that Flow-\nPrefill substantially sustains 4.7Ã—â€“5.6Ã— higher request rates than\nDistServe at the same SLO guarantee. Compared with vLLM, our\nsystem also achieves substantial latency improvement. These re-\nsults indicate that FlowPrefill provides an effective and practical\nfoundation for efficient multi-SLO LLM serving.",
            "content": "References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming {Throughput-Latency} tradeoff in {LLM} inference with {Sarathi-Serve}. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). 117134. [3] Amey Agrawal, Haoran Qiu, Junda Chen, ÃÃ±igo Goiri, Chaojie Zhang, Rayyan Shahid, Ramachandran Ramjee, Alexey Tumanov, and Esha Choukse. 2024. Medha: Efficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations. arXiv preprint arXiv:2409.17264 (2024). [4] Amazon. 2023. Amazon CodeWhisperer. https://aws.amazon.com/cn/q/ developer/. [5] Anthropic. 2023. Claude. https://claude.com/. [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2024. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 1: Long papers). 31193137. [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 18771901. [8] Siyuan Chen, Zhipeng Jia, Samira Khan, Arvind Krishnamurthy, and Phillip Gibbons. 2025. SLOs-Serve: Optimized Serving of Multi-SLO LLMs. arXiv preprint arXiv:2504.08784 (2025). [9] Wenyan Chen, Chengzhi Lu, Huanle Xu, Kejiang Ye, and Chengzhong Xu. 2025. Multiplexing Dynamic Deep Learning Workloads with SLO-awareness in GPU Clusters. In Proceedings of the Twentieth European Conference on Computer Systems. 589604. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv e-prints (2024), arXiv2407. [11] Ruwen Fan, Tingxu Ren, Minhui Xie, Shiwei Gao, Jiwu Shu, and Youyou Lu. 2025. {GPREEMPT}:{GPU} Preemptive Scheduling Made General and Efficient. In 2025 USENIX Annual Technical Conference (USENIX ATC 25). 263272. [12] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. 2021. Turbotransformers: an efficient gpu serving system for transformer models. In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. 389402. [13] Jingqi Feng, Yukai Huang, Rui Zhang, Sicheng Liang, Ming Yan, and Jie Wu. 2025. WindServe: Efficient Phase-Disaggregated LLM Serving with Stream-based Dynamic Scheduling. In Proceedings of the 52nd Annual International Symposium on Computer Architecture. 12831295. [14] Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, and Hao Zhang. 2024. Efficient llm scheduling by learning to rank. Advances in Neural Information Processing Systems 37 (2024), 5900659029. [15] GitHub. 2021. GitHub Copilot. https://github.com/features/copilot. [16] Ruihao Gong, Shihao Bai, Siyu Wu, Yunqian Fan, Zaijun Wang, Xiuhong Li, Hailong Yang, and Xianglong Liu. 2025. Past-future scheduler for llm serving under sla guarantees. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2. 798813. [17] Google. 2023. Gemini. https://gemini.google.com/. [18] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, and Jonathan Mace. 2020. Serving {DNNs} like clockwork: Performance predictability from the bottom up. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20). 443462. [19] Ke Hong, Lufang Chen, Zhong Wang, Xiuhong Li, Qiuli Mao, Jianping Ma, Chao Xiong, Guanyu Wu, Buhe Han, Guohao Dai, et al. 2025. semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage. arXiv preprint arXiv:2504.19867 (2025). [20] Ke Hong, Xiuhong Li, Lufang Chen, Qiuli Mao, Guohao Dai, Xuefei Ning, Shengen Yan, Yun Liang, and Yu Wang. [n. d.]. SOLA: Optimizing SLO Attainment for Large Language Model Serving with State-Aware Scheduling. In Eighth Conference on Machine Learning and Systems. [21] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, et al. 2024. Inference without interference: Disaggregate llm inference for mixed downstream workloads. arXiv preprint arXiv:2401.11181 (2024). [22] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186 (2024). [23] Hyperbolic AI. 2024. Hyperbolic AI. https://www.hyperbolic.ai/. [24] Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. 2023. S3: Increasing GPU Utilization during Generative Inference for Higher Throughput. Advances in Neural Information Processing Systems 36 (2023), 1801518027. [25] Andreas Kosmas Kakolyris, Dimosthenis Masouros, Petros Vavaroutsos, Sotirios Xydis, and Dimitrios Soudris. 2024. SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving. arXiv preprint arXiv:2408.05235 (2024). [26] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles. 611626. [27] Gunjun Lee, Jiwon Kim, Jaiyoung Park, Younjoo Lee, and Jung Ho Ahn. 2025. From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with Layered Prefill. arXiv preprint arXiv:2510.08055 (2025). [28] Jianxiong Liao, Quanxing Dong, Yunkai Liang, Zhi Zhou, and Xu Chen. 2026. Laser: Unlocking Layer-Level Scheduling for Efficient Multi-SLO LLM Serving. In Proceedings of the 31st ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming. 509521. [29] Zejia Lin, Hongxin Xu, Guanyi Chen, Xianwei Zhang, and Yutong Lu. 2025. Bullet: Boosting GPU Utilization for LLM Serving via Dynamic Spatial-Temporal Orchestration. arXiv preprint arXiv:2504.19516 (2025). [30] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024). [31] Tongxuan Liu, Tao Peng, Peijun Yang, Xiaoyang Zhao, Xiusheng Lu, Weizhe Huang, Zirui Liu, Xiaoyu Chen, Zhiwei Liang, Jun Xiong, et al. 2025. xLLM Technical Report. arXiv preprint arXiv:2510.14686 (2025). [32] Hongtao Lyu, Boyue Liu, Mingyu Wu, and Haibo Chen. 2025. ing: Fairness-Aware Batch Formation for LLM Inference. arXiv:2510.14392 (2025). FairBatcharXiv preprint [33] Microsoft Azure. 2017. Azure Public Datasets. https://github.com/Azure/ AzurePublicDataset. [34] Novita AI. 2024. Novita AI. https://novita.ai/. [35] NVIDIA. 2019. Faster Transformer. FasterTransformer. https://github.com/NVIDIA/ [36] NVIDIA. 2025. NVIDIA Dynamo. https://github.com/ai-dynamo/dynamo. [37] OpenAI. 2022. ChatGPT. https://chatgpt.com/. [38] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, ÃÃ±igo Goiri, Saeed Maleki, and Ricardo Bianchini. 2024. Splitwise: Efficient generative llm inference using phase splitting. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA). IEEE, 118132. [39] Archit Patke, Dhemath Reddy, Saurabh Jha, Haoran Qiu, Christian Pinto, Chandra Narayanaswami, Zbigniew Kalbarczyk, and Ravishankar Iyer. 2024. Queue management for slo-oriented large language model serving. In Proceedings of the 2024 ACM Symposium on Cloud Computing. 1835. [40] Python. 2025. Python 3.14 Free-Threading. https://docs.python.org/3.14/howto/ free-threading-python.html. FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving Conference XX, June 0305, 2018, Woodstock, NY [41] Ruoyu Qin, Zheming Li, Weiran He, Jialei Cui, Heyi Tang, Feng Ren, Teng Ma, Shangming Cai, Yineng Zhang, Mingxing Zhang, et al. 2024. Mooncake: kvcache-centric disaggregated architecture for llm serving. ACM Transactions on Storage (2024). [42] Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus Franke, Zbigniew Kalbarczyk, Tamer Basar, and Ravishankar Iyer. 2024. Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction. In International Conference on Architectural Support for Programming Languages and Operating Systems. [43] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018). [44] Rana Shahout, Chunwei Liu, Weifan Jiang, Minlan Yu, Michael Mitzenmacher, et al. [n. d.]. DONT STOP ME NOW: EMBEDDING BASED SCHEDULING FOR LLMS. In The Thirteenth International Conference on Learning Representations. [45] Ying Sheng, Shiyi Cao, Dacheng Li, Banghua Zhu, Zhuohan Li, Danyang Zhuo, Joseph Gonzalez, and Ion Stoica. 2024. Fairness in serving large language models. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). 965988. [46] Xiaoxiang Shi, Colin Cai, Junjia Du, and Zhihao Jia. 2025. Nexus: Proactive Intra-GPU Disaggregation of Prefill and Decode in LLM Serving. arXiv preprint arXiv:2507.06608 (2025). [47] Gursimran Singh, Xinglu Wang, Yifan Hu, Timothy Tin Long Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Bai Xiaolong, Yi Li, Ying Xiong, et al. [n. d.]. Efficiently Serving Large Multimodal Models Using EPD Disaggregation. In Forty-second International Conference on Machine Learning. [48] Biao Sun, Ziming Huang, Hanyu Zhao, Wencong Xiao, Xinyi Zhang, Yong Li, and Wei Lin. 2024. Llumnix: Dynamic scheduling for large language model serving. In 18th USENIX symposium on operating systems design and implementation (OSDI 24). 173191. [49] Qwen Team et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671 2, 3 (2024). [50] Together AI. 2022. Together AI. https://www.together.ai/. [51] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [53] Jiahao Wang, Jinbo Han, Xingda Wei, Sijie Shen, Dingyan Zhang, Chenguang Fang, Rong Chen, Wenyuan Yu, and Haibo Chen. 2025. KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at Large Cloud Provider. arXiv preprint arXiv:2506.02634 (2025). [54] Bingyang Wu, Yinmin Zhong, Zili Zhang, Shengyu Liu, Fangyue Liu, Yuanhang Sun, Gang Huang, Xuanzhe Liu, and Xin Jin. 2023. Fast distributed inference serving for large language models. arXiv preprint arXiv:2305.05920 (2023). [55] xAI. 2023. Grok. https://grok.com/. [56] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [57] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and ByungGon Chun. 2022. Orca: distributed serving system for {Transformer-Based} generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 521538. [58] Jiahuan Yu, Mingtao Hu, Zichao Lin, and Minjia Zhang. 2026. SuperInfer: SLOAware Rotary Scheduling and Memory Management for LLM Inference on Superchips. arXiv preprint arXiv:2601.20309 (2026). [59] Shan Yu, Jiarong Xing, Yifan Qiao, Mingyuan Ma, Yangmin Li, Yang Wang, Shuo Yang, Zhiqiang Xie, Shiyi Cao, Ke Bao, et al. 2025. Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving. arXiv preprint arXiv:2505.04021 (2025). [60] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. 2023. Lmsyschat-1m: large-scale real-world llm conversation dataset. arXiv preprint arXiv:2309.11998 (2023). [61] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MTBench and Chatbot Arena. arXiv:2306.05685 [cs.CL] [62] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. {DistServe}: Disaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). 193210."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "University of Science and Technology Beijing"
    ]
}