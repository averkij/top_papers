{
    "paper_title": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT",
    "authors": [
        "Jintao Zhang",
        "Haoxu Wang",
        "Kai Jiang",
        "Kaiwen Zheng",
        "Youhe Jiang",
        "Ion Stoica",
        "Jianfei Chen",
        "Jun Zhu",
        "Joseph E. Gonzalez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality."
        },
        {
            "title": "Start",
            "content": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT Jintao Zhang 1 Haoxu Wang 1 Kai Jiang 1 Kaiwen Zheng 1 Youhe Jiang 1 Ion Stoica 2 Jianfei Chen 1 Jun Zhu 1 Joseph E. Gonzalez 2 6 2 0 2 3 1 ] . [ 1 5 7 6 2 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify mismatch between SLA and direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) more faithful and direct sparselinear attention formulation that uses learnable ratio to combine the sparse and linear attention branches, and (III) sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6 attention speedup while preserving generation quality. 1. Introduction Trainable sparse attention methods (Zhang et al., 2025c;i; Wu et al., 2025; Zhan et al., 2025) have shown strong performance in diffusion models. They often achieve higher attention sparsity than training-free sparse attention methods (Zhang et al., 2025f; Xi et al., 2025; Chen et al., 2025a). Among them, Sparse-Linear Attention (SLA) (Zhang et al., 2025c) is promising approach that introduces linearattention branch to compensate for the sparse-attention branch, improving overall sparsity. SLA has been validated on both image and video diffusion models, such as TurboDiffusion (Zhang et al., 2025h). Motivation of SLA. SLA finds that, in diffusion models, 1Tsinghua University 2UC Berkeley. Preprint. 1 the attention map could be decomposed into high-sparse part P1 and low-rank part P2, and = P1 + P2. SLA can be formulated to = Ps + proj(Pl), where Ps and Pl are the attention maps of sparse and linear attention, and proj is trainable projection. Limitation of SLA and motivation of SLA2. (L1) Mismatch between SLA output and the original sparse-linear decomposition. After an analysis of the difference of the SLA formulation with the original SLA motivation, we find that the sparse attention map Ps of SLA differs from the decomposed sparse attention map P1 by constant scaling factor. Specifically, we find P1 = αPs, where α is ratio vector. To compensate for the mismatch, SLA introduces and trains an additional linear attention projection, which may fail to fully address it. We therefore aim to propose sparse-linear attention formulation that more directly matches the original motivation. (L2) Heuristic routing for sparse and linear attention branches. SLA does not optimally address the key design choice of how to split computation between the sparse and linear branches. In practice, SLA assigns attention associated with larger attention weights to the sparse branch and routes the remaining computation to the linear branch. This heuristic split is not optimal. For example, moving some weights from P1 to P2 via brute-force selection may not increase the rank of P2, while still improving the sparsity of P1. We therefore aim to design more principled split, guided by clear optimization objective. Finally, low-bit attention can be introduced to SLA to obtain an additional speedup. We thus aim to incorporate low-bit attention into SLA in way that introduces as little quantization error as possible, enabling further attention speedup. Our method. We propose SLA2, sparse-linear attention method that reformulates sparse linear attention to (1) better match the original motivation, and (2) optimally route between the sparse and linear attention branches. To address (L1), we directly learn the ratio α to combine the sparse and linear attention branches. This formulation aligns exactly with the sparse and linear components decomposition of attention. To address (L2), we formulate the approximation error of combining sparse attention and linear attention relative to full attention, and build learnable sparse-attention that supports gradient backpropagation. mask predictor SLA2: Sparse-Linear Attention with Learnable Routing and QAT We train this predictor by minimizing the formulated error. Furthermore, we build low-bit attention on top of sparse attention to achieve additional attention speedups. To reduce the error introduced by low-bit quantization, we integrate the quantization process into training in quantization-aware manner, enabling the model to better adapt to low-bit quantization and thus improve the accuracy of low-bit attention at inference time. Result. SLA2 achieves 97% attention sparsity and an attention runtime speedup on both Wan2.1-1.3B and 18.6 Wan2.1-14B. Please note that 97% sparsity corresponds to about 96.7% computation savings after accounting for the linear-attention branch in SLA2. In terms of video generation quality, even at 97% sparsity, SLA2 outperforms the baselines at 90% sparsity in end-to-end video quality, and it even exceeds full attention, which is 0% sparsity. Contribution. Our contributions are as follows: (1) We carefully analyze the limitations of SLA and propose SLA2, more reasonable sparse-linear attention method. SLA2 includes learnable router that splits computation between the sparse and linear attention branches, along with simple yet effective learnable combination for sparse and linear attention branches. For some insight on the design of SLA2, please see Sections 2.2 and 8. (2) We integrate quantization-aware training (QAT) into SLA2 to further accelerate attention without degrading endto-end video generation quality, demonstrating the effectiveness of QAT for low-bit attention. (3) Experiments show that SLA2 achieves 97% attention sparsity and an 18.6 attention runtime speedup on video diffusion models while maintaining video quality, surpassing baseline methods. 2. Preliminaries 2.1. Sparse-Linear Attention SLA (Sparse-Linear Attention) (Zhang et al., 2025c) combines sparse softmax attention and linear attention using heuristic sparse attention mask. Below, we describe the computation of SLA. RN be the query, key, and Notation. Let Q, K, value matrices, where is the sequence length and is the head dimension. Let = QK /d RN be the attention score matrix. We use softmax( row-wise softmax. We use ϕ( for linear attention. ) to denote ) as the activation function tion weights using pooled queries and keys: Pc = softmax (cid:16) pool(Q) pool(K)/d (cid:17) , (1) where pool( ) applies mean pooling over the token dimension within each token block. For each row of Pc, SLA assigns the top kh% entries to sparse attention and the bottom kl% entries to skipping, with the remaining entries handled by linear attention. In practice, kl is typically small and can be omitted. This procedure yields binary mask N/bk , where the top kh% entries in each Mc { row are set to 1 and the others to 0. Then, we obtain by expanding Mc. N/bq } 0, 1 0, { } Sparse attention output. Given , SLA computes sparse softmax attention only on entries selected by the mask: = softmax(S ) RN , Os = is element-wise multiplication. where RN (2) Linear attention output. For the remaining entries (1 SLA applies linear attention: ϕ(Q) (cid:0)ϕ(K)((1 ϕ(Q) (ϕ(K)(1 )V )(cid:1) )1) Ol = RN d, ), (3) where 1 element-wise to perform row-wise normalization. 1 is an all-ones vector, and the division is RN Final output. The final SLA output is = Os + Proj(Ol), (4) where proj( Rd ) is learnable linear projection. 2.2. Rethinking Sparse-Linear Attention Original motivation of Sparse-Linear Attention. Let = softmax(S) RN be the full-attention probability matrix. Given binary mask 0, } { , we decompose see full attention into two parts: = P1+P2, P1 = M, P2 = (1 ), (5) where P1 corresponds to the mask-selected attention positions (computed by sparse softmax attention), and P2 corresponds to the remaining positions (approximated by linear attention). The motivation of SLA is to approximate P1 with sparse-attention distribution and approximate P2 d, the with linear-attention distribution. With full-attention output is RN Mask construction. SLA first computes compressed attenOf = = P1V + P2V RN d. (6) 2 SLA2: Sparse-Linear Attention with Learnable Routing and QAT Error of the sparse attention branch. Sparse attention does not directly produce P1, because it renormalizes probabilities over the masked positions in each row. Let α denote the probability sum on the masked positions for each query: α = P11 RN 1, (7) 1 is an all-one vector. The sparse-attention RN where 1 distribution is , RN Ps = P1 α Therefore, Ps is not aligned with P1; it is obtained by rowwise normalizing P1 so that each row sums to 1. In terms d, the desired of attention output, with Os = PsV sparse attention output is RN (8) P1V = (α Ps)V = α Os. (9) As result, each row has scale mismatch controlled by α. How SLA compensates for the mismatch. SLA output is shown in Equation 4. Comparing Equation 6 and using Equation 9, we can interpret proj(Ol) as jointly accounting for the linear component P2V and the residual induced by the sparse attention branch mismatch: proj(Ol) P2V + (α 1) Os. (10) However, this correction is not directly aligned with the original decomposition motivation: the linear attention branch is also forced to offset the sparse attention branchs scaling error, making the compensation harder to learn. more reasonable formulation. more faithful way to match the decomposition in Equation 5 is α α) RN Ps + (1 1. Here, Ps, Pl are the attentionwhere α weight matrices corresponding to the sparse attention and the linear attention branchs, and each is row-normalized so that every row sums to 1. The attention output is RN Pl, (11) = α (PsV ) + ( α) (PlV ). Ps better matches P1, which removes the row-wise Here, α scaling mismatch in the sparse attention branch; therefore, ) on Ol for compensation is no longer needed. an extra proj( Moreover, (1 Pl is row-normalized, avoiding magnitude drift of the output. α) ensures that α Ps + (1 α) (12) 3. SLA2 Design According to the analysis in Section 2.2 and Equation 12, we present the overall formulation of SLA2 as follows: = α Os + ( α) Ol, (13) 3 Figure 1. Attention computation pipeline of SLA2. RN where α 0 and 1, and 1 is learnable vector with values between Os = softmax(QK /d Ol = norm(ϕ(Q)ϕ(K) (Q, K), = (1 )V, ))V, (14) is learnable module, which will be explained in where Section 4. ϕ( ) is an activation function for linear attention, and we use the softmax function. norm normalizes the sum of rows in matrix to 1. Implementation of getting Os and Ol. From Equation 14, it may appear that computing Os and Ol requires full matmuls QK and . In contrast, our implementation is highly efficient. For Os, built on top of the FlashAttention Algorithm, we only perform the matmuls QK and for the positions where = 1, and skip the other computations. For Ol, we also do not compute the matmul QK directly, but first compute according to the positions where = 0. Then we multiply with the result. See Algorithm 2 for more details. 4. Learnable Router The learnable router aims to dynamically output mask to decide which probabilities in should be computed by the sparse attention branch. Its decisions mainly depend on and K, and are independent of . We therefore take and as inputs to . However, the sequence length can be large, making expensive. To reduce its computational cost, we leverage the fact that adjacent tokens in and often exhibit similar distributions (Zhang et al., 2025f). Following (Jiang et al., 2024; Zhang et al., 2025f; Gao et al., 2024), we apply mean pooling over consecutive bq and bk tokens to compress and K: = pool(Q) = pool(K) RN/bk RN/bq d, d. (15) SLA2: Sparse-Linear Attention with Learnable Routing and QAT To make jections projq, projk To get , we perform learnable, we further introduce two linear prod for and K, respectively. Rd Pc = projq( Q) projk( K), Mc = Top-k(cid:0)k%, Pc (cid:1) RN/bq N/bk , (16) where Top-k is applied row-wise, setting the top k% positions to 1 and the others to 0. The compressed mask Mc can mask to support the computation be expanded to an in Equation 14. In practice, our forward and backward GPU kernels for SLA2 only require Mc, since we implement the method efficiently on top of block-wise FlashAttentionstyle algorithm. We will elaborate on this in Section 7. Finally, we note that Top-k avoids gradient propagation during training. We therefore replace Top-k with learnable version during training. The details and the full training procedure are provided in Section 6. 5. Quantization-aware Training Post-training quantization (PTQ) (Jacob et al., 2018) applies quantization after model is fully trained. In contrast, quantization-aware training (QAT) (Nagel et al., 2022) incorporates quantization effects during training, allowing the model to adapt its parameters to the quantization error and thereby improving low-bit accuracy at inference time. In SLA2, we further accelerate the sparse attention branch Os computation using low-bit attention in QAT manner. Concretely, during training, we use low-bit attention only in the forward pass, while the backward pass remains fully in FP16. This design enables the attention speedup brought by low-bit attention while minimizing the end-to-end accuracy drop caused by low-bit quantization. d, Forward (low-bit attention). Given Q, K, we apply low-bit quantized attention in the forward pass. We first quantize ( ˆQ, sQ = quant(Q)) and ( ˆK, sK = quant(K)), then compute RN Note that the equations above describe the mathematical computation rather than the GPU kernel implementation. We build the actual efficient kernel on the FlashAttention algorithm to avoid computing the full score matrix before applying mask . Instead, we skip unnecessary computations. The detailed algorithm is provided in Sections 6 and 7. Backward (FP16-only). Let dOs denote the gradient of Os. In our QAT setting, the backward pass is computed entirely in FP16, using the original FP16 inputs (Q, K, ) and the forward output Os. The gradient of Q, K, from the sparse attention branch can be formulated as: dQ, dK, dV = backward(dOs, Os, Q, K, ). The detailed backward GPU kernel, along with the complete training pipeline, is provided in Section 6. Algorithm 1 Fine-tuning diffusion model using SLA2. and α: 1: Stage 1: Initialize 2: Sample Q, K, tensors as dataset D. 3: = MSE(FullAttn(Q, K, ), SLA2(Q, K, V, k%, R, α)); 4: Train , α under different k% according to the loss ; 5: Stage2: Fine-tune the diffusion model Θ and α: 6: Replace the attention in Θ by SLA2; 7: Fine-tune Θ, α using an end-to-end diffusion loss. 6. Training with SLA2 To fine-tune diffusion model with SLA2, we adopt twostage training strategy. 1 In the first stage, we seek better initialization for and α to ensure stable and effective subsequent fine-tuning of the diffusion model. 2 In the second stage, we fine-tune the entire diffusion model endto-end. In this stage, we directly optimize the diffusion loss over all model parameters Θ, including α, without , so that the model adapts to high-sparsity attention and can even achieve better performance under high sparsity. S = dequant( ˆQ ˆK /d, sQ, sK), = softmax(S ), followed by quantizing ( ˆP , sP = quant(P )) and ( ˆV , sV = quant(V )) and computing Os = dequant( ˆP ˆV , sP , sV ). ) maps an FP16 tensor to low-bit tensor (e.g., Here, quant( INT8 or FP8) along with its scale, and dequant( ) rescales the result back to FP16. We use ˆQ, ˆK, ˆP , ˆV to denote the quantized tensors and sQ, sK, sP , sV to denote their scales. Our quantization/dequantization scheme follows SageAttention2++ (Zhang et al., 2025g). Specifically, in the first stage, we use the Q, K, and matrices from every attention layer at each diffusion timestep as training data. For each sparsity setting (i.e., different k%, we use 5%, 4%, and 3%), we train and α. Note that Top-k is non-differentiable. Therefore, throughout the entire training process, we replace the Top-k operator in Equation 16 with SoftTop-k operator (Ding et al., 2024): SoftTop-k(k%, Pc)ij = σ (cid:18) (Pc)ij τ (cid:19) + λi , (17) where σ denotes the sigmoid function, τ is temperature parameter, and λi is solved via binary search to enN/bk. The gradient of sure that each row sums to k% 4 SLA2: Sparse-Linear Attention with Learnable Routing and QAT , { } Rd RN/bq ; Kϕ } hj} { zj} { ; Mc[:, :] = 0 ; for = 1 to Tn do Qϕ } { Vi} , { d, bq, bk, k%, learn1. Algorithm 2 Forward pass of SLA2. RN d, and α Qi} { Ki} blocks { colmean(K) ; // smooth of SageAttention Q, = pool(Q), pool(K); and blocks 1: Input: Matrices Q, K, able projq, projk 2: = 3: Qϕ, ϕ = ϕ(Q), ϕ(K), 4: Divide Q, Qϕ to Tm = bq 5: Divide K, V, ϕ to Tn= bk (Kϕ )Vj} ; = 6: = { rowsum((Kϕ )) = 7: = { 8: Pc = softmax(projq( Q)projk(K)/d) ; 9: Mc = Top-k(Pc, k%) ; // SoftTop-k in stage1 training 10: for = 1 to Tm do 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: Os 24: Ol 25: end for 26: Os = Os } { 27: return = α Sij = dequant(quant(Qi)quant(Kj))/d; mij = max(mi,j Pij = exp(Sij lij = emi,j1 1 + rowsum(Pij) ; Otmp = dequant(quant(Pij)quant(Vj) ; ij = diag(emi,j1 Os 1 + Otmp ; else if Mc[i, j] = 0 then 1Os Zi); Li = mi,Tn + log(li,Tn ) ; 1, rowmax(Sij)) ; mij) ; = diag(lTn = Qϕ ) Hi/(Qϕ if Mc[i, j] = 1 then Ol ; i} Os + (1 end if end for mij )Os i,j Hi Hi + hj; Zi + zj ; Zi , Ol = mij li,j Ol ; i,Tn α) { ; SoftTop-k is computed using the reparameterization trick (see Ding et al. (2024)), which enables gradient backpropagation. This operator retains key properties of Top-k, such as enforcing row-wise sum of k% N/bk. The overall training algorithm is in Algorithms 1, where we use , α) as SLA2 operator. The forO = SLA2(Q, K, V, k%, ward and backward procedures of SLA2, are provided in Algorithms 2, and 3, respectively. 7. Inference with SLA2 During inference, we simply replace the attention modules in the diffusion model with SLA2 and run the SLA2 forward pass described in Algorithm 2. Note that the Top-k operation uses the hard Top-k in Equation 16, rather than SoftTop-k. 8. Insights We summarize key insights on SLA design and training in question-driven format. (1) Why is the design of (Equation 4) reasonable? The core motivation of sparse-linear attention is to decompose the attention weights as = P1 + P2, where P1 is handled by the sparse branch, and P2 is handled by the linear branch. It aims to route low-rank portion of to P2 and make P1 as sparse as possible without harming end-to-end quality. by answering three We explain the design choices of sub-questions: (1.a) Why the input of are and K? For each attention layer, the attention weights are determined by the score matrix = QK /d followed by row-wise softmax, i.e., = Softmax(S). Therefore, deciding which positions of should be assigned to the sparse branch is fundamentally decision about which positions of S, i.e., the matrix multiplication between and K, are likely to contribute most after softmax. This makes (Q, K) the natural and sufficient signals for routing, while does not affect the formation of and is thus not needed for the routing decision. (1.b) Why apply pooling to and in ? naive router (N 2) comthat operates on the full QK would incur plexity, which is too expensive. To reduce this cost, we pool adjacent tokens in and using mean pooling to obtain and K. This is still effective because nearby tokens in diffusion transformers often have similar distribution (Jiang et al., 2024; Zhang et al., 2025f; Gao et al., 2024), so the values in QK vary smoothly across adjacent positions. (1.c) Why using projections (projq and projk) in ? Using K followed by softmax and Top-k is simple heuristic and may not yield an optimal split of into sparse part and low-rank part. By introducing learnable projections projq and projk, the router can learn task-adaptive representation in which Top-k selection better matches the desired decomposition (making P1 much sparser while leaving portion that is easier for the linear branch to approximate). In particular, this design generalizes the heuristic routing: setting projq = projk = recovers the original form, while learning these projections under our training objective can produce more suitable partition. (2) Why does SLA2 needs two-stage training? We adopt two-stage training strategy for both training stability and traininference consistency. First, before end-to-end finetuning of the entire diffusion model, should be reasonably initialized. Otherwise, unstable and poor routing can make subsequent fine-tuning difficult. Second, the router used at inference relies on hard Top-k, which is non-differentiable and blocks gradient propagation. To train the projection parameters inside , we therefore use differentiable SoftTopk operator during Stage 1. After obtaining good initialization, Stage 2 fine-tunes the full diffusion model while keeping the routing computation aligned with inference (i.e., using hard Top-k for routing), ensuring that the trained SLA2: Sparse-Linear Attention with Learnable Routing and QAT Table 1. Quality and efficiency metrics of SLA2 and the baseline methods. Model Method Full Attention Wan2.1 -T2V -1.3B -480P Wan2.1 -T2V -14B -720P VMoBA VSA SLA SLA2 VMoBA VSA SLA SLA2 SLA2 Full Attention VMoBA VSA SLA SLA VMoBA VSA SLA SLA2 SLA2 Quality Efficiency IQ 63. 65.31 59.57 63.10 67.70 63.08 55.50 63.14 67.04 66.64 68.01 67.18 64.03 67.58 69.63 21.27 47.69 64.43 69. 66.93 OC 20.27 20.82 19.27 20.88 21.62 21.07 14.95 21.09 21.55 21. 22.44 20.85 21.27 21.62 20.68 7.96 13.90 20.89 21.11 21.12 AQ 64. 64.14 50.60 64.34 64.86 61.96 42.13 62.91 64.90 64.62 64.66 63.64 63.37 63.80 66.41 33.59 34.95 61.89 65. 65.14 MS 98.95 97.80 97.44 97.90 98.69 97.68 96.19 97.83 98.46 98. 99.14 98.55 98.90 98.78 98.84 99.99 97.09 98.86 98.89 98.71 SC 95. 86.69 87.98 92.54 95.54 79.83 88.34 94.36 95.27 94.83 95.93 94.50 93.65 95.74 95.74 100 91.12 94.41 95. 94.42 VR FLOPs Sparsity 0.1084 0.0936 -0.0881 0.0872 0. 0.0746 -0.1309 0.0881 0.1023 0.1039 0.1238 0.1117 0.1074 0.1166 0.1238 -0.0965 -0.1822 0.1078 0.1125 0. 52.75T 0% 5.28T 5.40T 5.40T 5.51T 2.64T 2.75T 2.75T 2.87T 1.82T 292.6T 29.26T 20.92T 20.92T 21.16T 14.63T 14.87T 14.87T 15.11T 9.26T 90% 95% 97% 0% 90% 95% 97% model matches the inference-time computation logic. 9. Experiments 9.1. Setup Model and Baselines. We fine-tune SLA2 and baseline methods on the Wan2.1-1.3B-480P and Wan-2.1-14B-720P models (Wan et al., 2025). For the dataset, we use private video dataset of 3,000 videos (about 5 seconds each) collected from public sources. To construct textvideo pairs, we generate caption for each video using Qwen3-VLFlash and use these captions as text conditioning for both fine-tuning and evaluation. For baselines, we use Full Attention (without training) implemented with FlashAttn2. We also select several state-of-the-art video generation methods with sparse attention mechanism, including SLA (Zhang et al., 2025c), VSA (Zhang et al., 2025i) and VMoBa (Wu et al., 2025). All results are obtained using the official open-source implementations. Metrics. Following Zhang et al. (2024); Yang et al. (2025b), we evaluate video quality using multiple dimensions from VBench (Zhang et al., 2024), including Imaging Quality (IQ), Overall Consistency (OC), Aesthetic Quality (AQ), Motion Smoothness (MS) and Subject Consistency (SC). In addition, we assess human preference using the Vision Reward metric (VR) (Xu et al., 2024). To quantify computational cost, we use FLOPs (floating-point operations). For kernel-level efficiency, we report C/t, where = 4N 2d denotes the theoretical amount of computation and is the execution latency. We also measure the end-to-end inference latency in seconds. Hyper-parameters. We fine-tune each method for 500 steps. The batch size is set to 64 for the 1.3B model and 15 for the 14B model. We set the block sizes to bq = 128 and bkv = 64. We use k% of 5%, 4%, and 3% for SLA2. For the temperature parameter τ in SoftTop-k, we use τ = 0.1. 9.2. Effectiveness Table 1 compares the video generation quality and efficiency of SLA2 against baseline methods on the Wan2.1-T2V-1.3B480P and Wan2.1-T2V-14B-720P models. At sparsity levels of 90% and 95%, SLA2 consistently outperforms all baselines across every video quality metric on both models. Even at higher sparsity of 97%, SLA2 still surpasses all baseline methods at 90% sparsity, while achieving 29 speedup over Full Attention. Interestingly, we observe that sparse attention methods can even outperform Full Attention on many metrics after fine-tuning. We attribute this to the higher quality of the fine-tuning dataset compared to the that used during pretraining. Visible examples. Figure 2 shows an example generated by different methods fine-tuned on Wan2.1-T2V-1.3B-480P. The videos produced by SLA2 exhibit the highest quality and maintain content similar to that generated by Full At6 SLA2: Sparse-Linear Attention with Learnable Routing and QAT Figure 2. Visible examples of SLA2 and baselines on Wan2.1-T2V-1.3B-480P model. The prompt used for generation is in Appendix B. Figure 3. Visible examples of SLA2 and baselines on Wan2.1-T2V-14B-720P model. The prompt used for generation is in Appendix B. tention. In contrast, videos from other methods either differ noticeably from Full Attention or show clear distortions. Figure 3 presents an example generated by Full Attention and SLA2 on Wan2.1-T2V-14B-720P model. SLA2 brings almost no degradation in video quality. 9.3. Efficiency speedup over FlashAttn2, and is 11.7 Figure 4 illustrates the forward kernel speed of SLA2 and the baseline methods on an RTX5090, measured in TOPS (trillion operations per second). At 97% sparsity, SLA2 achieves 18.7 and 2.6 faster than VMoBA and VSA at 95% sparsity, respectively. Note that SLA2 outperforms all baselines, even when SLA2 uses 97% sparsity and the baselines use 90% or 95% sparsity. Figure 5 presents the end-to-end video generation latencies for SLA2 and the baselines. On the Wan-1.3B-480P model, reducing attention latency from 97s to 7s (13.9 reduction in overall end-to-end latency. On the Wan-14B720P model, SLA2 further reduces end-to-end latency by speedup) enables SLA2 to achieve 2.30 Figure 4. Kernel speed of SLA2 and baselines with different sparsities. 4.35 . Since the Wan2.1-14B-720P model exceeds the VRAM capacity of single RTX5090, we enable sequential CPU offloading during evaluation. The reported latency already excludes the offloading overhead. SLA2: Sparse-Linear Attention with Learnable Routing and QAT learnable router significantly outperforms the Top-k router. Varying sparsity. We vary the sparsity from 85% to 97% and evaluate SLA2 under different sparsity levels. As summarized in Table 2, lower sparsity consistently leads to better performance. Notably, even with 97% sparsity, SLA2 already outperforms all baselines, as shown in Table 1. 10. Related Work Sparse attention and linear attention are two main ways to speed up attention in Transformer-based models. Sparse attention methods can be grouped by whether they require training. Training-free approaches (Xiao et al., 2024; Jiang et al., 2024; Gao et al., 2024; Xi et al., 2025; Zhang et al., 2025f; Ribar et al., 2023; Yang et al., 2025a; Li et al., 2025; Chen et al., 2025a; Lai et al., 2025; Zhang et al., 2023; Tang et al., 2024; Zhu et al., 2025a; Lin et al., 2025; Xu et al., 2025; Xia et al., 2025; Chen et al., 2025b; Zhang et al., 2025j; Yang et al., 2024b) reduce inference cost by masking attention patterns at test time, while trainable methods (Zhang et al., 2025i; Wu et al., 2025; Zhang et al., 2025c; Zhan et al., 2025; Zhou et al., 2025; Lu et al., 2025; Yuan et al., 2025; Liu et al., 2025a; Zhang et al., 2026; Cai et al., 2025; Liu et al., 2025b; Sun et al., 2025; Tan et al., 2025; Ding et al., 2023) encourage sparsity during training and can support higher sparsity. Linear attention methods (Wang et al., 2020; Choromanski et al., 2020; Katharopoulos et al., 2020; Qin et al., 2024; Yang et al., 2024a; Sun et al., 2023) are mainly studied in language models. In diffusion transformers, SANA (Xie et al., 2024) and Dig (Zhu et al., 2025b) show that linear attention can work for image-generation pre-training; however, for video generation, linear attention alone often cannot keep quality. In addition, hardwarefocused work (Dao et al., 2022; Dao, 2023; Shah et al., 2024; Zhang et al., 2025d;a;e) speeds up attention by improving GPU execution through tiling, kernel fusion, and quantization. 11. Conclusion We presented SLA2, an trainable sparse-linear attention method for diffusion models. It is motivated by two limitations of SLA: its heuristic routing based on the magnitude of attention weights and mismatch with the decomposition of sparse and linear attention, revealed by our error analysis. SLA2 addresses these issues by introducing learnable router and decomposition-consistent mixing formulation. Moreover, SLA2 adopt sparse + low-bit attention in quantization-aware fine-tuning way for further acceleration. Experiments show that SLA2 achieves up to 97% attention attention speedup, while preserving sparsity and an 18.6 video generation quality. We hope SLA2 offers an effective and practical way for efficient attention in diffusion models. Figure 5. End-to-end generation latency of SLA2 and baselines with different sparsities. Table 2. Ablation experiments results. Method Quality IQ OC AQ MS SC VR Full Attention 63.67 20.27 64.41 98.95 95.40 0.1084 w/o QAT Topk-router SLA2 SLA2 (85%) SLA2 (90%) SLA2 (95%) SLA2 (97%) 65.28 20.66 61.85 97.44 94.65 0.0850 63.66 62.65 97.86 94.26 0.0876 66.64 21.42 64.62 98.04 94.83 0. 20.9 67.97 21.98 64.79 98.75 95.79 0.1135 67.70 21.62 64.86 98.69 95.54 0.1093 67.04 21.55 98.46 95.27 0.1023 66.64 21.42 64.62 98.04 94.83 0.1039 64.9 9.4. Ablation Study Quantization-aware training. To evaluate the impact of quantization-aware training (QAT), we fine-tune the same model without QAT and perform quantized inference. As shown in Table 2, the quality of generated videos drops when inference is performed without QAT, which confirms its effectiveness. For efficiency, we evaluate SLA2 both with and without quantization. Low-bit quantization provides an approximately 1.3x kernel speedup. Learnable router. To evaluate the benefit of the learnable router, we compare it with the Top-k router used in SLA (Zhang et al., 2025c), which directly selects the largest scores in pool(Q)pool(K). As shown in Table 2, the 8 SLA2: Sparse-Linear Attention with Learnable Routing and QAT"
        },
        {
            "title": "References",
            "content": "Cai, S., Yang, C., Zhang, L., Guo, Y., Xiao, J., Yang, Z., Xu, Y., Yang, Z., Yuille, A., Guibas, L., et al. Mixture of contexts for long video generation. arXiv preprint arXiv:2508.21058, 2025. Chen, P., Zeng, X., Zhao, M., Ye, P., Shen, M., Cheng, W., Yu, G., and Chen, T. Sparse-vdit: Unleashing the power of sparse attention to accelerate video diffusion transformers. arXiv preprint arXiv:2506.03065, 2025a. Chen, R., Mills, K. G., Jiang, L., Gao, C., and Niu, D. Re-ttention: Ultra sparse visual generation via attention statistical reshape. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025b. Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In International Conference on Learning Representations, 2020. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with IO-awareness. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. Ding, G., Ye, Z., Zhong, Z., Li, G., and Shao, D. Separate, dynamic and differentiable (smart) pruner for block/output channel pruning on computer vision URL https://arxiv.org/abs/ tasks, 2024. 2403.19969. Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., Zheng, N., and Wei, F. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. Gao, Y., Zeng, Z., Du, D., Cao, S., Zhou, P., Qi, J., Lai, J., So, H. K.-H., Cao, T., Yang, F., et al. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko, D. Quantization and training of neural networks for efficient integerarithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 27042713, 2018. Jiang, H., Li, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A. H., Li, D., Lin, C.-Y., et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. Jiang, Y., Fu, F., Zhao, W., Rabanser, S., Lane, N. D., and Yuan, B. Cascadia: cascade serving system for large language models. arXiv preprint arXiv:2506.04203, 2025. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. Lai, X., Lu, J., Luo, Y., Ma, Y., and Zhou, X. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference. arXiv preprint arXiv:2502.20766, 2025. Li, X., Li, M., Cai, T., Xi, H., Yang, S., Lin, Y., Zhang, L., Yang, S., Hu, J., Peng, K., et al. Radial attention: (nlog n) sparse attention with energy decay for long video generation. arXiv preprint arXiv:2506.19852, 2025. Lin, C., Tang, J., Yang, S., Wang, H., Tang, T., Tian, B., Stoica, I., Han, S., and Gao, M. Twilight: Adaptive attention sparsity with hierarchical top-p pruning. arXiv preprint arXiv:2502.02770, 2025. Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025a. Liu, A., Zhang, Z., Li, Z., Bai, X., Han, Y., Tang, J., Xing, Y., Wu, J., Yang, M., Chen, W., et al. Fpsattention: Trainingaware fp8 and sparsity co-design for fast video diffusion. arXiv preprint arXiv:2506.04648, 2025b. Hu, Y., Huang, W., Liang, Z., Chen, C., Zhang, J., Zhu, J., and Chen, J. Identifying sensitive weights via postquantization integral. arXiv preprint arXiv:2503.01901, 2025. Lu, E., Jiang, Z., Liu, J., Du, Y., Jiang, T., Hong, C., Liu, S., He, W., Yuan, E., Wang, Y., et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. Hu, Y., Singh, H., Maheswaran, M., Xi, H., Hooper, C., Zhang, J., Tomar, A., Mahoney, M. W., Min, S., Farajtabar, M., et al. Residual context diffusion language models. arXiv preprint arXiv:2601.22954, 2026. Nagel, M., Fournarakis, M., Bondarenko, Y., and Blankevoort, T. Overcoming oscillations in quantizationaware training. In International Conference on Machine Learning, pp. 1631816330. PMLR, 2022. SLA2: Sparse-Linear Attention with Learnable Routing and QAT Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., and Zhong, Y. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024. Xi, H., Yang, S., Zhao, Y., Xu, C., Li, M., Li, X., Lin, Y., Cai, H., Zhang, J., Li, D., et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C., Luschi, C., and Orr, D. Sparq attention: Bandwidthefficient llm inference. arXiv preprint arXiv:2312.04985, 2023. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Sun, W., Tu, R.-C., Ding, Y., Jin, Z., Liao, J., Liu, S., and Tao, D. Vorta: Efficient video diffusion via routing sparse attention. arXiv preprint arXiv:2505.18809, 2025. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Tan, X., Chen, Y., Jiang, Y., Chen, X., Yan, K., Duan, N., Zhu, Y., Jiang, D., and Xu, H. Dsv: Exploiting dynamic sparsity to accelerate large-scale video dit training. arXiv preprint arXiv:2502.07590, 2025. Tang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han, S. Quest: Query-aware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Wu, J., Hou, L., Yang, H., Tao, X., Tian, Y., Wan, P., Zhang, D., and Tong, Y. Vmoba: Mixture-of-block attention for video diffusion models. arXiv preprint arXiv:2506.23858, 2025. Xi, H., Yang, S., Zhao, Y., Li, M., Cai, H., Li, X., Lin, Y., Zhang, Z., Zhang, J., Li, X., et al. Quant videogen: Auto-regressive long video generation via 2-bit kv-cache quantization. arXiv preprint arXiv:2602.02958, 2026. Xia, Y., Ling, S., Fu, F., Wang, Y., Li, H., Xiao, X., and Cui, B. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. Xiang, C., Liu, J., Zhang, J., Yang, X., Fang, Z., Wang, S., Wang, Z., Zou, Y., Su, H., and Zhu, J. Geometry-aware rotary position embedding for consistent video world model. 2026. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. Xie, E., Chen, J., Chen, J., Cai, H., Tang, H., Lin, Y., Zhang, Z., Li, M., Zhu, L., Lu, Y., et al. Sana: Efficient highresolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. Xu, J., Huang, Y., Cheng, J., Yang, Y., Xu, J., Wang, Y., Duan, W., Yang, S., Jin, Q., Li, S., et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. Xu, R., Xiao, G., Huang, H., Guo, J., and Han, S. Xattention: Block sparse attention with antidiagonal scoring. arXiv preprint arXiv:2503.16428, 2025. Yang, S., Kautz, J., and Hatamizadeh, A. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464, 2024a. Yang, S., Sheng, Y., Gonzalez, J. E., Stoica, I., and Zheng, L. Post-training sparse attention with double sparsity. arXiv preprint arXiv:2408.07092, 2024b. Yang, S., Xi, H., Zhao, Y., Li, M., Zhang, J., Cai, H., Lin, Y., Li, X., Xu, C., Peng, K., et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. Advances in Neural Information Processing Systems (NeurIPS 2025), 2025a. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025b. SLA2: Sparse-Linear Attention with Learnable Routing and QAT Yuan, J., Gao, H., Dai, D., Luo, J., Zhao, L., Zhang, Z., Xie, Z., Wei, Y., Wang, L., Xiao, Z., et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2307823097, 2025. Zhan, C., Li, W., Shen, C., Zhang, J., Wu, S., and Zhang, H. Bidirectional sparse attention for faster video diffusion training. arXiv preprint arXiv:2509.01085, 2025. Zhang, F., Tian, S., Huang, Z., Qiao, Y., and Liu, Z. Evaluation agent: Efficient and promptable evaluation framework for visual generative models. arXiv preprint arXiv:2412.09645, 2024. Zhang, J., Su, R., Liu, C., Wei, J., Wang, Z., Wang, H., Zhang, P., Jiang, H., Huang, H., Xiang, C., et al. Efficient attention methods: Hardware-efficient, sparse, compact, and linear attention. Zhang, J., Huang, H., Zhang, P., Wei, J., Zhu, J., and Chen, J. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In International Conference on Machine Learning (ICML 2025), 2025a. Zhang, J., Li, G., and Su, J. Sage: framework of precise retrieval for rag. In International Conference on Data Engineering (ICDE 2025), 2025b. Zhang, J., Wang, H., Jiang, K., Yang, S., Zheng, K., Xi, H., Wang, Z., Zhu, H., Zhao, M., Stoica, I., et al. Sla: Beyond sparsity in diffusion transformers via fine-tunable sparse-linear attention. arXiv preprint arXiv:2509.24006, 2025c. Zhang, J., Wei, J., Huang, H., Zhang, P., Zhu, J., and Chen, J. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In International Conference on Learning Representations (ICLR 2025), 2025d. Zhang, J., Wei, J., Zhang, P., Xu, X., Huang, H., Wang, H., Jiang, K., Zhu, J., and Chen, J. Sageattention3: Microscaling fp4 attention for inference and an exploration of 8-bit training. Advances in Neural Information Processing Systems (NeurIPS 2025), 2025e. Zhang, J., Xiang, C., Huang, H., Xi, H., Zhu, J., Chen, J., et al. Spargeattention: Accurate and training-free sparse In Fortyattention accelerating any model inference. second International Conference on Machine Learning, 2025f. Zhang, J., Xu, X., Wei, J., Huang, H., Zhang, P., Xiang, C., Zhu, J., and Chen, J. Sageattention2++: more efficient implementation of sageattention2. arXiv preprint arXiv:2505.21136, 2025g. Zhang, J., Zheng, K., Jiang, K., Wang, H., Stoica, I., Gonzalez, J. E., Chen, J., and Zhu, J. Turbodiffusion: Accelerating video diffusion models by 100-200 times. arXiv preprint arXiv:2512.16093, 2025h. Zhang, J., Jiang, K., Xiang, C., Feng, W., Hu, Y., Xi, H., Chen, J., and Zhu, J. SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning. 2026. Zhang, P., Chen, Y., Huang, H., Lin, W., Liu, Z., Stoica, I., Xing, E., and Zhang, H. Vsa: Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025i. Zhang, P., Chen, Y., Su, R., Ding, H., Stoica, I., Liu, Z., and Zhang, H. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025j. Zhang, P., Wei, J., Zhang, J., Zhu, J., and Chen, J. Accurate int8 training through dynamic block-level fallback. arXiv preprint arXiv:2503.08040, 2025k. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Zhao, M., Yan, B., Yang, X., Zhu, H., Zhang, J., Liu, S., Li, C., and Zhu, J. Ultraimage: Rethinking resolution extrapolation in image diffusion transformers. arXiv preprint arXiv:2512.04504, 2025a. Zhao, M., Zhu, H., Wang, Y., Yan, B., Zhang, J., He, G., Yang, L., Li, C., and Zhu, J. Ultravico: Breaking extrapolation limits in video diffusion transformers. arXiv preprint arXiv:2511.20123, 2025b. Zheng, K., Wang, Y., Ma, Q., Chen, H., Zhang, J., Balaji, Y., Chen, J., Liu, M.-Y., Zhu, J., and Zhang, Q. Large scale diffusion distillation via score-regularized continuoustime consistency. arXiv preprint arXiv:2510.08431, 2025. Zhou, Y., Xiao, Z., Wei, T., Yang, S., and Pan, X. Trainable log-linear sparse attention for efficient diffusion transformers. arXiv preprint arXiv:2512.16615, 2025. Zhu, K., Tang, T., Xu, Q., Gu, Y., Zeng, Z., Kadekodi, R., Zhao, L., Li, A., Krishnamurthy, A., and Kasikci, B. Tactic: Adaptive sparse attention with clustering and distribution fitting for long-context llms. arXiv preprint arXiv:2502.12216, 2025a. Zhu, L., Huang, Z., Liao, B., Liew, J. H., Yan, H., Feng, J., and Wang, X. Dig: Scalable and efficient diffusion models with gated linear attention. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 76647674, 2025b. 11 SLA2: Sparse-Linear Attention with Learnable Routing and QAT A. Backward Pass of SLA2 The backward pass of SLA2 is presented in Algorithm 3. Following SLA (Zhang et al., 2025c), we manually derive the gradients with respect to Q, K, V, Qϕ and ϕ, while all remaining gradients are computed via PyTorchs automatic differentiation. Note that dHi and dZi are precomputed, such that the main procedure involves only single matrix addition (Line 14), thereby improving computational efficiency. }, {Dl i} ; Zi) ; /(Qϕ Zi))Dl ; i(Hi) Dl Zi))dOl iZ i; dZi = (Qϕ )/(Qϕ Initialize dH = 0, dZ = 0 ; for = 1 to Tm do Algorithm 3 Backward pass of SLA2. 1: Input: Q, K, V, Qϕ, ϕ, Mc, {Li}, {Hi}, {Zi}, Os, Ol from the forward, dOs, dOl RN d. 2: Ds = rowsum(dOs Os), Dl = rowsum(dOl Ol), divide Ds, Dl into Tm blocks {Ds 3: for = 1 to Tm do dHi = (Qϕ /(Qϕ 4: dQϕ = (dOl 5: 6: end for 7: for = 1 to Tn do 8: 9: 10: 11: 12: 13: 14: end if 15: end for 16: dKϕ 17: 18: end for 19: return dQ = {dQi}, dK = {dKi}, dV = {dVi}, dQϕ = {dQϕ ; Pij = exp(Sij Li) ; dVj dVj + ) ; dQi dQi + dSijKj ; if Mc[i, j] = 1 then Sij = QiK / dSij = Pij (dPij Ds else if Mc[i, j] = 0 then dH dH + dHi; = Vj(dH) + (dZ); }, dK ϕ = {dKϕ dKj dKj + dS dZ dZ + dZi ; dVj = Kϕ ijdOs dH ; } ; ; dPij = dOs ijQi ; ijV ; B. Prompts Used The prompt we used for Figure 2 is: first-person perspective video of morning makeup routine in bright, minimalist bathroom. The hands apply moisturizer, followed by foundation, concealer, and setting powder using beauty sponges and brushes. Eyeshadow is blended in neutral tones, eyeliner drawn precisely, and mascara applied to define the lashes. The person dots on lip tint and blush for natural glow. The camera captures close-up details of each step. Natural light floods the scene. The prompt we used for Figure 3 is: fluffy domestic cat running joyfully across sunlit meadow, its ears perked forward and tail held high with excitement. The cats eyes are bright and focused, paws swiftly padding through the tall grass, creating natural motion blur. Golden afternoon light filters through the trees in the background, casting soft shadows. The scene radiates warmth and energy. Shot in smooth 4K slow-motion, low-angle close-up tracking shot following the cats playful sprint."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "UC Berkeley"
    ]
}