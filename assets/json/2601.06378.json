{
    "paper_title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
    "authors": [
        "Hao Zhang",
        "Jiahao Luo",
        "Bohui Wan",
        "Yizhou Zhao",
        "Zongrui Li",
        "Michael Vasilkovsky",
        "Chaoyang Wang",
        "Jian Wang",
        "Narendra Ahuja",
        "Bing Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant progress in 4D generation, rig and motion, the core structural and dynamic components of animation are typically modeled as separate problems. Existing pipelines rely on ground-truth skeletons and skinning weights for motion generation and treat auto-rigging as an independent process, undermining scalability and interpretability. We present RigMo, a unified generative framework that jointly learns rig and motion directly from raw mesh sequences, without any human-provided rig annotations. RigMo encodes per-vertex deformations into two compact latent spaces: a rig latent that decodes into explicit Gaussian bones and skinning weights, and a motion latent that produces time-varying SE(3) transformations. Together, these outputs define an animatable mesh with explicit structure and coherent motion, enabling feed-forward rig and motion inference for deformable objects. Beyond unified rig-motion discovery, we introduce a Motion-DiT model operating in RigMo's latent space and demonstrate that these structure-aware latents can naturally support downstream motion generation tasks. Experiments on DeformingThings4D, Objaverse-XL, and TrueBones demonstrate that RigMo learns smooth, interpretable, and physically plausible rigs, while achieving superior reconstruction and category-level generalization compared to existing auto-rigging and deformation baselines. RigMo establishes a new paradigm for unified, structure-aware, and scalable dynamic 3D modeling."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 1 8 7 3 6 0 . 1 0 6 2 : r RigMo: Unifying Rig and Motion Learning for Generative Animation Hao Zhang1,2 Jiahao Luo1,3 Bohui Wan2 Yizhou Zhao1,4 Zongrui Li5 Michael Vasilkovsky1 Chaoyang Wang1 Jian Wang1 Narendra Ahuja2 Bing Zhou1 1Snap Inc. 2University of Illinois Urbana-Champaign 3University of California, Santa Cruz 4Carnegie Mellon University 5Nanyang Technological University Figure 1. RigMo jointly learns rigging and motion by understanding the underlying structure of mesh sequences. Unlike optimizationbased methods that fit rig per sequence, RigMo is feed-forward framework that infers Gaussian bones, skinning weights, and motion parameters directly from input meshes for unified 4D animation generation. Colors visualize the influence of Gaussian bones on vertices (skinning weights) across the mesh surface; similar colors may appear for different bones as they are randomly assigned for visualization."
        },
        {
            "title": "Abstract",
            "content": "Despite significant progress in 4D generation, rig and motionthe core structural and dynamic components of animationare typically modeled as separate problems. Existing pipelines rely on ground-truth skeletons and skinning weights for motion generation and treat auto-rigging as an independent process, undermining scalability and interpretability. We present RigMo, unified generative framework that jointly learns rig and motion directly from raw mesh sequences, without any human-provided rig annotations. RigMo encodes per-vertex deformations into two compact latent spaces: rig latent that decodes into explicit Gaussian bones and skinning weights, and motion latent that produces time-varying SE(3) transformations. Together, these outputs define an animatable mesh with explicit structure and coherent motion, enabling feed-forward rig and motion inference for deformable objects. Beyond unified rigmotion discovery, we introduce Motion-DiT model operating in RigMos latent space and demonstrate that these structure-aware latents can naturally support downstream motion generation tasks. Experiments on DeformingThings4D, Objaverse-XL, and TrueBones demonstrate that RigMo learns smooth, interpretable, and physically plausible rigs, while achieving superior reconstruction and category-level generalization compared to existing auto-rigging and deformation baselines. RigMo establishes new paradigm for unified, structure-aware, and scalable dynamic 3D modeling. Project page: https://RigMoPage.github.io 1. Introduction Animation fundamentally couples structure and motion. The structure, typically represented by rig, defines how an object can deform, while motion describes how that structure evolves over time. Yet despite their inherent interdependence, most existing pipelines treat these components in isolation. Auto-rigging systems [20, 29] rely on artist-designed skeletons and skinning weights, training models to imitate human heuristics rather than understanding why certain rigs produce plausible deformations. As result, these methods depend heavily on human annotations, which limits scalability and consistency across datasets and object categories. second class of motion-generation approaches assumes that ground-truth rig is already provided. Human motion models [1, 11, 16, 22, 24, 25, 33, 34, 41, 44, 55, 57] and animalor object-centric pipelines [26, 58] such as AnyTop [9] operate entirely in pose space, predicting joint rotations or SE(3) transforms on top of predefined kinematic structure. These methods cannot infer rig structures, cannot handle arbitrary geometries, and fail when the assumed skeleton is mismatched or unavailable. third line of work removes rigging entirely. Modern vertex-space motion generators and 4D reconstruction methods [2, 3, 8, 13, 28, 38, 46, 53], including AnimateAnyMesh [38] and GVFDiffusion [46], predict deformations frame by frame without any structural abstraction. While flexible, these models are difficult to control, hard to interpret, and unable to produce reusable animatable assetsthe core purpose of rigging. Together, these paradigms expose fundamental gap: there is no unified framework that jointly learns rig structure and motion dynamics directly from raw mesh sequences, without predefined skeletons or per-sequence optimization. At the same time, relying on human-annotated rigs is costly and inconsistent, making fully data-driven alternative both necessary and practical. This motivates model capable of discovering structure from deformation and producing motion that conforms to that learned structure. We introduce RigMo, generative model that addresses this gap by jointly learning rig and motion with unified architecture. RigMo encodes dynamic mesh sequences into two complementary latent spaces capturing spatial articulation and temporal evolution. The decoder reconstructs an explicit set of Gaussian bones [42], from which skinning weights are derived, and predicts per-frame SE(3) transformations, together producing fully animatable representation. This Gaussian-bone formulation provides compact and scalable abstraction whose complexity depends only on the number of bones rather than mesh resolution and inherently resolution-insensitive. Furthermore, the model is trained in self-supervised manner and requires no human annotations, enabling learning from unlabeled datasets such as DeformingThings4D [18]. In contrast to classical Smooth Skinning Decomposition with Rigid Bones (SSDR), which solves separate optimization problem for each individual sequence, RigMo performs feed-forward rig and motion inference and generalizes across unseen categories and motion styles. The resulting representations are explicit, interpretable, and directly manipulable, supporting downstream applications such as motion generation, motion interpolation, and controllable editing. This naturally connects to our second contribution. Beyond unified rigmotion discovery, we introduce Motion-DiT, diffusion transformer operating in RigMos motion latent space. By generating temporal trajectories in structure-aware latent domain rather than raw vertex coordinates, Motion-DiT provides principled pathway toward controllable, high-quality motion synthesis and demonstrates the utility of RigMos learned latent representation for downstream motion-generation tasks. Across DeformingThings4D, Objaverse-XL [7], and TrueBones, RigMo recovers physically plausible skeletons, coherent skinning structures, and realistic motions directly from raw deformation data, establishing unified, structureaware, and controllable framework for 4D generative animation. 2. Related Works 4D Generative Models. Recent work has produced variety of generative models for 4D generation [5, 6, 8, 12, 19, 27, 35, 37, 39, 43, 45, 47, 48, 56]. common framework is the use of carefully designed 3D or 4D variational autoencoders to embed geometry and motion into latent spaces, with generative networks (i.e., diffusion models [10]) decoding these latents conditioned on texts or videos. Although such methods yield visually compelling results, the resulting motions often lack controllability: they produce animations without explicit rigging and corresponding motion parameters, which greatly limits their applicability in practical use. In this work, we address this issue by encoding deformable meshes into unified spatio-temporal latent representations that can be further decoded into highly controllable rig structures and associated motions. Auto-Rigging. Auto-rigging aims to convert static mesh into an animatable asset by predicting skeleton and pervertex skinning weights. Template-based approaches [4, 15, 23] attach predefined skeletal structures and optimize their poses and weights, but generalize poorly beyond humanoid shapes. Template-free methods such as RigNet [40] relax assumptions on joint topology, though they may produce overly dense skeletons or rely on handcrafted geometric cues. More recently, auto-regressive models [21, 30, 31, 52] treat skeleton generation as sequence prediction problem, leveraging large datasets such as Articulation-XL [31] to learn category-agnostic articulation priors. Despite progress, these approaches still depend on artist-annotated rigs at scale, which remain costly and difficult to obtain. Optimization-Based Inverse Skinning. Inverse skinning [32, 36, 4951] considers the complementary task to auto-rigging: given sequence of articulated mesh deformations, the objective is to recover the underlying rig parameters, including bone transformations, skinning weights, or elasticity properties. Classical solutions based on Linear Blend Skinning (LBS) offer computational efficiency but restrict deformation to linear blend of transformations, limiting realism. Recent physically grounded formulations [51] jointly optimize skeletal motion and material properties, enabling non-linear volumetric behavior. However, these pipelines require per-case nonlinear optimiza2 Figure 2. Overview of the RigMo-VAE framework. Given temporal vertex trajectories from deforming mesh sequences, RigMo employs dual-path encoder to disentangle static geometry (rigging branch) and dynamic motion (motion branch), learning compact latent representation that captures both spatial structure and temporal dynamics. The decoder maps these latent features to physically interpretable rig components: Gaussian bone descriptors defining geodesic-aware skinning weights and variational motion parameters for local and root transformations. Different colors indicate the influence regions of learned Gaussian bones, demonstrating semantically meaningful decomposition of mesh deformation without manual rigging supervision. tion, making them slow and difficult to scale. Moreover, the recovered rigs are tightly bound to individual sequences and fail to generalize across subjects or motion styles. In contrast, our method predicts both motion trajectories and implicit rig parameters in single feed-forward pass, jointly learning from all training sequences to obtain shared, generalizable articulation representation that transfers robustly across diverse objects and motions. 3. Method RigMo is novel framework for modeling deformable surface motion by learning rigging representations composed of Gaussian bones (see Gaussian Bone Definition in Sec. 3.1.3) and their associated motion parameters {qlocal, tlocal, qroot, troot}. Here denotes rotation quaternions and represents translation vectors, which jointly govern mesh deformation through differentiable skinning. Unlike SSDR methods that require per-case fitting, RigMo is feedforward framework capable of inferring rig structure and motion directly from input sequences. Our method consists of two main components: the RigMo VAE (Sec. 3.1), which learns the rigging representation and corresponding motion parameters, and the Motion DiT module (Sec. 3.2), which operates in RigMo VAEs learned latent space and uses rig-branch outputs as conditioning signals to diffuse motion latents for controllable animation generation. 3.1. RigMo VAE Given mesh sequence RBT 3, with batch size B, frame count , and vertices per mesh, RigMo reconstructs the deformable motion ˆV RB(T 1)N 3 from latent representations that encode both spatial structure and temporal dynamics. The network architecture consists of: (1) topology-aware encoder that aggregates geometric and motion cues, (2) decoder that predicts Gaussian bones and their transformations, and (3) Gaussian-based Linear Blend Skinning (LBS) module that synthesizes deformations under learned rig parameters. 3.1.1. Topology-Aware Encoder RigMos encoder jointly captures static geometry and dynamic motion through dual-path architecture designed to disentangle canonical shape information from temporal deformation cues. This ensures that rigging prediction reflects stable structure rather than specific motion instances, enabling generalizable, consistent, and interpretable bonevertex relationships. Rigging Branch. The rigging branch processes the canonical mesh geometry (first to establish bonevertex correspondence and predict Gaussian bone parameters. We encode the input vertices V0 RN 3 into per-vertex embeddings Vemb via topology-aware attention: frame) 0 hℓ = Attn(LN(hℓ1), ) + hℓ1. (1) Farthest Point Sampling selects bone tokens Bemb and coordinates Cbone. Cross-attention produces bonevertex correlation features: Arig = CrossAttn(Bemb, Vemb 0 , Vemb 0 ), (2) from which each bone predicts Gaussian parameters Gk = [ck, sk, qk]. 3 Motion Branch. The motion branch encodes temporal deformation patterns and predicts variational latent variables governing local and global motion. We first compute per-frame vertex displacements: Skinning Weight Computation. For each vertex vi, the skinning weight with respect to bone is computed using the Mahalanobis distance within the Gaussian bones coordinate system: = V[:, 1 :, :, :] V[:, : 1, :, :] RB(T 1)N 3. (3) are processed through tempoThese displacements ralspatial attention layers to obtain Vemb . Using the shared bone token coordinates Cbone, we extract bonemotion interaction features: Amotion = CrossAttn(Bemb, Vemb ) RB(T 1)Kdb . , Vemb (4) These features are fed into the local-motion posterior estimator, which predicts bone-level posterior parameters for each bone and frame t: [µlocal, log σlocal] = MLP(Amotion). (5) Latent variables are then sampled via: zlocal = µlocal + σlocal ϵ, ϵ (0, I). (6) For the global transformation, global-motion posterior estimator temporally aggregates motion features (average pooling), producing root-level posterior parameters: [µroot, log σroot] = MLP(Agg(Amotion)). root-level latent code is sampled as: zroot = µroot + σroot ϵ. 3.1.2. Rig-Motion Decoder (7) (8) The decoder maps the latent codes to physically meaningful transformations, comprising static Gaussian-bone decoder and motion decoders for local and root SE(3) transformations. Taking as input the latent features produced by the rigging branch, lightweight MLP-based module predicts soft Gaussian bone regions = [c, s, q] for each bone, while local and global motion are decoded from the latent variables predicted in the motion encoder. Concretely, local-motion decoder maps zlocal to per-bone transformations: {qlocal, tlocal} = Declocal(zlocal). (9) separate root-motion decoder maps zroot to global transformations: wraw ik = exp(cid:0) 1 2 j=1 exp(cid:0) 1 (vi ck) sk2(cid:1) 2 (vi cj) sj2(cid:1) , (cid:80)K (11) where Rk is the rotation matrix derived from quaternion qk, and denotes element-wise division. Linear Blend Skinning. Given the skinning weights and bone transformations, each vertex is deformed through Linear Blend Skinning: ˆvi = (cid:88) k= wik Tk vi, (12) where Tk = Troot Tk,local represents the hierarchical transformation combining root motion and local bone motion, and vi is the vertex in homogeneous coordinates. Topological Coherence via Geodesic-Aware Weight Refinement. While Gaussian weighting captures local proximity, it may erroneously couple topologically distant regions that are spatially close (e.g., armtorso contact). To mitigate such artifacts, we introduce geodesic-aware weight refinement strategy that enforces topological consistency on the mesh surface. Let = {v1, . . . , vN } denote vertices and = {a1, . . . , aK} denote bone anchors with raw weights raw RN K. We define the surface geodesic distance between vertex vi and anchor ak as: dg(vi, ak) = min πΠ(ak,vi) (cid:88) (vp,vq)π vp vq2, (13) where Π(ak, vi) enumerates edge-connected vertex paths. binary coherence mask is constructed as Mik = (cid:40) 1, dg(vi, ak) < τ, 0, otherwise. (14) {qroot, troot} = Decroot(zroot). (10) The refined skinning weights become: 3.1.3. Gaussian Skinning LBS Module Gaussian Bone Definition. Each Gaussian bone is defined by its geometric parameters Gk = [ck, sk, qk], where ck R3 represents the bone center, sk R3 denotes the anisotropic scaling factors, and qk R4 is the orientation quaternion. These parameters collectively define 3D Gaussian ellipsoid that acts as soft bone with spatiallyvarying influence. Wik = raw ik Mik, wik = Wik Wij + ε (cid:80)K j=1 , (15) with ε = 108 for numerical stability. Vertices with no reachable bones are assigned one-hot weight to their nearest bone. This refinement effectively suppresses cross-part influence and yields cleaner, more coherent skinning, with vertices typically retaining influence from only 23 bones. 4 Figure 3. Overview of the Motion DiT. Given static rigging features, condition encoder produces anchor and global tokens that guide diffusion transformer operating in RigMos motion-latent space. The model uses spatial, temporal, and frame-conditioned cross-attention to predict denoised motion latents, which are decoded into bone transformations and vertex sequences via Gaussian skinning. Self-Supervised Training Objectives. key advantage of RigMo is that it requires no rigging annotations or supervision during training. Instead, the model learns underlying rig structures in purely self-supervised manner directly from motion data alone. This eliminates the need for expensive manual rigging annotations from skilled artists, making the approach highly scalable to large-scale motion datasets. RigMo-VAE is trained end-to-end using only two self-supervised objectives: vertex-level reconstruction loss and latent regularization term: Ltotal = λreconLrecon + λKLLKL. (16) The reconstruction term supervises per-vertex motion fidelity without requiring any rig supervision: Lrecon = 1 BT (cid:88) b,t,i ˆvb,t,i vb,t,i2, (17) while the KL divergence regularizes latent distributions toward unit Gaussian prior: LKL = 1 2 (cid:88) (µ2 + σ2 log σ 1). (18) This pure motion-driven formulation enables the model to discover semantically meaningful bone structures and their relationships purely from observing vertex trajectories, bypassing the traditional bottleneck of manual rigging annotation and enabling scalable learning from diverse motion collections. 3.2. Motion DiT Given static rigging features as conditions, the Motion DiT generates motion latents in the RigMo-VAE space. condition encoder aggregates static/rigging cues (e.g., anchor/gaussian/skinning features) into anchor tokens RBKH and global token RBH , which remain fixed during generation and guide the denoising process. We first project the VAEs dynamic tokens Zdyn RBK(T 1)Ddyn and root tokens Zroot RB(T 1)Droot to common width and form unified motion-latent tensor Zmot RB(K+1)(T 1)H by concatenating the bone streams with the root stream. Conditioned on configurable frame-mask schedule that specifies observed and generated frames, the diffusion transformer predicts the velocity fields ˆvmot for the generated frames, while keeping the observed frames fixed as conditioning signals. ˆx0 is then recovered via v-prediction and decoded by RigMo-VAE decoder to animations. This conditioning-by-statics and generation-in-motion-latents design ties the synthesized dynamics tightly to the learned rig structure. The backbone consists of L=12 interleaved spatiotemporal attention (ISTA) blocks with hidden dimension H=512. Each block alternates spatial attention (within frame, across bones) and temporal attention (within bone, across frames), augmented by two conditioning pathways: (1) cross-attention injecting static and global priors (A, g), and (2) frame-level cross-attention that aligns generated frames with observed ones based on masks. The network outputs the velocity parameterization ˆv, from which the denoised latent state is reconstructed as ˆx0 = αtxt 1 αt ˆv. Losses are computed only on generated frames: = λlatLlat + λrotLrot + λtransLtrans + λvertLvert, where Llat denotes the latent-space L2 loss, Lrot the SO(3) geodesic rotation loss, Ltrans the translation L2 loss, and Lvert the vertex-space L2 loss. Weights are set to (λlat, λrot, λtrans, λvert) = (0.5, 1.0, 0.2, 0.1). This design enables Motion DiT to generate or interpolate complex, longrange motions in RigMos latent space while maintaining spatial coherence and temporal realism. 4. Experiments Our primary focus in this paper is the proposed RigMoVAE, which forms the core of our rigmotion representation. While we additionally introduce Motion DiT to demonstrate the effectiveness of RigMo-VAE as latent Figure 4. Results produced by the full RigMo. Given sparse input sequence, where subset of frames is observed according to frame mask, RigMo reconstructs complete animatable model by jointly predicting the rigging structure (Gaussian bones and skinning weights) and synthesizing the missing motion frames through diffusion in the RigMo latent space. The resulting rigged model produces coherent, articulated motion across humans, animals, and diverse non-human shapes, demonstrating that sparse observations are sufficient to recover full animation without category-specific priors. space for downstream motion generation, the main contributions of this work lie in the rigging and motion decomposition capabilities of the VAE itself. Due to space constraints, more results for the Motion DiT are provided in the supplementary material. 4.1. Datasets We curate large-scale corpus of 20,000 deformable mesh sequences spanning three complementary domains to ensure diverse motion coverage and strong generalization. DeformingThings4D [18] provides 1,972 real-world sequences capturing organic non-rigid deformations, where sequences shorter than 10 frames are excluded for temporal stability. TrueBones [14] contributes 1,287 highfidelity articulated animations with realistic skeletal motion. Objaverse-XL [7] offers 17,024 synthetic sequences (24 frames each) filtered through motion-quality classifier to retain diverse topologies and motion styles across categories. For each dataset, we adopt 5:1 split into training and test sets to enable consistent evaluation across domains. To handle heterogeneous mesh resolutions while preserving geometric consistency, we adopt topologypreserving vertex normalization protocol. Meshes exceeding 20K vertices are downsampled to 5K via Farthest Point Sampling (FPS) with geodesic neighborhood preservation, while lower-resolution meshes are iteratively subdivided until surpassing the target vertex count, then downsampled identically. This procedure yields uniform vertex embeddings with consistent local connectivity, facilitating robust and topology-aware rigging discovery. 4.2. Implementation Details RigMo-VAE is implemented in PyTorch and trained with mixed precision on 24A100 (80 GB) GPUs. Each batch contains 144 mesh sequences (T =20, =5K). We use AdamW (β1=0.9, β2=0.999, weight decay 104) with learning rate of 3104, 2K warm-up steps, cosine decay, gradient clipping (1.0), and EMA (0.999). The encoder uses six topology-aware attention layers (hidden dim 256, eight heads, neighborhood size k=5). Anchor sampling selects K=48/128 Gaussian bones, and skinning weights are sparsified to the top Ks=4. Training uses reconstruction and KL losses (λrecon=1.0, λKL=106), with KL annealed over the first 30% of training. On the full corpus, RigMo 6 Figure 5. Comparison between UniRig+Optimization and our RigMo Rigging Module. Although UniRig may produce visually plausible skinning weights in some cases (e.g., the fox), its rigging does not generalize and collapses under actual animation, leading to severe deformation artifacts. In contrast, RigMo learns robust and transferable rig structures directly from motion, without any ground-truth rig supervision, and achieves stable, high-fidelity deformations across diverse poses and animal species. Method Training Reconstruction CD-L2 CD-L1 Cross-Motion Transfer CD-L2 CD-L1 Mean CD-L1 CD-L2 Per-Case Optimization UniRig [52] + Optimization MagicArticulate [31] + Opt. 12.3 0.2 37.3 2.3 43.1 3.7 8.21 0.3 28.4 2.1 23.9 3.2 68.8 6.7 48.6 4.9 53.4 5. 43.5 5.4 31.2 4.6 28.7 3.9 RigMo (Ours) 11.1 0.6 7.64 0.3 13.82 0.49 11.83 0. 40.55 42.95 48.25 12.46 25.86 29.80 26.30 9.74 Table 1. Rigging Discovery and Cross-Motion Generalization (103). Lower is better. Experiments are done on DT4D test dataset. converges after 50K steps (10 days). Training only on DeformingThings4D reaches comparable reconstruction within 7.5K epochs (1.5 days). Inference is fully feedforward, reconstructing 20-frame 5K-vertex sequence in 40 ms per frame on an A100 GPU. Motion DiT is implemented in PyTorch with sinusoidal timestep embeddings and 12-block ISTA backbone applied to 512-d motion tokens. We adopt DDPM scheduler with 1000 steps and scaled-linear β, using the v-prediction parameterization. Training uses AdamW (β1=0.9, β2=0.999, weight decay 0.01) with an initial learning rate of 1104 and cosine annealing on 4 H200 GPUs (batch size 64). End-to-end training takes 40 hours and converges within 5000 epochs. The model remains stable without auxiliary losses, aided by its spatialtemporal attention design and sparse conditioning in RigMos motion-latent space. 4.3. Rigging Evaluation Evaluation Protocol. We conduct comprehensive rigging evaluation on the DeformingThings4D partition, leveraging its annotation-free nature to assess genuine unsupervised rig-discovery capabilities. Our experimental protocol randomly samples pairs of motion sequences from the same objects training data and testing data, forming the training and testing splits used for evaluation, with training sequences used for baseline optimization and testing sequences used to assess cross-motion rig transferability. Statistical significance is ensured through 100 independent random splits, reporting averaged metrics and confidence intervals. Competitive Baselines. As shown in Tab. 1, we establish rigorous comparisons against two paradigmatic approaches representing current practice: Per-Case Optimization. This baseline jointly optimizes rig parameters {G, W} and motion parameters {Tt} via gradient descent on the reconstruction loss Lrecon. During testing, rig parameters remain fixed while only transformations are optimized, directly measuring generalization capacity. However, because the predicted rigging is fully determined by the training sequence itself, this approach can only perform well on the specific cases it was optimized for. When applied to unseen motions, the optimized rig often collapses or becomes invalid, revealing lack of generalization. AutoRigging Pipeline. We employ state-of-the-art automatic rigging methods (UniRig [52], MagicArticulate [31]) to generate initial bone structures from canonical poses, followed by per-sequence transformation optimization. This pipeline reflects the current industry standard for automated character rigging. As illustrated in Fig. 5, the generalization ability of autorigging pipelines is also constrained by the diversity and structure of the training data. In many cases, these methods produce bone layouts that appear visually plausible, yet they fail to reproduce even common motions during animation. Importantly, even experienced artists cannot rely purely on visual priors to assign correct skinning weights; iterative adjustment is always required to produce rig that accurately reproduces target deformations. This observation is consistent with the core motivation behind RigMorobust rigging cannot be inferred from static geometry alone and must be learned directly from motion. 4.4. RigMo-VAE Reconstruction Evaluation We establish comprehensive reconstruction benchmarks against leading mesh-generation architectures on our curated subtest set of 500 sequences spanning diverse defor7 Method Geometric Accuracy CD-L1 CD-L2 Time (20f) AnimateAnyMesh [38] Step1X3D [17] Hunyuan3D [54] 1.81 0.13 3.63 0.21 3.21 0.19 1.32 0.01 2.96 0.18 2.67 0.14 RigMo (Ours) 1.73 0.11 1.26 0. 2.8s 22.6s 17.4s 0.74s Table 2. Reconstruction fidelity and inference efficiency (102 for CD). RigMo achieves the best geometric accuracy and the fastest inference among all baselines. mation patterns and object categories. We compare against representative paradigms spanning both sequence-aware and frame-independent generative models (as shown in Tab. 2): AnimateAnyMesh [38] employs sequence-aware VAE architecture for temporal mesh modeling. However, its design requires highdimensional 512-token representation to encode object motion, whereas RigMo-VAE achieves superior reconstruction quality using only 48/128 tokens, resulting in significantly faster inference and reduced memory footprint. Step1X3D [17] and Hunyuan 3D 2.1 [54] represent stateof-the-art frame-independent 3D VAEs that perform perframe generation, incurring substantial computational overhead for sequence reconstruction. These methods inherently break vertex correspondence across frames and tend to lose fine-grained surface details due to their framewise decoding process. Because their latent spaces operate on inconsistent canonicalizations, our evaluation applies poseinvariant correction: we record the canonical transformations {R, t, s} used for input normalization and apply the inverse transformations after reconstruction to enable fair, shape-focused geometric assessment. Configuration CD-L1 CD-L2 w/o Geodesic Refinement 48 Bone Tokens 128 Bone Tokens 2.37 0.15 1.91 0.13 1.73 0.11 2.07 0.18 1.48 0.12 1.26 0.08 Table 3. Ablation study on the DeformingThings4D validation set using CD-L1/L2 metrics (102). the predicted rig and motion parameters can be directly applied back to the original mesh resolution. This property results in consistent deformation quality even when the mesh tessellation changes, as the learned bones capture geometric regions rather than fixed vertex subsets. This contrasts with mesh-dependent approaches [38], where changing vertex count typically alters the learned skinning structure and degrades animation fidelity. Topological Refinement Impact. Next, we evaluate the effectiveness of our geodesic-aware weight refinement module. Removing this component leads to substantial decline in reconstruction quality as shown in Tab.3. These results highlight that spatial proximity alone is insufficient for articulated objects, where vertices may be close in Euclidean space but distant in the underlying kinematic topology. Bone Token Cardinality Trade-off. Finally, we analyze the effect of bone token cardinality. While increasing from 48 to 128 tokens yields 0.018% improvement in CD-L1, the gains diminish as tokens become overly fine-grained. Although 128 tokens achieve the best quantitative reconstruction, 48 tokens provide more favorable balance between efficiency, interpretability, and stability. Excessive tokens tend to fragment coherent anatomical regions without offering proportional improvements. 4.5. Qualitative Analysis 5. Conclusion In this paper, we introduce RigMo, unified generative framework that jointly learns rig structure and motion dynamics directly from raw mesh sequences without any rig annotations. RigMo factorizes per-vertex deformations into two compact latent spaces that decode into explicit Gaussian bones, skinning weights, and bone transformations, producing fully animatable representations by construction. Together with Motion-DiT model that operates on these structure-aware latents, RigMo enables controllable motion generation and seamless downstream applications. Experiments on multiple datasets show that RigMo discovers smooth, interpretable, and physically consistent rigs while outperforming existing auto-rigging and deformation methods in accuracy and generalization. As shown in Fig.4, we present comprehensive visualizations demonstrating RigMos semantic understanding and generation capabilities across three critical aspects: Learned Rigging Structures. Our visualizations reveal semantically meaningful bone-vertex correspondences emerging without supervision, with distinct Gaussian bones capturing intuitive anatomical regions (limbs, torso, extremities) across diverse object categories. Controllable Motion Synthesis. Motion DiT demonstrations exhibit precise conditional generation capabilities, producing plausible animations that respect learned rig constraints while enabling creative exploration of novel motion patterns. 4.6. Diagnostics Resolution-Agnostic Architecture. Regardless of the original resolution, all meshes are first resampled to 5K-vertex representation for processing in the encoder. Since Gaussian bones and motion transformations are defined continuously in 3D space, rather than tied to specific vertex indices,"
        },
        {
            "title": "References",
            "content": "[1] Kfir Aberman, Peizhuo Li, Dani Lischinski, Olga SorkineHornung, Daniel Cohen-Or, and Baoquan Chen. Skeletonaware networks for deep motion retargeting. ACM Transactions on Graphics (TOG), 39(4):621, 2020. 2 [2] Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: In ECCV, Trajectory-conditioned text-to-4d generation. 2024. 2 [3] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In CVPR, 2024. 2 [4] Ilya Baran and Jovan Popovic. Automatic Rigging and Animation of 3D Characters. In ACM TOG, 2007. 2 [5] Wei Cao, Chang Luo, Biao Zhang, Matthias Nießner, and Jiapeng Tang. Motion2vecsets: 4d latent vector set diffusion for non-rigid shape reconstruction and tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2049620506, 2024. [6] Jianqi Chen, Biao Zhang, Xiangjun Tang, and Peter Wonka. V2m4: 4d mesh animation reconstruction from single monocular video. arXiv preprint arXiv:2503.09631, 2025. 2 [7] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-XL: universe of 10m+ 3d objects. NeurIPS, 2023. 2, 6 [8] Zhoujie Fu, Jiacheng Wei, Wenhao Shen, Chaoyue Song, Xiaofeng Yang, Fayao Liu, Xulei Yang, and Guosheng Lin. Sync4d: Video guided controllable dynamics for physicsarXiv preprint arXiv:2405.16849, based 4d generation. 2024. 2 [9] Inbar Gat, Sigal Raab, Guy Tevet, Yuval Reshef, Amit Haim Bermano, and Daniel Cohen-Or. Anytop: Character animation diffusion with any topology. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. 2 [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [11] Seokhyeon Hong, Chaelin Kim, Serin Yoon, Junghyun Nam, Sihun Cha, and Junyong Noh. Salad: Skeleton-aware latent diffusion for text-driven motion generation and editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 71587168, 2025. [12] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao. Consistent4d: Consistent 360 {deg} dynamic obarXiv preprint ject generation from monocular video. arXiv:2311.02848, 2023. 2 [13] Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. Animate3d: Animating any 3d model with multi-view video diffusion. NeurIPS, 2024. 2 [14] Wonkwang Lee, Jongwon Jeong, Taehong Moon, HyeonJong Kim, Jaehyeon Kim, Gunhee Kim, and Byeong-Uk Lee. How to move your dragon: Text-to-motion synthesis for large-vocabulary objects. arXiv preprint arXiv:2503.04257, 2025. 6 [15] Peizhuo Li, Kfir Aberman, Rana Hanocka, Libin Liu, Olga Sorkine-Hornung, and Baoquan Chen. Learning Skeletal Articulations with Neural Blend Shapes. ACM TOG, 2021. 2 [16] Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen. Example-based motion synthesis via generative motion matching. ACM Transactions on Graphics (TOG), 42(4):112, 2023. 2 [17] Weiyu Li, Xuanyang Zhang, Zheng Sun, Di Qi, Hao Li, Wei Cheng, Weiwei Cai, Shihao Wu, Jiarui Liu, Zihao Wang, et al. Step1x-3d: Towards high-fidelity and controllable generation of textured 3d assets. arXiv preprint arXiv:2505.07747, 2025. [18] Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng, and Matthias Nießner. 4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface. In ICCV, 2021. 2, 6 [19] Zhiqi Li, Yiming Chen, and Peidong Liu. Dreammesh4d: Video-to-4d generation with sparse-controlled gaussianmesh hybrid representation. Advances in Neural Information Processing Systems, 37:2137721400, 2024. 2 [20] Zhouyingcheng Liao, Jimei Yang, Jun Saito, Gerard PonsMoll, and Yang Zhou. Skeleton-Free Pose Transfer for Stylized 3D Characters. In ECCV, pages 640656, 2022. 1 [21] Isabella Liu, Zhan Xu, Wang Yifan, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, and Zifan Shi. RigAnything: Template-free Autoregressive Rigging for Diverse 3D Assets. ACM TOG, 2025. 2 [22] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. 2023. 2 [23] Jing Ma and Dongliang Zhang. Tarig: Adaptive templateaware neural rigging for humanoid characters. Computers & Graphics, 2023. 2 [24] Ekkasit Pinyoanuntapong, Muhammad Saleem, Korrawe Karunratanakul, Pu Wang, Hongfei Xue, Chen Chen, Chuan Guo, Junli Cao, Jian Ren, and Sergey Tulyakov. Maskcontrol: Spatio-temporal control for masked motion synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 99559965, 2025. [25] Sigal Raab, Inbal Leibovitch, Peizhuo Li, Kfir Aberman, Olga Sorkine-Hornung, and Daniel Cohen-Or. Modi: Unconditional motion synthesis from diverse data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1387313883, 2023. 2 [26] Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit Bermano, and Daniel Cohen-Or. Single motion diffusion. arXiv preprint arXiv:2302.05905, 2023. 2 [27] Jiawei Ren, Cheng Xie, Ashkan Mirzaei, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, Huan Ling, et al. L4gm: Large 4d gaussian reconstruction model. Advances in Neural Information Processing Systems, 37:5682856858, 2024. 2 9 [28] Yahao Shi, Yang Liu, Yanmin Wu, Xing Liu, Chen Zhao, Jie Luo, and Bin Zhou. Drive any mesh: 4d latent diffusion for mesh deformation from video. arXiv preprint arXiv:2506.07489, 2025. 2 [29] Chaoyue Song, Jiacheng Wei, Ruibo Li, Fayao Liu, and Guosheng Lin. 3D Pose Transfer with Correspondence LearnIn NeurIPS, pages 31083120, ing and Mesh Refinement. 2021. [30] Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, and Jianfeng Zhang. Puppeteer: Rig and Animate Your 3D Models. NeurIPS, 2025. 2 [31] Chaoyue Song, Jianfeng Zhang, Xiu Li, Fan Yang, Yiwen Chen, Zhongcong Xu, Jun Hao Liew, Xiaoyang Guo, Fayao Liu, Jiashi Feng, et al. Magicarticulate: Make your 3d models articulation-ready. In CVPR, 2025. 2, 7 [32] Keqiang Sun, Dor Litvak, Yunzhi Zhang, Hongsheng Li, Jiajun Wu, and Shangzhe Wu. Ponymation: Learning articulated 3d animal motions from unlabeled online videos. In European Conference on Computer Vision, pages 100119. Springer, 2024. 2 [33] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit Bermano. Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022. 2 [34] Shashank Tripathi, Omid Taheri, Christoph Lassner, Michael Black, Daniel Holden, and Carsten Stoll. Humos: Human motion model conditioned on body shape. In European Conference on Computer Vision, pages 133152. Springer, 2024. 2 [35] Xinzhou Wang, Yikai Wang, Junliang Ye, Fuchun Sun, Zhengyi Wang, Ling Wang, Pengkun Liu, Kai Sun, Xintong Wang, Wende Xie, et al. Animatabledreamer: Textguided non-rigid 3d model generation and reconstruction In European Conference with canonical score distillation. on Computer Vision, pages 321339. Springer, 2024. 2 [36] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Magicpony: Learning articulated 3d animals in the wild, 2023. 2 [37] Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, and Xiang Bai. Sc4d: Sparse-controlled video-to-4d generation and motion transfer. In European Conference on Computer Vision, pages 361379. Springer, 2024. [38] Zijie Wu, Chaohui Yu, Fan Wang, and Xiang Bai. Animateanymesh: feed-forward 4d foundation model for textdriven universal mesh animation. ICCV, 2025. 2, 8 [39] Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency. arXiv preprint arXiv:2407.17470, 2024. 2 [40] Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth, and Karan Singh. Rignet: Neural rigging for articulated characters. arXiv preprint arXiv:2005.00559, 2020. 2 [41] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for skeleton-based action recognition. In Proceedings of the AAAI conference on artificial intelligence, 2018. 2 [42] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, Deva Ramanan, William T. Freeman, and Ce Liu. Lasr: Learning articulated shape reconstruction from monocular video, 2021. 2 [43] Chun-Han Yao, Yiming Xie, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d 2.0: Enhancing spatio-temporal consistency in multi-view video diffusion for high-quality 4d generation. arXiv preprint arXiv:2503.16396, 2025. 2 [44] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1601016021, 2023. [45] Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao. Stag4d: Spatial-temporal anchored generative 4d gaussians. In European Conference on Computer Vision, pages 163179. Springer, 2024. 2 [46] Bowen Zhang, Sicheng Xu, Chuxin Wang, Jiaolong Yang, Feng Zhao, Dong Chen, and Baining Guo. Gaussian variation field diffusion for high-fidelity video-to-4d synthesis. In ICCV, 2025. 2 [47] Hao Zhang, Di Chang, Fang Li, Mohammad Soleymani, and Narendra Ahuja. Magicpose4d: Crafting articulated modarXiv preprint els with appearance and motion control. arXiv:2405.14017, 2024. 2 [48] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion: Multi-view video diffusion model for 4d generation. Advances in Neural Information Processing Systems, 37:1527215295, 2024. 2 [49] Hao Zhang, Fang Li, Samyak Rawlekar, and Narendra Ahuja. S3o: dual-phase approach for reconstructing dynamic shape and skeleton of articulated objects from sinIn Proceedings of the 41st Intergle monocular video. national Conference on Machine Learning, pages 59191 59209, 2024. 2 [50] Hao Zhang, Fang Li, Samyak Rawlekar, and Narendra Ahuja. Learning implicit representation for reconstructing articulated objects. In The Twelfth International Conference on Learning Representations, 2024. [51] Hao Zhang, Haolan Xu, Chun Feng, Varun Jampani, and Narendra Ahuja. Physrig: Differentiable physics-based skinning and rigging framework for realistic articulated object modeling. ICCV, 2025. [52] Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, YanPei Cao, and Shi-Min Hu. to rig them all: Diverse skeleton rigging with unirig. arXiv preprint arXiv:2504.12451, 2025. 2, 7 One model [53] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. In IEEE TPAMI, 2024. 2 [54] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 8 [55] Chongyang Zhong, Lei Hu, Zihao Zhang, and Shihong Xia. Attt2m: Text-driven human motion generation with In Proceedings of multi-perspective attention mechanism. the IEEE/CVF international conference on computer vision, pages 509519, 2023. 2 [56] Hanxin Zhu, Tianyu He, Xiqian Yu, Junliang Guo, Zhibo Chen, and Jiang Bian. Ar4d: Autoregressive 4d generation from monocular videos. arXiv preprint arXiv:2501.01722, 2025. 2 [57] Qiran Zou, Shangyuan Yuan, Shian Du, Yu Wang, Chang Liu, Yi Xu, Jie Chen, and Xiangyang Ji. Parco: Partcoordinating text-to-motion synthesis. In European Conference on Computer Vision, pages 126143. Springer, 2024. 2 [58] Silvia Zuffi, Angjoo Kanazawa, David Jacobs, and Michael Black. 3d menagerie: Modeling the 3d shape and pose of animals. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 63656373, 2017. 2 11 RigMo: Unifying Rig and Motion Learning for Generative Animation"
        },
        {
            "title": "Overview of Supplementary Materials",
            "content": "Dynamic 3D Generation Due to the strict page limit of the main manuscript, several important details, analyses, and visualizations could not be presented. This supplementary document provides comprehensive extension of our method and experiments. Specifically, we include the following additions: Comparison with prior work. RigMo-VAE. We provide side-by-side comparisons of the 48-token and 128-token configurations to highlight their differences in reconstruction quality. MotionDiT. We include detailed description of the experimental settings, training configuration, dataset preparation, and complete quantitative evaluations, accompanied by an in-depth analysis of the observed trends. Video Demonstrations. We further provide demo videos showcasing RigMos generated results (1-frame 9-frame prediction), along with side-by-side visual comparisons of reconstruction results against state-ofthe-art baselines that could not fit in the main paper. Together, these supplementary materials offer more complete view of RigMos effectiveness and support the claims made in the main manuscript. Comparison with prior work. Existing approaches related to RigMo fall broadly into two categories: dynamic 3D generation methods and riggingstructure prediction methods. Despite impressive progress, none of these paradigms jointly model rigging and motion in unified, self-supervised manner, which fundamentally limits their applicability to general deformable objects. Dynamic 3D generation. Recent 4D VAE models such as AnimateAnyMesh and GVFDiffusion learn trajectories for each vertex or Gaussian primitive. While effective for short-term sequence reconstruction, these models do not recover an explicit rig or kinematic structure, and typically require hundreds of latent tokens (often 512+) to encode motion. In contrast, RigMo uses only 48 latent tokens yet produces compact, interpretable rig together with temporally coherent motion parameters. Another line of work extends static 3D VAEs, including Hunyuan3D and Step1X3D, to per-frame mesh generation. These methods operate on independent frames and therefore suffer from slow inference, lack of temporal smoothness, inconsistent geometry across time, and, most critically, no vertex correspondence between the generated shapes. Finally, motiongeneration Method Explicit Rig Temporal Consist. Rig Annotation AnimateAnyMesh / GVFDiff. Hunyuan3D / Step1X3D Human/AnyTop Motion Gen. (GT) UniRig / Magic Articul. Rigging Prediction RigMo RigMo (Ours) none none GT rigs GT rigs none Table 4. Comparison between RigMo and representative method families. models designed for humans or articulated categories (e.g., AnyTop, many human motion diffusion/transformer models) assume access to ground-truth rigs. Such assumptions break down for general objects where no consistent rig annotations exist, and rigging conventions vary widely across datasets and artists, resulting in poor cross-category generalization. Rigging prediction. Methods such as UniRig and Magic Articulate aim to predict rig structures from static geometry. These methods depend heavily on large-scale human-annotated rig datasets; however, manual rigging contains inconsistencies, annotation noise, and large variations across artists and modeling standards. Moreover, available datasets are small, category-specific, and difficult to scale, which limits generalization to diverse objects and motion patterns. RigMo, in contrast, requires no human annotation: it infers the underlying kinematic structure directly from observed motion. This motion-driven formulation yields rigging that is physically meaningful, consistent across instances, and automatically aligned with the actual deformation behavior of the object. By unifying rigging and motion into single generative model, RigMo overcomes the core limitations of both categories. It provides interpretable rigs, consistent mesh deformations, temporally stable trajectories, and strong generalization without relying on manual labelsoffering principled and scalable solution for general-object animation."
        },
        {
            "title": "MotionDiT Evaluation",
            "content": "To more comprehensively evaluate the motion generation capability of MotionDiT, we design set of controlled experiments that minimize sampling randomness and isolate the contribution of the diffusion-based temporal prediction module. Each experiment is conditioned on two complementary signals, described below. 1 Method 11 19 Train (L1/L2) Val (L1/L2) Train (L1/L2) Val (L1/L2) w/o frame cond. 2.130.46 / 1.520.33 2.480.48 / 1.740.35 2.630.94 / 1.980.72 3.100.91 / 2.220.69 1.890.42 / 1.310.32 2.010.43 / 1.450.31 2.150.85 / 1.440.64 2.510.84 / 1.780.60 latent rig cond. full MotionDiT 1.430.41 / 0.990.30 1.770.41 / 1.230.29 1.610.83 / 1.180.61 1.860.82 / 1.370.56 Table 5. Ablation study of MotionDiT (102) under two sparse-conditioning settings. We report Chamfer Distance (L1 / L2) for both training and validation sets. The three variants include: (1) w/o frame condition (no frame-mask guidance), (2) latent rig condition (using only the rig-branch latent feature), and (3) full MotionDiT (latent rig feature + decoded skinning weights + Gaussian bone centers + framemask guidance). Experiments are done on DT4D test dataset. served frames is encoded into frame-mask sequence and fed into the second cross-attention layer. This mask pattern controls which frames are visible to the model and enables controlled interpolation and long-horizon prediction. We evaluate MotionDiT under two sparse-conditioning settings: 1. 1-frame 1-frame prediction: Given single input frame at time t, the model predicts the next frame at + 1, focusing on local temporal smoothness and shortterm motion fidelity. 2. 1-frame 9-frame prediction: Given one observed frame, the model predicts the following nine frames. This more challenging setting evaluates the ability of the model to produce temporally coherent and physically plausible long-horizon motion. These sparse-conditioning setups avoid free-form motion sampling and instead provide controlled evaluation of the temporal prediction capability of MotionDiT. They also allow clean examination of the effect of different rigconditioning strategies. We compare three variants: w/o frame condition: Removes the frame-mask signal entirely, forcing the model to infer temporal structure without any observed-frame guidance. Latent rig condition: Conditions the model only on the latent rig feature extracted from the RigMo-VAE rig encoder, without using decoded skinning weights or Gaussian bone centers. Full MotionDiT: Uses the complete rig-conditioning bundle, including the latent rig feature, decoded skinning weights, and Gaussian bone centers, providing explicit structural and physically interpretable priors for motion prediction. Quantitative results on both the training and validation splits, measured using Chamfer Distance (CL1 and CL2), are summarized in Table 5. Across both prediction horizons, the full MotionDiT model achieves strong motion fidelity and temporal coherence. Figure 6. Side-by-side skinning weights comparisons of the 48token and 128-token configurations. Rigging condition: MotionDiT receives rig-aware structural cues extracted from the RigMo-VAE rig branch. These include the latent rig feature as well as the decoded, physically meaningful rig attributes such as skinning weights and Gaussian bone centers, injected into the first cross-attention layer  (Fig. 3)  . This conditioning provides the model with articulation structure and the expected ranges of deformation. Frame condition (mask pattern): sparse set of ob-"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Nanyang Technological University",
        "Snap Inc.",
        "University of California, Santa Cruz",
        "University of Illinois Urbana-Champaign"
    ]
}