{
    "paper_title": "Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models",
    "authors": [
        "Fan Zhang",
        "Shulin Tian",
        "Ziqi Huang",
        "Yu Qiao",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of a model's capabilities by observing only a few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only a few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation."
        },
        {
            "title": "Start",
            "content": "Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models Fan Zhang1, Shulin Tian2, Ziqi Huang2, Yu Qiao1(cid:66), Ziwei Liu2(cid:66) 1Shanghai Artificial Intelligence Laboratory, 2S-Lab, Nanyang Technological University zhangfan2@pjlab.org.cn, {shulin002, ziqi002}@ntu.edu.sg https://vchitect.github.io/Evaluation-Agent-project/ 4 2 0 2 6 1 ] . [ 2 5 4 6 9 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of models capabilities by observing only few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation."
        },
        {
            "title": "Introduction",
            "content": "Visual generative models have made significant progress in recent years, particularly driven by the advancement of diffusion models (Ho et al., 2020) and the availability of internet-scale datasets (Bain et al., 2021; Chen et al., 2024b; Xue et al., 2019). These advancements enable the generation of highquality images and videos, opening up wide range of applications in content creation, design inspiration, and beyond. equal contributions. (cid:66)corresponding authors. Code is available 1 Figure 1: An Example of Evaluation Agent. Existing evaluation methods typically assess visual generative models by extensively sampling from fixed benchmark. In contrast, our Evaluation Agent framework requires only small number of sampled images or videos, tailored to the users specific evaluation request. Additionally, it goes beyond providing simple numerical score by offering detailed explanations to the evaluation conclusions. With the advancement of visual generative models, effective evaluation is crucial for understanding their strengths, limitations, and areas for improvement. Existing evaluation frameworks, such as VBench (Huang et al., 2024a,b), EvalCrafter (Liu et al., 2023), and T2I-CompBench (Huang et al., 2023), assess models across multiple dimensions using specific prompts and tailored metrics to ensure comprehensive performance analysis. However, these approaches often demand generating numerous samples, resulting in long evaluation time and high computational costs, particularly for diffusion-based models where sampling is inherently slow due to iterative sampling. Furthermore, these evaluation frameworks are constrained by rigid evaluation pipelines and predefined dimensions, making them less adaptable to open-ended inputs or diverse user needs. Additionally, these methods often produce single numerical scores as outcomes, requiring users to invest additional effort to extract meaningful insights. In contrast, human evaluators can quickly gain general understanding of models performance by interactively testing few prompts, forming sufficient impression without taking too much time. This type of evaluation has several unique advantages for assessing visual generative models. First, it is fast, requiring only small number of samples to assess overall performance. Second, it is flexible, allowing intuitive evaluation of various aspects, such as realism, creativity, prompt adherence, or other user-defined criteria. Third, it is dynamic, enabling deeper, hierarchical evaluations through continuous adjustments in exploration. To leverage the strengths of human-like evaluations, we introduce the Evaluation Agent, paradigm that mimics human strategies for assessing visual generative models. The Evaluation Agent offers four key features: 1) Efficiency: It dynamically adjusts its evaluation pathway based on intermediate results, uncovering subtle model behaviors and limitations while avoiding redundant test cases for efficient evaluation. 2) Promptable Evaluation: Unlike existing benchmarks with fixed prompts and evaluation metrics, it accepts open-ended user input, allowing for flexible and customized assessments tailored to specific user needs. 3) Detailed and Interpretable Results: It provides interpretable, detailed insights beyond single numerical scores, making results accessible to both experts and non-experts. 4) Scalability: The framework supports seamless integration of new metrics and evaluation tools, ensuring adaptability and growth. The Evaluation Agent begins by accepting openended user input, specifying what to evaluate and which model(s) to assess. Based on this input, it identifies initial evaluation aspects and leverages appropriate tools to conduct the assessment. It then observes the intermediate results and dynamically refines the direction of further exploration. In the end, it generates detailed natural language response summarizing the evaluation results, providing comprehensive analysis of the evaluation process and clear summary of the models capabilities as specified in the user input. The Evaluation Agent can also automate various applications, including: 1) Model Comparison: Allowing users to compare models based on specific criteria to determine which performs better in given aspect. 2) Model Recommendation: Suggesting the most suitable model for the users needs by evaluating models against personalized criteria. We demonstrate the versatility of the Evaluation Agent through experiments on diverse scenarios, including the evaluation of image and video generation models. The results indicate that it delivers performance comparable to full benchmark pipelines while significantly reducing evaluation time. Furthermore, we create an open-ended user query dataset to showcase the Evaluation Agents flexibility, depth, and accuracy in addressing openended queries. We summarize our contributions as follows: We propose the Evaluation Agent, humanlike evaluation framework that overcomes the limitations of existing methods in flexibility and efficiency. Our approach will be fully open-sourced. We validated our approach on several widely adopted benchmarks, demonstrating that it achieves evaluation accuracy comparable to standard benchmarks while reducing evaluation time by over 90%. We also built an open-ended user query dataset to demonstrate our methods flexibility, depth, and accuracy in handling open-ended evaluation queries. Through comprehensive analysis of how standard benchmarks, human evaluators, and our Evaluation Agent perform evaluations, we provide in-depth insights that serve as important cornerstones for future research in evaluating visual generative models."
        },
        {
            "title": "2.1 Visual Generation and Evaluation",
            "content": "Visual generative models have gained significant attention in recent years. However, unlike perception tasks, which have clear evaluation metrics such as accuracy, evaluating visual generative tasks is more challenging due to the absence of definitive ground truth or single correct answer. Metrics such as FID (Heusel et al., 2017) and FVD (Unterthiner et al., 2018) are commonly used to measure the distance between generated samples and reference datasets. Recent benchmarks, including T2I-CompBench (Huang et al., 2023), VBench (Huang et al., 2024a), and EvalCrafter (Liu et al., 2023), provide multi-dimensional evaluations 2 Benchmark Analysis Customized Queries Supported Models # Required Samples Open Evaluation Request Support Dynamic Evaluation Open Tool-Use FID / FVD (Unterthiner et al., 2018; Heusel et al., 2017) T2I-CompBench (Huang et al., 2023) VBench (Huang et al., 2024a) Evaluation Agent (Ours) T2I / T2V T2I T2V T2I & T2V 2,048 18,000 4,730 400 (Fixed-Form) (Pre-Defined) (Pre-Defined) (Open-Ended) Table 1: Comparison of the Evaluation Agent Framework with Traditional T2I and T2V Benchmarks. The Evaluation Agent framework supports customized user queries in natural language and works with both T2I and T2V models. Unlike traditional benchmarks, it dynamically updates the evaluation process using multiple tools, providing comprehensive and explainable results with detailed textual analysis. tailored to specific model capabilities. However, whether relying on metrics like FID (Heusel et al., 2017) and FVD (Unterthiner et al., 2018) or benchmarks like VBench (Huang et al., 2024a), these evaluations are often time-intensive and limited in scope. 2.2 LLM as Judge Recently, the development of understanding and reasoning capabilities in Large Language Models (LLMs) demonstrates significant advantage, enabling them to serve as powerful evaluators (Jain et al., 2023; Chiang and yi Lee, 2023; Fu et al., 2023; Zheng et al., 2023). For instance, CoEval (Li et al., 2023b) introduces two-stage evaluation framework for open-ended natural language generation (NLG) tasks, offering scalable and costefficient alternative to human evaluations. (Pan et al., 2024) demonstrates how LLM-based evaluations enhance downstream tasks for digital agents. These studies showcase the ability of LLMs to reason, explain, and comprehend evaluation processes. Despite these advancements, prior work primarily focuses on improving general reasoning or minimizing hallucination, leaving the use of LLMs for evaluating visual generative models largely unexplored. Furthermore, though LLMs demonstrate considerable proficiency in zero-shot reasoning and planning, devising effective strategies for domainspecific problems remains challenging (Wang et al., 2024b), complicating their role as evaluators for visual generative tasks."
        },
        {
            "title": "2.3 Agent in Planning & Reasoning",
            "content": "Agents and agentic systems are gaining attention for their ability to automate complex tasks and design customized trajectories based on user queries. They have been explored across various domains, including web, mobile, desktop, and operating systems (OS) (Zhou et al., 2023; Xie et al., 2024; Wang et al., 2024a; Kapoor et al., 2024; Zhang et al., 2024b), showing effectiveness in improving long-horizon task completion. For example, Chain-of-Thought (CoT) (Wei et al., 2022) and Zero-shot-CoT (Kojima et al., 2022) use prompting techniques to enable step-by-step reasoning. Similarly, ReAct (Yao et al., 2022) introduces general paradigm for agent prompting by integrating reasoning traces with task-specific actions through interleaved triplets of thought-action-observation, thereby incorporating environmental feedback. Despite the broad applicability of agents in task automation, their potential to automate the evaluation process of visual generative models remains largely unexplored."
        },
        {
            "title": "Generative Models",
            "content": "C = {cjj Evaluation Benchmark. {1, 2, 3, ...., }}, where the conditions (i.e., test cases) is set of text prompts, input images, class labels, or conditionals in other formats. In the case of unconditional generation, is empty or consists of random seeds for unconditional generation. In existing evaluation approaches, is pre-defined and usually contains at least hundreds or thousands of items, which require intensive computation and time cost for sampling. In our Evaluation Agent framework, the test case set is dynamically determined during the evaluation process, and usually only contains few cases towards the end. Sampling. vj = G(cj) where is the visual generative model, which generates the visual output vj (i.e., images or videos) given an optional condition cj. = G(C) where = {vjj {1, 2, 3, ...., }} represents the set of generated visuals for the entire condition set C. Evaluation Pipeline. Existing evaluation methods usually follow fixed pipeline to evaluate all the images or videos sampled from the pre-defined benchmark C. yj = ek(vj, cj) (1) where ek is an evaluation function of some aspects such as aesthetic, compositionality etc.. 3 Figure 2: Overview of Evaluation Agent Framework. This framework leverages LLM-powered agents for efficient and flexible visual model assessments. As shown, it consists of two stages: (a) the Proposal Stage, where user queries are decomposed into sub-aspects, and prompts are generated, and (b) the Execution Stage, where visual content is generated and evaluated using an Evaluation Toolkit. The two stages interact iteratively to dynamically assess models based on user queries. Given the generated visual vj, and the optional condition cj, it produces the evaluation result yj. evaluation reference/statistics-based frameworks like FID and FVD also use reference datasets Vr for calculating the results. Some = E(V, Vr, C) (2) In existing evaluation approaches, is predefined set of evaluation dimensions, or single evaluation metric, which limits the possible evaluation aspects from the beginning, and requires assessing all the aspects even if some are not needed in some cases. In our Evaluation Agent framework, the evaluation tool ek is dynamically determined during the evaluation process."
        },
        {
            "title": "3.2 The Evaluation Agent Framework",
            "content": "Our Evaluation Agent framework is powered by LLM-based agents, leveraging their advanced planning capabilities to simulate human-like behaviors for efficient and flexible visual model assessments. As illustrated in Figure 2, the framework operates in two stages: the proposal stage and the execution stage. By iteratively interacting and looping between these stages, the framework dynamically evaluates models based on user queries."
        },
        {
            "title": "3.2.1 Proposal Stage\nthe\nThe Proposal Stage consists of two agents:\nPlan Agent and the PromptGen Agent. The Plan\nAgent is responsible for planning, observing, and\nsummarizing the evaluation process based on the\nuser’s query, while the PromptGen Agent focuses\nspecifically on the design aspects.\nPlan Agent. We design the Plan Agent to simu-\nlate human behavior during the evaluation process,",
            "content": "including planning and adjusting the evaluation direction, observing intermediate results, and summarizing the final outcomes. As the core component of the framework, the Plan Agent not only interacts with the user but also drives the entire evaluation process. Specifically, upon receiving user query, the Plan Agent identifies an initial sub-aspect for evaluation and iteratively refines it based on feedback from intermediate results. This process continues until sufficient information is collected, after which the agent provides detailed analysis and summary. Additional details can be found in Appendix A. PromptGen Agent. The PromptGen Agent mimics human behavior in designing prompts for visual generative models based on the plan developed during the evaluation process. Specifically, it generates tailored prompts for each sub-aspect identified by the Plan Agent, enabling focused content generation and evaluation. Additionally, the PromptGen Agent can reference and utilize well-established prompts from existing benchmarks. Further details are provided in Appendix A."
        },
        {
            "title": "3.2.2 Execution Stage",
            "content": "The Execution Stage is responsible for sampling and evaluating the model using the appropriate tools, as specified in the Proposal Stage, and for returning the final evaluation results. Visual Generative Models. This component takes prompts designed by the PromptGen Agent as input and generates corresponding visual content, which is then used for subsequent evaluation. Evaluation Toolkit. The Evaluation Toolkit consists of set of elementary evaluation tools for visual generative models. This module is open and extensible, allowing for continuous expansion. We 4 Table 2: Evaluation Results Comparison with VBench (Huang et al., 2024a). We evaluated 15 specific ability dimensions in VBench using our Evaluation Agent and compared its results against VBench in terms of conclusion accuracy. The numerical results show the percentages of the Evaluation Agents correct predictions falling either within the exact range (left) or within an error margin of one range (right) across ten trials. Models Latte-1 (Ma et al., 2024) ModelScope (Wang et al., 2023) VideoCrafter-0.9 (He et al., 2022) VideoCrafter-2 (Chen et al., 2024a) Multiple Objects 40% / 100% 50% / 100% 80% / 100% 20% / 60% Subject Consistency 50% / 80% 80% / 80% Background Consistency 0% / 30% 80% / 90% 100% / 100% 80% / 100% 60% / 100% 10% / 100% Human Action 10% / 10% 10% / 40% 10% / 30% 10% / 90% Color 30% / 70% 0% / 20% 10% / 40% 90% / 100% Motion Smoothness 40% / 70% 60% / 80% 70% / 100% 30% / 90% Spatial Relationship 10% / 80% 10% / 30% 20% / 100% 0% / 70% Dynamic Degree 30% / 70% 60% / 100% 80% / 100% 30% / 80% Scene 20% / 40% 20% / 100% 30% / 100% 0% / 10% Imaging Aesthetic Quality Quality 60% / 100% 70% / 100% 60% / 100% 100% / 100% 20% / 100% 90% / 100% 50% / 100% 80% / 100% Appearance Style 70% / 90% 90% / 100% 60% / 100% 80% / 100% Temporal Style 40% / 100% 50% / 90% 80% / 100% 80% / 100% Object Class 40% / 50% 0% / 50% 20% / 60% 70% / 100% Overall Consistency 70% / 100% 20% / 100% 0% / 80% 60% / 100% Table 3: Evaluation Results Comparison with T2I-CompBench (Huang et al., 2023). We evaluated four ability dimensions in T2I-CompBench using our Evaluation Agent and compared its results with those of T2I-CompBench in terms of conclusion accuracy. The numerical results show the percentages of the Evaluation Agents correct predictions falling either within the exact range (left) or within an error margin of one range (right) across ten trials. Models Color Binding Non-Spatial Relationships SD1.4 (Rombach et al., 2022) 50% / 100% 100% / 100% 0% / 100% 50% / 100% SD2.1 (Rombach et al., 2022) 100% / 100% 60% / 100% 80% / 100% 60% / 100% 100% / 100% 20% / 100% 80% / 100% 60% / 100% 80% / 90% 20% / 90% SDXL (Podell et al., 2023) SD3.0 (Esser et al., 2024) Shape Binding Texture Binding 0% / 70% 0% / 90% have integrated several existing evaluation tools from well-known benchmarks for different modalities of visual generation models. To support the evaluation of open-ended user queries, we have introduced paradigm based on vision-language models (VLMs), leveraging the Visual Question Answering (VQA) format to enable flexible assessments across various aspects of the models. Upon receiving the generated samples from visual generative models along with the corresponding prompts, the module utilizes the appropriate tools to evaluate each sample. All evaluation results are then compiled and returned to the Plan Agent for further proposals or summarization. For detailed information, please refer to the Appendix A."
        },
        {
            "title": "3.2.3 Overall Pipeline",
            "content": "The Evaluation Agents process is dynamic and multi-round, with each round comprising proposal stage and an execution stage. By interacting and looping through these stages, we achieve dynamic evaluation, where the evaluation process adapts based on intermediate observations and initial user query. This dynamic approach allows the Evaluation Agent to refine its focus iteratively, adjusting its exploration direction and prompt design based on an evolving understanding of the models capabilities. Consequently, the evaluation process becomes more efficient and targeted, systematically identifying the strengths and limitations of generative models."
        },
        {
            "title": "4 Experiments",
            "content": "We first validate the efficiency of our Evaluation Agent on established benchmarks for visual generative models and then demonstrate the flexibility, depth, and accuracy of our approach in handling open-ended user queries on our self-constructed dataset."
        },
        {
            "title": "4.1 Experiments on Existing Benchmarks",
            "content": "We validate the effectiveness of our framework on both the Text-to-Video (T2V) and Text-to-Image (T2I) tasks. For detailed settings and implementations, please refer to Appendix B."
        },
        {
            "title": "4.1.1 Experimental Setup\nVisual Generative Models. For the T2V task,\nwe select four open-source models: VideoCrafter-\n0.9 (He et al., 2022), VideoCrafter-2 (Chen et al.,\n2024a), Latte-1 (Ma et al., 2024), and Mod-\nelScope (Wang et al., 2023).\nSimilarly, for\nthe T2I task, we choose four well-known open-\nsource models: SD(Stable Diffusion)1.4 (Rom-\nbach et al., 2022), SD2.1 (Rombach et al., 2022),\nSDXL (Podell et al., 2023), and SD3.0 (Esser et al.,",
            "content": "5 Table 4: Time Cost Comparison across Models for VBench Dimensions. This table compares the evaluation time of four different models using the original VBench pipelines versus the Evaluation Agent. The Evaluation Agent significantly reduces the overall evaluation time. VBench (Total Cost) VBench (Avg. Cost per Dimension) Evaluation Agent (Ours) Models 2557 min, 4355 samples Latte-1 (Ma et al., 2024) 1160 min, 4355 samples ModelScope (Wang et al., 2023) VideoCrafter-0.9 (He et al., 2022) 1459 min, 4355 samples VideoCrafter-2 (Chen et al., 2024a) 4261 min, 4355 samples 170 min, 290 samples 77 min, 290 samples 97 min, 290 samples 284 min, 290 samples 15 min, 25 samples 6 min, 23 samples 9 min, 24 samples 24 min, 23 samples Table 5: Time Cost Comparison across Models for T2I-CompBench Dimensions. This table compares the evaluation costs for assessing four models across T2I-CompBench dimensions using both the original T2I-CompBench pipelines and our Evaluation Agent. The Evaluation Agent achieves substantial reduction in evaluation time compared to the traditional pipelines. Models SD1.4 (Rombach et al., 2022) SD2.1 (Rombach et al., 2022) SDXL (Podell et al., 2023) SD3.0 (Esser et al., 2024) T2I-Comp (Total Cost) T2I-Comp (Avg. Cost per Dimension) Evaluation Agent (Ours) 563 min, 12000 samples 782 min, 12000 samples 1543 min, 12000 samples 1410 min, 12000 samples 141 min, 3000 samples 196 min, 3000 samples 386 min, 3000 samples 353 min, 3000 samples 5 min, 26 samples 6 min, 26 samples 8 min, 26 samples 7 min, 25 samples 2024). We validate the efficiency and accuracy of our method using these models within the evaluation frameworks for both T2I and T2V tasks. Visual Generation Benchmarks. For the T2V and T2I tasks, we adopt well-established and comprehensive evaluation frameworks tailored to their respective domains. Specifically, for the T2V task, we use VBench (Huang et al., 2024a), comprehensive and fine-grained evaluation framework for video generation. For the T2I task, we select T2ICompBench (Huang et al., 2023), which assesses the compositional generation capabilities of T2I models across multiple dimensions. We validate the effectiveness of our method using these evaluation frameworks. Comparison Setup. To ensure comparability and fairness in our experiments, we restrict the evaluation tools and prompts available to the Evaluation Agent. For T2V tasks, we used only the metrics and prompt lists from VBench, and for T2I tasks, only those from T2I-CompBench. For comparison, we categorize performance into five levels based on the performance density distribution. For more details, see Appendix B."
        },
        {
            "title": "4.1.2 Results Analysis\nValidation on VBench. To highlight the efficiency\nof our proposed methods, we compare the time\nconsumption and sample counts between VBench\nand our approach. As shown in Table 4, our method\nreduces evaluation time by over 10X. Additionally,\nTable 2 confirms the consistency of our evaluation\nresults with VBench across various dimensions.\nThe quantitative results show that the Evaluation\nAgent achieves high prediction accuracy across",
            "content": "Figure 3: Validation on VBench Percentage Dimensions. We conducted additional validation experiments on VBench by increasing the number of prompts in each evaluation. For each model and dimension, lighter bars represent results with the original settings, darker bars with increased sample size. Hatched portions indicate predictions within the exact range, and solid portions within an error margin of one range. Specific numerical results are provided in Table 6 most dimensions, demonstrating that our approach maintains accuracy while significantly reducing evaluation time. Validation on T2I-CompBench. We evaluate the Evaluation Agents performance on T2I tasks, as shown in Table 5. Instead of requiring thousands of samples and several hours, the Evaluation Agent completes evaluations with just about 26 samples in five to eight minutes per dimension. Table 3 compares the evaluation results, showcasing high accuracy within an error margin of one range. Analysis of Percentage-Based Dimensions. In the validation experiments on VBench, we observe 6 Table 6: Validation on VBench Percentage Dimensions. The numerical results show the percentages of the Evaluation Agents correct predictions falling either within the exact range (left) or within an error margin of one range (right) across ten trials. Models Human Action Scene Color Object Class Latte-1 (default) (Ma et al., 2024) Latte-1 (30 prompts) (Ma et al., 2024) ModelScope (default) (Wang et al., 2023) 10% / 10% 20% / 40% 30% / 70% 40% / 50% 10% / 60% 30% / 50% 30% / 70% 40% / 80% 10% / 40% 20% / 100% 0% / 20% 0% / 50% ModelScope (30 prompts) (Wang et al., 2023) 30% / 50% 30% / 100% 10% / 30% 10% / 60% 4.2 Experiments on Open-Ended User Query We demonstrate the flexibility of our framework and the benefits of its dynamic evaluation through experiments on an open-ended user query dataset that we collect and construct. 4.2.1 Open-Ended User Query Dataset We create an open-ended user query dataset comprising 100 user queries focused on evaluating generative model capabilities. Each query is manually labeled with Ability, General/Specific, and Specific Domain tags. For detailed statistics, please refer to the Appendix C."
        },
        {
            "title": "4.2.2 Experimental Setup",
            "content": "The Evaluation Agent demonstrates strong planning capabilities and accepts any input, but its effectiveness is limited by restrictive evaluation tools, which hinder its ability to handle open-ended queries. To overcome this limitation, we propose simple yet intuitive solution: leveraging VLM as an evaluation tool in the form of VQA. During each evaluation round, the PromptGen Agent not only designs prompts for specific sub-aspects but also generates corresponding questions based on the content of each prompt and the aspects to be evaluated. The generated sample and questions are input into the VLM, which provides answers that are then fed back to the Plan Agent for further analysis and planning."
        },
        {
            "title": "4.2.3 Open-Ended User Query Evaluation",
            "content": "Most visual generation benchmarks use predefined dimensions and prompts to evaluate models, but this fixed approach often overlooks users specific needs, such as handling unique scenarios or objects. user study found that 67.44% (29 of 43) participants prioritized models meeting their specific needs over general performance. Additionally, fixed prompts can lead to targeted optimization, resulting in misleading evaluations. We address these issues using the Evaluation Agent, which conducts dynamic, multi-round asFigure 4: Case of Open-Ended User Query Evaluation. For open-ended user queries, the Evaluation Agent systematically explores the models capabilities in specific areas, starting from basic aspects and gradually delving deeper, culminating in detailed analysis and summary. Please refer to the Appendix E.2 for the complete results. lower evaluation results in dimensions like Human Action, Scene, Color, and Object Class. Further analysis reveals that these dimensions rely on statistical evaluations, where sample-level results are binary (0 or 1), making them highly sensitive to sample size. Since the Evaluation Agent limits samples per round to three to nine, we test this sensitivity by increasing the sample size to 30 using the ModelScope (Wang et al., 2023) and Latte1 (Ma et al., 2024) models. The results, shown in Figure 3, indicate that as the sample size increases, the performance of both models in each dimension steadily improves. More Results. The experiments described above were conducted using the GPT model as the backbone. To further validate the effectiveness and versatility of our framework, we extended this set of experiments to other models, including Claude and Gemini. Detailed results and analyses can be found in Appendix E.1. sessments of models capabilities for open-ended queries, with flexible prompt design at each stage. Figure 4 provides an example where user asks: Can the model generate variations of existing artwork while maintaining the original style? The agent begins by evaluating the models ability to replicate basic artistic styles, confirming its comIt then narrows its focus to determine petence. whether the model can preserve the original style while modifying specific artwork details. Through iterative evaluations, the agent analyzes and synthesizes the results to provide comprehensive feedback. For the full example, please refer to Appendix E.2. As shown, our Evaluation Agent framework provides precise, detailed, and user-focused evaluations for open-ended queries."
        },
        {
            "title": "5 Further Discussions",
            "content": "In this section, we discuss the unique aspects of the Evaluation Agent compared to traditional benchmarks, as well as its potential broader applications. Dynamic and Multi-Step Evaluation. One of the core features of the Evaluation Agent is its dynamic, multi-step evaluation process. This structured evaluation paradigm enables the discovery of nuanced differences in model capabilities and provides more detailed analysis of models strengths and weaknesses. Specifically, it allows for hierarchical, step-by-step evaluation, progressing from simple to complex tasks, as well as category-based assessments that measure performance across different content types within the same dimension. In contrast, traditional visual generation evaluation benchmarks, while incorporating diverse prompts carefully designed for each dimension, suffer from limitations such as fixed prompts and lack of fine-grained prompt categorization. These constraints reduce flexibility, make it harder to draw valuable insights, and increase the risk of models being over-optimized for specific prompts. Furthermore, dynamic evaluations help avoid redundant testing, significantly enhancing efficiency. Open-Ended Evaluation Toolkit. Our framework for evaluating open-ended queries relies on two key aspects: the planning and reasoning capabilities of LLM Agents and the evaluation toolkits ability to assess diverse dimensions. One approach to building the evaluation toolkit is integrating various tools, allowing the agent to select the most suitable one for each task. However, current evaluation tools are limited, often focusing only on general evaluations and lacking sensitivity to finegrained details. For instance, CLIPScore (Hessel et al., 2021) captures the general similarity between an image and caption but fails to detect subtle changes, such as variations in object counts, limiting its effectiveness in specific evaluations. An alternative approach is to design versatile evaluation tool capable of assessing multiple aspects. VQA-based format using VLMs is particularly promising, enabling fine-grained evaluation through targeted questions and providing detailed textual outputs that integrate well with LLM Agents. While this methods effectiveness depends on the VLMs capabilities, current VLMs already demonstrate impressive results, with future advancements poised to further enhance performance. Broader Applications. The Evaluation Agent not only evaluates the performance of single model but also facilitates the direct comparison of two models strengths and weaknesses in specific dimensions by assessing them simultaneously during execution. This feature enables users to determine which model excels in particular areas. Moreover, by accumulating evaluation results across various capabilities, database can be constructed. Once sufficient information about multiple models is collected, this database can serve as the foundation for building recommendation system, capable of suggesting the most suitable model based on the users specific needs."
        },
        {
            "title": "6 Conclusion",
            "content": "As the first of its kind, our Evaluation Agent redefines how visual generative models can be assessed, moving beyond rigid evaluation pipelines to offer an efficient and promptable approach. Unlike traditional methods that rely on fixed benchmarks and time-consuming sampling processes, the Evaluation Agent mimics human evaluation strategies. This allows for significant reduction in evaluation time while providing flexibility and adaptability to user-specified criteria. Our framework dynamically adjusts the evaluation process, enabling efficient assessments with fewer samples and offering scalable, customizable integration for wide range of evaluation tools and visual generative models. By open-sourcing this framework, we aim to inspire further research in the development of more flexible and efficient evaluation methods for visual generative models."
        },
        {
            "title": "7 Limitations",
            "content": "The performance of our Evaluation Agent framework is influenced by two orthogonal factors: 1) the reliability of the Evaluation Toolkit, and 2) the capability of the LLMs used to develop the agentic systems. Evaluation Toolkit. While we can integrate stateof-the-art (SOTA) toolkits, they may not always perfectly align with human perception, particularly when evaluating visual generative models. This misalignment can negatively impact the accuracy of evaluation results when the Evaluation Agent relies on these tools. Furthermore, although the Evaluation Agent is designed as an open framework capable of handling arbitrary open-form user queries, existing evaluation tools are still limited in covering certain edge cases or specific criteria that users may want to assess. LLMs. We found that even the most advanced LLMs occasionally fall short, such as producing inconsistent output formats or struggling with numerical comparisons. Possible solutions include employing post-processing techniques to refine the outputs and using external tools to handle numerical evaluations. With the release of more powerful models, such as o1, we believe these issues can be largely mitigated. Our primary contribution is introducing new evaluation paradigm. We are confident that the utility of the Evaluation Agent framework will continue to improve as stronger LLMs and more human-aligned evaluation toolkits are developed."
        },
        {
            "title": "8 Ethical Considerations",
            "content": "The Evaluation Agent could potentially be used to prompt visual generative models to synthesize unsafe or harmful visual content, such as deepfakes or offensive images and videos. This raises ethical concerns regarding the misuse of these visual generative models. We strongly advise users to approach any system involving visual generative models with caution, as improper use could lead to the creation and spread of harmful content."
        },
        {
            "title": "References",
            "content": "Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. 2021. Frozen in time: joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. 2024. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1768217690. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. 2024a. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047. Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, MingHsuan Yang, and Sergey Tulyakov. 2024b. Panda70m: Captioning 70m videos with multiple crossmodality teachers. arXiv preprint arXiv:2402.19479. Cheng-Han Chiang and Hung yi Lee. 2023. Can large language models be an alternative to human evaluations? Preprint, arXiv:2305.01937. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for highIn Forty-first Internaresolution image synthesis. tional Conference on Machine Learning. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. Preprint, arXiv:2302.04166. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. 2022. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: referencefree evaluation metric for image captioning. arXiv preprint arXiv:2104.08718. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. arXiv Denoising diffusion probabilistic models. preprint arxiv:2006.11239. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. 2024a. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. 2024b. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503. Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. 2023. Multi-dimensional evaluation of text summarization with in-context learning. In Findings of the Association for Computational Linguistics: ACL 2023, page 84878495. Association for Computational Linguistics. Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. 2024. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. arXiv preprint arXiv:2402.17553. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023a. Api-bank: comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244. Qintong Li, Leyang Cui, Lingpeng Kong, and Wei Bi. 2023b. Collaborative evaluation: Exploring the synergy of large language models and humans for open-ended generation evaluation. Preprint, arXiv:2310.19740. Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. 2023. Evalcrafter: Benchmarking and evaluating large video generation models. arXiv preprint arXiv:2310.11440. Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. 2024. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048. Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. 2023. T2i-compbench: comprehensive benchmark for open-world compositional text-toimage generation. arXiv preprint arXiv:2307.06350. Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. arXiv preprint arXiv:2308.00436. 10 Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. 2024. Autonomous evaluation and refinement of digital agents. Preprint, arXiv:2404.06474. Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis. Preprint, arXiv:2305.15334. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion modIn Proceedings of the IEEE/CVF conference els. on computer vision and pattern recognition, pages 1068410695. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36. Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, and Ming Jin. 2023. Algorithm of thoughts: Enhancing exploration of ideas in large language models. arXiv preprint arXiv:2308.10379. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2024. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. 2018. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. 2023. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571. Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. 2024a. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024b. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: survey. arXiv preprint arXiv:2309.07864. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. 2024. Osworld: Benchmarking multimodal agents for openended tasks in real computer environments. arXiv preprint arXiv:2404.07972. Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William Freeman. 2019. Video enhancement International Journal of with task-oriented flow. Computer Vision (IJCV), 127(8):11061125. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629. Yifan Zhang, Yang Yuan, and Andrew Chi-Chih Yao. 2024a. On the diagram of thought. arXiv preprint arXiv:2409.10038. Ziniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu. 2024b. Mmina: Benchmarking multihop multimodal internet agents. arXiv preprint arXiv:2404.09992. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. 11 Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. 2023. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854."
        },
        {
            "title": "Supplementary",
            "content": "In this supplementary file, we provide detailed explanation of the pipeline in Section A. Next, we present additional experimental details in Section and elaborate on the open-ended user query dataset in Section C. Furthermore, we discuss additional related work in Section D. Finally, in Section E, we present further experimental results and analyses using different base models, along with variety of comprehensive evaluation results for open-ended user queries."
        },
        {
            "title": "A Detailed Explanation of Pipeline",
            "content": "Our dynamic evaluation pipeline consists of two stages: the Proposal Stage and the Execution Stage. By iteratively interacting and looping between these stages, the framework dynamically evaluates models in response to user queries. A.1 Proposal Stage The Proposal Stage consists of two agents: the Plan Agent and the PromptGen Agent. The Plan Agent is responsible for planning each step and providing the final summary and analysis, while the PromptGen Agent specializes in designing prompts for the process. Plan Agent. When humans are interested in specific aspect of models capabilities, they often generate content to observe its performance. Through several rounds of iterative generation and observation, they can form preliminary evaluation of the models ability in that aspect. During this process, before each round of generation, humans typically consider which direction to focus on. We designed the Plan Agent to simulate this decision-making behavior. The Plan Agent primarily simulates the decisionmaking process in human evaluations. It is responsible for determining the direction of exploration at each step and for summarizing and analyzing the results. Specifically, at the beginning, the Plan Agent receives user query related to the models capabilities. We require the Plan Agent to first propose an initial aspect to explore based on this query. In each subsequent step, it needs to consider both the users original query and the observations from intermediate results to suggest further directions for exploration, until sufficient information is gathered to evaluate the models capability in relation to the query. When the Plan Agent believes it has gathered sufficient information, it will analyze and summarize all the observed results from the exploration process, and then return the final evaluation results to the human. In this process, we require the Plan Agent to provide an explanation (thought) for why it chooses particular direction to explore at each step. When it chooses to respond to the human, it is also required to explain why it believes sufficient information has been gathered to stop further exploration. For tool usage, in the case of closed-domain queries that involve experiments on existing benchmarks, the Plan Agent is required to not only propose the direction to explore at each step but also specify the appropriate evaluation tool for that direction. For open-ended queries, we employ Vision-Language Model (VLM) for evaluation, using the Visual Question Answering (VQA) format. PromptGen Agent. Once the human determines the direction to focus on, corresponding prompts must be designed based on that focus for model sampling and generation. To simulate this behavior, we developed the PromptGen Agent. Specifically, at each step, the PromptGen Agent receives the aspect to explore proposed by the Plan Agent and designs prompts aligned with the specified exploration direction. Additionally, while crafting each prompt, the PromptGen Agent provides an explanation of the reasoning behind its design. A.2 Execuation Stage The Execution Stage is responsible for sampling and evaluating the visual generation model based on the evaluation tools selected by the Plan Agent and the prompts designed by the PromptGen Agent. Sampling and Evaluation. For the designed prompts, we use visual generation model to sample and generate corresponding images or videos, and then we need to evaluate the quality of the generated content. For humans, the evaluation process is essentially an observation of the generated content. In our pipeline, for closed-domain questions, we evaluate specific dimensions by invoking existing evaluation tools within the closed domain. For open-ended queries, we use VLM to simulate human evaluation of the generated content in the form of VQA. Finally, we integrate the evaluation results of each generated piece and return them to the Plan Agent for observation and analysis. A.3 Dynamic Looping. Dynamic evaluation refers to initially providing preliminary focus based on the users query, and then continuously adjusting what aspects to focus on according to the intermediate evaluation results. This can involve adjustments in terms of wider variety of scenarios, more complex situations, or more intricate prompts, among other factors, until the Plan Agent believe it have gathered enough information to answer the users original query."
        },
        {
            "title": "B Experiment Implementation Details",
            "content": "All experiments in the main text were implemented using LLMs as the backbone, with gpt-4o-2024-08-06 as the core model set to temperature of 0.7. The system prompt design was inspired by the CoT (Wei et al., 2022) and ReAct (Yao et al., 2022) frameworks, guiding the agent to solve problems step-by-step and provide explanations at each stage. For T2V tasks, we validate our evaluation approach by comparing the consistency of our Evaluation Agent with VBenchs original evaluation scheme across multiple dimensions, with respect to evaluation time, sample count, and final assessment results on four open-source T2V models. For VBench, we selected 15 evaluation dimensions, including Subject Consistency, Background Consistency, and Motion Smoothness, among others. Details could be referred to in Table 2. We used VBenchs original scheme to sample and evaluate four models on these dimensions, recording generation time, evaluation time, and results. We categorized 5 tiers based on the sample-level results from VBench: Very High, High, Moderate, Low, and Very Low, based on the scores density distributions. For dimensions scored by proportions, like Dynamic Degree, we used scores from 42 models on VBenchs leaderboard for tiering. To ensure fairness, we restricted the evaluation tools and prompts to those in VBenchs existing metrics. Our Evaluation Agent assessed each dimension by answering targeted questions, invoking relevant tools, and selecting suitable prompts, guided by detailed definitions and score-tiering information to interpret results and provide an overall performance assessment for each model. For T2I tasks, we conducted the experiment following the similar settings used for T2V modFor T2I-CompBench, we selected 4 diels. mensions for the experiment, which are: Color Binding, Shape Binding, Texture Binding, Non-Spatial Relationships. Using T2ICompBenchs original evaluation scheme and prompt list, we evaluated the 4 models and categorized scores into 5 tiers, as in the T2V experiment. We excluded the Spatial Relationships dimension due to its reliance on statistical values and limited sample distribution, along with the absence of leaderboard-based references for tiering. The Evaluation Agent was restricted to using the evaluation tools and prompts for these four dimensions and was provided with detailed definitions and tiered results for accurate assessment. Open-Ended User Query Dataset C.1 Building an Open-Ended User Query Dataset. To create dataset of user queries focused on evaluating generative model capabilities, we conducted user study, gathering user queries from various sources about the aspects users find most important when assessing new models. After cleaning, filtering, and expanding the initial collection, we compiled dataset of 100 open-ended user queries. C.2 Dataset Statistics. We manually annotated each query with labels for Ability, General/Specific, and Specific Domain for analysis. The Ability label categorizes the model capabilities targeted by the question into five types: Prompt Following, Visual Quality, Creativity, Knowledge, and Others. For the General/Specific label, high-level questions like \"How well can it visualize my idea from my words?\" are classified as General, while questions focused on specific applications, such as \"How well can the model generate game characters with intricate details, like armor or facial expressions?\" are labeled as Specific Domain. The Specific Domain label further identifies the focus area for these questions, covering fields like Law, Film and Entertainment, Fashion, Game Design, Architecture and Interior Design, Medical, Science and Education, and History and Culture. We visualize the statistical distribution of the dataset across these categories in Figure 5."
        },
        {
            "title": "D More Related Work",
            "content": "D.1 Agents Planning & Reasoning Methods The Agent is designed to match human intelligence in decision-making and reasoning, leveraging the core capabilities of LLMs. Several design 14 Figure 5: Data Distribution of Open-Ended User Query Dataset. We analyze the constructed open-ended user query dataset from three aspects: General/Specific, Ability, and Specific Domain. The results indicate that our dataset exhibits relatively balanced distribution across these dimensions. Table 7: Evaluation Results Comparison with VBench (Huang et al., 2024a) using Claude as Base Model. We adhere to the same experimental settings and parameters as in the main experiments, but we replace the planning and reasoning agents backbones with claude-3-5-sonnet-20241022 as the base model. Models Latte-1 (Ma et al., 2024) ModelScope (Wang et al., 2023) VideoCrafter-0.9 (He et al., 2022) VideoCrafter-2 (Chen et al., 2024a) Multiple Objects 10% / 60% 90% / 100% 0% / 40% 50% / 100% Subject Consistency 0% / 10% 0% / 10% 40% / 100% 50% / 100% Human Action 60% / 70% 40% / 90% 20% / 40% 50% / 80% Background Consistency 0% / 10% 30% / 40% 30% / 80% 0% / 100% Color 10% / 60% 10% / 20% 10% / 40% 60% / 90% Motion Smoothness 0% / 30% 10% / 80% 40% / 90% 0% / 10% Spatial Relationship 30% / 80% 50% / 80% 40% / 100% 50% / 100% Imaging Quality Aesthetic Quality Dynamic Degree 100% / 100% 90% / 100% 0% / 40% 60% / 100% 40% / 100% 30% / 100% 90% / 100% 20% / 100% 90% / 100% 60% / 100% 100% / 100% 80% / 100% Scene 0% / 40% 40% / 100% 10% / 80% 0% / 50% Temporal Appearance Style Style 80% / 100% 30% / 100% 90% / 100% 70% / 100% 100% / 100% 90% / 100% 80% / 100% 10% / 100% Object Class 0% / 30% 20% / 50% 10% / 40% 60% / 90% Overall Consistency 80% / 100% 40% / 100% 60% / 100% 80% / 100% Table 8: Evaluation Results Comparison with T2I-CompBench (Huang et al., 2023) using Claude as Base Model. We follow the same experimental setting and the parameters in the main experiments but changing the planning and reasoning agents backbones with claude-3-5-sonnet-20241022 as the base model. Models Color Binding Non-Spatial Relationships SD1.4 (Rombach et al., 2022) 80% / 100% 70% / 100% 80% / 100% 70% / 100% SD2.1 (Rombach et al., 2022) 80% / 100% 30% / 100% 60% / 100% 70% / 100% 90% / 100% 60% / 100% 70% / 100% 30% / 100% 10% / 100% 20% / 100% 30% / 100% 20% / 100% SDXL (Podell et al., 2023) SD3.0 (Esser et al., 2024) Texture Binding Shape Binding Table 9: Time Cost Comparison across Models for VBench (Huang et al., 2024a) Dimensions using Claude as Base Model. This table compares the evaluation time of four different models using the original VBench pipelines versus the Evaluation Agent. The Evaluation Agent significantly reduces the overall evaluation time. VBench (Total Cost) VBench (Avg. Cost per Dimension) Evaluation Agent (Ours) Models 2557 min, 4355 samples Latte-1 (Ma et al., 2024) 1160 min, 4355 samples ModelScope (Wang et al., 2023) 1459 min, 4355 samples VideoCrafter-0.9 (He et al., 2022) VideoCrafter-2 (Chen et al., 2024a) 4261 min, 4355 samples 170 min, 290 samples 77 min, 290 samples 97 min, 290 samples 284 min, 290 samples 12 min, 15 samples 9 min, 16 samples 9 min, 12 samples 26 min, 11 samples Table 10: Time Cost Comparison across Models for T2I-CompBench (Huang et al., 2023) Dimensions using Claude as Base Model. This table compares the evaluation costs for assessing four models across T2I-CompBench dimensions using both the original T2I-CompBench pipelines and our Evaluation Agent. The Evaluation Agent achieves substantial reduction in evaluation time compared to the traditional pipelines. Models SD1.4 (Rombach et al., 2022) SD2.1 (Rombach et al., 2022) SDXL (Podell et al., 2023) SD3.0 (Esser et al., 2024) T2I-Comp (Total Cost) T2I-Comp (Avg. Cost per Dimension) Evaluation Agent (Ours) 563 min, 12000 samples 782 min, 12000 samples 1543 min, 12000 samples 1410 min, 12000 samples 141 min, 3000 samples 196 min, 3000 samples 386 min, 3000 samples 353 min, 3000 samples 6 min, 18 samples 7 min, 17 samples 12 min, 14 samples 16 min, 13 samples 15 paradigms in agent designed to boost the performance in agentic systems have been explored. Tree of Thoughts (ToT) (Yao et al., 2024) advances the process by constructing tree-like reasoning structure, where each node represents reasoning thought, and the final plan is derived through either breadth-first search (BFS) or depth-first search (DFS) strategy. Algorithm of Thoughts (AoT) (Sel et al., 2023) and Graph of Thoughts (GoT) (Besta et al., 2024) are the descending works which propel LLMs through algorithmic reasoning pathways and expand the tree-like reasoning structure to graph-like one respectively. Diagram of Thoughts (DoT) (Zhang et al., 2024a) is recently proposed approach that models the reasoning process as directed acyclic graph (DAG) within single model, effectively reducing circular dependencies and reflecting well-founded logical deduction. The general idea of the agent is to take the freeform natural language inputs from the users, plan accordingly, and take action, where LLMs are commonly used as the reasoning and planning backbones. To enhance the capability of agents in long-chain reasoning tasks, or generally defined task with compositionality, humans tend to decompose it into simpler subtasks and solve them procedurally, which also triggers the development of series of works that mimic the reasoning chain from humans. Chain-of-Thought (CoT) (Wei et al., 2022) and Zero-shot-CoT (Kojima et al., 2022) both leverage prompting to trigger them reasoning \"step by step\", while HuggingGPT (Shen et al., 2024) decomposes the tasks into sub-tasks first and solves them independently with Huggingface. However, although those methods attempted to mimic the human thinking process by decomposing tasks and solving each independently, they are still connected in cascading format, producing only single-path reasoning chain. Self-Consistent CoT (CoT-SC) (Wang et al., 2022) enhances the original CoT approach by generating multiple reasoning paths and selecting the final answer based on majority voting. Given the trade-off between time and performance, we found that the CoT framework is particularly well-suited for evaluation tasks, as its reasoning process aligns closely with the nature of these tasks. The reasonings are purely defined by the reasoning backbone of the core LLMs, without incorporating the feedback from either environments or agents themselves. Under this setting, agents reasoning is straightforward but less effective for longhorizon tasks as it requires the agent to generate flawless plan at the initial stage and it cannot tackle the intermediate failures properly. ReAct (Yao et al., 2022) proposed general paradigm for an agent prompting design by integrating the reasoning traces and task-specific actions in an interleaved triplets thought-action-observation\" to involve the environmental feedback; SelfCheck (Miao et al., 2023) allows agents to review and assess their reasoning steps at different stages, allowing them to identify and correct errors by comparing the intermediate results; Reflexion (Shinn et al., 2024) utilizes verbal reinforcement learning to augment LLMs with memory encodings. By introducing the feedback into the reasoning steps, agents are equipped with additional knowledge from the feedback to do the correction. D.2 Agent in Action Modelling with Tool-Use An important factor that differentiates the assistant and agent could be the action modeling capability. Agents should inherently possess the ability to perceive from the environment and interact with the environment via proposed actions (Xi et al., 2023; Wang et al., 2024b). trending approach to model the action goal of an agent is the tool-use functionality. (Li et al., 2023a; Qin et al., 2023) proposed benchmarks that can be used to evaluate the tool-use capabilities from the perspectives of API calling functions, requiring the agents to generate or select the appropriate API calls for various tasks and domains based on the natural language inputs. lot of model-based works also highlights the tool-use functionality, Toolformer (Schick et al., 2024) trained model in self-supervised manner to enhance the token prediction while maintaining the generality. Gorilla (Patil et al., 2023) is finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls."
        },
        {
            "title": "E More Results",
            "content": "E.1 Experiments on Different Base Models We conducted additional experiments using various base models, including API-based models such as Claude and Gemini, to demonstrate the high extensibility of our framework. Claude. We conducted validation experiments using the same setup as described in Section 4.1, but replaced the base model from GPT-4o to Claude-3.5-Sonnet, specifically using the claude-3-5-sonnet-20241022 version. Table 9 16 Figure 6: Performance Comparison across VBench Dimensions for Different Base Models. This visualization highlights the performance of all backbone models, including GPT-4o and Claude models, providing comprehensive comparison in each dimension for different backbone models. Hatched portions indicate predictions within the exact range, and solid portions within an error margin of one range. Specific numerical results are provided in Table 7 and Table 8 and Table 10 respectively present the time cost and the number of samples required for evaluations across VBench (Huang et al., 2024a) and T2I-CompBench (Huang et al., 2023) dimensions when the base model is replaced with Claude. Furthermore, Table 7 and Table 8 provide the evaluation accuracy results corresponding to these experiments. Notably, even with Claude as the base model, only small number of samples and few minutes are needed to achieve high-quality evaluation results. Figure 6 illustrates the evaluation outcomes using bar charts, comparing Claude and GPT as the base model. Gemini. In addition to using Claude, we also experimented with the current state-of-theart model from the Gemini family, specifically gemini-1.5-pro. However, we found that the evaluation results using Gemini were not ideal, as Gemini often failed to conduct complete and meaningful assessments. Through observation and analysis of the encountered issues, we identified the following common problems: 1) Incorrect Tool Selection: During the evaluation process, Gemini frequently selected the wrong tool for assessment, despite our detailed and accurate descriptions of 17 each tool provided in the system prompt. For example, as shown in Figure 7, when asked, How does the model perform in terms of aesthetics?, the evaluation should have used the tool corresponding to the Aesthetic Quality dimension. However, Gemini incorrectly chose the tool for the Subject Consistency dimension. 2) Repetitive and Ineffective Loops: We observed that Gemini often generated repetitive or nearly identical proposals (subaspects and corresponding thoughts for each round) without adjusting the evaluation direction based on intermediate results. This led to meaningless assessments. For instance, the example in Figure 8 demonstrates that the sub-aspect and thought proposed in the second round were identical to those in the first round. 3) Failure to Generate Final Results: The repetitive behavior mentioned in the second issue frequently resulted in non-stopping loop, ultimately failing to generate meaningful final response to the users query. These issues significantly hinder Geminis ability to produce meaningful evaluation results when used as the base model. being processed early in the attention mechanism. Conversely, information buried deep within long input might receive attenuated attention, despite the models theoretical capacity to consider the entire input. In evaluation agent tasks, the agent perceives the instructions with all rules and references set at the beginning by the system prompt, which means some of the essential details are positioned at the beginning of the prompt. Gemini might deprioritize them during the attention allocation process, which could lead to incomplete comprehension, affecting the quality of the evaluation. In summary, given these findings, we recommend using GPT-4o as the base model for tasks requiring high precision and alignment, particularly for generating prompts that effectively sample from generative models. Its superior performance in producing accurate, contextually aligned prompts contributes to better overall results. Nevertheless, the flexibility of our framework allows for seamless integration with other advanced models like Claude, ensuring its applicability across diverse scenarios and system configurations. This adaptability makes it valuable tool in environments where model availability or performance priorities may vary. E.2 Evaluation Results of Open-Ended User"
        },
        {
            "title": "Queries",
            "content": "In Figures 9-11, we present multiple comprehensive evaluation results for open-ended user queries, including the sub-aspects raised by the Evaluation Agent in each evaluation round, the corresponding thoughts, the step-by-step evaluation process for each round, and the final comprehensive conclusion. Figure 7: Common Failure Pattern in Tool Selection. As shown in the figure, Gemini frequently selected an incorrect tool for evaluation. In this case, the model should have selected the Aesthetic Quality tool, but it incorrectly chose Subject Consistency, leading to inaccuracies in subsequent assessments. Figure 8: Common Failures in Generating SubAspects and Finalizing Responses. The figure highlights two critical failures: first, Gemini fails to propose new sub-aspects based on observations from previous rounds, instead engaging in repetitive and meaningless loops without strictly adhering to the provided instructions. Second, this repetitive behavior leads to nonstopping loop, ultimately failing to generate meaningful final response to the users query. We try to analyze the failure modes here for the failure patterns. Although the Gemini model, designed with context window capable of handling up to 2 million tokens, certain input prompts might still be inadequately comprehended, particularly in complex evaluation tasks. Theoretically, Geminis extensive context window allows it to process lengthy inputs effectively. However, empirical evidence suggests that even models with large context windows may experience performance degradation when presented with extremely long prompts. This phenomenon is not solely attributable to the absolute length of the input but is also linked to the distribution of critical information across the context. Most transformer-based models, including Gemini, rely on positional encodings to maintain the sequential order of tokens. These encodings inherently influence how the model prioritizes and retrieves information. When key information is placed at the beginning of prompt, it benefits from 18 Figure 9: Case of Open-Ended User Query Evaluation. This figure illustrates the Evaluation Agents response to the user query, Can the model generate variations of existing artwork while maintaining the original style? 19 Figure 10: Case of Open-Ended User Query Evaluation. This figure illustrates the Evaluation Agents response to the user query, How precisely can the user specify object relationships? 20 Figure 11: Case of Open-Ended User Query Evaluation. This figure illustrates the Evaluation Agents response to the user query, How well the model can generate specific number of objects?"
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "Shanghai Artificial Intelligence Laboratory"
    ]
}