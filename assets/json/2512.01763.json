{
    "paper_title": "HiconAgent: History Context-aware Policy Optimization for GUI Agents",
    "authors": [
        "Xurui Zhou",
        "Gongwei Chen",
        "Yuquan Xie",
        "Zaijing Li",
        "Kaiwen Zhou",
        "Shuai Wang",
        "Shuo Yang",
        "Zhuotao Tian",
        "Rui Shao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction."
        },
        {
            "title": "Start",
            "content": "HiconAgent: History Context-aware Policy Optimization for GUI Agents Xurui Zhou1,Gongwei Chen1, Yuquan Xie1, Zaijing Li1, Kaiwen Zhou2, Shuai Wang2, Shuo Yang1, Zhuotao Tian1, Rui Shao1 1Harbin Institute of Technology, Shenzhen 2Huawei Noahs Ark Lab zhouxurui1314@gmail.com chengongwei@hit.edu.cn shaorui@hit.edu.cn https://github.com/JiuTian-VL/HiconAgent 5 2 0 2 1 ] . [ 1 3 6 7 1 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Graphical User Interface (GUI) agents require effective utilization of historical context to perform sequential navigation tasks. While incorporating past actions and observations can significantly improve decision-making, naively using full history leads to excessive computational overhead and potential distraction from irrelevant information. In this work, we introduce HiconAgent, GUI agent trained with History Context-aware Policy Optimization (HCPO) for effective and efficient utilization of historical information. HCPO explicitly optimizes history usage in both sampling and policy updates by integrating two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable-length histories during sampling, enabling adaptive use of the most relevant historical context to improve sequential decision quality; (2) Anchor-guided History Compression (AHC) refines the policy update phase via dual-branch optimization strategy, where the compressed branch drops history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through historyenhanced alignment loss to enforce consistent history usage, achieving efficiency with minimal performance degradation. Extensive experiments on mainstream GUI navigation benchmarks demonstrate the strong performance of our model. Despite its smaller size, HiconAgent-3B outperforms GUIR1-7B by +8.46% grounding and +11.32% step successful rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW, with up to 2.47 computational speedup and 60% FLOPs reduction. 1. Introduction Multimodal Large Language Model (MLLM)-based GUI agents [7, 35] have recently achieved strong performance in grounding and navigation tasks by leveraging textual instructions, visual observations, and historical trajectories. Among various training strategies, reinforcement learning (RL) [20, 25, 33] has demonstrated strong effectiveness by Figure 1. Comparison of existing GUI RL framework with our HCPO framework. HCPO jointly improves the sampling and update phases of training by integrating Dynamic Context Sampling (DCS) and Anchor-guided History Compression (AHC). allowing GUI agents to directly optimize task-oriented objectives such as grounding accuracy and successful rate. Compared to supervised learning approaches [10, 37], RL-based methods [22, 23] significantly improve decision quality, robustness and generalization. As result, RL has become mainstream paradigm for training advanced GUI agents. Despite these advances, the role of history usage in RLbased GUI agents remains largely underexplored. Most prior works [18, 22, 23] adopt simplified design in which history observations (past screenshots) are omitted, and only history actions are included as the input context. While this choice reduces memory and computational cost, it discards rich visual cues from past observations that are often essential for resolving ambiguous instructions, grounding visually similar elements, and maintaining temporal consistency across steps [39]. Conversely, naively incorporating complete history, including both past actions and observations, substantially increases computational overhead due to the quadratic complexity of attention mechanisms and the large number of visual tokens from high-resolution screenshots. This trade-off between decision quality and efficiency motivates the development of methods that can effectively retain the most informative parts of historical context while mitigating redundancy. To this end, we propose History Context-aware Policy Optimization (HCPO), training framework designed to improve both the effectiveness and efficiency of history usage in GUI agents. As illustrated in Figure 1, HCPO improves both the sampling and update phases of existing GUI RL framework through two complementary components: Dynamic Context Sampling (DCS) and Anchor-guided History Compression (AHC). In the sampling phase, DCS addresses the variability of history dependence across decision steps. Different from conventional RL approaches that use fixed-length history, DCS samples multiple history variants for each rollout using the exponential-biased distribution, encouraging the model to adaptively utilize the most relevant context. Our sampling distribution is motivated by empirical observations, ensuring stable learning and avoiding the degeneration observed with naive uniform sampling. In the update phase, AHC jointly optimizes compressed and uncompressed branches to enhance policy updates under history compression. The compressed branch drops history observations after early fusion, retaining only action tokens as anchors to preserve essential decision signals. It is jointly optimized with the uncompressed branch through history-enhanced alignment loss. This compression strategy is guided by our empirical layer-wise token-drop analysis. Our contributions can be summarized as follows: We conduct comprehensive empirical analysis on history usage in GUI agents. Our findings show that different tasks and decision steps prefer different history lengths and history actions act as critical anchors for visual information flow. These findings reveal important inefficiencies in existing designs and directly motivate our method. We propose History Context-aware Policy Optimization (HCPO), novel reinforcement fine-tuning framework that combines Dynamic Context Sampling and Anchorguided History Compression. Together, they enable agents to learn adaptive history usage while reducing redundancy and preserving decision quality. We validate our method on three GUI navigation benchmarks: GUI-Odyssey, AndroidControl, and AITW. HiconAgent-3B consistently outperforms existing reinforcement learning based agents. It outperforms larger 7B agents such as GUI-R1-7B by +11.32% in step successful rate on GUI-Odyssey, while achieving 2.47 computational speedup and 60% reduction in FLOPs. 2. Related Work 2.1. GUI Agents and History Utilization GUI agents aim to complete high-level tasks by interacting with graphical user interfaces through sequences of lowlevel actions such as clicking and typing. Recent advances in multimodal large language models [1, 3, 11, 1417, 2830, 32, 41, 43] have rapidly expanded their capability to integrate and reason over diverse input modalities. This progress has enabled GUI agents to achieve significant improvements in multi-step navigation tasks [5, 7, 3436]. Although historical information, including past observations and actions, is essential for GUI agents to understand user instructions and make decisions, the issue of redundancy in long histories remains persistent challenge. Existing studies [4, 21, 40] under supervised fine-tuning show that adding past actions improves performance with minimal input cost. In contrast, incorporating full visual history brings larger gains but incurs significantly higher computation, especially in long-horizon or high-resolution scenarios. This trade-off highlights the inefficiency of naively using full history and motivates more selective or compressed representations of historical context. 2.2. Rule-based Reinforcement Learning Large language models have shown remarkable improvements in reasoning-intensive tasks such as mathematics and programming when optimized with reinforcement learning. This reinforcement fine-tuning paradigm is now being acIn particular, tively explored in the multimodal domain. Group Relative Policy Optimization (GRPO) [8, 31] has emerged as an effective alternative to conventional methods such as PPO [27], especially for training multimodal models. It simplifies the training pipeline by evaluating responses using relative, normalized rewards computed within each sample group. In the GUI domain, rule-based RL typically uses exact match or distance-based criteria for action types, coordinates, or textual inputs, providing fine-grained rewards that guide the agent toward correct execution [23, 24]. Although existing RL approaches have achieved notable performance gains, there has been little to no discussion on effective and efficient history utilization, and existing methods typically overlook strategies for balancing the trade-off between computational cost and the retention of critical historical context. Our method extends rule-based RL with history-aware policy optimization to improve decision quality while reducing computational overhead. 3. Preliminaries Problem Definition. The goal of GUI agent is to complete high-level tasks by interacting with graphical user interfaces through sequence of low-level actions. At each timestep Figure 2. Different samples prefer different history lengths. Left: For each sample we evaluate set of different history lengths τ and take the τ that yields the highest mean reward. The preferred τ differs across samples and action types. Right: Providing more history does not necessarily yield the optimal result, suggesting effective usage of historical information is under exploration. t, the agent observes natural language task instruction I, current screenshot observation st, and history context Ht = (stτ , atτ ), . . . , (st1, at1), where τ denotes the history window size, which is the number of previous steps included in the history. The agent then generates an action at conditioned on (I, Ht, st) and executes it in the GUI environment. This interaction process defines sequential decision-making problem, which can be formulated as Markov Decision Process (MDP). Application of GRPO in GUI Agents. When applying GRPO to training GUI agents, we treat the agent as policy model and reorganize the inputs by defining = (I, Ht, st). For each input q, the policy model generates set of candidate responses = o1, . . . , oG, where each response oi consists of thought process ti and corresponding action ai, i.e., oi = (ti, ai). The standard GRPO algorithm adopts the following training objective: LGRP = (cid:34) (cid:88) (cid:16) i=1 min (cid:16) πθ(oi q) πθold (oi q) Ai, clip (cid:16) πθ(oi q) πθold (oi q) , 1 ϵ, 1 + ϵ Ai (cid:17) (cid:17) (cid:35) β KL(cid:0)πθ(oi q) πθref (oi q)(cid:1)(cid:17) (1) where ϵ is the clipping parameter, β is hyperparameter. Ai is the advantage calculated by normalizing the group level rewards {Ri}G i=1. By normalization, advantage Ai represents the relative quality of the i-th response. 4. Rethinking History Usage: Limitations of Fixed Context and the Anchoring Role of Actions key challenge in building strong GUI agents is how to appropriately leverage historical context. To address this, we conduct two empirical studies focusing on effectiveness and efficiency. The first study analyzes how different samples benefit from different history lengths, showing that fixed-length context is often suboptimal. This highlights the need for dynamically adjusting context length to match step-specific dependencies. The second study examines how historical information flows through the model and finds that action tokens serve as anchors for aggregating and delivering useful visual semantics. This highlights the importance of retaining history actions while compressing redundant visuals to improve efficiency without compromising performance. 4.1. Analyzing the Impact of History Length In GUI navigation, different steps may depend on varying lengths of history context. We conduct an empirical analysis to measure the impact of history length on decision quality across different samples and action types. As shown in the left plot of Figure 2, we perform rollout-based evaluation over the training set using base model with fixed policy weights. For each sample, we conduct 8 rollouts under different history lengths τ {0, 1, 2}, and record the average reward under each setting. We then determine the optimal history length per sample by comparing these mean rewards. Samples with negligible differences (mean reward difference < 0.05) are discarded. The result reveals clear pattern: different samples exhibit different optimal history lengths. While some steps benefit from short-term context, others require longer to achieve higher reward. Notably, we further analyze cases where shorter history lengths outperform longer ones, as shown in the right plot of Figure 2. For each such sample, we compute the mean-reward difference between the better-performing shorter context and longer one, i.e., Improvement = mean reward(τshort) mean reward(τlong). The resulting distribution illustrates that these improvements are nontrivial, shorter histories can yield significantly higher rewards in certain situations. This phenomenon suggests that longer history is not always more beneficial, and in some cases may even hinder performance (lower reward), likely due to introducing irrelevant information. Together with the left plot, this reinforces the insight that fixed context length τ cannot accommodate the diverse temporal dependencies across samples. Dynamically varying the length of historical context can lead to more effective model behavior and improved decision quality. Figure 3. Layer-wise token-drop analysis. Left: Schematic of the layer-wise token-drop probe, illustrating the information flow of image-drop and action-drop. Right: Dropping Ahis at shallow depths (k<12) causes much larger decline than dropping Vhis. Even if rich visual information is retained, later layers cannot directly extract effective cues from Vhis without the action anchors. As increases, the action-drop curve rises toward the image-drop curve and the imageaction drop curve converges rapidly. 4.2. History Actions as Information Flow Anchors Naively appending historical observations to the context inflates sequence length and computation cost, burdening the model with tokens that contribute little to decision making [38]. In practice, only the regions associated with past actions tend to carry meaningful semantics for decisionmaking, while most other areas tend to be redundant. This motivates designing method to compress redundant historical information to improve efficiency without harming performance. The central question is which tokens should be preserved during layer interaction. Different from previous work [4, 6] that adopts conclusions from information flow studies of single-image VQA scenario [42] to identify tokens for retention, we conduct an information flow analysis in the GUI navigation scenario within reinforcement learning framework, tracing how historical visuals and actions interact across model depth. Layer-wise token-drop setup. We probe how history propagates through the LLM with layer-conditioned token drop. Qwen2.5-VL-3B (36 layers) is trained on our dataset (Section 6) with history image/action context and evaluated on AndroidControl using step successful rate (SR). At depth k, we remove from layer k+1 onward: (i) history actions Ahis = {atτ , . . . , at1}; (ii) history images Vhis = {stτ , . . . , st1}; or (iii) both history actions and images Ht = (stτ , atτ ), . . . , (st1, at1). Sweeping yields the curves in Figure 3; gaps between them show that later layers access history chiefly via action tokens, with smaller direct contribution from visuals. Shallow depth (k<12): As shown in Figure 3, dropping Ahis in shallow layers causes large performance degradation, while dropping Vhis at the same depths is much less harmful. This pattern indicates that effective use of historical information depend on the action tokens as the anchor: even if rich visual information is retained, later layers cannot directly extract effective cues from Vhis without the action anchors. By contrast, dropping Vhis in this range is more tolerable, since the model has already merged part of the visual history into Ahis; the action anchors then pass that information forward through subsequent layers. Deeper depth (k12): For > 12, the action-drop curve rises quickly and, by mid depth, its gap to the imagedrop curve is small. This indicates that, in these layers, predictions obtain historical information through interactions with the action anchors. Once that interaction has already occurred before the drop point, removing Ahis or Vhis later causes only little decline, and performance continues to improve with depth. When 24, all three curves converge to the no-compression accuracy, indicating that the model has already integrated most historical information and can proceed without retaining these history tokens. Key discovery. From the layer-wise drop analysis in Figure 3, relying only on history actions (2A) yields limited benefit: without their paired screenshots, history action tokens Ahis lack grounded semantics and provide little guidance for decision-making. In contrast, incorporating history observations (2AO) proves critical for navigation success. However, the gain does not arise from directly attending to history images. Instead, it emerges mainly at intermediate depths, where Ahis interacts with the history visual tokens Vhis and delivers the extracted information to subsequent tokens, highlighting the role of history actions as anchors for multimodal information flow. Implications for designing compression methods. Guided by the above experimental results and analysis, we adopt single rule to balance efficiency and effectiveness: compress history only after an early fusion depth k, pruning Vhis while retaining Ahis. On the efficiency side, removing history images greatly reduces sequence length and computation cost; on the effectiveness side, keeping the action anchors preserves the historical cues that later tokens actually use. To make the proposed compression method work better, training is essential to strengthen cross-modal interaction in the first layers, so that by the time compression occurs, the relevant visual context has been sufficiently transferred into Ahis, thereby preserving the necessary information flow. Figure 4. Overview of our history context-aware optimization framework for building HiconAgent. HCPO improves both the sampling and update phases of policy optimization by incorporating two key components: (1) Dynamic Context Sampling (DCS), which introduces varied history lengths during training to encourage context-effective decision-making, and (2) Anchor-guided History Compression (AHC), which adopts dual-branch architecture where both branches share sampled responses and group-wise advantages. The compressed branch is trained using policy gradients, aligned with the uncompressed branch via history-enhanced alignment loss. 5. HiconAgent We propose History Context-aware Policy Optimization (HCPO), reinforcement learning framework that improves history utilization by strengthening both the sampling and update phases. Dynamic Context Sampling (DCS) varies history lengths during rollouts to guide effective context usage, while Anchor-guided History Compression (AHC) reduces redundancy during updates via alignment with full-history supervision. Together, they enable policy optimization with effective and efficient historical context usage. 5.1. Dynamic Context Sampling As shown in the analysis in Section 4, relying on fixedlength history context is often suboptimal. This motivates adapting the history length to step-specific dependencies, allowing the policy to leverage the appropriate amount of context as needed. To address this, we propose Dynamic Context Sampling, which exposes the policy to diverse history lengths during training. This allows the agent to adaptively focus on the most relevant context, thereby improving overall policy learning. During training, instead of always feeding fixed-length history Ht, we dynamically sample and select variants of truncated histories {H 1 , . . . , uses selected history length τi τ sampled from distribution p. However, naively adopting uniform distribution leads to degeneration phenomenon, as later shown in our sampling ablation. Motivated by this empirical observation, we prot }, where pose an exponential-biased distribution that mitigates training collapse by gradually shifting the sampling toward larger τ as training progresses. At training step u, our exponentialbiased distribution ExpBias(u) is defined as: (τi u) = (cid:1) exp (cid:0)λ(u) τi j=0 exp (cid:0)λ(u) j(cid:1) (cid:80)N (2) where λ(u) is linear function that increases with u. In the early stage of training, λ(u) 0 and the distribution is nearly uniform, encouraging random exploration. As training progresses, λ(u) gradually grows, yielding an increasingly biased exponential distribution that favors larger values of τi. This schedule smoothly shifts the sampling strategy from random selection to full-context history. Each variant forms an input qi = (I, , st), and produces response oi. These responses are evaluated as group, yielding an advantage value for each response. Responses with higher advantages receive stronger gradient updates, allowing the policy to adaptively learn which history lengths lead to improved policy behavior. Importantly, to maintain consistency between training and inference, we combine each sampled response oi with the full history context input (I, Ht, st) to compute the logits for optimization. This design allows the policy to explore effective solutions under varying historical conditions while being evaluated under unified context length. 5.2. Anchor-guided History Compression straint: Leveraging the observation in Section 4 that history actions preserve the historical cues that later tokens actually use, we propose Anchor-Guided History Compression. AHC keeps past actions as anchors, prunes visual history after early fusion, and leverages alignment with an uncompressed branch, preserving decision quality while reducing sequence length and FLOPs. Let = {I, Ht, st} be the model input at step t, and define the importance sampling ratio as ρi = πθ(oiq) πθold (oiq) . The responses from the uncompressed branch are optimized using the standard GRPO objective: Lw/o comp = (cid:88) (cid:16) i=1 min (cid:0)ρiAi, clip(ρi, 1 ϵ, 1 + ϵ) Ai (cid:1) β KL(cid:2)πθ(oi q) πθref (oi q)(cid:3)(cid:17) , (3) To reduce redundancy, we remove all history vision tokens Vhis = {stτ , . . . , st1} while retaining the past action tokens. The model continues forwarding with the compressed sequence {I, Ahis, st}, where the retained action tokens Ahis = {atτ , . . . , at1} constitute the compressed history . The compressed branch is further optimized with GRPO-style objective using the same responses and advantages, leveraging the superior quality of responses generated from the uncompressed branch, where qc = {I, , st} denotes the inputs with compressed history and the importance = πθ(oiqc) sampling ratio as ρc πθold (oiqc) . Lw/ comp = (cid:88) (cid:18) i=1 min (ρc Ai, clip(ρc , 1 ϵ, 1 + ϵ)Ai) β KL(cid:2)πθ(oi qc) πθref (oi qc)(cid:3) (4) (cid:19) . To ensure that the compressed branch retains the core decision-making ability of the original model, we introduce history-enhanced alignment objective. Specifically, given the original history Ht and its compressed counterpart , we perform parallel forward passes through both branches. We then minimize the KL divergence between their output distributions, effectively using the uncompressed branch as teacher to guide the compressed branch. Note that the uncompressed branch is used only for guidance; we detach its outputs to prevent gradient backpropagation in the KL loss. This approach allows the compression module to reduce redundancy while preserving critical behavioral patterns from the original branch. The alignment objective is defined as: LKL = (cid:88) i=1 KL [πθ(oi qc) πθ(oi q)] , (5) The final HCPO loss is the sum of the uncompressed branch loss, the compressed branch loss, and the alignment conLHCPO = Lw/o comp + Lw comp + λLKL, (6) where λ controls the strength of the alignment guidance. This framework enables effective policy optimization under compressed context while preserving temporal consistency. 5.3. Reward Design In GUI navigation tasks, each action consists of type and value. The type is chosen from set of discrete options (e.g., CLICK, SCROLL), while the value format varies depending on the type. Based on this characteristic of GUI tasks, we introduce the following three rewards: This the models response adheres to predefined structure like <think>...</think><answer>...</answer>. It returns 1 if the format is correct, 0 otherwise, promoting structured reasoning and output generation. Format reward (rf ): term ensures Action type reward (rt): We assign 1 if the predicted action type exactly matches the ground-truth type, and 0 otherwise. This term enforces correctness at the semantic level of action selection. Action value reward (rv): For actions without values (e.g., PRESS BACK), the reward is 1 if the type is correct. For actions with textual values (e.g., TYPE, OPEN APP), we compute the F1 score between prediction and ground truth, awarding 1 if F1 > 0.5. For actions with discrete values (e.g., SCROLL), the value must exactly match the ground truth. For coordinate-based actions (e.g., CLICK), we calculate the Euclidean distance between predicted and actual coordinates, and assign continuous reward rv = 1 to allow fine-grained feedback for grounding accuracy. The final reward used for policy optimization is the sum of the three components: = rf + rt + rv (7) 6. Experiments 6.1. Implementation Details Metrics. We evaluate our model on three representative navigation-oriented datasets, AndroidControl-High[13], AITW[12] and GUI-Odyssey[21], considering only their test splits under an out-of-distribution (OOD) evaluation setting to assess generalization performance. We use three standard metrics widely adopted in prior work on GUI agents, following the evaluation protocol of Os-Atlas [35]. Specifically, we report action type prediction accuracy (Type), GUI grounding accuracy (Grounding), and step success rate (SR). FLOPs is computed using deepspeed flops-profiler with batch size 1. We include all model components when calculating FLOPs. The reported number is averaged over 200 samples from the training set with history length τ = 2. Training and Evaluation. HiconAgent-3B is built upon Qwen2.5-VL-3B. We follow the same training setting in GUI-R1 [23]. In the reinforcement fine-tuning stage, we adopt the following hyperparameter settings to ensure stable optimization. The rollout batch size and global batch size is set to 64, with 8 rollouts per update step. We use small learning rate of 1 106 to stabilize training. To balance computational efficiency and generation quality, the maximum number of input pixels is capped at 1,003,520. For the language input and output, both the maximum prompt length and the maximum response length are restricted to 2048 tokens. The rollout temperature is fixed at 1.0 to encourage diverse yet consistent exploration during training. To keep the number of tokens consistent with prior work, we set the history window size to 2, meaning the agent can access up to two past interaction steps. Each historical step includes both the screenshot observation and the corresponding action, i.e., Ht = {(st2, at2), (st1, at1)}. All images are resized to fixed resolution, and the number of visual tokens after encoding is limited to maximum of 512 to ensure computational efficiency. Model Configuration. From an efficiency perspective, HiconAgent-3B adopts the drop = 6 configuration by default in the following experiments. This setting achieves up to 60% reduction in FLOPs while maintaining competitive accuracy, as shown in Table 5. 6.2. Experiment Results We present the main experimental results in Table 1 and Table 2 on three representative GUI navigation datasets: AndroidControl-High [13], AITW [26] and GUIOdyssey [21]. Table 1 provides detailed comparison under the same data scale and training settings, highlighting the effect of our history-aware optimization strategy against both supervised fine-tuning and reinforcement fine-tuning baselines. Table 2 further extends the comparison to recent advanced GUI agents of varying model sizes and training data volumes, demonstrating the generalization ability of our approach in out-of-distribution (OOD) scenarios. As shown in Table 1, HiconAgent-3B achieves consistent improvements over all baselines trained under the same data scale and training settings. We first observe that HiconAgent-3B consistently outperforms the supervised baseline Qwen2.5VL-3B, with clear gains in both grounding accuracy and step successful rate. When trained with reinforcement learning, HiconAgent-3B further surpasses the GRPO baseline GUI-R1-3B (+5.85% SR) and GUI-R1-7B (+0.73% SR) on AndroidControl under the same training setup, demonstrating stronger decision-making ability. The advantage of HiconAgent becomes especially pronounced on the long-horizon GUI-Odyssey benchmark. Despite having less than half the parameters of GUI-R1-7B, our 3B model achieves remarkable +8.46% improvement Models AC-High GUI-Odyssey Grounding SR Grounding SR Supervised Fine-Tuning GUI-R1-3B [23] GUI-R1-7B [23] Qwen2.5VL-3B 49.53 58.69 52.89 41.22 48.11 41.95 Zero-Shot GPT-4o [11] Qwen2.5VL-3B Qwen2.5VL-7B 30.90 46.51 59. 21.17 38.90 47.06 32.21 38.65 36.21 14.17 26.49 37.78 Reinforcement Fine-Tuning 27.31 34.44 31.74 5.36 26.69 34. GUI-R1-3B [23] GUI-R1-7B [23] HiconAgent-3B 65.51 (-0.05) 52.40 (+0.73) 52.10 (+8.46) 50.11 (+11.32) 41.52 43.64 41.33 38.79 46.55 51.67 56.24 65.56 Table 1. Performances on AndroidControl-High and GUIOdyssey. Red indicates improvement, green indicates degradation compared to GUI-R1-7B. Our 3B model outperforms GUI-R1-7B by +8.46% grounding and +11.32% SR on GUI-Odyssey. in grounding accuracy and +11.32% in step successful rate. Compared with GUI-R1, which does not explicitly exploit historical information, HiconAgent-3B adopts more effective strategy for leveraging historical context, leading to stronger sequential reasoning and consistent execution. Table 2 summarizes OOD generalization across the three benchmarks. Without relying on large-scale data filtering or dataset curation, HiconAgent is trained on only 3K unfiltered samples, yet it achieves the highest average step successful rate (51.47%) among all compared models. Remarkably, it surpasses stronger models such as OS-Atlas-7B, GUI-R17B, and infiGUI-3B, which are trained with much larger data volumes (13M-filtered, 3K-filtered, and 32K-filtered respectively). Although infiGUI-3B achieves notably high SR on the AndroidControl benchmark (71.1%), its performance drops considerably on AITW and GUI-Odyssey, indicating weaker generalization under OOD conditions. This demonModel #Data Filter AC-High AITW Odyssey Avg SR GPT-4o [11] OS-Atlas-7B [35] 13M 3K GUI-R1-7B [23] 2K UI-shift-7B [9] 9K UI-AGILE [18] 32K infiGUI-3B [19] 21.17 29.83 51.67 52.16 50.97 71.10 26.07 41.38 55.31 54.38 48.66 46.51 5.36 26.96 38.79 32.75 36.96 33.15 17.54 32.72 48.59 46.43 45.53 50.25 HiconAgent-3B 3K 52.40 51.91 50.11 51.47 Table 2. Step Successful Rate evaluated on three representative GUI navigation datasets, compared with recent models. Cells highlighted in red correspond to datasets that are IID for the respective models, whereas cells highlighted in green indicate OOD setting. Models DCS Update τ Sampling p(τ ) AC-High SR Train Hrs HCPO (w/o DCS) HCPO (Uniform) HCPO (Uniform) {0,1,2} HCPO (ExpBias) 2 2 2 (0, 2) (0, 2) ExpBias(u) 51.03 50.53 51.62 52.40 17h 17h 30h 17h Table 3. Ablation study on different sampling distributions in Dynamic Context Sampling (DCS). (0, 2) denotes uniform distribution over τ {0, 1, 2}, while ExpBias(u) represents the exponential-biased schedule defined in Eq. (2). strates that our HCPO framework is not only effective but also highly data-efficient, enabling strong generalization even without large-scale data curation. 6.3. Ablation Study Impact of sampling distribution p. We perform an ablation on the choice of p(τ ) to study how different sampling strategies affect learning. uniform sampling baseline (0, 2) is first adopted for comparison, where τ {0, 1, 2} denotes the sampled history length. Under plain uniform sampling (0, 2), we observe degeneration phenomenon during training: the sampling quality of the shorter histories (τ = 0, 1) weakens over time as shown in Figure 5. This is because we only compute gradients and update parameters using the context with τ = 2 to ensure traininginference consistency. To mitigate this, we design variant that enforces the inclusion of all input contexts with τ {0, 1, 2} in the optimization. While this all-τ update strategy partially recovers the performance lost by naive uniform sampling, it substantially increases training overhead and still fails to match the performance of our proposed exponential-biased distribution. The exponential-biased sampling ExpBias(u) mitigates collapse by gradually biasing the sampling toward larger τ as training progresses, while still encouraging exploration of shorter histories in the early stage. This strategy achieves the best trade-off between performance and computational cost, as reported in Table 3. Figure 5. Evolution of the short-vs-long history reward ratio under uniform τ sampling. The declining ratio reflects the gradual degradation of short-history response quality during training. Effect of DCS and AHC. Table 4 evaluates the impact of Dynamic Context Sampling (DCS) and Anchor-guided History Compression (AHC) across the three navigation benchmarks. Training only compressed branch using standard GRPO yields the weakest performance, indicating that unguided compression fails to retain useful historical signals. Adding an uncompressed branch without KL improves performance, showing that full-history responses help guide the compressed branch. Incorporating KL alignment between the two branches further enhances performance, validating the benefit of matching compressed outputs to the full-history teacher. Finally, enabling DCS achieves the best overall results, confirming that DCS enhances the models ability to utilize historical context more effectively. Models 2Branch. KL DCS AC-High AITW Odyssey GRPO HCPO (w/o KL, DCS) HCPO (w/o DCS) HCPO 44.89 45. 43.21 48.70(+3.81) 49.23(+3.61) 47.09(+3.88) 51.03(+6.14) 50.78(+5.16) 48.68(+5.47) 52.40(+7.51) 51.91(+6.29) 50.11(+6.90) Table 4. Ablation study on the dual-branch architecture, alignment loss, and DCS, evaluated using the SR metric. Experiments are conducted with compression enabled. Impact of layer drop position. We study how the position of the layer drop (k) affects the trade-off between computation and performance. As shown in Table 5, dropping earlier layers (e.g., = 1) leads to the largest FLOPs reduction (-33.81 % vs 3B), but comes at the cost of degraded performance across all metrics. As increases, computation gradually increases while performance steadily improves. Our choice of = 6 offers good balance, retaining competitive performance with 59.54% FLOPs reduction and 2.47 computational speedup (62.31T / 35.75). Computation Performance Drop FLOPs vs 3B vs 7B Tokens Type SR = 1 = 3 = 6 = 12 w/o drop 23.66 24.28 25.21 27.07 35. -33.81% -62.02% -32.07% -61.04% -29.47% -59.54% -24.28% -56.57% -42.64% - 674 674 674 674 1664 65.33 66.00 66.56 67.02 69.29 46.11 46.92 47.34 47.89 52.29 Table 5. FLOPs, token counts, and performance of base model under different layer-drop settings, with relative FLOPs reduction versus the 3B model (35.75T) and 7B model (62.31T). Impact of HCPO on History Utilization across Action Types. Beyond the overall improvements reported in Table 4, we further examine per-action performance to understand how HCPO influences different types of decisions. As shown in Figure 7, applying HCPO consistently improves accuracy across most action categories on both AndroidControl and GUI-Odyssey. The gains are especially pronounced on finished actions, which directly determine whether trajectory Figure 6. To illustrate HCPOs enhancement in leveraging historical information, we present two scenarios: Left (Flight Booking) and Right (Shopping Task). Our model correctly inputs Delhi by reasoning over historical context and selects Red Chief despite visual redundancy. While the base model trained without HCPO misinterprets history and fails in both cases. is successfully terminated, suggesting that HCPO strengthens the models ability to maintain global sequence control rather than overfitting to local step transitions. Improvements are also evident in challenging categories such as scroll, which demands exact direction prediction, and type, which often involves generating context-relevant input. Such findings highlight that our history-aware training paradigm remains effective in scenarios where historical cues are essential for accurate action generation. Figure 7. Per-action accuracy comparison before and after applying HCPO. Both AndroidControl (left) and GUI-Odyssey (right) benefit from history-compressed optimization, especially on finished actions, showing improved sequential decision quality. Case Study on Historical Context Utilization. To investigate how HCPO intuitively enhances the use of historical context, we conduct comparative case study on two representative tasks, as shown in Figure 6. In the flight-booking scenario (left), HiconAgent-3B correctly reasons over previous steps and inputs Delhi as the departure city, whereas the base model trained without HCPO mistakenly enters the destination Brussels. Similarly, in the shopping task (right), where repetitive history screenshots introduce visual ambiguity, our model robustly focuses on the current frame to accurately select the target brand Red Chief, while the base model fails to resolve the ambiguity and deviates from the intended action. These cases illustrate that HCPO enables more reliable and effective usage of historical context. 7. Conclusion In this paper, we present HiconAgent, history-aware GUI agent trained with History Context-aware Policy Optimization. Through extensive empirical investigations, we first revisited how history is utilized in GUI reinforcement learning agents. Our two key studies revealed that different decision steps prefer different history lengths and historical actions serve as information flow anchors. By pairing DCS and AHC, our model outperforms larger models with fewer FLOPs. These results highlight HiconAgent as practical path toward lightweight, high-performance GUI agents."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents. arXiv preprint, 2024. arXiv:2407.17490 [cs]. 1 [3] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: Empowering multimodal large language model with dual-level visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2654026550, 2024. 2 [4] Gongwei Chen, Xurui Zhou, Rui Shao, Yibo Lyu, Kaiwen Zhou, Shuai Wang, Wentao Li, Yinchuan Li, Zhongang Qi, and Liqiang Nie. Less is more: Empowering gui agent with context-aware simplification. arXiv preprint arXiv:2507.03730, 2025. 2, 4 [5] Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, et al. Spa-bench: comprehensive benchmark for smartphone agent evaluation. In NeurIPS 2024 Workshop on Open-World Agents, 2024. 2 [6] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. 4 [7] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. arXiv, 2024. arXiv:2401.10935 [cs]. 1, [8] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025. arXiv:2501.12948 [cs]. 2 [9] Longxi Gao, Li Zhang, and Mengwei Xu. Uishift: Enhancing vlm-based gui agents through self-supervised reinforcement learning. arXiv preprint arXiv:2505.12493, 2025. 7 [10] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for GUI agents. In The Thirteenth International Conference on Learning Representations, 2025. 1 [11] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 7 [12] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. AutoWebGLM: Bootstrap And Reinforce Large Language Model-based Web Navigating Agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. arXiv, 2024. arXiv:2404.03648 [cs]. 6 [13] Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37:92130 92154, 2024. 6, [14] Wei Li, Bing Hu, Rui Shao, Leyang Shen, and Liqiang Nie. Lion-fs: Fast & slow video-language thinker as online video assistant. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 32403251, 2025. 2 [15] Wei Li, Renshan Zhang, Rui Shao, Zhijian Fang, Kaiwen Zhou, Zhuotao Tian, and Liqiang Nie. Semanticvla: Semanticaligned sparsification and enhancement for efficient robotic manipulation. arXiv preprint arXiv:2511.10518, 2025. [16] Wei Li, Renshan Zhang, Rui Shao, Jie He, and Liqiang Nie. Cogvla: Cognition-aligned vision-language-action model via instruction-driven routing & sparsification. arXiv preprint arXiv:2508.21046, 2025. [17] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-1: Hybrid multimodal memory empowered agents excel in long-horizon tasks. In Advances in Neural Information Processing Systems, pages 4988149913, 2024. 2 [18] Shuquan Lian, Yuhang Wu, Jia Ma, Zihan Song, Bingqi Chen, Xiawu Zheng, and Hui Li. Ui-agile: Advancing gui agents with effective reinforcement learning and precise inferencetime grounding. arXiv preprint arXiv:2507.22025, 2025. 1, 7 [19] Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infiguir1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025. 7 [20] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-RFT: Visual Reinforcement Fine-Tuning, 2025. arXiv:2503.01785 [cs]. [21] Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. GUI Odyssey: Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices. arXiv preprint, 2024. arXiv:2406.08451 [cs]. 2, 6, 7 [36] Bin Xie, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Jie Liu, Min Zhang, and Liqiang Nie. Gui-explorer: Autonomous exploration and mining of transition-aware knowledge for gui agent. In Annual Meeting of the Association for Computational Linguistics (ACL), 2025. 2 [37] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction. arXiv preprint, 2024. arXiv:2412.04454 [cs]. 1 [38] Danyang Zhang, Situo Zhang, Ziyue Yang, Zichen Zhu, Zihan Zhao, Ruisheng Cao, Lu Chen, and Kai Yu. Progrm: Build better gui agents with progress rewards. arXiv preprint arXiv:2505.18121, 2025. 4 [39] Jiwen Zhang, Yaqi Yu, Minghui Liao, Wentao Li, Jihao Wu, and Zhongyu Wei. Ui-hawk: Unleashing the screen stream understanding for gui agents. Preprints, 2024. 1 [40] Jiwen Zhang, Yaqi Yu, Minghui Liao, Wentao Li, Jihao Wu, and Zhongyu Wei. UI-Hawk: Unleashing the Screen Stream Understanding for GUI Agents, 2024. 2 [41] Renshan Zhang, Rui Shao, Gongwei Chen, Miao Zhang, Kaiwen Zhou, Weili Guan, and Liqiang Nie. Falcon: Resolving visual redundancy and fragmentation in high-resolution multimodal large language models via visual registers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. [42] Zhi Zhang, Srishti Yadav, Fengze Han, and Ekaterina Shutova. Cross-modal information flow in multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1978119791, 2025. 4 [43] Yijie Zhu, Rui Shao, Ziyang Liu, Jie He, Jizhihui Liu, Jiuru Wang, and Zitong Yu. H-gar: hierarchical interaction framework via goal-driven observation-action refinement for robotic manipulation. arXiv preprint arXiv:2511.17079, 2025. 2 [22] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning, 2025. arXiv:2503.21620 [cs] version: 1. 1 [23] Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. 1, 2, 7 [24] Zhihao Luo, Wentao Yan abd Jingyu Gong, Min Wang, Zhizhong Zhang, Xuhong Wang, Yuan Xie, and Xin Tan. Navimaster: Learning unified policy for gui and embodied navigation tasks. arXiv preprint arXiv:2508.02046, 2025. 2 [25] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. ReFT: Reasoning with Reinforced Fine-Tuning, 2024. arXiv:2401.08967 [cs]. 1 [26] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: largescale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. 7 [27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. pages 112, 2017. arXiv: 1707.06347. 2 [28] Rui Shao, Xiangyuan Lan, Jiawei Li, and Pong Yuen. Multiadversarial discriminative deep domain generalization for In Proceedings of the face presentation attack detection. IEEE/CVF conference on computer vision and pattern recognition, pages 1002310031, 2019. 2 [29] Rui Shao, Tianxing Wu, and Ziwei Liu. Detecting and groundIn Proceedings of ing multi-modal media manipulation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69046913, 2023. [30] Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, and Ziwei Liu. Detecting and grounding multi-modal media manipulation and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2 [31] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, 2024. arXiv:2402.03300 [cs]. 2 [32] Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, and Liqiang Nie. Mome: Mixture of multimodal experts for generalist multimodal large language models. In Advances in Neural Information Processing Systems, pages 4204842070, 2024. 2 [33] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. ReasonRFT: Reinforcement Fine-Tuning for Visual Reasoning, 2025. arXiv:2503.20752 [cs]. 1 [34] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception. arXiv preprint, 2024. arXiv:2401.16158 [cs]. 2 [35] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. OS-ATLAS: Foundation Action Model for Generalist GUI Agents. arXiv preprint, 2024. arXiv:2410.23218 [cs]. 1, 6, 7 A. Visualization of SSR curve during training During training, we monitor the execution accuracy to evaluate the learning behavior of the policy over time. As shown in Figure 8, integrating DCS into the AHC framework leads to clear improvement in training dynamics. The curve with DCS achieves both higher convergence speed and better final accuracy compared to the AHC-only baseline. This indicates that naively relying on fixed-length histories limits the models ability to generalize. In contrast, DCS adaptively determines the effective history length based on task complexity, enabling the agent to access relevant information while avoiding overfitting to noisy or irrelevant historical states. These results verify that dynamically sampled history promotes more stable optimization and enhances the agents capability to learn meaningful sequential dependencies. B. Visualization of sampling distribution in DCS. Figure 9. Evolution of the sampling distribution from uniform exploration to exponentially biased selection as training progresses. We sample the number of preserved history blocks τi {0, 1, 2} from time-dependent exponential distribution. At training step u, the sampling distribution is defined as (τi u) = (cid:1) exp (cid:0)λ(u) τi j=0 exp (cid:0)λ(u) j(cid:1) , (cid:80)N where λ(u) is linear function that increases with u. As shown in Figure 9, in the early stage of training, λ(u) 0 and the distribution is nearly uniform, encouraging random exploration. As training progresses, λ(u) gradually grows, yielding an increasingly biased exponential distribution that favors larger values of τi. This schedule smoothly shifts the sampling strategy from random selection to full-context history. We define λ(u) as function that increases from 0 to λmax within the first αT steps, gradually shifting the sampling distribution from uniform to exponentially biased: λ(u) = λmax min (cid:16) 1, (cid:17) αT (8) denotes the total number of training steps. λmax controls the final steepness of the exponential bias (larger λmax yields more strongly peaked distribution that favors larger τi), while α determines the fraction of training used for warm-up. In our implementation, we set λmax = 2, α = 1 3 . C. GUI datasets We construct our training data from the open-source AMEX dataset[2], which contains high-level GUI interaction trajectories. Preserving the original action distribution, we randomly sample 3,000 steps without applying additional filtering or cleaning procedures. AMEX AMEX is large-scale mobile GUI dataset for training and evaluating control agents, comprising >104K high-resolution screenshots from 110 Android apps with multi-level annotations. Each episode contains about 13 actions on average. Action distribution (full dataset ratio): click(start box=(x,y)) 24815 (64.11%) scroll(direction=down or up or right or left) 7628 (19.71%) finished() 2828 (7.31%) type(content=) 2419 (6.25%) press enter() 651 (1.68%) impossible() 220 (0.57%) press back() 135 (0.35%) press home() 13 (0.03%) AndroidControl AndroidControl is diverse benchmark to study data scaling for UI control, containing 15,283 demonstrations spanning 14,548 unique tasks across 833 Android apps, with both high-level and low-level human-written instructions for each task. Each episode contains about 5 actions on average. Action distribution (test split): click(start box=(x,y)) 5074 (50.81%) finished() 1543 (15.45%) scroll(direction=down or up or right or left) 1211 (12.13%) type(content=) 632 (6.33%) open app(app name=) 608 (6.09%) wait() 567 (5.68%) press back() 343 (3.43%) long press(start box=(x,y)) 9 (0.09%) GUI-Odyssey GUI-Odyssey is cross-app mobile GUI navigation dataset for multi-step workflows across apps; the paper reports 7,735 episodes over 6 devices, 6 task types, 201 apps and 1.4K app combinations. Each episode contains about 15 actions on average. Figure 8. Training accuracy curves of Hicon-Agent with and without DCS under the AHC framework. Models trained with DCS exhibit consistently higher accuracy and faster convergence, demonstrating that adaptive history sampling facilitates more effective learning. Action distribution (test split): click(start box=(x,y)) 19142 (65.05%) type(content=) 3113 (10.58%) scroll(direction=down or up or right or left) 2764 (9.39%) press home() 2233 (7.59%) finished() 1875 (6.37%) long press(start box=(x,y)) 106 (0.36%) press recent() 74 (0.25%) press back() 61 (0.21%) impossible() 58 (0.20%) AITW Android in the Wild (AITW) is large-scale dataset for Android device control using natural language instructions. The paper reports 715,000 episodes, 30,000+ unique instructions, 8 device types (Pixel 2 XL through Pixel 6), 4 Android versions, covering hundreds of apps and websites. Each episode contains about 6.5 actions on average. D. Effect of history observation and compression We first examine the impact of historical observations by comparing Qwen2.5VL-3B(2A), which uses only past actions, with Qwen2.5VL-3B(2AO), which includes both actions and observations from the past two steps. Incorporating visual history brings clear improvements across all metrics (+8.96% SR), highlighting the importance of visual context in guiding decision-making. When applying inference-only compression to the 2AO model, performance drops significantly (-4.95% SR). In contrast, our HiconAgent-3B, trained with history-aware optimization, recovers most of this loss and improves performance(+5.06 % SR) compared to the compressed baseline. It also exceeds the uncompressed 2AO baseline in SR and grounding accuracy, demonstrating more effective and efficient usage of historical context. This highlights the advantage of our training strategy in mitigating the trade-off between computational efficiency and task performance. Models Hist. Comp. Type Ground. SR FLOPs Qwen2.5VL-3B 2A 61.05 59.61 43. 13.21 Qwen2.5VL-3B 2AO 69.29 Qwen2.5VL-3B 2AO 66.56(-2.73) 61.10(-2.78) 47.34(-4.95) 25.21(-10.54) Hicon-Agent-3B 2AO 67.79(+1.23) 65.01(+3.91) 52.40(+5.06) 25.21(-10.54) 35.75 63.88 52.29 Table 6. Study on different strategies for history utilization on the AndroidControl dataset. Red indicates improvement, green indicates drop. Hist. denotes the history context format, and Comp. indicates whether history compression is applied. E. Algorithm details Algorithm 1 details the HCPO training loop. We begin with on-policy, group-wise rollouts using Dynamic Context Sampling: for each of the samples, we first construct the full input context {I, Ht, st}, then draw truncated history by sampling history length τi τ sampled from the distribution p. Given this truncated context, we sample response oi πθold( I, , st). The corresponding reward {ri} is computed and converted into group advantages {Ai}. Each sampled response is then evaluated by two forward passes of the current policy with shared parameters: (i) an uncompressed branch that uses {I, Ht, st} end-to-end; and (ii) compressed branch that mirrors the first layers and then drops history vision tokens after layer k, retaining action and other tokens to form i,c . Reusing the same {oi} isolates the effect of compression. We optimize two clipped-ratio gradient policy losses against πθ, while adding token-level consistency term that pulls the compressed distribution πθ( I, i,c , st) toward the uncompressed distribution πθ( I, , st). Teacher logits from the uncompressed branch are detached to prevent gradient flow. The final objective LHCPO preserves on-policy learning under complete history and aligns the compressed path for efficient inference. Algorithm 1 History Context-aware Policy Optimization (HCPO) Require: Policy model πθ, old policy πθold, reward model R, task instruction I, current GUI observation st, history context Ht = {(stτ , atτ ), . . . , (st1, at1)}, group size G, compression layer k, consistency weight λ , st) Sample history length τi to get Sample and select response oi πθold( I, 1: # Group rollout with DCS 2: Initialize sampling distribution 3: Build full history sequence: {I, Ht, st} 4: for = 1 to do 5: 6: 7: end for 8: Evaluate rewards {r1, . . . , rG} R({o1, . . . , oG}) 9: Compute group-wise advantages {A1, . . . , AG} 10: # Full history branch forward pass 11: for = 1 to do 12: , st)) Compute logits πθold(oi I, Compute logits πθ(oi I, Compute reference logits πθref(oi I, 13: 14: 15: end for 16: # Compressed history branch forward pass 17: for = 1 to do 18: , st)) , st) Reuse response oi and perform forward pass with partial history compression: Use uncompressed input {I, , st} in first layers Drop history vision tokens from in layers 19: 20: + 1 to get i,c , st) 21: 22: , st) πθ(oi I, Compute compressed logits πθ(oi I, i,c Compute token-level KL divergence: DKL(πθ(oi I, i,c , st)) 23: end for 24: # Compute policy-gradient loss and consistency loss 25: Compute uncompressed policy loss Lw/o comp 26: Compute compressed policy loss Lw/ comp 27: Compute consistency loss: LKL = (cid:80)G 28: Compute total loss: LHCPO = Lw/o comp + Lw/ comp + DKL() i= λLKL 29: Update model: θ θ ηθLHCPO F. Prompts for training and evaluation You are GUI agent. You are given task and your action history, with screenshots. You need to perform the next action to complete the task. You FIRST need to think based on the current image, task, and historical actions. The reasoning process MUST BE enclosed within <think> </think> tags. Then output the action, which MUST BE put in <action> </action> and MUST BE in Action Space. ## Output Format <think>...</think><action>...</action> ## Action Space click(start_box=(x,y)) type(content=) scroll(direction=down or up or right or left) press_back() press_home() press_enter() finished() ## Example: <think>The user wants to search for shoes. The current screen has search bar at the top.</think> <action>click(start_box=(x,y))</ action> Listing 1. AMEX training prompt template. You are GUI agent. You are given task and your action history, with screenshots. You need to perform the next action to complete the task. You FIRST need to think based on the current image, task, and historical actions. The reasoning process MUST BE enclosed within <think> </think> tags. Then output the action, which MUST BE put in <action> </action> and MUST BE in Action Space. ## Output Format <think>...</think><action>...</action> ## Action Space click(start_box=(x,y)) long_press(start_box=(x,y)) type(content=) scroll(direction=down or up or right or left) open_app(app_name=) press_back() press_home() wait() finished() ## Example: <think>The user wants to search for shoes. The current screen has search bar at the top.</think> <action>click(start_box=(x,y))</ action> Listing 2. AndroidControl evaluation prompt template. You are GUI agent. You are given task and your action history, with screenshots. You need to perform the next action to complete the task. You FIRST need to think based on the current image, task, and historical actions. The reasoning process MUST BE enclosed within <think> </think> tags. Then output the action, which MUST BE put in <action> </action> and MUST BE in Action Space. ## Output Format <think>...</think><action>...</action> ## Action Space click(start_box=(x,y)) long_press(start_box=(x,y)) type(content=) scroll(direction=down or up or right or left) impossible() press_back() press_home() press_recent() finished() ## Example: <think>The user wants to search for shoes. The current screen has search bar at the top.</think> <action>click(start_box=(x,y))</ action> Listing 3. GUI-Odyssey evaluation prompt template. You are GUI agent. You are given task and your action history, with screenshots. You need to perform the next action to complete the task. You FIRST need to think based on the current image, task, and historical actions. The reasoning process MUST BE enclosed within <think> </think> tags. Then output the action, which MUST BE put in <action> </action> and MUST BE in Action Space. ## Output Format <think>...</think><action>...</action> ## Action Space click(start_box=(x,y)) long_press(start_box=(x,y)) type(content=) scroll(direction=down or up or right or left) impossible() press_enter() press_back() press_home() finished() ## Example: <think>The user wants to search for shoes. The current screen has search bar at the top.</think> <action>click(start_box=(x,y))</ action> Listing 4. AITW evaluation prompt template. G. Model Behavior Across Different History Lengths To better understand how history length affects agent behavior, we provide case study comparing the base model and our HiconAgent-3B under different history lengths τ {0, 1, 2}. As shown in Figure 10, the base model performs correctly when using shorter contexts (τ = 0 or τ = 1), but fails when the history is extended to τ = 2, where the additional observations introduce distracting or misleading information, causing the model to attend to an incorrect UI element and produce the wrong action. In contrast, our model, trained with Dynamic Context Sampling, still produces the correct action when τ = 2. Since DCS exposes the agent to diverse and progressively biased history lengths during optimization, the model learns to effectively utilize extended context. This qualitative evidence supports our quantitative results, demonstrating that naively increasing history is suboptimal, whereas HCPO equips the agent with robustness across variable context windows and enables it to benefit from longer history when necessary. H. Visualization of successful trajectories To better illustrate how our proposed HCPO framework facilitates robust decision-making, we visualize several representative successful trajectories from the evaluation benchmarks as shown in Figure 11a and Figure 11b. Figure 10. Case study of model behavior under different history length context. (a) Trajectory example 1 (b) Trajectory example Figure 11. Case studies of our model on downstream GUI navigation tasks."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, Shenzhen",
        "Huawei Noahs Ark Lab"
    ]
}