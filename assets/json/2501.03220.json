{
    "paper_title": "ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking",
    "authors": [
        "Tingyang Zhang",
        "Chen Wang",
        "Zhiyang Dou",
        "Qingzhe Gao",
        "Jiahui Lei",
        "Baoquan Chen",
        "Lingjie Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we propose ProTracker, a novel framework for robust and accurate long-term dense tracking of arbitrary points in videos. The key idea of our method is incorporating probabilistic integration to refine multiple predictions from both optical flow and semantic features for robust short-term and long-term tracking. Specifically, we integrate optical flow estimations in a probabilistic manner, producing smooth and accurate trajectories by maximizing the likelihood of each prediction. To effectively re-localize challenging points that disappear and reappear due to occlusion, we further incorporate long-term feature correspondence into our flow predictions for continuous trajectory generation. Extensive experiments show that ProTracker achieves the state-of-the-art performance among unsupervised and self-supervised approaches, and even outperforms supervised methods on several benchmarks. Our code and model will be publicly available upon publication."
        },
        {
            "title": "Start",
            "content": "ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking Tingyang Zhang1,2 Chen Wang1 Zhiyang Dou1,3 Qingzhe Gao4 Jiahui Lei1 Baoquan Chen2 1University of Pennsylvania 3The University of Hong Kong Lingjie Liu1 2Peking University 4Shandong University {tyzh,chenw30,zydou,leijh,lingjie.liu}@seas.upenn.edu; gaoqingzhe97@gmail.com; baoquan@pku.edu.cn Figure 1. Visualization of tracking trajectories in various videos. Our method robustly recovers each points complete trajectory without drifting over time, even in challenging scenarios such as occlusions and multiple similar regions."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction In this paper, we propose ProTracker, novel framework for robust and accurate long-term dense tracking of arbitrary points in videos. The key idea of our method is incorporating probabilistic integration to refine multiple predictions from both optical flow and semantic features for robust short-term and long-term tracking. Specifically, we integrate optical flow estimations in probabilistic manner, producing smooth and accurate trajectories by maximizing the likelihood of each prediction. To effectively re-localize challenging points that disappear and reappear due to occlusion, we further incorporate long-term feature correspondence into our flow predictions for continuous trajectory generation. Extensive experiments show that ProTracker achieves the state-of-the-art performance among unsupervised and self-supervised approaches, and even outperforms supervised methods on several benchmarks. Our code and model will be publicly available upon publication. Project page: https://michaelszj.github.io/ protracker. Point tracking models [11, 21, 26, 3739] provide critical motion and deformation cues in scenes, thus they are essential for video analysis, especially for tasks like 4D reconstruction [22, 41, 48] and video editing [14]. The recent focus of point tracking is long-term dense tracking of any pixel in video, also known as Tracking Any Point (TAP) [11]. Existing methods can be broadly classified into two categories. 1) Supervised tracking models [9 12, 15, 21, 24, 49]. Specifically, TAP-net [11] predicts trajectories by generating heatmaps that capture the relationship between the target point and the rest of the frames, while some others [15, 21, 24, 49] iteratively refine the trajectory of the same point within temporal window. These supervised learning-based trackers have achieved promising results on existing benchmarks, but they often struggle to generalize to out-of-domain inputs, as they are typically trained on specific datasets. Some of them either disregard temporal information [11] or suffer from context drift and loss particularly during extended occlusions as they rely on sliding window techniques [15, 21, 24, 49]. 2) Self-supervised models [25, 40, 46, 47]. Based on testtime optimization, they have gained attention by leveraging the priors in foundation models trained on web-scale datasets. For instance, some methods [25, 40, 47] represent the entire scene as quasi-3D canonical volume and use 3D bijections to map local coordinates to global 3D canonical space, allowing for consistent tracking of points. However, the proxy canonical space represented by neural networks tends to be overly smooth, which limits tracking accuracy. DINO-Tracker [46] fine-tunes feature extractor and heatmap refiner using the strong semantic priors from DINOv2 to track through long-term occlusions. However, challenges arise when the features are not distinct enough or when multiple similar parts are present in the scene. In this paper, we present ProTracker for accurate and robust point tracking. The key idea of our method is bidirectional Probabilistic Integration for both optical flow predictions and long-term correspondences, inspired by Kalman Filter [20]. Specifically, we begin with removing incorrect initial predictions to reduce their negative impact on subsequent estimations with hybrid filter including an objectlevel filter [34] and geometry-aware feature filter [50]. For the remaining rough optical flow predictions, we address the inherent noise in optical flow estimates by introducing probabilistic integration method that treats each prediction as Gaussian distribution and merges them into single Gaussian distribution to identify the most likely point prediction. The integration is done in both forward and backward directions for highly accurate and robust flow estimation. However, optical flow is limited to visible objects and tends to fail when point disappears and then reappears in different location, resulting in missing segments in the trajectory. To improve performance in challenging long-term point tracking as well as the occlusion problem, we train long-term feature correspondence model and use it to identify keypoint positions across frames with discriminative features. Then, we jointly integrate flow estimation and long-term keypoints to obtain the final prediction. This combination equips the model to robustly recover trajectory segments and mitigate drift during long-term tracking. We conduct extensive experiments to evaluate our method on TAP-Vid benchmarks. Among self-supervised or non-supervised approaches, our method surpasses all previous methods across all metrics. Additionally, it demonstrates competitive performance even when compared to data-driven methods and achieves the highest accuracy in position estimation among all approaches. In summary, our contributions are as follows: We propose ProTracker, novel probabilistic integration framework that merges multiple rough predictions and significantly enhances the accuracy and robustness of point tracking. We incorporate long-term correspondence matching into our probabilistic integration framework to address both long-term tracking and occlusion, enabling precise point tracking over extended durations. Our method achieves the state-of-the-art performance among self-supervised and unsupervised approaches while demonstrating competitive results compared to data-driven methods. 2. Related Work Optical flow aims to establish dense motion estimations between consecutive frames. Classical methods [2, 3, 17, 27] optimize warp field with smoothness as regularization. Modern data-driven methods [13, 18, 19, 42, 44] learn deep neural networks to generate or refine flow predictions based on large amounts of annotated data, which has significantly improved performance. Although optical flow methods can accurately predict displacements between adjacent frames, they often fail when the displacement is too large due to biases in the training data, tending to keep points stationary. This makes optical flow unsuitable for direct longterm tracking. Even chaining flow predictions across frames can lead to drift and other issues. In our approach, we use RAFT [44] as the primary tracking tool, with the aid of additional models to perform precise point tracking. correspondence involves finding Dense pixel-level matches between an arbitrary pair of images. Correlation volumes are constructed to measure the similarity between pairs of pixels based on classic [26] or learningbased [8, 29, 35, 45] feature descriptor, and the accurate point matches are decided accordingly. Recently, large pretrained visual foundation models [5, 31, 33, 36] have shown their ability to extract powerful features and can be combined for robust matching across different scene/object appearances [7, 16, 28, 43, 50]. While directly using these correspondences for point tracking lacks accuracy [1, 46] due to the lower resolution of the features compared to the original image, they can effectively serve as filtering tool to discard incorrect predictions. Tracking any point aims to track arbitrary points across whole video, recovering the full trajectory and occlusion state. TAP-net [11] directly predicts via finding the target in refined heatmap. PIPs [15] proposes to iteratively refine the trajectory within temporal window according to the spatial context. Many attempts have been made to improve the refinement process. Co-tracker [21] counted in the relation between points and designed self-attention to support them with each other. SpatialTracker [49] lifts points to 3d space and performs tracking with spatially meaningful information. TAPTR [24] treats points as queries and updates them in DETR [4] style. Some other methods Figure 2. Pipeline overview of our proposed method. (1) Sample & Chain: Key points are initially sampled and linked through optical flow chaining to produce preliminary trajectory predictions. (2) Long-term Correspondence: Key points are re-localized over longer time spans to maintain continuity, even for points that temporarily disappear. (3) Hybrid Filter: Masks and feature filters are applied to remove incorrect predictions, reducing noise for subsequent steps. (4) Probabilistic Integration: Filtered flow predictions across frames are first integrated and then combined with long-term keypoint to produce the final prediction, producing smoother and more consistent trajectories. like TAPIR [12] and LocoTrack [10] adopt coarse-tofine strategy, dividing the tracking process into initialization and iterative optimization phases, which allows the wellinitialized points to guide the trajectory in other frames. While those supervised methods may be limited to their spatial or temporal field of view due to large memory cost, Omnimotion [47] first proposes to learn 3d representation for each video with color and pre-computed optical flow as selfsupervision, in which bijective mapping enables the query of any point in different frame. Decomotion [25] decomposes the scene representation into static and dynamic and utilizes temporal invariant feature as extra supervision. CaDeX++ [40] leverages depth estimator to speed up and more efficient deformation network. DINO-Tracker [46] trains delta feature extractor as compensation for the powerful DINO [31] feature. MFT [30] is zero-shot method that directly chains optical flow and selects the most reliable estimation as the final tracking prediction. However, problems like drift may occur when facing long videos. 3. Method Given an image sequence {I t}T t=1 from monocular video, our goal is to take query pixel pt R2 from an arbitrary frame as input and predict its trajectories { ˆpt}T t=1 over the video, along with the occlusion prediction {ˆot}T t=1, which is known as the TAP (Tracking Any Point) problem. As shown in Fig. 2, our pipeline first obtains both initial rough optical flow predictions from multiple previous frames and long-term correspondence predictions. We filter unreliable point predictions with an object-level segmentation model [34] and geometry-aware semantic features [50] (Sec. 3.1). Then, we integrate multiple optical flow predictions in probabilistic manner to get an integrated prediction from flow (Sec. 3.2). We further use joint probabilistic integration between optical flow prediction and long-term semantic correspondence prediction to prevent drifting and allow robust re-localization after reappearance (Sec. 3.3). 3.1. Hybrid Filter Since our method relies on rough predictions from optical flow and long-term correspondence for probabilistic integration (as shown in Fig. 2), inaccurate rough predictions can lead to cumulative errors and distort entire trajectories, which may significantly degrade tracking accuracy. To mitigate these issues, we propose hybrid filter to abandon these predictions and avoid using them in the following probabilistic integration. Our hybrid filter consists of an object-level filter and geometry-aware feature filter. First, an object-level segmentation model [34] generates masks associated with target points, filtering out predictions outside relevant objects and using global context to improve tracking accuracy; see ablation study in Fig.5. This step significantly benefits optical flow-based systems like RAFT [44], which often struggle with occlusions due to their reliance on local feature matching. To further reduce ambiguity between semantically similar points and prevent flickering across different regions, an additional geometry-aware feature extractor [50] is employed. For each point, if its feature similarity to the original query point falls below 0.5, the point is classified as occluded, ensuring that only reliable predictions are retained Figure 3. point may frequently disappear and reappear during tracking. When it disappears, establishing long-term correspondence enables accurate re-localization in keyframe, ensuring the point is correctly re-identified, even in challenging situations like occlusion or scene changes. After re-localization, flow integration resumes, allowing smooth and precise trajectory generation to continue seamlessly for subsequent frames. and preventing errors from propagating due to semantic ambiguity. Together, the object-level and geometry-aware filters consist of robust module, enhancing accuracy by maintaining trajectory continuity across frames and minimizing the effects of occlusions and visual ambiguities for more precise tracking in complex video scenes. 3.2. Bidirectional Probabilistic Flow Integration We introduce probabilistic integration strategy inspired by the Kalman filter [20], enabling frame-by-frame trajectory recovery for robust and accurate tracking of any point. Our method incorporates both forward and backward passes, leveraging forward and backward flows to reconstruct complete trajectories. Further details are provided below. Forward Integration Our method sequentially predicts point trajectory and occlusion on the current frame based on previous predictions. Since every estimate is subject to noise, we extend both the track predictions and the optical flow into two-dimensional Gaussian distributions. Thus, we denote the predictions of frame as (µi, Σi), where µi and Σi are the mean and covariance of the Gaussian distribution. We further assume these Gaussian distributions are isotropic and simplify the covariance matrix as Σi = σ2 I, where is the identity matrix. For the initial frame, we assume zero uncertainty (σ0 = 0). For any frame > 0, we first calculate the flow chain estimations based on previous frames {j1, j2, ..., jn}. Given the prediction for frame {j1, j2, ..., jn} (j < i), denoted as (µj, σj), and the optical flow from frame to frame denoted as (fji, σji), we can obtain the mean and variance of the prediction for frame after the filtering process in Sec. 3.1: where µji and σji are the mean and variance of the chained prediction from frame to frame i. Jfji is the Jacobian matrix of the flow fji with respect to the position µj. For simplicity, we assume Jfji = I, the identity matrix, then: ji = σ2 σ2 + σ2 fji . (3) We then combine the predictions from previous frames {j1, j2, ..., jn} to frame by assuming that they are independent. Since the product of the probability density function (PDF) of Gaussian distributions remains Gaussian, we can merge multiple predictions for frame from different previous frames, {µj} with their corresponding variances {σ2 }, into single refined estimate. The refined mean is computed as weighted linear combination of {µj}, with the weights determined by the inverse of their variances: µi = (cid:80) µji/σ2 ji (cid:80) 1/σ2 ji . (4) Similarly, the refined variance is updated according to the following formula: σ2 = 1 1/σ2 ji (cid:80) . (5) However, previous predictions are typically correlated. To account for these correlations and simplify the calculations, we introduce constant correlation coefficient between any pair of estimates from previous frames. The final refined estimates are then given by: µji = µj + fji, jiI) = Jfji(σ2 (σ2 I)J fji + (σ2 fji I), (1) (2) µf = (cid:80) µji/σ2 ji (cid:80) 1/σ2 ji , (cid:115) σf = (N 1) + 1 1/σ2 ji (cid:80) . (6) where µi represents the final predicted position for frame i, and σ2 is the combined variance, reflecting the confii dence in the estimate based on multiple sources. Following MFT [30], we adopt {, 1, 2, 4, 8, 16, 32} as the time intervals, meaning that each frames prediction is computed by combining results from these previous frames, if the target point is predicted visible in those frames. Here, refers to the first frame of the video. If all predictions from previous frames to current frame are invalid, the point is marked occluded in frame i. Once we reach the last frame and complete the forward tracking, we perform similar backward pass to recover points that might have been missed, as some points are more easily tracked from future frames because the optical flow from previous frames is no longer accurate due to long-term occlusion. Backward Integration After the forward pass, we run backward pass starting from the last frame, focusing on points previously marked as occluded. For any frame i, given the prediction for frame {j1, j2...jn} (j > i), denoted as (µj, σj), and the optical flow from frame to frame i, (fji, σji), we can obtain the mean and variance of the prediction for frame after the same filtering process: µb = (cid:80) µji/σ2 ji (cid:80) 1/σ2 ji , (cid:115) σb = (N 1) + 1 1/σ2 ji (cid:80) . (7) If point marked as occluded in the forward pass is visible in the backward pass, we adopt the backward result instead. Otherwise, we retain the forward prediction. This ensures more robust tracking, particularly in cases of occlusion. The backward pass helps recover points that are difficult to track from earlier frames but can be more easily tracked from later ones. 3.3. Joint Flow and Long-term Correspondence Integration While flow integration can partially mitigate drift and produce smooth trajectories, accumulated errors still lead to drift over longer time spans. Moreover, in cases where an object disappears and reappears after some time, the optical flow method may struggle to track the point. To tackle these issues, we propose to integrate long-term correspondence into our flow-based prediction framework. We train feature extractor Φ and heatmap refiner of long-term correspondence-based keypoint tracker based on DINO-Tracker [46] for the input video, with the optical flow as self-supervised signal. For frame i, the feature map can be calculated as: = ΦDINO(I i) + Φ(I i) (8) After getting the query feature fquery by sampling on the query point in p0, long-term predictions pi are generated by applying SoftArgMax on the refined heatmap: pi = SoftArgMax(R(fquery i)) (9) To avoid the negative impact of incorrect correspondences, we only require high-confidence keypoints. Thus, we only select those points with cosine similarity greater than threshold as keypoints. We incorporate these points into our probabilistic integration framework, as described by the following equation: µkey = pi, if i(pi) fquery > ρ, (10) where ρ = 0.7. Specifically, whenever valid keypoints from the long-term correspondence are available, we treat them as another source of noisy observations besides optical flow. In this way, we can jointly integrate the flow prediction and long-term keypoint in our probabilistic integration framework (Sec. 3.2) to yield final optimal estimation of the points location. Formally, let µi and σi represent the mean and variance from flow integration, and µkey represent the mean and variance from the key point observations. Then the combined estimates µfinal are computed as: and σkey and σfinal i µfinal = µi/σ2 1/σ2 /(σkey )2 + µkey + 1/(σkey 1 + 1/(σkey ) σfinal = 1/σ2 )2 , . (11) (12) The final prediction for the point location is given by ˆpt = µfinal , which indicates that our methods result has maximum likelihood within the final prediction distribution. If neither the optical flow nor the long-term key point is valid in the current frame, the point is marked as occluded. By incorporating long-term key points, our approach mitigates drift, effectively aligning flow estimation with longterm keypoints to maintain trajectory accuracy. Moreover, it enables the model to re-localize points that have temporarily disappeared and reappeared in different locations. As result, the flow-based predictions continue to refine the points trajectory, ensuring accurate and smooth tracking across frames, as demonstrated in Fig. 3. 4. Experiments 4.1. Experiment Setup Dataset: We evaluate our method on the following datasets from TAP-Vid [11]: DAVIS, real-world dataset comprising 30 videos from DAVIS 2017 [32]. Each video contains between 34 and 104 RGB frames, capturing both camera movements and dynamic scene motions. We employ both the queryfirst mode and the query-strided mode for evaluation on DAVIS. Figure 4. To evaluate our method, we compare it with several state-of-the-art approaches, including data-driven models such as CoTracker [21], SpatialTracker [49], LocoTrack [10], and TAPTR [24] as well as the self-supervised CaDex++ [40] and DINO-Tracker [46]. The test scenes selected from the TAPVid-DAVIS dataset include bike-packing and goat. Track predictions are conducted at resolution of 256256, while the visualizations of tracked points are displayed in the original resolution. More qualitative results are included in the supplementary material. Method Omnimotion [47] MFT [30] CaDeX++ [40] DecoMotion [25] DINOTracker [46] Ours δx avg - 66.8 - 69.9 74.9 77.6 DAVIS-First DAVIS-Strided Kinetics-First OA - 77.8 - 84.2 86.4 87. AJ - 47.3 - 53.0 58.3 62.0 δx avg OA 67.5 70.8 77.4 74.4 78.2 80.8 85.3 86.9 85.9 87.2 87.5 88. AJ 51.7 56.1 59.4 60.2 62.3 65.3 δx avg OA - 60.8 - - 69.5 71.1 - 75.6 - - 86.3 89. AJ - 39.4 - - 55.5 56.7 Table 1. Comparisons with SOTA self-supervised or unsupervised trackers on TAP-Vid benchmark. Our method consistently outperforms others in all metrics. The best results are bolded. Method δx avg 48.6 TAP-Net [11] 64.8 PIPs [15] 70.0 TAPIR [12] CoTracker [21] 75.4 SpatialTracker [49] 76.3 75.9 TAPTR [24] 75.3 LocoTrack [10] 77.6 Ours DAVIS-First DAVIS-Strided Kinetics-First OA 78.8 77.7 86.5 89.3 89.5 91.4 87.2 87.3 AJ 33.0 42.2 56.2 60.6 61.1 63.5 62.9 62.0 δx avg OA 53.4 59.4 74.7 79.2 - 78.8 79.6 80.8 81.4 82.1 89.4 89.3 - 91.3 89.9 88.7 AJ 38.4 42.0 62.8 65.1 - 66.4 67.8 65.3 δx avg OA 56.3 47.6 63.6 65.9 67.1 64.4 68.8 71.1 83.6 78.5 86.4 88.0 88.3 85.7 87.5 89.6 AJ 42.7 31.1 52.6 52.8 53.9 50.8 56.0 56.7 Table 2. Comparisons with SOTA supervised trackers on TAP-Vid benchmark. Our method achieves the highest δx avg across all datasets, while also performing competitively in OA and AJ. Notably, on the Kinetics-First dataset, which contains videos with higher number of frames, our method outperforms all others across all metrics. The best results are bolded. Kinetics, includes 1,189 videos, each with 250 frames from Kinetics-700-2020 [6]. The dataset predominantly focuses on human activity, with both camera and object motion. Since our method includes test-time optimization steps, we use the subset of 100 videos sampled by Omnimotion [47] and the query-first mode for evaluation. Metrics: In accordance with the TAP-Vid [11] benchmark, we use the following metrics: δx avg measures the percentage of visible points that are tracked within specific pixel error from the ground truth. It is evaluated over five thresholds: {1,2,4,8,16} pixels, with the final score being the average fraction of points within these distances. Occlusion Accuracy (OA) measures the fraction of points with correct visibility predictions in each frame, including both visible and occluded points. Average Jaccard (AJ) measures both position and occlusion accuracy based on δx avg thresholds. It assesses the ratio of correctly predicted visible points to false predicted points. 4.2. Comparisons Baselines We compare our method to state-of-the-art supervised feedforward trackers, including PIPs [15], TAPNet [11], TAPIR [12], Co-Tracker [21], SpatialTracker [49], LocoTrack [10] and TAPTRv2 [23], as well self-supervised trackers such as Omnimotion [47], CaDeX++ [40], DecoMotion [25] and DINO-Tracker [46]. We additionally incorporate MFT [30], which leverages RAFT to directly obtain trajectory predictions in an unsupervised manner. Quantitative comparisons Tab. 1 and Tab. 2 show the performance of our method compared to other state-of-the-art trackers on the TAP-Vid benchmark. Our method achieves the highest δx avg across all datasets, demonstrating superior precision in tracking visible points compared to all other methods. This is because our approach not only leverages probabilistic model to enhance tracking accuracy but also exploits the complementary nature of optical flow and longterm correspondence. This combination allows our method to produce smoother and more accurate trajectory segments. In terms of Occlusion Accuracy (OA) and Average Jaccard (AJ), our approach performs on par with the best methods, showing strong capability in handling occlusion and maintaining overall geometric accuracy. Notably, among unsupervised or self-supervised trackers, our method outperforms all others across all metrics, achieving the best results in δx avg, OA, and AJ. This highlights the robustness of our approach in both position and occlusion tracking, even without the need for supervision. Compared to supervised trackers, our method also performs best on Kinetics, which contains longer video sequences. Qualitative results Fig. 4 shows that our method robustly generates accurate and smooth trajectories, even in challenging scenarios where objects frequently disappear and reappear. In the bike-packing sequence, our tracking points are placed on the person, and while some other methods (CoTracker, SpatialTracker, TAPTR, CaDeX++) sometimes mis-track these points onto the bike or background, or fail to capture finer details such as the hands (DINO-Tracker, LocoTrack), our approach consistently maintains accurate tracking throughout the video. In the goat sequence, where the hooves frequently cross and obscure each other, our method tightly follows the target points, unaffected by the distraction of overlapping limbs. The robustness and accuracy demonstrated by these results are achieved through the natural combination of multiple tools within our framework. The dual-stage filter effectively prevents point predictions from drifting onto other regions when points are occluded the bike in bike-packing and different legs in goat), (e.g. while the incorporation of long-term keypoints enables reliable re-identification of targets when they appear again. Finally, our probabilistic integration framework leverages these keypoints as anchors, using optical flow to accurately locate less distinctive points, thereby ensuring smooth and accurate trajectories throughout the video. 4.3. Ablation study We perform ablation on different components of our framework on Tapvid-DAVIS. w/o keypoint indicates directly using the results from the flow integration as output without the joint integration with long-term key points. w/o geoaware removes the process of filtering by the geometryaware feature. w/o mask uses the rough flow prediction without object-level filtering. w/o probabilistic replaces the probabilistic integration by selecting the prediction of the lowest σ as the final results. As shown in Tab. 3 and Fig. 5, the implementation of mask filtering effectively eliminates incorrect flow predictions, significantly improving both precision and occlusion handling. By utilizing long-term key points, we are able to recover trajectories beyond the capabilities of optical flow alone, leading to substantial gains in all metrics. The probabilistic integration further enhance overall positional accuracy by reducing uncertainty in the tracking process. Additionally, the geometry-aware feature reduces misalignment between visually similar parts, contributing to improved accuracy in handling occlusions. 5. Conclusion and Future Work In this paper, we introduced robust tracking framework that combines optical flow integration with long-term correspondence through probabilistic integration to achieve acMethod w/o key point w/o geo-aware w/o mask w/o probabilistic Ours Full DAVIS-First DAVIS-Strided δx avg 71.2 77.6 72.3 76.9 77.6 OA 79.2 85.7 82.3 87.2 87.3 AJ 47.8 60.0 57.1 61.3 62.0 δx avg 74.6 80.8 73.9 79.0 80.8 OA 87.6 88.4 82.6 88.2 88.7 AJ 62.8 63.3 57.8 63.9 65.3 Table 3. Ablation on different components on TAP-Vid-DAVIS. The best and second best results are bolded and underlined. Figure 5. The qualitative ablation study on different components of our method. curate and smooth point tracking in dynamic video sequences. By incorporating object-level filtering, bidirectional probabilistic integration, and geometry-aware feature extraction, our method effectively mitigates drift, handles occlusions, and re-localizes temporally disappearing points. Our method outperforms traditional methods in handling complex motions and extended time gaps, demonstrating the advantages of integrating short-term and long-term information for reliable tracking. While our method provides robust tracking, its reliance on test-time training for keypoint extraction reduces efficiency compared to supervised approachesa common This limitation of self-supervised tracking methods. dependency on test-time training arises due to the current feature extractors insufficient resolution and lack of temporal awareness. Future improvements in high-resolution feature extraction could help avoid test-time training and improve differentiation between objects and regions, allowing for fully unsupervised and real-time dense tracking."
        },
        {
            "title": "References",
            "content": "[1] Gorkay Aydemir, Weidi Xie, and Fatma Guney. Can visual foundation models achieve long-term point tracking?, 2024. 2 [2] Thomas Brox, Andres Bruhn, Nils Papenberg, and Joachim Weickert. High accuracy optical flow estimation based on theory for warping. In Computer Vision-ECCV 2004: 8th European Conference on Computer Vision, Prague, Czech Republic, May 11-14, 2004. Proceedings, Part IV 8, pages 2536. Springer, 2004. 2 [3] Thomas Brox, Christoph Bregler, and Jitendra Malik. Large In 2009 IEEE Conference on displacement optical flow. Computer Vision and Pattern Recognition, pages 4148. IEEE, 2009. 2 [4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers, 2020. 2 [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 2 [6] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 62996308, 2017. [7] Xinle Cheng, Congyue Deng, Adam Harley, Yixin Zhu, and Leonidas Guibas. Zero-shot image feature consensus with arXiv preprint arXiv:2403.12038, deep functional maps. 2024. 2 [8] Seokju Cho, Sunghwan Hong, Sangryul Jeon, Yunsung Lee, Kwanghoon Sohn, and Seungryong Kim. Cats: Cost aggregation transformers for visual correspondence. Advances in Neural Information Processing Systems, 34:90119023, 2021. 2 [9] Seokju Cho, Jiahui Huang, Seungryong Kim, and JoonYoung Lee. Flowtrack: Revisiting optical flow for longrange dense tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1926819277, 2024. 1 [10] Seokju Cho, Jiahui Huang, Jisu Nam, Honggyu An, Seungryong Kim, and Joon-Young Lee. Local all-pair correspondence for point tracking. arXiv preprint arXiv:2407.15420, 2024. 3, 6, 7 [11] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. TAP-vid: benchmark for tracking any point in video. Advances in Neural Information Processing Systems, 35:1361013626, 2022. 1, 2, 5, 7 [12] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and In Proceedings of the IEEE/CVF Intemporal refinement. ternational Conference on Computer Vision, pages 10061 10072, 2023. 1, 3, 7, [13] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 27582766, 2015. 2 [14] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with interactive semantic point correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 76217630, 2024. 1 [15] Adam Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In European Conference on Computer Vision, pages 5975. Springer, 2022. 1, 2, 7 [16] Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. Advances in Neural Information Processing Systems, 36, 2024. 2 [17] Berthold KP Horn and Brian Schunck. Determining optical flow. Artificial intelligence, 17(1-3):185203, 1981. [18] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer: transformer architecture for optical flow. In European conference on computer vision, pages 668685. Springer, 2022. 2 [19] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24622470, 2017. 2 [20] Rudolph Emil Kalman. new approach to linear filtering and prediction problems. 1960. 2, 4 [21] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoarXiv preprint tracker: arXiv:2307.07635, 2023. 1, 2, 6, 7 is better to track together. It [22] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds, 2024. 1 [23] Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Feng Li, Tianhe Ren, Bohan Li, and Lei Zhang. Taptrv2: Attention-based position update improves tracking any point. arXiv preprint arXiv:2407.16291, 2024. 7 [24] Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, and Lei Zhang. Taptr: Tracking any point with transformers as detection. In Proceedings of the IEEE/CVF European Conference on Computer Vision, 2024. 1, 2, 6, 7 [25] Rui Li and Dong Liu. Decomposition betters tracking everything everywhere. arXiv preprint arXiv:2407.06531, 2024. 2, 3, [26] David G. Lowe. Distinctive image features from scaleinvariant keypoints. International Journal of Computer Vision, 60:91110, 2004. 1, 2 [27] Bruce Lucas and Takeo Kanade. An iterative image registration technique with an application to stereo vision. In IJCAI81: 7th international joint conference on Artificial intelligence, pages 674679, 1981. 2 [28] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. Advances in Neural Information Processing Systems, 36, 2024. 2 [29] Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc Pollefeys, Esa Rahtu, and Juho Kannala. Dgc-net: Dense geometric correspondence network. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 10341042. IEEE, 2019. 2 [30] Michal Neoral, Jonaˇs ˇSer`ych, and Jiˇrı Matas. Mft: Longthe term tracking of every pixel. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 68376847, 2024. 3, 5, 7, 1 In Proceedings of [31] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2, 3 [32] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. 5 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2 [34] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 2, 3 [35] Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. Efficient neighbourhood consensus networks via submanifold sparse convolutions. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IX 16, pages 605621. Springer, 2020. 2 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [37] Michael Rubinstein and Ce Liu. Towards longer long-range motion trajectories. In Proceedings of the British Machine Vision Conference, pages 53.153.11. BMVA Press, 2012. 1 [38] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. Orb: An efficient alternative to sift or surf. In 2011 International conference on computer vision, pages 2564 2571. Ieee, 2011. [39] P. Sand and S. Teller. Particle video: Long-range motion esIn 2006 IEEE Computer timation using point trajectories. Society Conference on Computer Vision and Pattern Recognition (CVPR06), pages 21952202, 2006. 1 [40] Yunzhou Song, Jiahui Lei, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Track everything everywhere fast and robustly. arXiv preprint arXiv:2403.17931, 2024. 2, 3, 6, 7 [41] Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, and Leonidas Guibas. Dynamic gaussian marbles for novel view synthesis of casual monocular videos, 2024. 1 [42] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and In Proceedings of the IEEE conference on cost volume. computer vision and pattern recognition, pages 89348943, 2018. 2 [43] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36:13631389, 2023. 2 [44] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 2, 3, [45] Prune Truong, Martin Danelljan, and Radu Timofte. Glunet: Global-local universal network for dense flow and correspondences. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 62586268, 2020. 2 [46] Narek Tumanyan, Assaf Singer, Shai Bagon, and Tali Dekel. Dino-tracker: Taming dino for self-supervised point tracking in single video, 2024. 2, 3, 5, 6, 7, 1 [47] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1979519806, 2023. 2, 3, 7 [48] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. 2024. 1 [49] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: In Proceedings of Tracking any 2d pixels in 3d space. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2040620417, 2024. 1, 2, 6, 7 [50] Junyi Zhang, Charles Herrmann, Junhwa Hur, Eric Chen, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. Telling left from right: Identifying geometry-aware semantic correIn Proceedings of the IEEE/CVF Conference spondence. on Computer Vision and Pattern Recognition, pages 3076 3085, 2024. 2, 3 ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking"
        },
        {
            "title": "Supplementary Material",
            "content": "1. Video Results Please refer to our Supplementary Webpage for the corresponding videos of images illustrated in the paper and more results on different data. 2. Implementation Details 2.1. Flow Preparation We utilize the RAFT [44] model, as adopted by MFT [30], as our flow estimation model. It takes two images, Ij and Ii, captured at different times as input and outputs the flow map ji, occlusion map Oji, and uncertainty map ji. Since the uncertainty of an estimation can also be interpreted as its variance, the initial flow prediction from frame to frame at location is computed as follows: (fji, µji) = (cid:40) (S(F ji, p), S(U ji, p)) one if(Oji, p)) > ρ otherwise (1) where ρ = 0.1, and S(T arget, p) indicates sampling the target at location p. The initial flow predictions are then set as input to flow integration. Following MFT, we adopt {, 1, 2, 4, 8, 16, 32} as the time intervals, meaning that each frames prediction is computed by combining results from these previous frames. 2.2. Filter Thresholds During the dual filtering stage, we apply different thresholds to predictions from flow and long-term keypoints. For long-term keypoints, prediction is marked as invalid if the cosine similarity to the query point on the geometry-aware feature falls below 0.5. For predictions from flow, however, we use threshold of 0.3 instead. These distinct thresholds allow flow to track featureless areas while ensuring that long-term keypoints do not drift into visually similar regions. 2.3. Outlier Removal in Integration Even after dual filtering, occasional erroneous predictions may persist. To ensure more stable integration process and minimize the impact of these incorrect predictions, we discard rough predictions that deviate significantly from others. Denoting the input predictions as {(f1, µ1), (f2, µ2), . . . , (fN , µN )}, we remove outliers using the following criterion: (cid:40) (fi, µi) None if Norm(fi fT ) < ρdist otherwise (2) (fi, µi) = where ρdist = 10 and Norm(x) measures the magnitude of vector. 3. Dense Inference As discussed in Sec.3.1 in the main paper, we utilize geometry-aware feature extractor and video mask generator for the dual-filter stage. While optical flow and geometry-aware features can be computed densely, generating masks for each pixel is both time-intensive and memoryintensive. To address this, we adopt an iterative approach to efficiently generate set of masks that collectively cover all pixels, as described below: Algorithm 1 Dense Mask Generation Algorithm 1: Initialize: Set all pixels as unassigned. 2: while there exist unassigned pixels do 3: 4: 5: 6: Select the first unassigned pixel p. Generate new mask starting from pixel p. for each pixel in do if is unassigned then Assign to mask . 7: end if 8: end for 9: 10: end while 11: Output: All pixels assigned to corresponding masks. Subsequently, the dual filter can be applied to each pixel based on its corresponding mask. 4. Training and Inference Speed The total time consumed for our method includes the time for keypoint extraction, mask generation, geometry-aware feature extraction and probabilistic integration. During keypoint extraction, we follow DINO-Tracker [46] to train delta-DINO model and heatmap refiner, which takes about 1 hour for an 80-frame video on single RTX 4090 GPU. We refer to DINO-Tracker [46] for more details. However, our method skips the time-consuming occlusion prediction and directly uses points with high cosine similarity as keypoints, which saves much time. The mask generation and geometry-aware feature extraction together takes about 2 minutes and the probabilistic integration takes about 1 minute for the same video. In total, during the inference stage, the time spent tracking 3,000 points on single object in an 80-frame video is about 3 minutes, which is about 20x faster than DINOTracker [46]. For dense inference, an additional 4 minutes may be required due to the increased number of masks generated, but our method remains more than 30x faster than DINO-Tracker [46]. 5. More Qualitative Results To further illustrate our methods robustness. We conduct experiments on more challenging cases and show the qualitative results. Some of the previous methods rely on computing heatmap between the query point and the target frame. However, the per-frame heatmap lacks temporal-awareness and may confuse different objects. We address this issue by leveraging the mask and combining the heatmap with optical flow. As illustrated in Fig. 1 and Fig. 2, by comparing the results of our method with DINO-Tracker [46] and TAPIR [12], we show that although our method also relies on per-frame heatmap to extract keypoints,our method has strong temporal-awareness and is able to tell between similar objects. To further demonstrate the robustness of our method, we conduct experiments on extended videos from TAPVid-DAVIS, simulating high frame-rate videos by repeating each frame three times, as illustrated in Fig. 3 and Fig. 4. In contrast to typical sliding-window or flow-based trackers (such as TAPTR [24], SpatialTracker [49] and CoTracker [21]), which tend to accumulate errors and drift over time, our integration of long-term key points with short-term optical flow enables continuous, drift-free tracking of the same point through occlusions. Figure 1. Results of tracking single object. While DINO-Tracker may mispredict parts onto similar objects and TAPIR can be disrupted by similar patterns, our method avoids these errors. Figure 2. Results of tracking single object. While DINO-Tracker may lose some parts and TAPIR can be disrupted by multiple similar patterns, our method avoids these errors. Figure 3. Results of tracking at higher frame rate. Sliding window based methods can easily lose track after occlusion and drift due to accumulating errors, while ours exhibit robustness. Figure 4. Results of tracking at higher frame rate. Sliding window based methods can mispredict points to other regions during occlusion (e.g. the gun and rope in shooting and the wrong person in india), while ours exhibit robustness."
        }
    ],
    "affiliations": [
        "Peking University",
        "Shandong University",
        "The University of Hong Kong",
        "University of Pennsylvania"
    ]
}