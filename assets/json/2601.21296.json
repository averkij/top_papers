{
    "paper_title": "Grounding and Enhancing Informativeness and Utility in Dataset Distillation",
    "authors": [
        "Shaobo Wang",
        "Yantai Yang",
        "Guo Chen",
        "Peiru Li",
        "Kaixin Li",
        "Yufa Zhou",
        "Zhaorun Chen",
        "Linfeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Dataset Distillation (DD) seeks to create a compact dataset from a large, real-world dataset. While recent methods often rely on heuristic approaches to balance efficiency and quality, the fundamental relationship between original and synthetic data remains underexplored. This paper revisits knowledge distillation-based dataset distillation within a solid theoretical framework. We introduce the concepts of Informativeness and Utility, capturing crucial information within a sample and essential samples in the training set, respectively. Building on these principles, we define optimal dataset distillation mathematically. We then present InfoUtil, a framework that balances informativeness and utility in synthesizing the distilled dataset. InfoUtil incorporates two key components: (1) game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples, and (2) principled utility maximization by selecting globally influential samples based on Gradient Norm. These components ensure that the distilled dataset is both informative and utility-optimized. Experiments demonstrate that our method achieves a 6.1\\% performance improvement over the previous state-of-the-art approach on ImageNet-1K dataset using ResNet-18."
        },
        {
            "title": "Start",
            "content": "Published as conference paper at ICLR"
        },
        {
            "title": "GROUNDING AND ENHANCING INFORMATIVENESS\nAND UTILITY IN DATASET DISTILLATION",
            "content": "Shaobo Wang1,2 Yantai Yang1 Guo Chen1 Zhaorun Chen5 Linfeng Zhang1,2 1EPIC Lab, SJTU 2Shanghai Jiao Tong University 4Duke University {shaobowang1009,zhanglinfeng}@sjtu.edu.cn 5The University of Chicago Peiru Li2 Kaixin Li3 Yufa Zhou4 3National University of Singapore Corresponding authors 6 2 0 J 9 2 ] . [ 1 6 9 2 1 2 . 1 0 6 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Dataset Distillation (DD) seeks to create compact dataset from large, realworld dataset. While recent methods often rely on heuristic approaches to balance efficiency and quality, the fundamental relationship between original and synthetic data remains underexplored. This paper revisits knowledge distillationbased dataset distillation within solid theoretical framework. We introduce the concepts of Informativeness and Utility, capturing crucial information within sample and essential samples in the training set, respectively. Building on these principles, we define optimal dataset distillation mathematically. We then present InfoUtil, framework that balances informativeness and utility in synthesizing the distilled dataset. InfoUtil incorporates two key components: (1) game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples, and (2) principled utility maximization by selecting globally influential samples based on Gradient Norm. These components ensure that the distilled dataset is both informative and utility-optimized. Experiments demonstrate that our method achieves 6.1% performance improvement over the previous state-of-the-art approach on ImageNet-1K dataset using ResNet-18. Figure 1: Comparison of visualization results between previous method (a) RDED (Sun et al., 2024) and (b) our InfoUtil. Unlike prior methods relying on random selection and intuitive scoring, InfoUtil is both interpretable and theoretically grounded. It synthesizes images that more accurately capture semantically meaningful regions with principled scores. Prioritizing core content over irrelevant details like background elements ensures more focused and meaningful representation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Dataset distillation (DD) (Wang et al., 2018; Sachdeva & McAuley, 2023) has emerged as promising approach for enabling vision models to achieve performance comparable to training on large datasets, but with only small set of synthetic samples. The core idea behind DD is to compress large datasets by synthesizing and optimizing smaller, representative dataset. Models trained on distilled dataset are expected to match the performance of those trained on the original, larger dataset. 1 Published as conference paper at ICLR 2026 Figure 2: InfoUtils pipeline for optimal dataset distillation involves two key steps: (i) Step 1 maximizes informativeness via the Shapley Value (a game-theoretic attribution method), retaining the most informative patches to form compressed samples. (ii) Step 2 maximizes utility by scoring these candidates with judge modelusing Gradient Norm (proven as utility upper bound)and retaining top samples. The final distilled dataset contains only the most informative, high-utility compressed samples. Image reconstruction and soft label generation phases are omitted here. Currently, two primary lines of approaches are used to tackle DD: i.e., matching-based methods (Wang et al., 2018; Zhao & Bilen, 2022; Zhao et al., 2021; Cazenavette et al., 2022; Zhou et al., 2022), which aim to align the performance between the distilled dataset and the original dataset by matching gradients, features, distributions, or trajectories, and knowledge distillation-based methods (Yin et al., 2023; Shao et al., 2024), which decouple dataset distillation into two stages. In the first stage, the real data is compressed into teacher model. In the second stage, the teacher model transfers knowledge to the distilled images through deep inversion-like methods (Yin et al., 2020). Despite their success, these existing methods face two challenges: Challenge 1: Efficiency-Performance Trade-off. Most matching-based methods require significant GPU memory and time, making them impractical for real-world applications. For bi-level matching-based methods, the key challenge lies in the trade-off between performance and efficiency (Zhao et al., 2021; Zhao & Bilen, 2021; Lee et al., 2022; Wang et al., 2024a; Guo et al., 2023; Cui et al., 2023). For example, the state-of-the-art (SOTA) trajectory matching method (Guo et al., 2023) requires more than 4 NVIDIA A100 80GB GPUs to synthesize 50 image-per-class (IPC) dataset on Tiny-ImageNet. Such high resource demands severely limit scalability of these methods, making it extremely challenging to apply to larger datasets like ImageNet-1K. For knowledge distillation-based methods, although they often perform better, the lack of solid theoretical foundation impairs their interpretability (Yin et al., 2023; Shao et al., 2024; Sun et al., 2024) and prevents principled solution. This limitation leaves practitioners with limited insight into why certain samples are selected for compression or how the distillation process relates to underlying data. Therefore, despite demonstrating impressive empirical results, they fall short in providing the transparency required for high-stakes or regulated applications. Challenge 2: Lack of Interpretability. Current methods are largely heuristic, lacking principled framework to ensure the resulting distilled datasets are interpretable. To rethink previous methods within principled framework, we reconsider the knowledge distillation-based dataset distillation process by introducing Optimal Dataset Distillation (Definition 4). The concept is built on Informativeness (Definition 1) and Utility (Definition 3) for desired distilled dataset. Intuitively, Informativeness captures essential information in each sample, while Utility reflects the importance of each sample for model training, whether included or excluded. Built on the theoretical framework, we propose InfoUtil, Informativeness and Utility-enhanced Dataset Distillation (InfoUtil), method that balances both aspects. As illustrated in Figure 2, Step 1 focuses on extracting key information from each sample, compressing it into representation that captures its most informative components. This is achieved by maximizing the game-theoretic informativeness of each sample, which we measure using the Shapley Value (Shapley et al., 1953), principled attribution method first introduced in game theory. In Step 2, we maximize the utility of each sample, which is critical for model training. This is done by measuring the gradient norm of each sample and selecting those with the highest values, ensuring that only the most valuable samples are retained. The main contributions of this work are summarized as follows: 1. We propose Optimal Dataset Distillation (Definition 4), which builds on the concepts of patchwise Informativeness and sample-wise Utility for distilled datasets. This approach addresses the lack of interpretability in existing methods by providing solid theoretical framework. 2 Published as conference paper at ICLR 2026 2. We introduce InfoUtil, novel method balancing informativeness and utility in distilled dataset synthesis. It employs game-theoretic informativeness maximization via the Shapley Value and utility maximization to retain the most informative and valuable samples using the Gradient Norm. 3. InfoUtil demonstrates outstanding performance across various models and datasets. For instance, our method yields 16% improvement in performance over the previous state-of-the-art approach on the ImageNet-100, and 6.1% improvement on ImageNet-1K."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Given dataset = {(xi, yi)}n i=1, dataset distillation (DD) aims to synthesize smaller dataset (cid:101)D = {(xj, yj)}m j=1 with n. The desired (cid:101)D should enable model to achieve comparable, even lossless, performance to one trained on D, evaluated on held-out test dataset Dtest. Specifically, for model parameterized by θ trained with cross-entropy loss ℓ, the condition is: min (cid:101)D (cid:88) (x,y)Dtest ℓ(fθD (x), y) ℓ(fθ (cid:101)D (x), y), (1) where θD denotes the fixed parameters trained on D. Crucially, θ on the synthetic dataset (cid:101)D. Consequently, the term ℓ(fθ (cid:101)D tion trajectory of θ. (cid:101)D represents the parameters trained (x), y) depends on (cid:101)D through the optimizaThis paper focuses on knowledge distillation-based DD methods, which recently showed superior performance (Yin et al., 2023; Sun et al., 2024; Shao et al., 2024). Here, Ds information is first learned by teacher model fθD , which then synthesizes (cid:101)D. notable work, RDED (Sun et al., 2024), uses random cropping to generate candidate patches, pruned via cross-entropy scoring. The final image contains multiple compressed images, each cropped and retained in prior steps. While RDED achieves high performance efficiently, it lacks principled guarantees. As Figure 1 shows, RDEDs randomly selected patches often miss key ground truth category information."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 OPTIMAL DATASET DISTILLATION To theoretically analyze the above problems, we first propose the following properties before formally defining the optimal dataset distillation mathematically. Definition 1 (Informativeness) Given an arbitrary sample and the compressed size d, the informativeness of Rd for the model fθ is defined as: I(x; fθ) := (cid:13) (cid:13)fθ(s x) fθ(x)(cid:13) (cid:13), (2) where {0, 1}d and = is d-dimensional binary mask to be optimized, is the Hadamard/element-wise product, and denotes the input with mask s. The informativeness captures the key information for given sample. Intuitively, maximizing the informativeness of sample of given compression size can be regarded as learning the best informative mask vector that maximize the similarity of the performance between the original sample and the masked sample x. Next, we introduce Gradient Flow, key concept we use to define the Utility function. Definition 2 (Gradient Flow) Let ℓt be the cross-entropy loss for the model θ(t) at iteration t. We define the gradient flow computed on mini-batch as: ℓt(fθ(t)(x), y; B) := ℓt(fθ(t) (x), y) . (3) The gradient flow ℓt(fθ(t) (x), y; B) represents the instantaneous rate of change of the loss for specific example (x, y) during training, providing continuous-time approximation of training dynamics. Unlike discrete SGD updates, which introduce noise, gradient flow offers smooth, analytical 3 Published as conference paper at ICLR 2026 framework for quantifying data importance. By leveraging this, we assess the impact of removing single data point (xi, yi) and define utility function below as dataset pruning metric. Definition 3 (Utility) Let the gradient flow ℓt be defined as in Definition 2. For data point (xi, yi) in dataset D, let be the mini-batch at iteration t; define Bi := {(xi, yi)}. We measure the importance of (xi, yi) by how much its removal changes the gradient flow over all relevant pairs: U(xi, yi; fθ(t)) := max (xj ,yj )D (cid:12) (cid:12) (cid:12) ℓt(fθ(t)(xj), yj; B) ℓt(fθ(t) (xj), yj; Bi) (cid:12) (cid:12) (cid:12) . This utility definition captures the worst-case impact of removing data point on gradient flow, ensuring it reflects data importance. By maximizing the change in ℓt(fθ(t)(xj), yj; B) over all (xj, yj) D, it identifies points that most influence training dynamics. This aligns with dataset pruning by preserving critical samples while discarding those with minimal effect. Based on Definition 1 and Definition 3, we propose the optimal dataset distillation in Definition 4: Definition 4 (Optimal Dataset Distillation) Let fθ be the classifier model with parameter θ and the original training dataset. Let Dtest be the test dataset. Define as compressed subset, and (cid:101)D as the final distilled dataset. Let U(x, y; θ on test example (x, y) defined in Definition 3. Let I(x; fθ) measure the informativeness of original samples defined in Definition 1 and be the informative mask with compressed size d. The goal is to find the optimal pruned dataset (cid:101)D that maximizes both informativeness and utility on Dtest: θ) measure the utility of arg max (cid:101)DD (cid:101)D=m (cid:88) (x,y)Dtest U(x, y; θ), s.t. = xi si (cid:40) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:41)n arg max si{0,1}d si=d I(xi; fθ) . i= This formulation establishes the dataset distillation problem. The key challenge is then to define rigorous utility function that effectively quantifies (i) the importance of each component within sample for model prediction and also (ii) the importance of each sample for model training. 3.2 INFOUTIL In this subsection, we introduce InfoUtil, built upon the optimal dataset distillation formulation in Definition 4. The pipeline has two main steps: (i) game-theoretic informativeness maximization and (ii) principled utility maximization. Detailed algorithm pseudocode is in Appendix B. 3.2.1 GAME-THEORETIC INFORMATIVENESS MAXIMIZATION As in Definition 1, InfoUtil is to maximize the informativeness of each sample to obtain compressed sample x, represented by mask s. This task can be framed as feature attribution problem (Zhou et al., 2016; Selvaraju et al., 2020; Binder et al., 2016; Shapley et al., 1953; Qin et al., 2023), where the model attributes decisions to input variables based on their importance. Among attribution methods, the Shapley Value (Shapley et al., 1953) is regarded as robust approach grounded in game theory. Specifically, given an input with input variables = [x(1), x(2), . . . , x(d)], we can view deep neural network as game with players [d] := {1, 2, . . . , d}. Each player corresponds to an input variable x(i). Thus, the task of fairly assigning the reward in the game translates to fairly estimating attributions of input variables in the deep neural network . Formally, the Shapley value ϕ can be defined as: ϕf (x(i)) = 1 (cid:88) s:si=0 (cid:19) (cid:18)d 1 1s (f (x (s + ei)) (x s)) , (4) where ei Rd denotes the vector with one in the i-th position but zeros in the rest positions. Notably, the Shapley Value is renowned for satisfying four key axioms (Young, 1985): For detailed technical derivations, including the complete proof, please refer to Appendix E. Published as conference paper at ICLR 2026 Axiom 1 (Linearity. Proof in Appendix E.1) If two games can be merged into new game, then the Shapley Values in the two original games can also be merged. Formally, if fmerged = f1 + f2, then ϕfmerged (x(i)) = ϕf1 (x(i)) + ϕf2(x(i)), [d]. Axiom 2 (Dummy. Proof in Appendix E.2) dummy player is player that has no interactions with other players in the game . Formally, if : si = 0, (x (s + ei)) = (x s) + (x ei). Then, the dummy players Shapley Value is computed as (x ei). Axiom 3 (Symmetry. Proof in Appendix E.3) If two players contribute equally in every case, then their Shapley values in the game will be equal. Formally, if : si = sj = 0, (x (s + ei)) = (x (s + ej)), then ϕf (x(i)) = ϕf (x(j)). Axiom 4 (Efficiency. Proof in Appendix E.4) The total reward of the game is equal to the sum of the Shapley values of all players. Formally, (x) (0) = (cid:80) i[d] ϕf (x(i)). The Shapley value is the unique attribution method that satisfies the four key axioms (Young, 1985). However, directly computing the Shapley value is computationally expensive in practice. For instance, calculating the Shapley value for an image with 4 4 patches requires 216 inferences, assuming each patch is player. To address this issue, prior works (Charnes et al., 1988; Lundberg & Lee, 2017) have proposed using kernel-based estimation of the Shapley value, as follows: ϕ = arg min ϕ (cid:20)(cid:18) Esq(s) (x s) (0) sϕ (cid:19)2(cid:21) , s.t. 1ϕ = (x) (0), (5) , 1 < 1s < denotes the Shapley Kernel. We where q(s) = (d 1)/ follow KernelShap (Lundberg & Lee, 2017) to achieve fast estimation of the Shapley value based on Eq. (5), making it possible to be adept in practice. (cid:16)(cid:0) 1s (cid:17) (cid:1)(1s)(d 1s) After obtaining the Shapley value ϕf (x(i)) of each sample x(i), we apply average pooling of the Shapley value map ϕf (x) = [ϕf (x(1)), ϕf (x(2)), . . . , ϕf (x(d))] to obtain the most informative region inside image. This step would generate < size compressed image (e.g., = d/4) with the maximized informativeness, resulting compressed dataset with compressed samples D. Diversity control. The Shapley value attribution typically identifies only the most informative patch. To introduce diversity in the patch selection process, we incorporate random noise ε (0, σ2), where σ is the standard deviation fixed. Specifically, the random noise is employed on the average pooled attribution heatmap, resulting in diverse informative patches considered in the next phase. 3.2.2 PRINCIPLED UTILITY MAXIMIZATION After obtaining the compressed dataset, the next step is selecting samples to maximize dataset utility. Computing utility (Definition 3) is challenging, as it requires training models with and without each sample to assess its utility. We show the utility function can be upper-bounded by the gradient norm (Theorem 1), simplifying computation. We now define the gradient norm. Definition 5 (Gradient Norm) The gradient norm of training example (x, y) for model parameterized by θ(t) at time is denoted as θ(t) ℓt(fθ(t)(x), y). Given the definition of Gradient Norm, we then show that Utility can be upper bounded by the gradient norm through detailed analysis here. Theorem 1 (Utility is bounded by Gradient Norm. Proof in Appendix F) Let tion be defined as in Definition 3. Then there exists constant > 0 such that the utility funcU(xi, yi; fθ(t)) cθ(t) ℓt(fθ(t)(xi), yi). Proof of Theorem 1.For detailed technical derivations, including the complete proof of Theorem 1 and auxiliary lemmas, please refer to the supplementary materials. The full proof includes step-bystep expansions of gradient flow decompositions, rigorous bounds under SGD updates, and verification of assumptions underlying the utility-gradient norm relationship. Published as conference paper at ICLR 2026 Table 1: Performance comparison between InfoUtil and SOTA methods on seven datasets. We evaluate dataset distillation using ResNet-18, ResNet-101, and ConvNet, reporting top-1 accuracy (%).Datasets were distilled with ResNet-18 and ConvNet, then evaluated on matching architectures. Additionally, datasets distilled by ResNet-18 were also evaluated with ResNet-101. Dataset IPC ResNet-18 ResNetConvNet SRe2L RDED InfoUtil SRe2L RDED InfoUtil MTT IDM TESLA DATM RDED InfoUtil CIFAR-10 16.60.9 22.90.4 25.30.4 13.70.2 18.70.1 19.60.6 46.30.8 45.60.7 48.50.8 46.90.5 23.50.3 28.51.4 1 10 29.30.5 37.10.3 53.80.1 24.30.6 33.70.3 38.41.0 65.30.7 58.60.1 66.40.8 66.80.2 50.20.3 54.10.5 50 45.00.7 62.10.1 71.00.8 34.90.1 51.60.4 67.10.5 71.60.2 67.50.1 72.60.7 76.10.3 68.40.1 69.80.1 6.60.2 11.00.3 22.90.4 6.20.0 10.80.1 16.50.5 24.30.3 20.10.3 24.80.5 27.90.2 19.60.3 33.10.3 1 10 27.00.4 42.60.2 47.50.7 30.70.3 41.10.2 41.90.6 40.10.4 45.10.1 41.70.3 47.20.4 48.10.3 50.50.3 50 50.20.4 62.60.1 64.70.2 56.90.1 63.40.3 66.00.2 47.70.2 50.00.2 47.90.3 55.00.2 57.00.1 57.80.2 CIFAR-100 ImageNette ImageWoof Tiny-ImageNet 19.11.1 35.81.0 43.80.7 15.80.6 25.12.7 28.20.5 47.70.9 1 10 29.43.0 61.40.4 68.60.6 23.40.8 54.00.4 59.81.1 63.01.3 50 40.90.3 80.40.4 86.20.6 36.50.7 75.01.2 82.40.3 - 13.30.5 20.81.2 25.00.8 13.40.1 19.61.8 20.20.4 28.60.8 1 10 20.20.2 38.52.1 51.42.5 17.70.9 31.31.3 42.61.2 35.81.8 50 23.30.3 68.50.7 69.60.8 21.20.2 59.10.7 67.20.8 - - - - - - - 2.60.1 9.70.4 17.01.3 1.90.1 3.80.1 11.90.6 8.80.3 10.10.2 1 10 16.10.2 41.90.2 45.60.3 14.61.1 22.93.3 34.40.2 23.20.2 21.90.3 50 41.10.4 58.20.1 58.50.3 42.50.2 41.20.4 54.70.3 28.00.3 27.70.3 ImageNet-100 8.10.3 15.70.2 2.10. 6.10.8 11.40.2 3.00.3 1 9.50.4 36.00.3 50.50.4 6.40.1 33.90.1 49.90.4 10 50 27.00.4 61.60.1 68.30.4 25.70.3 66.00.6 69.70.4 ImageNet-1K 0.10.1 6.60.2 12.80.7 0.60.1 6.80.7 1 10 21.30.6 42.00.1 44.20.4 30.90.1 48.31.0 51.40.3 50 46.80.2 56.50.1 58.00.3 60.80.5 61.20.4 63.80.6 5.90. - - - - - - 11.20.5 17.10.6 26.30.4 - - - 7.70.2 17.81.3 27.91.2 - - - - - - - - - - - - - - - - - - 33.80.8 42.30.7 63.20.7 66.60.4 83.80.2 84.40. 18.50.9 22.80.4 40.62.0 43.81.3 61.50.3 62.60.4 17.10.3 12.00.1 19.60.5 31.10.3 39.60.1 40.20.3 39.70.3 47.60.2 48.00.5 - - - - - - 7.10.2 15.00.8 29.60.1 42.20.7 50.20.2 60.80.9 6.40.1 6.60.3 20.40.1 21.50.3 38.40.2 40.20. Given Theorem 1, we can efficiently calculate the utility of each sample using the upper bound of the gradient norm. Then, we can directly select the most influential samples with the highest gradient norms to maximize utility. Specifically, we employ gradient norm scoring for all compressed samples in with size n, and selected samples with top norm scores, resulting (cid:101)D with size n. Image Reconstruction. Following prior works (Yin et al., 2023; Sun et al., 2024; Shao et al., 2024), we reconstruct normal-sized images by combining compressed samples. Low-resolution datasets use single image per category, while high-resolution datasets merge four 1/4-resolution images from the same category into one full-size image. For soft label generation, patch-specific logits are assigned by resizing the compressed samples. Inspired by (Qin et al., 2024; Wang et al., 2024b), intermediate checkpoints of pretrained model are used to balance discriminativity and diversity, improving performance. Further details are in Section 5."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETTINGS Datasets and network architectures. We evaluated our approach using widely recognized datasets. For lower-resolution datasets, we employed CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009) (32 32) and Tiny-ImageNet (Deng et al., 2009) (64 64). For higher-resolution experiments, we used ImageNet-1K (Deng et al., 2009) (224 224) along with three commonly used ImageNet subsets: ImageNette, ImageWoof, and ImageNet-100 (all at 224 224). In line with previous works on dataset distillation, we adept the following backbone architectures: ConvNet (Liu et al., 2022), ResNet-18, 50, 101 (He et al., 2016), MobileNet-V2 (Howard et al., 2019), VGG-11 (Simonyan & Zisserman, 2014), and Swin-V2-Tiny (Liu et al., 2021). Specifically, dataset distillation is performed using 3-layer ConvNet for CIFAR-10/100, 4-layer ConvNet for Tiny-ImageNet and ImageNet1K, 5-layer ConvNet for ImageWoof and ImageNette, and 6-layer ConvNet for ImageNet-100. Baseline methods. Following previous studies, we assessed the quality of the condensed datasets by training neural networks from scratch using them. We reported the resulting test accuracies on the actual validation sets. Baseline include trajectory-matching approaches such as MTT (Cazenavette 6 Published as conference paper at ICLR 2026 Figure 3: Performance comparison on ResNet-18 and MobileNet. (a) Time cost in seconds (lower is better): TB denotes training-based methods (TESLA and SRe2L fall into this category); TF denotes training-free methods (others belong to this type). (b) Peak memory in GB (lower is better): InfoUtil performs competitively with far lower costs than training-based methods. Info denotes Informativeness only, while Util denotes Utility only. et al., 2022), TESLA (Cui et al., 2023), and DATM (Guo et al., 2023), and distribution-matching methods like IDM (Zhao et al., 2023). For our primary comparison, we also include SOTA knowledge distillation-based methods, SRe2L (Yin et al., 2023) and RDED (Sun et al., 2024). Implementation details of InfoUtil. Our setup follows RDED, using pretrained networks for dataset synthesis. For small IPC, we adopt the approach in (Qin et al., 2024), extracting trainingstage soft labels to capture rich semantics. For larger IPC, fully converged networks from RDED are used. Details are in Appendix B. For low-resolution datasets, one synthetic image per class is used, while high-resolution datasets use four per class. The 300-image subset matches RDEDs configuration. As in Table 1, AutoAug (Cubuk et al., 2018) is applied to enhance synthetic dataset performance. All experiments ran on single NVIDIA A100 GPU. 4.2 MAIN RESULTS We verified InfoUtils effectiveness on benchmark datasets across image-per-class (IPC) settings. Higher-resolution datasets. We benchmarked InfoUtil against state-of-the-art methods on higherresolution datasets like ImageNet-1K and its subsets. As Table 1 shows, InfoUtil achieves superior or comparable performance across IPC settings. Notably, on ImageNet-100 (ResNet-101, IPC=10), it outperforms RDED (Sun et al., 2024) by 16% in accuracy; on ImageWoof (ResNet-18, IPC=10), it gains 12.9% over RDED. Moreover, on ImageNet-1K (ResNet-18, IPC=1), InfoUtil surpasses RDED by 6.1%, highlighting its effectiveness in small IPC scenarios. CIFAR-10/100 and Tiny-ImageNet. We evaluated InfoUtil on lower-resolution datasets with additional experiments on CIFAR-10/100 and Tiny-ImageNet. Our method continues to show superior performance across most scenarios, highlighting robustness and generalizability of InfoUtil. Specifically, as in Table 1, on Tiny-ImageNet, using ResNet-101 at IPC = 50 yields 13.5% improvement; on CIFAR-10, ResNet-18 at IPC = 10 obtains 16.7% improvement. Cross-architecture generalization. We evaluated InfoUtils cross-architecture generalization across ResNet-18/50 (He et al., 2016), VGG-11 (Simonyan & Zisserman, 2014), MobileNetV2 (Howard et al., 2019), and Swin-V2-Tiny (Liu et al., 2021). Table 2 shows InfoUtil outperforms SOTA (SRe2L, RDED) by 10% in the VGG-11 (teacher) vs. Swin-V2-Tiny (student) setting, confirming versatility. Further validation with baselines SCDD, G-VBSM (structural regularization) and D3S (data efficiency) on ImageNet-1K across ResNet-18/101  (Table 3)  shows InfoUtil consistently outperforms SRe2L/RDED and these baselines across all IPC settings. Efficiency Analysis. We carefully measured InfoUtils runtime and GPU usage on single NVIDIA A100. (i) It is highly efficient: time is 50 lower and memory 100 smaller than TESLA across all distillation stages (Figure 3). (ii) For large-scale datasets like ImageNet-21K, distillation com7 Published as conference paper at ICLR 2026 Table 2: Cross-architecture performance (%) on ImageNet-1K (IPC=10). Using ResNet18/50, VGG-11, MobileNet-V2, and Swin-V2Tiny as teachers; ResNet-18, MobileNet-V2, and Swin-V2-Tiny as students. SqueezedEvaluation ResNet-18 MobileNet-V2 Swin-V2-Tiny Table 3: Cross-architecture comparison of InfoUtil with additional baselines on ImageNet-1K. Results are shown across ResNet-18 and ResNet-101 architectures under varied IPC settings. Model ResNet-18 ResNet-101 IPC 10 50 1 10 50 ResNetResNet-50 MobileNet-V2 VGG-11 Swin-V2-Tiny SRe2L 21.70.6 RDED 42.30.6 InfoUtil 44.80.4 - SRe2L RDED 33.90.5 InfoUtil 34.71.4 SRe2L 19.70.1 RDED 34.40.2 InfoUtil 39.20.3 SRe2L 16.50.1 RDED 22.70.1 InfoUtil 35.10.3 SRe2L 9.60.3 RDED 17.80.1 InfoUtil 18.40.4 15.40.2 40.40.1 37.10.5 - 26.00.3 28.10. 10.22.6 33.80.8 35.50.5 10.60.1 21.60.2 31.60.1 7.40.1 18.10.2 19.70.4 - 17.20.2 19.80.4 - 17.30.2 15.60.4 - 11.80.3 20.60. - 7.80.1 17.80.4 - 12.10.2 16.40.3 - - SRe2L 0.10.1 21.30.6 46.80.2 0.60.1 30.90.1 60.80.5 39.60.4 61.00.3 SCDD 38.20.4 61.00.4 G-VBSM 5.30.1 37.20.3 50.30.3 3.20.8 42.31.7 60.60.2 D3S RDED 6.60.2 42.00.1 56.50.1 5.90.4 48.31.0 61.20.4 InfoUtil 12.70.7 44.20.4 58.00.3 6.80.7 51.40.3 63.70.6 32.10.2 53.10.1 31.40.5 51.80.4 - - Table 4: Comparison of downstream tasks for distilled samples in 5-step continual learning. Higher values indicate better performance. Method Stage 1 Stage 2 Stage 3 Stage Stage 5 RDED InfoUtil 0.5153 0.6560 0.2918 0.5659 0.201 0.4927 0.1967 0. 0.2191 0.4739 Table 5: Comparison with baseline methods under large IPC settings. We used ResNet-18 for dataset synthesis on Tiny-ImageNet and ImageNet-1K, and evaluated on ResNet-18 and ResNet-50 models. Note that TESLA (Cui et al., 2023) used the downsampled ImageNet-1K dataset. Dataset IPC TESLA (R18) SRe2L (R18) RDED (R18) InfoUtil (R18) SRe2L (R50) InfoUtil (R50) Tiny-ImageNet ImageNet-1K 50 100 200 10 50 100 - - - 17.81.3 27.91.2 - - 41.10.4 49.70.3 51.20.6 21.30.6 46.80.2 52.80.3 57.00.4 58.20.1 59.90.4 61.50.3 42.00.1 56.50.1 58.20.6 62.50. 58.50.3 60.60.5 62.00.3 43.50.4 57.60.3 58.80.4 63.40.3 42.20.5 51.20.4 - 28.40.1 55.60.3 61.00.4 64.60.3 48.30.4 53.70.4 58.00.3 48.00.5 63.10.4 65.50.5 68.00. pletes in just 5.83 hours. This combination of remarkable efficiency and strong performance makes InfoUtil practical, scalable solution for modern dataset distillation. Performance on large IPC settings. We tested Tiny-ImageNet and ImageNet-1K under large IPC scenarios, comparing with bi-level Tesla (Cui et al., 2023) and uni-level SRe2L (Yin et al., 2023), RDED (Sun et al., 2024). Table 5 shows our method significantly outperforms existing SOTA in large IPC cases, demonstrating strong scalability and superior performance. For IPC=200 on ImageNet-1K, we used full images (not 22 cropped patches as prior work) to mitigate imbalance (following (Sun et al., 2024)); image count before scoring was 600 instead of 300. Downstream tasks of distilled samples. We explored the effectiveness of distilled samples in downstream tasks via experiments on ImageNette (50 IPC) with 5-step continual learning, where new classes are incrementally introduced at each stage. To ensure the robustness of results, experiments were repeated 5 times with varied class orders. As shown in Table 4, our method (InfoUtil) consistently surpasses the SOTA method RDED across all stages. Visualization. InfoUtil shows significant improvements in visual quality over existing methods. First, vs. optimization-based methods like SRe2L (Yin et al., 2023), it produces more realistic representations by preserving intricate details and maintaining natural color fidelity. Second, vs. optimization-free methods like RDED (Sun et al., 2024), InfoUtil is more interpretable and principled, effectively capturing key informative semantic content while minimizing focus on irrelevant regions. Due to space constraints, visualization images are provided in Appendix G. 4.3 ABLATION STUDIES To analyze the individual contributions of InfoUtils components, we conducted comprehensive ablation studies comparing three configurations: (1) the baseline RDED method (Rand. Crop + Loss Scoring), (2) Utility Maximization alone (GradN Scoring), and (3) the complete InfoUtil (GradN Scoring + Attri. Cropping). Results across multiple datasets (ImageWoof, ImageNette, ImageNet1K) with varying IPC values are presented in Table 6. 8 Published as conference paper at ICLR 2026 Methods ImageWoof ImageNette ImageNet-1K GradNorm Scoring Attribution Cropping IPC=1 IPC= IPC=50 IPC=10 38.5 43.6 45.2 68.5 68.8 69. 80.4 85.0 86.2 42.0 43.5 44.2 Table 6: Ablation study of InfoUtil components impact on image classification. Top1 accuracy (%) on ResNet-18 across datasets are reported. Figure 4: Analysis of teacher networks for soft label generation. ConvNet performance on ImageWoof using labels from five training stages (IPC=1/10). Full denotes pretrained teacher. (a) IPC=1: Early high-entropy labels beat full model, aiding low-data scenarios. (b) IPC=10: Full models low-entropy labels excel in data-rich conditions. Table 7: Ablation Study on Noise Injection: Top-1 Accuracy across various settings. Dataset IPC ImageNette Standard w/o Noise ImageWoof Standard w/o Noise ImageNet-100 Standard w/o Noise ImageNet-1K Standard w/o Noise 1 10 50 43.8 68.6 86. 35.4 59.8 70.6 25.0 51.4 69.6 23.2 40.0 59.4 15.7 50.5 68.3 12.6 43.8 56.3 12.8 44.2 58. 9.6 38.5 48.3 Table 8: Empirical Comparison of Attribution Methods: Shapley Value vs. Grad-CAM on ImageNet-1K (ResNet-18 Student Model). Model Dataset IPC Grad-CAM Shapley (Ours) ResNetImageNet-1K 1 10 50 4.418 30.394 52.610 7.154 43.880 56.920 Effect of Utility Maximization. Replacing Loss Scoring with GradN Scoring while maintaining random cropping brings significant performance improvements. As shown in Table 6, Utility Maximization alone achieves 4.6% performance boost on ImageNette (IPC=50, from 80.4% to 85.0%) and 1.5% improvement on ImageNet-1K (IPC=10, from 42.0% to 43.5%). These results demonstrate that gradient norm-based scoring plays crucial role in selecting more informative samples. Effect of Combined Components. The integration of both Utility Maximization and Informativeness Maximization through Attri. Cropping yields the best performance. InfoUtil achieves additional gains of 1.2% on ImageNette (reaching 86.2%) and 0.7% on ImageNet-1K (reaching 44.2%) compared to using Utility Maximization alone. This synergistic combination demonstrates that attribute-guided cropping effectively captures the most discriminative regions while gradient-based scoring ensures the selection of pedagogically valuable samples, together producing high-quality synthetic data that consistently outperforms the baseline across all experimental settings. Effect of Noise Injection. We investigate the role of noise injection in attribution-guided patch selection, which is essential for maintaining synthetic data diversity. Without noise, the selection process becomes deterministic and greedy, leading to redundant synthesis that hinders model generalization. As shown in Table 7, removing noise (w/o Noise) results in significant performance degradation across all settings. Notably, the performance gap exceeds 15% at higher IPC (e.g., 86.2% vs. 70.6% on ImageNette at IPC=50), confirming that noise-induced diversity is critical for effective feature space coverage. Even at IPC=1, the consistent gains suggest that noise helps identify robust central features rather than brittle local maxima in the attribution map. Attribution Method: Shapley vs. Grad-CAM. We justify the selection of Shapley Value over Grad-CAM by comparing their theoretical rigor and empirical performance. Unlike heuristic gradient-based methods like Grad-CAM, which often suffer from gradient saturation, the Shapley Value is the unique attribution method satisfying fundamental axioms such as Efficiency and Symmetry. This theoretical robustness ensures fair distribution of Informativeness, accurately capturing the marginal contribution of each patch. Empirical results in Table 8 confirm this advantage, with Shapley-based selection consistently outperforming Grad-CAM on ImageNet-1K. Notably, at IPC=10, Shapley achieves 43.88%, surpassing Grad-CAM by 13.49%, demonstrating its superior ability to identify semantically robust patches for effective distillation. 9 Published as conference paper at ICLR"
        },
        {
            "title": "5 DISCUSSION",
            "content": "Soft labels encode richer probabilistic supervision in dataset distillation. Prior works (Guo et al., 2023; Yin et al., 2023; Wang et al., 2024b; Qin et al., 2024; Sun et al., 2024) show they capture inter-class relationships. (Qin et al., 2024) finds early high-entropy labels help low-data regimes, while late low-entropy labels suit data-rich settings. (Wang et al., 2024b) notes effective labels balance diversity and discriminability. However, these focus on matching-based distillation, leaving knowledge-distillation-based DD with soft labels unexplored. To investigate this further, we explored the effectiveness of teacher model for soft label generation using ConvNet on ImageWoof. For small IPC settings, we extracted soft labels from models at an intermediate training stage (10-th epoch), leveraging the high-entropy, diverse information characteristic of early epochs. In contrast, for large IPC settings, we used fully pretrained networks from RDED, leveraging the low-entropy, precise labels typical of later training phases. Our findings, as it shown in Figure 4, clearly highlight the effectiveness of this strategy. In small IPC scenarios (e.g., IPC = 1), synthetic images with soft labels generated with models at 10-th epoch outperformed those from pretrained networks, emphasizing the importance of rich label information when limited data are provided. Conversely, in larger IPC scenarios (e.g., IPC = 10 or IPC = 50), labels from fully pretrained networks yielded superior results."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we present principled approach to dataset distillation, grounded in rigorous theoretical framework for modeling optimal distillation. We introduce Informativeness and Utility, capturing, the critical information within sample and essential samples for effective training. Building on these, we propose InfoUtil, framework that synergistically combines game-theoretic informativeness maximization with principled utility maximization. Specifically, InfoUtil leverages Shapley value attribution to extract informative features and employs gradient norm-based optimization to select samples optimized for utility. InfoUtil demonstrates superior performance in dataset distillation and cross-architecture generalization. Future work includes extending InfoUtil to more complex and diverse datasets, focusing on scalability and robustness in real-world applications."
        },
        {
            "title": "REFERENCES",
            "content": "Alexander Binder, Gregoire Montavon, Sebastian Lapuschkin, Klaus-Robert Muller, and Wojciech Samek. Layer-wise relevance propagation for neural networks with local renormalization layers. In Artificial Neural Networks and Machine LearningICANN 2016: 25th International Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II 25, pp. 6371. Springer, 2016. George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei Efros, and Jun-Yan Zhu. Dataset distillation In Proceedings of the IEEE/CVF Conference on Computer Vision and by matching training trajectories. Pattern Recognition, pp. 47504759, 2022. Charnes, Golany, Keane, and Rousseau. Extremal principle solutions of games in characteristic function form: core, chebychev and shapley value generalizations. Econometrics of planning and efficiency, pp. 123133, 1988. Ekin Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling up dataset distillation to imagenet-1k with constant memory. In International Conference on Machine Learning, pp. 65656590. PMLR, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255, 2009. doi: 10.1109/CVPR.2009.5206848. Jiawei Du, Yidi Jiang, Vincent YF Tan, Joey Tianyi Zhou, and Haizhou Li. Minimizing the accumulated trajectory error to improve dataset distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 37493758, 2023. 10 Published as conference paper at ICLR Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You. Towards lossless dataset distillation via difficulty-aligned trajectory matching. arXiv preprint arXiv:2310.05773, 2023. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 13141324, 2019. Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, and Hyun Oh Song. Dataset condensation via efficient synthetic-data parameterization. In International Conference on Machine Learning, pp. 1110211118. PMLR, 2022. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo Yun, and Sungroh Yoon. Dataset condensation with contrastive signals. In International Conference on Machine Learning, pp. 1235212364. PMLR, 2022. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1001210022, 2021. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1197611986, 2022. Scott Lundberg and Su-In Lee. unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017. Yue Min, Shaobo Wang, Jiaze Li, Tianle Niu, Junxin Fan, Yongliang Miao, Lijin Yang, and Linfeng Zhang. Imagebinddc: Compressing multimodal data with imagebind-based condensation. Annual AAAI Conference on Artificial Intelligence, 2026. Dong Qin, George Amariucai, Daji Qiao, Yong Guan, and Shen Fu. comprehensive and reliable feature attribution method: Double-sided remove and reconstruct (dorar), 2023. Tian Qin, Zhiwei Deng, and David Alvarez-Melis. label is worth thousand images in dataset distillation. arXiv preprint arXiv:2406.10485, 2024. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. why should trust you? explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 11351144, 2016. Noveen Sachdeva and Julian McAuley. Data distillation: survey. arXiv preprint arXiv:2301.04272, 2023. Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: visual explanations from deep networks via gradient-based localization. International journal of computer vision, 128:336359, 2020. Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang, and Zhiqiang Shen. Generalized large-scale data condensation via various backbone and statistical matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1670916718, 2024. Lloyd Shapley et al. value for n-person games. 1953. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. On the diversity and realism of distilled dataset: An efficient dataset distillation paradigm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 93909399, 2024. Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. Shaobo Wang, Yantai Yang, Qilong Wang, Kaixin Li, Linfeng Zhang, and Junchi Yan. Not all samples arXiv preprint should be utilized equally: Towards understanding and improving dataset distillation. arXiv:2408.12483, 2024a. 11 Published as conference paper at ICLR Shaobo Wang, Yantai Yang, Shuaiyu Zhang, Chenghao Sun, Weiya Li, Xuming Hu, and Linfeng Zhang. Drupi: Dataset reduction using privileged information. arXiv preprint arXiv:2410.01611, 2024b. Shaobo Wang, Tianle Niu, Runkang Yang, Deshan Liu, Xu He, Zichen Wen, Conghui He, Xuming Hu, and Linfeng Zhang. Videocompressa: Data-efficient video understanding via joint temporal compression and spatial reconstruction. arXiv preprint arXiv:2511.18831, 2025a. Shaobo Wang, Hongxuan Tang, Mingyang Wang, Hongrui Zhang, Xuyang Liu, Weiya Li, Xuming Hu, and Linfeng Zhang. Gnothi seauton: Empowering faithful self-interpretability in black-box transformers. In The Thirteenth International Conference on Learning Representations, 2025b. Shaobo Wang, Yicun Yang, Zhiyuan Liu, Chenghao Sun, Xuming Hu, Conghui He, and Linfeng Zhang. Dataset In Proceedings of the Computer distillation with neural characteristic function: minmax perspective. Vision and Pattern Recognition Conference, pp. 2557025580, 2025c. Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018. Max Welling. Herding dynamical weights to learn. In Proceedings of the 26th annual international conference on machine learning, pp. 11211128, 2009. Hongxu Yin, Pavlo Molchanov, Jose Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj Jha, and In Proceedings of the Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. IEEE/CVF conference on computer vision and pattern recognition, pp. 87158724, 2020. Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover and relabel: Dataset condensation at imagenet scale from new perspective. Advances in Neural Information Processing Systems, 36:7358273603, 2023. Peyton Young. Monotonic solutions of cooperative games. International Journal of Game Theory, 14(2): 6572, 1985. Bo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In International Conference on Machine Learning, pp. 1267412685. PMLR, 2021. Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching, 2022. Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. In International Conference on Learning Representations, 2021. Ganlong Zhao, Guanbin Li, Yipeng Qin, and Yizhou Yu. Improved distribution matching for dataset condensation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 78567865, 2023. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features In Proceedings of the IEEE conference on computer vision and pattern for discriminative localization. recognition, pp. 29212929, 2016. Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature regression. Advances in Neural Information Processing Systems, 35:98139827, 2022. 12 Published as conference paper at ICLR"
        },
        {
            "title": "A REALTED WORK",
            "content": "As contextualized in the main text, our work builds on two key research strands: dataset distillation and explainable AI attribution methods. Below, we expand on these areas, detailing existing limitations and the specific research gap our method addresses. A.1 DATASET DISTILLATION Dataset Distillation, or Dataset Condensation, aims to reduce large dataset into smaller one. Current methods can be categorized into two main approaches: i.e., matching-based methods (Zhao et al., 2021; Lee et al., 2022; Zhao & Bilen, 2021; Wang et al., 2024a; Cazenavette et al., 2022; Cui et al., 2023; Guo et al., 2023; Zhao & Bilen, 2022; Kim et al., 2022; Du et al., 2023; Zhou et al., 2022; Wang et al., 2025c; Min et al., 2026), and knowledge-distillation-based methods (Yin et al., 2023; Shao et al., 2024; Sun et al., 2024; Wang et al., 2025a). Matching-based methods are typically formulated as bi-level optimization problems but struggle with the trade-off between efficiency and the quality of the distilled dataset. In contrast, knowledge-distillation-based methods decouple the problem into two-step process but often lack theoretical guarantees and interpretability. Therefore, deeper investigation is needed to formalize knowledge-distillation-based methods in principled manner to ensure their reliability in practical scenarios with theoretical support, which we address in this paper. A.2 ATTRIBUTION METHODS IN EXPLAINABLE AI Attribution methods are essential for post-hoc explanations of black-box models, revealing each input variables contribution to the final prediction. Among them, the Shapley Value is considered principled tool due to its key axioms: i.e., linearity, dummy, symmetry, and efficiency (Shapley et al., 1953; Young, 1985). To reduce the computational burden, KernelShap (Lundberg & Lee, 2017) was introduced to efficiently approximate the Shapley Value using Linear LIME (Ribeiro et al., 2016) or neural network (Wang et al., 2025b). However, since none of the previous works have explored the application of attribution methods in dataset distillation, there is an opportunity to develop attribution-based approaches for extracting key information for dataset distillation."
        },
        {
            "title": "B DETAILED IMPLEMENTATION",
            "content": "In this section, we detail the implementation specifics of InfoUtil, including the computation of informativeness, tuning settings of teacher models, and provide the corresponding pseudocode in the Algorithm 1. B.1 COMPUTATION OF INFORMATIVENESS In our implementation of InfoUtil, we leveraged the PyTorch framework together with the Captum package to compute Shapley values. Captum provides robust and flexible interface for model interpretability, allowing us to quantitatively assess the contributions of individual features to the models predictions. By utilizing Captums KernalShap1 method, we could accurately determine the importance of each feature within sample, which in turn guides the data refinement process during dataset distillation. Moreover, in the first four cropping, we injected Gaussian noise drawn from the normal distribution (0, σ2), where σ is defined as the product of the overall standard deviation of the Shapley values after average pooling and hyperparameter α that controls the noise intensity. In our experiments, we set the kernal size to 2 2 with stride = 1, and the hyperparameter α = 2. The final (5th) cropping maintained the original Shapley values. This approach effectively reduced the probability of repeatedly cropping the same location. Besides, in most scenarios, we divided each original image into 4 4 grid of patches, computed the Shapley value for each individual patch and subsequently identified the center of the patch with the highest Shapley value as the optimal cropping center. 1https://captum.ai/api/shapley_value_sampling.html Published as conference paper at ICLR 2026 Algorithm 1 InfoUtil Pipeline Input: original dataset D, pre-trained teacher model fθD , teacher model at early t-th epoch fθt, compressed size d, noise variance σ, distilled dataset size m, number of patches k. for each class in do Dc = {(xi, yi) yi = c} // Stage 1: Informativeness Maximization for (xi, yi) Dc do Compute ϕf (xi) using fθD Apply average pooling to ϕf (xi) to obtain pooled heatmap Add noise ε (0, σ2) to the pooled heatmap Extract ξi of size from xi based on the highest heatmap value = {(ξi, yi)} end for // Stage 2: Utility Maximization for (ξi, yi) do Compute gi = θℓ(fθD (ξi), yi) end for Select top-k IPC samples {ξi1, . . . , ξi,IPCk} by gi for = 1 to IPC do Combine ξi,(j1)k+1 to ξi,jk into xj For each ξik in xj, set (cid:102)yjk = fθt(ξik) (cid:101)yj = [ (cid:102)yj1, . . . , (cid:102)yjk] (cid:101)D = (cid:101)D {(xj, yj)} end for end for Output: Distilled dataset (cid:101)D B.2 PRETRAINED TEACHER MODEL When generating soft labels, we utilized teacher models from the early stages of training. Specifically, for CIFAR-10 and CIFAR-100, the teacher models were pretrained for 10 epochs using learning rate of 0.001 on IPC = 1 and 10. Meanwhile, for other datasets (Tiny-ImageNet, ImageNette, ImageWoof, ImageNet-100, and ImageNet-1k), we trained the teacher models for 10 epochs using learning rate of 0.01 on IPC = 1 and 10. For IPC = 50 scenarios, we employed fully converged teacher models across all datasets to ensure that soft labels generated could reflect the comprehensive and stable representations learned from the entire training dataset. Compared to teacher models from early training stages, fully converged models provide richer, more accurate semantic information, which significantly benefit the distillation process, especially when synthesizing larger number of representative images."
        },
        {
            "title": "C CORESET SELECTION COMPARISON AND INFORMATION DENSITY",
            "content": "We investigate the fundamental distinction between data synthesis (InfoUtil) and traditional coreset selection methods, which aim to construct compact dataset by selecting unaltered real samples. While both approaches pursue dataset compression, InfoUtils ability to synthesize highly informative, compressed knowledge yields significant performance gap, especially under extreme data scarcity (IPC = 1 or 10). C.1 FUNDAMENTAL DISTINCTION AND EMPIRICAL ADVANTAGE Traditional coreset selection methods (such as Random, Herding Welling (2009) and Forgetting (Toneva et al., 2018)) are constrained by the quality and content of the original training samples. InfoUtil overcomes this limitation by dynamically synthesizing samples that are optimized for knowledge transfer, extracting informative patches, and utilizing soft labels to condense teacher knowledge. As shown in Tab. 9 and Tab. 10, to empirically demonstrate this advantage, we compare InfoUtil against coreset selection baselines across CIFAR, Tiny-ImageNet, and ImageNet-1K. 14 Published as conference paper at ICLR 2026 Table 9: Comparison of Top-1 Accuracy (%) of InfoUtil vs. Coreset Selection Methods (ConvNet). Model Dataset IPC Random Herding Forgetting InfoUtil (Ours) CIFAR-10 ConvNet CIFAR-100 Tiny ImageNet 1 10 1 10 50 1 10 50 14.4 2.0 26.0 1.2 43.4 1.0 4.2 0.3 14.6 0.5 30.0 0.4 1.4 0.1 5.0 0.2 15.0 0.4 21.5 1.2 31.6 0.7 40.4 0. 8.4 0.3 17.3 0.3 33.7 0.5 1.4 0.1 5.0 0.2 15.0 0.4 13.5 1.2 23.3 1.0 23.3 1.1 4.5 0.2 15.1 0.3 30.5 0.3 1.6 0.1 5.1 0.2 15.0 0.3 28.5 1.4 54.1 0.5 69.8 0. 33.1 0.3 50.5 0.3 57.8 0.2 19.6 0.5 40.2 0.3 48.0 0.5 Table 10: Comparison of Top-1 Accuracy (%) of InfoUtil vs. Coreset Selection (ResNet-18). Model Dataset IPC Random Herding K-Means InfoUtil (Ours) ResNet-18 Tiny ImageNet ImageNet-1K 10 10 7.5 0.1 4.4 0.1 9.0 0.3 5.8 0.1 8.9 0.2 5.5 0.1 45.6 0.3 44.2 0.4 C.2 PERFORMANCE ON LARGE-SCALE DATASETS As shown in Tab. 10, we further validate the results using deeper architecture (ResNet-18) on challenging large-scale datasets, comparing against K-Means coreset selection. The empirical results show massive performance gap. For example, on ImageNet-1K (IPC = 10), InfoUtil achieves 44.2%, which is nearly 7.6 times higher than the best coreset method (Herding, 5.8%). This clearly illustrates that simply selecting real images is insufficient for training deep networks from scratch on such limited budgets. Coreset methods inherently suffer from background noise and reliance on hard labels. In contrast, InfoUtils synthesis mechanism which incorporates attribution cropping (Informativeness) and soft labels, effectively condenses the necessary knowledge, making it far more efficient and powerful than standard subset selection."
        },
        {
            "title": "D ANALYSIS OF SOFT LABELING STRATEGY ROBUSTNESS",
            "content": "To ensure fair performance assessment, we rigorously isolate the contribution of our proposed InfoUtil from potential advantages conferred by the soft-labeling strategy employed by the teacher model. While previous distillation literature has explored utilizing early-stage teacher models for maximizing performance at low Images Per Class (IPC) settings, we demonstrate the intrinsic robustness of InfoUtil by unifying the teacher protocol. D.1 CONTROLLED EXPERIMENT WITH FULLY CONVERGED TEACHER We conducted controlled experiment on the ImageWoof dataset where both the baseline (RDED) and our InfoUtil method were strictly constrained to use the exact same Fully Converged Teacher model across all tested IPC settings (1, 10, and 50). This experimental setup eliminates any potential performance artifact stemming from differences in teacher model convergence stages, ensuring that measured gains are attributed solely to InfoUtils data synthesis mechanism (Shapley-based informativeness and GradNorm utility). The results across different student architectures (ConvNet, ResNet-18, and ResNet-101) are reported in Table 11. As evidenced by Table 11, our method consistently maintains clear performance advantage over the RDED baseline under this strict controlled setting, with improvements observed across every student architecture and IPC configuration. The gains are particularly substantial in deeper architectures and higher compression rates (e.g., 7.9% margin for ResNet-101 at IPC = 50). This data strongly validates that the performance gains are not an artifact of the labeling strategy but are directly attributable to InfoUtils core mechanism of selecting and synthesizing high-informativeness data. 15 Published as conference paper at ICLR Table 11: Accuracy (%) under the Controlled Fully Converged Teacher Setting on ImageWoof. Model Method IPC=1 IPC=10 IPC= ConvNet ResNet-18 ResNet-101 RDED InfoUtil (Ours) RDED InfoUtil (Ours) RDED InfoUtil (Ours) 18.5 20.0 20.8 21.4 19.6 19.8 40.6 42.4 38.5 43.6 31.3 35. 61.5 62.6 68.5 69.2 59.1 67."
        },
        {
            "title": "E PROOFS OF SHAPLEY VALUE AXIOMS",
            "content": "Building upon the game-theoretic formulation in Section 3, we now formally show that our feature attribution methodwhich maximizes informativeness via Shapley valuessatisfies the four axiomatic properties of Shapley values. These properties ensure that the attributions assigned to input variables are theoretically sound and fair. Consistent with our informativeness maximization framework, we define: Neural network as characteristic function: The deep neural network acts as the characteristic function in cooperative game, mapping each coalition of features to predictive score. Players: Each input variable x(i) (i [d] := {1, 2, . . . , d}) is treated as distinct player in the game. Coalitions: binary mask {0, 1}d represents coalition of active features, with si = 1 indicating inclusion of x(i) and si = 0 indicating its exclusion. Reward: The informativeness score (s x) is regarded as the reward contributed by the coalition s. The Shapley value for variable x(i) is computed as: ϕf (x(i)) = 1 (cid:88) s:si=0 (cid:19) (cid:18)d 1 1s (f (x (s + ei)) (x s)) , where ei Rd denotes the vector with one in the i-th position but zeros in the rest positions, and is binary mask indicating active input variables. E.1 PROOF OF AXIOM 1(LINEARITY) Axiom 1 (Linearity) If two games can be merged into new game, then the Shapley Values in the two original games can also be merged. Formally, if fmerged = f1 + f2, then ϕfmerged(x(i)) = ϕf1(x(i)) + ϕf2(x(i)), [d]. Proof of Axiom 1: For merged game fmerged = f1 + f2, by definition we have fmerged(x t) = f1(x t) + f2(x t) for any mask t. Substituting into the Shapley value formula: ϕfmerged (x(i)) = = = 1 1 1 (cid:88) s:si= (cid:88) s:si=0 (cid:88) s:si=0 (cid:18)d 1 1s (cid:18)d 1 1s (cid:18)d 1 1s (cid:18)d 1 1s 1 (cid:88) s:si=0 (cid:19) (fmerged(x (s + ei)) fmerged(x s)) (f1(x (s + ei)) + f2(x (s + ei))) (f1(x s) + f2(x s)) (cid:21) (cid:19)(cid:20) (cid:19) (f1(x (s + ei)) f1(x s)) + (f2(x (s + ei)) f2(x s)) (cid:19) = ϕf1(x(i)) + ϕf2(x(i)). Thus, if fmerged = f1 + f2, then ϕfmerged (x(i)) = ϕf1 (x(i)) + ϕf2 (x(i)), [d]. 16 Published as conference paper at ICLR 2026 E.2 PROOF OF AXIOM 2(DUMMY) Axiom 2 (Dummy) dummy player is player that has no interactions with other players in the game . Formally, if : si = 0, (x (s + ei)) = (x s) + (x ei). Then, the dummy players Shapley Value is computed as (x ei). Proof of Axiom 2: For dummy player satisfying : si = 0, (x(s+ei)) = (xs)+f (xei), substitute the condition into the Shapley value formula: ϕf (x(i)) = = 1 1 (cid:88) s:si= (cid:88) s:si=0 (cid:19) (cid:19) (cid:18)d 1 1s (cid:18)d 1 1s (f (x (s + ei)) (x s)) (f (x ei)) = (x ei) 1 (cid:88) s:si=0 (cid:18)d 1 1s (cid:19) . Note that the sum over all : si = 0 (subsets of the remaining 1 variables) satisfies: s:si=0 where we use the identity (cid:80)n (cid:0)n k=0 (cid:88) (cid:19) (cid:19) = d1 (cid:88) (cid:18)d 1 1s (cid:1) = 2n with = 1. Thus: (cid:18)d 1 = 2d1 k=0 = d, ϕf (x(i)) = (x ei) 1 = (x ei). E.3 PROOF OF AXIOM 3(SYMMETRY) Axiom 3 (Symmetry) If two players contribute equally in every case, then their Shapley values in the game will be equal. Formally, if : si = sj = 0, (x (s + ei)) = (x (s + ej)), then ϕf (x(i)) = ϕf (x(j)). Proof of Axiom 3: For symmetric players and satisfying : si = sj = 0, (x (s + ei)) = (x (s + ej)), consider their Shapley values: (cid:18)d 1 1s (cid:18)d 1 1s (f (x (s + ej)) (x s)) . (f (x (s + ei)) (x s)) , ϕf (x(j)) = ϕf (x(i)) = s:si=0 (cid:88) (cid:88) 1 1 (cid:19) (cid:19) s:sj =0 Define bijection between masks : si = 0 and : tj = 0 via = if / s, and = (s {j}) {i} if s. By symmetry, (x (s + ei)) = (x (t + ej)) and (x s) = (x t). Since 1s = 1t, the binomial coefficients are equal. Thus: (cid:18)d 1 1t (f (x (t + ej)) (x t)) = ϕf (x(j)). ϕf (x(i)) = (cid:88) 1 (cid:19) t:tj =0 E.4 PROOF OF AXIOM 4(EFFICIENCY) Axiom 4 (Efficiency) The total reward of the game is equal to the sum of the Shapley values of all players. Formally, (x) (0) = (cid:80) i[d] ϕf (x(i)). Proof of Axiom 4: Summing Shapley values over all players: ϕf (x(i)) = (cid:88) i[d] (cid:88) i[d] 1 (cid:88) s:si=0 (cid:19) (cid:18)d 1 1s (f (x (s + ei)) (x s)) = 1 (cid:88) (cid:88) s[d] /s (cid:19) (cid:18)d 1 1s (f (x (s + ei)) (x s)) . Published as conference paper at ICLR 2026 For fixed mask with 1s = k, there are players not in s. The inner sum becomes: (f (x (s + ei)) (x s)) = (cid:88) /s (cid:88) /s (x (s + ei)) (d k)f (x s). Summing over all and telescoping the series, all intermediate terms cancel, leaving: ϕf (x(i)) = (x) (0). (cid:88) i[d]"
        },
        {
            "title": "F PROOFS OF THEOREMS",
            "content": "This appendix presents the full derivation to formally establish Theorem 1, complementing the partial analysis in the main text. Recall the definition of utility: Theorem 1: Utility is bounded by Gradient Norm. Let the utility function be defined as in Definition 3. Then there exists constant > 0 such that U(xi, yi; fθ(t)) cθ(t) ℓt(fθ(t)(xi), yi). Using the chain rule for gradient flow, we have ℓt(fθ(t) (xj), yj; B) = θ(t)ℓt(fθ(t) (xj), yj) θ(t) (cid:12) (cid:12) (cid:12)B , and similarly for Bi. Thus, the change in gradient flow is (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Under SGD with learning rate η, the update step is ℓt(fθ(t)(xj), yj; B) ℓt(fθ(t) (xj), yj; Bi) (cid:12) (cid:12) (cid:12) = θ(t) ℓt(fθ(t)(xj), yj) (cid:18) θ(t) (cid:12) (cid:12) (cid:12)B θ(t) (cid:12) (cid:12) (cid:12)Bi (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) . Removing (xi, yi) gives θ(t) (cid:12) (cid:12) (cid:12)B (cid:88) = η (x,y)B θ(t) ℓt(fθ(t)(x), y). θ(t) (cid:12) (cid:12) (cid:12)Bi = η (cid:88) (x,y)Bi θ(t) ℓt(fθ(t)(x), y). Taking the difference, θ(t) (cid:12) (cid:12) (cid:12)B θ(t) (cid:12) (cid:12) (cid:12)Bi = ηθ(t) ℓt(fθ(t)(xi), yi). Substituting this into the gradient flow change gives (cid:12) (cid:12) (cid:12) (cid:12) ℓt(fθ(t) (xj), yj; B) ℓt(fθ(t) (xj), yj; Bi) (cid:12) (cid:12) = η θ(t) ℓt(fθ(t)(xj), yj) θ(t)ℓt(fθ(t) (xi), yi) ηθ(t)ℓt(fθ(t) (xj), yj) θ(t) ℓt(fθ(t)(xi), yi) where the last step follows from the CauchySchwarz inequality. Let θ(t) ℓt(fθ(t)(xj), yj) = η max (xj ,yj )D be constant independent of (xi, yi). Taking the maximum over (xj, yj) D, we obtain Note that = η max(xj ,yj )D θ(t)ℓt(fθ(t) (xj), yj) satisfies the following properties: U(xi, yi; fθ(t)) cθ(t) ℓt(fθ(t)(xi), yi). 1. It is independent of the current measured data (xi, yi), ensuring that the bound in Theorem 1 holds uniformly for all training examples. 2. It only assumes that the gradient norm θ(t) ℓt(fθ(t) (xj), yj) has an upper bound, which is reasonable assumption for any successfully converged model. 3. Since the learning rate η can be chosen to be small in practice, the value of remains controlled and does not become excessively large. 18 Published as conference paper at ICLR"
        },
        {
            "title": "G ADDITIONAL VISUALIZATIONS OF SYNTHETIC DATA",
            "content": "Compared to optimization-based SRe2L, InfoUtil creates more realistic images by preserving details and color consistency. Compared to optimization-free methods like RDED, InfoUtil stands out with its enhanced interpretability and structured framework, emphasizing key semantic details while reducing focus on irrelevant areas. We present further visual comparisons of synthetic ImageNet-1K images generated by SRe2L, RDED, and InfoUtil at IPC = 10. Specifically, Figures 6, 7, 8 and 9 illustrate results across three representative classes, clearly highlighting the superior visual quality attained by InfoUtil. As intuitive evidence, Figure 5 shows condensed images for ImageNet-1Ks indigo bunting category, including results from Original, Herding, SRe2L, RDED, and InfoUtil (Ours). InfoUtil focuses on the most discriminative object parts, yielding more informative results. Additional visualizations are provided in the supplements due to space constraints. Figure 5: Visualization of condensed images for the indigo bunting category on ImageNet-1K. (a) SRe2L (b) RDED (c) InfoUtil (ours) Figure 6: We visualized synthesized images generated by SOTA methods and InfoUtil on ImageNet1K. These images are distilled from the Welsh Springer Spaniel category. 19 Published as conference paper at ICLR 2026 (a) SRe2L (b) RDED (c) InfoUtil (ours) Figure 7: We visualized synthesized images generated by SOTA methods and InfoUtil on ImageNet1K. These images are distilled from the schooner category. (a) SRe2L (b) RDED (c) InfoUtil (ours) Figure 8: We visualized synthesized images generated by SOTA methods and InfoUtil on ImageNet1K. These images are distilled from the indigo bunting category. (a) SRe2L (b) RDED (c) InfoUtil (ours) Figure 9: We visualized synthesized images generated by SOTA methods and InfoUtil on ImageNet1K. These images are distilled from the Siamese cat category."
        }
    ],
    "affiliations": [
        "Duke University",
        "EPIC Lab, SJTU",
        "National University of Singapore",
        "Shanghai Jiao Tong University",
        "The University of Chicago"
    ]
}