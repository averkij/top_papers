{
    "paper_title": "Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction",
    "authors": [
        "Mahdi Pourmirzaei",
        "Farzaneh Esmaili",
        "Salhuldin Alqarghuli",
        "Mohammadreza Pourmirzaei",
        "Ye Han",
        "Kai Chen",
        "Mohsen Rezaei",
        "Duolin Wang",
        "Dong Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions, from sequence-level properties and residue-specific attributes to complex inter-protein interactions, into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling a single model to master numerous tasks with improved efficiency. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at https://github.com/mahdip72/prot2token ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 8 5 0 2 . 5 0 5 2 : r Prot2Token: Unified Framework for Protein Modeling via Next-Token Prediction Mahdi Pourmirzaei1,2 Farzaneh Esmaili 1 Salhuldin Alqarghuli 1 Mohammadreza Pourmirzaei3 Ye Han 1 Kai Chen Mohsen Rezaei 1 Duolin Wang 1 Dong Xu 1 1 University of Missouri {mpngf,f.esmaili,saakdr,yhhdh,dc57y,mrfrg,wangdu,xudong}@missouri.edu 2 ProGene 3 Politecnico di Milano, Milan, Italy mohammadreza.pourmirzaeioliaei@mail.polimi.it"
        },
        {
            "title": "Abstract",
            "content": "The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, unified framework that overcomes these challenges by converting wide spectrum of protein-related predictionsfrom sequence-level properties and residue-specific attributes to complex inter-protein interactionsinto standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable \"task tokens\", to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling single model to master numerous tasks with improved efficiency. We present extensive experimental validation across variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers significant step towards versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at GitHub."
        },
        {
            "title": "Introduction",
            "content": "Proteins are the fundamental building blocks of life, playing critical role in maintaining human health. However, understanding the complex language of proteinsencoded in their sequences and structuresremains significant challenge for researchers [1] . This complexity limits our ability to interpret, predict, and design proteins for various biomedical and therapeutic applications. Preprint. Under review. Figure 1: High-level architecture of Prot2Token highlighting multi-task capability in protein-level, residue-level, and protein-protein level tasks. Protein function prediction is particularly challenging due to the vast diversity of protein sequences, structural variations, and the limited availability of labeled data. Unlike natural languages, protein sequences do not follow explicit syntactic rules understandable by humans, making it difficult for models to learn meaningful representations without extensive biological knowledge [2]. Protein language models (PLMs) offer transformative solution by learning meaningful representations of protein sequences, enabling researchers to decode and translate protein data into more interpretable format [3, 4]. By leveraging PLMs, we can bridge the gap between raw protein information and human understanding, advancing research in drug discovery, disease mechanisms, and synthetic biology. While PLMs have significantly advanced protein-prediction tasks, current models require task-specific specialization after pre-training [5, 6]. This reliance on separate modules for distinct tasks leads to inefficient computational resource use and limited scalability. Most PLMs undergo post-training alignment with specialized predictor architectures for individual tasks, requiring independent training and fine-tuninga time-consuming and resource-intensive approach [7]. unified model capable of efficiently handling diverse protein-prediction tasks would overcome this limitation, streamlining protein function prediction and enhancing its accessibility for real-world applications. To the best of our knowledge, despite the emergence of foundation models for proteins, no comprehensive framework exists to systematically align them across broad spectrum of protein prediction tasks. Instead, researchers often modify existing foundation models to suit particular applications [8], such as predicting 3D protein structures from sequences using customized techniques [9, 10]. One key limitation is that most existing models are based on BERT-style architectures [11], while effective for providing meaningful representation, lack the flexibility needed for diverse and controllable prediction capabilities. In natural language processing (NLP), the transition from BERT-style models to autoregressive GPT-style models has enabled more dynamic and human-understandable instructions (prompts) to control the generation process and therefore, handling diverse set of predictions within the NLP domain [12, 13]. similar paradigm shift is necessary in protein research, moving beyond static encoders toward more advanced generative AI approaches that provide more comprehensive predictive capabilities. Although autoregressive transformer models have been explored for the language of proteinsuch as ProGen2 [14], RITA [15], and Ankh [16]they struggle with controllability and task , especially for protein-prediction tasks. Unlike language models in NLP, which effectively leverage prompting mechanisms for controllable and interpretable predictions, autoregressive PLMs currently lack robust methods to guide their outputs toward human-interpretable formats. This gap hinders their practical applicability and, in contrast to NLP, has compelled researchers to continue relying heavily on encoder-style PLMs, often building specialized architectures around these encoders for specific protein prediction tasks. To address these limitations, this work takes significant step toward unifying diverse protein-related prediction tasks within single comprehensive framework (Figure 1). We introduce universal protocol for tokenizing different protein-prediction tasks, enabling general autoregressive transformer predictor to leverage existing BERT-style PLMs. This unified autoregressive predictor, guided by next-token prediction loss, demonstrates strong generality across multiple protein-prediction task categories, including protein-level, residue-level, and protein-protein interaction-level tasks. We illustrate its versatility through extensive evaluation on diverse examples such as kinase phosphorylation site prediction, protein-ligand binding site prediction, protein 3D structure prediction, and protein mutation stability assessment. Furthermore, our framework inherently supports multi-task learning, and we provide initial analyses demonstrating synergistic performance improvements when related tasks are trained jointly. For certain specialized tasks, such as predicting binding sites, we show that 2 initializing the decoder through self-supervised pre-training significantly boosts performance. Specifically, for protein-ligand binding site prediction, we further analyzed the learned token representations, revealing meaningful relationships among ligand tokens that enabled us to enhance predictions for underrepresented ligands. We believe that our approach represents an essential step toward harnessing and upgrading large language models (LLMs) for robust and flexible protein prediction tasks. 1.1 Related work Many specialized or foundation models now exist for proteins [17], yet none provides single, prompt-controllable interface capable of both generation and diverse set of prediction tasks. We therefore group prior works into generative protein design, predictive representation learning, and unified models. Generative protein design. Autoregressive language models dominate de novo sequence generation. ProGen first demonstrated controllable generation using functional tags [18]. Subsequent scalingProtGPT2 1.2b [19], RITA 1.2b [15], and ProGen2 6.4b [14]improved perplexity and experimental success yet still require task-specific fine-tuning or filtering to steer functions. Most recently, ProGen3 extends this trend by scaling it up significantly, but reports limited controllability for fine-grained generation [20]. Predictive representation learning. parallel thread focuses on bidirectional encoders that power task-specific heads. Large masked-language models such as ESM2-15b yield embeddings for spectrum of downstream tasks [10] and even drive end-to-end folding with ESMFold [10]yet the folding module is specialized for 3-D structure prediction. Likewise, AlphaFold2 (AF2) couples EvoFormer encoders to bespoke structure decoder [21]. Such wrapper architectures excel at their dedicated outputs but do not form general predictor. We find only one cross-task autoregressive alternative: PTMGPT2 [22], which adapts GPT-2 with prompt-based fine-tuning to predict 19 classes of post-translational modifications (PTMs) in single modelstill restricted to the PTMs domain. Unified models. Recently, models have emerged that aim to link protein design and prediction within single system. HelixProtX unifies sequence, structure, and free text in one multimodal autoregressive transformer, capable of translating between any two of those modalities and predicting atom-level 3-D structure directly from sequence [23]. ProLLaMA [24] adapts LLaMA-2 through protein-specific instruction tuning so that one model, guided by natural-language prompts, can perform controllable sequence generation together with property-prediction tasks such as stability, fluorescence, binding affinity, and remote-homology classification [24]. InstructProtein aligns protein sequences with human language via knowledge-graphguided instruction tuning, allowing the model either to describe proteins function in free text or to generate plausible sequence that satisfies textual specification [25]. Although these systems demonstrate encouraging modality transfer, they still depend on prompt engineering for fine-grained control and have yet to be benchmarked across the full suite of standard prediction tasks addressed in this work."
        },
        {
            "title": "2 Method",
            "content": "2.1 Prot2Token architecture The Prot2Token framework is designed to unify diverse protein-related prediction tasks using shared architecture based on encoder-decoder transformers. The core idea is to integrate an autoregressive decoder language model with existing encoder-style protein and optional chemical language models via cross-attention layers, thereby converting prediction tasks into unified next-token prediction problem. The architecture employs pre-trained bidirectional transformer (ESM2) as the protein encoder. For tasks involving chemical information (e.g., ligand binding), an optional chemical encoder (BARTSmile) [26] is used to process SMILES representations. These encoders transform their respective input sequences into contextual embeddings: henc = fenc(x) where henc RN denc is the encoder output, is the sequence length, and denc is the encoders hidden dimension. We use distinct embedding tables for each encoder (protein and, if applicable, chemical) and the decoder to reflect their differing tokenization schemes and functional roles in the architecture. To enhance the position-awareness of the sequence embeddings, we introduce learnable positional embedding layer gpos(), producing augmented representations: where RN denc is the learnable positional embedding. haug = henc + gpos(p) To align the encoder output with the decoders hidden dimension ddec, we apply linear projection: hproj = haugWproj where Wproj Rdencddec This projected representation hproj RN ddec is fed into the decoder via cross-attention. The decoder is causal (autoregressive) transformer composed of standard transformer components such as multi-head self-attention, feed-forward layers, and GeLU activations. FlashAttention-2 is incorporated to improve training speed and memory efficiency. For specific architectural configurations used in this work, refer to Table 10. Figure 2: Detailed Architecture of Prot2Token Highlighting Multi-Task Capability. This diagram shows the Prot2Token components: bidirectional Protein encoder and an optional Chemical Encoder, Fusion block part, and an autoregressive Decoder guided by Task Token Embeddings for various prediction tasks (examples listed). This illustrates the frameworks potential for simultaneous multitask learning; however, practical training of this work only focused on combinations of fewer tasks due to computational costs, demonstrating the principle. To support multiple tasks within unified training process, we introduce task token. These tokens, placed at the beginning of each output sequence, serve as prompts that guide the decoders behavior 4 for each specific task. The task token sequence = (T1, T2, . . . , Tm) is embedded via learnable embedding function: etask = gtask(t) Rmddec The decoder receives the embedded task tokens and attends to both them and the projected encoder outputs: = fdec(hproj, etask) During inference, the decoder is autoregressive: it receives special beginning-of-sequence (<BOS>) token followed by the task token, and generates each output token sequentially. The decoder factorizes the probability of the output sequence = (x1, x2, . . . , xT ) as: p(x) = (cid:89) t=1 pθ(xt x1, . . . , xt1) The training objective is to minimize the negative log-likelihood: L(θ) = (cid:88) t=1 log pθ(xt x1, . . . , xt1) To better manage the role of prompt tokens, we assign token-specific weights wt [0, ) to control their contribution to the loss. Specifically, we set w1 = 0 to exclude the prompt (task token) from the loss, while allowing other tokens 2 to be weighted differently: L(θ) = (cid:88) t=1 wt log pθ(xt x1, . . . , xt1) This flexible weighting helps tune the models attention to different parts of the label sequence. Refer to Figure 2 for an overview of the Prot2Token architecture and Figure 5 for closer look at how task tokens interact with the decoder. Architectural variants and configuration details are summarized in Table 10. By representing diverse outputs as token sequences, this design allows Prot2Token to unify broad spectrum of protein prediction tasks under single decoder, facilitating both joint and independent training regimes. 2.2 Tokenization The Prot2Token framework utilizes distinct tokenization strategies for its input encoders and the output decoder. Input sequences, such as protein amino acid sequences or chemical SMILES strings, are processed by the native tokenizers of their respective pre-trained encoders (e.g., ESM2 for proteins, BARTSmiles [26] for chemicals). The core innovation resides in the unified tokenization strategy for the output labels predicted by the autoregressive decoder. This strategy is pivotal as it converts wide array of biological prediction targets into standardized sequences of discrete tokens, enabling the decoder to handle diverse tasks via consistent next-token prediction mechanism. All tokenized output sequences commence with <BOS> token and conclude with an <EOS> token, clearly demarcating sequence boundaries. As depicted in Figure 3, this approach transforms heterogeneous labels into uniform sequential format, facilitating task-agnostic decoding process. Specifically, for classification tasks, labels are mapped to unique discrete tokens, with multi-label tasks typically concatenating these tokens (often alphabetically). Regression tasks represent continuous numerical values through granular digit-by-digit encoding of their character components (e.g., sign, digits, decimal point). Sequence-tosequence tasks generate an output token for each residue in the input protein, maintaining direct correspondence. Binding site prediction involves tokenizing the sorted 1-based indices of residues participating in interactions. Other complex output types, such as for PTMs, are also converted into specific token sequences, for instance, by listing potential and confirmed modification sites separated by special <sep> token. This universal tokenization protocol is fundamental to Prot2Tokens ability to unify broad spectrum of protein prediction tasks within single decoding architecture. Refer to Appendix A.2 for comprehensive explanation of each specific tokenization method. 5 Figure 3: Prot2Token converts heterogeneous labels into uniform sequences: examples illustrate the five tokenization categories(i) sequence-to-sequence, (ii) classification (multi-class/ multi-label), (iii) regression, (iv) binding-site indexing, and (v) other composite outputs such as PTMhighlighting the frameworks task-agnostic decoding format. 2.3 Datasets This work leverages diverse set of tasks drawn from several established benchmarks and repositories, including PEER [27], ProteinShake [28], CATH [29], AlphaFoldDB [21], and other curated sources such as ProteinGym [30]. These datasets encompass wide range of protein-related prediction tasks, including regression, classification, binding site, and sequence-to-sequence predictions. Details for each task, including preprocessing steps, are provided in Appendix A.3. All tasks in these datasets are tokenized according to the unified protocol described in Section 2.2."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluated Prot2Token in multiple tasks on different datasets, including the protein-level, residuelevel, and protein-protein level. For subset of these tasks, we incorporated self-supervised pre-training stage for the autoregressive decoder as an initial step. In all experiments, the protein encoder in Prot2Token was initialized using the pre-trained ESM2-650m model. For the decoder part, we used an autoregressive language model with different configurations based on the size of the ESM encoder and hyperparameters of the autoregressive decoder (Appendix A.1). We only considered BARTSmiles as the chemical encoder for the protein-ligand affinity task and disabled it for the other tasks. Optimization was carried out with the AdamW optimizer [31], applying weight decay of 0.1 and using beta-1 and beta-2 values of 0.9 and 0.98, respectively, while setting epsilon to 1e-7. The learning rate followed cosine annealing schedule with an initial warm-up phase [32], starting at 1e-6 and gradually increasing to 5e-5 over the first 256 steps unless stated otherwise. The training was performed using the PyTorch 2 framework [33] on single computational node equipped with four Nvidia A100 GPUs (80GB each). 6 3.1 Classification This category includes multi-class, multi-label and hierarchical classification tasks such as Deeploc 2.0 and ER. The results are shown in Tables 1 and 2. In Deeploc 2 dataset, we significantly improved the performance compared to the original method, and also, the ER task result showed that the performance boosted 7.5 percent by using multi-task learning. We could not calculate the Fmax metric for the EC and GO tasks, so we only considered the accuracy and F1 scores to evaluate performance. Consequently, direct comparisons with other methods were not possible. Supplementary results with additional details are in Appendix A.4.1. Table 1: Localization prediction using Deeploc-2 dataset. The results are based on the independent test set. Method Macro-F1 Encoder Deeploc-2 [34] Prot2Token-B 0.46 0.5364 ProtT5 ESM2-650m 3.2 Regression Table 2: Comparing methods on ER dataset. PLA and ST stand for protein-ligand affinity and stability, respectively. : chemical encoder is attached. Method Aux-Tasks Accuracy Encoder Baseline CoupleNet [5] Prot2Token-B Prot2Token-B - - - Deeploc+PLA+ST 83.81 89.0 79.29 86.83 ESM2-650m ProtT5 ESM2-650m ESM2-650m This category encompasses four tasks: protein stability prediction, fluorescence intensity prediction, protein-ligand binding affinity estimation, and protein mutation stability assessment. The first two tasks utilize single protein sequence as input. In contrast, the protein-ligand affinity task takes both protein sequence and molecular SMILES string as input, while the mutation stability task uses pair of protein sequences representing wild-type and mutant variants. The results for these tasks are presented in Tables 3, 4, 5, and 6. Additional experimental details can be found in Appendix A.4.2. Across all regression tasks, Prot2Token consistently outperformed baseline methods from the PEER benchmark. Notably, in the fluorescence prediction task, multi-task learning led to performance gain of up to 5.6%  (Table 4)  . For mutation stability prediction, Prot2Token achieved substantial improvement of over 51.5% compared to the best-performing baseline model as shown in Table 6. Table 3: Comparing Prot2Token with other methods on stability prediction. Method Spearman Encoder Baseline PEER[27] (fine-tuned) PEER[27] (fine-tuned) Prot2Token-B 0.7527 0.75 0.771 0. ESM2-650m ESM1-1b ProtBert ESM2-650m Table 4: Comparing fluorescence prediction methods w/ and w/o multi-task learning. PLA and ST stand for protein-ligand affinity and stability, respectively. We considered the fine-tuned methods of PEER as the comparison. : chemical encoder is attached. Method Aux-tasks Spearman Encoder PEER [27] PEER [27] Prot2Token-B Prot2Token-B Prot2Token-B - - - PLA PLA+ST 0.679 0.679 0.7389 0.7766 0.78 ESM1-1b ProtBert ESM2-650m ESM2-650m ESM2-650m 3.3 Binding site We evaluated Prot2Token on two binding site prediction tasks: protein-ligand and protein-protein. For protein-ligand binding sites, each ligand type is represented by dedicated task token in the decoder, which enables the model to capture ligand-specific interactions directly from protein sequences and learnable task tokens. Table 7: F1 scores for the top 10 ligands across different training configurations on the test sets, with varying numbers of auxiliary ligands. The table summarizes the impact of jointly training with 10, 20, 30, and 41 ligands on binding site prediction. indicates that self-supervised tasks were excluded during supervised training. Ligand Average Weighted Average 10 ligands 0.1883 0.1849 10 ligands 0.6076 0.6297 20 ligands 0.5942 0.6277 30 ligands 0.6181 0. 41 ligands 0.6132 0.6353 7 Table 5: Comparing protein-ligand affinity prediction methods on the test set. : chemical encoder is attached. Method Spearman Encoder PEER [27] (fine-tuned) PEER [27] (fine-tuned) Prot2Token-B 1.559 1.562 1.3887 ESM1-1b ProtBert ESM2-650m Table 6: Comparison of mutation effect prediction models on the ProteinGym benchmark with supervised 5-fold cross-validation. Additional baselines are included from the original ProteinGym paper [30]. Method Spearman ESM-1v MSAT Tranception ProteinNPT 0.542 0.568 0.571 0.613 Prot2Token-C 0.9294 0.0018 We introduced self-supervised pre-training phase for the decoder to enhance model initialization and improve predictive performance of binding site prediction-type tasks. This strategy significantly improves the models ability across different tasks, particularly benefiting those with wide range of binding site indices. detailed rationale and methodology for this self-supervised pre-training are provided in Appendix A.4.3. We report high-level performance results of protein-ligand binding site prediction in Table 7, demonstrating that Prot2Token achieves competitive predictive accuracy across various ligand types with the help of self-supervised pre-training (see detailed results of this task and protein-protein binding site in Appendix A.4.4). Furthermore, to understand the representation learned by the task tokens, we explored their embeddings and identified relationships that correlate closely with biochemical properties (Appendix A.5). These findings, visualized in Figure 17 in the appendix, indicate that task tokens not only serve as input identifiers but also encode biologically relevant information. Leveraging these insights, we further utilized the learned relationships to boost predictive accuracy for underrepresented ligands, achieving significant performance gains as summarized in Table 34. More details in appendix A.5.5. 3.4 Sequence-to-sequence Figure 4: Randomly selected test set samples where our model achieved TM-score above 0.90 versus AF2 high-pLDDT predictions. On average, each sample was predicted and converted in approximately 1 second using an Nvidia A100 GPU. Table 8: 3D structure prediction on continuous automated model evaluation (CAMEO) (Jan 2024 to Jan 2025) [35]. TM-score and inference time (one A100-80GB GPU) are reported for representative 384-residue protein sequence. Duo to computational cost, results for AF2 methods are reported from the ESM2 publication, using the CAMEO benchmark from April 2022 and June 2022. Method TM-score A100 Wall-clock (384-aa) Speed-up Prot2Token-D ESMFold (ESM2-3B) [36] AF2 (w/o MSA) [21] AF2 (w/ MSA) [21] 0.54 0.79 0.41 0.88 12 14.2 2030 1825 min 1000 77 54 1 In this part, we evaluated Prot2Token on residue-wise sequences by formulating it as sequence labeling task, where the model generates discrete token for each residue in the input protein sequence. The main focus of this section is on the challenging task of sequence-to-3D structure prediction. Here, Prot2Token is trained to generate discrete 3D structure tokens from amino acid sequences using vector quantized variational autoencoder (VQ-VAE) based representation for protein backbone coordinates. The results are summarized in Table 8, which reports TM-score and runtime for representative structure prediction methods. Notably, Prot2Token-D demonstrates dramatic speed advantage, producing structure predictions for typical 384-residue protein in 12 seconds on single A100 GPUapproximately three orders of magnitude faster than AF2 with multiple sequence alignment (MSA) input, which typically requires 1825 minutes for inference. This substantial speed-up makes Prot2Token particularly well-suited for large-scale or real-time structure generation scenarios. Representative examples of successful and unsuccessful 3D structure predictions are illustrated in Figure 4 and Figure 14, respectively. Additionally, as shown in Figure 15, the training process has not yet fully converged, indicating that further improvements in predictive performance are likely with extended training. The result of secondary structure is reported in Table 27. 3.5 Other types Building on the models ability to predict binding sites (Section 3.3), we extended our approach to include protein-kinase phosphorylation site prediction, task with significant real-world applications. For this, we selected protein-kinase sequence pairs along with their corresponding phosphorylation sites and jointly trained them alongside 20 self-supervised tasks. The fine-tuning phase started from the latest checkpoint obtained during the self-supervised pre-training stage. In this task, the self-supervised tasks were reduced to total of 20,000 samples. Substrate sequences longer than 1,280 amino acids were excluded during training and evaluation. Additionally, the total sequence length, combining substrate and kinase sequences, was capped at 2,048 tokens, with kinase sequences truncated as necessary to fit within this limit. The batch size was set to process 98,304 tokens per iteration. We enabled fine-tuning of the last eight blocks of the protein encoder. Table 9, compares our results with two phosphorylation prediction tools, GPS 6.0 [37] and KinasePhos3 [38]. Predictions with scores above 0.7 were classified as true positives. For GPS 6.0, we generated results by selecting each kinase group individually on its platform. Since the training split of GPS 6.0 is not publicly available, there is risk of data contamination between our validation set and GPS 6.0s training data. This could result in artificially high-performance estimates for GPS 6.0, potentially reflecting memorization rather than true generalization. Table 9: Comparative F1 results of our method against leading tools (KinasePhos3 and GPS 6.0) across the validation, GPS test, and rare groups test sets. Method Validation Set (F1) GPS Test Set (F1) Rare Groups Test Set (F1) KinasePhos3 [38] GPS 6.0 [37] Prot2Token-C 0.0747 0.3076 0.4966 0.0421 0.2398 0. 0.3178 0.4000 0."
        },
        {
            "title": "4 Discussion",
            "content": "Our work introduces Prot2Token, unified framework designed to address broad spectrum of protein prediction tasks, ranging from protein-level and residue-level properties to complex protein-protein interactions. The core innovation lies in its versatile tokenization strategy, which demonstrates remarkable breadth in generalizing across these diverse predictive challenges by converting them into unified next-token prediction problem. Prot2Token effectively utilizes the embedded capabilities of pre-trained encoder-style PLMs by attaching them to general-purpose autoregressive predictor. This inherent generality facilitates robust multi-task learning, allowing single model to concurrently learn and execute variety of tasks, where the learning of certain tasks can have synergistic effect (Tables 2, 3 and 4), boosting the performance of others, thereby streamlining the prediction pipeline and notably improving computational efficiency when deploying single model to serve multiple predictive functions. This work also examines the current limitations of such systems (Appendix 5), particularly their weakness in tasks reliant on precise spatial information, such as binding site 9 prediction, and introduces auxiliary self-supervised learning objectives (Appendix A.4.3) specifically for the decoder component of our architecture to address this challenge  (Table 7)  . These auxiliary objectives are designed to instill specific inductive biases within the decoder, effectively teaching it to better capture the structural determinants crucial for accurate binding site predictions. significant implication of Prot2Token is its potential to distill the predictive power of numerous highly specialized models, including those for 3D structure prediction like AlphaFold2, into single, efficient system. This consolidation could yield substantial increase in prediction speed, potentially by three to four orders of magnitude, transforming current methodologies into high-throughput approaches  (Table 8)  . This avenue of research is therefore anticipated to present numerous readily attainable and impactful opportunities, enabling rapid annotation and functional characterization of the ever-expanding protein sequence universe. Furthermore, the Task tokens central to our framework offer an avenue for interpretability; analyzing their embeddings can potentially uncover novel biochemical relationships between different protein properties or tasks (Figure 17). Separately, this work also contributes to the critical goal of making protein language models more intrinsically structure-aware  (Table 28)  . In broader sense, the tokenization scheme presented here represents conceptual step towards empowering general-purpose autoregressive large language models with the capabilities required for sophisticated protein prediction tasks. Looking ahead, we believe several future directions are worth exploring. comprehensive multi-task training approach, encompassing an even wider array of biologically relevant tasks, will be crucial to fully realize the frameworks potential for generalization and synergistic learning. Investigating more sophisticated stochastic sampling methods, beyond simple greedy decoding, could unlock finer-grained control over predictions and potentially reveal richer landscape of possible outcomes. Perhaps most compelling is the extension of this approach to protein design. The ability to seamlessly integrate protein prediction and generation within single unified model opens the door to performing multiple in silico steps of the drug design and discovery pipeline, from target property prediction to novel candidate generation, all within one cohesive computational environment. This could dramatically accelerate the development and reduce the associated costs of new therapeutics and biomaterials."
        },
        {
            "title": "5 Limitations",
            "content": "While Prot2Token demonstrates considerable versatility and strong performance across range of protein prediction tasks, several limitations should be acknowledged. Firstly, the inherent nature of the autoregressive decoder initially presented challenges in tasks demanding precise spatial understanding, such as binding site prediction. Although the introduction of self-supervised decoder pre-training significantly mitigated this by instilling crucial inductive biases, the base architecture required this augmentation to achieve optimal performance on such spatially sensitive tasks. Secondly, the aspiration of truly universal model capable of concurrently learning an extremely broad spectrum of protein-related tasks is tempered by current computational realities. The experiments presented herein, while extensive, focused on combinations of subset of tasks. Scaling to an exhaustive multi-task learning scenario encompassing all possible protein prediction types simultaneously would entail substantial computational resources, potentially limiting the practical scope of joint training in its most comprehensive form. Furthermore, in sequence-to-sequence prediction tasks, we observed that the autoregressive decoder could occasionally produce output sequences with lengths inconsistent with the input protein sequence (1 to 3 amino acids). This necessitated the implementation of post-inference constraints on the endof-sequence token to ensure coherent and correctly dimensioned outputs, indicating that raw decoder outputs may not always perfectly align with expected structural constraints without such measures. The performance of Prot2Token is also intrinsically linked to the capabilities of the foundational pretrained protein encoders it utilizes for generating input embeddings. Any limitations or biases present in these underlying encoder models could invariably propagate and influence the predictive accuracy of the Prot2Token framework. Additionally, like many machine learning models, Prot2Tokens performance can be challenged by datasets exhibiting severe class imbalances. While the framework showed robust results on diverse benchmarks, tasks with highly skewed label distributions may require further specialized strategies to ensure equitable learning and prediction across all classes. 10 Finally, our current exploration of decoding strategies has primarily focused on greedy decoding. more comprehensive investigation into various stochastic sampling methods (e.g., top-k, nucleus sampling) could potentially unlock finer-grained control over the generation process and reveal richer landscape of possible predictions, but this remains an avenue for future work."
        },
        {
            "title": "6 Broader impact",
            "content": "The Prot2Token framework represents significant advancement in computational biology, with potentially transformative impacts on protein research, therapeutic discovery, and biotechnology applications. By unifying diverse protein prediction tasks within single, scalable architecture, Prot2Token substantially reduces computational requirements and simplifies model management. This democratization of sophisticated predictive capabilities could significantly enhance accessibility for research groups with limited computational resources, facilitating broader participation and innovation in the field. Moreover, the substantial speed improvements demonstrated by Prot2Token, particularly in protein structure prediction, may enable real-time applications in clinical and industrial settings, such as personalized medicine, real-time drug screening, and rapid biomarker discovery. However, alongside these benefits, the wide applicability and powerful predictive capacity of Prot2Token also necessitate careful ethical consideration. As the barrier to rapid protein prediction and generation lowers, it becomes increasingly important to implement responsible practices around data usage and sharing, ensuring that predictive outputs, especially those related to therapeutics or biologically active molecules, are validated rigorously before clinical or environmental deployment. Additionally, there is need to consider the implications of such advanced modeling capabilities on biosecurity. With models that can rapidly predict or design biologically active proteins, safeguards must be established to prevent misuse, including unintentional production of harmful or disruptive biological agents. Continued dialogue and collaboration between computational biologists, policymakers, and ethicists will be crucial to navigating these challenges responsibly."
        },
        {
            "title": "References",
            "content": "[1] Jung Eun Shim, Ji Hyun Kim, Junha Shin, Ji Eun Lee, and Insuk Lee. Pathway-specific protein domains are predictive for human diseases. PLoS computational biology, 15(5):e1007052, 2019. [2] Dan Ofer, Nadav Brandes, and Michal Linial. The language of proteins: Nlp, machine learning & protein sequences. Computational and Structural Biotechnology Journal, 19:17501758, 2021. [3] Jingmin An and Xiaogang Weng. Collectively encoding protein properties enriches protein language models. BMC bioinformatics, 23(1):467, 2022. [4] Noelia Ferruz and Birte Höcker. Controllable protein design with language models. Nature Machine Intelligence, 4(6):521532, 2022. [5] Bozhen Hu, Cheng Tan, Jun Xia, Jiangbin Zheng, Yufei Huang, Lirong Wu, Yue Liu, Yongjie Xu, and Stan Li. Learning complete protein representation by deep coupling of sequence and structure. bioRxiv, pages 202307, 2023. [6] Rahmatullah Roche, Bernard Moussad, Md Hossain Shuvo, Sumit Tarafder, and Debswapna Bhattacharya. Equipnas: improved proteinnucleic acid binding site prediction using proteinlanguage-model-informed equivariant deep graph neural networks. Nucleic Acids Research, 52 (5):e27e27, 2024. [7] Konstantin Weissenow and Burkhard Rost. Are protein language models the new universal key? Current Opinion in Structural Biology, 91:102997, 2025. [8] Robert Schmirler, Michael Heinzinger, and Burkhard Rost. Fine-tuning protein language models boosts predictions across diverse tasks. Nature Communications, 15(1):7407, 2024. [9] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. 11 [10] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. BioRxiv, 2022: 500902, 2022. [11] Serbulent Unsal, Heval Atas, Muammer Albayrak, Kemal Turhan, Aybar Acar, and Tunca Dogan. Learning functional properties of proteins with language models. Nature Machine Intelligence, 4(3):227245, 2022. [12] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [13] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [14] Erik Nijkamp, Jeffrey Ruffolo, Eli Weinstein, Nikhil Naik, and Ali Madani. Progen2: exploring the boundaries of protein language models. Cell systems, 14(11):968978, 2023. [15] Daniel Hesslow, Niccoló Zanichelli, Pascal Notin, Iacopo Poli, and Debora Marks. Rita: study on scaling up generative protein sequence models. arXiv preprint arXiv:2205.05789, 2022. [16] Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, and Burkhard Rost. Ankh: Optimized protein language model unlocks general-purpose modelling. arXiv preprint arXiv:2301.06568, 2023. [17] Lei Wang, Xudong Li, Han Zhang, Jinyi Wang, Dingkang Jiang, Zhidong Xue, and Yan Wang. comprehensive review of protein language models, 2025. URL https://arxiv.org/abs/ 2502.06881. [18] Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael Eguchi, Po-Ssu Huang, and Richard Socher. Progen: Language modeling for protein generation. arXiv preprint arXiv:2004.03497, 2020. [19] Noelia Ferruz, Steffen Schmidt, and Birte Höcker. Protgpt2 is deep unsupervised language model for protein design. Nature communications, 13(1):4348, 2022. [20] Aadyot Bhatnagar, Sarthak Jain, Joel Beazer, Samuel Curran, Alexander Hoffnagle, Kyle Ching, Michael Martyn, Stephen Nayfach, Jeffrey Ruffolo, and Ali Madani. Scaling unlocks broader generation and deeper functional understanding of proteins. bioRxiv, pages 202504, 2025. [21] Mihaly Varadi, Stephen Anyango, Mandar Deshpande, Sreenath Nair, Cindy Natassia, Galabina Yordanova, David Yuan, Oana Stroe, Gemma Wood, Agata Laydon, et al. Alphafold protein structure database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. Nucleic acids research, 50(D1):D439D444, 2022. [22] Palistha Shrestha, Jeevan Kandel, Hilal Tayara, and Kil To Chong. Post-translational modification prediction via prompt-based fine-tuning of gpt-2 model. Nature Communications, 15(1): 6699, 2024. [23] Zhiyuan Chen, Tianhao Chen, Chenggang Xie, Yang Xue, Xiaonan Zhang, Jingbo Zhou, and Xiaomin Fang. Unifying sequences, structures, and descriptions for any-to-any protein generation with the large multimodal model helixprotx. arXiv preprint arXiv:2407.09274, 2024. [24] Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, and Yonghong Tian. Prollama: protein large language model for multi-task protein language processing. arXiv preprint arXiv:2402.16445, 2024. [25] Zeyuan Wang, Qiang Zhang, Keyan Ding, Ming Qin, Xiang Zhuang, Xiaotong Li, and Huajun Chen. Instructprotein: Aligning human and protein language via knowledge instruction. arXiv preprint arXiv:2310.03269, 2023. 12 [26] Gayane Chilingaryan, Hovhannes Tamoyan, Ani Tevosyan, Nelly Babayan, Lusine Khondkaryan, Karen Hambardzumyan, Zaven Navoyan, Hrant Khachatrian, and Armen Aghajanyan. Bartsmiles: Generative masked language models for molecular representations. arXiv preprint arXiv:2211.16349, 2022. [27] Minghao Xu, Zuobai Zhang, Jiarui Lu, Zhaocheng Zhu, Yangtian Zhang, Ma Chang, Runcheng Liu, and Jian Tang. Peer: comprehensive and multi-task benchmark for protein sequence understanding. Advances in Neural Information Processing Systems, 35:3515635173, 2022. [28] Tim Kucera, Carlos Oliver, Dexiong Chen, and Karsten Borgwardt. Proteinshake: Building datasets and benchmarks for deep learning on protein structures. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. [29] Duolin Wang, Mahdi Pourmirzaei, Usman Abbas, Shuai Zeng, Negin Manshour, Farzaneh Esmaili, Biplab Poudel, Yuexu Jiang, Qing Shao, Jin Chen, et al. S-plm: Structure-aware protein language model via contrastive learning between sequence and structure. Advanced Science, 12(5):2404212, 2025. [30] Pascal Notin, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, Ada Shaw, Rose Orenbuch, Ruben Weitzman, Jonathan Frazer, Mafalda Dias, Dinko Franceschi, Yarin Gal, and Debora Marks. Proteingym: Largescale benchmarks for protein fitness prediction and design. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 6433164379. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ cac723e5ff29f65e3fcbb0739ae91bee-Paper-Datasets_and_Benchmarks.pdf. [31] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [32] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. [33] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pages 929947, 2024. [34] Vineet Thumuluri, José Juan Almagro Armenteros, Alexander Rosenberg Johansen, Henrik Nielsen, and Ole Winther. Deeploc 2.0: multi-label subcellular localization prediction using protein language models. Nucleic Acids Research, 50(W1):W228W234, 2022. [35] Jürgen Haas, Alessandro Barbato, Dario Behringer, Gabriel Studer, Steven Roth, Martino Bertoni, Khaled Mostaguir, Rafal Gumienny, and Torsten Schwede. Continuous automated model evaluation (cameo) complementing the critical assessment of structure prediction in casp12. Proteins: Structure, Function, and Bioinformatics, 86:387398, 2018. [36] ESM Team. Esm cambrian: Revealing the mysteries of proteins with unsupervised learning, 2024. URL https://evolutionaryscale.ai/blog/esm-cambrian. [37] Miaomiao Chen, Weizhi Zhang, Yujie Gou, Danyang Xu, Yuxiang Wei, Dan Liu, Cheng Han, Xinhe Huang, Chengzhi Li, Wanshan Ning, et al. Gps 6.0: an updated server for prediction of kinase-specific phosphorylation sites in proteins. Nucleic acids research, 51(W1):W243W250, 2023. [38] Renfei Ma, Shangfu Li, Wenshuo Li, Lantian Yao, Hsien-Da Huang, and Tzong-Yi Lee. Kinasephos 3.0: redesign and expansion of the prediction on kinase-specific phosphorylation sites. Genomics, proteomics & bioinformatics, 21(1):228241, 2023. [39] Benoit Gaujac, Jérémie Donà, Liviu Copoiu, Timothy Atkinson, Thomas Pierrot, and Thomas Barrett. Learning the language of protein structure. arXiv preprint arXiv:2405.15840, 2024. 13 [40] Jose Juan Almagro Armenteros, Marco Salvatore, Olof Emanuelsson, Ole Winther, Gunnar Von Heijne, Arne Elofsson, and Henrik Nielsen. Detecting sequence signals in targeting peptides using deep learning. Life science alliance, 2(5), 2019. [41] Marina Omelchenko, Michael Galperin, Yuri Wolf, and Eugene Koonin. Nonhomologous isofunctional enzymes: systematic analysis of alternative solutions in enzyme evolution. Biology direct, 5:120, 2010. [42] Gene Ontology Consortium. The gene ontology project in 2008. Nucleic acids research, 36 (suppl_1):D440D444, 2008. [43] Jie Hou, Badri Adhikari, and Jianlin Cheng. Deepsf: deep convolutional neural network for mapping protein sequences to folds. Bioinformatics, 34(8):12951303, 2018. [44] Edwin Webb et al. Enzyme nomenclature 1992. Recommendations of the Nomenclature Committee of the International Union of Biochemistry and Molecular Biology on the Nomenclature and Classification of Enzymes. Number Ed. 6. Academic Press, 1992. [45] Huaqing Liu, Peiyi Chen, Xiaochen Zhai, Ku-Geng Huo, Shuxian Zhou, Lanqing Han, and Guoxin Fan. Ppb-affinity: Protein-protein binding affinity dataset for ai-based protein drug discovery. Scientific Data, 11(1):111, 2024. [46] Tianlong Chen and Chengyue Gong. Hotprotein: novel framework for protein thermostability prediction and editing. NeurIPS 2022, 2022. [47] Anton Bushuiev, Roman Bushuiev, Petr Kouba, Anatolii Filkin, Marketa Gabrielova, Michal Gabriel, Jiri Sedlar, Tomas Pluskal, Jiri Damborsky, Stanislav Mazurenko, et al. Learning to design protein-protein interactions with enhanced generalization, 2024. URL https://arxiv. org/abs/2310.18515. [48] UniProt Consortium. Uniprot: worldwide hub of protein knowledge. Nucleic acids research, 47(D1):D506D515, 2019. [49] Brigitte Boeckmann, Amos Bairoch, Rolf Apweiler, Marie-Claude Blatter, Anne Estreicher, Elisabeth Gasteiger, Maria Martin, Karine Michoud, Claire ODonovan, Isabelle Phan, et al. The swiss-prot protein knowledgebase and its supplement trembl in 2003. Nucleic acids research, 31(1):365370, 2003. [50] Weizhong Li and Adam Godzik. Cd-hit: fast program for clustering and comparing large sets of protein or nucleotide sequences. Bioinformatics, 22(13):16581659, 2006. [51] Feixiang Zhou, Shuo Zhang, Huifeng Zhang, and Jian Liu. Procesa: Contrast-enhanced structure-aware network for thermostability prediction with protein language models. Journal of Chemical Information and Modeling, 65(5):23042313, 2025. [52] Helen M. Berman, John Westbrook, Zukang Feng, Gary Gilliland, T. N. Bhat, Helge Weissig, Ilya N. Shindyalov, and Philip E. Bourne. The protein data bank. Nucleic Acids Research, 28 (1):235242, 01 2000. ISSN 0305-1048. doi: 10.1093/nar/28.1.235. URL https://doi.org/ 10.1093/nar/28.1.235. [53] Nozomi Nagano. Ezcatdb: the enzyme catalytic-mechanism database. Nucleic acids research, 33(suppl_1):D407D412, 2005. [54] Chengxin Zhang, Xi Zhang, Lydia Freddolino, and Yang Zhang. Biolip2: an updated structure database for biologically relevant ligandprotein interactions. Nucleic acids research, 52(D1): D404D412, 2024. [55] Tim Kucera, Carlos Oliver, Dexiong Chen, and Karsten Borgwardt. Proteinshake: building datasets and benchmarks for deep learning on protein structures. Advances in Neural Information Processing Systems, 36, 2024. [56] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. Evaluating protein transfer learning with tape. Advances in neural information processing systems, 32, 2019. 14 [57] Baris Suzek, Yuqi Wang, Hongzhan Huang, Peter McGarvey, Cathy Wu, and UniProt Consortium. Uniref clusters: comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6):926932, 2015. [58] Xiuzhen Hu, Qiwen Dong, Jianyi Yang, and Yang Zhang. Recognizing metal and acid radical ionbinding sites by integrating ab initio modeling with template-based transferals. Bioinformatics, 32(21):32603269, 2016. [59] Chih-Hao Lu, Chih-Chieh Chen, Chin-Sheng Yu, Yen-Yi Liu, Jia-Jun Liu, Sung-Tai Wei, and Yu-Feng Lin. Mib2: metal ion-binding site prediction and modeling server. Bioinformatics, 38 (18):44284429, 2022. [60] Qianmu Yuan, Sheng Chen, Yu Wang, Huiying Zhao, and Yuedong Yang. Alignment-free metal ion-binding site prediction from protein sequence through pretrained language model and multitask learning. Briefings in Bioinformatics, 10 2022. ISSN 1477-4054. doi: 10.1093/bib/bbac444. URL https://doi.org/10.1093/bib/bbac444. [61] Dong-Jun Yu, Jun Hu, Jing Yang, Hong-Bin Shen, Jinhui Tang, and Jing-Yu Yang. Designing template-free predictor for targeting protein-ligand binding sites with classifier ensemble and spatial clustering. IEEE/ACM transactions on computational biology and bioinformatics, 10 (4):9941008, 2013. [62] Qianmu Yuan, Sheng Chen, Yu Wang, Huiying Zhao, and Yuedong Yang. Alignment-free metal ion-binding site prediction from protein sequence through pretrained language model and multi-task learning. Briefings in bioinformatics, 23(6):bbac444, 2022. [63] Clement Essien, Duolin Wang, and Dong Xu. Capsule network for predicting zinc binding sites in metalloproteins. In 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 23372341. IEEE, 2019. [64] Chenwei Wang, Haodong Xu, Shaofeng Lin, Wankun Deng, Jiaqi Zhou, Ying Zhang, Ying Shi, Di Peng, and Yu Xue. Gps 5.0: An update on the prediction of kinase-specific phosphorylation sites in proteins. Genomics, Proteomics & Bioinformatics, 18(1):7280, 2020. ISSN 16720229. doi: https://doi.org/10.1016/j.gpb.2020.01.001. URL https://www.sciencedirect. com/science/article/pii/S1672022920300279. [65] Jiaying Huang, Qiupeng Lin, Hongyuan Fei, Zixin He, Hu Xu, Yunjia Li, Kunli Qu, Peng Han, Qiang Gao, Boshu Li, et al. Discovery of deaminase functions by structure-based protein clustering. Cell, 186(15):31823195, 2023. [66] Tadeusz Calinski and Jerzy Harabasz. dendrite method for cluster analysis. Communications in Statistics-theory and Methods, 3(1):127, 1974. [67] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin Shoemaker, Paul Thiessen, Bo Yu, et al. Pubchem 2023 update. Nucleic acids research, 51(D1):D1373D1380, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Architecture The predictor decoder in Prot2Token is an autoregressive transformer that utilizes two information streams: (i) the fused encoder context, derived from protein (and optionally, chemical) embeddings processed and merged by the fusion block, and (ii) sequence of decoder input tokens (Figure 5). The fusion block employs straightforward architecture where, for instance, protein encoder outputs are first augmented with learnable positional encoding and subsequently passed through linear projection layer before being utilized by the decoder. In the standard setting (Figure 5A), the decoder input begins with special <BOS> token followed directly by the tokenized label sequence (e.g., the digits of regression target). Each position attends only to previous tokens via causal masking, while simultaneously receiving global context through cross-attention to the fused encoder features. The training objective is the negative log-likelihood of the full label sequence, so loss is accumulated over every decoder position. For multi-task training, we prepend task token Ti that specifies which prediction head the decoder should emulate (Figure 5B). This token is drawn from its own learnable embedding table and is passed through the same decoder stack as the label tokens, enabling the model to condition its hidden states on task identity. During optimization, we apply the token-weighted loss described in Section 2.2: the task token position is assigned weight w1 = 0, effectively masking it from gradient updates, whereas the remaining positions use token-specific weights wt, allowing each token to be penalized differently during training. This scheme enables the prompt to steer the generation process without being penalized for reconstruction errors. Figure 5: Task-token prompting and loss masking in the Prot2Token decoder. (A) Standard decoding starts with <BOS> token and predicts label tokens, computing loss over all positions. (B) Prompted decoding inserts task token (T1) before labels; this token is zero-weighted in the loss, guiding the model without affecting training error. Together, these mechanisms allow single decoder to (i) handle heterogeneous output formats, (ii) switch tasks via lightweight prompt tokens, and (iii) share parameters across tasks without duplicating specialized heads. Different configuration of Prot2Token is shown in Table 10. Table 10: Prot2Token model configurations. Name Encoder Prot2Token-A Prot2Token-B Prot2Token-C Prot2Token-D ESM2-35m ESM2-650m ESM2-650m ESM2-650m Embedding dimension Feedforward dimension heads Layers Decoder 960 1280 2560 8 8 16 16 4 8 16 16 480 640 640 1280 16 During inference, the decoder generates tokens autoregressively, starting from the initial input (<BOS> or Task token) and predicting one token at time. While various decoding strategies exist for autoregressive transformerssuch as top-k sampling, nucleus (top-p) sampling, and temperaturecontrolled samplingwe focus exclusively on greedy decoding in this work, where at each step the most probable token is selected. Exploring the effects of stochastic sampling methods on prediction performance is left for future investigation. A.2 Tokenization The Prot2Token framework employs distinct tokenization approaches for its input encoders and the output decoder. Input sequences, such as protein amino acid sequences or chemical SMILES representations, are processed using the built-in tokenizers associated with their respective pre-trained encoders. For instance, the protein encoder typically utilizes the character-level tokenizer from models like ESM2 (which includes 33 unique tokens, encompassing standard amino acids and special characters). Similarly, if chemical encoder is used (e.g., for protein-ligand tasks), it would employ its specific tokenizer, such as the unigram tokenizer from BARTSmiles [26](with 1025 unique tokens, including special characters). The core innovation of Prot2Token lies in its unified tokenization strategy developed for the output labels predicted by the autoregressive decoder. This strategy is crucial as it converts diverse biological prediction targets into sequences of discrete tokens. This conversion enables the decoder to handle wide array of tasks through consistent next-token prediction mechanism. All tokenized output sequences are standardized to begin with <BOS> token and end with an <EOS> (end-of-sequence) token. These special tokens clearly define the boundaries of the output sequence for the decoder. The specific methods for tokenizing different types of labels are categorized by the nature of the prediction task (Figure 3): A.2.1 Classification Classification tasks involve assigning one or more categorical labels to protein or pair of proteins. This category includes multi-class, multi-label, and hierarchical classification. Multi-class classification. In multi-class each input (single protein sequence or multiple sequences like protein pair) is assigned exactly one label from predefined set of mutually exclusive classes, and each possible class label is mapped to unique, discrete token. Examples include predicting protein fold class, subcellular localization, or enzyme reaction (ER) categorization. For tasks involving interactions, such as predicting if two proteins interact (a binary classification based on protein pair input), the output is also single token (e.g., Interacted). In general, the target output sequence for the decoder is single token representing the correct class. Multi-label classification. Multi-label is employed when single protein (or input entity) can be associated with multiple labels simultaneously from non-mutually exclusive classes. This is common in tasks such as predicting gene ontology (GO) terms (e.g., GO:0005737 for cytoplasm, GO:0005829 for cytosol) or certain subcellular localization tasks (e.g., DeepLoc 2.0 [34]) where protein might reside in multiple compartments. Each relevant label is converted into its unique token, and these tokens are concatenated into single target sequence, typically sorted alphabetically to ensure consistency (e.g., GO:0005737, GO:0005829). Hierarchical classification. In tasks such as enzyme commission (EC) and ER predictions, proteins are categorized hierarchically. For EC, each enzyme is assigned series of numbers representing its specific catalytic activity. If the goal is to do hierarchical classification, it necessitates specialized tokenization approach. As an example, the EC classification system is divided into four levels: the first level indicates the main enzyme class, the second level specifies the subclass, the third level defines the sub-subclass, and the fourth level denotes the serial number of the enzyme in its sub-subclass. We tokenize each EC number associated with an enzyme into hierarchical sequence of tokens. For example, an enzyme with EC numbers 1.1.1.1 and 2.2.2.2 is tokenized as {ec_1, 1, 1, 1, ec_2, 2, 2, 2}, with each part of the EC number being represented as an individual token. This approach allows the model to capture the hierarchical nature of enzyme classifications effectively, ensuring that the different levels of EC labels are properly represented and learned. In addition to this hierarchical tokenization, we could employ second approach where each complete EC number is treated as unique and distinct code similar to GO datasets. For example, an enzyme 17 with EC numbers 1.1.1.1 and 3.4.24.4 could be tokenized as {ec_1.1.1.1, ec_3.4.24.4}, with each token acting as representative for an entire EC number. This method is also applicable to the ER dataset. This alternative tokenization could yield different results depending on the task. In our early experiments, we found that converting ER labels into hierarchical format reduced performance compared to using multi-label classification format, while the opposite was true for the EC task. However, we did not investigate this thoroughly in our work. A.2.2 Regression Regression tokenization is employed for tasks requiring the prediction of continuous numerical values, represented as either floating-point or integer numbers, derived from single protein sequences, multiple sequences, or protein-ligand pairs. Illustrative examples include the prediction of protein stability (Tm), fluorescence intensity (single sequence input), protein-protein structure similarity scores (multi-sequence input), and protein-ligand affinity (protein sequence and ligand SMILES string as input). Two primary strategies exist for tokenizing such numerical labels. The first, binning, involves discretizing the range of continuous values into predefined number of fixed-size bins. For instance, if target scores range from 0.0 to 10.0, this range could be divided into 1.0-sized bins, yielding 11 distinct token categories. However, this method can suffer from limitations, particularly when data is unevenly distributed, as some bins may contain very few or no samples, leading to imbalanced data representation and potential biases during model training. To circumvent these issues, Prot2Token adopts second approach: digit-by-digit encoding strategy. In this method, each numerical value is transformed into sequence of its constituent characters, including the sign, digits, and decimal point. This technique offers more granular and inherently balanced representation of numerical values, promoting more uniform distribution of data for the model. For example, property value of -0.65 is tokenized into the sequence {minus, 0, ., 6, 5}. Similarly, value of 123.45 would become {1, 2, 3, ., 4, 5}. During the training phase, consistent numerical precision, typically four decimal places, is maintained for all regression labels prior to tokenization. Furthermore, if target values undergo normalization (e.g., to the [0, 1] range), the token sequences predicted by the decoder are first reconverted to numerical form and subsequently de-normalized to their original scale for evaluation. A.2.3 Sequence-to-sequence This tokenization is applied when the output is sequence of labels corresponding residue-by-residue to the input protein sequence, meaning the output token sequence length mirrors the input protein length. Examples include Secondary Structure (SS) prediction, where each amino acid is classified into states like α-helix (H), β-strand (E), or coil (C), forming target sequence like {H, H, C, ...}. Another application is 3D structure prediction using structural alphabets. For instance, the encoder part of pre-trained VQVAE) model ([39]) converts 3D coordinates into sequence of discrete 3D_number tokens (e.g., 4096), where each amino acid corresponds to one 3D_i token encoding 3D structural information. A.2.4 Binding site Binding site prediction involves identifying specific residue indices involved in molecular interactions, such as with ligands, ions, or other proteins. For protein-ligand or protein-ion binding tasks, the binding residues are represented directly by their sorted 1-based indices; for example, if residues at positions 2, 3, 5, and 9 are involved in binding, the target sequence is simply {2, 3, 5, 9}. Self-supervised learning tasks proposed in this work also utilizes index-based tokenization. For example, to predict the positions of all Serine (S) residues in sequence MSGLSNYT (Serines at positions 2 and 5), the target sequence would be {2, 5}. Twenty such tasks can be created, one for each standard amino acid, helping the decoder learn sequence-position relationships. A.2.5 Other types This category includes tasks like PTMs prediction and tasks that combine different output types. PTMs. This involves identifying potential modification sites and the actual modified sites. For single protein sequence input, the target sequence typically lists the 1-based indices of all potential PTM sites (e.g., S, T, for phosphorylation), followed by special separator token (<sep>), and then the 18 1-based indices of experimentally confirmed positive sites, all sorted numerically. For example, for sequence ASSKYKAMTV, phosphorylation prediction might yield {2, 3, 5, 9, <sep>, 3, 9}. In multi-sequence PTM tasks, such as substrate-kinase phosphorylation prediction, the input consists of both substrate and kinase sequences. The output tokenization still focuses on the substrate, listing potential and confirmed phosphorylation sites on the substrate sequence based on the interaction context provided by the kinase. Combination. Tasks like TargetP 2.0 [40] combine classification and regression. For instance, label might be represented as {sp, 96}, where sp is localization class token (Signal Peptide) and 96 is binding site representing the cleavage site position. This is tokenized by concatenating these two types. A.3 Dataset To assess Prot2Token across representative spectrum of proteinbiology problems, we assembled datasets from several public repositories and task-specific benchmarks. The statistics of each task are shown in Table 11. Table 11: Dataset Statistics Overview. This table presents the details of the datasets utilized in this study. : Randomly 300k of samples are used for the training in each fold. Dataset Train Validation Test Task Type Enzyme commission [41] Gene ontology [42] Fold classification - Fold [43] Enzyme reaction [44] Human PPI [27] DeepLoc 2.0 [34] Kinase group classification [37] Mutation stability [30] Structure similarity[28] Protein-ligand affinity [27] Protein-protein binding affinity [45] Stability [27] Fluorescence [27] Thermostability [46] Protein-protein binding site [47] Protein-ligand binding site [47] Structure prediction [21] Secondary structure [27] Target-P 2.0 [40] PTMs Kinase phosphorylation [37] 15,550 29,898 12,312 29,215 35,669 22,841 5,382 1.92 million 300,700 16,436 765 53,571 21,446 131,260 759,282 16,796 10,876,251 8,678 10,400 Table 12 5,382 1,720 3,322 736 2,562 315 5,462 969 480,000 (5-fold) 4,560 937 180 2,512 5,362 14,584 2,918 2,644 5,000 2,170 2,605 Table 12 969 1,919 3,415 718 5,651 237 1,717 - - 4,851 285 270 12,851 27,271 36,461 5,499 5,153 5,000 513 - Table 12 146 Classification Classification Classification Classification Classification Classification Classification Regression Regression Regression Regression Regression Regression Regression Binding site Binding site Sequence to sequence Sequence to sequence Other (classification, regression) Other (PTM) Other (PTM) A.3.1 PEER benchmark The PEER benchmark [27] provides unified evaluation suite for protein sequence understanding, integrating datasets for protein function, subcellular localisation, secondary structure, proteinprotein interaction (PPI), and proteinligand affinity prediction. Each task is delivered with homology-aware train/validation/test splits and pre-defined evaluation metrics, enabling direct comparison between conventional feature-engineering pipelines, sequence-embedding models, and large-scale protein language models. From PEER we adopt five datasets that align with our experimental focus: (i) human PPI pairs for binary interaction prediction, (ii) secondary-structure assignments for residuelevel sequence-to-sequence labelling, (iii) fluorescence intensities for single-sequence regression, (iv) stability (Tm) measurements for mutation-effect regression, and (v) proteinligand affinity (PLA) scores for sequenceSMILES binding prediction. A.3.2 DeepLoc 2 For subcellular localization we adopted the DeepLoc 2.0 dataset [34], which assigns up to ten compartment labels per eukaryotic protein: Cytoplasm, Nucleus, Extracellular, Cell membrane, Mitochondrion, Plastid, Endoplasmic reticulum, Lysosome/Vacuole, Golgi apparatus, and Peroxisome. DeepLoc 2.0 provides five-fold homology partition with maximum 30 % pairwise sequence identity between folds. In our experiments the first four folds are merged for training, while the fifth fold serves as the validation set. Evaluation is performed on the independent Human Protein Atlas (HPA) test set released with DeepLoc 2.0, which contains experimentally verified localizations for six 19 compartments (Cytoplasm, Nucleus, Cell membrane, Mitochondrion, Endoplasmic reticulum, and Golgi apparatus). Final performance is reported on this HPA test set. A.3.3 PTMs In this section, we describe the process of collecting PTM data. While numerous databases and publications provide PTM data, most only offer sequence fragments, typically 21 amino acids long, with the PTM located at the center position. The largest database with PTM annotations is UniProt [48] , which contains over 200 million protein sequences and provides annotations for more than 200 PTM types and their respective positions for some sequences. We downloaded full-length protein sequences and PTM annotations from UniProt, focusing on annotations in the Modified Residue, Lipidation, Glycosylation, and Cross-link sections and performed an advanced search in these sections using wildcard (*) to retrieve all values. This resulted in 106,195 protein sequences from the Reviewed (Swiss-Prot) [49] dataset and 4,173,205 sequences from the Unreviewed (TrEMBL) dataset. To ensure data quality, we exclusively used the protein sequences from the Reviewed (Swiss-Prot) dataset. We downloaded the 106,195 protein sequences as JSON files for further processing, only sequences with lengths of 1,022 amino acids or fewer were retained. Next, CD-HIT [50] was applied to cluster the sequences based on similarity threshold of 40% (c=0.4), grouping sequences with similarity above 40% into the same cluster. Subsequently, we split the data into training and testing sets in 4:1 ratio, ensuring that sequences within the same cluster were assigned to the same dataset. Given the distribution of PTM types, we focused on six types for this study: Phosphorylation (S), Methylation (R), N-glycosylation (N), O-glycosylation (T), Acetylation (K), and Ubiquitylation (K). Table 12 shows the statistics of the PTM datasets. Table 12: Statistics of PTM datasets. PTM type Ubiquitylation Phosphorylation Acetylation Methylation N-linked Glycosylation O-linked Glycosylation Succinylation Annotation in Uniprot Glycyl lysine isopeptide (Lys-Gly) (interchain with G-Cter in ubiquitin) Phosphoserine N6-acetyllysine Omega-N-methylarginine N-linked (GlcNAc...) asparagine O-linked (GalNAc...) threonine N6-succinyllysine Amino acid R Number of sequences 2,370 34,260 9,115 1,813 30,310 568 2,392 Number of positions 5,029 121,398 23,615 3,279 11,5767 2,723 7,446 A.3.4 Kinase-specific phosphorylation sites The dataset was gathered from GPS 6.0 [37] and contains 24,160 phosphorylation sites. We mapped IDs from the UniProt database [48] and obtained 13,374 sequences with kinase information. To retrieve kinase sequences, we used Kinase.com and the UniProt database. To reduce sequence similarity, we applied CD-HIT [50] with 70% similarity threshold to group similar protein substrate sequences. We kept representatives from each cluster and selected positive substrate-kinase pairs using two criteria: (1) cross-cluster selection, where pairs from different clusters were kept to increase diversity, and (2) within-cluster selection, where only one unique kinase pair per cluster was retained to avoid repetition. The final dataset includes kinase sequences, kinase information (group/family/kinase), substrate UniProt IDs, substrate sequences, and phosphorylation sites. It contains 386 kinase types across 12 groups. The dataset was randomly split into training (5,382 unique substrates) and validation (969 unique substrates) sets. To ensure rigorous evaluation, we defined three distinct test sets, carefully designed to prevent any data contamination between the test, training, and validation sets: Rare-Group Test Set. This set includes 14 samples from two rare kinase groups, RGC and PKL, which have limited number of available samples. These groups were completely excluded from the training set to assess the models ability to generalize to underrepresented kinase groups. This dataset is specifically used for evaluating on phosphorylation site prediction. GPS-Test Set. To have direct comparison with existing methods such as GPS 6.0, we adopted the test set used in the GPS study. This dataset contains 146 samples of substrate-kinase pairs, including phosphorylation site and kinase group annotations. All samples belong to the CMGC kinase group. Table 13 presents the number of samples in each set, while Table 14 details the distribution of samples across kinase groups in each dataset. 20 Table 13: Dataset statistics, including the number of samples, phosphorylation sites (p-sites), and kinase groups for the training, validation, GPS test, and rare group test sets, along with overall dataset totals. Dataset All samples Training set Validation set GPS-test Rare-Group Number of samples Number of p-sites Number of groups 13,374 10,621 2,455 278 6,511 5,382 969 146 14 12 10 9 1 2 Table 14: Distribution of samples across kinase groups for the training, validation, GPS test, and rare group test sets. Group AGC Atypical CAMK CK1 CMGC Other STE TK TKL RGC PKL Training set Validation set GPS-test Rare-Group 1,446 270 653 100 1,466 491 211 677 68 - - 231 58 96 27 264 99 34 149 11 - - - - - - 146 - - - - - - - - - - - - - - - 2 12 A.3.5 Protein mutation stability In this study, we used the supervised Deep Mutational Scanning (DMS) cross-validation subset of the ProteinGym [30] benchmark, large-scale and standardized resource for evaluating protein fitness prediction models. The supervised DMS dataset comprises over 250 high-throughput assays, covering more than 2.4 million amino acid substitutions across 217 proteins, and approximately 300,000 indel mutations across 66 proteins. Each assay provides experimentally measured phenotypic effects for wide range of mutations, reflecting properties such as thermostability, binding affinity, aggregation, and viral replication. We followed the five-fold cross-validation protocol defined by ProteinGym, conducting five independent training runs, each on 300,000-sample subset of the full dataset due to computational constraints. ProteinGym categorizes benchmarks by mutation type (substitutions vs. indels) and ground-truth source (DMS assays vs. clinical annotations); in this work, we utilized only substitution dataset within the supervised regime. A.3.6 Protein melting temperature We leveraged the HotProtein [46] sequence-only benchmark to predict protein melting temperatures from primary sequence alone. HotProtein comprises 182,000 amino acid sequences of 230 organisms, each labeled with the optimal growth temperature of its source organism (-20C to 120C) as lower bound proxy of the true melting temperature of the protein. For evaluation, the ProCeSa [51] paper defined 10-fold cross-validation splits on various subsets of the dataset, such as HP-S2C2 (binary: hot vs. cold), HP-S2C5 (five-class), and HP-S (full dataset). In our study, we used only the first fold of the provided splits, further dividing the training portion into training and validation sets. A.3.7 3D structure prediction For training our model on sequence-to-structure prediction, we constructed large-scale dataset from UniRef50 [48] , redundancy-reduced cluster of protein sequences derived from UniProt. This provided approximately 67 million unique sequences. We mapped these sequences to their predicted structures using the UniProt AF2 Structural Database [21], yielding 40 million PDB files. To ensure high structural confidence, we filtered out structures with mean pLDDT scores below 0.85, resulting in about 11 million high-confidence entries. From this filtered pool, we randomly selected 5,000 PDBs each for validation and test sets, ensuring all selected structures had average pLDDT scores above 0.90. The remaining structures were used for training. All 3D structures were converted into discrete token sequences using pre-trained VQ-VAE model [39], enabling their use as target labels for autoregressive sequence-to-structure modeling. The continuous automated model evaluation (CAMEO) [35] platform offers continuous, automated benchmarking of protein structure prediction methods by evaluating their performance on newly released target sequences each week, providing real-time complement to the biennial CASP experiment. In this study, we used CAMEO targets released between January 2024 and January 2025, comprising 668 protein sequences. After filtering for sequences between 50 and 512 amino acids in length, the final dataset contained 576 sequences. A.3.8 Protein-protein affinity We used data from PPB-Affinity [45], the largest publicly available dataset for protein-protein binding (PPB) affinity. PPB-Affinity provides key information, including crystal structures of protein-protein complexes, PPB affinity values, receptor protein chains, and ligand protein chains. Since PPB-Affinity does not include protein sequences, we retrieved them from the RCSB Protein Data Bank (PDB) [52] based on the protein names provided in PPB-Affinity. To construct relevant dataset for our model, we applied the following filtering steps: 1. Chain Filtering We removed samples containing more than two chains, retaining only those with single receptor chain and single ligand chain. 2. Mutation Removal Samples containing mutated sequences were excluded. 3. Affinity Label Processing For identical protein complexes with multiple PPB affinity values, we averaged the KD (M) values to obtain single affinity label. 4. Data Splitting The final dataset was split into training (50%), validation (20%), and testing (30%) sets, resulting in 765, 180, and 270 samples, respectively. The (KDKD) values, representing dissociation constants, were preprocessed to ensure numerical stability and improve model performance. First, log10 transformation was applied to address the wide dynamic range and skewed distribution of KD values, using the formula: KDlog = log10(KD + ϵ), where ϵ = 1016 prevents undefined values for extremely small inputs. The logtransformed values were then normalized to range between 0 and 1 using Min-Max scaling based on the training datasets minimum and maximum KDloglog values. Importantly, during model metric calculation and evaluation, both the log-transformation and normalization effects were reversed, ensuring that the calculated metrics accurately reflect the original KD scale. This preprocessing pipeline provided consistent and interpretable representation of KD values for both model training and evaluation. A.3.9 Gene ontology The GO knowledge-base provides curated associations between protein sequences and hierarchically organized terms spanning three sub-ontologies: Molecular Function, Biological Process and Cellular Component [42]. We downloaded the most recent GOA-UniProt annotation file, removed electronically inferred codes (IEA) and retained only leaf-level terms, yielding multi-label dataset in which each protein can carry dozens of GO terms. Following the convention in [42], proteins were clustered at 30 % global sequence identity with MMseqs2; clusters were then split 80 / 10 / 10 into training, validation and test partitions to avoid homologous leakage. Labels are tokenised as individual GO identifiers in alphabetical order, in accordance with the scheme in Section 2.2. A.3.10 Enzyme reaction The ER corpus collates detailed reaction schemata and catalytic-site annotations for enzymes, originally introduced by Webb [44]. We used the reactionprotein mappings distributed via EzCatDB [53], which capture bond-level changes and catalytic residue motifs. Each protein may participate in multiple reactions, making ER multi-label classification task. Sequences were clustered at 40 % identity and split into 70 % training, 15 % validation and 15 % test sets. Reaction identifiers were tokenised as discrete labels; hierarchical relations (substrate product) are ignored in this work. A.3.11 Enzyme commission The EC hierarchy assigns four-level numerical code to every known enzymatic function [41]. We retrieved the full set of Swiss-Prot entries with experimentally verified EC numbers from the UniProt enzyme.dat archive [41]. Proteins were redundancy-reduced at 40 % identity and stratified into train/val/test splits by superfamily. Tokenisation follows the hierarchical scheme in Section 2.2: 22 each digit of the EC code is emitted as an independent token (e.g. 1,1,1,1). This framing yields four-step sequence-to-sequence prediction task. A.3.12 Fold classification For remote-homology evaluation we use the dataset released with DeepSF [43], which maps protein sequences onto 1,195 folds derived from CATH [29] and SCOP. The authors provide non-redundant splits with maximum 40 % sequence identity between training (12,312 proteins), validation (736 proteins) and test (718 proteins) sets[43]. Each fold ID is tokenised as single class token, rendering the task large-scale multi-class classification benchmark. A.3.13 TargetP 2.0 localization TargetP 2.0 offers homology-partitioned dataset for predicting Nor C-terminal targeting peptides and corresponding subcellular localizations [40]. We downloaded the FASTA sequences and label CSVs from the official service repository. After filtering fragments and sequences shorter than 50 residues, the data comprise ten localization classes (Chloroplast, Mitochondrion, Secretory pathway, etc.), with an external HPA test set for human proteins[40]. We adhere to the original nested crossvalidation splits for training and use the HPA subset exclusively for final evaluation, casting the task as multi-class prediction with one token per localization label. A.3.14 Protein-ligand binding site BioLip2 [54] is one of the most comprehensive databases for ligand-protein interactions, primarily derived from the PDB database. Each entry in BioLip2 includes detailed annotations on ligandbinding residues, ligand-binding affinities, catalytic site residues, EC numbers and GO terms. The database is also cross-linked with external resources, including RCSB PDB, UniProt, and PubMed. To obtain protein sequences, we used receptor sequences clustered at 90% identity cutoff. For annotations, we retrieved data for each ligand-protein interaction site. To increase the complexity of binding site prediction and enhance model robustness, we further clustered the data at 40% identity cutoff. This additional clustering step helps prevent data leakage between training, evaluation, and testing datasets. We first removed DNA and RNA sequences and excluded any sequences with fewer than 50 residues. Next, we generated FASTA files containing residues and annotations for all 5,717 ligands. We then applied threshold cutoff, selecting ligands that bind with over 100 sequences, resulting in 41 ligands. We aimed to balance selecting the most significant ligands based on literature review while ensuring sufficient number of samples for training and testing the model. We used CD-HIT to cluster the data with 40% identity cutoff before splitting the data into training, evaluation, and testing datasets. Because of the limited number of samples and to ensure sufficient data for testing, we used two splitting ratios: 70%, 10%, and 20% for training, evaluation, and testing, respectively, for the first 30 ligands in Table 15, and also, 50%, 20%, and 30% for training, evaluation, and testing, respectively, for the remaining ligands. 23 Figure 6: Tokenisation workflow for proteinprotein binding sites. distance cut-off is applied to residueresidue distance matrix derived from the PDB complex to flag contacting residues. Rows with at least one contact are collapsed into sorted list of residue indices, which becomes the target token sequence. A.3.15 Protein-protein binding site To construct dataset for protein binding site prediction, we used the PPIRef [47] pair dataset, which specifies interacting protein chain pairs based on contact threshold. To ensure high-quality and complete data, we retrieved all corresponding PDB entries from the PDB databas and extracted the relevant chains based on PPIRef annotations. For each protein complex, we extracted the amino acid sequences and computed residue-level binding sites by analyzing spatial proximity. Specifically, we calculated the centroid of each residue by averaging the atomic coordinates (excluding hydrogens), then computed pairwise distance matrix between all centroids from the two chains. Residues were labeled as binding site residues if any cross-chain centroid distance fell below 6Å threshold (Figure 6). To augment the dataset, we alternated which chain was considered the \"target\" and which was the \"binder\" in each complex. The resulting dataset includes the fields: PDB ID, Target Chain, Binder Chain, Target Sequence, Binder Sequence, Target Binding Sites, and Binder Binding Sites. For training and evaluation, we performed randomized split grouped by PDB IDs, ensuring that each PDB complex appears in only one of the train, validation, or test sets to avoid data leakage. A.3.16 Protein-protein structure similarity ProteinShake [28] is Python toolkit developed to streamline dataset construction and benchmarking in protein structure-based deep learning. It supports both custom and pre-processed datasets sourced from the PDB database and AFDB, and associates each dataset with well-defined prediction tasks and evaluation metrics. The framework includes standardized data splits based on sequence and structural similarity, enabling rigorous and reproducible comparisons across models and modalities (e.g., graphs, voxel grids, and point clouds). In this work, we adopt the protein-protein structure similarity dataset provided by ProteinShake and follow their Structure Split protocol, applying 70% similarity threshold to partition the data for evaluation. Notably, we use only the protein sequence information as input and do not leverage 3D structural features. 24 Ligand No Chemical Formula 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Zn2+ CA2+ CLA FAD HEM NAD ADP MG2+ NAP ATP HEC SF4 FMN SAH NDP ANP GDP GLC PLP MN2+ COA SAM AMP BGC FE3+ MAN FES PO3 4 GTP UDP CU2+ GSH AGS ACO GAL SO2 4 CLR Y01 BMA FE2+ CO2+ Table 15: Dataset statistics of all ligands. Name Zinc Ion Calcium Ion Chlorophyll Flavin-Adenine Dinucleotide Heme Nicotinamide Adenine Dinucleotide Adenosine Diphosphate Magnesium Ion Nicotinamide Adenine Dinucleotide Phosphate Adenosine Triphosphate Heme Iron/Sulfur Cluster Flavin Mononucleotide S-Adenosyl-L-Homocysteine Nucleotide Diphosphate Adenylyl-imidodiphosphate Guanosine Diphosphate Glucose Pyridoxal-5-Phosphate Manganese Ion Coenzyme S-Adenosylmethionine Adenosine Monophosphate Beta-D-Glucose Ferric Ion Mannose Iron-Sulfur Cluster Phosphate Ion Guanosine Triphosphate Uridine Diphosphate Copper Ion Glutathione Agmatine Sulfate Aconitase Galactose Sulfate Ion Cholesterol Cholesterol Hemisuccinate Beta-Mannose Ferrous Ion Cobalt Ion BioLip Fasta Name Num Sequences Binding Sites ZN.fasta CA.fasta CLA.fasta FAD.fasta HEM.fasta NAD.fasta ADP.fasta MG.fasta NAP.fasta ATP.fasta HEC.fasta SF4.fasta FMN.fasta SAH.fasta NDP.fasta ANP.fasta GDP.fasta GLC.fasta PLP.fasta MN.fasta COA.fasta SAM.fasta AMP.fasta BGC.fasta FE.fasta MAN.fasta FES.fasta PO4.fasta GTP.fasta UDP.fasta CU.fasta GSH.fasta AGS.fasta ACO.fasta GAL.fasta SO4.fasta CLR.fasta Y01.fasta BMA.fasta FE2.fasta CO.fasta 4665 3043 342 825 845 658 941 2951 462 680 264 509 437 392 243 354 339 454 377 789 259 214 275 331 532 446 272 378 150 154 331 200 136 108 233 218 176 106 158 186 160 23310 22161 17690 16583 13118 10615 9899 9494 8108 7635 7296 5834 5789 4675 4301 3861 3792 3674 3608 3315 2870 2540 2430 2375 2268 2047 1986 1908 1724 1601 1530 1516 1512 1482 1188 1177 1112 991 696 675 660 A.4 Experiments A.4.1 Classification For the Fold classification task, we maintained the ESM model weights as fixed and only unlocked its last six layers to be fine-tuned and connected to the decoder. Many classes in this dataset have low number of samples, e.g., one sample for high number of classes. That is why we saw unstable training when we did single-task training on Prot2Token. However, when we combined Fold classification with auxiliary tasks like ER, the training became stable  (Table 29)  . Table 16: Fold classification training in single-task and multi-task training on Fold-fold test set. Method Aux-Tasks Accuracy Baseline (ESM2-650m) Prot2Token-B Prot2Token-B - - ER 32.87 N/A 31.47 Regarding the Human PPI task, we maintained the ESM model weights as fixed and only unlocked the last four layers of it to be fine-tuned and connected to the decoder. Note that to give the encoder two sequences at one feed for PPI, we concatenated two sequences using the <EOS> token. We observed that adding more tasks helped boost the performance of Human PPI  (Table 16)  . However, Prot2Token tended to overfit on this task, indicating that the improvement from adding auxiliary tasks may be due to the regularization effect of multi-task learning. 25 Table 17: Human PPI performance on PEER test set. Method Aux-Tasks Accuracy Encoder PEER [27] (fine-tuned) Prot2Token-B Prot2Token-B Prot2Token-B - - Deeploc Deeploc+ER+Fold 78.17 71.3 78.48 80. ESM1-1b ESM2-650m ESM2-650m ESM2-650m For the GO and EC tasks, we encountered limitation in calculating the Fmax metric, which is commonly used for performance evaluation in these tasks. Instead, we used accuracy and F1 score to assess our models performance. Consequently, we were unable to directly compare our results with those of other methods that report their performance in terms of Fmax. This discrepancy highlights significant challenge in benchmarking our approach against existing methods. The GO tasks are further divided into three categories: biological process (BP), molecular function (MF), and cellular component (CC). We jointly trained all four tasks (the three GO tasks and the EC task) together in multi-task learning manner. Detailed performance metrics for these tasks are presented in Table 17. We maintained the ESM model weights as fixed and only unlocked the last four layers of it to be fine-tuned and connected it to the decoder and linear classifier for Prot2Token. Note that labels in these tasks are highly imbalanced. Table 18: Comparing GO and EC tasks with the baseline on accuracy and F1 score metrics. The baseline is linear evaluation of ESM. All methods are based on ESM2-650m. Method Task Accuracy F1 Score Baseline Baseline Baseline Baseline Prot2Token-B Prot2Token-B Prot2Token-B Prot2Token-B EC GO-BP GO-MF GO-CC EC GO-BP GO-MF GO-CC 99.79 N/A N/A N/A 99.85 95.88 97.20 95.35 0.5383 0.0043 0.1028 0.1327 0.6796 0.0103 0.0116 0.0089 Next, we aimed to predict kinase groups based on substrate sequences. Specifically, we investigated how much information about the related kinase groups the model can infer solely from substrate sequences. To achieve this, we considered our processed training and validation datasets (refer to Appendix A.3), assigning multi-label classification labels by removing Unknown, RGC, PKL, and UNK samples from the training set and merging the remaining nine kinase groups associated with each substrate. The model takes substrate sequence as input and predicts the corresponding kinase groups in alphabetical order. We allow Prot2Token to fine-tune the weights of the last 6 blocks of the protein encoder (ESM2-650m). After convergence, the decoder achieves the per-group F1 scores listed in Table 19. Despite receiving no kinase information at inference time, Prot2Token recovers group memberships with macro-averaged F1 of 0.54, confirming that substrate context alone encodes considerable family-specific signal. Table 19: Per-group F1 scores for substrate-only kinase group classification via Prot2Token-C. Group AGC Atypical CMGC CAMK CK1 Other 0.605 0.634 0.493 0.555 0.420 0. F1 STE 0.357 TK 0.674 TKL 0.500 26 Figure 7: UMAP visualization of unique kinase sequences on the original and fine-tuned checkpoints of ESM2-650m. Figure 8: t-SNE visualization of unique kinase sequences on the original and fine-tuned checkpoints of ESM2-650m. Table 20: Unsupervised clustering metrics for kinase embeddings. Larger values denote better separability. Metric Original Silhouette (cosine) 0.039 CalinskiHarabasz 7.00 Fine-tuned 0.091 24. To further interpret the kinase group classification results, we analyzed the sequence embeddings of all unique kinase sequences present in the GPS 6.0 dataset before and after fine-tuning the protein encoder part of Prot2Token-C on the kinase group classification labels. Sequences from the RGC, PKL, and Unknown groups were excluded. Each remaining kinase sequence was passed through the pre-trained ESM2-650m model with maximum input length of 2048. Token-wise embeddings were extracted, then trimmed to remove the <BOS> and <EOS> tokens, and average pooling was applied to yield fixed-length 1280-dimensional representation per sequence, aligned with the models hidden size. We performed dimensionality reduction using t-SNE and UMAP to visualize these embeddings in two dimensions based on their known group assignments. Visualizations revealed that the original pretrained model exhibited weak separation among groups. However, when we repeated the same process using the fine-tuned ESM2 checkpoint (updated only via substrate-based kinase group classification), the resulting projections displayed improved clustering by group. These qualitative trends were confirmed with unsupervised clustering metrics, including the silhouette score and the Calinski-Harabasz index. As shown in Figures 7 and 8 and Table 20, the fine-tuned encoder 27 demonstrates clearer group structure despite never directly observing kinase sequences during trainingsuggesting that supervised signals from substrates alone can reorganize the encoders representation space in biologically meaningful way. A.4.2 Regression In the regression experiments, we fixed the majority of the ESM encoder parameters, unfreezing only the last six layers for joint fine-tuning with the decoder. For comparison, baseline models attach single linear regression head to frozen encoder. Because Spearman correlation is invariant to monotonic transformations, we found that minmax scaling the labels to [0, 1] with four digits precision after floating point markedly improved convergence and performance: the decoder learns the structure of numerical outputs more rapidly when they occupy consistent range. To evaluate sequence-based prediction of structural similarity, we tokenized the ProteinShake structure-similarity dataset (Structure Split) and concatenated each pair of sequences with an <EOS> separator. Only the last four encoder blocks were trainable, and batches contained 128 sequence pairs. Results are summarised in Table 18. Table 21: Proteinprotein structure similarity on the ProteinShake test set (Structure Split). All ProteinShake baselines rely on 3-D structural input; denotes linear layer fine-tuned on the last four encoder blocks. Method Spearman ρ ESM-2 ProteinShake (Graph) [55] ProteinShake (Point) [55] ProteinShake (Voxel) [55] Prot2Token-C 0.4653 0.5180 0.5640 0.5730 0. Regarding the protein-protein affinity task, the labels were normalized to [0, 1]. We used the same hyperparameters of structure-similarity task, with freshly initialized decoder. Performance, reported as RMSE (lower is better), appears in Table 21. Table 22: Proteinprotein binding affinity prediction on PPB-Affinity. Method RMSE () Prot2Token-C baseline [45] 1.6632 2. For the HotProtein HP-S regression split we applied the same minmax label normalisation as in the other regression tasks. Results are reported in Table 22, improving upon the HotProtein by roughly 2.53.0 pp in both correlation metrics. Table 23: Performance of predicting thermostability on HotProtein (HP-S test split, fold 1). Method Spearman Pearson TAPE [56] ESM-1B HotProtein [46] Prot2Token-C 0.504 0.807 0.823 0.8437 0.453 0.809 0.827 0.8439 A.4.3 Self-supervised pre-training of decoder In our preliminary experiments with the phosphorylation PTMs and protein-ligand binding site tasks, we initially focused on directly predicting positive sites using the Prot2Token framework. However, we observed suboptimal performance despite experimenting with different label formatting methods. Upon further analysis, we hypothesized that this issue arose primarily from the lack of inductive biases inherent to specialized models, which were missing in the Prot2Token models decoder due to its random initialization. 28 Figure 9: Illustration of self-supervised pre-training tasks designed for the decoder. For each amino acid (e.g., S, T, P, M, L), the model is trained to predict the positions of its occurrences within given protein sequence. Highlighted residues are the targets, and the output is list of their corresponding indices. This enables the decoder to learn position-aware amino acid representations in label-free manner. Specialized baseline approaches commonly restrict the prediction space by focusing on specific amino acids known to undergo modifications, such as serine (S), threonine (T), and tyrosine (Y) in phosphorylation tasks. These approaches implicitly encode biases about label structures into their predictive mechanisms. Conversely, Prot2Token, being an autoregressive model with randomly initialized decoder, initially lacked these intrinsic biases, severely impacting its predictive accuracy, especially in tasks with extensive label vocabularies. To address this challenge, we introduced self-supervised pre-training strategy to effectively embed inductive biases into the decoder before fine-tuning it on the main supervised tasks. The key idea the decoder is trained to behind this self-supervised approach is straightforward yet effective: recognize amino acid positions within sequences (Figure 9). For instance, given an amino acid sequence such as MSGLSNYT, the model learns to output positional indices 2, 5 corresponding to the amino acid S. We constructed twenty such self-supervised tasks, each dedicated to recognizing the positions of different amino acid type. Importantly, generating these self-supervised samples does not require human annotation, making it cost-effective method to improve model initialization and predictive performance. Our empirical results, presented in Table 23, demonstrate clear positive correlation between the volume of self-supervised auxiliary samples and model performance improvements on the phosphorylation task. Notably, incorporating broader range of amino acids, such as K, N, and R, alongside the typical phosphorylation targets (S, T, Y), significantly boosted model accuracy, highlighting the utility of teaching these biases to the decoder. Table 24: Phosphorylation site prediction. \"Aux\" denotes self-supervised auxiliary tasks. All results are based on Prot2Token-B model. Data Accuracy F1 Phosphorylation Phosphorylation + STY-Aux (150k) Phosphorylation + STY-Aux (250k) Phosphorylation + STYKNR-Aux (250k) 55.69 74.57 91.49 94.14 0.0198 0.0592 0.1799 0.3052 Furthermore, Figures 10 and 11 illustrate the frequency distribution of labels in phosphorylation and protein-ligand binding site tasks, respectively. These figures clearly show the imbalanced and sparse nature of labels, underscoring why explicit inductive biases provided through self-supervised pre-training are crucial for effective model training. We pre-trained the decoder once using 20 self-supervised taskseach targeting the positional prediction of one amino acid typeto serve as general-purpose initialization for all downstream tasks involving binding site prediction. This avoids the computational cost of re-adding auxiliary self29 supervised tasks per downstream task, while still equipping the decoder with biologically meaningful priors. Figure 10: Distribution of phosphorylation-site indices in the training set (n = 11,449 sites across 5,694 peptides). Only residues at positions 2048 are shown; 176 rarer sites at higher indices were excluded. Each bar corresponds to single residue position (bin width = 1). Figure 11: Distribution of proteinligand binding-site indices aggregated across all 41 ligand classes in the training set. Bars represent individual residue positions (bin width=1). Sites located beyond residue 2048 (< 2 % of all annotated positions) were excluded for clarity. critical consideration in applying this self-supervised learning strategy is maintaining frozen encoder during the pre-training phase. Allowing updates to the encoder parameters at this stage can inadvertently introduce shortcut learning effects, causing the model to collapse and diminishing its predictive capabilities on downstream tasks. Consequently, freezing the encoder ensures that the decoder robustly learns meaningful positional and structural biases, significantly enhancing its predictive performance on binding site types of tasks. We randomly sampled 4 million protein sequences from the UniRef50 database [57] for training and 4k for validation data. From them, we artificially created 80 million and 20k self-supervised samples, subsequently, by crafting each amino-acid-type/protein as one sample. Again, we sampled 1 million and 1k samples from them, respectively, to build the training and validation sets. We used input sequence length of 2048, weight decay of 0.01, and batch size of 192 samples, equivalent to 73,728 tokens. Also, we set the warmup steps to 512. We only froze the encoder weights and made other parameters trainable. After training for 16 epochs, the model showed perplexity of 2.31 on the validation set. This indicates that it almost perfectly converted the embeddings from the encoder back to their original protein sequences by learning to find the locations of each type of amino acid. 30 A.4.4 Binding site Based on the order of ligands presented in Table 24, we grouped the ligands into distinguishable sets of 10, 20, 30, and all 41 ligands. Each ligand in set was treated as separate task defined by task token figure 12, and trained together in one training session. We selected each of those sets and jointly trained them alongside 20 self-supervised tasks using the latest checkpoint from the self-supervised pre-training phase. For this fine-tuning phase, the self-supervised tasks were reduced to total number of 20k samples. Also, we removed protein samples with lengths greater than 1280 and set batch size to 98,304 tokens. During all training processes, only the last eight blocks of the encoder (ESM2-650m) were fine-tuned, while all non-encoder parameters of the supermodel were fully fine-tuned. Figure 12: Jointly training proteinligand binding-site across 41 types of ligands by representing ligands with task tokens. It is worth noting that while we could have excluded the self-supervised tasks entirely from the fine-tuning stage, retaining portion of these samples resulted in measurable improvement in the models performance on the supervised protein-ligand tasks. Direct comparison of our method with other available methods was not straightforward due to several technical issues and potential overlap between their training data and our test sets; however, results of the comparison are provided in Table 25. For fine-tuning on the protein-ligand datasets, the model was trained on combined training set of selected ligands. During training, validation was performed for each ligand individually, and the best checkpoint for each ligand was saved based on its validation set performance. At the end of training, these best checkpoints were evaluated on their respective test sets. Figure 13 shows the average validation F1 score across epochs, with the highest average performance observed at epoch 30. However, this checkpoint showed slightly lower average test performance compared to using individual best checkpoints for each ligand. The results for all ligands are presented in Table 25. To compute the metric for the autoregressive models output, each amino acid in protein was treated as an individual positive or negative sample. Predicted binding residues from the decoder were considered positive samples, while all other amino acids were treated as negative (zero) samples. The metrics were then calculated based on this classification. To provide comparison of our models performance with other available methods, we present the results in Table 26. However, the comparison process faced several challenges: some web servers were not operational during testing, while others only allowed predictions on individual samples, making bulk evaluation difficult and very slow to response. We attempted to evaluate IonCom [58], and MIB2 [59] server tools, but encountered several issues: MIB2 had extremely slow response times, and IonCom imposed strict sample limitations for evaluation. 31 Table 25: F1 scores of positive labels for all ligands across different training configurations, with varying numbers of auxiliary ligands on the test sets. The table summarizes the impact of jointly training with 10, 20, 30, and 41 ligands on binding-site prediction. * indicates that pre-trained decoder weights were not used, and indicates that self-supervised tasks were excluded during supervised training. Ligands Zn2+ Co2+ CLA FAD HEM NAD ADP Mg2+ NAP ATP Average (top 10) HEC SF4 FMN SAH NDP ANP GDP GLC PLP Mn2+ Average (top 20) COA SAM AMP BGC Fe3+ MAN FES PO3 4 GTP UDP Average (top 30) Cu2+ GSH AGS ACO GAL SO2 4 CLR Y01 BMA Fe2+ Co2+ Average (all) 10 tasks * 0.0678 0.1022 0.2749 0.1744 0.243 0.1662 0.1105 0.482 0.1559 0.1059 0.1883 10 tasks* 0.0657 0.0888 0.2519 0.1476 0.232 0.1248 0.1053 0.0326 0.1417 0.0927 0.1283 10 tasks 0.7434 0.6566 0.477 0.6882 0.6554 0.6862 0.6057 0.4466 0.6629 0.4538 0.6076 20 tasks 0.7498 0.6493 0.3763 0.6565 0.6698 0.6851 0.5779 0.4603 0.6813 0.4355 0.5942 0.6438 0.6508 0.6921 0.6385 0.7122 0.6153 0.5948 0.2091 0.7770 0.7278 0.6102 30 tasks 0.7594 0.6472 0.4936 0.6473 0.6871 0.6862 0.5897 0.4522 0.6861 0.5317 0.6181 0.6511 0.5840 0.6983 0.6473 0.7085 0.6214 0.6335 0.2237 0.7780 0.7245 0.6172 0.3978 0.6355 0.4316 0.2165 0.6756 0.1407 0.7162 0.2288 0.5332 0.5522 0.5660 41 tasks 0.7575 0.6474 0.4762 0.6537 0.6796 0.6952 0.5834 0.4575 0.6746 0.505 0.6130 0.6537 0.5685 0.6945 0.6503 0.6979 0.6217 0.6465 0.2214 0.7620 0.7376 0.6130 0.4011 0.6252 0.4432 0.1932 0.6606 0.1216 0.7018 0.2278 0.5461 0.5391 0.5615 0.5607 0.6924 0.5301 0.5026 0.2762 0.1386 0.0373 0.0419 0.2273 0.6033 0.5170 0.5115 Additionally, potential overlap between the training data of these methods and our crafted test sets further made fair evaluation complicated. This was particularly evident for LMetalSite [60], where their reported performance on their own test sets was significantly lower compared to their results on our test sets, indicating sign of data leaking in this comparison. 32 Figure 13: Average of F1 values for all 41 ligands during the training based on the validation sets. The performance peaked at epoch 30. Finally, preliminary experiments were conducted on Protein-Protein Binding Site Prediction using the PPIRef dataset [47]. The task involved predicting the binding interface residues on target protein given the sequences of both the target and binder proteins (concatenated as input). Binding residues were defined as those on the target protein within 6Å of the binder. Initial results using the simplified index tokenization yielded positive F1 score of 0.47 on the test set. While promising, this result is preliminary, and further investigation is required. Table 26: Comparison of our methods best performance for each ligand with other available methods on selected ligands based on F1 score. The main values are taken from the original papers; * indicates results recomputed on our test sets. Ligand Metrics Prot2Token-C Ca2+ Mg2+ Zn2+ Mn2+ F1 MCC Acc F1 MCC Acc F1 MCC Acc F1 MCC Acc 0.6566 0.4603 0.7594 0.7376 TargetS [61] 0.392 0.320 (0.431) 0.984 (0.977) 0.433 0.383 (0.450) 0.990 (0.992) 0.660 0.557 (0.660) 0.989 (0.989) 0.579 0.445 (0.574) 0.987 (0.989) LMetalSite [62] 0.526 (0.7370) 0.542 (0.7342) 0.9884 0.367 (0.5560) 0.419 (0.5773) 0.9949 0.760 (0.8299) 0.761 (0.8275) 0.9953 0.662 (0.8048) 0.661 (0.8024) 0.995 ZinCap [63] 0.451 0.540 (0.480) 0.870 (0.970) MIB2 [59] 0.941 0.946 0.948 0.950 A.4.5 Sequence-to-sequence For the secondary structure prediction task, Prot2Token was trained to assign secondary structure class to each residue in the input protein sequence, treating the problem as sequence-to-sequence token prediction. The dataset was preprocessed to map residues to standard secondary structure labels (helix, sheet, coil). Performance was evaluated using the macro-F1 score on the test set of PEER. As shown in Table 27. For the sequence-to-3D structure prediction task, we fine-tuned the last six blocks of the ESM2650m encoder within the Prot2Token framework. We used 8192 warmup steps for this particular task. The model was trained to generate discrete structure tokens corresponding to backbone coordinates, utilizing VQ-VAE-based tokenizer. The current VQ-VAE implementation supports protein sequences in the range of 50 to 512 residues. During training, model performance was evaluated using TM-score on the test set explained in A.3.7, and at the end, the best checkpoint is compared with other methods on subset of CAMEO dataset reported in A.3.7. 33 During inference, we encountered challenge where the decoder occasionally generated an output sequence with either more or fewer tokens than the actual number of amino acids in the input sequence. To address this issue, we applied constraint on the end <EOS> token probability. Specifically, during inference, we artificially adjusted the probability of the <EOS> token, ensuring that it was only allowed if the number of predicted 3D tokens matched the length of the input amino acid sequence. This adjustment effectively enforced sequence alignment and resolved inconsistencies in output length of generated structure. Table 27: Secondary structure prediction evaluation. The baseline involves linear classifier on top of the frozen ESM model. Method Macro-F Model PEER [27] (fine-tuned) Baseline Prot2Token-B 82.73 84.78 83.56 ESM1-1b ESM2-650m ESM2-650m) To further evaluate the learned structure-aware representations, we utilized the CATH-labeled protein sequences from [29], specifically the CATH_nonredundant_S40 dataset (release v4_3_0). In this dataset, no two sequences share more than 40% identity, and one representative (the longest) from each CATH superfamily is selected. This provides challenging testbed for assessing the structural awareness of protein embeddings across three hierarchical CATH levels: Class, Architecture, and Topology. In addition to structural benchmarks, we examined functional grouping using kinase and deaminase family datasets. Kinase domain sequences and their group labels were extracted from GPS 5.0 [64], resulting in 336 kinases from nine groups. Deaminase sequences and their respective family annotations were curated from reference dataset [65]. For each protein, we generated embeddings and assessed whether these could successfully separate functional classes. Figure 14: Randomly selected test set samples where the model achieved TM-score lower than 0.75. Table 28: Evaluation of protein embedding quality via clustering. Clustering performance is reported for CATH structural hierarchy levels using the Calinski-Harabasz index (CHI), and for functional groupings (kinase, deaminase) using Adjusted Rand Index (ARI). Higher values indicate more accurate and biologically meaningful clusters. S-ESM stands for structure-aware ESM. Methods CATH (CHI) Class Architecture Topology Kinase group clustering (ARI) Deaminase clustering (ARI) ESMC-600m [36] ESM2-650m S-ESM (Prot2Token-C encoder) 19.87 13.16 44.40 7.70 7.30 19. 4.06 4.35 11.50 0.1720 0.2691 0.5806 0.4067 0.6473 0.7963 Clustering quality for the functional groups (kinase and deaminase families) was quantified using the Adjusted Rand Index (ARI) after K-means clustering, while clustering for CATH structural categories 34 (Class, Architecture, Topology) was measured by the Calinski-Harabasz Index (CHI) [66], which captures the ratio of betweento within-cluster dispersion. Table 28 summarizes the results for all models. Prot2Tokens encoder achieves markedly higher CHI and ARI scores, especially in clustering kinase (ARI = 0.5806) and deaminase families (ARI = 0.7963), indicating improved capture of both structural and functional organization. For qualitative comparison, Figure 16 presents t-SNE visualization of protein embeddings colored by true structural or functional labels. Compared to ESM2-650m and ESMC-600m, Prot2Token embeddings yield more distinct and interpretable clusters that align with biological classification, demonstrating both stronger structural feature extraction and functional group separation. Figure 15: Validation perplexity curve for sequence-to-3D structure prediction. The continued decrease in perplexity throughout training indicates that the model has not yet reached convergence, and additional training may yield further gains in predictive accuracy. Figure 16: Comparison of protein representations generated by Prot2Token and the base encoder ESM2-650m. The t-SNE visualizations display protein embeddings for CATH structural classes, architecture, and topology, as well as clustering for deaminase families and kinase groups. Colors correspond to the known classes or families for each category. 35 A.4.6 Other We utilized the TargetP-2 dataset which encompasses both cleavage site data and five types of localization labels. We represented the label format as combination of classification and regression tasks, for instance, {sp, 96}, where sp denotes the localization label (Signal Peptide) and 96 indicates the cleavage sites location. Additionally, to evaluate the model, we implemented 5-fold cross-validation strategy. We considered fine-tuning only the last layer of the ESM models for both the Prot2Token model and the baseline comparison. Table 15 presents comparative analysis of Prot2Token against ESM with linear classifier head. The results suggest that by enabling the model to learn the locations of different amino acids through self-supervised auxiliary tasks, it achieves more accurate predictions of cleavage site positions. Furthermore, the performance in localization prediction also shows improvement with the integration of auxiliary tasks. We attribute this enhancement in performance to the models improved understanding of cleavage site positions. Note that the performance of bigger models was very similar to the smaller ones. Table 29: Localization and cleavage site prediction. \"Aux\" denotes self-supervised auxiliary tasks using STYKNR amino acids. Localization and cleavage site metrics are based on Macro-F1 and MAE, respectively. Method Aux-Tasks Cleavage Site localization Baseline (ESM2-35m) Prot2Token-A Prot2Token-A - - Aux (12k) - 3.6392 2. 90.96 90.56 92.30 In the next step, we fine-tuned the model starting from the latest checkpoint obtained during the self-supervised pre-training stage that is reported in Appendix A.4.3. This process involved jointly training six PTMs alongside self-supervised samples. The maximum sequence length for input protein sequences was set to 1024 tokens, and the batch size was adjusted to process 98,304 tokens per iteration. Notably, while it was possible to exclude self-supervised tasks entirely during fine-tuning, retaining subset of these samples led to improved generalization and enhanced performance on the proteinkinase phosphorylation site prediction. From the 33 total blocks in the protein encoder, we selectively fine-tuned the last eight blocks by unfreezing their weights for training. The results are presented in Table 30. Table 30: PTMs comparison based on F1 score on our test sets. : There is strong possibility of data contamination between our test set and the PTMGPT2 training set. As result, PTMGPT2 may achieve artificially high performance on our test set due to memorization, while its real-world performance on unseen samples could be lower. PTM Ubiquitylation Phosphorylation Acetylation Methylation N-linked Glycosylation O-linked Glycosylation Succinylation Prot2Token (Ours) 0.1382 0.4055 0.307 0.4608 0.9689 0.4695 0. ESM-2 0.1993 0.3908 0.3273 0.4532 0.9586 0.4597 0.3515 PTMGPT2 [22] 0.165 0.400 0.350 0.596 0.862 0.531 0.540 A.5 Protein-ligand binding site task tokens interpretation In this section, we scrutinized the task token embeddings of the decoder that was pre-trained on all 41 ligands in the previous section to find the sign of chemical properties of ligands and their relationships together. Empirically, based on the F1 scores of the ligands that the model was trained and evaluated on, the task token embeddings successfully captured meaningful representations of the ligands. However, to solidify this framework as foundation for future research, we aimed to validate these embeddings from an additional perspective. Our goal is to create robust infrastructure that can incorporate more ligands into single model, thereby addressing the scarcity of data for certain ligands through knowledge transfer between ligands. To achieve this, first, from all 41 ligands, we selected top 28 ligands based on F1 score and filtered the rest and then, we analyzed the task token embeddings of the remaining ligands by clustering them to explore ligand similarities in the embedding space. Simultaneously, we clustered the ligands based on their biochemical features in the real world, and in the last step, we investigated the correlation between these two clustering approaches. The purpose of this comparison was to determine whether the learned task token embeddings genuinely reflect realworld relationships between ligands or if they merely memorize specific patterns without capturing meaningful biochemical similarities. Figure 17 highlights the intersection between the two spaces of ligand representations: the embedding space and the biochemical feature space. It illustrates which ligands or sets of ligands have their relationships successfully captured by the generated task token embeddings, as reflected by their agreement with relationships derived from biochemical features, and which embeddings failed to capture such relationships. More details, including feature selection, methods, and the interpretation algorithm are in the following subsections. A.5.1 Interpretation In this study, we developed protein binding site prediction model using multi-task learning framework, where each task represents specific ligand. 640-dimensional task token was incorporated for each ligand alongside the protein sequences. During training, the model learned meaningful task token embeddings that effectively represent ligands and their unique characteristics. To validate the task token embeddings, we employed two clustering approaches: one based on the trained task token embeddings and the other on biochemical ligand features. For precise clustering and clearer analysis, ligands with an F1 score below 0.5 were excluded to minimize noise, leaving 28 out of 41 ligands for analysis. Task token embeddings were reduced to 27 principal components using PCA, preserving 99% of the variance, and clustered with k-means to generate target clusters. For validating all ligands, the full set of 41 ligands was included. In this case, task token embeddings were reduced to 40 components to preserve 99% of the variance, and the same clustering method was applied. For the ligand features, 26 biochemical descriptors were collected, covering physical, chemical, electronic, hydrophilic, lipophilic, and geometric properties. systematic feature selection process evaluated all possible combinations of up to 13 features selected from these 26 descriptors (approximately 39 million combinations) to optimize clustering quality against the target clusters. The ARI was used as the selection metric, while Normalized Mutual Information (NMI) and Pairwise Accuracy metrics were later employed to evaluate the final selection. The clustering results demonstrate that the learned task token embeddings are meaningful, as their clustering aligns closely with that based on ligand-specific biochemical features. Moderate-tohigh agreement metrics (ARI=0.447, NMI=0.614, and pairwise-accuracy=0.783) highlight the embeddings ability to capture key biochemical characteristics of ligands. Chemically significant features, such as MolecularWeight, NetCharge, and RotatableBonds, identified as part of the optimal feature set, further reinforce the relevance of the embeddings. The overlap and similarity in ligand grouping across both clustering approaches validate the hypothesis that the task token embeddings effectively encode biologically and chemically meaningful information. However, reducing task token embeddings or biochemical features to 2D for visualization causes significant information loss, making 2D clustering plots less informative (Figures 23 and 24). These findings emphasize the importance of preserving higher-dimensional information for accurate interpretation and highlight the value of task token embeddings in ligand characterization for protein binding site prediction. Figure 18 shows the embeddings-based clustering, while Figure 19 shows the features-based clustering, and Figure 17 illustrates the global, local, and no relationships between the two approaches of embeddings-based clustering and features-based clustering. Global relationships. Figure 17 highlights the ligands that have been clustered correctly across and within both clustering approaches. For instance, in Cluster 3, the solid circles for ACO, ATP, FAD, GTP, NAD, and SAM ligands represent ligands that have been consistently clustered across and within the same clusters in both approaches. This indicates that the task token embeddings successfully capture their similarity with each other and with the rest of the ligands. Local relationships. Figure 17 also depicts ligands that have been clustered correctly only within clusters in both clustering approaches. For example, the stars in cluster 3 for FE2+ and MN2+ indicate that these ligands are grouped but appear in different clusters across the two approaches. Nevertheless, the task token embeddings still manage to capture their similarity with each other, even if they fail to capture their similarity with other ligands. No relationships. For some ligands, the task token embeddings fail to accurately capture their global or local relationships. This may be due to the ligand features collected not being entirely representative and requiring further refinement, or because the task token embeddings themselves need improvement. Figure 17 illustrates these no relationships using triangles; for instance, the HEM ligand has been grouped with different ligands across different clusters in both approaches. For further investigation of the task token embeddings, we incorporated all 41 ligands into the clustering analysis. The metrics showed notable drop: ARI=0.259, NMI=0.333, and Pairwise-Accuracy=0.733. This decrease was expected, as including task token embeddings for ligands with low F1 scores introduced some misaligned clusters. However, closer examination reveals that the embeddings still effectively capture the global and local relationships between most ligands. Figures 20 and 21 depict the embeddings-based clustering and features-based clustering, respectively, while Figure 22 illustrates the global, local, and no relationships across all 41 ligands. Notably, out of the 41 ligands, the task token embeddings successfully represented 21 ligands globally, 13 ligands locally, and misrepresented 7 ligands. These results indicate that the task token embeddings consistently demonstrate strong global and local relationships, effectively capturing biochemical similarities among ligands. This reinforces the conclusion that the model has learned meaningful representations, even for ligands with low F1 scores. Figure 17: Global Relationships indicate that general biochemical features shared among many ligands have been captured. Local Relationships reflect the successful capture of biochemical properties between specific ligands and their closely related counterparts. No Relationships indicate that the biochemical properties were not captured at all. A.5.2 Features pool creation The feature pool of 26 descriptors was carefully designed to capture the physical, chemical, and structural properties of ligands, making them particularly suitable for describing protein-ligand interactions. These features were selected using domain knowledge of protein-ligand interactions and their ability to explain binding phenomena effectively. Two primary sources were used to collect these features: PubChem [67], free online database maintained by the National Center for Biotechnology Information (NCBI), which provides precomputed chemical information for small molecules, drugs, and bioactive compounds. Features were retrieved using Compound IDs (CIDs) and SMILES, text-based representation of molecular structures. The second source was RDKit, an open-source cheminformatics toolkit, where SMILES strings were converted into molecular objects and processed using various descriptors to compute additional features. 38 Table 31 shows the set of 26 features, categorized into seven groups, captures the properties of metal ions and molecules from multiple perspectives, providing comprehensive description of their binding potential with proteins. Table 31: All 26 features we used in the interpretation step. No. Name 1 MolecularWeight 2 ExactMass 3 MolecularVolume HeavyAtomCount 4 RingCount 5 CarbonCount 6 OxygenCount Description Total molecular mass High-precision mass of the molecule Estimated molecular volume Count of non-hydrogen atoms Total number of rings in the molecule Number of carbon atoms Number of oxygen atoms 8 LogP 9 MolLogP 10 HydrophobicSurfaceArea 11 12 Hydrophilicity Polarizability 13 Refractivity 14 TPSA Polarity and Hydrophobicity Features Partition coefficient (hydrophobicity) Alternative measure of hydrophobicity Hydrophobic interaction area (TPSA) Topological Polar Surface Area (polarity) Difference between molecular weight and hydrophobicity Molecular refractivity measure of molecules polarizability Charge and Electrostatics Features 15 NetCharge ElectrostaticPotential Net electrical charge of the molecule Approximate measure of electrostatic potential Flexibility and Rotational Features 17 18 RotatableBonds RotatableBondFraction 19 SingleBonds 20 DoubleBonds BalabanJ Number of rotatable bonds Fraction of single bonds that are rotatable Bond and Connectivity Features Count of single bonds Count of double bonds Balaban index (topological descriptor) Hydrogen Bonding Features Number of hydrogen bond donors Number of hydrogen bond acceptors 22 HBondDonors 23 HBondAcceptors 24 HydrogenBondingPotential Difference between molecular weight and TPSA 25 AromaticRings 26 PiPiInteractionSites Aromaticity and π-π Interactions Number of aromatic rings Number of π-π interaction sites Source PubChem PubChem RDKit RDKit RDKit RDKit RDKit PubChem RDKit RDKit PubChem RDKit RDKit RDKit PubChem RDKit PubChem RDKit RDKit RDKit RDKit PubChem PubChem RDKit RDKit RDKit A.5.3 Optimizing feature selection Our approach leverages clustered embeddings as reference to evaluate clustered features from various feature combinations, identifying the best set of features to describe ligands based on the highest ARI score. We began with task token embeddings of ligands that achieved high F1 scores to ensure noise reduction and high-quality clustering. These embeddings, initially 640-dimensional, were reduced to 27 principal components using PCA while retaining 99% of the variance. The reduced embeddings were then clustered using k-means, with the optimal number of clusters determined via the Elbow method, serving as the target clusters. To identify the most informative ligand features, we implemented search algorithm (Algorithm 1) that evaluates all possible combinations of up to 13 features from pool of 26. In the first iteration, the algorithm selects single feature (26 possible options). In the second iteration, it selects two features (325 possible combinations). This process continues up to 13 features, yielding approximately 39 million combinations. For each combination, the ligand-based feature clustering is performed, and the ARI score is computed. The feature combination that achieves the highest ARI score is selected as the best set. Next, we removed the threshold constraint and extended the algorithm to all 41 ligands, examining whether task token embeddings captured meaningful representations for ligands with F1 scores below 0.5. This analysis demonstrated that the embeddings retained significant information even for lower-performing ligands. Table 32 presents the output of our searching algorithm, showing the top three feature combinations based on the ARI metric for the top 28 ligands. Table 33 displays the top three feature combinations for the entire set of 41 ligands. 39 Algorithm 1 Ligand interpretation clustering Require: (features_pool (26 features), task_token_embeddings (640D), ligands (41), threshold (e.g., 1 > 0.5)) Ensure: (best_features_combination, best_ari) Step 1. Preprocessing: high_quality_ligands {ligand 1(ligand) > threshold} pca_embeddings PCA(task_token_embeddings, components, 99% variance) 1: (a) Filter Ligands: 2: (b) Reduce Embeddings: 3: (c) Find Clusters: 4: 5: koptimal ElbowMethod(pca_embeddings) target_clusters KMeans(pca_embeddings, koptimal) Step 2. Feature Combination Evaluation: best_ari 6: 7: for nfeatures = 1 to 13 do 8: 9: 10: 11: 12: 13: 14: combinations Combinations(features_pool, nfeatures) for each combination combinations do feature_clusters KMeans(combinations, koptimal) ari ComputeARI(feature_clusters, target_clusters) if ari > best_ari then best_ari ari best_features_combination combination Figure 18: Clustering results of embeddings on top 28 ligands based on F1 score. Table 32: Top three feature combinations for the 28 ligands. No. Features Features 7 12 13 MolecularWeight, NetCharge, RotatableBonds, HydrogenBondingPotential, CarbonCount, SingleBonds, BalabanJ LogP, NetCharge, RotatableBonds, ExactMass, Polarizability, AromaticRings, MolLogP, MolecularVolume, HydrogenBondingPotential, CarbonCount, SingleBonds, BalabanJ MolecularWeight, LogP, NetCharge, RotatableBonds, ExactMass, Polarizability, MolLogP, MolecularVolume, RingCount, HydrogenBondingPotential, CarbonCount, BalabanJ, Hydrophilicity ARI 0. NMI 0.614 Pairwise Accuracy 0.783 0.423 0.573 0.772 0. 0.577 0.778 Table 33: Top three feature combinations for the entire set of 41 ligands. No. Features Features 8 13 NetCharge, HBondDonors, HBondAcceptors, ExactMass, Refractivity, HydrogenBondingPotential, SingleBonds, PiPiInteractionSites MolecularWeight, LogP, RotatableBonds, TPSA, MolLogP, MolecularVolume, SingleBonds, Hydrophilicity, ElectrostaticPotential, PiPiInteractionSites LogP, NetCharge, HBondDonors, HBondAcceptors, TPSA, ExactMass, Polarizability, AromaticRings, Refractivity, DoubleBonds, BalabanJ, Hydrophilicity, PiPiInteractionSites ARI 0.248 NMI 0.317 Pairwise Accuracy 0.728 0. 0.319 0.259 0.333 0.709 0.733 A.5.4 Visualization To analyze the structural relationships within the high-dimensional ligand embeddings, we applied dimensionality reduction techniques to project the representation of 41 ligands from the 640 dimensional into two-dimensional space for visualization. The methods explored included t-SNE (Figure 23) and UMAP (Figure 24). 40 Figure 19: Clustering results of features on the 28 selected ligands. Figure 20: Clustering results of embeddings on all 41 ligands based on F1 score. Figure 21: Clustering results of features on the 41 selected ligands. Figure 22: Global Relationships indicate that general biochemical features shared among many ligands have been captured. Local Relationships reflect the successful capture of biochemical properties between specific ligands and their closely related counterparts. No Relationships indicate that the biochemical properties were not captured at all. The perplexity parameter for t-SNE was set to 3, and the number of neighbors (n_neighbors) for UMAP was also set to 3. These parameters were chosen to focus on capturing local relationships among ligand embeddings and to preserve some global structural details. Additionally, the dimensionality of the output was set to two (n_components=2) because the visualizations are in two dimensions. All other parameters were kept at their default settings. Figure 23: Visualization of task token embeddings using t-SNE. Figure 24: Visualization of task token embeddings using UMAP. 42 A.5.5 Multi-task learning effect In this section, we tried to investigate the effect of multi-task learning on three ligands that have low number of samples but share similar semanticity in the embedding space of task tokens (Figure 17). Therefore, we selected GSH, CO, AGS as our target ligands because (1) they belong to the same cluster, (2) showed global or local relationships and, (3) have less than 200 or lower number of protein sequences in total. We considered three groups to measure the performance of three ligands. There groups are defined as follow: Group 1. Combination of the target ligands, GSH, CO and AGS (Equivalent to 1,819 tokens). Group 2. Combination of CLA, FAD, HEM and NAD as ligands that did not share close semantic representation in Figure 17 (Equivalent to 43k tokens). Group 3. Combination of Zn, Ca, ADP, members from cluster 4 (Equivalent to 37k tokens). Need to point out, in order to make the comparison of group 2 and 3 fair, we considered the total number of tokens in these groups close to each other. Table 34 shows that group 3 which shares similar cluster with the target ligands, improves F1 score more than other groups. Table 34: The effect of jointly training under representative ligands based on different auxiliary groups with respect to F1 score. \"-\" means no auxiliary task is used during training the target task. Target Task GSH CO AGS Average - 66.53 35.87 31.63 44.68 Group 1 70.27 32.65 22.94 41.95 (-2.73) Group 2 69.18 29.26 43.18 47.21 (+2.53) Group 3 68.74 58.48 49.09 58.77 (+14.09)"
        }
    ],
    "affiliations": [
        "Politecnico di Milano, Milan, Italy",
        "ProGene",
        "University of Missouri"
    ]
}