{
    "paper_title": "VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary",
    "authors": [
        "Kevin Qinghong Lin",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human daily activities can be concisely narrated as sequences of routine events (e.g., turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, a novel video understanding framework that define video narrations as vocabulary, going beyond the typical subword vocabularies in existing generative video-language models. Built on the lightweight language model GPT-2, VLog feature three key innovations: (i) A generative retrieval model, marrying language model's complex reasoning capabilities with contrastive retrieval's efficient similarity search. (ii) A hierarchical vocabulary derived from large-scale video narrations using our narration pair encoding algorithm, enabling efficient indexing of specific events (e.g., cutting a tomato) by identifying broader scenarios (e.g., kitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary update strategy leveraging generative models to extend the vocabulary for novel events encountered during inference. To validate our approach, we introduce VidCap-Eval, a development set requiring concise narrations with reasoning relationships (e.g., before and after). Experiments on EgoSchema, COIN, and HiREST further demonstrate the effectiveness of VLog, highlighting its ability to generate concise, contextually accurate, and efficient narrations, offering a novel perspective on video understanding. Codes are released at https://github.com/showlab/VLog."
        },
        {
            "title": "Start",
            "content": "VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary Kevin Qinghong Lin, Mike Zheng Shou(cid:66) Show Lab, National University of Singapore 5 2 0 2 2 ] . [ 1 2 0 4 9 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Human daily activities can be concisely narrated as sequences of routine events (e.g., turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, novel video understanding framework that define video narrations as vocabulary, going beyond the typical subword vocabularies in existing generative video-language models. Built on the lightweight language model GPT-2, VLog feature three key innovations: (i) generative retrieval model, marrying language models complex reasoning capabilities with contrastive retrievals efficient similarity search. (ii) hierarchical vocabulary derived from large-scale video narrations using our narration pair encoding algorithm, enabling efficient indexing of specific events (e.g., cutting tomato) by identifying broader scenarios (e.g., kitchen) with expressive postfixes (e.g., by the left hand). (iii) vocabulary update strategy leveraging generative models to extend the vocabulary for novel events encountered during inference. To validate our approach, we introduce VidCap-Eval, development set requiring concise narrations with reasoning relationships (e.g., before and after). Experiments on EgoSchema, COIN, and HiREST further demonstrate the effectiveness of VLog, highlighting its ability to generate concise, contextually accurate, and efficient narrations, offering novel perspective on video understanding. Codes are released at https://github.com/showlab/VLog. 1. Introduction Life is succession of moments. Corita Kent Recent advances in Large Language Models (LLMs) [12, 34] have inspired several research into transferring textual knowledge to multi-modal domains [30, 61], leading to the development of Video Large Language Models (VideoLLM) [1, 32]. These models mainly leverage foundational elements of LLMs, such as subword vocabularies [42] and transformers with pretrained weights, then Figure 1. Key Idea of VLog. In contrast to existing video- . language models that rely on token-by-token generation on languages subword vocabulary, VLog introduces novel generative retrieval method based on narration vocabulary, achieving significant speedup (10) when processing long videos. . adapt them through multi-modal instruction tuning [24, 30]. As result, these models can generate video-conditioned textual outputs via next-token prediction, as illustrated on the left side of Fig. 1. Despite these advancements, original LLM designs are not inherently suited for video understanding. For example, LLM subword vocabularies are typically large (e.g., LLama-3s 128K vocabulary size [34]) to capture broad linguistic information, but incomplete subwords (e.g., happ) often lack visual interpretability. Moreover, token-wise generation during inference introduces bottleneck, limiting the models ability to process video in real time. In practical applications, models are not always required to provide exhaustive details. Instead, we often prefer video model that delivers concise, contextual responses in real timesuch as an AR glasses assistant customized for personal needs i.e., prioritize task-specific efficiency over generalist models. This raises the question: How can we tailor video model to meet our specific requirements? To 1 address this, we draw inspiration from how humans naturally organize experiences. When reflecting on our day, we often recall it as series of narrative eventssuch as washing dishes or reading bookthat form the narration vocabulary of our daily lives. Motivated by this, we propose VLog, novel efficient video understanding framework. Unlike generative VideoLLM that rely on subword vocabularies, VLog represents video narrations as minimal token units and builds the vocabulary. As shown on the right side of Fig.1, this narration vocabulary results significantly reduces decoding times. To leverage this narration vocabulary, straightforward approach is to use retrieval-based model for efficient narration retrieval. However, retrieval models often lack the reasoning capabilities of generative models, failing to handle advanced queries like Whats the next action? with reference video. To address this issue, VLog introduces novel generative retrieval architecture that repurposes the language models reasoning capabilities with retrieval token, embedding both visual and query information for reasoning-oriented retrieval. To construct the narration vocabulary, we develop Narrative Pair Encoding method applied to exisiting video narration datasets [15], generating prefix sets (e.g., Cut potato) and postfix sets (e.g., by the left hand). Additionally, to enable efficient indexing across large vocabulary, we organize the vocabulary hierarchically instead of Bruteforce search. This design allows rapid indexing of prefix vocabulary subset (e.g., Cut tomato) by first identifying scenarios (e.g.,kitchen) and then refining the search with the postfix, as illustrated in Fig.1. Recognizing that the initial vocabulary may not cover novel, out-of-vocabulary events, we devised vocabulary upgrade strategy using an agentic workflow. When low similarity scores are detected for vocabulary entries, they are treated as novel events and processed by generative models like LMM [22] to generate scene descriptions. LLM [47] is then prompted to extend the relevant vocabulary entries. Overall, our contributions are three folds: We propose the first generative retrieval framework for video understanding, enabling efficient vocabulary indexing with support for complex reasoning. We introduce the Narration Pair Encoding method and hierarchical strategy to build the narration vocabulary and enable efficient indexing. To handle novel narrations beyond the existing vocabulary, we develop an efficient vocabulary expansion strategy. We demonstrate the effectiveness and efficiency of VLog on the new development set Vidcab-Eval and including COIN, EgoSchema, and public datasets, HiREST, highlighting its strengths in efficient and accurate video understanding. 2. Related Works 2.1. Video Captioning Video captioning provides natural way to interpret and describe video content, with key focus on understanding human activity [7, 9, 11]. This includes various downstream tasks, such as dense captioning [53] and step captioning [46]. Some video captioning datasets emphasize long, descriptive paragraphs that aim to capture every detail [6]. While in real-world applications, brief, contextual sentences are often sufficient to convey what happens in videoessentially, video narration. First highlighted in [15] as an efficient alternative for documenting untrimmed video streams, video narration functions like minute, capturing key events or changes with sparse yet informative content over time. With large and diverse set of video narrations, as in [15, 16], we can reasonably assume that most common daily activities, such as turning off an alarm upon waking, can be reused across contexts, much like human experience. In our work, we treat these narrations as vocabulary of human behavior, using them to interpret new, incoming video streams. 2.2. Video-Language Models Retrieval Models. Early vision-language models [4,27,35, 39, 50] primarily relied on alignment approaches, leveraging contrastive objectives for scalable performance. These methods excelled in retrieval and classification tasks within pre-defined labeled spaces, offering high efficiency by dotproduct similarities calculation [25, 28, 38, 55]. But this fashion lacks the ability to model complex reasoning relationships between media inputs; for example, they fail to retrieve video clip based on query like, What occurred after? which conditioned on reference video clip. Generative Models. Recently, focus has shifted to generative LMMs, with several works [1, 10, 23, 24, 30, 52, 61] developing large multimodal models by projecting visual inputs into textual embeddings and aligning them with LLMs through visual instruction tuning. However, such straightforward adaptations are often unsuitable for video understanding, as large subword vocabularies in LLMs, while broadly inclusive, lack visual interpretability. Moreover, slow decoding during testing hinders real-time applications. Therefore, recent studies have focused on improving the efficiency of VideoLLM for (long) video modeling [8, 26, 43, 44, 51, 57]. Generative Retrieval. By examining the strengths and limitations of retrieval and generative models, natural approach is to unify them, leveraging reasoning alongside efficient retrieval. In the realm of LLMs, efforts [2, 20, 45, 48] have been made to improve text embeddings that capture complex relationships, showing promise for practical applications. However, these advancements are less explored in 2 Figure 2. Comparison between different Video-Language model architectures: (a) Generative Models: These model with complex reasoning but are slow, generating tokens one by one. (b) Retrieval Models: These enable fast vocabulary search but lack reasoning, useful only for simple alignment tasks. (c) Generative Retrieval (VLog): This approach combines fast vocabulary search with complex reasoning by using retrieval token, merging the advantages of both methods. multimodal domains [17, 19, 31], particularly in video understanding. In this work, we pursue this generative retrieval direction by introducing novel method that marrying lightweight GPT-2s reasoning [40] with the contrastive vision-text model SigLIP [54]. Moreover, we departing from traditional subword vocabularies, VLog redefines the narration vocabulary, incorporating retrieval-based similarity search for fast and interpretable reasoning. 3. VLog Task definitions: Given video = {vi}, where each vi represents frame, and conditional query Q, our objective is to generate an accurate narration of the video. To ensure consistency across all variants, we avoid specialized frame sampling strategies, focusing instead on the distinct strengths of generative and retrieval modeling. 3.1. Architecture a. Generative Models. In most generative approaches, i.e., large multimodal models, subword vocabulary = {oi} from language models is typically used, where = denotes the vocabulary size (each oi represents subword, such as the). Given visual inputs and query encoded into token sequence, the language model estimates the probability of the next token xn being the i-th token oi over the vocabulary O, conditioned on the visual inputs V, query Q, and previously generated tokens x<n: Pr (xn = oi x<n, V, Q) for oi O, (1) . maximum likelihood: xn = arg max Pr(xn = oi x<n, V, Q). (2) This autoregressive process enables the model to capture complex dependencies between the visual inputs and the query, as demonstrated in Fig.2(a). However, for dense narrations over long video, this approach incurs high inference costs due to token-by-token decoding [13, 49, 56]. b. Retrieval with Vocabularies. Inspired by how humans retrieve past experiences to interpret new coming events, we propose reframing the token generation process as retrieval task with predefined narration set = {cut potato, , walk round}, serving as behavior vocabulary as depicted in Fig.2(b). We employ vision-text contrastive model, SigLIP [54], to map video frames and vocabulary tokens into shared embedding space, yielding vocabulary embeddings oi SigLIP(oi) and frame embeddings vj SigLIP(vj). To model temporal relationships among frames, we use an additional module 2-layers transformer layersto produce compact clip representation {vj} from the sequence of frame embeddings. To sample prediction by leveraging the narration vocabulary, we define the probability between narration vocabulary oi and the video clip based on their cosine similarity over their projected embeddings: Pr (X = oi V) = vT oi for oi O, (3) = where Pr is parameterized by language model weights using cross-entropy. Normally, the next token is predicted by the arg maxoi prediction (cid:0)vT oi (cid:1) is then determined by 3 This approach offers two benefits: first, it enables efficient similarity calculation via dot product (i.e., Eq. 3); second, it directly outputs narration without requiring subword generation. However, despite leveraging the strengths of contrastive models, this method struggle to capture the complex relationships in queries Q, posing challenges for retrieving with queries e.g., What happened next. c. Generative Retrieval (Ours). To leverage the reasoning modeling of generative models while harnessing the efficiency of retrieval methods, we propose novel generative retrieval model. As shown in Fig.2(c), we introduce retrieval token with embedding to bridge the generative language model and retrieval model. The retrieval token is positioned as the last input in the language model sequence, allowing it to attend to both the front visual and query inputs. After passing through the language model, the output embedding is assumed to encode both visual and query information, enabling retrieval while preserving reasoning ability: Pr (X = oi V, Q) = tT oi for oi O, (cid:0)tT oi (4) (cid:1). maxoi the prediction is then yield by = arg Moreover, our model has the following considerations: 1. Retrieval tokens initialization: The retrieval token, appearing at the end of the sequence, can be initialized as learnable token, an EOS token, or using meanpooled visual inputs to encode overall visual informationexamined in our experiments. 2. Asymmetric structure: Unlike visual and query inputs, vocabulary embeddings are not projected by the language model and remain fixed after initial computation, reducing forward computation costs with large vocabularies. In this way, our architecture effectively addresses both reasoning and efficiency issues. Training objectives. Our generative retrieval model is trained with standard contrastive objective: = 1 (cid:88) iB log oi/τ ) exp(tT jB exp(tT oj/τ ) (cid:80) , (5) where is the batch size and τ = 0.05 is the temperature. This objective updates the language model weights to encourage retrieval tokens derived from visual and textual inputs to align with the target vocabulary oi. 3.2. Vocabulary Construction The narration vocabulary is key to our model. We source it from existing video datasets [15], which contain extensive narrations across various domains. We then clean these narrations and remove duplicates (see details in Supp). 4 Figure 3. Illustration of our Vocabulary Construction and In- . dexing. Upper side: Given the narrations, we process them using our NPE method, breaking down each narration into prefix and postfix parts. Lower side: For efficient indexing, we organize the vocabulary hierarchically, where first-level scenes help navigate subsets of prefix narrations. Next, we append the prefix and continue retrieving the postfix. Narration Pair Encoding. We observe that many narrations are often subjective and inconsistently formatted, as shown in Fig. 3 upper side, they share common prefixes (e.g. cut potato) but differ in their postfixes, which add context. In natural language processing, Byte Pair Encoding (BPE) [42] addresses this issue by tokenizing text corpora into subword units. However, in our setting, we aim to build narration-level vocabulary where BPE is not directly applicable. To address this, we introduce Narration Pair Encoding (NPE): We treat each narration as potential prefix and search for longer narrations that start with it, collecting their postfixesthe additional words beyond the prefix. This approach yields two setsa prefix set of narrations with non-empty postfixes and shared postfix set. We detailed NPE algorithm in our Supplement. To enable model for prefix and postfix retrieval, we first use the retrieval token to retrieve the prefix, concatenate it with the visual and query sequence, and then use the retrieval token to retrieve postfix from the postfix vocabulary. Hierarchical Indexing. After completing the NPE process, we obtain both prefix and postfix narration sets. However, the large scale of narrations (e.g., millions) poses challenge for efficient retrieval, making brute-force search impractical. Considering that human activity recordings in videos often align with specific scenarios, such as cut potato occurring in kitchen scene, we develop hierarchical indexing strategy by associate narrations to its videos belonged scenario. The full retrieval chain is displayed in the lower part of Fig.3. Given video, the model first identifies the video scenario and then retrieves the associated prefix narrations subset. This is efficient by reducing the search space. Then we continue retrieval the postfix from the postfix vocabulary. This helps to improve the narration expression. 3.3. Instruction-Tuning Data Once we developed such generative retrieval model, the next issue we faced was the lack of training data with complex reasoning relationships, as most video-text retrieval data are paired solely for alignment. Fortunately, untrimmed video streams offer natural solution by inherently modeling temporal relationships between narrations. Figure 4. Create video-text pairs that requires complex reasoning from untrimmed videos based on their temporal relationship. As illustrated in Fig. 4, we develop three types of training data representing before, next, and current relationships. For next as an example, given video clip Vi with its reference narration Xi, we trim preceding segment V<i and append prompt such as Whats the next action? Based on these temporal relationship, we create an 200K instruction-tuning training data from [15], namely VidcabTrain (i.e., Video vocabulary). Moreover, as we lack of such retrieval evaluation consider complex reasoning, we apply the same strategy but create generative retrieval development set with 4.6K samples named as Vidcab-Eval. Notably, we carefully curate the instruction training data and development set using our NPE method. For instance, in Vidcab-Eval, the model must accurately retrieve both the prefix and postfix to achieve higher score. We detail how to construct them in Supp. Moreover, we incorporate scenario information from videos, prompting the model to encode the entire sequence of video with queries like Whats the overall activity in this video? and answers such as Cooking. This organization supports hierarchical vocabulary indexing, which we will discuss in the next section. 3.4. Vocabulary Upgrading Despite the diversity and scale of initial vocabulary, novel Out-of-Vocabulary (OOV) events may still occur, requiring: (i) detection of novel events, and (ii) expansion of the vocabulary with new entries. Detect Novel Events. Our model uses the dot product in Eq. 3 as relevance metric between the query and candidate vocabulary, akin to logits as confidence measure in LLMs. Empirically, we set threshold of 0.4: if the top1 vocabulary match falls below this threshold for given visual input, we classify the event as OOV. Figure 5. Agentic workflow of VLog Vocabulary upgrade. When low retrieval score is detected, the visual inputs are sent to the LMM (LLaVA-OV-0.5B [22]) to generate scene descriptions. These descriptions are then processed by the LLM (Qwen2.50.5B [47]), which expands and updates the existing vocabulary. Seeking Help from Generative Models. To handle out-ofvocabulary narrations, we leverage both LMMs and LLMs, which offer complementary strengths: LMMs generate brief, visually grounded captions but may hallucinate on fine-grained actions, while LLMs, with broad knowledge and strong instruction-following, generate diverse candidate narrations. We integrate these models in an agentic workflow. As illustrated in Fig.5, where LMMs first provide concise visual scene descriptions, and LLMs then use this context to infer potential events that might occur in the scene, which are parsed as new vocabulary narrations. In practice, we use two models comparable in size to our GPT-2: the LMM LLaVA-OV-0.5B [22], and the LLM Qwen2.50.5B [47] guided by an in-context template. This workflow differs from existing RetrievalAugmented Generation [14], serving instead as new Generative-Augmented Retrieval approach, where generative models actively expand the vocabulary for improved retrieval accuracy. 4. Experiments In this section, we structure our experiments to answer the following questions: Q1: What key advantages does VLog offer? Q2: How to adapt VLog to different tasks (e.g., Reasoning QA beyond retrieval)? Q3: How effective is the vocabulary upgrading strategy (address out-of-vocabulary problem)? Q4: What are the key design choices in VLog (e.g., NPE and Vocabulary indexing)? 4.1. Datasets and Settings Vidcab-Eval is constructed by Sec. 3.3, we selected Ego4D [15] to build our initial vocabulary and training data due to its large scale and diversity, with millions of manually curated narrations. We ensure no overlap with downModels Visual Enc. Post process Vocabulary FT? Navie Retrieval Casual Retrieval Decode time CIDEr CIDEr R@1 R@1 sec Generative Retrieval Retrieval VLog VLog-prefix VLog-prefix&postfix SigLIP-L SigLIP-L SigLIP-L SigLIP-L SigLIP-L SigLIP-L GPT2-M MeanPool Adapter GPT2-M GPT2-M GPT2-M GPT2-32K Eval-4.6K Eval-4.6K Eval-4.6K Ego4D-0.8M Ego4D-0.8M 64.8 63.6 95.8 96.9 91.3 94.9 7.9 4.6 11.8 12.4 10.9 11.9 53.7 N/A 48.9 87.3 83.9 86.9 3.1 N/A 2.1 5.0 3.7 4. 0.362 0.001 0.016 0.018 (20) 0.035 (10) 0.053 (6) Table 1. Key ablation studies on Vidcab-Eval for naive and casual retrieval. Under same conditions, VLog provides accurate narration with significant speed improvements. stream tasks such as EgoSchema. To evaluate our proposed generative retrieval setting, we evaluate on Vidcab-Eval development set. COIN is an instructional video dataset comprising 11,827 videos across 180 tasks in 12 domains related to daily life. We evaluate our model on three common COIN benchmarks: step recognition, step forecasting, and task summarization. We use this benchmark to study VLogs ability for fine-grained activity recognition. EgoSchema is long-range video question-answering benchmark with 5K multiple-choice pairs across 250 hours of video, covering wide range of human activities. Unlike prior benchmarks, correctly answering question here requires at least 100 seconds of video viewingwell beyond existing standards. We use this benchmark to study VLogs high-level reasoning abilities. HiREST is new benchmark for hierarchical procedural information. It includes videos from novel domains that not appeared in Ego4D and COIN, with numerous and highquality step captions by human annotators. Thus, we use its step-captioning task to study the effectiveness of VLogs vocabulary upgrading strategy. Implementation Details. VLog builds on GPT2-medium [40] with SigLIP [54], extracting video clips at 2 frames per second. For long videos, we uniformly sample long video into multiple fix length clips (1s) and process them in streaming fashion. We pre-extract the visual features and store the textual embedding of narration vocabulary, which storage occupies 3.9 GB in total. During training, we finetune the models fully. 4.2. Key Advantages by VLog In Tab. 1, we evaluate key variants in VLog-Eval, which aims to produce accurate narrations based on both visual and textual queries. This includes two tracks: naıve track (without query, i.e. normal video-text alignment) and causal track (with query, e.g., before / after / current). To enable both generative and retrieval models to report scores, we introduce two metrics to assess narration quality: CIDEr (common used by generative models) and Recall@1 (common used by retrieval models) alongside generation speed . 6 per clip for comprehensive evaluation. Baselines: To ensure fair comparison, all variants use the same visual encoder, and both the generative and VLog models are based on the same GPT2 model trained on the same data. For the retrieval baseline, we provide both zero-shot and fine-tuned versions, using VLog-Eval4.6K as the restricted vocabulary, ensuring that the correct term is within the vocabulary. To adapt the retrieval model for casual retrieval, we pool the visual and textual query inputs to obtain unified embedding. For VLog, we provide variants using either Eval-4.6K or our full constructed vocabulary, with or without postfix retrieval. We have the following observation: (i) Overall, the scores in the naıve setting are higher than in the causal setting, indicating that causal retrieval is more challenging than standard alignment, as it requires finegrained understanding of temporal relationships. (ii) In the naıve setting, VLog achieves comparable CIDEr and Recall@1 scores to retrieval models. In more challenging causal setting, VLog significantly outperforms both generative and retrieval models, the latter of which pools both visual and query inputs but fails to model relationships effectively. This demonstrates that our generative retrieval approach effectively leverages the language models causal modeling within flexible retrieval framework. (iii) Using our constructed vocabulary (Ego4D-0.8M, where the ground truth may not be included), the prefixs performance is slightly lower than the baseline (Eval-4.6K), as the full vocabulary may not contain exact matches from the development set. However, when equipped with our postfix, performance improves, demonstrating the high expressiveness of our full narration vocabulary and postfix. (iv) In terms of decoding speed, VLog is comparable to the fastest retrieval model, achieving 20x speedup over generative models. Although indexing slows slightly with large vocabulary (0.8M), it remains significantly faster (10x) than generative models, highlighting its efficiency advantage with the narration vocabulary. These ablation studies demonstrate the great potential of VLog, video narrator that provide concise narrations with flexible query conditions and fast decoding speed. Method PT by? Time (s) ClipBERT [21] TimeSformer [5] Paprika [60] DistantSup [29] VideoTF [37] ProcedureVRL [59] VideoTaskGraph [3] VideoLLM-online-7B [8] COCO+VG HT100M HT100M HT100M HT100M HT100M HT100M N/A GPT2-medium VLog VLog N/A N/A Ego4D 0.21 0.01 0. Top-1 Acc Task Next 65.4 85.3 85.8 90.0 91.0 90.8 90.5 92.1 82.4 93.0 94.4 - 34.0 43.2 39.4 42.4 46.8 40.2 48.1 32.1 46.0 48. Step 30.8 46.5 51.0 54.1 56.5 56.9 57.2 59.8 44.6 56.1 57.4 Table 2. Activity perception results on COIN benchmarks: step recognition, task recognition, and next-step forecasting. . 4.3. Adapting VLog to Different Tasks Beyond casual retrieval tasks, we next demonstrate how VLog can be adapted to other mainstream tasks: Finegrained action perception and High-level reasoning QA. Fine-grained Action Perception: COIN. The COIN tasks consist of closed-set action categories, enabling our models to adapt seamlessly by substituting the vocabulary with COINs predefined category set. Additionally, different setting i.e. steps, tasks, or next actions are matched in VLog by using varied queries as conditions. We develop generative GPT2 baseline for comparison, which directly outputs the ground-truth string. All methods are fine-tuned on the COIN dataset, with results provided both with and without Ego4D vocabulary pretraining. As demonstrated in Tab. 2, VLog outperforms the GPT2 baseline by significant margin as well as process speed per clip (20). With lightweight model size (124M), VLog achieves performance comparable to the state-ofthe-art LMM [8], built on LLama-2-7B. Additionally, we demonstrate that generative retrieval pretraining from Ego4D successfully transfers to the COIN dataset. High-level Reasoning QA: EgoSchema. In Tab.3, we present evaluation results on the Egoschema MCQ task. Our focus is not on surpassing state-of-the-art methodsmost baselines [13, 49] rely on closed-source APIs (e.g., GPT-4o) or large-scale pretraining. Instead, we aim Methods Narrator Answerer OWL-ViT [36] Ego? LLaVA-13B MVU [41] Mixtral-8x7B LangRepo [18] LaViLa [58] LaViLa [58] LLoVi [56] LLama3-8B SigLIP&GPT-2 LLama3-8B Generative LLama3-8B VLog Ours Subset Full Time (s) 60.3 66.2 67.0 66.5 70.4 37.6 41.2 38.8 37.4 43.1 48.2 2.3 Video multiple-choice question-answering on Table 3. Egoschema. Each baseline consists of narrator (to provide video information as reference) and an answerer (to respond to the question). We report accuracy on both the subset and fullset, along with processing time per 180-second video. 7 to validate the transferability of VLogs reasoning capabilities. To enable QA, we use VLog as narrator to densely caption each long video, creating an informative document that LLMs can reference to answer questions accurately. An accurate narration should assist the LLM to correctly identify the answer. We list highly relevant baselines, which has narrator and (open-source LLMs) answers. We also develop generative baselines and compare their accuracy and runtime per 180 sec video. LLoVi [56], which uses the same LLama3-8B models as the answerer and GPT2 narrator [58] pretrained on [15], being comparable baseline. Our VLog demonstrates an improvement margin (+3.4% accuracy on the subset and +4.3% on the full set), indicating more accurate narrations. Additionally, it achieves faster processing times per long video compared to generative baselines. 4.4. Vocabularies Upgrading for OOV In Tab. 4, we study the vocabulary upgrading strategy to address OOV problem. To design the experiments, we use HiRESTs step captioning as benchmark, which includes several novel event (e.g., make diet coke and mentos rocket) not seen in our vocabulary. We include various baseline models for comprehensive comparison, selecting LLaVA-OV-0.5B [22] the LMM in our agentic framework, conditioned on LLMs [47] vocabulary. For VLog models, we develop several variants with different vocabulary sizes and sources, including Ego4D, and oracle HiREST vocabularies, we compare produce narrations with groundtruth narrations by captioning metrics. Models Vocab size METEOR CIDEr SPICE LLaVA-OV (Vid) Qwen2 (152K) LLaVA-OV (Img) Qwen2 (152K) VLog VLog VLog VLog VLog Ego4D (0.8M) COIN (778) COIN +Upgrade (1223) HiRESTs task+Upgrade HiREST 1.2 2.3 2.6 3.0 4.2 4.8 5.8 1.0 4.2 5.4 6.9 12.6 14.7 21.2 0.1 2.5 2.6 2.3 3.0 3.2 4. Table 4. Key studies on Vocabulary upgrade strategies for the HiREST Step Captioning Task across different vocabularies. In the experiment, we first observe that LLaVA-OV0.5B does not achieve high score with video inputs compared with image inputs, likely suffering hallucinations in small model size. When come to VLog, it is notable that the large Ego4D vocabulary underperforms compared to COIN, likely due to differences between egocentric and web-instruction videos. However, augmenting with 445 vocabulary items tailored for HiREST improves performance, validating this approach. Adding the oracle task name as condition results in slight performance boost, and using the oracle HiREST vocabulary achieves the best results. This demonstrates that vocabulary selection is crucial and that vocabulary upgrade strategy supports adaptation to unseen novel narrations. 4.5. Ablation of Key Design Chocies Hierarchical Indexing. In Fig. 6, we study the hierarchical indexing using the Egoschema val. set. BF denotes Brutal search over the full vocabulary (0.8M), while Hier. represents our proposed hierarchical indexing method. Compared to brutal search over large space, our strategy is 15 faster, only averaging 2.3 seconds per video, and yields comparable accuracy. Figure 6. Ablation of Vocabulary Indexing Strategy Figure 7. Ablation of Reference Video Number Ablation studies of Vocabulary indexing on Figure 8. Egoschema QA. Left: our hierarchical indexing (by scene) is 15 faster than Brutal search over full vocabulary while keeping the accuracy. Right: Using the same number of reference video for constructing narration vocabulary, selected by scene yields more relevant vocabularies. In Fig. 7, we compare VLog scene-based indexing with random indexing, adjusting the vocabulary size by varying the number of reference videos. We find that scene-based retrieval consistently yields more reliable narration vocabulary and outperforms random sampling for the same vocabulary size. Accuracy improves with larger vocabulary sizes and additional reference videos, ensuring matched narrations appear more frequently. Definition of Retrieval Token. In Tab.5, we study how to define the memory token within VLog, which could be the following variants: (i) EOS, (ii) learnable token, (iii) Pooling by all visual features. Method Casual Retrieval COIN-Step COIN-Task EOS Learnable Pooling 4.3 4.2 4.7 51.3 50.9 54.0 93.1 92.5 94.0 Table 5. Ablation of different retrieval tokens initialization. We found that the pooling strategy outperforms the EOS method, especially on COIN-step. However, for COINtask, which requires longer input durations, the scores are closer. This suggests that for fine-grained action recognition, effective initialization is crucial. Can LMs knowledge help Generative Retrieval? In Tab.6, we examine whether the pretrained knowledge of LLMs can enhance generative retrieval performance. For this, we develop several variants: GPT2 models with and without pretrained weights and models of different sizes to assess the strengths of LMs knowledge. Method Size Casual Retrieval COIN-Step COIN-Task GPT2 GPT2 (Random init.) GPT2-Medium GPT2-Large 124M 124M 355M 774M 4.7 3.8 4.6 4.9 54.0 51.5 55.4 56.4 94.0 91.0 94.8 93.2 Table 6. Ablation of LMs pretrained weights and sizes. In general, pretrained and larger GPT2 models are beneficial, achieving higher scores on COIN-Step, which indicates that increased model size and richer textual knowledge enhance casual relationship for generative retrieval. 4.6. Visualization Below, we provide an example to illustrate how VLog operates in full decoding process. Given video clip, VLog begins by identifying the scenariodoing yardworkthen proceeds to the corresponding prefix vocabulary, resulting in move lawn mower machine and finally completes the postfix as on grass. Additional examples, illustrations on handling OOV cases and failure cases are available in the Supp. quantitative analysis Sec. Figure 9. Illustration of VLogs full decoding process. 5. Conclusion and Limitations We present VLog, novel framework for video streaming with narration vocabulary. Built on lightweight GPT-2 model, VLog introduces an innovative generative retrieval approach, combining causal modeling with retrieval efficiency. Additionally, VLog incorporates hierarchical vocabulary indexing and vocabulary update mechanism. Experiments across several datasets demonstrate that VLog enables concise, contextually accurate, and efficient narrations, highlighting its potential for real-time video processing. We acknowledge that VLogs design is constrained by its predefined vocabulary. Future work will explore how to explore VLog to more diverse domains with generalized query support. A. Appendix A.1. VidCab construction Algorithm 1 Narration Pairing Encoding Require: List of narrations Ensure: Prefix list P, Postfix list We begin by sourcing video clips from EgoClip [27], excluding any videos associated with downstream tasks such as Egoschema [33]. Next, we clean the narrations by removing special tags like #C and perform deduplication within each video, resulting in approximately 0.8M narrations. Using our Narration Pairing Encoding method, we generate prefix set containing 0.6M entries and postfix set with 5K entries, where the postfix is shared across all narrations and deduplicated. Finally, we create training and evaluation split at 10:1 ratio, referred to as VidCabTrain and VidCab-Eval, respectively. A.2. Narration Pairing Encoding In the above algorithm, we display the process of our Narration Pairing Encoding algorithm, which mainly includes two parts: (i) Build Prefix Dictionary: This step exhaustively enumerates all possible word combinations for each phrase to build map between any prefix and the corresponding postfix narrations. (ii) Extract Prefixes and Postfixes: For each narration, we determine whether other narrations share its full prefix. If not, we add it to the prefix list. If they do, we extract and collect the differing postfixes from the narrations that share its prefix. Figure 10. Illustration of VLogs progressively decode prefix and postfix vocabulary respectively. Prefix w1 w2 . . . wi Add to D[p] Split into words [w1, w2, . . . , wk] for = 1 to do 1: Step 1: Build Prefix Dictionary 2: Initialize prefix dictionary empty 3: for all narration do 4: 5: 6: 7: 8: 9: end for 10: Step 2: Extract Prefixes and Postfixes 11: Initialize prefix list , postfix list 12: for all narration do 13: 14: if D[n] contains only then"
        },
        {
            "title": "Add n to P",
            "content": "end for else for all other narration D[n] do if = then Get suffix Remove prefix from Add to 15: 16: 17: 18: 19: 20: end if end for 21: end if 22: 23: end for List possible short actions that could take place in the scene. Write each action as short narration (a verb with noun). Separate by ; The following is examples. scene: In the heart of the kitchen, man skillfully slices into ripe mango, its golden flesh gleaming under the light. narration: Slice mango; Hold knife; Cut mango; Place seed; Wipe counter; Drop pieces; Grip mango; Rest knife; Smell mango; Gather chunks. scene: woman sits by the fireplace, knitting scarf as the flames crackle warmly in the background. narration: Knit scarf; Hold needles; Loop yarn; Adjust thread; Pull stitch; Rest hands; Drop yarn; Smell smoke; Listen flames; Rub hands; Fold scarf; Gather wool; Stare fire; Sit still; Tap needle. scene: {scene} narration: In Fig. 10, we display how VLog progressively decode the prefix and postfix respectively. It first use the memory token to retrieve the prefix narration, and next it append the prefix narration and use the memory token to retrieve the postfix for full narration. The {scene} is output by LLaVA-OV-0.5B [22] with prompt: What is the overall activity in the scene? Answer briefly in one sentence. A.4. Experimental Settings A.3. Vocabulary Update Templates Below, we attach the prompts for Qwen2.5 [47], which is used for produce narrations directly. is Our [54] SigLIP on model google/siglip-so400m-patch14-384. During training, we fully fine-tune the GPT-2 model with batch size of 32, learning rate of 3e-4, and sampling rate of 8 frames per short video clip. For long videos, such as those in the EgoSchema dataset [33], we do not based compress the entire video into single embedding. Instead, we uniformly sample long videos into multiple fixed-length clips (1 second each) and process them in streaming fashion. A.5. Complexities Analysis Let us clarify each term when generating narrations: (i) Encoding: We embed the entire vocabulary once and then reuse it O(1). (ii) Decoding: This should be O(αN ), where α is the speed per decoding step. (iii) Upgrading (optionally): O(C), where is the upgrade times (C ). For large , the overall complexity O(1) + O(αN ) + O(C) O(αN ) remains efficient as the encoding and upgrading costs become negligible. Below is the timing analysis on 4.6K VidCap-Eval: Models Vocab. size GPT-2 VLog VLog 32K 4.6K 4.6K (+486) Encoding(s) Decoding(s) Upgrading(s) R@1 Total (s) 3.6 3.6 207.8 10.4 10.5 38.4 7.9 12.4 13.7 207.8 14.0 52.4 A.6. Subwords v.s. Narration Vocabulary on Easy v.s. Complex tasks? Our VLog is prioritize task-specific efficiency over generalist models. We compare the two in the below Table. VideoLLMs General VLog Domain Vocabulary Subwords Backbone LLMs (2B+) Decoding Token Gen. Multi-Purpose Highlights Specific Narrations GPT-2 (345M) Retrieval Efficiency Whether Subwordsor NarrationVocabulary is depends on how tasks define minimal semantic units for videos. Subwords capture every detail but may be redundant for long videos, while narrations offer event contexts quickly but may miss finer details. To balance expressive granularity and efficiency, an idea is to cooperate two fashions like our vocab. upgrading or retrieve narration first and then generate minimal subwords as needed. We are interested in further exploring the latter. A.7. Improvment by Stronger LLM We chose GPT-2 because its simplicity and lightweight nature make it representative baseline. To demonstrate scaling, we upgraded GPT-2 to Qwen2-7B, resulting in significant performance gains, and beat its comparable baseline Qwen2-VL-7B. Models Qwen2-VL VLog (GPT-2) VLog (Qwen2) Size 7B 345M 7B EgoSchema QA val. 72.8 70.4 74.8 Decoding Time (s) 79.4 2.3 6. B. Qualitative Examples B.1. VLog for Reasoning Retrieval In Fig.12, we illustrate how VLog retrieves the vocabulary (blue indicating prefixes and green indicating postfixes) conditioned on different queries. For instance, in example (b), the query What is the next activity in the video? retrieves Grab bag of chips using the left hand as result, while the query What is the previous activity in the video? retrieves Adjust the steering wheel using the hand as result, demonstrating VLog capability to infer relationships between sequential events. B.2. How does Vocabulary Updating work? In Fig.13, we demonstrate how VLogs vocabulary updating process effectively expands its descriptive range. Given the first frame of video clip, LLaVA-OV [22] generates an initial brief description, which is then passed to Qwen2.5 [47] to imagine and expand possible vocabulary terms. For instance, in (a), LLaVA-OV identifies simple construction project involving multiple yellow pencils, and Qwen2.5 extends this by generating potential actions such as Arrange pencils and Hold pencils, which collectively capture most events in the video. However, limitations still exist with the models. For example, in (c), while the activity Make Pineapple Fritters is identified, the model struggles to detect the specific ingredient pineapple, making it challenging for the expanded vocabulary to recognize or describe the desired object accurately. These challenges highlight areas for improvement in object-specific vocabulary generation. B.3. Limtation by VLog. Figure 11. VLog still fail to capture the video with broad descriptive range or high-level information e.g. characters. We acknowledge that VLog still has limitations, as illustrated in 11. For example, when videos have broad expressive range, such as those involving multiple individuals or focusing on different aspects depending on subjective interpretation, it becomes challenging to rely on narrationwise closed-set vocabulary. Additionally, in more complex scenarios, such as movies, where character information and 10 Figure 12. VLog enables retrieval through reasoning, conditioned on different queries. Blue represents prefixes, while green represents postfixes. dialogues play central role, the current approach struggles. These cases may require return to generalist model capable of handling subword tokens for richer representations."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [2] Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. arXiv preprint arXiv:2211.09260, 2022. [3] Kumar Ashutosh, Santhosh Kumar Ramakrishnan, Triantafyllos Afouras, and Kristen Grauman. Video-mined task graphs for keystep recognition in instructional videos. In NeurIPS, 2023. [4] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17281738, 2021. [5] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, 2021. [6] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In Proceedbenchmark for human activity understanding. ings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. [7] Dibyadip Chatterjee, Fadime Sener, Shugao Ma, and Angela Yao. Opening the vocabulary of egocentric actions. Advances in Neural Information Processing Systems, 36, 2024. [8] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1840718418, 2024. [9] Tianyi Cheng, Dandan Shan, Ayda Hassen, Richard Higgins, and David Fouhey. Towards richer 2d understanding of hands at scale. Advances in Neural Information Processing Systems, 36:3045330465, 2023. 11 Figure 13. Illustration of VLogs vocabulary updating process. Given the first frame of video clip, LLaVA-OV [22] provides brief initial description, which is then passed to Qwen2.5 [47] to generate and expand the possible vocabulary. [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023. [11] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar, Richard Higgins, Sanja Fidler, David Fouhey, and Dima Damen. Epic-kitchens visor benchmark: Video segmentations and object relations. Advances in Neural Information Processing Systems, 35:1374513758, 2022. [12] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [13] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memory-augmented multimodal agent for video understanding. arXiv preprint arXiv:2403.11481, 2024. [14] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. [15] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: 12 Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022. [16] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. arXiv preprint arXiv:2311.18259, 2023. [17] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. Vlm2vec: Training vision-language arXiv models for massive multimodal embedding tasks. preprint arXiv:2410.05160, 2024. [18] Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo Park, and Michael Ryoo. Language repository for long video understanding. arXiv preprint arXiv:2403.14622, 2024. [19] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal inputs and outputs. In International Conference on Machine Learning, pages 1728317300. PMLR, 2023. [20] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvembed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. [21] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, pages 73317341, June 2021. [22] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. [24] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [25] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded the language-image pre-training. IEEE/CVF conference on computer vision and pattern recognition, pages 1096510975, 2022. In Proceedings of [26] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint arXiv:2311.17043, 2023. [27] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric XU, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric video-language pretraining. Advances in Neural Information Processing Systems, 35:75757586, 2022. [28] Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. Univtg: Towards unified videothe language temporal grounding. In Proceedings of 13 IEEE/CVF International Conference on Computer Vision, pages 27942804, 2023. [29] Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang, and Lorenzo Torresani. Learning to recognize procedural activities with distant supervision. In CVPR, pages 1384313853, 2022. [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. [31] Feipeng Ma, Hongwei Xue, Guangting Wang, Yizhou Zhou, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike Zheng Shou, and Xiaoyan Sun. Multi-modal generative embedding model. arXiv preprint arXiv:2405.19333, 2024. [32] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [33] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very arXiv preprint long-form video language understanding. arXiv:2308.09126, 2023. [34] Meta. Build the future of ai with meta llama 3. https: //llama.meta.com/llama3/, 2024. [35] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, and Josef Sivic. Ivan Laptev, Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. [36] Minderer, Gritsenko, Stone, Neumann, Weissenborn, Dosovitskiy, Mahendran, Arnab, Dehghani, Shen, et al. Simple open-vocabulary object detection with vision transformers. arxiv 2022. arXiv preprint arXiv:2205.06230, 2, 2022. [37] Medhini Narasimhan, Licheng Yu, Sean Bell, Ning Zhang, and Trevor Darrell. Learning and verification of task structure in instructional videos. arXiv:2303.13519, 2023. [38] Shraman Song, Pramanick, Sayan Nag, Yale Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou, Egovlpv2: Rama Chellappa, and Pengchuan Zhang. Egocentric video-language pre-training with fusion in the In Proceedings of the IEEE/CVF International backbone. Conference on Computer Vision, pages 52855297, 2023. [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, ICML, pages 87488763, 2021. [40] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, page 9, 2019. [41] Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Understanding long videos in arXiv preprint language model pass. and Michael Ryoo. one multimodal arXiv:2403.16998, 2024. [55] Chuhan Zhang, Ankush Gupta, and Andrew Zisserman. Helping hands: An object-aware ego-centric video recognition model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1390113912, 2023. [56] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. simple llm framework for long-range video question-answering. arXiv preprint arXiv:2312.17235, 2023. [57] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [58] Yue Zhao, Ishan Misra, Philipp Krahenbuhl, and Rohit Girdhar. Learning video representations from large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6586 6597, 2023. [59] Yiwu Zhong, Licheng Yu, Yang Bai, Shangwen Li, Xueting Yan, and Yin Li. Learning procedure-aware video representation from instructional videos and their narrations. In CVPR, pages 1482514835, 2023. [60] Honglu Zhou, Roberto Martın-Martın, Mubbasir Kapadia, Silvio Savarese, and Juan Carlos Niebles. Procedure-aware pretraining for instructional video understanding. In CVPR, pages 1072710738, 2023. [61] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [42] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. [43] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, and Vikas Chandra. Longvu: Spatiotemporal adaptive compression for long video-language understanding, 2024. [44] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449, 2023. [45] Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741, 2022. [46] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video In Proceedings of the IEEE/CVF Conference analysis. on Computer Vision and Pattern Recognition, pages 1207 1216, 2019. [47] Qwen Team. Qwen2.5: party of foundation models, September 2024. [48] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pretraining. arXiv preprint arXiv:2212.03533, 2022. [49] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understandarXiv preprint ing with large language model as agent. arXiv:2403.10517, 2024. [50] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. [51] Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, and Mike Zheng Shou. Videollm-mod: Efficient videolanguage streaming with mixture-of-depths vision computation. Advances in Neural Information Processing Systems, 37:109922109947, 2024. [52] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [53] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vidchapters-7m: Video chapters at scale. arXiv preprint arXiv:2309.13952, 2023. [54] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023."
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore"
    ]
}