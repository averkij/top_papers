{
    "paper_title": "VisualLens: Personalization through Visual History",
    "authors": [
        "Wang Bill Zhu",
        "Deqing Fu",
        "Kai Sun",
        "Yi Lu",
        "Zhaojiang Lin",
        "Seungwhan Moon",
        "Kanika Narang",
        "Mustafa Canim",
        "Yue Liu",
        "Anuj Kumar",
        "Xin Luna Dong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We hypothesize that a user's visual history with images reflecting their daily life, offers valuable insights into their interests and preferences, and can be leveraged for personalization. Among the many challenges to achieve this goal, the foremost is the diversity and noises in the visual history, containing images not necessarily related to a recommendation task, not necessarily reflecting the user's interest, or even not necessarily preference-relevant. Existing recommendation systems either rely on task-specific user interaction logs, such as online shopping history for shopping recommendations, or focus on text signals. We propose a novel approach, VisualLens, that extracts, filters, and refines image representations, and leverages these signals for personalization. We created two new benchmarks with task-agnostic visual histories, and show that our method improves over state-of-the-art recommendations by 5-10% on Hit@3, and improves over GPT-4o by 2-5%. Our approach paves the way for personalized recommendations in scenarios where traditional methods fail."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 2 ] . [ 1 4 3 0 6 1 . 1 1 4 2 : r VisualLens: Personalization through Visual History Wang Bill Zhu1,2,, Deqing Fu1,2,, Kai Sun1, Yi Lu1, Zhaojiang Lin1, Seungwhan Moon1, Kanika Narang1, Mustafa Canim1, Yue Liu1, Anuj Kumar1, Xin Luna Dong1 1Meta, 2University of Southern California Work done at Meta We hypothesize that users visual history with images reflecting their daily life, offers valuable insights into their interests and preferences, and can be leveraged for personalization. Among the many challenges to achieve this goal, the foremost is the diversity and noises in the visual history, containing images not necessarily related to recommendation task, not necessarily reflecting the users interest, or even not necessarily preference-relevant. Existing recommendation systems either rely on task-specific user interaction logs, such as online shopping history for shopping recommendations, or focus on text signals. We propose novel approach, VisualLens, that extracts, filters, and refines image representations, and leverages these signals for personalization. We created two new benchmarks with task-agnostic visual histories, and show that our method improves over state-of-theart recommendations by 5-10% on Hit@3, and improves over GPT-4o by 2-5%. Our approach paves the way for personalized recommendations in scenarios where traditional methods fail. Date: November 26, 2024 Correspondence: Wang Bill Zhu at wangzhu@usc.edu"
        },
        {
            "title": "1 Introduction",
            "content": "Imagine personal assistant observing what you do in your daily life. With her keen insight, she can make informed guesses about what you may enjoy or find intriguing. When you ask for recommendations on anything from restaurants and activities to movies, books, and products, based on her in-depth understanding of you she will provide suggestions tailored specifically to your tastes. Although the idea seems straightforward, comprehensive personal assistant that can make recommendations across various aspects of life has yet to become reality. Most existing recommendation systems are limited to specific domains and rely on interaction history within the particular verticals (Tan and Jiang, 2023; Papadakis et al., 2023). For instance, an e-commerce platform may suggest products based on your past purchases, but it will not take into account your dining habits or interests outside of shopping. Similarly, restaurant booking system may recommend eateries based on our previous reservations, but it will not know your new interest in cooking or your recent travels. video streaming service may propose movies based on your viewing history, but it will not be aware of your cultural curiosities. Recent advances in personalized conversation systems have utilized task-agnostic conversation logs to provide personalized answers (Harper and Konstan, 19; Ni et al., 2019; Liu et al., 2023). However, these approaches predominantly rely on textual data. We hypothesize that users visual history, comprising images reflecting their daily life, offers valuable insights into their interests and preferences. This work aims to investigate whether such visual history can enable recommendation system to better understand users individual preferences, and deliver more personalized recommendations. There are many challenges in achieving this goal. First, although we envision future system, akin to Vannevar Bushs Memex (Bush, 1945), that can record users visual history with her consent, such system remains elusive because of hardware limitations such as battery life, thermal constraints, and storage capacity. Second, users visual history is inherently diverse and noisy, containing images not directly related to specific recommendation ask, entities not accurately reflecting the users preferences (e.g., visiting museum without Figure 1 VisualLens leverages users task-agnostic visual history to provide personalized recommendations. Our method outperforms GPT-4o by 1.6%4.6% on Hit@3. enjoying it), and elements not preference-indicative (e.g., background objects like trash cans). Lastly, since many recommendation tasks require instant responses, we need to develop low-latency solution. In this paper, we propose novel approach VisualLens, as first step towards harnessing users visual history for personalized recommendation. Our first contribution tackles the challenge of history availability by leveraging photos taken by user. Unlike extensive visual logs, the photos user takes require significantly less storage, oftentimes are readily available in reviews and social media posts, and convey more meaningful signals about users interests and preferences. To facilitate evaluation, we created two benchmarksGoogle Review-V and Yelp-Vusing publicly available photos, providing foundation for personalization assessments. Our second contribution is recommendation solution that leverages users visual history to improve personalization. At the core of our solution is set of models that effectively extract the essential signals from the visual history to infer the users preferences. First, given recommendation request, we selectively retrieve only the most relevant images, thereby reducing unnecessary noises and distractions. Second, to effectively capture the signals photo may convey, we use not only visual embeddings but also text captions and aspect words extracted from the image. Third, we adopt an iterative refinement process that continually improves aspect extraction to better reflect user interests and inform recommendations. By jointly training aspect word extraction and prediction tasks within unified model, VisualLens not only reduces the overall parameter size, but also enhances the models capability to understand and utilize the visual history for accurate recommendation. Our third contribution is pipeline that applies multiple optimizations to accelerate recommendation generation. To minimize online processing time, we pre-generate image captions and extract aspects offline. Furthermore, we introduce grid-based approach that aggregates multiple images into grid, enabling simultaneous processing of multiple images and significantly reducing computation overhead. Our comprehensive experimental study shows promising recommendation quality of VisualLens. It achieved 82-91% Hit@10 on the Google Review-V and Yelp-V benchmarks, out-performing state-of-the-art (UniMP, Wei et al. (2024a)) by 10%. Even comparing with GPT-4o, our 8B model outperforms on all metrics, improving Hit@3 by 1.6% and 4.6% respectively on the two benchmarks."
        },
        {
            "title": "2 Related Works",
            "content": "Recommendation system with large language models. Large Language Models (LLMs) have demonstrated strong potential in recommendation systems with their advanced language processing capabilities (Tan and Jiang, 2023). On item-based recommendation, studies such as LLM4RS (Dai et al., 2023), LLMRank (Hou et al., 2024), CLLM4Rec (Zhu et al., 2024b), P5 (Geng et al., 2022), and Sanner et al. (2023) explored 2 Figure 2 VisualLens inference pipeline: the offline process augments images in the visual history with captions and aspect words; the runtime recommendation process retrieves relevant images, generate user profile accordingly, and then predict candidate preferences. various LLM prompt and bootstrapping strategies, showing competitive performance, especially in cold-start scenarios. Generative recommendation with open-domain prompts is explored by GenRec (Ji et al., 2023), though fine-tuning remains crucial. Fine-tuning approaches include personalized aspect extraction (Li et al., 2023c), and multitasking on LLaMa models (Yang et al., 2023). Retrieval-enhanced models, such as ReLLa, improve recommendation by retrieving relevant behavior sequences (Lin et al., 2024). Instruction-tuning and graph augmentation approaches are explored in InstructRec (Zhang et al., 2023), LLMRec (Wei et al., 2024b), and LKPNR (Hao et al., 2023). LLMs also work well with content-based recommendation, such as Li et al. (2023a) on prompting, and ONCE (Liu et al., 2023), LoID (Xu et al., 2024) on fine-tuning. Multimodal large language models. Multimodal LLMs are becoming increasingly powerful in processing images and videos and in generating human-like text, exemplified by models like GPT-4 family (OpenAI, 2024), Claude 3.5 family (Anthropic, 2024), Gemini and PaliGemma (Google, 2023; Beyer et al., 2024), LLaVA model family (Liu et al., 2024), and Llama 3 Vision models (Meta, 2024). However, they still suffer from strong language prior (Fu et al., 2024a; Parcalabescu and Frank, 2024), generalization (Zhu et al., 2022) and hallucinations (Fu et al., 2024b), or require heavy text transcription (Zhu et al., 2024a). The extent to which they can understand user history, in analogy to LLM recommendation systems, is still unclear. Multimodal recommendation systems. Multimodal recommendation systems leverage multiple data types, such as text and images, to improve recommendation relevance and personalization. Before the LLM era, Lee and Abu-El-Haija (2017) proposed systems for content-only video recommendation using similarity learning. PC2L (Yan et al., 2023) develop an LLM model that provides multimodal explanations for recommendations. On modalities beyond image and text, LSVCR (Zheng et al., 2024) built joint recommendation system integrating comments with video items (Davidson et al., 2010; Du et al., 2020; Gao et al., 2017; Zhou et al., 2024). The current state-of-the-art image-text recommendation, UniMP (Wei et al., 2024a), extended single-task multimodal personalization (He and McAuley, 2016; Tang et al., 2020; Wei et al., 2019, 2023) on multitask website-based shopping. Methods such as RecFormer (Li et al., 2023b) and UniSRec (Hou et al., 2022) convert images to short captions to utilize text-only models. However, none of these methods consider the scenario of personalized recommendation from the users visual history. In this work, we present novel method VisualLens leveraging multimodal LLM to achieve personalization with visual history."
        },
        {
            "title": "3 Problem Definition",
            "content": "Consider recommendation QA (question answering) task, where the user asks recommendation question q, and the recommender answers with ranked list of items. Good recommenders shall rank the items that the user is more likely to be interested in or willing to try early in the list. The recommender is facilitated with task-agnostic visual history Hu for each user u, which contains series of photos, taken or posted by the user, not necessarily related to q. We state three assumptions to allow generalization. First, the photos may not be directly relevant to the question. Second, an image may not be associated with any candidate item, and even so, the candidate ID is not given. Third, photo does not necessarily imply strong preferences. Figure 1 shows an example question, visual history, and candidates. To simplify the problem, we assume candidate retriever exists to retrieve all candidates that satisfy the users question. Each candidate is represented with (xs, Is) pair, where xs is the name and text descriptions, and Is is the image set for the item. Traditional recommendation setting considers two types of signals. The first is task-specific set of items in the candidate set, which captures user interest or at least user history. The second is set of user-specific attributes such as the users age, gender, and interests. This paper focuses on task-agnostic visual history and leaves integration of these traditional signals for future extensions."
        },
        {
            "title": "4 Recommendation Method",
            "content": "We start with describing the VisualLens solutions. Our solution contains two parts: offline history augmentation, and runtime recommendation generation. Offline history augmentation (4.1) augments each image in the visual history with captions and aspect words extracted from the image. Figure 2 shows example captions and aspect words of the images in the visual history. Runtime recommendation generation (4.2) answers recommendation question in three steps. First, the history retrieval step retrieves only images relevant to q, since the visual history can be diverse and not all photos are relevant to every query. Second, the preference profiling step uses the retrieved images and their augmented captions and aspects to generate the users preference profile. Third, the candidate matching step matches the users preference profile with each candidate, to generate the confidence score for each candidate for ranking. VisualLens utilizes the same backbone model with parameter θ for aspect generation and candidate matching."
        },
        {
            "title": "4.1 Offline history augmentation",
            "content": "To best capture various signals in an image, we encode it to visual embeddings and augment it with caption and set of aspect words. As we show in the experimental ablation study  (Table 3)  , these augmentations allow us to leverage the strengths of various encoding techniques and considerably improve the recommendation quality. These augmentations can be used directly at recommendation time to reduce runtime latency. Image encoding. We encode each image in the visual history using the CLIP ViT-L/14@336px model (Radford et al., 2021). This embedding will be directly leveraged in image retrieval in the history retrieval step at runtime. Caption generation. We generate image captions using frozen LLaVA-v1.6 8B model (Liu et al., 2024). We prompted the model to generate captions within 30 words, such that the captions are succinct and short of hallucinations. Aspect word generation. Aspect words are descriptive terms that capture essential features or attributes of an image (e.g., dome, balcony, plants). Aspect words provide crucial details about the image. We prompted the backbone model to list aspect words for an image. We do not constrain the number of extracted aspect words since each image may have varying numbers of aspect words essential for final preference prediction. 4 We will describe in Section 5.3 how we finetune the model to generate aspect words that are most useful for recommendation tasks."
        },
        {
            "title": "4.2 Runtime recommendation generation\nHistory retrieval. Given a query q and a user’s visual history Hu, VisualLens first retrieves images that\nare related to q, denoted by Iu,q. We choose up to w images to cap the number of images we process at\nruntime, and ensure that only the most contextually relevant images are retained for further processing,\nthereby reducing noise.",
            "content": "In general, we can use any image retrieval method such as DELG (Cao et al., 2020). Here, we present method for categorical recommendations such as restaurants and museums, popular in recommendation tasks. For each category c, we randomly select set of candidate items in the category and average the visual embeddings of their images as the image embedding of the category, denoted by vc. Specifically, the category embedding is calculated as: vc = 1 (cid:88) j=1 v(j) where is the number of candidates, and v(j) indicates the visual embedding of the j-th item image in category c. The retrieval step measures the cosine similarity between the visual embedding vi of each image in the users history and the image embedding vc of the relevant category c. We then select the top-w images based on the cosine similarity scores. Preference profiling. Given set of retrieved images Iu,q, VisualLens then generates the users preference profile relevant to the images, their captions, and aspect words. critical part of this step is image encoding. Even after retrieval, the number of images is still large. Most multimodal LLMs allow context windows of limited sizes, constraining the number of images we can process. For example, for an input image of resolution 896 896, the PaliGemma model would generate an embedding of up to 4,096 tokens. typical LLM with context window of 8,192 tokens can take at most 2 images. We propose to group relevant images Iu,q into grid, where d2 = w, and treat all images in the grid as single image. If we have retrieved fewer than images, we pad with black background. Let be the maximum available resolution in multimodal LLM. The gridify process takes the grid and generates an image of fixed size Rhh3. Additionally, we number each image from 1 to d2 to ensure the images are grounded to the corresponding caption and aspect words in the input to the candidate matching. We denote user us preference profile on question by (iu,q, xu,q), where iu,q denotes the gridified image, and xu,q denotes the concatenated captions and aspect words of relevant images. Candidate matching. Finally, VisualLens takes the users preference profile (iu,q, xu,q) and set of candidates, each represented by (xs, Is), and generates the matching score for each candidate, which will then be used for ranking. This is achieved by prompting the multimodal candidate predictor, where we packed the user profile and candidates to the prompt through the image channel and the text channel separately (see the template in Appendix C)."
        },
        {
            "title": "5 Iterative Refinement and Joint Training",
            "content": "VisualLens requires LLMs for image encoding, image captioning, aspect word generation, and final preferencecandidate matching. Out-of-box LLMs do not perform well, and we conduct fine-tuning in three steps. First, to better encode the grid of images, we fine-tune image encoding with multi-image caption tasks. Second, since aspect generation is not standard multimodal task, we iteratively refine the aspect word extractor, ensuring that the keywords precisely capture the aspects of the image in suitable granularity. 5 Third, to best leverage personal profile signals including image embeddings, captions, and aspect words, we finetune the candidate matching model. This is done through joint multitask training with the aspect words generation."
        },
        {
            "title": "5.1 Multi-image caption pretraining",
            "content": "To facilitate the model to ground to each grid faithfully, we perform LoRA continual pretraining on dense captions. We adopt the dense captioning dataset DOCCI dataset (Onoe et al., 2024), which contains over 15,000 images and their corresponding dense captions. Each time, we randomly sample images = {i1, , iw} and their corresponding caption = {x1, , xw}, and then we construct gridified input image G(I) and target output text description (C) = Image 1: x1, , Image w: xw. Then we LoRA finetune the pretrained backbone model (e.g., MiniCPM-V2.5) on all image-caption pairs {G(I), (C)} so that the model is able to process the gridified user history grid by grid. We then use the continual pretrained model as the starting point to apply joint training described in 5.3."
        },
        {
            "title": "5.2 Iterative aspect word refinement",
            "content": "Unlike image captioning, aspect word generation is not standard multimodal task and lacks the extensive pretraining data typically available. Hence, with zero-shot prompting, the generated aspect words have large quality gap across different images, and the extracted aspects may not indicate user preferences. To finetune aspect word generation, we first generate the training data. For each image i, we start from an initial set of aspect words, denoted as (0) , which is generated by LLaVA-v1.6. In the jth round, we prompt separate Llama-3.1 70B model with (j1) , candidates, and ground truths, and ask it to select useful aspect words that are helpful in ground truth prediction, which constitute (j) . This refinement process continues for several rounds, and the iterations allow for converging extracted aspect words toward more accurate and relevant subset. Empirically, we observe that the refinement converges after approximately 4 rounds, and denote the 4th refined aspect word set (4) The backbone model with parameter θ is finetuned to optimize the cross-entropy (CE) loss over all images I, as Wi, which serves as the training target. i Lasp = 1 (cid:88) iI CE(Wi, pθ(xasp, i)), (1) where xasp is the prompt for aspect words generation."
        },
        {
            "title": "5.3 Joint training of aspect word generation and candidate matching",
            "content": "To take advantage of multitask training in multimodal recommendation (Wei et al., 2024a), we jointly train the aspect word generator and the candidate predictor on the backbone model. This joint training strategy allows the model to simultaneously learn to identify useful aspect words and make accurate predictions, thus improving overall performance. The joint loss function balances aspect word generation and candidate matching with weighting factor λ, where the candidate matching is optimized with binary cross-entropy (BCE) loss to handle multiple ground truth labels. Lpred = 1 (cid:88) j= BCE(Sj, pθ(xpred,j, iuj ,qj )), Ljoint = Lasp + λLpred, (2) (3) where uj, qj, Sj is the user, the question, and the ground truth set of candidates of the j-th example. The , and candidates. We text prompt xpred,j consists of the question qj, the text user preference profiles xuj ,qj LoRA finetune the model under the joint loss Ljoint. Dataset name Train split Dev split Test split Categories Avg. # images Avg. # GT Avg. # cand. GR-V Yelp-V 15.69M 4.12M 2K 2K 200K 100K 66 35 157.0 263.6 2.7 8.2 43.1 66.7 Table 1 Dataset statistics of Google Review-Vision (GR-V) and Yelp-Vision (Yelp-V). Modality Size Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10 MRR Google Review-V Yelp-V Naive baselines Random Rank by rating - - Fine-tuned models UniMP (Wei et al., 2024a) Llama-3.1-8B-Instruct (Meta, 2024) MiniCPM-V2.5 (Yao et al., 2024b) + + Direct inference Llama-3.1-70B-Instruct (Meta, 2024) GPT-4o (OpenAI, 2024) + Our method VisualLens (PaliGemma) VisualLens (MiniCPM-V2.5) Human annotations + + - - - 3B 8B 8B 70B - 3B 8B - 7.6 3. 13.8 15.8 16.1 16.2 17.1 21.0 15.8 34.1 36.3 36.4 35.9 37.3 55.0 55. 73.0 77.2 78.4 75.7 80.1 21.2 17.7 30.5 32.9 33.2 33.1 34.3 13.0 8. 22.4 24.1 24.8 25.2 26.1 33.6 28.0 48.5 52.2 53.0 53.2 54.5 72.7 72. 85.0 88.5 89.3 88.5 90.5 30.0 25.9 38.3 39.6 40.3 40.6 41.7 16.7 18. 22.0 36.3 38.9 45.0 77.1 82.3 - 33.5 35. - 27.8 28.3 36.0 58.8 59.1 66.0 90.4 91. - 44.3 44.9 - Table 2 Hit rates and MRRs of VisualLens vs. multiple baselines on Google Review-V and Yelp-V. The result shows (a) VisualLens outperforms other baselines, though has gap with the human oracle; (b) model size greatly affects the performance; (c) simply rank by rating is worse design than the random baseline."
        },
        {
            "title": "6.1 Benchmark creation",
            "content": "To the best of our knowledge, there is no existing benchmark (Harper and Konstan, 19; Ni et al., 2019; Wu et al., 2020; Wan and McAuley, 2018; Salemi et al., 2023) to evaluate personalization with task-agnostic visual history. We created two benchmarks, Google Review-V and Yelp-V, leveraging publicly available data from Google Local Review (Li et al., 2022) and Yelp (Asghar, 2016). User logs: For each user in the two datasets, we take the list of reviews in chronological order. Each review is associated with business name, categories and description. In Google Review-V, each review is associated with few photos, used for image logs. Yelp-V does not associate review with photos, so we randomly subsample one-third of the store profile pictures, such that different reviews for the same business can be associated with different images. Questions and visual history: We consider special type of questions, category recommendations, like \"Recommend nearby museum\". Such questions are both popular in real applications, and hard as there are many candidates satisfying the constraint. We remove small categories and most ambiguous categories, such as place, and spot. For each review regarding business of category c, we create question asking to recommend businesses in category c. We take all (and only) photos in the reviews before to generate the visual history. We consider the visual history task-agnostic since the categories are highly diverse (see Figure 4, and the photos are quite diverse too (e.g., park photo to illustrate happiness mentioned in the review). We filter instances where the history is too short (<10) or does not contain the questioned category. Candidates and ground truths: For review we take all reviews starting from to generate candidates and ground truths. To be realistic, we consider only nearby businesses of the same category as the candidate set, and the number of candidates is random number in [30, 100]. Candidates that also appear in the 7 # Representation Ret. Training Google Review-V Yelp-V Asp. Cap. Img. Iter. Joint Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10 MRR 1 2 4 5 6 7 16.7 16.1 15.7 15.2 14.8 13.5 11.5 36.3 35.8 35.2 34.7 33.9 32. 27.9 77.1 76.2 75.4 74.2 73.0 71.9 67.3 33.5 33.0 32.5 31.9 31.2 29. 25.9 27.8 27.2 26.9 25.8 25.0 22.0 20.1 58.8 57.9 57.5 55.3 53.9 48. 45.7 90.4 88.9 88.2 86.1 84.9 83.6 81.7 44.3 43.3 42.9 41.2 40.4 38. 36.8 Table 3 Ablation study on PaliGemma. Different components of VisualLens model: joint training (Joint), iterative refinement (Iter), aspect words (Asp.), captions (Cap.), image embedding (Img.), and relevant image retrieval (Ret.). MRR difference over 0.4 has p-value<0.04. users future reviews are considered as ground truths. To avoid falling into classification problem, we filter examples with only 1 ground truth in Google Review-V and fewer than 5 in Yelp-V. Summary: Table 1 gives the benchmark statistics. The ratio of an average number of candidates and that of ground truths is 16:1 for Google Review-V and 8:1 for Yelp-V, showing the difficulties of recommendation. By default, the train, dev, and test data have disjoint users. We discuss other splitting in Table 4."
        },
        {
            "title": "6.2 Evaluation measures",
            "content": "We use two metrics to evaluate recommendation quality. Hit@k. Hit@k = 1 1[rank(ri) k] checks if any relevant item is within the top-k ranked results, where is the number of examples, and rank(ri) is the rank of the first relevant item. Normally we check Hit@3 for recommendations in voice, Hit@10 for recommendations on screens. i=1 (cid:80)N Mean Reciprocal Rank (MRR). MRR = 1 measures the ranking quality by averaging the reciprocal ranks of the first relevant item for each example. The MRR ranges from 1/S to 1, where is the number of candidates. 1 rank(ri) i=1 (cid:80)N"
        },
        {
            "title": "6.3 Implementation and baselines",
            "content": "We ran VisualLens with two backbone models, smaller 3B model PaliGemma and larger 8B model MiniCPM-V2.5. We used an image grid of 8 8, and loss weighting factor λ = 2 in Eqn. 3. We compared VisualLens with multiple other solutions. Baselines: Randomly rank or select top-k ratings. Fine-tuned models: We fine-tuned three state-of-the-art solutions: multimodal personalization model UniMP (Wei et al., 2024a) (RedPajama 3B model), with the adaptation to replace their item image and attributes with image tokens in the visual history; Llama-3.1-8B-Instruct (Meta, 2024) with text-only user preference profiles; and MiniCPM-V2.5 8B (Yao et al., 2024a) with image profiles. Direct inference: We compared with two out-of-box models: Llama-3.1-70B-Instruct (Meta, 2024) with text-only preference profiles; GPT-4o (OpenAI, 2024) with multimodal profiles. To control the API cost, we subsample 1k instances from the test sets. Human annotation: Finally, we subsampled 50 examples from the test set for human annotation."
        },
        {
            "title": "7 Results and Analysis",
            "content": "We conducted experiments to answer four questions: Q1: Can we effectively leverage users visual history to improve personalization? 8 (a) (b) Figure 3 (a) MRR distribution over number of candidates, (b) MRR distribution over number of images. Both are on the User ID test set. We find (1) MRR converges when number of candidates exceeds 50; (2) MRR increases and flattens after reaching 100 images. Google Review-V Yelp-V LongHis Category User ID LongHis Category User ID VisualLens (PaliGemma) VisualLens (MiniCPM-V2.5) 35.9 38.0 34.9 37.1 33.5 35. 46.6 47.2 45.2 45.5 44.3 44.9 Table 4 Transferability: MRR of VisualLens models to different test setups. LongHis: train until certain timestamp and test afterwards. Category: held-out set of categories for testing per user. Use ID: train and test set share no common user ID. (a) (b) Figure 4 (a) MRR distribution over categories on Google Review-V, (b) MRR distribution over categories on Yelp-V. We find (1) the performance per category is loosely correlated with number of training data; (2) when category is more general and less ambiguous, the performance on the category is better. Q2: How does each element of VisualLens contribute to the recommendation quality? Q3: Can VisualLens transfer across users, unseen categories and longer history? Q4: What is the robustness of VisualLens?"
        },
        {
            "title": "7.1 Recommendation effectiveness (Q1)\nVisualLens significantly outperforms baselines. Our first observation from Table 2 is that all recommendation\nmodels that leverage the visual history significantly outperform baseline solutions on all metrics. In particular,\nthe best version of VisualLens improves Hit@3 over Random by 18% on Google Review-V and by 26% on\nYelp-V, and even more over Rank by rating (apparently, overall ratings do not cater to specific users). There\nis still a gap between VisualLens and human annotations, but comparing w. Random, it fills ∼75% of the\ngaps for hit@3 on both datasets.",
            "content": "VisualLens outperforms state-of-the-art solutions. VisualLens outperforms UniMP with the same number of trainable parameters. With the 8B MiniCPM-V2.5 backbone, VisualLens outperforms MiniCPM-V2.5 itself by 2.5% on Hit@3 on Google Review-V, and by 6% on Yelp-V, and we observe similar trend for the 3B models. Even comparing with significantly larger models, including Llama-3.1-70B-Instruct and GPT-4o without fine-tuning, VisualLens 7B improves by 1.6-5.6% on Hit@3."
        },
        {
            "title": "7.2 Ablation studies (Q2)",
            "content": "We evaluate the usefulness of each component in VisualLens in Table 3, with PaliGemma as the backbone. 1. History retrieval significantly improves the results, and can reduce Hit@3 by 7% on Google Review-V and by 12% on Yelp-V (#7 vs. #3). 2. All three representations of the images (embedding, caption, aspects) play an important role. Removing captions and aspect words can reduce Hit@3 by 3% on Google Review-V and by 9% on Yelp-V, even without fine-tuning (#6 vs. #3). Between the two, aspect words play more important role than captions (#5 vs. #4). 3. Both iterative training and joint multi-task training improve the recommendation quality. Removing both of them lowers Hit@3 by 1%+ on both data sets (#3 vs #1)."
        },
        {
            "title": "7.3 Transferability (Q3)",
            "content": "We tested transferability over users (default setting), over categories, and over different (longer) history lengths. Table 4 compares the MRR of VisualLens on both benchmarks, showing good transferability. MRR is highest when applied to longer history, with the effectiveness of history retrieval. Transferability is higher across categories than across users, both demonstrating the promise of leveraging task-agnostic signals, and illustrating slightly more challenges to transfer between users of different interest patterns."
        },
        {
            "title": "7.4 Robustness (Q4)",
            "content": "Finally, we conducted few robustness tests. 1. Figure 3a shows that as the number of candidates grows, the recommendation becomes harder and MRR gets lower. However, when the number of candidates exceeds 50, MRR converges. 2. Figure 3b shows that as the history grows, MRR increases and flattens after reaching 100 images. This trend is related to our grid size 8 8, since smaller than 64 images would not leverage all spaces in the grid. On the other hand, flat MRR after 100 images shows robustness against history noises with the retrieval step. 3. Finally, Figure 4 plots MRR for different categories. There are few factors that can affect recommendation quality. First and foremost, ambiguous categories like area, station, and market get the lowest MRR in Google Review-V. Second, general categories (e.g., museum, hotel) with bigger category sizes obtain higher MRR than specific ones (e.g., historical landmark). Third, transfer learning happens between neighbor categories; for example, deli and takeout achieve the top-2 and top-3 performance with less training data, since they are similar to the largest category restaurant."
        },
        {
            "title": "8 Conclusion and Discussion",
            "content": "In this paper, we proposed novel approach VisualLens for personalized recommendation using task-agnostic visual history. We advanced multimodal LLMs to extract meaningful signals from images to serve as proxies of user preferences. We created two benchmarks, Google Review-V and Yelp-V, to evaluate VisualLens, affirming the efficacy and robustness of VisualLens. VisualLens lends itself as first step towards recommendation systems leveraging task-agnostic visual history. There are still many future directions to explore. First, we can integrate the VisualLens solution with other accessible information, such as the date and location each image is taken, the specific fine-granularity entities (e.g., particular products) recognized from the images, and other user profiles. Second, we can extend the recommendation problems studied in this paper with broader QA tasks. Finally, we shall study how to guarantee privacy in the training with techniques like federated learning."
        },
        {
            "title": "Acknowledgements",
            "content": "The robot icon used in Figure 1 is from https://www.flaticon.com/free-icon/robot_2432846. The padlocks icons used in Figure 2 are from https://www.flaticon.com/free-icon/lock_996365 and https://www. flaticon.com/free-icon/padlock_535143. The illustration images in Figures 1, 2 are from DOCCI (Onoe et al., 2024)."
        },
        {
            "title": "References",
            "content": "Anthropic. Claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. https://www.anthropic. com/news/claude-3-5-sonnet. Nabiha Asghar. Yelp dataset challenge: Review rating prediction, 2016. https://arxiv.org/abs/1605.05362. Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. Paligemma: versatile 3b vlm for transfer, 2024. https://arxiv.org/abs/2407.07726. Vannevar Bush. As we may think. The Atlantic Monthly, 176(1):101108, July 1945. Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep local and global features for image search. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XX 16, pages 726743. Springer, 2020. Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun In Proceedings of the 17th ACM Conference Xu. Uncovering chatgpts capabilities in recommender systems. on Recommender Systems, RecSys 23, page 11261132. ACM, September 2023. doi: 10.1145/3604915.3610646. http://dx.doi.org/10.1145/3604915.3610646. James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi Sampath. The youtube video recommendation system. In Proceedings of the Fourth ACM Conference on Recommender Systems, RecSys 10, page 293296, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781605589060. doi: 10.1145/1864708.1864770. https://doi.org/10.1145/1864708.1864770. Xingzhong Du, Hongzhi Yin, Ling Chen, Yang Wang, Yi Yang, and Xiaofang Zhou. Personalized video recommendation using rich contents from videos. IEEE Transactions on Knowledge and Data Engineering, 32(3):492505, 2020. doi: 10.1109/TKDE.2018.2885520. Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. IsoBench: Benchmarking multimodal foundation models on isomorphic representations. In First Conference on Language Modeling (COLM), 2024a. Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, and Lawrence Chen. Tldr: Token-level detective reward model for large vision language models, 2024b. https://arxiv.org/abs/2410.04734. Junyu Gao, Tianzhu Zhang, and Changsheng Xu. unified personalized video recommendation via dynamic recurrent neural networks. In Proceedings of the 25th ACM International Conference on Multimedia, MM 17, page 127135, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450349062. doi: 10.1145/3123266.3123433. https://doi.org/10.1145/3123266.3123433. Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as language processing (rlp): unified pretrain, personalized prompt & predict paradigm (p5). Proceedings of the 16th ACM Conference on Recommender Systems, 2022. https://api.semanticscholar.org/CorpusID:247749019. Google. Gemini: family of highly capable multimodal models, 2023. Yulong Gu, Zhuoye Ding, Shuaiqiang Wang, Lixin Zou, Yiding Liu, and Dawei Yin. Deep multifaceted transformers for multi-objective ranking in large-scale e-commerce recommender systems. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, CIKM 20, page 24932500, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450368599. doi: 10.1145/3340531.3412697. https://doi.org/10.1145/3340531.3412697. Chen Hao, Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan Zhanwei, and Zhang Kai. Lkpnr: Llm and kg for personalized news recommendation framework, 2023. https://arxiv.org/abs/2308.12028. F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS), 5(4), 19. Ruining He and Julian McAuley. Vbpr: visual bayesian personalized ranking from implicit feedback. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI16, page 144150. AAAI Press, 2016. 12 Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. Towards universal sequence representation learning for recommender systems. In KDD, 2022. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. Large language models are zero-shot rankers for recommender systems, 2024. https://arxiv.org/abs/2305.08845. Jianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Juntao Tan, and Yongfeng Zhang. Genrec: Large language model for generative recommendation, 2023. https://arxiv.org/abs/2307.00457. Joonseok Lee and Sami Abu-El-Haija. Large-scale content-only video recommendation. In 2017 IEEE International Conference on Computer Vision Workshops (ICCVW), pages 987995, 2017. doi: 10.1109/ICCVW.2017.121. Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang, and Michael Bendersky. Teach llms to personalize an approach inspired by writing education, 2023a. https://arxiv.org/abs/2308.07968. Jiacheng Li, Jingbo Shang, and Julian McAuley. UCTopic: Unsupervised contrastive learning for phrase representations and topic mining. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6159 6169, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.426. https://aclanthology.org/2022.acl-long.426. Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. Text is all you need: Learning language representations for sequential recommendation. Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2023b. Pan Li, Yuyan Wang, Ed H. Chi, and Minmin Chen. Prompt tuning large language models on personalized aspect extraction for recommendations, 2023c. https://arxiv.org/abs/2306.01475. Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation, 2024. https://arxiv.org/abs/2308.11131. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. https://llava-vl.github.io/blog/2024-01-30-llava-next/. Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. Once: Boosting content-based recommendation with both openand closed-source large language models, 2023. https://arxiv.org/abs/2305.06566. Meta. The llama 3 herd of models, 2024. https://arxiv.org/abs/2407.21783. Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled reviews and finegrained aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 188197, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1018. Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Jason Baldridge. Docci: Descriptions of connected and contrasting images, 2024. https://arxiv.org/abs/2404.19753. OpenAI. Gpt-4o, 2024. https://openai.com/index/hello-gpt-4o/. Harris Papadakis, Antonis Papagrigoriou, Eleftherios Kosmas, Costas Panagiotakis, Smaragda Markaki, and Paraskevi Fragopoulou. Content-based recommender systems taxonomy. Foundations of Computing and Decision Sciences, 48 (2):211241, 2023. doi: doi:10.2478/fcds-2023-0009. https://doi.org/10.2478/fcds-2023-0009. Letitia Parcalabescu and Anette Frank. Do vision & language decoders use images and text equally? how self-consistent are their explanations? ArXiv, abs/2404.18624, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. https: //api.semanticscholar.org/CorpusID:231591445. Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. LaMP: When large language models meet personalization, 2023. Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, and Lucas Dixon. Large language models are competitive near cold-start recommenders for languageand item-based preferences, 2023. https://arxiv.org/abs/2307.14225. 13 Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 19, page 14411450, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450369763. doi: 10.1145/3357384.3357895. https://doi.org/10.1145/3357384. 3357895. Zhaoxuan Tan and Meng Jiang. User modeling in the era of large language models: Current research and future directions, 2023. https://arxiv.org/abs/2312.11518. Jiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 18, page 565573, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450355810. doi: 10.1145/3159652.3159656. https://doi.org/10.1145/3159652.3159656. Jinhui Tang, Xiaoyu Du, Xiangnan He, Fajie Yuan, Qi Tian, and Tat-Seng Chua. Adversarial training towards robust multimedia recommender system. IEEE Transactions on Knowledge and Data Engineering, 32(5):855867, 2020. doi: 10.1109/TKDE.2019.2893638. Mengting Wan and Julian J. McAuley. Item recommendation on monotonic behavior chains. In Sole Pera, Michael D. Ekstrand, Xavier Amatriain, and John ODonovan, editors, Proceedings of the 12th ACM Conference on Recommender Systems, RecSys 2018, Vancouver, BC, Canada, October 2-7, 2018, pages 8694. ACM, 2018. doi: 10.1145/3240323. 3240369. https://doi.org/10.1145/3240323.3240369. Tianxin Wei, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui Sun, Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, and Xianfeng Tang. Towards unified multi-modal personalization: Large vision-language models for generative recommendation and beyond, 2024a. https://arxiv.org/abs/2403.10667. Wei Wei, Chao Huang, Lianghao Xia, and Chuxu Zhang. Multi-modal self-supervised learning for recommendation. In Proceedings of the ACM Web Conference 2023, pages 790800, 2023. Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. Llmrec: Large language models with graph augmentation for recommendation, 2024b. https://arxiv.org/abs/2311. 00423. Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. Mmgcn: Multi-modal graph convolution network for personalized recommendation of micro-video. In Proceedings of the 27th ACM International Conference on Multimedia, pages 14371445, 2019. Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, and Ming Zhou. MIND: large-scale dataset for news recommendation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 35973606, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.331. Wentao Xu, Qianqian Xie, Shuo Yang, Jiangxia Cao, and Shuchao Pang. Enhancing content-based recommendation via large language model. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM 24, page 41534157, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704369. doi: 10.1145/3627673.3679913. https://doi.org/10.1145/3627673.3679913. An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. Personalized showcases: Generating multi-modal explanations for recommendations. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 23, page 22512255, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394086. doi: 10.1145/3539618.3592036. https://doi.org/10. 1145/3539618.3592036. Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, and Yanbin Lu. Palr: Personalization aware llms for recommendation, 2023. https://arxiv.org/abs/2305.07622. Yuhao Yang, Chao Huang, Lianghao Xia, Yuxuan Liang, Yanwei Yu, and Chenliang Li. Multi-behavior hypergraphenhanced transformer for sequential recommendation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 22, page 22632274, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393850. doi: 10.1145/3534678.3539342. https://doi.org/10.1145/3534678. 3539342. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, 14 Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint 2408.01800, 2024a. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024b. Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. Recommendation as instruction following: large language model empowered recommendation approach, 2023. https://arxiv.org/abs/2305.07001. Bowen Zheng, Zihan Lin, Enze Liu, Chen Yang, Enyang Bai, Cheng Ling, Wayne Xin Zhao, and Ji-Rong Wen. large language model enhanced sequential recommender for joint video and comment recommendation, 2024. https://arxiv.org/abs/2403.13574. Peilun Zhou, Xiaoxiao Xu, Lantao Hu, Han Li, and Peng Jiang. model-based multi-agent personalized short-video recommender system, 2024. https://arxiv.org/abs/2405.01847. Wang Zhu, Jesse Thomason, and Robin Jia. Generalization differences between end-to-end and neuro-symbolic In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings vision-language reasoning systems. of the Association for Computational Linguistics: EMNLP 2022, pages 46974711, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.345. https://aclanthology.org/2022.findings-emnlp.345. Wang Zhu, Alekh Agarwal, Mandar Joshi, Robin Jia, Jesse Thomason, and Kristina Toutanova. Efficient end-to-end visual document understanding with rationale distillation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 84018424, Mexico City, Mexico, June 2024a. Association for Computational Linguistics. https://aclanthology.org/2024.naacl-long.465. Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language model for recommender systems. In Proceedings of the ACM Web Conference 2024, WWW 24, page 31623172. ACM, May 2024b. doi: 10.1145/3589334.3645347. http://dx.doi.org/10.1145/3589334.3645347."
        },
        {
            "title": "A Benchmark Creation Details",
            "content": "A.1 Algorithm for candidate generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Processing and filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Training data distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B More Related Works",
            "content": "B.1 Traditional recommendation methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Datasets and evaluating multimodal recommendation. . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Benchmark Creation Details",
            "content": "A.1 Algorithm for candidate generation 16 16 16 17 17 18 18 18 We list the complete candidate and ground truth sets generation algorithm in Algorithm 1, where the function nearest(G, loc, m) returns the nearest businesses of around certain location loc based on the graph G. The function unique_name(S) removes the businesses in the set with redundant names. In both Google Review-V and Yelp-V, rcmin = 30, rcmax = 100, cmin = 10. Figure 5 The Google Review-Vision (Google Review-V) training data consists of 66 categories. A.2 Processing and filtering Category removal and merge. For Google Review-V and Yelp-V, the categories are selected as the last word of the annotated tags in the business. However, some category requires multiple words to express its meaning, such as tourist attraction, steak house, historical landmark\", nature preserve, etc. We select and keep these multi-word categories. In Google Review-V, we remove smaller categories with less than 10k occurrence in the dataset. Then, we remove ambiguous or non-differentiable categories, including shop, store, complex, service, company, supplier, caterer, agency, center, organization, attraction, house, mall, landmark, wash, course, preserve, alley, groomer, field, peak, venue, delivery, dealer, lounge, office, arcade, court, spot, stop, maintenance, trainer, wholesaler, planner, place, facility, school, 16 Algorithm 1 Candidate and Ground Truth Sets Generation Input: Business geographics graph = (V, E), Visit set of user, Category c, Minimum random candidate count rcmin, Maximum random candidate count rcmax, Minimum final candidate count cmin , Minimum ground truth count gtcmin . Output: Candidate sets CD 1..n # Initialization for each business do flag fb unselected , Ground truth sets GT 1..n . end for count 0 # Main algorithm for each business do if is in category and fb = unselected then count count + 1 Candidate count random(rcmin, rcmax) Candidate set CD Candidate set CD Ground truth set GT if CD count nearest(G, bloc, m) count unique_name(S CD count count < gtcmin then count CD count) count < cmin or GT count count 1 continue end if for each business GT flag fb selected count do end for end if end for return (S CD 1..count, GT 1..count) stand, range, consultant, designer, veterinarian, ground, contractor, manufacturer, studio, point, lot. In Yelp-V, as the category is very centralized, we remove smaller categories with less than 50k occurrence in the dataset. Then, we remove ambiguous or non-differentiable categories, includingplanning, nightlife, services, wings, arts, dogs, tacos, caribbean, beer, spirits, wine, venues, fusion, entertainment, southern, spaces, lounges, breweries, shopping, smoothies, flavor, plates, eastern, tex-mex, shop, noodles, markets, market, donuts, gelato, sum, veggies, fruits, trucks, bagels, cheesesteaks, clubs, cuban, ramen, life, roasteries, stands, brewpubs, gluten-free, gardens, travel. A.3 Training data distribution We list the training data distribution of Google Review-V in Figure 5."
        },
        {
            "title": "B More Related Works",
            "content": "B.1 Traditional recommendation methods. Before the LLM era, there are also line of work for recommendation with other networks, smaller language models, or ensembling methods. For example, Sun et al. (2019) uses bidirectional Transformer to model the item sequence. Tang and Wang (2018) uses CNN to model user preference with convolutional filters. Gu et al. (2020) and Yang et al. (2022) uses ensembling methods for recommendation. 17 B.2 Datasets and evaluating multimodal recommendation. The development of multimodal recommendation systems has been facilitated by the availability of diverse datasets that incorporate various types of data, including text, images, and user interaction histories. Notable datasets in this domain include MovieLens-1M (Harper and Konstan, 19), which is extensively used for movie recommendations, and the Amazon dataset (Ni et al., 2019), which includes user reviews and metadata for product recommendations. For news and books recommendations, the MIND (Wu et al., 2020) and Goodreads (Wan and McAuley, 2018) datasets offer insights into user preferences in news and literature. Recently, LaMP (Salemi et al., 2023) introduces 7 text classification and text generation tasks across long contexts to evaluate LLMs personalization capablity. In the context of outdoor activities, datasets such as Google Local Data (Li et al., 2022; Yan et al., 2023) and Yelp (Asghar, 2016) are crucial. These datasets not only provide textual reviews but also include user ratings and geospatial data, which are essential for recommending local businesses and services. These benchmarks utilize the traditional recommendation systems that focus on ranking user preferences based on historical IDs. Instead, we introduce new visual history benchmarks Google Review-V and Yelp-V, using data from Google Local and Yelp, to evaluate the multimodal recommendation systems under more realistic visual history for task-agnostic setups."
        },
        {
            "title": "C Prompt Template",
            "content": "We list the prompt template for aspect word generation in Figure 6, and the prompt template for candidate matching in Figure 7. The model will fill in category information in the [[Category]] slot and predict the answer at the [[Answer]] slot. Figure 6 The prompt template for aspect word generation."
        },
        {
            "title": "D Additional Implementation Details",
            "content": "We present the hyperparameters for training VisualLens in PaliGemma and MiniCPM-V2.5 in Table 5 and Table 6. Note that since there are at most 100 candidates, we add special tokens <I1>, ..., <I100> as identifier of the candidates in the vocab of the models. By doing so, we can predict the rank by taking the probability of only those special tokens in the first output token. 18 Figure 7 The prompt template for candidate matching. Hyperparameters for training on PaliGemma Parameter Size Image Resolution Number of Image Tokens Hidden Dimension Size LoRA Rank LoRA α LoRA dropout GPU Batch Size Gradient Accumulation Steps Warmup Steps Learning Rate 3B 896 896 4096 2048 16 16 0.1 8 NVIDIA H100 8 8 200 1eTable 5 Hyperparameters for training VisualLens with PaliGemma Backbone. Hyperparameters for training on MiniCPM-V2.5 Parameter Size Image Resolution Number of Image Tokens Hidden Dimension Size LoRA Rank LoRA α LoRA dropout GPU Batch Size Gradient Accumulation Steps Warmup Steps Learning Rate 8B 980 980 96 4096 64 64 0.1 8 NVIDIA H100 8 8 200 1e-3 Table 6 Hyperparameters for training VisualLens with MiniCPM-V2.5 Backbone."
        }
    ],
    "affiliations": [
        "Meta",
        "University of Southern California"
    ]
}