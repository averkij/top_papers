{
    "paper_title": "SynCity: Training-Free Generation of 3D Worlds",
    "authors": [
        "Paul Engstler",
        "Aleksandar Shtedritski",
        "Iro Laina",
        "Christian Rupprecht",
        "Andrea Vedaldi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We address the challenge of generating 3D worlds from textual descriptions. We propose SynCity, a training- and optimization-free approach, which leverages the geometric precision of pre-trained 3D generative models and the artistic versatility of 2D image generators to create large, high-quality 3D spaces. While most 3D generative models are object-centric and cannot generate large-scale worlds, we show how 3D and 2D generators can be combined to generate ever-expanding scenes. Through a tile-based approach, we allow fine-grained control over the layout and the appearance of scenes. The world is generated tile-by-tile, and each new tile is generated within its world-context and then fused with the scene. SynCity generates compelling and immersive scenes that are rich in detail and diversity."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 0 2 4 6 1 . 3 0 5 2 : r SynCity: Training-Free Generation of 3D Worlds Paul Engstler Aleksandar Shtedritski"
        },
        {
            "title": "Christian Rupprecht",
            "content": "Andrea Vedaldi Visual Geometry Group, University of Oxford {paule,suny,iro,chrisr,vedaldi}@robots.ox.ac.uk Figure 1. We introduce SynCity, novel method that can generate from prompt complex and immersive 3D worlds that can be navigated freely. Our method is training-free and leverages powerful language, 2D and 3D generators via novel prompt engineering strategies."
        },
        {
            "title": "Abstract",
            "content": "We address the challenge of generating 3D worlds from textual descriptions. We propose SynCity, trainingand optimization-free approach, which leverages the geometric precision of pre-trained 3D generative models and the artistic versatility of 2D image generators to create large, highquality 3D spaces. While most 3D generative models are object-centric and cannot generate large-scale worlds, we show how 3D and 2D generators can be combined to generate ever-expanding scenes. Through tile-based approach, we allow fine-grained control over the layout and the appearance of scenes. The world is generated tile-by-tile, and *Equal contribution each new tile is generated within its world-context and then fused with the scene. SynCity generates compelling and immersive scenes that are rich in detail and diversity. 1. Introduction We consider the problem of generating 3D worlds from textual descriptions. Generating 3D content, for example, for video games, virtual reality, special effects and simulation, is highly laborious and time-consuming. When it comes to generating entire 3D scenes, much of the content is not of particular artistic value, and its manual creation, which is still necessary, may be seen as waste of human resources, talent and creativity. Generative models can help to reduce or even remove this burden by largely automating many of these mundane tasks. The advent of modern generative AI has significantly impacted 3D content generation and promises to reduce the cost of its production dramatically. DreamFusion [39] was among the first to co-opt state-of-the-art diffusion-based 2D image generators [43] to create 3D objects. The area has since matured significantly by fine-tuning 2D image generators to produce multiple consistent views of an object [14, 32, 47, 52] and by learning few-view 3D reconstruction networks [24, 63]. More recently, the focus has shifted to methods that learn 3D latent space [10, 25, 26, 61, 66], which can then be sampled to generate 3D objects. Because the latent space directly encodes the 3D structure of the object rather than its 2D appearance, these methods can generate much more accurate and regular geometry. Despite their advantages, 3D generative methods have been so far mostly limited to generating individual objects. However, the most promising usage of 3D generative AI is the construction of entire virtual worlds, as this is where automation can make by far the most difference. There is ample literature on generating 3D scenes from textual or image prompts. Most such methods are image-based, and progressively reconstruct larger scene regions by expanding from an initial image [8, 12, 13, 17, 30, 37, 42, 64, 67], combining depth prediction, image and depth outpainting, and 3D reconstruction using NeRF [35] or 3D Gaussian Splatting [19]. The main advantage of these approaches is that they can leverage powerful 2D image generator models to create the first and subsequent views of the scene. These 2D generators allow the overall system to understand complex textual prompts and generate corresponding 3D scenes with good artistic quality. However, it is difficult for these approaches to maintain coherent 3D structure across large scene. For example, while the reconstructed scene may envelop the observer in 360 manner, it is generally not possible to walk into the scene for more than few steps. This is the case even for state-of-the-art implementations like the one of World Labs [21], company specialized in SpatialAI. challenge with extending scenes beyond these 3D bubbles is that it is difficult for image-based methods to maintain consistency incrementally without drifting. We argue that 3D generative models might be preferable as they can regularize and constrain the reconstructed geometry, including hallucinating shape and textures in regions behind the visible sides of objects. This is clearly shown in prior works like BlockDiffusion [59] and LT3SD [33], where large coherent spaces can be generated. However, these methods are limited in the quality and diversity of the generated scenes as it is difficult to train 3D generative models directly for scene generation. In particular, unlike their view-based peers, these methods do not build on an image generator. They thus cannot benefit from the artistic quality and ability to interpret complex textual prompts that come from pre-training 2D generators on billions of images. In this work, we thus seek to build on 3D generative models while still building on the strengths of 2D image generators to generate large, high-quality 3D spaces that can be navigated freely  (Fig. 1)  . First, we note that while 3D models like TRELLIS [61] are trained for object-level reconstruction, they can reconstruct fairly complex local compositions of multiple objects. Borrowing ideas from video game world building, we show in particular that TRELLIS can effectively generate, if not an entire world, at least tile representing local region of the world. We show, in particular, how to prompt the model with an isometric view of the tile and then generate the tile in 3D. Given this basic capability, we then look at the problem of generating large scene by generating and stitching together multiple tiles. We build on text-to-image generator (Flux [20]) and propose novel way of prompting that stabilizes it to consistently produce tiles with similar isometric framing. In this manner, we encourage the tiles framing to be stable and compatible between different tiles, making it easier to stitch them together. To ensure tiles to fit together appropriately, we propose two mechanisms. First, we encourage consistency in appearance by using previously generated tiles to draw context for the image generator, where each new tile inpaints missing region in 2D isometric view of the scene. Secondly, we enforce geometric consistency by blending the 3D representations of neighbouring tiles using the 3D generative model. 2. Related Work Novel view synthesis for scenes. Expanding an image beyond its boundaries has been long-standing task in computer vision. Early methods that sought to expand object-centric scenes rely on layer-structured representations [23, 34, 48, 50, 54, 55], which disregard the scenes true geometry. SynSin [58] is pivotal work, where image features are projected and used as conditioning to generate novel views, achieving geometric and semantic consistency. ZeroNVS [44] introduces high-quality results with fine-grained control of the camera but remains objectcentric. GenWarp [45] integrates semantic information through cross-attention when generating novel view. The major challenges for these methods remain semantic drift and object permanence. To obtain an explicit 3D representation, the generated views need to be transferred into such representation, e.g., NeRF [35] or Gaussians [18, 19], where any geometric conflicts would need to be resolved. Figure 2. Overview of SynCity. 2D prompting: To generate new tile, we first render view of where that tile should be placed, including context from neighbouring tiles. 3D prompting: We extract the new tile image and construct an image prompt for TRELLIS by adding wider base under the tile. 3D blending: The 3D model that TRELLIS outputs is usually not well blended with the rest of the scene. To address that, we render view of the new tile next to each neighbouring tile, and inpaint the region between the two with an image inpainting model. Next, we condition using that well-blended view to refine the region between the two 3D tiles. Finally, the new, blended, tile is added to the world. Image projection-based scene generation. different line of work follows the paradigm of building the 3D representation of scene sequentially using 2D image generation models [8, 13, 17, 22, 37, 42, 57, 60, 64, 67]. Most of them employ an image generation model to outpaint the existing scene using pre-defined camera poses. The results are then fused in 3D with depth prediction models. Text2Room [17] generates meshes of indoor scenes. As the scene is clearly delimited by the bounds of the mesh, it can be freely explored. LucidDreamer [8] and Text2Immersion [37] go beyond indoor scenes, but their generated scenes reveal geometric inconsistencies when stepping away from the camera poses used to generate the scene. Invisible Stitch [12] addresses this issue by inpainting depth (rather than naively aligning it) and RealmDreamer [49] proposes multiple optimization losses to refine the generated scene. Despite these improvements, the resulting scenes still suffer from geometric artifacts and remain rather small. WonderJourney [64] introduces novel ideas for depth fusion, such as grouping objects at similar disparity to planes and sky depth refinement, enabling large scene journeys, where independent representations are built between scene keyframes, but these are not merged into one coherent scene. WonderWorld [65] leverages these improvements to build single scene, allowing interactive updates, but the true extent of the generated scenes remains limited. Other works use panoramas [51, 56] or implicit representations [2, 44] but the freedom of movement remains constricted. Procedural scene generation. Further methods permit long-range fly-overs over nature [57, 27, 30] or cities [29, 46, 62]. These usually generate procedural unbounded images (e.g., the terrain make-up or city layout). While those methods create realistic-looking images, they are often monotonous as the methods are domain-specific and thus highly constrained in the variety they can generate. 3D scene generation. Instead of merely generating images of scene or outpainting it only in 2D, further methods generate the representation directly. Set-the-Scene [9] adds layer of control to the layout of NeRF scenes by defining object proxies. BlockFusion [59] learns network to autoregressively diffuse small blocks to extend mesh. 2D layout conditioning is used to control the generation process, allowing users to generate scenes of rooms, village, and city. While the method allows building large-scale scenes, the variety of the objects it generates is severely limited as it requires domain-specific 3D training data. Furthermore, it generates untextured meshes. LT3SD [33] learns diffusion model that generates 3D environments in patchby-patch and coarse-to-fine fashion. However, this method is only trained to produce indoor scenes. At the same time, the synthesis of complex, high-fidelity objects has been enabled by the rapid progress in the fields of text-to-3D and image-to-3D generation [25, 26, 28, 31, 40, 41, 47, 53, 61, 68, 70]. Trained on large-scale curated subsets of 3D datasets such as Objaverse-XL [11], these models can generate large variety of different objects. However, to the best of our knowledge, no prior work has leveraged object generators for scene generation. 3. Method Our goal is to generate 3D world from an initial textual prompt p0. Our main result is to show how prompt engineering can be used in combination with off-the-shelf language, 2D and 3D generators to create the entire world automatically, with no need to retrain the models. We structure the world as grid = {0, . . . , 1} {0, . . . , 1} of square tiles, each of which can contain several complex 3D objects (e.g., building, bridge, trees, etc.) as well as the ground surface. We generate the world progressively, tile by tile, as shown in in Fig. 3. Hence, when tile (x, y) is generated, tiles (x, y) = {(x, y) : < (y = < x)} have already been generated. Figure 3. Left: Progressive generation of world tiles . Right: Isometric framing of tile for image-based prompting. An overview of our approach is shown in Fig. 2. The first step of our method is to expand the world description p0 into tile-specific prompts (Sec. 3.1). The second step is to pass these tile-specific prompts to 2D image generator and inpainter to create an isometric view of each tile, accounting for the part of the world generated so far (Sec. 3.2). The third step is to extract image prompts from these isometric views and use them as input to image-to-3D generator to reconstruct each tiles geometry and appearance in 3D (Sec. 3.3). The final step is to align and blend the 3D reconstructions of the tiles to create coherent 3D world (Sec. 3.4). 3.1. Prompting the Language Model The goal of language prompting is to take high-level textual description of the world p0 and expand it into set of tile-specific textual prompts that can be used to generate the 3D world. Specifically, is collection of sub-prompts pxy Σ, one for each tile, and world-level style prompt Σ, so that we can write = {pxy}(x,y)T {p}, where Σ is the set of all possible strings. The prompt could be constructed manually (which allows controlling the content of each tile) or generated by large language model (LLM) such as ChatGPT [36] from seed prompt. For the latter, we prompt ChatGPT o3mini-high to generate grid-like world with tile-specific descriptions, providing it with an example JSON file (see Appendix A.1 for details). 3.2. Prompting the 2D Generator We use the language prompts from Sec. 3.1 to prompt an off-the-shelf 2D image generator Φ2D to output 2D image I(x, y) of each tile to be generated, as shown for example in Fig. 4. The image I(x, y) must satisfy several constraints: (1) It must reflect the tile-specific instructions pxy of the target tile as well as the world-level instructions p. (2) It must be suitable for prompting the image-to-3D generator in the next step. (3) It must be consistent with the previously generated tiles. Our prompting strategy is designed to satisfy these constraints. The image is drawn as sample I(x, y) Φ2D(q, B, ) from the 2D image generator Φ2D, where = pxy is prompt that combines the tile-specific and world-level descriptions. The generator Φ2D is also provided with base image and an inpainting mask that constrain the output. We assume that Φ2D is capable of inpainting common feature of modern image generators. Tile inpainting. To satisfy constraint (2), we need to encourage the image generator to generate regular tiles so that the image-to-3D model can output tiles with regular geometry that fit well together. We assume that tiles have fixed square basis of unit size and that they are imaged in an isometric manner. This framing of the tiles is conducive to the generation of regular 3D tiles. Furthermore, it is common choice in video games and might have been observed by the image generator during training as these are often trained on game-like data. Hence, our goal is to condition the image generator Φ2D to produce images of this kind. While one possible approach is to fine-tune the generator on such images, we demonstrate that it is possible to obtain this effect through prompt engineering alone, avoiding any retraining. We achieve this by carefully constructing the inputs and as shown in Fig. 4. Specifically, we set to be the image of the base of the tile, as square, grey slab imaged from fixed isometric vantage point. The mask is binary mask covering cube on top of the base. Figure 4 shows the result of prompting the model in this manner as well as what happens if signals and are removed: the viewpoint and general frame of the tile is random and not suitable for 3D generation."
        },
        {
            "title": "The appendix discusses a special case for tiles at the",
            "content": "boundaries of the world (see Appendix A.2). 3.3. Prompting the 3D Generator Given the tile image I(x, y) obtained from the 2D image generator in Sec. 3.2, the next goal is to generate corresponding 3D reconstruction G(x, y) of the tile utilizing an image-to-3D model Φ3D. We opt for using robust 3D generator and select TRELLIS [61] due to its good performance, ability to generate both shape and texture, and latent space structure, which will be easy to manipulate for blending as we show later in Sec. 3.4. Hence, 3D reconstruction amounts to drawing sample G(x, y) Φ3D(J(x, y)) from the image-to-3D generator Φ3D. Rather than conditioning on the image I(x, y), we use pre-processed version J(x, y), as described next. 2D foreground extraction and rebasing. Recall that the image I(x, y) output by the 2D generator of Sec. 3.2 is an image of the tile and its context. However, the 3D generator Φ3D expects the input image to only show the object that needs to be reconstructed, i.e., the new tile. The first step is thus extracting only that part from I(x, y) that corresponds to the new tile, which we do by applying the inpainting mask and then running rembg [15] with alpha matting [4] to remove the background, as shown in Fig. 7. Figure 4. Left: Generation of the 2D image prompt for the first world tile at = 0 and = 0. The image generator Φ2D is conditioned on = p00 and tasked with inpainting the base image in the masked region . Right: If we do not frame the image by using and , the generator produces an image which is not suitable for tiling. Taking the context into account. Except for the first tile (0, 0), the tiles are generated in the context of the world already generated. In order to account for this context, for tiles with x, > 0, we modify the base image and the mask as shown in Fig. 5. For the base image B, we render the part of the 3D world generated so far, providing context for the inpainting network. We also modify the mask to avoid covering tiles already generated to the left (west; i.e., for tile (i, j) these are tiles {(x, y) : < = j}). Figure 5. Left: Base image and inpainting mask (white overlay) to prompt the image generator Φ2D to generate an image for > 0, > 0 world tile. Right: Result of inpainting. Because we wish to ensure continuity of the ground, before rendering this contextual image, we trim any 3D geometry that is sufficiently high to occlude the tile we wish to generate, as shown in Fig. 6 (see the result in Fig. 5). Figure 6. Trimming tall structures for 2D prompting. Figure 7. Left: Isolating the image of the new tile from I(x, y). Right: Placing slightly larger base underneath. The resulting image is narrowly cropped around the new tile. Similar to Sec. 3.2, we found it beneficial to hallucinate base for the tile, an operation that we call rebasing, as shown in Fig. 7. We simply compose the image of the tile with slightly larger gray slab (in 2D) to obtain J(x, y), which in effect provides frame for the 3D generator to work with. The base is reconstructed as part of the tiles geometry, which can be used for validation and as simpleto-detect handle for further 3D processing. The rebased image J(x, y) is fed to the 3D generator Φ3D to obtain the 3D reconstruction G(x, y) of the tile, which are 3D Gaussian Splats (3DGS). The effect of rebasing on the 3D result is shown in Fig. 8. 3D geometric validation. Because the generators are imperfect, we verify the 3D reconstruction G(x, y) to ensure Figure 8. Top: 3D reconstruction using tight base. Bottom: the same, but with slightly larger base, which helps to contain the tiles geometry above ground (see the back of the reconstruction), and creates an easy-to-detect 3D base. that it is of sufficient quality. If not, we discard it and generate the tile again using different random seed. To verify the tile, we use few heuristics that check that the tiles geometry occupies square region of sufficient size and that the base of the tile has been reconstructed faithfully. Please see Appendix A.3. 3D post-processing. At this point, we have verified the 3D reconstruction G(x, y) for the tile as mixture of 3D Gaussians. However, the actual 3D footprint, orientation, and size of the tile are controlled by the 3D generator and are inconsistent. The post-processing step applies simple heuristics to refine the 3D Gaussian representation by first cropping out the added base, then rescaling the tile to unit size, and finally reorienting it to match the 2D image prompt. We explain this in more detail in Appendix A.4. 3.4. 3D Blending At this point of the pipeline, we have reconstructed all 3D tiles G(x, y), (x, y) . As result of the prompting and post-processing steps in Secs. 3.2 and 3.3, the tiles are already approximately aligned and correctly oriented, with their ground level at roughly the same height in 3D space. Because the 3D reconstructions are 3D Gaussian Splats, it is easy to simply take their union as the reconstruction of the whole world. Even so, the boundaries of the tiles may not match perfectly. This is largely due to the fact that TRELLIS does not reconstruct the input images exactly and to the fact that only single view of each tile is provided to it, which only indirectly controls the reconstruction of the back of the tile. In this section, we thus propose method to improve the blending of tiles, ensuring that the 3D world is coherent and continuous. In particular, we regenerate the boundary region of two adjacent tiles in the latent space of TRELLIS, in essence Figure 9. Upsampling sparse latents. We need to resize or upsample sparse latents in order to stitch them. Due to the sparsity of the latents and the behaviour of the latent decoder, naively resampling in latent space leads to artifacts. Our proposed resizing of the sparse latents better preserves textures and fine structures. blending it, and then decode it into 3DGS. Next, we discuss the specifics of this process. Blending in 2D. To blend the latents of two neighbouring tiles, we first predict the appearance of the boundary between the two tiles. To that end, we place the two 3D tiles next to each other, render frontal view, and inpaint the middle region of the rendering  (Fig. 2)  with Φ2D. This leads to well-blended image, which we use to condition for Φ3D. Blending in 3D. Next, we use Φ3D to blend the latents. We take the latents of the two tiles γ1 and γ2 where γ1, γ2 RDRRR are D-dimensional features in the R-sized 3D grid that TRELLIS denoises. We put them together in new volume γ, where the side where they meet is in the middle: γ:,x,y,z = (cid:40) γ1 :,x+R/2,y,z, γ2 :,xR/2,y,z, if < R/2 if R/2. We apply the denoising function Ω, which is the latent denoiser of Φ3D, to the volume γ, but only within the middle region where we have applied the stitch, i.e., for [R/2 r, R/2 + r] for some < R/2, while keeping the rest fixed. Formally, we initialise γ (0, I) and at each denoising step t, we update γ as: γt+1,:,x,y,z = (cid:40) Ω(γt,:,x,y,z), γt+1,:,x,y,z, if R/2 otherwise, where γt is obtained by adding noise to the original γ at the corresponding noise level for step t. In practice, we only denoise the second stage of TRELLIS, keeping the occupancy latents of the first stage fixed. The reason for that is that the first stage is at very low spatial resolution (R = 16, compared to = 64 of the second stage), which gives little flexibility for the size of the denoised region r. Upsampling the latents. Remember that due to the rebasing, G(x, y) contains 3D base. While we have removed the base in 3DGS space, we have yet to do the same in the latent space. We use the same cuts we applied in 3DGS space to now crop the latents, rounding the cuts to account Win Rate (%) Method Base Area Squareness Completeness Overall Geometry Exploration Diversity Realism 90.9 81.8 90.9 90.9 86.4 No Rebasing Ours 2271 4096 0.92 1.00 0.73 1.00 Table 1. Win rates of our method against BlockFusion. We asked participants to select which scene they prefer overall, as well as which one has better geometry, would be more interesting to explore, is more diverse, and has better realism. Figure 10. Left: 22 grid generated with our method, where context is taken into account as described in Sec. 3.2 Right: Generated with our method using the same prompts, but not taking into account context here, the scale of the buildings is not consistent. for the discrete nature of the latent voxel grid. In the previous step, however, we assumed that the latents γ1 and γ2 have the same spatial resolution, γ RDRRR. After cropping, this is not the case any more if the cuts of neighbouring tiles differ. Thus, similarly to how we resize the tiles 3DGS to unit size, we have to upsample the nowcropped latents back to the original grid resolution R. We found that naively upsampling the latents by interpolation leads to poor reconstructions, as shown in Fig. 9. We attribute this to the sparse structure of the latents and quirks of the latent decoder of TRELLIS. We propose the following upsampling scheme. First, we upsample the cropped occupancy volume that TRELLIS predicted to the original resolution {0, 1}RRR. Next, we denoise new set of latents γ on the upsampled occupancy volume. To preserve the details and textures of the original 3D tile, we render it from multiple views and jointly condition the denoising on all of them. In practice, when denoising with multiple conditioning views, at each timestep, the denoising step is computed as the average denoising step across all views. We show that this upsampling scheme leads to superior reconstructions in Fig. 9. 4. Experiments Experimental details We generate the text prompts using ChatGPT o3-mini-high. For the 2D inpainter, we use the Flux ControlNet of [1]. Human preference. We evaluate human preference for the results generated by our method and those obtained with BlockFusion [59]. In particular, we compare city scene, showing the entire scene as well as close-up detail views. As seen in Tab. 1, participants (n = 22) find our method Table 2. Average tile 3D geometry metrics for an approach without rebasing and our method. Rebasing is crucial to ensure tile is square and its base has been reconstructed faithfully. The metrics we use are the area of the base in voxels, measure for the squareness of the base, and how many border voxels have been faithfully reconstructed. For details, please refer to the appendix. better overall, with better geometry, realism, and diversity. 4.1. Ablations Here we ablate several components of our approach, showing the importance of each of them. Building grid. naive approach to generating 3D scene is querying the image generator to produce an image of large-scale scene (using our 2D image prompt setup) and then obtaining the entire 3D world directly with TRELLIS. To achieve the same level of control provided by our method, the textual prompt needs to be highly detailed and include layout instructions. However, we found neither precise nor abstract prompts to be effective at steering the generations of Flux (for details, see A.4). 2D prompting context. We remove context from neighbouring tiles, as described in Sec. 3.2. Doing that, each tile is sampled independently, and the relative scale between objects is inconsistent, as shown in Fig. 10. Rebasing. To place tiles on grid, they need to be square (otherwise the grid would be jagged) and their base needs to have been reconstructed faithfully (clearly delimiting where the tile stops). Without rebasing, the geometry generated by TRELLIS might extend beyond the base and makes the tiles true extent difficult to detect, as shown in Fig. 8. We ablate the effect of rebasing, using small 2 2 scene to curb the effect of error accumulation. As seen in Tab. 2, no rebasing causes TRELLIS to generate tiles that are, on average, neither perfectly square nor have solid border. Method LPIPS SSIM FID KID Naive upsampling Ours (single frame) Ours (multi frame) 0.5914 0.3517 0. 0.3093 0.5149 0.5312 200.5 111.6 89.1 0.243 0.069 0.051 Table 3. Perceptual metrics for our methods and the naive approach. Lower values for LPIPS [69], FID [16], and KID [3] are better, while higher values for SSIM are better. We see that even using single conditioning frame leads to better upsampling results, and multiple frames further improve performance. Figure 11. Exploring 3D world. We show trajectories exploring the 3D worlds we generate. Please see the supplement for videos. mentary material for many more examples. Exploring generated world. We can sample trajectories exploring the generated 3D worlds  (Fig. 11)  . skybox has been added for visual effect. Unlike the trajectories generated by world video models [38], ours are guaranteed to be consistent and do not suffer from semantic drift. Different from other systems that only generate bubble, our method creates spaces sufficiently large to be navigated in non-trivial manner. 5. Conclusion We have introduced SynCity, an approach to generate diverse, high-quality, and complex 3D worlds with finegrained control over their layout and appearance. SynCity creates worlds by autoregressively generating tiles on grid, enabling scalability to arbitrary grid sizes. By accounting for local context and by means of 3D inpainting, the tiles are seamlessly stitched together into coherent scenes. SynCity is flexible: it can either generate worlds from brief world text prompt or allow control of the individual tiles via tilespecific instructions, all the while maintaining an overall thematic consistency of the generated world. The rich detail of the generated worlds can be fully explored, not restricted to single 3D bubble as in many prior works. We have demonstrated the effectiveness of off-the-shelf generation by utilizing pre-trained language, 2D, and 3D generators through carefully designed prompting strategies and without requiring retraining of any of these components. Nevertheless, we expect that in cases where 3D scene scale data is available, fine-tuning some components would result in further improved results and simplifications in the Figure 12. Left: Tiles before applying the 3D blending step Right: After the 3D blending step. We see that where boundaries between tiles were obvious, they are now well-blended. Latent upsampling. We sample 10 random views each from 200 tiles generated by TRELLIS and compute perceptual metrics in Tab. 3 when upsampling with our proposed upsampling approach in Sec. 3.4 and naive interpolation. We see that the proposed method leads to better results across range of metrics, even when using single conditioning frame. 3D blending. In Fig. 12, we generate scene where we do not apply the 3D blending (Sec. 3.4), resulting in discontinuities between the tiles. 4.2. Qualitative Results We present example scenes generated by our method in Fig. 1. Further, we show detail views, highlighting the quality and diversity of the scene. Please see the supplealignment and rebasing steps of the pipeline. Future work could also consider relaxing the tile structure, for example, by randomly shifting and scaling tiles and using coarse-tofine modeling to ensure coherent global structure and finegrained local details. Ethics. For details on ethics, data protection, and copyright, please see https://www.robots.ox.ac.uk/ vedaldi/research/union/ethics.html. Acknowledgments. The authors of this work are supported by ERC 101001212-UNION, AIMS EP/S024050/1, and Meta Research."
        },
        {
            "title": "References",
            "content": "[1] AlimamaCreative. Flux-controlnet-inpainting. https: / / github . com / alimama - creative / FLUX - Controlnet-Inpainting, 2024. GitHub repository. 7 [2] Miguel Ángel Bautista et al. GAUDI: neural architect for immersive 3D scene generation. arXiv.cs, abs/2207.13751, 2022. 3 [3] Mikołaj Binkowski, Danica Sutherland, Michael Arbel and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. 7 [4] Ron Brinkmann. The art and science of digital compositing: Techniques for visual effects, animation and motion graphics. Morgan Kaufmann, 2008. 5 [5] Shengqu Cai, Eric Ryan Chan, Songyou Peng, Mohamad Shahbazi, Anton Obukhov, Luc Van Gool and Gordon Wetzstein. DiffDreamer: Towards consistent unsupervised single-view scene extrapolation with conditional diffusion models. In Proc. ICCV, 2023. 3 [6] Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola and Noah Snavely. Persistent nature: generative model of unbounded 3d worlds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2086320874, 2023. [7] Zhaoxi Chen, Guangcong Wang and Ziwei Liu. Scenedreamer: Unbounded 3d scene generation from 2d image collections. IEEE transactions on pattern analysis and machine intelligence, 45(12):1556215576, 2023. 3 [8] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee and Kyoung Mu Lee. Luciddreamer: Domain-free arXiv.cs, generation of 3d gaussian splatting scenes. abs/2311.13384, 2023. 2, 3 [9] Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes and Daniel Cohen-Or. Set-the-scene: Global-local training for generating controllable nerf scenes. In Proc. ICCV Workshops, 2023. 3 [10] Deemos. Rodin text-to-3D gen-1 (0525) v0.5, 2024. 2 [11] Matt Deitke et al. Objaverse-XL: universe of 10M+ 3D objects. CoRR, abs/2307.05663, 2023. 4 [12] Paul Engstler, Andrea Vedaldi, Iro Laina and Christian Rupprecht. Invisible stitch: Generating smooth 3D scenes with depth inpainting. In Proceedings of the International Conference on 3D Vision (3DV), 2025. 2, [13] Rafail Fridman, Amit Abecasis, Yoni Kasten and Tali Dekel. Scenescape: Text-driven consistent scene generation. CoRR, abs/2302.01133, 2023. 2, 3 [14] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan T. Barron and Ben Poole. CAT3D: create anything in 3d with multi-view diffusion models. arXiv, 2405.10314, 2024. 2 [15] Daniel Gatis. rembg, 2025. 5 [16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. In Proc. NeurIPS, 2017. 7 [17] Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson and Matthias Nießner. Text2Room: Extracting textured 3D meshes from 2D text-to-image models. In Proc. ICCV, 2023. 2, 3 [18] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. 2 [19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler and George Drettakis. 3D Gaussian Splatting for real-time radiance field rendering. Proc. SIGGRAPH, 42(4), 2023. 2 [20] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2023. 2 [21] World Labs. Generating worlds, 2024. 2 [22] Jiabao Lei, Jiapeng Tang and Kui Jia. RGBD2: generative scene synthesis via incremental view inpainting using RGBD diffusion models. In Proc. CVPR, 2023. 3 [23] Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang and Gim Hee Lee. MINE: towards continuous depth In Proc. ICCV, MPI with NeRF for novel view synthesis. 2021. 2 [24] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich and Sai Bi. text-to-3D with sparse-view generation and large reconstruction model. Proc. ICLR, 2024. 2 Instant3D: Fast [25] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan and Xiaoxiao Long. CraftsMan: high-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv, 2405.14979, 2024. 2, [26] Yangguang Li et al. synthesis using large-scale rectified flow models. 2502.06608, 2025. 2, 4 TripoSG: high-fidelity 3D shape arXiv, [27] Zhengqi Li, Qianqian Wang, Noah Snavely and Angjoo Infinitenature-zero: Learning perpetual view Kanazawa. In Eugeneration of natural scenes from single images. ropean Conference on Computer Vision, pages 515534. Springer, 2022. 3 [28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu and Tsung-Yi Lin. Magic3D: High-resolution text-to-3D content creation. arXiv.cs, abs/2211.10440, 2022. 4 [29] Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-Hsuan Yang and Sergey In ProTulyakov. Infinicity: Infinite-scale city synthesis. ceedings of the IEEE/CVF international conference on computer vision, pages 2280822818, 2023. [30] Liu, Tucker, Jampani, Makadia and Snavely. . . . Infinite nature: Perpetual view generation of natural scenes from single image. In Proc. ICCV, 2021. 2, 3 [31] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3D object. In Proc. ICCV, 2023. 4 [32] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni and Filippos Kokkinos. IM-3D: Iterative multiview diffusion and reconstruction for high-quality 3D generation. In Proceedings of the International Conference on Machine Learning (ICML), 2024. 2 [33] Quan Meng, Lei Li, Matthias Nießner and Angela Dai. arXiv, trees for 3D scene diffusion. LT3SD: latent 2409.08215, 2024. 2, 3 [34] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng and Abhishek Kar. Local light field fusion: practical view synthesis with prescriptive sampling guidelines. Proc. SIGGRAPH, 38(4), 2019. [35] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proc. ECCV, 2020. 2 [36] OpenAI et al. GPT-4 technical report. arXiv, 2303.08774, 2024. 4 [37] Hao Ouyang, Kathryn Heal, Stephen Lombardi and Tiancheng Sun. Text2Immersion: Generative immersive scene with 3D gaussians. arXiv.cs, abs/2312.09242, 2023. 2, 3 [38] Jack Parker-Holder et al. Genie 2: large-scale foundation world model, 2024. [39] Ben Poole, Ajay Jain, Jonathan T. Barron and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. In Proc. ICLR, 2023. 2 [40] Guocheng Qian et al. Magic123: One image to high-quality 3D object generation using both 2D and 3D diffusion priors. arXiv.cs, abs/2306.17843, 2023. 4 [41] Amit Raj et al. DreamBooth3D: subject-driven text-to-3D generation. In Proc. ICCV, 2023. 4 [42] Chris Rockwell, David F. Fouhey and Justin Johnson. PixelSynth: Generating 3D-consistent experience from single image. In Proc. ICCV, 2021. 2, 3 [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser and Björn Ommer. High-resolution image synIn Proc. CVPR, 2022. thesis with latent diffusion models. [44] Kyle Sargent et al. ZeroNVS: Zero-shot 360-degree view synthesis from single real image. arXiv.cs, abs/2310.17994, 2023. 2, 3 [45] Junyoung Seo, Kazumi Fukuda, Takashi Shibuya, Takuya Narihira, Naoki Murata, Shoukang Hu, Chieh-Hsin Lai, Seungryong Kim and Yuki Mitsufuji. GenWarp: single image to novel views with semantic-preserving generative warping. arXiv, 2405.17251, 2024. 2 [46] Yu Shang, Yuming Lin, Yu Zheng, Hangyu Fan, Jingtao Ding, Jie Feng, Jiansheng Chen, Li Tian and Yong Li. Urbanworld: An urban world model for 3d city generation. arXiv preprint arXiv:2407.11965, 2024. 3 [47] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li and Xiao Yang. MVDream: Multi-view diffusion for 3D generation. In Proc. ICLR, 2024. 2, 4 [48] Meng-Li Shih, Shih-Yang Su, Johannes Kopf and Jia-Bin Huang. 3d photography using context-aware layered depth inpainting. In Proc. CVPR, 2020. [49] Jaidev Shriram, Alex Trevithick, Lingjie Liu and Ravi Ramamoorthi. RealmDreamer: text-driven 3d scene generation with inpainting and depth diffusion. In Proc. 3DV, 2025. 3 [50] Pratul Srinivasan, Richard Tucker, Jonathan Barron, Ravi Ramamoorthi, Ren Ng and Noah Snavely. Pushing the boundaries of view extrapolation with multiplane images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 175184, 2019. 2 [51] Gabriela Ben Melech Stan et al. LDM3D: Latent diffusion model for 3D. arXiv.cs, (2305.10853), 2023. 3 [52] Stanislaw Szymanowicz, Christian Rupprecht and Andrea Vedaldi. Viewset diffusion: (0-)image-conditioned 3D generative models from 2D data. In Proceedings of the International Conference on Computer Vision (ICCV), 2023. 2 [53] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng and Ziwei Liu. LGM: Large multi-view Gaussian model for high-resolution 3D content creation. arXiv, 2402.05054, 2024. 4 [54] Richard Tucker and Noah Snavely. Single-view view synthesis with multiplane images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 551560, 2020. 2 [55] Shubham Tulsiani, Richard Tucker and Noah Snavely. Layer-structured 3d scene inference via view synthesis. In Proc. ECCV, 2018. [56] Guangcong Wang, Peng Wang, Zhaoxi Chen, Wenping Wang, Chen Change Loy and Ziwei Liu. PERF: panoramic neural radiance field from single panorama. tpami, 2024. 3 [57] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. 3 [58] Olivia Wiles, Georgia Gkioxari, Richard Szeliski and Justin Johnson. Synsin: End-to-end view synthesis from single image. In Proc. CVPR, 2020. 2 [59] Zhennan Wu et al. BlockFusion: Expandable 3D scene generation using latent tri-plane extrapolation. arXiv.cs, 2024. 2, 3, 7 [60] Jianfeng Xiang, Jiaolong Yang, Binbin Huang and Xin Tong. 3D-aware image generation using 2D diffusion models. arXiv.cs, abs/2303.17905, 2023. 3 [61] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong and Jiaolong Yang. Structured 3D latents for scalable and versatile 3d generation. arXiv, 2412.01506, 2024. 2, 4, 5 [62] Haozhe Xie, Zhaoxi Chen, Fangzhou Hong and Ziwei Liu. Citydreamer: Compositional generative model of unbounded In Proceedings of the IEEE/CVF conference on 3d cities. computer vision and pattern recognition, pages 96669675, 2024. [63] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen and Gordon Wetzstein. GRM: Large gaussian reconstruction model for efficient 3D reconstruction and generation. arXiv, 2403.14621, 2024. 2 [64] Hong-Xing Yu et al. Wonderjourney: Going from anywhere to everywhere. arXiv.cs, abs/2312.03884, 2023. 2, 3 [65] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Interactive 3d arXiv preprint Freeman and Jiajun Wu. Wonderworld: scene generation from single image. arXiv:2406.09394, 2024. 3 [66] Biao Zhang, Jiapeng Tang, Matthias Niessner and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. In ACM Transactions on Graphics, 2023. 2 [67] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang and Jing Liao. Text2NeRF: Text-driven 3D scene generation with neural radiance fields. arXiv.cs, abs/2305.11588, 2023. 2, 3 [68] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. [69] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proc. CVPR, pages 586 595, 2018. 7, 13 [70] Zibo Zhao et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 4 SynCity: Training-Free Generation of 3D Worlds"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Appendix A.1. Language Model Prompting Details While the prompt can be constructed manually, an LLM may also be employed. For it to understand the task it is asked to solve, we utilize the following prompt for ChatGPT o3-mini-high: Assume you had access to an AI model that can generate small-scale cities on an isometric grid by creating individual tiles. For each of these tiles (identified by their 2D position), short but expressive text prompt has to be provided. Additionally, global prompt is used, which provides context, lighting, time of day, as well as the art style. The prompts of the tiles can be generic but they might have semantic connection to neighbouring tiles (such that river can flow through the city on multiple tiles). The format for the instructions to the AI model is JSON. Consider the following example: <TEMPLATE> The art style and perspective mentioned in the prompt should be maintained. The rest may be freely adapted. There are no limits to the setting, the sky is your limit. Now, please generate 3 3 grid. In this prompt, template or seed prompt is provided. We use simple JSON file, as exemplified in Fig. 13. {\"tiles\": [ { \"prompt\": \"ancient stone bridge over stream \", \"x\": 0, \"y\": 0 }, { \"prompt\": \"lively stream past mossy banks\", \" x\": 1, \"y\": 0 }, { \"prompt\": \"serene pond reflecting moonlight\", \"x\": 0, \"y\": 1 }, { \"prompt\": \"bustling medieval market street\", \"x\": 1, \"y\": 1 } ], \"prompt\": \"{tile_prompt}, medieval setting, isometric view, glowing lanterns, soft shading, vibrant colors, detailed textures\" } Figure 13. Example JSON file to describe each tile in 2 2 world. A.2. 2D Prompting Details In Sec. 3.2, we describe how tiles are generated in the context of those that already exist. There is special case that we address separately, where context has to be bootstrapped, namely tiles := {(x, y) : = 0 > 0}. Due to our build order and the trimming of obstructing 3D geometry, these tiles might lack sufficient contextual cues. As remedy, we temporarily provide context with previously generated tile: For tile (0, y) L, we duplicate the tile (0, 1) and place the copy at position (1, y). During inpainting, this tile serves to provide context in terms of scale and general appearance. Once inpainting is completed, this copy is removed. Figure 14. Tile geometry validation. To check the geometric qualities of reconstructed tile, we look at the occupancy grid {0, 1}RRR generated by TRELLIS. Activated voxels are indicated in orange (). Left: The extent of an object in an object at height (slice visualized in 2D). Right: An example of 3D tile base template VB. A.3. 3D Geometry Validation Details TRELLIS is two-stage method and produces an occupancy volume {0, 1}RRR in the first stage (before the 3D Gaussian mixture is output), where = 64 is the resolution of the grid. To perform geometric validation, we utilize this occupancy volume, which captures the rough 3D geometry of the tile, and check that it conforms to the desired geometry. First, we test whether the reconstructed tile is supported by square by computing its 2D rectangular footprint and ensuring that the latter is sufficiently large and isotropic. To this end, let (u, v, w) index the voxel grid, where and corresponds to world directions and y. Let (umin, umax, vmin, vmax, ) be the bounding box containing all the active voxels at height w.* Let extu = max{0, 1 + umax umin} be the width of the bounding box and extv its height. We discard the tile if the area is too small, i.e., if extu extv < (R/2)2. We also discard it if it is not square, i.e., if min{extu, extv}/ max{extu, extv} < α = 1. Second, we check that the base that we have added in 2D in the rebasing step has been faithfully reconstructed in 3D. *So for instance umin = min{u : v, : (u, v, w) = 1}. We define 3D tile base template, which we call VB. Let umin(w) be the minimum of the bounding box that contains the volume slice at height w, and define umax(w) and so on in similar manner, so for instance umin(w) = min{u : : (u, v, w) = 1}. Let be the height at which the base is the largest, i.e., = argmaxw extu(w) extv(w). Then, VB is the indicator function of the voxels (u, v, w) such that = and max (cid:26) 2u umax(w) umin(w) umax(w) umin(w) , 2v vmax(w) vmin(w) vmax(w) vmin(w) (cid:27) = 1. Note that the template VB is constructed adaptively to match the input tile . We discard generated tile if (V VB)/(VB VB) < β = 0.95, where denotes the inner product of tensors. A.4. 3DGS Post-Processing Details 3D cropping, resizing and centering. Given the 3D Gaussian mixture G(x, y) initially output by the 3D generator, we first identify the extent of the tile proper (discounting the extended base). We consider the xy footprint of the tile (i.e., we look at the tile from above) and seek to identify four cuts (from the left, right, top, and bottom) that define an axis-aligned rectangle strictly containing the tile. For example, to determine the location of the left cut x, we consider slices Vx = {(x, y, z) R3 : δ < + δ}. We find the 3D Gaussians whose centers falls within Vx and compute their average color cx. Then, we compute the distance d(x) = cx cxmin where cxmin is the average color of the leftmost slice (used as reference). We set = min{x : d(x) > τ } where τ is threshold, which corresponds to the slice that transition from the background color to something else. We find in this manner the four cuts, keep only the Gaussians contained in the resulting rectangular footprint, and recenter and resize this footprint to fill the standard tile size. Additionally, the base allows us to figure out the position of the tiles surface: As TRELLIS centers objects vertically, the ground surface level of any two tiles will vary. We use the average height of the tiles four corners to determine the position of the surface, allowing us to align it with others. 3D reorientation. We also note that TRELLIS generates the 3D object with an arbitrary orientation with respect to the input image I(x, y). However, the tile must be inserted with the correct orientation in the 3D world otherwise the continuity between tiles, which the inpainting method of In practice, the ambiguFig. 4 encourages, will be lost. ity is limited to 90-degree rotations around the vertical axis and is very easy to remove.* To do so, we test four possible 90-degree rotations of the tile around the vertical axis, rendering the corresponding views and comparing them to *This is likely due to the implicit bias in the TRELLIS training set that consists of synthetic 3D objects which are almost invariably axis aligned. I(x, y) using the LPIPS [69] loss. The minimizer is taken as the correct orientation. A.5. Ablation Details Building Grid. In the following, we present results from our experiments attempting to generate large scene noniteratively. Here, we generate single image with Flux that is used as conditioning for TRELLIS to directly create the desired scene. Figure 15. Non-iterative city building. We obtain conditioning images generated by Flux (left) and directly use them to build large-scale scene with TRELLIS (center). While the generated 3D structures are visually appealing, the level of detail (right) is very limited. The first row used generic prompting for the conditioning image (a city scene on top of base), whereas the second row uses more involved prompt with an explicit layout (e.g., house in the bottom left corner, pharmacy in the top right corner). In the first set of experiments, we do not use our 2D prompting design. To obtain an isolated 3D object that can be generated by TRELLIS, we use prompts with the prefix 3d object of. We show those results in Fig. 15. While the generated objects are visually appealing, they have several limitations: (i) The resolution of the conditioning image and the 3D structures TRELLIS can generate is limited. Therefore, this approach is not scalable to arbitrarily large scenes. (ii) Due to the lack of perfect control over the base structure, the result cannot be easily extended or edited. (iii) The layout instructions are mostly ignored, thus severely limiting the level of control over the generation. For the second set of experiments, we use our 2D prompting design along with the Flux ControlNet for inpainting  (Fig. 16)  . However, with this setup, the quality of the results is not improved. The layout instructions in the prompt are mostly ignored, again. Querying Flux to generate large-scale scenes directly has not been successful in our experiments, prompting the need for our grid-based method that allows fine-grained layout and appearance control for each tile. Figure 16. Non-iterative city building (with our 2D prompting). We obtain conditioning images generated by Flux (left) and directly use them to build large-scale scene with TRELLIS (center). Despite the initial visual appeal, the structures lack in detail. The first row used generic prompting for the conditioning image (a vibrant city scene), whereas the second row uses more involved prompt with an explicit layout (e.g., house in the bottom left corner, pharmacy in the top right corner). A.6. Additional Qualitative Results In Figs. 17 to 20, we show additional results of our method. As we leverage pre-trained 2D image generator trained on very large dataset, we are able to generate highly diverse scenes. Thanks to our fine-grained control at the tile level, we can generate interesting patterns, such as transition between seasons across grid (observe the largest grid in Fig. 20). A.7. Limitations While our method allows creating large and diverse scenes, there are some limitations to be addressed in future work. Atomic tiles. Although we inpaint tiles conditioned on their surroundings, they remain individual units. While structures can be created that span across multiple tiles, this requires harmonious cooperation between Flux and TRELLIS. Use of heuristics. To determine the ground surface height for each tile and removing the base we added during rebasing, we employ heuristics. We have designed these carefully with fallback mechanisms, but they are not infallible. Inherited limitations. As our method builds on top of Flux and TRELLIS, their limitations also apply to ours. During our experiments, we have observedthat despite good inpainting resultsTRELLIS at times only vaguely adheres to the conditioning image in terms of appearance, in particular color. Thus, transitions between tiles might not look perfectly smooth (even if they were generated that way in the inpainting result). Figure 17. Exploring 3D world. We show trajectories exploring the 3D worlds we generate. sky box has been added for visual effect. Figure 18. Generated scenes. We show scenes generated with the same prompts, but different seeds in 2D inpainting. Figure 19. Generated scenes. Figure 20. Generated scenes. Our method can easily generate large scenes. Further, interesting patterns can be injected thanks to finegrained control over each tile. Top: The scene transitions in season, from winter to spring to summer to autumn. Bottom: The scenery transitions from city-like to rural environment."
        }
    ],
    "affiliations": [
        "Visual Geometry Group, University of Oxford"
    ]
}