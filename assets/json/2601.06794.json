{
    "paper_title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
    "authors": [
        "Zhicong Li",
        "Lingjie Jiang",
        "Yulan Hu",
        "Xingchen Zeng",
        "Yixia Li",
        "Xiangwen Zhang",
        "Guanhua Chen",
        "Zheng Pan",
        "Xin Li",
        "Yong Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 1 ] . [ 1 4 9 7 6 0 . 1 0 6 2 : r No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning Zhicong Li1,2*, Lingjie Jiang3*, Yulan Hu2, Xingchen Zeng4, Yixia Li5, Xiangwen Zhang2, Guanhua Chen5, Zheng Pan 2, Xin Li2, Yong Liu1 1Gaoling School of Artificial Intelligence, Renmin University of China, 2Amap, Alibaba Group, 3Peking University, 4The Hong Kong University of Science and Technology (Guangzhou), 5Southern University of Science and Technology {zhicongli, liuyonggsai}@ruc.edu.cn, lingjiejiang@stu.pku.edu.cn {huyulan, zhangxiangwen.zxw, panzheng.pan, beilai.bl}@alibaba-inc.com xzeng159@connect.hkust-gz.edu.cn, liyixia@me.com, ghchen08@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Critique-guided reinforcement learning (RL) has emerged as powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agents error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for HindsightGuided Optimization), framework that jointly optimizes the policy and critic through synchronized co-evolutionary loop. ECHO utilizes cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critics feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (Sutton et al., 1998) has emerged as promising paradigm for training Large Language Model (LLM)-based agents (Anthropic, 2024; Team et al., 2025), enabling them to navigate complex tasks through environmental interactions. Within this paradigm, reward signals (Wen et al., 2025) serve as the fundamental *Equal contribution. Corresponding author. 1 compass for policy optimization. However, these signals often lack actionability, as they merely reflect the final outcome without providing the diagnostic insights necessary for effective refinement, ultimately leading to significant data inefficiency (Gao et al., 2025; Yang et al., 2025b). To bridge this gap, recent research has introduced linguistic critics to provide diagnostic feedback (Dhuliawala et al., 2024). common line of work uses template-based critiques (Wang et al., 2025; Liu et al., 2025; Huang et al., 2025), which are computationally inexpensive but lack the adaptability to deliver feedback tailored to the agents specific actions. To provide more targeted guidance, another line of work employs independently fine-tuned, separate critic models to refine policy outputs (McAleese et al., 2024). These models are typically designed to act as external supervisors, aiming to provide the diagnostic feedback necessary to resolve complex failures. Although these methods overcome the limitations of static templates by offering more detailed feedback, they remain decoupled from the policys learning process, implicitly assuming that the optimal critique strategy is stationary. In on-policy RL, however, the policy continuously evolves, inducing shifting trajectory distribution and corresponding drift in failure patterns: early-stage rollouts may be dominated by coarse mistakes that benefit from high-level hints, whereas later-stage policies are more often bottlenecked by subtle, hardto-localize defects. Consequently, critic trained (and then frozen) on an earlier distribution can become stale, producing feedback that is redundant, miscalibrated in granularity, or even misleading for the current policy, and causing its marginal utility to decay as training progresses. This critic staleness fundamentally limits sample efficiency and prevents critique-guided RL from sustaining improvement in long-horizon refinement. Motivated by this observation, we posit that the critic should be treated as co-evolving module rather than stationary supervisor, adapting alongside the policy (Figure 1). Concretely, we propose ECHO (Evolving Critic for Hindsight-Guided Optimization), framework that fosters symbiotic optimization loop between the policy and the critic. Instead of rewarding the critic for sounding plausible, we directly optimize it for policy improvement: critiques are evaluated by the performance gains they induce after refinement, and the critic is updated in lockstep with the policy to track its changing failure modes. To make this co-evolution stable and sample-efficient, ECHO employs cascaded diagnostic-and-corrective rollout that generates group-structured trajectories for relative advantage estimation, and introduces saturation-aware gain shaping to provide informative learning signals even when improvements become incremental. Our main contributions are: (1) We identify and empirically demonstrate critic staleness in critiqueguided RL, freezing the critic leads to clear decay in critique utility as the policy improves. (2) We introduce ECHO, synchronized co-evolutionary optimization paradigm that jointly aligns the critic and the policy via dual-track GRPO. (3) We propose saturation-aware reward design and group-relative optimization scheme that jointly improve training stability and boost performance across tasks."
        },
        {
            "title": "2 Related Work",
            "content": "In long-horizon decision-making for LLM-based agents, scalar outcome rewards are often nondiagnostic, motivating language-based critiques as actionable supervision (Gao et al., 2025; Yang et al., 2025b). Prior work typically implements language critics either as static, template/offline-generated feedback, or as separately trained critic models. Template-based Critics. lightweight line of work injects pre-defined hints as critique signals, avoiding training separate critic model. HINT (Wang et al., 2025) steers ineffective rollouts toward effectiveness by appending generic, hand-crafted hints to trigger regeneration. Tang et al. (2025) further adopts small set of errorconditioned prompt templates, routing different failure cases to different pre-defined guidance patterns. Moving beyond generic guidance, LUFFY (Yan et al., 2025) mitigates inefficient exploration by injecting teacher models correct answer as the rollout outcome. To better control the granularity of the guidance, more structured hints have also been explored. GHPO (Liu et al., 2025) and ADHint (Zhang et al., 2025a) provides stronger supervision by injecting masked partial reference solutions as hints, effectively revealing part of the answer to stabilize and accelerate learning. StepHint (Zhang et al., 2025b) uses teacher model to generate full chain-of-thought, splits it into reasoning steps, and forms hints by concatenating different numbers of prefix steps. In contrast, Scaf-GRPO (Zhang et al., 2025c) designs critic templates that progress from abstract to concrete guidance, providing coarse-to-fine guidance conditioned on the models current performance. Training-based Critics. Another line of work trains dedicated critic models to generate more informative, diagnostic feedback. Early attempts (Saunders et al., 2022; Ke et al., 2024; Xi et al., 2024; Tang et al.) primarily rely on singlestage fine-tuning, typically by curating critique datasets and training models to generate naturallanguage feedback for evaluation and verification. Yu et al. (2025) propose Refinement-oriented Critique Optimization (RCO), which trains critic in critiquerefinement loop by rewarding critiques according to the utility of the actors refined outputs. Multi-stage training has also been investigated to stabilize learning across different training objectives. CGI (Yang et al., 2025b) leverages critiqueguided iterative improvement for agents through staged updates, typically treating the critic as fixed supervisor. CTRL (Xie et al., 2025) introduces two-stage training pipeline that first distills critiques via SFT and then applies GRPO to optimize critique generation directly for downstream refinement success. Despite these advances, most training-based critics are trained off-policy and then frozen or updated asynchronously, remaining decoupled from on-policy policy learning. As the policys trajectory distribution and failure patterns shift over time, the critic becomes stale, and its ability to provide useful critiques gradually decays. 2 Figure 1: Comparison of critic paradigms. (a) Conventional Static Paradigms: Use decoupled, frozen critic modules initialized from off-the-shelf templates or fine-tuned separate models, resulting in static evaluation and inflexible feedback. (b) Our ECHO Paradigm: Policy and critic co-evolve organically. The policy first generates an initial rollout τo, refined to τr using the critics diagnostic guidance c. Both models are jointly updated, ensuring the critics diagnostic precision synchronizes with the policys evolving failure patterns."
        },
        {
            "title": "3 Methodology",
            "content": "To address critic staleness caused by decoupled training under on-policy failure-pattern drift, we propose ECHO, co-evolutionary interplay between Policy Pθ and Critic Cψ, rather than static supervision task. Within this paradigm, we treat the refinement process as dynamic synchronization problem where two models co-evolve in shared on-policy trajectory space: Pθ (The Actor) learns to convert diagnostic feedback into corrective actions. Rather than relying on unguided exploration, it conditions on the critics current diagnoses to produce refinements that directly improve task reward. Cψ (The Diagnostic Evolver) is rewarded for feedback that maximizes the policys performance gain, thereby learning to pinpoint the flaws that causes the policys failure. This joint evolution ensures that the critics diagnostic depth is continuously calibrated to the policys shifting failure patterns. By optimizing both models through dual-track GRPO mechanism, we transform the refinement process into selfimproving system where evaluative precision and execution capability evolve in tandem. Figure 2 summarizes the overall training loop and illustrates concrete refinement example."
        },
        {
            "title": "3.1 Cascaded Evolutionary Rollout",
            "content": "To facilitate the symbiotic optimization of both models, ECHO employs cascaded rollout mechanism that generates group-structured trajectories through diagnostic-and-corrective cycle. Stage 1: Multi-view Diagnosis. Given query q, the policy Pθ first generates an initial trajectory τo Pθ( q). To provide an objective basis for diagnosis, an external reward model evaluates the proposal to obtain baseline score so = R(q, τo). Conditioned on both the trajectory and its corresponding score, the critic Cψ is invoked times independently to produce set of diverse diagnostic feedbacks GC = {c(j) j=1: }N c(j) Cψ( q, τo, so), = 1, 2, . . . , N. (1) By incorporating so into the prompt, the critic is empowered to provide \"score-aware\" explanations, identifying the specific gaps that prevent the trajectory from achieving higher reward. Stage 2: Conditional Refinement. Following the diagnosis, the policy Pθ is required to internalize these critiques into precise corrective actions. Conditioned on the augmented input q(j) = (q, c(j) ), the policy samples corresponding set of refined trajectories: Pθ( q(j)), τ (j) = 1, 2, . . . , N. (2) The reward model evaluates each refinement to yield the post-correction scores s(j) = R(q, τ (j) ). This cascaded rollout produces the baseline score so, the critique group GC, and the refinement group GP = {τ (j) j=1, which serve as the empirical signals for the co-evolutionary optimization. }N 3 Figure 2: Overview of ECHO training with saturation-aware (SA) critic rewards. At step t, the policy πθt produces rollouts τo, which are scored by reward model to obtain so. critic πψt generates critiques that are appended to the original query to elicit refined rollouts τr, scored as sr. We compute the SA critic reward rc to emphasize last-mile improvements near saturation, and update the critic and policy synchronously to obtain πψt+1 and πθt+1."
        },
        {
            "title": "3.2 Saturation-Aware Reward Design",
            "content": "A straightforward approach to quantifying the utility of critique is to measure the linear improvement in reward, i.e., = sr so. However, this linear metric fails to account for the saturation effect in model optimization: as the initial score so approaches the performance ceiling (e.g., 1), the marginal effort and information required to achieve further increment surge. Treating an improvement from 0.9 to 0.95 as equivalent to one from 0.1 to 0.15 creates an \"equidistant fallacy,\" which discourages the critic from diagnosing subtle yet critical flaws in high-quality proposals and leads to optimization plateaus. To address this, we hypothesize that the reward space is non-linear and governed by difficulty weighting function ω(s). We define ω(s) as soft barrier function that captures the increasing difficulty of entropy reduction as perfection is approached: ω(s) = 1 1 + η , (3) where η > 0 is smoothing hyperparameter. We define the intrinsic gain of refinement as the path integral of ω(s) from so to sr: (cid:90) sr g(so, sr) = ω(s)ds = ln so (cid:18) 1 so + η 1 sr + η (cid:19) . (4) This choice yields principled shaping signal (Ng et al., 1999) with three desirable properties. First, it is saturation-aware: for the same s, the gain is larger when the improvement happens in higher-score region, encouraging the critic to focus on subtle yet impactful flaws in near-correct proposals. Second, it is additive (path-consistent): g(so, sm) + g(sm, sr) = g(so, sr), (5) invariant to which makes the training signal whether refinement is performed in one step or through multiple intermediate edits. Third, the gain is antisymmetric, g(so, sr) = g(sr, so), providing unified measure that rewards improvements and penalizes regressions under the same scale. Finally, we use this intrinsic gain directly as the critic reward: rc = g(so, sr) = ln (cid:18) 1 so + η 1 sr + η (cid:19) . (6)"
        },
        {
            "title": "Optimization",
            "content": "Instead of treating the critic as static oracle, we operationalize the co-evolution as synchronized dual-track alignment problem. We formulate closed-loop optimization where both Pθ and Cψ explore shared trajectory space, mutually anchoring each others learning progress. This is achieved by constructing two interdependent group structures: , τ (2) GP (q) = {τ (1) , c(2) GC(q, τo) = {c(1) , . . . , τ (N ) }, , . . . , c(N ) }. (7) (8) Here, GC represents the diagnostic hypothesis space containing distinct interpretations of the 4 proposals flaws, while GP represents the corrective action space conditioned on those hypotheses. Dual-Track Advantage Estimation. To maximize sample efficiency, we compute group-relative advantages that capture the marginal utility of each models output. For the policy Pθ, the advantage A(j) is computed by normalizing the scores s(j) within GP . This allows the policy to efficiently identify the most effective refinement paths from diverse diagnostic samples (Wang et al., 2022b; Cobbe et al., 2021). For the critic Cψ, the advantage A(j) is derived by performing group-relative normalization on the saturation-aware rewards r(j) defined in Section 3.2. By amplifying high-score gains and balancing penalties via λ, the mechanism enables the critic model to rapidly converge on effective feedback. Synchronized Update. Following the GroupRelative Policy Optimization (GRPO) objective (Shao et al., 2024), both Pθ and Cψ are updated by maximizing surrogate objective that incorporates advantage-weighted likelihood and KL divergence constraint: hold settings; for scientific tasks, SciWorld (Wang et al., 2022a) provides simulator for complex experimental reasoning and hypothesis verification; and for deep search, we adopt the RAG-based DeepSearch environment from Xi et al. (2025), which requires multi-turn information synthesis for open-domain question answering. More details are shown in Appendix A. Baselines and backbone models. We utilize Qwen3-4B-Instruct-2507 (Yang et al., 2025a) (denoted as Qwen3-4B in the following) and Qwen2.57B (Team et al., 2024) as primary backbone models. By default, the critic Cψ uses the same backbone as the policy Pθ. To ensure rigorous and comprehensive evaluation, we compare our method against diverse set of strong baselines spanning both proprietary and open-source large language models. Specifically, for proprietary models, we include GPT series (Achiam et al., 2023), Gemini2.5-pro (Comanici et al., 2025), and Claude-Sonnet4.5. In addition, we consider Open-sourced Models as competitive baselines, including Qwen3235B-A22B (Yang et al., 2025a) and DeepSeekR1-0528 (Guo et al., 2025). . The implementation detail is described in Appendix B. (ϕ) =E qD,{oi}N i=1 Mϕold (cid:34) 1 1 oi oi (cid:88) t=1 (cid:16) min ρi,t(ϕ)Ai, (cid:88) i=1 (cid:35) (cid:17) clip(ρi,t(ϕ), 1 ϵ, 1 + ϵ)Ai βDKL(MϕMref),"
        },
        {
            "title": "5 Results",
            "content": "Mϕold (9) where ϕ {θ, ψ} represents the parameters of the policy or critic, and oi denotes the generated sequence. The importance sampling ratio is defined as ρi,t(ϕ) = Mϕ(oi,tctx,oi,<t) (oi,tctx,oi,<t) , where ctx is the corresponding input context for each model. Ai {AP , AC} is the respective group-relative advantage. This synchronized optimization ensures the critics diagnostic focus is continuously calibrated to the policys evolving failure patterns, fostering self-reinforcing curriculum for continuous improvement. For completeness, the full pseudocode of ECHO is provided in Appendix C."
        },
        {
            "title": "4 Experiment Setup",
            "content": "Scenarios and tasks. To evaluate ECHO across broad spectrum of cognitive challenges, we conduct experiments in four diverse environments. Specifically, for web navigation, we use WebShop (Yao et al., 2022), requiring agents to navigate e-commerce platforms and make purchasing decisions; for embodied tasks, ALFWorld (Shridhar et al., 2020) challenges agents with longhorizon planning and object manipulation in houseTable 1 presents the main results. We organize our analysis around three research questions: RQ1 evaluates the overall effectiveness of ECHO on open-world agent benchmarks; RQ2 investigates whether failure patterns drift during on-policy learning and whether this drift causes frozen critic to become stale; and RQ3 studies why the proposed saturation-aware reward is beneficial, especially for last-mile improvements near the reward ceiling."
        },
        {
            "title": "5.1 RQ1: How effective is ECHO for\nopen-world agent learning?",
            "content": "ECHO consistently outperforms standard GRPO and other strong baselines. As shown in Table 1, ECHO consistently surpasses GRPO under the same training budget, supporting our hypothesis that synchronized, on-policy critiques reduce unproductive exploration and thus improve data efficiency. The most salient gains appear on Qwen3-4B in long-horizon search and web interaction: on DeepSearch, ECHO improves from 33.25 to 47.25, roughly 42% relative increase; on WebShop, it rises from 82.37 to 90.03, about 9% relative increase. These boosts indicate that ECHO is 5 Table 1: Main results on four open-world agent benchmarks. Bold indicates the best result within each benchmark. Models GPT-4o-mini GPT-4o GPT-4-turbo GPT-4.1 GPT-5 Gemini-2.5-pro Claude-Sonnet-4.5 Qwen3-235B-A22B DeepSeek-R1Qwen3-4B Qwen3-4B + GRPO Qwen3-4B + ECHO Qwen2.5-7B Qwen2.5-7B + GRPO Qwen2.5-7B + ECHO WebShop SciWorld 56.59 58.20 52.45 58.07 46.12 65.58 58.80 ALFWorld Proprietary Models 40.68 45.20 45.78 44.45 34.14 42.64 35.65 43.56 13.06 35.09 12.50 68.04 56.83 64.73 Open-sourced Models 100B 23.50 26.60 25.26 44.81 4.50 72.50 Open-sourced Models < 100B & RL 0.32 6.12 87.50 82.37 91.25 90.03 2.00 13.98 89.50 83.55 93.75 89.97 4.50 79.14 82.88 1.50 81.24 85. DeepSearch Overall 31.43 26.19 61.90 61.46 72.19 36.50 65.00 28.25 40.25 20.25 33.25 47.25 15.50 42.25 46.75 43.48 43.66 47.78 49.67 41.62 45.66 61.34 25.90 40. 7.80 70.57 77.85 8.25 74.14 79.03 especially effective when success depends on diagnosing and repairing specific failure causes across multiple steps. Importantly, in more complex embodied and scientific environments where failures are more diverse and harder to localize, ECHO also brings consistent gains on Qwen3-4B, improving ALFWorld from 87.50 to 91.25 and SciWorld from 79.14 to 82.88. Overall, ECHO improves performance across all four benchmarks, achieving an average gain of 7.28 points over GRPO, and it delivers highly competitive results against much stronger baselines: except for DeepSearch where GPT-5 attains the best score, ECHO matches or surpasses all listed strong models by clear margin on the other benchmarks. ECHO generalizes across backbone sizes. To test whether ECHO is applicable across different backbone sizes, we also evaluate it on Qwen2.57B. The results show that ECHO is not restricted to specific capacity regime. Instead, it consistently improves over GRPO on both backbones and yields strong performance across environments. This demonstrates that the benefit of synchronized critic-policy co-evolution transfers across model scales, highlighting the versatility and generalizability of ECHO for open-world agent learning."
        },
        {
            "title": "5.2 RQ2: Does fail-pattern drift happen",
            "content": "during on-policy learning?"
        },
        {
            "title": "5.2.1 How Failures Change Over Training",
            "content": "To further examine whether failure patterns drift under on-policy training, we analyze the training trajectory of Qwen3-4B and partition it into three phases: early, intermediate, and late. In each phase, we select three adjacent policy checkpoints, and for every checkpoint we run rollouts on the same heldout test set. We collect all unsuccessful trajectories produced in each phase and treat them as samples from the phase-specific failure distribution. For each unsuccessful trajectory, we use Gemini-2.5pro to produce concise diagnosis describing the underlying error cause. We then embed these diagnoses using Qwen3-8B-Embedding and visualize the resulting representations with t-SNE (Maaten and Hinton, 2008). Phase-wise drift of dominant failure modes. Figure 3 shows clear distributional drift across all four environments. In WebShop and DeepSearch, failures in each phase form relatively compact clusters, and the high-density centers shift substantially from early to late. This indicates that training changes which error causes dominate, rather than simply shrinking fixed set of mistakes. 6 Figure 3: Failure-pattern drift across training phases. We visualize failed trajectories from early, intermediate, and late checkpoints in diagnosis embedding space using t-SNE, with contours indicating density regions. Higher diversity and partial persistence in complex environments. In the more complex environments ALFWorld and SciWorld, the failure distributions are more dispersed and partially overlap across phases, reflecting higher failure-mode diversity and the persistence of some recurring errors. Even in these settings, the density mass still migrates across training phases, confirming that the dominant failure patterns remain non-stationary."
        },
        {
            "title": "5.2.2 Limitations of Frozen Critics under",
            "content": "Failure-Pattern Drift Table 2: Ablation results of ECHO on Qwen3-4B. w/o denotes removing the specified component. Methods WebShop ALFWorld SciWorld DeepSearch GRPO ECHO ECHO w/o evolving ECHO w/o SA-aware 82.37 90.03 83.60 86. 87.50 91.25 85.75 79.14 82.88 68.58 78.55 33.25 47.25 40.25 To further validate the need for critic adaptation under failure-pattern drift, we freeze the critic and rerun the experiments with all other components of ECHO unchanged. Results are presented in Table 2 and illustrated by the training curves in Figure 5. Final performance drops with frozen critic. We find that this simple change leads to performance degradation across all environments, indicating that keeping critiques synchronized with the evolving policy is important for maintaining their effectiveness. Meanwhile, the degradation is most severe on ALFWorld and SciWorld, and even underperform standard GRPO. We conjecture that in these more complex environments, stale critic more frequently produces redundant or off-target diagnoses, which the policy may over-condition on during refinement, turning critiques into noise and amplifying long-horizon errors. Training dynamics reveal phase-dependent effects. Figure 5 further shows that the benefit of co-evolution depends on both training phase and environment. On WebShop, the frozen-critic variant can look strong early on, but its improvement slows later and is overtaken by ECHO, consistent with later-stage errors becoming more fine-grained such that stale critiques are increasingly miscalibrated and act as noise that reduces sampling efficiency. In ALFWorld and SciWorld, ECHO stays close to GRPO at the beginning and separates mainly in the mid-to-late stage, suggesting short calibration period in which the critic learns to produce environment-specific, actionable diagnoses for long-horizon failures before its advantage becomes visible. By contrast, on DeepSearch, ECHO improves more steeply in the early stage; we hypothesize this is because the evaluator is highly sensitive to output format and interaction protocol, so the critic can quickly correct systematic, easy-to-specify early failures. Overall, these curves support our claim that critique strategies are non-stationary under on-policy training: as failure modes drift, frozen critic becomes increasingly mismatched, whereas synchronized co-evolution helps maintain critique utility throughout training."
        },
        {
            "title": "5.3 RQ3: Why is the saturation-aware (SA)",
            "content": "reward design effective? To examine whether SA gain shaping provides more informative learning signal than linear improvement reward, we compare two reward designs on Qwen3-4B in WebShop and SciWorld: the linear reward = sr so, and our saturation-aware gain g(so, sr) in Eq. 6. Since SA shaping relies on meaningful reward magnitudes, we focus on these two benchmarks with non-binary rewards. 7 Figure 4: Effect of saturation-aware gain shaping on last-mile refinement. We plot density scatter maps of prerefinement and post-refinement rewards (so, sr) on WebShop and SciWorld using Qwen3-4B. Points in the green region satisfy sr > so and correspond to reward-improving refinements, where higher density indicates more effective critiques. The highlighted high-score square marks the near-ceiling regime. with so = 1, and visualize the joint distribution of (so, sr) as density scatter plots in Figure 4. Overall refinement effectiveness. Across both WebShop and SciWorld, saturation-aware shaping concentrates substantially more probability mass in the improvement region where sr > so, shown as the green upper-left triangle in Figure 4. Higher density in this region indicates that critiques more reliably translate into reward-increasing refinements, suggesting that the saturation-aware design yields stronger overall refinement effectiveness than the linear alternative. Last-mile improvement near the reward ceiling. In the high-score regime highlighted by the yellow square, the most desirable outcomes lie in its upper-left area, where trajectories start near full reward and still improve after refinement. For both datasets, saturation-aware shaping exhibits higher density in this region, indicating better ability to convert near-correct trajectories into full-reward solutions. In contrast, the linear reward shows many samples remaining close to the diagonal in this regime, especially on SciWorld, indicating that refinements tend to preserve the original score and struggle to achieve the small but critical gains required near the ceiling."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented ECHO, co-evolution framework for open-world LLM agents. By synchronizing critic and policy updates, ECHO mitigates critic staleness under on-policy failure drift. The proposed cascaded rollout provides group-structured samples for group-relative optimization, while the Figure 5: Training reward curves across four environments (Qwen3-4B). As shown in Table 2, disabling SA shaping while keeping the rest of ECHO unchanged leads to consistent drops on both datasets. Notably, the degradation is larger on WebShop. We attribute this to the different regimes reached by the policy: SciWorld is more challenging and the learned agent remains further from saturation, so training is less dominated by last-mile refinements where SA shaping is designed to provide extra signal; in contrast, WebShop more often enters near-ceiling regime, making SA shaping more impactful. To further understand where SA shaping helps during refinement, we next visualize the joint distribution of pre-refinement and post-refinement rewards (so, sr) in Figure 4. Since saturation effects are most salient when trajectory rewards are already high, we focus on the middle-to-late stage of training. Specifically, we extract window of 10 consecutive rollout batches, remove trajectories 8 saturation-aware gain shaping boosts last-mile improvements. Together, these designs enable the critics diagnostic granularity to stay aligned with the policys evolving failure modes, supporting more stable training and sustained refinement."
        },
        {
            "title": "Limitations",
            "content": "Our framework updates both the policy and the critic using improvement signals computed from the same external reward model. Therefore, its effectiveness depends on reward quality and calibration: if the reward is noisy, biased, or underspecified, the critic may optimize toward evaluator artifacts rather than truly diagnostic feedback, and the policy may inherit the same misalignment. Moreover, reward evaluation and critique generation are handled by separate models in our current implementation. natural next step is to unify them into single model that both scores trajectories and produces actionable critiques, which could simplify the training pipeline and improve consistency between what is rewarded and what is suggested. We leave this integration to future work."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. claude-3 model card. In Conference on Natural Language Processing, volume 1. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2024. Chain-of-verification reduces hallucination in large language models. In Findings of the association for computational linguistics: ACL 2024, pages 35633578. Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, and 1 others. 2025. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1458814604. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Qihan Huang, Weilong Dai, Jinlong Liu, Wanggui He, Hao Jiang, Mingli Song, Jingyuan Chen, Chang Yao, and Jie Song. 2025. Boosting mllm reasoning with text-debiased hint-grpo. arXiv preprint arXiv:2503.23905. Pei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, and 1 others. 2024. Critiquellm: Towards an informative critique generation model for evaluation of large language model generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1303413054. Ziru Liu, Cheng Gong, Xinyu Fu, Yaofang Liu, Ran Chen, Shoubo Hu, Suiyun Zhang, Rui Liu, Qingfu Zhang, and Dandan Tu. 2025. GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning. arXiv preprint. ArXiv:2507.10628 [cs]. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):25792605. Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. 2024. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215. Andrew Ng, Daishi Harada, and Stuart Russell. 1999. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pages 278287. Citeseer. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew 9 Zhihui Xie, Jie Chen, Liyu Chen, Weichao Mao, Jingjing Xu, and Lingpeng Kong. 2025. Teaching language models to critique via reinforcement learning. arXiv preprint arXiv:2502.03492. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. 2025. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Ruihan Yang, Fanghua Ye, Jian Li, Siyu Yuan, Yikai Zhang, Zhaopeng Tu, Xiaolong Li, and Deqing Yang. 2025b. The lighthouse of language: Enhancing llm agents via critique-guided improvement. arXiv preprint arXiv:2503.16024. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757. Tianshu Yu, Chao Xiang, Mingchuan Yang, Pei Ke, Bosi Wen, Cunxiang Wang, Jiale Cheng, Li Zhang, Xinyu Mu, Chuxiong Sun, and 1 others. 2025. Training language model to critique for better refinement. arXiv preprint arXiv:2506.22157. Feng Zhang, Zezhong Tan, Xinhong Ma, Ziqiang Dong, Xi Leng, Jianfei Zhao, Xin Sun, and Yang Yang. 2025a. Adhint: Adaptive hints with difficulty priors for reinforcement learning. arXiv preprint arXiv:2512.13095. Kaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, and Rui Yan. 2025b. Stephint: Multi-level stepwise hints enhance reinforcement learning to reason. arXiv preprint arXiv:2507.02841. Xichen Zhang, Sitong Wu, Yinghao Zhu, Haoru Tan, Shaozuo Yu, Ziyi He, and Jiaya Jia. 2025c. Scafgrpo: Scaffolded group relative policy optimization for enhancing llm reasoning. arXiv preprint arXiv:2510.19807. Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768. Richard Sutton, Andrew Barto, and 1 others. 1998. Reinforcement learning: An introduction, volume 1. MIT press Cambridge. Chenming Tang, Hsiu-Yuan Huang, Weijie Liu, Saiyong Yang, and Yunfang Wu. 2025. Do not step into the same river twice: Learning to reason from trial and error. arXiv preprint arXiv:2510.26109. Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, and 1 others. Self-evolving critique abilities in large language models. In Second Conference on Language Modeling. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, and 1 others. 2025. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534. Qwen Team and 1 others. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2(3). Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. 2022a. Scienceworld: Is your agent smarter than 5th grader? arXiv preprint arXiv:2203.07540. Xinyi Wang, Jinyi Han, Zishang Jiang, Tingyun Li, Jiaqing Liang, Sihang Jiang, Zhaoqian Dai, Shuguang Ma, Fei Yu, and Yanghua Xiao. 2025. Hint: Helping ineffective rollouts navigate towards effectiveness. arXiv preprint arXiv:2510.09388. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Xumeng Wen, Zihan Liu, Shun Zheng, Shengyu Ye, Zhirong Wu, Yang Wang, Zhijian Xu, Xiao Liang, Junjie Li, Ziming Miao, and 1 others. 2025. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms. arXiv preprint arXiv:2506.14245. Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, and 1 others. 2025. Agentgym-rl: Training llm agents for long-horizon decision making through multi-turn reinforcement learning. arXiv preprint arXiv:2509.08755. Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, and 1 others. 2024. Enhancing llm reasoning via critique models with testtime and training-time supervision. arXiv preprint arXiv:2411.16579."
        },
        {
            "title": "A Environments and Scoring Criteria",
            "content": "The evaluation environments used in our experiments are summarized in Table 3, including their task settings, the core abilities required of the agent, and the official scoring criteria. Table 3: Overview of evaluation environments. We summarize each environments setting, the core abilities required from the agent, and the scoring criterion used by the official evaluator. Environments Description Required Agent Ability Score Criterion the buys Purchase-based success, rewarding that requested match product and constraints (attributes/options/price). type Binary episode success judged by completion of the specified household task. Progress-based scoring that rewards completing required main subgoals in order (with optional bonus goals), while any out-of-order main step triggers failure score. Answer-level correctness judged by the official QA evaluator (exact-match style). WebShop (Yao et al., 2022) ALFWorld (Shridhar et al., 2020) SciWorld (Wang et al., 2022a) An interactive e-commerce website simulator where the agent navigates product pages and selects an item that satisfies natural-language shopping goal. embodied text-based environment household derived from ALFRED, requiring multi-step manipulation (e.g., pick, place, clean, heat) to accomplish instructions. simulated scientific discovery environment where the agent performs experiments, uses instruments, and reasons over observations to satisfy scientific objective. Goal parsing, web navigation, information retrieval, constraint tracking, reasoning over semistructured fields. Subgoal decomposition, spatial and physical commonsense, hierarchical planning, multi-step action execution, instrument operation. Scientific skills, experimental design, instrument operation, causal and mechanistic reasoning. DeepSearch (Xi 2025) et al., retrieval-augmented multiturn QA environment where the agent must search, read, and synthesize evidence to answer open-domain questions. decomposition, Query eviterative retrieval, idence aggregation, faithful synthesis, termination control."
        },
        {
            "title": "B More Implementation Details",
            "content": "All experiments are conducted with sixteen H20-100GB GPUs. We use the same learning rate for both the policy Pθ and the critic Cψ, setting lrθ = lrψ = 1 106. We set the rollout group size to = 8 by default, i.e., for each query we sample 8 independent critiques from the critic and generate 8 corresponding refinements conditioned on these critiques. For the policy model, we follow the official setup for both reward design and evaluation protocols to ensure fair and consistent comparison. For the critic model, we use the reward function in Eq. (6) and set the η to 0.1 in all experiments. Pseudo-code for ECHO Algorithm 1 provides the complete training procedure of ECHO. It summarizes the cascaded rollout pipeline (on-policy proposal τo, multi-view critiques {c(j) }), the saturation-aware critic reward computation in Eq. (6), and the synchronized dual-track GRPO updates for the policy and the critic performed on the same on-policy batch. }, and critique-conditioned refinements {τ (j) Algorithm 1: ECHO: Evolving Critic for Hindsight-Guided Optimization Input :Dataset D; reward model R; policy Pθ; critic Cψ; group size ; GRPO hyperparams (ϵ, β); smoothing η > 0. Output :Updated parameters (θ, ψ). 1 foreach training step do 2 3 5 6 7 8 9 11 12 13 14 15 17 18 Sample batch of queries // Stage 0: On-policy proposal and baseline score Sample initial trajectory τo Pθ( q) Compute baseline score so R(q, τo) // Stage 1: Multi-view diagnosis (critic group) for 1 to do Sample critique c(j) Cψ( q, τo, so) GC {c(j) }N j=1 // Stage 2: Conditional refinement (policy group) for 1 to do Form augmented input q(j) (q, c(j) ) Sample refinement τ (j) Pθ( q(j)) Evaluate post-correction score s(j) R(q, τ (j) ) GP (q) {τ (j) }N j=1 // Saturation-aware critic reward for each critique for 1 to do r(j) ln (cid:17) (cid:16) 1so+η 1s(j) +η // Eq. 6 // Dual-track group-relative advantage estimation Compute policy advantages {A(j) Compute critic advantages {A(j) j=1 by group-relative normalization of {s(j) }N j=1 by group-relative normalization of {r(j) }N }N }N // Synchronized GRPO updates (same batch, two tracks) Update θ by maximizing the GRPO surrogate objective (θ) using sequences {τ (j) advantages {A(j) } Update ψ by maximizing the GRPO surrogate objective (ψ) using sequences {c(j) advantages {A(j) } j= j=1 } with } with"
        },
        {
            "title": "D Prompt for Critic Model",
            "content": "We provide the exact prompting template used to elicit critiques from the critic model in Box A.1. The prompt constrains the critic to ground its feedback in the official scoring information and to output at most 12 high-level, actionable suggestions in fixed format, which stabilizes training and keeps critiques consistent across rollouts. Box A.1 : The prompt for critique generation. You are **Critic model**, used to provide guidance on models overall performance on given task. The system will provide you with: - The users task description; - Several rounds of the models interactions with the environment as it attempts to complete the task; - The official final scoring. Your task: **Strictly based on the official scoring information**, output clear improvement suggestions and behavioral guidance inside the <critic> tag. Only point out issues and directions for improvement. Do not mention strengths, and do not give praise or encouragement. ## Input Description Each round of the models actions is provided within <model_response>...</model_response>. Environment feedback is provided within <env_feedback>...</env_feedback>. ## Official Scoring Criteria (for your understanding; do not modify or question) {Detail scoring criteria on specific task.} You must: - Treat the official scoring result as absolutely correct and final. - Not reinterpret, question, or adjust the official score. ## Original Question {original_prompt} ## Response to be Evaluated {initial_response} ## Official Scoring Information {score_info_text} ## Output Format You must output **only** the following two tags and their contents. Do not add or remove tags, and do not output anything outside these tags: ``` <reason> Your detailed reasoning process: - Based on the official scoring information, analyze the models performance on each scoring dimension; - Strictly follow the official scoring information; do not question, revise, or supplement it; 13 - Whenever you have any doubts or subjective judgments about any evaluation dimension, always take the official scoring conclusion as the final basis. </reason> <critic> Your final guidance: - If the models score is full score (1 point), directly return \"none\". - Give **at most 12** brief, high-level suggestions; only mention the most critical issues. - Do **not** refer to specific reply text or dialogue details; only describe how model should change its general behavior or strategy. - Do not give any praise or encouragement; only point out problems and how You should improve. - Always address the model as **\"You\"** (second person), not \"the model\", \"it\", or similar. </critic> ```Based on the above standards, provide guidance on the given models behavior, and output in the specified format. ## Output Now, begin your reasoning!"
        },
        {
            "title": "E Task Examples and Case Studies",
            "content": "In this section, we showcase two illustrative case studies with task examples. 14 WebShop Instruction You are web shopping. will give you instructions about what to do. You have to follow the instructions. Every round will give you an observation and list of available actions, you have to respond an action based on the state and instruction. You can use search action if search is available. You can click one of the buttons in clickables. An action should be of the following structure: search[keywords], click[value]. If the action is not valid, perform nothing. Keywords in search are up to you, but the value in click must be value in the list of available actions. Remember that your keywords in search should be carefully designed. Your response should use the following format: Thought: think ... Action: click[something]. [SEP] Instruction: [SEP] Find me machine wash mens dress shirts with cotton spandex, classic fit, short sleeve with color: deep atlantic, and size: xx-large tall, and price lower than 60.00 dollars [SEP] Search. Your admissible actions of the current situation are: [search[<your query>], click[search]]. Original Trajectory in WebShop Case Thought 1: need to search for machine wash mens dress shirts with specific attributes: cotton spandex, classic fit, short sleeve, deep atlantic color, XX-Large tall size, and price under $60.00. will craft precise search query to find such items. Action 1: search[machine wash mens dress shirt cotton spandex classic fit short sleeve deep atlantic color XX-Large tall under 60] Observation 1: Back to Search [SEP] Page 1 (Total results: 50) [SEP] Next > [SEP] B07HRFSNL4 [SEP] Nautica Mens Solid Crew Neck Short-Sleeve Pocket T-Shirt [SEP] $16.05 to $40.98 [SEP] B09QQP3356 [SEP] HAUKLIE Mens Sports Waffle Ribbed Polo Shirts Summer Short Sleeve Cotton Muscle ... Thought 2: The product B09QQP3356 (HAUKLIE Mens Sports Waffle Ribbed Polo Shirts) matches the required attributes: mens dress shirt, cotton spandex, classic fit, short sleeve, deep atlantic color (as inferred from the description), XX-Large tall size, and price at $10.99, which is under $60.00. will click on this product to proceed. Action 2: click[b09qqp3356] Observation 2: Back to Search [SEP] < Prev [SEP] color [SEP] black [SEP] gray [SEP] light blue [SEP] size [SEP] small [SEP] medium [SEP] large [SEP] x-large [SEP] xx-large [SEP] 3x-large [SEP] HAUKLIE Mens Sports Waffle Ribbed Polo Shirts Summer Short Sleeve Cotton Muscle Quarter-Zip Henley T-Shirt Tunics Tops... Thought 3: The current product (HAUKLIE Mens Sports Waffle Ribbed Polo Shirts) is available in the desired size (XX-Large) and color (deep atlantic is not explicitly listed, but the description suggests cotton blend, which aligns with the requirement). The price is $10.99, which is under $60.00. Since the exact color \"deep atlantic\"is not available in the current UI, and no other product matches the search criteria precisely, will proceed with the closest match by clicking \"Buy Now\"to complete the purchase. Action 3: click[buy now] Observation 3: Thank you for shopping with us! [SEP] Your code: MTurk interface.) [SEP] Purchased [SEP] asin [SEP] B09QQP3356 [SEP] ... [SEP] None [SEP] (Paste it in your"
        },
        {
            "title": "Critique in a WebShop Case",
            "content": "<reason>The model successfully made purchase, so the final score is not zero. The type match (r_type) is 1.0, indicating that the product type (mens dress shirt) aligns with the instruction, and this is confirmed by the official scoring. The price match (r_price) is true, as the product priced at $10.99 is below $60.00. However, the final score is 0.42857142857142855, which suggests that the number of attribute and option matches is insufficient. The instruction explicitly requests \"cotton spandex\", \"classic fit\", \"short sleeve\", \"deep atlantic color\", and \"XX-Large tall size\", but the selected product (B09QQP3356) is described as \"waffle ribbed polo shirt\"with \"cotton muscle\"and \"cotton blend\", which does not explicitly confirm \"cotton spandex\"as constituent. Moreover, the exact color \"deep atlantic\"is not available in any visible option, and the model does not select any color or size option during the interactiononly selecting \"buy now\"without explicitly confirming the required color or size. The absence of explicit option selection (e.g., clicking on \"deep atlantic\"or \"XX-Large tall) means that num_option_matches is zero. Since no attributes are explicitly matched in the products displayed attributes, num_attr_matches is also zero. The final score is therefore calculated as (1 + 0 + 0 + 1) / (target attributes + target options + 1), but with insufficient alignment on critical attributes and options, the score remains low. The official scoring does not consider inferred or implied attributes as valid matchesonly explicitly stated or selected ones are considered</reason> <critic>Ensure that the model explicitly verifies and selects the required options (e.g., color: deep atlantic, size: xx-large tall) before proceeding to purchase. Only click on options that are explicitly listed and match the instruction exactly. Avoid inferring attributes or relying on product descriptions that do not directly confirm the required specifications. Always confirm that the selected products attributes and options exactly match the instruction before clicking buy.</critic> 15 Refined Trajectory in WebShop Case Thought 1: need to search for machine wash mens dress shirts with specific attributes: cotton spandex, classic fit, short sleeve, color: deep atlantic, size: xx-large tall, and price below $50.00. will construct precise search query to find such product. Action 1: search[mens dress shirts machine wash cotton spandex classic fit short sleeve color deep atlantic size xx-large tall price less than 50] Observation 1: Back to Search [SEP] Page 1 (Total results: 50) [SEP] Next > [SEP] B07HRFSNL4 [SEP] Nautica Mens Solid Crew Neck Short-Sleeve Pocket T-Shirt [SEP] 16.05to40.98 [SEP] B09QQP3356 [SEP] ... Thought 2: The product B07HRFSNL4 is Nautica Mens Solid Crew Neck Short-Sleeve Pocket T-Shirt, which matches the requirement of machine wash dress shirt with cotton spandex, classic fit, short sleeve. However, the color is not explicitly stated as \"deep atlantic\"and the size is not confirmed as \"xx-large tall\". Since the exact color and size are not clearly specified in the product listing, cannot confirm that this item fully meets the instruction. must proceed with caution. As no other product explicitly confirms both \"deep atlantic\"color and \"xx-large tall\"size, and the available options do not provide such details clearly, will instead select the product that best aligns with the description and hope the UI provides further options for customization. Action 2: click[b07hrfsnl4] Observation 2: Back to Search [SEP] < Prev [SEP] size [SEP] x-small [SEP] small [SEP] medium [SEP] large [SEP] large tall [SEP] x-large [SEP] x-large tall [SEP] xx-large [SEP] xx-large tall [SEP] 3x-large tall [SEP] 4x-large tall [SEP] 5x-large tall [SEP] 1x [SEP] 2x [SEP] 3x [SEP] 4x [SEP] 5x [SEP] 6x [SEP] color [SEP] navy [SEP] ... Thought 3: The instruction specifies the color as \"deep atlantic\"and size as \"xx-large tall\". These options are explicitly available in the dropdowns. will select \"deep atlantic\"for color and \"xx-large tall\"for size to precisely match the requirement. After confirming these attributes, can proceed with the purchase. Action 3: click[deep atlantic] Observation 3: Back to Search [SEP] < Prev [SEP] size [SEP] x-small [SEP] small [SEP] medium [SEP] large [SEP] large tall [SEP] x-large [SEP] x-large tall [SEP] xx-large [SEP] xx-large tall [SEP] 3x-large tall [SEP] 4x-large tall [SEP] 5x-large tall [SEP] 1x [SEP] 2x [SEP] 3x [SEP] 4x [SEP] 5x [SEP] 6x [SEP] color [SEP] navy [SEP] ... Thought 4: The color selection did not appear to update after clicking deep atlantic, and can see navy but not confirmation of the desired color. To ensure the listing matches the required size, will select xx-large tall next and look for any confirmation of both chosen attributes before proceeding. Action 4: click[xx-large tall] Observation 4: Back to Search [SEP] < Prev [SEP] size [SEP] x-small [SEP] small [SEP] medium [SEP] large [SEP] large tall [SEP] x-large [SEP] x-large tall [SEP] xx-large [SEP] xx-large tall [SEP] 3x-large tall [SEP] 4x-large tall [SEP] 5x-large tall [SEP] 1x [SEP] 2x [SEP] 3x [SEP] 4x [SEP] 5x [SEP] 6x [SEP] color [SEP] navy [SEP] ... Thought 5: The products color is explicitly set to \"deep atlantic\"and the size option \"xx-large tall\"is available. These match the exact requirements in the instruction. now confirm the correct attributes are selected. can proceed to buy the product. Action 5: click[buy now] Observation 5: Thank you for shopping with us! [SEP] Your code: MTurk interface.) [SEP] Purchased [SEP] asin [SEP] B09QQP3356 [SEP] ... [SEP] None [SEP] (Paste it in your 16 SciWorld Instruction You are an agent for science world. Every round will give you an observation, you have to respond an action based on the observation to finish the given task. Here are the actions you may take: [{\"action\": \"open/close OBJ\", \"description\": \"open/close container\"}, {\"action\": \"de/activate OBJ\", \"description\": \"activate/deactivate device\"}, {\"action\": \"connect OBJ to OBJ\", \"description\": \"connect electrical components\"}, {\"action\": \"disconnect OBJ\", \"description\": \"disconnect electrical components\"}, {\"action\": \"use OBJ [on OBJ]\", \"description\": \"use device/item\"}, {\"action\": \"look around\", \"description\": \"describe the current room\"}, {\"action\": \"look at OBJ\", \"description\": \"describe an object in detail\"}, {\"action\": \"look in OBJ\", \"description\": \"describe containers contents\"}, {\"action\": \"read OBJ\", \"description\": \"read note or book\"}, {\"action\": \"move OBJ to OBJ\", \"description\": \"move an object to container\"}, {\"action\": \"pick up OBJ\", \"description\": \"move an object to the inventory\"}, {\"action\": \"put down OBJ\", \"description\": \"drop an inventory item\"}, {\"action\": \"pour OBJ into OBJ\", \"description\": \"pour liquid into container\"}, {\"action\": \"dunk OBJ into OBJ\", \"description\": \"dunk container into liquid\"}, {\"action\": \"mix OBJ\", \"description\": \"chemically mix container\"}, {\"action\": \"go to LOC\", \"description\": \"move to new location\"}, {\"action\": \"eat OBJ\", \"description\": \"eat food\"}, {\"action\": \"flush OBJ\", \"description\": \"flush toilet\"}, {\"action\": \"focus on OBJ\", \"description\": \"signal intent on task object\"}, {\"action\": \"wait\", \"description\": \"take no action for 10 iterations\"}, {\"action\": \"wait1\", \"description\": \"take no action for 1 iteration\"}, {\"action\":\"examine OBJ\",\"description\":\"provides description of the objects present on or in receptacle.\"}, {\"action\": \"task\", \"description\": \"describe current task\"}, {\"action\": \"inventory\", \"description\": \"list your inventory\"}] Your response should use the following format: Thought: your thoughts. Action: your next action. Your task is to measure the melting point of gallium, which is located around the kitchen. First, focus on the thermometer. Next, focus on the gallium. If the melting point of gallium is above 200.0 degrees celsius, focus on the red box. If the melting point of gallium is below 200.0 degrees celsius, focus on the green box. The boxes are located around the kitchen.This room is called the bedroom. In it, you see: the agent. substance called air. bed. On the bed is: mattress. On the mattress is: white pillow. book shelf (containing book (Moby Dick) titled Moby Dick by Herman Melville, book (Pride and Prejudice) titled Pride and Prejudice by Jane Austen, book (Sherlock Holmes) titled Sherlock Holmes by Arthur Conan Doyle). closet. The closet door is closed. drawing. table. On the table is: nothing. You also see: door to the hallway (that is closed) Original Trajectory in SciWorld Case - Part 1 Thought 1: The task is to measure the melting point of gallium, which is located in the kitchen. However, the current room is the bedroom, and do not see gallium or thermometer in the current view. To achieve the task, need to first go to the kitchen. Action 1: go to kitchen Observation 1: No known action matches that input. Thought 2: need to correct my action \"go to kitchen\"is not valid action in the available action list. must use an action from the provided list. The only way to move to new location is by using \"go to LOC\", if that is allowed. Looking back at the action list, \"go to LOC\"is valid action. So will use it to move to the kitchen. Action 2: go to kitchen Observation 2: No known action matches that input. Thought 3: must carefully check the available actions. The list includes \"go to LOC\"as valid action. may have been incorrectly interpreting the input. Since the current room is the bedroom and the kitchen is the next location, and \"go to kitchen\"is not in the list, must explicitly write the correct action using the valid format. Correct action: \"go to kitchen\" this is *allowed* by the action list. Action 3: go to kitchen Observation 3: No known action matches that input. must confirm that \"go to kitchen\"is indeed not in the action list. Rechecking the list proThought 4: vided: Valid actions include: - open/close OBJ - de/activate OBJ - connect OBJ to OBJ - disconnect OBJ - use OBJ [on OBJ] - look around - look at OBJ - look in OBJ - read OBJ - move OBJ to OBJ - pick up OBJ - put down OBJ - pour OBJ into OBJ - dunk OBJ into OBJ - mix OBJ - go to LOC - eat OBJ - flush OBJ - focus on OBJ - wait - wait1 - examine OBJ - task - inventory Yes \"go to LOC\"is valid action. So, using \"go to kitchen\"*should* be acceptable, as it matches the format \"go to LOC\". Possibility: Perhaps the system has restrictions on room names (e.g., must be exact). Since the user said \"kitchen\"and the observation mentions the kitchen as where gallium is, it should be valid. My action must be valid and in the correct format. Action 4: go to kitchen Observation 4: No known action matches that input. Thought 5: Given that \"go to kitchen\"is not matching, perhaps the room name is not \"kitchen\"in the observation. Rechecking the initial observation: > \"Your task is to measure the melting point of gallium, which is located around the kitchen.\" And the current room is explicitly stated: > \"This room is called the bedroom.\" So, must confirm whether the kitchen is reachable or if need to find an exit. Observation says: > \"a door to the hallway (that is closed)\" So, the bedroom has door to the hallway. Perhaps the hallway leads to the kitchen. Thus, to go to the kitchen, must first go through the hallway. Action 5: go to hallway (since \"go to LOC\"is valid and \"hallway\"is mentioned) Observation 5: The door is not open. Thought 6: The door to the hallway is closed, so cannot proceed through it without opening it. The only way to open door is via the \"open\"action. Action 6: open door to the hallway 18 Original Trajectory in SciWorld Case - Part 2 Observation 6: The door is now open. Thought 7: The door to the hallway is now open. where the gallium is located. can now proceed to the hallway to reach the kitchen, Action 7: go to hallway Observation 7: You move to the hallway. Thought 8: have successfully moved to the hallway. The next step is to proceed to the kitchen, as the gallium is located \"around the kitchen.\"Since \"go to kitchen\"is valid action and supported by the action list, should perform it. Action 8: go to kitchen Observation 8: The door is not open. Thought 9: This contradicts earlier information previously opened the door to the hallway. However, the system now indicates the door is not open. must verify my actions. Possibly, made mistake in assuming the door was opened. Since the observation says: \"a door to the hallway (that is closed)\", and no prior action was taken, must confirm that did not open it. Now, if the door is closed again, must open it before proceeding. Action 9: open door to the hallway Observation 9: No known action matches that input. Thought 10: valid. So, to open the door, can say: must use valid action from the list... Looking at the allowed actions, \"open/close OBJ\"is Action 10: open door to the hallway Observation 10: No known action matches that input. Thought 11: OBJ\"is allowed, and no error in structure, must use: must strictly follow the correct format of the allowed actions... Given that \"open/close Action 11: open door to the hallway But the system says \"No known action matches that input\" so this implies the object name may be different. Alternative possibility: the door may be part of wall or has specific label. But theres no such detail. Could it be that need to name it simply \"door\"? Let me reframe the only object mentioned is \"a door to the hallway (that is closed)\". So, can use: Observation 11: Ambiguous request: Please enter the number for the action you intended (or blank to cancel): 0: open door between bedroom and hallway 1: open door between workshop and hallway 2: open door between art studio and hallway 3: open door between greenhouse and hallway 4: open door between kitchen and hallway 5: open door between living room and hallway Thought 12: The system now presents numbered list of possible \"open door\"actions, indicating that the door is not open, and must choose one from the available options... So, to proceed, must open the bedroom-to-hallway door. Action 12: Observation 12: The door is already open. Thought 13: The user previously stated \"The door is not open\" but now says \"The door is already open.\"This suggests contradiction in the state, likely due to prior action. Since just tried to open it and the system now confirms it is open, should now proceed to the hallway. The only valid action now is to move to the next location. Action 13: go to hallway Observation 13: Its not clear how to get there from here. 19 Critique in SciWorld Case <reason>The official scoring indicates final score of 0.01, which means the task has not progressed sufficiently to achieve the ordered main sub-goals. The task requires measuring the melting point of gallium, which involves first locating gallium and thermometer, then using them to measure the temperature. The models actions are focused only on moving from the bedroom to the hallway, repeatedly attempting \"go to hallway\"despite repeated interface feedback stating \"its not clear how to get there.\"This suggests that the model did not achieve the first main sub-goal locating the gallium and failed to proceed to the next required step. The failure to progress past the initial room transition indicates clear oversight: the model did not verify the presence of the thermometer or the gallium once in the kitchen. The task demands strict sequence: first, focus on the thermometer, then on the gallium. Since the model never checks for these objects, it skipped the first required steps in the ordered sub-goals. The lack of actions such as \"look at thermometer\"or \"look at gallium\" even after entering the kitchen demonstrates failure to perform the required observations. Additionally, despite the environment indicating the kitchen contains the gallium, the model never attempts to interact with or examine the target objects. This results in no measurable progress on the ordered part of the task, which is the primary component of the score. The small reward delta of 0.01 may be due to the opening of the door, but it does not reflect actual task completion. The absence of any action to examine or use the thermometer or gallium means the task is effectively stalled. There is no indication of completing any unordered goals (e.g., adding sugar or tidying), so the unordered bonus remains zero. The total score is capped at 0.01, indicating only minimal progress insufficient to satisfy the ordered part of the task. The model repeatedly tries the same invalid inputs (e.g., \"go to kitchen\") without recognizing it must use \"go to hallway\"as an intermediate step and still fails to move toward the actual object. This shows lack of scenario awareness and incomplete planning of the required steps in sequence. </reason> <critic>You must ensure that after entering each new location, you explicitly check for required objects (such as the thermometer and gallium) before proceeding. You must not skip verifying objects in the required order, as doing so will lead to failure in the ordered part of the task. You must also confirm object availability before taking further actions, and never assume progress based on movement alone. </critic> 20 Refined Trajectory in SciWorld Case - Part 1 Thought 1: The task requires measuring the melting point of gallium, which is located in the kitchen. Currently, am in the bedroom, and the required objects (thermometer and gallium) are not visible there. The next logical step is to go to the kitchen to look for the thermometer and the gallium. Before proceeding, need to confirm that the kitchen is accessible and contains the necessary objects. Action 1: go to kitchen Observation 1: No known action matches that input. need to move to the kitchen to begin the task. However, Thought 2: the current observation only describes the bedroom. There is no explicit mention of kitchen or its contents, including the thermometer or gallium. Since the task specifies that gallium is \"located around the kitchen,\"I must first confirm the existence of the kitchen and verify whether the thermometer and gallium are present there. To proceed correctly, must first move to the kitchen, and after entering, check for the thermometer and gallium before continuing. Action 2: go to kitchen Observation 2: No known action matches that input. Thought 3: The error suggests that the action \"go to kitchen\"is not valid in the current context. Since the initial observation only describes the bedroom and does not mention kitchen, must first verify if there is path or accessible location to the kitchen. The only exit mentioned is door to the hallway, which is closed. Therefore, should first check the door to the hallway to see if it can be opened or if the hallway leads to the kitchen. must follow the required procedure: verify the location before proceeding. Action 3: look at door to the hallway Observation 3: door to the hallway (that is closed) Thought 4: The door to the hallway is closed, so cannot proceed into the hallway without opening it. Before opening the door, must confirm whether it leads to the kitchen, as that is where the gallium and thermometer are located. Since no direct mention of the kitchen is in the current room, need to determine if the hallway is the way to reach it. The task requires me to first focus on the thermometer, then on the gallium. Therefore, must verify access to the kitchen before attempting to locate those items. Action 4: open door to the hallway Observation 4: The door is now open. Thought 5: The door to the hallway is now open. can now proceed to enter the hallway to reach the kitchen, where the thermometer and gallium are said to be located. Before moving forward, must ensure am in location where the required objects might be. Since the kitchen is not explicitly described in the current observation, need to go to the kitchen to confirm the presence of the thermometer and gallium. Action 5: go to kitchen Observation 5: No known action matches that input. Thought 6: have successfully opened the door to the hallway. Now, need to move into the hallway to proceed to the kitchen. Since \"go to kitchen\"is not valid action here, and the only valid actions are those listed, must first confirm whether can now observe the hallway or if need to explore it before moving to the kitchen. To proceed safely and correctly, should first explore the hallway to determine if the kitchen is present there and whether the necessary objects (thermometer and gallium) are visible. Action 6: look around Observation 6: This room is called the bedroom. the bed is: mattress. On the mattress is: white pillow... In it, you see: the agent substance called air bed. On Thought 7: The door to the hallway is now open, but am still in the bedroom. To proceed with the task, must enter the hallway to reach the kitchen, where the thermometer and gallium are located. Since \"go to kitchen\"is not valid action, must first move into the hallway. After entering, will need to verify the presence of the thermometer and gallium before continuing to focus on them. Action 7: go to hallway Observation 7: You move to the hallway. 21 Refined Trajectory in SciWorld Case - Part 2 Thought 8: have now entered the hallway. The next step is to check the hallway for the required objects: the thermometer and the gallium. Since the task specifies that gallium is located \"around the kitchen,\"and the kitchen is the next logical destination, should explore the hallway to see if the kitchen is visible or accessible. must not proceed without confirming the presence of the thermometer and gallium. Action 8: look around Observation 8: This room is called the hallway. ing... In it, you see: the agent substance called air drawThought 9: am now in the hallway. The door to the kitchen is closed, but it is the only entrance mentioned that leads to the location where the gallium and thermometer are supposed to be. Since the kitchen is the required location to find the objects, must proceed to open the door to the kitchen before searching inside. Before opening the door, must confirm that it is the correct path to the kitchen. The observation confirms it is present and closed. Action 9: open door to the kitchen Observation 9: The door is now open. Thought 10: The door to the kitchen is now open. Once inside, need to explicitly check for the thermometer and the gallium before continuing. must now enter the kitchen to proceed with the task. Action 10: go to kitchen Observation 10: You move to the kitchen. Thought 11: and the gallium, as per the task instructions. must not proceed until both objects are confirmed to be in the room. am now in the kitchen. The next required step is to verify the presence of the thermometer Action 11: look around Observation 11: This room is called the kitchen. On the chair is: nothing. ... In it, you see: the agent substance called air chair. Thought 12: am now in the kitchen and have confirmed the presence of both required objects: - The thermometer is on the counter, currently reading 10C. - The gallium is inside plastic cup on the table. This satisfies the first two steps of the task: focusing on the thermometer and then on the gallium. The next step is to determine whether the melting point of gallium is above or below 200.0C to decide which box to focus on (red or green). know that the actual melting point of gallium is approximately 29.76C well below 200.0C. Therefore, should now focus on the green box. Action 12: focus on thermometer Observation 12: You focus on the thermometer. have already verified the presence of both the thermometer and the gallium in the kitchen. Thought 13: The thermometer is now focused on, which satisfies the first requirement of the task. Next, must focus on the gallium as required, before proceeding to determine whether its melting point is above or below 200.0C. Action 13: focus on gallium Observation 13: You focus on the gallium. have now focused on both the thermometer and the gallium as required by the task. The Thought 14: gallium is present in plastic cup on the table, and have verified its location. The next step is to determine whether the melting point of gallium is above or below 200.0C. Since the actual melting point of gallium is approximately 29.76C, it is below 200.0C. Therefore, according to the instructions, must now focus on the green box. Action 14: focus on green box Observation 14: You focus on the green box."
        }
    ],
    "affiliations": [
        "Amap, Alibaba Group",
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "Peking University",
        "Southern University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}