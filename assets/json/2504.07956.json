{
    "paper_title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning",
    "authors": [
        "Yukun Qi",
        "Yiming Zhao",
        "Yu Zeng",
        "Xikun Bao",
        "Wenxuan Huang",
        "Lin Chen",
        "Zehui Chen",
        "Jie Zhao",
        "Zhongang Qi",
        "Feng Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 6 5 9 7 0 . 4 0 5 2 : r VCR-Bench: Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning Yukun Qi1,2, Yiming Zhao1,2, Yu Zeng1,2, Xikun Bao1,2, Wenxuan Huang3, Lin Chen1, Zehui Chen1, Jie Zhao2, Zhongang Qi2, Feng Zhao1 1University of Science and Technology of China 2Huawei Noahs Ark Lab 3East China Normal University Project Page: https://vlm-reasoning.github.io/VCR-Bench/"
        },
        {
            "title": "Abstract",
            "content": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, novel benchmark designed to comprehensively evaluate LVLMs Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs key bottleneck in temporal-spatial information processing for complex video reasoning. robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as standardized evaluation framework and expose the actual drawbacks in complex video reasoning task."
        },
        {
            "title": "Introduction",
            "content": "The emergence of Chain-of-Thought (CoT) reasoning [40] has significantly enhanced the reasoning capability of large language models (LLMs), as evidenced by the recent breakthroughs of DeepSeekR1 [13] and OpenAI o1 [31]. By generating human-like, interpretable reasoning steps, these reasoning models have demonstrated remarkable advantages in solving complex visual tasks. Recently, large vision-language models (LVLMs) [30, 46] have achieved groundbreaking progress in multiple visual fields, especially in research on CoT reasoning for video data. However, video understanding field still lacks scientifically effective evaluation suit for CoT reasoning, with existing benchmarks primarily suffering from the following two shortcomings: First, Project leader. Corresponding author. Preprint. Figure 1: Failure case of accuracy-based evaluation. The video contains two news anchors, but the model missed one while misclassify non-anchor as an anchor, yet reached the correct answer. This suggests that relying solely on accuracy is insufficient for appropriately evaluating models performance under video CoT reasoning. current video benchmarks [44, 26, 56, 55] often lack comprehensive annotations of CoT steps, focusing only on the accuracy of final answers during model evaluation while neglecting the quality of the reasoning process. This evaluation approach makes it difficult to comprehensively evaluate models actual drawbacks during the CoT reasoning process. As shown in Figure 1, the model captures one piece of erroneous information while missing one correct piece during its reasoning process, yet ultimately arrives at the correct final answer. Second, existing video understanding benchmarks [21, 12] fail to effectively distinguish performance differences in perception and reasoning capabilities. The absence of an effective evaluation suit has become significant bottleneck that hinders the indepth development of complex reasoning research in the field of video understanding. To fill this gap, we propose VCR-Bench, benchmark specifically designed to evaluate the Video Chain-of-Thought Reasoning capabilities of LVLMs. We have constructed multi-dimensional evaluation framework, defining seven distinct task dimensions that comprehensively cover diverse range of video types and durations. For each data sample, in addition to providing standard answer, we have meticulously curated detailed and accurate reference stepwise rationals as CoT annotation. All samples underwent rigorous manual annotation and quality control, ultimately resulting in the creation of VCR-Bench, which includes 859 videos and 1,034 high-quality question-answer pairs. We draw on existing work in the field of image understanding [19, 7, 36] to innovatively design an evaluation framework specifically for assessing generated CoT reasoning steps. This framework first categorizes the CoT steps into visual perception steps and logical reasoning steps, then systematically evaluates the CoT steps across multiple dimensions including recall rate and precision rate to derive the CoT score, thereby providing basis for comprehensively measuring models reasoning capabilities. We conducted through evaluation of multiple models on our VCR-Bench. The experimental results reveal significant limitations in current models: even the top-performing model, o1 [31], achieves only 62.8% CoT score and 56.7% accuracy, while most models score below 40%. This performance gap highlights the notable shortcomings of existing LVLMs in video reasoning tasks and underscores substantial room for improvement. The consistently lower average perception scores compared to reasoning scores indicate that the primary performance bottleneck in current LVLMs for complex 2 video reasoning tasks remains the extraction and comprehension of temporal-spatial information. Further analysis revealed strong positive correlation between the models CoT scores and the accuracy. This effectively validates the effectiveness and reliability of our evaluation framework. In nutshell, our core contributions are as follows: To our knowledge, VCR-Bench is the first benchmark specifically designed for video CoT reasoning. Through rigorous manual annotation, we provide detailed reasoning steps for each sample, ensuring data accuracy and reliability while offering the research community high-quality video reasoning evaluation benchmark. We have successfully introduced the CoT evaluation framework into the field of video reasoning, assessing the entire reasoning process based on step-by-step annotated CoT rationales, thereby providing an effective approach to measure the video reasoning performance of LVLMs. Through extensive evaluation experiments, we have validated the effectiveness of our assessment methods and data, while also demonstrating that current LVLMs still exhibit significant limitations in video reasoning, especially in the extraction of temporal-spatial information. Furthermore, our experiments demonstrate strong correlation between CoT step quality and final answer accuracy."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 LVLMs for Video Understanding The rapid advancement of image-based LVLMs [6, 25, 48, 28] has significantly boosted video understanding and question answering capabilities, revitalizing AI research. Early attempts like VideoChat and Video-ChatGPT [28] paved the way for recent advancements such as CogVLM2Video [17], InternVL2 [10, 9], and LLaVA-Video [53], which process videos as image sequences by leveraging powerful image comprehension. To address the computational challenges of high frame rates and long videos, techniques like QFormer-based feature extraction in InternVideo2 [38] and Video-LLaMA [51], and adaptive pooling in PLLaVA [45] have been developed. With the enhancement of model capabilities and the increasing complexity of tasks, the strong reasoning and thinking abilities of LVLMs in the field of video understanding are receiving growing attention. 2.2 Video Understanding Benchmarks Traditional video understanding benchmarks focus on evaluating specific model capabilities in particular scenarios. For example, MSRVTT-QA [44], ActivityNet-QA [49], and NExT-QA [42] test basic action recognition and video question answering, while MMBench [43], SEED-Bench [21], and MVBench [24] assess short video clips. Benchmarks like LongVideoBench [41], Video-MME [12], and LVBench [37] provide longer videos and more diverse tasks. Latest work, such as V2PBench [55], has constructed set of data based on visual prompts by simulating human-computer interactions. However, these tasks are generally simple and do not require complex reasoning from models. Recently, there has been growing interest in video CoT reasoning tasks. VideoEspresso [15] uses keyframe captions for complex scene reasoning, MMVU [54] introduces annotated educational video reasoning questions, and VideoMMMU [18] focuses on knowledge reasoning from subject explanation videos. While these efforts aim to measure video CoT reasoning, their scenarios are limited, and they primarily evaluate final results rather than the reasoning process itself. 2.3 Reasoning Evaluation In the multimodal domain, research on evaluating reasoning processes remains relatively scarce and is primarily focused on the image domain. Early efforts to assess reasoning capabilities were mainly concentrated in scientific fields, such as MathVista [27], MathVerse [52], and OlympiadBench [16], which are limited to overly specific scenarios. Recent works have extended the evaluation of reasoning processes to the general image domain. For instance, M³CoT [7] and SciVerse [14] incorporate commonsense tasks, scientific reasoning, and knowledge-based assessment into multimodal benchmarks. However, these works still lack comprehensive evaluation of the reasoning process. LlamaV-o1 [36] constructs multi-dimensional evaluation framework to meticulously assess 3 Table 1: Key Statistics of VCR-Bench. Statistic Total Videos - Short Videos ( 1 min) - Medium Videos (1 5 min) - Long Videos (> 5 min) Total Questions - Dimensions Fundamental Temporal Reasoning Video Temporal Counting Video Temporal Grounding Video Knowledge Reasoning Temporal Spatial Reasoning Video Plot Analysis Temporal Spatial Grounding - Types Multiple-choice Open-ended Total Reference Reasoning Steps - Visual Perception Steps - Logical Reasoning Steps Reasoning Steps per Sample (avg/max) Reasoning Step Word Count (avg/max) Question Word Count (avg/max) Answer Word Count (avg/max) Number 859 418 (48.7%) 293 (34.1%) 148 (17.2%) 1034 159 (15.4%) 161 (15.6%) 143 (13.8%) 153 (14.8%) 135 (13.1%) 139 (13.4%) 144 (13.9%) 510 (49.3%) 524 (50.7%) 4078 2789 (68.4%) 1289 (31.6%) 3.9/12 27.0/129 22.1/161 3.5/49 Figure 2: Video source and categories. image reasoning processes, while MME-CoT [19] achieves promising results in process evaluation within the image domain by matching output steps with annotated steps and establishing an F1 score calculation criterion. These methodologies can be adapted and applied to the field of video reasoning."
        },
        {
            "title": "3 VCR-Bench",
            "content": "3.1 Dataset Curation As shown in Figure 2, to ensure the diversity of video data and the richness of sample information, we curated the VCR-Bench by selecting and integrating data from multiple existing video benchmarks. These include datasets focused on video perception and comprehension, such as Perception Test [32], NExTVideo [42], TVbench [11], MLVU [56], VCGBench-Diverse [29] and COIN [34]; datasets targeting subject knowledge understanding and reasoning, such as videoMMMU [18] and MMVU [54]; datasets emphasizing long-form video understanding, including Video-MME [12] and LongVideoBench [41]; datasets specialized in video temporal localization and analysis, such as ActivityNet Captions [20] and ReVOS Videos [46]; as well as datasets dedicated to video scene reasoning, exemplified by VideoEspresso [15], among others. 3.1.1 Task Definition To comprehensively evaluate the differences in LVLMs capabilities for video Chain-of-Thought (CoT) reasoning from multiple perspectives, we define seven distinct dimensions of task categories, as illustrated in Figure 3. These dimensions encompass various aspects such as spatiotemporal perception, logical reasoning, and knowledge-based analysis. The specific task types are as follows: Fundamental Temporal Reasoning (FTR): FTR task represents basic temporal reasoning problem, requiring the model to develop deep understanding of the temporal order and to analyze and compare the sequence in which events or actions occur. Video Temporal Counting (VTC): VTC task requires the model to calculate the frequency of events or actions and to perceive the number of occurrences of specific objects. Video Temporal Grounding (VTG): VTG task requires the model to locate the specific moment or time interval corresponding to given action or event. Video Knowledge Reasoning (VKR): VKR task requires the model to extract specific knowledgerelated information from the video and apply domain-specific logical reasoning to solve targeted problems. 4 Figure 3: Cases across dimensions. VCR-Bench encompasses seven distinct task dimensions spanning multiple competency levels, including spatiotemporal perception, logical reasoning, and knowledge-based analysis. Temporal Spatial Reasoning (TSR): TSR task focuses on the spatial position changes of characters within the video, including their movement trajectories and specific locations. Video Plot Analysis (VPA): VPA task requires the model to understand the narrative logic of the video and provide explanations for specific events that occur within the plot. Temporal Spatial Grounding (TSG): TSG task requires the model to locate the spatial position of corresponding object within specified temporal sequence. 3.1.2 Data Annotation and Review To enable CoT evaluation, we provide questions, answers, and CoT annotations (reference reasoning steps) for all data. These reference steps represent the essential reasoning path to derive correct answers. Our annotation pipeline combines automated generation (using Gemini 2.0 [33]) followed by human verification. This ensures both diversity and accuracy. Each samples reasoning steps form an ordered set = {r1, r2, ..., rN } of atomic sub-steps, designed to facilitate granular evaluation. Figure 4: Overview of VCR-Bench. For each sample, we provide detailed CoT annotations. During evaluation, we decompose model responses into reasoning steps and match them with reference CoT to compute recall/precision. Final answers are extracted and compared against ground-truth. 3.1.3 Data Analysis After data annotation and verification, we have ultimately constructed dataset comprising 859 videos and 1034 question-answer pairs. As shown in Table 1, our video dataset encompasses wide range of different scenarios, including indoor daily life, sports competitions, outdoor nature, and urban architecture. It covers multiple categories such as personal photography, documentaries, films and television, educational videos, and news reports. The duration of the videos ranges from less than one minute to over 30 minutes, ensuring rich diversity in content and high density of informational cues. Meanwhile, our question-answer pair data achieves rough balance across seven different dimensions, ensuring the richness and balance of the benchmark tasks. 3.2 CoT Evaluation Strategy Current video understanding benchmarks primarily evaluate the correctness of models final answers while neglecting intermediate CoT reasoning steps. This evaluation approach fails to provide comprehensive assessment of models reasoning capabilities. When addressing complex problems, models must perform multiple cognitive operations including perception and reasoning - evaluating only the final answers cannot reveal their actual shortcomings. As shown in Figure 4, to address this limitation, our proposed VCR-Bench incorporates two additional evaluation components alongside conventional final-answer assessment: CoT Reasoning Deconstruction and CoT Quality Evaluation. 3.2.1 CoT Reasoning Deconstruction The reasoning process of LVLMs involves multiple distinct operations, reflecting diverse capabilities. To systematically evaluate model performance across these competencies, we propose CoT Reasoning Deconstruction, which breaks down the process into two core dimensions: Visual Perception assesses the models ability to extract spatiotemporal information (e.g., actions, object locations) from videosthe foundational skill for vision tasks. 6 Logical Reasoning evaluates the models capacity to derive conclusions from perceived information, critical for complex problem-solving. Formally, we represent reference reasoning steps as: = Rp Rr, where the Rp and Rr denote perception and reasoning subprocesses, respectively. 3.2.2 CoT Quality Evaluation As described in Section 3.1.2, the question-answer pairs in the VCR-Bench provide accurate and concise reference reasoning steps R. The core of evaluating the models reasoning content is to establish matching relationship between the models reasoning steps and the reference reasoning steps R, to determine the correctness of the models reasoning. To this end, we use GPT4o [30] to decompose the models reasoning content into independent and structurally similar sub-steps, and categorize them into two sub-processes, as shown in Eq. 1. Then, we evaluate the reasoning process of the model under test based on the following metrics: = Sp Sr = {s1, s2, s3, . . . , sK} (1) Recall. For each sub-step ri in R, we prompt GPT4o to evaluate whether the corresponding content of ri also appears in S. If the same content appears in and is entirely correct including accurate temporal localization, correct entity recognition, and consistent logical reasoning then ri is considered matched and denoted as rmatch . The set of all matched sub-steps is denoted as Rmatch, and Rmatch Rmatch = Rmatch . The Recall can be calculated as shown in the following Eq. 2. Recallp = (cid:12) (cid:12) (cid:12) (cid:12)Rmatch Rp , Recallr = (cid:12) (cid:12) (cid:12) (cid:12)Rmatch Rr , Recall = (cid:12)Rmatch(cid:12) (cid:12) (cid:12) (2) The Recall metric comprehensively evaluates the reasoning process by comparing the models output with the reference solutions key reasoning steps. This metric not only verifies answer correctness but also rigorously examines the logical robustness of the reasoning, effectively eliminating random guessing scenarios, thereby enabling in-depth assessment of the models reasoning capabilities. Precision. For each sub-step sj in S, we prompt GPT4o to evaluate based on the content of whether sj is accurate. If sj matches and is correct according to the content in R, it is considered correct step, denoted as scorrect . If sj does not match or contradicts the content in R, such as errors in the temporal localization of key events, or mistakes in causal reasoning, it is considered an incorrect step, denoted as sincorrect . If sj does not appear in R, or it is impossible to determine whether sj is correct based on the content in R, it is considered an irrelevant reasoning step in solving the problem, denoted as sirrelevant . The set of correct steps and incorrect steps are denoted as correct and incorrect. Similarly, both correct and incorrect can be further decomposed into the form as shown in 3. S correct = correct correct , incorrect = incorrect incorrect Accordingly, the recision can be calculated as shown in the following Eq. 4 and Eq. 5. recisionp = (cid:12) (cid:12) (cid:12)S correct (cid:12) incorrect (cid:12) (cid:12)S correct (cid:12) (cid:12) , recisionr = correct incorrect correct recision = correct correct incorrect (3) (4) (5) The recision metrics evaluate the models output reasoning steps, assessing whether each step is truly reliable and closely related to the answer. By combining recision and Recall metrics, we can calculate the models output F1 score as shown in Equation 6 to serve as the final CoT score, thereby enabling more reliable and comprehensive evaluation of the models CoT response quality. F1 = 2 recision Recall recision + Recall 7 (6) Table 2: CoT Evaluation Results for Different Models in VCR-Bench. The best results are bold and the second-best are underlined. The F1 represents the final CoT score. Model Perception Reasoning Rec re Rec re F1 Rec 52.1 47.1 52.4 51.4 47.7 Closed-Source Models Gemini-2.0-Flash Gemini-1.5-Pro o1 GPT-4o Claude 3.5 Sonnet Open-Source Models 16.1 InternVL2.5-8B 18.7 InternVL2.5-78B 20.2 VideoLLaMA3-7B LLaVA-OneVision-7B 10.1 LLaVA-OneVision-72B 14.1 6.0 mPLUG-Owl3-7B MiniCPM-o2.6-8B 27.5 2.1 Llama-3.2-11B-Vision 31.7 Qwen2.5-VL-7B 46.2 Qwen2.5-VL-72B 11.1 LLaVA-Video-7B 15.6 LLaVA-Video-72B 18.5 Aria-25B 6.9 InternVideo2.5-8B 66.6 57.8 70.0 61.0 58.1 52.6 74.1 52.2 92.3 94.7 86.5 49.4 86.4 53.4 60.2 95.7 95.3 68.6 98.4 58.5 51.9 59.9 55.8 52.4 24.6 29.9 29.1 18.3 24.5 11.1 35.3 4.2 39.8 52.3 19.9 26.9 29.1 12.9 57.4 54.8 66.6 55.3 49.1 33.0 35.2 39.1 28.7 35.5 20.7 34.6 6.8 34.7 47.4 33.1 39.8 36.2 26. 64.6 54.3 71.4 52.4 47.5 36.9 53.9 39.9 51.2 58.3 43.7 35.0 52.5 37.4 46.1 52.0 57.1 52.3 61.3 60.8 54.5 68.9 53.8 48.3 34.8 42.6 39.5 36.8 44.1 28.1 34.8 12.0 36.0 46.7 40.4 46.9 42.8 36.6 54.0 49.4 56.9 52.7 47.6 22.1 23.9 26.6 16.7 20.8 10.4 29.9 3.6 33.4 47.5 18.1 23.2 23.9 12. Avg re 62.1 54.3 70.1 56.9 53.6 38.2 56.8 40.1 55.1 61.5 45.4 38.7 52.5 44.6 53.8 56.4 60.6 56.0 66.0 F1 57.7 51.7 62.8 54.7 50. 28.0 33.7 32.0 25.6 31.1 17.0 33.8 6.8 38.2 50.5 27.3 33.6 33.5 21.2 3.3 Accuracy Evaluation Strategy For the accuracy evaluation of the models final results, we adopted the following approach: First, we used the GPT4o [30] model to extract the final answer from the models output CoT steps. For general question-answering tasks, GPT4o [30] was employed to evaluate whether the extracted final answer was correct based on human-annotated reference answers. For more specialized tasks such as VTG and TSG, we calculated the Intersection over Union (IoU) between the extracted final answer and the reference answer. Samples with an IoU greater than specified threshold were judged as correct. The IoU threshold was set to 0.7 for VTG tasks and 0.5 for TSG tasks."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiment Setup Evaluation Models. To thoroughly evaluate the effectiveness of VCR-Bench, we conducted assessments on multiple models. These include mainstream and powerful closed-source models such as Gemini (1.5 Pro, 2.0 Flash) [35, 33], GPT4o [30], o1 [31], and Claude 3.5 [2], as well as commonly used open-source models like InternVL2.5 (8B, 78B) [10, 9, 8], VideoLLaMA3 (7B) [50], LLaVA-OneVision (7B, 72B) [22], mPLUG-Owl3 (7B) [48], MiniCPM-o2.6 (7B) [47], Llama-3.2Vision (11B) [1], Qwen2.5-VL (7B, 72B) [3], LLaVA-Video (7B, 72B) [53], Aria (25B) [23], and InternVideo2.5 (8B) [39]. This essentially covers all the mainstream LVLMs currently available. Implementation Details. For models supporting direct video input, such as Gemini [35, 33], we processed the videos directly. For models currently without native video support (e.g., GPT-4o [30]), we extracted 64 frames per video with corresponding timestamp annotations, using multi-image input for evaluation. All other model parameters strictly followed official specifications. During inference, all models were required to answer questions step-by-step using our defined CoT prompt: \"Please provide step-by-step solution to the given question.\" All other prompts used during evaluation are provided in the Appendix A. 8 Table 3: Accuracy Evaluation Results for Different Models in VCR-Bench. The best results are bold and the second-best are underlined. Model Closed-Source Models Gemini-2.0-Flash Gemini-1.5-Pro o1 GPT-4o Claude 3.5 Sonnet Open-Source Models InternVL2.5-8B InternVL2.5-78B VideoLLaMA3-7B LLaVA-OneVision-7B LLaVA-OneVision-72B mPLUG-Owl3-7B MiniCPM-o2.6-8B Llama-3.2-11B-Vision Qwen2.5-VL-7B Qwen2.5-VL-72B LLaVA-Video-7B LLaVA-Video-72B Aria-25B InternVideo2.5-8B FTR VTC VTG VKR TSR VPA TSG Avg 66.2 55.1 66.7 54.7 45.3 32.7 40.9 44.7 35.8 47.8 13.2 31.4 4.4 37.1 45.0 47.2 49.7 45.3 40.9 51.2 45.3 52.2 49.1 46.3 29.8 39.8 36.6 34.8 42.2 6.2 30.4 4.3 26.7 39.9 36.6 49.1 45.0 43. 62.0 52.9 56.9 44.8 34.3 11.9 9.8 24.5 24.5 25.9 2.8 12.6 7.0 29.4 34.1 18.9 17.5 33.6 14.0 64.4 62.0 74.3 68.6 64.2 33.3 52.9 43.1 39.9 52.3 5.9 43.8 6.5 47.1 56.2 41.8 49.7 56.2 41.2 54.1 45.0 61.0 48.9 44.0 25.9 29.6 36.3 37.8 45.9 15.6 30.4 6.7 34.8 38.1 40.7 43.7 43.7 48. 58.1 45.6 60.2 57.6 49.3 30.9 39.6 39.6 41.0 38.1 7.2 38.1 5.8 36.0 48.9 40.3 43.2 38.8 41.7 4.2 0.7 0.0 2.8 0.7 0.7 0.0 0.7 0.0 0.0 0.0 0.0 0.0 0.7 2.1 0.0 0.0 2.8 0.0 51.7 44.0 56.7 46.9 41.0 23.9 30.9 32.5 30.7 36.4 7.3 26.9 4.9 30.4 37.9 32.5 36.6 38.2 33. 4.2 CoT Evaluation Results We first evaluated the output CoT steps of each model, and the experimental results are shown in Table 2. From the results, it can be observed that the quality of output CoT varies significantly across different models, and the overall CoT scores are not particularly high. Among them, the o1 [31] model, which focuses on strong reasoning capabilities, achieved the highest CoT scores in both the Perception and Reasoning dimensions, with comprehensive CoT score of 62.8, the highest among all models. Further analysis of the results leads us to the following conclusions: Closed-source models and large-scale parameter models possess stronger reasoning capabilities. As shown in the results of Table 2, the CoT evaluation CoT scores of common closed-source models are generally higher than those of open-source models. Additionally, for the same open-source model with different parameter sizes, such as Qwen2.5-VL 7B and 72B [3], the model with larger parameters achieves higher CoT score. This reflects that video CoT reasoning places high demands on the overall performance of LVLMs, and only models with larger parameters can ensure better step-by-step analysis and reasoning capabilities. more common issue that models encounter during multi-step reasoning is omission rather than inaccuracy. Experimental results demonstrate that most models achieve higher precision scores than recall scores. For some models with weaker CoT reasoning capabilities (e.g., LLaVA-Video-7B [53]), their outputs typically contain only one or two reasoning steps, which further widens this performance gap. This indicates that while the majority of the reasoning steps generated by the models are accurate and valid, there still exists significant omission of critical reasoning steps. The logical reasoning performance of the models is generally stronger than their visual perception performance. The models logical reasoning performance is generally stronger than their visual perception performance. Quantitative analysis of the table results demonstrates that their average reasoning capability (mean CoT score 42.5) surpasses their average perception ability (mean CoT score 33.5), with this performance gap being particularly pronounced among open-source models exhibiting performance deviations. This reveals that the current performance bottleneck of LVLMs in complex video reasoning tasks primarily lies in visual perception information extraction and comprehension. Table 4: Accuracy Evaluation Results for Different Durations. Table 5: Accuracy Evaluation Results under Different Settings. Model Closed-Source Models Gemini-2.0-Flash Gemini-1.5-Pro o1 GPT-4o Claude 3.5 Sonnet Open-Source Models InternVL2.5-8B InternVL2.5-78B VideoLLaMA3-7B LLaVA-OneVision-7B LLaVA-OneVision-72B mPLUG-Owl3-7B MiniCPM-o2.6-8B Llama-3.2-11B-Vision Qwen2.5-VL-7B Qwen2.5-VL-72B LLaVA-Video-7B LLaVA-Video-72B Aria-25B InternVideo2.5-8B Short Med Long Avg 44.2 37.4 53.6 44.4 39.8 20.7 30.4 30.2 29.2 35.1 6.1 27.5 5.3 27.1 33.4 31.7 35.5 36.4 31.5 60.3 49.9 61.3 48.7 42.2 25.7 30.5 38.2 33.4 40.6 9.9 26.0 5.1 34.0 42.8 33.4 40.6 39.9 35.0 53.5 48.7 54.7 49.7 41.4 28.3 32.6 26.7 28.9 31.0 4.8 26.7 3.7 31.6 39.8 32.6 38.5 39.6 32. 51.7 44.0 56.7 46.9 41.0 23.9 30.9 32.5 30.7 36.4 7.3 26.9 4.9 30.4 37.9 32.5 37.9 38.2 33.0 Model Closed-Source Models Gemini-2.0-Flash GPT-4o Claude 3.5 Sonnet Open-Source Models InternVL2.5-78B Qwen2.5-VL-72B Text 1 Frame Direct CoT 13.8 9.8 9.1 7.2 12.7 25.2 21.6 11.3 18.7 16.7 44.8 46.3 39. 51.7 46.9 41.0 35.4 42.7 30.9 37.9 Figure 5: Correlation between CoT Evaluation Results and Accuracy Evaluation Results. 4.3 Accuracy Evaluation Results As shown in Table 3, we evaluated the final answer accuracy of all models across different dimensions. Combined with the results from Table 2, we can draw the following conclusions: The CoT evaluation results are highly positively correlated with the final answer evaluation results. As shown in Figure 5, the experimental results demonstrate strong positive correlation (r=0.89) between models CoT reasoning quality and final answer accuracy. This robust relationship confirms that effective CoT reasoning is critical for successful video question answering, with higher-quality CoT steps consistently leading to more accurate final responses. Models with stronger instruction-following capabilities can achieve relatively higher CoT scores. closer examination of Figure 5 reveals that some models exhibit relatively high accuracy but low CoT scores, such as LLaVA-Video-7B [53] and LLaVA-OneVision-7B [22]. These models generally struggle to properly follow CoT instructionseven when provided with CoT prompts, their outputs remain overly concise, and their reasoning processes are insufficiently detailed, resulting in lower CoT scores. In contrast, models like Qwen2.5-VL [3], which demonstrate stronger instruction-following capabilities, produce more comprehensive reasoning chains, thus achieving comparatively higher CoT scores. The spatiotemporal grounding capabilities of the models are generally weak. The TSG task proves exceptionally challenging, with even the top model (Gemini-2.0-Flash [33]) achieving merely 4.2% accuracy, while many models fail completely. This stems from the tasks unique demands: (1) combined spatiotemporal reasoning (temporal localization + coordinate output), and (2) current models fundamental limitations in extracting precise spatial coordinates from video data. For concrete examples, please refer to Figure 7 in the Appendix B. 4.4 More Evaluation Results Accuracy Evaluation Results for Different Durations. We also statistically analyzed the models performance across videos of different durations, as shown in Table 4. The results indicate that the model generally achieves better performance on medium-length videos. In comparison, long videos contain more complex temporal information and richer content, which poses greater challenges for the models comprehension. As for short videos, since our dataset is primarily based on manual annotations and corrections, human annotators tend to find them easier to understand and are thus able to produce more in-depth and sophisticated annotations. Meanwhile, the model shows significant deficiencies in the TSG dimension, which mainly consists of short videos. This partially contributes to its weaker performance on short-form content. Accuracy Evaluation Results under Different Settings. To further validate the rationality of VCR-Bench, we conducted experiments under different settings, including: text-only input without video, text plus single frame extracted from video, and full text plus video with direct answering (without CoT), compared with our standard setup of full text plus video with CoT answering. As shown in Table 5, both the text-only and single-frame input settings lead to significant performance degradation, indicating that our question-answer data highly depend on video content and temporal information. Meanwhile, for stronger closed-source models, using CoT prompting results in higher accuracy than direct answering, whereas the opposite is true for weaker open-source models. This demonstrates that effective CoT reasoning heavily relies on the models overall capabilityonly models with sufficiently strong reasoning skills can fully benefit from CoT."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce VCR-Bench, the first benchmark specifically designed to evaluate the CoT reasoning capabilities of LVLMs in video understanding tasks. Our benchmark comprises high-quality dataset of 859 videos and 1,034 QA pairs spanning seven distinct task types, each annotated with rigorous CoT reasoning references. We propose novel evaluation framework that assesses reasoning quality through recall, precision, and their harmonic mean (F1 score). Comprehensive evaluations reveal significant limitations in current LVLMs, with even the top-performing o1 model achieving only 62.8 CoT score and most open-source models scoring below 40, highlighting substantial room for improvement in video-grounded reasoning. VCR-Bench establishes standardized framework to advance research in this critical area."
        },
        {
            "title": "References",
            "content": "[1] AI@Meta. Llama 3 model card, 2024. [2] Anthropic. family: https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, 2024. sonnet, claude model haiku. Opus, The 3 [3] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. [5] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [6] L. Chen, X. Wei, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, Z. Tang, L. Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024. [7] Q. Chen, L. Qin, J. Zhang, Z. Chen, X. Xu, and W. Che. M3 cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473, 2024. [8] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [9] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [10] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 11 [11] D. Cores, M. Dorkenwald, M. Mucientes, C. G. Snoek, and Y. M. Asano. Tvbench: Redesigning video-language evaluation. arXiv preprint arXiv:2410.07752, 2024. [12] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [13] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] Z. Guo, R. Zhang, H. Chen, J. Gao, P. Gao, H. Li, and P.-A. Heng. Sciverse. https://sciverse-cuhk.github.io, 2024. [15] S. Han, W. Huang, H. Shi, L. Zhuo, X. Su, S. Zhang, X. Zhou, X. Qi, Y. Liao, and S. Liu. Videoespresso: large-scale chain-of-thought dataset for fine-grained video reasoning via core frame selection. arXiv preprint arXiv:2411.14794, 2024. [16] C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, J. Liu, L. Qi, Z. Liu, and M. Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. [17] W. Hong, W. Wang, M. Ding, W. Yu, Q. Lv, Y. Wang, Y. Cheng, S. Huang, J. Ji, Z. Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. [18] K. Hu, P. Wu, F. Pu, W. Xiao, Y. Zhang, X. Yue, B. Li, and Z. Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. [19] D. Jiang, R. Zhang, Z. Guo, Y. Li, Y. Qi, X. Chen, L. Wang, J. Jin, C. Guo, S. Yan, et al. Mmecot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. [20] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. C. Niebles. Dense-captioning events in videos. In International Conference on Computer Vision (ICCV), 2017. [21] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [22] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P. Zhang, Y. Li, Z. Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [23] D. Li, Y. Liu, H. Wu, Y. Wang, Z. Shen, B. Qu, X. Niu, F. Zhou, C. Huang, Y. Li, et al. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024. [24] K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [25] B. Lin, Y. Ye, B. Zhu, J. Cui, M. Ning, P. Jin, and L. Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [26] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [27] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [28] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [29] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arxiv, 2024. [30] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. [31] OpenAI. Introducing openai o1, 2024., 2024. 12 [32] V. Patraucean, L. Smaira, A. Gupta, A. Recasens, L. Markeeva, D. Banarse, S. Koppula, M. Malinowski, Y. Yang, C. Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. [33] S. Pichai, D. Hassabis, and K. Kavukcuoglu. Introducing gemini 2.0: our new ai model for the agentic era, 2024. [34] Y. Tang, D. Ding, Y. Rao, Y. Zheng, D. Zhang, L. Zhao, J. Lu, and J. Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12071216, 2019. [35] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [36] O. Thawakar, D. Dissanayake, K. More, R. Thawkar, A. Heakl, N. Ahsan, Y. Li, M. Zumri, J. Lahoud, R. M. Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. [37] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y. Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. [38] Y. Wang, K. Li, X. Li, J. Yu, Y. He, G. Chen, B. Pei, R. Zheng, Z. Wang, Y. Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pages 396416. Springer, 2024. [39] Y. Wang, X. Li, Z. Yan, Y. He, J. Yu, X. Zeng, C. Wang, C. Ma, H. Huang, J. Gao, M. Dou, K. Chen, W. Wang, Y. Qiao, Y. Wang, and L. Wang. Internvideo2.5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. [40] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [41] H. Wu, D. Li, B. Chen, and J. Li. Longvideobench: benchmark for long-context interleaved videolanguage understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. [42] J. Xiao, X. Shang, A. Yao, and T.-S. Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97779786, 2021. [43] C. Xu, X. Hou, J. Liu, C. Li, T. Huang, X. Zhu, M. Niu, L. Sun, P. Tang, T. Xu, et al. Mmbench: Benchmarking end-to-end multi-modal dnns and understanding their hardware-software implications. In 2023 IEEE International Symposium on Workload Characterization (IISWC), pages 154166. IEEE, 2023. [44] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653, 2017. [45] L. Xu, Y. Zhao, D. Zhou, Z. Lin, S. K. Ng, and J. Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. [46] C. Yan, H. Wang, S. Yan, X. Jiang, Y. Hu, G. Kang, W. Xie, and E. Gavves. Visa: Reasoning video object segmentation via large language models. arXiv preprint arXiv:2407.11325, 2024. [47] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [48] J. Ye, H. Xu, H. Liu, A. Hu, M. Yan, Q. Qian, J. Zhang, F. Huang, and J. Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. In The Thirteenth International Conference on Learning Representations, 2024. [49] Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao. Activitynet-qa: dataset for understanding In Proceedings of the AAAI Conference on Artificial complex web videos via question answering. Intelligence, volume 33, pages 91279134, 2019. [50] B. Zhang, K. Li, Z. Cheng, Z. Hu, Y. Yuan, G. Chen, S. Leng, Y. Jiang, H. Zhang, X. Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 13 [51] H. Zhang, X. Li, and L. Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. [52] R. Zhang, D. Jiang, Y. Zhang, H. Lin, Z. Guo, P. Qiu, A. Zhou, P. Lu, K.-W. Chang, P. Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? ECCV 2024, 2024. [53] Y. Zhang, J. Wu, W. Li, B. Li, Z. Ma, Z. Liu, and C. Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [54] Y. Zhao, L. Xie, H. Zhang, G. Gan, Y. Long, Z. Hu, T. Hu, W. Chen, C. Li, J. Song, et al. Mmvu: Measuring expert-level multi-discipline video understanding. arXiv preprint arXiv:2501.12380, 2025. [55] Y. Zhao, Y. Zeng, Y. Qi, Y. Liu, L. Chen, Z. Chen, X. Bao, J. Zhao, and F. Zhao. V2p-bench: Evaluating video-language understanding with visual prompts for better human-model interaction. arXiv preprint arXiv:2503.17736, 2025. [56] J. Zhou, Y. Shu, B. Zhao, B. Wu, S. Xiao, X. Yang, Y. Xiong, B. Zhang, T. Huang, and Z. Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024."
        },
        {
            "title": "A Prompt Template",
            "content": "Recall Evaluation Prompt You are an expert system for verifying solutions to video-based problems. Your task is to match the ground truth middle steps with the provided solution. INPUT FORMAT: 1. Problem: The original question/task 2. Solution of model 3. Ground Truth: Essential steps required for correct answer MATCHING PROCESS: You need to match each ground truth middle step with the solution: Match Criteria: - The middle step should exactly match in the content or is directly entailed by certain content in the solution - All the details must be matched, including the specific value and content - You should judge all the middle steps for whether there is match in the solution Step Types: 1. Logical Inference Steps - Contains exactly one logical deduction - Must produce new derived conclusion - Cannot be just summary or observation 2. Video Description Steps - Pure visual observations - Only includes directly visible elements - No inferences or assumptions - Contains event time OUTPUT FORMAT: JSON array of judgments: [ { } \"step\": ground truth middle step, \"step_type\": \"Video Description StepsLogical Inference Steps\", \"judgment\": \"Matched\" \"Unmatched\" ] ADDITIONAL RULES: 1. Only output the json array with no additional information. 2. Judge each ground truth middle step in order without omitting any step. Here is the problem, answer, solution, and the ground truth middle steps: [Problem]: {question} [Answer]: {answer} [Solution]: {solution} Precision Evaluation Prompt Given solution with multiple reasoning steps for video-based problem, reformat it into well-structured steps and evaluate their correctness. 15 Step 1: Reformatting the Solution Convert the unstructured solution into distinct reasoning steps while: - Preserving all original content and order - Not adding new interpretations - Not omitting any steps Step Types 1. Logical Inference Steps - Contains exactly one logical deduction - Must produce new derived conclusion - Cannot be just summary or observation 2. Video Description Steps - Pure visual observations - Only includes directly visible elements - No inferences or assumptions - Contains event time 3. Background Review Steps: - Repetition or review of the problem - Not directly related to solving the problem. Step Requirements - Each step must be atomic (one conclusion per step) - No content duplication across steps - Initial analysis counts as background information - Final answer determination counts as logical inference Step 2: Evaluating Correctness Evaluate each step against: Ground Truth Matching For video descriptions: - Key elements must match ground truth descriptions For logical inferences: - Conclusion must EXACTLY match or be DIRECTLY entailed by ground truth For Background review: - Without special circumstances are deemed to be redundant Reasonableness Check (if no direct match) If Step: - Premises must not contradict any ground truth or correct answer - Logic is valid - Conclusion must not contradict any ground truth - Conclusion must support or be neutral to correct answer - Helpful in solving the problem, non-redundant steps this Step be viewed as matched. Judgement Categories - \"Match\": Aligns with ground truth - \"Wrong\": Contradictory with ground truth - \"Redundant\": Redundant steps that do not help solve the problem Output Requirements 1. The output format MUST be in valid JSON format without ANY other content. 2. For highly repetitive patterns, output it as single step. 3. Output maximum 35 steps. Always include the final step that contains the answer. 16 Output Format [ { } ] \"step\": \"reformatted the solution step\", \"step_type\": \"Video Description StepsLogical Inference Steps Background Review Steps\", \"reasons_for_judgment\": \"The reason for judging...\", \"judgment\": \"MatchedWrongRedundant\" Input Data [Problem]: {question} [Solution]: {solution} [Ground Truth Information]: {gt_annotation} Answer Extraction Prompt You are an AI assistant who will help me to extract an answer of question. You are provided with question and response, and you need to find the final answer of the question. Extract Rule: [Multiple choice question] 1. The answer could be answering the option letter or the value. You should directly output the choice letter of the answer. 2. You should output single uppercase character in A, B, C, D, E, F, G, H, (if they are valid options), and Z. 3. If the answer is about certain time period, such as from 1 minute 30 seconds to 2 minutes 30 seconds, it should be given in the format [90, 150]. 4. If the meaning of all options are significantly different from the final answer, output Z. [Non Multiple choice question] 1. Output the final value of the answer. It could be hidden inside the last step of calculation or inference. Pay attention to what the question is asking for to extract the value of the answer. 2. The final answer could also be short phrase or sentence. 3. If the response doesnt give final answer, output Z. Output Format: Directly output the extracted answer of the response Example 1: Question: What is the main object in image? Options: A. teddy bear B. rabbit C. cat D. dog Response: cute teddy bear Your output: Example 2: Question: What is the main object in image? Options: A. teddy bear B. rabbit C. cat D. dog Answer: Spider Your output: Example 3: Question: On flat playground, choose Cartesian Oxy coordinate system (in units of meters). child running at constant velocity = 1 m/s around heart-shaped path satisfies the following 17 order-6 algebraic equation: (cid:0)x2 + y2 L2(cid:1)3 Lx2y3 = 0, = 10. Response: Given the radius of curvature 10 m: Using the radius of curvature: (1 s1)2 10 The magnitude of the acceleration is the sum of tangential and centripetal components. Since tangential acceleration is zero (constant speed): = 0.1 s2 2 ac = = Magnitude of acceleration = ac = 0.1 Your output: 0.1 s2 Example 4: Question: Given the sequence of images, please identify the image consistent with the text description: Clean And Jerk. A. <image1> B. <image2> C. <image3> D. <image4>, Response: Based on the temporal analysis, Answer: The second image Your output: Example 5: Question: Please specify the start and end time of the event An intro screen appears with gray background and the red and white words that say PRO TRAINING. Response: The start and end time of the event An intro screen appears with gray background and the red and white words that say PRO TRAINING. are as follows: Step 1: Identify the start time of the event. The video starts at 00:00. The intro screen is visible from the very beginning. Start time: 00:00 Step 2: Identify the end time of the event. The intro screen starts to disappear and transition to the next scene at 00:11. By 00:12, the intro screen is completely gone. So, the intro screen is visible until 00:11. End time: 00:11 Therefore, the event An intro screen appears with gray background and the red and white words that say PRO TRAINING starts at 00:00 and ends at 00:11. Your output: [0,11] Example 6: Question: In the diagram, OA = 15, OP = 9 and = 4. Determine the equation of the line through and B. Explain how you got your answer. Response: The equation of the line through and is = 1 To find the equation of the line through and B, we first need to find the slope of the line. The slope of line passing through two points (x1, y1) and (x2, y2) is given by the formula: 3 + 12. Slope = y2 y1 x2 x1 Using the coordinates of points A(0, 15) and B(4, 0), we can calculate the slope as follows: Slope = 0 15 4 0 = 15 = 15 4 Now that we have the slope, we can use the point-slope form of linear equation to find the equation of the line. The point-slope form is given by: y1 = m(x x1) 18 where (x1, y1) is point on the line and is the slope. In this case, we can use point A(0, 15) and the slope 15 4 : Simplifying the equation, we get: 15 = 15 4 (x 0) 15 = 15 = 15 4 + 15 Therefore, the equation of the line through and is = 15 Your output: = 15 4 + 4 + 15. Example 7: Question: <image> <image> <image> <image> <image> <image> <image> <image> <image>In the paper An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion, what aspect of the proposed method does this figure aim to demonstrate? Response: the potential of this 3D generation paradigm to generate objects with PBR materials Your output: the potential of this 3D generation paradigm to generate objects with PBR materials [Question]: {question} [Answer]: {response} [Your output]: Answer Scoring Prompt You are an AI assistant who will help me to judge whether two answers are consistent. Input Illustration: [Standard Answer] is the standard answer to the question [Model Answer] is the answer extracted from models output to this question. Task Illustration: Determine whether [Standard Answer] and [Model Answer] are consistent. Consistent Criteria: [Multiple-Choice questions] 1. If the [Model Answer] is the option letter, then it must completely matches the [Standard Answer]. 2. If the [Model Answer] is not an option letter, then the [Model Answer] must completely match the option content of [Standard Answer]. [Nan-Multiple-Choice questions] 1. The [Model Answer] and [Standard Answer] should exactly match. 2. If the meaning is expressed in the same way, it is also considered consistent, for example, 0.5m and 50cm. Output Format: 1. If they are consistent, output 1; if they are different, output 0. 2. DIRECTLY output 1 or 0 without any other content. Example 1: Question: What is the main object in image? Options: A. teddy bear B. rabbit C. cat D. dog [Model Answer]: cute teddy bear [Standard Answer]: Your output: 1 19 Example 2: Question: Find the value of AB. Choices: A.1; B.5; C.9; D.10 [Model Answer]: 5 [Standard Answer]: Your output: 1 Example 3: Question: Three of the following four slides are from the same presentation, but one is from different one. Please identify the outlier: <image> <image> <image> <image> [Model Answer]: the forth image [Standard Answer]: the third image Your output: 0 [Question]: {question} [Model Answer]: {extract answer} [Standard Answer]: {gt answer} Your output:"
        },
        {
            "title": "B Error Analysis",
            "content": "Figure 6: Common error examples of the model. The top image shows reasoning failures from missing critical visual features, while the bottom image demonstrates excessive inferences beyond the video content, leading to incorrect answers. 21 Figure 7: Common error examples of the model. The top example shows incorrect reasoning due to the models lack of domain-specific knowledge, while the bottom one highlights significant deviations caused by poor spatiotemporal localization capabilities. 22 Figure 8: Common error examples of the model (Correct result, wrong process). The top example shows the model incorrectly located the initial position of the object (cat) to be identified, while the bottom one shows the model mistakenly identified the person inside the car in the video as the standing person mentioned in the question."
        },
        {
            "title": "C More Qualitative Examples",
            "content": "Figure 9: Examples of FTR. 24 Figure 10: Examples of VTC. 25 Figure 11: Examples of TSG. Figure 12: Examples of VPA. 27 Figure 13: Examples of TSR. 28 Figure 14: Examples of VKR. Figure 15: Examples of VTG."
        }
    ],
    "affiliations": [
        "East China Normal University",
        "Huawei Noahs Ark Lab",
        "University of Science and Technology of China"
    ]
}