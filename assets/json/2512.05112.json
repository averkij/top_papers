{
    "paper_title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "authors": [
        "Dongzhi Jiang",
        "Renrui Zhang",
        "Haodong Li",
        "Zhuofan Zong",
        "Ziyu Guo",
        "Jun He",
        "Claire Guo",
        "Junyan Ye",
        "Rongyao Fang",
        "Weijia Li",
        "Rui Liu",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 2 1 1 5 0 . 2 1 5 2 : r DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept"
        },
        {
            "title": "Generation",
            "content": "Dongzhi Jiang1, Renrui Zhang1(cid:66), Haodong Li4, Zhuofan Zong1, Ziyu Guo2, Jun He3, Claire Guo5, Junyan Ye3, Rongyao Fang1, Weijia Li3, Rui Liu1(cid:66), Hongsheng Li1(cid:66) 1CUHK MMLab 2CUHK IMIXR 3Sun Yat-Sen University 4SCUT 5CUHK (Shenzhen) {dzjiang, renruizhang}@link.cuhk.edu.hk Equal Contribution Project Leader (cid:66)Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chainof-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the models inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with superresolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute To support training, we curate DraCocombinations. 240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT. The project is at https://github.com/CaraJ7/DraCo. 1. Introduction Recently, unified multimodal large language models (MLLMs) [3, 8, 27, 42, 50, 52, 60, 6264, 77] have emerged Figure 1. Conceptual Comparison of CoT Reasoning for T2I Generation. (b) Employing (a) Generation without reasoning. exterior reward models to guide generation. (c) Generating Text CoT before producing image. (d) DraCo: Producing visual draft for detailed planning and verify it with text reasoning, then correct and refine the draft for final output. as powerful architectures that integrate both visual understanding and generation capabilities, as exemplified by recent works such as Bagel [8], EMU3 [57], and Janus [60]. By consolidating these two abilities within single framework, unified MLLMs have demonstrated remarkable performance and exhibit emergent properties that models possessing only one ability fail to acquire. For instance, unified Figure 2. Visualization of DraCo Output. For each example, the larger image represents the final output, while the smaller image is the visual draft. The corresponding prompt is located in the corner of each set. MLLMs can understand interleaved context including both images and texts and subsequently produce images according to the input instruction [61, 66]. Concurrently, advances in chain-of-thought (CoT) reasoning [24, 58] have shown remarkable success across various domains, including mathematical problems [1, 18, 32, 72, 73], visual reasoning [4, 22, 34, 70, 71], and multi-agent systems [33, 59]. While several pioneering works have extended CoT to text-to-image (T2I) generation tasks [8, 14, 17, 21, 38, 75] on unified MLLMs, their exploration has not fully exploited the unified architecture of MLLMs. For example, Image-Gen-CoT [17] leverages reward model to evaluate the quality potential of an image during the early generation stage. This paradigm treats unified MLLM merely as text-to-image generator, only employing its image generation capabilities. Later, several methods [8, 21] have been proposed to generate textual reasoning for the given prompt prior to image synthesis. However, for generating dense modality like an image, only planning with text offers too vague and coarse guidance. Besides, only textual understanding ability is leveraged for CoT. This raises natural question: Can we design CoT mechanism with both textual and visual content as better planning and verifier for improved text-to-image generation? To answer this question, we propose Draft-as-CoT (DraCo), an interleaved CoT reasoning approach that fully leverages the unified framework for the T2I task. Our method first generates low-resolution draft image as visual planning, then leverages the models inherent understanding capability to verify the draft with any semantic misalignments, and ultimately refines and enriches the draft through selective correction and super-resolution. We show the comparison of DraCo with previous methods in Fig. 1. Our method is motivated by two key limitations of textual reasoning for T2I generation. First, text is too abstract to design every aspect of an image in detail, especially on low-level features like appearance and styles. In contrast, planning with draft image could sufficiently display all the essential visual information during planning. Second, reasoning with draft image provides an opportunity to preview the image to generate. This addresses fundamental challenge where current T2I models suffer: difficulty in producing the correct image in one pass. Specifically, due to the natural distribution of real-world data, rare attributes or object combinations are significantly underrepresented in training datasets [5, 46]. For example, when prompted with white orange, models often struggle because they have learned to strongly associate the object orange with its typical color attribute rather than treating them as independent concepts. This binding leads to systematic failures in generating unusual yet valid attribute combinations. How2 Figure 3. Framework of DraCo. DraCo contains three steps for generation: draft sketching, draft verification, and corrective refinement. ever, by previewing the draft, we do not force the model to directly generate the perfect image. Instead, we make the unified MLLM itself identify and refine its flawed planning for the final output. To facilitate our proposed DraCo reasoning paradigm, the unified MLLM must be capable of identifying mistakes and precisely controlling and operating on drafts to ensure successful corrections. Since no existing dataset provides such capabilities, we carefully curate training dataset, DraCo-240K, targeting three atomic correction capabilities. For each capability, we design rigorous data synthesizing pipelines that full exploits the synergy between the MLLM [53], advanced editing models [25], and segmentation models [44]. Additionally, how to conduct classifierfree guidance (CFG) [19] for interleaved reasoning remains an open question. We propose DraCo-CFG, specific type of CFG for DraCo to explicitly strengthen the two major conditions for final generation: visual semantics from the draft image and correction instructions from verification. Experimental results show that DraCo significantly enhances strong baseline, Bagel, achieving an 8% improvement on GenEval [13] and surpassing the text-CoT-based method by 4%. We find that DraCo consistently demonstrates superior performance on more challenging benchmarks such as ImagineBench and GenEval++ [69]. The visualization output of DraCo is shown in Fig. 2. In summary, our contributions are as follows: novel interleaved reasoning paradigm: We introduce Draft-as-CoT, new interleaved reasoning mechanism that incorporates detailed visual planning and preview for rare concept and combination generation. comprehensive training dataset with automated pipeline: We design and construct DraCo-240K to empower the model with more precise control and diverse operations to correct the drafts following verifications. CFG method for interleaved reasoning: We present CFG strategy designed specifically for DraCo, enabling explicit emphasis on different conditioning signals. 2. Method We first introduce the structure of our adopted unified MLLM, Bagel, in Section 2.1. Then we elaborate on the design idea and pipeline of DraCo in Section 2.2. Finally, we introduce the construction of DraCo-240K in Section 2.3. 2.1. Preliminary Our work is built upon popular unified MLLM, Bagel, which preserves both visual generation capability, including text-to-image generation and image editing, and visual understanding capability. Bagel is composed of three main components: ViT vision encoder [54] for understanding, VAE encoder [23] for generation, and two transformers [55] forming Mixture-of-Transformer-Experts (MoT). One branch of transformer specifically handles VAE tokens, while another branch processes ViT tokens and text tokens. For visual understanding tasks, an image is first encoded by ViT into sequence of image tokens, which are then fed into the transformer to autoregressively generate the text tokens. For visual generation tasks, Bagel adopts the Rectified Flow [10, 28, 31] method to produce VAE tokens. 2.2. Draft-as-CoT We present the pipeline of DraCo in Fig. 3. The entire process can be split into three steps: draft sketching, draft verification, and corrective refinement. 3 Draft Sketching. The goal of the first step in DraCo is to form sketch of the specified semantics in the prompt, including foregrounds like objects with their attributes and relations, and holistic information like backgrounds and image styles. Bagel is typically employed to generate highresolution images, such as 10241024. While these images exhibit excellent details, such granularity is unnecessary for the visual planning stage. Since models often struggle to directly generate images that align with the prompt, subsequent modifications of the initially generated images are still required. Therefore, we only need the model to first present the basic semantics of the intended final image, with other visual details to be supplemented in later process. Consequently, we propose using Bagel to generate lowerresolution images in the first step, such as 384384. This resolution is sufficiently small to ensure efficient draft generation compared to generating 10241024 images, while being large enough to explicitly display all semantic information, particularly smaller objects. Thus, in the first step of DraCo, we only input prompt to generate lowresolution draft image. Draft Verification. Leveraging the exceptional visual understanding capabilities inherent in unified MLLMs, we encode the generated draft image through the ViT encoder and re-input it into the unified MLLM. We then instruct the unified MLLM to generate draft verification v: first comprehensively understanding the image content, then comparing it with the prompt, and if misalignments are detected, finally summarizing what edits should be applied to the image. Notably, in the original design of Bagel for understanding interleaved context, such as in editing tasks, it simultaneously inputs both ViT and VAE features. The ViT features facilitate understanding of high-level information, while VAE features maintain low-level details. In contrast, we propose inputting only the ViT features of the draft image without the VAE features. This design choice stems from the fundamental difference between our task and editing tasks. Editing tasks operate at the same resolution, modifying corresponding elements according to editing instructions while preserving all other information unchanged. Under this requirement, the model requires low-level information to maintain image consistency. Conversely, our design focuses solely on the high-level semantics of the draft image. We could ignore details of irrelevant elements and even irrelevant aspects of prompt-related objects. Dropping the VAE feature could eliminate the constraints from the low-level feature and facilitate more substantial changes. Corrective Refinement with DraCo-CFG. Eventually, the unified MLLM is required to generate the final highresolution, detail-rich image aligned with the prompt. This involves both upscaling the draft with improved details and correcting any mistakes detected in the verification. However, since multiple conditions exist during final image generation (prompt, draft image, and verification), notable question arises regarding how to organize classifier-free guidance (CFG) to adequately emphasize different conditions in the multimodal context. We identify two core conditions that should control final image generation: one from the draft images visual condition, constraining the final image to maintain relevant semantic consistency with the draft, and another from the prompt and self-reflection, constraining the image to be modified according to the verification and generated following the prompt. Therefore, with unified model and noise xt, we design three forms of multimodal context for CFG: the unconditional input m(ϕ, ϕ, ϕ), the draft-only input m(ϕ, vit, ϕ), and the fully-conditioned input m(p, vit, v). Notably, following our CFG design, we specifically incorporate m(ϕ, vit, ϕ) during training, where we train the model to output high-resolution version of the draft image. Concretely, we employ the following CFG formulation during inference: ˆm(p, vit, v) = m(ϕ, ϕ, ϕ) + sdraft (m(ϕ, vit, ϕ) m(ϕ, ϕ, ϕ)) (cid:125) (cid:124) (cid:123)(cid:122) draft condition + stext (m(p, vit, v) m(ϕ, vit, ϕ)) (cid:125) (cid:123)(cid:122) prompt and correction condition (cid:124) (1) . 2.3. DraCo-240K The correction capability is the most critical among all atom capabilities in DraCo, integrating super-resolution and advanced editing to generate final images. We first introduce the necessity of constructing the dataset in Section 2.3.1. Then we detail the two stages with generation pipeline for each capability in Section 2.3.2. Finally, we illustrate how to organize the collected data for training in Section 2.3.3. 2.3.1. Necessity of Dataset Construction Our zero-shot pilot study reveals that Bagel struggles with correction in two key ways: (1) Despite receiving correct verification, the model fails to follow correction instructions. Despite its inherent editing capability, the model still lacks the precise object control and unconventional editing approaches needed for correcting flawed drafts, like image layout adjustment. (2) Rather than adhering to draft semantics, Bagel generates entirely new images. This undermines our core motivation of using drafts to reduce one-shot generation difficulty. Existing datasets cannot address these challenges. Therefore, we curate training dataset for DraCo with over 240K interleaved reasoning instances. We identify three essential atomic capabilities required for correction and then design data curation pipeline for each one. Throughout data 4 Figure 4. Construction Pipeline and Examples of DraCo-240K. We design specialized data pipelines for each of the three atomic correction capabilities: general correction, instance manipulation, and layout reorganization. We then employ Qwen3-VL [53] to generate prompts and verifications based on the collected image pairs. Finally, we organize the data into two categories for training: corrections needed and corrections not needed. construction, we ensure semantic correlation between draft and final images, ensuring the model to learn to modify only the erroneous elements and retain correct ones. 2.3.2. Construction Details As illustrated in Fig. 4, the dataset construction consists of two stages: The first stage collects image pairs where the two images differ in some semantic aspects but maintain overall consistency. These images serve as the draft and final images during training. The second stage generates the textual prompt and verification based on the image pairs. We detail the three pipelines in the first stage below: General Correction. Existing editing datasets are natural sources for general correction data. We denote the pre-editing image as A-Image and the post-editing image as B-Image. The difference between them exists only in the edited objects, while all other characteristics remain identical, perfectly satisfying our data requirements. Instance Manipulation. Instance Manipulation refers to finer-grained control over objects in the image, which is crucial for correction. For example, if the prompt specifies generating three dogs but the model generates five, the verification will likely require removing the two dogs on the left. However, we observe that Bagel struggles to edit instances of the same category, often leading to misinterpretations or incorrect modifications. To address this, we design an automated data pipeline for similar scenarios. We first synthesize numerous prompts specifying multiple instances of the same object, then use Bagel to generate multiple images for each prompt. Next, we extract the objects in the prompt to GroundingDINO [30] to obtain their bounding boxes. Then, we randomly select some of them and mask them with black rectangles. We input this masked image to FLUX-Kontext [25], instructing it to inpaint all black masks using the background, thereby obtaining the B-Image. This yields image pairs with multiple identical objects but different quantities. Additionally, we leverage the RefEdit dataset [39]. Due to its relatively poor quality, we use GPT-4o [37] to regenerate the edited images. These image pairs feature multiple identical objects, but one object differs in certain attributes across the images. Figure 5. Detailed Visualization of DraCo Output. We showcase the prompt, verification, draft (smaller image), and final output (larger image). DraCo successfully identifies the misalignment within the draft and conducts the correction based on the suggested modification. Layout Reorganization. As for image layout understanding, we find that Bagel lacks the ability to comprehend spatial relationships in editing instructions, let alone modify them for correction. Therefore, we specifically design subset for spatial correction. Similarly, we use prompts specifying spatial layouts to have Bagel generate numerous images. We then feed the objects from the prompt into GroundedSAM [44] to obtain segmentation masks for each object. We randomly swap the positions of these segmentation masks and paste them onto pure black background. Subsequently, if there is no overlap with the original masks, we paste the periphery of the original image onto the black background to make the newly generated images background as consistent as possible with the original image. Finally, we use FLUX-Kontext to inpaint the black regions based on the background around the images periphery, obtaining B-Image, where the number and appearance of each object remains same but the layout is changed. In the second stage, we randomly swap A-Image and B-Image in the image pairs and input them to Qwen3-VL235B-A22B [53]. The first task is to output caption of BImage as the prompt for training, ensuring it is misaligned with A-Image. The second task is to generate the verification, namely, if text-to-image model generates A-Image given the prompt from task one, how should it be modified to become B-Image. The verification needs to first observe the misalignments between the current image and prompt, and if any exist, provides the modification approach. 2.3.3. Training Set these two types of data, as well as text-to-image data. The text-to-image data is trained on two resolutions: low resolution, same as the draft, and high resolution, same as the final output. This is to ensure the model produces meaningful draft images and preserves its original generation capability. More details and examples of each capability are shown in the Appendix C. 2.4. Training Loss We conduct supervised fine-tuning from Bagel. For our curated training dataset, we sequentially input the prompt tokens, the ViT feature of draft images, the verifications, and finally the noisy VAE tokens of the final image. We only calculate Binary Cross Entropy (BCE) loss on the verifications and Mean Squared Error (MSE) on the VAE tokens: Lverification = (cid:88) log(vi), 1 i=1 m(t, xt) (x1 x0)2(cid:105) (cid:104) Lfinal image = Et,x0,x (2) . (3) To support DraCo-CFG, we apply two dropout strategies during training, each with 5% probability: (1) dropping all conditions for unconditional generation, or (2) preserving only the ViT feature, as mentioned in Section 2.2. 3. Experiment 3.1. Experimental Setting Finally, we organize the image pairs, the prompt, and the verification outputs from Qwen3-VL into two types of data: modification needed (A-Image as draft) and no modification needed (B-Image as draft). During training, we utilize Evaluation. We evaluate our proposed method on and ImagineBench [69]. GenEval GenEval consists of six subcategories focusing on object existence, attributes, counting, and positions. Building [13], GenEval++, 6 Table 1. Evaluation results on GenEval. The best results are in bold fonts with the second best underlined. Method Single Obj. Two Obj. Counting Colors Position Color Attri. Overall LlamaGen [35] SDXL [40] DALL-E 3 [2] SD3-Medium [48] Show-o [65] Janus-Pro-7B [6] BLIP3-o 8B [3] BAGEL [8] Show-o+PARM [17] T2I-R1 [21] BAGEL-Think [8] DraCo (Ours) 0.71 0.98 0.96 0. 0.95 0.99 - 0.99 0.98 0.99 0.99 1.00 Generation Only 0.21 0.39 0.47 0.72 Unified MLLM 0.49 0.59 - 0.81 Unified MLLM w/ CoT 0.54 0.53 0.81 0.81 0.58 0.85 0.83 0.89 0.82 0.90 - 0. 0.83 0.91 0.88 0.91 0.34 0.74 0.87 0.94 0.52 0.89 - 0.94 0.55 0.91 0.94 0. 0.07 0.15 0.43 0.33 0.11 0.79 - 0.64 0.13 0.76 0.64 0.70 0.04 0.23 0.45 0.60 0.28 0.66 - 0. 0.29 0.65 0.63 0.76 0.32 0.55 0.67 0.74 0.53 0.80 0.84 0.78 0.55 0.79 0.82 0. upon this benchmark, GenEval++ presents more challenging scenarios with complex combinations and stricter scoring criteria. Additionally, we test on ImagineBench, which specifically targets unusual object-attribute combinations where text-to-image models typically struggle. We generate one image per prompt for evaluation, due to the slow inference speed of Bagel and follow the official scoring method in each benchmark. During our evaluation, sdraf and stext are set to 2 to 6, respectively. Training Details. We empirically observe that Bagel struggles to generate valid 384384 images. Therefore, we first conduct text-to-image fine-tuning stage to empower the model with low-resolution image generation capability before training for DraCo. Additional details are provided in the Appendix D. Subsequently, we perform fullparameter fine-tuning for 16K steps and retain the EMA weights. The learning rate is set to 2e-5 with 2K warmup steps. We freeze the ViT encoder and its connector during training to preserve valid visual encoding of high-level information. For each training iteration, the maximum length of concatenated training samples is set to 36K per GPU. We utilize 8 H800 GPUs for training. MLLMs, and unified MLLMs with CoT planning. Our method substantially outperforms other methods with CoT across all three benchmarks. On GenEval, DraCo achieves an overall score of 0.86 and attains the highest scores in five out of six subtasks. Notably, DraCo excels particularly in the color attribute subtask, which involves objects with different specified colors. This demonstrates that DraCo is especially effective at handling complex attribute combinations. As shown in the table, the exterior reward model PARM [17] brings 2% improvement on Showo [65], from 0.53 to 0.55. However, our method showcases 6% greater improvement based on more strong baseline starting from 0.78. On ImagineBench, compared to the baseline Bagel without reasoning, DraCo achieves notable improvement of 0.91 points. DraCo also surpasses textonly reasoning by 0.18 points, indicating that visual drafting and preview are more effective for generating rare attribute combinations. Furthermore, DraCo also demonstrates advantages on the more challenging GenEval++ with overall score of 0.40. Interestingly, Bagel-Think underperforms vanilla Bagel, suggesting potential limitations of text-based planning. In contrast, DraCo consistently delivers superior performance across all benchmarks. 3.2. Main Results 3.3. Ablation Study We present quantitative results in Tables 1 and 2, with qualitative results shown in Figures 2 and 5. We compare our method against generation-only models (including diffusion-based and autoregressive models), unified Effectiveness of DraCo-CFG. We demonstrate the effectiveness of our proposed DraCo-CFG by comparing it with the original CFG method adopted by Bagel, with details provided in the Appendix B. As shown in Table 3 and Fig. 6, 7 Table 2. Evaluation results on Imagine-Bench and GenEval++. The best results are in bold fonts with the second best underlined. Method Attr. shift Spatiotem. Hybrid. Multi-Obj. Overall Color Count Color/Count Color/Pos Pos/Count Pos/Size Multi-Count Overall Imagine-Bench GenEval++ SDv2.1 [45] SDXL [40] FLUX.1-dev [25] Janus-Pro-7B [6] BLIP3-o 4B [3] BLIP3-o 8B [3] Bagel [8] T2I-R1 [21] Bagel-Think [8] DraCo (Ours) 4.46 4.42 5.68 5.30 5.48 5.80 5.37 5.85 6. 6.40 5.06 6.32 7.13 7.28 6.79 7.08 6.93 7.70 7.13 7.30 4.12 4.93 6. 6.73 6.93 7.06 6.50 7.36 7.74 7.99 3.49 4.50 5.24 6.04 6.09 6.44 6.41 6.68 6. 7.20 Generation Only 0.00 0.05 0.35 0.33 0.38 0.63 Unified MLLM 0.45 0.13 0.25 0. 0.30 0.23 0.25 0.60 Unified MLLM w/ CoT 0.68 0.35 0.45 0.33 0.38 0. 4.30 4.97 6.06 6.22 6.23 6.51 6.20 6.78 6.93 7.11 0.03 0.00 0.15 0.13 0.10 0.13 0. 0.20 0.35 0.28 0.00 0.00 0.28 0.30 0.45 0.60 0.33 0.35 0.45 0. 0.00 0.00 0.20 0.08 0.13 0.13 0.25 0.08 0.13 0.28 0.03 0.00 0.38 0.35 0.55 0.58 0. 0.25 0.48 0.53 0.08 0.00 0.23 0.13 0.23 0.23 0.38 0.30 0.35 0. 0.06 0.06 0.31 0.25 0.26 0.31 0.37 0.31 0.35 0.40 Table 3. Ablation Results of Key Designs in DraCo. Draft Resolution CFG Draft GenEval VAE Input Overall 128 1024 384 384 384 DraCo-CFG DraCo-CFG Original DraCo-CFG DraCo-CFG 0.76 0.75 0.83 0.84 0.86 DraCo-CFG exhibits superior performance both quantitatively and qualitatively. Quantitatively, DraCo-CFG surpasses the original CFG by 3% on the overall score of GenEval. Qualitatively, compared to the blurriness of the original CFG, DraCo-CFG demonstrates significantly improved clarity and high quality, as evidenced by the details such as the clock figures. Is the VAE feature of the draft needed? We investigate the necessity of incorporating VAE features from the draft alongside ViT features during training. Results in Table 3 show that including VAE features yields an overall score of 0.84, which is 2% lower than the model without VAE features. This suggests that low-level features impose constraints on the final correction step. Qualitatively, as shown in Fig. 6, we observe that VAE features may introduce artifacts in the final output due to over-adherence to the draft, including unnatural lighting on the horse, improper intersection between the horses legs and the sofa, and disconnected frame at the top of the clock. Draft Resolution. We examine the impact of draft resolution on final results by testing resolutions of 128128 and 10241024, as shown in Table 3. At 128128 resolution, the model cannot adequately express its planning in the excessively small draft. Besides, the resolution is too small for the model to clearly identify the image for the verificaFigure 6. Qualitative Comparison Between Imputing VAE Feature of the Draft, Original CFG and DraCo-CFG. Zoom in for better visualization. tion step. These problems result in poor performance with score of 0.76. Conversely, the excessively large resolution of 10241024 substantially increases the token length per training sample, thereby reducing the number of training samples within the same training iterations. This also yields unsatisfactory results, with score of 0.75. 4. Conclusion In this work, we introduce DraCo, novel interleaved reasoning paradigm that fully exploits the unified architecture of MLLMs by incorporating both visual and textual chainof-thought for text-to-image generation. DraCo addresses the coarse-grained nature of textual planning and enables generation of rare attribute combinations that models typically struggle with due to training data biases. We also 8 curate DraCo-240K with specialized automated pipelines targeting three atomic correction capabilities, and propose DraCo-CFG to explicitly strengthen visual semantics from drafts and correction instructions from verification. Extensive experiments demonstrate DraCos superiority, achieving +8% on GenEval, +0.91 on ImagineBench, and +3% on GenEval++. Our work offers new direction for enhancing visual generation tasks through interleaved multimodal reasoning in unified MLLMs."
        },
        {
            "title": "References",
            "content": "[1] Aida Amini, Saadia Gabriel, Peter Lin, Rik Konceland Hannaneh Hajishirzi. interpretable math word problem arXiv preprint Kedziorski, Yejin Choi, Mathqa: solving with operation-based formalisms. arXiv:1905.13319, 2019. 2 Towards [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. 2023. 7 [3] Jiuhai Chen, Le Xue, Manli Shu, Yunhao Zhang, Yu Zhang, Ran Xu, Xin Eric Wang, and Caiming Xiong. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. 1, 7, 8 [4] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in visionlanguage models with less than $3. https://github. com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. [5] Muxi Chen, Zhaohua Zhang, Chenchen Zhao, Mingyang Chen, Wenyu Jiang, Tianwen Jiang, Jianhuan Zhuo, Yu Tang, Qiuyong Xiao, Jihong Zhang, et al. Failureatlas: Mapping the failure landscape of t2i models via active exploration. arXiv preprint arXiv:2509.21995, 2025. 2 [6] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 7, 8, 13 [7] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-ofthought reasoning. arXiv preprint arXiv:2506.05331, 2025. 13 [8] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 1, 2, 7, 8, 13, 16 [9] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. 13 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 3 [11] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. 13 [12] Jun Gao, Yongqi Li, Ziqiang Cao, and Wenjie Li. In Proceedings of the Interleaved-modal chain-of-thought. Computer Vision and Pattern Recognition Conference, pages 1952019529, 2025. 13 [13] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 3, 6 [14] Zeqi Gu, Markos Georgopoulos, Xiaoliang Dai, Marjan Ghazvininejad, Chu Wang, Felix Juefei-Xu, Kunpeng Li, Yujun Shi, Zecheng He, Zijian He, et al. Improving chainof-thought efficiency for autoregressive image generation. arXiv preprint arXiv:2510.05593, 2025. 2, 13 [15] Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, and Pheng-Ann Heng. Sciverse: Unveiling the knowledge comprehension and visual reasoning of lmms on multi-modal scientific problems. arXiv preprint arXiv:2503.10627, 2025. [16] Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, and PhengInterleaving texAnn Heng. Thinking-while-generating: tual reasoning throughout visual generation. arXiv preprint arXiv:2511.16671, 2025. 13 [17] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025. 2, 7, 13 [18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. 2 [19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3 [20] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, et al. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv preprint arXiv:2409.12959, 2024. 13 [21] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. 2, 7, 8, 13, 16 [22] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. 2, 13 [23] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3 [24] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. 2 [25] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 3, 5, 8 [26] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 13 [27] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 1 [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 13 [30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 5, 14 [31] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [32] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun yue Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. ArXiv, abs/2310.02255, 2023. 2, 13 [33] Xinji Mai, Haotian Xu, Zhong-Zhi Li, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang, et al. Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving. arXiv preprint arXiv:2505.07773, 2025. 2 [34] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. 2 [35] Meta AI. LlamaGen: Autoregressive image generation https : / / github . with llama-based transformers. com/facebookresearch/llamagen, 2024. Code and model released June 2024. 7 [36] OpenAI. Introducing openai o1, 2024., 2024. 13 [37] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. 5, 14 [38] Kaihang Pan, Wendong Bu, Yuruo Wu, Yang Wu, Kai Shen, Yunfei Li, Hang Zhao, Juncheng Li, Siliang Tang, and Yueting Zhuang. Focusdiff: Advancing fine-grained text-image alignment for autoregressive visual generation through rl. arXiv preprint arXiv:2506.05501, 2025. 2, 13 [39] Bimsara Pathiraja, Maitreya Patel, Shivam Singh, Yezhou Yang, and Chitta Baral. Refedit: benchmark and method for improving instruction-based image editing model on referring expressions. arXiv preprint arXiv:2506.03448, 2025. [40] Dustin Podell, Tom Marks, Spencer Nølle, et al. SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 7, 8 [41] Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, and Hao Li. Uni-cot: Towards unified chain-of-thought reasoning across text and vision. arXiv preprint arXiv:2508.05606, 2025. 13 [42] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. 1 [43] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. 13 [44] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 3, 6 [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 8 [46] Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, and Gal Chechik. Generating images of rare concepts using pretrained diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 46954703, 2024. 2 [47] Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, et al. Mathcanvas: Intrinsic visual chainof-thought for multimodal mathematical reasoning. arXiv preprint arXiv:2510.14958, 2025. [48] Stability AI. Stable diffusion 3 medium. arXiv preprint arXiv:2402.13753, 2024. Model released May 2024. 7 [49] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think 10 with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. 13 [50] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. arXiv: 2312.13286, 2023. 1, 13 [51] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv: 2307.05222, 2023. 13 [52] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [53] Qwen Team. Qwen3 technical report, 2025. 3, 5, 6, 14 [54] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 3 [55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 [56] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 13 [57] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1 [58] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2 [59] Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, et al. Webagent-r1: Training web agents via endarXiv preprint to-end multi-turn reinforcement learning. arXiv:2505.16421, 2025. 2 [60] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 1, [61] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 2, 13 [62] Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable multi-modal generators. arXiv preprint arXiv:2412.04332, 2024. 1 [63] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 13 [64] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 1, 13 [65] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 7 [66] Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, et al. Lumina-dimoo: An omni diffusion large language model for multi-modal generation and understanding. arXiv preprint arXiv:2510.06308, 2025. [67] Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, and Ivan Vulic. Visual planning: Lets think only with images. arXiv preprint arXiv:2505.11409, 2025. 13 [68] Junyan Ye, Dongzhi Jiang, Jun He, Baichuan Zhou, Zilong Huang, Zhiyuan Yan, Hongsheng Li, Conghui He, and Weijia Li. Blink-twice: You see, but do you observe? reasoning benchmark on visual perception. arXiv preprint arXiv:2510.09361, 2025. 13 [69] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, et al. Echo-4o: Harnessing the power of gpt4o synthetic images for improved image generation. arXiv preprint arXiv:2508.09987, 2025. 3, 6 [70] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. 2, 13 [71] Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment in large vision-language models arXiv preprint via vision-guided reinforcement learning. arXiv:2503.18013, 2025. 2 [72] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? ECCV 2024, 2024. 2, [73] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024. 2 [74] Xinchen Zhang, Xiaoying Zhang, Youbin Wu, Yanbin Cao, Renrui Zhang, Ruihang Chu, Ling Yang, and Yujiu Yang. 11 Generative universal verifier as multimodal meta-reasoner. arXiv preprint arXiv:2510.13804, 2025. 13 [75] Yu Zhang, Yunqi Li, Yifan Yang, Rui Wang, Yuqing Yang, Dai Qi, Jianmin Bao, Dongdong Chen, Chong Luo, and Lili Qiu. Reasongen-r1: Cot for autoregressive image generation models through sft and rl. arXiv preprint arXiv:2505.24875, 2025. 2, 13 [76] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. DeepIncentivizing thinking with images via reinforceeyes: ment learning. arXiv preprint arXiv:2505.14362, 2025. 13 [77] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 1, 13 [78] Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui Zhang, Yi Xin, Peng Gao, Mohamed Elhoseiny, and Hongsheng Li. From reflection to perfection: Scaling inferencetime optimization for text-to-image diffusion models via reIn Proceedings of the IEEE/CVF Internaflection tuning. tional Conference on Computer Vision, pages 1532915339, 2025. [79] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024."
        },
        {
            "title": "Appendix Overview",
            "content": "Section A: Related work. Section B: More Details of DraCo-CFG. Section C: More Dataset Details. Section D: More Experiment Details. Section E: Limitations and Future Work. Section F: Qualitative Examples. A. Related Work Unified Multimodal Large Language Models Many prior works have focused on empowering single model with both understanding and generation capabilities. Although most methods are built upon MLLMs [26, 29, 56, 79], current mainstream approaches have diverged into different streams. One direction employs an external diffusion model for image generation [9, 50, 51, 64]. For example, OmniGen2 [61] uses diffusion transformer to accept highlevel conditions from the MLLM to produce images. Another direction adopts the image generation process into the auto-regressive paradigm [6, 60, 63]. [60] employs CLIP for vision understanding and VQVAE to discretize images for auto-regressive generation. Recently, some works have attempted to merge auto-regressive generation and diffusion into single framework [8, 77]. Transfusion employs single transformer model to generate text in an auto-regressive manner and images through iterative denoising. Extending this method further, Bagel employs mixture of transformer architectures to separately process text tokens and VAE tokens of images. Reasoning with Images With the introduction of OpenAIs o1 [36], reasoning with images has been widely explored for image understanding tasks [15, 20, 22, 32, 68, 70, 72]. These methods have attempted to incorporate more information from images into the reasoning process, including the use of external tools [49, 76], the reuse of image tokens from the vision encoder [7, 12], and the generation of auxiliary images [47, 67]. More recently, the field of visual generation has also adopted this paradigm [11, 14, 16, 17, 21, 38, 75]. Image-Gen-CoT [17] evaluates the effectiveness of direct preference optimization [43] for image generation and proposes PARM as reward model for autoregressive generation in test-time scaling. Later, T2I-R1 [21] proposes generating semanticlevel CoT to analyze the given prompt and design the image. The semantic-level CoT is then fed back into the model to generate the image. Recently, several methods have proposed reflecting on generated images for improved image generation [41, 74, 78]. [78] utilizes text-to-image diffusion model to generate the image, then verifier produces reflection and refined prompt instead of correction method, and then the generator generates new image. This method does not sufficiently connect the first generated image with the subsequently generated one, and therefore does not adequately address the difficulty of generating an image in one pass. Concurrently, [74] and [41] propose editing the first generated image instead of creating an entirely new one. However, there are several key differences between DraCo and these methods: (1) These methods follow post-reflection strategy, rather than the pre-planning strategy adopted by DraCo. Therefore, they need to generate the image at the same resolution as the final output and then edit it, incurring much extra cost. (2) Additionally, these methods strictly utilize the image editing setting to edit the first generated image, such as restricting the background to remain exactly the same, thereby limiting the editing content. (3) In our pilot study, we find that the inherent editing capability is insufficient to handle various correction scenarios. Therefore, these methods that rely solely on editing capability cannot sufficiently meet the requirements for correcting errors. B. More Details of DraCo-CFG B.1. Original CFG of Bagel Bagel [8] does not directly support DraCo, so no CFG can be directly applied to DraCo. However, the task that most closely resembles DraCo is thinking before editing. In this scenario, Bagel takes as input: the image to edit (with both ViT features vit and VAE features vae) and an edit instruction edit. Bagel then generates textual chain-of-thought think to analyze how to perform the editing, followed by generating the edited image. Bagel employs three forms of multimodal context in CFG: m(ϕ, ϕ, edit, ϕ), m(vit, vae, ϕ, ϕ), and the full condition m(vit, vae, edit, think). The final output is computed through two sequential steps: ˆm(vit, vae, edit, think) = m(vit, vae, edit, think) +stext (m(vit, vae, edit, think) m(vit, vae, ϕ, ϕ)), (4) ˆm(vit, vae, edit, think) = ˆm(vit, vae, edit, think) +simg ( ˆm(vit, vae, edit, think) m(ϕ, ϕ, edit, ϕ)). (5) This formulation has two key issues: 1. Incomplete decoupling of conditions. The three input conditions, image, edit instruction, and text CoT, are not fully decoupled in the two multimodal contexts used for CFG. Instead, two conditions are always dropped simultaneously. In m(ϕ, ϕ, edit, ϕ), both the image and CoT are absent, while in m(vit, vae, ϕ, ϕ), both the edit instruction and CoT are absent. This means the CoT is emphasized twice across these two CFG types. Moreover, emphasizing two different conditions simultaneously may produce unexpected results. 13 2. Sequential computation couples the scales. Unlike DraCo-CFG, which computes CFG in one round, this approach uses two sequential rounds. This couples the scales stext and simg. Formally, let m(vit, vae, edit, think), Mtext m(vit, vae, ϕ, ϕ), and Mimg m(ϕ, ϕ, edit, ϕ) denote the latent vectors from the full model, the text-only model, and the image-only model, respectively. By substituting ˆm(vit, vae, edit, think) from Equation 4 into Equation 5, we obtain: ˆm = (1 + stext)(1 + simg)M stext(1 + simg)Mtext simgMimg (6) As shown, Mtext is controlled by both stext and simg, meaning the condition from Mtext may be overemphasized. B.2. System Prompt In DraCo-CFG, we adopt different system prompts for m(ϕ, ϕ, ϕ), m(ϕ, vit, ϕ), and m(p, vit, v) to ensure more emphasis on the specific condition. We omit the details of the system prompt in the full submission for brevity. In practice, we do not adopt system prompt for m(ϕ, ϕ, ϕ). For m(ϕ, vit, ϕ), we want the model to fully construct the draft image, so the prompt is: Draft-Only System Prompt You should generate larger image with the same content as the input image, with better details and clarity. For full condition m(p, vit, v), the system prompt is: Full-Condition System Prompt You should first generate scratch image. Then you should analyze whether the scratch image is aligned with the prompt. If not, you should think about how to modify the scratch image to make it aligned with the prompt and then modify the scratch image. The analysis process is enclosed within <think> i.e., <think> analysis and </think> tags, process here modification process here (optional) </think> modified image here (optional). C. More Dataset Details We provide additional examples of our proposed DraCo240K in Figure 7. As shown, the subset for general correction includes various correction scenarios such as object replacement and background modification. For instance manipulation, the focus is on handling objects of the same class. The training data includes either adding, removing, or changing attributes for one or multiple instances in the image. Regarding layout reorganization, we include examples that require swapping the positions of objects, or adding or removing an object at specific position relative to existing objects. To guarantee the accuracy of the generated prompts, we conduct cross-validation for prompts in instance manipulation and layout reorganization. The reason is that prompts in instance manipulation contain numeracy information about objects, and prompts in layout reorganization contain spatial information, which is universally acknowledged that current MLLMs, even powerful ones like Qwen3-VL [53], still struggle to accurately capture. Therefore, we also use GroundingDINO [30] to detect all the objects present in the prompt and record their positions and numbers. We only retain samples where the detection results align with the prompt. Besides, the draft images in the dataset are roughly the same size as the final images. We downsample the draft images to size of 384 384 during training. For the verification, we deliberately add conclusion about what needs to be changed so that the model can better follow the correction instruction. D. More Experiment Details To facilitate Bagel to generate valid small resolution images as valid draft, we first conduct text-to-image fine-tuning stage before training for DraCo. We input various forms of prompts to Bagel to generate 1024 1024 images. Then we resize these images to 384 384 to train the model. We also adopt images generated from GPT-4o [37]. During training, half of the images in trained in 384 384, and half is trained in 1024 1024 to ensure the models generation capability of both sizes of images. We finetune the model for 14K steps and adopt the EMA weight. We continue training on this weight for DraCo. E. Limitations and Future Work While DraCo demonstrates significant improvements in text-to-image generation through interleaved reasoning, several limitations remain. The application of this paradigm to other fields has not been extensively studied. Specifically, the low-resolution draft designed in DraCo cannot be directly or optimally used in other scenarios, such as videos, 3D assets, or scenes. Clearly, using low-resolution draft still seems computationally expensive for video generation. The draft for different modalities requires capturing the major difficulty during generation, such as consistency in the video, and then constructing the form of the draft to give second chance for the most challenging part. These are not clearly studied in this work. Moreover, the role of humans in improving data curation and the training loop is not well studied. Incorporating humans in this process could help 14 Figure 7. More Examples of DraCo-240K. better align the generation and correction methods. F. Qualitative Examples We show more qualitative examples and comparison in In Fig. 8 (a), we showcase the draft image, final Fig. 8. image, and its corresponding prompt. The examples include various correction scenarios, including: position correction, rare attribute generation, precise object editing, and numeracy correction. We also show more detailed examples including the verification in Fig. 8 (b). During verification, the model first analyzes the inconsistency between the prompt and the image, then proposes correction method, and also highlights what should not be changed. Ultimately, the model summarizes the correction in its final outcome. In Fig. 8 (c), we compare our method with the CoT-powered 15 Figure 8. More Qualitative Examples of DraCo. method, T2I-R1 [21], and our baseline model, Bagel [8]. As shown, T2I-R1 tends to generate artifacts, e.g., half of the giraffe and the monkey, or over-saturated images. While In contrast, Bagel cannot accurately follow the prompt. DraCo produces satisfying results with both high quality and precise alignment of the prompt."
        }
    ],
    "affiliations": [
        "CUHK (Shenzhen)",
        "CUHK IMIXR",
        "CUHK MMLab",
        "SCUT",
        "Sun Yat-Sen University"
    ]
}