{
    "paper_title": "Training a Foundation Model for Materials on a Budget",
    "authors": [
        "Teddy Koker",
        "Tess Smidt"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foundation models for materials modeling are advancing quickly, but their training remains expensive, often placing state-of-the-art methods out of reach for many research groups. We introduce Nequix, a compact E(3)-equivariant potential that pairs a simplified NequIP design with modern training practices, including equivariant root-mean-square layer normalization and the Muon optimizer, to retain accuracy while substantially reducing compute requirements. Built in JAX, Nequix has 700K parameters and was trained in 500 A100-GPU hours. On the Matbench-Discovery and MDR Phonon benchmarks, Nequix ranks third overall while requiring less than one quarter of the training cost of most other methods, and it delivers an order-of-magnitude faster inference speed than the current top-ranked model. We release model weights and fully reproducible codebase at https://github.com/atomicarchitects/nequix"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] - c . s [ 1 7 6 0 6 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Tess Smidt",
            "content": "Department of Electrical Engineering and Computer Science Massachusetts Institute of Technology Cambridge, MA 02139 {tekoker,tsmidt}@mit.edu"
        },
        {
            "title": "Abstract",
            "content": "Foundation models for materials modeling are advancing quickly, but their training remains expensive, often placing state-of-the-art methods out of reach for many research groups. We introduce Nequix, compact E(3)-equivariant potential that pairs simplified NequIP design with modern training practices, including equivariant root-mean-square layer normalization and the Muon optimizer, to retain accuracy while substantially reducing compute requirements. Built in JAX, Nequix has 700K parameters and was trained in 500 A100-GPU hours. On the Matbench-Discovery and MDR Phonon benchmarks, Nequix ranks third overall while requiring less than one quarter of the training cost of most other methods, and it delivers an order-of-magnitude faster inference speed than the current topranked model. We release model weights and fully reproducible codebase at https://github.com/atomicarchitects/nequix."
        },
        {
            "title": "Introduction",
            "content": "Machine learned inter-atomic potentials (MLIPs) are rapidly improving in capability and scope, with foundation models trained on broad datasets of atomistic materials offering the promise of augmenting or replacing expensive ab initio density functional theory (DFT) calculations [Batatia et al., 2023]. While performance on community benchmarks such as Matbench-Discovery [Riebesell et al., 2025] is rising, the computational costs of both data generation and curation as well as the training of MLIP models on these datasets remain prohibitively expensive for many labs. We pursue an orthogonal goal to scaling: lower computational cost recipe that preserves strong downstream accuracy. Concretely, we revisit simplified E(3)-equivariant architecture based on NequIP [Batzner et al., 2022] with modern training practices: root-mean-square layer normalization for stability, dynamic batching to maximize GPU utilization, and optimizer choices inspired by speedrunning deep learning workflows [Jordan et al., 2024a]. The resulting model, Nequix, has 700K parameters and can be trained in 500 GPU hours, while remaining competitive with larger and more costly to train models on Matbench-Discovery and other phonon prediction tasks. Our contributions are threefold: (1) simplified NequIP architecture featuring an equivariant layernormalization and an efficient JAX implementation; (2) budget-conscious training pipeline leveraging dynamic batching and the Muon optimizer [Jordan et al., 2024b] which achieves fast convergence; and (3) evaluations on the Matbench-Discovery and MDR phonon [Loew et al., 2025] benchmarks. Compared to prior MPtrj-trained models [Chen and Ong, 2022, Deng et al., 2023, Batatia et al., 2023, Bochkarev et al., 2024, Neumann et al., 2024, Barroso-Luque et al., 2024, Fu et al., 2025, Zhang et al., 2025, Yan et al., 2025] , we rank third on both benchmarks with one quarter the training cost of any other published training cost and 10 faster inference speed than the current top ranking model. Preprint. (a) (b) (c) Figure 1: (a) Nequix architecture, simplified version of NequIP [Batzner et al., 2022], with species-independent residual connection and layer normalization. (b) Combined performance scores of compliant models on the Matbench-Discovery (unique prototypes subset). (c) Available published training times of current compliant models."
        },
        {
            "title": "2 Methods",
            "content": "Architecture Nequix follows simplified version of the NequIP [Batzner et al., 2022] architecture, as shown in figure 1a. We adopt two modifications suggested by Park et al. [2024]: the speciesspecific self-connection layer within the interaction block is replaced with single linear layer, and unused non-scalar representations are discarded from the final layer. Lastly, we add an equivariant root-mean-square layer normalization (RMSNorm) Liao et al. [2023], which we find improves performance in our optimization setting. We document the full architecture hyper-parameters and the rationale behind each decision in Table A.1. Implementation Nequix is implemented in JAX [Bradbury et al., 2018] with Equinox [Kidger and Garcia, 2021], taking advantage of just-in-time compilation and efficient automatic differentiation. Following standard energy-conserving MLIP practice [Fu et al., 2025], forces are obtained as the negative energy gradient with respect to atomic positions, rE, and stresses as the energy derivative with respect to strain, normalized by volume, σ = 1 E/ε, where is the predicted total energy of the system, is an atom position, ε is the strain tensor, is the simulation cell volume, and σ denotes the stress tensor. Dynamic batching Materials radius graphs vary widely in their numbers of nodes and edges due to differences in unit-cell size and atom count. With fixed-size batching, the largest graphs dictate memory usage, leaving GPUs underutilized for most batches. To keep batch workloads more uniform while respecting memory limits, we use dynamic batching [Speckhard et al., 2025]: each batch is filled up to caps on the total nodes and edges. We set these caps to 1.1 (ideal batch size) (dataset-average nodes-per-graph and edges-per-graph), respectively. Batches are then padded to these caps to fulfill the static shape requirements of JAX [Godwin* et al., 2020]. Optimization and normalization We compare the widely used Adam optimizer Kingma and Ba [2014] with the recently proposed Muon optimizer Jordan et al. [2024b], which uses the NewtonSchulz algorithm to orthogonalize weight updates. Using smaller version of Nequix on MPtrj [Jain et al., 2013, Deng et al., 2023] with hidden irreps of 128x0e + 64x1o, we sweep learning rates of {0.03, 0.01, 0.003, 0.001} for Adam and Muon, each with and without RMSNorm. The lowest validation error runs for each optimizer are shown in Fig. 2. We find that the Muon configuration achieves comparable energy/force errors to Adam in 60-70% of the epochs, and results in 7% reduction in energy MAE. We also find significant reduction in the variance of stress error, which we notice in runs that use RMSNorm. Notably, the presence of the RMSNorm layer generally resulted in lower validation error for Muon-based training configurations, and higher for those using Adam. Training procedure The final Nequix model is trained for 100 epochs on MPtrj [Jain et al., 2013, Deng et al., 2023], of which we hold out 5% for validation. More details on the training settings are provided in Sec. A.1. The model was trained on 4 NVIDIA A100 GPUs in 125 hours, for total cost of 500 GPU hours. Figure 2: Validation metrics during training of smaller version of Nequix configuration with Adam and Muon, trying learning rates in {0.03, 0.01, 0.003, 0.001} and with/without RMSNorm. This model configuration uses the same hyperparameters as the final model, except with hidden irreps of 128x0e + 64x1o. The dotted horizontal line shows the best validation performance reached during the Adam training."
        },
        {
            "title": "3.1 Matbench-Discovery benchmark",
            "content": "Matbench-Discovery [Riebesell et al., 2025] provides standard framework for evaluating interatomic potentials in high-throughput materials screening task consisting of geometry optimization and energy prediction on set of 257,487 generated structures, and thermal conductivity prediction on set of 103 structures. Ground truth is calculated with DFT/PBE level of theory, the same as MPtrj. The primary metrics include: 1) the F1 for stable/unstable classification after relaxation; 2) root mean squared displacement (RMSD) between predicted and reference structures after relaxation; and 3) symmetric relative mean error in predicted phonon mode contributions to thermal conductivity κ (κSRME). normalized and weighted combination of these metrics are then used to compute combined performance score (CPS-1), which is used for ranking. Following Riebesell et al. [2025], we integrate our interatomic potential as Atomic Simulation Environment (ASE) calculator, which is then used to perform structure relaxation and phonon calculations with the default settings of the benchmark. For comparison, we consider only models in the compliant subset of the benchmark. This consists only of models that are trained on MPtrj or subsets, which limits data leakage and offers more fair comparison among methods. Table 1 contains the performance of Nequix along with all current compliant models at the time of writing. We also include the reported training cost for the models when available, visualized in Fig. 1. We find that Nequix ranks third by CPS-1, outperforming most models at fraction of the training cost. It is noteworthy that this high ranking is due to high performance in the thermal conductivity task, however, the F1 score is still comparable to many of the other methods. Table 1: Matbench-Discovery v1 compliant leaderboard, sorted by combined performance score (CPS-1). Metrics are shown for the unique prototypes subset. Train cost is measured in A100 hours. Data as of 2025-08-17. Model Train cost RMSD CPS-1 Params κSRME F1 eSEN-30M-MP Eqnorm MPtrj Nequix DPA-3.1-MPtrj SevenNet-l3i5 HIENet MatRIS v0.5.0 MPtrj GRACE-2L-MPtrj MACE-MP-0 eqV2 DeNS ORB v2 MPtrj M3GNet CHGNet 30.1M 1.31M 708K 4.81M 1.17M 7.51M 5.83M 15.3M 4.69M 31.2M 25.2M 228K 413K 0.075 0.084 0.085 0.080 0.085 0.080 0.077 0.090 0.092 0.076 0.101 0.112 0.095 0.340 0.408 0.446 0.650 0.550 0.642 0.861 0.525 0.647 1.676 1.725 1.412 1. 0.831 0.786 0.750 0.803 0.760 0.777 0.809 0.691 0.669 0.815 0.765 0.569 0.613 0.797 0.756 0.729 0.718 0.714 0.707 0.681 0.681 0.644 0.522 0.470 0.428 0.400 - 2000 500 - - 2888 - - 2600 - - - -"
        },
        {
            "title": "3.2 MDR phonon benchmark",
            "content": "Performance is also evaluated on the MDR phonon benchmark [Loew et al., 2025], set of 10,000 phonon calculations also done with DFT/PBE level of theory. We follow the identical procedure to Loew et al. [2025], first performing geometry relaxation, then phonon calculations using displacements of supercells. We report the mean absolute error (MAE) of properties derived from the phonon calculation: maximum phonon frequency ωmax, vibrational entropy S, Helmholtz free energy , and heat capacity at constant volume CV . Table 2 demonstrates the performance of Nequix compared to other MPtrj-trained models. Similarly to Matbench-Discovery, we achieve performance within the top three of models, with fraction of the parameter count of other methods. Table 2: Model performance of MPtrj-trained models on the MDR phonon benchmark, sourced from Loew et al. [2025] and Fu et al. [2025]. Metrics are MAE of maximum phonon frequency ωmax (K), vibrational entropy (J/K/mol), Helmholtz free energy (kJ/mol) and heat capacity at constant volume CV (J/K/mol). Model MAE(ωmax) MAE(S) MAE(F ) MAE(CV ) eSEN30M SevenNetl3i5 Nequix SevenNet0 GRACE2L (r6) MACE CHGNet M3GNet 21 26 26 40 40 61 89 98 13 28 33 48 25 60 114 5 10 12 19 9 24 45 56 4 5 6 9 5 13 21 22 3."
        },
        {
            "title": "Inference speed",
            "content": "Lastly, we compare inference speed against existing materials potentials using script from the MACE [Batatia et al., 2022] repository1, modified to work with Nequix and eSEN. We run each model in the default configuration in which it is used within its ASE calculator, without compilation or cuEquivariance kernels for the PyTorchbased MACE-MP-0 and eSEN models, and just-in-time compilation for the JAX-based Nequix. Figure 3 compares performance of each model in terms of steps per day vs. number of atoms. In this study, Nequix is about 10 faster than eSEN, and roughly 2 slower than MACE-MP-0, offering new option in the accuracy vs. speed Pareto frontier at fraction of the training cost."
        },
        {
            "title": "4 Conclusion",
            "content": "Figure 3: models in steps per day."
        },
        {
            "title": "Inference speed of various",
            "content": "We presented Nequix, an E(3)-equivariant interatomic potential that pairs simplified NequIP architecture with modern training practices. Our results show that Nequix achieves competitive accuracy on Matbench-Discovery and the MDR phonon benchmark at less than one quarter of the reported training cost of many contemporaries. This resource-efficient recipe provides practical alternative to large-scale foundation models and helps broaden access to high-quality atomistic modeling in settings with more limited compute. We release trained weights and JAX codebase to streamline reuse and extension. Looking ahead, we see several promising directions: scaling training duration and data while maintaining budget discipline, exploring pretraining and fine-tuning regimes across broader datasets, and pushing cost even lower through model distillation, pruning, quantization, kernel implementations, or more data-efficient training. We hope Nequix serves as strong, efficient baseline for future work on accessible materials foundation models. 1https://github.com/ACEsuit/mace/blob/main/tests/test_benchmark.py"
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We thank Mit Kotak for helpful discussions and insights. This work was supported by the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions, http://iaifi.org/) and by DOE ICDI grant DESC0022215. This research used resources of the National Energy Research Scientific Computing Center (NERSC), Department of Energy User Facility using NERSC award ERCAP0033254."
        },
        {
            "title": "References",
            "content": "Ilyes Batatia, Philipp Benner, Yuan Chiang, Alin Elena, Dávid Kovács, Janosh Riebesell, Xavier Advincula, Mark Asta, Matthew Avaylon, William Baldwin, et al. foundation model for atomistic materials chemistry. arXiv preprint arXiv:2401.00096, 2023. Janosh Riebesell, Rhys EA Goodall, Philipp Benner, Yuan Chiang, Bowen Deng, Gerbrand Ceder, Mark Asta, Alpha Lee, Anubhav Jain, and Kristin Persson. framework to evaluate machine learning crystal stability predictions. Nature Machine Intelligence, 7(6):836847, 2025. Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess Smidt, and Boris Kozinsky. (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature communications, 13(1):2453, 2022. Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, @fernbear.bsky.social, Boza Vlado, You Jiacheng, Franz Cesista, Braden Koszarsky, and @Grad62304977. modded-nanogpt: Speedrunning the nanogpt baseline, 2024a. URL https://github.com/KellerJordan/modded-nanogpt. Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024b. URL https://kellerjordan.github. io/posts/muon/. Antoine Loew, Dewen Sun, Hai-Chen Wang, Silvana Botti, and Miguel AL Marques. Universal machine learning interatomic potentials are ready for phonons. npj Computational Materials, 11(1):178, 2025. Chi Chen and Shyue Ping Ong. universal graph deep learning interatomic potential for the periodic table. Nature Computational Science, 2(11):718728, 2022. Bowen Deng, Peichen Zhong, KyuJung Jun, Janosh Riebesell, Kevin Han, Christopher Bartel, and Gerbrand Ceder. Chgnet as pretrained universal neural network potential for charge-informed atomistic modelling. Nature Machine Intelligence, 5(9):10311041, 2023. Anton Bochkarev, Yury Lysogorskiy, and Ralf Drautz. Graph atomic cluster expansion for semilocal interactions beyond equivariant message passing. Physical Review X, 14(2):021036, 2024. Mark Neumann, James Gin, Benjamin Rhodes, Steven Bennett, Zhiyi Li, Hitarth Choubisa, Arthur Hussey, and Jonathan Godwin. Orb: fast, scalable neural network potential. arXiv preprint arXiv:2410.22570, 2024. Luis Barroso-Luque, Muhammed Shuaibi, Xiang Fu, Brandon Wood, Misko Dzamba, Meng Gao, Ammar Rizvi, Lawrence Zitnick, and Zachary Ulissi. Open materials 2024 (omat24) inorganic materials dataset and models. arXiv preprint arXiv:2410.12771, 2024. Xiang Fu, Brandon Wood, Luis Barroso-Luque, Daniel Levine, Meng Gao, Misko Dzamba, and Lawrence Zitnick. Learning smooth and expressive interatomic potentials for physical property prediction. arXiv preprint arXiv:2502.12147, 2025. Duo Zhang, Anyang Peng, Chun Cai, Wentao Li, Yuanchang Zhou, Jinzhe Zeng, Mingyu Guo, Chengqian Zhang, Bowen Li, Hong Jiang, et al. Graph neural network model for the era of large atomistic models. arXiv preprint arXiv:2506.01686, 2025. Keqiang Yan, Montgomery Bohde, Andrii Kryvenko, Ziyu Xiang, Kaiji Zhao, Siya Zhu, Saagar Kolachina, Doguhan Sarıtürk, Jianwen Xie, Raymundo Arróyave, et al. materials foundation model via hybrid invariant-equivariant architectures. arXiv preprint arXiv:2503.05771, 2025. Yutack Park, Jaesun Kim, Seungwoo Hwang, and Seungwu Han. Scalable parallel algorithm for graph neural network interatomic potentials in molecular dynamics simulations. Journal of chemical theory and computation, 20(11):48574868, 2024. Yi-Lun Liao, Brandon Wood, Abhishek Das, and Tess Smidt. Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations. arXiv preprint arXiv:2306.12059, 2023. 5 James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/jax-ml/jax. Patrick Kidger and Cristian Garcia. Equinox: neural networks in JAX via callable PyTrees and filtered transformations. Differentiable Programming workshop at Neural Information Processing Systems 2021, 2021. Daniel Speckhard, Tim Bechtel, Sebastian Kehl, Jonathan Godwin, and Claudia Draxl. Analysis of static and dynamic batching algorithms for graph neural networks. arXiv preprint arXiv:2502.00944, 2025. Jonathan Godwin*, Thomas Keck*, Peter Battaglia, Victor Bapst, Thomas Kipf, Yujia Li, Kimberly Stachenfeld, Petar Veliˇckovic, and Alvaro Sanchez-Gonzalez. Jraph: library for graph neural networks in jax., 2020. URL http://github.com/deepmind/jraph. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen Dacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, et al. Commentary: The materials project: materials genome approach to accelerating materials innovation. APL materials, 1(1), 2013. Ilyes Batatia, David Peter Kovacs, Gregor N. C. Simm, Christoph Ortner, and Gabor Csanyi. MACE: Higher order equivariant message passing neural networks for fast and accurate force fields. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=YPpSngE-ZU."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Training and model configuration Table A.1 shows the hyper-parameters used to train Nequix. The model is trained for 100 epochs, using an MAE loss function on energy and stress, and l2 loss on forces. We use linear warmup with cosine decay learning rate schedule. Figure A.1 shows the energy, force, and stress MAE on the validation set throughout training. The final MAEs are 10.05 meV/atom, 32.79 meV/Å, and 0.22 meV/Å3/atom for energy, forces, and stress respectively. Figure A.1: Validation curves for Nequix training on MPtrj. 6 Table A.1: Hyper-parameters used and rationale behind selection Hyper-parameter Radial cutoff Hidden irreps Lmax Nlayers Radial basis size Radial MLP size Radial MLP layers Polynomial cutoff Radial basis function Learning rate Warmup epochs Warmup factor Optimizer Weight decay Energy weight Force weight Stress weight Batch size Number of epochs Value 6 Å 128x0e + 64x1o + 32x2e + 32x3o 3 4 8 64 2 6.0 Bessel 0.01 0.1 0.2 Muon 0. 20 20 5 256 (dynamic) 100 Notes/Rationale Most models use 5 or 6 Å; 6 performed slightly better in preliminary validation performance. From SevenNet-l3i5. Consistent with hidden irreps. Balance of performance and efficiency. From NequIP and analysis from Sec. 5.2 of Fu et al. [2025] From NequIP. From NequIP. From NequIP. From NequIP. Also tried Gaussian, which had minimal difference on validation performance. Selected from {0.03, 0.01, 0.003, 0.001} based on validation performance early in training. From eSEN. From eSEN. See Sec. 2. From eSEN. Also tried 0.0, which led to worse validation performance. From eSEN. From eSEN. From eSEN. See Sec. 2 Standard training duration."
        }
    ],
    "affiliations": [
        "Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA 02139"
    ]
}