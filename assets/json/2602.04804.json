{
    "paper_title": "OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models",
    "authors": [
        "Yue Ding",
        "Yiyan Ji",
        "Jungang Li",
        "Xuyang Liu",
        "Xinlong Chen",
        "Junfei Wu",
        "Bozhou Li",
        "Bohan Zeng",
        "Yang Shi",
        "Yushuo Guan",
        "Yuanxing Zhang",
        "Jiaheng Liu",
        "Qiang Liu",
        "Pengfei Wan",
        "Liang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks."
        },
        {
            "title": "Start",
            "content": "OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models Yue Ding 1 2 * Yiyan Ji 3 * Jungang Li 4 Xuyang Liu 5 Xinlong Chen 1 Junfei Wu 1 Bozhou Li 6 Bohan Zeng 6 Yang Shi 6 Yushuo Guan 2 Yuanxing Zhang 2 Jiaheng Liu 3 Qiang Liu 1 Pengfei Wan 2 Liang Wang 1 6 2 0 2 4 ] . [ 1 4 0 8 4 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Omni-modal Large Language Models (OmniLLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omnimodal Spatio-temporal Informed Fine-grained Token compression), modality-asymmetric token compression framework tailored for OmniLLMs. Specifically, OmniSIFT adopts twostage compression strategy: (i) spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks. *Equal contribution 1New Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA) 2Kling Team, Kuaishou Technology 3Nanjing University 4The Hong Kong University of Science and Technology (Guangzhou) 5Sichuan University 6Peking University. Correspondence to: Qiang Liu <qiang.liu@nlpr.ia.ac.cn>. Preprint. February 5, 2026. 1 Figure 1. Performance comparison across five audiovideo benchmarks. Results are obtained using Qwen2.5-Omni-7B with 35% token retained ratio, comparing OmniSIFT against three baseline token compression methods and the full-token baseline. 1. Introduction The rapid evolution of Omni-LLMs (Cheng et al., 2024; Xu et al., 2025b; Liu et al., 2025a) has significantly advanced holistic audio-video-language understanding (Hong et al., 2025; Zhou et al., 2025; Li et al., 2025). However, video signals are composed of densely sampled consecutive frames (Chen et al., 2024b; Jiang et al., 2025a), and audio streams must be encoded at high temporal resolution to capture acoustic dynamics (Ji et al., 2024). When these high-resolution streams are tokenized and interleaved for joint reasoning, the resulting sequence length grows rapidly. For example, typical 20-second multimodal clip can yield more than 20K tokens (Xu et al., 2025a). Such long token sequences significantly increase computational cost, particularly for long video understanding (Fu et al., 2025). Token compression (Chen et al., 2024a; Liu et al., 2025d;b; Ye et al., 2025) has emerged as practical solution to mitigate the prohibitive computational cost caused by excessive token sequences. In the context of vision-centric MLLMs, OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models be resolved using visual cues alone, whereas the saliency of audio signals depends on whether the visual scene provides semantic anchor (Zhao et al., 2018; Arandjelovic & Zisserman, 2017), such as visible speaker or visually grounded event (Chowdhury et al., 2025). This perceptual asymmetry suggests that effective omni-modal token compression should be guided by visual semantics rather than treated symmetrically across modalities. Taken together, these observations suggest three design principles for Omni-LLM token compression: (1) Modalityasymmetric, vision-guided compression; (2) Lightweight compression; (3) Compatibility with efficient operators. Based on the above analysis, we present OmniSIFT (Omnimodal Spatio-temporal Informed Fine-grained Token compression), modality-asymmetric framework for visually guided token compression. As illustrated in Figure 2, OmniSIFT first prunes spatial and temporal redundancy in video to produce compact set of visual anchors, and then uses these anchors to select the audio tokens that are most informative for the scene. This two-stage pipeline removes uninformative signals while preserving the key multimodal cues required for reasoning. With only 4.85M additional parameters, OmniSIFT achieves lower latency than training-free baselines such as OmniZip on Qwen2.5-Omni-7B. Moreover, with only 25% of the original tokens retained, it consistently outperforms all compression baselines and even surpasses the full-token model on several settings, as illustrated in Figure 1. Our main contributions are summarized as follows: Based on the asymmetric dependency between audio and video, we derive practical design principles for omni-modal token compression. We present OmniSIFT, modality-asymmetric framework that first removes spatial and temporal redundancy in video tokens and then uses the resulting visual anchors to select informative audio tokens. Extensive experiments across five benchmarks show that OmniSIFT delivers strong performanceefficiency gains, achieving higher accuracy even with only 25% of the original tokens. 2. Related Works 2.1. Omni-modal Large Language Models Omni-LLMs (Jiang et al., 2025b) extend large language models to process heterogeneous modalities within unified autoregressive framework. Unlike conventional VideoLLMs (An et al., 2025; Bai et al., 2025; Wu et al., 2025a; Figure 2. Compression paradigm comparison for Omni-LLMs. Token compression for Omni-LLMs can be categorized into three paradigms: (a) modality-decoupled compression (left top), which applies audio and video compression independently; (b) modalitysymmetric compression (right top), which treats the two modalities equally informative; and (c) modality-asymmetric compression (bottom, ours), which first prunes visual redundancy and then performs visually guided audio compression. substantial body of work has explored effective strategies for pruning redundant visual tokens (Chen et al., 2024a; Tao et al., 2025a; Yao et al., 2025), demonstrating that significant efficiency gains can be achieved with minimal performance degradation. However, directly extending these approaches to audiovideo understanding in Omni-LLMs is far from straightforward. As illustrated in Fig. 2, the modality-decoupled compression method directly transfers vision-only techniques to both video and audio streams. While simple, this strategy completely ignores cross-modal semantic dependencies (Seo et al., 2023) and may discard tokens that are jointly informative. recent line of work adopts modality-symmetric token compression paradigm. OmniZip (Tao et al., 2025b) follows this paradigm by first compressing audio tokens using attention scores from the audio encoder, and then guiding video token pruning with audio-derived saliency. Its reliance on attention-based saliency limits compatibility with efficient operators such as FlashAttention (Shah et al., 2024). In addition, treating the two modalities as equally informative collapses the compression process into selecting salient temporal positions, rather than capturing modality-specific semantic cues. EchoingPixels (Gong et al., 2025) also adopts modality-symmetric design, performing global cross-modal contextualization over all audio and video tokens via four additional LLM decoder layers before compression. This compression method delays compression to late stage and introduces substantial computational overhead. In practice, humans process audiovideo content asymmetrically (Koppen et al., 2008). Visual redundancy can typically 2 OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models Figure 3. Architecture of OmniSIFT, modality-asymmetric compression framework. The framework operates in two stages. In the first stage, STVP removes spatial and temporal redundancy in video tokens to obtain compact set of visual anchors. In the second stage, VGAS selects audio tokens conditioned on these visual anchors. The resulting compressed multimodal sequence is then fed into the LLM backbone for downstream reasoning. Chen et al., 2025b; Wu et al., 2025b), which primarily focus on the interaction between visual sequences and textual instructions, Omni-LLMs additionally incorporate audio signals (Cheng et al., 2024; Tang et al., 2025; Liu et al., 2025a; Chen et al., 2026). Proprietary systems such as GPT-4o (Hurst et al., 2024) and Gemini (Comanici et al., 2025) further demonstrate strong performance on audiovisual understanding tasks (Li et al., 2025; Hong et al., 2025). In the open-source community, models like Qwen2.5-Omni (Xu et al., 2025a) adopt typical architecture that aligns modality-specific encoders with an LLM through learned projection layers. 2.2. Token Compression in Multimodal Models In the video domain, token compression methods such as VisionZip (Yang et al., 2025), VidCom2 (Liu et al., 2025c), TimeChat-Online (Yao et al., 2025), and DyCoke (Tao et al., 2025a) estimate token importance through various saliency or similarity metrics. Recent work has begun to explore compression in the audiovideo setting. OmniZip (Tao et al., 2025b) represents an early attempt, selecting salient audio tokens based on encoder attention and using them to guide video compression. EchoingPixels (Gong et al., 2025) takes more tightly coupled approach, performing global audiovideo contextualization before token compression. 3. Method 3.1. Preliminary typical Omni-LLM architecture (Xu et al., 2025a) includes modality-specific encoders Φv and Φa, cross-modal projectors, and generative LLM backbone. Given video clip and synchronized audio A, the encoders map each modality into token sequences compatible with the LLM backbone. Specifically, Zv = Φv(V), Za = Φa(A) (1) where Zv RNvD and Za RNaD are the encoded visual and audio token sequences, with Nv and Na denoting the numbers of visual and audio tokens extracted by the encoders, and denoting the LLM hidden dimension. To maintain temporal alignment, Omni-LLMs group tokens 3 OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models ; Z(t) ], where Z(t) from both modalities into aligned chunks. Let Ct denote the t-th chunk. We define the multimodal block as Ct = [Z(t) RnaD are the visual and audio tokens in the chunk, with nv and na denoting the number of visual and audio tokens per chunk, respectively. The final input sequence = {C1, . . . , CK} is interleaved with textual instructions as the LLMs input. RnvD and Z(t) Each visual sub-sequence Z(t) corresponds to two consecutive frames. Let np nv/2 denote the number of visual tokens per frame. Let F(t) 1 , F(t) 2 RnpD be the token sequences of the two frames. 3.2. OmniSIFT As illustrated in Figure 3, OmniSIFT operates in two stages: (1) Spatio-Temporal Video Pruning (STVP) module that removes spatial and temporal redundancy from visual tokens within each chunk, and (2) Vision-Guided Audio Selector (VGAS) module that selects audio tokens with the refined visual context. Each multimodal chunk Ct serves as the basic processing unit for OmniSIFT. Let ρv, ρa (0, 1] denote the visual and audio compression ratios, which represent the proportions of tokens removed from the visual and audio modalities, respectively. The corresponding retention ratios used for token selection are αv = 1 ρv, αa = 1 ρa. 3.3. Spatio-Temporal Video Pruning Visual tokens in Omni-modal LLMs exhibit substantial redundancy, arising from spatial redundancy within each frame and temporal overlap across consecutive frames. The problem we aim to solve is: how can we retain spatially distinctive regions and temporally changing areas, while discarding redundant patches under fixed visual compression ratio ρv? We introduce Spatio-Temporal Video Pruning (STVP) module that operates at the chunk level. We adopt twostage pruning strategy: (1) compute spatial saliency scores on F(t) 2 , and (2) select the top-ranked tokens according to the retention ratio αv. 1 and temporal saliency scores on F(t) Spatial Saliency Estimation. The first frame encapsulates the static visual layout of the scene. To identify spatially distinctive patches, frame-level representation v(t) is com1 puted via mean pooling to aggregate the global visual context: vector: s(t) 1,i = 1 1,i v(t) v(t) 1 1,i v(t) v(t) 1 . (3) Tokens characterized by higher scores represent patches that exhibit significant divergence from the global frame context and are therefore considered more informative. Temporal Saliency Estimation. The second frame reflects temporal evolution, such as object motion or newly content. Using positional encodings, each token vt 2 can be matched to its corresponding patch token vt 1,i in the first frame, enabling the computation of temporal saliency: 2,i s(t) 2,i = 1 2,i v(t) v(t) 1,i 2,i v(t) v(t) 1,i . (4) higher temporal saliency score indicates stronger deviation over time, capturing motion dynamics or appearance changes that contribute new information. Saliency-guided Token Selection. Given the spatial and temporal saliency scores, STVP retains the most informative patches under the visual retention ratio αv. Let ˆnp = αvnp denote the number of tokens to keep per frame. We select the top-scoring tokens from each frame: 1 = TopK(F(t) ˆF(t) 2 = TopK(F(t) ˆF(t) The pruned visual sequence is ˆZ(t) 1 , ˆnp), 2 , ˆnp). 1 , s(t) 2 , s(t) = [ ˆF(t) 1 ; ˆF(t) 2 ]. (5) 3.4. Vision-Guided Audio Selector Audio streams are typically sampled at high temporal resolutions, which inevitably leads to substantial redundancy. The fundamental challenge is: given fixed audio compression ratio ρa, how can we identify the most salient audio tokens while safely discarding those that are uninformative? We rely on the intrinsic modality-asymmetric nature of audiovideo data: whether sound is vital can only be judged when paired with the corresponding visual cues. Motivated by this, we design the Vision-Guided Audio Selector (VGAS) module, which leverages compressed video tokens to guide audio token selection. Formally, for the t-th chunk Ct, VGAS takes the complete audio token sequence Z(t) RnaD and the pruned video token sequence ˆZ(t) RˆnvD as inputs: v(t) 1 = 1 np np (cid:88) i=1 v(t) 1,i, Z(t) RnaD, ˆZ(t) RˆnvD (6) (2) where ˆZ(t) ated by the STVP module. is the compressed visual representation generThe spatial saliency of each token v(t) 1,i is subsequently defined as the cosine distance relative to this global mean Vision-Guided Semantic Interaction. VGAS utilizes lightweight cross-attention mechanism, where the audio 4 OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models Table 1. Performance comparison Results. Results are evaluated on Qwen2.5-Omni-7B and Qwen2.5-Omni-3B across multiple benchmarks, using retained ratios of 35% and 25%. The best result among token compression methods for each metric is bolded. Method Retained Ratio (%) WorldSense () Full Tokens 100 OmniZip Random DyCoke OmniSIFT OmniZip Random DyCoke OmniSIFT 35 35 35 35 25 25 25 25 Full Tokens 100 OmniZip Random DyCoke OmniSIFT OmniZip Random DyCoke OmniSIFT 35 35 35 35 25 25 25 25 49.7 48.9 48.3 48.6 50.0 48.1 47.1 48.1 49.9 45. 44.1 45.5 45.3 45.7 43.8 43.3 44.1 45.8 OmniVideo () VideoMME () video-SALMONN- () testset Bench 35.6 35.1 33.4 34.4 35.6 34.1 32.6 34.1 35. 33.5 33.7 33.4 32.8 33.7 32.4 33.0 33.0 33.1 Short Medium Long Avg. Miss Hal Total Qwen2.5-Omni-7B 78.9 77.1 77.2 78.7 79.0 76.4 77.0 76.4 78. 66.9 67.0 68.1 68.0 67.9 66.1 66.1 66.2 67.8 Qwen2.5-Omni-3B 76.1 74.7 74.3 73.7 76. 72.7 74.0 73.3 75.0 63.4 63.8 61.6 62.7 62.2 61.9 61.9 62.3 62.0 57.1 56.0 56.6 56.9 58. 55.3 55.1 55.0 58.3 52.9 53.1 52.1 53.7 52.8 52.3 50.9 51.9 52.1 67.6 66.7 67.3 67.9 68. 66.0 66.1 65.9 68.2 64.2 63.5 62.7 63.3 63.7 62.3 62.3 62.5 63.0 29.1 34.1 33.2 32.6 30. 35.8 36.2 35.3 30.9 32.8 36.9 37.0 36.9 35.2 39.5 39.3 40.2 36.4 19.0 20.0 19.9 20.1 19. 21.4 20.7 20.0 20.3 20.8 22.2 21.7 21.6 21.8 22.6 22.6 21.7 21.9 48.1 54.1 53.1 52.7 50. 57.2 56.9 56.3 51.2 53.6 59.1 58.7 58.5 56.9 62.1 62.0 61.9 58.3 tokens serve as queries Qa, while the pruned video tokens constitute the keys Kv and values Vv. Specifically, the attention operation is formulated as: H(t) = Softmax (cid:18) QaK (cid:19) Vv, (7) where denotes the dimension of the attention head. This process produces context-aware audio representations H(t) RnaD, in which each audio token incorporates visual information to highlight acoustic features that are semantically aligned with the observed scene. Saliency Scoring and Token Selection. The context-aware audio representations H(t) are projected through two-layer MLP followed by sigmoid activation function to compute scalar saliency score for each audio token: a,j = σ(MLP(h(t) s(t) a,j)). (8) a,j}na These individual scores constitute the saliency sequence s(t) = {s(t) j=1. Subsequently, given the audio retention ratio αa, TopK operator is utilized to select the ˆna = αana tokens with the highest scores, resulting in the pruned audio sequence ˆZ(t) . End-to-End Optimization. To facilitate gradient-based optimization through the non-differentiable TopK selection, 5 VGAS is trained using straight-through estimator (STE). Specifically, during the forward pass, binary mask mj {0, 1} is generated for each audio token such that mj = 1 if its saliency score s(t) a,j is among the top-k values, and mj = 0 otherwise. Only the tokens selected by this mask are propagated to the LLM backbone. To overcome the zerogradient issue of discrete selection during the backward pass, we employ an identity surrogate gradient that approximates mj/s(t) a,j 1. This mechanism allows gradients to flow directly to the saliency scores, thereby enabling seamless end-to-end training of the entire architecture. 4. Experiment 4.1. Experimental Setting Model and Data. Following OmniZip (Tao et al., 2025b), we evaluate OmniSIFT on the Qwen2.5-Omni series (Xu et al., 2025a). To achieve cross-modal alignment for VGAS, we perform fine-tuning on the AVoCaDO SFT dataset (Chen et al., 2025a), which comprises 107K synchronized audiovisual captioning pairs. Benchmarks. We evaluate OmniSIFT on four audiovisual QA benchmarks: VideoMME (with audio) (Fu et al., 2025), DailyOmni (Zhou et al., 2025), WorldSense (Hong et al., OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models Table 2. Performance comparison results on DailyOmni. Results are evaluated on Qwen2.5-Omni-7B and Qwen2.5-Omni-3B, using retained ratios of 35% and 25%. The best result among token compression methods is bolded. Method Retained Ratio (%) Event Sequence AV Event Alignment Inference Reasoning Context Understanding Comparative Average Full Tokens OmniZip Random DyCoke OmniSiFT OmniZip Random DyCoke OmniSiFT 35 35 35 35 25 25 25 25 Full Tokens OmniZip Random DyCoke OmniSiFT OmniZip Random DyCoke OmniSiFT 35 35 35 35 25 25 25 25 66.7 63.7 58.5 61.4 66. 61.8 61.1 57.2 66.7 60.1 60.5 55.9 53.9 57.8 57.8 53.3 52.6 58.5 Qwen2.5-Omni-7B 79. 77.3 77.9 77.9 83.1 75.3 78.6 80.0 82.5 76.6 76.6 73.7 75.4 78.9 75.4 71.4 74.3 77.7 Qwen2.5-Omni-3B 78.6 76.6 76.0 79.2 77.3 75.3 76.6 74.7 75.3 74.9 72.0 74.3 76.0 73.7 70.3 72.0 74.3 73. 70.6 63.0 61.8 63.9 70.2 59.7 56.7 56.7 68.9 62.2 56.7 54.2 52.5 58.8 55.0 54.2 54.6 59. 69.9 59.1 63.2 63.7 69.9 60.6 60.1 61.1 71.0 62.2 59.6 60.1 60.1 64.8 58.5 55.4 58.0 60. 77.1 74.8 74.0 74.8 79.4 74.0 73.3 71.0 76.3 74.8 72.5 68.7 72.5 69.5 70.0 67.9 69.5 70. 72.2 67.7 66.3 67.9 73.2 66.2 65.2 64.7 72.5 67.0 64.7 62.9 63.2 65.3 64.2 61.2 61.7 64. 2025), and OmniVideoBench (Li et al., 2025), as well as the video-SALMONN-2 captioning testset (Tang et al., 2025). Baselines. We choose three baselines: (i) OmniZip (Tao et al., 2025b), the first compression method designed for Omni-LLMs; (ii) DyCoke (Tao et al., 2025a), videocentric token compression approach whose TTM module we adapt to prune video and audio tokens independently; (iii) Random Pruning, which drops video and audio tokens uniformly at random. Implementation Details. The VGAS module uses lightweight multi-head cross-attention layer with 8 heads and 512-dimensional hidden size. We fine-tune only the LLM decoder and the VGAS module using learning rate of 1 105 and total batch size of 128. For fair comparison, we first fine-tune the Qwen2.5-Omni backbone under the same setting and then apply compression baselines to this model. Additional details are provided in Appendix B. 4.2. Main Results State-of-the-Art Compression Performance. As shown in Table 1 and Table 2, we evaluate OmniSIFT on five audio-visual benchmarks using Qwen2.5-Omni-7B and 3B under 35% and 25% token retention ratios. Across all settings, OmniSIFT consistently achieves the highest accuracy among compression methods. Notably, the performance of OmniSIFT matches or even exceeds that of the full-token baseline across multiple benchmarks. For instance, while retaining only 35% of the tokens on Qwen2.5-Omni-7B, OmniSIFT achieves score of 50.0 on WorldSense, surpassing the 49.7 score attained by the full-token model. We attribute it to OmniSIFTs ability to remove redundant audiovisual tokens that may introduce noise, while preserving the key audio-visual cues required for reasoning. Fine-Grained Category Results. Table 2 presents the finegrained results on DailyOmni for both Qwen2.5-Omni-7B and Qwen2.5-Omni-3B across two retention ratios. In challenging categories that require intricate temporal or crossmodal reasoning, existing token compression methods often suffer from substantial performance degradation. For instance, at 25% retention ratio with Qwen2.5-Omni-7B, OmniZip achieves only 61.8 on Event Sequence and 59.7 on AV Event Alignment. These results highlight the limitations of current baselines in capturing temporal dynamics and cross-modal consistency under aggressive compression. In contrast, OmniSIFT achieves 66.7 and 68.9 in these respective categories, demonstrating its resilience even under extremely constrained token budgets. Robustness Across Compression Ratios. Figure 4 illustrates the performance of OmniSIFT compared to other 6 OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models Figure 4. Ablation results for video and audio compression ratios, evaluated on the Qwen2.5-Omni-7B model using the WorldSense benchmark. Left: Varying the video compression ratio ρv with audio compression ratio ρa = 0.5; Right: Varying the audio compression ratio ρa with video compression ratio ρv = 0.8. Table 3. Efficiency comparison results. Results are evaluated on Qwen2.5-Omni-7B and Qwen2.5-Omni-3B using the WorldSense benchmark, reporting peak GPU memory usage, inference latency. The best result among token compression methods for each metric is in bold, the second best result is underlined. Method Retained Ratio (%) GPU Mem (GB) Total Time (s) Prefill Lat. (s) E2E Lat. (s) Acc (%) Qwen2.5-Omni-7B Full Tokens 100 27.59 15097. OmniZip DyCoke OmniSIFT 35 35 35 22.92 23.09 22.91 8886.4 8718.3 8756.0 Qwen2.5-Omni-3B Full Tokens 100 18.91 11399.4 OmniZip DyCoke OmniSIFT 35 35 35 14.75 14.92 14. 7750.4 7578.8 7596.3 4.76 2.80 2.75 2.76 3.59 2.44 2.39 2.39 4. 49.7 2.89 2.85 2.86 48.9 47.3 50.0 3.79 45.8 2.59 2.53 2. 44.1 43.9 45.7 token compression baselines under various visual and audio compression ratios. As shown in the right panel, as the audio compression ratio ρa increases from 0.3 to 0.9, the accuracy of OmniZip drops significantly from over 48.9% to approximately 44.0%. In contrast, OmniSIFT maintains stable performance above 49.3% across the entire range, exhibiting only minimal degradation even under extreme compression levels. Overall, these results demonstrate that OmniSIFT achieves the best balance between compression and performance, maintaining reliable audio-visual understanding even when retaining only small fraction of the original tokens. Figure 5. Ablation results for OmniSIFTs architecture. w/o Spatial Component: all visual tokens are selected using temporal saliency only. w/o Temporal Component: all visual tokens are selected based on spatial saliency only. Audio-Only Selector: audio tokens are selected solely based on intra-audio self-attention without any visual guidance. overhead compared to the full-token model. Specifically, for the 7B variant, OmniSIFT reduces total inference time by over 40% and lowers peak memory usage by more than 4.6 GB, with consistent improvements observed for the 3B variant. Notably, despite the inclusion of learned crossmodal module, the end-to-end latency and peak memory requirements of OmniSIFT remain on par with training-free approaches such as OmniZip and DyCoke, demonstrating its high operational efficiency. 4.4. Ablation Study We conduct ablation studies to examine two primary aspects of OmniSIFT: the individual contributions of the video and audio compression modules, and the impact of the asymmetric token compression paradigm. For consistency, all ablation experiments are performed using the Qwen2.5Omni-7B model as the base architecture. 4.3. Efficiency Analysis Table 3 presents comprehensive efficiency comparison on the WorldSense benchmark for both Qwen2.5-Omni-7B and Qwen2.5-Omni-3B, detailing the inference latency and peak GPU memory consumption of OmniSIFT alongside other token compression baselines. Across both model scales, OmniSIFT achieves substantial reductions in computational Structural Ablation: STVP and VGAS. Figure 5 illustrates the performance impact of the STVP and VGAS modules within OmniSIFT. For the STVP module, we assess the individual contributions of its spatial and temporal components. The removal of either component results in noticeable reduction in accuracy on both DailyOmni and WorldSense, underscoring their complementary roles. Re7 OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models Figure 6. Visualization of token compression methods for Omni-LLMs. White blocks denote discarded video and audio tokens. The vertical amplitude of the waveform reflects the audio information density. As illustrated, OmniZip prunes critical visual features and audio cues, leading to an erroneous interpretation of the score change. In contrast, OmniSIFT preserves both the salient visual dynamics and the informative audio segments required for accurate event reasoning. garding the VGAS module, we compare OmniSIFT against an Audio-Only Selector baseline, where audio token selection relies exclusively on intra-audio dependencies. In this configuration, the cross-modal attention mechanism in VGAS is replaced by an audio self-attention module within each chunk. This modification leads to significant accuracy declines of 3.9% and 2.9% on DailyOmni and WorldSense, respectively. These results demonstrate that the importance of audio tokens is highly context-dependent and necessitates visual guidance for accurate assessment. Additional ablation results for the VGAS module are detailed in Appendix D.4. Token Compression Paradigm Ablation. Table 4 compares our modality-asymmetric paradigm, which employs vision-guided audio selection, with modality-symmetric paradigm that utilizes audio-guided video pruning. Both paradigms are evaluated across three different retention ratios on DailyOmni and WorldSense. To implement the symmetric baseline, we fine-tune the Qwen2.5-Omni-7B backbone following the pruning methodology of OmniZip. Across all retention ratios, OmniSIFT consistently outperforms the symmetric variant, with the performance gap becoming more pronounced as the retention ratio decreases. These results demonstrate that the asymmetric strategy of OmniSIFT effectively preserves more salient tokens by explicitly modeling the cross-modal dependencies between visual and audio modalities. 4.5. Case Study Figure 6 presents case study comparing OmniSIFT with OmniZip on OmniVideoBench, illustrating key limitation of modality-symmetric compression methods: assume audio and video signals at the same time carry comparable importance. In this example, when the score changes, the audio signal receives low saliency score and allocates small compression budget to video; as result, the scoreboard patches are pruned, yielding an incorrect answer. In contrast, OmniSIFT adopts modality-asymmetric compression that preserves the salient video patches and contextually informative audio cues necessary for correct reasoning. 8 OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models Table 4. Ablation results for compression paradigm. We compare our video-guided audio compression with an OmniZip-style trained compression method. All experiments use the Qwen2.5Omni-7B backbone and evaluate three retained ratios on DailyOmni and WorldSense. The best results are bolded. Yang, M., Li, Z., Wan, J., Wang, P., Ding, W., Fu, Z., Xu, Y., Ye, J., Zhang, X., Xie, T., Cheng, Z., Zhang, H., Yang, Z., Xu, H., and Lin, J. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Method Retained Ratio (%) DailyOmni WorldSense OmniZip-Trained OmniSIFT OmniZip-Trained OmniSIFT OmniZip-Trained OmniSIFT 35 30 30 25 25 70.5 73.2 69.3 72.8 68.8 72.5 49.7 50. 49.3 50.0 48.7 49.9 5. Conclusion In this work, we introduce OmniSIFT, modalityasymmetric token compression framework for Omni-LLMs. Inspired by the asymmetric nature of human audiovideo perception, OmniSIFT first decouples spatial and temporal redundancy in video tokens to obtain compact visual cues, and then uses these cues to guide audio token selection. Experiments on five audiovisual benchmarks show that OmniSIFT consistently outperforms existing compression baselines and, in several settings, even exceeds the performance of full-token models. It also delivers substantial gains in inference speed and memory usage. Overall, OmniSIFT provides an effective and efficient approach for reducing token counts in Omni-LLMs while preserving the key audiovisual information required for downstream tasks."
        },
        {
            "title": "Impact Statement",
            "content": "OmniSIFT improves the efficiency of Omni-modal LLMs by reducing redundant tokens while preserving or enhancing performance, enabling wider deployment in resourceconstrained or real-time settings. By encouraging semantically meaningful, cross-modal representations, it benefits applications such as audio-visual QA and video captioning."
        },
        {
            "title": "References",
            "content": "An, X., Xie, Y., Yang, K., Zhang, W., Zhao, X., Cheng, Z., Wang, Y., Xu, S., Chen, C., Zhu, D., et al. Llavaonevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. Arandjelovic, R. and Zisserman, A. Look, listen and learn. In Proceedings of the IEEE international conference on computer vision, pp. 609617, 2017. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., Zhong, H., Zhu, Y., 9 Chen, L., Zhao, H., Liu, T., Bai, S., Lin, J., Zhou, C., and Chang, B. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models. In European Conference on Computer Vision, pp. 1935. Springer, 2024a. Chen, X., Ding, Y., Lin, W., Hua, J., Yao, L., Shi, Y., Li, B., Zhang, Y., Liu, Q., Wan, P., et al. Avocado: An audiovisual video captioner driven by temporal orchestration. arXiv preprint arXiv:2510.10395, 2025a. Chen, X., Zhang, Y., Guan, Y., Zeng, B., Shi, Y., Yang, S., Wan, P., Liu, Q., Wang, L., and Tan, T. Versavid-r1: versatile video understanding and reasoning model from question answering to captioning tasks. arXiv preprint arXiv:2506.09079, 2025b. Chen, X., Lin, W., Hua, J., Yao, L., Ding, Y., Li, B., Zeng, B., Shi, Y., Liu, Q., Zhang, Y., et al. Diadem: Advancing dialogue descriptions in audiovisual video captioning for multimodal large language models. arXiv preprint arXiv:2601.19267, 2026. Chen, Y., Xue, F., Li, D., Hu, Q., Zhu, L., Li, X., Fang, Y., Tang, H., Yang, S., Liu, Z., et al. Longvila: Scaling longcontext visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024b. Cheng, Z., Leng, S., Zhang, H., Xin, Y., Li, X., Chen, G., Zhu, Y., Zhang, W., Luo, Z., Zhao, D., et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. Chowdhury, S., Nag, S., Dasgupta, S., Wang, Y., Elhoseiny, M., Gao, R., and Manocha, D. Avtrustbench: Assessing and enhancing reliability and robustness in audio-visual llms. 2025. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Fu, C., Dai, Y., Luo, Y., Li, L., Ren, S., Zhang, R., Wang, Z., Zhou, C., Shen, Y., Zhang, M., et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2410824118, 2025. OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models Gong, C., Wang, D., Wei, Z., Guo, Y., Zhu, H., and Chen, J. Echoingpixels: Cross-modal adaptive token reduction for efficient audio-visual llms. arXiv preprint arXiv:2512.10324, 2025. Hong, J., Yan, S., Cai, J., Jiang, X., Hu, Y., and Xie, W. Worldsense: Evaluating real-world omnimodal arXiv preprint understanding for multimodal llms. arXiv:2502.04326, 2025. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Ji, S., Jiang, Z., Wang, W., Chen, Y., Fang, M., Zuo, J., Yang, Q., Cheng, X., Wang, Z., Li, R., et al. Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling. arXiv preprint arXiv:2408.16532, 2024. Jiang, J., Li, X., Liu, Z., Li, M., Chen, G., Li, Z., Huang, D.-A., Liu, G., Yu, Z., Keutzer, K., et al. Storm: Tokenefficient long video understanding for multimodal llms. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 58305841, 2025a. Jiang, S., Liang, J., Wang, J., Dong, X., Chang, H., Yu, W., Du, J., Liu, M., and Qin, B. From specific-mllms to omnimllms: survey on mllms aligned with multi-modalities. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 86178652, 2025b. Koppen, C., Alsius, A., and Spence, C. Semantic congruency and the colavita visual dominance effect. Experimental brain research, 184(4):533546, 2008. Li, C., Chen, Y., Ji, Y., Xu, J., Cui, Z., Li, S., Zhang, Y., Tang, J., Song, Z., Zhang, D., et al. Omnivideobench: Towards audio-visual understanding evaluation for omni mllms. arXiv preprint arXiv:2510.10689, 2025. Liu, K., Li, J., Sun, Y., Wu, S., Gao, J., Zhang, D., Zhang, W., Jin, S., Yu, S., Zhan, G., et al. Javisgpt: unified multi-modal llm for sounding-video comprehension and generation. arXiv preprint arXiv:2512.22905, 2025a. Liu, X., Gui, X., Zhang, Y., and Zhang, L. Mixing importance with diversity: Joint optimization for kv cache compression in large vision-language models. arXiv preprint arXiv:2510.20707, 2025b. Liu, X., Wang, Z., Chen, J., Han, Y., Wang, Y., Yuan, J., Song, J., Zhang, L., Huang, S., and Chen, H. Global compression commander: Plug-and-play inference acceleration for high-resolution large vision-language models. arXiv preprint arXiv:2501.05179, 2025d. Seo, P. H., Nagrani, A., and Schmid, C. Avformer: Injecting vision into frozen speech models for zero-shot av-asr. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2292222931, 2023. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2024. Tang, C., Li, Y., Yang, Y., Zhuang, J., Sun, G., Li, W., Ma, Z., and Zhang, C. video-salmonn 2: Captioningenhanced audio-visual large language models. arXiv preprint arXiv:2506.15220, 2025. Tao, K., Qin, C., You, H., Sui, Y., and Wang, H. Dycoke: Dynamic compression of tokens for fast video large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1899219001, 2025a. Tao, K., Shao, K., Yu, B., Wang, W., liu, J., and Wang, H. Omnizip: Audio-guided dynamic token compression for fast omnimodal large language models. arXiv preprint arXiv:2511.14582, 2025b. Wu, J., Ding, Y., Liu, G., Xia, T., Huang, Z., Sui, D., Liu, Q., Wu, S., Wang, L., and Tan, T. SHARP: Steering hallucination in LVLMs via representation engineering. In Christodoulopoulos, C., Chakraborty, T., Rose, C., and Peng, V. (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 1434614361, Suzhou, China, November 2025a. Association for Computational Linguistics. ISBN 979-8-89176-332-6. doi: 10.18653/v1/2025.emnlp-main. 725. URL https://aclanthology.org/2025. emnlp-main.725/. Wu, J., Guan, J., Feng, K., Liu, Q., Wu, S., Wang, L., Wu, W., and Tan, T. Reinforcing spatial reasoning in visionlanguage models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025b. Xu, J., Guo, Z., He, J., Hu, H., He, T., Bai, S., Chen, K., Wang, J., Fan, Y., Dang, K., et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025a. Liu, X., Wang, Y., Ma, J., and Zhang, L. Video compression commander: Plug-and-play inference acceleration for video large language models. arXiv preprint arXiv:2505.14454, 2025c. Xu, J., Guo, Z., Hu, H., Chu, Y., Wang, X., He, J., Wang, Y., Shi, X., He, T., Zhu, X., Lv, Y., Wang, Y., Guo, D., Wang, H., Ma, L., Zhang, P., Zhang, X., Hao, H., Guo, Z., Yang, B., Zhang, B., Ma, Z., Wei, X., Bai, S., Chen, OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models K., Liu, X., Wang, P., Yang, M., Liu, D., Ren, X., Zheng, B., Men, R., Zhou, F., Yu, B., Yang, J., Yu, L., Zhou, J., and Lin, J. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025b. Yang, S., Chen, Y., Tian, Z., Wang, C., Li, J., Yu, B., and Jia, J. Visionzip: Longer is better but not necessary in vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1979219802, 2025. Yao, L., Li, Y., Wei, Y., Li, L., Ren, S., Liu, Y., Ouyang, K., Wang, L., Li, S., Li, S., et al. Timechat-online: 80% visual tokens are naturally redundant in streaming videos. In Proceedings of the 33rd ACM International Conference on Multimedia, pp. 1080710816, 2025. Ye, W., Wu, Q., Lin, W., and Zhou, Y. Fit and prune: Fast and training-free visual token pruning for multi-modal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 22128 22136, 2025. Zhao, H., Gan, C., Rouditchenko, A., Vondrick, C., McDermott, J., and Torralba, A. The sound of pixels. In Proceedings of the European conference on computer vision (ECCV), pp. 570586, 2018. Zhou, Z., Wang, R., and Wu, Z. Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities. arXiv preprint arXiv:2505.17862, 2025. 11 OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models A. Expanded Benchmark Details Table 6. ρv (video) and ρa (audio) for different retrained ratios. To rigorously evaluate OmniSIFT across broad spectrum of audio-visual understanding tasks, we select five representative benchmarks that encompass: (i) long-horizon temporal reasoning, (ii) cross-modal alignment and fusion, (iii) fine-grained multi-dimensional comprehension, and (iv) generative captioning. Our selection process is governed by two key criteria. The first is capability coverage, which focuses on competencies essential for practical omni-modal assistants, such as temporal integration and causal reasoning. The second is protocol availability, which prioritizes benchmarks with standardized evaluation protocols to ensure the reproducibility of results. Table 5 summarizes the benchmarks, dataset scales, and metrics employed. Table 5. Evaluation benchmarks used in this work. Benchmark #Videos #QA #Caps Metric DailyOmni Video-MME WorldSense OmniVideoBench video-SALMONN-2 684 900 1,662 628 1,197 2,700 3,172 1,000 483 Acc. (overall & by QA type) Acc. Acc. Acc. GPT-judge (Comp., Hall.) indicates not applicable (QA-only or caption-only evaluation). Methods OmniZip DyCoke Random OmniSIFT 35% 25% ρa 0.4 0.4 0.4 0.4 ρv 0.7 0.9 0.67 0. ρa 0.6 0.6 0.5 0.5 ρv 0.98 0.99 0.77 0.77 also follows the original methodology (Tang et al., 2025), as shown in Figure 8. To assess the quality of these generated captions, we adopt an LLM-as-a-judge framework where GPT-4.1 serves as the evaluator. The specific judgment prompt utilized by the evaluator model is presented in Figure 9. C. Computing Cost Evaluation The computational overhead of OmniSIFT primarily consists of the parameter requirements within the VGAS module and the operational complexity of the STVP module. We analyze these costs specifically for the Qwen2.5-Omni-7B backbone, where dmodel = 3584. B. Expanded Implementation Details C.1. Parameter Efficiency Input Configuration and Preprocessing For both training and inference, video inputs are uniformly sampled at rate of 2 frames per second (FPS), with the total frame count restricted to maximum of 256. The spatial resolution for each individual frame is configured at maximum of 320 28 28 pixels. Configuration of Visual and Audio Compression Ratios Table 6 details the specific visual (ρv) and audio (ρa) compression ratios selected for various methods across different total retention levels. For the 35% total retention setting, the compression ratios for each method are initialized based on the protocols defined in OmniZip (Tao et al., 2025b). Given the architectural differences between token compression methods, we dynamically calibrate these values to ensure that the actual quantity of retained audio and video tokens remains consistent across all baselines. For the 25% retention setting, the optimal balance between visual and audio compression is determined through empirical evaluation, while maintaining the same principle of parity in the final token budget across different methods. Evaluation Prompts The input prompts utilized for evaluating QA benchmarks, such as VideoMME, DailyOmni, WorldSense, and OmniVideoBench, are formatted in accordance with the protocol established by (Fu et al., 2025), as illustrated in Figure 7. For the Video-SALMONN-2 benchmark, the input prompt used to elicit detailed descriptions The VGAS module is designed to be highly lightweight, ensuring minimal impact on the overall memory footprint. For the Qwen2.5-Omni-7B configuration (dmodel = 3584), the module utilizes projections to an internal dimension of 512, followed by single-layer cross-attention mechanism and compact MLP-based score head. The cumulative parameter count for these components is approximately 4.85M. This additional overhead represents less than 0.1% of the total parameters in the 7B-class LLM backbone, demonstrating the extreme parameter efficiency of OmniSIFT. Table 7. Efficiency comparison in theoretical FLOPs in evaluation on the WorldSense with Qwen2.5-Omni-7B. Method Retained Ratio Selector FLOPs (T) LLM FLOPs(T) Total FLOPs (T) Full Tokens OmniSIFT OmniSIFT 100% 35% 25% / 0.06 0.04 555.74 292.10 250.79 555.74 292.16 250. C.2. Computational Complexity The computational complexity of the compression modules is significantly lower than the self-attention mechanism of the LLM backbone. Specifically, STVP operations scale linearly with the sequence length, while the VGAS module performs efficient cross-attention within localized chunks using reduced dimensions. Compared to the quadratic overhead of the backbone, the additional FLOPs introduced by 12 OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models"
        },
        {
            "title": "Prompt for QA Evaluation",
            "content": "Select the best answer to the following multiple-choice question based on the video. Respond with only the letter (A, B, C, or D) of the correct option. What visual elements were displayed immediately after Dr. Rajanis BOTOX WITHOUT THE BOTOX video concluded? A. Still product bottle Price text overlay B. Facial treatment demonstration Presenter holding product while explaining C. Presenters torso shot Secondary screen activation D. Bookshelf backdrop Close-up of string lights The best answer is: Figure 7. Input prompt template utilized for question-answering (QA) benchmarks, following the protocol of VideoMME (?)."
        },
        {
            "title": "Prompt for Video Description",
            "content": "Please provide thorough description of all the content in the video, including every detail. As you describe, ensure that you also cover as much information from the audio as possible, and be mindful of the synchronization between the audio and video as you do so. Figure 8. Inference prompt employed to elicit detailed descriptions for evaluation on the Video-SALMONN-2 benchmark. these modules are negligible. Table 7 provides an empirical comparison of the FLOPs required by the full-token model versus OmniSIFT at 25% and 35% retention ratios, further demonstrating the computational efficiency of the proposed framework. Specifically, at 25% retention ratio, OmniSIFT requires only 250.83T FLOPs, reduction of over 50% compared to the 555.74T required by the fulltoken baseline. D. More Experimental Results D.1. Visualization of Attention Sparsity To investigate the internal mechanisms of multi-modal understanding, we visualize the attention score distributions of the Qwen2.5-Omni-7B backbone at Layer 15 and Layer 27. As illustrated in Figure 10, high degree of attention sparsity is observed across both layers, with the majority of video and audio tokens receiving near-zero attention scores; this indicates that the original dense representation contains substantial redundant information that does not influence the final output generation. This sparsity pattern becomes even more pronounced in the deeper layers, as evidenced by the noticeably lower attention scores in Layer 27 compared to Layer 15, suggesting that the model progressively filters out irrelevant spatio-temporal details to prioritize high-level semantics. Such empirical observations provide strong motivation for OmniSIFT, which significantly reduces the computational load without compromising the representative capacity of the model. D.2. Efficiency Gains across Video Lengths To investigate the scalability of OmniSIFT, we analyze its performance in terms of both computational efficiency and memory consumption as video duration increases from 0s to 120s. As illustrated in Figure 11, OmniSIFT effectively transforms the scaling behavior of the system by preventing the prohibitive resource growth. While the end-to-end (E2E) latency of the full-token baseline escalates significantly due to the quadratic complexity of self-attention, OmniSIFT maintains substantially more sustainable growth trajectory, achieving latency reduction of over 60% for videos exceeding 60 seconds. Similarly, the proposed framework exhibits superior memory scalability; although the peak GPU memory consumption of the baseline model increases rapidly with extended temporal contexts, the growth rate for OmniSIFT remains modest, achieving reduction of approximately 28% when the video duration reaches 120s. These empirical results demonstrate that OmniSIFT is not merely localized optimization but necessary prerequisite for scaling Omni-LLMs to handle extended video sequences within restricted computational budgets. D.3. Ablation on Selector Depth To verify whether the complexity of the VGAS module impacts pruning quality, we conduct comparative study between the default single-layer (N = 1) design and 3-layer (N = 3) variant. As summarized in Table 8, increasing the depth of the cross-modal interaction does not yield performance gains; instead, it leads to slight degradation in 13 OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models Prompts used to evaluate captions in video-SALMONN-2 Task: good video description should capture the detailed events in the video. The task is to judge whether given description is good or not. The model is provided with list of base events and candidate description, and must determine which base events are covered by the description. Instruction: Besides correctly described events, the description may contain missed events, incorrect events, or hallucinated events. Missed Event: An event in the base set whose main action, participants, and context are absent from the description. Incorrect Event: An event in the base set that is mentioned but described with significant factual errors. Hallucination Event: An event mentioned in the description but not included in the base set and not plausible inference from it. The model must also enumerate these events. Incorrect and hallucination events must not overlap. Input Format: There are {event num} base events given as Python list: [\"xxx\", ...]. video description to be evaluated is provided. Output Format (strict): {Missed: x, Incorrect: x, Hallucination: x, Missed Event: [...], Incorrect Event: [...], Hallucination Event: [...]} Events in the Video: {events in video} Video Description to be Rated: {cap to be rated} Given the base events and the candidate description, count missed, incorrect, and hallucinated events and list them out. Figure 9. Prompt for caption evaluation on video-SALMONN-2 test set. Table 8. Performance comparison of various selector depths at 35% retention ratio across representative benchmarks. All results are obtained using the Qwen2.5-Omni-7B model. The best result for each metric is bold. Configuration VideoMME (%) DailyOmni (%) WorldSense (%) GPU Mem (GB) 1-Layer (Ours) 3-Layer Variant 68.3 67.2 73.2 72.3 50.0 49.0 22.62 22.67 peak GPU memory consumption from 22.62 GB to 22.67 GB. These results suggest that shallow architecture is sufficient to capture the cross-modal correlations necessary for token selection, and increasing the depth may introduce unnecessary complexity that hinders the identification of salient audio tokens. Consequently, the single-layer configuration provides the optimal balance between computational efficiency and pruning effectiveness. Figure 10. Attention score distribution maps for layers 15 and 27 of the LLM decoder in the Qwen2.5-Omni-7B model. accuracy across all evaluated benchmarks. Specifically, at 35% retention ratio, the 3-layer variant achieves scores of 67.2, 72.3, and 49.0 on VideoMME, DailyOmni, and WorldSense, respectively, which are lower than the 68.3, 73.2, and 50.0 obtained by the single-layer configuration. Furthermore, the 3-layer design marginally increases the 14 OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models audio-visual inputs, suggesting that the loss of critical visual tokens is more detrimental and harder for the model to recover than the removal of audio tokens. Furthermore, the SSM-based selector significantly underperforms the VGAS module, yielding the lowest scores (67.3 and 47.4). These results confirm that both the STVP and VGAS modules are essential for effectively capturing cross-modal dependencies and preserving salient information. Figure 11. Comparison of peak GPU memory and end-to-end latency between OmniSIFT and full-token baseline using Qwen2.5Omni-7B on WorldSense videos of varying durations. D.4. Extended Ablation Results Figure 12. Results of extended ablation experiments on the architecture of OmniSIFT, conducted using the Qwen2.5-Omni-7B model at 35% retention ratio. Visual Random Pruning: replacing the STVP module with random selection for video tokens; Audio Random Pruning: replacing the VGAS module with random selection for audio tokens; SSM Selector: utilizing State Space Model as the selector for audio tokens. In addition to the experiments in Section 4.4, to further validate the architectural components of OmniSIFT, we conduct extended ablation studies using the Qwen2.5-Omni-7B backbone at 35% retention ratio. In these experiments, we compare the proposed modules against three alternatives: (i) replacing the STVP module with visual random pruning, (ii) substituting the VGAS module with audio random pruning, and (iii) employing State Space Model (SSM) as the audio token selector. All variants are trained using identical datasets and optimization settings to ensure fair comparison. As illustrated in Figure 12, OmniSIFT achieves superior performance on both DailyOmni (73.2) and WorldSense (50.0). Notably, the replacement of STVP with visual random pruning leads to more severe decline in accuracy (67.8 on DailyOmni and 47.8 on WorldSense) compared to audio random pruning (71.0 and 49.1, respectively). This observation underscores the modality-asymmetric nature of"
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology",
        "Nanjing University",
        "New Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)",
        "Peking University",
        "Sichuan University",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}