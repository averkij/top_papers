{
    "paper_title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources",
    "authors": [
        "Weizhi Wang",
        "Yu Tian",
        "Linjie Yang",
        "Heng Wang",
        "Xifeng Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine \"fully open\" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model."
        },
        {
            "title": "Start",
            "content": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources Weizhi Wang1, Yu Tian2, Linjie Yang2, Heng Wang3, Xifeng Yan1 1UC Santa Barbara, 2 Seed Vision Team, ByteDance, 3 Nvidia Research 5 2 0 2 1 ] . [ 1 5 9 5 0 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pretraining efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-ofthe-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine \"fully open\" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised finetuning data used to develop the model. Website https://victorwz.github.io/Open-Qwen2vl Code Models Data https://github.com/Victorwz/Open-Qwen2VL https://huggingface.co/weizhiwang/Open-Qwen2vl https://huggingface.co/datasets/weizhiwang/Open-Qwen2VL-Data 1. Introduction The Multimodal Large Language Models (MLLMs) [4, 9, 35, 44, 48] present strong emergent capabilities on multimodal understanding and visual reasoning, eliciting the Artificial Intelligence Applications to comprehend and analyze images, charts, and PDF documents. Different from conventional Vision-Language Models (VLMs) [19, 37], trained on image-text caption data from scratch with small model size, the MLLMs are typically constructed on well-trained text-only LLM and then continually pre-trained on diverse large-scale multimodal data. However, recent state-of-the-art MLLMs are neither fully-open to the community for reproduction nor compute-friendly to academic institutions with limited GPUs. In Table 1, we compare the openness of recent SOTA MLLMs of VILA [28], MM1 [54], Ideflics [24], BLIP-3 [49], Llama-3.2Vision [13], Phi-3.5-Vision [1], and Qwen2VL [44]. Even if most of the SOTA MLLMs release their base or instruction-tuned model checkpoints, their killer secrets of data filtering techniques, sequence packing scripts, pre-training data, training codebase, etc are completely close-source, in which they even hide such technical details in their technical reports. In this work, we introduce Open-Qwen2VL, 2B-parameter MLLM which outperforms closesource Qwen2-VL-2B on various multimodal benchmarks and achieves outstanding compute efficiency. Open-Qwen2VL is pre-trained on approximately 5B well-curated high-quality caption data tokens, which is 0.36% of 1.4T multimodal tokens of Qwen2-VL [44] pre-training. Such remarkable data-efficiency enables us to perform the pre-training on academic-level computing resources of 8*A100-40G GPUs. In addition, we conduct compressive visual projector [50] to scale-down 729 image patches to 144 visual tokens and perform multimodal sequence packing to further enhance the pre-training efficiency. We perform comprehensive ablation studies on pre-training data mixture strategies and data filtering models. The best pre-training data consists of CC3M-CC12M-SBU [36, 40] caption dataset curated by CLIP and DataComp-Medium caption dataset curated by both the DFN-CLIP and MLM-Filter. Adopting efficient MLLM as the data filtering model significantly enhances the model capabilities on various benchmarks. Additionally, we scale up the visual supervised fine-tuning (SFT) data to 10M level [16] to further enhance the model capabilities. We open-source everything to the community to help easy and convenient reproductions to our model Open-Qwen2VL, including the data filtering details, sequence packing scripts, pre-training data in webdataset format, training codebase based on FSDP, and both base model and instruction-tuned model checkpoints. Meanwhile, our open-source codebase is the first comprehensive solution that supports all stages of Multimodal Large Language Model training, including large-scale caption data preparation, quality score generation, data filtering, multimodal sequence packing, pre-training, supervised fine-tuning, and evaluations on multimodal benchmarks. We redefine \"fully open\" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model. We wish to demonstrate that the research on pretraining is not only game for giant tech companies and encourage the academic community to work on pre-training data and pipeline research even with very limited computing resources. 2. Compute-Efficient Multimodal Pre-Training 2.1. Dataset Choices and High-Quality Data Filtering The current advanced multimodal LLMs are continually pre-trained on large-scale imagetext caption dataset. In addition to image-text caption dataset, some of latest MLLMs like VILA[28], MM1[34], DeepSeek-VL2 [48] also mix the image-text interleaved data with caption data for multimodal pre-training. Mixing image-text caption data and interleaved data will"
        },
        {
            "title": "Models",
            "content": "VILA MM1 Ideflics BLIP-3 Llama-3.2-Vision Phi-3.5-Vision Qwen2VL Open-Qwen2VL"
        },
        {
            "title": "Sequence",
            "content": "Pre-Training Pre-Training"
        },
        {
            "title": "Checkpoint",
            "content": "None Closed Closed Closed Closed Closed Closed Open None Closed Closed Closed Closed Closed Closed Open Open Closed Closed Open Closed Closed Closed Open Open Closed Closed Closed Closed Closed Closed Open Open Closed Closed Open Open Closed Open Open Closed Open Closed Closed Closed Closed Open Open Open Closed Open Open Open Open Open Open Table 1 Comparisons of openness between several state-of-the-art MLLMs. enhance the multimodal in-context learning and multi-image reasoning capabilities of MLLMs. However, MM1 [34] demonstrates introducing image-text interleaved documents into pretraining data will reduce the zero-shot single-image reasoning and understanding capabilities of base MLLMs. Thus, to control the scale of pre-training data and ensure the pre-training efficiency, Open-Qwen2VL focuses on the pre-training paradigm on image-text caption data only. To motivate the easy reproduction to our work from the community, we choose 4 most popular image-text caption datasets shown in Table 2, which are widely used in open-source vision-language model pre-training. BLIP-1 [26] releases the high-quality caption data curated from combination of CC3M-CC12M-SBU (CCS) using CLIP-based [17, 37] filtering. LAION400M [39] implements strict 0.3 threshold based on CLIP image-text cosine similarity to curate its high-quality dataset of 400M image-text caption pairs. We download the CCS and LAION caption datasets based on the release image-urls using img2dataset [7] tool. We only download 15M LAION data to perform controlled-size data mixture ablation studies in Section 2.5. Secondly, we choose DataComp-Medium-128M [15] as another pre-training data choice. Based on the leaderboard of DataComp medium filtering track performance, Data-FilteringNetwork (DFN) [14] is the top-1 independent data filter on the leaderboard1. We successfully download 99.8M out of 128M original released 128M DataComp-Medium data. Then we adopt the official resharder script2 to select the DFN-curated high-quality subset based on the released top-15% data uids from DFN3. It is worth noting that the DFN only releases the top-15% curated data rather than the DFN model checkpoint. Thus, it is impossible to change the retained data fraction based on the quality scores generated by DFN-model. For DataComp-DFN high-quality dataset, finally we get 15M image-text caption data. The MLLM-based data filtering method emerges since the introduction of MLM-Filter [46], in which these methods adopt efficient MLLM as the high-quality data filter instead of CLIP model. MLM-Filter provides four distinct image-text data quality metric for high-quality data filtering, including image-text matching (ITM), object detail fulfillment (ODF), caption text quality (CTQ), and semantic understanding (SU). Based on the conclusions from ATIQE [18], the Semantic Understanding (SU) quality metric yields the best performance for MLLMs trained on the high-quality data curated from such metric. Thus, we generate the SU quality scores for DataComp-Medium data using mlm-filter-qwen2.5-1.5b-gpt4o4 data filtering model and set filtering score threshold as 85 out of 100. With such threshold, We get 8M MLM-Filter 1https://www.datacomp.ai/dcclip/leaderboard.html 2https://github.com/mlfoundations/datacomp/blob/main/resharder.py 3https://huggingface.co/datasets/apf1/datafilteringnetworks_2b/blob/main/datacomp_med ium_dfn_20m_inds.npy 4https://huggingface.co/weizhiwang/mlm-filter-qwen2.5-1.5b-gpt4o"
        },
        {
            "title": "Filtering\nModel",
            "content": "#Image-Text Pairs"
        },
        {
            "title": "Resources",
            "content": "1 2 3 4 CCS CLIP DataComp DFN LAION CLIP DataComp MLMFilter & DFN 19.9M 8.5M 15M 15M https://github.com/salesforce/BLIP huggingface:apf1/datafilteringnetworks_2b https://github.com/salesforce/BLIP - Table 2 Image-Text Caption Datasets for Open-Qwen2VL pre-training. curated data and union them with DFN-15M. After deduplication, we get 19.9M high-quality data. 2.2. Model Architecture with Low-to-High Image Resolution We adopt simple architecture with Qwen2.5-1.5B-Instruct LLM Backbone [43], Adaptive Average-Pooling Visual Projector [50], and SigLIP-SO-400M Vision Encoder [53]. Specifically, the Adaptive Average-Pooling Visual Projector contains an Adaptive Average-Pooling layer followed by two-layer MLP. With the Adaptive Average-Pooling layer, we can scale the 729 output visual patches from SigLIP to any resolution. We adopt 144 visual tokens for representing an image in the pre-training stage and scale up the resolution to vanilla 729 visual tokens in SFT stage. Such low-to-high image resolution significantly enhances the MLLM pre-training efficiency and does not hurt the high-resolution image understanding of the final MLLM after SFT stage. Open-Qwen2VL does not adopt advanced designs of 2d-Multimodal RoPE [44] and naive dynamic resolution [44] to save computes and ensure the training efficiency. Moreover, for academic computing resources, downloading images and saving in original resolution require huge disk space, which is unavailable in most of academic institutions. During our data downloading process with img2dataset [7], we resize the smaller side of the image to 512 pixels and keep the aspect ratio, which makes us not able to adopt naive dynamic resolution in the pre-training stage. For both the pre-training and SFT stages, we freeze the parameters of vision encoder and make the parameters of projector and LLM backbone trainable to save more computes. However, recent studies [44, 54] demonstrate that making the vision encoder trainable can further enhance the visual understanding capabilities of the MLLMs. We leave it as an ablation study for investigations. Figure 1 Model Architecture of Open-Qwen2VL. Here ùëÄ = 729, ùëÅ = 144 are the number of image patch tokens and number of projected visual tokens during the pre-training stage, respectively. Algorithm 1 Multimodal Sequence Packing if (cid:205) ùêº for all ùëë do (ùëáùëë, ùëâùëë) ProcessCaption(ùëë) ùëôùëíùëõùëë ùëáùëë + ùëâùëë ùêº [ùëë] ùëôùëíùëõùëë (ùëë,ùëôùëíùëõ) ùëèùëñùëõ ùëôùëíùëõ + ùëôùëíùëõùëë ùêø then ùëèùëñùëõ.append((ùëë, ùëôùëíùëõùëë)) ùëùùëôùëéùëêùëíùëë true break end for ùêº SortByLength(ùêº) ùêµ for all (ùëë, ùëôùëíùëõùëë) ùêº do ùëùùëôùëéùëêùëíùëë false for all ùëèùëñùëõ ùêµ do Require: 1: D: Set of image-text caption data 2: ùêø: Maximum context length 3: ùëù: Padding token Ensure: Packed sequences within context length ùêø 4: function PA E N S(D, ùêø, ùëù) 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: end for ùëÉ for all ùëèùëñùëõ ùêµ do ùëáùëèùëñùëõ ùëâùëèùëñùëõ for all ùëë ùëèùëñùëõ do end for if ùëáùëèùëñùëõ < ùêø then ùëáùëèùëñùëõ.pad( ùëù, ùêø) end if ùëÉ.append((ùëáùëèùëñùëõ, ùëâùëèùëñùëõ)) ùëáùëèùëñùëõ.append(ùëáùëë) ùëâùëèùëñùëõ.append(ùëâùëë) end for if ùëùùëôùëéùëêùëíùëë then ùêµ.append({(ùëë, ùëôùëíùëõùëë)}) end if end if end for return ùëÉ 39: end function Initialize items dictionary Get text tokens and visual tokens ùëâùëë = Sort in descending order Initialize bins Initialize packed sequences Concatenated text tokens Concatenated PIL images Pad to context length 2.3. Multimodal Sequence Packing Because the large-scale image-text data are varied in its length, simply batchfying set of examples based on similar length and padding them to the longest sequence will lead to large portion of padding tokens in each training batch. Such large amount of padding tokens will result in heavy compute-waste and training inefficiency. Thus, we introduce multimodal sequence packing to regroup the image-text caption data into sequence groups closest to 4096 context length. The algorithm for the multimodal sequence packing is presented in Algorithm 1. Since we download and pack all image-text caption data into webdataset format and each webdataset tar file contains exactly 10k image-text caption data, the proposed multimodal sequence packing"
        },
        {
            "title": "Benchmark",
            "content": "1 + 2 (23.5M) Data Mixture 1 + 3 (23.5M) 1 + 2 + 3 (38.5M) 1 + 4 (28.4M)"
        },
        {
            "title": "General Benchmark",
            "content": "MMMUval MMBenchdev SEEDBench Imgdev MMStar 38.9 75.6 68.9 39.6 37.2 75.9 69.6 41.7 AI2Dtest TextVQAval"
        },
        {
            "title": "OCR VQA",
            "content": "56.3 55.1 55.5 57."
        },
        {
            "title": "Math Reasoning",
            "content": "36.7 75.9 68.9 41.7 57.3 57.4 38.0 77.3 68.7 41.3 56.8 57."
        },
        {
            "title": "MathVistatestmini",
            "content": "28.6 28.1 28.7 28."
        },
        {
            "title": "Hallucination",
            "content": "79.2 55.3 80.1 55.4 77.7 55. 80.1 56.0 Table 3 Benchmark performance of MLLMs pre-trained on different pre-train data mixture and fine-tuned on controlled LLaVA-665k instructions. Remarks for each dataset: 1: CCS-CLIP; 2: DataComp-DFN; 3: LAION-CLIP; 4: DataComp-MLM-Filter & DFN. intends to regroup such 10k pairs into several 4k multimodal sequences. The multimodal sequence packing involves three major steps: computing the multimodal length of each image-text caption sample, regroup data into several bins in which the total length of each bin is closest to 4096, and concatenate the input_ids vectors and pillow-format images. We adopt First-fit-decreasing (FFD) bin packing algorithm [20] to pack each image-text caption data into several bins. We also follows LLaVA to insert an <image> placeholder token at the beginning of each image-text caption. We use the default <im_end> token as the separator between each image caption text. We store each packed multimodal sequence to pickle file, because pickle support storing data in different formats like pillow-image and torch input_ids tensor in one file. Finally, each pickle file contains the following dictionary for each packed multimodal sequence: images: list of pillow image objects; input_ids: torch Long Tensor with image placeholder token; lengths: list of integers to record the multimodal length of each image-text caption data. 2.4. Training Infrastructure and Codebase We develop our training codebase based on Prismatic-VLM [21]. The original codebase only supports SFT on single-image instructions and we heavily modify its dataloader and batch preparation to support the multimodal packed sequences with multiple-images in one sequence. We retain its Fully-Sharded Distributed Parallel (torch-FSDP) trainer, which we find that it significantly accelerates the training compared with LLaVA codebase using DeepSpeedZero3. Although FSDP and DeepSpeed-Zero3 utilize the same model sharding algorithm, our FSDP-based implementation achieves approximately 17% faster for each training step than the DeepSpeed implementation, consistent with findings reported by Karamcheti et al. [21]. 6 2.5. Ablations on the Data Mixture After preparing and sequentially-packing the 4 image-text caption dataset, we conduct ablation studies to investigate the effects of different data mixtures to the performance of final MLLMs. Since there is 16 combinations between the four datasets, we only consider 4 combinations. The CCS-CLIP data is fixed and we incrementally add other three datasets with it. For each dataset group, we pre-train the MLLM for one epoch on packed multimodal sequences and then fine-tune the base MLLM on LLaVA-665k instruction dataset [30]. The training details and hyperparameters are available in Appendix Table 7. Then we evaluate each model ablations on multimodal benchmarks of AI2D-test [22], TextVQA-val [42], POPE [27], MMMU-val [52], MMBench-v1.0-dev [31], SEEDBench-imge-dev [25], MMStar [8], and MathVista-test-mini [32]. Results. The benchmark results of each pre-trained and fine-tuned MLLM using different data mixtures are presented in Table 3. Since the DataComp-DFN and LAION are both web-crawled data and adopt similar CLIP-based data filtering techniques, the data mixtures of these two dataset with CCS achieves very similar model performance. Moreover, simply mixing three CCS-CLIP, DataComp-DFN, and LAION-CLIP does not achieve better performance, which might be caused by the high data homogeneity between DataComp-DFN data and LAIONCLIP data. Surprisingly, adding very small amount of high-quality data (5M) curated by different efficient-MLLM based data filter, MLM-Filter can significantly enhance the model performance, achieving +0.5 average performance improvements. We suppose that MLLMbased data filter may introduce different data distribution into the pre-training set, which brings new knowledge for enhancing the MLLM capabilities. Finally, the pre-training of Open-Qwen2VL on the best data mixture takes about 220 A100-40G GPU hours, and the SFT on LLaVA-665k instructions takes 48 A100-40G GPU hours. 3. Scaling-Up Supervised Fine-Tuning 3.1. SFT Dataset After the ablation studies on the pre-training mixture, we further scale-up the visual SFT data from LLaVA-665k [30] to MAmmoTH-VL-10M [16] to further enhance the understanding and reasoning capabilities of MLLM. We only use the 10M single-image subset for visual SFT and do not include the additional LLaVA-OneVision-2M for further SFT on mixed image and video data. The MAmmoth-VL-10M data requires over 200GB CPU memory if the original LLaVA-style dataloader is adopted to load the full 10M json file data into memory in distributed multi-process. To comply with the limited CPU memory of our server, we store each data sample in the 10M full json data into single json file, and meanwhile generate 10M-indices file for loading into the memory. Each index contains the path to the data sample json, the boolean value of text-only or image-text data, and the pre-computed image-text data length for batchfying. The SFT hyperparameters also follow Appendix Table 7. 3.2. Scaling Effects and Results We save the checkpoints for every 2M sft instructions, which is 15625 steps under the batch size of 128. We illustrate the benchmark performance of each saved checkpoint in Figure 2. We can conclude that scaling-up SFT remarkably improves model performance on various multimodal benchmarks. Most of the benchmarks like POPE, MMMU, MMBench, and SEEDBench performance converges at the SFT scale of 8M instructions and do not improve for the final 2M data. The curves of TextVQA and MathVista performance vary from others, which present steady 7 Figure 2 The scaling effects of visual SFT data from LLaVA-665k to MAmmoTH-VL-si-10M. We evaluate the checkpoints every 2M training samples. improvement over the data scale. It might be caused by the lack of such pre-training math or OCR data in our curated pre-training caption dataset, making the visual math reasoning and text-based VQA become out-of-distribution tasks. For the general-pupose knowledge-based benchmarks of MMMU, SEEDBench, and MMStar, we even observe the slight performance degradation in the final 6M instruction tuning data. We compare the final Open-Qwen2VL model with the state-of-the-art partially-open MLLMs of InternVL2.5-2B-MPO [45], DeepSeekVL-2-Tiny [48], and Qwen2-VL-2B-Instruct [44] on set of general multimodal benchmarks, OCR VQA datasets, multimodal math reasoning benchmark, and hallucination benchmark. Concluded from the results in Table 4, Open-Qwen2VL demonstrates competitive performance across benchmarks compared to other 2B-parameter state-of-the-art MLLMs. It particularly excels in MMBench, achieving the highest score of 80.9, while maintaining comparable performance in SEEDBench and MMStar benchmarks. Moreover, Open-Qwen2VL outperforms the most relevant competitor Qwen2-VL-2B-Instruct on MMBench, SEEDBench-img, MMStar, and MathVista, while it is only trained on 0.35% of tokens of Qwen2VL. However, it shows relatively weaker results in OCR VQA tasks of AI2D and TextVQA. This is because the pre-training data of Open-Qwen2VL does not include the OCR-specific caption dataset like SynthDoG [23] or LAIONCOCO-OCR [38]. Simply introducing such OCR-related pre-training data will significantly enhance the OCR-VQA task performance of MLLMs. 4. Analysis 4.1. Impacts of Sequence Packing on Multi-Image In-Context Learning and Reasoning. Flamingo [3] proposes MultiModal MassiveWeb (M3W) dataset to construct pseudo interleaving data structure using caption data to elicit the multimodal in-context learning capabilities of MLLMs. The multimodal sequence packing also constructs similar pseudo image-text interleaving data strcture. Thus, we conduct experiments to evaluate the few-shot multimodal in-context learning capabilities of the pre-trained base MLLM trained on packed multimodal sequences. We evaluate the base non-sft MLLM trained on the caption data mixture of CCSCLIP and DataComp-MLM-Filter & DFN on GQA, VQA-v2, VizWiz, OKVQA, and Text-VQA datasets. This base model is the best model we get based on the ablation studies on the pretraining data mixture in Table 3. We select 5 random seeds for the 8-shot multimodal in-context"
        },
        {
            "title": "Models",
            "content": "InternVL2.5-2B-MPO DeepSeekVL-2-Tiny Qwen2-VL-2B-Ins Open-Qwen2VL # Pretrain Tokens 277B 8.1T"
        },
        {
            "title": "MMMUval\nMMBenchdev\nSEEDBenchdev\nMMStar",
            "content": "AI2Dtest TextVQAval"
        },
        {
            "title": "POPE",
            "content": "41.2 72.5 73.2 54.3 75.3 77.2 55.3 89."
        },
        {
            "title": "General Benchmark",
            "content": "39.6 68.3 72.5 49."
        },
        {
            "title": "OCR VQA",
            "content": "74.6 80."
        },
        {
            "title": "Math Reasoning",
            "content": "54."
        },
        {
            "title": "Hallucination",
            "content": "88.8 1.4T 41.1 68.8 72.0 46.3 72.3 78.8 48.0 87. 5B 39.8 80.9 72.5 49.7 66.3 63.3 53.1 84.4 Table 4 Benchmarks performance of Open-Qwen2VL and other 2B-parameter state-of-the-art MLLMs. learning experiments and report the average performance over the 5 random seeds. The results in Table 5 demonstrate that the base MLLM trained with packed multimodal sequences can learn from multimodal demonstration examples for completing the task well. The 8-shot incontext learning can gain +3% to +12% performance improvements on VQA dataset compared with 0-shot reasoning. This also demonstrates the necessity and significance of performing multimodal sequence packing as it can enhance the multimodal in-context learning capabilities of the MLLMs. 4.2. Impact of Unfreezing Vision Encoder Parameters. Most of the state-of-the-art MLLMs like InternVL-2.5 [9] demonstrate that making the vision encoder trainable during the SFT stages will enhance the multimodal understanding capabilities of MLLMs. Therefore, we perform such ablation studies on our base non-sft MLLM pre-trained on CCS+DataComp-MLM-Filter&DFN data mixture. We use the LLaVA-665k data as the visual SFT dataset and evaluate the two SFT-ed MLLMs with frozen and trainable vision encoder during the SFT stage. The results in Table 6 demonstrates that make the vision encoder parameters trainable during SFT stage can achieve better average performance while there is significant performance degradation on MMMU benchmark. 5. Related Work Open-Source Multimodal Large Language Models. Close-source MLLMs like GPT-4o [2] and Claude-3.7-Sonnet [4] have strong multimodal understanding and reasoning capabilities. To replicate the strong capabilities of close-source MLLMs, research teams from industry develop the partially open-source strong MLLMs including InternVL-2.5 [9], DeepSeek-VL2 [48], and Qwen2.5-VL [6], which can achieve equivalent capability with close-source models. However, the training data, codebase and data filtering details of such models are not open-sourced for reproduction. Large-Scale Image Text Data. Starting from ImageNet [11], the large-scale image dataset has significantly driven advances in both computer vision and multimodal foundational models. 9 # shots GQA VQA-v2 VizWiz OKVQA Text-VQA 0 8 27.1 35.4 40.2 51.8 26.1 31. 24.7 27.1 30.4 30.6 Table 5 Results of the 0-shot and 8-shot multimodal in-context learning capabilities of pretrianed base MLLMs."
        },
        {
            "title": "Frozen\nTrainable",
            "content": "AI2D test 56.8 57."
        },
        {
            "title": "MMBench\nDev",
            "content": "SEEDBench Img-Dev"
        },
        {
            "title": "MMStar",
            "content": "MathVista test-mini 57.0 57.6 80.1 82.3 38.0 36.1 77.3 76.5 68.7 69. 41.3 41.4 28.6 29.3 Avg. 56.0 56.3 Table 6 Ablation study on the trainable or frozen vision encoder during the SFT stage on LLaVA-665k data. We freeze vision encoder for pre-training stage due to limited computing resources. MSCOCO [29], SBU [36], Conceptual Captions (CC) [41] scales up the image dataset size to near million level, which significantly enhances the image captioning performance of VLMs. OpenAI pre-trained contrastive VLM, CLIP with 400M WebImage data without releasing them. Then LAION-400M and COYO-700M are open-source efforts to further scale up the image-text dataset to hundreds of million level. Then LAION-5B and DataComp-commonpool-12.8B scale up the image-text dataset size to billions level for supporting the data-intensive MLLM pretraining. Most of SOTA MLLMs like DeepSeek-VL [48], Qwen-VL [5], Intern-VL [10], SAIL [12] construct and curate their own large-scale image-text dataset with more then 10B image-text data, while such dataset will not be released for public research. High-Quality Image-Text Data Filtering. Beyond the conventional rule-based or heuristicbased data filtering methods in constructing image-text dataset, image-text dataset with much larger scale for training contrastive VLMs adopts CLIPScore-based filtering methods for highquality data curation. LAION-400M [39] set hard filtering threshold using OpenAI-CLIP model for its data filtering. Later, DataComp [15] is the first effective benchmark for fairly evaluating the effectiveness of each data filtering method on selecting high-quality data for CLIP pre-training. Various methods [33, 47, 51] try to combine CLIPScore filteirng with other metrics to achieve better filtering performance on DataComp, while DFN [14] directly scales up the CLIP-based data filtering model and achieves the top-1 performance. Moreover, another line of data filtering method based on efficient MLLM [18, 46] emerges and presents better capabilities in selecting high-quality data for MLLM pre-training. 6. Conclusion We demonstrate that the efficient MLLM-based high-quality data filtering techniques and welldesigned data mixture strategies can achieve compute-efficient pretraining for developing SOTA MLLMs. Adopting the multimodal sequence packing and dynamic image token number with average-pooling layer can further enhance such pre-training efficiency. The induced final MLLM, Open-Qwen2VL outperforms the partially-open MLLM Qwen2-VL-2B on various multimodal benchmarks, in which Open-Qwen2VL is trained on only 0.36% pre-training tokens of Qwen2VL. The training is conducted on academic-level computing resources and demonstrates that the advanced training pipeline and data filtering can overcome the limitation of computing resources. We wish Open-Qwen2VL can motivate the fully-open compute-efficient multimodal pre-training research from academic community."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank for Facebook (now Meta) for donating the 8xA100-40G GPUs for conducting the experiments. We appreciate the codebase of prismatic-vlms [21] and vlmevaluation5, on which we build our codebase."
        },
        {
            "title": "References",
            "content": "[1] M. Abdin et al. Phi-3 technical report: highly capable language model locally on your phone. In: arXiv preprint arXiv:2404.14219 (2024). [2] [3] J. Achiam et al. Gpt-4 technical report. In: arXiv preprint arXiv:2303.08774 (2023). J.-B. Alayrac et al. Flamingo: visual language model for few-shot learning. In: Advances in Neural Information Processing Systems 35 (2022), pp. 2371623736. [4] Anthropic. Claude 3.7 Sonnet and Claude Code. https://www.anthropic.com/news/ claude-3-7-sonnet. Accessed on Feb 24, 2025. 2025. [5] J. Bai et al. Qwen-vl: frontier large vision-language model with versatile abilities. In: arXiv preprint arXiv:2308.12966 (2023). [6] S. Bai et al. Qwen2. 5-vl technical report. In: arXiv preprint arXiv:2502.13923 (2025). [7] R. Beaumont. img2dataset: Easily turn large sets of image urls to an image dataset. https: //github.com/rom1504/img2dataset. 2021. [8] L. Chen et al. Are we on the right way for evaluating large vision-language models? In: arXiv preprint arXiv:2403.20330 (2024). [9] Z. Chen et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. In: arXiv preprint arXiv:2412.05271 (2024). [10] Z. Chen et al. InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks. In: arXiv preprint arXiv:2312.14238 (2023). [11] J. Deng et al. Imagenet: large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248255. [12] H. Dong et al. Scalable vision language model training via high quality data curation. In: arXiv preprint arXiv:2501.05952 (2025). [13] A. Dubey et al. The llama 3 herd of models. In: arXiv preprint arXiv:2407.21783 (2024). [14] A. Fang et al. Data filtering networks. In: arXiv preprint arXiv:2309.17425 (2023). [15] S. Y. Gadre et al. DataComp: In search of the next generation of multimodal datasets. In: arXiv preprint arXiv:2304.14108 (2023). [16] [17] J. Guo et al. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. In: arXiv preprint arXiv:2412.05237 (2024). J. Hessel et al. Clipscore: reference-free evaluation metric for image captioning. In: arXiv preprint arXiv:2104.08718 (2021). [18] H. Huang et al. Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM Pretraining. In: arXiv preprint arXiv:2410.16166 (2024). [19] C. Jia et al. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In: ICML. 2021. [20] D. S. Johnson. Near-optimal bin packing algorithms. PhD thesis. Massachusetts Institute of Technology, 1973. 5https://github.com/TRI-ML/vlm-evaluation 11 [21] S. Karamcheti et al. Prismatic vlms: Investigating the design space of visually-conditioned language models. In: Forty-first International Conference on Machine Learning. 2024. [22] A. Kembhavi et al. diagram is worth dozen images. In: Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14. Springer. 2016, pp. 235251. [23] G. Kim et al. OCR-Free Document Understanding Transformer. In: European Conference on Computer Vision (ECCV). 2022. [24] H. Lauren√ßon et al. What matters when building vision-language models? In: arXiv preprint arXiv:2405.02246 (2024). [25] B. Li et al. Seed-bench: Benchmarking multimodal llms with generative comprehension. In: arXiv preprint arXiv:2307.16125 (2023). [26] J. Li et al. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: International Conference on Machine Learning. PMLR. 2022, pp. 1288812900. [27] Y. Li et al. Evaluating object hallucination in large vision-language models. In: arXiv preprint arXiv:2305.10355 (2023). [28] J. Lin et al. Vila: On pre-training for visual language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 2668926699. [29] T.-Y. Lin et al. Microsoft coco: Common objects in context. In: Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13. Springer. 2014, pp. 740755. [30] H. Liu et al. Improved baselines with visual instruction tuning. In: arXiv preprint arXiv:2310.03744 (2023). [31] Y. Liu et al. Mmbench: Is your multi-modal model an all-around player? In: European conference on computer vision. Springer. 2024, pp. 216233. [32] P. Lu et al. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In: arXiv preprint arXiv:2310.02255 (2023). [33] P. Maini et al. T-mars: Improving visual representations by circumventing text feature learning. In: arXiv preprint arXiv:2307.03132 (2023). [34] B. McKinzie et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. In: arXiv preprint arXiv:2403.09611 (2024). [35] OpenAI. GPT-4V(ision) Technical Work and Authors. In: (2023). [36] V. Ordonez, G. Kulkarni, and T. Berg. Im2text: Describing images using 1 million captioned photographs. In: Advances in neural information processing systems 24 (2011). [37] A. Radford et al. Learning Transferable Visual Models From Natural Language Supervision. In: ICML. 2021. [38] C. Schuhmann et al. Laion coco: 600m synthetic captions from laion2b-en. In: URL https://laion. ai/blog/laion-coco 5 (2022). [39] C. Schuhmann et al. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. In: arXiv preprint arXiv:2111.02114 (2021). [40] P. Sharma et al. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018, pp. 25562565. [41] P. Sharma et al. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018, pp. 25562565. 12 [42] A. Singh et al. Towards vqa models that can read. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019, pp. 83178326. [43] Q. Team. Qwen2.5: Party of Foundation Models. Sept. 2024. [44] P. Wang et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. In: arXiv preprint arXiv:2409.12191 (2024). [45] W. Wang et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. In: arXiv preprint arXiv:2411.10442 (2024). [46] W. Wang et al. Finetuned multimodal language models are high-quality image-text data filters. In: arXiv preprint arXiv:2403.02677 (2024). [47] Y. Wang et al. Cliploss and norm-based data selection methods for multimodal contrastive learning. In: Advances in Neural Information Processing Systems 37 (2024), pp. 15028 15069. [48] Z. Wu et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. In: arXiv preprint arXiv:2412.10302 (2024). [49] L. Xue et al. xGen-MM (BLIP-3): Family of Open Large Multimodal Models. In: arXiv preprint arXiv:2408.08872 (2024). [50] L. Yao et al. DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models. In: arXiv preprint arXiv:2405.20985 (2024). [51] H. Yu et al. The devil is in the details: deep dive into the rabbit hole of data filtering. In: arXiv preprint arXiv:2309.15954 (2023). [52] X. Yue et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 95569567. [53] X. Zhai et al. Sigmoid loss for language image pre-training. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023, pp. 1197511986. [54] H. Zhang et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. In: arXiv preprint arXiv:2409.20566 (2024). 13 A. Training Settings of MLLM Pre-Training The training details and hyperparameters for MLLM pre-training are presented in Tab. 7. The pre-training is only one-stage process. We do not follow Qwen-VL or DeepSeek-VL to split the MLLM pre-training into two stages of VL alignment and VL pre-training."
        },
        {
            "title": "Details",
            "content": "Vision Encoder Visual Projector LLM Backbone # Tokens per Image Context Length Sequence Packing Precision Global Batch Size # Training Epoch # GPUs Peak LR # Warmup Steps Ratio LR Scheduler Weight Decay Pre-Training"
        },
        {
            "title": "Visual SFT",
            "content": "SigLIP-so400m-384px 2d Adaptive Average Pooling + MLP Qwen2.5-1.5B-Instruct 144 4096 Yes SigLIP-so400m-384px MLP Qwen2.5-1.5B-Instruct 729 4096 No BF16 256 1 8 A100-40G 5e-5 3% linear-warmup+cosine-decay 0.01 BF16 128 1 8 A100-40G 2e-5 3% linear-warmup+cosine-decay 0.1 Table 7 Training details and hyper-parameters for MLLM pre-training and visual supervised fine-tuning. B. Examples Figure 3 An example for generating caption towards famous painting, which requires the background knowledge. 14 Figure 4 An example for generating long captions for complicated 2x2 image."
        }
    ],
    "affiliations": [
        "Nvidia Research",
        "Seed Vision Team, ByteDance",
        "UC Santa Barbara"
    ]
}