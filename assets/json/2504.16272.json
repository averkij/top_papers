{
    "paper_title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization",
    "authors": [
        "Ryan Koo",
        "Ian Yang",
        "Vipul Raheja",
        "Mingyi Hong",
        "Kwang-Sung Jun",
        "Dongyeop Kang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current reinforcement learning from human feedback (RLHF) pipelines for large language model (LLM) alignment typically assign scalar rewards to sequences, using the final token as a surrogate indicator for the quality of the entire sequence. However, this leads to sparse feedback and suboptimal token-level credit assignment. In this work, we frame reward shaping as an optimization problem focused on token-level credit assignment. We propose a reward-shaping function leveraging explainability methods such as SHAP and LIME to estimate per-token rewards from the reward model. To learn parameters of this shaping function, we employ a bilevel optimization framework that integrates Bayesian Optimization and policy training to handle noise from the token reward estimates. Our experiments show that achieving a better balance of token-level reward attribution leads to performance improvements over baselines on downstream tasks and finds an optimal policy faster during training. Furthermore, we show theoretically that explainability methods that are feature additive attribution functions maintain the optimal policy as the original reward."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 2 7 2 6 1 . 4 0 5 2 : r Preprint. Under review."
        },
        {
            "title": "Learning Explainable Dense Reward Shapes via Bayesian\nOptimization",
            "content": "Ryan Koo1, Ian Yang2, Vipul Raheja3, Mingyi Hong1, Kwang-Sung Jun4, Dongyeop Kang1 University of Minnesota1, Georgia Institute of Technology2, Grammarly3, University of Arizona4 {koo00017, mhong, dongyeop}@umn.edu, iyang30@gatech.edu, vipul.raheja@grammarly.com, kjun@cs.arizona.edu"
        },
        {
            "title": "Abstract",
            "content": "Current reinforcement learning from human feedback (RLHF) pipelines for large language model (LLM) alignment typically assign scalar rewards to sequences, using the final token as surrogate indicator for the quality of the entire sequence. However, this leads to sparse feedback and suboptimal token-level credit assignment. In this work, we frame reward shaping as an optimization problem focused on token-level credit assignment. We propose reward-shaping function leveraging explainability methods such as SHAP and LIME to estimate per-token rewards from the reward model. To learn parameters of this shaping function, we employ bilevel optimization framework that integrates Bayesian Optimization and policy training to handle noise from the token reward estimates. Our experiments show that achieving better balance of token-level reward attribution leads to performance improvements over baselines on downstream tasks and finds an optimal policy faster during training. Furthermore, we show theoretically that explainability methods that are feature additive attribution functions maintain the optimal policy as the original reward. The code is publicly available."
        },
        {
            "title": "Introduction",
            "content": "One of the fundamental challenges in reinforcement learning (RL) arises from the sparsity of rewards, where feedback signals are typically only provided at the end of trajectory, with little to no evaluative information about the intermediate states. As result, this limitation leads to gaps about the favorability of certain intermediate states, as the agent lacks the required granular feedback regarding which actions were beneficial to the outcome. similar challenge has been observed in many recent applications of reinforcement learning from human feedback (RLHF), where sparse rewards are common (Zheng et al., 2023b; Chaudhari et al., 2024). Given the sequential nature of language models and the token-level value function typically optimized in RLHF (Zhong et al., 2025; Rafailov et al., 2024), it is crucial to determine the contribution of individual tokens to the overall reward. The challenge lies in the fact that the reward is often awarded at the end of sequence generation and represents the quality of the entire sequence as scalar, which is known to be unstable (Razin et al., 2024; Engstrom et al., 2020) and encodes low-bandwidth signal that does not help determine the relative quality of intermediate tokens. Hence, to address this limitation, it can be beneficial to assign token-level rewards for fine-grained feedback to the policy (Wu et al., 2023; Xie et al., 2024). However, assigning scores to tokens as they get autoregressively generated is computationally intensive, and collecting large set of fine-grained human annotations for supervised learning setup is expensive and subject to high disagreement. Thus, some works have explored reward shaping techniques to transform sparse rewards into dense rewards (Sutton 1https://github.com/minnesotanlp/explainable-dense-rewards 1 Preprint. Under review. Figure 1: Overview of the bilevel optimization setup to find the best reward shape and the optimal policy. The pipeline involves an outer and inner training loop, where the outer step optimizes the Bayesian optimization model and samples the weights for our reward shape. The inner step optimizes the classic RLHF objective. & Barto, 2018; Ng et al., 1999), thus facilitating more efficient and interpretable optimization, as well as providing finer-grained control over intermediate decisions. For instance, recent attempts examine process rewards (Lightman et al., 2023; Uesato et al., 2022), which provide intermediate feedback for chain-of-thought generations or directly utilize the attention map (Chan et al., 2024) of the reward model to redistribute the reward. However, these steps are complex and often require high-quality human feedback, and the attention on each token may not be directly correlated to its output as an explanation (Jain & Wallace, 2019). In this work, we propose two-part approach to densify sparse rewards: (1) using explainability methods to construct dense reward signal, and (2) using Bayesian optimization to learn the weights for new reward shaping function composed of the explainability scores. We frame reward shaping as bi-level optimization problem: at the higher level, we optimize the coefficients of the shaped reward function, while at the lower level, we learn the corresponding optimal policy, as illustrated in Figure 1. First, we estimate token-level contributions using explainability techniques such as LIME (Ribeiro et al., 2016) or SHAPley values (Lundberg & Lee, 2017). Next, since these explainability methods are known to be sensitive to noise (Li et al., 2020), we treat them as uncertain estimates and seek to learn an optimal weighting scheme over them. However, due to the complexity of the reward landscape, exhaustively evaluating all possible weight configurations is computationally infeasible. To address this, we treat the problem as black-box optimization and employ Bayesian optimization to learn the best reward coefficients as natural method robust to noisy objective functions (Fr ohlich et al., 2020; Daulton et al., 2022). We demonstrate that explainability offers natural way to extract more information from the reward model to densify rewards and satisfies as potential-based reward shaping (Ng et al., 1999) so that we satisfy policy invariance or that the optimal policy with the original reward function remains unchanged. Furthermore, we introduce new optimization setup incorporating different sources of token-level information and show that Bayesian Optimization can aid in learning the best reward-shaping function. Empirically, we show that explainability methods positively impact the RL training compared to sparse rewards through accelerating learning and more stable updates in the value function. We also show that adding Bayesian Optimization to properly shape rewards improves generation quality on downstream tasks compared to its naive setups."
        },
        {
            "title": "2 Preliminaries",
            "content": "In this section, we first introduce the classic RLHF approach and build the relevant background for explainability and Bayesian optimization for our method. 2 Preprint. Under review. 2.1 Token-level MDP for RLHF We cast RL for language modeling in the sequential decision-making setting as Markov Decision Process (MDP) by tuple = (S, A, P, γ, r). Here the state space contains states st = {x, y0:t}, where denotes the input prompt and y0:t the sequence produced up to token t. The action set consists of next-token choices at sampled from the models token distribution. Transition dynamics are governed by the kernel P, which assigns probability P(s s, a) to moving from state to after taking action a. scalar discount factor γ [0, 1) down-weights future returns, and the r(st, at) is the reward. In the RLHF pipeline, model is first trained over preference dataset = {(x, yc, yr)} consisting of prompts and pair of chosen and rejected responses yc, yr via supervised finetuning (SFT) followed by two-stage process (1) reward modeling and (2) RL training. From the SFT model, the reward function is modeled under preference model encoded over the sentence level and trained as binary classification problem via maximum likelihood: P(yc yr) = exp{r(x, yc)} exp{r(x, yc)} + exp{r(x, yr)} After learning the reward model, language model is usually trained to optimize the reward via policy gradient algorithm such as PPO (Schulman et al., 2017) with KL-regularization to maximize the expected accumulated rewards. Here, since we optimize over finite horizon, we set γ = 1 and consider the undiscounted return: J(πθ) = max θ Eatπθ (cid:34) t= r(st, at) β log πθ(atst) πref(atst) (cid:35) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) s0 where πθ is the language model agent and πref is reference policy, typically the SFT model. In practice2, the policy gradient optimizes token-level value function given by rewards: r(st, at) = r(x, y) β log πθ (atst) πref(atst) β log πθ (atst) πref(atst) if is terminal otherwise As result, the signal received is very sparse (Zhong et al., 2025; Chan et al., 2024) in which the model receives no relevant feedback about intermediate token generations beyond the KL-regularized term. Thus, it is of interest to understand how we can design rewards on the token level to provide quality, dense signals to make learning easier and potentially improve performance. 2.2 Explainability as Token-level Rewards We first introduce general definition for estimating token-level information with explainability in the textual environment. To explain the predictions of complex black-box function (i.e., our reward model), often small linear model is used to locally approximate it concerning an input of the prompt and completion. Here, we consider only the family of additive feature attribution functions (Ribeiro et al., 2016; Lundberg & Lee, 2017) such that the optimal policy with the original reward remains unchanged (Ng et al., 1999): g(z) = ϕ0 + i=1 ϕiz (1) In the text domain, we define {0, 1}M as binary mask that masks the i-th token from the original input where is the number of tokens in the sequence such that g(z) (hx(z)) and hx is mapping function that reconstructs into valid text input. The value of interest for each token is ϕi, representing the feature effect or their marginal 2We use the TRL (von Werra et al., 2020) implementation of PPO for our method 3 Preprint. Under review. contribution to the prediction (x). To solve for the explainability score ϕi for each token, we optimize the local accuracy to sample by minimizing the penalized linear regression problem: = arg min gG L( , g, πx) + Ω(g) (2) where defines the set of local functions (i.e., small linear model), πx is kernel transformation to the input (i.e., the SHAPley kernel in Eq. 6), and Ω is complexity penalty of g. Solving the regression problem gives us or the set of explanation scores per token ϕi for given sequence x. The form of Eq. 2 is general for any method of additive feature attribution functions, and we see later that formulating this as reward-shaping function optimizes for the same policy as the original sparse reward objective in Section 3.1. We detail complete example of solving the explainability objective in Appendix to produce token-level scores. 2.3 Bayesian Optimization for Reward Shaping To optimize the token-level weights in our reward shaping function, we employ Bayesian Optimization (BO) as zeroth-order method to efficiently search for high-reward configurations. BO aims to find the global maximizer = arg maxx (x) of black-box function , which in our case represents the scalar reward produced by shaped reward model under given weight configuration. Since is expensive to evaluate, BO uses surrogate modeltypically Gaussian Process (GP)to approximate from past observations and suggest new candidate points. In this work, it is sufficient to understand the GP as distribution over collection of function outputs mapping the observed function values = (x) (here, the reward prediction) to multivariate Normal: (x) (µ(x), Σ(x)), where µ(x) = E[yx] is the mean function, and Σ(x) = Cov[yx] is the covariance function. To update the GP model, we can combine the observed values with the prior distribution to update the belief about and generate posterior distribution, which we continue to do iteratively. After building the surrogate model, BO samples points by maximizing the expected utility3 based on the current posterior under some acquisition function to sample areas of improvement and balance between exploration and exploitation to choose the next point. At each BO step, we use the surrogate to select new candidate weight vector by maximizing an acquisition function u, which quantifies the expected utility (i.e., predicted improvement in validation reward) of sampling new point: = arg max u(w D1:t1) (3) where D1:t1 is accumulated observations from the objective function. For u, we adopt log Noisy Expected Improvement (Ament et al., 2025), which handles noisy evaluations well and is easy to optimize numerically. We implement BO using the Ax API (Bakshy et al., 2018), which manages the optimization loop for the surrogate GP and acquisition functions."
        },
        {
            "title": "3 Explainable Reward Shaping as a Bilevel Optimization",
            "content": "In this section, we discuss the optimization setup and the preliminary algorithm to find the best reward shape from each token-level score. First, we discuss the formulation of reward shape to compute dense reward from locally approximating token-level rewards via explainability methods and obtain more fine-grained feedback. Next, we discuss the bilevel optimization problem, where we model BO and the classic RLHF problem as nested objective to optimize both the reward shape and the policy. 3Formally, it is usually also referred to as the reward but we separate definitions to avoid confusion from RL rewards. 4 Preprint. Under review. Figure 2: Redistribution sequence of the scalar reward prediction over the explanation feature attributions after softmax normalization. darker red highlights much stronger positive contribution, while deeper blue indicates more negative contribution. 3.1 Computing the Reward Shape At each step of the optimization procedure, after sampling the weights from the upperlevel step, we shape the reward by computing weighted linear combination of all tokenlevel scores and broadcast the reward over the sequence: r(s, a) = WE r(s, a) = w1 ESHAP r(s, a) + w2 r(s, a) where = [ESHAP, 1] is the matrix of token-level explanations (not limited to just SHAP); is the vector of weights sampled from the Bayesian optimization step. Importantly, we sample each wi from the BO model such that wi = 1 to consider the convex combination of all token-level scores over the reward. In practice, we compute the dense reward by first taking the softmax over the token-level scores to get probability vector over the sequence. Then, following Chan et al. (2024), we distribute the sparse reward or assign credit to each token by broadcasting it over the sequence. As result, the scores at intermediate states are assigned to provide fine-grained feedback for the next token generated transitioning from state s. (4) Additionally, we show that any explainability method in the family of additive feature attribution functions follows policy invariance and can be formulated as potential-based shaping function (Ng et al., 1999). This ensures that adding reward for transitions between states does not yield suboptimal policy but better guides the learning agent towards the same optimal policy. We detail complete proof in Appendix B. 3.2 BO and RLHF as Nested Problems Conventionally, the lower level is an auxiliary problem that helps solve the upper-level problem (Zhang et al., 2023).In contrast, here, we are equally interested in the solution to the lower-level problem as result of the solution retrieved from the upper-level one. In other words, we have the following bilevel optimization setup: max (w; π θ (w)) s.t. π θ (w) = arg max πθ Π (cid:20) Eaπθ r(s, a) β log (cid:21) πθ(as) πref(as) (5) where we have the upper-level optimization problem maximizing the acquisition function in each BO iteration to sample weights based on the validation set performance (i.e., average validation reward) of the best-performing policy π θ . In the lower-level optimization step, we have the standard RLHF problem to find the best policy π θ given the weights for reward shaping from the upper-level step. In principle, our optimization procedure can be 5 Preprint. Under review. Models Baselines SFT RLHF with Sparse (original) Dense RLHF Attention (Chan et al., 2024) SHAP* LIME* Dense RLHF with BO BO-SHAP BO-LIME BO-SHAP-Attn BO-LIME-Attn BO-SHAP-LIME BO-SHAP-LIME-Attn # - - 1 1 1 1 2 2 2 3 HH-RLHF Ultrafeedback Score Win (%) Score Win (%) MTBench 5.40 5.94 6.24 5.91 5.86 5.99 6.26 6.47 6.30 6.58 5.74 48. 51.40 53.95 54.36 55.75 57.62 56.90 54.09 56.28 51.00 4.34 4.35 4.42 4.46 4. 4.37 4.37 4.48 4.45 4.51 4.44 57.64 61.40 53.81 59.20 60.82 56. 51.08 55.03 64.82 59.64 7.2 7.17 7.28 7.81 7.28 7.37 7.59 7.71 7.03 7. 7.38 Table 1: Comparison of HH-RLHF (Only Helpful) and Ultrafeedback. Winrate is calculated via AlpacaEval, with the reference model being the SFT models generation. * indicates strong baselines that we have implemented. # means the number of dense reward types used. Score refers to the average test set reward and Win (%) refers to the length-controlled win-rate against the baseline SFT generations. We use gpt4-turbo as the judge for both AlpacaEval2 and MTBench. Bolded and underlined numbers highlight the best performance. more concretely understood as hyperparameter tuning problem, as we do not explicitly parameterize the reward transformation, but where the outer step looks to find the best set of weights (or hyperparameters) to compute the reward shape for the optimal policy. We outline practical implementation of our method in Algorithm 1."
        },
        {
            "title": "4 Experiments",
            "content": "Tasks. In this section, we empirically verify the effectiveness of our method on downstream tasks. Namely, we train each model on two different single-turn dialogue datasets: 1) HH-RLHF (helpfulness) (Bai et al., 2022) and 2) Ultrafeedback (Cui et al., 2024) datasets. For HH-RLHF, we utilize the OpenLLaMA family of models with an instruction-tuned 7B model as the SFT model and the 3B parameter reward model from Dong et al. (2023) following the setup in (Chan et al., 2024). For Ultrafeedback experiments, we utilize LLAMA-3.2-Instruct 3B (Team, 2024) as the SFT model and fine-tune LLAMA-3.2-Instruct 1B on Ultrafeedback preferences as the reward model. To verify the effectiveness of each method, we consider first the average holistic reward predicted by the reward model over the test splits for each dataset. We also evaluate on open benchmarks such as AlpacaEval-2 (Dubois et al., 2025) and MTBench (Zheng et al., 2023a) to examine whether our methods lead to better local optimum versus optimizing with only the sparse reward while also avoiding reward overfitting. Baselines To measure the improvement of our method, we consider several different baselines: 1) the SFT model to calibrate improvements, 2) RLHF on the sparse reward, 3) attention-based credit (Chan et al., 2024), and 4) each individual explanation method before optimizing the reward shape with Bayesian Optimization. Training setup For every experiment, we adopt PPO as our methods principle policy gradient algorithm. Due to the computational complexity of PPO and to run sufficient number of Bayesian trials, we must compromise training samples per trial to avoid inducing too much computation overhead. Hence, instead of running over the whole dataset for each BO trial, we run = 25 trials for the Bayesian Optimization step and batch each trial into small number of training epochs (i.e., 10) of batch size 8 (80 samples) for total of 2000 samples. To initially sample weights for reward shaping, we employ Sobol sampling for the 6 Preprint. Under review. (a) SHAP vs. BO variants (b) Validation reward after each BO trial. (c) Mean dense reward shape Figure 3: (Left) The mean training reward per timestep with increasing BO dimensionality. (Middle) The mean validation reward over each BO trial. The highlighted dots indicate the best validation reward received at trial n. (Right) The average dense reward attribution over each trial for SHAP + ATTN. The highlighted row indicates the shape in trial 17 that received the highest validation reward. first five trials to build prior for the Gaussian Process model and then sample from the GP model for the remaining trials. After each BO iteration, we evaluate the trained model on the validation split 4 and take the average validation reward as the utility to update the BO surrogate function. To continue training, we employ model checkpointing to resume training from the best checkpoint instead of running complete training cycle from scratch. Thus, we run at most only twice over the dataset D1:n (once for BO, once for full PPO). To ensure randomness, we randomly sample subset from the training dataset at each Bayesian trial to train PPO. We also randomly sample subset from the validation set at each validation step. We detail our model setups and hyperparameters and discuss the computational complexity in Appendix and D. 4.1 How well do explanation-based rewards optimize the RLHF objective? We first analyze the impact of explanation-based rewards in optimizing the vanilla RLHF objective. First, we do not expect to see any significant improvements from explanation methods alone. As we formulate explanation scores as potential-based reward shaping transformation, we are not guaranteed to find better local optima Ng et al. (1999), but carefully chosen reward shape could improve the learning complexity of reinforcement learning (Fu et al., 2025; Gupta et al., 2022). Foremost, we are interested in whether explainability can provide any meaningful improvements or optimizations over simply considering the sparse, scalar reward. (b) PPO Value loss (a) Explanation-based rewards training curves vs. baselines Figure 4: Helpfulness. (Left) The average training reward per timestep. (Right) The average value head loss per timestep. The shading represents the standard error (95% confidence interval) as training progresses. 4We construct the validation set by splitting the training dataset 90%/10%. Preprint. Under review. Figure 5: (Top) The weight transition between trials for SHAPley scores. (Bottom) The weight transition between trials for LIME scores. The black boxes indicate the best weights sampled by the BO model. Credit assignment with explainability helps exploration Before applying any Bayesian optimization, we manually select weighting combination, placing higher emphasis on the token-level scores with = 0.8 following Chan et al. (2024). Overall, explanation-based methods achieve high average reward relatively early during training that is competitive with the attention-based baseline observed in Figure 4(a). Furthermore, explanation-based methods achieve more stable updates than sparse rewards from Figure 4(b). In particular, the dense rewards from explainability drastically reduce the PPO value head loss, signaling that we have good approximation of states to long-term returns for more stable policy updates during training. Explanation-based rewards avoid reward overfitting One of the main risks associated in reinforcement learning from human feedback is reward overfitting. This is particularly undesirable since the reward model is proxy for human preferences, and overoptimizing its value can hinder ground truth performance (Gao et al., 2022). To study this relationship, we observe the performance of baselines and our methods on open benchmarks for instructionfollowing and multi-turn dialogue capabilities, which contain data samples not strictly in distribution to our training datasets. From Table 1, we see that explanation-based methods do not substantially outperform baselines on the HH-RLHF test split; however, they achieve markedly higher win rate on open benchmarks, indicating that we are not particularly overfitting to the reward function and also maintaining general performance. Notably, we see the sparse reward approach underperforms relative to the SFT model on AlpacaEval2 for the HH-RLHF models. We suspect this stems from overfitting on the reward function, which can degrade performance on data splits not directly related to the helpfulness split. To verify this, we examine the win rate against the SFT model only the helpfulness split of AlpacaEval2 and find the win rate improves, being preferred 56.34% 4.03 of the time. 4.2 Does Bayesian optimization help balance token-level rewards better? Next, we evaluate the impact of Bayesian Optimization (BO) in sampling optimal weights to construct new, dense reward function based on token-level interpretability scores. We consider several combinations of token-level scores derived from SHAP, LIME, and the attention map of the reward model. Specifically, our goal is to assess whether BO can effectively weigh these different sources of token-level importance to produce more informative reward signal during policy optimization. However, due to the limited number of trials available, we acknowledge that convergence of BO is not guaranteed, particularly given the high-dimensional and potentially non-convex search space of reward weight combinations (Loeppky et al., 2012; Snoek et al., 2012). The challenge of balancing exploration and exploitation in such setting further complicates the optimization process, especially when evaluating noisy, non-stationary reward surfaces. 8 Preprint. Under review. Figure 6: (Ultrafeedback) The top left represents the baseline per-token reward without shaping. The color of each token represents the reward received, with darker color representing higher proportion of the reward assigned. more uniform coloring indicates more uniform assignment of the scalar reward to each token, while contrasting light/dark coloring indicates more skewed assignment. Despite these limitations, Table 1 and Figure 3(a) show that the weights sampled by BO generally lead to improved downstream task performance compared to baselines and that models guided by BO-optimized dense rewards tend to achieve even higher training rewards. Explore-Exploit Tradeoff To better understand BOs behavior, we analyze the evolution of weight distributions and dense reward shape across trials in Figure 5 and 3(c). In particular, we highlight that the final combination incorporating all scores with an input dimension of = 4 does not outperform simpler combinations. In particular, this is expected as by increasing the search space complexity, we should also increase the number of trials; however, due to the computation constraints, we keep the number of trials constant. Ideally, if BO had successfully identified optimal weights, we would expect to see lower performance bound on any sub-combination of scores; that is, more scores should not hurt performance if adequately weighted. However, the observed degradation in performance for = 4 implies that BO may not have sufficiently explored or exploited the reward space, potentially due to early convergence to suboptimal regions. Across all trials in Figure 5, we see that after Sobol sampling from the first five trials, BO tends to consistently switch between exploration and exploitation, suggesting that just more exploration is needed. BO and Credit Assignment Additionally, we inspect the before and after of the reward shape on representative example from the Ultrafeedback (Cui et al., 2024) dataset, where the model is asked to choose two sentences out of three that seamlessly connect with an unfinished story. Here, in Figure 6, we show several variants of the shaping method on completion from the SFT model where the tokens are colored proportionally to the reward they received after computing the shaping weights. In particular, almost all shaping methods still place importance on the terminating token. We suspect that since the reward model is trained to backpropogate on the terminal token, we cannot ignore it in order to maximize the reward function best. Specifically, we see the combination of SHAP + LIME, which highlights the explanation of the models rationale for choosing the first and second sentences. We similarly see this for the = 4 combination with all token-level scores. 9 Preprint. Under review. However, we see more uniform assignment of scores as the scalar reward was given zero weight in the final shaped score. Limitations of Current Explainability Methods While Bayesian Optimization (BO) can leverage complementary effects from multiple explainability signals, individual mechanistic interpretability methods, such as models intrinsic attention, remain fundamentally suboptimal. As shown in Figure 6, these methods often fail to align precisely with human perception of relevant features Bereska & Gavves (2024). promising direction for future research is to incorporate more fine-grained, human-aligned signals, potentially sourced from explicit token-level annotations Hayati et al. (2021) or cognitive indicators like eye movements when reading text De Langis & Kang (2022)."
        },
        {
            "title": "5 Related Work",
            "content": "Reward Shaping is technique in reinforcement learning (RL) that supplements traditionally sparse rewards with more informative reward signals (Hu et al., 2020). Early work in potential-based reward shaping (PBRS) demonstrates that reward shaping can preserve the optimal policy while reducing training time or introducing domain knowledge (Hu et al., 2020; Cao et al., 2024a). However, challenges still arise because it is often difficult to guarantee that the additional reward is helpful for any given task (Hu et al., 2020). Additionally, we note that we are not the first to inspect reward shaping or RLHF under bi-level optimization lens. For example, bi-level optimization of parameterized reward shaping (BiPaRS) (Hu et al., 2020) adaptively utilizes given reward function, optimizing both the policy itself, as well as the shaping weight for the reward function. Shen et al. (2024) Also inspects RLHF as bilevel formulation under penalty function. Other reward shaping methods focus on adaptability to overcome limitations, such as AlphaPO (Gupta et al., 2025) builds on simple policy optimization (SimPO) by introducing new scalar parameter α, demonstrating that the shape of the reward function can alter the likelihood displacement. Token-Level RLHF Other methods aim to improve reward signals by distributing the reward at the token-level, hoping to capture more nuance in particular words or phrases (Yoon et al., 2024) or discourses (Kim et al., 2025) in the generation ; examples include Token-Level Continuous Reward (TLCR), which uses token-level preference discriminator (Yoon et al., 2024), RLMEC which leverages minimum editing constraints to produce tokenlevel supervision (Chen et al., 2024b), and DRLC, which employs LLMs to identify dense positive and negative labelings within response (Cao et al., 2024b). Other work has also explored using LLMs themselves to provide the dense reward signal (Huang et al., 2024). common theme with these works is training contrastively: an initial output is compared with revised output, and the fine-grained reward signal is achieved by comparing the differences (Guo et al., 2023). Methods such as token-level PPO (TPPO) (Ouyang et al., 2024) and token-level DPO (TDPO) (Zeng et al., 2024) further support the approach of token-level fine-tuning. Bayesian Optimization (BO) has also emerged to augment LLM training and fine-tuning (Austin et al., 2024; Liu et al., 2024; Yang et al.; Gao et al., 2024; Agarwal et al., 2025; Kristiadi et al., 2024; Opsahl-Ong et al., 2024; Chen et al., 2024a). BO functions by iteratively estimating the optimal posterior predictive distribution by adding new information/candidates (Agarwal et al., 2025). Some recent work uses BO with LLMs to perform selection across discrete search space (Kristiadi et al., 2024; Opsahl-Ong et al., 2024). Other work investigates using BO to optimize prompts for LLMs, such as BOPRO (Agarwal et al., 2025), InstructZero (Chen et al., 2024a), leveraging the Bayesian Optimization to explore the prompt search space. BO can also be leveraged in pre-training, by finding the optimal weighting for checkpoint merging (Liu et al., 2024). BO can also be leveraged for uncertainty estimation (Yang et al.), bias mitigation (via win rate calibration) (Gao et al., 2024), and guided query generation (Austin et al., 2024). These tasks ultimately benefit from BOs robustness to noise (Opsahl-Ong et al., 2024) by calibrating for uncertainty in the optimization process. 10 Preprint. Under review."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we show that explainability offers an effective way to reshape rewards for token-level credit assignments. Furthermore, our findings suggest that BO is promising tool for learning and balancing different sources of token-level information for reward shaping. As combination, we show that explainable dense reward shapes optimized under Bayesian framework positively impact RL training compared to sparse rewards by accelerating learning and providing more stable updates to the value function. Furthermore, properly attributing tokens via interpretable methods also improves performance over the sparse reward on downstream tasks while also theoretically being policy invariant to stay faithful to the original reward model. In the future, we could consider incorporating contextual information into the BO weights to dynamically shape token-level rewards. For instance, we could learn set of highdimensional weights corresponding to the token-embeddings of policy models generation to dynamically compute the reward shape instead of the current statically weighted reward shape."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was mainly supported by the research gift from Grammarly and UMN Data Science Institute (DSI) Seed Grant. We also thank Minnesota NLP group members for providing us with valuable feedback and comments on the initial draft."
        },
        {
            "title": "References",
            "content": "Dhruv Agarwal, Manoj Ghuhan Arivazhagan, Rajarshi Das, Sandesh Swamy, Sopan Khosla, and Rashmi Gangadharaiah. Searching for optimal solutions with LLMs via bayesian optimization. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=aVfDrl7xDV. Sebastian Ament, Samuel Daulton, David Eriksson, Maximilian Balandat, and Eytan Bakshy. Unexpected improvements to expected improvement for bayesian optimization, 2025. URL https://arxiv.org/abs/2310.20708. David Austin, Anton Korikov, Armin Toroghi, and Scott Sanner. Bayesian optimization In with llm-based acquisition functions for natural language preference elicitation. Proceedings of the 18th ACM Conference on Recommender Systems, RecSys 24, pp. 7483, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400705052. doi: 10.1145/3640457.3688142. URL https://doi.org/10.1145/3640457.3688142. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac HatfieldDodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. URL https://arxiv.org/abs/2204. 05862. Eytan Bakshy, Lili Dworkin, Brian Karrer, Konstantin Kashin, Ben Letham, Ashwin Murthy, and Shaun Singh. Ae: domain-agnostic platform for adaptive experimentation. In NeurIPS Systems for ML Workshop, 2018. URL http://learningsys.org/nips18/assets/ papers/87CameraReadySubmissionAE%20-%20NeurIPS%202018.pdf. Leonard Bereska and Efstratios Gavves. Mechanistic interpretability for ai safetya review. arXiv preprint arXiv:2404.14082, 2024. Meng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, and Lei Meng. Enhancing reinforcement learning with dense rewards from language model critic. In Yaser AlOnaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on 11 Preprint. Under review. Empirical Methods in Natural Language Processing, pp. 91199138, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.515. URL https://aclanthology.org/2024.emnlp-main.515/. Meng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, and Lei Meng. Enhancing reinforcement learning with dense rewards from language model critic. In Yaser AlOnaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 91199138, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.515. URL https://aclanthology.org/2024.emnlp-main.515/. Alex J. Chan, Hao Sun, Samuel Holt, and Mihaela van der Schaar. Dense reward for free in reinforcement learning from human feedback, 2024. URL https://arxiv.org/abs/2402. 00782. Shreyas Chaudhari, Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande, and Bruno Castro da Silva. Rlhf deciphered: critical analysis of reinforcement learning from human feedback for llms, 2024. URL https://arxiv.org/abs/2404.08555. Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. InstructZero: Efficient instruction optimization for black-box large language models. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 65036518. PMLR, 2127 Jul 2024a. URL https://proceedings.mlr.press/v235/chen24e.html. Zhipeng Chen, Kun Zhou, Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang, and Ji-Rong Wen. Improving large language models via fine-grained reinforcement learning with minimum editing constraint. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 56945711, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.findings-acl.338. URL https://aclanthology.org/2024.findings-acl.338/. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with scaled ai feedback, 2024. URL https://arxiv.org/abs/ 2310.01377. Samuel Daulton, Sait Cakmak, Maximilian Balandat, Michael A. Osborne, Enlu Zhou, and Eytan Bakshy. Robust multi-objective bayesian optimization under input noise, 2022. URL https://arxiv.org/abs/2202.07549. Karin De Langis and Dongyeop Kang. comparative study on textual saliency of styles from eye tracking, annotations, and language models. arXiv preprint arXiv:2212.09873, 2022. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment, 2023. URL https://arxiv.org/abs/2304.06767. Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Lengthcontrolled alpacaeval: simple way to debias automatic evaluators, 2025. URL https://arxiv.org/abs/2404.04475. Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters in deep policy gradients: case study on ppo and trpo, 2020. URL https://arxiv.org/abs/2005.12729. Lukas P. Fr ohlich, Edgar D. Klenske, Julia Vinogradska, Christian Daniel, and Melanie N. Zeilinger. Noisy-input entropy search for efficient robust bayesian optimization, 2020. URL https://arxiv.org/abs/2002.02820. Preprint. Under review. Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, and Yanghua Xiao. Reward shaping to mitigate reward hacking in rlhf, 2025. URL https://arxiv.org/abs/2502. 18770. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760. Yicheng Gao, Gonghan Xu, Zhe Wang, and Arman Cohan. Bayesian calibration of win rate estimation with LLM evaluators. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 47574769, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.273. URL https: //aclanthology.org/2024.emnlp-main.273/. Geyang Guo, Ranchi Zhao, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong Wen. Beyond imitation: Leveraging fine-grained quality signals for alignment. arXiv preprint arXiv:2311.04072, 2023. Abhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham M. Kakade, and Sergey Levine. Unpacking reward shaping: Understanding the benefits of reward engineering on sample complexity, 2022. URL https://arxiv.org/abs/2210.09579. Aman Gupta, Shao Tang, Qingquan Song, Sirou Zhu, Jiwoo Hong, Ankan Saha, Viral Gupta, Noah Lee, Eunki Kim, Siyu Zhu, Parag Agrawal, Natesh Pillai, and S. Sathiya Keerthi. Alphapo reward shape matters for llm alignment, 2025. URL https://arxiv.org/abs/ 2501.03884. Shirley Anugrah Hayati, Dongyeop Kang, and Lyle Ungar. Does bert learn as humans perceive? understanding linguistic styles through lexica. arXiv preprint arXiv:2109.02738, 2021. Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, and Changjie Fan. Learning to utilize shaping rewards: new approach of reward shaping. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Chengyu Huang, Zeqiu Wu, Yushi Hu, and Wenya Wang. Training language models to generate text with citations via fine-grained rewards. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 29262949, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 161. URL https://aclanthology.org/2024.acl-long.161/. Sarthak Jain and Byron C. Wallace. Attention is not explanation, 2019. URL https://arxiv. org/abs/1902.10186. Zae Myung Kim, Anand Ramachandran, Farideh Tavazoee, Joo-Kyung Kim, Oleg Rokhlenko, and Dongyeop Kang. Align to structure: Aligning large language models with structural information, 2025. Agustinus Kristiadi, Felix Strieth-Kalthoff, Marta Skreta, Pascal Poupart, Alan AspuruGuzik, and Geoff Pleiss. sober look at llms for material discovery: are they actually good for bayesian optimization over molecules? In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Xiaoxiao Li, Yuan Zhou, Nicha C. Dvornek, Yufeng Gu, Pamela Ventola, and James S. Duncan. Efficient shapley explanation for features importance estimation under unIn Medical Image Computing and Computer Assisted Intervention MICCAI certainty. 2020: 23rd International Conference, Lima, Peru, October 48, 2020, Proceedings, Part I, pp. 792801, Berlin, Heidelberg, 2020. Springer-Verlag. ISBN 978-3-030-59709-2. doi: 10.1007/978-3-030-59710-8 77. URL https://doi.org/10.1007/978-3-030-59710-8 77. 13 Preprint. Under review. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li, Zhiying Tu, Dianhui Chu, Bo Li, and Dianbo Sui. Checkpoint merging via bayesian optimization in llm pretraining. CoRR, 2024. Jason Loeppky, Jerome Sacks, and William Welch. Choosing the sample size of computer experiment: practical guide. Technometrics, 54(4):435446, 2012. Scott Lundberg and Su-In Lee. unified approach to interpreting model predictions. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper files/paper/2017/ file/8a20a8621978632d76c43dfd28b67767-Paper.pdf. S. opez and Martha Saboya. On the relationship between shapley and owen values. Central European Journal of Operations Research, 17:415423, 12 2009. doi: 10.1007/ s10100-009-0100-8. Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning, ICML 99, pp. 278287, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. ISBN 1558606122. Krista Opsahl-Ong, Michael Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, and Omar Khattab. Optimizing instructions and demonstrations for multi-stage language model programs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 93409366, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.525. URL https://aclanthology.org/ 2024.emnlp-main.525/. Yichen Ouyang, Lu Wang, Fangkai Yang, Pu Zhao, Chenghua Huang, Jianfeng Liu, Bochen Pang, Yaming Yang, Yuefeng Zhan, Hao Sun, et al. Token-level proximal policy optimization for query generation. arXiv preprint arXiv:2411.00722, 2024. Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From to q: Your language model is secretly q-function, 2024. URL https://arxiv.org/abs/2404.12358. Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, and Etai Littwin. Vanishing gradients in reinforcement finetuning of language models, 2024. URL https://arxiv.org/abs/2310.20703. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. why should trust you?: Explaining the predictions of any classifier, 2016. URL https://arxiv.org/abs/1602. 04938. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Han Shen, Zhuoran Yang, and Tianyi Chen. Principled penalty-based methods for bilevel reinforcement learning and rlhf, 2024. URL https://arxiv.org/abs/2402.06886. Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning algorithms, 2012. URL https://arxiv.org/abs/1206.2944. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd. html. Meta Llama Team. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407. 21783. 14 Preprint. Under review. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcome-based feedback, 2022. URL https://arxiv.org/abs/2211.14275. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouedec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training, 2023. URL https://arxiv.org/abs/2306. 01693. Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Reward shaping with language models for reinforcement learning, 2024. URL https://arxiv.org/abs/2309.11489. Adam Yang, Maxime Robeyns, Thomas Coste, Zhengyan Shi, Jun Wang, Haitham Bou Ammar, and Laurence Aitchison. Bayesian reward models for llm alignment. In ICML 2024 Workshop on Structured Probabilistic Inference {&} Generative Modeling. Eunseop Yoon, Hee Suk Yoon, SooHwan Eom, Gunsoo Han, Daniel Nam, Daejin Jo, KyoungWoon On, Mark Hasegawa-Johnson, Sungwoong Kim, and Chang Yoo. Tlcr: Token-level continuous reward for fine-grained reinforcement learning from human feedback. In Findings of the Association for Computational Linguistics ACL 2024, pp. 1496914981, 2024. Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Tokenlevel direct preference optimization. In Proceedings of the 41st International Conference on Machine Learning, pp. 5834858365, 2024. Yihua Zhang, Prashant Khanduri, Ioannis Tsaknakis, Yuguang Yao, Mingyi Hong, and Sijia Liu. An introduction to bi-level optimization: Foundations and applications in signal processing and machine learning, 2023. URL https://arxiv.org/abs/2308.00788. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023a. URL https://arxiv.org/abs/2306.05685. Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. Secrets of rlhf in large language models part i: Ppo, 2023b. URL https://arxiv.org/abs/ 2307.04964. Han Zhong, Zikang Shan, Guhao Feng, Wei Xiong, Xinle Cheng, Li Zhao, Di He, Jiang Bian, and Liwei Wang. Dpo meets ppo: Reinforced token optimization for rlhf, 2025. URL https://arxiv.org/abs/2404.18922."
        },
        {
            "title": "A Computing the Explainability Scores from a LLM Reward Model",
            "content": "We discuss simple setting of computing the SHAPley scores and also well-define its terms such as the simplified mapping z, the mapping function hx, and show by example the satisfaction of the faithfulness property: g(z) (hx(z)). Consider an input text x: like apples that receives reward score r(x) = 2.1. Then we consider 3 tokens Token 1: Token 2: like 15 Preprint. Under review. Token 3: apples 1, The binary vector = [z 0, 1 which masks each token as (1) present or (0) absent. The mapping function hx reconstructs valid text from the simplified input. For example, if = [1, 0, 1] then hx(z) might return [MASK] apples and r(hx(z)) is the true reward for the masked text. We draw table for each permutation and toy example of the reward for each masked text 3] has each 2, z [0,0,0] [1,0,0] [0,1,0] [0,0,1] [1,1,0] [1,0,1] [0,1,1] [1,1,1] Masked Text hx(z) [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] like [MASK] [MASK] [MASK] apples like [MASK] [MASK] apples [MASK] like apples like apples r(hx(z)) 0 0.3 0.5 1.2 0.9 1.3 1.7 2. From this mapping, we can estimate token-level scores at intermediate states by computing the explainability metric or the feature attribution for each token to interpret each prediction of the reward model. Here, we consider the SHAPley kernel, which provides natural way to identify the marginal contribution of each token concerning the overall reward: ϕi( , x) = sx{i} s!(x 1)! x! [ (s {i}) fx(s)] (6) where is all subsets of tokens excluding token that also respect the order of the original sentence, and is the bag of all tokens in the sequence. Here, we have = {1, 2, 3} as the bag of all tokens from the sequence, and constructs each subset of tokens but also maintains the order of the original sentence. Here, we can define the terms to properly solve Eq. 2 as penalized linear regression: Ω(g) = 0 πx(z) = z!(x 1)! x! = s!(x 1)! x! L( , g, πx) = zZ (cid:2) (hx(z)) g(z)(cid:3)2 πx(z) Now, in relation to the simplified input z, we consider subset with the token {2} which corresponds to = [0, 1, 0]. Additionally {2} = {1, 2} corresponds to = [1, 1, 0]. Hence, using Eq. 3 we can compute the feature attribution for each token ϕi. We continue with an example computation of the feature attribution for the token i. Example 1. We compute the marginal contribution of element = 1 across all subsets {1}, using the Shapley value formula. Case 1: = Weight: Marginal: Contribution: s!(x 1)! x! = 0! (3 0 1)! 3! = 1 2! 6 = 2 6 = 1 3 (s {1}) (s) = ({1}) () = 0.3 0 = 0.3 1 0.3 = 0.1 16 Preprint. Under review. Case 2: = {2} Case 3: = {3} Weight: Marginal: Contribution: 1! (3 1 1)! 3! = 1 1! 6 = 1 6 ({1, 2}) ({2}) = 0.9 0.5 = 0.4 1 6 0.4 = 0.0666 0.067 Weight: Marginal: Contribution: 1! (3 1 1)! 3! = 1 1! 6 = 1 6 ({1, 3}) ({3}) = 0.6 0.2 = 0.4 1 0.4 = 0.067 Case 4: = {2, 3} Weight: Marginal: Contribution: = = 2! (3 2 1)! 3! 2 0! 6 ({1, 2, 3}) ({2, 3}) = 1.0 0.7 = 0.3 1 3 0.3 = 0.1 2 6 1 = Continuing this sequence gives eventually the rest of the terms in which summing them computes ϕ1 as: ϕ1 = 0.1 + 0.067 + 0.017 + 0.133 0.32 Continuing this with the rest of the SHAPley values gives eventually ϕ1 = 0.32, ϕ2 = 0.62, ϕ3 = 1.17 Hence, we see that the summation of all feature attributions give approximately the full reward as well as satisfying the additive surrogate g(z) = r() + 3 i=1 ϕiz r(hx(z)) In this example, it is important to note that we overview the standard SHAPley formula, which requires computing every partition of tokens, resulting in 2k operations. However, in practice, we employ the Owen values, which provably produce the same solution as SHAPley values (L opez & Saboya, 2009) and also reduce the necessary operations drastically, requiring to compute only x2 (where is the total sequence length) operations per prediction."
        },
        {
            "title": "B Proof of Policy Invariance of Explainability Methods",
            "content": "Proposition 1. Given policy πθ and Markov Decision Process = (S, A, P, γ, R), any reward shaping function in the family of additive feature attribution methods follows potentialbased shaping function and then the optimal πθ for is also optimal for the original R. Proof. We first have R(s, a, s) = R(s, a, s) + F(s, a, s) where is potential-based shaping function defined as F(s, a, s) = γΦ(s) Φ(s) for Φ : (cid:55) R. We also have by locality from Eq. 1 that g(x) Rϕ(s, a, s) or: i=1 ϕi( , x) R(s, a, s) 17 (7) Preprint. Under review. where refers to the index up to state in the sequence. Then, without loss of generality, consider the case with heuristics = {ϕs , 1} where the constant is just the identity for the original sparse reward. We omit the case for the constant, as it is known that any linear transformation also preserves the optimal policy. Then, by substituting Eq. 4 for with γ = 1 since we care about all future states equally, we have: Φ(s) Φ(s) = w2ϕs (R, x) (8) Then defining Φ(s) = w2 i=1 ϕi(R, x) we have: Φ(s) Φ(s) = w2 i=1 ϕi(R, x) w2 i=1 ϕi(R, x) = w2ϕs (R, x) (9) Since by Eq. 8 we have that i=1 ϕi(R, x) i=1 ϕi(R, x) = ϕs (R, x)"
        },
        {
            "title": "C Computation Overhead of Explanation Methods",
            "content": "To measure the computational overhead of explanation methods, we can represent the time complexity required for inference of the reward model as O(N F(k)) where F(k) represents the number of floating point operations required in one forward pass on sequence of length k, then our method is upper-bounded by O(N k2 F(k)) computations when also calculating SHAPley values. We run simple test measuring the total wall clock time, varying batch size and generation length to run inference with and without explainability methods: 18 Preprint. Under review."
        },
        {
            "title": "D Practical Implementation Details",
            "content": "Algorithm 1 Explainable Reward Shaping with Bayesian Optimization 1: Initialize Gaussian Process (GP) surrogate model: BO GP 2: for = 1, . . . , do 3: 4: 5: 6: 7: 8: 9: 10: wk BO.next action() for xi D1:n do yi πθ(xi) r(xi, yi); (r) w Update θ via θ J(πθ; r) end for i=1 r(cid:0)xi, πθ(xi)(cid:1) rval 1 BO.update(wk, rval) if rval > rbest then Update the PPO checkpoint for next BO iteration 11: 12: 13: 14: 15: end for 16: wbest BO.best params() 17: Train new PPO using w end if bestE Bayesian Optimization iterations Subsampled set of points and train PPO Compute sparse and token-level scores (Eq. 2) Compute avg. validation reward Retrieve the best parameters Link Component Dataset Anthropic/hh-rlhf Reward Model Base weqweasdas/hh rlhf rm open llama 3b SFT Model Base VMware/open-llama-7b-open-instruct Component Dataset Reward Model Base meta-llama/Llama-3.2-1B-Instruct meta-llama/Llama-3.2-3B-Instruct SFT Model Base Link princeton-nlp/llama3-ultrafeedback 19 Preprint. Under review."
        },
        {
            "title": "E Qualitative Examples",
            "content": "Prompt: Can you describe the ways in which insomnia is treated? Model SHAP (no BO) Score 4.63 Generated Text from HH-RLHF Test split The main treatments for insomnia are cognitive behavioral therapy (CBT), pharmaceutical drugs, and sleep hygiene practices. Cognitive behavioral therapy involves techniques such as sleep hygiene education, relaxation training, and cognitive restructuring, and is shown to be significantly more effective than medication or simple advice for shortterm insomnia. Medication is generally used as second-line treatment for chronic insomnia, and can include hypnotics such as benzodiazepines or nonbenzodiazepine sedatives and inhibitors of the wakepromoting hormones such as histamine and melatonin. Sleep hygiene practices such as avoiding SHAP (d = 2) Insomnia is typically treated with one of the three following methods: 6.5 SHAP + ATTN (d = 3) SHAP + LIME (d = 3) 1. Medication 2. Cognitive behavioral therapy (CBT) 3. Sleep hygiene practices, such as regular sleep schedules and avoiding heavy meals before bedtime. Medications like benzodiazepines and selective serotonin reuptake inhibitors (SSRIs) can help with sleep problems. Cognitive behavioral therapy can be helpful in changing maladaptive thoughts and behaviors around sleep, such as staying up too late, which prolongs the time spent awake. It can also help teach sleep-promoting Insomnia is treated based on the underlying cause. The most common treatments include medications such as Benzodiazepines (e.g. triazolam, temazepam, alprazolam), Z-drugs (e.g. doxepin, zolpidem), or Minor tranquillizers (e.g. buspirone). Cognitive behavioral therapy (CBT) and relaxation techniques are also common treatments. </s> Treatment for insomnia includes changes in lifestyle and environmental factors, as well as medications and other therapies. Common treatments include cognitive-behavioural therapy, which focuses on identifying and changing thought patterns and other behavioural factors that may contribute to insomnia, and medication that may help regulate the sleep cycle. Other treatments may include relaxation techniques, such as meditation or breathing exercises, which can help calm the mind and promote sleep. </s> 8. 9.31 Table 2: Generation topography across different models as input dimension increases for BO."
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "Grammarly",
        "University of Arizona",
        "University of Minnesota"
    ]
}