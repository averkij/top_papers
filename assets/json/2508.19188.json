{
    "paper_title": "FastMesh:Efficient Artistic Mesh Generation via Component Decoupling",
    "authors": [
        "Jeonghwan Kim",
        "Yushi Lan",
        "Armando Fortes",
        "Yongwei Chen",
        "Xingang Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent mesh generation approaches typically tokenize triangle meshes into sequences of tokens and train autoregressive models to generate these tokens sequentially. Despite substantial progress, such token sequences inevitably reuse vertices multiple times to fully represent manifold meshes, as each vertex is shared by multiple faces. This redundancy leads to excessively long token sequences and inefficient generation processes. In this paper, we propose an efficient framework that generates artistic meshes by treating vertices and faces separately, significantly reducing redundancy. We employ an autoregressive model solely for vertex generation, decreasing the token count to approximately 23\\% of that required by the most compact existing tokenizer. Next, we leverage a bidirectional transformer to complete the mesh in a single step by capturing inter-vertex relationships and constructing the adjacency matrix that defines the mesh faces. To further improve the generation quality, we introduce a fidelity enhancer to refine vertex positioning into more natural arrangements and propose a post-processing framework to remove undesirable edge connections. Experimental results show that our method achieves more than 8$\\times$ faster speed on mesh generation compared to state-of-the-art approaches, while producing higher mesh quality."
        },
        {
            "title": "Start",
            "content": "FASTMESH: Efficient Artistic Mesh Generation via Component Decoupling Jeonghwan Kim Yushi Lan Armando Fortes Yongwei Chen Xingang Pan* S-Lab, Nanyang Technological University https://jhkim0759.github.io/projects/FastMesh 5 2 0 2 6 2 ] . [ 1 8 8 1 9 1 . 8 0 5 2 : r Figure 1. Example of meshes generated by FASTMESH. Our approach efficiently produces 3D objects by substantially reducing the number of tokens required for generation. Note that all meshes are directly generated from point clouds."
        },
        {
            "title": "Abstract",
            "content": "Recent mesh generation approaches typically tokenize triangle meshes into sequences of tokens and train autoregressive models to generate these tokens sequentially. Despite substantial progress, such token sequences inevitably reuse vertices multiple times to fully represent manifold meshes, as each vertex is shared by multiple faces. This redundancy leads to excessively long token sequences and inefficient generation processes. In this paper, we propose an efficient framework that generates artistic meshes by treating vertices and faces separately, significantly reducing redundancy. We employ an autoregressive model solely for vertex generation, decreasing the token count to approximately 23% of that required by the most compact existing tokenizer. Next, we leverage bidirectional transformer to complete the mesh in single step by capturing intervertex relationships and constructing the adjacency matrix that defines the mesh faces. To further improve the generation quality, we introduce fidelity enhancer to refine vertex positioning into more natural arrangements and propose post-processing framework to remove undesirable edge connections. Experimental results show that our method achieves more than 8 faster speed on mesh generation compared to state-of-the-art approaches, while producing higher mesh quality. 1. Introduction Industries such as gaming, visual effects, and virtual reality rely heavily on 3D meshes as the core representation, due to their compactness and compatibility with mature rendering pipelines. As the demand for 3D assets continues to grow, creating high-quality meshes through traditional manual modeling remains time-consuming and labor-intensive. This has driven wave of research into data-driven generative models that aim to automate mesh creation, either 1 from scratch or conditioned on point clouds [5, 8, 12, 14, 37, 46, 48]. These approaches typically represent mesh as sequence of tokens and train an LLMstyle architecture to generate the tokens autoregressively. the Despite promising advances in mesh generation, next-token prediction paradigm faces key limitation: it is inefficient in both time and memory due to the long token sequences. In mesh, multiple faces often share common vertices. As result, tokenization requires repeatedly recording the same vertices across different tokens, leading to significant redundancy. Consequently, these methods typically take 30 seconds to one minute to generate mesh with 500 vertices on an A6000 GPU, and they struggle to scale to complex meshes with large number of vertices. While some works have proposed more efficient tokenization strategies to improve compression ratios [7, 22, 44, 49], repeated references to the same vertex tokens remain unavoidable when encoding geometric information into single sequence. In this paper, we introduce FASTMESH, an efficient framework for generating high-quality 3D meshes within seconds. The key idea is to decouple the generation of mesh componentsvertices and facesand process them sequentially to avoid duplication issues commonly encountered in mesh tokenization. For vertex generation, we employ an autoregressive model, as it inherently accommodates varying vertex counts. Once vertices are generated, we recognize that their connectivity primarily depends on local information, allowing for parallel processing. To facilitate this, we leverage bidirectional transformer [43] to model the relationships between vertices, from which edge connections can be directly derived in one step. These edges form an adjacency matrix that is then used to extract faces by identifying closed triangles. Thanks to this design, FASTMESH reduces the token count to approximately 23% of that required by the previous most compact tokenizer [49], significantly mitigating issues associated with long token sequences, such as high inference latency and quality degradation. Beyond this, we introduce fidelity enhancer module and prediction filtering process to further improve mesh quality. The fidelity enhancer maps discretized vertex positions, which are constrained by indexing [49], back to continuous coordinates. This results in smoother surfaces and more natural vertex distributions. In addition, the prediction filtering, post-processing technique for face generation, refines the adjacency matrix by progressively masking irrelevant connections. This reduces redundant or spurious faces while preserving the intended geometric structure, yielding meshes that are cleaner, more compact, and better aligned with downstream requirements. Extensive experiments on the Toys4K dataset [38] demonstrate that FASTMESH can generate higher-quality 3D meshes with significantly less time than previous methods. When generating meshes with 1,000 and 4,000 vertices, our method only takes about 7 seconds and 30 seconds respectively, achieving an 8 speedup over BPT [49]. In particular, since our mesh is generated in continuous space with higher vertex count, it achieves not only greater geometric accuracy but also higher visual quality. Furthermore, we show that our method can be effectively integrated with other 3D generation pipelines that produce non-mesh 3D assets, enabling the creation of artistic meshes aligned with diverse input modalities such as images or text. As illustrated in Fig. 1, FASTMESH accurately represents diverse range of complex structures conditioned on shape inputs. Our contributions can be summarized as follows: We propose an efficient framework for high-quality mesh generation, which treats vertices and faces separately, each with dedicated model suitable for the task. In particular, once the vertices are ready, all the mesh edges are generated in parallel via bidirectional transformer. Our design reduces the token count to about 23% of that required by the previous most compact tokenizer [49]. We integrate fidelity enhancer to improve the precision of vertex representations by restoring information lost during quantization. Additionally, we propose prediction filtering to reduce connectivity errors while preserving geometric structure and mesh quality. FASTMESH shows clear advantage through the Toys4K dataset by generating more detailed and accurate meshes at significantly faster speed compared to prior methods. 2. Related Work 2.1. 3D Mesh Generation Early studies [13, 16, 27, 30, 42, 51] on mesh generation, constrained by the scarcity of large-scale 3D datasets, trained on limited object categories, and often failed to generalize to out-of-distribution classes. DreamFusion [31] pioneered the use of score distillation loss to train 3D models guided by diffusion priors [33, 34], inspiring series of follow-up works [4, 6, 21, 40, 45] aimed at mitigating the lack of 3D data. With the release of large-scale 3D datasets [10, 11], feed-forward methods [15, 20, 23, 47, 52] demonstrated high-speed 3D generation, while subsequent diffusion-based approaches [9, 17, 18, 24, 50] produced more detailed and higher-quality outputs. Most of these methods utilize intermediate representations (e.g., point clouds, implicit functions, and triplane) rather than meshes to facilitate the training of 3D geometry. These are typically converted into meshes through post-processing techniques [2, 25, 35], enabling use in downstream tasks. However, such pipelines often yield dense meshes with overly smooth surfaces or cause distortions through interpolation, failing to faithfully represent the intended 3D struc2 Figure 2. (a) Overall architecture of FASTMESH. Note that our pipeline consists of two stages, where we first generate the vertices from the shape condition and then construct the faces to complete the mesh. (b) Visualization of the block-wise indexing scheme introduced by BPT [49], which we adopt for vertex tokenization. (c) Structure of the fidelity enhancer in the first stage. The 7-bit discretized vertices and shape condition are fed into the network to estimate the offset that can make the coordinate continuous value. (d) Details of face reconstruction. The generated vertices are embedded to capture inter-vertex relationships in multi-head manner. Each head computes matrix, where the output represents one feature dimension used in edge prediction. ture. Motivated by these limitations, line of research has emerged on generating complete meshes end-to-end with neural networks. Notably, PolyGen [29], MeshGPT [37], and PolyDiff [1] were among the first to leverage generative models for creating mesh triangles, producing results that closely resemble those crafted by human artists. 2.2. Shape-conditioned Artistic Mesh Generation Since the initial works on artistic mesh generation were published, MeshXL [5] and MeshAnything [8] proposed the autoregressive model to create meshes by using shape information in the form of point clouds as input. It is worth noting that these designs enhance the scalability of artistic mesh generation, since all 3D representations can be easily converted into point clouds. However, since each triangle requires three vertex coordinatesnine tokens in totalthese approaches struggle as the sequence length grows with the number of faces. This makes it difficult for the network to handle complex meshes. Meshtron [14] designed hierarchical structure to efficiently process large amounts of tokens. More recent efforts [7, 22, 41, 44, 49] have proposed compact tokenization mechanisms that encode mesh geometry into shorter sequences while preserving topological consistency. Among them, BPT [49] introduces two promising schemesblock-wise indexing and patchified aggregationachieving compression rate of about 75%. This substantial reduction in computational burden enables the network to process more complex and large-scale meshes with up to 8,000 faces. However, token compression alone does not fully resolve scalabilitythe sequence length remains lower-bounded by the number of faces, and discretization artifacts introduced by block-wise indexing persist. On the other hand, SpaceMesh [36] adopts diffusion model for vertex generation, followed by embeddings to capture inter-vertex relationships, and constructs faces by traversing edges in half-edge representation. Although this approach can generate meshes within few seconds, it suffers from low geometric accuracy and is restricted to low-poly outputs, primarily due to instability in predicting both vertices and edges. In contrast, FASTMESH is capable of generating high-complexity meshes with detailed structures while maintaining faster inference. This design not only overcomes the limitations of existing methods but also underscores the value of revisiting decoupled design principles. 3. Proposed Method The proposed method addresses the inherent redundancy in vertex usage when tokenizing entire meshes as single sequence, thereby facilitating faster inference and more effective handling of complex mesh structures. Similar to prior works [7, 41, 49], we extract shape condition features from input point clouds and normals using pretrained encoder [53]. Subsequently, the network generates vertices and faces to complete the mesh. Accordingly, the overall architecture consists of two main stages, as shown in Fig. 2(a). 3 3.2. Stage II: Face Generation In this stage, we embed the vertices by using the bidirectional transformer to capture inter-vertex relationships and predict the edges to finalize the mesh structure. Since SpaceMesh [36] demonstrates the effectiveness of the spacetime distance function [19] for modeling relational structures, we adopt this function to compute interactions between vertices, defined as follows: dst(u, v) = dst([u1, u2], [v1, v2]) = u1 v12 2 u2 v22 2 (1) where dst() denotes the spacetime distance function, and and are input vectors, corresponding to embedded vertex features in our setting. Each vector is split in half, and the Euclidean distance is computed separately for the two parts. The final value is computed by subtracting the second distance from the first, enabling the representation of both positive and negative values. Instead of using the function as the final activation, as done in SpaceMesh, we employ multi-head approach in which the feature vector is split and the function is applied independently across the heads, with each result forming the distinct dimension of the edge feature. This edge feature is then passed through prediction network, which outputs the edge logits for determining vertex connectivity(see Fig. 2). This strategy significantly improves the networks capacity to represent diverse and complex connectivity. Lastly, we threshold the logits at zero to determine edge connectivity, then construct mesh faces by identifying triplets of vertices that are mutually connected. In the training phase, we adopt the asymmetric loss [32] to guide edge predictions, emphasizing the scarce positives whose proportion declines as the vertex count grows. Additionally, we prioritize reducing false-positive edges that create holes, rather than pursuing class balance, to preserve geometric fidelity. The loss is defined as follows: AsyLoss(p) = (cid:40) L+ = (1 p)γ+ log(p), = pγ log(1 p), (2) where L+ and represent the loss values for positive and negative samples, respectively, and γ+ and γ are scaling factors controlling how steeply the loss decreases as predictions become more confident, set to 0 and 3 in our experiments. It is noteworthy that the face generation process is executed in single feedforward, which can achieve fast inference while producing sophisticated results. 3.3. Post-processing: Prediction Filtering Our face generation process occasionally predicts incorrect links, most of which do not significantly affect the mesh structure due to overlaps, but they increase computational cost. To address this, we introduce prediction filtering, (a) Example of high-resolution mesh containing 7,694 Figure 3. vertices. (b) Mesh obtained by discretizing (a) in 7-bit coordinate space, resulting in 3,636 vertices. (c) Mesh reconstructed using the same 3,636 vertices as in (b), with continuous coordinates refined by our fidelity enhancer. 3.1. Stage I: Vertex Generation Since vertex generation significantly influences the overall mesh quality, we designed our approach to ensure highquality vertex prediction. We first generate vertex sequences using an autoregressive model and then refine the vertex positions with the fidelity enhancer to achieve more precise and naturally arranged vertex distributions. To support efficient generation in the autoregressive model, we employ the block-wise indexing method to compress the vertex representation sequence effectively. This technique, originally introduced in BPT [49], is illustrated in Fig. 2(b). Specifically, block-wise indexing maps XYZ coordinates into two distinct values (i.e., the block index and the offset index), which represent the discretized 3D space separated by the global block and the local offset. Furthermore, the vertices are integrated within each block, assigning the block index once per group in the sequence. This compresses the final sequence length to roughly match the number of vertices. Where the number of faces (F ) is about twice the number of vertices (V ), and each face in the vanilla mesh representation (S) requires nine tokens, we obtain = 9F 18V . This reduction in sequence length significantly improves inference speed and stabilizes generation by reducing the accumulation of prediction error. Even though block-wise indexing efficiently compresses the sequential representation, its reliance on 3D space discretizations leads to the loss of fine geometric details. As the complexity of 3D objects increases and the number of vertices grows, higher spatial resolution becomes necessary, which further amplifies this degradation. This issue primarily stems from two factors: the shifting of vertex positions to quantized points and the reduction of points through the merging of neighbors. As shown in Fig. 3, discretizing an object with 7-bit resolution noticeably reduces geometric detail, while adjusting only the vertex positions to continuous values restores most of the original geometry. Motivated by this observation, we design the fidelity enhancer, small transformer structure, that receives 7-bit discretized vertex positions and shape information to convert each vertex into continuous coordinate by predicting the residual, as illustrated in Fig. 2(c). 4 Table 1. Quantitative comparison on the Toys4K dataset. The best and second best results in each category are bold and underlined respectively. Method MeshAnything MeshAnythingV2 TreeMeshGPT BPT FASTMESH-V1K FASTMESH-V4K CD (%) HD(%) Inf. Time(s) 12.02 10.23 5.46 5. 4.09 4.05 26.87 24.98 13.96 12.02 10.32 10.22 26.06 31.94 27.32 49.23 3.41 6.60 #V 218.6 533.3 706.3 525. 467.2 1040.6 Evaluation Metrics. To quantitatively evaluate the quality of mesh generation, we adopt Chamfer Distance (CD) and Hausdorff Distance (HD) as evaluation metrics. Both the generated and ground-truth meshes are normalized in scale, and 5,000 points are uniformly sampled from each mesh surface. CD measures the average closest-point distance between two point sets, capturing the overall structural similarity. HD captures the largest deviation by measuring the point that is farthest from any point in the other set, making it sensitive to local errors such as holes or incorrectly reconstructed regions. Additionally, we calculate the average inference time (Inf. Time) and number of vertices (#V) per mesh to demonstrate the efficiency of our framework. In the experiment for ablation studies, we also use the number of faces (#F), F1-score, and recall. 4.2. Performance Evaluation To demonstrate the effectiveness of our method, we conduct both quantitative and qualitative evaluations by comparing it with state-of-the-art approaches for shape-conditioned artistic mesh generation (i.e., MeshAnything [7, 8], TreeMeshGPT [22] and BPT [49]). For quantitative evaluation, we use the Toys4K dataset [38], which none of the compared models have used for training. In qualitative evaluation, we additionally utilize the samples from the ObjaverseXL dataset [11]. The results for comparisons are conducted without using post-processing. Quantitative. For evaluation, we used the Toys4K dataset by selecting meshes with fewer than 5,000 vertices, resulting in total of 2,063 samples. The comparison results are presented in Table 1. As shown in the results, the FASTMESH-V4K achieves the best performance, with scores of 4.05 and 10.22 on Chamfer distance and HausIn terms of generation time dorff distance, respectively. per vertex, FASTMESH-V4K is eight times faster compared with BPT while having better geometry representation. Moreover, FASTMESH-V1K demonstrates the fastest inference time, with an average of approximately 3.41 seconds per mesh, which is nearly twice as fast as FASTMESHIt also shows V4K, owing to its use of fewer vertices. Figure 4. The detailed structure of the prediction filtering. We use the initial adjacency matrix from the first face generation to perform BFS reordering. Based on this reordering, we apply the maximum bandwidth mask and the minimum candidate mask as attention masks. post-processing strategy applied after the initial adjacency matrix prediction. The bandwidth of an adjacency matrix is defined as the maximum j for which A[i][j] = 0, representing the farthest distance from the diagonal where connection occurs. We first reorder nodes via breadth-first search to reduce bandwidth and construct minimal bandwidth mask that retains edges with j B. Using this mask, we re-predict edges over two iterations, narrowing the candidate set at each step. Subsequently, we apply minimum candidate mask, which assigns each node maximum valid connection distance ri (retaining neighbors with j ri), and repeat the two-iteration refinement. The entire process is illustrated in Fig 4. This two-stage, iterative filtering reduces unnecessary faces while preserving geometric fidelity. 4. Experiments 4.1. Implementation details Model Variants. We introduce two variants of our method, i.e., FASTMESH-V4K and FASTMESH-V1K. The two variants share an identical model structure, but differ in the filtering applied to the training data: FASTMESH-V4K is trained on meshes with up to 4,000 vertices, whereas FASTMESH-V1K is trained on subset filtered to include only meshes with no more than 1,000 vertices. Datasets. To train the full network, we combine ShapeNet [3], Objaverse [10], and portion of ObjaverseXL [11], selecting samples with fewer than 4,000 vertices. We further filter out undesirable meshes with our algorithm and finally obtain dataset of 100K high-quality meshes. For inference, we use the Toys4K dataset [38], which contains 4,000 meshes with diverse range of complexity. Figure 5. Qualitative comparison of shape-conditioned mesh generation on the Toys4K dataset [38]. All meshes were generated from the same input point clouds that were sampled from the original meshes. that the stability of mesh generation by using such long sequences often fails due to the accumulation of prediction errors during generation, which further deteriorates overall performance. It is noteworthy that our proposed approach deals with shorter sequences that mitigate not only the inference speed but also the overall performance. Qualitative. We conducted qualitative evaluation using high-resolution meshes from the Toys4K dataset, as shown in Fig 5. As illustrated, the proposed method consistently produces more accurate and refined meshes compared to other approaches. Notably, other methods often fail to finalize the mesh structure, resulting in incomplete or distorted shapes. In contrast, our method generates complete and coherent geometry, effectively preserving fine-grained features throughout the generation process. This is due not only to token-length constraints but also to the growing instability at longer sequences, as shown in the first example. There are also cases where the overall shape is preserved, but detailed features are not well represented. For instance, in the fourth row of the comparison with TreeMeshGPT, the rabbit exhibits simplified facial structures, particularly poorly defined eyes and unnaturally thin arms, leading to lower visual quality. We also demonstrate the effectiveness of our fidelity enhancer through the example in the last row by showing smoother surfaces. Additionally, we present multi-angle views of our generated meshes across samples of varying complexity from the Objaverse dataset in Fig. 6, demonstrating that our results are consistently well-structured. 4.3. Ablation Studies In this subsection, we demonstrate the validity structure and loss function for face generation by the comparative experimental results according to the change of components. For the experiment, we train the model on the ShapeNet dataset and evaluate it on the Toys4K dataset by using meshes with fewer than 500 vertices in both. Moreover, we demonstrate the effectiveness of the fidelity enhancer, which locates vertex positions in natural arrangement. Edge Prediction. To demonstrate the effectiveness of the structure for the final step in the face generation, i.e., predicting edge connectivity using embedded vertex features, 6 Figure 6. Diverse view of mesh results from the proposed method in the Objaverse dataset [10]. Table 2. Performance analysis of the face generation according to the change of the edge prediction structure based on the Toys4K dataset. Table 4. Performance analysis of face generation with and without prediction filtering (PF) on the Toys4K dataset. #F and #V denote the number of faces and vertices, respectively."
        },
        {
            "title": "Function\nSpacetime\nSpacetime\nCosine\nSpacetime",
            "content": "Multi-head MLP CD (%) 7.27 5.72 5.78 5.06 HD(%) 25.32 21.41 22.05 18.55 Table 3. Performance analysis of the face generation according to the change of the loss function based on the Toys4K dataset. Loss BCE Dice Asymmetric CD(%) 11.62 10.88 5.06 HD(%) 36.27 35.68 18.55 F1(%) 69.45 67.60 70.58 Recall(%) 64.07 71.20 81.32 we compare our method against four different designs: using spacetime distance function as final activation as used in SpaceMesh [36]; computing the function multiple times across split feature groups and averaging the results as logits; replacing the spacetime distance function to cosine similarity; and following the multi-head manner which use the MLP block to predict logits. The following results are shown in Table 2. Based on the comparison between the first and second settings, the multi-head configuration provides significant improvement over the case that uses single vector. This indicates that single vector is insufficient to capture all complex relationships between vertices. Furthermore, the design using the spacetime distance function outperforms the one using cosine similarity, reaffirming the effectiveness of the spacetime distance function in graph representation, as demonstrated in previous work. [19, 36]. PF CD(%) HD(%) Inf. Time(s) 10.22 11.02 6.77 12.59 4.05 4.03 #F #V 6799.2 1040.6 2811.1 1040. Loss Function. We conducted comparative analysis of loss functions to evaluate their importance in training face generation model. We compared asymmetric loss [32] with binary cross-entropy, dice loss [28], which are commonly used in binary classification tasks. In addition to standard geometry-based metrics, we also used F1 score and recall as evaluation metrics to better understand the relationship between the accuracy of matrices and the actual quality of the mesh. The experimental results show that the asymmetric loss consistently outperformed the other loss functions across all metrics. Since the asymmetric loss forces the model to focus more heavily on learning from positive samples, it leads to more accurate predictions, despite the low proportion of positive samples in the matrix. Dice loss, which also emphasizes learning from positive values, showed improved performance in Chamfer Distance (CD) and Hausdorff Distance (HD) compared to BCE loss. However, it does not enforce this emphasis as strongly as the asymmetric loss, resulting in comparatively lower overall performance. Notably, although the F1 score remains similar across all loss functions, the model trained with asymmetric loss achieves significantly higher recall with improved shape metric scores. This suggests that positive predictions contribute more critically to the representation of mesh structures, as improved recall is strongly associated with better shape metric scores. 7 Figure 7. Comparison of the mesh results from FASTMESH-V4K according to the usage of the fidelity enhancer. Figure 8. Comparison results of the two variants of the proposed method. The meshes are generated from input point clouds extracted from the original meshes for each model. Prediction Filtering. We conducted an experiment to evaluate how effectively unnecessary faces can be removed through prediction filtering. We compare with generation metrics along with the average number of faces and vertices in each result, as shown in Table 4. As observed, the results without post-processing show that the number of faces is excessively higher than the number of vertices. Most of these extra faces are overlapping surfaces lying on the same plane. Although they do not affect the mesh geometry, they result in unnecessary computational overhead in downstream applications. Although post-processing leads to slight decrease in Hausdorff Distance due to removing and making holes in the failure cases, it improves Chamfer Distance while substantially reducing the number of faces. This indicates that the method effectively removes unnecessary faces without compromising geometric quality. Fidelity Enhancer We evaluated meshes generated with and without the fidelity enhancer under identical conditions and found clear improvements in facial details and smoother surfaces  (Fig. 7)  , demonstrating that our simple refinement network can significantly improve mesh quality by adjusting vertex arrangements. 4.4. Discussion Variants We introduced two variants that exhibit different generation behaviors by training with different data filtering. FASTMESH-V1K is aimed at accelerating the process, while FASTMESH-V4K prioritizes mesh quality. As shown in Table 1, although FASTMESH-V1K handles significantly fewer vertices than FASTMESH-V4K, the performance in terms of Chamfer Distance (CD) and Hausdorff Distance (HD) remains comparable. This is because these geometrybased metrics are more sensitive to overall structural accuracy than to fine-grained details, and FASTMESH-V1K is still able to capture the mesh structure effectively with fewer vertices. As illustrated in Fig. 8, even when given complex structure, FASTMESH-V1K successfully reconstructs the overall geometry without structural collapse. On the other hand, FASTMESH-V4K processes larger number of tokens, trading off some generation speed for higher detail. This allows it to generate meshes with greater vertex density, resulting in smoother and more elaborate surfaces. Limitations and Future Work While our method shows strong performance, several limitations remain. In vertex generation, the model can occasionally produce overly finegrained sequences that exceed the maximum vertex limit. In face generation, it may remove valid faces, retain invalid ones, or produce overlapping faces, meaning manifoldness is not guaranteed. Details and examples are provided in the supplementary material. Future work will explore relative positional encoding [39] to allow longer sequence generation, refine the face generation architecture, and introduce constraints or additional pipelines to ensure manifoldness. 5. Conclusion In this paper, we propose simple yet powerful framework for artistic mesh generation. We address the challenge of representing the mesh in single sequence by treating the vertices and surfaces of the mesh separately. We enable the autoregressive model to process only few tokens for the process, which results in faster inference speed and more stable results. To mitigate the discretization issue introduced by the vertex quantization process, we apply the fidelity enhancer, which refines vertex positions. Furthermore, we reduce the number of unnecessary faces through the prediction reduction process, making the results from our method to be in well-designed. Our experimental results demonstrate that FASTMESH delivers significantly more robustness across diverse shapes compared to prior methods, while generating highly detailed meshes up to 8 faster. We have shown that the decoupled approach offers an efficient solution for mesh generation, and we believe it will inspire further research in the field. 8 6. Acknowledgements This research is supported by cash and in-kind funding from NTU S-Lab and industry partner(s)."
        },
        {
            "title": "References",
            "content": "[1] Antonio Alliegro, Yawar Siddiqui, Tatiana Tommasi, and Matthias Nießner. Polydiff: Generating 3d polygonal meshes with diffusion models. arXiv preprint arXiv:2312.11417, 2023. 3 [2] Fausto Bernardini, Joshua Mittleman, Holly Rushmeier, Claudio Silva, and Gabriel Taubin. The ball-pivoting algorithm for surface reconstruction. IEEE transactions on visualization and computer graphics, 5(4):349359, 2002. 2 [3] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 5, 1 [4] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highquality text-to-3d content creation. In CVPR, pages 22246 22256, 2023. 2 [5] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Zhibin Wang, Jingyi Yu, Gang Yu, BIN FU, and Tao Chen. MeshXL: Neural coordinate In The Thirtyfield for generative 3d foundation models. eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 3 [6] Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, and Ziwei Liu. Comboverse: Compositional 3d assets creation using spatially-aware diffusion guidance. In ECCV, pages 128146. Springer, 2024. 2 [7] Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. MeshAnything V2: Artist-created mesh generation with adjacent mesh tokenization, 2024. 2, 3, [8] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Zhongang Cai, Lei Yang, Gang Yu, Guosheng Lin, and Chi Zhang. Meshanything: Artist-created mesh generation with autoregressive transformers, 2025. 2, 3, 5 [9] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, Liang Pan, Dahua Lin, and Ziwei Liu. 3dtopia-xl: High-quality 3d pbr asset generation via primitive diffusion. In CVPR, 2025. 2 [10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022. 2, 5, 7, 1 [11] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. 2, 5, 1 [12] Daoyi Gao, Yawar Siddiqui, Lei Li, and Angela Dai. Meshart: Generating articulated meshes with structure-guided In Proceedings of the Computer Vision and transformers. Pattern Recognition Conference, 2025. 2 [13] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: generative model of high quality 3d textured shapes learned from images. Advances In Neural Information Processing Systems, 35:3184131854, 2022. 2 [14] Zekun Hao, David W. Romero, Tsung-Yi Lin, and Ming-Yu Liu. Meshtron: High-fidelity, artist-like 3d mesh generation at scale, 2024. 2, [15] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to In The Twelfth International Conference on Learning 3d. Representations, 2024. 2 [16] Heewoo Jun and Alex Nichol. ing conditional 3d implicit functions. arXiv:2305.02463, 2023. 2 Shap-e: GeneratarXiv preprint [17] Yushi Lan, Fangzhou Hong, Shuai Yang, Shangchen Zhou, Xuyi Meng, Bo Dai, Xingang Pan, and Chen Change Loy. Ln3diff: Scalable latent neural fields diffusion for speedy 3d In European Conference on Computer Vision, generation. pages 112130. Springer, 2024. 2 [18] Yushi Lan, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, and Chen Change Loy. Gaussiananything: Interactive point cloud latent difIn International Conference on fusion for 3d generation. Learning Representations, 2025. [19] Marc Law and James Lucas. Spacetime representation In The Eleventh International Conference on learning. Learning Representations, 2022. 4, 7, 1 [20] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. 2 [21] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, pages 300309, 2023. 2 [22] Stefan Lionar, Jiabin Liang, and Gim Hee Lee. Treemeshgpt: Artistic mesh generation with autoregressive tree sequencing. CVPR, 2025. 2, 3, 5 [23] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without pershape optimization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [24] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d dif9 fusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1007210083, 2024. 2 Dai, and Matthias Nießner. MeshGPT: Generating triangle meshes with decoder-only transformers. In CVPR, pages 1961519625, 2024. 2, 3 [25] William E. Lorensen and Harvey E. Cline. Marching cubes: high resolution 3d surface construction algorithm. SIGGRAPH Comput. Graph., 21(4):163169, 1987. 2 [38] Stefan Stojanov, Anh Thai, and James M. Rehg. Using shape to categorize: Low-shot learning with an explicit shape bias. In CVPR, pages 17981808, 2021. 2, 5, 6, [26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 2 [27] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44604470, 2019. 2 [28] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric In 2016 fourth international medical image segmentation. conference on 3D vision (3DV), pages 565571. Ieee, 2016. 7 [29] Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Peter W. Battaglia. PolyGen: An autoregressive generative model of 3d meshes. 2020. 3 [30] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. [31] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben MildenIn The hall. Dreamfusion: Text-to-3d using 2d diffusion. Eleventh International Conference on Learning Representations, 2023. 2 [32] Tal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi Zelnik-Manor. In CVPR, Asymmetric loss for multi-label classification. pages 8291, 2021. 4, 7 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [35] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization. ACM TOG, 42 (4), 2023. 2 [36] Tianchang Shen, Zhaoshuo Li, Marc Law, Matan Atzmon, Sanja Fidler, James Lucas, Jun Gao, and Nicholas Sharp. SpaceMesh: continuous representation for learning manifold surface meshes. In SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers 24), page 11, New York, NY, USA, 2024. ACM. 3, 4, [37] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela [39] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2024. 8 [40] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d In Procreation from single image with diffusion prior. ceedings of the IEEE/CVF international conference on computer vision, pages 2281922829, 2023. 2 [41] Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. Edgerunner: Auto-regressive auto-encoder for artistic mesh generation. In The Thirteenth International Conference on Learning Representations, 2025. 3 [42] Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis, et al. Lion: Latent point diffusion models for 3d shape generation. Advances in Neural Information Processing Systems, 35:1002110039, 2022. 2 [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [44] Yuxuan Wang, Xuanyu Yi, Haohan Weng, Qingshan Xu, Xiaokang Wei, Xianghui Yang, Chunchao Guo, Long Chen, and Hanwang Zhang. Nautilus: Locality-aware autoencoder for scalable mesh generation, 2025. 2, [45] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillaIn Thirty-seventh Conference on Neural Information tion. Processing Systems, 2023. 2 [46] Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, and Xiaohui Zeng. LLaMA-Mesh: Unifying 3d mesh generation with language models, 2024. 2 [47] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convolutional reconstruction model. In European Conference on Computer Vision, pages 5774. Springer, 2024. 2 [48] Haohan Weng, Yikai Wang, Tong Zhang, C. L. Philip Chen, and Jun Zhu. Pivotmesh: Generic 3d mesh generation via pivot vertices guidance. In The Thirteenth International Conference on Learning Representations, 2025. 2 [49] Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, Tong Zhang, Shenghua Gao, and C. L. Philip Chen. Scaling mesh generation via compressive tokenization. CVPR, 2025. 2, 3, 4, 5, 1 [50] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 2, 3 [51] Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, and Shengping Zhang. Pix2vox: Context-aware 3d reconstruction from single and multi-view images. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26902698, 2019. 2 [52] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 2 [53] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, BIN FU, Tao Chen, Gang YU, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. 3 11 FASTMESH: Efficient Artistic Mesh Generation via Component Decoupling"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Implementation Details 7.1. Dataset Filtering For training, we used data from Objaverse [10], ObjaverseXL [11], and ShapeNet [3]. To enable the network to learn high-quality meshes, we filtered out samples that had either non-manifold geometry or undesirable structure. Non-manifoldness Rather than eliminating all instances of non-manifoldness, we retained certain meshes where the issue was minor and did not negatively affect model performance, as illustrated in Fig. 9(a). To quantify the degree of non-manifoldness, we analyzed each mesh by checking the usage count of all edges, ensuring that each edge is referenced exactly twice across faces by following the definition of manifoldness. Instead of removing all non-manifold meshes, we filtered out only those with severe non-manifold characteristicsspecifically, meshes where more than 10% of edges were used only once or more than twice, as shown in Fig. 9(b) Undesirable Structure We removed meshes containing unnecessary coplanar faces, as shown in Fig. 9(c). These structures often introduce redundant vertices that interfere with training. To find them, we analyzed vertex normals and discarded more than half of the vertices that shared identical normals, which typically indicates large coplanar regions. Remove duplicate meshes We also found significant number of overlapping meshes in the Objaverse dataset. To eliminate redundancy, we recorded the vertex and face counts for each mesh and removed duplicates by identifying meshes with equivalent vertex-face count pairs. 7.2. Architecture The autoregressive model for vertex generation is trained to predict the next token based on the full input sequence. The length of the vertex sequence indexed in block-wise manner corresponds to the sum of the number of blocks and vertices, and thus, the maximum number of tokens is determined by the upper limits of both. Note that we follow the configuration of BPT [49] by setting the block size and the offset resolution to 83 and 163, respectively. We employ absolute positional embeddings, where learnable embedding vector is added at each position in the sequence. The vocabulary size is 83+163+2, accounting for block indices, offset indices, and two special tokens (i.e., start and end tokens). The sequence is then passed through transformer Figure 9. Examples from the dataset before and after filtering. (a) non-manifold mesh with only minor defects is retained for training due to its limited impact on model performance. (b) non-manifold mesh that is removed during filtering. (c) An undesirable structure with excessive faces used to represent coplanar surface, also removed through filtering. model with causal mask using 24 transformer layers in the training phase. Lastly, the output features are projected to vocabulary-sized logits through linear layer for nexttoken prediction. At the inference phase, the autoregressive model iteratively infers the next tokens and detokenizes the final sequence into discrete vertex positions. These initial positions are then refined by the fidelity enhancer, which has six transformer encoder layers to predict continuous offsets, improving the spatial accuracy of the generated geometry. In the second stage, we reconstruct mesh faces using the given vertices and their spatial arrangement. To input the vertices into the transformer, we first convert the XYZ coordinates into three 256-dimensional vectors, resulting in 768-dimensional representation. These vertex features are then processed through 24 layers of transformer blocks to capture geometric connectivity and spatial projection. To reduce the computational cost in the edge prediction, we compress the feature to 326 size, where 32 is the number of heads and 6 is the size of each vector. After that, we extract edge features by computing the Spacetime Distance [19] for each head, as described in Section 3.2 of the main paper. Each distance value is then used as the corresponding value for each dimension in the edge feature. Lastly, we perform prediction filtering process to refine the face generation. This filtering model is fine-tuned version of the face generation network. During training, we ensured that all three types of attention masksminimum candidate mask, minimum bandwidth mask, and no maskingwere applied. To this end, we used probabilistic ratio of 7:2:1, allowing the model to learn under diverse masking conditions while still being optimized for the minimum candidate setting. This sampling strategy encourages the model to generalize across different attention conditions while optimizing performance under the minimum candidate mask, which is used during final inference. 1 where and ˆv are predicted vertices and ground truth vertices, respectively. 7.4. Training Details Our framework consists of three networks: an autoregressive model and fidelity enhancer for vertex generation, and face generation model. To accelerate training, we initialize the autoregressive model with pre-trained weights from MeshAnythingV2 [7]. We trained all models using the AdamW [26] optimizer with weight decay of 0.99, for 400 epochs, with an additional 50 epochs for post-processing. The learning rate was linearly warmed up from 106 to 104 during the first epoch. After the warm-up phase, it was gradually decreased to 2105 using cosine annealing scheduling. After completing the initial training of all components, we further fine-tuned the face generation model by applying prediction filtering. For evaluations and ablation studies, all experiments are conducted on single A6000 GPU. 7.5. Sampling Strategy In inference, we adopt sampling strategy that balances diversity and stability in the autoregressive model (i.e., vertex generation) by controlling temperature, top-k, and top-p. Temperature controls randomness; top-k limits sampling to the most probable candidates; and top-p ensures sampling from tokens with cumulative probability p. These factors directly affect sequence completeness and quality during generation. Empirically, we found temperature = 1.2, top-k = 100, and top-p = 0.9 to provide the best trade-off between mesh completeness and quality. 8. Detail Analysis in Toys4K dataset We present more detailed analysis of the Toys4K quantitative evaluation results shown in the main paper. Specifically, we break down the results by vertex count ranges and report the performance for each range in Table 5. As observed, our method consistently outperforms baselines in terms of Chamfer Distance (CD) across most categories, indicating high overall shape fidelity. However, in the 01000 vertex range, TreeMeshGPT and BPT slightly outperform our method in terms of Hausdorff Distance (HD). This suggests that autoregressive methods are highly precise when modeling short sequences, as fewer tokens are needed to represent simple meshes. Since mesh complexity increases and the required sequence length grows, these models tend to struggle with maintaining structural accuracy as shown on the Table. In contrast, our method not only captures the overall shape effectively but also demonstrates robust performance even as the mesh complexity increases, maintaining high-quality outputs in longer sequences. Figure 10. Results with and without prediction filtering. The postprocessing reduces face count while preserving mesh structure. The inference process consists of five iterative steps. In the first step, the initial adjacency matrix is generated using the face generation model. From the second step onward, the adjacency matrix is progressively refined through reordering and masking. Specifically, the second and third steps use the minimum bandwidth mask, while the fourth and fifth steps apply the minimum candidate mask to improve precision. This multi-step filtering process enables the model to correct early-stage errors and progressively eliminate unnecessary connections, thereby reducing the number of faces while preserving the original mesh shapes as shown in Fig 10. 7.3. Loss Function FASTMESH utilizes two loss functions for vertex generation and one loss function for face generation. To train the autoregressive model in the first stage, we update the transformer parameters θ to maximize the likelihood of the sequence probability (x), as defined by the following equation: (x) = (cid:89) t=1 (xt x<t, c; θ), (3) where is the number of tokens, xt is the t-th token in the sequence, and denotes the shape condition. This probability is maximized by minimizing the cross-entropy loss as follows: CELoss(x) = (cid:88) t=1 log (xt x<t, c, θ) = log (x). (4) In addition, we employ L1 loss to train the fidelity enhancer as regression problem as follows: L1Loss(v, ˆv) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 vi ˆvi, (5) 2 Table 5. Quantitative detail comparison on the Toys4K dataset. We evaluated meshes with 5,000 or fewer vertices, partitioned them into five groups in increments of 1,000, and compared performance across each group. Methods MeshAnything MeshAnythingV2 TreeMeshGPT BPT 0-1000 (872) HD CD 28.77 12.95 25.16 10.32 9.65 4.02 9.31 4.64 1000-2000 (505) HD CD 24.34 10.39 26.91 10.50 14.46 5.61 11.04 5.31 FASTMESH-V1K FASTMESH-V4K 3.94 3. 9.88 9.74 4.09 4.06 10.12 10.05 2000-3000 (320) HD CD 29.13 12.86 25.48 10.08 18.19 7.14 15.69 7.51 4.24 4.13 10.58 10. 3000-4000 (212) HD CD 22.38 10.88 21.58 9.55 19.85 7.51 17.58 8.00 4.36 4.23 11.56 11.19 4000-5000 (154) HD CD 25.80 11.88 21.35 10.11 19.86 6.85 15.33 6.14 4.24 4.34 11.28 11. Figure 11. Examples of Imageand Text-to-3D Generation. The initial meshes are generated by TRELLIS [50], from which we extract point clouds to serve as shape conditions. inputs, right: our results) (left: 9. Applications and Additional Results We utilize our framework in 3D generation models as post-processing network to generate the results to be artistic meshes. We leverage TRELLIS [50] to generate initial meshes from single image or text prompt, and sample the points to process the mesh generation in our framework. It can be seen that results are well-aligned with images and text prompts, producing structured artistic meshes as shown in Fig. 11. Furthermore, we additionally provide our results in Fig 12, 13 to further demonstrate that our method can accurately generate meshes across diverse cases. 3 Figure 12. Additional qualitative comparison in Toys4K [38] dataset 4 Figure 13. Additional qualitative comparison in Objaverse [10] and ObjaverseXL [11] dataset"
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University"
    ]
}