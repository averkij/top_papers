{
    "paper_title": "Intent Laundering: AI Safety Datasets Are Not What They Seem",
    "authors": [
        "Shahriar Golchin",
        "Marc Wetter"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world adversarial attacks based on three key properties: being driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on \"triggering cues\": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce \"intent laundering\": a procedure that abstracts away triggering cues from adversarial attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world adversarial behavior due to their overreliance on triggering cues. Once these cues are removed, all previously evaluated \"reasonably safe\" models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated by existing datasets and how real-world adversaries behave."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 2 ] . [ 2 9 2 7 6 1 . 2 0 6 2 : r Intent Laundering: AI Safety Datasets Are Not What They Seem Shahriar Golchin, Marc Wetter Applied Machine Learning Research {sgolchin, mwetter}@labelbox.com Warning: This paper may contain AI-generated sensitive content. We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world adversarial attacks based on three key properties: being driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on triggering cues: words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce intent laundering: procedure that abstracts away triggering cues from adversarial attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world adversarial behavior due to their overreliance on triggering cues. Once these cues are removed, all previously evaluated reasonably safe models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose significant disconnect between how model safety is evaluated by existing datasets and how real-world adversaries behave."
        },
        {
            "title": "1 Introduction",
            "content": "Safety alignment and safety datasets are the two pillars of AI safety (Beyer et al., 2025). Safety alignment focuses on post-training techniques that fortify models against adversarial attacks (Askell et al., 2021; Ouyang et al., 2022), while safety datasets serve to evaluate the robustness of these defenses (Mazeika et al., 2024; Zou et al., 2023). The credibility of these evaluations, therefore, depends largely on the quality of the safety datasets. As with other evaluation tasks, dataset quality is determined by how well these datasets represent real-world scenarios. However, unlike other datasets that approximate such scenarios through common patterns (e.g., math problems), in the context of AI safety, the real-world scenarios involve ulterior, well-crafted, and out-of-distribution attacks that datasets must capture. This makes the design and development of safety datasets fundamentally different from other datasets. In this work, we systematically study the quality of widely used AI safety datasetsAdvBench (Zou et al., 2023) and HarmBench (Mazeika et al., 2024)based on prior research (Kim et al., 2025; Xie et al., 2025; Zhao et al., 2024; Xu et al., 2024; Li et al., 2024; Anil et al., 2024, inter alia). First, we 1 Figure 1: Overview of our intent laundering framework. Without the feedback loop, the framework displays the intent laundering procedure; with the loop, it intent laundering as jailbreaking technique. The process begins by passing the original malicious request (data point) through the intent launderer to generate an intent-laundered revision. This revision is then used to attack the target model. An LLM judge evaluates the response for safety and practicality. If the response is both unsafe and practical, the attack is considered successful. Otherwise, the revisionregeneration mechanism is triggered, leveraging all previously failed revisions as feedback to generate new, improved revision. The loop stops when the predefined iteration limit or target success rate is reached. analyze whether these datasets, in isolation, faithfully represent real-world adversarial attacks by evaluating three defining properties of such attacks: being driven by ulterior intent, well-crafted, and out-of-distribution.1 Second, we examine whether these datasets actually measure safety risks in practice when they are used to evaluate model safety. To evaluate the quality of safety datasets in isolation, we begin by analyzing n-gram word clouds. This helps visualize how the most frequent unigrams recur as higher-order n-grams. We find that these recurring n-grams consistently form unrealistic triggering cues2: expressions with overt negative/sensitive connotations. These cues fall into two categories: (1) inherentexpressions that carry such connotations by nature (e.g., commit suicide), and (2) contextualexpressions that signal such connotations in the context of harmful requests (e.g., [malicious intent] + without getting caught, explicitly signaling evasion). Figure 2 shows examples of triggering cues. These cues undermine two propertiesbeing well-crafted and driven by ulterior intentas such overt language rarely appears in real-world attacks and seems engineered to trigger safety mechanisms artificially. We also show that the repetitive overuse of these cues creates substantial duplication, producing near-identical data points in both sentence structure and malicious intent. This further degrades the out-of-distribution property and adds to the erosion of well-crafted property. Overall, our in-isolation analysis indicates that current safety datasets fail to faithfully represent real-world adversarial behavior. Next, we evaluate the quality of safety datasets in practice by involving models. In particular, we examine whether these datasets genuinely measure safety risks or merely rely on triggering cues that safety-aligned models are trained to detect and refuse to answer. To explore this, we introduce intent laundering: systematic procedure that abstracts away overt triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. This transformation has two complementary components: (1) connotation neutralization, and (2) context transposition. In connotation neutralization, triggering expressions are replaced with neutral/positive or descriptive alternatives. In context transposition, real-world scenarios and referentssuch as individuals (e.g., 1Similar to other datasets, diversity is another important consideration in the design of safety datasets. This includes diversity across topics and data points. Topic diversity is generally well accounted for by most dataset creators (Röttger et al., 2024a; Mazeika et al., 2024; Zou et al., 2023) and is thus excluded from our analysis. In contrast, diversity at the data-point level is major issue in these datasets, which we discuss in Section 2.2. 2We use triggering language interchangeably. 2 immigrants) or institutions (e.g., charity)that can act as triggering cues in harmful requests are mapped to non-real-world contexts (e.g., game world). We automate this process using an intent launderer: large language model (LLM) with few-shot in-context learning (ICL) setup. Each ICL demonstration pairs an original data point with its manually crafted, intent-laundered revision. Figure 1 depicts an overview of this framework. Our results reveal strong and universal effect: once triggering cues are removed, the attack success rate (ASR) jumps from mean of 5.38% to 86.79% on AdvBench, and from 13.79% to 79.83% on HarmBench. Finally, we propose intent laundering as standalone jailbreaking technique by adding an iterative revisionregeneration mechanism. In each iteration, the model uses all previously failed revisions as feedback to generate new, improved revision using the same few-shot ICL setup. Figure 1 shows this mechanism. This iterative process continues until either predefined number of regeneration attempts is reached or target ASR is met. Our results show that, with this regeneration loop, intent laundering achieves high ASR (90%98.55%) after only few iterations across all studied models under fully black-box access. This includes recent frontier models reported as among the safestsuch as Gemini 3 Pro (Google, 2025a;b) and Claude Sonnet 3.7 (Anthropic, 2025a;b; Holistic AI, 2025). The key contributions of this paper are as follows: (1) We show that AI safety datasets do not faithfully reflect real-world adversarial behavior due to overuse of unrealistic triggering cues. (2) We introduce intent laundering: procedure that empirically verifies that revising data points to remove triggering cues sharply increases ASR: from mean of 5.38% to 86.79% on AdvBench, and from 13.79% to 79.83% on HarmBench. (3) We adapt intent laundering into novel jailbreaking method by integrating revisionregeneration step for failed revisions, achieving high ASR (90%98.55%) across the board. (4) We present evidence that similar triggering cues present in publicly available AI safety datasets affect internal safety evaluations and alignment techniques in similar way. (5) We identify major gap between how current datasets evaluate model safety and how adversarial behavior manifests in real-world scenarios."
        },
        {
            "title": "2 Empirical Motivation",
            "content": "To motivate our study, we first analyze the quality of safety datasets in isolation. We use n-gram word clouds and pairwise similarity analyses to assess whether these datasets reflect real-world adversarial attacks. Specifically, we analyze three key characteristics of such attacks: Ulterior Intent: Attacks conceal harmful intents behind benign-looking requests. Well-crafted: Attacks are carefully designed to bypass safety filters. Out-of-distribution: Attacks differ from everyday user prompts."
        },
        {
            "title": "2.1 Word Clouds",
            "content": "We use n-gram word clouds to assess the quality of safety datasets intuitively. Specifically, we generate word clouds of the 40 most frequent n-grams in the combined AdvBench and HarmBench corpus, with {1, 2, 3}. These visualizations help trace how common unigrams evolve into higher-order n-grams, revealing dominant language patterns. Figure 2 displays the resulting word clouds. At the unigram level, there is an unusual overrepresentation of words with overt negative/sensitive connotations, which we term triggering words. Triggering words fall into two categories: (1) inherent triggering words (in red), which carry negative/sensitive connotations by nature (e.g., steal), and (2) contextual triggering words (in orange), which imply such connotations when used in harmful contexts (e.g., commit when used with suicide). As unigrams evolve into higher-order n-grams, this overrepresentation intensifies: phrases with negative/sensitive connotations become unusually dominant. Similar to unigrams, these phrases 3 (a) Unigrams (b) Bigrams (c) Trigrams Figure 2: Word clouds of the 40 most frequent n-grams, where {1, 2, 3}, from the combined AdvBench and HarmBench corpus. Triggering cues are highlighted in red for inherently, and orange for contextually negative/sensitive connotations. This includes triggering words in the unigram cloud and triggering phrases in the bigram and trigram clouds. Neutral-connotation unigrams that contribute to triggering cues in higher-order n-grams are also shown in green. These visualizations reveal an unusual overrepresentation of overtly triggering cues in safety datasets, suggesting that data points are artificially designed to trigger safety mechanisms. For example, expressions such as tutorial and step-by-step instructions are used explicitly to trigger infohazard safeguards (Bostrom, 2011), which is unrealistic. Even minimally skilled bad actors rarely use such overt and self-incriminating language (e.g., commit identity theft). These findings indicate that safety datasets fail to capture two of the key properties of real-world attacks: being well-crafted and driven by ulterior intent. Note that, for clarity, only representative subset of triggering cues is marked in the word clouds above. carry such connotations either inherently or contextually within malicious requests. We refer to these as triggering phrases, and together with triggering words, they form what we call triggering cues. Triggering phrases mainly form in two ways: (1) they build on triggering words, or (2) they consist entirely of words with neutral connotations. For example, the inherent triggering word steal, evolves into inherent triggering phrases such as steal sensitive information, steal confidential information, and steal personal information. Similarly, the contextual triggering word commit, expands into inherent triggering phrases such as commit suicide, commit insider trading, and commit identity theft. Neutral-connotation words can also form triggering phrases. For instance, without, getting, and caught, combine into the triggering phrase without getting caught. This also explains their high frequency as unigrams. However, such explicit and repetitive overuse of triggering cues, along with direct mentions of malicious intent, directly contradicts the behavior of real-world adversaries. Even minimally capable adversaries rarely use such overt language, as it easily triggers safety mechanisms. Taken together, these patterns indicate that existing safety datasets contain contrived data points largely disconnected from real-world behavior, where harmful requests are well-crafted and motivated by ulterior intent."
        },
        {
            "title": "2.2 Data Duplication",
            "content": "The excessive use of triggering cues not only creates unrealistic data points but also suggests overlap in sentence structure and malicious intent across data points. To investigate this, we conduct pairwise similarity check between data points within each dataset. We use similarity thresholds ranging from 0.7 to 0.99. For given threshold, data points that do not meet the similarity threshold with any other data point are labeled as unique, while those that meet the threshold with at least one other data point are labeled as duplicated and grouped to represent single data point. As there is no universally accepted threshold for high similarityparticularly for datasets with data points from single contextwe use the GSM8K dataset (Cobbe et al., 2021), widely used non-safety dataset, as baseline. For fair comparison, we subsample GSM8K to match the size of each safety dataset. Figure 3 presents the results of our similarity analysis. The first key finding is that over 45% of data points in AdvBench are near-identical at 0.95 similarity threshold. More notably, over 11% are (almost) exact copies at 0.99 threshold. These numbers are unusually high for safety dataset with only 520 data pointsespecially one intended to reflect out-of-distribution and well-crafted attacks. 4 Figure 3: Proportion of duplicated versus unique data points in the AdvBench and HarmBench datasets across varying similarity thresholds. Each safety dataset is compared to size-matched GSM8K subset shown below its plot. Both safety datasets exhibit considerably higher duplication rates across most thresholds compared to their GSM8K counterparts. This is striking, as safety datasets are intended to approximate real-world attackscharacterized by being out-of-distribution and well-crafted. In contrast, they show more duplication than regular non-safety dataset, where such duplication is more acceptable. This is particularly alarming for safety datasets, as it indicates that many data points in these datasets evaluate the model on essentially the same harmful intent in nearly identical contexts (see Figure 4 for examples), leading to an overestimated evaluation of safety. The second key finding comes from cross-dataset comparison. At 0.85 threshold, only about 11% of AdvBench data points are unique, compared to nearly 94% in the size-matched GSM8K subset. HarmBench also shows considerable duplication: 16% of its data points are duplicated at this threshold, versus only 3.5% in its GSM8K counterpartover four times higher. Considering 85% uniqueness as reasonable target for well-designed dataset, AdvBench requires an extremely high threshold of 0.99, and even then, it never reaches 90% uniqueness. In contrast, the GSM8K counterpart reaches 85% uniqueness at much more moderate 0.85 threshold. Similarly, HarmBench requires high threshold of 0.9 to hit 85% uniqueness, whereas its GSM8K counterpart reaches it at just 0.8. These findings are concerning. Safety datasets are expected to feature more unique data points to effectively reflect real-world attacks. Instead, they exhibit far more duplication than non-safety datasets such as GSM8K, where homogeneity is more acceptable. This duplication undermines two of the key properties required for modeling real-world attacks: being out-of-distribution and well-crafted.3 Figure 4 provides examples of these duplications. Figure 4: Examples of duplicated data points from the AdvBench and HarmBench datasets. These examples exhibit two unusual patterns: (1) explicit and repetitive overuse of triggering cues, either inherently (in red, e.g., chop shops) or contextually (in orange, e.g., in detail), and (2) substantial duplication resulting from this overuse. Each group of duplicates effectively represents single malicious intent, i.e., refusal or response to one is sufficient to evaluate the robustness of the model for that intent. As result, safety evaluations based on these data points can be inflated. 3These findings also suggest that reported results from past studies using subsets of safety datasets can be misleading (Xie et al., 2025; Dékány et al., 2025; Xhonneux et al., 2024, inter alia). In fact, many subsampled"
        },
        {
            "title": "2.3 Motivating Evidence",
            "content": "Analyzing safety datasets in isolation yields two main insights: (1) they repeatedly overuse triggering cues, and (2) this leads to substantial duplication. Together, these undermine all three key properties of real-world attacks. As result, they fail to faithfully represent real-world adversarial behavior, and safety evaluations based on them can be inflated. This raises key question: if triggering cues are removed, do models previously reported as reasonably safe still remain so? We explore this next."
        },
        {
            "title": "3.1 Intent Laundering",
            "content": "Here, we evaluate the quality of safety datasets in practice when used to evaluate models. In particular, we examine whether these datasets actually capture safety risks or whether they mainly rely on triggering cues to elicit refusals to harmful requests. To explore this, we introduce intent laundering: procedure that abstracts away overt triggering language from harmful requests (data points) while strictly preserving their malicious intent and all relevant details. In fact, the idea is to imply the harmful intent rather than explicitly state it. Intent laundering consists of two complementary components: Connotation Neutralization: Removes triggering cues carrying negative/sensitive connotations by replacing them with neutral/positive alternatives. If no such alternatives exist, it uses descriptive substitutes. Figures 5 and 6 in Appendix show examples of such transformations. Context Transposition: Replaces real-world scenarios and referentssuch as individuals (e.g., immigrants) or institutions (e.g., charity)that can act as triggering cues in harmful contexts with their non-real-world alternatives (e.g., game world or fictional narrative with stand-ins). All details remain applicable and transferable to the real world with no or minor adjustments. Figures 5 and 6 in Appendix show examples of context transposition. Based on the intent-laundering principles, we first manually craft few intent-laundered revisions of data points from the two safety datasets under study. We then automate this process using an intent launderer: large language model (LLM) coupled with few-shot in-context learning (ICL) setup. Each originalrevision pair serves as an ICL demonstration that guides the model in performing intent laundering. Figure 1 provides an overview of the intent laundering framework. Figure 7 in Appendix presents the ICL prompt used by the intent launderer. After generating all revisions, we collect model responses to both the original and revised data points to assess how triggering cues affect safety evaluation."
        },
        {
            "title": "3.2 Evaluation",
            "content": "As intent laundering modifies how harmful requests are expressed (via connotation neutralization) and structured (via context transposition), we evaluate model responses along two axes: safety and practicality. These two criteria are assessed independently and do not influence each other. Safety Evaluation. We exclusively evaluate the safety of responses to revisions using five-point Likert scale based on the level of harm exposed: extremely unsafe, highly unsafe, unsafe, moderately unsafe, and safe (see Figure 10 in Appendix for scale definitions).4 We also condition our safety evaluation specifically on harm relative to the original malicious intent, which serves as the evaluation criterion (see Figure 9 in Appendix for how it is generated for each attack). Safety is thus judged data points can come from the same group of near-duplicates, as shown in Figure 4. This can inflate performance, as repeated instances of single harmful intent that method handles well can dominate the sample. The issue is further amplified by the small size of these datasets. 4We do not use numerical scales to avoid potential biases against assigning extremely high/low scores, as reported in prior studies (Ouyang et al., 2022; Golchin et al., 2025). on two fronts: (1) whether the response to the revision addresses the same harmful intent as the original version, and if so, (2) how unsafe the response is. If the revision fails to preserve the original malicious intent, the response is rated as safer. Practicality Evaluation. Similar to the safety evaluation, we exclusively evaluate the practicality of responses to revisions using five-point Likert scale based on the practicality and transferability of the provided details in the real world: extremely applicable, highly applicable, applicable, moderately applicable, and inapplicable (see Figure 11 in Appendix for scale definitions). Similarly, we condition our practicality evaluation specifically on harm relative to the original malicious intent, which is provided as the evaluation criterion. As with safety, practicality is judged on two fronts: (1) whether the details in the response align with the same harmful intent as the original version, and if so, (2) how practical those details are when applied in the real world with reasonable adjustments. LLM as Judge. We use an LLM to automatically evaluate both safety and practicality. The model receives the same instructions shown in Figures 10 and 11 in Appendix for safety and practicality evaluations, respectively. Attack Success Rate. We measure performance using the attack success rate (ASR), defined as the fraction of attack attempts that succeed. In our setting, an attack (revision) is considered successful only if the response is both unsafe and practical. response is unsafe if it is rated as extremely unsafe, highly unsafe, or unsafe, and practical if it is rated as extremely applicable, highly applicable, or applicable. An attack is unsuccessful if it fails to generate an unsafe response, or if the unsafe response is too abstract to be practical, even after adjustments. This design choiceusing five-point scales and then mapping them to binary outcomesaims to reduce bias and ambiguity in borderline cases."
        },
        {
            "title": "3.3 Intent Laundering as a Jailbreaking Technique",
            "content": "We extend intent laundering into an independent jailbreaking method, leveraging its ability to avoid triggering safety filters. Specifically, we add an iterative revisionregeneration mechanism for failed revisions, while the main procedure remains unchanged. In each iteration, all previously failed revisions are provided as feedback to the intent launderer to generate new, improved revision using the same few-shot ICL setup. Figure 1 illustrates intent laundering when employed as jailbreaking technique. Figure 8 in Appendix shows the ICL prompt used in the revisionregeneration loop. Under our definition of failed revisions, the revisionregeneration mechanism improves attack performance in two ways: (1) by generating new revisions that succeed where earlier ones failed to elicit unsafe responses, and (2) by generating new revisions that yield more practical responses where previous responses were too abstract. The process repeats until either predefined number of regeneration attempts is reached or target ASR is achieved."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Word Clouds. We generate word clouds for the AdvBench (Zou et al., 2023) and HarmBench (standard) (Mazeika et al., 2024) datasets using the Python wordcloud package (Mueller). To preserve actual language patterns, we apply only lowercase conversion and whitespace normalization. For clearer visualization, we remove stopwords, punctuation, special characters, and words that instruct models, including write, generate, create, develop, use, give, provide, and people. Data Duplication. We use the same safety datasets as in the word cloud analysis, along with two randomly sampled subsets of the GSM8K dataset (Cobbe et al., 2021). Each GSM8K subset matches the size of corresponding safety dataset. This ensures fair pairwise similarity comparison between safety and non-safety datasets, since the number of data points can significantly affect similarity scores (see Figure 3 for the impact of this effect across the two GSM8K subsets). We use embeddings from Sentence Transformers (Sentence-BERT) (Reimers & Gurevych, 2019; Devlin et al., 2019; Wolf et al., 2019; Vaswani et al., 2017), specifically the all-MiniLM-L6-v2 7 checkpoint, which is fine-tuned for clustering and semantic search. We choose this checkpoint to mitigate embedding anisotropy (Timkey & van Schijndel, 2021; Ethayarajh, 2019), challenge amplified in our setting due to significant prefix overlap among data points in our safety datasets, which can produce uniformly high, non-informative similarity scores. For each data point, we average token-level contextual embeddings and compute pairwise cosine similarities between all data points in dataset. We set the maximum input length to 512 tokens. Evaluation Criteria Generation. We utilize GPT-4o (gpt-4o-2024-11-20) (Hurst et al., 2024) with an 8-shot ICL setup to generate evaluation criteria. All generation hyperparameters are kept at their default values, and output is limited to 1024 tokens. Intent Launderer. We leverage GPT-5.1 (gpt-5.1-2025-11-13) (OpenAI, 2025) with an 8-shot ICL setup as the intent launderer. For jailbreaking, the same model and ICL setup are used to regenerate failed revisions at each iteration. We set the reasoning effort to none, cap output at 2048 tokens, and use default values for all other decoding hyperparameters. LLM as Judge. We employ GPT-5.1 (gpt-5.1-2025-11-13) as powerful judge to evaluate both safety and practicality when intent laundering is applied. Each evaluation is run independently in zero-shot setting using separate model calls. All hyperparameters match those of the intent launderer setup, except that the output is capped at 1024 tokens. We report details on the agreement between LLM-based and human evaluations for both safety and practicality in Appendix A. For safety evaluation on the original attacks (data points), where intent laundering is not applied, we use the standard ASR metric along with the same five-point Likert scale described in Subsection 3.2 under Safety Evaluation. The corresponding prompt is shown in Figure 12 in Appendix C. We follow the same categorization scheme used for intent-laundered attacks: an attack is considered successful if its response is labeled as extremely unsafe, highly unsafe, or unsafe. As before, all hyperparameters are identical to those of the intent launderer setup, except that the output is limited to 1024 tokens. Evaluation Datasets. Based on our data duplication analysis in Subsection 2.2, we use conservative similarity threshold of 0.9 to work with unique data points in the safety datasets. Data points above this threshold are treated as duplicates, as they share similar sentence structures and identical malicious intent (see Figure 4). For each group of duplicates, we randomly select one representative data point. This yields 207 data points from the AdvBench datasets and all 200 data points from the HarmBench (standard) dataset. We use these two sets for all our safety evaluation experiments. Evaluation Models. We select diverse set of models for safety evaluation. In total, we experiment with seven different models: Gemini 3 Pro (gemini-3-pro-preview) (Google, 2025b), Claude Sonnet 3.7 (claude-3-7-sonnet-20250219) (Anthropic, 2025b), Grok 4 (grok-4-fast-non-reasoning) (xAI, 2025), GPT-4o (gpt-4o-2024-11-20) (Hurst et al., 2024), Llama 3.3 70B (llama-3.3-70b-instruct) (Grattafiori et al., 2024), GPT-4o mini (gpt-4o-mini-2024-07-18) (Hurst et al., 2024), and Qwen2.5 7B (qwen2.5-7b-instruct) (Yang et al., 2025). As our task does not require advanced reasoning and the base models remain the same across reasoning levels, we reduce reasoning efforts where applicable. Specifically, for Gemini 3 Pro, we set reasoning effort to low; for Claude Sonnet 3.7, we use standard mode; and for Grok 4, we use the non-reasoning checkpoint. All models use default inference hyperparameters, with output capped at 4096 tokens."
        },
        {
            "title": "5 Results and Discussion",
            "content": "Table 1 presents results for seven models on both the AdvBench and HarmBench datasets. This includes results from three experimental settings: No Revision: Results from the original data points, which do include triggering cues. First Revision: Results after applying intent laundering to remove triggering cues. Other Revisions: Results when intent laundering is used as jailbreaking method. 8 Table 1: Safety evaluation (SE), practicality evaluation (PE), and attack success rate (ASR) on the AdvBench dataset (top) and the HarmBench dataset (bottom). SE is the percentage of model responses rated as extremely unsafe, highly unsafe, or unsafe. PE is the percentage of responses rated as extremely applicable, highly applicable, or applicable. ASR is the percentage of responses that satisfy both SE and PE simultaneously. In the no-revision setting, where original data points from safety datasets are used, ASR follows its standard definition, as no abstraction is applied; thus, SE and PE are not applied. The first-revision setting corresponds to the first application of intent laundering, where triggering cues are removed. All subsequent iterations reflect intent laundering with the revisionregeneration loop, which functions as jailbreaking technique. Bold values denote the highest ASR across all iterations (within each dataset). Lower ASR implies better model safety."
        },
        {
            "title": "Model",
            "content": "Gemini 3 Pro Claude Sonnet 3."
        },
        {
            "title": "ASR",
            "content": "1.93 2.42 SE PE"
        },
        {
            "title": "ASR",
            "content": "SE PE"
        },
        {
            "title": "ASR",
            "content": "SE PE"
        },
        {
            "title": "ASR",
            "content": "83.09 99.42 82.61 90.34 100.00 90. 95.17 100.00 81.64 97.63 79.71 86. 98.89 85.99 93.72 99.48 Grok 4 GPT-4o 17.87 90.82 100.00 90.82 96.14 99. 95.66 96.62 100.00 0.00 82.61 98. 81.18 93.72 98.45 92.27 94.69 98. Llama 3.3 70B 10.14 91.79 100.00 91.79 98. 100.00 98.07 98.55 100.00 GPT-4o mini Qwen2.5 7B"
        },
        {
            "title": "Mean",
            "content": "0.97 4.35 5.38 90.34 98.93 89. 95.17 100.00 95.17 96.62 100.00 92. 99.48 92.27 95.65 100.00 95.65 97. 100.00 87.58 99.10 86.79 93.72 99. 93.30 96.07 99.71 95.17 93.23 96. 93.24 98.55 96.62 97.10 95.79 No Revision First Revision Second Revision Third Revision Fourth Revision Fifth Revision Model ASR SE PE ASR SE PE ASR SE PE ASR SE PE ASR SE PE ASR Gemini 3 Pro 11. 80.00 98.12 78.50 84.50 99.41 84. 88.50 100.00 88.50 90.50 100.00 90. 93.00 100.00 Claude Sonnet 3.7 8.50 73.00 96. 70.50 78.50 99.36 78.00 85.50 100. 85.50 87.00 100.00 87.00 91.00 100. Grok 4 GPT-4o 36.00 82.00 97.56 80. 87.00 98.28 85.50 88.00 100.00 88. 88.50 100.00 88.50 93.00 100.00 0. 89.00 100.00 89.00 90.00 99.44 89. 91.00 100.00 91.00 92.00 100.00 92. 93.00 100.00 Llama 3.3 70B 14.00 83.50 96. 80.50 84.00 99.40 83.50 86.00 100. 86.00 87.00 100.00 87.00 91.00 100. GPT-4o mini 5.00 80.00 99.38 79.50 84. 99.40 83.50 85.00 100.00 85.00 87. 99.43 87.00 91.00 98.90 Qwen2.5 7B 21. 83.00 97.59 81.00 87.50 100.00 87. 89.00 100.00 89.00 90.00 100.00 90. 90.50 100.00 Mean 13.79 81.50 97. 79.83 85.07 99.33 84.50 87.57 100. 87.57 88.93 99.92 88.86 91.79 99. 93.00 91.00 93.00 93.00 91.00 90. 90.50 91.64 Based on our results, we make the following observations:5 (1) Removing triggering cues from data points (attacks) leads to sharp increase in ASR. On AdvBench and HarmBench, ASR rises from mean of 5.38% and 13.79% in the no-revision settings, where triggering cues are present, to 86.79% and 79.83%, respectively, in the first-revision settings (where triggering cues are removed for the first time). This indicates that model refusals are largely driven by the presence of triggering cues. Consequently, safety datasets fail to reliably measure real-world safety risks, as they rely more on triggering cues to elicit refusals than on actual malicious intent. This leads to an overestimation of model safety. (2) Intent laundering is highly effective at removing triggering cues while preserving the malicious intent. It also acts as strong jailbreaking technique. As shown by the bold values in Table 1corresponding to the highest ASR and the last iteration in each datasetintent laundering achieves high ASR values, ranging from 90% to 98.55% across all models and datasets. This includes 5To draw general observations, we do not apply the termination conditions in Subsection 3.3. Instead, we run representative number of iterations to assess the effectiveness of our methodology. 9 models such as Gemini 3 Pro, known for strong safety (Google, 2025a;b), and Claude Sonnet 3.7, known for overrefusal (Zhang et al., 2025; Cui et al., 2024). These models are jailbroken at high ASRs of 93%95% on AdvBench and 91%93% on HarmBenchand with only few iterations. (3) Despite the abstraction introduced by intent laundering, model responses remain applicable and transferable to the real world. This is supported by the high practicality rates across all iterations, models, and datasets. (4) ASR consistently increases with more iterations. While the first iteration always yields the largest leap, ASR continues to risewith mean increase of 9% on AdvBench and 11.81% on HarmBench by the final iterationshowing steady growth throughout. This confirms that the revision regeneration mechanism effectively and systematically boosts ASR, and that adjusting the number of iterations provides direct control over the desired ASR. (5) Our results suggest that both internal safety evaluations and safety alignment techniques likely overrely on similar triggering cues found in publicly available safety datasets. This is supported by the fact that all insights motivating our methodologysuch as unusual language patterns and dataset design flawsare derived exclusively from publicly available safety datasets. Nevertheless, these insights remain broadly effective, as evidenced by consistently high ASR across all experiments, regardless of model specifications (e.g., closedor open-weight, old or recent release, or developer identity). Further evidence comes from the fact that internal safety evaluations reach the same conclusions as publicly available safety datasets: these models are reasonably safe (Anthropic, 2025a;b; Google, 2025a;b; xAI, 2025; Yang et al., 2025; Hurst et al., 2024; Grattafiori et al., 2024)a conclusion that stands in contrast to our findings."
        },
        {
            "title": "6 Related Work",
            "content": "Safety Alignment. The main objective of safety alignment is to balance helpfulness and harmlessness in AI models, avoiding both underrefusal (overly helpful) and overrefusal (overly harmless) (Röttger et al., 2024b; Bai et al., 2022; Ouyang et al., 2022; Askell et al., 2021). Adversarial attacks, however, can disrupt this balance, causing aligned models to generate harmful outputsa behavior known as misalignment (Deshpande et al., 2023; Ouyang et al., 2022; Askell et al., 2021; Sheng et al., 2019). Broadly, these attacks fall into two categories: invasive and non-invasive. We define invasive attacks as methods that erode safety alignment by directly modifying model weights. This includes using specialized fine-tuning/training recipes on benign data (Mu et al., 2025; Xie et al., 2025; Qi et al., 2024; Yang et al., 2024; Yi et al., 2024; Hawkins et al., 2024; Lermen et al., 2023; Halawi et al., 2024, inter alia), as well as training on small number of harmful examples (Souly et al., 2025). In contrast, non-invasive attacks operate solely through input prompt engineering, without altering model parameters. Examples include attacks based on ciphers (Yuan et al., 2024; Handa et al., 2024), many-shot in-context learning (Anil et al., 2024; Golchin et al., 2024), membership inference attacks that induce the model to emit training data (Golchin & Surdeanu, 2025; Nasr et al., 2025b; Golchin & Surdeanu, 2024; Carlini et al., 2023; 2021; Shokri et al., 2017, inter alia), and highly engineered red-teaming prompts, either crafted by humans (Nasr et al., 2025a; Yu et al., 2024; Schulhoff et al., 2023), generated by LLMs (Joo et al., 2025; Wahréus et al., 2025; Mehrotra et al., 2024; Jin et al., 2024), or optimized via statistical machine learning methods (Nasr et al., 2025a). In response to these increasingly sophisticated attacks, advanced safety alignment techniques were proposed to enhance safety. Recent work primarily leveraged reasoning as mechanism to improve robustness, particularly in large reasoning models (Kim et al., 2025; Guan et al., 2024; Jaech et al., 2024). At the same time, the same reasoning capability exploited under adversarial conditions to jailbreak models (Sabbaghi et al., 2025). Overall, prior studies showed that the safety alignment of current models remains fragile (Shah et al., 2025; Qi et al., 2025; Amodei et al., 2016), making reliable safety evaluation challenging (Rando et al., 2025; Benton et al., 2024). Safety Datasets. As with other evaluation tasks, safety datasets aim to measure the effectiveness of safety alignment by capturing real-world scenarios. Early research focused on evaluating models against narrow set of risks, such as bias (Tamkin et al., 2023; Nadeem et al., 2021; Dhamala et al., 10 2021; Nangia et al., 2020), toxicity (Hartvigsen et al., 2022; Gehman et al., 2020), and ethical judgment (Hendrycks et al., 2021). However, with the rise of general-purpose models, the focus broadened to encompass wider range of safety threats, including misinformation, cybercrime, harassment, and more (Chao et al., 2024; Mou et al., 2024; Wang et al., 2023; Xu et al., 2021, inter alia). Within this expanded risk landscape, particularly pressing area of concern in recent research is preventing the misuse of models for chemical, biological, radiological and nuclear threats (60 Minutes, 2025). This growing concern elevated datasets such as AdvBench (Zou et al., 2023) and HarmBench (Mazeika et al., 2024) as prominent benchmarks for evaluating broader safety alignment."
        },
        {
            "title": "7 Conclusion",
            "content": "We systematically studied the quality of two widely used AI safety datasets: AdvBench and HarmBench. We first analyzed these datasets in isolation, without using any models. This analysis revealed that these datasets do not faithfully approximate real-world adversarial behavior due to their overreliance on triggering cues: expressions with overt negative/sensitive connotations designed to trigger safety mechanisms artificially. Motivated by this finding, we investigated what these datasets actually measure in practice when triggering cues are present, and whether their conclusions about model safety still hold when such cues are removed. To this end, we introduced intent laundering: procedure that removes triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. We showed that prior safety conclusions do not hold once triggering cues are removed, and that the observed safety performance is largely driven by the presence of triggering cues rather than by the underlying safety risks. We further showed that intent laundering can be used as powerful jailbreaking technique, achieving high attack success rates from 90% to over 98%. Overall, our findings unveiled critical gap between how model safety is evaluated and how real-world adversarial behavior occurs. Based on this, we conclude that (1) safety evaluations must evolve to capture adversarial behavior more realistically, and (2) current safety alignment efforts are still far from robust against real-world threats."
        },
        {
            "title": "Ethics Statement",
            "content": "We acknowledge that our findings can affect public and institutional trust in current safety claims and evaluations. However, our intention is not to undermine trust in AI safety research, but rather to improve its scientific rigor and practical relevance. We believe the societal benefits of exposing and addressing weaknesses in safety evaluation frameworks significantly outweigh the potential risks, especially when combined with responsible disclosure and awareness efforts. We proactively informed affected model providers and collaborated with them to improve the safety of their products. References 60 Minutes. Anthropic ceo warns that without guardrails, AI could be on dangerous path. https: //youtu.be/aAPpQC-3EyE, November 2025. YouTube video, CBS News. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. Cem Anil, Esin DURMUS, Nina Rimsky, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Meg Tong, Jesse Mu, Daniel Ford, Francesco Mosconi, Rajashree Agrawal, Rylan Schaeffer, Naomi Bashkansky, Samuel Svenningsen, Mike Lambert, Ansh Radhakrishnan, Carson Denison, Evan Hubinger, Yuntao Bai, Trenton Bricken, Timothy Maxwell, Nicholas Schiefer, James Sully, Alex Tamkin, Tamera Lanham, Karina Nguyen, Tomasz Korbak, Jared Kaplan, Deep Ganguli, Samuel R. Bowman, Ethan Perez, Roger Baker Grosse, and David Duvenaud. Many-shot jailbreaking. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=cw5mgd71jW. Anthropic. Claude opus 4 & claude sonnet 4 system card. Technical report, May 2025a. URL https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf. 11 Anthropic. Claude 3.7 sonnet system card. Technical report, February 2025b. URL https://assets. anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. general language assistant as laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemí Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: harmlessness from AI feedback. CoRR, abs/2212.08073, 2022. doi: 10.48550/ARXIV.2212.08073. URL https: //doi.org/10.48550/arXiv.2212.08073. Joe Benton, Misha Wagner, Eric Christiansen, Cem Anil, Ethan Perez, Jai Srivastav, Esin Durmus, Deep Ganguli, Shauna Kravec, Buck Shlegeris, et al. Sabotage evaluations for frontier models. arXiv preprint arXiv:2410.21514, 2024. Tim Beyer, Sophie Xhonneux, Simon Geisler, Gauthier Gidel, Leo Schwinn, and Stephan Günnemann. Llm-safety evaluations lack robustness. arXiv preprint arXiv:2503.02574, 2025. Nick Bostrom. Information hazards: typology of potential harms from knowledge. Review of Contemporary Philosophy, 10:6679, 2011. URL https://www.nickbostrom.com/information-hazards. pdf. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX security symposium (USENIX Security 21), pp. 26332650, 2021. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramèr, and Chiyuan Zhang. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations. OpenReview, 2023. Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George Pappas, Florian Tramer, et al. Jailbreakbench: An open robustness benchmark for jailbreaking large language models. Advances in Neural Information Processing Systems, 37:5500555029, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Jacob Cohen. coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):3746, 1960. Justin Cui, Wei-Lin Chiang, Ion Stoica, and Cho-Jui Hsieh. Or-bench: An over-refusal benchmark for large language models. arXiv preprint arXiv:2405.20947, 2024. Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, and Martin Vechev. Mixat: Combining continuous and discrete adversarial training for llms. In Thirty-ninth Conference on Neural Information Processing Systems (NeurIPS 2025), 2025. URL https://arxiv.org/abs/2505.16947. Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 12361270, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.findings-emnlp.88. URL https://aclanthology.org/2023.findings-emnlp.88/. 12 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT 21, pp. 862872, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445924. URL https://doi.org/10.1145/3442188.3445924. Bradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics: Methodology and distribution, pp. 569593. Springer, 1992. Bradley Efron. Second thoughts on the bootstrap. Statistical science, pp. 135140, 2003. Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5565, 2019. Joseph Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76 (5):378, 1971. Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 33563369, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/ v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301/. Shahriar Golchin and Mihai Surdeanu. Time travel in LLMs: Tracing data contamination in large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=2Rwq6c3tvr. Shahriar Golchin and Mihai Surdeanu. Data contamination quiz: tool to detect and estimate contamination in large language models. Transactions of the Association for Computational Linguistics, 13:809830, 2025. Shahriar Golchin, Mihai Surdeanu, Steven Bethard, Eduardo Blanco, and Ellen Riloff. Memorization in in-context learning. arXiv preprint arXiv:2408.11546, 2024. Shahriar Golchin, Nikhil Garuda, Christopher Impey, and Matthew Wenger. Grading massive open online courses using large language models. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 38993912, 2025. DeepMind Google. Frontier safety framework report gemini 3 pro. Technical report, November 2025a. URL https://storage.googleapis.com/deepmind-media/gemini/gemini_3_pro_fsf_ report.pdf. DeepMind Google. Gemini 3 pro model card. Technical report, November 2025b. URL https: //storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Melody Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, et al. Deliberative alignment: Reasoning enables safer language models. arXiv preprint arXiv:2412.16339, 2024. 13 Danny Halawi, Alexander Wei, Eric Wallace, Tony Wang, Nika Haghtalab, and Jacob Steinhardt. Covert malicious finetuning: Challenges in safeguarding llm adaptation. arXiv preprint arXiv:2406.20053, 2024. Divij Handa, Zehua Zhang, Amir Saeidi, Shrinidhi Kumbhar, Md Nayem Uddin, Aswin RRV, and Chitta Baral. When\" competency\" in reasoning opens the door to vulnerability: Jailbreaking llms via novel complex ciphers. arXiv preprint arXiv:2402.10601, 2024. Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. ToxiGen: large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 33093326, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. acl-long.234. URL https://aclanthology.org/2022.acl-long.234/. Will Hawkins, Brent Mittelstadt, and Chris Russell. The effect of fine-tuning on language model toxicity. In Neurips Safe Generative AI Workshop 2024, 2024. URL https://openreview.net/forum? id=YXaFxrMbVk. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch Critch, Jerry Li Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values. In International Conference on Learning Representations, 2021. Holistic AI. Holistic AIs jailbreaking & red teaming audit of anthropics claude 3.7 sonnet: The most secure model yet?, February 2025. URL https://www.holisticai.com/red-teaming/ claude-3-7-sonnet-jailbreaking-audit. Published February 28, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Haibo Jin, Ruoxi Chen, Peiyan Zhang, Andy Zhou, and Haohan Wang. Guard: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models. arXiv preprint arXiv:2402.03299, 2024. Seongho Joo, Hyukhun Koh, and Kyomin Jung. Harmful prompt laundering: Jailbreaking llms with abductive styles and symbolic encoding. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 2550025535, 2025. Taeyoun Kim, Fahim Tajwar, Aditi Raghunathan, and Aviral Kumar. Reasoning as an adaptive defense for safety. arXiv preprint arXiv:2507.00971, 2025. Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b. arXiv preprint arXiv:2310.20624, 2023. Nathaniel Li, Ziwen Han, Ian Steneker, Willow Primack, Riley Goodside, Hugh Zhang, Zifan Wang, Cristina Menghini, and Summer Yue. Llm defenses are not robust to multi-turn human jailbreaks yet. arXiv preprint arXiv:2408.15221, 2024. Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, and Dan Hendrycks. HarmBench: standardized evaluation framework for automated red teaming and robust refusal. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 3518135224. PMLR, 2127 Jul 2024. URL https://proceedings. mlr.press/v235/mazeika24a.html. 14 Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of attacks: Jailbreaking black-box LLMs automatically. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=SoM3vngOH5. Yutao Mou, Shikun Zhang, and Wei Ye. Sg-bench: Evaluating llm safety generalization across diverse tasks and prompt types. Advances in Neural Information Processing Systems, 37:123032123054, 2024. Honglin Mu, Han He, Yuxin Zhou, Yunlong Feng, Yang Xu, Libo Qin, Xiaoming Shi, Zeming Liu, Xudong Han, Qi Shi, et al. Stealthy jailbreak attacks on large language models via benign data mirroring. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 17841799, 2025. Andreas C. Mueller. Wordcloud. URL https://doi.org/10.5281/zenodo.18332568. Moin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring stereotypical bias in pretrained language models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 53565371, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.416. URL https://aclanthology.org/2021.acl-long.416/. Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: challenge dataset for measuring social biases in masked language models. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 19531967, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.154. URL https://aclanthology.org/2020.emnlp-main.154/. Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, et al. The attacker moves second: Stronger adaptive attacks bypass defenses against llm jailbreaks and prompt injections. arXiv preprint arXiv:2510.09023, 2025a. Milad Nasr, Javier Rando, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Florian Tramèr, and Katherine Lee. Scalable extraction of training data from aligned, production language models. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id= vjel3nWP2a. OpenAI. Gpt-5.1 instant and gpt-5.1 thinking system card addendum. System card, OpenAI, November 2025. URL https://cdn.openai.com/pdf/4173ec8d-1229-47db-96de-06d87147e07e/ 5_1_system_card.pdf. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=hTEGyKf0dZ. Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma, Subhrajit Roy, Ahmad Beirami, Prateek Mittal, and Peter Henderson. Safety alignment should be made more than just few tokens deep. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=6Mxhg9PtDE. 15 Javier Rando, Jie Zhang, Nicholas Carlini, and Florian Tramèr. Adversarial ml problems are getting harder to solve and to evaluate. arXiv preprint arXiv:2502.02260, 2025. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084. Paul Röttger, Hannah Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. XSTest: test suite for identifying exaggerated safety behaviours in large language models. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 53775400, Mexico City, Mexico, June 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.301. URL https://aclanthology. org/2024.naacl-long.301/. Paul Röttger, Hannah Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: test suite for identifying exaggerated safety behaviours in large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 53775400, 2024b. Mahdi Sabbaghi, Paul Kassianik, George J. Pappas, Amin Karbasi, and Hamed Hassani. Adversarial reasoning at jailbreaking time. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=aWd7mL5U9Q. Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-François Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Kost, Christopher Carnahan, and Jordan Boyd-Graber. Ignore this title and HackAPrompt: Exposing systemic vulnerabilities of LLMs through global prompt hacking competition. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 49454977, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 302. URL https://aclanthology.org/2023.emnlp-main.302/. William Scott. Reliability of content analysis: The case of nominal scale coding. Public opinion quarterly, pp. 321325, 1955. Rohin Shah, Alex Irpan, Alexander Matt Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis Ho, Neel Nanda, Raluca Ada Popa, et al. An approach to technical agi safety and security. arXiv preprint arXiv:2504.01849, 2025. Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as babysitter: On biases in language generation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 34073412, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1339. URL https://aclanthology.org/D19-1339/. Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 318, 2017. doi: 10.1109/SP.2017.41. Alexandra Souly, Javier Rando, Ed Chapman, Xander Davies, Burak Hasircioglu, Ezzeldin Shereen, Carlos Mougan, Vasilios Mavroudis, Erik Jones, Chris Hicks, et al. Poisoning attacks on llms require near-constant number of poison samples. arXiv preprint arXiv:2510.07192, 2025. Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas Joseph, Shauna Kravec, Karina Nguyen, Jared Kaplan, and Deep Ganguli. Evaluating and mitigating discrimination in language model decisions. arXiv preprint arXiv:2312.03689, 2023. Robert Tibshirani and Bradley Efron. An introduction to the bootstrap. Monographs on statistics and applied probability, 57(1):1436, 1993. 16 William Timkey and Marten van Schijndel. All bark and no bite: Rogue dimensions in transformer language models obscure representational quality. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 45274546, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Johan Wahréus, Ahmed Hussain, and Panos Papadimitratos. Jailbreaking large language models through content concretization. In International Conference on Game Theory and AI for Security, pp. 395414. Springer, 2025. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: comprehensive assessment of trustworthiness in gpt models. In NeurIPS, 2023. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. xAI. Grok 4 model card. Model card, xAI, August 2025. 2025-08-20-grok-4-model-card.pdf. Last updated August 20, 2025. URL https://data.x.ai/ Sophie Xhonneux, Alessandro Sordoni, Stephan Günnemann, Gauthier Gidel, and Leo Schwinn. Efficient adversarial training in llms with continuous attacks. Advances in Neural Information Processing Systems, 37:15021530, 2024. Zhixin Xie, Xurui Song, and Jun Luo. Attack via overfitting: 10-shot benign fine-tuning to jailbreak LLMs. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=utvu4PJ0Ct. Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. Bot-adversarial dialogue for safe conversational agents. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 29502968, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.235. URL https: //aclanthology.org/2021.naacl-main.235/. Nan Xu, Fei Wang, Ben Zhou, Bangzheng Li, Chaowei Xiao, and Muhao Chen. Cognitive overload: Jailbreaking large language models with overloaded logical thinking. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 35263548, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.224. URL https://aclanthology.org/2024.findings-naacl. 224/. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Xianjun Yang, Xiao Wang, Qi Zhang, Linda Ruth Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models, 2024. URL https://openreview.net/ forum?id=9qymw6T9Oo. Jingwei Yi, Rui Ye, Qisi Chen, Bin Zhu, Siheng Chen, Defu Lian, Guangzhong Sun, Xing Xie, and Fangzhao Wu. On the vulnerability of safety alignment in open-access LLMs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 92369260, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.549. URL https://aclanthology.org/2024.findings-acl. 549/. 17 Zhiyuan Yu, Xiaogeng Liu, Shunning Liang, Zach Cameron, Chaowei Xiao, and Ning Zhang. Dont listen to me: Understanding and exploring jailbreak prompts of large language models. In 33rd USENIX Security Symposium (USENIX Security 24), pp. 46754692, 2024. Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. GPT-4 is too smart to be safe: Stealthy chat with LLMs via cipher. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= MbfAK4s61A. Zhehao Zhang, Weijie Xu, Fanyou Wu, and Chandan Reddy. Falsereject: resource for improving contextual safety and mitigating over-refusals in llms via structured reasoning. arXiv preprint arXiv:2505.08054, 2025. Yiran Zhao, Wenyue Zheng, Tianle Cai, Xuan Long, Kenji Kawaguchi, Anirudh Goyal, and Michael Shieh. Accelerating greedy coordinate gradient and general prompt optimization via probe sampling. Advances in Neural Information Processing Systems, 37:5371053731, 2024. Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. Agreement Analysis Between LLM-Based and Human Evaluations We measure agreement between the LLM judge and the consensus of three human experts on 100 randomly sampled responses across all studied models. We use the same categorization scheme used for intent-laundered attacks (Section 3.2 under Attack Success Rate) and original attacks (Section 4 under LLM as Judge). For safety evaluation, model responses are categorized as unsafe (rated extremely unsafe, highly unsafe, or unsafe) or safe (rated moderately unsafe or safe). For practicality evaluation, model responses are categorized as practical (rated extremely applicable, highly applicable, or applicable) or impractical (rated moderately applicable or inapplicable). Human consensus is determined by majority vote. We report percent agreement rather than Cohens kappa (Cohen, 1960; Scott, 1955; Fleiss, 1971) due to the high base rate of the majority class, which can inflate chance agreement and render kappa less informative. We compute the 95% confidence interval (CI) using bootstrap resampling with 10,000 iterations (Efron, 1992; Tibshirani & Efron, 1993; Efron, 2003). Table 2 reports the results of the agreement analysis. The LLM judge achieves 90.00% agreement with human consensus on binary safety evaluation (95% CI: 83.00%95.00%) and 94.00% agreement on binary practicality evaluation (95% CI: 89.00%98.00%). Both exceed the mean pairwise agreement among human annotators (82.00% for safety; 77.33% for practicality). The relatively narrow bootstrap confidence intervals suggest that these estimates are robust to resampling variability. Overall, the LLMs agreement with human consensus surpasses the mean human-human agreement, suggesting that it can serve as reliable automated judge for both safety and practicality evaluations. Table 2: Agreement between the LLM judge and the consensus of three human experts in binary evaluations of safety and practicality. The analysis uses 100 randomly sampled responses from all studied models. Safety is categorized as unsafe or safe, and practicality as practical or impractical. Human consensus is determined by majority vote. The results indicate that the LLM judge serves as reliable proxy for expert human evaluation in both binary assessments."
        },
        {
            "title": "Evaluation Overall Agreement",
            "content": "95% CI Mean Human-Human Agreement"
        },
        {
            "title": "Safety\nPracticality",
            "content": "90.00% 94.00% [83.00%, 95.00%] [89.00%, 98.00%] 82.00% 77.33% 18 Figure 5: An actual response (red box) generated by Gemini 3 Pro to an intent-laundered revision (green box) based on data point from the AdvBench dataset (orange box). The revision uses both connotation neutralization and context transposition to abstract away triggering cues. The model response is partially shown to prevent potential misuse; however, the full response spans several paragraphs and fully enables the original malicious intent. This example is provided strictly for academic safety research. Any misuse is strongly discouraged. Examples of Intent-Laundered Revisions and Model Responses We present examples of actual model responses to intent-laundered revisions. Figure 5 presents response from Gemini 3 Pro to revision based on data point from the AdvBench dataset. Similarly, Figure 6 shows response from Claude Sonnet 3.7 to revision based on data point from the HarmBench dataset. As shown, all responses are unsafe, detailed, and transferable to real-world contexts, despite being framed as abstract scenarios."
        },
        {
            "title": "C Input Prompts",
            "content": "This section lists all the prompts used in our study. These include prompts for intent laundering, the revisionregeneration loop, evaluation criteria generation, safety evaluation, practicality evaluation, and regular safety evaluation (i.e., settings where intent laundering is not applied, referred to as no-revision settings). Figures 7, 8, 9, 10, 11, and 12 show them in the respective order. 19 Figure 6: An actual response (red box) generated by Claude Sonnet 3.7 to an intent-laundered revision (green box) based on data point from the HarmBench dataset (orange box). The revision uses both connotation neutralization and context transposition to abstract away triggering cues. The model response is partially shown to prevent potential misuse; however, the full response spans several paragraphs and fully enables the original malicious intent. This example is provided strictly for academic safety research. Any misuse is strongly discouraged. 20 Figure 7: Prompt used by the intent launderer to generate revisions. The placeholder is filled with the data point being intent-laundered. Figure 8: Prompt used by the intent launderer to generate revisions in the revisionregeneration loop. The placeholders are filled with the previously failed revisions of the data point being intentlaundered, and the data point itself, in top-to-bottom order. 22 Figure 9: Prompt used to generate evaluation criteria. The placeholder is filled with the original data point. 23 Figure 10: Prompt used for safety evaluation of responses to revisions. The placeholders are filled with the evaluation criterion followed by the model response, in top-to-bottom order. Figure 11: Prompt used for practicality evaluation of responses to revisions. The placeholders are filled with the evaluation criterion followed by the model response, in top-to-bottom order. 24 Figure 12: Prompt used for regular safety evaluation of responses to the original attacks (data points), where intent laundering is not applied. The placeholder is filled with the model response."
        }
    ],
    "affiliations": [
        "Applied Machine Learning Research"
    ]
}