{
    "paper_title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective",
    "authors": [
        "Wen Yang",
        "Junhong Wu",
        "Chong Li",
        "Chengqing Zong",
        "Jiajun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes a novel cross-linguistic perspective to investigate reasoning generalization. This raises a crucial question: $\\textit{Does the reasoning capability achieved from English RPT effectively transfer to other languages?}$ We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing a metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct a thorough parallel training study. Experimental results yield three key findings: $\\textbf{First-Parallel Leap}$, a substantial leap in performance when transitioning from monolingual to just a single parallel language, and a predictable $\\textbf{Parallel Scaling Law}$, revealing that cross-lingual reasoning transfer follows a power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as $\\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 2 7 2 2 0 . 0 1 5 2 : r Preprint. PARALLEL SCALING LAW: UNVEILING REASONING GENERALIZATION THROUGH CROSS-LINGUISTIC PERSPECTIVE Wen Yang1,2, Junhong Wu1,2, Chong Li1,2, Chengqing Zong1,2, Jiajun Zhang1,2,3 1 School of Artificial Intelligence, University of Chinese Academy of Sciences 2 Institute of Automation, Chinese Academy of Sciences 3 Wuhan AI Research {yangwen2023, wujunhong2021, lichong2021}@ia.ac.cn {cqzong, jjzhang}@nlpr.ia.ac.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes novel cross-linguistic perspective to investigate reasoning generalization. This raises crucial question: Does the reasoning capability achieved from English RPT effectively transfer to other languages? We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct thorough parallel training study. Experimental results yield three key findings: First-Parallel Leap, substantial leap in performance when transitioning from monolingual to just single parallel language, and predictable Parallel Scaling Law, revealing that cross-lingual reasoning transfer follows power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as Monolingual Generalization Gap, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in Reinforcement Post-Training (RPT) (Jaech et al., 2024; Kimi et al., 2025; Qwen, 2025) have emerged as transformative paradigm for advancing the capabilities of Large Reasoning Models (LRMs). Techniques like Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024; Guo et al., 2025) have even enabled models to surpass human-level performance on complex math reasoning benchmarks such as MATH (Hendrycks et al., 2021) and AIME (Maxwell, 2024). Given these impressive gains in the mathematical domain, central question has emerged: Can these RL-driven reasoning abilities generalize effectively? growing body of work (Chu et al., 2025; Liu et al., 2025a; Hu et al., 2025a; Huan et al., 2025; Zhou et al., 2025) has investigated this by exploring generalization across tasks or modalities. However, crucial and largely unexplored dimension of this generalization is its cross-lingual transferability. While RL-based reasoning models have shown remarkable performance in English, it Corresponding author 1 Preprint. remains unclear whether these learned skills are fundamentally language-agnostic or are tied to the specific linguistic patterns of their training data. This lack of understanding regarding LRMs stands in contrast to findings from cognitive neuroscience, which have long demonstrated that human reasoning operates largely independently of language (Carruthers, 1998; Brannon, 2005; Fedorenko & Varley, 2016; Coetzee et al., 2022). In this ideal scenario, reasoning abilities should generalize across languages, as reasoning and linguistic processing are fundamentally decoupled. This provides strong theoretical motivation for our work, which seeks to answer critical question: (Q) Does reasoning ability learned by LLMs from English training generalize to other languages, akin to human cognitive processes? In this work, we address this question by providing three-stage studies to investigate cross-lingual reasoning generalization. We start with proposing the Multilingual Transferability Index (MTI) to quantify cross-lingual transferability. We then conduct an Observational Study, systematically evaluating the reasoning transferability of 13 open-source English-centric LRMs spanning 11 typologically diverse languages across 4 multilingual reasoning benchmarks. This study sheds the first light on cross-lingual reasoning generalization, revealing that transferability varies significantly across the initial model, target language, and training paradigm. Building on the initial findings of our observational study, we conducted series of strict Interventional Studies to address the confounding variables present in open-source models, such as inconsistencies in training data, hyperparameters, initial models, and training paradigms. This approach allows for precise analysis of how different training paradigms, model architectures, and model sizes influence cross-lingual generalization. Through this rigorous methodology, we found universal principle: models with stronger initial English capabilities exhibit an over-reliance on English-specific patterns, which in turn diminishes their cross-lingual generalization. To address this specific limitation of English-centric RPT, we conducted comprehensive Parallel Training Study using parallel data from one to seven languages. Through our experiments, we established three key findings: First, we identify significant First-Parallel Leap, which is substantial jump in cross-lingual generalization performance when transitioning from monolingual to single parallel language. Second, we uncover predictable Parallel Scaling Law, which reveals that models multilingual reasoning performance scales in power-law fashion with the number of parallel languages. Third, we identify significant Monolingual Generalization Gap. This gap is large discrepancy between the performance predicted by the fitted power-law function and the actual monolingual performance. The existence of this gap indicates that reasoning skills learned by English-centric LRMs are not consistent with human reasoning, as they fail to generalize completely to other languages. In summary, we explore new perspective on the reasoning capabilities of LLMs through the lens of cross-lingual generalizability. Our work addresses the following research questions (RQs) that have not been systematically examined in prior work. RQ1: To what extent do English-centric LLMs generalize their reasoning abilities across languages? (See Section 2) RQ2: What factors influence models cross-lingual reasoning generalization? (See Section 3) RQ3: How can we effectively improve cross-lingual reasoning generalization? (See Section 4)"
        },
        {
            "title": "2 OBSERVATIONAL STUDY",
            "content": "To address the RQ1, we perform an observational study by evaluating popular open-source reasoning models on diverse multilingual benchmarks. This study is designed to provide systematic view into the cross-lingual reasoning generalization of LRMs."
        },
        {
            "title": "2.1 OBSERVATIONAL SETUP",
            "content": "Models We selected diverse set of state-of-the-art open-source LRMs, particularly those finetuned with Supervised Fine-Tuning (SFT) or Reinforcement Post-Training (RPT) that have demonstrated strong performance on English reasoning benchmarks. Specifically, we evaluate the Simple2 Preprint. Zoo (Zeng et al., 2025), s1 (Muennighoff et al., 2025), OpenThinker (Guha et al., 2025), OpenReasoner-Zero (Hu et al., 2025b) and DeepSeek-R1-Distill (Guo et al., 2025) series models."
        },
        {
            "title": "Observational Study",
            "content": "Across Initial Models: The inherent properties of the initial model significantly influence cross-lingual transferability. SFT vs. RL: While SFT leads to performance degradation in low-resource languages, RL yields substantial improvements."
        },
        {
            "title": "Interventional Study",
            "content": "Base vs. Math-specific vs. Instruct models: Models that retain more general pretrained knowledge generalize better across languages. In contrast, instruction-tuned models exhibit weaker cross-lingual generalization due to over-reliance on Englishspecific patterns. Qwen vs. Llama: model with weaker initial performance (Llama3.1) often demonstrates superior cross-lingual generalization ability compared to models whose stronger initial performance (Qwen2.5) may derive from greater reliance on languagespecific patterns. 1.5B vs. 7B: Models with weaker initial capabilities (1.5B) achieve greater improvements on in-domain math reasoning and other reasoning domains. Conversely, models with stronger initial capabilities (7B) demonstrate more robust cross-lingual transfer of reasoning across challenging math reasoning benchmarks."
        },
        {
            "title": "Parallel Training Study",
            "content": "First-Parallel Leap: The initial jump from monolingual to bilingual setup yields disproportionately large improvement compared to gains from adding any further parallel languages. Parallel Scaling Law: Cross-lingual reasoning performance follows predictable, non-linear scaling law where gains exhibit diminishing returns as the number of parallel languages increases. Crucially, the benefit is primarily in transferability (β = 0.29)teaching the model how to generalizerather than in absolute accuracy (β = 0.02). This scaling behavior suggests that parallel training helps the model abstract more universal, language-agnostic reasoning component. Monolingual Generalization Gap: The performance of English-only models fails to meet the prediction of the Parallel Scaling Law, revealing Monolingual Generalization Gap. This gap suggests that the reasoning skills acquired through monolingual training are built on language-specific patterns, not universal, transferable reasoning component. Benchmarks For evaluation, we utilized comprehensive suite of multilingual reasoning benchmarks of challenging questions. This suite comprises multilingual version of MATH500 (Hendrycks et al., 2021), AIME2024 (Maxwell, 2024), AIME2025 (Kaggle, 2025), and GPQA-Diamond (Rein et al., 2024) from the XReasoning benchmark (Qi et al., 2025). The details of these benchmarks are described in Appendix C.1. These benchmarks are constructed from original English questions that have been meticulously translated into ten additional languages: Spanish (es), Russian (ru), German (de), French (fr), Bengali (bn), Swahili (sw), Thai (th), Japanese (ja), Chinese (zh), and Telugu (te), resulting in total of eleven languages for evaluation. Experimental Setup Our evaluation is guided by the first principle in multilingual scenarios: For large language models, thinking in the users native language is as important as achieving high accuracy. Aligning the models reasoning language with that of the user makes its reasoning trace more readable and verifiable, which is crucial for real-world multilingual reasoning applications (Yong et al., 2025; Wang et al., 2025). Therefore, we adopted prompt hack techniques to induce models to reason in the users language. The detailed prompt prefix provided in the Appendix F.2 follows prior Preprint. work (Qi et al., 2025), which has shown that such techniques can effectively control the response language. Performance Metrics We report reasoning accuracy (Acc) to evaluate model performance, and the off-target rate (Off-tag) to measure the proportion of instances in which the LRMs fail to follow the instruction to respond in the specified language using the LangDetect library. Cross-lingual Transfer Metrics To better quantify transferability, we adopt the concept of relative gain and introduce the Multilingual Transferability Index (MTI), following prior work (Huan et al., 2025) that evaluated transferability across diverse tasks. Let Strained b,l denote the accuracy score of the trained model and base model, respectively, b,l on benchmark for language l. For each language l, we define its relative gain on benchmark as: and Sbase Rb,l = Strained b,l Sbase b,l Sbase b,l . (1) For training language set Ltrain containing one or more languages (e.g., en, or en & ru), the overall relative gain is obtained by averaging over the training languages: Rb,Ltrain = 1 Ltrain (cid:88) lLtrain Rb,l. (2) The MTI for an unseen language lunseen (not included in the training set) on benchmark is defined as: MTIb,lunseen = . (3) Rb,lunseen Rb,Ltrain where Rb,lunseen is computed as in Eq. (1). Finally, to obtain single cross-lingual transferability score across all benchmarks (MATH500, AIME24/25, GPQA-Diamond), we average the per-benchmark MTI: MTIlunseen = 1 (cid:88) bB Rb,lunseen Rb,Ltrain . (4) positive MTI value indicates that models reasoning gains have successfully transferred to the target language lunseen, relative to its training language set Ltrain. value greater than 1 signifies that the reasoning gain on the target language actually exceeds that of the training languages."
        },
        {
            "title": "2.2 RESULTS",
            "content": "Our comprehensive observational study reveals that reasoning gains acquired in English do not consistently transfer to other languages. As shown in Figure 1, the degree of transferability varies substantially across multiple dimensions, with off-target metrics results and additional details provided in Appendix E.1. Across Initial Models Our findings indicate that the choice of initial model is critical factor influencing cross-lingual reasoning transfer. Even with the same training data, training paradigms, and hyperparameters, different initial models lead to different transfer abilities. For instance, the Qwen2.5-7B-SimpleRL-Zoo model exhibits slightly higher MTI than Qwen-2.5-Math-7B-SimpleRLZoo, despite their similar training setup. This demonstrates that the inherent properties of the initial model influence cross-lingual transferability. Across Training Paradigms and Languages In Figure 1, the top subfigure shows that RL-tuned models consistently achieve higher MTI than SFT-tuned models except DeepSeek-R1-Distill-Qwen7B, which is fine-tuned on massive amount of high-quality data. However, fairer comparison requires complete control over variables, as the initial models, training parameters, and training data differ among the open-source models. The bottom subfigure illustrates that all languages except for the low-resource languages (bn, sw, te) exhibit positive transfer in both SFT and RL-tuned 4 Preprint. Figure 1: Cross-lingual reasoning transferability across open-source LRMs. The top subfigure shows the average Multilingual Transferability Index (MTI) of various English-centric LRMs across four benchmarks and eleven languages, with the x-axis representing the base models. The bottom subfigure presents the average Transferability Index (TI) performance of SFTand RL-tuned models on individual languages on the MATH500 benchmark. models. However, SFT and RL exhibit completely opposite effects on reasoning transfer in these low-resource languages. Specifically, SFT-tuned models exhibit negative transfer, indicating that the SFT degrades rather than enhances performance in these languages. Conversely, RL-tuned models achieve substantial positive transfer, with the TI for sw and te even exceeding that of all medium and high-resource languages. This finding suggests that RL provides crucial solution for the lowresource language dilemma, revealing consistent pattern: while SFT leads to degradation in low-resource settings, RL yields substantial improvements."
        },
        {
            "title": "INTERVENTIONAL STUDY",
            "content": "While our comprehensive observational study provides an overview of existing LRMs cross-lingual reasoning transfer capabilities, it cannot definitively isolate the underlying causes due to the varying training configurations, including datasets, training paradigms, initial model, and hyperparameters across different models. To address RQ2: What factors influence models cross-lingual reasoning generalization?, we designed series of interventional studies. These studies systematically control key experimental settings, enabling more focused analysis of the isolated impacts of datasets, initial models, and training paradigms. 3."
        },
        {
            "title": "INTERVENTIONAL SETUP",
            "content": "Dataset To facilitate efficient interventional studies and inspired by prior work such as LIMO (Ye et al., 2025) and s1 (Muennighoff et al., 2025), we curated specialized dataset. This dataset com5 Preprint. prises 1000 samples meticulously selected from the MATH training set, and all control studies are conducted using this dataset. The details of the dataset could be found in Appendix D.1. (5) (6) Training paradigm To explore the impact of RPT on reasoning transfer, we utilize Group Rollout Policy Optimization (GRPO) (Shao et al., 2024) as our RPT algorithm. GRPO is simplified PPObased algorithm that significantly reduces training costs by eliminating the need for value model. It operates by sampling rollouts {o1, ..., oG} from the current policy for given input, calculating their cumulative rewards = {R1, ..., RG}, and then using these rewards to estimate advantages ˆAi,t to guide policy updates. The optimization objective for GRPO is defined as follows: (cid:34) (cid:35) LGRPO(θ) = qD,{oi}G i=1πθold (q) oi (cid:88) (cid:16) (cid:88)"
        },
        {
            "title": "1\nG",
            "content": "1 oi i=1 t="
        },
        {
            "title": "Lclip",
            "content": "i,t (θ) βDKL(πθπref) (cid:17) where"
        },
        {
            "title": "Lclip",
            "content": "i,t (θ) = min (cid:16) ri,t(θ) ˆAi,t, clip (cid:16) ri,t(θ), 1 ε, 1 + ε (cid:17) ˆAi,t (cid:17) ri,t(θ) = πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) , ˆAi,t = Ri mean(R) std(R) The clipping term with ratio ε (Schulman et al., 2015) keeps the new policy close to the old one, improving training stability. In our GRPO setup, the models policy is optimized using composite reward function that captures reasoning accuracy Racc, format Rformat, and language consistency Rlang. Specifically, the reward for each solution is defined as weighted sum of these three components: = λ1Racc + λ2Rformat + λ3Rlang (7) where λ1,2,3 are hyperparameters controlling the relative importance of each reward component. All training hyperparameters are provided in Appendix D.3."
        },
        {
            "title": "3.2 CONTROLLED SETTING AND RESULTS",
            "content": "For each experiment, we maintain all other hyperparameters and dataset configurations constant, only varying the specific factor. The Impact of Initial Model Types To assess the influence of initial model types, we conducted controlled experiments with three distinct starting points: base model, instruction model, and mathspecialized model in the Qwen2.5-7B series. Table 1 reveals the following key findings: (1) The instruction model demonstrates multilingual reasoning that most aligns with real-world multilingual application scenarios, achieving the highest reasoning accuracy after training on English data (Avg: 23.51) and the strongest reasoning language consistency (Off-tag: 0.94). (2) When trained on English data, base and math-specific models exhibit higher cross-lingual transferability than their instruction-tuned counterparts. Specifically, they achieved substantially higher MTI of 1.95 and 2.12, respectively. This finding is particularly notable because these models, unlike instruction-tuned models, are not fine-tuned to be perfectly aligned with English prompts. Their superior transferability suggests that retaining more of their general pre-trained knowledge allows them to avoid over-reliance on language-specific patterns. In contrast, the strong English alignment of instruction-tuned models appears to come at the cost of cross-lingual generalization, as they become overly reliant on specific linguistic patterns. The Impact of Different Initial Model Families We selected Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct as our initial models to investigate the influence of the model family. Figure 2 illustrates the changes in accuracy and off-target rates from the initial models to the trained models on MATH500 benchmark. Results and analysis on more benchmarks are detailed in Appendix E.2.1. First, fine-tuning with GRPO on English data enhances LLM reasoning performance not only on the trained language but also generalizes to other languages, regardless of the model family. Interestingly, we find that the effect of cross-lingual transfer is inversely correlated with the initial models capability. Although the Llama3.1 model has weaker initial performance on English, it demonstrates stronger generalization ability. This finding suggests that model with less specialized foundation may be better suited for broad cross-lingual transfer. The Llama model likely possesses more robust, less-constrained generalizable reasoning component, while the Qwen models stronger initial performance may come from greater reliance on language-specific patterns. 6 Preprint. Table 1: The Impact of Initial Model Type on Interventional Study. Accuracy (%), Off-target rate (%) and MTI across different initial model types."
        },
        {
            "title": "Average accuracy across all languages",
            "content": "MATH500 AIME24 AIME25 GPQA"
        },
        {
            "title": "Avg",
            "content": "Off-tag MTI Qwen2.5-7B-Base GRPO on En Data Qwen2.5-7B-Instruct GRPO on En Data Qwen2.5-Math-7B GRPO on En Data 26.55 52.16 50.91 54.24 29.18 45.73 1.23 7.10 5.93 7.41 4.11 8.84 0.42 3.35 3.28 3.92 1.88 3.96 19.79 27.18 29.71 28.47 13.82 18.96 12.00 22.45 22.45 23.51 12.25 19. 11.41 3.12 1.43 0.94 22.59 9.50 - 1.95 - 1.23 - 2.12 Figure 2: The Impact of Different Initial Model Families on Interventional Study. Multilingual reasoning performance across languages on MATH500 benchmark, comparing the influence of model family using Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct as initial models. Base represents the performance of the initial model, while +GRPO denotes performance after fine-tuning with GRPO on English data. The light red area denotes the improvement in accuracy between the Base and +GRPO models, while the light gray area represents the reduction in the off-target rate between the two. The Impact of Model Size To explore the multilingual performance of different model sizes, we selected the 1.5B and 7B models, which are the most common for RL training in previous research. Figure 3 shows the Performance on various multilingual benchmarks; detailed results are provided in Appendix E.2.2. On the in-domain multilingual MATH500 benchmark, the smaller 1.5B model shows substantially larger gains than the 7B model across both the training and untrained languages, indicating that models with weaker initial capabilities achieve greater improvements on in-domain math reasoning tasks. On the multilingual AIME24/25 benchmarks, which are used to evaluate models generalization to more challenging math reasoning tasks, our results show that models with stronger initial capabilities demonstrated more robust transfer of reasoning capabilities to these challenge benchmarks. The multilingual GPQA-Diamond benchmark evaluates models reasoning capabilities in biology, physics, and chemistry. We found clear distinction in performance between the models: 1.5B model shows significant gains on GPQA across all languages, whereas the 7B model exhibits only marginal improvements and even degradation in English. Figure 3: The Impact of Different Model Size on Interventional Study. Performance on various benchmarks across models of different sizes. Performance denotes the average difference in accuracy performance between the trained model and its initial model, averaged across both the training language and unseen languages, respectively. Preprint."
        },
        {
            "title": "4 PARALLEL TRAINING STUDY",
            "content": "Based on the findings from the interventional study, this section directly addresses RQ3: How can we effectively improve cross-lingual reasoning generalization? We propose simple, yet highly efficient training strategy: Just Go Parallel. This approach involves simultaneously training models on bilingual or more parallel problem sets in different languages. To evaluate its effectiveness, we conducted comprehensive parallel training study analyzing how this strategy impacts cross-lingual reasoning performance."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP AND RESULTS",
            "content": "We selected Qwen2.5-7B-Instruct as our initial model and fine-tuned it using the GRPO-based RPT paradigm on specialized parallel multilingual problem sets. This dataset was built from the 1,000 English samples used in our Interventional Study and extended with seven typologically diverse languages: es, ru, de, fr, bn, th, zh. All non-English samples were carefully aligned with their English counterparts, enabling controlled study of parallel exposure on reasoning transfer. To examine the effect of the number of parallel training languages on performance, we increased the number of parallel languages from one to seven, see Table 8 for details. The resulting models were evaluated on both accuracy (Acc) and cross-lingual transfer metrics (MTI) using the multilingual MATH500 benchmark across eleven languages. Figure 4 illustrates the reasoning performance and cross-lingual transferability of models trained with different numbers of parallel languages. Detailed results are present in Appendix E.3. The First-Parallel Leap We observe striking phenomenon: the jump from monolingual to bilingual parallel languages yields disproportionately large improvement compared to adding more parallel languages. Specifically, MTI rises from 1.16 to 2.50 (+1.34), and accuracy from 54.24 to 57.87 (+3.63). In contrast, expanding from one to seven parallel languages yields only modest gainsMTI from 2.50 to 3.63 (+1.13) and accuracy from 57.87 to 59.52 (+1.65). We term this phenomenon as the First-Parallel Leap, highlighting that the leap from zero to one parallel language far exceeds the cumulative gains from additional parallel languages. Figure 4: The Parallel Scaling Law in Multilingual Reasoning Performance. The x-axis Number of Training Languages is defined as English plus the specified number of parallel languages. Experimental Data shows the performance metrics of the model under different training numbers of parallel languages. The curves are fitted to the Experimental Data. Monolingual Baseline refers to fine-tuning on English data only, without parallel data. First-Parallel Leap denotes the performance difference between model with one parallel language and the Monolingual Baseline. The Parallel Scaling Law Our observations reveal clear scaling pattern: while the rate of improvement in both transferability and accuracy diminishes as the number of parallel languages increases from one to seven, substantial leap in performance occurs in the initial transition from monolingual baseline to one parallel language. This non-linear behavior, with large initial gains Preprint. followed by diminishing returns, is consistent with the characteristics of power-law scaling. To model this behavior, we propose the following scaling law for cross-lingual reasoning performance, specifically for both transferability and accuracy, as function of the number of parallel languages X: (X) = α β (8) where α and β are coefficients to be fit. Our results yield the following fitted curves for transferability and accuracy, respectively: For Transferability: (X) = 2.00 0.29 & For Accuracy: (X) = 56.98 0. (9) Figure 4 presents that the fitted power-law curves demonstrate clear and predictable scaling relationship. We term this predictable, non-linear behavior as the Parallel Scaling Law. Specifically, the fact that both power-law exponents are less than 1 provides mathematical proof that the models performance gain exhibits diminishing returns as the number of parallel languages increases. The specific values of the power-law exponents (β) provide further insight. The significantly higher exponent for transferability (β = 0.29) compared to accuracy (β = 0.02) suggests that the primary benefit of parallel training is not in boosting absolute performance but in teaching the model how to transfer reasoning from English to other languages. Monolingual Generalization Gap Based on the Parallel Scaling Law, we estimate the expected performance of monolingual training (denoted as Expected Monolingual in Figure 4). However, when compared to the actual performance of monolingual training (denoted as Monolingual Baseline), clear discrepancy emerges, which we term the Monolingual Generalization Gap. For instance, while the power-law fit for transferability predicts an expected monolingual MTI of approximately 2.00, the actual measured value is only 1.16. similar gap exists for accuracy, with predicted value of 56.98% compared to an actual value of 54.24%. This gap reveals crucial insight: the reasoning abilities acquired by English-centric models through monolingual training do not adhere to the same scaling behavior observed in multilingual training. This indicates that these English-centric models, despite their impressive capabilities, are likely relying on language-specific patterns rather than universal, language-agnostic reasoning component."
        },
        {
            "title": "4.2 ANALYSIS AND DISCUSSION",
            "content": "Parallel vs. Unparallel The use of parallel data is critical component of our proposed training strategy. Unlike unparallel data, which simply exposes the model to wider variety of languages, parallel data provides an explicit signal for semantic equivalence across languages. This forces the model to learn unified, language-agnostic representation for reasoning. Figure 5 shows the performance differences between using parallel and non-parallel data, highlighting the critical importance of training with parallel data. Figure 5: Accuracy difference comparison across parallel and unparallel data training. Is the selected language important for the parallel training? Figure 6 shows that the choice of parallel language results in only minor variations in MTI and off-target metrics. Among the parallel languages, training with Russian achieves the highest MTI of 2.84 (higher than Bengali at 2.73, German at 2.56, and Chinese at 2.50) and also attains the lowest off-target rate. Overall, adding one parallel language consistently enhances both cross-lingual transferability and multilingual reasoning performance. More detailed analyses are presented in Appendix E.3.4. 9 Preprint. Figure 6: Multilingual reasoning performance across different parallel languages. Only en denotes only fine-tuned on English data. en&LANGUAGE indicates the model was fine-tuned on English and parallel language, with LANGUAGE representing ru, bn, de, zh, respectively."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Large Reasoning Models Chain of Thought (CoT) (Wei et al., 2022) has unlocked the reasoning capabilities of large language models (LLMs), enabling step-wise thought and improving reasoning performance. OpenAIs O1 (Jaech et al., 2024) marked paradigm shift by using reinforcement learning (RL) for test-time scaling, simulating human-like reflective reasoning. Building on this, DeepSeek R1 (Guo et al., 2025) employed GRPO (Shao et al., 2024) with rule-based rewards, fostering long CoT sequences and self-reflection. Recent advancements have sparked wave of opensource efforts to replicate or extend R1s methods, refining RL algorithms and applying the GRPO series to broad range of scenarios and domains (Liu et al., 2025b; Yu et al., 2025; Sun et al., 2025; Chen et al., 2025a;b; Jian et al., 2025). Reasoning Generalization Reasoning generalization in RL-based LLMs has attracted growing interest, particularly in transferring mathematical reasoning to other tasks or modalities. Hu et al. (2025a) shows that RL improves structured reasoning but transfers poorly to unstructured tasks. Huan et al. (2025); Chu et al. (2025) find that RL encourages broader transfer, whereas SFT often leads to domain-specific overfitting. X-REASONER (Liu et al., 2025a) demonstrates that rule-based RL can generalize reasoning across domains and modalities. While prior works explore reasoning generalization across domains and modalities, our work proposes new cross-linguistic perspective to investigate reasoning generalization. Cross-Lingual Transferability Improving the performance of English-centric LLMs in other languages has become major research focus. Prior work has explored zero-shot or minimal fine-tuning to realize cross-lingual transfer (Li et al., 2024; Chirkova & Nikoulina, 2024), showing that English reward models (Wu et al., 2024; Hong et al., 2025) and preference alignment (Yang et al., 2025a;b) can generalize across languages. In parallel learning, Mu et al. (2024) shows that leveraging parallel multilingual input as form of in-context learning achieves superior performance than conventional in-context learning, while Qorib et al. (2025) conducts systematic study on how adding parallel data during pretraining affects LLMs multilingual capabilities. In the era of reasoning, Bandarkar et al. (2025) transfers math skills to other languages by swapping few layers between mathspecific and multilingual model. Yong et al. (2025) demonstrates that cross-lingual test-time scaling improves multilingual reasoning. Distinct from these studies, our work adopts cross-lingual perspective to systematically analyze the reasoning generalization of RL-based models."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This work presents systematic study of cross-lingual reasoning generalization in English-centric LRMs. Through observational and interventional studies, we reveal that stronger English-centric models often overfit to language-specific patterns, limiting cross-lingual transfer. In our parallel training study, we uncover three key phenomena that characterize cross-lingual reasoning: FirstParallel Leap, Parallel Scaling Law, and Monolingual Generalization Gap, providing principled framework for enhancing cross-lingual reasoning generalization. These results highlight both the limitations of current LRMs and shed light on building more language-agnostic LRMs. 10 Preprint."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "Codes and model weights will be released after review to facilitate future research. For evaluation, we follow prior works and report averaged results over 16 sampled generations per question on datascarce benchmarks. All evaluations are conducted with temperature set to 0.6 and top-p to 0.95, with the random seed fixed to ensure deterministic outputs across runs. Note that minor variations in inference results may still occur due to differences in hardware or the version of the inference framework."
        },
        {
            "title": "REFERENCES",
            "content": "Lucas Bandarkar, Benjamin Muller, Pritish Yuvraj, Rui Hou, Nayan Singhal, Hongjiang Lv, and Bing Liu. Layer swapping for zero-shot cross-lingual transfer in large language models. In The Thirteenth International Conference on Learning Representations, 2025. Elizabeth Brannon. The independence of language and mathematical reasoning. Proceedings of the National Academy of Sciences, 102(9):31773178, 2005. Peter Carruthers. Language, thought and consciousness: An essay in philosophical psychology. Cambridge University Press, 1998. Jianghao Chen, Wei Sun, Qixiang Yin, Lingxing Kong, Zhixing Tan, and Jiajun Zhang. Ace-rl: Adaptive constraint-enhanced reward for long-form generation reinforcement learning. arXiv preprint arXiv:2509.04903, 2025a. Jianghao Chen, Zhenlin Wei, Zhenjiang Ren, Ziyong Li, and Jiajun Zhang. Lr2 bench: Evaluating long-chain reflective reasoning capabilities of large language models via constraint satisfaction problems. arXiv preprint arXiv:2502.17848, 2025b. Nadezhda Chirkova and Vassilina Nikoulina. Zero-shot cross-lingual transfer in instruction tuning of large language models. In Proceedings of the 17th International Natural Language Generation Conference, pp. 695708, 2024. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. In Forty-second International Conference on Machine Learning, 2025. John Coetzee, Micah Johnson, Youngzie Lee, Allan Wu, Marco Iacoboni, and Martin Monti. Dissociating language and thought in human reasoning. Brain Sciences, 13(1):67, 2022. Evelina Fedorenko and Rosemary Varley. Language and thought are not the same thing: evidence from neuroimaging and neurological patients. Annals of the New York Academy of Sciences, 1369 (1):132153, 2016. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Jiwoo Hong, Noah Lee, Rodrigo Martınez-Castano, Cesar Rodrıguez, and James Thorne. Crosslingual transfer of reward models in multilingual alignment. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pp. 8294, 2025. 11 Preprint. Chuxuan Hu, Yuxuan Zhu, Antony Kellermann, Caleb Biddulph, Suppakit Waiwitlikhit, Jason Benn, and Daniel Kang. Breaking barriers: Do reinforcement post training gains transfer to unseen domains? arXiv preprint arXiv:2506.19733, 2025a. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025b. Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, and Xiang Yue. Does math reasoning improve general llm capabilities? understanding transferability of llm reasoning. arXiv preprint arXiv:2507.00432, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Pu Jian, Junhong Wu, Wei Sun, Chen Wang, Shuo Ren, and Jiajun Zhang. Look again, think slowly: Enhancing visual reflection in vision-language models. arXiv preprint arXiv:2509.12132, 2025. Kaggle. Aime2025. 2025. URL https://www.kaggle.com/datasets/hengck23/ hengck23-aime-2025. Team Kimi, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Chong Li, Wen Yang, Jiajun Zhang, Jinliang Lu, Shaonan Wang, and Chengqing Zong. Xinstruction: Aligning language model in low-resource languages with self-curated cross-lingual instructions. In Findings of the Association for Computational Linguistics ACL 2024, pp. 546 566, 2024. Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, et al. X-reasoner: Towards generalizable reasoning across modalities and domains. arXiv preprint arXiv:2505.03981, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025b. Jia Maxwell. Aime2024. 2024. URL https://huggingface.co/datasets/ Maxwell-Jia/AIME_2024. Yongyu Mu, Peinan Feng, Zhiquan Cao, Yuzhang Wu, Bei Li, Chenglong Wang, Tong Xiao, Kai Song, Tongran Liu, Chunliang Zhang, et al. Revealing the parallel multilingual learning within large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 69766997, 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Jirui Qi, Shan Chen, Zidi Xiong, Raquel Fernandez, Danielle Bitterman, and Arianna Bisazza. When models reason in your language: Controlling thinking trace language comes at the cost of accuracy. arXiv preprint arXiv:2505.22888, 2025. 12 Preprint. Muhammad Reza Qorib, Junyi Li, and Hwee Tou Ng. Just go parallel: Improving the multilingual capabilities of large language models. arXiv preprint arXiv:2506.13044, 2025. Team Qwen. Qwq-32b: Embracing the power of reinforcement learning, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 18891897. PMLR, 2015. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Wei Sun, Wen Yang, Pu Jian, Qianlong Du, Fuwei Cui, Shuo Ren, and Jiajun Zhang. Ktae: modelfree algorithm to key-tokens advantage estimation in mathematical reasoning. arXiv preprint arXiv:2505.16826, 2025. Mingyang Wang, Lukas Lange, Heike Adel, Yunpu Ma, Jannik Strotgen, and Hinrich Schutze. Language mixing in reasoning language models: Patterns, impact, and internal causes. arXiv preprint arXiv:2505.14815, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Zhaofeng Wu, Ananth Balashankar, Yoon Kim, Jacob Eisenstein, and Ahmad Beirami. Reuse your rewards: Reward model transfer for zero-shot cross-lingual alignment. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 13321353, 2024. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, and Jiajun Zhang. Language imbalance driven rewarding for multilingual self-improving. In The Thirteenth International Conference on Learning Representations, 2025a. Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, and Jiajun Zhang. Implicit cross-lingual rewarding for efficient multilingual preference alignment. arXiv preprint arXiv:2503.04647, 2025b. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Zheng-Xin Yong, Farid Adilazuarda, Jonibek Mansurov, Ruochen Zhang, Niklas Muennighoff, Carsten Eickhoff, Genta Indra Winata, Julia Kreutzer, Stephen Bach, and Alham Fikri Aji. Crosslingual reasoning through test-time scaling. arXiv preprint arXiv:2505.05408, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Ruochen Zhou, Minrui Xu, Shiqi Chen, Junteng Liu, Yunqi Li, Xinxin Lin, Zhengyu Chen, and Junxian He. Does learning mathematical problem-solving generalize to broader reasoning? arXiv preprint arXiv:2507.04391, 2025. 13 Preprint."
        },
        {
            "title": "C Evaluation Details and Setup",
            "content": "C.1 Multilingual Reasoning Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . C.2 An Overview of Open-source LRMs . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Implementation Details",
            "content": "D.1 Training Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Experiments Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Detailed Results and Analysis",
            "content": "E.1 Observational Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1.1 How to select template for Base model? . . . . . . . . . . . . . . . . . . E.1.2 The Performance of Initial models . . . . . . . . . . . . . . . . . . . . . . E.1.3 The Performance of Open-source models . . . . . . . . . . . . . . . . . . E.2 Interventional Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2.1 The Impact of Different Model Families . . . . . . . . . . . . . . . . . . . E.2.2 The Impact of Model Size . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Parallel Scaling Law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3.1 The Language Settings in Parallel Scaling Law . . . . . . . . . . . . . . . E.3.2 The Detailed results in Parallel Scaling Law . . . . . . . . . . . . . . . . . E.3.3 Scaling Law Interpretation: The Drivers Behind the Exponents . . . . . . . E.3.4 The Impact of Selected Languages . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "F Prompts Template",
            "content": "F.1 Multilingual Reasoning Instruction . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Prompt hacking to force response language . . . . . . . . . . . . . . . . . . . . . F.3 Template for R1-like Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 15 15 16 16 17 17 17 17 17 19 19 20 20 20 22 24 26 26 26 14 Preprint."
        },
        {
            "title": "A LIMITATIONS AND FUTURE WORK",
            "content": "Generalizability and Domain Expansion primary limitation of our work is its focus on the mathematical reasoning domain. While our findings on the Parallel Scaling Law and the Just Go Parallel strategy reveal valuable phenomenon, their generalizability to other domains, such as coding, agent planning, remains to be verified. Future work could explore the extent to which our Parallel Scaling Law holds true across different domains. key direction is to develop more sophisticated parallel training strategies that can better overcome the diminishing returns shown in the scaling law. Furthermore, investigating how to apply our findings to the challenge of lowresource languages, key dilemma identified in our observational study, would be critical next step. Interpretability While our interventional and parallel studies provide strong evidence for the correlations between initial model capabilities, training strategies, and cross-lingual transfer, the underlying causal mechanisms are more hypothetical. For instance, the precise reason for RLs advantage in low-resource languages or the specific linguistic patterns that hinder cross-lingual transfer remain open questions. Future work could focus on mechanistic interpretability to dissect the internal representations and analyze specific failure modes, providing deeper understanding of how reasoning and language are coupled in these models."
        },
        {
            "title": "B THE USAGE OF LARGE LANGUAGE MODEL",
            "content": "We declare that the LLM was only used to refine the fluency of certain sentences during the writing of this paper. Every sentence polished with the LLM was carefully reviewed and approved by the authors. The LLM was not used for any other part of this research."
        },
        {
            "title": "C EVALUATION DETAILS AND SETUP",
            "content": "C.1 MULTILINGUAL REASONING BENCHMARKS We use the multilingual version of these four reasoning benchmarks provided in Qi et al. (2025), which use GPT-4o-MINI (Jaech et al., 2024) to translate all questions into the other ten languages Spanish (es), Russian (ru), German (de), French (fr), Bengali (bn), Swahili (sw), Thai (th), Japanese (ja), Chinese (zh), and Telugu (te), resulting in total of eleven languages for evaluation. MATH500 The MATH500 (Hendrycks et al., 2021) benchmark assesses the mathematical reasoning and problem-solving abilities of language models, addressing the need for more challenging evaluations as their general capabilities advance. It consists of 500 problems across five core mathematical domains: algebra, combinatorics, geometry, number theory, and precalculus. Each problem is designed to test multi-step reasoning and complex problem-solving skills, going beyond simple calculations or factual recall. AIME24&25 The AIME24 (Maxwell, 2024) and AIME25 (Kaggle, 2025) datasets contain problems from the American Invitational Mathematics Examination (AIME) for 2024 and 2025, respectively. AIME is prestigious high school mathematics competition renowned for its challenging problems, consisting of 30 questions. GPQA-Diamond GPQA-Diamond (Rein et al., 2024) consists of 198 multiple-choice questions across biology, chemistry, and physics, with difficulty levels ranging from challenging undergraduate to postgraduate. It is the highest quality subset, which includes only questions where both experts answer correctly and the majority of non-experts answer incorrectly. C.2 AN OVERVIEW OF OPEN-SOURCE LRMS Table 2 provides an overview of the various open-source LLMs evaluated in our observational study. These models, which include the DeepSeek-R1-Distill-Qwen-7B (Guo et al., 2025), OpenThinker 15 Preprint. series (Guha et al., 2025), Simple-RL-Zoo series (Zeng et al., 2025), s1 series (Muennighoff et al., 2025), and DAPO-Qwen-32B (Yu et al., 2025), range in size from 1.5B to 32B. Table 2: The Overview of the Open-source LLMs Used in Observational Study, including their initial model, parameter size, and training paradigm."
        },
        {
            "title": "Training Paradigm",
            "content": "DeepSeek-R1-Distill-Qwen-7B Open-Reasoner-Zero-7B OpenThinker2-7B OpenThinker3-7B Qwen-2.5-1.5B-SimpleRL-Zoo Qwen-2.5-7B-SimpleRL-Zoo Qwen-2.5-14B-SimpleRL-Zoo Qwen-2.5-Math-7B-SimpleRL-Zoo Qwen2.5-Math-7B-Oat-Zero s1.1-7B DAPO-Qwen-32B OpenThinker2-32B s1.1-32B Qwen2.5-Math-7B-Base Qwen2.5-7B-Base Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-1.5B-Base Qwen2.5-7B-Base Qwen2.5-14B-Base Qwen2.5-Math-7B-Base Qwen2.5-Math-7B-Base Qwen2.5-7B-Instruct Qwen2.5-32B-Base Qwen2.5-32B-Instruct Qwen2.5-32B-Instruct 7B 7B 7B 7B 1.5B 7B 14B 7B 7B 7B 32B 32B 32B"
        },
        {
            "title": "D IMPLEMENTATION DETAILS",
            "content": "D.1 TRAINING DATASET The Distribution of Parallel Questions Figure 7a shows the type and level distributions of the 1,000 English training questions sampled from the MATH dataset (Hendrycks et al., 2021). The type distribution is relatively balanced, and the number of questions increases steadily from Level 1 to Level 5. The Distribution of Unparallel Questions Moreover, Figure 7b presents the type and level distributions of 1,000 Russian questions used for an unparalleled training analysis experiment, which form separate, non-overlapping set from the 1000 English questions. The distributions of both type and level closely match those of the English training questions. This indicates that, in the analysis comparing parallel and unparallel training, the performance drop observed in unparallel training is not due to distributional differences between the unparallel and English datasets. (a) (b) Figure 7: Distribution of question difficulty. (a) The 1,000 English questions were utilized in the interventional study and for parallel training. (b) The 1,000 Russian questions for unparalleled training, comprising separate and non-overlapping set from the questions in (a). D.2 EXPERIMENTS ENVIRONMENTS All training and inference experiments were conducted on Ubuntu 22.04 equipped with 8 NVIDIA A800 GPUs. For RL training, we RL-tune all models using VeRL v0.2 (Sheng et al., 2024) with 16 Preprint. customized rewards. For Inference, we performed with vLLM 0.8.5 (Kwon et al., 2023). For Evaluation, we used Qwens Math codebase (Yang et al., 2024) for evaluation, following the prior work (Zeng et al., 2025; Liu et al., 2025b). D.3 HYPERPARAMETERS RL Training The maximum generation length was set to 4096 tokens, and the maximum prompt length to 1024 tokens, such that their sum matches the models maximum context length. The learning rate was fixed at 1 106. Training was performed with batch size of 128 questions. For each question, = 16 rollouts were sampled, using sampling temperature of 1.0. λ1 = 0.8, λ2 = 0.1, and λ3 = 0.1. In the evaluation setup, we used temperature of 0.6, top-p value of 0.95, and Inference maximum generation length of 8912 tokens for all models in the 1.5B14B series. For 32B models, we used the same temperature (0.6) and top-p value (0.95), but set the maximum generation length to 32,768 tokens, except for DAPO-Qwen-32B, which followed the official recommended settings: temperature of 1.0, top-p value of 0.7, and maximum generation length of 20,480 tokens. For AIME2024 and AIME2025, we report accuracy by averaging over 16 sampled generations per question, while for MATH500 and GPQA, accuracy is computed using single sampled generation per question."
        },
        {
            "title": "E DETAILED RESULTS AND ANALYSIS",
            "content": "E.1 OBSERVATIONAL STUDY E.1.1 HOW TO SELECT TEMPLATE FOR BASE MODEL? To accurately measure the reasoning capabilities of our base models for the transfer efficiency calculation, we evaluated them across different template settings. Specifically, we tested the Qwen2.57B-Base and Qwen2.5-Math-7B-Base models using Qwen-Math Template, Qwen-Instruct Template, and No Template setting. Qwen-Instruct Template: <im_start>systemn You are Qwen, created by Alibaba Cloud. You are helpful assistant. <im_end>n <im_start>usern{instruction}<im_end>n <im_start>assistantn Qwen-Math Template: <im_start>systemn Please reason step by step, and put your final answer within boxed{}. <im_end>n <im_start>usern{instruction}<im_end>n <im_start>assistantn No Template: {instruction} As shown in Table 3, the Qwen-Instruct Template consistently yielded better reasoning accuracy and improved reasoning language consistency for both base models on the multilingual MATH500 benchmark. This result guided our decision to use the Qwen-Instruct Template as the default for evaluating all math-based and general-based models. E.1.2 THE PERFORMANCE OF INITIAL MODELS To address the lack of detailed analysis on the influence of initial model properties on cross-lingual transfer and multilingual reasoning, we conducted comprehensive evaluation of various base mod17 Preprint. Table 3: The Performance of Base Models with Different Template Settings. Accuracy (%) and Off-target rate (%) across languages for different template settings on multilingual MATH500 benchmark."
        },
        {
            "title": "Settings",
            "content": "en es ru"
        },
        {
            "title": "Accuracy per language\nde",
            "content": "bn th fr sw zh"
        },
        {
            "title": "Average",
            "content": "ja te Acc Off-tag 23.0 2.4 27.0 15.2 1.4 21.7 Qwen-Math Template Qwen-Instruct Template 50.6 38.0 30.0 33.2 38.4 10.4 26.8 2.4 30.0 27.8 4.4 26.5 17.0 0.2 29.2 19.8 1.2 22.6 No Template 44.4 38.2 28.2 28.4 35.6 49.2 31.8 25.2 28.2 30.0 6. Qwen2.5-7B-Base 5.8 Qwen2.5-Math-7B-Base 1.2 19.7 Qwen-Math Template Qwen-Instruct Template 56.6 46.4 11.2 33.4 36.8 31.4 28.2 4.2 44.4 25.2 3.2 29.2 29.8 37.4 29.0 12.4 0.0 36.2 17.2 3.0 21.8 No Template 21.8 21.4 26.2 15.0 2.2 36.6 37.8 33.0 43.4 36. 4.2 2.8 9.4 16.6 15.7 18.5 30.5 18.0 31.5 els. Table 4 presents the detailed results of this analysis, including the accuracy and off-target rate across languages. Our evaluation reveals several key insights from the Qwen2.5-7B series. From linguistic perspective, the Instruct model exhibits the lowest off-target rate, followed by the General Base model and the Math Base model. However, when evaluated on multilingual reasoning accuracy, the order is reversed: the Instruct model significantly outperforms the Math Base model, which in turn performs better than the General Base model. Furthermore, clear scaling trend is observed within the Qwen2.5-Base models. As model size increases from 1.5B to 32B, the off-target rate decreases while multilingual reasoning accuracy steadily improves. particularly striking finding is that the Qwen2.5-7B-Instruct model achieves greater multilingual reasoning accuracy than the much larger Qwen2.5-32B model. This suggests that the instructionfollowing capability is critical factor for activating models multilingual reasoning abilities. This result challenges the popular wisdom that math-specific models are more amenable to RL training, especially when viewed through the lens of cross-lingual reasoning transfer. We posit that the superior multilingual capabilities of general instruction models make them more suitable initial model for RL training compared to both general base and math base models. These results highlight instruction-tuned models as the most advantageous starting point for enhancing multilingual reasoning through RL. E.1.3 THE PERFORMANCE OF OPEN-SOURCE MODELS Tables 5 and 6, in conjunction with Figure 8, provide detailed analysis of open-source model performance. These results illustrate the Multilingual Transferability Index (MTI), accuracy, and off-target rates of open-source models, and highlight the distinct performance differences between RL-tuned and SFT-tuned models across languages. Figure 8a further shows that all 7B SFT-tuned models exhibit performance degradation on bn, sw, and te, with the sole exception of DeepSeek-R1-Distill-Qwen-7B. Notably, this model was finetuned on massive amount of high-quality data generated by the DeepSeek-R1 model. Scaling the model size up to 32B provides modest performance gains across most languages, suggesting that larger models can partially mitigate the negative effects of SFT. However, the degradation in low-resource languages remains unresolved, as evidenced by the performance drop of OpenThinker32B on sw. Figure 8b shows that all RL-tuned models improve performance across all languages, with particularly large gains in bn, sw, and te. The heatmaps in Figure 8a and Figure 8b clearly illustrate the performance gap between SFT-tuned and RL-tuned models on low-resource languages, revealing consistent pattern: while SFT leads to degradation in low-resource settings, RL yields substantial improvements. 18 Preprint. Table 4: The Performance of Initial Models. Accuracy (%) and Off-target rate (%) across languages for different Initial models. Settings en es ru de Accuracy per language bn th fr sw zh ja te Average Acc Off-tag Multilingual MATH Qwen2.5-1.5B Qwen2.5-Math-7B Qwen2.5-7B-Instruct Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Qwen2.5-32B-Instruct Qwen2.5-1.5B Qwen2.5-Math-7B Qwen2.5-7B-Instruct Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Qwen2.5-32B-Instruct Qwen2.5-1.5B Qwen2.5-Math-7B Qwen2.5-7B-Instruct Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Qwen2.5-32B-Instruct Qwen2.5-1.5B Qwen2.5-Math-7B Qwen2.5-7B-Instruct Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Qwen2.5-32B-Instruct 19.60 56.60 74.80 50.60 42.20 54.00 78.60 0.21 13.75 10.42 2.29 2.50 2.71 15. 0.00 6.04 7.08 0.83 1.25 1.04 11.25 15.66 16.16 36.36 28.79 26.26 28.28 45.45 10.60 46.40 69.00 38.00 40.40 50.80 73.60 0.42 6.46 8.96 1.67 2.50 3.13 12.71 0.00 3.13 5.63 0.83 2.08 1.04 7.29 14.65 13.64 32.83 24.24 15.15 30.30 41. 7.80 11.20 59.60 30.00 36.00 42.00 68.00 0.00 1.67 8.13 2.08 2.29 2.08 11.25 0.00 0.83 5.21 0.21 2.50 1.46 7.29 15.15 3.54 23.74 20.71 20.71 28.28 38.89 1.00 33.40 56.60 33.20 29.60 37.80 68.40 0.00 3.33 8.75 2.71 2.50 3.33 11. 0.00 1.46 3.96 1.25 1.67 0.21 6.04 9.80 36.80 62.60 38.40 36.20 46.80 69.40 1.00 31.40 37.60 10.40 22.60 24.60 53.20 3.00 28.20 49.80 26.80 26.60 33.00 60.60 0.00 4.20 18.00 2.40 5.60 13.60 37.40 8.60 44.40 53.00 30.00 18.80 28.00 61. Multilingual AIME24 0.00 4.79 8.54 2.08 2.71 2.71 12.08 0.00 2.71 2.92 0.21 0.63 0.21 5.42 0.21 3.33 4.58 0.63 0.21 1.04 7.50 Multilingual AIME25 0.00 2.29 4.79 0.63 1.46 0.83 6. 0.00 0.42 0.83 0.21 0.00 0.21 1.25 0.00 0.83 1.67 0.00 0.21 0.00 2.71 Multilingual GPQA-Diamond 15.15 17.68 29.80 21.72 24.24 24.75 38.38 11.62 15.66 33.84 18.18 27.78 23.74 44.44 7.58 17.68 26.77 11.11 20.20 15.66 29.80 16.67 17.17 36.36 22.22 21.21 33.33 41. 0.00 0.00 1.04 0.00 0.00 0.00 2.92 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.63 6.88 5.63 1.25 1.04 1.25 7.29 0.21 4.79 3.75 0.42 0.42 1.04 5.42 13.64 11.62 24.75 17.68 22.22 17.68 32.83 15.15 22.22 30.81 23.23 12.12 27.27 36. 2.20 25.20 52.40 27.80 25.20 42.60 65.00 0.00 1.67 4.38 0.63 0.83 2.08 10.63 0.00 0.42 2.71 0.21 1.46 1.04 2.71 6.06 0.51 29.80 17.17 10.61 27.78 38.38 0.00 3.20 26.60 4.40 5.60 11.60 43.40 5.78 29.18 50.91 26.55 26.25 34.98 61. 0.00 0.63 1.88 0.00 0.21 0.00 2.50 0.00 0.42 0.42 0.00 0.21 0.21 0.42 0.13 4.11 5.93 1.23 1.40 1.69 9.05 0.02 1.88 3.28 0.42 1.02 0.64 4.64 15.15 16.16 21.72 12.63 16.16 17.17 27.27 13.31 13.82 29.71 19.79 19.70 24.93 37. 21.02 17.96 0.18 15.69 15.55 5.56 0.13 19.26 21.76 0.34 9.89 16.02 4.07 0.51 61.14 23.47 0.55 10.38 16.99 4.02 0.30 20.02 27.18 0.64 9.69 20.98 9.00 0.28 (a) SFT-tuned Models (b) RL-tuned Models Figure 8: The Performance of Various Open-source Models. Part 2: The transferability difference between SFT-tuned and RL-tuned models across languages. Note that None values indicate that Qwen-2.5-1.5B achieved zero accuracy in most languages on AIME24 and AIME25, making relative gain undefined. E."
        },
        {
            "title": "INTERVENTIONAL STUDY",
            "content": "E.2.1 THE IMPACT OF DIFFERENT MODEL FAMILIES Figure 9 compares the influence of model family on cross-lingual reasoning by using Qwen2.57B-Instruct and Llama3.1-8B-Instruct as initial models. We find that reinforcement learning (RL) 19 Preprint. Table 5: The Performance of Various Open-source Models. Part 1: Multilingual Transferability Index (MTI) of various models across benchmarks. The columns ID, OOD, and Avg refer to the MTI on in-domain (MATH500), out-of-domain (AIME24, AIME25, GPQA-Diamond), and all tasks, respectively. Note that None values indicate that Qwen-2.5-1.5B achieved zero accuracy in most languages on AIME24 and AIME25, making relative gain undefined."
        },
        {
            "title": "Models",
            "content": "Multilingual Reasoning Benchmarks MATH500 AIME24 AIME25 GPQA-D DeepSeek-R1-Distill-Qwen-7B Open-Reasoner-Zero-7B OpenThinker2-7B OpenThinker3-7B Qwen-2.5-1.5B-SimpleRL-Zoo Qwen-2.5-7B-SimpleRL-Zoo Qwen-2.5-14B-SimpleRL-Zoo Qwen-2.5-Math-7B-SimpleRL-Zoo Qwen2.5-Math-7B-Oat-Zero s1.1-7B DAPO-Qwen-32B OpenThinker2-32B S1.1-32B 3.493 3.195 0.093 0.157 5.322 4.543 2.381 3.920 2.807 0.310 3.634 0.936 1.382 2.312 2.677 0.876 1.502 None 3.189 3.360 2.884 1.324 0.920 2.337 1.513 1.583 2.864 1.479 1.843 2.434 None 1.217 0.959 3.079 3.158 1.192 2.066 4.235 3.429 4.168 1.320 1.604 1.318 1.383 6.531 1.655 4.335 2.149 0.671 0.854 0.201 0."
        },
        {
            "title": "MTI\nOOD",
            "content": "3.115 1.825 1.441 1.752 1.383 3.646 1.991 3.433 2.210 0.928 1.752 1.983 1."
        },
        {
            "title": "Avg",
            "content": "3.209 2.168 1.104 1.353 3.353 3.870 2.089 3.555 2.359 0.773 2.223 1.721 1.804 ID 3.493 3.195 0.093 0.157 5.322 4.543 2.381 3.920 2.807 0.310 3.634 0.936 1.382 consistently improves reasoning performance across all languages, regardless of the initial model family. However, notable difference is that Llama3.1 exhibits substantially greater performance gain on various benchmarks compared to Qwen2.5. This result suggests counter-intuitive principle: models with weaker initial English capabilities may possess greater potential for cross-lingual generalization. We posit that while stronger English-capable models, such as Qwen2.5, excel at English reasoning, they may become too entrenched in English-specific reasoning patterns, thereby limiting their ability to transfer these skills to other languages. E.2.2 THE IMPACT OF MODEL SIZE Table 7 presents the detailed results of our controlled study on model scaling, comparing the performance of Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct as initial models. We found clear distinction in transferability based on model size. The smaller 1.5B model exhibits larger relative gains on its in-domain training task (MATH500) and out-of-domain tasks (GPQADiamond), likely due to its weaker initial capabilities. In contrast, the larger 7B model shows smaller training gains in MATH500 and GPQA-Diamond but demonstrates superior transfer to more challenging tasks such as AIME24 and AIME25. This observation suggests key trade-off: models with stronger initial English performance have less potential for large relative gains in cross-lingual generalization, whereas smaller, weaker models possess greater capacity for significant improvement across languages. E.3 PARALLEL SCALING LAW E.3.1 THE LANGUAGE SETTINGS IN PARALLEL SCALING LAW Table 8 outlines the language settings for our experiment to validate the parallel scaling law, systematically increasing the number of parallel languages from 1 to 7. 20 Preprint. Table 6: The Performance of Various Open-source Models. Part 3: Accuracy (%) and Off-target rate (%) across languages for various open-source models. Settings en es ru de Accuracy per language bn th fr sw zh ja te Average Acc Off-tag Multilingual MATH500 86.20 76.20 67.00 66.40 69.80 40.20 53.80 18.40 70.00 53.20 17.60 56.25 DeepSeek-R1-Distill-Qwen-7B 81.60 76.00 70.40 69.80 71.20 45.20 63.20 15.00 62.80 61.80 17.60 57.69 Open-Reasoner-Zero-7B 86.00 74.80 64.80 63.00 73.00 34.40 58.80 12.60 65.80 70.00 OpenThinker2-7B 55.60 85.80 81.80 75.00 69.20 76.40 17.00 48.80 17.40 69.60 65.00 10.40 56.04 OpenThinker3-7B 57.60 41.40 36.80 38.20 42.40 11.80 28.80 14.60 33.60 31.00 31.24 Qwen-2.5-1.5B-SimpleRL-Zoo 77.60 72.00 65.00 64.20 68.00 42.20 60.40 24.20 62.00 59.80 25.80 56.47 Qwen-2.5-7B-SimpleRL-Zoo Qwen-2.5-14B-SimpleRL-Zoo 82.40 74.40 71.00 69.40 73.60 54.80 69.00 33.20 65.60 68.80 41.00 63.93 Qwen-2.5-Math-7B-SimpleRL-Zoo 80.40 72.60 66.00 68.60 70.60 45.80 57.40 15.00 61.80 54.40 14.20 55.16 79.80 72.40 32.80 55.60 50.40 47.60 49.40 16.80 58.00 43.40 11.80 47.09 Qwen2.5-Math-7B-Oat-Zero 75.80 68.20 60.80 59.00 69.60 37.00 57.40 16.40 57.20 51.60 20.40 52.13 s1.1-7B 68.80 65.00 58.80 60.20 63.00 52.80 58.40 44.80 54.20 56.80 44.80 57.05 DAPO-Qwen-32B 96.00 88.60 85.20 84.20 85.00 73.00 80.60 35.40 77.40 75.60 47.20 75.29 OpenThinker2-32B 95.40 91.20 85.00 83.60 88.00 73.00 81.20 57.00 77.40 80.80 53.60 78.75 S1.1-32B 8.40 7.40 Multilingual AIME24 6.46 9.58 18.64 9.79 40.63 27.71 26.25 23.13 27.50 DeepSeek-R1-Distill-Qwen-7B 12.86 16.25 18.13 17.29 15.21 17.71 14.79 Open-Reasoner-Zero-7B 18.11 37.08 18.33 17.08 13.75 20.83 11.04 20.21 OpenThinker2-7B 19.32 16.25 26.25 32.08 23.54 26.46 29.38 OpenThinker3-7B 0.25 0.21 0.21 0.42 0.00 Qwen-2.5-1.5B-SimpleRL-Zoo 5.36 5.42 6.25 8.33 6.25 Qwen-2.5-7B-SimpleRL-Zoo 10.25 9.79 Qwen-2.5-14B-SimpleRL-Zoo 12.71 13.13 13.13 10.42 13.33 10.53 8.13 Qwen-2.5-Math-7B-SimpleRL-Zoo 25.83 15.42 13.96 11.25 13.33 9.30 8.75 28.33 12.92 Qwen2.5-Math-7B-Oat-Zero 11.67 5.42 8.03 7.92 14.38 10.42 10.42 10.21 11.46 s1.1-7B 54.58 50.00 51.67 46.04 50.00 42.50 36.04 19.17 40.83 45.42 27.29 42.14 DAPO-Qwen-32B 74.17 61.88 55.42 56.67 55.42 59.17 49.17 13.96 56.88 37.71 34.58 50.45 OpenThinker2-32B 58.75 55.21 49.17 51.25 53.33 36.04 41.25 19.38 44.58 46.88 17.08 42.99 S1.1-32B 30.42 9.79 14.79 14.79 25.63 27.29 26.46 19.38 0.21 1.25 5.42 4.38 10.42 11.46 8.54 10.21 6.67 12.29 7.71 8. 0.83 1.25 5.42 3.54 0.00 2.50 5.63 2.50 0.42 0.21 2.50 1.67 2.50 3.54 0.42 2.08 3.54 1.67 1.04 1.67 5.63 0.00 3.75 9.17 5.00 6.25 5.21 0.00 7.29 0.00 7.29 8. Multilingual AIME25 9.58 14.96 DeepSeek-R1-Distill-Qwen-7B 8.33 Open-Reasoner-Zero-7B 17.25 OpenThinker2-7B 17.95 OpenThinker3-7B 0.02 Qwen-2.5-1.5B-SimpleRL-Zoo 2.56 Qwen-2.5-7B-SimpleRL-Zoo 7.82 Qwen-2.5-14B-SimpleRL-Zoo 5.91 9.58 Qwen-2.5-Math-7B-SimpleRL-Zoo 13.75 4.30 10.00 Qwen2.5-Math-7B-Oat-Zero 9.38 13.96 11.67 s1.1-7B 7.12 38.13 38.54 37.29 36.25 34.58 30.83 32.71 18.33 31.67 34.17 22.29 32.25 DAPO-Qwen-32B 57.29 50.00 48.13 52.29 43.96 45.42 41.88 12.50 52.50 36.04 25.63 42.33 OpenThinker2-32B 50.00 43.54 38.33 43.33 42.71 29.38 31.88 16.04 38.75 35.42 14.58 34.91 S1.1-32B 29.58 20.21 21.25 22.29 19.79 14.58 13.33 11.88 11.04 28.33 21.67 20.63 17.08 21.46 22.50 27.71 23.33 20.00 27.50 0.00 0.00 0.00 5.42 2.08 4.58 13.96 11.67 10.42 10.83 10.00 9.79 6.04 4.38 2.29 11.88 9.58 26.67 10.00 10.21 9.79 25.00 24.38 28.54 21.67 0.00 0.21 2.50 1.46 8.54 6.67 3.75 6.25 2.92 6.67 5.21 9.79 8.75 9.38 17.08 14.79 0.00 3.13 6.46 5.63 2.08 7. 0.42 0.00 2.71 3.13 0.00 0.63 1.88 1.04 1.04 0.21 0.00 0.21 2.08 1.67 0.00 0.21 2.08 1.04 0.42 0.00 5.63 1.67 9.38 6.67 0.00 0.83 3.54 2.71 1.46 2.08 5.42 6.67 6.88 0.00 3.96 0.00 3. Multilingual GPQA-Diamond 32.32 33.33 33.33 35.35 35.35 18.18 21.21 23.74 29.80 14.65 14.14 26.49 DeepSeek-R1-Distill-Qwen-7B 27.23 37.37 26.77 31.82 33.33 33.33 24.24 32.83 12.63 33.33 26.77 Open-Reasoner-Zero-7B 28.79 17.68 17.17 16.67 22.73 22.22 25.76 14.14 22.22 18.69 14.14 20.02 OpenThinker2-7B 16.02 23.23 18.69 22.22 16.67 24.24 14.65 12.63 21.72 10.10 OpenThinker3-7B 20.71 15.66 23.74 16.67 24.75 10.10 18.18 13.13 17.17 21.21 17.26 Qwen-2.5-1.5B-SimpleRL-Zoo 30.30 31.82 29.80 31.82 33.84 20.71 23.74 17.17 29.80 23.74 10.10 25.71 Qwen-2.5-7B-SimpleRL-Zoo 41.92 40.40 34.85 40.91 39.39 27.78 34.85 29.80 39.39 33.33 26.26 35.35 Qwen-2.5-14B-SimpleRL-Zoo 8.08 21.12 Qwen-2.5-Math-7B-SimpleRL-Zoo 30.81 26.26 22.73 27.78 28.28 18.18 17.17 12.63 16.94 25.76 17.17 Qwen2.5-Math-7B-Oat-Zero 21.21 15.66 19.19 21.72 7.07 17.68 14.14 20.20 22.22 29.29 s1.1-7B 17.17 16.16 24.75 16.67 18.69 18.73 52.50 44.44 40.91 48.99 41.92 37.37 42.93 31.82 46.97 47.98 30.81 42.42 DAPO-Qwen-32B 42.93 62.63 57.58 58.08 59.09 58.59 50.51 47.47 21.72 56.57 OpenThinker2-32B 64.65 57.58 57.58 59.60 56.57 41.41 48.48 36.36 56.57 53.03 32.83 51.33 S1.1-32B 27.27 16.67 6.06 30.81 9.09 9.09 7.07 8.59 7. 0.00 0.00 5.05 9.09 2.69 5.20 36.13 60.75 21.51 4.31 0.47 8.33 15.11 12.76 11.85 13.02 27.40 7.77 10.78 39.72 63.28 60.42 77.16 0.42 11.29 20.57 7.95 5.91 22.08 7. 6.97 10.04 39.77 63.28 61.14 79.51 0.61 12.12 22.12 6.17 4.56 22.65 8.05 6.11 6.20 38.15 59.23 14.69 3.49 2.62 12.26 19.74 11.98 5.88 22.91 11.85 21 Preprint. (a) MATH500 (b) AIME (c) AIME25 (d) GPQA-Diamond Figure 9: The Impact of Different Model Families in Interventional Study. Multilingual reasoning performance across languages, comparing the influence of model family using Qwen2.57B-Instruct and Llama3.1-8B-Instruct as initial models. Base represents the performance of the initial model, while +GRPO denotes performance after fine-tuning with GRPO on English data. The light red area denotes the improvement in accuracy between the Base and +GRPO models, while the light gray area represents the reduction in the off-target rate between the two. E.3.2 THE DETAILED RESULTS IN PARALLEL SCALING LAW Table 9 presents the detailed accuracy across languages with different numbers of parallel languages in Parallel Scaling Law. Table 10 presents the multilingual transfer metrics across languages with different numbers of parallel languages in Parallel Scaling Law. E.3.3 SCALING LAW INTERPRETATION: THE DRIVERS BEHIND THE EXPONENTS We argue that the sublinear exponents in our power-law fits for accuracy and transferability arise from the principle of diminishing returns in the models progression toward unified, languageagnostic representation. The very low exponent for accuracy (β = 0.02) indicates that reasoning performance is not primarily constrained by lack of multilingual exposure, but rather by the intrinsic difficulty of the reasoning task itself. Since large language models are already pre-trained on massive corpora, they 22 Preprint. Table 7: The Impact of Model Size in Interventional Study. Performance on various benchmarks across Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Settings en es ru de Performance bn fr th sw zh ja te Average across languages Training Untraining MATH500 AIME24 AIME25 GPQA-Diamond 20.40 2.08 1.04 9.09 27.20 0.42 0.21 20.71 17.40 -0.21 0.00 -0.51 MATH500 AIME24 AIME25 GPQA-Diamond 4.40 2.71 1.25 -3. 1.00 1.04 -1.04 4.55 1.40 0.83 0.00 0.00 Qwen2.5-1.5B-Instruct with GRPO on En Data 16.80 14.40 -0.21 1.25 0.42 0.21 8.59 5.05 17.40 0.42 0.00 12.12 7.80 0.21 0.00 -2.53 4.40 0.00 -0.21 2. 6.20 0.21 0.00 1.52 Qwen2.5-7B-Instruct with GRPO on En Data 0.40 5.60 2.08 1.46 0.63 2.50 4.04 -1.01 5.60 0.42 1.67 -1.01 4.20 0.42 1.46 -2.02 1.40 2.08 0.00 -7.07 5.60 0.42 0.00 7. 14.20 0.21 0.21 3.54 7.40 0.21 0.21 0.51 6.20 4.58 0.83 -3.03 0.80 0.21 -0.21 0.51 20.40 2.08 1.04 9.09 4.40 2.71 1.25 -3. 13.32 0.25 0.10 5.10 3.22 1.35 0.58 0.20 Table 8: The Language Settings in Parallel Scaling Law."
        },
        {
            "title": "Only English",
            "content": "en en, ru en, ru, fr w. One parallel w. Two parallel w. Three parallel en, ru, fr, es w. Four parallel w. Five parallel w. Six parallel w. Seven parallel en, ru, fr, es, de, bn, th, zh en, ru, fr, es, de en, ru, fr, es, de, bn en, ru, fr, es, de, bn, th Table 9: The Detailed Results in Parallel Scaling Law. Part 1: Accuracy across languages with different numbers of parallel languages."
        },
        {
            "title": "Settings",
            "content": "en es ru"
        },
        {
            "title": "Accuracy per language\nbn",
            "content": "th fr de sw zh ja te Acc Off-tag"
        },
        {
            "title": "Average",
            "content": "Multilingual MATH"
        },
        {
            "title": "Only English",
            "content": "79.2 70.0 61.0 62.2 68.2 41.8 55.4 53.4 19.4 58.6 27.4 54.2 w. One parallel 78.4 73.4 66.0 65.6 67.2 48.8 57.4 57.8 26.2 62.0 33.8 57.9 w. Two parallel 79.0 73.4 64.4 67.4 69.2 45.2 60.2 63.0 26.2 61.6 32.6 58.4 w. Three parallel 77.8 73.6 64.6 68.4 69.8 46.0 60.8 62.2 24.0 60.8 34.2 58.4 w. Four parallel 77.2 71.2 66.2 66.8 68.0 47.6 61.8 62.0 28.6 60.2 35.2 58.6 w. Five parallel 77.4 71.4 62.2 66.2 66.0 48.6 62.0 62.2 32.4 63.8 37.0 59.0 w. Six parallel 76.4 70.8 63.8 65.6 66.8 48.6 61.8 34.6 63.4 63.4 38.4 59.4 w. Seven parallel 76.6 71.2 63.6 66.2 66.2 49.4 62.6 33.8 63.5 63.4 38.2 59.5 0.5 0.2 0.2 0.4 0.6 0.4 0.5 0.2 possess strong logical foundations and broad factual knowledge. Parallel training mainly helps refine how this existing knowledge is applied across languages, rather than imparting fundamentally new reasoning abilities. As result, the incremental accuracy gains from each additional language remain marginal. By contrast, the much higher exponent for transferability (β = 0.29) represents the central finding of our study. This value reflects that the main advantage of parallel training lies not in boosting raw accuracy but in reshaping the models internal mechanisms. Specifically, it signals the emergence of learning-to-learn skill: the ability to abstract away from language-specific surface patterns and consolidate more robust cross-lingual representation. While each added parallel language 23 Preprint. Table 10: The Detailed Results in Parallel Scaling Law. Part 2: Relative gain across languages with varying numbers of parallel training languages. Rtrain and Rtarget denote relative gains on training and target languages, respectively. MTI indicates multilingual transfer index. Settings en es ru de Relative Gain bn fr th sw zh ja te Rtrain Rtarget MTI Transfer Metrics Multilingual MATH500 Only English 0.059 0.014 0.023 0.099 0.089 0.112 0.112 0.078 0.008 0.118 0.030 w. One parallel 0.048 0.064 0.107 0.159 0.073 0.298 0.153 0.456 0.091 0.183 0.271 w. Two parallel 0.056 0.064 0.081 0.191 0.105 0.202 0.209 0.456 0.189 0.176 0.226 w. Three parallel 0.040 0.067 0.084 0.208 0.115 0.223 0.221 0.333 0.174 0.160 0.286 w. Four parallel 0.024 0.032 0.121 0.180 0.070 0.266 0.241 0.644 0.170 0.149 0.323 w. Five parallel 0.008 0.049 0.047 0.201 0.102 0.319 0.209 0.633 0.174 0.218 0.391 w. Six parallel 0.021 0.026 0.070 0.159 0.067 0.293 0.241 0.922 0.196 0.210 0.444 w. Seven parallel 0.024 0.032 0.067 0.170 0.058 0.314 0.257 0.878 0.198 0.210 0.436 0.059 0.078 0.081 0.076 0.088 0.105 0.125 0.140 0.068 0.194 0.214 0.229 0.290 0.365 0.443 0. 1.163 2.496 2.650 3.002 3.282 3.475 3.534 3.631 strengthens this capacity, the marginal benefit diminishes as the representation stabilizes, naturally producing sublinear scaling curve. Theoretical Intuition The emergence of the Parallel Scaling Law can be understood through the intuitive principle of diminishing returns in learning abstract representations. When model is finetuned with only one or two parallel languages, it is forced to move beyond language-specific surface features and begin to form language-agnostic, unified representation for reasoning. This initial shift is highly impactful and yields disproportionately large gain in performance and transferability, which perfectly explains the First-Parallel Leap. As more parallel languages are added, the models core mechanism for cross-lingual abstraction becomes increasingly robust. At this point, each additional language contributes less and less marginal information, as the model has already mastered the core skill of mapping reasoning concepts across languages. E.3.4 THE IMPACT OF SELECTED LANGUAGES Figures 10 present detailed analysis of accuracy and relative gain across selection of parallel languages. As shown in Figure 10a, 10b and 10c, the relative gain on low-resource languages (bn, sw, and te) is consistently the largest, regardless of the chosen parallel language. In contrast, for high-resource languages (ru, de, and zh), the models accuracy remains comparable across all settings of parallel training. notable exception arises for bn: when trained with bn as the parallel language, accuracy on bn improves substantially compared to training with any other language. Figure 10d presents the accuracy and relative gain on GPQA-Diamond. We observe that ru achieves the largest relative gain. This is because, as shown in Table 4, Qwen2.5-7B-Instruct performs relatively poorly on ru in GPQA compared to other high-resource languages, thereby yielding larger relative gain. These results suggest that while low-resource languages consistently benefit the most from parallel training, and certain languages (e.g., bn and ru) exhibit language-specific effects, the overall outcomes of parallel training are largely robust to the choice of the parallel language. 24 Preprint. (a) MATH500 (b) AIME24 (c) AIME (d) GPQA-Diamond Figure 10: The Analysis of Parallel Scaling Law across Selected Parallel Languages. The accuracy and relative gain across various benchmarks with different parallel languages. Only en denotes only fine-tuned on English data. en&LANGUAGE indicates the model was fine-tuned on English and parallel language, with LANGUAGE representing ru, bn, de, zh, respectively. 25 Preprint."
        },
        {
            "title": "F PROMPTS TEMPLATE",
            "content": "F.1 MULTILINGUAL REASONING INSTRUCTION"
        },
        {
            "title": "The Instruction Used in Multilingual Reasoning Prompt",
            "content": "Please always think in [LANGUAGE]. Solve the following mathematics problem step by step. At the end, provide your final answer enclosed in boxed{}. Problem: {} F.2 PROMPT HACKING TO FORCE RESPONSE LANGUAGE The Prefixes Used in Prompt Hacking. Note that we list seven out of eleven languages. English: By request, will start thinking in English. Japanese: 要求があれば日本語で考え始めます Chinese: 应要求我将开始用中文思考 Spanish: peticion, empezare pensar en espanol. French: Sur demande, je commencerai `a penser en francais. German: Auf Anfrage werde ich anfangen, in Deutsch zu denken. Swahili: Kwa ombi, nitaanza kufikiria kwa Kiswahili. F.3 TEMPLATE FOR R1-LIKE REASONING The Template for R1-like Reasoning You are helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. The final answer must be put in boxed{}. Respond in the following format: <think>n...n</think>n<answer>n...n</answer>"
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Sciences",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "Wuhan AI Research"
    ]
}