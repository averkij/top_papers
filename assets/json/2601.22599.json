{
    "paper_title": "A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation",
    "authors": [
        "Kai Li",
        "Jintao Cheng",
        "Chang Zeng",
        "Zijun Yan",
        "Helin Wang",
        "Zixiong Su",
        "Bo Zheng",
        "Xiaolin Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset $\\sim$500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive."
        },
        {
            "title": "Start",
            "content": "A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Kai Li * 1 2 Jintao Cheng * 1 Chang Zeng 2 Zijun Yan 1 Helin Wang 3 Zixiong Su 2 Bo Zheng 2 Xiaolin Hu 1"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 6 2 0 2 0 3 ] . [ 1 9 9 5 2 2 . 1 0 6 2 : r Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on huge dataset 500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-ofdistribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive. The work was done while Kai Li is an intern there *Equal contribution 1Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing, China. 2Shanda AI Research Tokyo. 3Johns Hopkins University. 4Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China. 5Chinese Institute for Brain Research (CIBR), Beijing, China. Correspondence to: Xiaolin Hu <xlhu@tsinghua.edu.cn>. Technical Report. 1 Real-world acoustic environments are inherently polyphonic and contain multiple overlapping sound events (Heittola et al., 2013). While the human auditory system effectively isolates target sources from mixtures, capability known as the Cocktail Party Effect (Cherry, 1953; Arons, 1992), previous computational methods have focused on restricted domains like speech (Li et al., 2023; 2025) or music (Uhlich et al., 2024). Recently, computational auditory scene analysis (CASA) has expanded toward query-based universal sound separation (USS) (Lee et al., 2025a; Wang et al., 2025). In contrast to domain-specific methods, query-based USS targets arbitrary sound categories, including environmental and mechanical events. This capability is pivotal for applications such as immersive audio rendering (Gupta et al., 2022), machine hearing (Lyon, 2010), and intelligent audio editing (Yan et al., 2025). Conventional blind source separation (Li et al., 2023) is constrained by fixed output cardinality and the permutation problem, limiting its applicability to open-domain scenarios (Li et al., 2025). To address these limitations, query-based universal sound separation has emerged, leveraging multimodal prompts (e.g., text, audio, visual) to explicitly target specific sources (Kavalerov et al., 2019). Early approaches adopted discriminative paradigms, ranging from end-to-end architectures (Liu et al., 2022) to weakly supervised visual bridging (Dong et al., 2023) and large-scale open-domain frameworks (Liu et al., 2024). Recently, the field has shifted toward generative paradigms, such as FlowSep utilizing flow matching (Yuan et al., 2025), while DGMO (Lee et al., 2025b), ZeroSep (Huang et al., 2025) and ZETA (Manor & Michaeli, 2024) explore unsupervised strategies via testtime optimization or zero-shot settings. More recently, research has advanced toward unified prompting, where frameworks like OmniSep (Cheng et al., 2025) and SAM-Audio (Shi et al., 2025) integrate diverse modalities, including text, vision, and time segments, aiming to enhance controllability and facilitate separation-on-demand. Despite architectural evolution across both discriminative and generative paradigms, query-based USS methods persistently exhibit residual interference, characterized by the audible leakage of background noise or concurrent events Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Table 1. Overview of the original dataset and mixing pipelines adopted by USS methods. Train. Dur. and Val. & Test Dur. refer to the total duration of the original unmixed training set and the combined validation and test sets, respectively. Principled mix: whether the mixing process follows explicit semantic or acoustic constraints rather than random combinations. Pub. mixed data and Pub. mix tool: whether pre-mixed data and mixing code are publicly released. (= yes, = partially, = no) Train. Dur. (h) Val. & Test Dur. (h) Sample-rate Single-label Principled mix Pub. mixed data Pub. mix tool 32k 16k 32k 16k 16k 48k (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) Dataset (Liu et al., 2022) (Dong et al., 2023) (Liu et al., 2024) (Yuan et al., 2025) (Cheng et al., 2025) (Shi et al., 2025) 17.3 550 14,100 1,680 560 1,000, 4 50 109 9 10 47 Hive 2,442 292 44.1k into the separated output (Huang et al., 2024; Hai et al., 2024; Lee et al., 2025b). We attribute this deficiency to systematic biases in training data. Current training paradigms predominantly rely on large-scale in-the-wild datasets, such as AudioSet (Gemmeke et al., 2017) and VGGSound (Chen et al., 2020), due to their vast scale and category diversity. However, these datasets inevitably suffer from weak labels and high co-occurrence of events, resulting in severe label-signal misalignment (Fonseca et al., 2021). Consider the case where Rain clips frequently contain concurrent wind or traffic: lacking fine-grained supervision, models inevitably hallucinate these environmental artifacts as inherent acoustic characteristics of the target category. This systematic bias causes the model to treat background noise as part of the desired signal. Given these constraints, the scarcity of high-purity, single-event supervision remains critical bottleneck in training robust USS models (Kong et al., 2023; Wisdom et al., 2021). Prevailing paradigms prioritize scaling both dataset size and model capacity, exemplified by unified models trained on millions of audio hours (Shi et al., 2025). While yielding impressive performance, such brute-force scaling imposes prohibitive computational costs, creating significant barriers to reproducibility and accessibility for the broader research community. It motivates us to ask: Is it possible to achieve competitive separation performance with significantly greater data efficiency? To answer this question, we propose an automated pipeline for high-quality data cleaning and synthesis in query-based USS. It mines high-purity single-event segments from complex auditory scenes via unified label system and rigorous quality control. We then propose semantically consistent strategy to mix the single-event segments, yielding new dataset, named Hive. It comprises 2.4k hours of highquality raw audio. We trained representative discriminative (AudioSep) and generative (FlowSep) models on Hive and evaluated them alongside million-hour scale baselines like SAM-Audio. Remarkably, despite utilizing only 0.2% of the data scale used by SAM-Audio, our trained models achieved competitive perceptual quality to SAM-Audio. These results empirically indicate that prioritizing purity 2 of supervised signals offers data-efficient alternative to brute-force scaling, providing reproducible pathway for the efficient training of query-based USS models. 2. Related Work 2.1. Query-Based Universal Sound Separation Methods Blind Source Separation (BSS) has traditionally relied on internal statistics but is often constrained by the permutation problem and fixed source counts (Liutkus et al., 2013). To address these issues, the field has shifted toward querybased USS, which utilizes auxiliary prompts (e.g., text, audio) to isolate arbitrary targets from complex environments (Li et al., 2025). Current USS approaches are broadly categorized into discriminative and generative paradigms. Discriminative paradigm treats separation as signal estimation task, learning direct mapping function to filter the target from the mixture, typically via time-frequency masking. LASS-Net (Liu et al., 2022) pioneered this by introducing language queries, while AudioSep (Liu et al., 2024) and CLIPSep (Dong et al., 2023) further scaled this paradigm using contrastive audio-text or image-text alignment. Although computationally efficient, these methods are often constrained by the fixed resolution of the input representation. Conversely, generative paradigm models the data distribution of the source signal, formulating separation as conditional synthesis to reconstruct targets from noise or latent representations. This category includes FlowSep (Yuan et al., 2025), which utilizes flow matching, and training-free methods like DGMO (Lee et al., 2025b) and ZETA (Manor & Michaeli, 2024) that leverage pretrained priors. Recently, unified frameworks such as OmniSep (Cheng et al., 2025) and SAM-Audio (Shi et al., 2025) have integrated these capabilities with multimodal prompts. However, despite these architectural advances, models trained on weakly labeled in-the-wild data continue to suffer from severe interference in complex scenarios. We argue that this bottleneck stems from fundamental label-signal misalignment in current training datasets, suggesting that further progress hinges on the construction of high-quality, Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Figure 1. Overview of the proposed pipeline. The framework consists of three coupled stages: (1) ontology reconstruction & data preprocessing. (2) single-event semantic-acoustic alignment. (3) super-resolution-based standardization. event-aligned data rather than scale or architecture alone. 2.2. Data Cleaning and Synthesis Current USS methods rely heavily on data synthesis using in-the-wild datasets like AudioSet (Gemmeke et al., 2017) and VGGSound (Chen et al., 2020) (see Table 1). Although extensive, these datasets are inherently polyphonic and suffer from severe co-occurrence of events (e.g., wind inherent in rain recordings). This introduces systematic supervision noise, as models inevitably learn to associate background artifacts with the target category, establishing performance ceiling on separation purity (Fonseca et al., 2021). Furthermore, training data is typically constructed via naive random mixing, which ignores semantic consistency (Liu et al., 2022). While some methods attempt to filter anchors using pre-trained event detectors (Shi et al., 2025; Wisdom et al., 2021), these detectors are often trained on the same noisy data, leading to circular propagation of bias. Consequently, achieving robust separation without bruteforce scaling requires dual solution: mining high-purity single-event segments to fix the source, and implementing semantically consistent synthesis to fix the process (Kong et al., 2023). 3. Single-Event Data Collection Pipeline In-the-wild audio recordings are widely utilized in auditory research due to their massive scale and acoustic diversity (Liu et al., 2024; Shi et al., 2025), serving as cornerstone for generalizing to real-world environments. However, these uncurated data typically suffer from complex acoustic backgrounds and high co-occurrence of events, rendering raw supervision signals unreliable for high-fidelity separation tasks (Fonseca et al., 2021). To mitigate this, we propose an automated pipeline designed to mine high-quality singleevent segments from uncurated corpora and assign precise semantic annotations. As illustrated in Figure 1, the pipeline operates through three distinct stages: ontology reconstruction & data preprocessing, single-event semantic-acoustic alignment, and super-resolution-based standardization. 3.1. Ontology Reconstruction and Data Preprocessing discriminative and semantic-consistent label system is fundamental for USS. We adopted the AudioSet ontology (474 leaf nodes) as foundational taxonomy (Gemmeke et al., 2017). However, AudioSet exhibits significant semantic overlap and excessive granularity, which impedes effective class separation. To address this, we execute rigorous taxonomy reconstruction process validated by human experts. First, we merged synonymous or highly overlapping labels to reduce ambiguity. For instance, action-oriented labels (e.g., Drum beat) are unified with their corresponding entities (Drum). Second, fine-grained categories with minimal acoustic distinctiveness like specific biological sounds (Fowl and Coo) were aggregated into their superordinate concepts. Furthermore, we explicitly excluded labels describing broad acoustic environments (e.g., indoor, countryside), format tags (e.g., MP4), and abstract attributes. These categories typically represent background textures rather than separable, localizable foreground events. This pruning yielded refined, separation-oriented label space of 283 leaf nodes (see Table A1). The detailed reconstruction protocol is provided in Appendix A. Following the ontology definition, we standardized raw audio streams into uniform tensors. We employed sliding window strategy with 10s duration and 5s overlap. To filter invalid silence, segments with root mean square (RMS) energy below 5 104 were automatically discarded (Uh3 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation lich et al., 2024). Finally, to ensure uniform sequence length across the dataset and avoid padding-induced artifacts, any residual tail segment shorter than the full 10s window duration was discarded. 3.2. Single-Event Semantic-Acoustic Alignment The primary challenge in constructing separation datasets lies in resolving the ambiguity between acoustic cooccurrence of events and weak semantic labels (Fonseca et al., 2021; Kong et al., 2023). To address this, we propose cascaded alignment framework designed to ensure samples exhibit acoustic singularity while mapping precisely to mutually exclusive ontology leaf nodes. For instance-level purification, we maximized purity by filtering samples based on metadata, explicitly excluding segments tagged with multiple labels. However, to tackle unannotated background noise or transient interference, we introduced content-based neural polyphony detector. Leveraging the multimodal capabilities of Qwen3-Omni (Xu et al., 2025), we performed zero-shot binary classification to reject segments containing multi-event mixtures, thereby guaranteeing the acoustic isolation of training samples. While this establishes event singularity, precise semantic alignment under long-tail distributions remains non-trivial. We adopted coarse-to-fine strategy leveraging ontology topology. First, an audio-tag model1 predicted coarsegrained parent nodes, pruning ambiguous samples via confidence threshold of 0.7 (Yao et al., 2024). Subsequently, using the predicted coarse category as semantic prior, we employed Qwen3-Omni to refine the classification into specific leaf nodes within the restricted candidate subset. Samples failing to match any candidate leaf nodes are rejected. This design effectively integrates discriminative robustness with generative reasoning, ensuring precise alignment from acoustic signals to ontology labels. The detailed prompts employed by Qwen3-Omni are provided in Appendix B. 3.3. Super-Resolution-Based Standardization In-the-wild datasets exhibit significant sampling rate heterogeneity due to diverse acquisition conditions. Naive integration of such data introduces unnatural spectral discontinuities, biasing models to ignore high-frequency details. To ensure spectral consistency, we standardized the global sampling rate to 44.1 kHz via dual-strategy approach. For bandwidth-limited segments, we leveraged the Apollo model to plausibly reconstruct high-frequency harmonics from low-frequency priors (Li & Luo, 2025). Conversely, high-resolution inputs exceeding the target rate were downsampled via anti-aliasing filters, eliminating computational 1https://k2-fsa.github.io/sherpa/onnx/ audio-tagging/index.html Figure 2. Proportional composition of the 12 heterogeneous source datasets. Figure 3. Distribution of the number of sources in the Hive. redundancy while preserving spectral integrity. 3.4. Source Curation and Properties We aggregated raw audio from 12 different public datasets, including AudioSet (Gemmeke et al., 2017), VGGSound (Chen et al., 2020), FreeSound (Fonseca et al., 2017), and BBC Sound Effects (British Broadcasting Corporation, 1991). Unlike controlled studio recordings, these in-the-wild sources capture complex acoustic environments and longtail event distributions, which are essential for domain generalization. Following the rigorous cleaning and alignment as described in Section 3.1 and 3.2, the final source pool comprises 0.9M unique clips totaling 2,442 hours. As shown in Figure 2, the distribution exhibits realistic longtail structure: general-domain datasets (BBC, AudioSet) constitute the backbone (> 77%), while specialized subsets (e.g., DCASE (Mesaros et al., 2019), ESC50 (Piczak, 2015)) supplement rare events to prevent domain overfitting. Detailed compositions are provided in Appendix C. 4 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Figure 4. Label frequency statistics of Hive dataset. (a) Overall label distribution visualized as word cloud (token size mixture count). (b) Top-10 most frequent labels. (c) Bottom-10 least frequent labels. 4. Dataset Construction 4.1. Semantically Consistent Mixing Strategy Given the high-quality single-event segments obtained from the proposed pipeline, we proceed to synthesize training mixtures. However, naive random mixing often yields implausible combinations (e.g., aquatic animals co-occurring with urban traffic), introducing incorrect contextual priors. To mitigate this, we propose semantic compatibility protocol governed by logical co-occurrence matrix. We define binary matrix {0, 1}N where entries indicate semantic validity. is derived via Qwen3-Omni using the prompt in Figure A3, which enforces strict criteria: events must be capable of coexisting without spatiotemporal conflict (e.g., allowing typing with air conditioning while rejecting logical contradictions). Based on M, we generated training data. For each mixture, source count is sampled from {2, . . . , 5}. We ensure that the label set of sampled clips {l1, . . . , lC} satisfies pairwise compatibility (Mli,lj = 1, i, j). Valid tuples undergo duration normalization (4s training, 10s testing) and energy unification (RMS = 0.1). The mixture follows an additive superposition model: = x1 +(cid:80)C c=2 10SNRc/20 xc, where xc denotes the c-th normalized source. Relative gains are determined by SNRs randomly sampled from [5, 5] dB, simulating diverse acoustic conditions and enabling the model to adapt to varied energy ratios. 4.2. Hive Dataset Statistics Hive serves as large-scale dataset comprising 19.6 million mixtures (22.4k hours). The dataset is partitioned into training (17.5M), validation (1.75M), and test (350k) sets. Notably, as detailed in Table 1, Hive reserves 292 hours of distinct unmixed sources for validation and testing. This scale significantly exceeds the evaluation sets of prior large-scale baselines (e.g., 109 hours for AudioSep and 47 hours for SAM-Audio), ensuring that our zero-shot evaluation covers far broader spectrum of unseen acoustic events. While training and validation samples are cropped to 4 seconds for computational efficiency, test samples utilize 10-second clips to rigorously evaluate temporal consistency and long-term dependency modeling. Crucially, Hive adopts complexity-biased distribution rather than uniform one. As shown in Figure 3, dense scenarios with 5 concurrent sources dominate the training set ( 35%), effectively pushing the boundaries of separation robustness. The synthesized mixtures maintain broad coverage across the 283-class ontology. Figure 4 provides an overview of the label frequency distribution, while complete per-class breakdown over all 283 labels is provided in Appendix E. 5. Experiments Settings Datasets and Benchmarks We evaluated zero-shot generalization and robustness by testing the original pretrained checkpoints of all baselines on the Hive test set without fine-tuning. This setting highlights the difficulty of semantically consistent but acoustically dense scenes in Hive. To assess Hive as training resource, we trained AudioSep and FlowSep from scratch on Hive, and compared them with their original checkpoints on two out-of-domain benchmarks, MUSDB18-HQ (Rafii et al., 2019) and USS-Bench2. See Appendix for details. Baselines and Metrics We compared discriminative models (LASS-Net (Liu et al., 2022), CLIPSep (Dong et al., 2023), AudioSep (Liu et al., 2024), OmniSep (Cheng et al., 2025)) and generative models (ZETA (Manor & Michaeli, 2024), FlowSep (Yuan et al., 2025), ZeroSep (Huang et al., 2025), DGMO (Lee et al., 2025b), SAM-Audio (Shi et al., 2025)). Performance is quantified across three dimensions: (1) Signal Fidelity: SDR (Vincent et al., 2006), SI-SDR (Le Roux et al., 2019), and STOI (Taal et al., 2010); (2) Perceptual & Semantic Quality: FAD (Gui et al., 2024), LPAPS (Manor & Michaeli, 2024), and CLAP similarity (CLAP Audio, CLAP Text) (Huang et al., 2023); (3) Reference-free Qual2https://mab.to/GsSVrdZK0dfQ1/us3 5 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation ity: SAM-Audio Judge (Shi et al., 2025), reporting overall quality (OQ), recall (Rec), precision (Pre), and faithfulness (Fai). Detailed configurations for baselines and metrics are provided in Appendix G. Implementation Details All experiments were implemented in PyTorch and conducted on 8 NVIDIA A100 (80GB) GPUs. To ensure fair comparison, we adhered to the original hyperparameter architectures of the trained models. AudioSep was trained with batch size of 64 using AdamW (Loshchilov & Hutter, 2017) and learning rate of 1103, decayed by factor of 0.5 upon validation plateaus (patience=20). FlowSep utilized constant learning rate of 5 105. Both models were trained for 3M steps to ensure convergence. During evaluation, all outputs were resampled to 44.1 kHz. Inference efficiency was measured using 1-second segments. 6. Results 6.1. Validation of Semantic-Acoustic Alignment To quantitatively validate the label purity achieved by our pipeline, we conducted 4-alternative forced choice (4AFC) identification task on 20 randomly sampled clips. We established the ground truth for these samples through human consensus. Specifically, we recruited = 38 participants to evaluate each audio clip against four candidate labels (sampled from the same ontology level). The label receiving the majority of votes was defined as the ground truth. Inter-rater reliability was robust (Fleiss Îº = 0.863), confirming high consistency in the human consensus. Based on this consensus-derived ground truth, we calculated the average human performance (91.89%) by averaging the accuracy of individual participants choices against the majority vote. We then benchmarked our pipeline against human raters and three representative large audio-language models (LALMs): MiDashengLM (Dinkel et al., 2025), MiMoAudio-7B (Zhang et al., 2025), and Qwen3-Omni (Xu et al., 2025). For the LALM baselines, we employed standard multiple-choice prompt template instructing the models to select the most appropriate category (A/B/C/D). In contrast, the result for our method was derived by executing the automated annotation workflow described in Section 3. As summarized in Table 2, our pipeline achieved an accuracy of 95.00%, surpassing both the average human performance (91.89%) and the Qwen3-Omni baseline (90.00%). This suggested that our coarse-to-fine alignment and ensemble voting effectively rectified raw model hallucinations. paired t-test confirms that this improvement over human performance is statistically significant (p = 0.03 < 0.05). These results empirically demonstrated that our pipeline generates supervision signals with purity superior to human annotation. Further details on the experimental setup and Table 2. Comparative performance on the 4-AFC audio event recognition task. Evaluation on 20 clips (chance = 25%). Method Accuracy (%) Human Average MiDashengLM (Dinkel et al., 2025) MiMo-Audio (Zhang et al., 2025) Qwen3-Omni (Xu et al., 2025) Ours 91.89 90.00 90.00 90.00 95.00 statistical analysis are provided in Appendix H. 6.2. Zero-shot Benchmarking on the Hive Test Set Tables 3 and 4 present the zero-shot performance and computational efficiency of baselines on the Hive test set. We evaluated official pretrained checkpoints directly to expose their limitations in handling semantically consistent, acoustically dense scenarios. Performance Analysis. While AudioSep achieved state-ofthe-art SDR (2.37 dB), early methods like CLIPSep failed catastrophically (< 0 dB). This collapse implied heavy reliance on co-occurrence bias, where models trained on inthe-wild data exploit environmental cues (e.g., wind implies rain) as shortcuts (Wisdom et al., 2021). Hives decorrelated mixtures expose this brittleness by removing contextual cheat codes, as demonstrated by the audio samples and spectrogram visualizations available on our demo page3. Conversely, generative models (e.g., SAM-Audio) excelled in perceptual metrics (FAD 1.0) but lagged in semantic fidelity (CLAP-T). This indicates tendency towards conditional synthesis rather than true extraction, leading to hallucinated textures. Finally, the consistent degradation as mixtures scale from 2 to 5 sources (Appendix Table A3) confirms that acoustic density remains critical bottleneck, even with precise semantic alignment. Efficiency Analysis. Table 4 highlights the trade-off between performance and cost. Discriminative models demonstrated significant advantages regarding inference speed and resource consumption. LASS-Net and AudioSep maintain minimal latency (e.g., 0.26s CPU time for LASS-Net), making them suitable for real-time edge deployment despite their generalization limits. Conversely, generative approaches incur prohibitive costs. SAM-Audio, with over 8B parameters, demands 32GB GPU memory, precluding consumer-grade deployment. Test-time optimization methods like DGMO are even slower (4900 CPU time per second of audio), rendering them impractical for real-world applications. No existing method currently balanced high-fidelity separation with low-latency inference in the rigorous scenarios embodied by Hive. Discriminative methods satisfied realtime requirements but currently suffered from generalization bottlenecks potentially stemming from co-occurrence bias, 3https://shandaai.github.io/Hive Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Table 3. Zero-shot results on the Hive test set (averaged 2-to-5 mix). Models marked with (Hive) were trained on the Hive dataset. Detailed results are in Table A3. Best results are bold, second-best are(cid:58)(cid:58)(cid:58)(cid:58)wavy(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) underlined. Generative models without waveform preservation guarantees are marked with for signal fidelity metrics. Signal Fidelity Perceptual & Semantic Quality Reference-free Quality SDR SI-SDR STOI LPAPS FAD CLAP-A CLAP-T OQ Rec Pre Fai Model Discriminative Models LASS-Net (Liu et al., 2022) CLIPSep (Dong et al., 2023) AudioSep (Liu et al., 2024) OmniSep (Cheng et al., 2025) AudioSep (Hive) Generative Models -2.97 -6.20 (cid:58)(cid:58)(cid:58)(cid:58)2.37 -2.85 5.67 -4.04 -9.38 (cid:58)(cid:58)(cid:58)1.58 -4.67 5.02 0.41 0.26 (cid:58)(cid:58)(cid:58)0.49 0.35 0.52 ZETA (Manor & Michaeli, 2024) FlowSep (Yuan et al., 2025) ZeroSep (Huang et al., 2025) DGMO (Lee et al., 2025b) SAM-Audio (Shi et al., 2025) FlowSep (Hive) 4.78 5.95 4.22 4.93 3.86 5.43 (cid:58)(cid:58)(cid:58)4.18 4.50 4.94 5.21 4.25 1.04 1.05 0.90 0.96 0.80 1.04 0.87 0.92 0.97 1.03 (cid:58)(cid:58)(cid:58)0. 0.43 0.43 0.57 0.52 0.65 0.44 0.57 0.55 0.51 0.41 (cid:58)(cid:58)(cid:58)0.61 0.14 0.10 0.26 0.16 0.31 (cid:58)(cid:58)(cid:58)0.28 0.16 0.25 0.27 0.16 0.19 2.44 2.45 2.94 2.65 3.34 2.82 2.76 2.87 2.85 2.90 (cid:58)(cid:58)(cid:58)3. 4.42 4.11 4.86 4.58 (cid:58)(cid:58)(cid:58)4.81 4.10 4.06 4.76 4.48 3.77 4.18 2.48 2.63 2.96 2.76 (cid:58)(cid:58)(cid:58)3.40 3.54 2.90 2.97 3.19 3.33 3.35 4.06 3.37 4.47 3.82 (cid:58)(cid:58)(cid:58)4.39 3.19 3.69 4.27 3.85 3.63 3. Table 4. Comparison of computational efficiency across different audio separation models. MACs, CPU and GPU times are measured for 1-second audio, averaged over 1000 runs after 5 warm-up iterations. Model Params (M) MACs (G/s) CPU Time (s) GPU Time (s) GPU Mem (MB) Discriminative Models LASS-Net (Liu et al., 2022) CLIPSep (Dong et al., 2023) AudioSep (Liu et al., 2024) OmniSep (Cheng et al., 2025) Generative Models ZETA (Manor & Michaeli, 2024) FlowSep (Yuan et al., 2025) ZeroSep (Huang et al., 2025) DGMO (Lee et al., 2025b) SAM-Audio (Shi et al., 2025) 63.40 181.57 (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) 238.60 1231.09 420.98 693.59 420.98 420.98 8248.14 15.57 35.39 (cid:58)(cid:58)(cid:58)(cid:58) 120.94 48.97 28619.85 2304.50 1264.48 2577.33 770.36 0.26 (cid:58)(cid:58)(cid:58)(cid:58)0.76 1.99 1. 117.78 59.97 32.64 4902.61 65.30 0.01 (cid:58)(cid:58)(cid:58)(cid:58)0.02 (cid:58)(cid:58)(cid:58)(cid:58)0.02 0.08 14.29 1.30 4.29 42.36 1.32 296.35 573.69 (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) 1094.73 4883.00 1730.68 5851.35 1723.50 2642.30 32164.97 whereas generative methods show potential in perceptual metrics but typically necessitate iterative inference and high resource consumption. These gaps underscore the necessity of Hives high-purity supervision to train efficient, robust query-based USS models. 6.3. Performance Comparison on Third Party Test Sets The quantitative comparisons on out-of-distribution (OOD) benchmarks are presented in Table 5. We first compared the generalization capabilities of models trained on Hive against baselines trained on their original in-the-wild datasets. Despite utilizing significantly less training data than the original AudioSep (2.4k hours vs. 14.1k hours), the model trained on Hive demonstrates superior cross-domain performance. Specifically, on USS-Bench, AudioSep (Hive) achieves substantial improvement of over 4 dB in SDR (1.86 dB 2.29 dB) and improves the reference-free OQ from 2.97 to 3.56. Similarly, in the zero-shot music separation task (MUSDB18-HQ), AudioSep (Hive) increases the SDR from 1.01 dB to 1.36 dB. The superior performance on real-world datasets (MUSDB18-HQ) confirms that these models, despite being trained on synthetic mixtures, generalize effectively to realistic acoustic environments. Furthermore, we benchmarked our method against SAMAudio, state-of-the-art foundation model trained on approximately 1 million hours of audio. It is notable that while the Hive dataset constitutes only 0.2% of the data volume of SAM-Audios massive corpus (2.4k vs. 1M), models trained on Hive remain highly competitive. On USS-Bench, AudioSep (Hive) outperforms SAM-Audio in terms of FAD (0.75 vs. 0.90) and CLAP-Audio similarity (0.69 vs. 0.57). These results substantiate our central hypothesis: the purity of supervision signals is often more decisive than mere data scale. By eliminating co-occurrence noise, Hive enables the data-efficient training of foundation models that generalize robustly to diverse audio environments. 6.4. Impact of Training Data Scale To investigate the marginal effect of training data scale on general audio separation performance and the scaling behavior of the Hive dataset, we constructed training subsets with logarithmic growth ranging from 175k to 17.5M sam7 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Table 5. Performance comparison on third-party test sets. Orig. stands for Original Datasets. Model (Train Set, Scale) MUSDB18-HQ AudioSep (Orig., 14.1k h) FlowSep (Orig., 1.7k h) SAM-Audio (Orig., 1M h) AudioSep (Hive, 2.4k h) FlowSep (Hive, 2.4k h) USS-Bench AudioSep (Orig., 14.1k h) FlowSep (Orig., 1.7k h) SAM-Audio (Orig., 1M h) AudioSep (Hive, 2.4k h) FlowSep (Hive, 2.4k h) Signal Fidelity Perceptual & Semantic Quality Reference-free Quality SDR SI-SDR STOI LPAPS FAD CLAP-A CLAP-T OQ Rec Pre Fai -1.01 (cid:58)(cid:58)(cid:58)(cid:58) 1.36 -1.86 (cid:58)(cid:58)(cid:58)(cid:58) 2.29 -1.46 (cid:58)(cid:58)(cid:58)(cid:58) 0.89 -3.63 (cid:58)(cid:58)(cid:58)(cid:58) 0.30 (cid:58)(cid:58)(cid:58)0.44 0.52 (cid:58)(cid:58)(cid:58)0.43 0.59 4.94 4.72 4.33 4.68 (cid:58)(cid:58)(cid:58)4.34 5.02 5.19 4.96 4.22 (cid:58)(cid:58)(cid:58)4.62 0.97 0.94 0.83 (cid:58)(cid:58)(cid:58)0.91 0. 0.91 0.93 0.90 0.75 (cid:58)(cid:58)(cid:58)0.88 0.49 0.54 0.65 (cid:58)(cid:58)(cid:58)0.58 0.65 (cid:58)(cid:58)(cid:58)0.58 0.56 0.57 0.69 (cid:58)(cid:58)(cid:58)0.58 0.09 0.11 (cid:58)(cid:58)(cid:58)0.12 0.15 (cid:58)(cid:58)(cid:58)0.12 0.11 0.12 (cid:58)(cid:58)(cid:58)0.15 0.22 (cid:58)(cid:58)(cid:58)0.15 2.39 2.21 3.24 3.01 (cid:58)(cid:58)(cid:58)3. 2.97 2.33 (cid:58)(cid:58)(cid:58)3.47 3.56 2.89 3.92 3.54 4.54 4.18 (cid:58)(cid:58)(cid:58)4.22 4.01 3.56 (cid:58)(cid:58)(cid:58)4.14 4.22 3.80 2.40 2.30 3.68 3.14 (cid:58)(cid:58)(cid:58)3.22 3.04 2.58 3.77 (cid:58)(cid:58)(cid:58)3.68 3.34 3.84 3.33 4.27 (cid:58)(cid:58)(cid:58)4.05 3. 4.17 3.14 3.91 (cid:58)(cid:58)(cid:58)4.03 3.75 Figure 5. Scaling trends of AudioSep and FlowSep on the Hive test set across logarithmically increasing training data volumes (175k to 17.5M). ples and trained AudioSep under identical hyperparameter settings. As shown in Figure 5 and detailed in Table A4, increasing the training data consistently leads to marked and stable improvements across key metrics, including signal fidelity and perceptual quality. Specifically, when the training set size expands from 175k to the full 17.5M samples, the model achieves 1.55 dB gain in SDR on the Hive test set. Meanwhile, the FAD metric, which reflects the naturalness of generated audio, improves from 0.85 to 0.80, and the semantic alignment metric CLAP-A score steadily rises to 0.65. This consistent performance improvement indicates that the Hive dataset exhibits high information density and diversity, supporting that continued model learning without saturation or overfitting at the current experimental scale. For detailed analysis of the scaling behavior, please refer to Appendix J. More importantly, these results strongly support the core argument that the purity of supervisory signals is more critical than data volume for general separation tasks. cross comparison with baseline results in Table 3 shows that model trained on only 875k samples (approximately 1,000 hours) already achieves an SDR of 4.96 dB, substantially surpassing the original AudioSep model trained on 14,100 hours of large-scale in-the-wild data from AudioSet and VGGSound (2.37 dB). This counterintuitive observation highlights that label-signal misalignment, which is prevalent in in-the-wild data, seriously limits model optimization. In contrast, the Hive dataset eliminates co-occurrence noise through strict semantic consistency constraints, enabling models to learn more precise source features and semantic mappings from significantly fewer training samples. 7. Conclusions In this paper, we propose an automated data cleaning and synthesis pipeline and constructed Hive, large-scale, highfidelity synthetic dataset. By combining rigorous ontology reconstruction with semantic filtering powered by multimodal large models, we successfully mined high-purity single-event segments from complex natural acoustic environments and synthesized training mixtures via semantically consistent strategy. Extensive experiments demonstrate that models trained on the Hive dataset achieve separation accuracy and perceptual quality competitive with million-hour-scale SAM-Audio, while utilizing data volume only 0.2% that of SAM-Audio. Furthermore, these models trained on the Hive dataset exhibit remarkable zeroshot generalization to out-of-distribution real-world benchmarks. These findings validate that prioritizing purity of supervised signals offers data-efficient alternative to bruteforce scaling. By significantly reducing the data and computational resources required for training, this work provides reproducible paradigm for developing robust USS. 8 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation"
        },
        {
            "title": "Impact Statement",
            "content": "This work advances computational auditory scene analysis by addressing the high computational barriers in querybased universal sound separation. We introduce method for constructing high-fidelity synthetic datasets (Hive) that enables models to achieve state-of-the-art generalization with significantly less data. By mitigating the co-occurrence noise prevalent in in-the-wild recordings, our approach supports the training of robust open-domain separation models with reduced energy consumption and infrastructure costs. This contributes to the democratization of AI, making advanced auditory perception accessible to researchers with limited computational resources. Potential applications range from immersive audio and assistive listening to on-device machine hearing. However, the capabilities of universal sound separation also carry risks. The technology could facilitate privacy intrusion by enabling the extraction of specific speech or sensitive acoustic cues from mixed recordings. Furthermore, separated outputs might be misused for misleading audio editing or raise copyright concerns regarding isolated content. Additionally, while our pipeline automates data cleaning, it relies on model-based decisions that may propagate inherent biases, and synthetic mixtures may not fully capture the chaotic nuances of all real-world distributions. We therefore encourage the responsible release and use of the associated data and models, including adherence to licensing terms, safeguards against misuse, and thorough bias evaluation prior to deployment in high-stakes settings."
        },
        {
            "title": "References",
            "content": "Arons, B. review of the cocktail party effect. Journal of the American Voice I/O society, 12(7):3550, 1992. Botinhao, C. V., Wang, X., Takaki, S., and Yamagishi, J. Investigating rnn-based speech enhancement methods for noise-robust text-to-speech. In 9th ISCA speech synthesis workshop, pp. 159165, 2016. British Broadcasting Corporation. BBC Sound Effects Library. Films for the Humanities & Sciences, 1991. Cherry, E. C. Some experiments on the recognition of speech, with one and with two ears. Journal of the acoustical society of America, 25:975979, 1953. Dinkel, H., Li, G., Liu, J., Luan, J., Niu, Y., Sun, X., Wang, T., Xiao, Q., Zhang, J., and Zhou, J. Midashenglm: Efficient audio understanding with general audio captions. arXiv preprint arXiv:2508.03983, 2025. Dong, H.-W., Takahashi, N., Mitsufuji, Y., McAuley, J., and Berg-Kirkpatrick, T. Clipsep: Learning text-queried sound separation with noisy unlabeled videos. In The Eleventh International Conference on Learning Representations, 2023. Drossos, K., Lipping, S., and Virtanen, T. Clotho: An audio captioning dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 736740. IEEE, 2020. Fonseca, E., Pons, J., Favory, X., Font, F., Bogdanov, D., Ferraro, A., Oramas, S., Porter, A., and Serra, X. Freesound datasets: platform for the creation of open audio datasets. In Proceedings of the 18th International Society for Music Information Retrieval Conference, pp. 486493. ISMIR, 2017. Fonseca, E., Favory, X., Pons, J., Font, F., and Serra, X. Fsd50k: an open dataset of human-labeled sound events. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:829852, 2021. Gemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A., Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 776780. IEEE, 2017. Gui, A., Gamper, H., Braun, S., and Emmanouilidou, D. Adapting frechet audio distance for generative music evaluation. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 13311335. IEEE, 2024. Chen, H., Xie, W., Vedaldi, A., and Zisserman, A. Vggsound: large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 721725. IEEE, 2020. Gupta, R., He, J., Ranjan, R., Gan, W.-S., Klein, F., Schneiderwind, C., Neidhardt, A., Brandenburg, K., and Valimaki, V. Augmented/mixed reality audio for hearIEEE Signal ables: Sensing, control, and rendering. Processing Magazine, 39(3):6389, 2022. Cheng, X., Zheng, S., Wang, Z., Fang, M., Zhang, Z., Huang, R., Ji, S., Zuo, J., Jin, T., and Zhao, Z. Omnisep: Unified omni-modality sound separation with query-mixup. In The Thirteenth International Conference on Learning Representations, 2025. Hai, J., Wang, H., Yang, D., Thakkar, K., Dehak, N., and Elhilali, M. Dpm-tse: diffusion probabilistic model for target sound extraction. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 11961200. IEEE, 2024. 9 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Heittola, T., Mesaros, A., Eronen, A., and Virtanen, T. Context-dependent sound event detection. EURASIP Journal on audio, speech, and music processing, 2013(1): 1, 2013. Liu, X., Liu, H., Kong, Q., Mei, X., Zhao, J., Huang, Q., Plumbley, M. D., and Wang, W. Separate what you describe: Language-queried audio source separation. In Proc. Interspeech 2022, pp. 18011805, 2022. Huang, C., Liang, S., Tian, Y., Kumar, A., and Xu, C. Highquality visually-guided sound separation from diverse categories. In Proceedings of the Asian Conference on Computer Vision, pp. 3549, 2024. Liu, X., Kong, Q., Zhao, Y., Liu, H., Yuan, Y., Liu, Y., Xia, R., Wang, Y., Plumbley, M. D., and Wang, W. Separate anything you describe. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. Huang, C., Ma, Y., Huang, J., Liang, S., Tang, Y., Bi, J., Liu, W., Mesgarani, N., and Xu, C. Zerosep: Separate anything in audio with zero training. arXiv preprint arXiv:2505.23625, 2025. Huang, R., Huang, J., Yang, D., Ren, Y., Liu, L., Li, M., Ye, Z., Liu, J., Yin, X., and Zhao, Z. Make-an-audio: Text-toaudio generation with prompt-enhanced diffusion models. In International Conference on Machine Learning, pp. 1391613932. PMLR, 2023. Kavalerov, I., Wisdom, S., Erdogan, H., Patton, B., Wilson, K., Le Roux, J., and Hershey, J. R. Universal sound separation. In 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pp. 175179. IEEE, 2019. Kong, Q., Chen, K., Liu, H., Du, X., Berg-Kirkpatrick, T., Dubnov, S., and Plumbley, M. D. Universal source separation with weakly labelled data. arXiv preprint arXiv:2305.07447, 2023. Le Roux, J., Wisdom, S., Erdogan, H., and Hershey, J. R. Sdrhalf-baked or well done? In 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 626630, 2019. Lee, D., Kwon, Y., and Choi, J.-W. Deepasa: An objectoriented one-for-all network for auditory scene analysis. arXiv preprint arXiv:2509.17247, 2025a. Lee, G., Han, G., and Seo, P. H. Dgmo: Training-free audio source separation through diffusion-guided mask optimization. arXiv preprint arXiv:2506.02858, 2025b. Li, K. and Luo, Y. Apollo: Band-sequence modeling for high-quality audio restoration. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2025. Li, K., Yang, R., and Hu, X. An efficient encoder-decoder architecture with top-down attention for speech separation. In The Eleventh International Conference on Learning Representations, 2023. Li, K., Chen, G., Sang, W., Luo, Y., Chen, Z., Wang, S., He, S., Wang, Z.-Q., Li, A., Wu, Z., et al. Advances in speech separation: Techniques, challenges, and future trends. arXiv preprint arXiv:2508.10830, 2025. Liutkus, A., Durrieu, J.-L., Daudet, L., and Richard, G. An overview of informed audio source separation. In 2013 14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS), pp. 14. IEEE, 2013. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Lyon, R. F. Machine hearing: An emerging field [exIEEE signal processing magazine, ploratory dsp]. (5):131139, 2010. Manor, H. and Michaeli, T. Zero-shot unsupervised and text-based audio editing using ddpm inversion. In Proceedings of the 41st International Conference on Machine Learning, pp. 3460334629, 2024. Mesaros, A., Diment, A., Elizalde, B., Heittola, T., Vincent, E., Raj, B., and Virtanen, T. Sound event detection in the dcase 2017 challenge. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27(6):9921006, 2019. Piczak, K. J. Esc: Dataset for environmental sound classification. In Proceedings of the 23rd ACM international conference on Multimedia, pp. 10151018, 2015. Rafii, Z., Liutkus, A., Stoter, F.-R., Mimilakis, S. I., and Bittner, R. Musdb18-hq - an uncompressed version of musdb18, August 2019. URL https://doi.org/ 10.5281/zenodo.3338373. Shi, B., Tjandra, A., Hoffman, J., Wang, H., Wu, Y.-C., Gao, L., Richter, J., Le, M., Vyas, A., Chen, S., et al. Sam audio: Segment anything in audio. arXiv preprint arXiv:2512.18099, 2025. Taal, C. H., Hendriks, R. C., Heusdens, R., and Jensen, J. short-time objective intelligibility measure for timefrequency weighted noisy speech. In 2010 IEEE international conference on acoustics, speech and signal processing, pp. 42144217. IEEE, 2010. Tian, Y., Shi, J., Li, B., Duan, Z., and Xu, C. Audio-visual In ECCV, event localization in unconstrained videos. 2018. 10 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Zhao, H., Gan, C., Ma, W.-C., and Torralba, A. The sound of motions. In Proceedings of the IEEE International Conference on Computer Vision, pp. 17351744, 2019. Uhlich, S., Fabbro, G., Hirano, M., Takahashi, S., Wichern, G., Le Roux, J., Chakraborty, D., Mohanty, S., Li, K., Luo, Y., et al. The sound demixing challenge 2023 cinematic demixing track. Transactions of the International Society for Music Information Retrieval, 7(1), 2024. Vincent, E., Gribonval, R., and Fevotte, C. Performance IEEE measurement in blind audio source separation. Transactions on Audio, Speech, and Language Processing, 14(4):14621469, 2006. Wang, H., Hai, J., Lu, Y.-J., Thakkar, K., Elhilali, M., and Dehak, N. Soloaudio: Target sound extraction with language-oriented audio diffusion transformer. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2025. Wisdom, S., Erdogan, H., Ellis, D. P., Serizel, R., Turpault, N., Fonseca, E., Salamon, J., Seetharaman, P., and Hershey, J. R. Whats all the fuss about free universal sound separation data? In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 186190. IEEE, 2021. Xu, J., Guo, Z., Hu, H., Chu, Y., Wang, X., He, J., Wang, Y., Shi, X., He, T., Zhu, X., Lv, Y., Wang, Y., Guo, D., Wang, H., Ma, L., Zhang, P., Zhang, X., Hao, H., Guo, Z., Yang, B., Zhang, B., Ma, Z., Wei, X., Bai, S., Chen, K., Liu, X., Wang, P., Yang, M., Liu, D., Ren, X., Zheng, B., Men, R., Zhou, F., Yu, B., Yang, J., Yu, L., Zhou, J., and Lin, J. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025. Yan, C., Jin, C., Huang, D., Yu, H., Peng, H., Zhan, H., Gao, J., Peng, J., Chen, J., Zhou, J., et al. Minguniaudio: Speech llm for joint understanding, generation and editing with unified representation. arXiv preprint arXiv:2511.05516, 2025. Yao, Z., Guo, L., Yang, X., Kang, W., Kuang, F., Yang, Y., Jin, Z., Lin, L., and Povey, D. Zipformer: faster and better encoder for automatic speech recognition. In The Twelfth International Conference on Learning Representations, 2024. Yuan, Y., Liu, X., Liu, H., Plumbley, M. D., and Wang, W. Flowsep: Language-queried sound separation with rectified flow matching. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2025. Zhang, D., Wang, G., Xue, J., Fang, K., Zhao, L., Ma, R., Ren, S., Liu, S., Guo, T., Zhuang, W., et al. Mimo-audio: Audio language models are few-shot learners. arXiv preprint arXiv:2512.23808, 2025. 11 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation A. Detailed Taxonomy Refinement for AudioSet Labels In this section, we introduce the principles and process underlying our systematic reconstruction of the AudioSet label hierarchy (Gemmeke et al., 2017). While AudioSet covers comprehensive range of concepts from specific sound sources to abstract acoustic attributes, this breadth introduces semantic ambiguity and discriminative conflicts in the query-based USS task. Based on the characteristics of source separation, we proposed three-fold label refinement strategy to construct semantically compact, hierarchically clear, and separable-source-oriented taxonomy. Furthermore, five professional audio practitioners were recruited to cross-validate this process. Merging and normalization of synonyms. The AudioSet contains pairs of labels that are semantically equivalent but expressed differently, such as Applause and Clapping, Bow-wow and Bark, Meow and Cat, Moo and Cattle, and Oink and Pig. The presence of these synonyms increases the dimensionality of the label space and causes implicit dispersion of training data. For example, Applause and Clapping correspond to the same acoustic event, yet the model must learn two essentially identical concepts. To eliminate this redundancy, we merged semantically equivalent label pairs and adopted the more representative or acoustically essential label as the standard form. For instance, we retained Clapping while merging Applause, retained Bark while merging Bow-wow, retained Cat while merging Meow and Roar (for felines), retained Cattle while merging Moo, and retained Boat while merging Ship. Notably, some synonym merged involve cross-level semantic consolidation; for example, Ringtone was merged into Telephone bell ringing, Train horn and Train whistle were merged into Train, and Toot was merged into Vehicle horn/Honking. Hierarchical aggregation of fine-grained categories. We observed numerous overly fine-grained leaf nodes in AudioSet with minimal differences in acoustic texture, such as Accelerating, revving, vroom vs. Engine, Acoustic guitar vs. Guitar, and Alto saxophone vs. Saxophone. Although semantically distinct, these labels often exhibit highly overlapping distributions in the spectral feature space, making it difficult for classifiers to establish stable decision boundaries. Moreover, in source separation scenarios, users typically focus on the broad category of the source rather than subtle subtype differences; for instance, separating guitar sound is more practical than distinguishing between acoustic guitar and electric guitar. Consequently, we unified these fine-grained labels into their parent nodes or semantic superordinates. For example, Baby cry, infant cry was merged into Crying, sobbing; Bassoon was merged into Wind instrument, woodwind instrument; and Bugle, Cornet, and French horn are consolidated into Brass instrument. We adhered to the original AudioSet hierarchy during aggregation to maintain semantic integrity and logical consistency. Systematic exclusion of abstract acoustic attributes. Unlike traditional audio event classification, the core objective of query-based USS is to extract sound source entities with specific physical origins from mixed signals rather than describing global attributes or environmental features. AudioSet contains many abstract labels describing physical or perceptual attributes, spatial or acoustic environments, technical media, or functional roles, such as Inside, small room, Outside, rural or natural, Reverberation, Echo, Background music, Soundtrack music, Field recording, and MP3. While valuable for scene understanding or retrieval, these labels cause conceptual confusion in source separation as they often correspond to background noise, reverberation effects, or recording conditions rather than independent foreground events. For instance, Inside, small room describes spatial attributes, Reverberation characterizes wave propagation, and Background music indicates functional role rather than acoustic content. These traits provide no direct aid in determining which sources are separable. Therefore, we systematically excluded all such abstract attribute labels, ensuring the retained set includes only physically explicit, independently active, and separable source categories. Specifically, we removed spatial environment labels (e.g., Inside, large room or hall), acoustic attribute labels (e.g., Bass (frequency range), Echo), technical media labels (e.g., MP3, Television), functional role labels (e.g., Background music), and perceptual attribute labels (e.g., Dance music, Tinnitus). 12 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Table A1. Summary of non-retained AudioSet leaf nodes grouped by exclusion criteria. The arrow () denotes fine-grained categories aggregated into their parent concepts; red strikethrough indicates abstract acoustic attributes excluded from the taxonomy; and the equivalence symbol () represents synonym merging operations. Group / Labels"
        },
        {
            "title": "Synonym merge",
            "content": "Applause Clapping; Bow-wow Bark; Caw Crow; Clunk Thunk; Coo Pigeon, dove; Ding-dong Doorbell; Harmony Chord; Hoot Owl; Honk Goose; Meow Cat; Moo Cattle; Oink Pig; Ringtone Telephone bell ringing; Roar Cat; Rock and roll Rock music; Ship Boat; Tick-tock Clock; Tick Clock; Thunk Thump, thud; Toot Vehicle horn/Honking; Train horn Train; Train whistle Train;Yell Shout Fine-grained aggregation Accelerating, revving, vroom Engine; Acoustic guitar Guitar; Afrobeat Music of Africa; Alto saxophone Saxophone; Angry music Music mood; Artillery fire Gunshot, gunfire; Babbling Speech; Baby cry, infant cry Crying, sobbing; Baby laughter Laughter; Bass drum Drum; Bass guitar Guitar; Bassline Musical concepts; Bassoon Wind instrument, woodwind instrument; Battle cry Shout; Bay Dog; Bellow Shout; Belly laugh Laughter; Bird flight, flapping wings Bird; Birthday music Music role; Blare Onomatopoeia; Bleat Goat; Bluegrass Country; Booing Human group actions; Bugle Brass instrument; Busy signal Telephone; Cacophony Noise; Cap gun Gunshot, gunfire; Caterwaul Cat; Cellphone buzz, vibrating alert Telephone; Chainsaw Light engine (high frequency); Cheering Human group actions; Carnatic music Music of Asia; Chipmunk Rodents, rats, mice; Chink, clink Glass; Children shouting Human group actions; Chirp tone Sine wave; Chorus effect Effects unit; Chord Musical concepts; Clavinet Electric piano; Chuckle, chortle Laughter; Clip-clop Clicking; Cluck Chicken, rooster; Compact disc Sound reproduction; Christmas music Music role; Clickety-clack Clicking; Crash cymbal Cymbal; Cornet Brass instrument; Computer keyboard Typing; Croak Frog; Cumbia Music of Latin America; Crowing, cock-a-doodle-doo Chicken, rooster; Dental drill, dentists drill Drill; Dial tone Telephone; Drone music Ambient music; Drum roll Snare drum; Drum beat Beat; Dub Reggae; Drone Musical concepts; Drum machine Drum kit; Dubstep Electronic music; Distortion Background noise; Electric guitar Guitar; Duck call (hunting tool) Miscellaneous sources; Electric toothbrush Toothbrush; Electronic dance music Electronic music; Electro Electronic music; Electronica Electronic music; Electronic organ Organ; Electronic tuner Sound equipment; Engine knocking Engine; Engine starting Engine; Exciting music Music mood; Firecracker Fireworks; French horn Brass instrument; Funk carioca Music of Latin America; Flamenco Music of Latin America; Gasp Breathing; Giggle Laughter; Fusillade Gunshot, gunfire; Funny music Music mood; Glockenspiel Mallet percussion; Gobble Turkey; Gospel music Christian music; Fizz Onomatopoeia; Grunge Rock music; Gramophone record Sound reproduction; Guitar amplifier Sound equipment; Hammond organ Organ; Gull, seagull Bird; Gush Pour; Grind Surface contact; Growling Canidae, dogs, wolves; Heart murmur Heart sounds, heartbeat; Heavy engine (low frequency) Engine; Heavy metal Rock music; Hi-hat Cymbal; Harmonic Sine wave; Happy music Music mood; Headphones Sound reproduction; House music Electronic music; Howl (wind) Wind; Idling Engine; Infrasound Other sourceless; Howl Canidae, dogs, wolves; Jet engine Aircraft engine; Kwaito Music of Africa; Jingle (music) Music role; Kuduro Music of Latin America; Lullaby Music role; Lawn mower Light engine (high frequency); Machine gun Gunshot, gunfire; Loudspeaker Sound reproduction; Loop Musical concepts; Mantra Chant; Mains hum Background noise; Maraca Rattle (instrument); Medium engine (mid frequency) Engine; Marimba, xylophone Mallet percussion; Mellotron Synthesizer; Motorboat, speedboat Boat, Water vehicle; Melody Musical concepts; Mouse Rodents, rats, mice; Music of Bollywood Music of Asia; Musical note Musical concepts; Narration, monologue Speech; Nicker Horse; Neigh, whinny Horse; Oldschool jungle Drum and bass; Noise music Electronic music; Musical ensemble Musical instrument; Opera Classical music; Pant Breathing; Pizzicato Violin, fiddle; Pink noise Noise; Packing tape, duct tape Domestic sounds, home sounds; Psychedelic rock Rock music; Propeller, airscrew Aircraft engine; Patter Rodents, rats, mice; Progressive rock Rock music; Pulse Other sourceless; Quack Duck; Purr Cat; Punk rock Rock music; Continued on next page... 13 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Puff Onomatopoeia; Raindrop Rain; Rhodes piano Electric piano; Rimshot Snare drum; Radio Sound reproduction; Ringing (of resonator) Other sourceless; Rowboat, canoe, kayak Boat, Water vehicle; Rustling leaves Wind; Sailboat, sailing ship Boat, Water vehicle; Sad music Music mood; Salsa music Music of Latin America; Scary music Music mood; Sampler Synthesizer; Scratching (performance technique) Musical instrument; Single-lens reflex camera Camera; Sidetone Background noise; Shatter Glass; Snicker Laughter; Snort Breathing; Snoring Breathing; Soca music Music of Latin America; Snort (horse) Horse; Sonic boom Boom; Screech Brief tone; Soprano saxophone Saxophone; Squawk Bird vocalization, bird call, bird song; Speech synthesizer Speech; Steel guitar, slide guitar Guitar; Steelpan Mallet percussion; Static Background noise; Tabla Drum; Strum Guitar; Tape hiss Background noise; Techno Electronic music; Tapping (guitar technique) Guitar; Telephone dialing, DTMF Telephone; Tender music Music mood; Telephone bell ringing Telephone; Throat clearing Cough; Thunder Thunderstorm; Throbbing Noise; Theme music Music role; Timpani Drum; Trance music Electronic music; Trickle, dribble Pour; Trombone Brass instrument; Trumpet Brass instrument; Twang Brief tone; Typewriter Typing; Vibraphone Mallet percussion; UK garage Electronic music; Velcro, hook and loop fastener Domestic sounds, home sounds; Video game music Music role; Whimper Crying, sobbing; Whimper (dog) Dog; Wheeze Breathing; Vibration Noise; Whoop Shout; Wind chime Chime; Wolf-whistling Whistling; Yak Cattle, bovinae; Yawn Human voice; Wedding music Music role; Yip Dog; Zing Onomatopoeia"
        },
        {
            "title": "Abstract acoustic attribute",
            "content": "Bass (frequency range); Cat communication; Background music; Field recording; Children playing; Environmental noise; Inside, public space; Inside, small room; MP3; Outside, rural or natural; Outside, urban or manmade; Rain on surface; Reverberation; Sound effect; Soundtrack music; Song; Television; Traditional music; Swing music; Tinnitus, ringing in the ears; Wind noise (microphone); White noise Inside, large room or hall; Bass (instrument role); Conversation; Dance music; Chatter; Creak; Crowd; Echo; B. Prompts for Semantic-Acoustic Alignment In this section, we provide the detailed prompts utilized by Qwen3-Omni4 for the semantic-acoustic alignment framework described in Section 3.2. To enhance the robustness of the automated annotation and mitigate hallucination, we employed majority voting strategy during inference. Specifically, we set the generation temperature to 1.0 to encourage diversity and performed = 10 independent generation passes for each audio sample. The final label was determined by selecting the most frequent prediction among the ten outputs. B.1. Task I: Instance-Level Purification To filter out samples containing background noise, transient interference, or multi-event mixtures, we instructed Qwen3Omni to act as an audio quality auditor. The model is required to explicitly identify whether the audio clip consists of single, distinct acoustic event. The prompt design is presented in Figure A1. B.2. Task II: Coarse-to-Fine Hierarchical Relabeling For samples that passed the purification stage, we employed coarse-to-fine strategy. Leveraging the coarse category predicted by the ZipFormer-based AudioTag model (e.g., Domestic animals), we constrained the search space for Qwen3Omni to identify the specific leaf node in the ontology. This prompt injects the coarse prediction as semantic prior. The prompt formulation is detailed in Figure A2. B.3. Co-occurrence Validation Prompt We provide the prompt template used to query Qwen3-Omni for judging whether two audio events can naturally co-occur in real-world recordings. Figure A3 shows the full prompt. 14 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Figure A1. Prompt template used for Instance-level purification. Table A2. Summary of the twelve source datasets integrated into Hive. The collection spans diverse categories including general sound events, music, speech, and environmental soundscapes, all utilized under compliant academic or creative commons licenses. Dataset Primary Type License Type # Clips Duration (h) BBC Sound Effects (British Broadcasting Corporation, 1991) AudioSet (Gemmeke et al., 2017) VGGSound (Chen et al., 2020) MUSIC21 (Zhao et al., 2019) FreeSound (Fonseca et al., 2017) ClothoV2 (Drossos et al., 2020) Voicebank-DEMAND (Botinhao et al., 2016) AVE (Tian et al., 2018) SoundBible DCASE (Mesaros et al., 2019) ESC50 (Piczak, 2015) FSD50K (Fonseca et al., 2021) Sound Effects / Ambience Remix License (NC Use) General Audio Events General / Real-world Musical Instruments Diverse Crowdsourced Soundscapes / Captioning Clean Speech Audio-Visual Events Short Sound Effects Acoustic Scenes Environmental Sounds General Audio Events CC BY CC BY 4.0 YouTube Standard CC0 / CC BY / CC BY-NC Non-Commercial Research CC BY 4.0 CC BY-NC-SA CC BY 4.0 Academic Use CC BY-NC 3.0 Creative Commons Total Aggregated Heterogeneous Sources 369,603 326,890 115,191 32,701 17,451 14,759 12,376 3,054 2,501 1,969 1,433 636 898,564 1,020.62 896.61 319.10 90.28 46.90 38.19 9.94 6.91 5.78 5.46 1.99 0.80 2,442. C. Source Datasets Details D. Dataset Integration Details To achieve broad coverage of real acoustic conditions and effectively capture rare events in the long-tailed distribution, we integrated high-quality audio resources from 12 public datasets, as shown in Table A2. All raw data were standardized and cleaned using our automated pipeline to remove silent segments and ensure label semantic consistency. The resulting pool contained 898, 564 independent audio files with total duration of 2, 442.60 hours. Throughout the integration process, we strictly adhered to the license terms of each source dataset and utilized data solely within permitted academic or licensed scopes. As the foundation of our collection, BBC Sound Effects (British Broadcasting Corporation, 1991) provides professional sound effects library with extensive coverage. It mainly includes natural ambience, complex mechanical operations, and background sounds from daily environments, featuring broadcast-level recording fidelity. Under its noncommercial education and research license (Remix License), we selected 369,603 high-quality clips totaling 1,020.62 hours, which 4https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Figure A2. Prompt template used for hierarchical relabeling (leaf node selection). The variable {leaf labels} is dynamically populated with the list of candidate leaf nodes derived from the coarse category (e.g., [\"Dog\", \"Cat\", \"Horse\"]). The model outputs an integer index or -1. provide diverse acoustic event prototypes. AudioSet (Gemmeke et al., 2017)5 is widely used large-scale benchmark for audio event classification, largely composed of audio tracks from YouTube videos under the Creative Commons license (CC BY). Despite label noise in the raw data, its category coverage is essential for open-domain separation. After extensive cleaning, we utilized 326,890 clips with total duration of 896.61 hours, forming another core pillar of our training data. To increase the coverage of natural recordings, we included VGGSound (Chen et al., 2020)6, which contains large set of real-world audio sequences following the CC BY 4.0 license. We extracted 115,191 valid clips totaling 319.10 hours. For music, MUSIC21 (Zhao et al., 2019)7 provides key samples of solo and ensemble instruments. It follows YouTube copyright constraints and contains tracks that help the model learn complex harmonic structures. We incorporated 32,701 clips with total duration of 90.28 hours to better distinguish music signals from broadband noise. We also leveraged crowdsourced audio from FreeSound (Fonseca et al., 2017)8, uploaded by users worldwide and exhibiting substantial diversity in recording devices. After filtering files under CC0, CC BY, and CC BY-NC licenses, we retained 17,451 clips totaling 46.90 hours. This device heterogeneity was crucial for reducing overfitting to specific channel characteristics. For long-form audio description and complex soundscapes, ClothoV2 (Drossos et al., 2020)9 provides samples with rich temporal evolution. It is released under noncommercial academic research license and is primarily used for audio captioning. After segmentation, we retained 14,759 clips totaling 38.19 hours. For speech processing, Voicebank DEMAND (Botinhao et al., 2016)10 offers clean speech signals and is standard dataset for speech enhancement. It follows CC BY 4.0 and contributed 12,376 clips totaling 9.94 hours, which were used to improve vocal separation clarity. For audio-visual event localization, AVE (Audio Visual Event) (Tian et al., 2018)11 provides event samples with clear temporal boundaries. It is also based on YouTube and follows the CC BY-NC-SA license. After processing, the subset included 3,054 clips totaling 6.91 hours, supporting the modeling of transient event characteristics. To supplement short sound effects for specific categories, we used SoundBible 12, which contains curated short audio clips and is primarily licensed under CC BY 4.0. Although smaller in scale, we acquired 2,501 clips totaling 5.78 hours; its high 5https://huggingface.co/datasets/agkphysics/AudioSet 6https://huggingface.co/datasets/Loie/VGGSound 7https://github.com/roudimit/MUSIC_dataset 8https://huggingface.co/datasets/Meranti/CLAP_freesound 9https://zenodo.org/records/4783391 10https://huggingface.co/datasets/JacobLinCool/VoiceBank-DEMAND-16k 11https://drive.google.com/open?id=1FjKwe79e0u96vdjIVwfRQ1V6SoDHe7kK 12https://huggingface.co/datasets/nyuuzyou/soundbible 16 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Figure A3. Prompt template used for evaluating the co-occurrence of audio events. signal-to-noise ratio offered clear supervision for rare events. For synthetic data, DCASE (Mesaros et al., 2019)13, derived from acoustic scene detection challenges, provides synthetic and small amount of real recordings. We employed 1,969 clips totaling 5.46 hours as complementary validation beyond real data. As high-precision benchmark for environmental sound classification, ESC50 (Piczak, 2015)14 is small manually curated dataset with highly accurate labels. It follows CC BY-NC 3.0 and yielded 1,433 clips totaling 1.99 hours, mainly for validating classification accuracy on standard environmental sound categories. Finally, as high-quality subset related to FreeSound, FSD50K (Fonseca et al., 2021)15 provides finely annotated data based on the AudioSet ontology. We selected 636 additional clips totaling 0.80 hours to fill gaps in the long-tailed distribution. By integrating these twelve sources with compliant licensing and complementary characteristics, we constructed long-tailed acoustic space that supports training for universal sound separation. E. Full Class Frequency Statistics of the Hive Dataset In this section, we provide detailed characterization of the scale and semantic distribution of the Hive dataset, complementing the main paper discussion on data diversity. The resulting Hive dataset serves as large-scale benchmark with high semantic consistency. It comprises 19.6 million synthetic mixed audio samples, organized under standard split into training, validation, and test sets, and covering highly concurrent mixtures from two-source to five-source settings. The dataset is built upon 283 fine-grained semantic classes, which ensures broad acoustic coverage while more faithfully reflecting the inherent long-tailed distribution observed in natural and man-made environments. Our distribution analysis reveals pronounced class imbalance, which is visually evident in the full statistics shown in Figure A4. Among the 283 classes, head classes are dominated by frequently occurring natural sounds and human activities, such as Bird (approximately 3.115 million instances), Male speech (approximately 2.205 million instances), and transportation-related sounds such as Boat (approximately 1.523 million instances). These classes establish the primary supervision signal for model training. Simultaneously, the dataset preserves challenging long-tailed characteristics, with extremely sparse tail classes, including rare instrument sounds such as Oboe (only 10 instances), natural disaster sounds 13https://zenodo.org/records/10886481 14https://huggingface.co/datasets/ashraq/esc50 15https://huggingface.co/datasets/Fhrozen/FSD50k 17 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Figure A4. Complete frequency distribution and index mapping for the 283 labels in Hive. The top panel shows the rank frequency curve after sorting classes by decreasing frequency. The lower right inset zooms into the tail region (the last 20 classes) to highlight extremely low frequency categories. The bottom panel provides the full ranklabel index table, where each class frequency is visually encoded by log normalized background bars. such as Wildfire (12 instances), and specific mechanical sound effects such as Pulleys (16 instances). This vast span from millions to tens of samples, covering six orders of magnitude, not only captures the complexity of open-world auditory scenes but also provides strict and discriminative benchmark for evaluating the robustness of audio separation models under long-tailed distributions and their generalization to few-shot categories. F. Details of Out-of-Domain Datasets To comprehensively evaluate the zero-shot generalization ability and robustness of AudioSep and FlowSep without finetuning, we employed two challenging out-of-domain test benchmarks: MUSDB18-HQ (Rafii et al., 2019) and USS-Bench16. These datasets differ substantially from the Hive training set in acoustic characteristics and semantic distributions, enabling an effective assessment of practical performance on high-fidelity music separation and complex mixtures. MUSDB18-HQ is widely used benchmark for music source separation. It contains 150 full-length songs across multiple 16https://mab.to/GsSVrdZK0dfQ1/us 18 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Figure A5. Demographic distribution of participants. (a) Pie chart of gender ratio (male/female). (b) Histogram of age distribution, with the mean age indicated by dashed line. Figure A6. Annotation interface for the screening phase (A) and the main study phase (B). genres, with total duration of approximately 10 hours, and is split into 100 training songs and 50 test songs. All audio is stereo at 44.1 kHz. Unlike MUSDB18, which uses AAC compression with bandwidth limited to 16 kHz, MUSDB18-HQ provides uncompressed WAV files that preserve the full bandwidth up to 22 kHz, which is crucial for models that aim to recover high-frequency details. Each song includes mixture and four isolated stems, namely drums, bass, vocals, and others. The tracks are collected from multiple high-quality sources, including DSD100, MedleyDB, the Native Instruments stems pack, and The Easton Ellises, and each mixture is the linear sum of its stems. For evaluation, we follow the standard protocol and use the museval toolkit to compute metrics such as the Signal-to-Distortion Ratio (SDR), which quantifies separation accuracy in complex music scenarios. USS-Bench (Universal Sound Separation Benchmark) provides more challenging wide-domain sound separation setting, primarily focusing on mixtures of speech and instruments. It combines Mandarin speech from AIShell-1 with instrument audio from the MUSIC dataset, forming mixture tasks with four sources where speech and instruments are randomly combined. USS-Bench includes both simulation data and real-world recordings, which are designed to test model adaptability under idealized and realistic acoustic conditions. The real-world recordings are collected in single room using microphone array with 2.8 cm spacing, where the distance between M1 and M4 is 8.4 cm, producing two-channel stereo signals and imposing stronger spatial resolving requirements on the model. We strictly follow the official validation set configuration with 297 valid samples and directly evaluate transfer from Hive without any additional fine-tuning, focusing on dense 19 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation four-source mixtures and cross-microphone array signals. G. Details of Baselines and Evaluation Metrics G.1. Baseline Methods We evaluate set of recent methods on the Hive test set. These baselines fall into two categories, discriminative models and generative models. The former typically separates sources by estimating masks or filters, while the latter synthesizes the target signal based on learned priors. LASS-Net (Liu et al., 2022) is an early attempt for language queried universal audio source separation. It adopts querybased separation framework consisting of query network and separation network. Specifically, it uses BERT to extract semantic embeddings from natural language descriptions and employs ResUNet-based architecture to predict spectrogram masks conditioned on these embeddings. By jointly modeling acoustic and language information in an end-to-end manner, LASS-Net can separate target source given arbitrary text queries, beyond predefined label set. CLIPSep (Dong et al., 2023) addresses data scarcity in text-queried separation by leveraging the shared embedding space of contrastive language image pretraining. Unlike supervised methods that require paired audio-text annotations, CLIPSep is trained on unlabeled noisy videos using visual queries extracted from video frames. At inference time, due to modality alignment in CLIP, it can transfer to text-based queries in zero-shot manner. It uses query vector modulation mechanism, where image or text embeddings modulate an audio separation network to bridge visual and auditory modalities for universal sound separation. AudioSep (Liu et al., 2024) is foundation model for open-domain sound separation. It expands the training paradigm by leveraging large-scale multimodal datasets and integrating contrastive language-audio pretraining model as the text encoder. AudioSep adopts frequency domain ResUNet separation backbone and predicts magnitude masks and phase residuals conditioned on CLAP text embeddings. Training on diverse datasets provides strong zero-shot generalization, enabling separation of wide range of audio events, instruments, and speech described in natural language. OmniSep (Cheng et al., 2025) proposes unified all-modality sound separation framework that supports text, audio, and visual queries. Its key design is Query Mixup training strategy that mixes query features from different modalities during training to encourage unified representation space. It uses ImageBind to extract cross-modality aligned features and introduces negative query mechanism to suppress undesired interference. This design enables joint optimization across modalities and robust separation performance regardless of the query modality at inference time. ZETA (Manor & Michaeli, 2024) presents zero-shot text-based method for audio editing and separation without taskspecific training. It leverages generative priors from pretrained audio diffusion models such as AudioLDM. ZETA first applies DDPM inversion to map an input mixture into the latent noise space, and then performs guided denoising conditioned on the target text description. This approach repurposes generative diffusion models for discriminative separation, enabling extraction or manipulation of specific sources using text prompts. FlowSep (Yuan et al., 2025) is generative method based on rectified flow matching. Unlike diffusion models that simulate stochastic denoising trajectories, FlowSep constructs linear trajectories between prior noise distribution and the data distribution in the latent space of variational autoencoder. It uses FLAN T5 encoder for text queries and conditions flow matching network to generate latent features of the target source. By learning deterministic flow trajectories, FlowSep achieves high-quality separation and improves theoretical properties and inference efficiency compared with standard diffusion baselines. ZeroSep (Huang et al., 2025) investigates training-free separation using pretrained text-guided audio diffusion models without fine-tuning. It relies on two generative inference steps, latent inversion and text conditional denoising. The mixture is first inverted into the latent space to capture composite acoustic information, and then denoising guided by source-specific text prompt reconstructs the isolated signal. ZeroSep shows that strong generative priors can enable effective source separation in open-set scenarios, challenging the assumption that supervised training is necessary. DGMO (Lee et al., 2025b), diffusion guided mask optimization, proposes test-time optimization framework that repurposes pretrained diffusion models for zero-shot separation. Instead of directly generating outputs, DGMO optimizes learnable spectrogram mask to ensure accurate temporal alignment with the input mixture, guided by the diffusion score function. This hybrid design mitigates phase inconsistency and hallucinated artifacts that are common in purely generative separation, 20 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation improving signal fidelity. SAM-Audio (Shi et al., 2025) is foundation model for universal audio separation that integrates text, visual, and temporal span prompts. It is built on diffusion Transformer architecture and trained via flow matching. SAM-Audio formulates separation as conditional generation and supports flexible prompting, including natural language, visual masks that segment target objects in videos, and time segments. The model operates in compressed latent space and is trained on largescale datasets covering speech, music, and environmental sounds. By effectively fusing multimodal cues, it achieves state-of-the-art source separation performance. G.2. Evaluation Metrics Our evaluation protocol assesses separated audio from three complementary perspectives: signal fidelity, perceptual quality, and semantic alignment. To measure physical accuracy, we use standard objective metrics, including Source-to-Distortion Ratio (SDR) (Vincent et al., 2006)17 and Scale-Invariant Source-to-Distortion Ratio (SI-SDR) (Le Roux et al., 2019)18. We emphasize SI-SDR since it provides robust measure that is insensitive to the scale of the prediction. We also report Short-Time Objective Intelligibility (STOI) (Taal et al., 2010)19 to evaluate speech intelligibility when applicable. However, these reference-based metrics require strict waveform-level alignment and have inherent limitations for generative methods. Since generative models often reconstruct perceptually plausible high-frequency details that may not match the ground truth in phase or fine waveform structure, such metrics can underestimate perceptual quality. Therefore, we do not report these metrics for generative methods in this paper. Because generative models may synthesize plausible high-frequency components that are not sample-aligned with the reference waveform, conventional signal metrics may not accurately reflect perceptual quality. We thus employ Frechet Audio Distance (FAD) (Gui et al., 2024)20 and Learned Perceptual Audio Patch Similarity (LPAPS) (Manor & Michaeli, 2024)21. FAD measures the distribution-level distance between generated audio and reference set, capturing realism and the absence of artifacts, while LPAPS quantifies perceptual similarity in deep feature space. To verify semantic consistency between separated audio and the conditioning query, we use the CLAP score (Huang et al., 2023)22. We compute cosine similarity between the separated audio and the ground truth audio (CLAP Audio) to measure content preservation, and between the separated audio and the text description (CLAP Text) to measure query adherence. Given the limitations of reference-based metrics for open-ended generation, we further adopt large language model-based reference-free metric, SAM-Audio Judge (Shi et al., 2025)23. It reports overall quality (OQ), recall (Rec), precision (Pre), and faithfulness (Fai). Compared with purely signal-based measures, it correlates better with human preference and can more finely evaluate the extent to which separated audio matches user intent while maintaining high audio quality. H. Details of 4-AFC Evaluation H.1. Evaluation Setup We operationally defined semantic alignment as the consistency between the label predicted by model and the acoustic event that actually occurred in the audio clip. The semantic judgment of the audio event was established by human consensus under four-alternative forced-choice (4-AFC) protocol. Specifically, each trial contained 10-second audio clip and four candidate event labels, and the annotator (human or model) was required to select exactly one label. The evaluation set consisted of 20 audio clips; thus, the expected accuracy of random guessing was 25%. To ensure the representativeness of this compact test set within our extensive ontology, we employed stratified sampling strategy based on class frequency. Specifically, the 20 clips were carefully curated to span the entire distribution spectrum, comprising samples from head 17https://lightning.ai/docs/torchmetrics/stable/audio/signal_distortion_ratio.html 18https://lightning.ai/docs/torchmetrics/stable/audio/scale_invariant_signal_distortion_ ratio.html 19https://github.com/mpariente/pystoi 20https://github.com/HilaManor/AudioEditingCode/blob/codeclean/evals/fadtk_utils.py 21https://github.com/HilaManor/AudioEditingCode/blob/codeclean/evals/lpaps.py 22https://github.com/HilaManor/AudioEditingCode/blob/codeclean/evals/meta_clap_ consistency.py 23https://huggingface.co/facebook/sam-audio-judge 21 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation (high-frequency), body, and tail (rare) categories. This design allowed the small-scale set to serve as an effective proxy for evaluating model robustness across the datasets inherent long-tailed imbalance. 1 , 2 , To reduce obvious cues and control task difficulty, each trial contained one target label and three distractor labels, where distractors were sampled from the same ontology level as the target. Concretely, for clip with target label y, we sampled distractors {y 3 } from the label set that shared the same ontology depth as y, excluding itself. We also balanced candidate construction to mitigate frequency bias by monitoring distractor usage counts across the full evaluation set and prioritizing labels that were under-represented. The four candidates were then randomly ordered and presented in the same manner to all evaluators, including models. This yielded shared retrieval-style multiple-choice setup for all humans, our pipeline, and all LALM baselines. H.2. Human Study We recruited 38 participants (mean age 30.4; age and gender distributions are shown in Figure A5) and conducted the study online. To control experimental conditions, participants were required to wear headphones throughout the study and were allowed to replay audio during annotation. To filter unreliable annotations, we included screening stage before the formal evaluation. Only participants with at least 60% accuracy on the screening set proceeded to the formal evaluation. An example annotation interface is shown in Figure A6. All participants provided informed consent and received compensation. In the formal evaluation, each audio clip was independently annotated by all 38 participants. We reported two metrics: (i) mean human accuracy, computed by averaging each participants accuracy over all clips and then averaging across participants; (ii) clip-level consensus label, obtained by majority voting over annotations for the same clip. To quantify agreement among evaluators, we computed Fleiss Îº over the 20 trials, where the four categories corresponded to the four candidate options in each trial. We obtained Fleiss Îº = 0.863, indicating high agreement. H.3. LALM Baseline Setup We evaluated three recent LALM baselines: MiDashengLM (Dinkel et al., 2025), MiMo-Audio-7B (Zhang et al., 2025), and Qwen3-Omni (Xu et al., 2025). For each model, we provided the same input as in the human task, namely an audio clip and four candidate labels. For comparability, we imposed the same constraints on all baselines: the model was required to output exactly one option among the four, and it was not allowed to generate any explanatory text. As shown in Figure A7, all models used the same prompt template and performed inference with their default decoding strategy and hyperparameters on all trials. Figure A7. Prompt template and example trial for LALM evaluation. The model receives an audio clip and multiple-choice question with four candidate labels, and must respond with single letter (A/B/C/D). 22 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation H.4. Result Analysis For our pipeline and each LALM baseline, we define accuracy as the fraction of clips for which the selected option matches the target label. For human annotators, we report the mean individual accuracy over 38 participants, computed by first measuring each participants accuracy over all clips and then averaging across participants. We estimate confidence intervals using clip-level bootstrap resampling. We resample the 20 clips with replacement and recompute accuracy (and paired differences) on each resampled set, then report percentile-based 95% confidence intervals. To test whether our pipeline differs significantly from average human performance, we conduct two-sided paired t-test based on clip-level paired outcomes. For each clip {1, . . . , 20}, let our pipeline correctness be bi {0, 1}, where bi = 1 if our pipeline selects the target label and bi = 0 otherwise. Human performance on clip is represented as the clip-level accuracy hi = 1 38 38 (cid:88) j=1 I[yij = Ëyi], (1) where yij denotes the choice of the j-th annotator on clip i, and Ëyi is the target label. We further define the paired difference and perform two-sided paired t-test on {di}20 i=1 to test the null hypothesis E[di] = 0. The test yields = 0.033. di = bi hi, (2) H.5. Analysis of Failure Cases To provide granular assessment of model reliability, we analyzed the specific error distribution of our proposed pipeline relative to human consensus. The pipeline demonstrated exceptional robustness, yielding single prediction discrepancy across the 20 evaluation trials, corresponding to an error rate of 5.0%. rigorous inspection revealed that this solitary error occurred on Sample #19, the most ambiguous instance in the test set, where the human ground truth was highly contested. Specifically, the model predicted Boat for sample labeled Rain, yet human consensus for this sample was notably low at 48.65% (N = 18), representing significant deviation from the 91.89% average consensus observed across the remainder of the dataset. The human voting pattern exhibited near-bimodal distribution, with 18 participants voting for Rain and 17 (45.95%) for Boat, marginal difference of single vote. Acoustically, the sample is characterized by continuous broadband water textures, features inherent to both heavy precipitation and the hydrodynamic displacement of boat hull. Consequently, the models prediction aligned with the strong secondary human preference, effectively distinguishing the acoustic event from unrelated distractors. In contrast, baseline models exhibited failures on high-confidence samples where human agreement reached 98.65%, highlighting vulnerability in their zero-shot semantic mapping capabilities. Thus, our approach faltered only on sample with distinct acoustic ambiguity, whereas baselines failed on unambiguous instances, underscoring the precision and high-fidelity synthesis capabilities of the proposed framework. I. Details of Zero-Shot Results Table A3 provides comprehensive breakdown of zero-shot separation performance across varying acoustic densities, ranging from standard two-source mixtures to highly complex five-source scenarios. This granular evaluation exposes specific failure modes in existing methods when facing acoustically dense environments driven solely by text labels. clear trend observed across all baselines is the significant performance degradation as the number of concurrent sources increases. For discriminative approaches, this is most evident in the sharp decline of SDR. For example, AudioSep achieves high SDR in two-source mixtures but experiences dramatic drop to negative values in five-source settings. This sign inversion suggests that in dense acoustic environments, the model struggles to effectively suppress interference, likely due to the reliance on co-occurrence biases present in its original training data which are absent in our semantically decorrelated test set. Similarly, early methods like CLIPSep fail to yield positive SDRs even in simple scenarios, indicating fundamental inability to handle open-domain text queries without relying on contextual shortcuts found in uncurated data. Regarding generative baselines such as SAM-Audio and FlowSep, we observe distinct trade-off between perceptual plausibility and semantic fidelity. These models generally maintain lower FAD scores across all densities compared to discriminative counterparts, implying that they generate audio that sounds acoustically natural. However, their semantic adherence, measured by CLAP-Text similarity, deteriorates significantly in four-source and five-source scenarios. This 23 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation discrepancy reveals that while generative models can synthesize high-fidelity audio textures, they are prone to semantic drift or hallucination in complex scenes, often generating plausible-sounding artifacts that do not align with the specific text query. In contrast to these baselines, models trained on Hive demonstrate superior robustness in high-interference settings. As shown in Table 3, the Hive-trained AudioSep maintains positive signal fidelity even in the most challenging five-source scenarios. This resilience confirms that the proposed data construction pipeline effectively mitigates the co-occurrence noise and label-signal misalignment that typically severely limit models trained on larger but noisier in-the-wild datasets. J. Data Scaling Analysis K. Scaling Law Analysis In this section, we provide granular analysis of how model performance evolves as the volume of high-purity training data increases logarithmically from 175k to 17.5M samples. By comparing representative discriminative model (AudioSep) and generative model (FlowSep), we observe distinct scaling laws that shed light on the different data requirements for signal estimation versus conditional synthesis. We first examine the scaling dynamics of discriminative models. As detailed in Table A4, AudioSep exhibits continuous, log-linear improvement in signal fidelity metrics without saturation. The SDR increases monotonically from 4.12 dB at 175k samples to 5.67 dB at 17.5M, suggesting that the model continuously benefits from the high informational density of additional clean samples to refine its separation masks. This advantage is most pronounced in complex scenarios; in the challenging 5-mix setting, the model transitions from near-failure (0.05 dB SDR) at the smallest scale to robust separation (1.46 dB SDR) at the largest. Remarkably, even at an intermediate scale of 875k samples, AudioSep achieves an Overall SDR of 4.96 dB, surpassing the official baseline trained on millions of in-the-wild samples. This empirically validates that the purity of supervised signals is more decisive factor than raw data volume for discriminative alignment. In contrast, the generative FlowSep model reveals more complex, two-phase scaling behavior, as shown in Table A5. Distance-based perceptual metrics such as LPAPS and FAD improve rapidly in the low-data regime but saturate early, with FAD stabilizing around 0.85 by the 1.75M scale. This indicates that the model learns to synthesize acoustically plausible textures with relatively limited data. However, semantic adherence and overall quality exhibit critical mass effect. Reference-free metrics like OQ remain relatively flat between 175k and 1.75M but experience sudden surge at the 7.9M scale, rising to 3.09. Similarly, the precision score increases significantly from 3.11 to 3.39. This implies that while the model quickly learns to mimic acoustic textures, it requires significantly larger-scale data to cross the threshold for accurate semantic control and to mitigate hallucinations in complex scenes. This comparative analysis demonstrates that Hive supports efficient training for both paradigms through distinct mechanisms. For discriminative models, the dataset provides steep, continuous gradient for optimizing signal fidelity. For generative models, the scale of Hive is sufficient to cross the usability threshold, enabling models to advance beyond mere acoustic mimicry to achieve high-fidelity, semantically consistent generation. 24 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Table A3. Detailed performance breakdown on the Hive test set across eight models and four mixture complexities. Model Mix Discriminative Models Signal Fidelity Perceptual & Semantic Quality Reference-free Quality SDR SI-SDR STOI LPAPS FAD CLAP-A CLAP-T OQ Rec Pre Fai LASS-Net CLIPSep AudioSep OmniSep 2-mix 3-mix 4-mix 5-mix 2.77 -2.26 -5.22 -7.15 Overall -2.97 2-mix 3-mix 4-mix 5-mix -2.08 -5.50 -7.85 -9.37 Overall -6.20 2-mix 3-mix 4-mix 5-mix 9.55 2.05 0.80 -2.94 Overall 2.37 2-mix 3-mix 4-mix 5-mix 1.57 -2.17 -4.61 -6.18 Overall -2.85 1.24 -3.28 -6.11 -8.01 -4.04 -6.23 -8.51 -10.64 -12.13 -9.38 8.31 1.30 0.18 -3. 1.58 -0.91 -3.93 -6.17 -7.66 -4.67 Generative Models ZETA FlowSep"
        },
        {
            "title": "DGMO",
            "content": "SAM-Audio 2-mix 3-mix 4-mix 5-mix Overall 2-mix 3-mix 4-mix 5-mix Overall 2-mix 3-mix 4-mix 5-mix"
        },
        {
            "title": "Overall",
            "content": "2-mix 3-mix 4-mix 5-mix Overall 2-mix 3-mix 4-mix 5-mix"
        },
        {
            "title": "Overall",
            "content": "0.57 0.44 0.35 0.30 0.41 0.33 0.28 0.23 0.20 0. 0.66 0.48 0.46 0.35 0.49 0.47 0.37 0.30 0.25 0.35 0.56 0.45 0.37 0.32 0.43 0.52 0.44 0.39 0.36 0. 0.72 0.62 0.53 0.42 0.57 0.63 0.54 0.47 0.43 0.52 0.45 0.45 0.44 0.44 0. 0.68 0.59 0.53 0.48 0.57 0.63 0.57 0.52 0.49 0.55 0.56 0.52 0.49 0.47 0. 0.52 0.43 0.37 0.32 0.41 3.84 4.67 5.15 5.45 4.78 5.52 5.91 6.13 6.26 5. 3.14 4.26 4.42 5.04 4.22 4.27 4.87 5.19 5.39 4.93 5.37 5.43 5.45 5.46 5. 3.50 4.10 4.45 4.65 4.18 4.02 4.46 4.69 4.84 4.50 4.52 4.93 5.10 5.21 4. 4.60 5.11 5.45 5.66 5.21 0.89 1.02 1.10 1.15 1.04 0.95 1.04 1.09 1.12 1. 0.70 0.91 0.95 1.06 0.90 0.82 0.94 1.01 1.06 0.96 1.03 1.04 1.04 1.05 1. 0.72 0.85 0.93 0.98 0.87 0.84 0.91 0.96 1.00 0.92 0.92 0.96 0.99 1.01 0. 0.91 1.01 1.08 1.13 1.03 25 0.21 0.16 0.12 0.08 0.14 0.17 0.12 0.08 0. 0.10 0.32 0.26 0.26 0.20 0.26 0.21 0.18 0.14 0.12 0.16 0.26 0.27 0.28 0. 0.28 0.19 0.17 0.15 0.14 0.16 0.30 0.26 0.23 0.21 0.25 0.29 0.28 0.27 0. 0.27 0.20 0.17 0.14 0.12 0.16 2.70 2.48 2.33 2.24 2.44 2.83 2.48 2.29 2. 2.45 3.44 2.99 2.74 2.61 2.94 2.91 2.66 2.54 2.51 2.65 2.74 2.79 2.85 2. 2.82 2.93 2.78 2.69 2.65 2.76 3.02 2.92 2.81 2.72 2.87 2.88 2.86 2.84 2. 2.85 3.16 2.97 2.79 2.66 2.90 4.52 4.47 4.38 4.33 4.42 4.27 4.22 4.06 3. 4.11 4.87 4.87 4.85 4.85 4.86 4.63 4.61 4.56 4.53 4.58 4.02 4.06 4.13 4. 4.10 3.98 4.02 4.08 4.17 4.06 4.76 4.77 4.76 4.75 4.76 4.50 4.46 4.48 4. 4.48 3.81 3.71 3.74 3.81 3.77 2.76 2.53 2.37 2.28 2.48 3.11 2.65 2.44 2. 2.63 3.46 3.00 2.75 2.62 2.96 3.05 2.76 2.64 2.59 2.76 3.59 3.58 3.52 3. 3.54 3.07 2.93 2.83 2.78 2.90 3.13 3.02 2.90 2.81 2.97 3.26 3.22 3.18 3. 3.19 3.63 3.42 3.22 3.07 3.33 4.15 4.11 4.02 3.96 4.06 3.49 3.47 3.32 3. 3.37 4.52 4.48 4.45 4.44 4.47 3.88 3.84 3.80 3.78 3.82 3.00 3.12 3.26 3. 3.19 3.66 3.66 3.70 3.76 3.69 4.28 4.28 4.27 4.26 4.27 3.75 3.79 3.87 3. 3.85 3.69 3.59 3.60 3.64 3.63 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Table A4. Performance of AudioSep trained on Hive dataset with different training scales (175K, 875K, 1.75M, 7.9M, and 17.5M samples) across four mixture complexities on the Hive test set."
        },
        {
            "title": "Scale Mix",
            "content": "Signal Fidelity Perceptual & Semantic Quality Reference-free Quality"
        },
        {
            "title": "SDR",
            "content": "SI-SDR STOI LPAPS FAD CLAP-A CLAP-T OQ Rec"
        },
        {
            "title": "Fai",
            "content": "0.73 0.63 0.56 0.51 0.61 0.75 0.65 0.59 0.54 0.63 0.76 0.67 0.60 0.55 0. 0.76 0.66 0.60 0.55 0.64 0.77 0.67 0.61 0.56 0.65 0.32 0.30 0.28 0.26 0. 0.32 0.31 0.29 0.27 0.30 0.33 0.32 0.30 0.28 0.31 0.33 0.32 0.30 0.28 0. 0.33 0.32 0.30 0.28 0.31 3.13 3.13 3.12 3.09 3.11 3.26 3.27 3.24 3.21 3. 3.13 3.13 3.12 3.09 3.12 3.34 3.34 3.32 3.29 3.32 3.35 3.36 3.33 3.31 3. 4.88 4.84 4.81 4.83 4.84 4.87 4.83 4.79 4.80 4.82 4.88 4.84 4.81 4.83 4. 4.86 4.81 4.77 4.78 4.80 4.86 4.81 4.77 4.78 4.81 3.14 3.17 3.17 3.15 3. 3.28 3.31 3.31 3.28 3.30 3.14 3.17 3.17 3.15 3.16 3.36 3.40 3.40 3.38 3. 3.37 3.41 3.42 3.40 3.40 4.53 4.43 4.36 4.36 4.40 4.52 4.42 4.34 4.32 4. 4.53 4.43 4.36 4.36 4.42 4.51 4.40 4.32 4.29 4.38 4.52 4.41 4.33 4.30 4. 175K 875K 1.75M 7.9M 17.5M 2-mix 3-mix 4-mix 5-mix 9.52 4.94 2.07 -0."
        },
        {
            "title": "Overall",
            "content": "4.12 2-mix 3-mix 4-mix 5-mix 10.41 5.83 2.88 0."
        },
        {
            "title": "Overall",
            "content": "4.96 2-mix 3-mix 4-mix 5-mix 11.00 6.45 3.56 1."
        },
        {
            "title": "Overall",
            "content": "5.60 2-mix 3-mix 4-mix 5-mix 11.01 6.50 3.60 1."
        },
        {
            "title": "Overall",
            "content": "5.63 2-mix 3-mix 4-mix 5-mix 11.11 6.52 3.61 1."
        },
        {
            "title": "Overall",
            "content": "5.67 8.51 4.19 1.42 -0.65 3.37 9.50 5.13 2.27 0.15 4.26 10.15 5.74 2.91 0. 4.90 10.17 5.82 2.97 0.83 4.95 10.31 5.86 3.01 0.90 5.02 0.66 0.52 0.43 0. 0.49 0.67 0.53 0.45 0.39 0.51 0.68 0.54 0.46 0.40 0.52 0.68 0.54 0.46 0. 0.52 0.68 0.55 0.46 0.40 0.52 3.15 4.00 4.47 4.79 4.10 3.00 3.86 4.35 4. 3.97 2.93 3.78 4.28 4.61 3.90 2.93 3.78 4.27 4.60 3.89 2.88 3.75 4.24 4. 3.86 0.70 0.83 0.91 0.97 0.85 0.67 0.80 0.88 0.94 0.82 0.65 0.79 0.86 0. 0.81 0.66 0.79 0.86 0.92 0.81 0.64 0.78 0.86 0.91 0.80 Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Table A5. Performance of FlowSep trained on Hive dataset with different training scales (175K, 875K, 1.75M, 7.9M, and 17.5M samples) across four mixture complexities on the Hive test set."
        },
        {
            "title": "Scale Mix",
            "content": "Perceptual & Semantic Quality Reference-free Quality LPAPS FAD CLAP-A CLAP-T OQ Rec"
        },
        {
            "title": "Fai",
            "content": "0.21 0.18 0.15 0.13 0.17 0.21 0.17 0.14 0.12 0.16 0.22 0.18 0.15 0.13 0. 0.24 0.20 0.17 0.15 0.19 0.23 0.20 0.17 0.15 0.19 2.97 2.79 2.70 2.68 2. 2.94 2.76 2.68 2.66 2.76 3.06 2.87 2.77 2.75 2.86 3.33 3.11 2.98 2.93 3. 3.27 3.05 2.95 2.92 3.05 4.39 4.19 4.11 4.13 4.21 4.39 4.15 4.04 4.04 4. 4.40 4.16 4.05 4.04 4.16 4.43 4.18 4.06 4.05 4.18 4.43 4.18 4.06 4.04 4. 3.13 3.02 2.95 2.93 3.01 3.09 2.99 2.94 2.92 2.98 3.23 3.12 3.06 3.04 3. 3.54 3.41 3.33 3.29 3.39 3.48 3.35 3.29 3.27 3.35 3.86 3.63 3.56 3.56 3. 3.91 3.65 3.55 3.54 3.66 3.94 3.68 3.57 3.56 3.69 4.00 3.73 3.61 3.60 3. 3.96 3.69 3.58 3.58 3.70 175K 875K 1.75M 7.9M 17.5M 2-mix 3-mix 4-mix 5-mix"
        },
        {
            "title": "Overall",
            "content": "2-mix 3-mix 4-mix 5-mix"
        },
        {
            "title": "Overall",
            "content": "2-mix 3-mix 4-mix 5-mix"
        },
        {
            "title": "Overall",
            "content": "2-mix 3-mix 4-mix 5-mix"
        },
        {
            "title": "Overall",
            "content": "2-mix 3-mix 4-mix 5-mix"
        },
        {
            "title": "Overall",
            "content": "3.47 4.20 4.55 4.75 4.24 3.41 4.19 4.54 4.75 4.22 3.43 4.18 4.53 4.74 4. 3.46 4.15 4.51 4.72 4.21 3.52 4.21 4.54 4.74 4.25 0.69 0.84 0.92 0.97 0. 0.68 0.85 0.93 0.98 0.86 0.68 0.84 0.92 0.97 0.85 0.67 0.81 0.89 0.94 0. 0.69 0.82 0.90 0.95 0.84 0.72 0.61 0.55 0.50 0.60 0.73 0.61 0.54 0.49 0. 0.73 0.62 0.55 0.50 0.60 0.73 0.63 0.57 0.53 0.62 0.72 0.63 0.57 0.53 0."
        }
    ],
    "affiliations": [
        "Chinese Institute for Brain Research (CIBR), Beijing, China",
        "Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing, China",
        "Johns Hopkins University",
        "Shanda AI Research Tokyo",
        "Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China"
    ]
}