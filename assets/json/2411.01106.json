{
    "paper_title": "LoRA-Contextualizing Adaptation of Large Multimodal Models for Long Document Understanding",
    "authors": [
        "Jian Chen",
        "Ruiyi Zhang",
        "Yufan Zhou",
        "Tong Yu",
        "Franck Dernoncourt",
        "Jiuxiang Gu",
        "Ryan A. Rossi",
        "Changyou Chen",
        "Tong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large multimodal models (LMMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page, visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and efficiency limitations, while directly presenting all pages to LMMs leads to inefficiencies, especially with lengthy documents. In this work, we present a novel framework named LoRA-Contextualizing Adaptation of Large multimodal models (LoCAL), which broadens the capabilities of any LMM to support long-document understanding. We demonstrate that LMMs can effectively serve as multimodal retrievers, fetching relevant pages to answer user questions based on these pages. LoCAL is implemented with two specific LMM adapters: one for evidence page retrieval and another for question answering. Empirical results show state-of-the-art performance on public benchmarks, demonstrating the effectiveness of LoCAL."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 1 6 0 1 1 0 . 1 1 4 2 : r LORA-CONTEXTUALIZING ADAPTATION OF LARGE MULTIMODAL MODELS FOR LONG DOCUMENT UNDERSTANDING Jian Chen1,2, Ruiyi Zhang2, Yufan Zhou2, Tong Yu2, Franck Dernoncourt2 Jiuxiang Gu2, Ryan Rossi2, Changyou Chen1, Tong Sun2 University at Buffalo1, Adobe Research2 {jianc, ruizhang}@adobe.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Large multimodal models (LMMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and efficiency limitations, while directly presenting all pages to LMMs leads to inefficiencies, especially with lengthy ones. In this work, we present novel framework named LoRA-Contextualizing Adaptation of Large multimodal models (LoCAL1), which can broaden horizons of any LMM to support long-document understanding. We demonstrate that LMMs themselves can be an effective multimodal retriever to fetch relevant pages and then answer user questions based on these pages. LoCAL is implemented with two specific LMM adapters, one for evidence page retrieval and the other for question answering. Empirical results show state-of-the-art performance on public benchmarks, demonstrating the effectiveness of LoCAL."
        },
        {
            "title": "INTRODUCTION",
            "content": "Documents serve as critical medium for the preservation and dissemination of information, with millions produced annually. These documents are not limited to simple text; they encompass complex layouts and variety of modalities such as text, tables, charts, and images. Visually-rich document understanding (VDU) is thus an essential and challenging area of research. Recently, Large Multimodal Models (LMMs) has emerged, showcasing remarkable abilities to process and understand documents. These models span both proprietary and open-source domains, like GPT-4o (OpenAI, 2023), Gemini-1.5 (Team et al., 2023), and Claude-3 among closed-source models, and InternLM-XC2-4KHD (Dong et al., 2024), InternVL-Chat (Chen et al., 2023b), LLaVA-NeXT (Liu et al., 2024a), Mini-CPM (Hu et al., 2024), mPLUG-DocOwl (Ye et al., 2023b), and TextMonkey (Liu et al., 2024d) in open-source space. Their performance has been particularly notable in single-page DU tasks demonstrated on datasets like DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022) and InfoVQA (Mathew et al., 2022). In real-world applications, they often present documents that are much longer, containing dozens or hundreds of pages(Ma et al., 2024c; Tanaka et al., 2023; Islam et al., 2023; Zhu et al., 2021). Addressing the understanding of such lengthy documents presents LMMs with new challenges (Ma et al., 2024c). One way is to utilize classical document parser (Rausch et al., 2021) to extract information and formulate prompt for LLM (Wang et al., 2023; Lamott et al., 2024), which is difficult to recover the layout in prompts and suffers performance degeneration from the document parser. The other way is to exploit the long context windows of large models, allowing them to take multiple pages at once. However, most of the input pages are not relevant to user requests, and efficiency will be compromised when the document contains hundreds of pages Ma et al. (2024c); Islam et al. (2023) or there is document collection (Tito et al., 2021). Equal contribution, work done when JC is at Adobe Research. 1The source code is available at: https://github.com/puar-playground/LoCAL 1 In this work, we first retrieve evidence pages to obtain relevant information within vast and varied landscape of content. Unlike using classical document parser, we propose using LMMs as the information encoder, which have shown great generalization ability as they have been trained on huge text corpus. After obtaining the embedding of each page, we further utilize contextualized late interaction for relevance scoring (Khattab & Zaharia, 2020). This design shows significantly better efficiency and accuracy than using the classical document parser to extract information. Top-k pages are then selected from hundreds of pages and provided to LMMs to answer user questions on documents. Figure 1: Overview of the LoCAL pipeline. The multi-page document and query are encoded by customized LMM (yellow). The most relevant page is retrieved through similarity-based matching, and fine-tuned LMM (blue) generates the final answer from the evidence. Based on this design demonstrated in Figure 1, we introduce the LoCAL framework for multi-page document understanding, which includes modules for evidence page retrieval and answer generation. Our contributions can be summarized as follows. We propose novel framework named LoCAL to broaden the horizons of LMMs, where we use intermediate LMMs hidden embedding for efficient question-based evidence page retrieval. We finetune LMMs through dual LoRA adapters for evidence page retrieval and question answering, respectively, enabling LoCAL to be edge-friendly with great memory efficiency. We collect visually-rich document QA dataset, LoCAL-bench, comprising nine domains including magazine, flyer, newsletter, product manual, and presentations, etc. This dataset is built upon web-crawl documents, containing 226 documents and 471 question answer pairs. We empirically show that LoCAL, with only 4B parameters, achieves state-of-the-art performance on LoCAL-bench and four public benchmarks, rivaling Gemini-1.5-pro on MMLongBench-Doc and demonstrating its effectiveness."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Visually-rich Document Understanding Visual Document Understanding (VDU) is the field focused on interpreting text-rich images such as tables (Zhong et al., 2019), charts (Masry et al., 2022), and webpage screenshots (Liu et al., 2024c; Tanaka et al., 2021). These images are complex, featuring mix of text and visual elements that convey abundant information (Gu et al., 2024). To evaluate multimodal document understanding, tasks range from low-level recognition tasks, such as information extraction, to high-level cognitive tasks, such as visual question answering (Mathew et al., 2020). Models in VDU are typically divided into two categories: OCR-dependent (Xu et al., 2020) and OCR-free (Kim et al., 2022), based on their reliance on Optical Character Recognition (OCR). OCR-dependent models are often trained to synchronize textual and visual data. For instance, UDoP (Tang et al., 2023) is pre-trained to restore obscured textual and layout details using both image and partial text inputs. OCR-free approaches must include text recognition training. Dount (Kim et al., 2022) is an example of an OCR-free model that focuses on producing unbroken text sequences, disregarding structural details. In contrast, Pix2Struct (Lee et al., 2023a), another OCR-free model, focuses on interpreting the structure by creating HTML DOM trees from webpage screenshots. However, this technique does not easily transfer to other image types. Our method focuses on the visual question-answering task, specifically targeting questions over long documents consisting of multiple pages of multimodal information. Multimodal Retrieval-Augmented Generation Augmenting language models with information from various knowledge sources has been found to boost their performance in different NLP tasks. The Dense Passage Retriever (DPR) (Karpukhin et al., 2020) trains its retrieval mechanism with documents 2 from within the same batch, using contrastive learning with negatively sampled examples, which enhances its capabilities in open-domain question answering. Document Screenshot Embedding (DSE) (Ma et al., 2024b) uses large multimodal models (LMMs) as encoders for both document screenshots and queries, training through contrastive learning to achieve enhanced multimodal retrieval. Both REALM (Guu et al., 2020) and Retrieval-Augmented Generation (RAG) (Gao et al., 2023b) consider the passages they retrieve as hidden variables and train the retrieval and generation components together, improving the efficiency of the retrieval-augmented generation approach. Taking cues from textual RAG, the Plug-and-play (Chen et al., 2024c) approach uses GradCAM (Selvaraju et al., 2020) to fetch pertinent image segments corresponding to given query. The MuRAG (Chen et al., 2022) model introduces multimodal retrieval-augmented Transformer that utilizes an external multimodal memory for language generation enhancement. Unlike other approaches that retrieve information from various knowledge sources, LoCAL focuses on retrieving relevant evidence pages from given document. This helps LMMs generate accurate and explainable answers based on the retrieved content. Large Multimodal Models While Large Language Models (LLMs) excel at text-only question answering (QA) (Dasigi et al., 2021; Lee et al., 2023b), they cannot process other modalities. To enable multimodal tasks like Visual Question Answering (VQA), Large Multimodal Models (LMMs) transform images and videos into visual tokens that LLMs can understand. To train these LMMs, MiniGPT-4 (Zhu et al., 2023) leverages ChatGPT to produce data compliant with high-quality instructions, while LLaVA (Liu et al., 2023b) prompts GPT-4 with image captions and bounding boxes. Chen et al. (2023a; 2024a) have prompted OpenAI GPT-4V to generate more than 1M pieces of quality data to train LMMs. LLaMA-Adapter (Zhang et al., 2023; Gao et al., 2023a) and mPLUG-Owl (Ye et al., 2023b) align text and image features with large-scale image-text pairs. InstructBLIP (Dai et al., 2023) has restructured 13 vision-language tasks to fit an instruction-based approach. mPLUG-Owl (Ye et al., 2023a;b) implements multi-task instruction fine-tuning with public document datasets. Recent research (Liu et al., 2023a; 2024a; Bai et al., 2023; Dong et al., 2024; Xu et al., 2024; Luo et al., 2024) improves visual encoders by increasing resolution, leading to significant advances in downstream applications but also raising memory costs, especially in multi-page tasks. Our method addresses this by extending LMMs to handle multi-page documents, retrieving only relevant pages to reduce computation and avoid distractions from long visual token sequences."
        },
        {
            "title": "3 LOCAL METHOD",
            "content": "Multi-page document understanding aims to answer questions related to long and complex documents containing both text and images from users. We denote document of n-pages as sequence of images, = {x1, x2, . . . , xn}. Text token sequence of question is denoted as {q1, q2, . . . , qn}. Traditional approaches that begin with parsing step to extract content elements such as images, tables, and forms from documents, then generate answers based on these contents using LLMs (SaadFalcon et al., 2023; Wang et al., 2023). Here, we first consider using LMMs to handle this task and avoiding the heuristic document parsing process, where we directly convert each page into single image. It is not desired, as most pages in document are irrelevant to user questions and performing an evidence page retrieval can further enhance the efficiency. We introduce LoCAL, method that efficiently leverages the capabilities of pre-trained large multimodal models (LMMs) for long document question-answering (QA). LoCAL can broaden the horizon of LMMs to answer questions over long documents or document collections with hundreds of pages. This finding is based on the fact that hidden states of LMMs can be effective page representations for question-based retrieval, as shown in Section 5.6. This representation ability can be further enhanced with contrastive training using LoRA adapter, demonstrating surprising retrieval performance of LMMs. Furthermore, we can finetune LoRA-adpter of QA to further enhance the performance of LoCAL on specific domains. In summary, we first retrieve evidence pages to rank these images based on their relevance score to given question q, then select the most relevant images, which are then fed into the LMM to generate the answer. In this section, we introduce the LoCAL architecture in Section 3.1, retrieval training in Section 3.2 and dual-adapter designs in Section 3.3. 3.1 ARCHITECTURE Figure 2 presents an overview of our model architecture, which comprises two LMM-based modules for the retrieval of evidence pages and question answers. 3 Figure 2: Model overview of LoCAL. It contains two modules, which are finetuned using LoRA (Hu et al., 2021), sharing the same pretrained LLM backbone. The retrieval module selects evidence pages for the other QA module, which provides responses to user questions. Col-Retrieval Module Building on the approach introduced in ColPali (Faysse et al., 2024), we employ modified large multimodal model for retrieval, comprising vision encoder fv, large language model (LLM) , and Col-projection layer fp. For an input image X, the vision encoder computes sequence of visual embeddings fv(X), which are then concatenated with token embeddings yv derived from fixed text prompt: nDescribe the image. This combined input is fed into the LLM. The projection layer fp then transforms the LLMs last hidden states into low-dimensional feature space, resulting in feature sequences that can be represented as Ev = fp(f (fv(X), yv)). Similarly, for an input question q, the question is first augmented into yq using prompt template. Then, its token embedding yq is processed without visual input as Eq = fp(f (yq)). Finally, late-interaction score sLI(Eq, Ev) is computed between the feature sequences, measuring the relevance of page image to the question text. More details about scoring method is provided in section 3.2. Question-Answering Module The QA module uses classic LLaVA-like architecture (Liu et al., 2024b), utilizing vision encoder fv to compute visual embeddings, which are combined with token embeddings and processed by an LLM . The LLM then generates text answers autoregressively through next-word prediction. 3.2 CONTEXTUALIZED LATE INTERACTION We utilize the contextualized late interaction (Col) technique (Khattab & Zaharia, 2020) to compute relevance scores for evidence retrieval. Unlike traditional single-vector encoders, such as CLIP (Radford et al., 2021), the Col technique introduces an inter-sequence similarity metric called the late-interaction score, which captures more fine-grained question-image relevance. Formally, the late-interaction score between text feature sequence Eq = {eq1 , . . . , eqn } of length and visual feature sequence Ev = {ev1, . . . , evm } of length is defined as: sLI(Eq, Ev) = (cid:88) i=1 max j{1,..,m} eqi eT vj . (1) We use it as similarity score in contrastive learning to facilitate ranked retrieval. Specifically, we train our retrieval module to maximize the late-interaction score between question and its corresponding evidence image, considering these as positive pairs. We then identify the most similar, but unassociated, image within the batch to form the hardest negative pair and minimize the score for this pair. Figure A.1 shows training pair example. The loss function is defined as: = log(1 + exp(sLI(Eq, The training process of the Col-retrieval module is summarized in Algorithm 1. ) sLI(Eq, E+ ))). (2) training batch of evidence image and question pairs Algorithm 1 Col-retrieval training Require: Pre-trained LMM {fv, l }, {(X1, y1), , (Xb, yb)}. 1: Initialize the Col-projection layer fp. 2: while not converged do 3: 4: 5: (fv(Xi), yi)), {1, ..., b}. (yi)), {1, ..., b}. q, Ej = fp(f Get Ei Get Ei = fp(f Compute Si,j = sLI(Ei Get negative image index ˆi for each yi: ˆi = arg maxj{1,...,b},j=i(Si,j) Gradient update using loss function Eq.(2), where Ej v). 6: 7: = Eˆi. 8: end while 3.3 PARAMETER SHARING VIA DUAL-ADAPTER To reduce memory usage, we optimize the model by sharing single LMM that includes both the vision encoder fv and the language model fl across both the retrieval and QA modules. To accommodate the different tasks required by each module, we insert two sets of adapters into the fl using the LoRA method (Hu et al., 2021). In the retrieval module, we use set of adapters Wr to create the retrieval-LLM, . For the QA module, different set of adapters Wa is added to the fl, creating the QA-LLM, . In this way, we support both tasks using single LLM and vision encoder, adding only 2% additional parameters."
        },
        {
            "title": "4 LOCAL-BENCH",
            "content": "Visually-rich Document Selection About 4,000 PDF documents are crawled from the Web and contents of these documents are extracted via the document parser2. We keep the document with figures and throw away text-only or scan documents. To select documents with specific types of figures, we build figure scheme that includes 19 figure types after reviewing different documents. We find some types of figures are not informative, such as logo and banner. We use the pretrained CLIP model ViT-L/14-336 (Radford et al., 2021) to perform figure classification on the extracted figures and keep 6 out of 19 types of figures, including tables, maps, diagrams, infographics, data charts, workflows, and screenshots. After that, we also annotate the document types for all selected documents. Question-Answer Collection Document parser returns all document elements in JSON format and the figures are saved separately as image files. We retrieve the JSON file for the document to obtain the contexts of each figure. Then we combine the figures with their contexts and use GPT-4o (API version 2024-02-15-preview) to generate QA pairs. For the GPT-4o prompts, we provide two demonstrations and ask GPT-4o to generate QA pair. In addition, we perform automatic verification using GPT-4o to ensure the quality of the generation. Specifically, we only provide the figure to GPT-4o and ask it with the generated question; if GPT-4o can answer it correctly, we will keep that QA pair in the LoCAL Bench. This heuristic filter ensures that the answers are from document figures and double-checks the correctness of generated answers. Dataset Statistics LoCAL-Bench contains 226 documents and 471 human-verified question-answer pairs. Figure 3 shows the distributions of the document types and the length distribution by document type. LoCAL-Bench has great diversity of documents compared to previous work (Tanaka et al., 2023; Islam et al., 2023; Ma et al., 2024c). 2Adobe Extract API: https://developer.adobe.com/document-services/apis/pdf-extract/ 5 Figure 3: Distribution of document types (left) and average document lengths in each types (right)."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We assess the performance of LoCAL in evidence page retrieval and visual question answering capabilities. We first evaluate the retrieval accuracy of the Col-retrieval module within LoCAL and compare it with several baselines on SlideVQA (Tanaka et al., 2023), MM-LongBench (Ma et al., 2024c), DUDE (Van Landeghem et al., 2023), DocVQA (Mathew et al., 2020; 2021) and LoCAL-Bench. We then conduct experiments on question answering using LoCAL and compare the results with other LMM baselines, inlcuding single-page and cross-page VQA. All experiments are implemented with PyTorch and conducted on Nvidia A100 GPUs. The Col-retrieval modules are fine-tuned for 4 epochs with batch size of 32 and learning rate of 5e-5, using the AdamW optimizer and LoRA adapters on all linear layers in the LLM. The LoRA rank is set to 32. 5.1 DATASETS Finetuning Dataset We train our Col-retrieval modules using the original training data of ColPali (Faysse et al., 2024), which includes 39,463, 10,074, 13,251, 10,000, and 45,940 question-page pairs filtered from DocVQA, InfoVQA (Mathew et al., 2022), TATDQA (Zhu et al., 2024), arXivQA (Li et al., 2024), and synthetic data across various topics, including government reports, healthcare, artificial intelligence, and energy. We also combined DocMatix-IR (Ma et al., 2024a) and PFLDocVQA (Tito et al., 2023b) for training. We fine-tuned our QA modules using the training split of the SlideVQA dataset (Tanaka et al., 2023). The SlideVQA dataset contains 1,919 slides in the training set, 400 in the test set, and 300 in the development set, with each slide consisting of 20 pages. The training split includes 10,290 samples, each annotated with questions, answers, and corresponding evidence. Evaluation Dataset We evaluated our methods performance on four public datasetsSlideVQA, MMLongBench-Doc (Ma et al., 2024c), DocVQA (Mathew et al., 2021), and DUDE (Van Landeghem et al., 2023)along with our proposed LoCAL-Bench dataset. The evaluation was conducted in both single-evidence (SP) and cross-evidence (MP) settings, where questions require information from either single page or multiple pages within long document. For DocVQA, we used 5,349 SP and 5,187 MP QA pairs from the validation split. Similarly, we combined the test and dev splits of SlideVQA to form 2,995 SP and 763 MP QA pairs for evaluation. For DUDE, we evaluated 6,307 QA pairs from the validation split. MMLongBench-Doc, which consists of 135 PDF documents averaging 50.4 pages (ranging from 9 to 468 pages), contains 1,081 QAs in total. From these, we extracted 488 single-evidence QAs to assess the performance of large multimodal models (LMMs) designed for single-image tasks. Additionally, we report the results of our best-performing model across all categories in MMLongBench-Doc, providing comprehensive comparison against state-of-the-art LMMs."
        },
        {
            "title": "5.2 EVALUATION METRICS",
            "content": "We evaluate our models performance on evidence retrieval and question-answering using several key metrics: Top-k Accuracy, Exact Match (EM) (Tanaka et al., 2023), Generalized Accuracy (G-Acc) (Ma et al., 2024c), Average Normalized Levenshtein Similarity (ANLS) (Biten et al., 2019), and Partial Normalized Levenshtein Similarity (PNLS) (Chen et al., 2024b). detailed explanation of each metric can be found in Appendix B."
        },
        {
            "title": "5.3 COMPARATIVE RETRIEVAL ACCURACY ANALYSIS",
            "content": "We evaluated the accuracy of the Col-retrieval module in SlideVQA, MMLongBench-Doc, SPDocVQA, and LoCAL-Bench, comparing it with the baseline methods including CLIP (Radford et al., 2021), BM25 (Robertson et al., 2009), and SBERT (Reimers & Gurevych, 2019). For CLIP, we used its text and image encoders to compute the cosine similarity between the feature of the question and the page. BM25 and SBERT (text-based), first used Paddle-OCR 3 to extract page text. For SBERT, we computed feature vectors for each OCR-text box in page and used the highest similarity score for page relevance. For BM25, the extracted text was concatenated into single caption for relevance scoring. The results are shown in Table 1. accuracy CLIP BM25 SBERT Col-Retrieval Modules Col-Paligemma Col-Phi-3-V Col-InternVL2 SlideVQA top5 top1 54.7 27.0 91.1 69.3 91.0 73.0 89.0 90.0 88. 98.7 98.7 98.3 MMLong top5 top1 48.2 20.4 26.0 5.2 70.2 44.7 60.7 64.0 61.3 82.0 84.6 83.0 LoCAL-B top5 top1 57.1 23.7 57.5 32.2 72.1 38.8 67.9 66.9 69. 90.8 90.4 90.7 SP-DocVQA top5 top1 64.1 29.9 61.7 30.9 74.0 47.4 62.3 65.4 63.2 85.9 87.3 85.9 Table 1: Retrieval accuracy results on four datasets, where MMLong refers to MMLongBench-Doc, LoCAL-B refers to LoCAL-Bench. Bold font indicates the best model. The results indicate that Col-retrieval outperforms all baselines, achieving more than 98% in top5 retrieval accuracy on the SlideVQA dataset, where each slide consists of 20 pages. However, performance decreases on other datasets as the data become more complex and document lengths increase significantly. 5.4 MAIN RESULTS We compared the performance of our method with popular lightweight LMMs on document question answering tasks, using PaliGemma (Beyer et al., 2024), Phi-3-v (Abdin et al., 2024), and InternVL24B (Chen et al., 2023b) as the backbone LMMs for both retrieval and QA modules, following the dual adapter design from Section 3.3. We fine-tuned the retrieval module using the 118,695 training question-page pairs used in ColPali (Faysse et al., 2024). The QA module is fine-tuned using SlideVQAs training split. We reported the original evaluation metrics used in prior works, including EM, G-Acc, and ANLS, and additionally reported PNLS, which better evaluates LLM-generated responses. Table 2 presents the comparison results. We first evaluate LoCAL on single-evidence questions from SP-SlideVQA, MMLongBench-Doc, and SP-DocVQA, where the required information is on single page. To demonstrate the question-answering capabilities of LMMs, we include four cheating baselines where models are given the ground truth evidence page. Next, we test LoCAL on cross-evidence questions from MP-SlideVQA, MP-DocVQA, and DUDE, where information spans multiple pages. We only test LoCAL with InternVL2-4B backbone, since the other two LMM are pretrained for single-page understanding. LoCALs performance is compared with classical encoder-only and encoder-decoder models, including BERT (Kenton & Toutanova, 2019), Longformer (Beltagy et al., 2020), Big Bird (Zaheer et al., 2020), T5 (Raffel et al., 2020), Hi-VT5 (Tito et al., 2023a), and 3PaddleOCR: https://github.com/PaddlePaddle/PaddleOCR 7 LayoutLMv3 (Huang et al., 2022), with results taken from the best settings in the original papers. InternVL2-8B and GPT-4o, processing all pages, serve as the state-of-the-art baselines for opensource and proprietary multipage LMMs, respectively. Finally, we demonstrate how the limitations of the retrieval and QA modules can impact overall performance through challenging examples from the SlideVQA dataset, as shown in Appendix C. Table 2: Quantitative Results in Multi-Page QA: \"#Param\" refers to number of parameters. \"Evidence\" reports evidence setting: (true evidence page), (all pages), and Rk (top-k retrieved). Reported metrics include PNLS, Exact Match, Generalized Accuracy, and ANLS. indicates models with LoRA adapter on QA module. Results for all encoder/decoder models are taken from their respective papers, with - indicating missing or not applicable results. Bold font indicates the best open-source model, excluding cheating baselines. Method #Param Evidence SP-SlideVQA PNLS EM Single-Page Evidence MMLongBench PNLS G-Acc SP-DocVQA ANLS PNLS Cheating Baselines PaliGemma Phi-3-v InternVL2 GPT-4o Multi-image LMMs InternVL2 GPT-4o LoCAL Models (Proposed) LoCAL-PaliGemma LoCAL-PaliGemma LoCAL-Phi-3-V LoCAL-Phi-3-V LoCAL-InternVL2 LoCAL-InternVL2 3B 4B 4B - 8B - 3B 3B 4B 4B 4B 4B T R1 R1 R1 R1 R5 R5 Method #Param Evidence Encoder/Decoder models BERT-Large Longformer Big Bird T5-Base LayoutLMv3 Hi-VT5 Multi-image LMMs InternVL2 GPT-4o LoCAL Models (Proposed) LoCAL-InternVL2 LoCAL-InternVL2 334M 148M 131M 223M 125M 316M 8B - 4B 4B - - - - - - R5 R5 37.30 13.72 15.03 30.59 12.62 27. 35.03 49.75 12.85 58.13 16.40 45.07 0.63 0.80 0.58 0.84 0.65 0.81 0.60 0.65 0.78 0.77 0.58 0.77 23.9 33.7 40.4 56.8 14.1 54. 23.9 23.1 30.7 28.4 33.2 34.0 0.38 0.52 0.55 0.62 0.22 0.57 0.35 0.38 0.50 0.44 0.48 0.49 0.65 0.65 0.84 0.87 0.50 0. 0.56 0.56 0.55 0.68 0.70 0.71 0.79 0.85 0.88 0.94 0.55 0.80 0.69 0.68 0.75 0.73 0.76 0.75 Cross-Page Evidence MP-SlideVQA PNLS EM MP-DocVQA PNLS ANLS DUDE ANLS PNLS - - - - - - - - - - - - 17.04 16. 24.25 31.98 0.53 0.73 0.61 0.59 0.53 0.55 0.58 0.51 0.55 0.62 0.68 0.67 0.70 0. - - - - - - 0.75 0.79 0.76 0.76 0.25 0.27 0.26 0.42 0.20 0.23 0.37 0.54 0.36 0. - - - - - - 0.56 0.70 0.57 0.54 Retrieval vs Multipage We observe LoCAL consistently outperforms InternVL2-8B, across various settings. The primary issue with LMMs is that long documents are transformed into excessively long visual token sequences, leading to significant memory burdens, as reported later in section 5.5. In datasets like MMLongBench-Doc and DocVQA, some documents exceed hundreds of pages, causing out-of-memory errors, even on servers with 8 A100 (80GB) GPUs. In such cases, we assigned zero score in our experiments. In contrast, GPT-4o exhibits strong multi-page reasoning capabilities. However, the accuracy of the cheating baseline slightly surpasses that of using all pages, as providing only the evidence pages helps GPT-4o avoid distractions from irrelevant information in the longer context. Moreover, LoCAL with InternVL2-4B backbone perform slightly better than the one with 8 Phi-3-V backbone on MMLongBench-Doc and SP-DocVQA, possibly due to the improvement in retrieval accuracy by using top-5 pages, which is more crucial for longer documents. Impact of Fine-tuning We observe that LoCAL QA modules with PaliGemma and InternVL2-4B backbones show significant increase in EM on the SlideVQA dataset, surpassing their cheating baselines after fine-tuning on SlideVQA. The model with the Phi-3-V backbone shows notable improvements in Exact Match (EM) scores without gains in PNLS, suggesting that fine-tuning primarily enhanced the models attention and answer formatting. This could be because the pretrained model was already optimized for these question types. Nevertheless, as shown in Figure D.1, we empirically find that fine-tuning still improves answering performance. However, we notice performance drop for fine-tuned LoCAL-Phi-3-V on MMLongBench-Doc, indicating that fine-tuning can harm LLM generalization. similar trend is seen with the InternVL2-4B backbone on the DUDE dataset. Comparison with SOTA LMMs Finally, we present the complete results of LoCAL-InternVL2-4B on the MMLongBench-Doc dataset to highlight the advantages of our method. As shown in Table 3, our model, with only 4 billion parameters, outperforms all open-source LMMs and achieves performance comparable to proprietary models such as Claude-3 Opus and Gemini-1.5-Pro. Method Open-source Models DeepSeek-VL-Chat Idefics2 MiniCPM-Llama3-V2.5 InternLM-XC2-4KHD mPLUG-DocOwl 1.5 Qwen-VL-Chat Monkey-Chat CogVLM2-LLaMA3-Chat InternVL-Chat-v1.5 EMU2-Chat LoCAL Models (Proposed) LoCAL-InternVL2 (R5) LoCAL-InternVL2 (R5) Proprietary Models Claude-3 Opus Gemini-1.5-Pro GPT-4V GPT-4o #Param TXT Evidence Source LAY CHA TAB 7.3B 8B 8B 8B 8.1B 9.6B 9.8B 19B 26B 37B 4B 4B - - - - 7.2 9.0 11.9 9.9 8.2 5.5 6.8 3.7 14.0 6. 26.5 26.3 24.9 21.0 34.4 46.3 6.5 10.6 10.8 14.3 8.4 9.0 7.2 2.7 16.2 9.7 18.8 22.1 24.7 17.6 28.3 46.0 1.6 4.8 5.1 7.7 2.0 5.4 3.6 6.0 7.1 2. 22.3 25.0 14.8 6.9 28.2 45.3 5.2 4.1 5.9 6.3 3.4 2.2 6.7 3.2 10.1 3.8 19.6 20.7 13.0 14.5 32.4 50.0 Evidence Page SIN MUL UNA ACC F1 5.2 7.7 9.5 12.6 7.4 5.2 6.6 3.9 14.9 5.7 33.2 34.0 25.6 21.1 36.4 54.5 7.0 7.2 9.5 7.6 6.4 7.1 6.2 5.3 12.2 6. 13.1 10.6 13.8 11.1 27.0 41.5 12.8 5.0 4.5 9.6 6.2 6.2 6.2 3.7 17.5 16.5 12.4 15.7 7.6 69.2 31.2 20.2 7.4 7.0 8.5 10.3 6.9 6.1 6.2 4.4 14.6 8. 22.2 23.0 17.4 28.2 32.4 42.8 5.4 6.8 8.6 9.8 6.3 5.4 5.6 4.0 13.0 5.5 22.8 24.2 18.1 20.6 31.2 44.9 FIG 7.6 8.7 12.2 13.0 9.9 6.9 9.4 6.9 16.6 7.7 23.6 25.2 17.1 15.2 26.8 44.1 Table 3: Performance of various models on MMLongBench-Doc. Questions are categorized in two ways: (1) by evidence source typetext (TXT), layout (LAY), chart (CHA), table (TAB), and image (IMG); and (2) by evidence pagessingle-page (SIN), cross-page (MUL), and unanswerable (UNA). Models using LoRA adapters fine-tuned on SlideVQA for the QA module are marked with . Bold font indicates the best open-source model. The results of baseline models are adopted from the original MMLongBench-Doc paper Ma et al. (2024c). 5.5 EFFICIENCY OF DIFFERENT MODELS To evaluate the efficiency of LoCAL, we conducted experiments on the SlideVQA dataset, which has 20 pages per question with resolution of 1024x768. We recorded peak GPU memory usage and time costs for retrieval and QA modules separately. The GPU memory is manually recorded using the nvidia-smi command, which tends to report higher numbers than the actual memory required by the application due to overhead and memory management processes. We tested backbones including PaliGemma, Phi-3-v, and InternVL2-4B, all equipped with LoRA adapters. Since PaliGemma and Phi-3-v are single-page models, we used top-1 retrieved image as input. InternVL2-4B, however, supports multi-image input, allowing us to test with the top-1, 5, and 12 retrieved images. 9 LoCAL-Backbone Page Paligemma Phi-3-v InternVL2-4B InternVL2-4B InternVL2-4B R1 R1 R1 R5 R12 Retrieval Time Mem 9.2 2.3 11.6 4.1 14.2 9.2 14.2 9.2 14.2 9. QA Time Mem 12.4 1.0 12.9 0.9 14.6 1.4 40.8 2.8 76.4 4.1 Table 4: Time (s) cost and Peak GPU memory (GB) cost of LoCAL models with different backbones. As shown in Table 4, the memory consumption of the QA module increases with the number of evidence pages, with 13 images (1024x768) exceeding the 80GB limit on an A100 GPU resulting in out-of-memory error. In contrast, the memory usage of the retrieval module remains low, as LoCAL processes pages independently during page retrieval, with memory costs equivalent to single-page reasoning. Despite the higher memory demands for multi-evidence QA, LoCAL remains compact and time-efficient, making it well-suited for answering questions from fewer evidence pages in resource-constrained environments. This demonstrates LoCALs ability to balance performance and resource usage, ensuring scalability across diverse deployment scenarios. 5.6 ABLATION In our experiment, we use the hidden states from the last transformer layer (index 31) as the feature sequence. However, LLMs consist of multiple transformer layers, each encoding different types of information. To assess the impact of layer selection, we conduct an ablation study on the hidden states used to compute the late interaction score in Eq.(1). Given the high computational cost of training the col-retrieval module across all layers, we instead evaluate top-1 accuracy on the MMLongBench-Doc dataset using hidden states from different layers of the Phi-3-Vision model with pre-trained weights. Figure 4: Top-1 retrieval accuracy on MMLongBench-Doc using different hidden states across all layers of Phi-3-Vision. Figure 4 shows that the hidden states of the 21th layer yield the highest accuracy. After fine-tuning model with hidden states from this layer, we observed improved accuracy compared to using hidden states of the final layer. In particular, using hidden states from earlier layers can significantly reduce computational costs, enabling faster retrieval during inference."
        },
        {
            "title": "6 CONCLUSIONS",
            "content": "In this paper, we propose LoCAL, lightweight LMMs for visually-rich document understanding. LoCAL has unique design to facilitate multi-page document understanding using dual LoRA adapters. The research highlights that small open-source models are great at processing multipage documents and underscored the importance of efficient retrieval mechanisms in filtering irrelevant pages. Furthermore, we collect the LoCAL-bench dataset for document understanding, and empirical results on benchmarks demonstrated the effectiveness of LoCAL. We hope these findings provide valuable insights for optimizing lightweight LMMs, aiming to improve accuracy and efficiency in visually-rich document understanding."
        },
        {
            "title": "7 LIMITATIONS",
            "content": "LoCAL is the first LMM that can perform retrieval and question answering simultaneously. However, it still requires computational resources for training and inference, which may limit its practical applicability in resource-constrained environments. LoCAL should be mobile friendly, as it only requires single base model. This base model can be Phi-3-Silica within MS operating systems or an Apple on-device model within IOS 18. routing mechanism in Apple Intelligence can better balance computational cost and performance. However, our experiments are not performed on these real-world cases, which are useful for pushing forward document intelligence."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 42914301, 2019. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024a. Jian Chen, Ruiyi Zhang, Yufan Zhou, Ryan Rossi, Jiuxiang Gu, and Changyou Chen. Mmr: Evaluating reading ability of large multimodal models. arXiv preprint arXiv:2408.14594, 2024b. Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Ziyong Feng, Yongle Zhao, and Yin Xie. Plug-and-play grounding of reasoning in multimodal large language models. arXiv preprint arXiv:2403.19322, 2024c. Lin Chen et al. Sharegpt4v: Improving large multi-modal models with better captions, 2023a. Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. arXiv preprint arXiv:2210.02928, 2022. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023b. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah Smith, and Matt Gardner. dataset of information-seeking questions and answers anchored in research papers. arXiv preprint arXiv:2105.03011, 2021. 11 Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. Manuel Faysse, Hugues Sibille, Tony Wu, Gautier Viaud, Céline Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. arXiv preprint arXiv:2407.01449, 2024. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model, 2023a. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023b. Jiuxiang Gu, Xiangxi Shi, Jason Kuen, Lu Qi, Ruiyi Zhang, Anqi Liu, Ani Nenkova, and Tong Sun. ADOPD: large-scale document page decomposition dataset. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= x1ptaXpOYa. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pp. 39293938. PMLR, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 40834091, 2022. Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: new benchmark for financial question answering. arXiv preprint arXiv:2311.11944, 2023. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1, pp. 2. Minneapolis, Minnesota, 2019. Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pp. 3948, 2020. Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. Computer Vision ECCV 2022, pp. 498517, 2022. ISSN 1611-3349. doi: 10.1007/978-3-031-19815-1_29. URL http://dx.doi.org/10.1007/978-3-031-19815-1_ 29. Marcel Lamott, Yves-Noel Weweler, Adrian Ulges, Faisal Shafait, Dirk Krechel, and Darko Obradovic. Lapdoc: Layout-aware prompting for documents. arXiv preprint arXiv:2402.09841, 2024. 12 Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In International Conference on Machine Learning, pp. 1889318912. PMLR, 2023a. Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in Lee, and Moontae Lee. Qasa: advanced question answering on scientific articles. In International Conference on Machine Learning, pp. 1903619052. PMLR, 2023b. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024c. Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint arXiv:2403.04473, 2024d. Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003, 2024. Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying multimodal retrieval via document screenshot embedding. arXiv:2406.11251, 2024a. Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying multimodal retrieval via document screenshot embedding. arXiv preprint arXiv:2406.11251, 2024b. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. arXiv preprint arXiv:2407.01523, 2024c. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for vqa on document images, 2020. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 16971706, 2022. OpenAI. Gpt-4 technical report, 2023. 13 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 87488763. PMLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Johannes Rausch, Octavio Martinez, Fabian Bissig, Ce Zhang, and Stefan Feuerriegel. Docparser: Hierarchical document structure parsing from renderings. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 43284338, 2021. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084. Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389, 2009. Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, Ryan Rossi, and Franck Dernoncourt. Pdftriage: question answering over long, structured documents. arXiv preprint arXiv:2309.08872, 2023. Peter Sellers. The theory and computation of evolutionary distances: pattern recognition. Journal of algorithms, 1(4):359373, 1980. Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: visual explanations from deep networks via gradient-based localization. International journal of computer vision, 128:336359, 2020. Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension In Proceedings of the AAAI Conference on Artificial Intelligence, pp. on document images. 1387813888, 2021. Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on multiple images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1363613645, 2023. Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. Unifying vision, text, and layout for universal document processing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1925419264, 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. Document collection visual question answering. In Document Analysis and RecognitionICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 510, 2021, Proceedings, Part II 16, pp. 778792. Springer, 2021. Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multipage docvqa. Pattern Recognition, 144:109834, 2023a. Rubèn Tito, Khanh Nguyen, Marlon Tobaben, Raouf Kerkouche, Mohamed Ali Souibgui, Kangsoo Jung, Lei Kang, Ernest Valveny, Antti Honkela, Mario Fritz, and Dimosthenis Karatzas. Privacyaware document visual question answering. arXiv preprint arXiv:2312.10108, 2023b. Jordy Van Landeghem, Rubèn Tito, Łukasz Borchmann, Michał Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, Mickaël Coustaty, Bertrand Anckaert, Ernest Valveny, et al. Document understanding dataset and evaluation (dude). In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1952819540, 2023. 14 Wenjin Wang, Yunhao Li, Yixin Ou, and Yin Zhang. Layout and task aware instruction prompt for zero-shot document image question answering, 2023. Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. arXiv preprint arXiv:2403.11703, 2024. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. pp. 11921200, 2020. Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126, 2023a. Qinghao Ye et al. mplug-owl: Modularization empowers large language models with multimodality, 2023b. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. Image-based table recognition: data, model, and evaluation. arXiv preprint arXiv:1911.10683, 2019. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. Tat-qa: question answering benchmark on hybrid of tabular and textual content in finance. arXiv preprint arXiv:2105.07624, 2021. Fengbin Zhu, Chao Wang, Fuli Feng, Zifeng Ren, Moxin Li, and Tat-Seng Chua. Doc2SoarGraph: Discrete reasoning over visually-rich table-text documents via semantic-oriented hierarchical graphs. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 51195131, Torino, Italia, 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.456."
        },
        {
            "title": "A EXAMPLE OF TRAINING PAIRS FOR RETRIEVAL MODULE",
            "content": "Figure A.1: Example of training pairs within batch (batch size: 4) for contrastive training, using samples from the SlideVQA dataset."
        },
        {
            "title": "B EVALUATION METRICS",
            "content": "We evaluate the models performance on evidence retrieval and question-answering using five metrics explained as follows: Top-k Accuracy In our experiment, we focus on questions that have evidence from single page. We use top-k accuracy to evaluate retrieval methods, which measures the percentage of times the evidence image appears within the top most similar images. Exact Match Following (Tanaka et al., 2023), we report exact match (EM) frequency between generated answers and the ground truth, allowing for case insensitivity and extra spaces. While effective for fine-tuned models, this metric is less suited for LLM responses, which often include full sentences. Correct answers with extra context may thus be unfairly penalized. Generalized Accuracy We report generalized accuracy (G-Acc) from MMLongBench-Doc (Ma et al., 2024c), GPT-dependent, rule-based evaluation protocol . Model responses are simplified using GPT-4o and scored based on answer-type-specific rules. However, G-Acc has two limitations: it introduces randomness from GPTs stochastic outputs and relies on answer-type annotations, limiting its applicability across datasets. ANLS Average Normalized Levenshtein Similarity (ANLS) (Biten et al., 2019) measures the similarity between predicted and ground truth text using the Levenshtein distance, normalized by the longer strings length. It outputs similarity score between 0 and 1. ANLS allows mismatches, insertions, and deletions making it useful for OCR and document understanding tasks when exact matches are not required. PNLS The partial normalized Levenshtein similarity (PNLS) (Chen et al., 2024b) generalizes ANLS by not penalizing extra prefixes or suffixes while allowing mismatches, insertions, and deletions within the matched region. This makes it more suitable for evaluating LLM responses, which are often verbose to improve user experience. The PNLS metric is formally defined as follows: String T1,m = t1 . . . tm represents the true answer and S1,n = s1 . . . sn is model generated string. We first use using the approximate string matching algorithm (Sellers, 1980) to identify the sub-string of that has the minimum edit distance to . Specifically, we first construct scoring matrix of size (m + 1) (n + 1), where Fi,j stores the smallest edit distance between the i-prefix T1,i and any sub-string Sx,j, {1, . . . , 1} that ends at position j. The scoring matrix can be computed recursively Fi,j = 0 min Fi1,j1 + c(ti, sj) Fi1,j + 1 Fi,j1 + 1 if = 0 if = 0 otherwise, where is the substitution cost that takes value of 0 if ti = sj and 1 otherwise. Once is computed, the minimum value in the last row is the optimal edit distance and the end index of the matched substring = arg minj(Fm+1,j). The start index can be found by tracing back the the computation of Eq.(B) using arg min operation. Finally, the PNLS is computed as: m/(m + i + 1). In our experiments we use binary cost function: c(ti, sj) = 0 if ti = sj else c(ti, sj) ="
        },
        {
            "title": "C EXAMPLE OF INFERENCE FAILURE SCENARIO",
            "content": "Figure C.1: Inference example of challenging case in the SlideVQA dataset. LoCAL-Paligemma retrieved the wrong evidence page due to limitations in its retrieval module, leading to an incorrect answer. LoCAL-Phi-3-V retrieved the correct page but provided wrong answer due to limitations in its QA module. Meanwhile, LoCAL-InternVL2-4B also assigned the highest relevance score to an incorrect page. However, since it processes multiple pages (top 5), the correct evidence page was included in the input, allowing its fine-tuned QA module to deliver the correct answer. 17 C.1 ADDITIONAL EXAMPLES OF RETRIEVAL FAILURES Figure C.2: Failure cases from the SlideVQA dataset, highlighting retrieval module errors. In the first two examples, some of the relevant information (highlighted in red boxes) on the true evidence pages is difficult even for human eyes to detect. In the third example, the retrieved page has high similarity to the true evidence page, making it challenging to rank correctly. Additionally, answering the question accurately requires deep understanding of the concept of chemical damage and related topics. 18 QAULITATIVE RESULTS IN QUESTION-ANSWERING Figure D.1: Question answering examples on the SlideVQA dataset using different QA modules. Models without fine-tuning, such as Phi-3-V and InternVL2-4B, tend to produce verbose and errorprone responses. However, in the second example, fine-tuning with the LoRA adapter significantly improves the accuracy of Phi-3-V and InternVL2-4B."
        }
    ],
    "affiliations": [
        "University at Buffalo",
        "Adobe Research"
    ]
}