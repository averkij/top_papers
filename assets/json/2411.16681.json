{
    "paper_title": "Factorized Visual Tokenization and Generation",
    "authors": [
        "Zechen Bai",
        "Jianxiong Gao",
        "Ziteng Gao",
        "Pichao Wang",
        "Zheng Zhang",
        "Tong He",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual tokenizers are fundamental to image generation. They convert visual data into discrete tokens, enabling transformer-based models to excel at image generation. Despite their success, VQ-based tokenizers like VQGAN face significant limitations due to constrained vocabulary sizes. Simply expanding the codebook often leads to training instability and diminishing performance gains, making scalability a critical challenge. In this work, we introduce Factorized Quantization (FQ), a novel approach that revitalizes VQ-based tokenizers by decomposing a large codebook into multiple independent sub-codebooks. This factorization reduces the lookup complexity of large codebooks, enabling more efficient and scalable visual tokenization. To ensure each sub-codebook captures distinct and complementary information, we propose a disentanglement regularization that explicitly reduces redundancy, promoting diversity across the sub-codebooks. Furthermore, we integrate representation learning into the training process, leveraging pretrained vision models like CLIP and DINO to infuse semantic richness into the learned representations. This design ensures our tokenizer captures diverse semantic levels, leading to more expressive and disentangled representations. Experiments show that the proposed FQGAN model substantially improves the reconstruction quality of visual tokenizers, achieving state-of-the-art performance. We further demonstrate that this tokenizer can be effectively adapted into auto-regressive image generation. https://showlab.github.io/FQGAN"
        },
        {
            "title": "Start",
            "content": "Zechen Bai1, Jianxiong Gao2, Ziteng Gao1, Pichao Wang3, Zheng Zhang3, Tong He3, Mike Zheng Shou1* 1Show Lab, National University of Singapore 2Fudan University 3Amazon 4 2 0 2 5 ] . [ 1 1 8 6 6 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Visual tokenizers are fundamental to image generation. They convert visual data into discrete tokens, enabling transformer-based models to excel at image generation. Despite their success, VQ-based tokenizers like VQGAN face significant limitations due to constrained vocabulary sizes. Simply expanding the codebook often leads to training instability and diminishing performance gains, making scalability critical challenge. In this work, we introduce Factorized Quantization (FQ), novel approach that revitalizes VQ-based tokenizers by decomposing large codebook into multiple independent sub-codebooks. This factorization reduces the lookup complexity of large codebooks, enabling more efficient and scalable visual tokenization. To ensure each sub-codebook captures distinct and complementary information, we propose disentanglement regularization that explicitly reduces redundancy, promoting diversity across the sub-codebooks. Furthermore, we integrate representation learning into the training process, leveraging pretrained vision models like CLIP and DINO to infuse semantic richness into the learned representations. This design ensures our tokenizer captures diverse semantic levels, leading to more expressive and disentangled representations. Experiments show that the proposed FQGAN model substantially improves the reconstruction quality of visual tokenizers, achieving state-of-the-art performance. We further demonstrate that this tokenizer can be effectively adapted into auto-regressive image generation. https://showlab.github.io/FQGAN 1. Introduction In recent years, the success of discrete token-based approaches in natural language processing [2, 21] has sparked growing interest in discrete image tokenization and generation [6, 12, 33]. Visual tokenizers play crucial role by converting image data into discrete tokens, thereby enabling the application of powerful transformer-based generative models. The quality of visual tokenization significantly impacts high-fidelity image reconstruction and generation. *Corresponding author Figure 1. Performance comparison of popular tokenizers at various codebook sizes, including VQ (Taming) [6], VQ (LlamaGen) [25], VQ-LC [41], LFQ (OpenMAGVIT2) [15], and FQGAN. Lower rFID values indicate better performance. Popular visual tokenizers, such as VQGAN [6], adopt an encoder-quantizer-decoder structure, where the quantizer converts the latent feature into discrete tokens via vector quantization (VQ). These approaches have shown remarkable performance on image reconstruction and generation [3, 12, 25]. Despite these successes, visual tokenization involves inherently lossy compression, especially compared to continuous encoding, since visual data is naturally continuous and complex. common strategy to address this is to enlarge the codebook, enhancing its capacity to approximate continuous representations. However, traditional VQ-based models are constrained by codebook size. Existing research [25, 41] indicates that increasing codebook sizes beyond 16,384 can lead to training instability, such as low codebook utilization and performance saturation. Recent works have proposed innovative strategies to address these limitations. For example, FSQ [16] and LFQ [35] are introduced to eliminate the need for an explicit codebook, achieving state-of-the-art reconstruction quality using massive codebook size. Among VQ tokenizers, VQGAN-LC [41] employs pre-trained feature clusters to help stabilize training with larger codebooks. Nevertheless, VQ tokenizers still exhibit inferior performance to LFQ ones and, more importantly, the inherent challenges of VQ remain unresolved. Large codebooks complicate quantization by necessitating the calculation of pairwise distances between encoder outputs and all codebook entries, followed by an argmin() operation to select the nearest code. As the codebook size increases, the lookup process becomes more computationally expensive and unstable, leading to inconsistent results. To tackle these challenges, we draw inspiration from the divide-and-conquer principle, breaking down complex problem into smaller, more manageable components to enhance both stability and performance. We propose novel factorized codebook design, wherein large codebook is split into several smaller, independent sub-codebooks. This factorization simplifies the tokenization process, improving the stability of the quantization. By combining entries from multiple sub-codebooks, we construct more expressive and scalable representation space. It provides greater flexibility for capturing image features at varying levels of granularity, improving overall tokenization quality. However, to fully harness this expressiveness and ensure that each sub-codebook contributes uniquely to the representation, it is essential to disentangle the learned features. Factorizing the codebook alone is insufficient unless each sub-codebook learns to capture unique and complementary information. To address this, we introduce disentanglement regularization mechanism that enforces orthogonality between sub-codebooks, encouraging each sub-codebook to focus on distinct aspects of the visual data, such as spatial structures, textures, colors, etc. This is akin to having different specialists analyzing various aspects of an image, ultimately resulting in richer and more comprehensive representation. To further enhance the specialization of the subcodebooks, we integrate representation learning as an essential part of the training framework. By seamlessly weaving into the training objective, the sub-codebook is guided to capture semantically meaningful features that contribute to the overall representation. Traditional reconstruction objectives often lead to overfitting on high-variance visual details, which results in features that lack semantic meaning for perception tasks [1]. Our representation learning objective addresses this issue by guiding the factorized codebooks to learn robust, semantically rich features capable of generalizing beyond simple reconstruction. Specifically, by leveraging different vision backbones, e.g., CLIP [21] and DINOv2 [17], the sub-codebooks essentially learn to establishes complementary hierarchy of semantics: low-level structures (e.g., edges), mid-level details (e.g., textures), and high-level concepts (e.g., abstract appearance). By seamlessly integrating factorized codebook design, disentanglement regularization, and representation learning objectives, our visual tokenizer captures diverse and rich set of features. This holistic approach greatly enhances reconstruction quality, as each sub-codebook learns to represent different aspects of the image in balanced and semantically meaningful way. Leveraging these three core innovations, our visual tokenizer achieves state-of-the-art performance in discrete image reconstruction, as illustrated in Fig. 1. Additionally, we extend our analysis to autoregressive (AR) generation tasks. Unlike conventional tokenizers that produce single token per image patch, our tokenizer encodes each patch into multiple tokens, resulting in richer and more expressive representation. Drawing inspiration from related works on handling extremely large codebooks [15] and multi-code [12], we design factorized AR head that predicts sub-tokens for each patch, adapting our tokenizer effectively for downstream image generation. In summary, our contributions include: novel factorized quantization design that revitalizes VQ-based tokenizers, achieving state-of-the-art performance on discrete image reconstruction. Introduction of disentanglement and representation learning mechanisms that enable diverse and semantically meaningful codebook learning. Demonstration that our tokenizer enhances downstream AR models, improving image generation quality on the ImageNet benchmark. 2. Related Work 2.1. Visual Tokenizers Visual tokenizers map images into latent space for downstream tasks, such as visual understanding [14] and visual generation [6]. Visual tokenizers generally fall into two categories: continuous feature-based models and discrete token-based models. We mainly discuss the discrete ones as they are closely related to our work. Popular visual tokenizers, exemplified by VQGAN [6], use an encoder-quantizerdecoder structure: the encoder maps image data to latent space, the quantizer transforms this representation into discrete tokens using vector quantization, and the decoder reconstructs the image from these tokens. Building on VQGAN framework, numerous works [12, 16, 32, 33, 39] have been developed to improve performance from various perspectives. ViT-VQGAN [33] upgrades the encoder-decoder architecture from CNN-based network to transformerbased one. RQ-VAE [12] proposes modeling residual information using multiple codes to capture finer details. Despite advancements, VQ tokenizers still struggle with Rethe critical challenge of limited codebook size. search [25, 41] indicates that expanding the codebook size beyond 16,384 can lead to performance saturation or even degradation. This issue is often accompanied by low usage rate of the large codebook. To address this, FSQ (Finite Scalar Quantization) [16] and LFQ (Lookup Free Quantization) [35] are proposed to eliminate the need for an explicit codebook and significantly alleviates this issue. Within the VQ family, VQGAN-LC [41] uses pre-trained feature clusters to implicitly regularize the large codebook, helping to maintain higher usage rate. This work suggests that semantic information can benefit visual tokenizers, concept further explored in recent studies [7, 13, 30, 31, 34, 40]. For instance, VILA-U [31] demonstrates that pre-trained vision model can be fine-tuned into visual tokenizer while preserving its semantic capabilities. LG-VQ [13] and VQKD [30] show that incorporating language supervision or image understanding models can improve visual tokenizers. Our work, as part of the VQ family, aims to revitalize VQ tokenizers by addressing the large codebook problem through factorized quantization and leveraging semantic supervision. 2.2. Auto-regressive Visual Generation Auto-regressive visual generation uses next-token prediction approach to sequentially generate images or videos. VQGAN [6], pioneering model, utilizes transformer to predict tokens sequentially. RQ-VAE [12] extends VQGAN by incorporating residual tokenization mechanism and adding an AR transformer head to predict residual tokens at finer depth. LlamaGen [25] extends the VQGAN transformer architecture to the Llama [28] framework, demonstrating promising scaling behaviors. VAR [26] extends next-token prediction to next-scale prediction, reducing auto-regressive steps and enhancing performance. Open-MAGVIT2 [15], similar to LlamaGen [25], adopts Llama-style auto-regressive transformer as its backbone. To manage an extremely large codebook, it predicts two subtokens during the AR generation phase and composes them to obtain the original code. It also employs an RQ-like architecture, termed intra-block, to predict sub-tokens. In this work, our factorized codes share similarities with RQVAE [12] and Open-MAGVIT2 [15], specifically in predicting multiple tokens at each AR step. Consequently, we use factorized AR head atop the AR backbone to predict subtokens for each patch. 3. Method 3.1. Preliminary VQGAN [6] employs learnable discrete codebook RKD to represent images, where is the codebook size while is the dimensionality of the codes. Given an input image x, the encoder transforms it into latent feature = Enc(x). Then, the closest codebook entry for each patch is retrieved from the codebook to serve as the quantized representation: qi = Quant(hi, C) := arg min cj (cid:13)hi cj(cid:13) (cid:13) (cid:13) , (1) where hi RD, cj RD, qi RD denotes the encoded latent feature at patch i, codebook entry, and quantized feature at patch i, respectively. After that, VQGAN uses decoder to reconstruct the image ˆx = Dec(q). The training objective of VQGAN is to identify the optimal compression model of {Enc, Dec, C}, involving the following loss: LVQGAN = Lrec + LVQ + Lperceptual + LGAN, (2) where Lrec denotes the pixel reconstruction loss between and ˆx. LVQ denotes the codebook loss that pulls the latent features and their closest codebook entries closer. Lperceptual denotes the perceptual loss between and ˆx by leveraging pre-trained vision model [38]. LGAN introduces an adversarial training procedure with patchbased discriminator [11] to calculate the GAN loss. As these losses are widely adapted in most VQ tokenizer designs [6, 12, 25, 41], we omit the detailed definitions for simplicity. 3.2. Factorized Quantization Despite the remarkable performance achieved by the classical VQGAN model, it is known to suffer from unstable training and low codebook usage rate when increasing the codebook size. One prominent issue is the unstable lookup process among large set of embeddings. To alleviate this, we propose factorized quantization approach that decomposes singe large codebook into small sub-codebooks. The main framework is illustrated in Fig. 2. Encoder. We regard the original VQGAN encoder as base feature extractor. On top of that, feature adapters are introduced to transform the base image features into their respective feature space. Formally, hbase = Enc(x), (3) h1, h2, ..., hk = F1(hbase), F2(hbase), ..., Fk(hbase), (4) where F1, ..., Fk are the adapters for each factorized branch. Quantizer. Our method maintain unique codebook for each factorized branch. After extracting the branch-specific features, the quantization process is conducted at each codebook independently. Formally, q1, ..., qk = Quant(h1, C1), ..., Quant(hk, Ck), (5) where C1, ..., Ck are the factorized sub-codebooks. Decoder. Given the quantized feature from each subcodebook, we employ simple yet effective aggregation approach that concatenates them along the latent (channel) dimension. After that, the aggregated features are fed into the pixel decoder, which is inherited from the VQGAN model. Formally, ˆx = Dec([q1; q2; ...; qk]), (6) where ; denotes the concatenation operation. Figure 2. Illustration of the our method. The left part shows FQGAN-Dual, the factorized tokenizer design in an example scenario when = 2. This framework is extendable to factorization of more codebooks. The right part demonstrate how we leverage an additional AR head to accommodate the factorized sub-codes based on standard AR generative transformer. The factorized quantization design presents several appealing properties. First, the factorized and parallelized lookup process greatly alleviates the lookup instability in single large codebook. Second, maintaining factorized sub-codebooks and independent feature adapters allow the model to learn more diverse features. Lastly, the code aggregation before decoding essentially builds super large conceptual codebook with size of Cik. E.g., suppose = 2, C1 = C2 = 1024, there are 1, 0242 = 1, 048, 576 unique combinations of the sub-codes. Although the actual freedom of this conceptual codebook is smaller than real codebook with the same size, it already provides much larger capacity, given that we only maintain Ci codes. Prior arts reaches reconstruction saturation with codebook size 16, 384. In Tab. 3 of experiment, it is shown that factorizing the 32, 768 codebook into two 16, 384 sub-codebooks can further significantly improve the reconstruction performance. 3.2.1. Disentanglement The factorized quantization design allows diverse feature learning, given the sufficient capacity in the feature adapters and sub-codebooks. However, without explicit constraints, the sub-codebooks risk learning redundant and overlapping codes, particularly as the codebook size increases. To address this issue, we propose disentanglement regularization mechanism for the factorized sub-codebooks. For simplicity, we take = 2 as an example scenario. Through Eq. 5, we obtain q1 RLD and q2 RLD, where is the number of patches. We design the disentanglement regularization mechanism as follows: Ldisentangle = 1 (cid:88) (q 1 q2)2, i=1 (7) where is the number of samples in batch. This regularization mechanism minimizes the squared dot product between the two involved codes. The dot product directly measures the affinity between the two codes after L2 normalization, ranging from [1, 1], where -1/1 indicates negative/positive correlation and 0 denotes orthogonality. Minimizing the squaring function encourages the dot product value to approach 0. It also provides smooth gradient for optimization. Note that this regularization does not directly apply to the entire codebook. Instead, it operates on patches of each image instance. In other words, for each patch, it encourages the involved sub-codes to capture different aspects. 3.2.2. Representation Learning Typically, the main training objective of visual tokenizers is pixel reconstruction. Research [1] suggests that the reconstruction objective can hardly learn meaningful semantic features for perception, as the features mainly capture highvariance details. However, recent work [37] finds that learning semantic features can benefit visual generation model training. In this work, we show that representation learning plays crucial role in tokenizer training, especially in the context of factorized quantization. Consider the example of an image patch depicting an ear. traditional VQ code may capture its appearance, such as color, texture, etc. However, it is unaware of the species, e.g., cat or dog. While such code may effectively reconstruct the patch, introducing semantic information is expected to be beneficial. When informed with semantics, the decoder (and generation model) can better handle the corresponding visual reconstruction and generation tasks. Moreover, compared to high-variance signals, semantic information tends to generalize better. Building on this intuition, we introduce representation learning as training objective to encourage the model to learn meaningful semantic features. We continue to use = 2 as an example scenario. Specifically, one subcodebook, say C2, is tasked with predicting the features of pre-trained vision model using lightweight feature prediction model. C2 essentially serves as the semantic codebook that embeds the semantic information. The other codebook C1 functions as the visual codebook that captures the visual details, complementing C2. We note that semantic is still not well-defined concept in the community. As studied in the multimodal domain [27], pre-trained vision models place varying emphasis on the semantic property. For instance, CLIP [21], which is pre-trained for cross-modal alignment, encodes high-level semantic features, while DINOv2 [17], selfsupervised vision model, captures mid-level visual features. Incorporating diverse vision models into the factorized subcodebooks establishes hierarchy of semantics: low-level structures (e.g., edges), mid-level details (e.g., textures), and high-level concepts (e.g., abstract appearance). The total loss is weighted sum of all the losses: Ltotal = LVQGAN + λ1Ldisentangle + λ2Lrep, (8) where λ1 and λ2 are weights. In this paper, we present two variants of the implementation of FQGAN, including = 2 (FQGAN-Dual) and = 3 (FQGAN-Triple). FQGANDual employs CLIP [20] as the pre-trained vision model to provide semantic features for the representation learning objective. For FQGAN-Triple, CLIP [20] and DINOv2 [17] are jointly adopted to form semantic hierarchy. 3.3. Auto-Regressive Model , z2 , . . . , zk The factorized quantization design produces multiple subtokens for each spatial position, represented as Zt = (z1 ), where denotes the time step. Standard AR transformers, such as those in VQGAN [6] and LlamaGen [25], predict only the index of the next token based on the hidden feature gt, which makes them inherently unsuitable for handling factorized sub-tokens. One simple solution is to apply classifiers to the hidden feature gt, yielding the indices for the sub-tokens as = clsi(gt), {1, . . . , k}. However, this method is zi shown to be suboptimal (see Tab. 4). To address this, we introduce factorized AR head that sequentially predicts the distributions of these factorized sub-tokens, allowing for better modeling of their dependencies. Fig. 2 illustrates the full Factorized Auto-Regressive model (FAR). For each patch, the hidden feature gt serves as prefix condition, which is processed by an additional AR head to autoregressively predict the list of sub-tokens, formulated as , . . . , zi1 zi = headAR(gt; z1 ). Following scaling pattern similar to previous works [15, 25], FAR has Base and Larger versions, differentiated by their parameter sizes. The detailed configurations are provided in the Appendix. , z2 4. Experiment 4.1. Setup In experiments, we follow previous works to use the standard benchmark, ImageNet [4], to train and evalaute the tokenizers and AR generation models. For the factorization configuration, we experiment with = 2 and = 3. λ1 and λ2 are empirically set to 0.1 and 0.5 respectively. The training schedule of the visual tokenizer is adapted from LlamaGen [25]. Specifically, the tokenizer is trained with global batch size of 256 and constant learning rate of 2e-4 across 8 A100 GPUs. For the AR model, we adopt Llama-style [25, 28] transformer architecture as the backbone. To accommodate the factorized codes, the model employs embedding layers on the input side, each embeds separate sub-code, followed by linear layer that aggregates these embeddings into single representation. On the output side, we adapt factorized AR head that predicts the factorized codes for each patch. The AR models are trained for 300 epochs with constant learning rate of 2e-4 and global batch size of 256 across 8 A100 GPUs. Metric. We adopt Frechet inception distance (FID) [8] as the main metric to evaluate visual tokenizers and generation models. For tokenizers, we use the ImageNet validation set, consisting of 50k samples, to compute the reconstruction FID (rFID). Additionally, we use PSNR and Inception Score [24] as auxiliary metrics for comparison. For generation models, we follow the widely adapted ADM [5] evaluation protocol to compute the generation FID (gFID). Besides, Inception Score, Precision, and Recall are also used for comparison, following prior works. In both quantitative and qualitative evaluations, we use classifier-free guidance [9] (CFG), with the weight set to 2.0. We do not use any top-k or top-p sampling strategy unless specified. 4.2. Comparison on Tokenizers We first compare our method with popular visual tokenizers listed in Tab. 1. Our FQGAN model sets new stateof-the-art performance in discretized image reconstruction across various settings, including different codebook sizes and downsample ratios. Compared to VQGAN and its advanced variants, our method outperforms them by large margin. Note our method is also built based on the vectorquantization mechanism. This comparison effectively validates the advantage of our factorized quantization design. Interestingly, compared to the state-of-the-art tokenizer Open-MAGVITv2, which employs an advanced lookupfree quantization mechanism, our method still exhibits superior image reconstruction performance, with 0.41 rFID gap. This result suggests that VQ-based methods still hold great potential for visual tokenization, which may have been overlooked previously. Existing work often regards the codebook as bottleneck, while our approach provides Table 1. Comparisons with other image tokenziers. Reconstruction performance of different tokenizers on 256 256 ImageNet 50k validation set. All models are trained on ImageNet, except on OpenImages and on unknown training data. Bold denotes the best scores; underline denotes the second place. Table 2. Class-conditional generation on 256 256 ImageNet. Models with the suffix -re use rejection sampling. The evaluation protocol and implementation follow ADM [5]. Our model employs cfg-scale of 2.0. Method VQGAN [6] SD-VQGAN [23] RQ-VAE [12] LlamaGen [25] Titok-B [36] VQGAN-LC [41] VQ-KD [30] VILA-U [31] Open-MAGVIT2 [15] FQGAN-Dual FQGAN-Triple SD-VAE [23] SDXL-VAE [19] ViT-VQGAN [33] VQGAN [6] SD-VQGAN [23] OmniTokenizer [29] LlamaGen [25] Open-MAGVIT2 [15] FQGAN-Dual FQGAN-Triple Downsample Codebook Code Dim Ratio Size rFID PSNR 16 16 16 16 16 16 16 16 16 16 8 8 8 8 8 8 8 8 8 16384 16384 16384 16384 4096 100000 8192 16384 262144 16384 2 16384 3 8192 16384 16384 8192 16384 262144 16384 2 16384 3 256 4 256 8 12 8 32 256 1 8 8 4 32 4 4 8 8 1 8 8 4.98 5.15 3.20 2.19 1.70 2.62 3.41 1.80 1.17 0.94 0.76 0.74 0.68 1.28 1.19 1.14 1.11 0.59 0.34 0.32 0.24 20.79 23.80 - - 21.90 22.02 22.73 25.68 26. 23.38 25.45 26.19 26.27 27.58 novel perspective. An explicit codebook offers the opportunity for more sophisticated designs on code embeddings, such as disentanglement and representation learning. Another key finding is the comparison between SDVAE [23], SDXL-VAE [19], and our FQGAN. SD-VAE and SDXL-VAE are advanced continuous visual tokenizers widely used in Stable Diffusion models [18, 19, 22]. We observe that our FQGAN, with 16 downsample ratio, achieves performance comparable to these continuous models, which use an 8 downsample ratio. In fairer comparison, with both methods using an 8 downsample ratio, our method achieves significantly lower reconstruction FID, suggesting that discrete representation in image tokenization is no longer bottleneck for image reconstruction. 4.3. Comparison on Generation Models We compare our FAR model with mainstream image generation models, including diffusion models, LFQ-based AR models, and VQ-based AR models, as shown in Tab. 2. Among VQ-based AR models, we observe that FAR achieves competitive image generation performance. When comparing models with similar parameter sizes, specifically FAR-Base vs. LlamaGen-L and FAR-Large vs. LlamaGenXL, our FAR model consistently achieves superior performance in both FID and Inception Score. This validates the effectiveness of the proposed method. Among the other methods, RQ-Transformer [12] is similar to our method, as Type Model #Para. FID IS Precision Recall Diffusion ADM [5] CDM [10] LDM-4 [23] DiT-XL/2 [18] 554M 10.94 4.88 400M 3.60 675M 2.27 LFQ AR Open-MAGVIT2-B [15] Open-MAGVIT2-L [15] 343M 3.08 804M 2.51 VQGAN [6] VQGAN [6] VQGAN-re [6] ViT-VQGAN [33] ViT-VQGAN-re [33] RQTran. [12] RQTran.-re [12] LlamaGen-L [25] LlamaGen-XL [25] FAR-Base FAR-Large 227M 18.65 1.4B 15.78 5.20 1.4B 4.17 1.7B 3.04 1.7B 7.55 3.8B 3.8B 3.80 343M 3.80 775M 3.39 415M 3.38 898M 3. VQ AR 101.0 158.7 247.7 278.2 258.26 271.70 80.4 74.3 280.3 175.1 227.4 134.0 323.7 248.28 227.08 248.26 272.52 0.69 0.83 0.85 0. 0.78 0.83 0.81 0.81 0.82 0.63 0.57 0.51 0.54 0.26 0.51 0.54 0.54 0.54 it also adopts an additional AR head to accommodate multiple sub-codes at each step. The performance gap between RQ-Transformer and FAR further validates the power of our FQGAN tokenizer and its transferability to the downstream generation model. When comparing FAR with Open-MAGVIT2 [15], which shares similar AR model design, our method exhibits comparable or higher Inception Score, though with slightly worse FID score. The Inception Score suggests that our FQGAN tokenizer has the potential to match LFQ performance, while the FID score gap still demonstrates the superiority of LFQ compared to VQ, as studied in MAGVITv2 [35]. Mitigating the generation performance gap between LFQ and VQ is critical yet challenging problem, which is beyond the scope of this work. FQGAN is crucial step toward this direction as it significantly improves image reconstruction performance, surpassing both VQ and LFQ tokenizers. Tab. 2 also suggests that the improvement on tokenization and reconstruction can be effectively transferred to AR generation. We hope the FQGAN tokenizer will inspire related further research. Qualitative results of the FAR model is shown in Fig. 5. The FAR model in this section is trained with tokens from FQGAN-Dual tokenizer. More training details and settings of the FAR model are provided in the Appendix. 4.4. Ablation Studies Factorized Quantization. We investigate the design components of the FQGAN tokenizer, including the factorized codebook, disentanglement regularization mechanism, and representation learning objective. In this study, we adopt FQGAN-Dual, i.e., = 2. All experiments are conducted for 10 epochs on the ImageNet training set to ensure fair comparison. As shown in Tab. 3, we start with Table 3. Ablation study on different components of the proposed factorized quantization, using the FQGAN-Dual variant. Model Codebook Size Dis. Rep. Regular. Learn. rFID IS PSNR Usage VQGAN FQGAN 16384 32768 16384 2 16384 2 16384 2 16384 2 3.71 3.60 2.00 1.84 1.73 1.66 50.05 50.60 54.72 55.04 55.00 55.21 20.56 20.56 22.21 22.04 21.61 21. 98% 84% 97% 98% 98% 98% vanilla VQGAN tokenizer. Increasing the codebook size from 16, 384 to 32, 768 results in drop in codebook usage, yielding only marginal performance gains even with double codebook size. Previous studies [25] have shown that with training schedule of more epochs, the 32, 768 version ultimately performs worse than the 16, 384 version. Next, we consider vanilla factorized codebook design, which splits the single 32, 768 codebook into two 16, 384 subcodebooks. Such factorization brings significant performance gain, as reflected in the rFID score change from 3.60 to 2.00. Compared to single codebook with the same number of codes (i.e., capacity), the factorized design greatly reduces lookup complexity. It also yields more diverse code combinations, improving performance by large margin. Disentanglement and Representation Learning. Next, we gradually incorporate the proposed additional designs into the factorized codebooks. By making only one change in each experiment, we find that both the disentanglement regularization and the representation learning objective lead to better reconstruction results. When applied together, the two designs achieve even better performance. We attribute this performance gain to the fact that disentanglement regularization forces the factorized codes to learn more diverse aspects of the image, while the representation learning objective explicitly incorporates semantic information. It is worth mentioning that an rFID = 2.0 for the vanilla factorization version is already very strong result, rarely achieved by previous VQ tokenizers. Pushing the performance further is particularly challenging, which effectively demonstrates the strength of the proposed designs. What has each sub-codebook learned? To better understand the underlying behavior of the factorized subcodebooks, we provide comprehensive visualization. Fig. 3 demonstrates the reconstruction results, including standard reconstruction and reconstruction using only single sub-codebook, achieved by setting the rest of the code embeddings to zero. In the two sub-codebooks of FQGANDual, we observe that sub-codebook 1 highlights low-level features, such as essential structures, shapes, and edges of Figure 3. Visualization of standard reconstruction by FQGANDual and reconstruction using only single sub-codebook. Figure 4. T-SNE visualization of VQ codes from different subcodebooks in FQGAN-Dual. the image. Sub-codebook 2, jointly supervised by CLIP features, presents high-level abstract version of the original image, where colors are blurred together, and textures are preserved in softened manner. When factorizing further into three sub-codebooks, i.e., FQGAN-Triple, we observe that sub-codebook 1 still emphasizes the low-level strong edges and overall shape. Sub-codebook 2, jointly supervised by DINO features, highlights textural elements, preserving surface patterns and fine details without clear structural outlines, representing mid-level features. Finally, subcodebook 3 concentrates on higher-level appearance and produces an abstract or blurry version of the original image. This visualization suggests that the factorized subcodebooks are indeed tasked with capturing different aspects of the image. With the supervision of representation learning, the sub-codebooks naturally form complementary hierarchical levels of visual features. Furthermore, we illustrate the distribution of VQ codes from different sub-codebooks. Following previous practice [30], we randomly sample four classes from the ImageNet dataset, encode them with our tokenizer, and visualize the distribution using the t-SNE technique. The left part of Fig. 4 shows that VQ codes from sub-codebook 1, without additional regularization, are distributed in an unordered manner in the space. This is likely because this sub-codebook is solely trained for reconstruction, capturing high-variance detail while lacking awareness of semantic categories. In contrast, the right part suggests that the CLIPsupervised sub-codebook 2 exhibits better semantic awareness, as its codes from the same category are distributed within cluster. The two visualizations effectively demonstrate what each sub-codebook has learned qualitatively. We provide more visualizations in the Appendix. Table 4. Ablation study on the generation model head design with the proposed FQGAN tokenizer. We use FAR-Large model with cfg-scale=1.75 in this study. Generation Model Head Top-k Sampling gFID Linear Classifiers MLP Classifiers Factorized AR Head 4096 8192 4096 8192 4096 8192 5.19 6. 5.59 8.88 4.37 3.74 Effect of AR Head. Adapting the FQGAN tokenizer to auto-regressive visual generation models presents the challenge of handling multiple sub-codes at each step. This is crucial, as predicting wrong sub-code at specific position can invalidate the entire patch. We present this investigation in Tab 4. We begin with simple solution that employs independent linear classifier heads to decode the hidden embedding of the AR backbone into their respective subcodebooks in parallel. This strategy yields decent results but lags behind auto-regressive models with the same parameter level. We hypothesize that this is due to the parallel decoding scheme placing too heavy burden on the classifier. Therefore, we attempt to increase the capacity of the classifier by using multiple layers with non-linear activation function in between. However, as shown in the table, the MLP version performs even worse, suggesting that simply increasing the capacity and computation is not the key to addressing this issue. In factorized auto-regressive generation, the key issue is that the mismatch between sub-codes within position (patch) can significantly affect the results. This suggests that an effective design is module that not only decodes from the AR backbone but also models the dependency between sub-codes. To this end, we explore using an additional auto-regressive head to decode the factorized subcodes. The last row of Tab. 4 shows that this design can improve performance by considerable margin. For example, when decoding code z2 , the vanilla classifier or MLP version only references the hidden embedding gt output by the AR backbone, whereas the AR module allows the decoding process to also attend to code z1 , strengthening the dependency among sub-codes of the current patch and improving overall generation quality. 5. Discussion and Future Work In this work, we design factorized quantization method and explore dual and triple sub-codebooks. Future research on factorizing more sub-codebooks could be promising direction. Secondly, since the sub-codebook is jointly supervised by strong vision models, such as CLIP, it is inFigure 5. Qualitative examples generated by our FAR model. teresting to probe its performance on multimodal understanding tasks. We provide preliminary exploration in the Appendix. In the long term, building truly unified tokenizer that excels at both generation and understanding tasks would be beneficial to the community. We believe the factorized design is promising direction toward this ultimate goal, as it entails various levels of visual information. Regarding limitations, as discussed in Sec. 4.3, our method outperforms previous VQ-based methods in both reconstruction and generation. However, in downstream generation, our model still lags behind LFQ-based methods in generation FID metric. Our work, with strong reconstruction performance, serves as an initial step toward bridging the gap between VQ and LFQ. We hope this work inspires future research to push the boundary further. 6. Conclusion We focus on critical limitation of current VQ tokenizers: their difficulty in handling large codebooks. To address this, we propose novel factorized quantization approach that decomposes large codebook into multiple independent sub-codebooks. To facilitate learning of the sub-codebooks, we design disentanglement regularization mechanism that reduces redundancy while promoting diversity. Additionally, we introduce representation learning objective that explicitly guides the model to learn meaningful semantic features. The proposed visual tokenizer, FQGAN, effectively handles large codebooks and achieves state-of-theart performance in discrete image reconstruction, surpassing both VQ and LFQ methods. Experimental results show that this tokenizer can be integrated into auto-regressive image generation models by adding factorized AR head, demonstrating competitive image generation performance. Besides, we provide an in-depth analysis to unveil how the factorized codebooks function. Finally, we discuss several limitations to inspire future works."
        },
        {
            "title": "References",
            "content": "[1] Randall Balestriero and Yann LeCun. Learning by reconstruction produces uninformative features for perception. arXiv preprint arXiv:2402.11337, 2024. 2, 4 [2] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 1 [3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In CVPR, pages 1130511315, 2022. 1 [4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. 5 [5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 34:87808794, 2021. 5, [6] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 1287312883, 2021. 1, 2, 3, 5, 6 [7] Yuchao Gu, Xintao Wang, Yixiao Ge, Ying Shan, and Mike Zheng Shou. Rethinking the objectives of vectorquantized tokenizers for image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 76317640, 2024. 3 [8] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. 5 [9] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [10] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. 23(1):22492281, 2022. 6 [11] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Image-to-image translation with conditional adverEfros. sarial networks. In CVPR, pages 59675976, 2017. 3 [12] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, pages 1151311522, 2022. 1, 2, 3, 6 [13] Guotao Liang, Baoquan Zhang, Yaowei Wang, Xutao Li, Yunming Ye, Huaibin Wang, Chuyao Luo, Kola Ye, et al. Lg-vq: Language-guided codebook learning. arXiv preprint arXiv:2405.14206, 2024. 3 [14] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2 [15] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation, 2024. 1, 2, 3, 5, 6 [16] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023. 1, 2 [17] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. 2, [18] William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, pages 41954205, 2023. 6 [19] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. 6 [20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 5 [21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 2, 5 [22] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. pages 88218831, 2021. 6 [23] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 6 [24] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. 5 [25] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1, 2, 3, 5, 6, [26] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable imarXiv preprint age generation via next-scale prediction. arXiv:2404.02905, 2024. 3 [27] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 5 [28] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut [39] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for highfidelity image generation. Advances in Neural Information Processing Systems, 35:2341223425, 2022. 2 [40] Lei Zhu, Fangyun Wei, and Yanye Lu. Beyond text: Frozen In large language models in visual signal comprehension. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2704727057, 2024. 3 [41] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vqgan to 100,000 with utilization rate of 99%. arXiv preprint arXiv:2406.11837, 2024. 1, 2, 3, 6 Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3, [29] Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. arXiv preprint arXiv:2406.09399, 2024. 6 [30] Luting Wang, Yang Zhao, Zijian Zhang, Jiashi Feng, Si Image understanding makes for arXiv preprint Liu, and Bingyi Kang. good tokenizer for image generation. arXiv:2411.04406, 2024. 3, 6, 7 [31] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 3, 6 [32] Tackgeun You, Saehoon Kim, Chiheon Kim, Doyup Lee, and Bohyung Han. Locally hierarchical auto-regressive modeling for image generation. Advances in Neural Information Processing Systems, 35:1636016372, 2022. 2 [33] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In ICLR, 2022. 1, 2, [34] Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms. Advances in Neural Information Processing Systems, 36, 2024. 3 [35] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In ICLR, 2024. 1, 2, 6 [36] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. arXiv preprint arXiv:2406.07550, 2024. 6 [37] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 4 [38] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, pages 586 595, 2018."
        }
    ],
    "affiliations": [
        "Amazon",
        "Fudan University",
        "Show Lab, National University of Singapore"
    ]
}