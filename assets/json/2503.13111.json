{
    "paper_title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs",
    "authors": [
        "Erik Daxberger",
        "Nina Wenzel",
        "David Griffiths",
        "Haiming Gang",
        "Justin Lazarow",
        "Gefen Kohavi",
        "Kai Kang",
        "Marcin Eichner",
        "Yinfei Yang",
        "Afshin Dehghan",
        "Peter Grasch"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage large-scale high-quality 3D scene data with open-set annotations to introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data covers diverse spatial tasks including spatial relationship prediction, metric size and distance estimation, and 3D grounding. We show that CA-VQA enables us to train MM-Spatial, a strong generalist MLLM that also achieves state-of-the-art performance on 3D spatial understanding benchmarks, including our own. We show how incorporating metric depth and multi-view inputs (provided in CA-VQA) can further improve 3D understanding, and demonstrate that data alone allows our model to achieve depth perception capabilities comparable to dedicated monocular depth estimation models. We will publish our SFT dataset and benchmark."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 1 1 1 3 1 . 3 0 5 2 : r MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs"
        },
        {
            "title": "Erik Daxberger",
            "content": "Nina Wenzel David Griffiths"
        },
        {
            "title": "Apple",
            "content": "Figure 1. (Left) We generate the Cubify Anything VQA (CA-VQA) dataset and benchmark, covering various 1) input signals: single image, metric depth (sensor-based and estimated), multi-frame/-view, and 2) spatial understanding tasks: e.g., relationship prediction, metric estimation, 3D grounding. (Right) We train MM-Spatial, generalist multimodal LLM that excels at 3D spatial understanding. It supports Chain-of-Thought spatial reasoning involving 2D grounding and depth estimation, and can also leverage depth input via tool-use."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage largescale high-quality 3D scene data with open-set annotations to introduce 1) novel supervised fine-tuning dataset and 2) new evaluation benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data covers diverse tasks including spatial relationship prediction, spatial metric size and distance estimation, and 3D grounding. We show that CA-VQA enables us to train MM-Spatial, strong generalist MLLM that also achieves state-of-the-art performance on 3D spatial understanding benchmarks, including our own. We show how incorporating metric depth and multi-view inputs (provided in CA-VQA) can further improve 3D understanding, and demonstrate that data alone allows our model to achieve depth perception capabilities comparable to dedicated monocular depth estimation models. We will publish our SFT dataset and benchmark. Understanding object locations and spatial relationships in both 2D and 3D space is crucial for interpreting complex visual scenes. While multimodal large language models (MLLMs) have achieved notable success for 2D visual tasks including referring and grounding [122, 127] and spatial relation prediction (e.g., left vs. right, above vs. below) [27], they still struggle with 3D object perception tasks such as estimating 1) relative depth (in front vs. behind), 2) object distances or sizes in metric units (A is 2.74m away / 1.32m wide.), and, ultimately, 3) precise 3D bounding boxes. Yet, the ability to reason about objects in 3D scenes is not only part of general visual comprehension, but is also foundational in domains like robotics and AR / VR, e.g., for navigation and manipulation tasks [83]. There have been comparatively few works on 3D object perception with MLLMs [15, 20, 27, 28, 32, 98]; moreover, they only consider subset of tasks, and do not comprehensively assess depth and multi-view inputs. To address these Equal contribution. E-Mail: edaxberger@<company>.com 1 Dataset Data source(s) Training Datasets OpenSpatialDataset [27] SpatialQA-E [15] OpenSpaces [5] Spatial Aptitude Training [92] RoboSpatial [98] EmbSpatial [32] SpatialQA [15] Spatial-VQA [20] LV3D [28] Evaluation Benchmarks SpatialRGPT-Bench [27] CV-Bench [104] 3DSRBench [82] VSI-Bench [116] Q-Spatial [73] ScanRefer [21] Nr3D / Sr3D [3] SpatialBench [15] Rel3D [42] OpenImages [56] Robot manipulation images [15] The Cauldron [60] ProcTHOR-10K [30] Multiple 3D datasets [18, 29, 35, 107109] Multiple 3D datasets [18, 29, 55] Multiple image datasets Web-crawled images Multiple datasets Omni3D [13] ADE20k [136], COCO [77], Omni3D [13] COCO [77], HSSD [54] ScanNet/++ [29, 121], ARKitScenes [11] ScanNet [29] ScanNet [29] ScanNet [29] subset is from MME [37] ShapeNet [19], YCB [16] CA-VQA (ours) CA-1M [61] / ARKitScenes [11] High-quality 3D Ground-truth Depth maps Sensor Monoc. Multi-view images Tasks Splits Relation Metric 3D Ground. Train Eval Public synthetic subset subset subset synthetic Table 1. 3D Spatial Dataset Overview. Comparison of object-centric 3D spatial MLLM datasets to CA-VQA (in gray: non-public ones). CA-VQA is the first dataset that is based on high-quality 3D ground truth, includes depth maps (both from sensors and monocular) and multi-view images, covers variety of tasks (relationships, metric estimation, 3D grounding), and has both an SFT dataset and benchmark. limitations and facilitate more holistic exploration of 3D understanding in MLLMs, we make these contributions: 1. We propose new data generation pipeline that leverages high-quality 3D scene data to produce image-text QA pairs for 3D object perception. We apply this pipeline to CA-1M [61] to generate Cubify Anything VQA (CAVQA), new spatial understanding dataset for MLLM fine-tuning, covering diverse indoor scenes. As additional inputs, CA-VQA uniquely includes multi-view images and different types of metric depth maps, both sensor-based and SOTA monocular (estimated) depth. 2. We release new spatial understanding benchmark derived from CA-VQA. Compared to existing benchmarks, ours 1) includes diverse tasks (incl. relative and metric distance / size estimation and 3D grounding), 2) provides rich input signals (multi-view and depth), and 3) is less susceptible to language priors (i.e., more vision-reliant) and hence more challenging. We show that even SOTA models such as GPT-4o struggle on our benchmark. 3. We run extensive experiments illustrating the benefits of CA-VQA as testbed for spatial perception research. We show that 1) we can train MM-Spatial, generalist MLLM achieving SOTA on spatial understanding benchmarks (CV-Bench, SpatialRGPT-Bench, CA-VQA), while retaining performance on other tasks (incl. general, knowledge, text-rich); 2) using multi-view and depth inputs further enhances 3D understanding; 3) MLLMs can achieve strong monocular depth estimation via SFT. We also study the efficacy of different depth maps, the impact of full encoding vs. tool-use for leveraging depth, and indoor-to-outdoor scene generalization. 2. Related Work 2.1. General MLLMs MLLMs [49, 50, 64, 86, 103] have attracted significant research focus, tracing back to Frozen [106] and Flamingo [6, 8], with more recent works such as LLaVA [80] and MiniGPT-4 [140] introducing visual instruction tuning. The rise of open-source MLLMs has led to models rivaling SOTA commercial offerings like GPT-4o on certain tasks. Notable examples include Emu2 [100, 101], VILA [76], Idefics2/3 [59, 60], and Qwen2-VL [10], among others. MLLMs research has explored several fronts: (i) scaling up the pre-training data [9, 68, 76, 85, 115] and supervised fine-tuning data [48, 60, 102, 104]; (ii) enhancing highresolution image comprehension [24, 31, 40, 41, 70, 71, 78, 81, 114, 131]; (iii) studying various vision encoders [22, 36, 96, 105] and vision-language connectors [14, 17, 69, 118]; (iv) using mixture-of-experts [66, 74]; (v) extending models to region-level [23, 88, 110, 122, 125, 127, 130, 133] and pixel-level [58, 91, 94, 124] understanding, multiimage reasoning [52, 65], UI comprehension [45, 72, 123], and video understanding [75, 112, 113, 120], among others. 2.2. 3D Spatial Understanding with MLLMs To complement work on (primarily) 2D spatial relationships / reasoning [4, 53, 79, 84, 87, 90, 97, 99, 117, 129, 132], recent research has aimed to also enable 3D reasoning with MLLMs, roughly split into two directions. Firstly, works focusing on scene-level 3D understanding (i.e., scene captioning and VQA) by enabling MLLMs to process representations of entire scenes, often leveraging multiple views and 2 Figure 2. CA-VQA Data Example. Example of single sample from our dataset. Each reference frame has between 0-4 multi-view support frames. All frames (reference and support) come with three metric depth maps: Ground truth (FARO laser), ARKit Depth (LiDARfused) and Monocular (DepthPro). Each support frame contains the relative pose from the reference image, along with camera intrinsics. depth information. This includes ScanReason [138], 3DCLR [46], 3D-LLM [47], ConceptGraphs [43], LLaVA-3D [139], Scene-LLM [38], M3DBench [67], Video-3D LLM [134], LSceneLLM [135], and 3DGraphLLM [126]. Secondly, works focusing on object-level 3D spatial perception. Spatial-VLM [20] and Cube-LLM [28] both use vanilla image-based VLMs (without any explicit 3D input) to address spatial relationship and metric distance estimation (Spatial-VLM) as well as 3D grounding (Cube-LLM). SpatialRGPT [27] and VCoder [51] encode (relative) depth maps as additional inputs via the image encoder plus dedicated depth connector. SpatialBot [15] instead leverages depth maps via tool-use [95] by training the model to query the depth value at given coordinate. In this work we build on these ideas, and further compare the utility of depth maps collected with dedicated specialized hardware to those derived from monocular depth estimators. We also study the benefits of providing additional views (images) to the model, i.e., frames preceding the main image in the video. We provide an overview of previous SFT datasets and benchmarks for object-centric 3D spatial understanding in Tab. 1, highlighting the novelty and unique characteristics of our proposed CA-VQA dataset, to be detailed next. 3. Data We build upon the Cubify Anything 1M (CA-1M) [61] dataset, which contains exhaustive 3D bounding boxes (gravity-aligned 7-DOF boxes with yaw orientation) for every object in the ARKitScenes [11] dataset. Additionally, we provide human-labeled annotations for each object consisting of an open-set label (3.3k unique noun labels for 350k objects), material, primary color, and shape. 3.1. Data Generation Pipeline We generate QA pairs from these annotations as follows: Frame Sub-sampling. To reduce the data volume and redundancy, we sub-sample the videos at 1 FPS for the training set and at 0.1 FPS for the evaluation benchmark. 3D Ground Truth Processing. For each frame, we transform the 3D boxes from world to camera space using pose Rti. In contrast to CA-1M, 1) we include all boxes visible from the current view, irrespective of distance; 2) we do not clip boxes to the visible part, but rather store amodal 3D coordinates. We also construct point cloud based on the ground truth depth map and camera intrinsics. QA Pair Generation. Based on our 3D and semantic annotations, we automatically generate template-based QA pairs (both open-ended and multi-choice ones), without any human supervision. We consider variety of spatial task categories, detailed in Sec. 3.2 and App. A. We further ensure that questions are unambiguous. For example, asking What is the distance to the chair?, is only valid question is there is single instance of chair. Blind Filtering. [25] found that many samples of multimodal benchmarks can be solved without vision input due to the strong language priors of MLLMs. To reduce such bias we follow [25] and remove all benchmark examples which are correctly answered blindly by at least three out of seven judges: GPT-4 [2], GPT-4V [86], GPT-4o [50], Phi-3-Vision-4B [1], LLaVA-OneVision-7B [63], SpatialRGPT-VILA1.5-8B [27], and our MM-Spatial-3B. App. demonstrates the effectiveness of this strategy. Overall, we obtain 10M QA pairs over 220K frames from 2K videos for the CA-VQA training set, and 62K QA pairs over 2.6K frames from 265 videos for the evaluation benchmark. Fig. 5 shows QA examples from CA-VQA. 3.2. Spatial Task Categories CA-VQA covers the spatial task categories outlined below. App. provides further details on the QA definitions. Counting (How many are there?). Answers are computed by counting the 3D boxes of the given object class. Viewpoint-dependent (Is behind Y?). Answers are based on the 2D / 3D boxes and depend on the camera pose. Metric regression (How far away is from / the camera? How wide / tall is X?) Answers to size questions 3 are computed using the 3D bounding box. Answers to distance questions are computed based on the object point clouds; we reject samples for which the 3D boxes overlap. 2D referring/grounding. We use 2D bounding boxes computed by projecting the 3D bounding boxes to image space. 3D referring/grounding. We use the 3D boxes in CA-1M. Binary (e.g., Is taller than Y?). This covers viewpointdependent and (relative) regression questions, as well as object presence questions (Is present in the image?). Multi-choice. We also formulate multi-choice QAs covering the other categories (except for 2D / 3D grounding). External benchmark templates. We also generate examples using the QA templates proposed in CV-Bench [104] and SpatialRGPT-Bench [27]. This removes potential instruction following issues due to differences in QA formulations, hence allowing us to faithfully evaluate our models actual spatial understanding ability on those benchmarks. 3.3. Multi-view and Metric Depth Data Fig. 2 visualizes the multi-view images and depth maps. Multi-view. For each reference frame It, we sample 4 preceding support frames It1,...,tN , which are triggered when camera pose Rti has angular movement of 15 or movement of 30cm from the current key frame Rtb. Metric Depth. For each frame (ref. & support), we provide: Ground truth depth acquired from high-precision stationary FARO laser scanner, and rendered to each frame using the Barrabandi pipeline from ARKitScenes [11]. ARKit Depth provided by the ARKit framework. It utilizes the iPad Pros on-device sparse LiDAR sensor and color image to produce per-pixel dense depth map. Monocular depth generated using DepthPro [12], state-of-the-art monocular metric depth estimation model. Depth: Chain-of-Thought (CoT) / Tool-Use. Sec. 5 will explore different ways to use the metric depth information. As an alternative to encoding the full 2D depth maps with the model (see Sec. 4.1), we investigate simpler approach that uses the individual depth values of the given objects as text. To this end, we prepare step-by-step examples involving textual GT depth in the format illustrated in Fig. 3. At test time, the depth values are then obtained either via 1) tool-use [15, 95] (see Fig. 3), or 2) model prediction (CoT). 4. Model 4.1. Model Architecture We use the MM1.5 architecture [85, 128] (focusing on the mobile-friendly 3B variant), comprising of DFN-CLIP [34, 89] image encoder and decoder-only LLM backbone, which are bridged via C-Abstractor [17]. We use an image resolution of 672672 during fine-tuning, and further increase the effective resolution by using (static) image splitting [78] with 4 sub-image splits (plus an overview image). Figure 3. Example of leveraging depth maps via tool-use. The model predicts the objects 2D bounding boxes and function calls, receives the tool outputs (which is the median depth value within the box, marked with an ), and finally reasons about the answer. We also consider variants of our model that incorporate either multiple views or depth maps as additional inputs: Multi-view. Our model supports multi-image input, allowing us to concatenate multiple views into sequences ItN , ..., It1, It. In this multi-view setting, we only apply image splitting to the reference (final) image It. Depth: Full Encoding. We use the image encoder to encode the normalized and colorized depth maps (i.e., replicating the depth map along the channel dimension), and introduce separate depth connector, following SpatialRGPT [27]. Notably, this approach is limited to using relative (normalized) depth. We also explore using textual metric depth in purely data-driven way via Chainof-Thought (CoT) or tool-use, see Secs. 3.3 and 5.1. 4.2. Data and Training We follow the 1) pre-training and 2) continual pre-training stages of MM1.5 [128]. For the 3) supervised fine-tuning (SFT) stage, we start from the MM1.5 single-image SFT mixture, which includes datasets across multiple categories: General VQA, Knowledge (math, code, science), Text-rich, and 2D Referring & Grounding (VQA enriched with bounding boxes). We then add our CV-VQA data within new Spatial category and select the mixture ratio based on the ablations discussed in App. B. We use the same training hyperparameters as MM1.5 [128], with unfrozen image encoder and LLM. We use AXLearn [7] for model training. 5. Experiments 5.1. Model Variants We explore the following model variants in our study, leveraging the various input signals provided within CA-VQA: MM-Spatial. Trained on single-view RGB inputs, without depth information. This is our baseline model. 4 Figure 4. Qualitative Example. We show the predictions of various models on challenging example from our CA-VQA benchmark. Strong commercial (2a&b) and research models (2c&d) fail. MM-Spatial (1a) is much better, and even more so with CoT enabled (1b), demonstrating our models strong object grounding (see predicted 2D boxes in the image), depth estimation, and spatial reasoning ability. Accuracy improves further when leveraging ground-truth depth via tool-use (1c), although our CoT models (1b) predictions are very close to that, for both the intermediate depth values and final answer; monocular estimated depth (1d) is less accurate and yields worse result. Benchmark Category Averages Model Spatial General Knowl. Text-rich Ref./Ground Avg. MM1.5-3B [128] MM-Spatial-3B 39.9 70.1 64.7 65.0 46.2 46. 62.1 62.1 77.7 79.1 58.1 64.5 Table 2. Benchmark Category Results MM-Spatial is generalist MLLM that improves strongly on the Spatial category while rivaling the MM1.5 baseline across the other task categories. MM-Spatial + Multi-view. Trained on multi-view RGB inputs as described in Sec. 4.1. We use up to four support frames plus one reference frame. We additionally provide the camera intrinsics and pose information for each view (relative to the reference view) as JSON strings to the model (see Fig. 2), interleaved with the images. MM-Spatial + Depth (Tool). Trained on single-view RGB plus textual metric depth, with tool-use at test time, as described in Sec. 3.3 and Fig. 3. This approach relies solely on data, using the same model as MM-Spatial. We denote the depth source as (Tool; GT) for ground truth / FARO depth or (Tool; Mon.) for monocular depth. MM-Spatial + CoT. Trained like MM-Spatial + Depth (Tool), but without using the depth tool at test time. Instead, the model predicts the metric depth values on its own based solely on the image input, producing Chainof-Thought (CoT) [111] style response as in Figs. 1 and 3. MM-Spatial + Depth (Encoded). Trained on singleview RGB inputs plus fully encoded depth maps, as described in Sec. 4.1. The depth source is denoted as above. MM-Spatial (Blind eval). Trained like MM-Spatial, but evaluated with text input only (i.e., without image input). Some variants naturally lend themselves to combinations, e.g., MM-Spatial + Multi-view + CoT is model that sees multiple views and responds with CoT style answers. 5.2. Overview of Benchmark Category Results We aim to train generalist MLLM that excels across variety of tasks instead of specialist model that only excels at spatial reasoning. To this end, we follow MM1.5 [128] and evaluate our models across 24 multimodal benchmarks using an internal fork of lm-eval-harness [39], covering the categories outlined in Sec. 4.2. To evaluate 2D and 3D Spatial understanding we use CV-Bench [104], SpatialRGPTBench [27], and our proposed CA-VQA benchmark. Tab. 2 shows results on aggregated metrics across the various benchmark categories. MM-Spatial significantly improves on the Spatial category while maintaining performance competitive with MM1.5 across the other categories, suggesting that spatial reasoning can be improved without meaningful compromise. See App. for additional discussion. We now present an in-depth analysis of the Spatial results, comparing MM-Spatial with SOTA baselines. We use the full data mixture in Sec. 4.2 by default; some ablations use Specialist Models trained only on CA-VQA. 5.3. Results on our CA-VQA Benchmark We assess the model variants outlined in Sec. 4.1, and also study the metric depth estimation ability of our CoT model. CA-VQA results are shown in Tab. 3; qualitative example is shown in Fig. 4. We make the following observations: MM-Spatial vs. Baselines. MM-Spatial-3B 8 substantially outperforms various (much larger) top opensource and commercial models 1 - 6 , incl. the SOTA GPT-4o model 3 , demonstrating 1) their limitations in terms of spatial understanding and 2) the effectiveness of SFT on CA-VQA. Despite its focus on spatial reasoning, SpatialRGPT-VILA-1.5-8B 6 underperforms, likely for few reasons: 1) their OpenSpatialDataset (OSD) used for SFT leverages axis-aligned 3D boxes pseudo-annotated on OpenImages [56], resulting in sig5 Model Binary Count. Grounding 2D 3D Multi-c. 1 2 3 GPT-4 [2] (gpt-4-0613) GPT-4V [86] (gpt-4-turbo-2024-04-09) GPT-4o [50] (gpt-4o-2024-08-06) Phi-3-Vision-4B [1] LLaVA-OneVision-7B [65] SpatialRGPT-VILA1.5-8B [27] 4 5 6 7 MM1.5-3B [128] 8 MM-Spatial-3B 9 MM-Spatial-3B + CoT 10 MM-Spatial-3B + Depth (Tool; Mon.) 11 MM-Spatial-3B + Multi-view + CoT 12 MM-Spatial-3B + Multi-view + Depth (Tool; GT) Specialist Models 13 MM-Spatial-3B 14 MM-Spatial-3B + CoT 15 MM-Spatial-3B + Depth (Tool; Mon.) 16 MM-Spatial-3B + Depth (Tool; GT) 17 MM-Spatial-3B + Depth (Encoded; GT) 18 MM-Spatial-3B + Depth (Encoded; GT) + CoT 19 MM-Spatial-3B + Multi-view 20 MM-Spatial-3B + Multi-view + CoT 21 MM-Spatial-3B + Multi-view + Depth (Tool; Mon.) 22 MM-Spatial-3B + Multi-view + Depth (Tool; GT) 23 MM-Spatial-3B (Blind eval) Acc 9.6 39.2 44.2 52.3 52.0 53.6 59.1 68.8 69.6 69.6 69.2 69.2 69.6 70.1 70.1 70.1 69.5 69.8 71.5 71.1 71.1 71.1 34. Acc AP@50 AP@15 8.5 63.3 69.0 45.7 62.1 68.8 9.1 75.8 75.9 75.9 76.1 76.1 73.3 73.3 73.3 73.3 73.1 73.5 74.1 73.8 73.8 73.8 60.8 0.0 5.8 0.0 7.8 16.1 5.5 32.6 53.2 54.5 54.5 55.0 55.0 54.7 55.8 55.8 55.8 55.6 55.3 56.2 57.2 57.2 57.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 20.7 21.9 21.9 23.6 23. 24.0 25.1 25.1 25.1 24.2 24.4 26.8 27.5 27.5 27.5 0.0 Acc 9.6 32.9 36.6 32.2 42.5 37.2 38.6 74.2 74.7 74.7 75.3 75.3 77.4 77.7 77.7 77.7 78.3 77.7 77.9 78.9 78.9 78.9 60.7 Regression (Metric Estimation) Ego-Dist. Obj.-Dist. Obj.-Size Average Acc @ 10% Relative Error (ℓ1) 6.2 11.4 11.7 6.6 9.3 10.5 0.6 40.0 46.0 40.9 46.1 65.8 47.3 49.5 42.1 74.0 48.3 51.4 52.4 55.2 42.1 73.1 10.1 6.2 9.3 10.0 4.4 8.1 8.7 2.2 18.7 23.2 23.8 24.0 27.2 24.4 27.9 26.1 32.4 25.4 27.6 26.2 29.7 26.7 33.0 8. 5.8 10.1 11.0 6.1 6.4 7.0 3.4 24.4 26.7 26.6 28.2 27.3 24.3 26.7 26.1 27.4 24.5 26.5 26.1 28.6 26.7 28.1 17.9 5.7 21.5 22.8 19.4 24.6 23.9 18.2 47.0 49.1 48.5 49.7 52.4 49.4 50.8 49.5 54.5 49.9 50.8 51.4 52.7 50.5 55.3 24.0 Table 3. CA-VQA Results. MM-Spatial-3B significantly outperforms (much larger) top open-source and commercial models across all tasks, demonstrating its strong spatial understanding ability. Model performance is further improved by incorporating multi-view and/or depth as additional input signals, as well as by leveraging CoT, which relies on our models ability to accurately estimate metric depth. nificant discrepancies in spatial concept definitions (especially for metric quantities) compared to the high-quality (yaw-)oriented 3D boxes used for CA-VQA; see App. for further discussion on this difference; 2) in OSD, objects are referred to via segmentation masks or 2D bounding boxes (How tall is Region [0] <mask/box>?), while CA-VQA simply uses class names (How tall is the chair?); 3) SpatialRGPT is limited to using relative depth due to its full depth encoding approach. Finally, SpatialRGPT / OSD lacks support for 3D grounding. Blind vs. Vision evaluation. To validate our blind filtering strategy (see Sec. 3.1) we note that GPT-4 1 performs poorly, whereas its vision counterpart, GPT-4V 2 , performs better. MM-Spatial (Blind) 23 , which is trained on similar data, also performs poorly on most tasks. While the performance on Counting and Multi-choice is still acceptable likely due to inherent remaining biases such as the naturally skewed distribution of object counts providing vision input 13 still further improves performance by 15 points. App. provides further detailed analysis of the effectiveness of our blind filtering strategy. Overall, our results suggest that our benchmark is less susceptible to strong language prior compared to, e.g., SpatialRGPT-Bench (see Sec. 5.5). Multi-view vs. Single-view. Multi-view is consistently better (e.g. 19 vs. 13 ), suggesting that our model can successfully use additional views to improve 3D perception. Multi-view 19 vs. Single-view + Depth (Tool; GT) 16 . While multi-view is competitive overall, on Regression using GT depth is much better. This suggests that using multiple views can partially compensate if depth sensors are not available, but on tasks that most directly rely on accurate depth (i.e., Regression), GT depth is unmatched. CoT vs. Direct Prediction. CoT prediction consistently improves over direct prediction (e.g. 9 vs. 8 ), suggesting that the additional multi-step supervision signal (incl. 2D object grounding and depth prediction) and/or leveraging more test-time compute benefits model accuracy. Depth (GT): Tool-use vs. Full Encoding. Full depth encoding 17 performs much worse than tool-use 16 , and is only slightly better than the RGB-only baseline 13 . Firsty, this highlights the effectiveness of the simple tooluse approach in utilizing metric depth (while full encoding can only use relative depth). Secondly, this indicates the difficulty of effectively encoding and interpreting full depth maps with an MLLM the simple architecture proposed by [27] may be too limited in this regard, suggesting that further research is required in this direction. Depth (Tool): Ground Truth vs. Monocular. Despite DepthPro being strong monocular depth estimator, its 6 Model 2 3 GPT-4V [86, 104] LLaVA-NeXT-8B [81, 104] Cambrian-1-8B [104] Phantom-7B [62] 4 LLaVA-1.5-13B + SAT Dyn. [92] 5 LLaVA-NeXT-34B [81, 104] 6 7 Mini-Gemini-HD-34B [70, 104] 8 9 MM1.5-3B [128] 10 MM-Spatial-3B 11 MM-Spatial-3B + CoT 12 MM-Spatial-3B + Depth (Tool; Mon.) Cambrian-1-34B [104] 13 MM-Spatial-3B (Specialist; Blind eval) 2D Tasks (CV-Bench2D) 3D Tasks (CV-Bench3D) Object Count Spatial Relation. Average (2D) Depth Order Relative Distance Indoor Outdoor Avg. Indoor Outdoor Avg. Average (2D+3D) Average (3D) 62.9 58.6 88.7 88.1 88.1 92.0 85.8 64.5 94.0 96.2 96. 59.1 64.3 62.2 72.3 74.4 73.0 71.5 74.0 61.3 91.1 91.8 91.8 77.1 67.0 96.8 96.5 99.0 64.5 71.5 87.0 88.0 92. 50.0 76.6 68.5 93.5 93.7 96.7 59.7 68.3 95.8 98.5 97.8 59.0 69.0 75.5 78.0 80. 51.5 71.6 68.5 89.0 91.7 91.8 56.5 73.8 65.3 72.0 74.1 74.8 79.2 79.7 68.5 91.3 92.7 94.3 58.1 69.1 63.8 72.2 74.9 74.3 73.9 75.4 76.9 64.9 91.2 92.3 93. 67.6 Table 4. CV-Bench Results. MM-Spatial-3B substantially outperforms the (much larger) SOTA models, with CoT and depth input further improving performance. It almost fully solves the indoor 3D tasks, while also excelling at the out-of-domain outdoor 3D tasks. Model ARKit Depth Monocular (DepthPro [12]) MM-Spatial-3B + CoT Generalist Specialist δ1 96.7 82.2 84.6 AbsRel 4.2 13.4 12.8 δ1 96.1 82.0 89. AbsRel 4.6 13.6 11.0 is general-purpose depth estimation model whereas MMSpatial is trained only on indoor scenes which aligns well with the CA-VQA benchmark1, these results still intriguingly suggest that MLLMs are capable of acquiring strong metric depth estimation abilities solely via data curation. Table 5. Metric Depth Estimation Results. We evaluate the metric depth estimates of our CoT model produced as part of its responses on the CA-VQA benchmark. We compare against the tool-use estimates based on Monocular (DepthPro [12]) and ARKit Depth, i.e., the median depth value within the 2D box predicted by MM-Spatial + CoT (generalist & specialist). We report the δ1 (accuracy at 25% relative error) and AbsRel (absolute relative error) metrics [57] commonly used in the depth estimation literature [12], computed against GT FARO depth. MM-Spatial + CoT outperforms DepthPro. LiDAR-derived ARKit Depth is best. limitations are still apparent on our benchmark, especially for metric estimation. This is particularly visible on the Ego-Distance task which most directly relies on accurate depth estimation: monocular depth 15 1) performs substantially worse than GT depth 16 , and 2) even regresses compared to the RGB-only baseline 13 , suggesting that our model itself can learn to accurately predict depth. Depth-tool (Monocular) vs. CoT. Our CoT approach 14 (which requires MM-Spatial to explicitly predict depth) performs better than tool-use with monocular depth 15 , again most noticeable on Ego-Distance. This again hints at MM-Spatials strong inherent metric depth estimation ability, which we analyze in more detail below. MM-Spatials Metric Depth Estimation Ability. Tab. 5 and Fig. 4 show (quantitatively and qualitatively, respectively) that, surprisingly, our models monocular metric depth estimation accuracy can even rival that of the SOTA DepthPro [12] specialist model. While DepthPro 5.4. CV-Bench Results The CV-Bench results in Tab. 4 demonstrate that MMSpatial-3B 10 significantly outperforms the much larger SOTA Cambrian-1-34B 8 , highlighting the effectiveness of SFT on similar data. CoT 11 and leveraging monocular (DepthPro) depth input via tool-use 12 again further boost performance. MM-Spatial achieves almost perfect accuracy on the indoor splits of the 3D tasks, and also demonstrates strong out-of-domain generalization to the outdoor splits. Notably, MM-Spatial (Blind eval) 13 achieves the best accuracy among all models on the 2D Object Count task, revealing substantial bias in this benchmark. In contrast, on our CA-VQA benchmark, using vision input outperforms the blind baseline on Counting by 13 points. 5.5. SpatialRGPT-Bench Results Tab. 6 shows SpatialRGPT-Bench results. Notably, to align with the OpenSpatialDataset (OSD) used to train SpatialRGPT, SpatialRGPT-Bench is also based on axis-aligned 3D boxes (AABBs), resulting in spatial concept definitions different to CA-VQA (see App. E). SpatialRGPT thus underperformed on CA-VQA, and similar issues arise when evaluating MM-Spatial on their benchmark 7 . To enable fair comparison of model capabilities, we thus align with the benchmark by generating CA-VQA, variant of CA-VQA adopting their AABB-based definitions, and train 1Note that DepthPros training data mixture includes ARKitScenes [12, App. C.1, Tab. 13] (which CA-VQA and thus MM-Spatials training data is based upon), so this is not zero-shot evaluation for either model. Model Spatial SFT Data 1 2 3 GPT-4 [2] GPT-4V [86] SpatialRGPT-7B (RGB-only) [27] SpatialRGPT-7B [27] SpatialRGPT-VILA-1.5-8B [27] 4 5 6 MM1.5-3B [128] 7 MM-Spatial-3B 8 MM-Spatial-3B 9 MM-Spatial-3B + Depth (Tool; Mon.) CA-VQA 10 MM-Spatial-3B + Depth (Tool; Mon.) CA-VQA (scale aug.) 11 MM-Spatial-3B + Depth (Tool; Mon.) OSD 12 MM-Spatial-3B + Depth (Tool; Mon.) CA-VQA + OSD OSD [27] OSD [27] OSD [27] CA-VQA (our defs.) CA-VQA + OSD Specialist Models 13 MM-Spatial-3B + Depth (Tool; Mon.) CA-VQA + OSD 14 MM-Spatial-3B + Depth (Tool; Mon.) CA-VQA 15 16 17 MM-Spatial-3B + Depth (Tool; Mon.) CA-VQA (scale aug.) 18 indoor outdoor 19 20 MM-Spatial-3B + Depth (Tool; Mon.) OSD 21 22 23 MM-Spatial-3B (Blind eval) indoor outdoor indoor outdoor OSD Qualitative (Binary) Tasks Quantitative (Metric) Tasks Below / Above 64.1 63.3 99.2 99.2 99.2 35.8 98.3 98.3 98.3 98.3 98.3 98. 98.3 98.3 Left / Right 42.8 46.6 99.0 99.0 100.0 46.7 97.1 99.1 96.2 98.1 100.0 99.1 Big / Small Tall / Short Wide / Thin Behind / Front 42.8 64.1 79.2 80.2 84.9 44.3 80.2 94.3 94.3 92.5 91.5 93.4 61.6 60.7 89.2 92.0 89.3 52.7 82.1 93.8 92.9 95.5 94.6 94.6 61.6 68.2 83.6 87.5 91.3 47.1 73.1 92.3 92.3 92.3 95.2 94.2 49.0 45.4 87.2 91.8 90.9 50.9 74.6 93.6 97.3 97.3 96.4 97.3 Avg. 53.7 58.0 89.6 91.6 92.6 46.3 84.2 95.2 95.2 95.7 96.0 96.2 98.1 94.3 93.4 93.4 94.6 90.2 93.3 89.4 96.4 95. 95.7 93.5 98.3 98.1 94.3 93.8 93. 96.4 95.7 98.3 100.0 89.6 92. 92.3 96.4 94.9 100.0 98.1 74. 81.3 86.5 80.1 86.9 Direct Dist. Horizon. Dist. Vertical Dist. Width Height Avg. 21.6 29.7 35.1 41.2 45.9 4.7 14.2 33.1 41.9 60.8 41.2 47.3 59.5 37.8 50.0 5.0 60.1 60.2 60.0 39.2 32.4 57.5 21.6 11.5 25.4 59.0 65.6 68.0 10.7 12.3 74.6 63.9 72.1 71.3 77.9 82.0 61.5 90.2 2.5 75.4 84.2 57.5 73.0 76.8 65.0 22. 33.0 33.0 53.8 51.9 56.6 2.8 47.2 61.3 58.5 58.5 59.4 65.1 63.2 63.2 63.2 67.9 67.9 59.4 59.4 43.4 52.3 51.1 51.9 49.6 48.9 1.5 30.0 55.6 51.9 48.1 54.1 60.2 55.6 51.9 60.0 0 48.1 55.7 0 53.4 54.8 44.4 27.1 48.1 68.4 54.9 57.9 61.7 12.0 53.4 77.4 56.4 58.7 68.4 79.0 83.5 56.4 71.8 3.3 59.4 71.8 16.7 82.0 78.6 93.3 21. 33.3 41.5 50.9 53.2 56.2 6.4 31.4 60.4 54.5 59.6 58.9 65.9 68.7 54.2 67.0 2.7 62.2 68.0 33.6 61.4 60.4 65.1 27.2 Avg. 43.5 49.8 70.3 72.4 74.4 26.3 57.8 77.8 74.9 77.7 77.5 81.0 82.2 73.8 78. 78.1 57.0 Table 6. SpatialRGPT-Bench Results. MM-Spatial-3B achieves SOTA with both image-only input and tool-use monocular depth, outperforming SpatialRGPT-VILA-1.5-8B (which fully encodes depth). Training on mixture of CA-VQA and OSD performs best. MM-Spatial on that. We also train on OSD for comparison. For MM-Spatial + Depth we use monocular (DepthPro) metric depth via tool-use. We observe the following: Indoor vs. Outdoor. While MM-Spatial trained on CAVQA 9 achieves strong performance, it cannot significantly outperform the SOTA 5 . Analysis reveals that while the model excels at indoor samples 15 , it fails to generalize to outdoor samples 16 , particularly on the Metric tasks. We hypothesize that this is mainly attributed to the vast difference in metric scales between indoor and outdoor scenes, especially for object distances. We verify this with simple scale augmentation approach: we generate additional Distance examples with scaling factors (sampled uniformly from [1, 10]) applied to our underlying indoor scenes (i.e., 3D boxes and point clouds), resulting in wider range of metric distances. We confirm that MM-Spatial trained on CA-VQA + scale aug. 17 substantially improves performance on the outdoor Distance tasks 19 , resulting in SOTA performance overall. CA-VQA vs. OSD. Training on OSD 20 (based on diverse OpenImages [56] data) results in strong performance both indoor 21 and outdoor 22 . However, training on CA-VQA still yields significantly better indoor performance overall 15 , suggesting that our high-quality 3D GT is more effective than OSDs pseudo-annotations. When using CA-VQA + scale aug. 19 , we become competitive with OSD 22 even on outdoor Distance, but still lack on outdoor Width / Height. Combining CA-VQA with OSD 13 results in significant improvements, emphasizing the complementary benefits of the two datasets. MM-Spatial vs. SpatialRGPT. MM-Spatial-3B outperforms the SOTA SpatialRGPT-VILA-1.5-8B 5 with different data mixtures 10 - 12 , with and without 8 depth input (SpatialRGPT uses depth maps via full encoding). Depth vs. Image-only. Leveraging monocular metric depth via tool-use significantly improves performance for MM-Spatial ( 12 vs. 8 ). SpatialRGPT-7B benefits less from fully encoding relative depth ( 4 vs. 3 ). Blind vs. Vision evaluation. MM-Spatial trained on OSD (Blind eval) 23 performs well on several tasks, and GPT-4 1 was the prior SOTA for Width. This suggests that SpatialRGPT-Bench and OSD suffer from significant biases and do not probe spatial perception alone. 6. Conclusion We made several contributions towards unlocking objectcentric 3D spatial understanding in MLLMs. First, we proposed data generation pipeline, resulting in the CAVQA SFT dataset for 3D perception tasks (incl. multiview and depth inputs). Second, we introduced new 3D spatial understanding benchmark, which includes tasks such as spatial relationships, metric estimation, and 3D grounding. Third, we demonstrated that our MM-Spatial model can achieve SOTA performance on spatial reasoning benchmarks, while preserving general MLLM capabilities. Lastly, we investigated how adding multi-view and depth as input modalities can further improve the models spatial perception ability, and demonstrated that MLLMs can acquire strong monocular depth estimation capabilities via SFT. In future work, we aim to extend our scope to outdoor scenes to complement our high-quality indoor dataset."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Anshul Shah, Lin Chen, Wei Liu, Ian Fasel, Alkesh Patel, Omer Hadad, Haoxuan You, Haotian Zhang, Wentao Wu, Philipp Dufter, Sergiu Sima, Sai Aitharaju, Albert Antony, David Haldimann, Michael for helpful discussions and feedback. Emmersberger,"
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 3, 6, 5 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3, 6, 8 [3] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in realworld scenes. In ECCV, 2020. 2 [4] Palaash Agrawal, Haidi Azaman, and Cheston Tan. Stupd: synthetic dataset for spatial and temporal relation reasoning. arXiv preprint arXiv:2309.06680, 2023. 2 [5] Remyx AI. Openspaces. https://huggingface.co/datasets/remyxai/OpenSpaces, 2024. 2 [6] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 2022. 2 [7] Apple. The AXLearn Library for Deep Learning. https://github.com/apple/axlearn, 2024. [8] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 2 [9] Anas Awadalla, Le Xue, Oscar Lo, Manli Shu, Hannah Lee, Etash Kumar Guha, Matt Jordan, Sheng Shen, Mohamed Awadalla, Silvio Savarese, et al. Mint-1t: Scaling opensource multimodal data by 10x: multimodal dataset with one trillion tokens. arXiv preprint arXiv:2406.11271, 2024. 2 [10] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2 [11] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon 9 Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. ARKitscenes - diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. In NeurIPS Datasets and Benchmarks Track (Round 1), 2021. 2, 3, [12] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. 4, 7 [13] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. Omni3d: large benchmark and model for 3d object detection in the wild. In CVPR, 2023. 2 [14] Mu Cai, Jianwei Yang, Jianfeng Gao, and Yong Jae Lee. Matryoshka multimodal models. In ICRL, 2025. 2 [15] Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. In ICRA, 2025. 1, 2, 3, 4 [16] Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In ICAR, 2015. [17] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced projector for multimodal llm. In CVPR, 2024. 2, 4 [18] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. In 3DV, 2017. 2 [19] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 2 [20] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. 1, 2, 3 [21] Dave Zhenyu Chen, Angel Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In ECCV, 2020. 2 [22] Hong-You Chen, Zhengfeng Lai, Haotian Zhang, Xinze Wang, Marcin Eichner, Keen You, Meng Cao, Bowen Contrastive loZhang, Yinfei Yang, and Zhe Gan. arXiv preprint calized language-image pre-training. arXiv:2410.02746, 2024. [23] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 2 [24] Kezhen Chen, Rahul Thapa, Rahul Chalamala, Ben Athiwaratkun, Shuaiwen Leon Song, and James Zou. Dragonfly: Multi-resolution zoom supercharges large visuallanguage model. arXiv preprint arXiv:2406.00977, 2024. 2 [25] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? In NeurIPS, 2024. 3, 6 [26] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. Science China Information Sciences, 67(12): 220101, 2024. 5 [27] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language model. In NeurIPS, 2024. 1, 2, 3, 4, 5, 6, [28] Jang Hyun Cho, Boris Ivanovic, Yulong Cao, Edward Schmerling, Yue Wang, Xinshuo Weng, Boyi Li, Yurong You, Philipp Krahenbuhl, Yan Wang, et al. LanguagearXiv preprint image models with 3d understanding. arXiv:2405.03685, 2024. 1, 2, 3 [29] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 2 [30] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. In NeurIPS, 2022. 2 [31] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-xcomposer24khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. In NeurIPS, 2024. 2 [32] Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large visionlanguage models. In ACL, 2024. 1, 2 [33] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. 5 [34] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. In ICLR, 2024. [35] Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu. Graspnet-1billion: large-scale benchmark for general object grasping. In CVPR, 2020. 2 [36] Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Guilherme Turrisi da Costa, Louis Bethune, Zhe Gan, et al. Multimodal autoregressive pre-training of large vision encoders. In CVPR, 2025. 2 [37] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 2 [38] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. Scene-llm: Extending language model for 3d visual understanding and reasoning. In WACV, 2025. 3 [39] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 2024. 5 [40] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. In ICML, 2024. [41] Chunjiang Ge, Sijie Cheng, Ziming Wang, Jiale Yuan, Yuan Gao, Jun Song, Shiji Song, Gao Huang, and Bo Convllava: Hierarchical backbones as visual Zheng. arXiv preprint encoder for large multimodal models. arXiv:2405.15738, 2024. 2 [42] Ankit Goyal, Kaiyu Yang, Dawei Yang, and Jia Deng. Rel3d: minimally contrastive benchmark for grounding spatial relations in 3d. In NeurIPS, 2020. 2 [43] Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, et al. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. In ICRA, 2024. 3 [44] Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, and Bo Zhao. Efficient multimodal arXiv preprint learning from data-centric perspective. arXiv:2402.11530, 2024. 5 [45] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In CVPR, 2024. 2 [46] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. 3d concept learning and reasoning from multi-view images. In CVPR, 2023. [47] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. NeurIPS, 2023. 3 [48] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895, 2024. 2 [49] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. NeurIPS, 2023. 2 [50] Raisa Islam and Owana Marzia Moushi. Gpt-4o: The cutting-edge advancement in multimodal llm. Authorea Preprints, 2024. 2, 3, 6, 5 10 [51] Jitesh Jain, Jianwei Yang, and Humphrey Shi. Vcoder: Versatile vision encoders for multimodal large language models. In CVPR, 2024. [52] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multiimage instruction tuning. arXiv preprint arXiv:2405.01483, 2024. 2 [53] Amita Kamath, Jack Hessel, and Kai-Wei Chang. Whats investigating their up with vision-language models? struggle with spatial reasoning. In EMNLP, 2023. 2 [54] Mukul Khanna*, Yongsen Mao*, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel X. Chang, and Manolis Savva. Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation. arXiv preprint, 2023. 2 [55] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017. 2 [56] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. 2, 5, 8 [57] Lubor Ladicky, Jianbo Shi, and Marc Pollefeys. Pulling things out of perspective. In CVPR, 2014. 7 [58] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, 2024. 2 [59] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Tronchon. Building and better understanding visioninsights and future directions. arXiv language models: preprint arXiv:2408.12637, 2024. 2 [60] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? In NeurIPS, 2025. 2 [61] Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, and Afshin Dehghan. Cubify anything: Scaling indoor 3d object detection. arXiv preprint arXiv:2412.04458, 2024. 2, 3, 1 [62] Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, BeomPhantom of latent arXiv preprint chan Park, and Yong Man Ro. for large language and vision models. arXiv:2409.14713, 2024. 7 [63] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 3, 5 [64] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1214, 2024. 2 [65] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. In ICLR, 2025. 2, 6 [66] Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-Wen Kuo, Lu Xu, Fan Chen, Jitesh Jain, Humphrey Shi, and Longyin Wen. Cumo: Scaling multimodal llm with co-upcycled mixture-of-experts. In NeurIPS, 2024. [67] Mingsheng Li, Xin Chen, Chi Zhang, Sijin Chen, Hongyuan Zhu, Fukun Yin, Gang Yu, and Tao Chen. M3dbench: Lets instruct large models with multi-modal 3d prompts. arXiv preprint arXiv:2312.10763, 2023. 3 [68] Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, et al. Omnicorpus: An unified multimodal corpus of 10 billion-level images interleaved with text. In ICLR, 2025. 2 [69] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392, 2024. 2 [70] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 2, 7 [71] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In CVPR, 2024. 2 [72] Zhangheng Li, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal, Xiujun Li, Mohana Prasad Sathya Moorthy, Jeff Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui 2: Mastering universal user interface understanding across platforms. In ICLR, 2025. 2 [73] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. In EMNLP, 2024. [74] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947, 2024. 2 [75] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In EMNLP, 2024. 2 [76] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, 2024. 2, 5 [77] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. 2 [78] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. 2, 4 11 [79] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. TACL, 2023. 2 [80] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2 [81] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 7 [82] Wufei Ma, Haoyu Chen, Guofeng Zhang, Celso de Melo, Alan Yuille, and Jieneng Chen. 3dsrbench: comprehensive 3d spatial reasoning benchmark. arXiv preprint arXiv:2412.07825, 2024. 2 [83] Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng, Jia-Wang Bian, Philip Torr, Marc Pollefeys, Matthias Nießner, Ian Reid, Angel X. Chang, Iro Laina, and Victor Adrian Prisacariu. When llms step into the 3d world: survey and meta-analysis of 3d tasks via multi-modal large language models, 2024. 1 [84] Julius Mayer, Mohamad Ballout, Serwan Jassim, Farbod Nosrat Nezami, and Elia Bruni. ivisparan interactive visual-spatial reasoning benchmark for vlms. arXiv preprint arXiv:2502.03214, 2025. 2 [85] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. In ECCV, 2024. 2, 4 [86] OpenAI. Gpt-4 vision. OpenAI, 2024. https:// openai.com/research/gpt-4-vision. 2, 3, 6, 7, 8, [87] Georgios Pantazopoulos, Alessandro Suglia, Oliver Lemon, and Arash Eshghi. Lost in space: Probing fine-grained spaIn tial understanding in vision and language resamplers. NAACL, 2024. 2 [88] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. In ICLR, 2024. 2 [89] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 4 [90] Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cognition emerge in frontier models? In ICLR, 2025. 2 [91] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In CVPR, 2024. 2 [92] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. 2, [93] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 5 [94] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In CVPR, 2024. 2 [95] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In NeurIPS, 2023. 3, 4 [96] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. In ICLR, 2025. 2 [97] Fatemeh Shiri, Xiao-Yu Guo, Mona Golestan Far, Xin Yu, Gholamreza Haffari, and Yuan-Fang Li. An empirical analysis on spatial reasoning capabilities of large multimodal models. In EMNLP, 2024. 2 [98] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. In CVPR, 2025. 1, [99] Yu-Chuan Su, Soravit Changpinyo, Xiangning Chen, Sathish Thoppay, Cho-Jui Hsieh, Lior Shapira, Radu Soricut, Hartwig Adam, Matthew Brown, Ming-Hsuan Yang, et al. 2.5 visual relationship detection. Computer Vision and Image Understanding, 224:103557, 2022. 2 [100] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In CVPR, 2024. 2 [101] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. In ICLR, 2024. 2 [102] Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, et al. Textsquare: Scaling up text-centric visual instruction tuning. arXiv preprint arXiv:2404.12803, 2024. 2 [103] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 5 [104] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. 2, 4, 5, [105] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, 2024. 2 12 [106] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. NeurIPS, 2021. 2 [107] Stephen Tyree, Jonathan Tremblay, Thang To, Jia Cheng, Terry Mosier, Jeffrey Smith, and Stan Birchfield. 6-dof pose estimation of household objects for robotic manipulation: An accessible dataset and benchmark. In IROS, 2022. 2 [108] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Nießner. Rio: 3d object instance re-localization in changing indoor environments. In ICCV, 2019. [109] Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, et al. Embodiedscan: holistic multimodal 3d perception suite towards embodied ai. In CVPR, 2024. [110] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. In NeurIPS, 2023. 2 [111] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. 5 [112] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 2 [113] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. SlowFast-LLaVA: strong training-free basearXiv preprint line for video large language models. arXiv:2407.15841, 2024. 2 [114] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. In ECCV, 2024. 2 [115] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. 2, 5 [116] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. 2 [117] Kaiyu Yang, Olga Russakovsky, and Jia Deng. Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition. In ICCV, 2019. 2 [118] Huanjin Yao, Wenhao Wu, Taojiannan Yang, YuXin Song, Mengxi Zhang, Haocheng Feng, Yifan Sun, Zhiheng Li, Wanli Ouyang, and Jingdong Wang. Dense connector for mllms. In NeurIPS, 2024. 2 [119] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 5 [120] Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, et al. MM-Ego: Towards building egocentric multimodal llms. In ICLR, 2025. 2 [121] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In ICCV, 2023. 2 [122] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In ICLR, 2024. 1, [123] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms. In ECCV, 2024. 2 [124] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel In CVPR, understanding with visual instruction tuning. 2024. 2 [125] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models. IJCV, 2024. 2 [126] Tatiana Zemskova and Dmitry Yudin. 3dgraphllm: Combining semantic graphs and large language models for 3d arXiv preprint arXiv:2412.18450, scene understanding. 2024. 3 [127] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferretv2: An improved baseline for referring and grounding with large language models. In COLM, 2024. 1, 2 [128] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. In ICLR, 2025. 4, 5, 6, 7, [129] Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee. Countercurate: Enhancing physical and semantic visiolinguistic compositional reasoning via counterfactual examples. In ACL, 2024. 2 [130] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-ofinterest. arXiv preprint arXiv:2307.03601, 2023. 2 [131] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llavahd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. 2 [132] Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, and Ziqiao Ma. Do vision-language models represent space and how? evaluating spatial frame of reference under ambiguities. In ICLR, 2025. 2 [133] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt: Enabling visual grounding in multi-modal llms. arXiv preprint arXiv:2307.08581, 2023. 2 13 [134] Duo Zheng, Shijia Huang, and Liwei Wang. Video-3d llm: Learning position-aware video representation for 3d scene understanding. In CVPR, 2025. [135] Hongyan Zhi, Peihao Chen, Junyan Li, Shuailei Ma, Xinyu Sun, Tianhang Xiang, Yinjie Lei, Mingkui Tan, and Chuang Gan. Lscenellm: Enhancing large 3d scene understanding using adaptive visual preferences. In CVPR, 2025. 3 [136] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. 2 [137] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024. 5 [138] Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, and Liu. Scanreason: Empowering 3d visual grounding with reasoning capabilities. In ECCV, 2024. 3 [139] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125, 2024. 3 [140] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language In understanding with advanced large language models. ICLR, 2024. 2 MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs A. More Details about the CA-VQA Data object classes."
        },
        {
            "title": "Supplementary Material",
            "content": "A.1. Spatial Task Categories We here provide more details on the different spatial task categories covered in CA-VQA, with visualizations of examples provided in Figs. 5 to 7. Binary. Viewpoint-Dependent. We consider the spatial relationships left vs. right and in front vs. behind between two objects, as determined from the current camera pose / viewpoint:2 * Left vs. Right. We determine the answer based on the horizontal coordinates of the objects 2D bounding box centers. * In front vs. Behind. We determine the answer based on the distances between the camera and the objects 3D bounding box centers. Relative Object Size. We determine the answer based on the objects width, length or height, as defined in Regression below. Object Presence. For each sample asking about an object present in the image, we also generate negative sample which asks about (randomly sampled) object not present in the image, to ensure uniform distribution over answers (Yes / No). Counting. We determine the answer by simply counting the number of bounding boxes present in the image for given object class. We also generate negative samples with (randomly sampled) objects not present in the image (i.e., such that the correct answer is 0). Multi-choice. This covers questions across the other spatial task categories, except for 2D and 3D grounding. We randomize the order of the options, obtaining the incorrect options as follows: Regression (Metric Estimation). We compute three wrong options with either 10% increments deviating from the real answer, or 5cm, whichever value is larger. Counting. We always ensure that 0 is an option (i.e., object is not present). We then randomly sample (additional) wrong options among the non-zero integers within [GT 3, GT + 3] (where GT is the correct answer), s.t. the total number of options is 4. 2D / 3D Referring. We randomly sample three wrong 2Note that we do not consider above vs. below to avoid ambiguity: above could either refer to 2D image space (i.e., the 2D bounding box of is above that of B), or to 3D space, where the latter can be ambiguous as well (i.e., do we just require that the 3D bounding box of is located higher in terms of vertical dimension, or do we also require that is located directly above in terms of horizontal dimensions the latter might best match with how humans colloquially define above). Regression (Metric Estimation). Egocentric Distance. The distance between the camera and the closest point of the objects point cloud. Object Distance. We consider both (minimum) distance and center distance between two objects: * (Minimum) Distance. The distance between the closest points of the two objects point clouds (i.e., minimum point distance). * Center Distance. The distance between the center points of the two objects 3D bounding boxes. Object Size. We consider the 3D dimensions width, length and height, defined as follows: * Width. The length of the larger horizontal edge of the objects 3D bounding box (i.e., max(xlen, zlen)). * Length. The length of the shorter horizontal edge of the objects 3D bounding box (i.e., min(xlen, zlen)) * Height. The length of the vertical edge of the objects 3D bounding box (i.e., ylen). 2D Grounding. We use the 2D bounding box obtained from projecting the objects 3D bounding box into 2D image space. 3D Grounding. We directly use the 3D bounding boxes provided in CA-1M [61]. A.2. Depth: Chain-of-Thought (CoT) / Tool-Use We prepare multi-step CoT responses involving depth for questions within the Binary (only behind vs. in front) and Regression (Metric Estimation) categories, as the ground truth answers for those rely on depth information. We also did preliminary experiments with 3D Grounding samples, but found that performance does not improve / even slightly regresses there, so we did not include any such samples in the final dataset.3 The sequence format of the samples is illustrated in Fig. 3, involving the target objects 2D bounding boxes and depth values and the final (original) answer. We use the GT depth maps for generating the training examples, extracting the median depth value within the objects 2D bounding box4. At test time, we then consider two alternative approaches for obtaining the depth values: 3We hypothesize that 3D grounding is too complex of task to benefit from the simple depth information provided in the multi-step CoT answers, and that the model might just get confused. We leave more comprehensive study of how to benefit 3D grounding with CoT for future work. 4We also did preliminary experiments with other ways to extract single depth value from the depth map within the 2D bounding box, such as the value at the center of the box or percentiles other than the median, but did not see significant improvements over using the median, which we found to be robust choice. 1 Figure 5. CA-VQA Overview. Example QA pairs from our Cubify Anything VQA (CA-VQA) dataset, aiming to unlock object-centric 3D spatial understanding in MLLMs. Using high-quality 3D ground truth annotations from CA-1M [61], we generate spatial perception questions across variety of different tasks, e.g., involving relative relationships, metric measurements, and 3D object bounding boxes. Model prediction (CoT). We let the model predict the depth values (called CoT in the experiments). As the model was trained on sequences involving the ground truth depth values, the models learn to predict depth. Our experiments reveal the accuracy of the resulting depth estimates. Tool-use. We allow the model to leverage given depth map via tool-use. I.e., for function call of the form Depth(bbox) we extract the median depth value within the 2D bounding box, insert the depth value into the sequence, and then let the model continue its prediction to arrive at the final answer (see Fig. 3). B. Optimal Data Mixture for MM-Spatial We aim to build generalist MLLM that excels across variety of diverse tasks as opposed to specialist that only excels at spatial understanding. To this end, we identify the mixture weight for the new spatial data that achieves the best performance trade-off between the spatial vs. all other benchmark categories: general, knowledge, text-rich, 2D referring & grounding. Investigating the effect of adding new model capability is particularly relevant for models with limited capacity, such as the 3B model we consider. Results are shown in Tab. 7. MM-Spatial maintains similar performance as the MM1.5 baseline across most task categories, while significantly improving on the Spatial category. This suggests that our model can successfully adopt the new spatial understanding capability without regressing on all the other capabilities, resulting in generalist MLLM. The data mixture ratio of 2:1 (spatial:general) provides good performance trade-off and is used for MM-Spatial throughout. We also consider spatial Specialist Model that is trained on CA-VQA only; however, this model provides only small improvement on the spatial category, while regressing substantially on all other benchmark categories. We use specialist models for some of our ablations to speed up experimentation. Appendix shows the detailed result breakdowns across the different task categories, compared to SOTA models. C. Results on Further Benchmark Categories We here present more detailed analysis of MM-Spatial compared with SOTA baselines across the different benchmark categories. Results on general and knowledge benchmarks are shown in Tab. 8, results on text-rich benchmarks are shown in Tab. 9, and results on 2D referring & grounding benchmarks are shown in Tab. 10. Overall, we observe that our MM-Spatial model maintains level of performance similar to the vanilla MM1.5 baseline. This suggests that our model is able to successfully adopt the new spatial understanding capability without sacrificing performance on all the other model capabilities, resulting in generalist MLLM. D. Analysis of Blind Filtering Procedure Tab. 11 analyses the effectiveness of our blind filtering procedure outlined in Sec. 3.1 in ensuring that our CA-VQA benchmark becomes more reliant on vision input. This is in contrast to some of the tasks from the other spatial understanding benchmarks we consider (CV-Bench and SpatialRGPT-Bench), where we found that blind models can perform very strongly and even rival models with vision input in some cases (see Sec. 5). Hence, these benchmarks would likely also benefit from blind filtering. 2 Figure 6. Examples of CA-VQA data samples from the Binary, Counting and Multi-choice categories. E. Axis-aligned vs. Oriented 3D Boxes Fig. 8 emphasizes the fundamental difference between axisaligned (AABB) and oriented (OBB) 3D bounding boxes and how they affect the resulting object dimensions. This provides an indication of the misalignment issues arising when evaluating model trained on data based on OBB ground truth (i.e., MM-Spatial, which is based on the gravity-aligned 7-DOF yaw-oriented 3D bounding boxes from CA-1M) on benchmark based on AABB ground truth (i.e., SpatialRGPT-Bench) and vice versa (i.e., evaluating SpatialRGPT on CA-VQA), as seen in Secs. 5.3 and 5.5. Figure 7. Examples of CA-VQA data samples from the Regression (Metric Estimation) and 2D Grounding categories. Benchmark Category Averages Model Mix. Ratio Spatial Understanding Rel. Eff. CA-VQA CV-Bench SRGPT-Bench Avg. General Knowledge Text-rich Refer&Ground Avg. MM1.5-3B [128] MM-Spatial-3B MM-Spatial-3B 0:1 1:1 2:1 4:1 8:1 1:0 0:100 12:88 22:78 36:64 54:46 100: 28.9 66.3 67.1 67.3 67.4 67.1 64.9 91.2 92.4 93.0 93.1 93. 26.0 52.8 53.7 52.7 53.7 54.1 39.9 70.1 71.1 71.0 71.4 71. 64.7 65.0 64.8 65.0 64.8 42.6 46.2 46.2 46.7 44.9 46.8 34. 62.1 62.1 61.4 60.7 61.2 17.2 77.7 79.1 78.8 78.0 79.0 23. 58.1 64.5 64.5 63.9 64.6 38.0 Table 7. Data Mixture Ratio Results. Comparison of different data mixture ratios both (Rel)ative to the General category (as in MM1.5), and (Eff)ective when considering the dataset sizes on aggregated metrics across the different benchmark categories. Overall, MM-Spatial is generalist MMLM that improves lot on the Spatial category while maintaining strong performance on the other categories. The data mixture ratio of 2:1 (spatial:general) provides good performance trade-off and is used for MM-Spatial throughout. The last line considers spatial Specialist Model that is trained on CA-VQA only; this model provides only minor improvement on the spatial category, while regressing substantially on all other benchmark categories. 4 Model Knowledge Benchmarks AI2D (test) MMMU (val) MathV (testmini) MiniCPM-V 2.0-3B [119] VILA1.5-3B [76] SpatialRGPT-VILA-1.5-3B [27] TinyLLaVA [137] Gemini Nano-2 [103] Bunny [44] BLIP-3 [115] Phi-3-Vision-4B [1] MM1.5-3B [128] MM-Spatial-3B Gemini-1.5-Pro [93] GPT-4V [86] GPT-4o [50] 62.9 51.0 76.7 64.5 63.6 79.1 75.9 84.6 38.2 33.3 33.0 32.6 41.4 41.1 40.4 37.1 36.6 60.6 53.8 69.2 38.7 30.6 39.6 44.5 37.1 38.4 57.7 48.7 61. MME (P/C) 1808.2 1442.4/ 1424.0/ 1464.9/ 1581.5/361.1 1441.6/320.0 1423.7/277.9 1530.5/251.8 2110.6 1771.5 2310.3 General Benchmarks SEEDI POPE LLaVAW MM-Vet RealWorldQA 67.1 67.9 69.0 72.5 72.2 71.8 70.2 71.3 71.6 77. 87.8 85.9 85.5 86.4 87.2 87.0 85.8 87.9 88.0 88.2 75.4 85.6 69.2 75.8 71.6 74.3 69.9 95.3 93.1 102.0 38.2 38.2 32.0 46.2 37.1 38.0 64.0 56.8 69. 55.8 60.5 59.4 57.7 59.0 64.1 56.5 75.4 Table 8. Knowledge and General Benchmark Results. Comparison with SOTA models on knowledge and general benchmarks. () Sum of and scores. Gemini-1.5-Pro, GPT-4V and GPT-4o numbers are from [33]. Model WTQ (test) TabFact (test) OCRBench (test) ChartQA (test) TextVQA (val) DocVQA (val) InfoVQA (val) MiniCPM-V 2.0-3B [119] TinyLLaVA [137] Gemini Nano-2 [103] BLIP-3-4B [115] Phi-3-Vision-4B [1] MM1.5-3B [128] MM-Spatial-3B Gemini-1.5-Pro [93] GPT-4V [86] GPT-4o [50] 24.2 47.4 37.3 36.2 58.2 67.8 70.5 71.0 60.5 63.7 63.0 60.0 75.4 64.5 73.6 59.8 51.9 81.4 73.6 75.0 87.2 78.5 85.7 74.1 59.1 65.9 71.0 70.1 74.4 75.3 78.7 71.9 74.3 83.3 82.0 82.7 93.1 88.4 92.8 37.6 54.5 49.0 45.5 43.7 81.0 Table 9. Text-rich Benchmark Results. Comparison with SOTA models on text-rich benchmarks. () Numbers are obtained from [63]. Model MiniCPM-v2-3B [119] Phi-3-Vision-4B [1] InternVL2 [26] MM1.5-3B [128] MM-Spatial-3B RefCOCO (testA/B) 46.3 / 36.1 88.2 / 75.9 91.7 / 85.7 92.2 / 85.9 RefCOCO+ (testA/B) RefCOCOg (test) Flickr30k (test) LVIS-Ref (box/point) 42.0 / 28.8 82.8 / 63.3 87.67 / 75.23 88.3 / 76.8 37.6 78.3 85.9 86.8 27.12 51.6 85.1 85.1 48.2/47.7 53.8/54.5 51.0 / 51.1 74.0 / 58.2 75.9 / 58.5 Table 10. 2D Referring & Grounding Benchmark Results. Comparison with SOTA models on 2D referring and grounding benchmarks. 5 Model Eval Inputs Binary Count. Multi-c. Ego-Dist. Obj.-Dist. Obj.-Size Average Regression (Metric Estimation) Acc Acc Acc Acc @ 10% Relative Error (ℓ1) Before Blind Filtering 1 2 3 GPT-4 [2] GPT-4V [86] Improvement from using vision 4 MM-Spatial-3B (Specialist) 5 MM-Spatial-3B (Specialist) 6 Improvement from using vision Text Image + Text = 2 1 Text Image + Text = 5 - 4 After Blind Filtering 7 8 9 GPT-4 [2] GPT-4V [86] Improvement from using vision 10 MM-Spatial-3B (Specialist) 11 MM-Spatial-3B (Specialist) Improvement from using vision Text Image + Text = 8 7 Text Image + Text = 11 10 57.9 61.6 +3.7 69.3 83.8 +14.5 9.6 39.2 +29. 34.3 69.6 +35.3 35.1 68.1 +33.0 69.5 76.9 +7.4 8.5 63.3 +54.8 60.8 73.3 +12.5 52.7 63.2 +10. 77.6 84.2 +6.6 9.6 32.9 +23.3 60.7 77.4 +16.7 Increase in Vision Improvement: Before vs. After Blind Filtering 13 GPT-4/V 14 MM-Spatial-3B (Specialist) = 9 3 = 12 +25.9 +20.8 +21.8 +5.1 +12.8 +10.1 8.9 6.4 -2.5 12.9 46.9 +34.0 6.2 11.4 +5. 10.1 47.3 +37.2 +7.7 +3.2 8.2 8.4 +0.2 11.0 25.4 +14.4 6.2 9.3 +3.1 8.4 24.4 +16. +2.9 +1.6 17.0 19.7 +2.7 25.2 29.5 +4.3 5.8 10.1 +4.3 17.9 24.3 +6.4 +1.6 +2. 30.0 37.9 +7.9 44.3 57.8 +13.5 7.7 27.7 +20.0 32.0 52.7 +20.7 +12.1 +7.2 Table 11. CA-VQA Blind Filtering Analysis. We study how the improvement from using vision (i.e., comparing vision-evaluated model vs. blind-evaluated model) changes after applying the blind filtering strategy outlined in Sec. 3.1, which follows [25]. Our results confirm that after applying our filtering strategy, 1) blind models perform substantially worse, and 2) vision improvements (i.e., the delta between vision and blind models) increase substantially, for both GPT-4/V and MM-Spatial. This highlights the effectiveness of our blind filtering procedure in ensuring that our CA-VQA benchmark becomes more reliant on vision input (i.e., less susceptible to strong language prior). Figure 8. Comparative visualization of axis-aligned vs. oriented 3D bounding boxes, taken from the SpatialRGPT paper [27, Appendix K, Figure 11]. The object dimensions computed from AABBs can differ substantially from those computed from OBBs, depending on the objects rotation. For sake of illustration, assume that the sofa is 2m wide and 0.8m deep. We then obtain the following altered object dimensions when using an AABB instead of an OBB, at different yaw rotation angles (i.e., considering 7-DOF bounding boxes that are gravity-aligned / parallel to the ground, as in CA-1M / CA-VQA): width 2.1m and depth 1.7m with 30 rotation; width 1.7m and depth 2.1m with 60 rotation; and width = 0.8m and depth = 2m with 90 rotation (i.e., full rotation resulting in swapped dimensions)."
        }
    ],
    "affiliations": []
}