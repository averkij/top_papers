{
    "paper_title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models",
    "authors": [
        "Zhuojun Ding",
        "Wei Wei",
        "Chenghao Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework's effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework."
        },
        {
            "title": "Start",
            "content": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models Zhuojun Ding1, Wei Wei*1 and Chenghao Fan1 1 School of Computer Science & Technology, Huazhong University of Science and Technology {dingzj, weiw}@hust.edu.cn, facicofan@gmail.com 5 2 0 2 8 2 ] . [ 1 3 1 8 2 2 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domainspecific models is costly. Existing works typically train unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our frameworks effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) demonstrate remarkable performance across wide range of tasks (Achiam et al., 2023; Yang et al., 2024; Guo et al., 2025a), but still struggle with information extraction (IE) tasks (Xu et al., 2024; Ding et al., 2024b; Fan et al., 2024b), such as Named Entity Recognition (NER). The inherent gap between task formulations and LLM training objectives is critical factor underlying this limitation. To mitigate *Corresponding author 1https://github.com/Ding-ZJ/SaM Figure 1: (a) Existing methods train general unified model across multiple domains, while we dynamically select and merge expert models at inference time. (b) trained system struggles to accommodate changes in training data, while we flexibly add or remove expert models, ensuring great scalability. this, supervised fine-tuning (SFT) has become widely used strategy, demonstrating significant improvements (Wang et al., 2023a; Zhou et al., 2024b; Fan et al., 2025). However, annotating data and training domainspecific models each time is costly, particularly for fine-grained IE tasks. Most existing approaches collect large-scale training data from multiple domains to train unified model (Wang et al., 2023a; Sainz et al., 2024; Yang et al., 2025). Although such models exhibit cross-domain generalization capabilities, they frequently exhibit suboptimal performance in both in-domain and out-of-domain test scenarios. This limitation arises primarily because (1) not all training samples universally enhance performance on given target domain (Liu et al., 2024; Zhou et al., 2024a), and (2) inherent conflicts may emerge across heterogeneous domains during joint training, leading to compromised optimization efficacy (Sainz et al., 2024; Yang et al., 2025; Fan et al., 2024a). Additionally, even when data from the target domain is available, effectively integrating it into trained model without compromising performance remains challenging. To address these issues, we adopt model merging strategy (Ilharco et al., 2023) to dynamically select domain-specific models for different target domains and fuse their parameters to obtain taskspecific models. Specifically, we first train multiple expert models in different domains with available data. Then, we design the SaM framework to derive task-specific models by Selecting and Merging experts from two perspectives: (i) Domain similarity. We assess the domain similarity between the target domain and each expert model, and select the most relevant experts for parameter fusion to create task-specific model tailored to the target domain. (ii) Sampling evaluation. We randomly sample data instances from the target domain and integrate the predictions of all experts as pseudo-labels (no ground truth labels required). Based on these labels, we assess the performance of each expert on the sampled data and select the best-performing experts to merge into another task-specific model. The two perspectives synergistically complement each other: domain similarity provides high-level, coarse-grained assessment that establishes theoretical priors, while sampling evaluation delivers fine-grained, empirical quantification of expert performance to yield actionable practical guidance. By integrating the outputs of both task-specific models, we achieve more comprehensive results. Compared to previous methods (Figure 1), we allow for task-specific customization across diverse target domains, providing improved generalization ability without extra training. It also provides great scalability, as experts can be easily added or removed based on practical needs. Notably, our approach is orthogonal to previous studies. By leveraging their practical insights, we can train more effective expert models, thereby enhancing the performance of our framework. In terms of resource requirements, our approach does not incur additional training costs. By leveraging parameter-efficient fine-tuning methods (Hu et al., 2022), we only introduce minimal storage overhead. Additionally, by employing either strategy individually or further integrating the target models derived from both strategies, we can achieve comparable performance without incurring additional inference costs. In summary, our contributions are as follows: (1) We introduce model-merging paradigm for LLM-based Named Entity Recognition, enhancing adaptability and scalability. (2) We propose model selection strategy based on domain similarity and sampling evaluation, which effectively selects expert models beneficial to the target domain for merging. (3) Experimental results demonstrate the effectiveness of our framework, which outperforms the unified model by an average of 10% and by up to 20% in certain domains. Further experiments analyze potential improvements, practical experience, and framework generalizability, providing deeper practical insights."
        },
        {
            "title": "2 Related Works",
            "content": "LLMs for Information Extraction Current LLMs-based IE mainly fall into two paradigms. One paradigm uses larger models. Training them needs significant computational resources, and finetuning them specifically for IE tasks may be not cost-effective. However, these models excel in instruction-following and reasoning. Therefore, such methods focus on optimizing task instructions, reasoning strategies, or in-context learning (ICL) demonstrations. Li et al. (2023) show that codestyle prompts enhance IE tasks. Pang et al. (2023) and Tong et al. (2025) prompt LLMs with more comprehensive information to improve task understanding. Xie et al. (2023) and Wan et al. (2023) introduce reasoning techniques such as Chain-ofThought (CoT) to guide the model in step-by-step task completion. Xie et al. (2024) employ selfconsistency to generate reliable ICL examples. Another paradigm uses smaller models. While these models have weaker instruction-following capabilities, they require much fewer training resources. Such methods enhance LLMs through supervised fine-tuning (Wang et al., 2023a). Many studies design optimization strategies on the data side. Yang et al. (2025) resolves conflicts and redundancy in training data. Zhou et al. (2024b) distills more diverse data from ChatGPT. Li et al. (2024) formats training data in code style. Sainz et al. (2024) enriches instructions with detailed task descriptions. Ding et al. (2024a) emphasizes negative samples. In addition to instruction tuning, Qi et al. (2024) further employs alignment training (Rafailov et al., 2024), and Guo et al. (2025b) Inincorporates contrastive learning objectives. stead of training one universal model, we train several domain experts and design merging method to improve adaptability and scalability. Additionally, the backbone model is also critical. Recently, code-based LLMs have gained popularity, as they may better suit IE tasks than natural language-based LLMs (Li et al., 2023). Model Merging Model merging integrates multiple task-specific models at the parameter level to create unified model, which could handle multiple tasks simultaneously and exhibit better out-ofdomain generalization. Unlike multi-task learning, model merging reuses existing models, reducing computational and data demands since only model parameters are needed. Beyond simple parameter averaging, Matena and Raffel (2022) assigns different importance to model parameters. Ilharco et al. (2023) applies arithmetic operations for finer control over model behavior. Jin et al. (2023) enforces output consistency between the merged model and its constituent models. Yu et al. (2024) and Yadav et al. (2023) mitigate inter-model interference by addressing parameter redundancy or sign inconsistency, and weight sparsity, respectively. Lu et al. (2024) decomposes model parameters into shared and task-specific components. In this paper, we introduce model merging to improve adaptability and scalability across different target domains."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we first provide an overview of the training process of domain expert models. Then we explain our SaM framework for selecting and merging experts, as shown in Figure 2."
        },
        {
            "title": "3.1 Training Domain Experts",
            "content": "Data Collection. We first collect more than 20 commonly used NER datasets and classify them into six domains based on their sources: News, Social media, Biomedical, STEM (Science, Technology, Engineering, and Mathematics), Legal, and Traffic. We remove 90% of the NA data that contains no entities. Through sampling or redundancy, we limit the total samples per domain to between 10,000 and 50,000. The number of sampled instances from each dataset was proportional to the number of entity types it contained. Detailed information is provided in Appendix A. Training Data Construction. Refering to practice of prior studies (Wang et al., 2023a; Qi et al., 2024), we format the raw data into task instructions, inputs, and outputs for training. The task instructions consist of: (1) Data source description: brief overview of the dataset source. (2) Entity type description: Concise definition of entity types. (3) In-context learning demonstrations: 1 5 randomly selected input-output pairs from the training data. (4) Label drop: Excluding the requirement for recognizing certain entity types. (5) Label masking: Replacing entity labels with abstract placeholders such as Type1. Empirically, we apply (1), (2), and (3) to 70% of the data; (4) to 30%; and (5) to 5%. These modifications are applied independently, except (5), which should co-occur with (2). For output parts, we adopt three formats: JSON (e.g., {entity span: entity type}), enumeration (e.g., Type: span1, span2, ...), and natural language descriptions. These strategies help enhance model robustness to some extent. However, we claim this is not an optimal configuration, as our focus is not on training best-performing model. Model Training. Following prior work, we train the model using instruction tuning. Given dataset DA = {(I, X, )} from domain A, where is the task instruction, is the input sequence, and = {yi}L i=1 is the output sequence (i.e., entity predictions), the training loss of the domain-specific expert model MA is defined as: LθA = (cid:88) (cid:88) DA t=1 log PθA(yt I, X, y<t) (1) where θA denotes the parameters of MA."
        },
        {
            "title": "3.2 Selecting and Merging Experts",
            "content": "When handling specific target domain, we select subset of expert models and fuse their parameters to obtain task-specific models. As shown in Figure 2, this process is conducted from two perspectives. Selecting with Domain Similarity. Given domain with raw data DA = {xi}, we obtain the corresponding data embeddings HA = {hi} through text encoder. The domain embedding is then defined as the centroid of these data embeddings: 1 HA hA = (cid:88) (2) hi hiHA We compute domain embeddings {hei} for domains of all expert models and ht for the target domain. Then we compute the similarity between the expert domains and the target domain with cosine distances. Finally, the top-m similar expert models are selected for model merging. The domain embedding inherently captures the data distribution in the embedding space. Thus, in theory, the selected expert models exhibit certain degree of similarity and are expected to perform well in the target domain due to the resemblance in these data distributions. Figure 2: Framework overview. Given target domain, we select expert models from two perspectives: (a) Domain Similarity, which selects experts from domains most similar to the target domain. We compute the centroid of all data embeddings as the domain embedding and measure similarities by cosine distance. (b) Sampling Evaluation, which selects experts with better performances on sampled instances from the target domain. To reduce reliance on ground-truth labels, we ensemble predictions from all experts as (pseudo) labels. We merge models within each expert subset to obtain two task-specific models. The final result integrates the outputs of the two task models. Selecting with Sampling Evaluation. The selection of experts based on domain similarity is theoretically sound. However, in practice, we observe that the model with the highest domain similarity to the target domain does not always yield the best performance. Thus, we propose another selection strategy driven by model performance. Specifically, we first randomly sample data instances from the target domain. Then each expert model generates predictions for them. To reduce dependence on ground-truth labels, we aggregate predictions via majority voting to construct pseudolabels, which are subsequently used to assess expert performance. The top experts with the highest performance are selected for model merging. Unlike domain similarity-based selection, this approach prioritizes practical effectiveness. As result, the selected experts generally perform better individually in the target domain. However, we aim to obtain superior task model through model merging, where individual performance is not the sole determining factor. Merging Experts. Given base model Mbase and the supervised fine-tuned model Msf t, we denote their parameters as θbase and θsf t, respectively. The delta parameter δsf = θsf θbase serves as parametric representation of the models learned capabilities and is also referred to as the task vector. Given multiple task-specific models {Msf ti}, we can merge them into unified model Mmerge with diverse capabilities (Matena and Raffel, 2022; Ilharco et al., 2023): θmerge = θbase + Merge(δsf t1, δsf t2, ) (3) where Merge() denotes the model merging technique, such as simple averaging and task arithmetic (Ilharco et al., 2023). We employ the TiesMerging (Yadav et al., 2023) method, which addresses parameter redundancy and sign inconsistency to mitigate inter-model interference when merging multiple models. We selected two sets of expert models based on Domain Similarity (DS) and Sampling Evaluation (SE), respectively. These experts are subsequently merged to obtain two taskspecific models, MDS and MSE. 3."
        },
        {
            "title": "Inference",
            "content": "For target domain, we obtain two task-specific models, MDS and MSE, following the methodology described in Section 3.2. Each model independently generates predictions, producing two output sets, YDS and YSE. Taking the intersection of two sets of predictions typically enhances reliability. However, our two task-specific models are already tailored for the target domain and capture different and complementary perspectives. Therefore, we adopt their union as the final result. Recent Studies InstructUIE UniNER GoLLIE KnowCoder GLiNER B2NER Fully-trained SaM (Ours) Llama Qwen Llama Qwen CrossNER MIT Average Literature Music Politics Science Movie Restaurant 48.80 64.90 62.70 61.10 64.40 63.70 63.44 53. 54.40 70.60 67.80 70.00 69.60 68.60 72.53 61.12 49.90 66.90 57.20 72.20 72.60 67.80 62.92 54.99 49.40 70.80 55.50 59.10 62.60 72.00 59.09 59. 63.00 61.20 63.00 50.00 57.20 67.60 64.81 66.66 20.99 35.20 43.40 48.20 42.90 53.30 49.68 52.69 47.84 61.79 58.39 60.13 60.90 64.57 60.94 57. AI 48.40 62.90 59.10 60.30 57.20 59.00 54.11 51.38 60.9812.7% 66.935.50% 73.531.4% 74.4718.4% 62.605.9% 72.1711.4% 60.0115.8% 61.9916.0% 65.937.9% 67.0521.9% 62.415.1% 71.657.50% 52.996.7% 52.900.4% 66.248.70% 63.1310.6% Table 1: Experimental results. We compare our method with recent studies and our fully-trained model (e.g., unified model trained on all data we used). The best results are highlighted in bold, while suboptimal results are underlined. The right subscript denotes the percentage improvement compared to the fully trained model."
        },
        {
            "title": "4.1 Setup",
            "content": "Benchmarks, Baselines, and Metrics We evaluate our framework on two widely used benchmarks CrossNER (Liu et al., 2021) and MIT (Ushio and Camacho-Collados, 2021), which contain datasets from seven domains (AI, Literature, Music, Politics, Science, Movie, and Restaurant) in total. Our experiments are under zero-shot settings (i.e., no labeled target domain data), follow prior work. The source data for training and the target data for evaluation have different distributions. We introduce two types of baselines for comparison. The first is the Fully-trained model, single unified model trained on data from all domains using the same training configuration as us. This serves as the primary baseline to assess the effectiveness of our framework. The second includes recent studies that also train unified models but incorporate other advanced training optimizations, including InstructUIE (Wang et al., 2023a), UniNER (Zhou et al., 2024b), GoLLIE (Sainz et al., 2024), KnowCoder (Li et al., 2024), GLiNER (Zaratiana et al., 2024), and B2NER (Yang et al., 2025). Most of these models use LLMs as the foundation, except for GLiNER, which contains only 300 million parameters. Following prior studies (Wang et al., 2023a), we use the entity-level micro-F1 score as the evaluation metric, where both the entity boundary and entity type should be correctly predicted. Implementations We employ models from the Qwen and Llama series as base models for our experiments. Specifically, we adopt the base version of Qwen2.5-7B and Llama3.1-8B as foundations and train expert models using LoRA (Hu et al., 2022). We employ the all-MiniLM-L6-v22 text encoder to produce text embeddings. We set (the number of selected models for merging) to 3. We set (the number of sampled data instances) to 10. More details are reported in Appendix A."
        },
        {
            "title": "4.2 Main Results",
            "content": "As shown in Table 1, our approach significantly outperforms the fully trained model across all target domains, achieving an average improvement of approximately 10%, with gains of up to 20% in specific domains. This demonstrates the effectiveness and superior domain adaptability of our approach. To ensure practical comparability, we also compare our results with recent studies. These methods employ various training optimization strategies. For example, B2NER mitigates redundant and conflicting information in the training data. These techniques are orthogonal to ours. Notably, comparing these methods with our fully-trained model suggests that refining our training configuration could enhance our expert models and further improve the performance of our approach. Our approach may incur slight computational and storage overhead, which is acceptable, as discussed in Appendix C."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "We conduct ablation studies to validate the effectiveness of our design, as shown in Table 2: (1) w/o Merging, which directly uses the best expert model. (2) w/o Domain Similarity, which selects experts solely based on Sampling Evaluation. (3) w/o Sampling Evaluation, which selects experts 2https://huggingface.co/sentence-transformers/ all-MiniLM-L6-v2 AI Literature Music Politics Science Movie Restaurant Average Llama Qwen w/o Merging w/o Domain Similarity w/o Sampling Evaluation w/o Selection SaM (Ours) w/o Merging w/o Domain Similarity w/o Sampling Evaluation w/o Selection SaM (Ours) 53.72 58.43 56.42 60. 60.98 57.90 58.57 58.23 57.47 60.01 59.95 59.87 63.92 63.74 66.93 59.01 61.61 60.39 60. 61.99 71.61 72.89 72.25 74.83 73.53 65.33 65.63 64.11 64.66 65.93 71.88 74.20 72.99 73. 74.47 65.73 68.51 64.57 60.28 67.05 59.09 59.92 61.71 63.68 62.60 60.21 59.67 61.00 61. 62.41 66.43 68.91 71.08 67.30 72.17 65.45 70.66 71.04 69.43 71.65 50.62 51.69 52.48 47. 52.99 51.11 52.69 50.11 47.78 52.90 61.90 63.70 64.41 64.33 66.24 60.68 62.48 61.35 60. 63.13 Table 2: Ablation studies. Removing certain components typically results in performance degradation, confirming their significance. The best results are in bold, and the suboptimal ones are underlined. Mode1 Mode2 Mode3 SaM (Ours) AI Literature Music Politics Science Movie Restaurant Average 60.36 56.86 63.78 66.05 58.37 70.66 52. 61.27 58.19 62.68 66.50 68.27 61.73 70.66 52.85 62.98 61.31 58.90 66.85 61.30 63.12 70.50 52.23 62.03 60.01 61.99 65.93 67.05 62.41 71.65 52. 63.13 Table 3: Merging into single task model (based on Qwen). Modei denotes the method used to further extract final set from two expert sets for model merging. based on Domain Similarity. (4) w/o Selection, which merges all experts without selection. Results demonstrate that the merged model consistently outperforms the best individual expert, even when the selected models are not necessarily optimal. Overall, both expert selection strategies are effective and complementary, yielding the best results when combined. However, in some cases, using single selection strategy or none at all yields better results, likely due to expert redundancy or insufficiency, as we fix the number of selected experts to = 3 in our experiments. Further analysis of are presented in Section 4."
        },
        {
            "title": "4.4 Merging into a Single Task Model",
            "content": "Since our approach employs two task-specific models, the inference cost is doubled. To mitigate this, we derive single set from the two selected expert sets, reducing the number of task models to one. We propose and evaluate three strategies, with results in Table 3: (1) Mode1 leverages the intersection of the two selected expert sets. (2) Mode2 normalizes the evaluation metrics across both selection strategies (e.g., the domain similarity scores Figure 3: Performance changes with the number of experts. The horizontal axis is the number of experts for merging, and the vertical denotes the F1 scores. and F1-scores on sampled data points) to common scale and selects the top three experts. (3) Mode3 takes the union of the two expert sets while limiting the total number of selected experts to three. Experimental results show that Mode2 and Mode3 achieve comparable performance to us, making them effective alternatives without increasing inference costs. We refer to these as the economic versions of our framework (SaMeco)."
        },
        {
            "title": "4.5 Numbers of Experts for Merging",
            "content": "This section analyzes the impact of the number of merged models, k, with results shown in Figure 3. Here, = 1 denotes the performance of the best individual expert, while = 6 corresponds to merging all expert models. The optimal varies across target domains, typically ranging from 2 to 4. We set = 3 as it yields the best average performance. Notably, this corresponds to the merging algorithm as well. We adopt the Ties-Merging technique, where selecting 2 4 models for merging is commonly used configuration."
        },
        {
            "title": "Source Domains",
            "content": "AI"
        },
        {
            "title": "Nine",
            "content": "56.42 60.98 59.90 63.01 66.93 66. 71.69 72.24 73.53 74.47 73.53 73. 61.17 62.60 62.78 69.91 72.17 72. 50.62 52.99 53.84 63.58 66.24 66. Table 4: Performance under different number of source domains. Using six source domains yields better results than three, but increasing to nine provides no further gains. AI"
        },
        {
            "title": "Literature Music Politics Science Movie Restaurant Average",
            "content": "Fully-trained 70.40 Sam(Ours) 83.45 71.81 71.81 75. 79.41 77.11 81.56 50.71 77.14 58. 78.01 69.33 73.33 67.98 77.49 Table 5: Performance under extreme scenarios with only one test sample (average of five trials). Our method still functions properly and adapts better than the fully trained single-model approach."
        },
        {
            "title": "4.6 Number of Source Domains",
            "content": "Mode1 Mode2 SaM (Ours) We set six source domains in our experiments. This section presents preliminary analysis of how the number of source domains affects performance, as shown in Table 4. While setting more source domains increases the diversity of candidate models during selection, it does not always lead to better results. The performance relies on the similarity between source and target domains, including conceptual relevance, data distribution, and overlap in entity types. Therefore, its important to balance the number of source domains, their similarity to target domains, and the overall complexity."
        },
        {
            "title": "4.7 Limited Target Resources Scenarios",
            "content": "The model selection process leverages targetdomain raw texts for domain similarity calculation and sampling evaluation, typically few hundred samples for the former and 10 for the latter. To test our method under more constrained settings, we simulate an extreme case with only one targetdomain instance. As shown in Table 5, our method remains effective. However, very small sample pools can undermine the stability of expert selection, especially for the strategy using domain similarity. In such cases, we recommend using data augmentation to expand the sample pool."
        },
        {
            "title": "4.8 Weighting Experts for Merging",
            "content": "We employ two metrics, domain similarity and sampling evaluation, to select expert models. These metrics reflect the importance of experts. Our framework does not consider the importance of experts but instead assigns equal weight to all expert models. To investigate the impact, we pro-"
        },
        {
            "title": "Average",
            "content": "60.07 62.00 66.35 65.54 62.27 71.20 53.84 63.04 59.33 62.03 65.17 66.45 62.15 71.48 52.84 62.78 60.01 61.99 65.93 67.05 62.41 71.65 52.90 63. Table 6: Weighting experts for merging. Modei denotes the method used to weight model parameters. pose two simple weighting strategies, as shown in Table 6: (1) Mode1 empirically assigns weights (1.5, 1.0, 0.5) to the top three selected experts. (2) Mode2 uses the middle-ranked metric value as normalization factor to scale the three experts metrics for weighting. Experimental results suggest that weighting has the potential to improve performance (Mode1), aligning with intuitive expectations. However, excessive reliance on heuristics may not always be justified. For example, while Mode2 theoretically provides more precise weighting based on expert importance, it underperforms compared to Mode1 and Ours."
        },
        {
            "title": "4.9 Finer Adaptation for Target Domains",
            "content": "Beyond the weighted merging strategy in Section 4.8, another potential approach for improvements is adopting finer adaptation. Specifically, we apply clustering to divide the target domain into multiple splits, selecting and merging expert models separately for each. However, as shown in Figure 4, this finer adaptation does not enhance performance. Instead, it results in an overall decline. dare-linear ties dare-ties"
        },
        {
            "title": "Average",
            "content": "54.67 56.22 66.31 69.11 62.01 68.31 52.12 61.25 60.01 61.99 65.93 67.05 62.41 71.65 52.90 63.13 52.57 55.81 66.22 66.42 61.07 65.12 51.40 59. Table 7: Comparison of different merging techniques. We compare linear, dare, ties, and some combinations. Llama Qwen T-SC E-SC Ours T-SC E-SC Ours AI Literature Music Politics Science Movie Restaurant 54.20 64.13 70.01 63.58 59.01 64.72 50.21 60.31 58.98 71.06 66.27 60.03 67.78 50.31 60.98 66.93 73.53 74.47 62.60 72.17 52.99 51.47 54.61 61.58 55.76 58.73 66.58 52.50 57.36 56.08 68.01 65.99 60.99 67.86 50.43 60.01 61.99 65.93 67.05 62.41 71.65 52. Average 60.84 62.11 66.24 57.32 60. 63.13 Table 8: Comparison with self-consistency (SC) methods. T-SC employs fully trained model to generate multiple outputs by adjusting the Temperature hyperparameter to the ensemble, while E-SC ensemble outputs from different Expert models. ful content but almost lose the ability to generate structured outputs, which is crucial for NER and other IE tasks. Consequently, their performance is bad, though minor improvements can be achieved through extensive post-processing on model outputs. However, combining dare with linear yields improved results. Both dare and ties address the issue of parameter redundancy for merging. These findings suggest that handling parameter redundancy is crucial for NER and similar structured output tasks. Additionally, the relatively weaker performance of the dare-ties combination may stem from excessive redundancy reduction, which could compromise useful capabilities of models."
        },
        {
            "title": "4.11 Comparing with Self-Consistency",
            "content": "Considering that we trained multiple expert models, an intuitive approach is self-consistency (SC) (Wang et al., 2023b), which ensembles multiple outputs through voting. Results are shown in Table 8, where T-SC employs full-trained model to generate multiple outputs by adjusting the Temperature hyperparameter to ensemble, while Figure 4: Performance changes with the number of data splits. The horizontal axis is the number of splits, and the vertical denotes the entity-level F1 scores. Figure 5: Performance changes with the number of data splits. We aggregate data from all domains into unified dataset and subsequently conduct the split analysis. To investigate this further, we aggregate data from all seven domains into single dataset and conduct experiments. As shown in Figure 5, clustering improves performance across various strategies, including Sampling Evaluation (SE), Domain Similarity (DS), and their combination. Notably, despite the dataset spanning seven domains, the best performance is achieved when clustering divides it into only two or three groups. This is intuitively reasonable, as these seven domains originate from two broader datasets (CrossNER and MIT). The above analysis indicates that while finer adaptation to the target domain may bring improvements, excessive refinement without constraints may be counterproductive. For example, treating each data point as distinct domain might seem optimal in theory but leads to poor performance in practice."
        },
        {
            "title": "4.10 Analysis of Merging Technique",
            "content": "In addition to the Ties-Merging algorithm (ties) we employed, this section analyzes alternative merging strategies, including linear, dare (Yu et al., 2024), and their combinations. As shown in Table 7, dare and ties are effective. We do not present the performance of linear strategy, as this strategy leads to significant degradation. Specifically, models merged via the linear approach still produce some meaningE-SC ensemble outputs from different Experts (though this slightly deviates from the strict definition of self-consistency, we refer to \"SC\" for simplicity). For our NER task, traditional SC (TSC) shows limited improvement, while E-SC offers more significant gains due to greater output diversity. However, both SC strategies perform worse than our method and require higher inference costs."
        },
        {
            "title": "4.12 Framework Generalization",
            "content": "Non-strict Domain In our experiments, each expert corresponds to domain with real-world significance, such as news or law. To explore more flexible scenario, we further test on non-strict domains. Specifically, we cluster each domain dataset into five subsets, and then sample one subset from each domain to create five new datasets, which are used to train five new expert models. The results in Table 9 indicate that our framework remains effective. Notably, the experts in this setting are no longer tied to specific domains but function as general-domain models while retaining diverse capabilities. This suggests that the key requirement is set of experts with complementary strengths, which can enhance overall performance through mutual reinforcement. Multilingual Scenarios We extend our framework to multilingual scenarios and conduct preliminary experiments on six languages from the WikiANN dataset (Pan et al., 2017), including German (de), English (en), Spanish (es), Dutch (nl), Russian (ru), and Chinese (zh). We train model for each language. When evaluating target language, the model trained on that language is not used. The results in Table 10 provide several initial evidence that our framework has the potential to extend to multilingual scenarios."
        },
        {
            "title": "5 Future Work",
            "content": "Unified IE and Other Tasks We conduct experimental analyses with NER as case study. Some prior studies train unified model for multiple IE tasks, including NER, relation extraction (RE), event extraction (EE), etc. Our framework can be naturally extended to broader IE setting by incorporating additional IE data to train IE experts. Additionally, our method extends beyond these tasks to wide range of applications. Detailed and Complete Design Our extended experiments investigated several optimization strategies. Further improvements could be realized by: Expert Model E1 E2 E4 E5 AI Literature Music Politics Science Movie Restaurant 56.98 65.38 62.38 60.94 62.56 68.75 44.06 57.98 65.45 66.00 66.49 63.31 61.97 34.94 56.26 55.30 63.63 63.52 61.38 66.58 49. 48.51 56.27 63.88 57.54 58.23 64.68 50.05 55.46 62.20 65.17 58.50 65.72 65.02 43.35 Ours 60.88 67.03 67.42 66.09 66.75 67.98 52.66 Average 60. 59.45 59.46 57.02 59.35 64.12 Table 9: Analysis of experts for non-strict domains. We employ clustering to build five source domains and train expert models. Expert Model de en es nl ru zh de en es nl ru zh 72.29 86.09 84.31 75.91 49.07 80.69 87.28 86.75 75.68 49.18 80.75 77.10 85.02 74.44 46.56 81.00 77.51 91.30 76.93 48. 81.74 73.08 88.46 84.79 49.93 74.99 69.18 78.85 81.41 61.29 Ours 82.42 76.84 89.67 87.59 78.16 51.37 Avg 73. 75.92 72.77 74.99 75.60 73.14 77. Table 10: Analysis of experts for different languages. When evaluating target language, the model trained on that language is not used. (1) Incorporating more task-specific designs. For instance, alongside domain-level similarity, we could also leverage entity-type similarity when selecting source models. (2) Dynamically determining the number of merged models, k. Model merging may be unnecessary for certain target domains. As shown in Appendix B, when the target and source domains coincide, the single corresponding model already delivers optimal results. Section 4.5 demonstrates that the best choice of varies across different target domains. Additionally, our framework is agnostic to the model architecture and readily extends to other model types other than the Llama and Qwen LLM families."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose the Select and Merging (SaM) framework for NER, which dynamically selects valuable domain expert models for the target domain and employs model merging to obtain the task-specific model. Compared to prior studies, we possess superior adaptability and scalability. Experimental results demonstrate the effectiveness of our framework. Extensive analysis further provides insights into potential improvements, practical experience, and broader extensions of our approach."
        },
        {
            "title": "Limitations",
            "content": "We acknowledge the following limitations of our work: (1) Maintaining multiple expert models introduces some additional storage overhead, despite the use of LoRA. (2) For domain similarity calculation and clustering analysis, we simply employed widely used encoder model from the HuggingFace repository to obtain text embeddings. Further optimization is possible. (3) Our analysis is limited to Named Entity Recognition (NER). Further experiments are needed for other IE tasks."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported in part by the National Natural Science Foundation of China under Grant No.62276110, No.62172039. The authors would also like to thank anonymous reviewers for their comments on improving the quality of this paper."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Ting Wai Terence Au, Vasileios Lampos, and Ingemar Cox. 2022. E-neran annotated named entity recogIn Proceedings of the nition corpus of legal text. Natural Legal Language Processing Workshop 2022, pages 246255. Pei Chen, Haotian Xu, Cheng Zhang, and Ruihong Huang. 2022. Crossroads, buildings and neighborhoods: dataset for fine-grained location recognition. In Proceedings of the 2022 conference of the North American chapter of the association for computational linguistics: human language technologies, pages 33293339. Nigel Collier, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Jin-Dong Kim. 2004. Introduction to the In Proceedbio-entity recognition task at jnlpba. ings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP), pages 7378. Leon Derczynski, Kalina Bontcheva, and Ian Roberts. 2016. Broad twitter corpus: diverse named entity recognition resource. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1169 1179. Yuyang Ding, Juntao Li, Pinzheng Wang, Zecheng Tang, Yan Bowen, and Min Zhang. 2024a. Rethinking negative instances for generative named entity recognition. In Findings of the Association for Computational Linguistics: ACL 2024, pages 34613475. Zhuojun Ding, Wei Wei, Xiaoye Qu, and Dangyang Chen. 2024b. Improving pseudo labels with globallocal denoising framework for cross-lingual named entity recognition. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, pages 62526260. Rezarta Islamaj Dogan, Robert Leaman, and Zhiyong Lu. 2014. Ncbi disease corpus: resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47:110. Chenghao Fan, Zhenyi Lu, Sichen Liu, Xiaoye Qu, Wei Wei, Chengfeng Gu, and Yu Cheng. 2025. Make lora great again: Boosting lora with adaptive singular values and mixture-of-experts optimization alignment. Chenghao Fan, Zhenyi Lu, Wei Wei, Jie Tian, Xiaoye Qu, Dangyang Chen, and Yu Cheng. 2024a. On giants shoulders: Effortless weak to strong by dynamic logits fusion. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Chenghao Fan, Wei Wei, Xiaoye Qu, Zhenyi Lu, Wenfeng Xie, Yu Cheng, and Dangyang Chen. 2024b. Enhancing low-resource relation representations through multi-view decoupling. Proceedings of the AAAI Conference on Artificial Intelligence, 38:1796817976. Runwei Guan, Ka Lok Man, Feifan Chen, Shanliang Yao, Rongsheng Hu, Xiaohui Zhu, Jeremy Smith, Eng Gee Lim, and Yutao Yue. 2024. Findvehicle and vehiclefinder: ner dataset for natural languagebased vehicle retrieval and keyword-based crossmodal vehicle retrieval system. Multimedia Tools and Applications, 83(8):2484124874. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Quanjiang Guo, Yihong Dong, Ling Tian, Zhao Kang, Yu Zhang, and Sijie Wang. 2025b. Baner: Boundaryaware llms for few-shot named entity recognition. In Proceedings of the 31st International Conference on Computational Linguistics, pages 1037510389. Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations. Leon Derczynski, Eric Nichols, Marieke Van Erp, and Nut Limsopatham. 2017. Results of the wnut2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 140147. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2023. Editing models with task arithmetic. In The Eleventh International Conference on Learning Representations. Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. 2023. Dataless knowledge fusion by merging weights of language models. In The Eleventh International Conference on Learning Representations. Martin Krallinger, Obdulia Rabal, Florian Leitner, Miguel Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan Lu, Donghong Ji, Daniel Lowe, et al. 2015. The chemdner corpus of chemicals and drugs and its annotation principles. Journal of cheminformatics, 7:117. Aman Kumar and Binil Starly. 2022. fabner: information extraction from manufacturing process science domain literature using named entity recognition. Journal of Intelligent Manufacturing, 33(8):2393 2407. Jiao Li, Yueping Sun, Robin Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn Mattingly, Thomas Wiegers, and Zhiyong Lu. 2016. Biocreative cdr task corpus: resource for chemical disease relation extraction. Database, 2016. Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuan-Jing Huang, and Xipeng Qiu. 2023. Codeie: Large code generation models are better fewshot information extractors. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1533915353. Zixuan Li, Yutao Zeng, Yuxin Zuo, Weicheng Ren, Wenxuan Liu, Miao Su, Yucan Guo, Yantao Liu, Lixiang Lixiang, Zhilei Hu, Long Bai, Wei Li, Yidan Liu, Pan Yang, Xiaolong Jin, Jiafeng Guo, and Xueqi Cheng. 2024. KnowCoder: Coding structured knowledge into llms for universal information extraction. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 87588779. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2024. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations. Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya, Andrea Madotto, and Pascale Fung. 2021. Crossner: Evaluating crossdomain named entity recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1345213460. Zhenyi Lu, Chenghao Fan, Wei Wei, Xiaoye Qu, Dangyang Chen, and Yu Cheng. 2024. Twin-merging: Dynamic integration of modular expertise in model merging. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Michael Matena and Colin Raffel. 2022. Merging models with fisher-weighted averaging. In Advances in Neural Information Processing Systems. Alexis Mitchell, Stephanie Strassel, Shudong Huang, and Ramez Zakhary. 2005. Ace 2004 multilingual training corpus. LDC corpora. Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Cross-lingual name tagging and linking for 282 languages. In Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: long papers), pages 19461958. Chaoxu Pang, Yixuan Cao, Qiang Ding, and Ping Luo. 2023. Guideline learning for in-context information extraction. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1537215389. Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Björkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using ontonotes. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143152. Sampo Pyysalo and Sophia Ananiadou. 2014. Anatomical entity mention recognition at literature scale. Bioinformatics, 30(6):868875. Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, and Juanzi Li. 2024. Adelie: Aligning large language models on information extraction. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 73717387. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36. Oscar Sainz, Iker García-Ferrero, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, and Eneko Agirre. 2024. Gollie: Annotation guidelines improve zero-shot information-extraction. In The Twelfth International Conference on Learning Representations. Larry Smith, Lorraine Tanabe, Rie Johnson nee Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, YuShi Lin, Roman Klinger, Christoph Friedrich, Kuzman Ganchev, et al. 2008. Overview of biocreative ii gene mention recognition. Genome biology, 9:119. Zeliang Tong, Zhuojun Ding, and Wei Wei. 2025. Evoprompt: Evolving prompts for enhanced zero-shot named entity recognition with large language models. In Proceedings of the 31st International Conference on Computational Linguistics, pages 51365153. Asahi Ushio, Francesco Barbieri, Vitor Sousa, Leonardo Neves, and Jose Camacho-Collados. 2022. Named entity recognition in twitter: dataset and analysis on short-term temporal shifts. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 309319. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Yuming Yang, Wantong Zhao, Caishuang Huang, Junjie Ye, Xiao Wang, Huiyuan Zheng, Yang Nan, Yuran Wang, Xueying Xu, Kaixin Huang, Yunke Zhang, Tao Gui, Qi Zhang, and Xuanjing Huang. 2025. Beyond boundaries: Learning universal entity taxonomy across datasets and languages for open named entity recognition. In Proceedings of the 31st International Conference on Computational Linguistics. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning. Urchade Zaratiana, Nadi Tomeh, Pierre Holat, and Thierry Charnois. 2024. Gliner: Generalist model for named entity recognition using bidirectional transformer. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 53645376. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2024a. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36. Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung Poon. 2024b. Universalner: Targeted distillation from large language models for open named entity recognition. In The Twelfth International Conference on Learning Representations. Asahi Ushio and Jose Camacho-Collados. 2021. Tner: An all-round python library for transformerbased named entity recognition. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 5362. Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. Ace 2005 multilingual training corpus. LDC corpora. Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, and Sadao Kurohashi. 2023. Gpt-re: In-context learning for relation extraction using large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 35343547. Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, et al. 2023a. Instructuie: Multitask instruction tuning for unified information extraction. arXiv preprint arXiv:2304.08085. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Zihan Wang, Jingbo Shang, Liyuan Liu, Lihao Lu, Jiacheng Liu, and Jiawei Han. 2019. Crossweigh: Training named entity tagger from imperfect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 51545163. Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu Liu, and Hongwei Wang. 2023. Empirical study of zero-shot ner with chatgpt. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 79357956. Tingyu Xie, Qi Li, Yan Zhang, Zuozhu Liu, and Hongwei Wang. 2024. Self-improving for zero-shot named entity recognition with large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 583593. Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Yang Wang, and Enhong Chen. 2024. Large language models for generative information extraction: survey. Frontiers of Computer Science, 18(6):186357. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Ties-merging: Resolving interference when merging models. In Annual Conference on Neural Information Processing Systems."
        },
        {
            "title": "A Experimental Details",
            "content": "Training Data We collected 17 widely used NER datasets and categorized them into six domains based on their data sources. Table 11 presents detailed statistics for each dataset. As described in Section 3.1, we sample these datasets based on the proportion of their entity types while ensuring that the total data volume per domain remains between 10,000 and 50,000. The distribution of the training data across domains is illustrated in Figure 6, and the exact number of sampled instances per dataset is listed in the #Sampled column of Table 11. Figure 7 and 8 show examples of the formatted training data we constructed. Implementation Details We adopt the Qwen2.57B 3 and Llama3.1-8B 4 as foundations and train expert models using LoRA (Hu et al., 2022). The LoRA rank is set to 32, with three training epochs, batch size of 16, learning rate of 2e 5, and warmup ratio of 0.05. During LoRA training, all linear layers are activated. We set the temperature to 0 for LLMs when inference. All experiments are conducted on one NVIDIA 4090 GPU."
        },
        {
            "title": "B Data Merging or Model Merging",
            "content": "We use Data Merging to denote the prior approaches of training unified model by integrating data from multiple domains and Model Merging to denote synthesizing new model by merging the parameters of expert models. This section presents preliminary analysis that motivates the adoption of the model merging strategy. Specifically, we compare three types of models: (1) Experts, which are trained on single-domain data. (2) Data Merging, which is trained on mixture of all domain data. (3) Model Merging, which is obtained by merging the parameters of all expert models. We conduct evaluations on both in-domain and out-of-domain settings, with the results presented in Tables 12 and 13. Our key observations are as follows: (1) Data Merging consistently yields suboptimal performance in both settings. (2) In in-domain tasks, Model Merging also performs suboptimally and is inferior to Data Merging. (3) In out-of-domain tasks, Model Merging generally achieves the best performance. The aforementioned observation is our primary motivation for introducing model merging. For 3https://huggingface.co/Qwen/Qwen2.5-7B 4https://huggingface.co/meta-llama/Llama-3. 1-8B Figure 6: Distribution of the training data. in-domain tasks, directly utilizing the corresponding expert model is sufficient. For out-of-domain tasks, merging expert models improves generalization. However, experimental results indicate that in some cases, single expert model performs better, especially for in-domain tasks where merging may be unnecessary. Combined with Section 4.5, the number of selected experts, k, could be dynamically determined, with = 1 being valid consideration. This is promising direction for further enhancing the adaptability of our framework."
        },
        {
            "title": "C Analysis of Cost",
            "content": "Parameter Count and Storage Cost We assume denotes the model dimension, the rank of LoRA adapters, the number of layers, the number of domain-specific experts, and the vocabulary size. For computational simplicity, we adopt simplified Transformer architecture (e.g., omitting grouped-query attention mechanisms) in our base models (Llama3.1 and Qwen2.5). Since our LoRA implementation applies to all linear layers, each domain-specific expert requires 18HrL additional storage parameters. Consequently, storing experts incurs total overhead of 18HrL. The base model itself requires approximately (12H 2 + 13H)L + parameters. Given that nr H, the additional storage overhead remains negligible. During inference, we merge LoRA adapters into one or two task-specific models, achieving state-ofthe-art performance with only equivalent or doubled storage costs compared to the base model. Domain Dataset #Type #Train #Dev #Test #Sampled Biomedical Law News AnatEM (Pyysalo and Ananiadou, 2014) JNLPBA (Collier et al., 2004) bc2gm (Smith et al., 2008) bc4chemd (Krallinger et al., 2015) bc5cdr (Li et al., 2016) ncbi (Dogan et al., 2014) E-NER (Au et al., 2022) ACE04 (Mitchell et al., 2005) ACE05 (Walker et al., 2006) conllpp (Wang et al., 2019) OntoNotes (Pradhan et al., 2013) Social media WNUT2017 (Derczynski et al., 2017) HarveyNER (Chen et al., 2022) BroadTweetCorpus (Derczynski et al., 2016) TweetNER7 (Ushio et al., 2022) STEM Traffic FabNER (Kumar and Starly, 2022) FindVehicle (Guan et al., 2024) 1 5 1 1 2 1 7 7 7 4 18 6 4 3 11 8 5,861 16,691 12,500 30,682 4,560 5,432 2,118 1,855 2,500 30,639 4,581 923 3,830 3,856 5,000 26,364 4,797 940 9, 1,164 1,165 6,202 7,299 14,041 59,924 3,394 3,967 5,334 7,111 745 971 3,250 8,528 1,009 1,301 2,001 812 1,060 3,452 8,262 1,287 1,303 2,000 576 9,435 2,182 2,064 21, 20,777 20,777 3,297 16,487 3,297 3,297 6,594 3,297 10,000 9,722 9,722 5,555 25,000 6,094 4,063 3,047 7, 10,000 21,565 Table 11: Statistics of raw training data and the number of sampled instances for training."
        },
        {
            "title": "Model Merging",
            "content": "82.57 40.40 53.83 53.11 28.22 42.14 80.53 67.57 40.02 84.79 48.40 41.31 16.26 28.23 81.87 64. 36.61 42.36 85.94 42.06 22.81 32.67 84.97 55.47 48.09 46.90 48.28 66.57 23.53 42.01 60.27 56. 27.27 17.92 23.53 24.17 76.99 20.23 21.05 32.48 41.84 22.96 24.96 99.96 77.40 98.91 31.80 45. Table 12: In-domain performance of expert models, full data trained model (Data Merging), and model obtained from merging all expert models (Model Merging). AI"
        },
        {
            "title": "Model Merging",
            "content": "56.90 39.85 41.77 53.84 41.01 49.97 51.38 57.47 59.01 56.88 41.03 55.83 41.35 50.82 53.46 61. 63.98 64.33 54.74 61.14 43.35 62.20 65.73 62.72 40.36 60.50 45.41 64.09 61.12 54.99 64.66 60. 60.21 55.92 49.66 57.24 37.38 53.25 59.39 61.08 57.96 63.26 62.13 59.29 46.61 65.45 66.66 69. 39.41 47.31 39.28 42.63 26.78 52.11 52.69 47.78 Table 13: Our-of-domain performance of expert models, full data trained model (Data Merging), and model obtained from merging all expert models (Model Merging). Computation FLOPs Analysis Compared to multi-task full fine-tuning (FFT), our approach utilizes the same amount of training data to produce multiple LoRA models, resulting in identical computational costs during training. During inference, however, our method employs single task-specific"
        },
        {
            "title": "SaM SaMeco",
            "content": "InstructUIE UniNER GoLLIE B2NER"
        },
        {
            "title": "Ours",
            "content": "Training Instances Inference Times Storage (Normalized) 215.9K 1 1 Performance (Average) 47.84 45.9K 1 1 61. 165.2K 1 1 51.9K 148.1K 148.1K 148.1K 1 1 1 1 2 1+0.02n 1 1+0.02n 58.39 64.57 60.94 66.24 65.49 Table 14: Comparison of Resource Requirements. We compare unified models trained across multiple domains with our merging-based approach. SaMeco (economic) refers to integrating two task-specific models into single one (details in Section 4.4). For storage, we normalize the value by setting the model size to 1. Here, represents the number of experts. We achieve superior results with minimal additional overhead, particularly with our SaMeco."
        },
        {
            "title": "Qwen",
            "content": "Mode1 Mode2 Mode3 SaM(Ours) Mode1 Mode2 Mode3 SaM(Ours)"
        },
        {
            "title": "Average",
            "content": "60.36 56.86 63.78 66.05 58.37 70.66 52.82 61.27 58.19 62.68 66.50 68.27 61.73 70.66 52.85 62.98 61.31 58.90 66.85 61.30 63.12 70.50 52.23 62. 60.01 61.99 65.93 67.05 62.41 71.65 52.90 63.13 54.73 60.16 68.98 71.16 62.19 67.07 51.69 62.28 55.62 59.87 72.89 75.42 59.58 68.91 51.69 63. 57.90 63.42 72.42 73.61 62.78 73.44 54.84 65.49 60.98 66.93 73.53 74.47 62.60 72.17 52.99 66.24 Table 15: Merging into single task model. Complete experimental results of Section 4.4. modeleither derived from single strategy or by integrating models from both strategieswhich requires only one LoRA adapter. This achieves comparable inference costs to multi-task FFT while delivering superior performance. When leveraging models from both strategies simultaneously, our approach incurs twice the inference cost but further enhances performance, offering flexible trade-off between efficiency and effectiveness. Table 14 compares our approach with several recent works that train unified model across multiple domains. We compare (1) the amount of training data (with instance-level data size provided for reference), (2) the number of inference rounds required for model prediction, and (3) storage space requirements (with model size as the reference unit). We also report the average performance."
        },
        {
            "title": "D Experimental Supplements",
            "content": "Section 4.4 extracts single expert set for merging and presents the results based on Qwen. Here, we supplement the results of Llama in Table 15. Our framework is based on two perspectives: Domain Similarity (DS) and Sampling Evaluation (SE). The experimental section reports the overall performance of the framework. Here, we provide several additional experimental results regarding these two strategies, respectively. Section 4.5 explores the relationship between model performance and the number of merged experts. Here, we present results for each strategy, as shown in Figures 9 and 10. It can be seen that the two strategies exhibit similar overall trends, but with distinct differences. This further indicates that both strategies are important, highlighting the importance and complementarity of each. Section 4.9 discusses fine-grained adaptation. Due to the compact nature of the target domain, this does not bring improvements and may even reduce the performance. Here, we analyze the performance of each strategy individually. As shown in Figure 11, the Domain Similarity strategy has minimal impact on the subdivision (with performance difference of less than 0.2 points on average), supporting the hypothesis that the target domain is already too homogeneous to generate distinct splits. In contrast, Figure 12 shows significant changes when using Sampling Evaluation, with general downward trend, which accounts for the overall performance degradation. Figure 7: Formatted training data example. The example consists of task instructions, inputs, and outputs for training. For the task instructions, the Data Source Description, Entity Type drop/mask, and ICL Demonstrations are optional, with details in section 3.1. We adopt three output formats: JSON, enumeration, and natural language descriptions. The output format of ICL Demonstrations and Answers should be consistent with the specified. Figure 8: Another example of our formatted training data. The instance here is the same as that of Figure 7, adopting the entity type drop and mask processing methods. Figure 9: Performance changes with the number of expert models (denotes as k). The horizontal axis is the number of experts for merging, and the vertical denotes the entity-level F1 scores. Only using Domain Similarity for expert selection. It can be seen that the optimal varies across target domains, typically ranging from 2 to 4. Figure 10: Performance changes with the number of expert models. The horizontal axis is the number of experts for merging, and the vertical denotes the entity-level F1 scores. Only using Sampling Evaluation for expert selection. It exhibits similar overall trends to Figure 9, but with distinct differences, indicating that both selecting strategies are important and complementary. Figure 11: Performance changes with the number of data splits. The horizontal axis is the number of splits, and the vertical denotes the entity-level F1 scores. Only using Domain Similarity for expert selection. It can be seen that the Domain Similarity strategy has minimal impact, since data from the target domain may be too homogeneous. Figure 12: Performance changes with the number of data splits. The horizontal axis is the number of splits, and the vertical denotes the entity-level F1 scores. Only using Sampling Evaluation for expert selection. It shows significant changes when using Sampling Evaluation, with general downward trend."
        }
    ],
    "affiliations": [
        "School of Computer Science & Technology, Huazhong University of Science and Technology"
    ]
}