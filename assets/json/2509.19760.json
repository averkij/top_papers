{
    "paper_title": "Logics-Parsing Technical Report",
    "authors": [
        "Xiangyang Chen",
        "Shuzhao Li",
        "Xiuwen Zhu",
        "Yongfan Chen",
        "Fan Yang",
        "Cheng Fang",
        "Lin Qu",
        "Xiaoxiao Xu",
        "Hu Wei",
        "Minggang Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Large Vision-Language models (LVLM) have spurred significant progress in document parsing task. Compared to traditional pipeline-based methods, end-to-end paradigms have shown their excellence in converting PDF images into structured outputs through integrated Optical Character Recognition (OCR), table recognition, mathematical formula recognition and so on. However, the absence of explicit analytical stages for document layouts and reading orders limits the LVLM's capability in handling complex document types such as multi-column newspapers or posters. To address this limitation, we propose in this report Logics-Parsing: an end-to-end LVLM-based model augmented with reinforcement learning. Our model incorporates meticulously designed reward mechanisms to optimize complex layout analysis and reading order inference. In addition, we expand the model's versatility by incorporating diverse data types such as chemical formulas and handwritten Chinese characters into supervised fine-tuning. Finally, to enable rigorous evaluation of our approach, we introduce LogicsParsingBench, a curated set of 1,078 page-level PDF images spanning nine major categories and over twenty sub-categories, which will be released later. Comprehensive experiments conducted on LogicsParsingBench have validated the efficacy and State-of-the-art (SOTA) performance of our proposed model across diverse document analysis scenarios. Project Page: https://github.com/alibaba/Logics-Parsing"
        },
        {
            "title": "Start",
            "content": "Logics-Parsing Technical Report Logics Team Alibaba Group (For the complete list of authors, please refer to the Contributors section.) 5 2 0 2 4 2 ] . [ 1 0 6 7 9 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Large Vision-Language models (LVLM) have spurred significant progress in document parsing task. Compared to traditional pipeline-based methods, end-toend paradigms have shown their excellence in converting PDF images into structured outputs through integrated Optical Character Recognition (OCR), table recognition, mathematical formula recognition and so on. However, the absence of explicit analytical stages for document layouts and reading orders limits the LVLMs capability in handling complex document types such as multi-column newspapers or posters. To address this limitation, we propose in this report Logics-Parsing: an end-to-end LVLM-based model augmented with reinforcement learning. Our model incorporates meticulously designed reward mechanisms to optimize complex layout analysis and reading order inference. In addition, we expand the models versatility by incorporating diverse data types such as chemical formulas and handwritten Chinese characters into supervised fine-tuning. Finally, to enable rigorous evaluation of our approach, we introduce LogicsParsingBench, curated set of 1,078 pagelevel PDF images spanning nine major categories and over twenty sub-categories, which will be released later. Comprehensive experiments conducted on LogicsParsingBench have validated the efficacy and State-of-the-art (SOTA) performance of our proposed model across diverse document analysis scenarios. Project Page: https://github.com/alibaba/Logics-Parsing 1. Introduction Automatically parsing structured content from document PDF images has been longstanding challenge in Document AI. The extracted elements including plain text, tables, formulas are foundational elements for advanced AI-based document analysis, Retrieval-augmented Generation (RAG) and ultimately the Artificial General Intelligence (AGI). Traditional OCR systems [7, 20] typically adopt pipelinebased architectures which sequentially deploy expert models to address sub-tasks including element detection, region cropping, element recognition and so on. However, this modular approach suffers from three key limitations: First, each expert model only optimizes its own local objectives, which does not guarantee global optima for the entire document parsing task. Second, the computational overhead of training and deploying multiple models is substantial. Last but not least, the independent processing of multiple sub-components in one document disrupts inter-element information exchange and impairs holistic context modeling. To alleviate these drawbacks, end-to-end OCR-2.0 models [24, 25] have been proposed to reduce maintenance costs and provide global optimization for various sub-tasks. In recent years, the rapid development of large visionlanguage models (LVLM) [2, 3, 12, 15] has largely improved the ability to handle diverse modalities of data. Despite the strong generalization ability, most LVLMs are primarily trained for reasoning tasks while exhibiting limitations for visual-text perception scenarios. Moreover, the CLIP objective [18] adopted by popular LVLMs visual encoder like LLaVAR [27], mPLUG-DocOwl [26] and olmOCR [17] mainly focus on coarse-grained visual-text semantic alignment which neglects the fine-grained text information contained in images. Additionally, the pretrained clip vision models typically support fixed input resolutions (e.g., 224224 or 336336 pixels), which are inadequate to meet the demands of parsing documents containing dense, small-sized text elements. To address these issues, Vary [24] has parallelly trained high-resolution visual encoder specially for document-level OCR task. Qwen2-VL series [3, 22] have introduced the native dynamic resolution mechanism to eliminate the spatial constraints. Despite these improvements, end-to-end document parsing paradigms still face critical challenges. The next-token prediction objective widely adopted in current LVLMs naively optimizes the token-level alignment between input text images and output text, while overlooking the disruptive effects of complex layouts on reading order inference. To resolve this, Infinity-Parser [21] introduced Reinforcement learning into the document parsing task with three layout-aware rewards. Despite the specially designed rewards, the paragraph count objective did not consider the explicit position and sepaFigure 1. The overview of our proposed Logics-Parsing for layout-aware document parsing. ration of corresponding element block by only calculating the number of paragraphs, and GRPO [19] performed directly on Qwen-2.5-VL-7B [3] base model does not ensure prior foundation model for document parsing task, which may potentially mislead the direction of GRPO optimization. Recent study [4] has highlighted critical principle in post training of large-scale model: SFT memorizes and RL generalizes, which emphasizes the importance of SFT for model output format stabilization before RL for generalization. To address above limitations, in this report, we propose Logics-Parsing, an end-to-end LVLM-based framework augmented with reinforcement learning to establish robust and layout-aware document parsing model as shown in Fig. 1. Two-stage SFT-then-RL training strategy is adopted to ensure the effectiveness of our model. Firstly, through incorporating diverse data types including normal texts, mathematical formulas, tables, chemical formulas and handwritten Chinese characters into training, we fine-tune the Qwen2.5-VL-7B [3] with more than 300K high-quality page-level document images to build strong foundation for document parsing. Secondly, through layout-aware multicomponent reward mechanisms together with novel hardsample mining strategy, we augment the model with the ability to correctly identify the page layout structure, then enforce adherence to natural reading sequences under the identified layout. This stage markedly improves performance on complex layouts such as multi-column newspapers and posters. In addition, in order to rigorously evaluate our proposed Logics-Parsing model, we construct comprehensive benchmark containing 1,078 page-level PDF images spanning nine major categories (e.g., academic papers, technical reports) and over twenty sub-categories, called LogicsParsingBench. Compared to OminiDocBench [16], top-leading benchmark covering diverse document types with rich annotation information, our proposed LogicsParsingBench places greater emphasis on assessing complex layout handling and scientific content parsing, with several enhanced evaluation protocols introduced. In summary, the contributions of our work are listed as follows: We propose Logics-Parsing, an end-to-end Large VisionLanguage Model (LVLM)-based framework augmented with reinforcement learning. By explicitly modeling document layouts and supporting multimodal recognition of heterogeneous content types (e.g., text, mathematical formulas, tables), the framework enables the construction of highly capable and broadly applicable model on complex, real-world document parsing scenarios. To endow Logics-Parsing with robust recognition across diverse content types, we collected more than 300k high quality page-level document images and annotate them via an automated pipeline followed by human-in-the-loop verification. The co-occurrence of plain text, mathematical formulas, tables, and other elements on single page offers comprehensive training for complex realworld document analysis. Extensive evaluations on LogicsDocBench comprehensive benchmark comprising 1,078 page-level PDF images across nine categories effectively validate that our method significantly improves structural understanding and content ordering in complex layouts, achieving superior performance in practical document parsing tasks. 2. Methodology In this section, we present the methodology of LogicsParing, with two-stage training paradigm designed to establish outstanding capabilities for both document parsing and fine-grained structural alignment. We will first provide an overview of this SFT-then-RL framework, then describe the construction of high-quality training datasets, and finally detail each training stage. and RL subset. This dual-subset design ensures the coherence between data preparation and corresponding model optimization phases. SFT Dataset. To enrich training diversity and establish strong foundational model for document parsing, we assembled data from two complementary sources. On the one hand, we systematically incorporate and normalize multiple well-established public datasets into our unified HTML annotation schema. Specifically, for pagelevel parsing, we curate subset of samples from olmOCRmix-0225 [17], systematically converting their original annotations into our standardized HTML format. For table recognition, which is crucial sub-task, we collect data from leading benchmarks including FinTabNet [28], TNCR [1], and PubTabNet [29]. This has provided the model with extensive exposure to wide variety of tabular structures. For chemical structure recognition, we directly employ the ChEBI-20-MM dataset [13], benchmark for molecular OCR. This offers targeted training for interpreting the specific syntax and layout rules of chemical diagrams. On the other hand, we build large-scale in-house dataset annotated via two-stage annotation pipeline consisting of automated extraction and expert model-based refinement. In the first place, Mathpix 1 is employed to preannotate broad corpus of public documents by extracting content, bounding boxes, and semantic types. Subsequently, to address complex layouts that Mathpix may struggle to parse, verification and correction step is conducted using Gemini 2.5 Pro [5]. This step focuses on correcting errors in challenging scenarios such as nested formulas and multi-layered tables, thereby ensuring the high accuracy and reliability of the annotations. Nevertheless, there still exists large number of highly complex documents and edge cases that the proposed automated annotation pipeline struggles to handle. For these samples, we introduce manual intervention, where approximately 10,000 page samples undergo rigorous review and correction by expert human annotators. Apart from this, we further provide manual annotations on document reading order for these samples which remain challenging to achieve through automated tools. This human-in-theloop approach has strictly guaranteed the annotation accuracy and broadens annotation coverage, yielding goldstandard subset that significantly strengthens the models understanding of both content and structural organization in real-world layouts. RL Dataset. Reinforcement learning usually benefits from small but high-quality dataset. To this end, we propose novel hard-sample mining strategy to identify focused subset from the SFT corpus described above. First, we manually curated around 4,000 pages from the 1https://mathpix.com/ Figure 2. The dataset construction and training pipeline of our proposed Logics-Parsing. 2.1. Overview As demonstrated in Fig. 2, the whole training process beIn this gins with Supervised Fine-Tuning (SFT) stage. stage, the model is trained on large-scale, versatile document dataset using standard next-token prediction objective. This equips the model with essential capabilities such as text recognition and associating text blocks with spatial coordinates. However, we argue that the autoregressive token-bytoken supervision of SFT provides indirect and often insufficient signals for page structure learning. For example, the token-level cross-entropy loss does not explicitly penalize an incorrect paragraph-level reading order, nor does it differentiate between minor and major errors in bounding-box coordinates. To address these limitations, the Layout-Centric Reinforcement Learning (LC-RL) is introduced. This stage leverages smaller, curated set of challenging, high-quality samples for direct structual analysis optimization. By employing multi-component reward function that explicitly evaluates text accuracy, layout precision, and logical reading order, we steer the models policy towards generating outputs that are not only textually accurate but also structurally coherent and logically sequenced. 2.2. Dataset To enable effective model training, we construct largescale, diverse, and high-quality dataset, with the HTML annotation format as one of its key features. As is known, HTML natively encodes the hierarchical structures of document (e.g., nested tables, lists) and enables efficient rendering for visualization. The construction of this comprehensive dataset is designed to align with the two-stage SFTthen-RL training strategy, which consists of SFT subset above human-in-the loop subset. These samples are specifically chosen for their intricate and challenging layouts, which have been shown to present substantial challenges for standard autoregressive models. Second, to collect samples that are difficult yet informative for the RL stage learning, we employ an inferenceguided hard-mining strategy. We run SFT model over the entire training set and select samples whose normalized edit distance between the prediction and the ground truth falls within specific range [0.5, 0.8]. This range effectively captures instances that the SFT model may partially understand but fails to parse perfectly, forming an ideal learning zone for subsequent RL-based refinement. In this way, approximately 4,000 additional samples are collected. Together, the above data-mining strategy has yielded roughly 8,000 high-quality, high-difficulty training samples for the LC-RL stage, whose effectiveness heavily hinges on training set that specifically targets the weakness of the SFT model. 2.3. Supervised Fine-Tuning"
        },
        {
            "title": "Rather",
            "content": "The first stage of our training paradigm focuses on Supervised Fine-Tuning (SFT), designed to imbue the model with foundational document understanding capabilities and align the model output with the target structured HTML format. than training large-scale Vision-Language Model (VLM) from scratch, we strategically choose to build our model upon powerful and pre-existing foundation, where Qwen2.5-VL-7B-Instruct [3] is selected. This decision is driven by its state-of-the-art performance among open-source VLMs, which not only established robust baseline for wide range of vision-language tasks, but also enabled efficient adaptation for downstream domainspecific tasks. Specifically, the model is fine-tuned on the comprehensive dataset described in Section 2.2 for single epoch through the standard next-token prediction objective, from which it learns to generate the target HTML sequence conditioned on the input document image. This process has yielded our baseline model called Logics-Parsing-SFT. Upon completion of this stage, the model has already demonstrated remarkable proficiency in core document analysis tasks, which not only performs accurate text recognition and spatial grounding, but also exhibits high degree of instruction-following capability, consistently generating structured HTML outputs aligned with prompt specifications. Despite solid foundation model the Logics-Parsing-SFT has established, the inherent limitations of token-level supervision motivates subsequent layout-centric reinforcement learning stage. 2.4. Layout-centric Reinforcement Learning As illustrated above, the next-token prediction objective adopted in the SFT stage exhibits inherent limitations for complex layout and reading order learning, particularly in processing documents with highly intricate layouts such as newspapers, magazines, or multi-column reports. For these samples, parsing the correct reading order can be non-trivial task even for humans. To overcome this challenge, we propose the second training phase based on Layout-centric Reinforcement Learning (LC-RL), employing Group Relative Policy Optimization (GRPO), which we find well-suited to this problem class. Through comprehensive multi-component reward function that performs direct evaluation on the output answer quality and carefully mined difficult-yet-informative dataset, we guide the model towards generating results that are more consistent with human reading habits. Concretely, in each training iteration, we parse both the models prediction and the ground-truth label to extract their constituent elements: textual content (including text, formulas, etc.) and their corresponding bounding box coordinates. These are then used to compute three distinct reward components. The first reward measures the character-level similarity between predicted text sequences and ground-truth ones through the negative normalized Levenshtein distance [9]. The second reward is designed to evaluate the localization accuracy between the predicted and target bounding boxes, which guides the model to ground each content element at the correct page location. The last reward mainly pays attention on optimizing the logical reading flow of the parsed content, which is calculated as the pairwise inversion count between reference and predicted paragraphs. This reward directly penalizes out-oforder content and is crucial for learning complex, non-linear reading paths. Finally, we form the final reward as linear combination of these three components for each sample. 3. Experiments 3.1. LogicsParsingBench: Benchmark for Complex Document Parsing To address the limitations of existing evaluation benchmarks, particularly their underrepresentation of documents with complex logical structures and specialized scientific content, we introduce LogicsParsingBench, new challenging benchmark meticulously curated to rigorously assess the capabilities of modern document parsing models. Comprising 1,078 high-quality pages, LogicsParsingBench is designed to reflect real-world complexity and advance the frontiers of the document intelligence. The benchmark is distinguished by three core features. Firstly, the samples in LogicsParsingBench span nine major categories specified in Fig. 3 and over twenty sub-categories in both English and Chinese, which ensures diverse and comprehensive coverage. One critical category our proposed benchmark strongly emphasizes on is the academic Figure 3. The overview of document types included in our proposed LogicsParsingBench. and scientific documents sourced from wide range of STEM fields (e.g., mathematics, physics, chemistry), characterized by high density of complex, nested mathematical formulas and intricate chemical structures. Additionally, rare and challenging document types such as music scroes and Chinese ancient books are also incorporated into the benchmark to further enrich the diversity of test samples. Secondly, LogicsParsingBench includes substantial portion of complex pages featuring multi-column layouts and text-graph mixed content, which not only imposes higher demands but also provides more strict evaluation for models layout analysis and generalization capabilities. Last but not least, more complex benchmark naturally necessitates more nuanced evaluation protocol. Built upon the top-leading open-source evaluation code from OmniDocBench [16], the evaluation protocols in LogicsParsingBench feature two key improvements listed as follows: Global Text Evaluation: We claim that the commonly used block-by-block text matching evaluation would impose excessive penalty for minor differences in paragraph segmentation granularity. To mitigate this, we adopt global text evaluation strategy, where all textual contents from one page (excluding headers and footers) are concatenated into single string before calculating the Levenshtein distance. This provides more holistic and robust measure for the core text recognition capability, invariant to the granularity of block segmentation. Stricter Content Normalization: To ensure fair comparisons across heterogeneous model outputs, we implemented stricter normalization protocols, especially for tabular data. By eliminating redundant whitespace and simplifying specific LaTeX formatting tags, the evaluation would emphasize on semantic accuracy rather than superficial formatting differences. 3.2. Implementation Details In our experiments, the Logics-Parsing model is built upon Qwen2.5-VL-7b-Instruct [3], which is strong foundation model for various multi-modality tasks. Leveraging the native dynamic resolution mechanism proposed in Qwen, we preserve the original aspect ratio of the input image while only constrain the total pixel count within 1024 1024 and the output token length within 8192. This design balances the high-resolution input expectation of the task and the computation overhead. The model is trained in two-stage manner. During the first SFT stage, the LLM component of Logics-Parsing is optimized to build foundational capabilities for document parsing, while the parameters of the vision encoder and vision-language projector are frozen. The model is optimized with batch size of 256 for 1 epoch, and the learning rate is set to 2e 5. During the second LC-RL stage, we transition to reinforcement learning setup implemented via the ROLL [23], adopting GRPO for policy optimization. For this fine-grained alignment, the hyperparameters are set more conservatively: the batch size is reduced to 32, and the learning rate is lowered to 1e 6. The model is trained for fixed 250 steps on our curated RL dataset. To enable rigorous evaluation of our approach, we benchmark Logics-Parsing against state-of-the-art (SOTA) methods over the proposed LogicsParsingBench. Most evaluation metrics are aligned with OmniDocBench for consistency. Specifically, we calculate Normalized Edit Distance (NED) [10] between the predicted outputs and ground-truth references across all text components, including the pure texts, the HTML sequences for tabular data, the LaTeX expressions for mathematical formulas and the Simplified Molecular Input Line Entry System (SMILES) strings for chemical molecules. For reading order evaluation, we likewise report NED computed over text-only content, with tables, images, and ignored components excluded from the final reading order calculation. 3.3. Main Results 3.3.1. Comparison with State-of-the-Art Methods In this section, we report the experimental results of our proposed Logics-Parsing against wide range of existing document parsing models in Tab. 1, including state-of-theart open-source models [3, 6, 8, 25] as well as leading commercial tools like doc2x 2 and TextIn 3. Fig. 4 also provides visualized comparison on the performance across 2https://doc2x.noedgeai.com/ 3https://www.textin.com Table 1. Comparisons with State-of-the-art methods on LogicsParsingBench. Model Type Methods Overall Edit ZH EN Text Edit ZH EN Formula Edit Table TEDS EN EN ZH ZH Table Edit ZH EN ReadOrder Edit Chemistry Edit HW Edit EN ALL ALL ZH Pipeline Tools Expert VLMs General VLMs doc2x 2 textin 3 Mathpix 1 pp structure v3 [6] mineru2 [20] marker 4 pix2text 5 Dolphin [8] dots.ocr 6 MonkeyOcr [11] OCRFlux 7 gotocr [25] olmocr [17] SmolDocling [14] Qwen2-VL-72B [22] Qwen2.5-VL-72B [3] doubao-1.6 8 GPT-5 9 Gemini2.5 pro [5] 0.209 0.153 0.128 0.220 0.212 0.324 0.447 0.208 0.186 0.193 0.252 0.247 0.341 0.657 0.298 0.233 0.188 0.242 0. 0.188 0.158 0.146 0.226 0.245 0.409 0.547 0.256 0.198 0.259 0.254 0.249 0.382 0.895 0.342 0.263 0.248 0.373 0.20 0.128 0.132 0.128 0.172 0.134 0.188 0.485 0.149 0.115 0.127 0.134 0.181 0.125 0.486 0.142 0.162 0.129 0.119 0. 0.194 0.190 0.152 0.29 0.195 0.289 0.577 0.189 0.169 0.236 0.195 0.213 0.205 0.932 0.244 0.24 0.219 0.36 0.155 0.377 0.185 0.06 0.272 0.280 0.285 0.312 0.334 0.291 0.262 0.326 0.231 0.719 0.859 0.431 0.251 0.273 0.398 0. 0.321 0.223 0.142 0.276 0.407 0.383 0.465 0.346 0.358 0.325 0.405 0.318 0.766 0.972 0.363 0.257 0.336 0.456 0.326 Expert VLMs Logics-Parsing 0. 0.145 0.089 0.139 0.106 0.165 81.1 76.7 86.2 66 67.5 65.5 64. 72.9 79.5 78.4 58.3 59.5 57.1 18.5 64.2 69.6 74.9 67.9 82.6 76.6 85.3 86.3 86.6 71.5 71.8 50.4 63.0 60.1 82.5 74.7 70.2 74.7 56.6 1.5 55.5 67 69.7 55.8 80. 79.5 0.148 0.176 0.120 0.237 0.228 0.593 0.566 0.192 0.172 0.186 0.358 0.38 0.327 0.86 0.425 0.313 0.180 0.26 0.154 0.115 0.113 0.127 0.193 0.203 0.702 0.613 0.35 0.141 0.294 0.260 0.299 0.389 0. 0.581 0.353 0.288 0.397 0.182 0.146 0.118 0.204 0.201 0.205 0.23 0.424 0.160 0.165 0.197 0.191 0.195 0.191 0.413 0.193 0.205 0.171 0.191 0.181 0.165 0. 0.136 0.122 0.104 0.164 0.143 0.177 0.262 0.534 0.139 0.123 0.180 0.156 0.164 0.169 0.695 0.182 0.204 0.148 0.28 0.136 0.113 Note: Bold text indicates the best result, and underlined text indicates the second-best result. 1.0 1.0 0.552 1.0 1.0 1.0 1.0 0.984 1.0 1.0 1.0 0.969 1.0 1.0 0.792 0.597 0.601 0.88 0.535 0.519 0.307 0.344 0.253 0.382 0.387 0.50 0.95 0.433 0.255 0.623 0.284 0.446 0.294 0. 0.359 0.349 0.317 0.46 0.26 0.252 gregate edit distance on both English (0.124) and Chinese (0.145) documents. detailed breakdown has further revealed Logics-Parsings superior capabilities over all other methods in parsing pure text, chemical structures, and handwritten content. Despite an inferior performance in reading order prediction compared with the commercial tool TextIn, our proposed Logics-Parsing significantly surpasses all other open-source alternatives. Some qualitative comparisons of reading order prediction are presented in Fig. 5. The results show that the reading orders predicted by our LogicsParsing are both visually clear and structurally coherent, closely following the standard left-to-right and topto-bottom reading conventions, while accurately preserving contextual relationships among text paragraphs and semantic regions. Moreover, while Logics-Parsing has demonstrated superior performance across most dimensions, we acknowledge performance gap existed in table structure recognition and mathmetic formula recognition compared to the top-leading methods. These shortcomings highlight direction for future improvement, particularly in modeling complex twodimensional layouts. 4https://github.com/datalab-to/marker 5https://github.com/breezedeus/Pix2Text 6https://github.com/rednote-hilab/dots.ocr 7https://ocrflux.pdfparser.io/ 8https://seed.bytedance.com/zh/seed1_6 9https : / / openai . com / zh - Hans - CN / index / introducing-gpt-5/ Figure 4. Visualization of the comparisons between our propose Logics-Parsing and other methods across different document types. multiple document types, which gives more intuitive yet comprehensive demonstration on the superiority and wellbalanced capability of our proposed Logics-Parsing against other state-of-the-art methods. Specifically, as is shown in Tab. 1, we categorize existing methods into three groups, namely Pipeline Tools, Expert VLMs and General VLMs, which collectively represent the mainstream technical paradigms in the field of document parsing. Our Logics-Parsing falls within the Expert VLMs category, which aims to build professional task-specific model for document parsing. The quantitative results have demonstrated that Logics-Parsing achieves the State-of-theArt (SOTA) performance overall, attaining the lowest agFigure 5. Qualitative comparison of reading order prediction. Red arrows denote the predicted flow. Our Logics-Parsing model generates reading sequence fully aligned with the ground-truth. Table 2. Ablation study on the effectiveness of the SFT-then-RL paradigm."
        },
        {
            "title": "Methods",
            "content": "Overall Edit ZH EN Text Edit ZH EN Formula Edit Table TEDS EN ZH ZH EN Table Edit ZH EN ReadOrder Edit Chemistry Edit HW Edit EN"
        },
        {
            "title": "ALL",
            "content": "ZH Qwen2.5-VL-7B(baseline) Logics-Parsing-SFT Logics-Parsing 0.316 0.133 0.124 0.319 0.159 0.145 0.149 0.093 0.089 0.185 0.165 0. 0.33 0.113 0.106 0.302 0.169 0.165 64.5 74.1 76.6 63.3 80.32 79.5 0.585 0.176 0.165 0.637 0.149 0. 0.202 0.149 0.136 0.152 0.155 0.113 0.75 0.520 0.519 0.337 0.276 0.252 3.3.2. Ablation Study We perform ablation experiments to validate the effectiveness of our proposed SFT-then-RL framework. We explicitly compare two models, one of which is the LogicsParsing-SFT defined in Sec. 2.3, another one is the final Logics-Parsing model which is augmented with layoutcentric reinforcement learning. The experimental results are presented in Tab. 2. We observe that after fine-tuning on self-constructed SFT dataset, the Logics-Parsing-SFT model has developed remarkable document parsing abilities across different content types compared with the original Qwen2.5-VL-7B baseline. This performance provides strong validation on the high quality of both the collected training data and the annotations generated through combination of automated and humanin-the-loop manner. After the second LC-RL stage, the final Logics-Parsing model consistently outperforms the LogicsParsing-SFT model across nearly all metrics for both English (EN) and Chinese (ZH). The most significant improvement appears on the ReadOrder Edit distance, which our proposed multi-component reward mechanism mainly optimizes on. Specifically, the ReadOrder score for English drops from 0.149 to 0.136, and for Chinese, substantial reduction from 0.155 to 0.113 is observed. This phenomenon has provided strong evidence that fine-tuning on small, curated set of explicitly mined hard samples with our proposed layout-centric GRPO strategy effectively enhances the models ability to handle complex document layouts and inference accurate reading order. 4. Conclusion We present Logics-Parsing, an end-to-end LVLM-based document parsing model augmented with layout-centric reinforcement learning. Through specially designed twostage SFT-then-RL training framework and carefully curated training datasets tailored to distinct characteristics of each stage, Logics-Parsing has equipped strong abilities for both explicit document layout modeling and multimodal recognition of heterogeneous content types. To rigorously evaluate our proposed framework, we proposed new benchmark called LogicsParsingBench consisting of 1,078 page-level high-quality PDF images across diverse categories. With more diverse and complex document types such as academic papers included and improved evaluation protocols compared to OminiDocBench, LogicsParsingBench offers more fair and challenging benchmark reflecting real-world document complexity and advancing the frontiers of the document intelligence research. In the future, we will pay more attention to explore architectural innovation for the SFT training and develop more focused and fine-grained reward mechanism for improved layout-centric reinforcement learning. These directions hold promising advancement toward document parsing model that is highly versatile yet deeply specialized, capable of professionally handling wide array of document understanding tasks with greater accuracy and adaptability. 5. Contributors Xiangyang Chen, Shuzhao Li, Xiuwen Zhu, Yongfan Chen, Fan Yang, Cheng Fang, Lin Qu, Xiaoxiao Xu, Hu Wei, Minggang Wu."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abdallah, Alexander Berendeyev, Islam Nuradin, and Daniyar B. Nurseitov. TNCR: table net detection and classification dataset. CoRR, abs/2106.15322, 2021. 3 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 1 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 2, 4, 5, 6 [4] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. 2 [5] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 3, 6 [6] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. Paddleocr 3.0 technical report, 2025. 5, 6 [7] Yuning Du, Chenxia Li, Ruoyu Guo, Cheng Cui, Weiwei Liu, Jun Zhou, Bin Lu, Yehua Yang, Qiwen Liu, Xiaoguang [21] Baode Wang, Biao Wu, Weizhen Li, Meng Fang, Yanjie Liang, Zuming Huang, Haozhe Wang, Jun Huang, Ling Chen, Wei Chu, et al. Infinity parser: Layout aware reinforcement learning for scanned document parsing. arXiv preprint arXiv:2506.03197, 2025. [22] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 6 [23] Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for large-scale learning: An efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025. 5 [24] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large In European Conference on Comvision-language model. puter Vision, pages 408424. Springer, 2024. 1 [25] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. 1, 5, 6 [26] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, et al. mplug-docowl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499, 2023. 1 [27] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023. 1 [28] Xinyi Zheng, Douglas Burdick, Lucian Popa, and Nancy Xin Ru Wang. Global table extractor (GTE): framework for joint table identification and cell structure recognition using visual context. CoRR, abs/2005.00589, 2020. [29] Xu Zhong, Elaheh ShafieiBavani, and Antonio JimenoYepes. Image-based table recognition: data, model, and evaluation. CoRR, abs/1911.10683, 2019. 3 Hu, et al. Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144, 2021. 1 [8] Hao Feng, Shu Wei, Xiang Fei, Wei Shi, Yingdong Han, Lei Liao, Jinghui Lu, Binghong Wu, Qi Liu, Chunhui Lin, et al. Dolphin: Document image parsing via heterogeneous anchor prompting. arXiv preprint arXiv:2505.14059, 2025. 5, 6 [9] VI Lcvenshtcin. Binary coors capable or correcting deleIn Soviet physics-doklady, tions, insertions, and reversals. 1966. 4 [10] Vladimir Iosifovich Levenshtein. Binary codes capable of In Doklady correcting deletions, insertions, and reversals. Akademii Nauk, pages 845848. Russian Academy of Sciences, 1965. 5 [11] Zhang Li, Yuliang Liu, Qiang Liu, Zhiyin Ma, Ziyang Zhang, Shuo Zhang, Zidun Guo, Jiarui Zhang, Xinyu Wang, and Xiang Bai. Monkeyocr: Document parsing with structure-recognition-relation triplet paradigm, 2025. 6 [12] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [13] Pengfei Liu, Jun Tao, and Zhixiang Ren. quantitative analysis of knowledge-learning preferences in large language models in molecular science, 2025. 3 [14] Ahmed Nassar, Andres Marafioti, Matteo Omenetti, Maksym Lysak, Nikolaos Livathinos, Christoph Auer, Lucas Morin, Rafael Teixeira de Lima, Yusik Kim, A. Said Gurbuz, Michele Dolfi, Miquel Farre, and Peter W. J. Staar. Smoldocling: An ultra-compact vision-language model for end-toend multi-modal document conversion, 2025. 6 [15] OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5):1, 2023. 1 [16] Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, et al. Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2483824848, 2025. 2, 5 [17] Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in pdfs with vision language models. arXiv preprint arXiv:2502.18443, 2025. 1, 3, [18] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 1 [19] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2 [20] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution arXiv preprint for precise document content extraction. arXiv:2409.18839, 2024. 1,"
        }
    ],
    "affiliations": [
        "Alibaba Group"
    ]
}