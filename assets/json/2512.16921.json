{
    "paper_title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
    "authors": [
        "Qihao Liu",
        "Chengzhi Mao",
        "Yaojie Liu",
        "Alan Yuille",
        "Wen-Sheng Chu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement."
        },
        {
            "title": "Start",
            "content": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification Qihao Liu1,2* Chengzhi Mao1 Yaojie Liu1 Alan Yuille2 Wen-Sheng Chu1 1Google 2Johns Hopkins University https://auditdm.github.io/ 5 2 0 D 8 1 ] . [ 1 1 2 9 6 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM finetunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement. 1. Introduction The rapid evolution of Multimodal Large Language Models (MLLMs) has led to surge in high-performing models [17, 21, 30, 75]. However, despite steady improvements on public benchmarks, selecting the proper models for real-world deployment remains challenging, as conventional evaluations often obscure how models truly differ. This is particularly evident when models are retrained on new data: although retraining may improve targeted knowledge, its impact on broader capabilities remains unclear. Similar concerns arise with task-specific fine-tuning and edge deployment, where practitioners must weigh trade-offs between accuracy, model size, and generalization. In practice, the critical question is not who wins the leaderboard but what changes, and why: identifying which inputs flip, what skills improve, and where brittle behaviors persist. *This work was done during Qihao Lius internship at Google. Figure 1. Overview of AuditDM. We propose to train an auditor model to systematically discover capability gaps in an MLLM by generating failure-inducing questionimage pairs. We show three automatically generated examples of weaknesses in object relationships. The proposed framework offers diagnostic insight and enables targeted rectification via auditor-guided feedback. While common benchmarks [3, 29, 53, 81] are the de facto standard for model comparison, they fall short in two key respects for answering the above questions. First, closed-set evaluations are bounded by fixed knowledge scope and inevitably leave blind spots. As result, comparisons based on closed sets can be inherently selective and biased. Second, benchmarks tend to compress complex behavior into sparse scores, thus obscuring heterogeneous shifts across data slices, whereas the most significant capability gaps are often entangled and concentrated in the long tail. Prior work proposed human online testing [67], yet it is prohibitively expensive and time-consuming to scale. To bridge this gap, we introduce model auditing, an automatic evaluation paradigm designed to uncover and interpret hidden divergence patterns missed by closed sets online. An effective auditor must go beyond simple de1 Gemma3-4B from 67.6 to 73.8, reaching performance comparable to its 12B counterpart. As standard data sources near exhaustion, leveraging model disagreement for improvement provides practical avenue for continual learning. Our contributions are three-fold: We introduce model auditing as new paradigm for systematically discovering model capability gaps and diagnosing weaknesses in MLLMs. We present AuditDM, reinforcement learningbased framework that trains an MLLM as an auditor to automatically identify and interpret failure modes. We show that AuditDM converts discovered failure modes into annotation-free training data, enabling targeted retraining and continual improvement. 2. Related Work Multimodal Large Language Models. Recent advances have markedly improved the ability of LLMs to handle complex tasks [1, 4, 9, 73], and have extended them with multimodal capabilities [7, 8, 17, 21, 30, 71, 75]. Early work like CLIP [63] aligns visual and textual data within shared representation space via contrastive learning [31, 51, 69, 85], establishing common choice of visual encoder for MLLMs. Recent approaches enhance multimodal integration by projecting visual outputs through Q-Formers [5, 18, 41, 72] or MLPs [47, 48, 59, 90], discretizing visual features to combine with text tokens [70], or directly injecting continuous visual features into Transformer layers using Perceiver Resamplers [2] or embedded visual expert modules [77, 87]. Beyond architecture, training algorithms [60, 64] and dataset curation [19] play crucial role in driving the remarkable performance of modern MLLMs. However, as data benefits plateau and evaluations lag, new strategies are needed to deepen understanding, strengthen evaluation, and drive further advances in MLLMs. In this paper, we propose systematic auditing to uncover capability gaps and failure modes, evaluate inconsistencies, and guide targeted improvements. Adversarial attack for LLM/MLLM. Prior work on adversarial attacks mainly focuses on model safety, e.g., jailbreaks [52, 82, 88, 92], data exfiltration [25, 62, 86], and malicious intent [45, 50], and largely relies on optimizationbased methods [42, 65]. In contrast, our model auditing targets inherent weaknesses and recurring failure patterns without intentional attacks, proposing an optimization-free approach that identifies failures in single inference step. Synthetic data for MLLM. MLLMs require extensive pretraining and fine-tuning data [47, 63, 75], but data collection is often labor-intensive and biased [61]. To address this, synthetic generation is widely used: aligning images and text via GPT-4V-based captioning [14], enriching text instructions with large pretrained models [19], and rendering images from captions using diffusion [54]. High-quality (a) Improving PaliGemma2 (b) Improving Gemma3 Figure 2. Model improvement with AuditDM. We report average performance over all benchmarks per model (excluding MME due to its incompatible score scale). Once trained, AuditDM generates targeted, large-scale data points aligned with discovered weaknesses, training on which can produce consistent gains across diverse models and benchmarks. tection: it should systematically discover capability gaps, summarize interpretable weaknesses, and provide feedback to guide rectification and model improvement. To this end, we propose to Audit the Differences that Matter, and introduce AuditDM, an annotation-free framework that exploits cross-model divergence to discover failure modes in target MLLM  (Fig. 1)  . Within the context of VQA1, rather than relying on human inspection [67], AuditDM trains an MLLM auditor that generates questionimage exemplars maximizing response divergence among target models, thereby exposing capability gaps. Concretely, AuditDM learns to propose naturallanguage probes that (i) directly query the target MLLM (e.g., complex questions probing image details) or (ii) instruct an image diffusion model to synthesize targeted perturbations (e.g., counterfactual images). We train the auditor with Group Relative Policy Optimization (GRPO) [66], iteratively refining it to generate instructions that maximize cross-model disagreement. This RL-style approach enables optimization over an interpretable yet non-differentiable language interface, yielding human-interpretable failure patterns rather than isolated errors. At inference, since the auditor has learned the patterns that induce failures of the target model, it can expose model blind spots in single pass. This enables synthesizing largescale, weakness-targeted data that can be used immediately for failure rectification and model improvement. We validate AuditDM through extensive experiments on Gemma3 [71] and PaliGemma2 [8]. Our analysis uncovers diverse failure modes within the PaliGemma2 family, directly pinpointing their capability boundaries and risk profiles. Notably, AuditDM identifies failure categories where 28B model underperforms 3B model, including hallucination avoidance, counting, and color recognition. Moreover, we convert these diagnostic probes and counterfactuals into training signals, achieving substantial gains across 16 benchmarks. For example, on AI2D [34], we raise PaliGemma2-3B from 76.0 to 85.3, even surpassing the PaliGemma2-28B model. On MMBench [53], we improve 1We focus on VQA capability to study MLLM performance gaps. 2 Table 1. Comparison of related work on finding and fixing MLLM failures Method Data scale Weaknesses Seeking Image Weaknesses Text Weaknesses Failure Interpretability Failure Rectification Conventional evaluation [3, 34, 53, 81] fixed set Attacks Visual adversarial attacks [24] Jailbreak attacks [52, 88, 92] open-ended open-ended Data synthesis Caption generation [14] Prompt rewriting [19] Image synth/render [54] Concept perturbation [11, 27] AuditDM (ours) fixed set open-ended open-ended fixed set open-ended limited active active no no no passive active - - - - - limited - - - - - limited - - - - - - adversarial only - - - - - instruction-tuning sets are often constructed with formatted data [12, 18, 47, 76, 87], while diversity and complexity are increased through feature swapping or concept perturbations [11, 27]. In contrast, we move beyond generic alignment and diversity objectives by using an auditor to generate weakness-targeted samples guided by cross-model disagreement. We generate data that directly closes capability gaps, improves evaluation fidelity, and enables label-free, continual rectification and model improvement. Self-Evolution and weak-to-strong learning. To advance LLMs toward, and potentially beyond, human-level performance, recent work explores self-evolution methods (e.g., self-play [74], self-instruct [78], self-improving [28, 91], self-training [26]) and weak-to-strong learning [10, 38], enabling autonomous improvement under shifting environments. Among them, self-play unifies iterative improvements through adversarial dynamics induce curricula and self-generated data with critics [13, 16, 83]. Extending to the zero-data regime, recent work replaces external traces with task-space exploration and language self-play [36, 89]. While our approach draws on similar principle of iterative improvement from self-generated data, we propose to explicitly train model-specific auditor for the target MLLM to uncover capability gaps and failure modes. The auditor then synthesizes weakness-targeted data that closes these gaps and drives continual improvement. 3. AuditDM AuditDM is reinforcement-learning (RL) framework for auditing MLLMs by actively discovering and rectifying their VQA failure modes. As shown in Fig. 3, we fine-tune an MLLM auditor and pair it with diffusion models to generate challenging questionimage exemplars that maximize response disagreement between target MLLM and an ensemble of reference MLLMs. This section describes the process for generating failure-inducing pairs, ensembling reference MLLMs, training the RL-based auditor, and the mechanism for rectifying discovered failure modes. 3.1. Question-Image Pair Generation AuditLM first identifies question-images pairs that induce failures in target MLLM. Given an input image and prompt p, the auditor analyzes the input image and generates textual outputs for two generation tasks, which together construct the failure-inducing pair (Q, ). Text instruction for generation. To probe visual weaknesses in the target MLLM, the auditor collaborates with either an image diffusion model or an image editing model to create counterfactuals. For image generation, the auditor produces detailed caption = A(I, pc) for the input image I, and enriches it with challenging semantic elements. This caption is then used to condition the diffusion model to synthesize an altered image Ig = G(C) that embodies these challenging attributes. For image editing, which supports more targeted analysis, the auditor generates editing commands = A(I, pe) for an image-editing model to apply controlled modifications to produce the edited image Ie = E(I, E). This process systematically tests how the target model responds to visual perturbations and thus reveals potential weaknesses. While image editing yields fewer counterfactuals than full image synthesis, it offers more precise and interpretable insights into specific visual failure modes. Direct generation. The auditor also generates complex, nuanced questions specifically designed to challenge the target MLLM with the instruction prompt for question generation pq. This step drives the auditor to identify intricate semantic concepts in the image, learn textual and visual patterns the target model struggles with, and craft targeted queries that probe these weaknesses. We generate from either the original image or the counterfactual images Ig, Ie, i.e., = A(I , pq), where {I, Ig, Ie}. In practice, we combine all three pairing levels, i.e., (Q, ), (Q, I), and (Q, ). An ablation of their combinations is provided in Sec. 4.2.3. The instruction prompts pq, pc, pe for all three generation tasks are provided in Sec A.1. 3.2. Divergence of Models Given the generated pairs (Q, ), we then evaluate the target MLLM Mtar against reference MLLM Mref . When auditing the difference between two models, the second model is taken as the reference. When auditing single target for model weaknesses, we instead use an ensemble whose consensus serves as an oracle, and search for (Q, ) that maximize divergence from this oracle. By comparing responses from Mtar and Mref , we quantify their disagreement to identify capability gaps and pinpoint In particular, inconsistencies between the failure modes. 3 Figure 3. AuditDM architecture. AuditDM fine-tunes an MLLM into an auditor that generates challenging probing questions and counterfactual images (via captions for image regeneration or editing commands), yielding questionimage pairs on which the target model fails while the MLLM ensemble agrees, thus exposing capability gaps and failure modes. The auditor is trained to maximize prediction discrepancy between the target and the ensemble. Once trained, it identifies weaknesses and failure cases in single inference pass. target models output and the ensembles consensus provide an interpretable signal that exposes the targets deficiencies and risk areas. This disagreement-driven evaluation is critical diagnostic tool, offering deeper understanding of the target MLLMs behavior and decision boundaries, which subsequently guides targeted updates to improve its performance (details in Sec. 3.4). To attribute observed failures to the target model Mtar, rather than artifacts of the auditor (e.g., generating meaningless questions), diffusion model (e.g., generating inaccurate or unrealistic images), or ensemble (e.g., producing incorrect answers), we adopt two assumptions. (1) Answerable instances: If the ensemble agrees on an answer, the questionimage pair is likely meaningful and answerable. (2) Rarity of target correctness: It is rare for Mtar to be uniquely correct while all ensemble models are wrong. Under these assumptions, we treat the ensemble consensus as strong proxy for correctness and update only on instances where its prediction conflicts with that consensus. These assumptions are empirically validated in Sec. B.2. 3.3. RL-based Auditor Training We train the auditor using Group Relative Policy Optimization (GRPO) [66] to produce reliable (Q, ) pairs that maximize discrepancies between the target model Mtar and the reference Mref . For each randomly generated pair (Q, ) in Section 3.1, we compute disagreement signal s(Q, ) by comparing the model responses: s(Q, ) = (cid:16) (cid:17) Mtar(Q, ), Mref (Q, ) , (1) where the distance metric is binary semantic-agreement judge that returns 1 if the two answers differ in meaning and 0 otherwise. GRPO uses group-relative normalization of this signal to form advantages within each group: (cid:2)sj(Q, )(cid:3) sk(Q, ) meanj ˆAk(Q, ) = . (2) stdj (cid:2)sj(Q, )(cid:3) + ϵ We optimize the GRPO objective [66] to train the auditor to favor examples that maximize cross-model discrepancies. By emphasizing these differences, the auditor learns to detect weaknesses and failure modes in Mtar, produces valuable training signals, and ultimately serves as fast, singlepass diagnostic tool for multimodal systems. 3.4. Failure Mode Rectification Once trained, the target models auditor is used to systematically identify failure modes and convert them into training signals. However, fine-tuning on these failures is intuitive but difficult in practice [49]. We provide two main strategies to leverage these signals for rectifying such failure modes. (1) Augmenting labeled data: standard strategy is to augment the original training data with auditor-generated samples (as detailed in Sec 4.2.1). It creates comprehensive training set that specifically covers identified weaknesses and failure cases. This strategy mitigates overfitting to isolated examples, thereby improving the models robustness and overall performance. (2) Bootstrapping unlabeled data: To bootstrap data generation without labels, we leverage both the untrained auditor and auditors saved at different training steps to synthesize training samples (as in Sec. 4.2.2). Specifically, for collection of unlabeled images, each auditor produces set of questions, new images, and corresponding pseudo-labels; we then aggregate and deduplicate them to form combined training set for MLLM fine-tuning. This process is iterated until performance converges: fine-tune the MLLM, refresh auditor checkpoints with the latest MLLM, and regenerate data from the unlabeled pool. The auditor-synthesized, weakness-specific data is critical for refining the target MLLM and improving its realworld performance. Starting from base MLLM, we train an auditor to generate this weakness-specific data, and then fine-tune the MLLM on mixtures that include it. We explore two data mixture strategies and leave broader strategy design to future work. This iterative mechanism enables continual rectification and improvement of MLLMs: by leveraging evolving evaluations to refresh the training set, AuditDM leads to higher benchmark accuracy, greater task robustness, and lower failure rates over time. 4. Experiments We demonstrate the effectiveness of our proposed framework in evaluating, understanding, and improving MLLMs. In Sec. 4.1, we show that it efficiently and systematically uncovers weaknesses in given MLLM, enabling clearer behavioral understanding and more faithful evaluation of its performance. In Sec. 4.2, we show that it delivers significant performance gains across range of benchmarks. Implementation details. We evaluate AuditDM on two target model families (PaliGemma2 [68], and Gemma3 [71]) by training auditors to automatically uncover their weaknesses and improve their performance, and we use mixture of these model variants as the ensemble. For PaliGemma2, we default to 448px2 unless otherwise noted. For all experiments, we fine-tune Gemma3-4B as the auditor, use FLUX.1-dev [37] for image generation, and FLUX.1-Kontext-dev for image editing. The auditor is finetuned for 1K steps with AdamW [55] using an initial learning rate of 3 106 with 10% warm-up and cosine learning rate decay to 1 106, and global batch size of 256. Further details are provided in Sec. A.2. 4.1. AuditDM for Model Failure Detection In Sec. 4.1.1, we demonstrate the effectiveness of AuditDM in finding model weaknesses; in Sec. 4.1.2, we provide detailed analysis of the failure modes automatically discovered by our method. We focus on PaliGemma2 here. 4.1.1. Effectiveness of Finding Model Weaknesses We demonstrate the effectiveness of the fine-tuned auditor in identifying model weaknesses. The baseline uses the same system without fine-tuning, relying solely on prompt engineering to identify failure cases (see Sec. A.1 for the introduction prompts). We randomly sample 20K imagequestion pairs from the VQAv2 [3] training split Then, for each pair, AuditDM and the baseline each generate new imagequestion pair to test the target model (PaliGemma2-3B [68]). To ensure accurate evaluation, we use Gemini 2.5 Pro and the GPT-5 API, with human reverification on disagreements, to produce reliable pseudoannotations for the generated imagequestion pairs. We then measure the search success rate, defined as the fraction of generated samples that expose validated error, i.e., the target models answer disagrees with the annotation. As shown in Table 2, AuditDM discovers failures far more efficiently than the baseline, achieving substantially higher search success rate under the same generation budget. Moreover, the weaknesses discovered by AuditDM span diverse set of skills (e.g., world knowledge, clock reading, size comparison; see Fig. 5) and are more explainable and interpretable (see Fig. 6). Together, they provide richer and more actionable understanding of the models weaknesses and failure modes. Table 2. Effectiveness of finding model weaknesses. The baseline uses the same system without fine-tuning, relying solely on prompt engineering to find failures. We report the success rate of identifying valid errors over 20K attempts. Baseline AuditDM (Ours) Search Success Rate 21.4% 91.1% 4.1.2. Weakness Analysis of PaliGemma2 The PaliGemma2 [68] family attains strong results across diverse benchmarks, with the 28B model significantly outperforming the 3B variant. However, standard benchmarks report only aggregate performance on predefined tasks, whereas AuditDM enables automatic discovery of weaknesses and failure modes. In this section, we analyze their weaknesses and highlight cases where the 28B model underperforms the 3B model. First, to ensure fair comparison between the 3B and 28B models while removing potential confounds from image style, we consider AuditDM in setting that generates probing questions only for the original images. Fig. 4 summarizes the top 15 failure patterns for the 3B and 28B models, automatically identified by AuditDM, and Fig. 5 provides representative examples for each failure mode. For each detected failure, we use Gemma3 to summarize the root cause and the weakness category. While AuditDM identifies wide range of limitations and challenges in current models, we present only subset of the most notable weaknesses due to space constraints. Our AuditDM demonstrates that, compared with the 28B model, the 3B model shows pronounced weaknesses in world knowledge and struggles with tasks such as reading clocks, comparing sizes, recognizing actions, and understanding signs. Interestingly, we also identify several weaknesses that are more prominent in the larger 28B model. Specifically, the 28B model shows significantly higher tendency toward hallucination errors, and struggles more with tasks related to emotion understanding, color recognition, and counting. Additional details and examples are available in Sec C. In addition, by leveraging the image-editing component, AuditDM efficiently discovers small visual modifications that substantially alter model predictions. This capability clarifies which visual factors drive behavior and makes model weaknesses and vulnerabilities more interpretable  (Fig. 6)  . Visualizations are provided in Fig. 6. Notably, we observe that certain minor alterations (which may be irrelevant to the task or question) can cause the 28B model to fail while the 3B model remains correct. These observations suggest that the two models have different decision boundaries and rely on different cues for model predictions. By systematically revealing prediction-altering elements, AuditDM provides valuable interpretability that highlights the susceptibility and limitations of state-of-the-art models, showing how subtle perturbations can exploit unexpected 5 Figure 4. AuditDM identifies the top 15 failure modes and challenging task categories for PaliGemma2-3B and 28B models at 448px2, and we report normalized per-category failure rates. Tasks are ordered left to right, beginning with the most pronounced weaknesses of the 3B model and progressing to those of the 28B. Notably, we observe that for certain tasks, the 28B model performs significantly worse than the 3B model. For example, on challenging images, the 28B model struggles more with color recognition and counting, and is more prone to hallucination. Figure 5. Generated examples for each failure category. To better demonstrate the effectiveness, we focus on examples with original images and generated questions. Image-question pairs with both generated images and questions are provided in Fig. 6. Some images are cropped or rotated for better figure layout. Original images and additional examples are provided in Sec. C. sensitivities in large-scale model decision making. 4.2. AuditDM for Model Improvement AuditDM also enables efficient, scalable synthesis of weakness-aligned training data. Once the auditor is trained, single inference on any image yields tailored imagequestion pair. In this section, we demonstrate effectiveness of AuditDM for improving MLLMs. 4.2.1. Per-task Fine-tuning Results Experimental setup. Following PaliGemma2 [68], we evaluate across range of academic benchmarks and finetune PaliGemma2-3B for each task. We use AuditDM to generate synthetic dataset equal in size to the tasks training set (one new example per training instance), filter it, and mix it with the original data to form the final training set. Our evaluation spans general VQA, text-oriented VQA, image segmentation, and image captioning, covering eight representative datasets. Full details on benchmarks, task setup, metrics, data splits, and benchmark-specific training configurations are provided in Sec. A.3. Results. Table 3 reports results. Baselines are fine-tuned only on the original training set, whereas our method finetunes on mixture of original data and auditor-generated examples. Our method consistently improves the target 6 Figure 6. AuditDM efficiently detects fine-grained visual cues that are irrelevant to the task/question yet still alter model predictions. We target PaliGemma2-28B (448px2) and showcase small modifications that fool the 28B model but not the 3B model, highlighting the effectiveness of our method in finding failures even in very powerful models. For each example, we show the original image (left), the modified image (right), and the corresponding answers. Our analysis shows that even the PaliGemma-2 28B model is highly sensitive to minor changes in task-irrelevant objects, suggesting that current MLLMs may still fail to ground visual reasoning in the correct evidence. Our results further indicate that the 28B model is not necessarily more robust than the smaller 3B variant and continues to exhibit various weaknesses. By isolating fine-grained cues that change predictions, AuditDM reveals what visual evidence drives model behavior and affects outputs, characterizing the models vulnerabilities and brittleness. Additional examples are provided in Sec C. Table 3. Per-task fine-tuning results. Following PaliGemma2, each model is fine-tuned separately for each task. AuditDM yields substantial improvements to the 3B target model, allowing it to surpass the 10B (bold) or 28B ( highlighted ) variants on several benchmarks. Model PaliGemma2-10B 448px2 PaliGemma2-28B 448px2 PaliGemma2-3B 448px2 PaliGemma2-3B 448px2 + AuditDM (Ours) VQAv2 GQA OK-VQA AI2D DocVQA ChartQA RefCOCO COCOCap 85.8 85. 68.3 68.3 68.6 70.6 84.4 84.6 76.6 76.1 66.4 61.3 78.2 77. 145.0 145.2 84.8 86.7 (+1.9) 68.1 71.1 (+3.0) 64.1 69.2 (+5.1) 76.0 85.3 (+9.3) 73.6 77.5 (+3.9) 54.0 63.8 (+9.8) 76.3 77.8 (+1.5) 143.4 145.1 (+1.7) model across all benchmarks by substantial margin. Gains on grounding tasks that require precise bounding boxes are smaller, as synthesized or edited images can shift object locations and thus misalign bounding-box annotations (see Sec. 4.2.3 for details). Notably, on several benchmarks, the 3B model fine-tuned with AuditDM outperforms the official 28B model fine-tuned on the original dataset, highlighting the significant improvements enabled by AuditDM. 4.2.2. General Benchmark Results Experimental setup. To better assess improvements under realistic user behavior, we further experiment with Gemma3-4B on eight widely used, task-diverse multimodal benchmarks, including MMBench [53], SEED-Bench [39], and others. These benchmarks provide comprehensive coverage across diverse subtasks and ability dimensions. We follow recent work [40, 79] to collect images, apply additional filtering and cleaning, and then use AuditDM to generate training samples. For fair comparison, all models are evaluated with the same recipe using VLMEvalKit [20]. Note that we rewrite the matching script to extract final answers from model outputs. Further details on training data, benchmarks, and configurations are provided in Sec. A.4. Results. As shown in Table 4, AuditDM delivers substantial gains over the target models on every benchmark without any human annotations, demonstrating strong model improvement. Although larger base models (Gemma312B and Gemma3-27B) still lead on some tasks, augmenting Gemma3-4B with AuditDM markedly narrows the gap and even surpasses the 12B model on Seed-Bench-IMG, MMMU, and RealWorldQA. These results suggest that weakness-targeted auditing is scalable, data-efficient path to improve multimodal models without additional supervision, positioning AuditDM as an effective tool for uncovering and remedying capability gaps in MLLMs. 7 Table 4. General benchmark results. By systematically identifying model failure modes, AuditDM delivers significant gains across all tested benchmarks, and even enables the 4B model to outperform or match its 12B (bold) or 27B ( highlighted ) variant on several tasks. Model Gemma3-12B Gemma3-27B MMBench-v1.1 MMTBench Seed-Bench-IMG MME MMMU MMStar RealWorldQA POPE 73.8 78.3 58.5 59. 70.6 73.2 1517.3 1526.6 44.8 49.7 55.7 58.7 58.3 62.5 86.0 85. Gemma3-4B Gemma3-4B + AuditDM (Ours) 67.6 75.0 (+7.4) 53.2 58.9 (+5.7) 65.7 72.9 (+7.2) 1376.0 1450.3 (+74.3) 39.6 45.2 (+5.6) 46.1 52.4 (+6.3) 54.5 61.4 (+6.9) 85.1 85.5 (+0.4) Table 5. Ablation on different auditing components. Results are reported for PaliGemma2-3B at 224 px2. - indicates performance drop as the amount of generated data increases. is markedly more effective for this task than for the other two, suggesting that improving question quality is more reliable way to strengthen diagram understanding. GQA RefCOCO AI2D Baseline Probing question Image generation Image editing Best Combination 66.2 68.5 66.9 67.2 69. 73.4 - - 74.6 74.6 74.7 78.2 - 76.3 79. 4.2.3. Ablation on Different Auditing Components AuditDM comprises three auditing components that jointly learn textual and visual representations: (i) generating probing questions, (ii) synthesizing new images, and (iii) producing edited images. Although our final system uses all three components, different combinations offer distinct benefits across tasks. We ablate their contributions in Table 5. On general VQA tasks like GQA [29], all three policies yield performance gains. Interestingly, image editing slightly outperforms image regeneration, despite regeneration uncovering more diverse weaknesses. We hypothesize that image editing makes only small stylistic or bias-related changes, whereas regeneration can inject additional biases and artifacts from the generative model, thereby increasing distribution shift. Notably, the probing-question policy delivers the largest gains, indicating that asking more informative, targeted questions is an especially effective way to improve MLLM performance. For dense prediction tasks like RefCOCO [32], carefully designed image editing yields consistent improvements. We filter generated editing commands to ensure that the target objects position remains unchanged. However, for these tasks, probing-question generation and image regeneration are more challenging, as it is difficult to produce accurate pseudo annotations for newly generated questions and images. This limitation could be mitigated by using stronger ensemble to provide more reliable annotations, or by generating questions for images that already have dense labels (e.g., SA-1B [35]). We leave these directions to future work. For text/diagram-oriented OCR tasks such as AI2D [34], regenerating entire images is challenging and degrades performance, likely because our diffusion model struggles to generate accurate diagrams. By contrast, probing question 8 5. Limitations and Future Work Despite AuditDMs effectiveness in finding failure modes and improving performance, it has two main limitations. (1) Limitations in image generation. As discussed in Sec. 4.2.3, AuditDM requires images with dense annotations to generate probing questions for studying model behavior on tasks such as grounding and segmentation. In addition, due to the difficulty of synthesizing images with dense text and complex diagrams, AuditDM also struggles to regenerate images for text/diagram-oriented OCR tasks. We believe this limitation can be mitigated by bootstrapping pseudo-labels from strong vision annotators and employing text/diagram-specialized generators. (2) Computational complexity. While single inference pass can uncover failure case, the pipeline relies on both an MLLM and diffusion model, which makes large-scale dataset synthesis time-consuming. For example, to finetune Gemma3-4B, we spend about 5 days on 8 H100 to generate new textimage pairs. More runtime analysis is provided in Sec. B.3. However, we emphasize that using LLMs or diffusion models for data generation is widely adopted but also time-consuming [14, 54]. Since all methods rely on single inference pass to generate data, our overall time cost is comparable to theirs. 6. Conclusion We introduce AuditDM, an MLLM auditor that automatically diagnoses capability gaps and failure modes of target model, enabling more faithful evaluation and systematic understanding of its model weaknesses. AuditDM also enables large-scale generation training data tailored to failure modes and capaof targeted data bility gaps, driving closed loop of imsynthesis, retraining, and re-auditing for continual provement. Experiments on PaliGemma2 and Gemma3 show that AuditDM effectively identifies failure modes and weaknesses in state-of-the-art models and delivers significant performance gains across diverse benchmarks, demonstrating both the effectiveness of our approach and the importance of auditing MLLM performance gaps."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 2 [3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. 1, 3, 5, 2 [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 2 [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 1(2):3, 2023. 2 [6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [7] Tadas Baltruˇsaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence, 41(2):423443, 2018. 2 [8] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 2 [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 2 [10] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-tostrong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. 3 [11] Paola Cascante-Bonilla, Hui Wu, Letao Wang, Rogerio Feris, and Vicente Ordonez. Simvqa: Exploring simulated environments for visual question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 50565066, 2022. 3 [12] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. arXiv preprint arXiv:2305.04160, 2023. 3 [13] Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, and Kwan-Yee Wong. Spc: Evolving self-play critic via adversarial games for llm reasoning. arXiv preprint arXiv:2504.19162, 2025. 3 [14] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. 2, 3, [15] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. 2 [16] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak lanarXiv preprint guage models to strong language models. arXiv:2401.01335, 2024. 3 [17] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 1, 2 [18] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Advances in neural information processing systems, 36:4925049267, 2023. 2, 3 [19] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 2, 3 [20] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. 7, 2 [21] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. 1, [22] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 2 [23] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. In search of the next generation of multiDatacomp: modal datasets. Advances in Neural Information Processing Systems, 36:2709227112, 2023. 2 9 [24] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. arXiv Explaining and harnessing adversarial examples. preprint arXiv:1412.6572, 2014. 3 [25] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what youve signed up for: Compromising real-world llm-integrated apIn Proceedings plications with indirect prompt injection. of the 16th ACM workshop on artificial intelligence and security, pages 7990, 2023. [26] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. 3 [27] Vipul Gupta, Zhuowan Li, Adam Kortylewski, Chenyu Zhang, Yingwei Li, and Alan Yuille. Swapmix: Diagnosing and regularizing the over-reliance on visual context in visual question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 50785088, 2022. 3 [28] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. 3 [29] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 1, 8, 2 [30] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1, 2 [31] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representaIn International tion learning with noisy text supervision. conference on machine learning, pages 49044916. PMLR, 2021. [32] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Referitgame: Referring to objects in Tamara Berg. photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 8 [33] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787798, Doha, Qatar, 2014. Association for Computational Linguistics. 2 [34] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European conference on computer vision, pages 235251. Springer, 2016. 2, 3, 8 [35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer WhiteSegment head, Alexander Berg, Wan-Yen Lo, et al. anything. conference on computer vision, pages 40154026, 2023. 8 In Proceedings of the IEEE/CVF international [36] Jakub Grudzien Kuba, Mengting Gu, Qi Ma, Yuandong Tian, and Vijai Mohan. Language self-play for data-free training. arXiv preprint arXiv:2509.07414, 2025. 3 [37] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 5, 1 [38] Jan Leike and Ilya Sutskever. Introducing superalignment, 2023. OpenAI Blog. 3 [39] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 7, 2 [40] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 7, [41] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730 19742. PMLR, 2023. 2 [42] Jiahui Li, Yongchang Hao, Haoyu Xu, Xing Wang, and Yu Hong. Exploiting the index gradients for optimization-based jailbreaking on large language models. In Proceedings of the 31st International Conference on Computational Linguistics, pages 45354547, 2025. 2 [43] Xiangtai Li, Tao Zhang, Yanwei Li, Haobo Yuan, Shihao Chen, Yikang Zhou, Jiahao Meng, Yueyi Sun, Shilin Xu, Lu Qi, et al. Denseworld-1m: Towards detailed dense grounded caption in the real world. arXiv preprint arXiv:2506.24102, 2025. 2 [44] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 2 [45] Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, and JiRong Wen. Images are achilles heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large In European Conference on Computer language models. Vision, pages 174189. Springer, 2024. [46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. European conference on computer vision, pages 740755. Springer, 2014. 2 [47] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 2, 3 [48] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Improved baselines with visual instruction tuning. Lee. 10 In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2629626306, 2024. 2 [49] Qihao Liu, Adam Kortylewski, and Alan Yuille. Poseexaminer: Automated testing of out-of-distribution robustness in human pose and shape estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 672681, 2023. [50] Qihao Liu, Adam Kortylewski, Yutong Bai, Song Bai, and Alan Yuille. Discovering failure modes of text-guided diffusion models via adversarial search. In Twelfth International Conference on Learning Representations, pages 125. OpenReview. net, 2024. 2 [51] Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, and Mannat Singh. Flowing from words to pixels: noise-free frameIn Proceedings of the work for cross-modality evolution. Computer Vision and Pattern Recognition Conference, pages 27552765, 2025. 2 [52] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451, 2023. 2, 3 [53] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 1, 2, 3, 7 [54] Zheng Liu, Hao Liang, Bozhou Li, Tianyi Bai, Wentao Xiong, Chong Chen, Conghui He, Wentao Zhang, and Bin Cui. Synthvlm: High-efficiency and high-quality synarXiv preprint thetic data for vision language models. arXiv:2407.20756, 2024. 2, 3, 8 [55] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 5, 2 [56] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. [57] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the association for computational linguistics: ACL 2022, pages 22632279, 2022. 2 [58] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 2 [59] Ron Mokady, Amir Hertz, and Amit Bermano. CliparXiv preprint cap: Clip prefix for image captioning. arXiv:2111.09734, 2021. 2 [60] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. 2 [61] Amandalynne Paullada, Inioluwa Deborah Raji, Emily Bender, Emily Denton, and Alex Hanna. Data and its (dis) contents: survey of dataset development and use in machine learning research. Patterns, 2(11), 2021. [62] Yuefeng Peng, Junda Wang, Hong Yu, and Amir Houmansadr. Data extraction attacks in retrieval-augmented generation via backdoors. arXiv preprint arXiv:2411.01705, 2024. 2 [63] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 2 [64] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. 2 [65] Javier Rando, Hannah Korevaar, Erik Brinkman, Ivan Evtimov, and Florian Tram`er. Gradient-based jailbreak images for multimodal fusion models. arXiv preprint arXiv:2410.03489, 2024. 2 [66] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 4 [67] Sasha Sheng, Amanpreet Singh, Vedanuj Goswami, Jose Magana, Tristan Thrush, Wojciech Galuba, Devi Parikh, and Douwe Kiela. Human-adversarial visual question answering. Advances in Neural Information Processing Systems, 34:2034620359, 2021. 1, 2 [68] Andreas Steiner, Andre Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, et al. Paligemma 2: family of versatile vlms for transfer. arXiv preprint arXiv:2412.03555, 2024. 5, 6, 1, 2 [69] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 2 [70] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [71] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. arXiv preprint Gemma 3 technical arXiv:2503.19786, 2025. 2, 5, 1 report. [72] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. 2 [73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 11 integrated large language model agents. arXiv:2403.02691, 2024. arXiv preprint [87] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. 2, 3 [88] Yihao Zhang and Zeming Wei. Boosting jailbreak attack with momentum. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 2, 3 [89] Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. 3 [90] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2 [91] Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, et al. Agent-as-a-judge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934, 2024. [92] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. 2, 3 [74] Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et al. Towards conversational diagnostic ai. arXiv preprint arXiv:2401.05654, 2024. 3 [75] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2 [76] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an openended decoder for vision-centric tasks. Advances in Neural Information Processing Systems, 36:6150161513, 2023. 3 [77] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37:121475121499, 2024. 2 [78] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. 3 [79] Luis Wiedmann, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Leandro von Werra, Aritra Roy Gosthipaty, and Andres Marafioti. Finevision: Open data is all you need. arXiv preprint arXiv:2510.17269, 2025. 7, 2 [80] x.ai. Grok-1.5 vision preview. https://x.ai, 2024. Accessed: 2025-11-19. [81] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. 1, 3, 2 [82] Zonghao Ying, Aishan Liu, Tianyuan Zhang, Zhengmin Yu, Siyuan Liang, Xianglong Liu, and Dacheng Tao. Jailbreak vision language models via bi-modal adversarial prompt. IEEE Transactions on Information Forensics and Security, 2025. 2 [83] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Jing Xu, and Jason WearXiv preprint Self-rewarding language models. Cho, Sainbayar Sukhbaatar, ston. arXiv:2401.10020, 3, 2024. 3 [84] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. [85] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 2 [86] Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. Injecagent: Benchmarking indirect prompt injections in tool12 Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification"
        },
        {
            "title": "Overview",
            "content": "In the appendix, we provide additional information that could not be included in the main paper due to space limitations. The appendix is structured as follows: Sec. A. Method and implementation details A.1. Instruction prompts in AuditDM A.2. Auditor training A.3. PaliGemma2 training A.4. Gemma3 training Sec. B. Additional experimental results B.1. Training Gemma3-4B on the same data without"
        },
        {
            "title": "AuditDM",
            "content": "B.2. Empirical validation of assumptions B.3. Computational complexity Sec. C. Additional qualitative examples A. Method and Implementation Details A.1. Instruction Prompts in AuditDM As mentioned in Sec. 3.1, the auditor relies on three instruction prompts, pc, pe, and pq, to guide the generation of image-regeneration captions = A(I, pc), imageediting commands = A(I, pe), and new questions = A(I , pq). We provide the prompts here: Prompt for image regeneration (pc): You are given an image. Produce detailed, literal caption that would allow model to regenerate the image, but also introduce small alterations to certain visual attributes. Return single final caption describing the modified version only. Prompt for image editing (pe): You are given an image. Generate single image-editing command that describes how to modify the image. The modification must remain plausible in the real world. The command should be specific, actionable, and unambiguous. Return the editing command only. Prompt for question generation (pq): You are given an image. Generate single question that can be answered solely based on its visible content. Return the question only. In addition, in Table 2, we compare our model to baseline to evaluate the failure search success rate. The baseline uses the same system without fine-tuning, relying solely on prompt engineering to identify failure cases. The prompts used for the baseline are provided below: Baseline prompt for image regeneration (pb ): You are given an image. Produce detailed, literal caption that would allow model to regenerate the image, but introduce small changes that are easy for models to get wrong. Return single caption describing the modified version only. ): Baseline prompt for image editing (pb You are given an image. Generate single image-editing command that makes realistic change but is challenging for vision models. The modification must remain plausible in the real world. The command should be specific, actionable, and unambiguous. Return the editing command only. Baseline prompt for question generation (pb You are given an image. Generate single question answerable from its visible content but challenging for vision-language models. Return the question only. q): A.2. Auditor Training For all experiments in this paper, we fine-tune pretrained Gemma3-4B [71] as the auditor, and use FLUX.1-dev [37] for image generation, FLUX.1-Kontext-dev for image editing. For the reference MLLM, we consider two scenarios: (1) Model comparison. When training the auditor to compare the behavior of two models, we designate one as the target MLLM and the other as the reference MLLM, and fine-tune the auditor to generate imagequestion pairs that maximize the discrepancy between their outputs. (2) Single-model analysis. When training the auditor to probe the weaknesses and failure modes of single target model, we take that model as the target MLLM, and construct reference ensemble from PaliGemma2 [68], Gemma3 [71], and Qwen2.5-VL [6]. Note that the target model is not included in the ensemble. 1 Table 6. PaliGemma2-3B training hyperparameters for each task. A.4. Gemma3 Training Epochs Batch size Learning rate Weight Decay LLM dropout Label smoothing VQAv2 GQA OK-VQA AI2D DocVQA ChartQA RefCOCO COCOCap 14 4 10 16 10 40 120 8 256 256 256 256 256 256 256 256 1 105 1 105 5 106 1 105 1 105 1 105 1 105 1 105 1 107 1 107 0.0 1 106 1 106 1 106 0.0 1 106 0.0 0.0 0.0 0.0 0.0 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.3 0. During training, only the auditor is updated, while all other modules remain fixed. The auditor is fine-tuned for 1, 000 steps with AdamW [55], using an initial learning rate of 3 106, 10% warm-up, cosine learning rate decay to 1 106, and global batch size of 256. A.3. PaliGemma2 Training We provide additional experimental details for PaliGemma2 here. For all experiments involves PaliGemma2, we default to 448px2 unless otherwise noted. Benchmarks. Following PaliGemma2 [68], we evaluate on range of academic benchmarks and fine-tune PaliGemma2-3B separately for each task. We consider four common tasks: general VQA, text-oriented VQA, image segmentation, and image captioning. For general VQA, we use VQAv2 [3], GQA [29], and OK-VQA [56]; for text-oriented VQA, we use AI2D [34], DocVQA [58], and ChartQA [57]; for segmentation and captioning, we use RefCOCO [33] and COCOCap [46], respectively. Following PaliGemma [8], the models are trained on the corresponding training splits and evaluated on the validation or test splits, depending on their availability. For the VQA tasks, we report exact-match accuracy; for RefCOCO, we report mean Intersection-over-Union (mIoU); and for COCOCap, we report CIDEr score. Training configurations. For each task, we first fine-tune an auditor on random subset of the training data using the hyperparameters in Sec. A.2. Specifically, since each model is trained with global batch size of 256 for 1, 000 steps, the auditor is exposed to at most 256K images during fine-tuning. To construct the training pool for the auditor, we first filter the training set by image resolution and then randomly select 256K images to form the image pool for auditor training. After training the auditor, we use it to generate new imagequestion pairs from the training data, producing one new pair for each original example. We then mix these generated pairs with the original training set to further fine-tune the PaliGemma2-3B model at 448px2 resolution. Following PaliGemma [8], all models are trained until convergence. We provide hyperparameters for each task in Table 6 To further demonstrate the effectiveness of AuditDM for model failure rectification and model improvement, we also apply AuditDM to Gemma3-4B and evaluate it on eight widely used, task-diverse multimodal benchmarks. We provide the experimental details below. Training data. AuditDM requires only images as input. Therefore, we collect data from DataComp-1B [23] and mixture of academically annotated datasets, following LLaVA-OneVision [40] and FineVision [79]. In addition to standard image filters such as resolution and caption alignment, we also aim to ensure sufficient diversity and complexity in the images. To this end, we follow DenseWorld [43] and generate dense understandings of all filtered images, which allows us to further select images based on factors such as the number of objects, the diversity of object categories and relationships, overall scene richness, and the presence of fine-grained visual details. This additional filtering step removes overly simple, overly complex, or lowcontent images and ensures that the final training set contains images that can give rise to challenging yet meaningful probing questions or counterfactual images. As result, we obtain pool of 1.3M images for training, including 736K from DataComp-1B and 600K from the remaining academically annotated datasets. Benchmarks. We use the VLMEvalKit framework [20] to evaluate all models on diverse suite of 8 benchmarks, including MMBench-v1.1 [53], MMTBench [81], SeedBench-IMG [39], MME [22], MMMU [84], MMStar [15], RealWorldQA [80], and POPE [44]. Training details. After constructing the training image set, we train the auditor for 1,000 steps on these images to identify weaknesses of the Gemma3-4B model, saving checkpoints every 250 steps. The resulting auditors (checkpoints saved at different steps) are then used to generate new imagequestion pairs. We aggregate and deduplicate these images, and then use them to further fine-tune the Gemma3-4B model. We subsequently train new auditors on the newly fine-tuned Gemma3-4B model to analyze its remaining weaknesses. This sequence, consisting of (i) training the auditor with the latest MLLM, (ii) regenerating data from the unlabeled pool using the trained auditor, and (iii) fine-tuning the MLLM with the new data, constitutes one refinement iteration, and we perform two such iterations. While we observe further improvements with additional iterations in preliminary experiments, we restrict ourselves to two iterations to keep the compute cost within reasonable budget (see Sec. B.3). In each iteration, the auditor is trained following the hyperparameters in Sec. A.2. Then, given the newly generated samples, we fine-tune Gemma34B with global batch size of 512 and learning rate of 2 106 for one epoch. 2 Table 7. Training Gemma3-4B on the same images without AuditDM. Model MMBench-v1.1 MMTBench Seed-Bench-IMG MME MMMU MMStar RealWorldQA POPE Gemma3-4B Gemma3-4B (with the same images) Gemma3-4B + AuditDM (Ours) 67.6 69.6 (+2.0) 75.0 (+7.4) 53.2 54.8 (+1.6) 58.9 (+5.7) 65.7 66.9 (+1.2) 72.9 (+7.2) 1376.0 1321.6 (-54.4) 1450.3 (+74.3) 39.6 40.5 (+0.9) 45.2 (+5.6) 46.1 45.6 (-0.5) 52.4 (+6.3) 54.5 56.3 (+1.8) 61.4 (+6.9) 85.1 84.2 (-0.9) 85.5 (+0.4) Table 8. Empirical validation of assumptions. Type Percentage (%) Target failures Ambiguous questions Unanswerable questions 81.3 11.5 7.2 B. Additional Experimental Results B.1. Training Gemma3-4B without AuditDM In the main paper, we demonstrate that AuditDM significantly improves the performance of Gemma3-4B on various benchmarks. To further verify that these gains come from AuditDM rather than from additional fine-tuning on the selected images, we also fine-tune Gemma3-4B on the same images under the same settings. Since some of the images do not have original questions, we use pre-trained Gemma3-4B model (the same initial model used by AuditDM) to generate new questions. The results are shown in Table 7. We observe that fine-tuning on the same data without AuditDM yields only marginal improvements on few benchmarks, whereas our method achieves substantial gains across all benchmarks. B.2. Empirical Validation of Assumptions As discussed earlier, to ensure that the observed failures are attributable to the target model rather than artifacts of the auditor (e.g., generating meaningless questions), the diffusion model (e.g., producing inaccurate or unrealistic images), or the ensemble (e.g., providing incorrect answers), (1) Answerable instances: we rely on two assumptions: when the ensemble models agree on an answer, the questionimage pair is likely to be meaningful and answerable. (2) Rarity of target correctness: it is relatively rare for target model to be uniquely correct while all models in the ensemble are wrong. In this section, we show that the ensemble indeed identifies genuine failures of the target model, i.e., (1) the questionimage pairs are meaningful and answerable, and (2) they expose weaknesses of the target model. Specifically, we use the auditor to generate 1,000 new questionimage pairs on which the target model and the ensemble disagree. We then manually verify the correctness of their answers. We report (i) the percentage of samples that truly expose weaknesses of the target model (i.e., target failures), (ii) the percentage of samples where the target model is still accurate despite disagreeing with the ensemble (i.e., ambiguous questions, where both answers can be considered correct), and (iii) the percentage of samples where the question itself is not answerable from the image (i.e., unanswerable questions). The results are provided in Table 8. We can see that 81.3% of the samples indeed expose genuine weaknesses of the target model. In addition, we expect higher percentage of target failures when using more powerful ensemble. B.3. Computational Complexity To improve target model such as Gemma3-4B, AuditDM consists of three steps: (1) training an auditor model, (2) generating large-scale data with the trained auditor, and (3) fine-tuning the target model. In our experiments with Gemma3-4B, the first stage requires 29 hours on 16 H100 GPUs for 1,000 training steps, and the second stage requires 63 hours on 16 H100 GPUs to generate all images using Ray implementation. However, we emphasize that using LLMs or diffusion models for data generation is already widely adopted but time-consuming practice [14, 54]. As all these methods depend on inference with an LLM or diffusion model for data generation, our overall time cost is comparable to theirs. C. Additional Qualitative Examples We provide additional examples of the failures identified by our model in PaliGemma2 in Fig. 7, Fig. 8, and Fig. 9. Specifically, Fig. 7 and Fig. 8 show additional examples in their original image ratios for each weakness category. Note that, to ensure fair comparison between the 3B and 28B models while eliminating potential influences of image style, we generate probing questions on the original images without altering their visual appearance. These results further confirm that the identified failures stem from intrinsic model limitations rather than superficial visual differences. Fig. 9 presents additional examples involving visual changes. Our model efficiently discovers small image modifications that significantly alter the models predictions. This highlights the models sensitivity to fine-grained, taskirrelevant visual cues and reveals lack of robustness in its visual reasoning process. At the same time, these examples provide valuable insights into the visual factors that influence model behavior, making its weaknesses and vulnerabilities more interpretable. Figure 7. Generated examples for each failure category. To better demonstrate the effectiveness, we focus on examples with original images and generated questions. 4 Figure 8. Generated examples for each failure category. To better demonstrate the effectiveness, we focus on examples with original images and generated questions. Figure 9. Additional examples of fine-grained visual cues that are irrelevant to the task or question but still alter the models predictions. We target PaliGemma2-28B (448px2) and showcase small modifications that fool the 28B model but not the 3B model, highlighting the effectiveness of our method in finding failures even in very powerful models. For each example, we show the original image (left), the modified image (right), and the corresponding answers."
        }
    ],
    "affiliations": [
        "Google",
        "Johns Hopkins University"
    ]
}