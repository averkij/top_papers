{
    "paper_title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning",
    "authors": [
        "Caijun Xu",
        "Changyi Xiao",
        "Zhongyuan Peng",
        "Xinrun Wang",
        "Yixin Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 1 ] . [ 2 9 0 8 4 0 . 1 0 6 2 : r SCALER: Synthetic Scalable Adaptive Learning Environment for Reasoning Caĳun Xu1,2, Changyi Xiao1, Zhongyuan Peng1, Xinrun Wang3, Yixin Cao1,2,, 1Fudan University, 2Shanghai Innovation Institute, 3Singapore Management University"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) offers principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability or when training is dominated by narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), framework that sustains effective learning signals through adaptive environment design. SCALER introduces scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the models capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics. Correspondence: cjxu25@m.fudan.edu.cn Website: https://github.com/molumolua/SCALER"
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has become key post-training paradigm for enhancing the reasoning capabilities of large language models (LLMs) [8, 24]. By optimizing policy under explicit, verifiable rewards, RL can sharpen decision making, improve long-horizon credit assignment, and expand the models reasoning frontier [16, 30, 39]. However, scaling RL for reasoning is often bottlenecked not by the optimizer itself, but by the availability of continuously effective reward signals throughout training [27, 28, 42]. In this work, we argue that the training signal, for RL to continue improving LLMs, should remain effective in two complementary senses. First, problems should stay near the models current capability boundary during training [7, 26], i.e., remain neither trivial nor unsolvable. When the model mostly sees easy problems, learning saturates; when it mostly sees overly difficult problems, exploration becomes unproductive and the reward becomes sparse. Second, the training distribution should retain sufficient diversity over time [18]. Even if difficulty is well matched, repeatedly interacting with narrow task distribution can lead to overfitting Equal Contribution. Corresponding authors. 1 to limited set of patterns, weakening generalization and reducing exploration. Note that difficulty variation is not equivalent to diversity. Even if single environment can generate infinitely many problems by scaling parameters (e.g., longer arrays or larger graphs), it still shares limited set of templates and failure modes, so learning can plateau once those patterns are mastered. To this end, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), system that combines scalable reasoning environment synthesis with adaptive multi-environment reinforcement learning. For the first limitation, SCALER provides way to generate verifiable tasks at scale with controllable difficulty. We develop synthesis pipeline that programmatically converts real-world programming problems into reasoning environments with (i) verifiable interaction via deterministic oracles and unit tests, (ii) controllable difficulty via explicit scale parameters, and (iii) unbounded instance generation within each environment through randomized testcase generation. The pipeline automatically extracts meta-information, validates testcase generators via breadth/depth checks. This enables scaling training beyond finite reasoning datasets or small set of hand-crafted environments, while retaining strong correctness guarantees. On the other hand, SCALER designs an adaptive multi-environment RL framework to adapt both instance difficulty and environment selection so that training continues to encounter informative challenges as the model improves. Specifically, within each environment, an online difficulty controller adjusts scale parameters based on on-policy rollout accuracy to keep sampled instances near target success rate, thereby tracking the models capability frontier and avoiding degenerate all-correct or all-wrong regimes. Across environments, an environment curation mechanism maintains an active set of environments and replaces those whose learning signal has saturated (e.g., difficulty no longer increases or the environment becomes consistently trivial/unlearnable). This realizes an environment-level effectiveness. As the model and environments co-evolve, the marginal learning benefit can diminish, so continuously refreshing the active set helps preserve novelty and sustained learning signals. Our contributions can be summarized as follows: We highlight the importance of difficulty and diversity controllable environments for scaling RL posttraining in reasoning. We propose SCALER that combines verifiable, difficulty-controllable environment synthesis with adaptive multi-environment RL. Extensive experiments demonstrate SCALER yields consistent improvements across diverse reasoning benchmarks and exhibits more sustained training dynamics than dataset-based RL baselines under comparable budgets."
        },
        {
            "title": "2 Related Work",
            "content": "Data-centric RL. data-centric line of work improves the scalability of RL for LLM reasoning by continuously expanding the training distribution with synthetic data [29, 38] and self-play [4, 9, 20, 36]. On the synthetic-data side, works like Synthetic Data RL [13] and SWiRL [11] generate task-specific supervision from question answer pairs to multi-step reasoning and tool-use trajectories, and then apply RL on the resulting synthetic corpus. On the self-play side, recent methods produce interactions and hard cases via adversarial games or role-based play, enabling continual data self-generation for policy improvement [22, 44]. However, these approaches are easy to encounter bottlenecks: as the max difficulty of tasks is bounded by the generator agent [3], the data may drift out of sync with the evolving policy, collapsing into instances that are too easy or too hard and thus weakening learning signals in later stages. In contrast, SCALER applies difficulty controller and environment curation mechanism on synthesized verifiable reasoning environment to keep training informative throughout RL. Difficulty-aware RL. To mitigate vanishing learning signals, difficulty-aware RL often employs curriculum learning [5, 31, 33] or difficulty scheduling [6, 35]. Curriculum-based method trains models from easy to hard [23, 25], keeping effective learning signals in training process. However, curricula designed over static 2 Figure 1 An illustrative training example of SCALER. The model interacts with set of active environments: instances are used for training, online accuracy updates the in-environment difficulty controller, and environments whose learning signal saturates are retired and replaced by new environments via the curation mechanism. dataset faces the challenge of being coarse-grained and hard to design. This motivates environment-based formulations where difficulty of each environment can be monitored and adjusted online [12]. Reasoning Gym [32] offers suite of procedurally generated, verifiable reasoning environments with tunable difficulty, while RLVE [41] further adapts the difficulty distribution online as the policy improves. Despite enabling difficulty tracking and adaptation, existing environment suites are largely hand-engineered and limited in environment diversity at scale. SCALER addresses this by automatically synthesizing large and diverse set of reasoning environments with verifiable oracles and controllable difficulty, supporting sustained multi-environment RL."
        },
        {
            "title": "3 SCALER",
            "content": "To enable the model to explore, adapt, and generalize in multiple dynamic environments, we design SCALER with two components: (i) an adaptive multi-environment training framework that learns through interaction with set of verifiable environments, and (ii) systematic synthesis pipeline that converts programming problems into difficulty-controllable reasoning environments. Specifically, we first introduce the learning strategy over environments, including in-environment difficulty controller and the environment curation mechanism (3.1), then describe the environment synthesis process from programming problems (3.2)."
        },
        {
            "title": "3.1 Multi-environment Training Framework",
            "content": "We first illustrate the SCALER environment to facilitate understanding. We define an environment as context-conditioned, parameterized problem space in which individual instances can evolve through controllable parameters. For example, as shown in the math-operations environment in Figure 1, all problems 3 for each environment do Algorithm 1 SCALER Training Framework Require: Policy πθ, Environment Pool Epool, Target accuracy τ , Step size β, Active set size 1: Initialize: Sample initial active set {e1, . . . , eM } Epool,Difficulty dmin for all 2: for step = 1, 2, . . . , Tmax do 3: Dbatch 4: 5: 6: 7: 8: 9: Update πθ with Dbatch 10: 11: 12: 13: 14: Sample instances from based on difficulty Collect trajectories using πθ and calculate accuracy acct Dbatch Dbatch Trajectories Update difficulty for this environment 0 acc = 0 for Kzero steps = De for Ksat steps then Retire e: {e}; Epool Epool {e} Sample new enew Epool {enew}"
        },
        {
            "title": "Calculate slope ae\nt\nif ae",
            "content": "for each environment do over last Kslope steps 15: Eq. 1 Eq. 3 share common contextual description, calculate operations, while problem difficulty is modulated by the scale parameter . More illustrative cases are provided in Appendix 5. To fully unleash the potential of these difficulty-controllable reasoning environments, we adopt two complementary learning strategies: (i) Difficulty Controller. In environments with explicit verifiability, we dynamically adjust the environments scale parameters according to the online accuracy, which provides continuous and stable learning signals. (ii) Environment Curation Mechanism. For general environments, we introduce environment curation mechanism within which the model is trained exclusively on the current active set of environments. As the model and the environment gradually co-adapt and the learning signal diminishes in marginal utility, the current environment is retired and replaced with newly sampled one. The detailed implementation of SCALER is provided in Algorithm 1."
        },
        {
            "title": "3.1.1 Difficulty Controller",
            "content": "A key goal of SCALER is to maintain training at the agents capability frontier by continuously sampling instances near the boundary of what the current policy can solve. To achieve this, difficulty is dynamically adjusted within each environment based on current accuracy. Specifically, the difficulty of the instances is characterized by the array length or the number of edges in the graph and we discretize the difficulty into distinct difficulty levels. Let acct 0, 1 denote the average accuracy over sampled instances at step in given environment, and let τ 0, 1 denote the target accuracy. The continuous difficulty score dt ℝ, initializing d0 = 0, evolves according to: dt1 = clipdt β acct τ , 0, D, (1) where β > 0 controls the adaptation rate and ℕ is the maximum discrete difficulty level in the environment. This update rule increases difficulty when acct > τ and decreases it otherwise, thereby maintaining the training distribution near the agents capability boundary. For the next sampling step, while dt1 is real-valued, the environment can only generate instances at integer-valued levels in {0, 1, . . . , D}. To approximate dt1 ℝ under this integer constraint, we construct multiset of integers whose mean closely matches dt1. Specifically, let ℓ = dt1 and define = round(cid:0)kdt1 ℓ(cid:1). We assign instances the value ℓ 1 and the remaining instances the value ℓ, thereby balancing rounding up and rounding down; for example, when dt1 = 2.3 and = 10, this yields three instances at level 3 and 4 seven at level 2, whose average equals 2.3. The proposed controller naturally tracks model competence. As the model improves, online accuracy rises above the target and difficulty is pushed upward to stay near the performance boundary. Conversely, if the model forgets or the policy distribution shifts, accuracy drops and the controller reduces difficulty, recovering usable learning signals without manual retuning."
        },
        {
            "title": "3.1.2 Environment Curation Mechanism",
            "content": "To keep sufficient diversity during training and avoid \"tourist\" learning pattern where the model randomly explores variety of environments without sustained progress, SCALER introduces the environment curation mechanism which maintains an active set of environments and restricts training to this dynamic set, continuously refreshing it to avoid spending budget on environments that no longer provide meaningful learning signals. Let Wt = {e1, . . . , eW} denote the active environment set at training step t. At each step, instances are sampled only from environments in Wt and used for policy optimization. Environments are periodically evaluated at every step and may be retired from the set and replaced by newly synthesized environments. For each environment e, let de denote its difficulty at step t. To detect stalled progress, we argue that when the recent difficulty-step slope is less or equal to zero, the learning on this environment is saturable and should be retired. Specifically, we estimate the slope by fitting least-squares line over the last Kslope steps, defining the index set = {t Kslope 1, . . . , t} and fitting ae de be , T, where the slope ae is given in closed form by = iT ide ae iT i2 de , (2) (3) iT and de = 1 with = 1 temporarily saturated in that environment. iT de . An environment is retired if ae 0, indicating that difficulty is To accelerate retirement decisions, two additional heuristics are applied. Let acce 0, 1 denote the accuracy at step and let De be the maximum difficulty of environment e. An environment is retired early if it is unlearnable, i.e., acce = 0 for Kzero consecutive steps, or saturated, i.e., de = De for Ksat consecutive steps. When an environment is retired, it is replaced by newly synthesized environment sampled from the environment pool, keeping the set size fixed. Such retirement is temporary, as it targets situations where the model has reached plateau in learning within the current environment, rendering further progress unproductive. However, as training continues and the models capability boundary improve, there remains potential for continued learning within these environments. Therefore, retired environments are reintegrated into the environment pool and will be resampled in future iterations, offering renewed opportunities for the model to explore and adapt. In general, the environment curation mechanism supports sustained learning within multiple environments while preventing training from stagnating on environments that have become either trivial or unproductive."
        },
        {
            "title": "3.2 From Code Generation to Reasoning Environments",
            "content": "Our multi-environment training framework continuously tracks the models capability frontier while maintaining data diversity and freshness. This requires steady supply of environments that are difficultycontrollable and automatically verifiable. We therefore introduce an environment synthesis pipeline that converts real-world programming problems into such environments. Accordingly, our synthesis pipeline is organized into three components: 5 Meta information extraction (3.2.1). Ensures rewardable and unambiguous supervision by selecting problems with well-defined, verifiable outputs, mitigating output-format mismatch and reward hacking. Testcase generation and verification (3.2.2). Provides stream of diverse, valid, and automatically verifiable instances, addressing the mismatch between static problem statements and stochastic environment inputs. Heuristic difficulty calibration (3.2.3). Defines an executable difficulty range under context and runtime budgets, addressing the scale mismatch between real-world inputs and agentic prompting."
        },
        {
            "title": "3.2.1 Extracting Meta Information",
            "content": "To offer high-quality programming problems for reasoning environment synthesis and mitigate the risk of reward hacking, we adopt prompt-based method to extract key meta-information from candidate problems, followed by rule-based filtering process. The complete extraction prompt is provided in Appendix 5. For each programming problem p, we extract metadata tuple metap comprising (i) scale parameters that characterize complexity-related constraint variables, and (ii) output requirements that describe the output type and whether the correct output is unique. Accordingly, we discard ill-formed problems and retain only those with unique output whose type lies in {number, array, string}."
        },
        {
            "title": "3.2.2 Generating and Verifying Testcases",
            "content": "To sample different instances with fixed extracted scale parameters from one specific environment, we construct testcase generator agent that takes target scale parameter configuration and produces an input whose content is randomized but conforms to the original problem specification. We provide the full prompting details in Appendix 5. Since the environment relies on the synthetic agent, we validate generator functions with two complementary checks. Breadth check. We sample diverse scale parameters and corresponding inputs , when following [10], each instance is evaluated by multiple independent ground-truth solutions. Consistency across solutions is required, which simultaneously (i) detects malformed generator function and (ii) re-validates the assumption that the output is unique. Deep check. For fixed scale parameter configuration, we call generator function multiple times and compare the resulting ground-truth outputs across generated instances. Based on the numbers and max size requirement of output clustering, we enforce sufficient diversity to prevent the training process from overfitting to narrow pattern distribution or exploiting reward hacking."
        },
        {
            "title": "3.2.3 Calibrating Heuristic Difficulty Levels",
            "content": "As inputs for real-world programming problems can be millions, which is not suitable for the prompt used in agentic training, we aim to re-define the difficulty range for each environment that is both scalable and executable. Apparently, the effective maximum difficulty is bounded by two practical constraints: (i) the maximum prompt length accepted by the policy model, and (ii) the execution time limit enforced by the original problem setting. To estimate the largest feasible scale parameter configuration smax under the prompt-length constraint, binary search is performed over single global scale factor. Feasibility is determined by whether the testcase fits within the context window while respecting the fixed execution time limit. Given smin and smax, the environment defines finite set of difficulty levels by discretizing the scaling range. To keep the number of levels within reasonable budget, the discretization strategy depends on the span of each parameter: when the range is small, levels follow an arithmetic progression; when the range is large, levels follow geometric progression. This heuristic yields compact but expressive difficulty ladder, enabling smooth scheduling while preserving meaningful granularity across scales. 6 Model MATHAMC23 AIME24 MMLU-Pro BBEH AVG Qwen3-1.7B-base +DeepMath-103K +MATH-7.5k +SCALER Qwen3-4B-Base +DeepMath-103K +MATH-7.5k +SCALER 59.6 73.6 75.6 75.8 66.4 86.6 86.2 84. 29.21 47.97 50.78 49.53 44.70 65.60 70.63 75.0 3.33 14.58 15.20 12. 8.75 22.29 24.16 27.29 33.30 49.64 46.78 50.89 51.60 68.03 69.19 70. 3.26 9.56 6.08 11.74 8.10 12.82 10.00 25.74 39.07 38.89 40.18 35.91 51.08 52. 14.56 54.25 Table 1 Performance comparison of Dataset-based RL and SCALER on five reasoning benchmarks. AVG is the unweighted mean over the five benchmarks. The highest performance is bolded."
        },
        {
            "title": "4 Experiments",
            "content": "SCALER addresses two key challenges: (i) providing synthesis pipeline that systematically generates environments with infinite data generation capabilities, and (ii) developing multi-environment training framework, which enables sustaining model improvement through adaptive learning across diverse environments. To valid the efficiency of our approach, experiments are organized around three research questions (RQs): RQ1: How does SCALER compare with dataset-based training baselines under comparable training budgets? RQ2: How does performance vary as the number of environments increases? RQ3: Are all components of SCALER Necessary for their gains?"
        },
        {
            "title": "4.1 Experimental Setup\nSynthesis Pipeline Settings. We use CodeContests [19] as the seed dataset and GLM-4.6 [40] as the data-\nsynthesis agent. For code execution, we use SandboxFusion [2]. After filter, we obtain the subset consisting of\n4973 programming problems and synthesize 2739 SCALER environments. Specifically, we offer environment\ncases in Appendix 5.",
            "content": "Training setup. All experiments start from Qwen3 [34]series, and we use Qwen3-1.7B-base and Qwen3-4Bbase as the policy model. For the environment curation mechanism, we set Kslope = 10 and Kzero = Ksat = 5. The training batch size equals the environment set size: at each optimization step, one problem is sampled from each of the 64 environments according to the environment-specific difficulty controller, resulting in 64 prompts per step. Reinforcement learning is performed with GRPO [30]. For each prompt, nresp=8 responses are sampled. Training-time decoding uses temperature =1.0 and top_p=1.0. The prompt length budget is capped at 4096 tokens and the response length budget at 8192 tokens. Additional hyperparameters and infrastructure details are deferred to Appendix 5. Evaluation protocol. Performance is evaluated on five benchmarks: AIME24 [43], AMC23 [1], MATH-500 [21], MMLU-Pro [37], and BBEH [17]. Results are reported as avg@16 for AIME24, AMC23 and MMLU-Pro, avg@1 for the remaining benchmarks. Unless otherwise specified, decoding uses temperature = 0.6 and top_p = 0.95."
        },
        {
            "title": "4.2 RQ1: Environment-based training is better than Dataset-based training",
            "content": "A common paradigm for improving reasoning is to scale up training on static datasets, such as curated corpora with verified solutions or collections augmented by teacher-generated answers. However, static corpora provide limited effective learning signals, where agents must repeatedly adapt to new environments and 7 Figure 2 Left: average performance across the five evaluation benchmarks during Qwen3-4B-base training, comparing dataset-based baselines (MATH, DeepMath) and SCALER. Right: effective sampling statistics under SCALER, indicating that most sampled instances remain near the models capability boundary. Figure 3 Accuracy improvements for both Qwen3-4B-base and Qwen3-1.7B-base as environment size increases. Both models show consistent increase in performance with larger environment sizes. generalize beyond previously seen distributions. To evaluate whether interactive environment-based learning better supports the improvement of model capability, this RQ compares SCALER with two dataset-based baselines under comparable training budgets: MATH [15], curated corpus with verified solutions, and DeepMath [14], dataset of teacher-generated solutions. Table 1 shows that both dataset baselines substantially improve over the Qwen3-1.7B-Base and Qwen3-4B-Base, with particularly large gains on math-centric benchmarks. In contrast, SCALER achieves the best overall average and delivers consistent improvements across all five evaluations, including three math benchmarks, MMLU-Pro and BBEH, suggesting stronger transfer beyond narrowly curated math data. The training dynamics in Figure 2 provide additional evidence for this gap. Figure 2 (left) shows that SCALER not only reaches higher level, but also exhibits more sustained improvement trend: the aggregated evaluation performance continues to rise for more than 1,000 training steps, whereas dataset-based baselines plateau earlier. Importantly, this behavior is supported by boundary-focused sampling: Figure 2 (right) indicates that SCALER keeps higher effective sampling rate than dataset-based baselines, where most prompts remain neither trivial nor intractable. By continuously providing instances near the models current capability boundary from SCALER environments, SCALER sustains informative reward during RL, mitigates 8 Figure 4 Training dynamics of Qwen3-4B-Base with different numbers of environments. The plot shows that even with smaller environments, the model continues to learn with increasing difficulty levels. premature saturation, and enables longer-horizon performance gains."
        },
        {
            "title": "4.3 RQ2: Scaling environment size leads to incremental performance gains",
            "content": "SCALER introduces the automatic environment synthesis pipeline which significantly reduces labor costs. This RQ examines how model performance changes as the number of environments increases, with results summarized in Figure 3. We conduct the experiment by randomly sample 8, 64, 512 environments from 2739 SCALER environments, where the larger size of environments always contain the smaller size one. As shown in Figure 3, increasing the number of environments from 8 to 2739 leads to incremental performance improvements. The model benefits from encountering greater diversity of tasks, which allows it to maintain consistent learning progress. With exposure to wider variety of challenges and environments facilitating the development of broader reasoning skills, scaling the number of environments enables the model to continuously adapt and enhance its reasoning capabilities. It is worth noting that even with smaller number of environments, the model continues to engage in ongoing exploration, with increasing difficulty levels within each individual environment, as illustrated in Figure 4. The key insight is that scaling the number of environments with fixed training budgets, inherently involves trade-off between task difficulty and diversity. Striking right balance is critical: focusing solely on increasing difficulty may result in limited improvements, while excessive diversity without proper difficulty controller, i.e. DeepMath dataset with 103k samples, does not necessarily lead to optimal performance."
        },
        {
            "title": "4.4 RQ3: All components of SCALER are necessary for its gains",
            "content": "This experiment studies whether SCALERs improvements require its two core components in multienvironment training framework: adaptive difficulty controller and environment curation mechanism. Two ablations based on Qwen3-4B-base are compared against the full SCALER system in Figure 5. Removing the curation mechanism disables environment replacement, forcing training to random sample every environment. To model the ablation of difficulty controller, we replace the boundary tracking with random difficulty sampling, selecting 5 instances per environment. This modification reduces the consistency of the supervision signal, as the environment selection is no longer guided by difficulty-based approach. Both ablations lead to lower performance than the full system, indicating that difficulty controller and the curation mechanism each make material contribution. Qualitatively, difficulty controller prevents training 9 Figure 5 Ablation study of SCALER on Qwen3-4B-Base. Validation accuracy over training steps for the full system and two variants that remove difficulty controller or the environment curation mechanism. Both components contribute to stronger and more sustained performance improvements. from drifting into regimes that are too easy or too hard, while the curation mechanism promotes sustained learning within learnable environments and mitigates shallow adaptation."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we proposes novel approach SCALER containing two key components: (i) reasoning environment synthesis pipeline, (ii) multi-environment training framework. Through our synthesis pipeline to generating variety of difficulty-controllable environments , our approach offers platform for research community to explore the impact of properties of environments on RL training. Moreover, the multienvironment training framework ensures the difficulty of instances consistently matches the models capabilities and maintains diversity and freshness, jointly preserving the continuously effective reward signals to improve model capabilities in long horizon."
        },
        {
            "title": "Limitations",
            "content": "While our work provides comprehensive analysis of SCALERs ability to improve model performance through dynamic environment synthesis and multi-environment training, there are several limitations that warrant further exploration: Exploration of Environment Internal Attributes: Our study primarily focuses on scaling environment size and its components. However, the internal characteristics of environments, such as the richness of context, intrinsic difficulty, and other environment-specific properties, have yet to be thoroughly investigated. Future research should examine how these factors influence model performance, especially in the context of dynamic difficulty adjustment and environment transitions during training. Limited Scope of Environments: Our study is based on 2739 SCALER environments, and while this is larger number compared to hand-crafted ones, the scaling still hasnt subsided due to our experiments. Further research is needed to explore scaling laws related to environment size, model size, and computational resources, and to understand how these scaling factors impact both model training efficiency and performance."
        },
        {
            "title": "Extract Meta Information Prompt",
            "content": "Prompt 1 Prompt used for extracting meta information. You are given as input the full statement of single algorithmic problem . Your job is to ** emit one JSON code block ** that captures : 1. All numeric * scale parameters * that are relevant to the ** time complexity ** of typical solutions . 2. The ** type ** of the required output . 3. Whether the required output is ** unique ** , whether the problem has exactly one correct output for each valid input . ## 1. Scale parameters * scale parameter * is any integer quantity that bounds : - The number of items , elements , or positions (n= number of elements , = number of edges , = number of queries ). - The size ( length ) of grid , string , or sequence (e.g. length up to 2e5 , grid up to 1000 * 1000) . - The size of state space or iteration space that an algorithm must explicitly handle . ### What to INCLUDE as scale parameters Include parameter only if ** all ** of the following are true : - It directly bounds the size or count of something that is iterated over , e.g .: - number of elements / vertices / edges / queries (n , , , etc .) , - length of string or array , - rows / columns of grid . - The bounds appear explicitly in the statement , usually in the Input / Constraints section , like : - 1 <= <= 2*10^5 - 0 <= <= 2*10^5 - You can clearly identify the parameter name (e.g., , , , , , etc .) . ### What to EXCLUDE from scale parameters - Number of test cases / groups : ** never ** include , , or similar when it means \" number of test cases \". - Pure value ranges for single items that do ** not ** change the input size , e.g .: - -10^9 <= a_i <= 10^9 when a_i is just the value of an element . - Coordinate or weight ranges that are not used as sizes of arrays / grids . - Any quantity that only affects output format or precision . ### Representation of scale parameters In the JSON , represent scale parameters under the key \" scale_params \" as : < scale_params example > ## 2. Output type classification You must classify the type of the required output into exactly one of the following strings : - \" string \": The required output is single string or small number of strings . - \" number \": The required output is single numeric value ( integer , real , etc .) , e.g. \" print one integer as the answer \". - \" array \": The required output is one - dimensional sequence ( list ) of values , e.g. an array of integers , permutation , sequence of answers for each query when printed as space - separated numbers or in multiple lines . - \" graph \": The required output is graph structure , such as set of edges , tree description , adjacency list , or any structure where the output itself is naturally graph . - \" matrix \": The required output is 2D grid or matrix . - \" bool \": The required output is logically boolean answer , e.g. \" YES / NO \", \" True / False \", \" Alice / Bob \", etc . - \" others \": The required output is complex or mixed structure that does not fit clearly into any of the above categories . ## 3. Output uniqueness You must also decide whether the required output is unique for each valid input . Define \" is_output_unique \" as : - true if , for any fixed valid input , there is exactly one correct output that satisfies the problem statement . - false if the statement allows multiple different outputs to be accepted as correct for the same input . ## JSON Output Specification 11 You must produce exactly one JSON object in fenced JSON code block . The JSON must have the following top - level keys : - \" scale_params \": an object mapping parameter names to {{ \" min \": <int >, \" max \": <int > }}. - \" output_type \": one of \" string \", \" number \", \" array \", \" graph \", \" matrix \", \" bool \", \" others \". - \" is_output_unique \": boolean . ## Example 1: < example 1> ## Example 2: < example 2> ## Example 3: < example 3> ## Final instruction Now , read the provided problem statement and output the single JSON code block accordingly . { problem }"
        },
        {
            "title": "Generate Test Case Prompt",
            "content": "Prompt 2 Prompt used for constructing generate_testcase function. You are given as input single * algorithmic problem statement * ( like those from programming contests ). Your job is to ** emit one Python code block ** that defines * test - case generator * function for this problem . The generator must produce ** exactly one ** valid test case per call , parameterized only by the numeric scale values provided via JSON object . ## Input You will be given : 1. raw problem statement in natural language that fully specifies : - the input format , - the constraints , - and the meaning of each variable . 2. An example json_obj instance : - This is only an example to clarify field names and typical ranges . - Your code must work for any valid json_obj that matches the described schema . ## Required Python output ( emit exactly one Python code block ) You must output Python code block that defines ** one single function ** with the following signature : python def generate_testcase ( json_obj : dict ) -> tuple [ str , dict ]: \"\"\" Generate test case based on the given json_obj . Parameters : - json_obj ( dict ): The input JSON object containing problem parameters . Returns : - tuple [ str , dict ]: tuple containing : - The first element is string representing the test case in input format . - The second element is dictionary representing the same test case . \"\"\" ... ### Return value - Your function must return both the string and the dictionary representation of the test case in tuple . The first element of the tuple should be the string format , and the second element should be the dictionary format . - output_str : - single string that is valid input for the problem according to the Input section , representing exactly one logical test case . - If the problem statement defines format with multiple test cases controlled by an integer in the input , you must set = 1. - Example : \"1 n5 n1 2 3 4 5\" - output_dict : - Python dict that is structured , formal description of the same test case . - If the problem statement contains multiple test cases , ** do not ** introduce or any extra wrapper . - Example : {\" \": 5, \" list \": [1 , 2, 3, 4, 5]} ## Constraints - All sizes ( counts , lengths , number of operations , etc .) must be determined only from json_obj . 12 - All other values ( elements of arrays , weights , edges , indices , etc .) must be generated randomly within reasonable range and ** strictly smaller than 10000** , while satisfying the problem constraints at the same time . - If the problem allows \"no - solution \" cases (e.g., the intended output is -1 when no solution exists ) , you should ** bias your random generation towards test cases that admit at least one valid solution ** , and explicitly construct values to satisfy any hidden feasibility conditions , so that the correct solution is not trivially always the \"no - solution \" output . ## Problem statement { problem } ## Example json_obj { example_json_obj }"
        },
        {
            "title": "Environment Cases",
            "content": "Example Problem: 33_C. Wonderful Randomized Sum"
        },
        {
            "title": "Problem\nDescription",
            "content": "33_C. Wonderful Randomized Sum Learn, learn and learn again Valera has to do this every day. He is studying at mathematical school, where math is the main discipline. The mathematics teacher loves her discipline very much and tries to cultivate this love in children. Thats why she always gives her students large and difficult homework. Despite that Valera is one of the best students, he failed to manage with the new homework. Thats why he asks for your help. He has the following task. sequence of numbers is given. prefix of sequence is the part of the sequence (possibly empty), taken from the start of the sequence. suffix of sequence is the part of the sequence (possibly empty), taken from the end of the sequence. It is allowed to sequentially make two operations with the sequence. The first operation is to take some prefix of the sequence and multiply all numbers in this prefix by - 1. The second operation is to take some suffix and multiply all numbers in it by - 1. The chosen prefix and suffix may intersect. What is the maximum total sum of the sequence that can be obtained by applying the described operations?"
        },
        {
            "title": "Generate Testcase",
            "content": "def generate_testcase ( json_obj , output_format =\" str \"): # Generate random test case for the problem = int ( json_obj . get (\"n\" , 10) ) numbers = random . sample ( range (1 , 100) , n) if output_format == \" dict \": return {\"n\": , \" numbers \": numbers } else : return f\"{n } n{ . join ( map ( str , numbers ))}\""
        },
        {
            "title": "Difficulty\nMapping",
            "content": "The first and the only line of the output should contain the answer to the problem. { \"0\": 0, \"1\": 1, \"2\": 2, \"3\": 2, \"4\": 3, \"5\": 4, \"6\": 5, \"7\": 6, \"8\": 8, \"9\": 11, \"10\": 14, \"11\": 18, \"12\": 23, \"13\": 30, \"14\": 39, \"15\": 51, \"16\": 67, \"17\": 87, \"18\": 112, \"19\": 146, \"20\": 190, \"21\": 247, \"22\": 318 } Example Problem: 1497_D. Genius"
        },
        {
            "title": "Name",
            "content": "1497_D. Genius"
        },
        {
            "title": "Problem\nDescription",
            "content": "Please note the non-standard memory limit.There are problems numbered with integers from 1 to n. i-th problem has the complexity ci = 2i, tag tagi and score si.After solving the problem its allowed to solve problem if and only if IQ < ci cj and tagi tagj. After solving it your IQ changes and becomes IQ = ci cj and you gain si sj points.Any problem can be the first. You can solve problems in any order and as many times as you want.Initially your IQ = 0. Find the maximum number of points that can be earned."
        },
        {
            "title": "Generate Testcase",
            "content": "def generate_testcase ( json_obj , output_format =\" str \"): # Generate test case for the Genius problem = int ( json_obj . get (\"n\" , 10) ) tags = random . sample ( range (1 , +1) , n) scores = random . sample ( range (1 , 100) , n) if output_format == \" dict \": return {\"n\": , \" tags \": tags , \" scores \": scores } else : return f\"{n } n{ . join ( map ( str , tags )) } n{ . join ( map ( str , scores ))}\""
        },
        {
            "title": "Difficulty\nMapping",
            "content": "For each test case print single integer the maximum number of points that can be earned. { \"0\": 0, \"1\": 2, \"2\": 3, \"3\": 4, \"4\": 7, \"5\": 10, \"6\": 17, \"7\": 27, \"8\": 43, \"9\": 69, \"10\": 110, \"11\": 176, \"12\": 281, \"13\": 450, \"14\": 721, \"15\": 1153 } Example Problem: 1466_B. Last Minute Enhancements"
        },
        {
            "title": "Problem\nDescription",
            "content": "1466_B. Last Minute Enhancements Athenaeus has just finished creating his latest musical composition and will present it tomorrow to the people of Athens. Unfortunately, the melody is rather dull and highly likely wont be met with warm reception. His song consists of notes, which we will treat as positive integers. The diversity of song is the number of different notes it contains. As patron of music, Euterpe watches over composers and guides them throughout the process of creating new melodies. She decided to help Athenaeus by changing his song to make it more diverse.Being minor goddess, she cannot arbitrarily change the song. Instead, for each of the notes in the song, she can either leave it as it is or increase it by 1.Given the song as sequence of integers describing the notes, find out the maximal, achievable diversity."
        },
        {
            "title": "Generate Testcase",
            "content": "def generate_testcase ( json_obj , output_format =\" str \"): # Generate test case for the song diversity problem = int ( json_obj . get (\"n\" , 5) ) notes = random . sample ( range (1 , 10) , n) if output_format == \" dict \": return {\"n\": , \" notes \": notes } else : return f\"{n } n{ . join ( map ( str , notes )) }\""
        },
        {
            "title": "Output\nRequirement",
            "content": "For each test case, you should output single line containing precisely one integer, the maximal diversity of the song, i.e. the maximal possible number of different elements in the final sequence."
        },
        {
            "title": "Difficulty\nMapping",
            "content": "{ \"0\": 0, \"1\": 1, \"2\": 2, \"3\": 2, \"4\": 3, \"5\": 4, \"6\": 5, \"7\": 6, \"8\": 8, \"9\": 11, \"10\": 14, \"11\": 18, \"12\": 23, \"13\": 30, \"14\": 39, \"15\": 51, \"16\": 67, \"17\": 87, \"18\": 112, \"19\": 146, \"20\": 190, \"21\": 247, \"22\": 321, \"23\": 418 }"
        },
        {
            "title": "Clip Ratio High",
            "content": "Temperature (Training) Top (Training) Top (Training)"
        },
        {
            "title": "Number of Response per Prompt",
            "content": "Qwen3-4B-Base Qwen3-1.7B-Base 10 5 5 1e-6 20 4096 8192 0 64 64 grpo 0.0 0.2 0.2 1.0 1. -1 0.6 0.95"
        },
        {
            "title": "References",
            "content": "[1] Art of Problem Solving. Amc 12 (2023) problems. https://artofproblemsolving.com/wiki/index.php/2023_AMC_ 12A_Problems, 2023. We use the 2023 AMC 12A/12B problems as hosted by AoPS; processed version is available at https://huggingface.co/datasets/zwhe99/amc23. [2] Bytedance-Seed-Foundation-Code-Team, :, Yao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, Wentao Chen, Zhengyu Chen, Shĳie Geng, Aoyan Li, Bo Li, Bowen Li, Linyi Li, Boyi Liu, Jiaheng Liu, Kaibo Liu, Qi Liu, Shukai Liu, Siyao Liu, Tianyi Liu, Tingkai Liu, Yongfei Liu, Rui Long, Jing Mai, Guanghan Ning, Z. Y. Peng, Kai Shen, Jiahao Su, Jing Su, Tao Sun, Yifan Sun, Yunzhe Tao, Guoyin Wang, Siwei Wang, Xuwu Wang, Yite Wang, Zihan Wang, Jinxiang Xia, Liang Xiang, Xia Xiao, Yongsheng Xiao, Chenguang Xi, Shulin Xin, Jingjing Xu, Shikun Xu, Hongxia Yang, Jack Yang, Yingxiang Yang, Jianbo Yuan, Jun Zhang, Yufeng Zhang, Yuyu Zhang, Shen Zheng, He Zhu, and Ming Zhu. Fullstack bench: Evaluating llms as full stack coders, 2025. URL https://arxiv.org/abs/2412.00535. [3] Justin Yang Chae, Md Tanvirul Alam, and Nidhi Rastogi. Towards understanding self-play for llm reasoning, 2025. URL https://arxiv.org/abs/2510.27072. [4] Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, and Kwan-Yee K. Wong. Spc: Evolving self-play critic via adversarial games for llm reasoning, 2025. URL https://arxiv.org/abs/ 2504.19162. [5] Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, and Ehsan Kamalloo. Self-evolving curriculum for llm reasoning. arXiv preprint arXiv:2505.14970, 2025. [6] Xinjie Chen, Minpeng Liao, Guoxin Chen, Chengxi Li, Biao Fu, Kai Fan, and Xinggao Liu. From data-centric to samplecentric: Enhancing llm reasoning via progressive optimization, 2025. URL https://arxiv.org/abs/2507.06573. [7] Zhuoyue Chen, Jihai Zhang, Ben Liu, Fangquan Lin, and Wotao Yin. Scale down to speed up: Dynamic data selection for reinforcement learning. Training, 2500:3000, 2025. [8] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zĳia Zhu, Zĳun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [9] Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, and Dacheng Tao. Serl: Self-play reinforcement learning for large language models with limited data, 2025. URL https://arxiv.org/abs/2505.20347. [10] Jia Fu, Xinyu Yang, Hongzhi Zhang, Yahui Liu, Jingyuan Zhang, Qi Wang, Fuzheng Zhang, and Guorui Zhou. 16 Klear-codetest: Scalable test case generation for code reinforcement learning, 2025. URL https://arxiv.org/abs/ 2508.05710. [11] Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, and Christopher D. Manning. Synthetic data generation & multi-step rl for reasoning & tool use, 2025. URL https://arxiv.org/abs/2504.04736. [12] Jiacheng Guo, Ling Yang, Peter Chen, Qixin Xiao, Yinjie Wang, Xinzhe Juan, Jiahao Qiu, Ke Shen, and Mengdi Wang. Genenv: Difficulty-aligned co-evolution between llm agents and environment simulators, 2025. URL https://arxiv.org/abs/2512.19682. [13] Yiduo Guo, Zhen Guo, Chuanwei Huang, Zi-Ang Wang, Zekai Zhang, Haofei Yu, Huishuai Zhang, and Yikang Shen. Synthetic data rl: Task definition is all you need, 2025. URL https://arxiv.org/abs/2505.17063. [14] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. [15] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/ 2103.03874. [16] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [17] Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, and Orhan Firat. BIG-bench extra hard. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26473 26501, Vienna, Austria, July 2025. Association for Computational Linguistics. doi: 10.18653/v1/2025.acl-long.1285. URL https://aclanthology.org/2025.acl-long.1285/. [18] Shipeng Li, Shikun Li, Zhiqin Yang, Xinghua Zhang, Gaode Chen, Xiaobo Xia, Hengyu Liu, and Zhe Peng. Learnalign: Reasoning data selection for reinforcement learning in large language models based on improved gradient alignment, 2025. URL https://arxiv.org/abs/2506.11480. [19] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Axel Gimeno Gil, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Alphacode data materials, December 2022. URL https://doi.org/10.5281/zenodo.6975437. Includes dm-code_contests (CodeContests competitive programming dataset used in AlphaCode). [20] Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, and Weizhu Chen. Sws: Self-aware weakness-driven problem synthesis in reinforcement learning for llm reasoning, 2025. URL https://arxiv.org/abs/2506.08989. [21] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. URL https://arxiv.org/abs/2305.20050. Releases PRM800K and the MATH-500 (500-problem) test subset via https://github.com/openai/prm800k. [22] Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, et al. Spiral: Self-play on zero-sum games incentivizes reasoning via multi-agent multi-turn reinforcement learning. arXiv preprint arXiv:2506.24119, 2025. [23] Huanyu Liu, Jia Li, Hao Zhu, Kechi Zhang, Yihong Dong, and Ge Li. Saturn: Sat-based reinforcement learning to unleash language model reasoning. arXiv preprint arXiv:2505.16368, 2025. [24] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret 17 Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontĳo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. [25] Shubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang, James Caverlee, Dileep Kalathil, and Shuiwang Ji. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning, 2025. URL https://arxiv.org/abs/2506.06632. [26] Shubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang, James Caverlee, Dileep Kalathil, et al. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning. arXiv preprint arXiv:2506.06632, 2025. [27] Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, and Etai Littwin. Vanishing gradients in reinforcement finetuning of language models. arXiv preprint arXiv:2310.20703, 2023. [28] Noam Razin, Zixuan Wang, Hubert Strauss, Stanley Wei, Jason Lee, and Sanjeev Arora. What makes reward model good teacher? an optimization perspective. arXiv preprint arXiv:2503.15477, 2025. [29] Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold, 2024. URL https://arxiv.org/abs/2406.14532. [30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [31] Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, and Jieyu Zhao. Efficient reinforcement finetuning via adaptive curriculum learning. arXiv preprint arXiv:2504.05520, 2025. [32] Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, and Andreas 18 Köpf. Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards. arXiv preprint arXiv:2505.24760, 2025. [33] Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms, 2025. [34] Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [35] Hong Wang, Zhezheng Hao, Jian Luo, Chenxing Wei, Yao Shu, Lei Liu, Qiang Lin, Hande Dong, and Jiawei Chen. Scheduling your llm reinforcement learning with reasoning trees, 2025. URL https://arxiv.org/abs/2510.24832. [36] Pinzheng Wang, Juntao Li, Zecheng Tang, Haĳia Gui, and Min zhang. Improving rationality in the reasoning process of language models through self-playing game, 2025. URL https://arxiv.org/abs/2506.22920. [37] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlupro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. URL https://arxiv.org/abs/2406.01574. Accepted at NeurIPS 2024 Datasets and Benchmarks (Spotlight). Dataset: https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro. [38] Zĳian Wu, Jinjie Ni, Xiangyan Liu, Zichen Liu, Hang Yan, and Michael Qizhe Shieh. Synthrl: Scaling visual reasoning with verifiable data synthesis, 2025. URL https://arxiv.org/abs/2506.02096. [39] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. [40] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. [41] Zhiyuan Zeng, Hamish Ivison, Yiping Wang, Lifan Yuan, Shuyue Stella Li, Zhuorui Ye, Siting Li, Jacqueline He, Runlong Zhou, Tong Chen, et al. Rlve: Scaling up reinforcement learning for language models with adaptive verifiable environments. arXiv preprint arXiv:2511.07317, 2025. [42] Bonan Zhang, Zhongqi Chen, Bowen Song, Qinya Li, Fan Wu, and Guihai Chen. Confclip: Confidence-weighted and clipped reward for reinforcement learning in llms, 2025. URL https://arxiv.org/abs/2509.17730. [43] Yifan Zhang and Team Math-AI. Aime 2024. https://huggingface.co/datasets/math-ai/aime24, 2024. [44] Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Innovation Institute",
        "Singapore Management University"
    ]
}