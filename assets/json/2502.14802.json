{
    "paper_title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language Models",
    "authors": [
        "Bernal Jiménez Gutiérrez",
        "Yiheng Shu",
        "Weijian Qi",
        "Sizhe Zhou",
        "Yu Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Our code and data will be released at https://github.com/OSU-NLP-Group/HippoRAG."
        },
        {
            "title": "Start",
            "content": "From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Bernal Jimenez Gutierrez * 1 Yiheng Shu * 1 Weijian Qi 1 Sizhe Zhou 2 Yu Su"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 5 2 0 2 0 2 ] . [ 1 2 0 8 4 1 . 2 0 5 2 : r Our ability to continuously acquire, organize, and leverage knowledge is key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Our code and data will be released at https://github.com/ OSU-NLP-Group/HippoRAG. *Equal contribution 1The Ohio State University, Columbus, OH, USA 2University of Illinois Urbana-Champaign, IL, USA. Correspondence to: Bernal Jimenez Gutierrez <jimenezgutierrez.1@osu.edu>, Yiheng Shu <shu.251@osu.edu>, Yu Su <su.809@osu.edu>. 1 In an ever-evolving world, the ability to continuously absorb, integrate, and leverage knowledge is one of the most important features of human intelligence. From lawyers navigating shifting legal frameworks to researchers tracking multifaceted scientific progress, much of our productivity relies on this incredible capacity for continual learning. It is imperative for AI systems to approximate this capability in order to become truly useful human-level assistants. In recent years, large language models (LLMs) have made remarkable progress in many aspects of human intelligence. However, efforts to endow these models with our evolving long-term memory capabilities have faced significant challenges in both fully absorbing new knowledge (Zhong et al., 2023; Hoelscher-Obermaier et al., 2023) and avoiding catastrophic forgetting (Cohen et al., 2024; Gu et al., 2024), due to the complex distributional nature of their parametric knowledge. Retrieval-augmented generation (RAG) has emerged as way to circumvent these obstacles and allow LLMs to access new information in non-parametric fashion without altering an LLMs parametric representation. Due to their simplicity and robustness (Zhong et al., 2023; Xie et al., 2024), RAG has quickly become the de facto continual learning solution for production LLM systems. However, their reliance on simple vector retrieval results in the inability to capture two vital aspects of our interconnected long-term memory system: sense-making (Klein et al. (2006); the ability to interpret larger, more complex, or uncertain contexts) and associativity (Suzuki (2005); the capacity to draw multi-hop connections between disparate pieces of knowledge). Several RAG frameworks that engage an LLM to explicitly structure its retrieval corpus have been recently proposed to address these limitations. To enhance sense-making, such structure-augmented RAG methods allow an LLM to either generate summaries (Edge et al., 2024; Sarthi et al., 2024; Chen et al., 2023) or knowledge graph (KG) structure (Guo et al., 2024) to link groups of disparate but related passages, thereby improving the RAG systems ability to understand longer and more complex discourse such as long stories. To address the associativity gap, the authors of HippoRAG (Gutierrez et al., 2024) use the Personalized From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Figure 1. Evaluation of continual learning capabilities across three key dimensions: factual memory (NaturalQuestions, PopQA), sensemaking (NarrativeQA), and associativity (MuSiQue, 2Wiki, HotpotQA, and LV-Eval). HippoRAG 2 surpasses other methods across all benchmark categories, bringing it one step closer to true long-term memory system. PageRank algorithm (Haveliwala, 2002) and an LLMs ability to automatically construct KG and endow the retrieval process with multi-hop reasoning capabilities. Although these methods demonstrate strong performance in both of these more challenging memory tasks, bringing RAG truly closer to human long-term memory requires robustness across simpler memory tasks as well. In order to understand whether these systems could achieve such robustness, we conduct comprehensive experiments that not only simultaneously evaluate their associativity and sensemaking capacity through multi-hop QA and large-scale discourse understanding, but also test their factual memory abilities via simple QA tasks, which standard RAG is already well-equipped to handle. As shown in Figure 1, our evaluation reveals that all previous structure-augmented methods underperform against the strongest embedding-based RAG methods available on all three benchmark types. Perhaps unsurprisingly, we find that each method type experiences the largest performance decay in tasks outside its own experimental setup. For example, HippoRAGs performance drops most on large-scale discourse understanding due to its lack of query-based contextualization, while RAPTORs performance deteriorates substantially on the simple and multi-hop QA tasks due to the noise introduced into the retrieval corpora by its LLM summarization mechanism. In this work, we leverage this experimental setting to help us address the robustness limitations of these innovative approaches while avoiding the pitfalls of focusing too narrowly on just one task. Our proposed method, HippoRAG 2, leverages the strength of HippoRAGs OpenIE and Personalized PageRank (PPR) methodologies while addressing its querybased contextualization limitations by integrating passages into the PPR graph search process, involving queries more deeply in the selection of KG triples as well as engaging an LLM in the online retrieval process to recognize when retrieved triples are irrelevant. Through extensive experiments, we find that this design provides HippoRAG 2 with consistent performance improvements over the most powerful standard RAG methods across the board. More specifically, our approach achieves an average 7 point improvement over standard RAG in associativity tasks while showing no deterioration and even slight improvements in factual memory and sense-making tasks. Furthermore, we show that our method is robust to different retrievers as well as to the use of strong open-source and proprietary LLMs, allowing for wide degree of usage flexibility. All of these results suggest that HippoRAG 2 is promising step in the development of more human-like non-parametric continual learning system for LLMs. 2. Related Work 2.1. Continual Learning for LLMs Continual learning methods applied to LLMs aim to allow them to acquire and integrate new knowledge over time while preserving past information. Given the high computational cost of full-scale LLM pretraining, various techniques have been used to achieve this goal. These approaches gen2 From RAG to Memory: Non-Parametric Continual Learning for Large Language Models erally fall into three categories: continual fine-tuning, model editing, and RAG (Shi et al., 2024). level knowledge, integrating graph structures with vector retrieval. Continual fine-tuning involves periodically training an LLM on new data. This can be achieved through methods like continual pretraining (Jin et al., 2022), instruction tuning (Zhang et al., 2023), and alignment fine-tuning (Zhang et al., 2024). While effective in incorporating new linguistic patterns and reasoning skills, continual fine-tuning suffers from catastrophic forgetting (Huang et al., 2024), where previously learned knowledge is lost as new data is introduced. Moreover, its computational expense makes frequent updates impractical for real-world applications. Model editing techniques (Yao et al., 2023) provide more lightweight alternative by directly modifying specific parameters in the model to update its knowledge. However, these updates have been found to be highly localized, having little effect on information associated with the update that should also be changed. RAG has emerged as scalable and practical alternative for continual learning. Instead of modifying the LLM itself, RAG retrieves relevant external information at inference time, allowing for real-time adaptation to new knowledge. We will discuss several aspects of this non-parametric continual learning solution for LLMs in the next section. 2.2. Non-Parametric Continual Learning for LLMs Encoder model improvements, particularly with LLM backbones, have significantly enhanced RAG systems by generating high-quality embeddings that better capture semantic relationships, improving retrieval quality for LLM generation. Recent models (Li et al., 2023; Muennighoff et al., 2024; Lee et al., 2025) leverage LLMs, large corpora, improved architectures, and instruction fine-tuning for notable retrieval gains. NV-Embed-v2 (Lee et al., 2025) serves as the primary comparison in this paper. Sense-making is the ability to understand large-scale or complex events, experiences, or data (Koli et al., 2024). Standard RAG methods are limited in this capacity since they require integrating information from disparate passages, and thus, several RAG frameworks have been proposed to address it. RAPTOR (Sarthi et al., 2024) and GraphRAG (Edge et al., 2024) both generate summaries that integrate their retrieval corpora. However, they follow distinct processes for detecting what to summarize and at what granularity. While RAPTOR uses Gaussian Mixture Model to detect document clusters to summarize, GraphRAG uses graph community detection algorithm that can summarize documents, entity clusters with relations, or combination of these elements. LightRAG (Guo et al., 2024) employs dual-level retrieval mechanism to enhance comprehensive information retrieval capabilities in both low-level and highAlthough both GraphRAG and LightRAG use KG just like our HippoRAG 2 approach, our KG is used to aid in the retrieval process rather than to expand the retrieval corpus itself. This allows HippoRAG 2 to introduce less LLM-generated noise, which deteriorates the performance of these methods in single and multi-hop QA tasks. Associativity is the capacity to draw multi-hop connections between disparate facts for efficient retrieval. It is an important part of continual learning, which standard RAG cannot emulate due to its reliance on independent vector retrieval. HippoRAG (Gutierrez et al., 2024) is the only RAG framework that has addressed this property by leveraging the PPR algorithm over an explicitly constructed open KG. HippoRAG 2 is closely inspired by HippoRAG, which allows it to perform very well on multi-hop QA tasks. However, its more comprehensive integration of passages, queries, and triples allows it to have more comprehensive performance across sense-making and factual memory tasks as well. 3. HippoRAG 2 3.1. Overview HippoRAG (Gutierrez et al., 2024) is neurobiologically inspired long-term memory framework for LLMs, with each component designed to emulate aspects of human memory. The framework consists of three primary components: the artificial neocortex (LLM), the parahippocampal region (PHR encoder), and the artificial hippocampus (open KG). These components collaborate to replicate the interactions observed in human long-term memory. For HippoRAG offline indexing, an LLM processes passages into KG triples, which are then incorporated into the artificial hippocampal index. Meanwhile, the PHR is responsible for detecting synonymy to interconnect information. For HippoRAG online retrieval, the LLM neocortex extracts named entities from query, while the PHR encoder link these entities to the hippocampal index. Then, the Personalized PageRank (PPR) algorithm on the KG is conducted for context-based retrieval. Although HippoRAG seeks to construct memory from non-parametric RAG, its effectiveness is hindered by critical flaw: an entity-centric approach that causes context loss during both indexing and inference, as well as difficulties in semantic matching. Built on the neurobiologically inspired long-term memory framework proposed in HippoRAG (Gutierrez et al., 2024), the structure of HippoRAG 2 follows similar two-stage process: offline indexing and online retrieval, as shown in Figure 2. Additionally, however, HippoRAG 2 introduces several key refinements that improve its alignment with 3 From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Figure 2. HippoRAG 2 methodology. For offline indexing, we use an LLM to extract open KG triples from passages, with synonym detection applied to phrase nodes. Together, these phrases and passages form the open KG. For online retrieval, an embedding model scores both the passages and triples to identify the seed nodes of both types for the Personalized PageRank (PPR) algorithm. Recognition memory filters the top triples using an LLM. The PPR algorithm then performs context-based retrieval on the KG to provide the most relevant passages for the final QA task. The different colors shown in the KG nodes above reflect their probability mass; darker shades indicate higher probabilities induced by the PPR process. Table 1. Dataset statistics NQ PopQA MuSiQue 2Wiki HotpotQA LV-Eval NarrativeQA Num of queries Num of passages 1, 000 9, 633 1, 000 8, 1, 000 11, 656 1, 000 6, 119 1, 000 9, 811 124 22, 849 293 4, 111 human memory mechanisms: 1) It seamlessly integrates conceptual and contextual information within the open KG, enhancing the comprehensiveness and atomicity of the constructed index (3.2). 2) It facilitates more context-aware retrieval by leveraging the KG structure beyond isolated KG nodes (3.3). 3) It incorporates recognition memory to improve seed node selection for graph search (3.4). In the following sections, we introduce the pipeline in more detail and elaborate on each of these refinements. Offline Indexing. 1) HippoRAG 2 leverages an LLM to extract triples from each passage and integrates them into schema-less open KG. We call the subject or object of these triples phrase and the edge connecting them relation edge. 2) Next, the encoder identifies synonyms by evaluating phrase pairs within the KG, detecting those with vector similarity above predefined threshold, and adding synonym edge between such pair. This process enables the KG to link synonyms across different passages, facilitating the integration of both old and new knowledge during learning. 3) Finally, this phrase-based KG is combined with the original passages, allowing the final open KG to incorporate both conceptual and contextual information (3.2). Online Retrieval. 1) The query is linked to relevant triples and passages using the encoder, identifying potential seed nodes for graph search (3.3). 2) During triple linkage, the recognition memory functions as filter, ensuring only relevant triples are retained from the retrieved set (3.4). 3) Given seed nodes, the PPR algorithm is then applied for context-aware retrieval, refining the linking results to retrieve the most relevant passages. 4) Finally, the retrieved passages serve as contextual inputs for the final QA task. Next, we describe each of the improvements in HippoRAG 2 in more detail. 3.2. Dense-Sparse Integration The nodes in the HippoRAG KG primarily consist of phrases describing concepts, which we refer to as phrase nodes in this paper. This graph structure introduces limitations related to the concept-context tradeoff. Concepts are concise and easily generalizable but often entail information loss. In contrast, context provide specific circumstances that shape the interpretation and application of these concepts, enriching semantics but increasing complexity. However, in human memory, concepts and contexts are intricately interconnected. The dense and sparse coding theory offers insights into how the brain represents and processes information at different granularities (Beyeler et al., 2019). Dense coding encodes information through the simultaneous activation of many neurons, resulting in distributed and redundant representation. Conversely, sparse coding relies on minimal From RAG to Memory: Non-Parametric Continual Learning for Large Language Models neural activation, engaging only small subset of neurons to enhance efficiency and storage compactness. Inspired by the dense-sparse integration observed in the human brain, we treat the phrase node as form of sparse coding for the extracted concepts, while incorporating dense coding into our KG to represent the context from which these concepts originate. First, we adopt an encoding approach similar to how phrases are encoded, using the embedding model. These two types of coding are then integrated in specific manner within the KG. Unlike the document ensemble in HippoRAG, which simply aggregates scores from graph search and embedding matching, we enhance the KG by introducing passage nodes, enabling more seamless integration of contextual information. This approach retains the same offline indexing process as HippoRAG while enriching the graph structure with additional nodes and edges related to passages during construction. Specifically, each passage in the corpus is treated as passage node, with the context edge labeled contains connecting the passage to all phrases derived from this passage. 3.3. Deeper Contextualization Building upon the discussion of the concept-context tradeoff, we observe that query parsing in HippoRAG, which relies on Named Entity Recognition (NER), is predominantly concept-centric, often overlooking the contextual alignment within the KG. This entity-focused approach to extraction and indexing introduces strong bias toward concepts, leaving many contextual signals underutilized (Gutierrez et al., 2024). To address this limitation, we explore and evaluate different methods for linking queries to the KG, aiming to more effectively align query semantics with the starting nodes of graph searches. Specifically, we consider three approaches: 1) NER to Node: This is the original method used in HippoRAG, where entities are extracted from the query and subsequently matched with nodes in the KG using text embeddings. 2) Query to Node: Instead of extracting individual entities, we leverage text embeddings to match the entire query directly to nodes in the KG. 3) Query to Triple: To incorporate richer contextual information from the KG, we match the entire query to triples within the graph using text embeddings. Since triples encapsulate fundamental contextual relationships among concepts, this method provides more comprehensive understanding of the querys intent. By default, HippoRAG 2 adopts the query-to-triple approach, and we evaluate all three methods later (6.1). 3.4. Recognition Memory Recall and recognition are two complementary processes in human memory retrieval (Uner & Roediger III, 2022). Recall involves actively retrieving information without external cues, while recognition relies on identifying information with the help of external stimuli. Inspired by this, we model the query-to-triple retrieval as two-step process. 1) Query to Triple: We use the embedding model to retrieve the top-k triples of the graph as described in 3.3. 2) Triple Filtering: We use LLMs to filter retrieved and generate triples . The detailed prompts are shown in Appendix A. 3.5. Online Retrieval We summarize the online retrieval process in HippoRAG 2 after introducing the above improvements. The task involves selecting seed nodes and assigning reset probabilities for retrieval. HippoRAG 2 identifies phrase nodes from filtered triples generated by query-to-triple and recognition memory. If no triples are available, it directly retrieves top-ranked passages using the embedding model. Otherwise, up to phrase nodes are selected based on their average ranking scores across filtered triples they originate. All passage nodes are also taken as seed nodes, as broader activation improves multi-hop reasoning. Reset probabilities are assigned based on ranking scores for phrase nodes, while passage nodes receive scores proportional to their embedding similarity, adjusted by weight factor (6.2) to balance the influence between phrase nodes and passage nodes. The PPR search is then executed, and passages are ranked by their PageRank scores, with the top-ranked passages used for downstream QA. An example of the pipeline is in Appendix and the PPR initialization is detailed in Appendix G.1, 4. Experimental Setup 4.1. Baselines We select three types of comparison methods: 1) The classic retrievers BM25 (Robertson & Walker, 1994), Contriever (Izacard et al., 2022) and GTR (Ni et al., 2022). 2) Large embedding models that perform well on the BEIR leaderboard (Thakur et al., 2021), including Alibaba-NLP/GTEQwen2-7B-Instruct (Li et al., 2023), GritLM/GritLM7B (Muennighoff et al., 2024), and nvidia/NV-Embedv2 (Lee et al., 2025). 3) Structure-augmented RAG methods, including RAPTOR (Sarthi et al., 2024), GraphRAG (Edge et al., 2024), LightRAG (Guo et al., 2024), and HippoRAG (Gutierrez et al., 2024). 4.2. Datasets To evaluate how well RAG systems retain factual memory while enhancing associativity and sense-making, we select datasets that correspond to three critical challenge types: 1) Simple QA primarily evaluates the ability to recall and retrieve factual knowledge accurately. 2) Multi-hop QA measures associativity by requiring the model to connect multiple pieces of information to derive an answer. 3) Discourse understanding evaluates sense-making by testing the 5 From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Table 2. QA performance (F1 scores) on RAG benchmarks using Llama-3.3-70B-Instruct as the QA reader. No retrieval means evaluating the parametric knowledge of the readers. HippoRAG (and HippoRAG 2) uses Llama-3.3-70B-Instruct as the extractor (and the triple filter) and NV-Embed-v2 as the retriever. This table, along with the following ones, highlight the best and second-best results. Retrieval NQ PopQA MuSiQue 2Wiki HotpotQA LV-Eval NarrativeQA Avg Simple Baselines Simple QA Multi-Hop QA Discourse Understanding None Contriever (Izacard et al., 2022) BM25 (Robertson & Walker, 1994) GTR (T5-base) (Ni et al., 2022) GTE-Qwen2-7B-Instruct (Li et al., 2023) GritLM-7B (Muennighoff et al., 2024) NV-Embed-v2 (7B) (Lee et al., 2025) RAPTOR (Sarthi et al., 2024) GraphRAG (Edge et al., 2024) LightRAG (Guo et al., 2024) HippoRAG (Gutierrez et al., 2024) 54.9 58.9 59.0 59.9 62.0 61.3 61.9 50.7 46.9 16.6 55.3 42.8 41.9 51.2 52.8 26.1 31.3 28.8 34. 32.5 53.1 49.9 56.2 Large Embedding Models 60.0 56.3 60.6 55.8 55.7 61.5 Structure-Augmented RAG 56.2 52.1 58.6 48.1 11.6 2.4 71.8 55.9 28.9 38.5 1.6 35.1 40.9 44.8 45.7 47.3 62.3 63.4 62.8 71.0 73.3 75.3 69.5 68.6 2.4 63. HippoRAG 2 63.3 56.2 48.6 71.0 75. 6.0 8.1 5.9 7.1 7.1 9.8 9.8 5.0 11.2 1.0 8.4 12.9 12.9 19.7 18.3 19.9 21.3 23.9 25. 21.4 23.0 3.7 16.3 38.4 46.9 47.7 50.4 54.9 56.1 57.0 48.8 49.6 6.6 53.1 25.9 59. Table 3. Retrieval performance (passage recall@5) on RAG benchmarks. * denotes the report from the original paper. The compared structure-augmented RAG methods are reproduced with the same LLM and retriever as ours for fair comparison. GraphRAG and LightRAG are not presented because they do not directly produce passage retrieval results. Simple QA Multi-Hop QA Retrieval NQ PopQA MuSiQue 2Wiki HotpotQA Avg BM25 (Robertson & Walker, 1994) Contriever (Izacard et al., 2022) GTR (T5-base) (Ni et al., 2022) Simple Baselines 56.1 54.6 63.4 35.7 43.2 49.4 Large Embedding Models GTE-Qwen2-7B-Instruct (Li et al., 2023) GritLM-7B (Muennighoff et al., 2024) NV-Embed-v2 (7B) (Lee et al., 2025) 74.3 76.6 75.4 50.6 50.1 51.0 43.5 46.6 49.1 63.6 65.9 69.7 RAPTOR (Sarthi et al., 2024) HippoRAG* (Gutierrez et al., 2024) HippoRAG (reproduced) HippoRAG 2 Structure-Augmented RAG 68.3 44.4 78.0 48.7 53.8 51. 57.8 51.9 53.2 74.7 65.3 57.5 67.9 74.8 76.0 76.5 66.2 89.1 90.4 90. 74.8 75.3 73.9 89.1 92.4 94.5 86.9 77.7 77.3 96.3 55.1 55.4 60.7 70.5 72.2 73. 65.6 63.8 78.2 capability to interpret and reason over lengthy, complex narratives. The statistics for our sampled dataset are summarized in Table 1. Simple QA. This common type of QA task primarily involves questions centered around individual entities, making it particularly well-suited for embedding models to retrieve relevant contextual information intuitively. We randomly collect 1, 000 queries from the NaturalQuestions (NQ) dataset (collected by Wang et al. (2024)), which contains real user questions with wide range of topics. Additionally, we select 1, 000 queries from PopQA (Mallen et al., 2023), with the corpus derived from the December 2021 Wikipedia dump.1 Both datasets offer straightforward QA pairs, enabling evaluation of single-hop QA capabilities in RAG systems. Notably, PopQA from Wikipedia is especially entity-centric, with entities being less frequent than NaturalQuestions, making it an excellent resource for evaluating entity recognition and retrieval in simple QA tasks. Multi-hop QA. We randomly collect 1, 000 queries from MuSiQue, 2WikiMultihopQA, and HotpotQA following HippoRAG (Gutierrez et al., 2024), all requiring multi-passage reasoning. Additionally, we include all 124 queries from LV-Eval (hotpotwikiqa-mixup 256k) (Yuan 1https://github.com/facebookresearch/ atlas?tab=readme-ov-file#corpora 6 From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Table 4. Ablations: passage recall@5 on multi-hop benchmarks. Table 6. Passage recall@5 on MuSiQue subset. HippoRAG 2 supports different dense retrievers. MuSiQue 2Wiki HotpotQA Avg HippoRAG 2 w/ NER to node w/ Query to node w/o Passage Node w/o Filter 74.7 53.8 44.9 63.7 73.0 90. 91.2 65.5 90.3 90.7 96.3 87.1 Retriever Dense Retrieval HippoRAG 2 78.8 68.3 88.9 95.4 74.6 59.6 81. 86.4 GTE-Qwen2-7B-Instruct GritLM-7B NV-Embed-v2 (7B) 63.6 66.0 69.7 68.8 71.6 74.7 Table 5. Passage recall@5 with different weight factors for passage nodes on our MuSiQue dev set and NaturalQuestions (NQ) dev set, where each set has 1, 000 queries. is shown in Appendix A. We use top-5 triples ranked by retriever for filtering. For hyperparameters, we follow the default settings from HippoRAG. More implementation and hyperparameter details can be found in Appendix G. Weight MuSiQue NQ 0.01 79.9 75.6 0.05 80.5 76. 0.1 79.8 76.9 0.3 78.4 76.7 0.5 77.9 76. et al., 2024), challenging dataset designed to minimize knowledge leakage and reduce overfitting through keyword and phrase replacements. Thus, unlike Wikipedia-based datasets, LV-Eval better evaluates the models ability to synthesize knowledge from different sources effectively. For corpus collection, we segment long-form contexts of LVEval into shorter passages while maintaining the same RAG setup as other multi-hop datasets. Discourse Understanding. This category consists of only NarrativeQA, QA dataset that contains questions requiring cohesive understanding of full-length novel. This datasets focus on large-scale discourse understanding allows us to leverage it in our evaluation of sense-making in our chosen baselines and our own method. We randomly select 10 lengthy documents and their corresponding 293 queries from NarrativeQA and collect retrieval corpus just as in the above LV-Eval dataset. 4.3. Metrics Following HippoRAG (Gutierrez et al., 2024), we use passage recall@5 to evaluate the retrieval task. For the QA task, we follow evaluation metrics from MuSiQue (Trivedi et al., 2022) to calculate F1 scores for the final answer. 4.4. Implementation Details For HippoRAG 2, we use the open-source Llama-3.3-70BInstruct (AI@Meta, 2024) as both the extraction (NER and OpenIE) and triple filtering model, and we use nvidia/NVEmbed-v2 as the retriever. We also reproduce the compared structure-augmented RAG methods using the same extractor and retriever for fair comparison. For the triple filter, we use DSPy (Khattab et al., 2024) MIPROv2 optimizer and Llama-3.3-70B-Instruct to tune the prompt, including the instructions and demonstrations. The resulting prompt 5. Results We now present our main QA and retrieval experimental results, where the QA process uses retrieved results as its context. More detailed experimental results are presented in Appendix C. The statistics for all constructed KGs are shown in Appendix A. QA Performance. Table 2 presents the QA performance of various retrievers across multiple RAG benchmarks using Llama-3.3-70B-Instruct as the QA reader. HippoRAG 2 achieves the highest average F1 score, demonstrating robustness across different settings. Large embedding models outperform smaller ones, with NV-Embed-v2 (7B) scoring 6.6% higher on average than GTR (T5-base). These models also surpass structure-augmented RAG methods with lower computational costs but excel mainly in simple QA while struggling in complex cases. otably, HippoRAG 2 outperforms NV-Embed-v2 by 9.5% F1 on 2Wiki and by 3.1% on the challenging LV-Eval dataset. Compared to HippoRAG, HippoRAG 2 shows even greater improvements, validating its neuropsychology-inspired approach. These results highlight HippoRAG 2 as state-of-the-art RAG system that enhances both retrieval and QA performance while being effectively powered by an open-source model. Table 8 in Appendix presents additional QA results (EM and F1) using Llama or GPT-4o-mini as the QA reader, along with an extractor or triple filter. GPT-4o-mini follows Llamas trend, with NV-Embed-v2 outperforming structureaugmented methods in most cases, except for HippoRAG in multi-hop QA. HippoRAG 2 consistently outperforms all other methods across nearly all settings. Retrieval Performance. We report retrieval results for datasets with supporting passage annotations and models that explicitly retrieve passages in Table 3. Large embedding models (7B) significantly outperform classic smaller LM-based models like Contriever and GTR, achieving at least 9.8% higher F1 score. While our reproduction of HippoRAG using Llama-3.3-70B-Instruct and NV-Embed-v2 shows slight improvements over the original paper, the gains 7 From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Table 7. We show exemplary retrieval results (the title of passages) from HippoRAG 2 and NV-Embed-v2 on different types of questions. Bolded items denote the titles of supporting passages. Question NV-Embed-v2 Results HippoRAG 2 Filtered Triples HippoRAG 2 Results Simple QA In what city was I.P. Paul born? 1. I. P. Paul 2. Yinka Ayefele - Early life 3. Paul Parker (singer) (I. P. Paul, from, Thrissur) (I. P. Paul, was mayor of, Thrissur municipal corporation) 1. I. P. Paul 2. Thrissur 3. Yinka Ayefele Multi-Hop QA What county is Erik Horts birthplace part of? 1. Erik Hort 2. Horton Park (Saint Paul, Minnesota) 3. Hertfordshire (Erik Hort, born in, Montebello) (Erik Hort, born in, New York) 1. Erik Hort 2. Horton Park (Saint Paul, Minnesota) 3. Monstebello, New York are minimal, with only 1.3% increase in F1. Although HippoRAG excels in entity-centric retrieval, achieving the highest recall@5 on PopQA, it generally lags behind recent dense retrievers and HippoRAG 2. Notably, HippoRAG 2 achieves the highest recall scores across most datasets, with substantial improvements of 5.0% and 13.9% in Recall@5 on MuSiQue and 2Wiki, respectively, compared to the strongest dense retriever, NV-Embed-v2. Additionally, the cost and efficiency analysis is presented in Appendix F. 6. Discussions 6.1. Ablation Study We design ablation experiments for the proposed linking method, graph construction method, and triple filtering method, with the results reported in Table 4. Each introduced mechanism boosts HippoRAG 2. First, the linking method with deeper contextualization leads to significant performance improvements. Notably, we do not apply filtering process to the NER-to-node or query-to-node methods; however, the query-to-triple approach, regardless of whether filtering is applied, consistently outperforms the other two linking strategies. On average, query-to-triple improves Recall@5 by 12.5% compared to NER-to-node. Moreover, query-to-node does not provide an advantage over NER-to-node, as queries and KG nodes operate at different levels of granularity, whereas both NER results and KG nodes correspond to phrase-level representations. 6.2. Controlling Reset Probabilities When setting the reset probability before starting PPR, we find that it is necessary to balance the reset probabilities between two types of nodes: phrase nodes and passage nodes. Specifically, the reset probability of all passage nodes is multiplied by weight factor to balance the importance of two types of nodes during PPR. Here, we present the results obtained on the validation set in Table 5, which shows that this factor is crucial for the PPR results. Considering the model performance across different scenarios, we set the factor to be 0.05 by default. 6.3. Dense Retriever Flexibility The dense retriever employed by HippoRAG 2 is fully plugand-play, offering seamless integration. As demonstrated in Table 6, HippoRAG 2 consistently surpasses direct dense retrieval across various retrievers. Notably, these performance gains remain robust regardless of the specific dense retriever used. 6.4. Qualitative Analysis We show examples from PopQA and MuSiQue in Table 7. For the first example, In what city was I. P. Paul born?, NV-Embed-v2 ranks the entity mentioned in the query I. P. Paul as the top 1, where the passage is enough to answer this question. But HippoRAG 2 does even better. It directly finds the answer Thrissur when linking the triples, and during the subsequent graph search, it places the passage corresponding to that entity in the second position, which is perfect retrieval result. For the second multi-hop question, What county is Erik Horts birthplace part of? NV-Embed-v2 also easily identifies the person mentioned, Erik Hort. However, since this question requires two-step reasoning, it is not sufficient to fully answer the question. In contrast, HippoRAG 2 retrieves passage titled Montebello during the query-to-triple step, which contains geographic information that implies the answer to the question. In the subsequent graph search, this passage is also ranked at the top. Apart from this, the error analysis of HippoRAG 2 is detailed in Appendix E. 7. Conclusion We introduced HippoRAG 2, novel framework designed to address the limitations of existing RAG systems in approximating the dynamic and interconnected nature of human long-term memory. It combining the strengths of the Personalized PageRank algorithm, deeper passage integration, and effective online use of LLMs. HippoRAG 2 opens new avenues for research in continual learning and long-term memory for LLMs by achieving comprehensive improvements over standard RAG methods across factual, sensemaking, and associative memory tasks, showing capabilities 8 From RAG to Memory: Non-Parametric Continual Learning for Large Language Models that previous methods have either overlooked or been incapable of achieving in thorough evaluation. Future work could consider leveraging graph-based retrieval methods to further enhance the episodic memory capabilities of LLMs in long conversations."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work on Retrieval-Augmented Generation (RAG) to advance the field of long-term memory for large language models. While our work may have various societal implications, we do not identify any concerns that warrant specific emphasis beyond those generally associated with large language models and information retrieval systems."
        },
        {
            "title": "Acknowledgments",
            "content": "We would also like to extend our appreciation to colleagues from the OSU NLP group for their constructive comments. This work is supported in part by ARL W911NF2220144, NSF 2112606, and gift from Cisco. We also thank the Ohio Supercomputer Center for providing computational resources. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notice herein."
        },
        {
            "title": "References",
            "content": "AI@Meta. Llama 3 model card. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md. 2024. Beyeler, M., Rounds, E. L., Carlson, K. D., Dutt, N., and Krichmar, J. L. Neural correlates of sparse coding and dimensionality reduction. PLoS Comput Biol, 15(6): e1006908, 2019. doi: 10.1371/journal.pcbi.1006908. Chen, H., Pasunuru, R., Weston, J., and Celikyilmaz, A. Walking down the memory maze: Beyond context limit through interactive reading, 2023. URL https: //arxiv.org/abs/2310.05029. Cohen, R., Biran, E., Yoran, O., Globerson, A., and Geva, M. Evaluating the ripple effects of knowledge editing in language models. Transactions of the Association for Computational Linguistics, 12:283298, 2024. doi: 10. 1162/tacl 00644. URL https://aclanthology. org/2024.tacl-1.16/. rag approach to query-focused summarization, 2024. URL https://arxiv.org/abs/2404.16130. Gu, J.-C., Xu, H.-X., Ma, J.-Y., Lu, P., Ling, Z.-H., Chang, K.-W., and Peng, N. Model editing harms general abilities of large language models: Regularization to the rescue. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1680116819, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 934. URL https://aclanthology.org/2024. emnlp-main.934/. Guo, Z., Xia, L., Yu, Y., Ao, T., and Huang, C. LightRAG: Simple and fast retrieval-augmented generation, 2024. URL https://arxiv.org/abs/2410.05779. Gutierrez, B. J., Shu, Y., Gu, Y., Yasunaga, M., and Su, Y. Hipporag: Neurobiologically inspired long-term memory for large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=hkujvAPVsg. Haveliwala, T. H. Topic-sensitive pagerank. In Lassner, D., Roure, D. D., and Iyengar, A. (eds.), Proceedings of the Eleventh International World Wide Web Conference, WWW 2002, May 7-11, 2002, Honolulu, Hawaii, USA, pp. 517526. ACM, 2002. doi: 10.1145/ 511446.511513. URL https://dl.acm.org/doi/ 10.1145/511446.511513. Hoelscher-Obermaier, J., Persson, J., Kran, E., Konstas, I., and Barez, F. Detecting edit failures in large language models: An improved specificity benchIn Rogers, A., Boyd-Graber, J., and Okazaki, mark. N. (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1154811559, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl. 733. URL https://aclanthology.org/2023. findings-acl.733/. Huang, J., Cui, L., Wang, A., Yang, C., Liao, X., Song, L., Yao, J., and Su, J. Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14161428, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.77. URL https: //aclanthology.org/2024.acl-long.77/. Edge, D., Trinh, H., Cheng, N., Bradley, J., Chao, A., Mody, A., Truitt, S., and Larson, J. From local to global: graph Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised From RAG to Memory: Non-Parametric Continual Learning for Large Language Models dense information retrieval with contrastive learning. Trans. Mach. Learn. Res., 2022, 2022. URL https: //openreview.net/forum?id=jKN1pXi7b0. Jin, X., Zhang, D., Zhu, H., Xiao, W., Li, S.-W., Wei, X., Arnold, A., and Ren, X. Lifelong pretraining: Continually adapting language models to emerging corpora. In Carpuat, M., de Marneffe, M.-C., and Meza Ruiz, I. V. (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 47644780, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. naacl-main.351. URL https://aclanthology. org/2022.naacl-main.351/. Khattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., Santhanam, K., Vardhamanan, S., Haq, S., Sharma, A., Joshi, T. T., Moazam, H., Miller, H., Zaharia, M., and Potts, C. DSPy: Compiling declarative language model calls into self-improving pipelines. 2024. Klein, G., Moon, B., and Hoffman, R. R. Making sense of sensemaking 1: Alternative perspectives. IEEE intelligent systems, 21(4):7073, 2006. Koli, V., Yuan, J., and Dasgupta, A. Sensemaking of socially-mediated crisis information. In Blodgett, S. L., Cercas Curry, A., Dev, S., Madaio, M., Nenkova, A., Yang, D., and Xiao, Z. (eds.), Proceedings of the Third Workshop on Bridging HumanComputer Interaction and Natural Language Processing, pp. 74 81, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. hcinlp-1.7. URL https://aclanthology.org/ 2024.hcinlp-1.7/. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. NV-Embed: Improved techniques for training llms as generalist embedding models, 2025. URL https://arxiv.org/abs/2405.17428. Li, Z., Zhang, X., Zhang, Y., Long, D., Xie, P., and Zhang, M. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. L`u, X. H. BM25S: Orders of magnitude faster lexical search via eager sparse scoring, 2024. URL https://arxiv. org/abs/2407.03618. Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., and Hajishirzi, H. When not to trust language models: Investigating effectiveness of parametric and nonparametric memories. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 98029822, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.546. URL https: //aclanthology.org/2023.acl-long.546/. Muennighoff, N., Su, H., Wang, L., Yang, N., Wei, F., Yu, T., Singh, A., and Kiela, D. Generative representational instruction tuning. CoRR, abs/2402.09906, 2024. doi: 10.48550/ARXIV.2402.09906. URL https://doi. org/10.48550/arXiv.2402.09906. Ni, J., Qu, C., Lu, J., Dai, Z., Hernandez Abrego, G., Ma, J., Zhao, V., Luan, Y., Hall, K., Chang, M.-W., and Yang, Y. Large dual encoders are generalizable retrievers. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 98449855, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main. 669. URL https://aclanthology.org/2022. emnlp-main.669/. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E. Z., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An imperative style, high-performance deep learning library. In Wallach, H. M., Larochelle, H., Beygelzimer, A., dAlche-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024 URL https://proceedings. 8035, neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract. html. 2019. Robertson, S. E. and Walker, S. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Croft, W. B. and van Rijsbergen, C. J. (eds.), Proceedings of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval. Dublin, Ireland, 3-6 July 1994 (Special Issue of the SIGIR Forum), pp. 232241. ACM/Springer, 1994. doi: 10.1007/978-1-4471-2099-5 24. Sarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., 10 From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Yao, Y., Wang, P., Tian, B., Cheng, S., Li, Z., Deng, S., Chen, H., and Zhang, N. Editing large language models: Problems, methods, and opportunities. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1022210240, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 632. URL https://aclanthology.org/2023. emnlp-main.632/. Yuan, T., Ning, X., Zhou, D., Yang, Z., Li, S., Zhuang, M., Tan, Z., Yao, Z., Lin, D., Li, B., Dai, G., Yan, S., and Wang, Y. LV-Eval: balanced long-context benchmark with 5 length levels up to 256k, 2024. URL https: //arxiv.org/abs/2402.05136. Zhang, H., Gui, L., Zhai, Y., Wang, H., Lei, Y., and Xu, R. Copr: Continual learning human preference through optimal policy regularization, 2024. URL https:// arxiv.org/abs/2310.15694. CITB: benchmark for continual Zhang, Z., Fang, M., Chen, L., and Namazi-Rad, M.- instruction R. tuning. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 94439455, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp. 633. URL https://aclanthology.org/2023. findings-emnlp.633/. Zhong, Z., Wu, Z., Manning, C., Potts, C., and Chen, D. MQuAKE: Assessing knowledge editing in language models via multi-hop questions. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1568615702, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 971. URL https://aclanthology.org/2023. emnlp-main.971/. and Manning, C. D. RAPTOR: recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/ forum?id=GN921JHCRw. Shi, H., Xu, Z., Wang, H., Qin, W., Wang, W., Wang, Y., Wang, Z., Ebrahimi, S., and Wang, H. Continual learning of large language models: comprehensive survey. arXiv preprint arXiv:2404.16789, 2024. Suzuki, W. A. Associative learning and the hippocampus. Psychological Science Agenda, February 2005. Thakur, N., Reimers, N., Ruckle, A., Srivastava, A., and Gurevych, I. BEIR: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum? id=wCu6T5xFjeJ. Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. doi: 10. 1162/tacl 00475. URL https://aclanthology. org/2022.tacl-1.31/. Uner, O. and Roediger III, H. L. Do recall and recognition lead to different retrieval experiences? The American Journal of Psychology, 135(1):3343, 2022. Wang, Y., Ren, R., Li, J., Zhao, X., Liu, J., and Wen, J. REAR: relevance-aware retrieval-augmented frameIn Alwork for open-domain question answering. Onaizan, Y., Bansal, M., and Chen, Y. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 56135626. Association for Computational Linguistics, 2024. URL https:// aclanthology.org/2024.emnlp-main.321. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., and Brew, J. Huggingfaces transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. URL http://arxiv.org/ abs/1910.03771. Xie, J., Zhang, K., Chen, J., Lou, R., and Su, Y. Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=auKAUJZMO6. 11 From RAG to Memory: Non-Parametric Continual Learning for Large Language Models"
        },
        {
            "title": "Appendices",
            "content": "Within this supplementary material, we elaborate on the following aspects: Appendix A: LLM Prompts Appendix B: HippoRAG 2 Pipeline Example Appendix C: Detailed Experimental Results Appendix D: Graph Statistics Appendix E: Error Analysis Appendix F: Cost and Efficiency Appendix G: Implementation Details and Hyperparameters A. LLM Prompts We show LLM prompts for triple filter in Figure 3, including the instruction, the few-shot demonstrations and the input format. B. Pipeline Example We show pipeline example of HippoRAG 2 online retrieval in Figure 4, including query-to-triple, triple filtering and using seed nodes for PPR. C. Detailed Experimental Results We show QA performance and retrieval performance with the proprietary model GPT-4o-mini as well as more metrics here, as shown in Table 8 and Table 9. QA Performance As shown in Table 8, when using GPT-4o-mini for indexing and QA reading, HippoRAG 2 consistently achieves competitive EM and F1 scores across most datasets. Notably, it leads in the MuSiQue and 2Wiki benchmarks. Our method also demonstrates superior performance in the NarrativeQA and LV-Eval tasks. When compared to the strong NV-Embed-v2 retriever, HippoRAG 2 exhibits comparable or enhanced F1 scores, particularly excelling in the LV-Eval dataset with reduced knowledge leakage. Retrieval Performance As shown in Table 9, the improvement trend of HippoRAG 2 in recall@2 is similar to that in recall@5. D. Graph Statistics We show the knowledge graph statistics using Llama-3.3-70B-Instruct or GPT-4o-mini for OpenIE in Table 10. E. Error Analysis We provide an error analysis of 100 samples generated by HippoRAG 2 with recall@5 less than 1.0. Among these samples, 26%, 41%, and 33% are classified as 2-hop, 3-hop, and 4-hop questions, respectively. Triple filtering and the graph search algorithm are the two main sources of errors. Recognition Memory In 7% of the samples, no phrase from the supporting documents is matched with the phrases obtained by the query-to-triple stage before triple filtering. In 26% of the samples, no phrase from the supporting documents is matched with the phrases after triple filtering. After the triple filtering step, 8% of the samples show decrease in the proportion of phrases in the triples that match phrases from the supporting passages. For instance, the first case from Table 11 shows an empty list after triple filtering, which eliminates all relevant phrases. Additionally, 18% of the samples are left with zero triples after filtering. Although not necessarily an error in filtering, this indicates that the attempt to link to 12 From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Figure 3. LLM prompts for triple filtering (recognition memory). From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Figure 4. An example of HippoRAG 2 pipeline. 14 From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Table 8. QA performance (EM / F1 scores) on RAG benchmarks. No retrieval means evaluating the parametric knowledge of the readers. HippoRAG (and HippoRAG 2) uses the denoted LLM for OpenIE (triple filtering) and QA reading. Simple QA Multi-Hop QA Discourse Understanding Retrieval NQ PopQA MuSiQue 2Wiki HotpotQA LV-Eval NarrativeQA Avg Llama-3.3-70B-Instruct None Contriever (Izacard et al., 2022) BM25 (Robertson & Walker, 1994) GTR (T5-base) (Ni et al., 2022) GTE-Qwen2-7B-Instruct (Li et al., 2023) GritLM-7B (Muennighoff et al., 2024) NV-Embed-v2 (7B) (Lee et al., 2025) RAPTOR (Sarthi et al., 2024) GraphRAG (Edge et al., 2024) LightRAG (Guo et al., 2024) HippoRAG (Gutierrez et al., 2024) HippoRAG 2 40.2 / 54.9 45.0 / 58.9 44.7 / 59.0 45.5 / 59.9 46.6 / 62.0 46.8 / 61.3 47.3 / 61.9 36.9 / 50.7 30.8 / 46.9 8.6 / 16.6 43.0 / 55.3 48.6 / 63.3 28.2 / 32.5 41.6 / 53.1 39.1 / 49.9 43.2 / 56.2 43.5 / 56.3 42.8 / 55.8 42.9 / 55.7 43.1 / 56.2 31.4 / 48.1 2.1 / 2.4 42.7 / 55.9 42.9 / 56.2 17.6 / 26.1 24.0 / 31.3 20.3 / 28.8 25.8 / 34.6 30.6 / 40.9 33.6 / 44.8 34.7 / 45.7 20.7 / 28.9 27.3 / 38.5 0.5 / 1.6 26.2 / 35.1 37.2 / 48.6 GPT-4o-mini 36.5 / 42.8 38.1 / 41.9 47.9 / 51.2 49.2 / 52.8 55.1 / 60.0 55.8 / 60.6 57.5 / 61.5 47.3 / 52.1 51.4 / 58.6 9.4 / 11.6 65.0 / 71.8 65.0 / 71. 37.0 / 47.3 51.3 / 62.3 52.0 / 63.4 50.6 / 62.8 58.6 / 71.0 60.7 / 73.3 62.8 / 75.3 56.8 / 69.5 55.2 / 68.6 2.0 / 2.4 52.6 / 63.5 62.7 / 75.5 4.0 / 6.0 5.7 / 8.1 4.0 / 5.9 4.8 / 7.1 5.7 / 7.1 7.3 / 9.8 7.3 / 9.8 2.4 / 5.0 4.8 / 11.2 0.8 / 1.0 6.5 / 8.4 9.7 / 12.9 None NV-Embed-v2 (7B) (Lee et al., 2025) RAPTOR (Sarthi et al., 2024) GraphRAG (Edge et al., 2024) LightRAG (Guo et al., 2024) HippoRAG (Gutierrez et al., 2024) HippoRAG 2 35.2 / 52.7 43.5 / 59.9 37.8 / 54.5 38.0 / 55.5 2.8 / 15.4 37.2 / 52.2 43.4 / 60.0 16.1 / 22.7 41.7 / 55.8 41.9 / 55.1 30.7 / 51.3 1.9 / 14.8 42.5 / 56.2 41.7 / 55.7 11.2 / 22.0 32.8 / 46.0 27.7 / 39.2 27.0 / 42.0 2.0 / 9.3 24.0 / 35.9 35.0 / 49. 30.2 / 36.3 54.4 / 60.8 39.7 / 48.4 45.7 / 61.0 2.5 / 12.1 59.4 / 67.3 60.5 / 69.7 28.6 / 41.0 57.3 / 71.0 50.6 / 64.7 51.4 / 67.6 9.9 / 20.2 46.3 / 60.0 56.3 / 71.1 3.2 / 5.0 7.3 / 10.0 5.6 / 9.2 4.9 / 11.0 0.9 / 5.0 4.8 / 7.6 10.5 / 14.0 3.4 / 12.9 6.5 / 19.7 4.4 / 18.3 6.8 / 19.9 7.9 / 21.3 8.2 / 23.9 8.9 / 25.7 5.1 / 21.4 6.8 / 23.0 1.0 / 3.7 4.4 / 16.3 8.9 / 25.9 2.7 / 14.1 5.1 / 24.2 4.1 / 21.8 5.4 / 20.9 1.0 / 9.0 2.1 / 16.1 5.8 / 25.2 29.7 / 38.4 37.4 / 46.9 38.0 / 47.7 40.0 / 50.4 43.8 / 54.9 44.9 / 56.1 45.9 / 57.0 38.1 / 48.8 36.7 / 49.6 4.2 / 6.6 42.8 / 53.1 48.0 / 59. 22.6 / 33.1 42.9 / 55.7 36.9 / 49.7 36.0 / 52.6 3.6 / 13.9 38.9 / 51.2 44.3 / 58.1 the triples has failed, where HippoRAG 2 directly uses the results from dense retrieval as substitute. Overall, though recognition memory is an essential component, the precision of the triple filter has room for further improvement. Graph Construction Graph construction is challenging to evaluate, but we find that only 2% of the samples do not contain any phrases from the supporting passages within the one-hop neighbors of the linked nodes. Given our dense-sparse integration, we can assume that the graphs we construct generally include most of the potentially exploitable information. Personalized PageRank In 50% of the samples, at least half of the linked phrase nodes appear in the supporting documents. However, the final results remain unsatisfactory due to the graph search component. For example, in the second case from Table 11, the recognition memory identifies the key phrase Philippe, Duke of Orleans from the query, but the graph search fails to return perfect results among the top-5 retrieved passages. F. Cost and Efficiency For LLM deployment, we run Llama-3.3-70B-Instruct on machine equipped with four NVIDIA H100 GPUs, utilizing tensor parallelism via vLLM (Kwon et al., 2023). We also employ the gpt-4o-mini-2024-07-18 model from OpenAIs official endpoint, leveraging its batch API2. For offline indexing, we execute NER and Open IE on the MuSiQue corpus (11, 656 passages). Processing each passage takes approximately 1.1 seconds using Llama-3.3-70B-Instruct, while utilizing the gpt-4o-mini batch API allows indexing to complete within 24 hours at cost of under $2 USD. Comparison With Structure-Augmented RAG Methods We count the token usage across different structure-augmented RAG methods when indexing the MuSiQue corpus using the Llama-3.3-70B-Instruct model, and we compare the number of input and output tokens against RAPTOR (Sarthi et al., 2024), LightRAG (Guo et al., 2024), and GraphRAG (Edge et al., 2024) in Table 12. HippoRAG 2 not only outperforms these RAG methods in QA and retrieval performance but also uses much fewer tokens compared to LightRAG and GraphRAG. G. Implementation Details and Hyperparameters G.1. HippoRAG 2 We provide detailed explanation of the PPR initialization process used in HippoRAG 2 here. The key goal is to determine the seed nodes for the PPR search and assign appropriate reset probabilities to ensure an effective retrieval process. 2https://platform.openai.com/docs/guides/batch From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Table 9. Passage recall@2 / @5 on RAG benchmarks. * denotes the report from the original paper while we reproduce the HippoRAG results with aligned LLM and retriever. Simple Multi-hop NQ PopQA MuSiQue 2Wiki HotpotQA Avg Contriever (Izacard et al., 2022) BM25 (Robertson & Walker, 1994) GTR (T5-base) (Ni et al., 2022) GTE-Qwen2-7B-Instruct (Li et al., 2023) GritLM-7B (Muennighoff et al., 2024) NV-Embed-v2 (7B) (Lee et al., 2025) RAPTOR (GPT-4o-mini) RAPTOR (Llama-3.3-70B-Instruct) HippoRAG* (Gutierrez et al., 2024) HippoRAG (GPT-4o-mini) HippoRAG (Llama-3.3-70B-Instruct) 34.8 / 46.6 32.4 / 43.5 37.4 / 49.1 48.1 / 63.6 49.7 / 65.9 52.7 / 69.7 Simple Baselines 27.0 / 43.2 24.0 / 35.7 40.1 / 49.4 Large Embedding Models 47.7 / 50.6 44.0 / 50.1 45.3 / 51.0 Structure-augmented RAG 37.2 / 48.1 40.2 / 48.7 36.5 / 52.2 40.0 / 53.8 49.1 / 61.0 47.0 / 57.8 40.9 / 51.9 41.8 / 52.4 41.2 / 53.2 29.1 / 54.6 28.2 / 56.1 35.0 / 63. 44.7 / 74.3 46.2 / 76.6 45.3 / 75.4 40.5 / 69.4 40.3 / 68.3 21.6 / 45.1 21.3 / 44.4 46.6 / 57.5 55.3 / 65.3 60.2 / 67.9 58.4 / 75.3 57.3 / 74.8 59.3 / 73.9 39.2 / 55.4 39.4 / 55.1 46.4 / 60.7 66.7 / 74.8 67.3 / 76.0 67.1 / 76. 75.8 / 89.1 79.2 / 92.4 84.1 / 94.5 56.6 / 70.5 57.3 / 72.2 58.9 / 73.4 58.4 / 66.0 58.3 / 66.2 70.7 / 89.1 68.4 / 87.0 71.9 / 90.4 78.6 / 90.2 76.8 / 86.9 60.5 / 77.7 60.1 / 78.5 60.4 / 77.3 52.8 / 67.0 52.5 / 65.6 45.7 / 63.0 47.0 / 63.8 HippoRAG 2 (GPT-4o-mini) HippoRAG 2 (Llama-3.3-70B-Instruct) 44.4 / 76.4 45.6 / 78.0 43.5 / 52.2 43.9 / 51.7 53.5 / 74.2 56.1 / 74.7 74.6 / 90.2 76.2 / 90.4 80.5 / 95.7 83.5 / 96.3 59.3 / 77.7 61.1 / 78. Seed Node Selection The seed nodes for the PPR search are categorized into two types: phrase nodes and passage nodes. All the scores given by the embedding model below use normalized embedding to calculate. 1) Phrase Nodes: These seed nodes are selected from the phrase nodes within the filtered triples, which are obtained through the recognition memory component. If recognition memory gives an empty triple list and no phrase node is available, HippoRAG 2 directly returns top passages using the embedding model without any graph search. Otherwise, we keep at most 5 phrase nodes as the seed nodes, and the ranking score of each phrase node is computed as the average score of all filtered triples it appears in. 2) Passage Nodes: Each passage node is initially scored using an embedding-based similarity, and these scores are processed as follows. All passage nodes are taken as seed nodes since we find that activating broader set of potential passages is more effective for uncovering passages along multi-hop reasoning chains compared to focusing only on the top-ranked passages. Reset Probability Assignment After determining the seed nodes, we assign reset probabilities to control how likely the PPR algorithm will return to these nodes during the random walk. The rules are: 1) Phrase nodes receive reset probabilities directly as their ranking scores. 2) Passage nodes receive reset probabilities proportional to their embedding similarity scores, i.e., to balance the influence of phrase nodes and passage nodes, we apply weight factor to the passage node scores. Specifically, the passage node scores are multiplied by the weight factor discussed in Section 6.2. This ensures that passage nodes and phrase nodes contribute appropriately to the retrieval process. PPR Execution and Passage Ranking Once the seed nodes and their reset probabilities are initialized, we run PPR over the constructed graph. The final ranking of passages is determined based on the PageRank scores of the passage nodes. Top-ranked passages are then used as inputs for the downstream QA reading process. We manage our KG and run the PPR algorithm using the python-igraph library.3 By incorporating both phrase nodes and passage nodes into the PPR initialization, our approach ensures more effective retrieval of relevant passages, especially for multi-hop reasoning tasks. Hyperparameters We perform hyperparameter tuning on 100 examples from MuSiQues training data. The hyperparameters are listed in Table 13. G.2. Comparison Methods We use PyTorch (Paszke et al., 2019) and HuggingFace (Wolf et al., 2019) for dense retrievers and BM25s (L`u, 2024) for the BM25 implementation. For GraphRAG (Edge et al., 2024) and LightRAG (Guo et al., 2024), we adhere to their 3https://python.igraph.org/en/stable/ 16 From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Table 10. Knowledge graph statistics using different LLMs for OpenIE. The nodes and triples are counted based on unique values. NQ PopQA MuSiQue 2Wiki HotpotQA LV-Eval NarrativeQA Llama-3.3-70B-Instruct # of phrase nodes # of passage nodes # of total nodes # of extracted edges # of synonym edges # of context edges # of total edges 68, 375 9, 633 78, 008 125, 777 899, 031 126, 757 1, 151, 565 76, 539 8, 676 85, 215 124, 579 845, 014 118, 909 1, 088, 502 85, 288 11, 656 96, 944 140, 830 1, 125, 951 132, 586 1, 399, 367 44, 004 6, 119 50, 123 68, 881 593, 298 64, 132 726, 311 81, 200 9, 811 91, 011 130, 058 994, 187 122, 437 1, 246, 175, 195 22, 849 198, 044 314, 324 2, 674, 833 375, 424 3, 364, 581 GPT-4o-mini # of phrase nodes # of passage nodes # of total nodes # of extracted edges # of synonym edges # of context edges # of total edges 86, 904 9, 633 96, 537 114, 900 1, 094, 651 142, 419 1, 351, 970 85, 744 8, 676 94, 420 108, 989 901, 528 127, 568 494, 082 101, 641 11, 656 113, 297 125, 903 1, 304, 605 146, 293 1, 576, 49, 544 6, 119 55, 663 62, 626 715, 763 68, 348 846, 737 95, 105 9, 811 104, 916 119, 630 1, 126, 501 133, 220 1, 379, 351 217, 085 22, 849 239, 934 303, 491 3, 268, 084 404, 210 3, 975, 785 9, 224 4, 111 13, 335 26, 208 72, 494 33, 395 132, 097 15, 365 4, 111 19, 476 24, 373 14, 075 38, 632 77, 080 Table 11. Two examples from MuSiQue where passage recall@5 is less than 1.0. Query Answer Where is the district that the person who wanted to reform and address Bernhard Lichtenbergs religion preached sermon on Marian devotion before his death located? Saxony-Anhalt Supporting Passages (Title) Retrieved Passages (Title) 1. Mary, mother of Jesus 2. Reformation 3. Wittenberg (district) 4. Bernhard Lichtenberg 1. Bernhard Lichtenberg 2. Mary, mother of Jesus 3. Ambroise-Marie Carre 4. Reformation 5. Henry Scott Holland (Recall@5 is 0.75) Query to Triple (Top-5) (Bernhard Lichtenberg, was, Roman Catholic Priest) (Bernhard Lichtenberg, beatified by, Catholic Church) (Bernhard Lichtenberg, died on, 5 November 1943) (Catholic Church, beatified, Bernhard Lichtenberg) (Bernhard Lichtenberg, was, Theologian) All above subjects and objects appear in supporting passages Filtered Triple Empty Query Answer Who is the grandmother of Philippe, Duke of Orleans? Marie de Medici Supporting Passages (Title) Retrieved Passages (Title) 1. Philippe I, Duke of Orleans 2. Leonora Dori 1. Philippe I, Duke of Orleans 2. Louise Elisabeth dOrleans 3. Philip III of Spain 4. Anna of Lorraine 5. Louis Philippe (Recall@5 is 0.5) Query to Triple (Top-5) Filtered Triple (Bank of America, purchased, Fleetboston Financial) (Fleetboston Financial, was acquired by, Bank of America) (Bank of America, acquired, Fleetboston Financial) (Bank of America, announced purchase of, Fleetboston Financial) (Bank of America, merged with, Fleetboston Financial) All above subjects and objects appear in supporting passages (Bank of America, purchased, Fleetboston Financial) (Fleetboston Financial, was acquired by, Bank of America) All above subjects and objects appear in supporting passages From RAG to Memory: Non-Parametric Continual Learning for Large Language Models Table 12. Token usage of different structure-augmented RAG methods for indexing the MuSiQue corpus (11, 656 passages) and their relative proportions. HippoRAG 2 RAPTOR LightRAG GraphRAG Input Tokens Output Tokens 9.2M (100.0%) 3.0M (100.0%) 1.7M (18.5%) 0.2M (6.7%) 68.5M (744.6%) 18.3M (610.0%) 115.5M (1255.4%) 36.1M (1203.3%) Table 13. Hyperparameters set on HippoRAG Hyperparameter Value Synonym Threshold Damping Factor of PPR Temperature 0.8 0.5 0.0 default hyperparameters and prompts. To ensure consistent evaluation, the same QA prompt that HippoRAG 2 adopts from HippoRAG (Gutierrez et al., 2024) is applied to rephrase the original response of GraphRAG and LightRAG. Hyperparameters We keep the default indexing hyperparameters for LightRAG and GraphRAG. For QA, we perform hyperparameter tuning on the same 100 samples as Appendix G.1. Table 14. Hyperparameters set on LightRAG and GraphRAG Hyperparameters LightRAG GraphRAG Mode Response Type Top-k Phrases for QA Chunk Token Size Chunk Overlap Token Size Community Report Max Length Max Input Length Max Cluster Size Entity Summary Max Tokens Local Short phrase 60 1, 200 100 2, 000 8, 000 10 Local Short phrase 60 1, 200"
        }
    ],
    "affiliations": [
        "The Ohio State University, Columbus, OH, USA",
        "University of Illinois Urbana-Champaign, IL, USA"
    ]
}