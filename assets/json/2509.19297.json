{
    "paper_title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction",
    "authors": [
        "Weijie Wang",
        "Yeqing Chen",
        "Zeyu Zhang",
        "Hengyu Liu",
        "Haoxiao Wang",
        "Zhiyuan Feng",
        "Wenkang Qin",
        "Zheng Zhu",
        "Donny Y. Chen",
        "Bohan Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat."
        },
        {
            "title": "Start",
            "content": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction Weijie Wang1,2, Yeqing Chen3, Zeyu Zhang2, Hengyu Liu2,4, Haoxiao Wang1, Zhiyuan Feng5, Wenkang Qin2, Zheng Zhu2, Donny Y. Chen6, Bohan Zhuang1 1Zhejiang University 2GigaAI 3University of Electronic Science and Technology of China 4The Chinese University of Hong Kong 5Tsinghua University 6Monash University 5 2 0 2 3 2 ] . [ 1 7 9 2 9 1 . 9 0 5 2 : r Abstract Feed-forward 3D Gaussian Splatting (3DGS) has emerged as highly effective solution for novel view synthesis. Existing methods predominantly rely on pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from predicted 3D voxel grid, it overcomes pixel alignments reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novelview rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat. I. INTRODUCTION 3D reconstruction is cornerstone of modern robotics, empowering autonomous systems with the critical ability to perceive, map, and comprehend their physical environment, which is fundamental for advanced navigation, object manipulation, and intelligent interaction. Traditional optimization based approaches, including Neural Radiance Fields (NeRF) [1] and 3D Gaussian Splatting (3DGS) [2], obtain high fidelity results by iteratively enforcing photometric or geometric consistency. These methods achieve excellent accuracy but are computationally intensive and slow to run at inference time. By contrast, feed-forward approaches [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13] trade per instance optimization for fast learned inference. single forward pass predicts scene geometry or 3D representation directly from input images. This speed and simplicity make feed-forward systems attractive for real time applications, * Equal contribution. Corresponding authors. Fig. 1: Comparison between the pixel-aligned feedforward method and our approach. Pixel-aligned feedforward 3DGS methods suffer from two primary limitations: 1) 2D feature matching struggles to effectively resolve the multi-view alignment problem, and 2) the Gaussian density is constrained and cannot be adaptively controlled according to scene complexity. We propose VolSplat, framework that directly regresses Gaussians from 3D features based on voxel-aligned prediction strategy. This approach achieves adaptive control over scene complexity and resolves the multi-view alignment challenge. large scale datasets, and downstream tasks that require many reconstructions. Prior feed-forward 3DGS methods [6], [8], [9], [11], [13] commonly rely on pixel alignment as their fundamental mechanism for associating image features with pixel aligned Gaussians. In this design, per-pixel features from precomputed image feature maps are unprojected to define the corresponding Gaussians. The prevailing consensus has been to perform fusion directly within the 2D feature representation. However, pixel aligned designs inherit several intrinsic limitations. Sampling at discrete pixel locations is sensitive to camera calibration and discretization error, produces inconsistent sampling patterns across views, and struggles in regions affected by occlusion, motion parallax, or low texture. Because the association is mediated entirely the resulting 3D by two dimensional predictions can suffer from depth ambiguities and unstable geometric reasoning, particularly when input viewpoints are sparse or when depth cues are weak. image coordinates, In this work we shift the alignment paradigm from pixels to voxels, illustrated in Fig. 1. Instead of sampling features at projected pixel coordinates, we align and aggregate image features directly into 3D voxel grid that shares canonical volumetric coordinate frame with the predicted geometry. This voxel aligned formulation changes where and how information is associated. Features are deposited into and retrieved from consistent 3D locations, enabling the model inductive biases to reason volumetrically and to exploit of 3D U-Net [14] and sparse volumetric operators. Voxel alignment removes the need for per query 2D prediction patterns and results in more stable multi-view fusion, cleaner occlusion handling, and better support for joint geometry and appearance inference. There are practical and conceptual advantages to voxel alignment. Volumetric aggregation reduces floaters and view dependent inconsistency because information from multiple views is fused into shared 3D container before Gaussian prediction. Operating in 3D grid enables the use of well studied 3D decoder and regularization strategies, which naturally encode locality and geometrical context. Voxel alignment simplifies the integration of auxiliary 3D signals such as depth maps [9] and point clouds [15]. These signals can be injected into or extracted from the same voxel frame without ad hoc reprojection heuristics. Finally, voxel centric representations are amenable to modern acceleration strategies such as sparse data structures, making the approach practical at the resolutions required for high quality reconstruction. In this paper we present feed-forward three dimensional reconstruction framework built around voxel alignment. As shown in Fig. 2, we first construct 3D feature grids using the extracted 2D image features, then refine the 3D features and use them to predict voxel-aligned Gaussians. We analyze the alignment errors that arise in pixel aligned pipelines and show how voxel alignment reduces these errors both conceptually and empirically. Through systematic experiments on synthetic and real world benchmarks, we demonstrate that voxel aligned feed-forward models achieve more accurate and robust reconstructions than comparable pixel aligned baselines on large-scale benchmarks such as ScanNet [16] and RealEstate10K [17]. Our contributions are as follows: We introduce voxel alignment as principled alternative to pixel alignment for feed-forward 3DGS and present practical end-to-end framework. We provide an analysis of alignment induced errors in pixel aligned systems and show how volumetric aggregation mitigates these failure modes. Experimental results suggest VolSplat achieves state-ofthe-art performance on both public benchmarks ScanNet [16] and RealEstate10K [17]. II. RELATED WORK A. Novel View Synthesis Traditional approaches to Novel View Synthesis (NVS) primarily rely on geometry-based rendering methods that reconstruct explicit 3D scene geometry from images [18], image-based rendering techniques that interpolate between captured views without full 3D reconstruction [19], and light field rendering that samples and reprojects densely captured rays in space [20]. These methods required either accurate geometric proxies, densely sampled viewpoints, or both to produce convincing visual results, limiting their applicability in real-world scenarios. The emergence of NeRF [1] marked paradigm shift, significantly improving both rendering quality and robustness over prior methods, which learns continuous, implicit scene representation by utilizing MLP to map position and viewing direction to corresponding color and volume density. While NeRF-based methods [21], [22] require long training time due to the per-ray rendering. 3DGS [2] and its variants [23], [24], [25] have been introduced to represent the 3D scene using set of anisotropic 3D Gaussians. B. 3D Voxelization Voxelization, which discretizes 3D space into regular voxel grids, has been foundational representation in 3D reconstruction and modeling [26]. Prior methods used dense grids for their simplicity, but suffered from high memory costs and poor scalability [27]. To address this, sparse structures like octrees were introduced for more efficient storage and computation [28]. In modern applications, voxels are widely used as input to 3D Convolutional Neural Network (CNN) for tasks such as object detection [29] and semantic segmentation [30]. More recently, voxels are often used as sparse scaffolding rather than as the final representation, supporting more advanced rendering techniques. Representative methods include Plenoxels [31] and K-Planes [32], which optimize voxel-based radiance fields for fast, high-quality rendering, as well as structured strategies such as ScaffoldGS [33] and Octree-GS [34], which leverage voxel grids to organize and accelerate 3DGS. C. Feed-Forward 3D Gaussian Splatting Recent advances in feed-forward 3DGS [6], [8], [9], [11], [13], [35], [36] offer compelling alternative that images in directly predicts 3D Gaussians from input single forward pass: pixelSplat [6] proposes two-view feed-forward pipeline that combines epipolar transformers and depth prediction to generate Gaussians. MVSplat [8] introduces cost-volume-based fusion strategy to enhance multi-view consistency. DepthSplat [9] leverages monocular depth features to improve fine 3D structure reconstruction from sparse views. Follow-up work extends feed-forward 3DGS to more complex scenarios, including pose-free inputs [37], [38], online stream inputs [11], and more dense inputs [10]. While these works adopt pixel-aligned strategy to predict Gaussian primitives, the pixel-wise formulation struggles to handle multiple input views due to redundancy Fig. 2: Overview of VolSplat. Given multi-view images as input, we first extract 2D features for each image using Transformer-based network and construct per-view cost volumes with plane sweeping. Depth Prediction Module then estimates depth map for each view, which is used to unproject the 2D features into 3D space to form voxel feature grid. Subsequently, we employ sparse 3D decoder (details in Sec. III-C.1) to refine these features in 3D space and predict the parameters of 3D Gaussian for each occupied voxel. Finally, novel views are rendered from the predicted 3D Gaussians. and inconsistency across pixels. Existing methods attempt to improve the per-pixel strategy by pruning the number of Gaussians [35], token merging [39] and voxel-based fusion [36]. However, these approaches do not fundamentally address the limitations inherent in per-pixel processing. EVolSplat [40] has explored voxel features in autonomous driving scenarios, but it has not been generalized to general scenarios and requires explicit 3D point clouds as intermediate representations. In contrast, our method introduces voxel-aligned method, which eliminate the need for perquery 2D prediction patterns. This alignment enables more stable multi-view fusion, cleaner occlusion handling, and more coherent joint inference of geometry and appearance. III. METHOD A. Preliminary and Observation Feed-forward 3D reconstruction aims to learn mapping i=1 where Ii RHW 3 from input images = {Ii}N and their corresponding camera poses = {Pi}N i=1, to 3D scene representation. In the context of pixel-aligned 3DGS, features are extracted from images and refined by cross-view interaction: = {Fi}N i=1 = h(Φimage(I, P)), Fi p C (1) where Φimage is pretrained image encoder. The function is responsible for processing these features from different viewpoints, with its core purpose being to perform crossview feature matching and fusion. For pixel-aligned Gaussian the features must be upsampled to the same prediction, resolution as the input image: Ffull = (F), Ffulli RHW (2) where is feature upsampler such as CNN-based network (in MVSplat [8]) and deconvolution-based network [41] (in DepthSplat [9]) and per-pixel Gaussian predictions are then performed using the upsampled features: = {(µi, Σi, αi, ci)}HW i=1 = Ψpred(Ff ull, P) (3) Where the position of the Gaussians are determined by the predicted depth and pixel location. While straightforward, this pixel-aligned formulation introduces two critical limitations. First, the geometric accuracy of the reconstruction is critically dependent on the quality of the predicted depth map. After depth unprojecting features into 3D space, the lack of interaction with neighboring points within the 3D space significantly contributes to the generation of floaters. Second, the structure of the 3D representation is rigidly tied to the 2D image grid. The total number of Gaussians is fixed at = , which is often suboptimal and cause an over-densification of Gaussians on simple, texture-less surfaces and an insufficient number for representing complex geometry not captured at the pixel level. These observations reveal fundamental bottleneck and motivate our proposed voxel-aligned framework, designed to decouple the 3D representation from the 2D pixel grid. B. 3D Feature Construction 1) Feature Extraction and Feature Matching: For input images, we first apply weight-sharing ResNet [42] backbone to each RGB image to obtain downsampled feature maps. These features are then refined with crossview attention that exchanges information with the two nearest neighboring views. For efficiency, this cross-attention is implemented using the Swin Transformers [43] local window attention. After this stage we obtain cross-viewaware Transformer features {Fi}N C) , where denotes the feature dimension. i=1 (F Next, we build per-view cost volumes {Ci}N i=1 using plane-sweep strategy [44]. For each view i, we sample candidate depths {dm}D m=1, warp the feature from neighboring views to the reference view at each hypothesized depth, and compute pairwise feature similarities [8].These similarities are aggregated by dot-product matching and stacked along the depth axis to form {Ci}N i=1 , (Ci D) W mono (cid:9)N i=1 (F the monocular mono To produce robust, multi-view consistent depth estifeatures mates, depth module fuses (cid:8)F s C) with the cost volume and regresses dense per-pixel depth map Di RHW , which serves as geometric prior for lifting image features into 3D space. These per-view features and depths Di are used in the next stage to construct 3D point clouds and voxel-based features for volumetric reasoning. 2) Lifting to 3D Feature: Given the predicted depth maps Di and camera parameters, we conveniently aggregate different depth map views by transforming the point clouds into global coordinate system. First each pixel (u, v) in image space is unprojected to 3D point in the camera coordinate frame using the camera intrinsics. Then the 3D point is transformed into the world coordinate system via the corresponding extrinsic parameters, including the rotation matrix Ri and translation Ti vector. Pworld = Ri Pcam + Ti Di(u, v) 1 = Ri 1 + Ti (4)"
        },
        {
            "title": "To convert",
            "content": "By repeating this process across all views, we obtain dense = point cloud in world space, where each 3D point is associated with its corresponding image feature. the unstructured dense point cloud into structured volumetric representation, we voxelize the points [45]. For each 3D point = (xp, yp, zp) we compute integer voxel index (i, j, k) by dividing by the voxel size vs and rounding. = rnd (cid:19) (cid:18) xp vs , = rnd (cid:19) (cid:18) yp vs , = rnd (cid:19) (cid:18) zp vs (5) where rnd() denotes rounding to the nearest integer. Let Si,j,k be the set of all points falling into voxel (i, j, k) and fpbe the image feature corresponding to each point Si,j,k The features within this voxel are aggregated via average pooling along the channel dimension, resulting in the voxel feature. resulting in the voxel feature Vi,j,k: Vi,j,k = 1 Si,j,k (cid:88) fp pSi,j,k (6) Fig. 3: Architecture of Sparse 3D Decoder. Sparse 3D features are fed into 3D U-Net for processing, which predicts residual features for each voxel. These residual features are then added to the original 3D voxel features to obtain the refined features. C. Feature Refinement and Gaussian Prediction 1) Feature Refinement: To improve the spatial consistency and structural fidelity of the voxel representation, we apply an explicit voxel feature refinement stage as shown in Fig. 3. Given an input voxel grid (with per-voxel feature vectors), sparse convolutional 3D U-Net [14] predicts residual voxel field R: = R(V ), Ri RVC (7) where denotes the set of occupied voxels and the refined voxel features are obtained by residual update: = + R, RVC (8) The refinement network is implemented with hierarchical sparse 3D convolutional blocks, symmetric encoderdecoder stages, and upsampling layers connected by skip connections. This architecture enables multi-scale fusion of local and global geometric context while keeping computation efficient through sparsity. The residual formulation encourages the network to learn correction terms (fine geometric detail and consistency cues) rather than relearning the entire feature content, which empirically stabilizes training and preserves the coarse voxel information supplied by the lifting stage. 2) Gaussian Prediction: The output of our network for each voxel is set of learnable Gaussian parameters {[µj, αj, Σj, cj] R18}. These include the offset of the Gaussian center µj, opacity αj, covariance Σj, and spherical harmonic color representation cj. To obtain the final rendering parameters, we apply the following transformations: µj = σ(µj) + Centerj, αj = σ(αj) (9) where µj denotes the 3D center of the Gaussian, Centerj is the centroid of voxel and is hyperparameter controlling the spatial extent of the offset (typically set to three times the voxel size), σ() denotes the sigmoid function. D. Optimization Our network predicts collection of 3D Gaussians (µv, αv, Σv, cv)}vV . These per-voxel Gaussians are subsequently used to synthesize images at novel camera poses. [35] [8] [9] [11] Fig. 4: Qualitative comparison under multi-view input conditions. The results on the left are from RealEstate10K [17], and the results on the right are from ScanNet [16]. Models are trained on each indicated dataset and tested on the same dataset. VolSplat achieved state-of-the-art results on both of them. The network is trained end-to-end using ground-truth RGB images as supervision. For forward pass that renders novel views, we optimize combined photometric and perceptual loss: = (cid:88) (cid:16) m="
        },
        {
            "title": "LMSE",
            "content": "(cid:0)I (m) render, (m) gt (cid:1) + λ LLPIPS (cid:0)I (m) render, (m) gt (cid:1)(cid:17) (10) where is the number of novel views to render in single forward pass. The LLPIPS [46] loss weight λ is set to 0.05. IV. EXPERIMENTS A. Experimental Setup test Datasets. We train our method using two expansive datasets, RealEstate10K [17] and ScanNet [16], and evaluate its performance on the held-out splits of both. For RealEstate10K, we adopt the conventional partition of 67,477 training scenes and 7,289 test scenes. As for ScanNet, which consists of 1,513 videos of indoor scenes, we follow past work [47], [48], [11] in using roughly 100 scenes for training and 8 scenes for evaluation. These datasets span wide variety of environments, including indoor and outdoor realestate walkthroughs (RealEstate10K), and real-world videos of numerous scenes suitable for indoor robot applications (ScanNet). We resize training and test images to 256 256. Baselines and metrics. We benchmark VolSplat against several recent feed-forward methods for sparse-view novel view synthesis, including both pixel-aligned and enhanced pixel-aligned Gaussian splatting approaches. Pixel-aligned methods predict Gaussian parameters on per-pixel basis in image space before unprojecting to 3D. These include pixelSplat [6], MVSplat [8], FreeSplat [11], TranSplat [49] and DepthSplat [9]. In contrast to the enhanced pixel-aligned approach, Gaussian Graph Network (GGN) [35] refines the pixel-aligned approach by modeling the relationships between groups of predicted Gaussians across different views while building upon it."
        },
        {
            "title": "In contrast",
            "content": "to both pixel-aligned and enhanced pixelaligned methods, VolSplat employs voxel-aligned approach, predicting Gaussian primitives within 3D voxel grid. This method aggregates multi-view evidence in 3D space, aligning Gaussian predictions to voxel structure, which facilitates better geometric consistency and efficient redundancy reduction. For quantitative evaluation, we adopt standard image quality metrics commonly used in NVS, including pixel-level Peak Signal-to-Noise Ratio (PSNR), patch-level Structural Similarity Index Measure (SSIM),and feature-level Learned Perceptual Image Patch Similarity (LPIPS). Implementation details. We implement VolSplat using PyTorch [50] and optimize the model with the AdamW [51] optimizer and cosine learning rate schedule. The monocular Vision Transformer backbone is implemented using the xFormers [52] library. For the pre-trained Depth Anything V2 [53] backbone, we use lower learning rate of 2 106, while other layers are trained with learning rate of 2104 following DepthSplat [9]. For experiments on the RealEstate10K [17] and ScanNet [16] dataset, we train the model for 150,000 iterations using 4 A100 GPUs with total batch size of 4. Following the setting of the baseline, we use 256 256 as input resolution. In the training stage, the number of input views TABLE I: Quantitative comparisons on RealEstate10K [17]. The first five methods are all pixel-aligned methods, and GGN [35] performs post-processing on pixel-aligned Gaussians. Model uses more input views to get more Gaussians for pruning. \"PGS\" stands for \"average number of per-view Gaussians\". Method PSNR SSIM LPIPS PGS pixelSplat [6] MVSplat [8] TranSplat [49] DepthSplat [9] GGN [35] VolSplat 26.09 26.39 26.69 27.47 26.18 31.30 0.863 0.869 0.875 0.889 0.825 0. 0.136 0.128 0.125 0.114 0.154 0.075 196608 65536 65536 65536 9375 TABLE II: Quantitative comparisons on ScanNet [16]. FreeSplat [11] and FreeSplat++ [54] performs Gaussian fusion after pixel-aligned Gaussian prediction. Method PSNR SSIM LPIPS PGS FreeSplat [11] FreeSplat++ [54] VolSplat 27.45 27.45 28.41 0.829 0.829 0. 0.222 0.223 0.127 63668 69569 65406 is set to 6, and we evaluate the models performance with same numbers of input views. B. Experimental Results and Analysis Comparisons with SoTA models. As shown in Tab. and Tab. II, we report VolSplats performance compared to current mainstream pixel-aligned models [6], [8], [49], [11], [9] and their variant [35]. On both the RealEstate10K [17] and ScanNet [16] datasets, VolSplat achieves state-of-the-art results. Our experiments reveal critical distinction between pixel-aligned and voxel-aligned paradigms. key observation is that under sparse multi-view settings, all pixel-aligned models exhibit significant degradation in performance. In contrast, VolSplat demonstrates promising performance to these challenging conditions. As illustrated in Fig. 4, images rendered by our method are largely free of the common floaters and artifacts that plague competing methods at object boundaries. This visual improvement stems directly from our models ability to resolve multi-view alignment issues within its 3D feature representation, resulting in cleaner edges and more coherent 3D scene reconstruction. Cross-dataset generalization. Following MVSplat [8], we assess the generalization capabilities of our model on unseen outdoor datasets to verify its broad reliability. To this end, we conducted cross-dataset generalization experiment by taking our model pre-trained on the RealEstate10K [17] dataset and evaluating it directly on the ACID [55] dataset without any fine-tuning. As demonstrated in Tab. III, VolSplat maintains significantly higher performance in this zero-shot transfer setting. We attribute this superior generalization to the inherent robustness of our voxel-aligned framework. Pixel-aligned models exhibit much higher sensitivity to the variations in [9] Fig. 5: Visualization on Gaussians of DepthSplat [9] and VolSplat. The Gaussian distribution in the pixel-aligned method is limited by the pixel distribution of the input view. Since the density must remain uniform, it cannot learn the complex geometry of the corners of the washbasin in the center and forces the Gaussians to be distributed to unnecessary edge areas. VolSplat is more realistic and reasonable. TABLE III: Cross-dataset generalization results on ACID [55]. All models were trained on RealEstate10K [17] and evaluated on the test set without any fine-tuning. Method PSNR SSIM LPIPS PGS pixelSplat [6] MVSplat [8] TranSplat [49] DepthSplat [9] VolSplat 27.64 28.15 28.17 28.37 32.65 0.830 0.841 0.842 0.847 0.932 0.160 0.147 0.146 0.141 0. 196608 65536 65536 65536 65527 data complexity and distribution between different datasets. In contrast, VolSplat is less susceptible to these domain shifts. Analysis of Gaussian Density. fundamental principle of efficient 3D reconstruction is that the complexity of the representation should adapt to the complexity of the scene. Real-world environments contain mix of simple, planar surfaces and intricate, high-frequency geometric details. An ideal model should allocate its descriptive capacity accordingly. However, pixel-aligned methods are inherently limited in this regard. Their paradigm of predicting one Gaussian per pixel results in fixed number of primitives, predetermined by the input image resolution (e.g., Gaussians from reference view), regardless of whether the scene is simple room or complex outdoor environment. In stark contrast, our voxel-aligned framework enables adaptive control over the density of the 3D Gaussians. By predicting primitives based on the occupancy of 3D voxel features, VolSplat naturally allocates higher concentration of Gaussians to regions of high geometric detail while using sparser representation for simple or empty spaces. This adaptive capability is quantitatively validated by the results we reported in Tab. I, Tab. II and Tab. III. Here, we analyze these findings in greater detail. The data shows that pixel-aligned methods consistently generate large and constant number of Gaussians, irrespective of the scene content. TABLE IV: Analysis of voxel size. The default voxel size is more suitable for real-world data than other options. Voxel Size (cm) PSNR SSIM LPIPS PGS 0.05 0.1 (default) 0.5 1 31.03 31.30 28.73 21. 0.939 0.941 0.916 0.782 0.077 0.075 0.132 0.315 65535 65529 64455 57806 TABLE V: Ablation of sparse 3D decoder. \"w/ 3D CNN\" means replacing the 3D U-Net with sparse 3D CNN, and \"w/o decoder\" means removing the refinement stage. Components PSNR SSIM LPIPS Memory(GB) default w/ 3D CNN w/o decoder 31.30 29.84 29.34 0.941 0.926 0.921 0.075 0.096 0. 8.06 8.04 8.00 This leads to significant redundancy and computational inefficiency, especially in simpler scenes. Conversely, Gaussians for VolSplat demonstrate significant variance across different scenes, confirming its ability to tailor the representations complexity, as shown in Fig. 5. Notably, VolSplat often achieves superior rendering quality with more efficient and, in many cases, more compact set of Gaussians compared to the brute-force density of pixel-aligned approaches. C. Ablations Study In this section, we study the properties of our key components on the RealEstate10K [17] dataset. Ablation of Voxel Size. The voxel size is critical hyperparameter in our framework, as it dictates the resolution of the 3D feature grid. This choice involves fundamental tradeoff between the fidelity of the geometric representation and computational resource consumption. In Tab. IV, we analyze this trade-off by comparing our default setting against configurations with different voxels. Using small voxel size increases the granularity of the 3D grid, allowing the model to capture finer geometric details. It comes at significant cost, substantially increasing memory usage and processing time due to the cubic growth of the voxel volume. Conversely, employing large voxel size reduces the computational footprint but results in coarser quantization of the 3D space. Our default configuration strikes an effective balance, achieving state-of-the-art performance while maintaining manageable computational requirements. Ablation of Model Architecture. Directly predicting Gaussians from the initial unprojected 3D features is less effective, particularly for challenging scenes with complex geometry or sparse viewpoints. To address this, we incorporate 3D U-Net architecture to refine and enhance this raw feature volume. To validate the necessity and efficacy of this design, we conduct an ablation study with two variants: 1) removing the refinement module entirely, and 2) replacing the 3D UNet with standard 3D CNN. The results, presented in Tab. V, confirm our architectural choices. Removing the refinement stage altogether leads to significant drop in performance, demonstrating that processing the initial voxel features is critical for producing coherent 3D representation. While substituting our module with sparse 3D CNN yields better results than no refinement, it still falls short of our full models performance. The multi-scale feature fusion inherent in the U-Net structure is crucial for capturing both fine-grained local details and broader spatial context. V. CONCLUSION In this paper, we have addressed the fundamental limitations inherent in the prevailing pixel-aligned paradigm for feed-forward 3D Gaussian Splatting. We have identified that existing methods suffer from rigid coupling of Gaussian density to input image resolution and high sensitivity to multi-view alignment errors. To overcome these challenges, we have introduced VolSplat, novel framework that fundamentally shifts the reconstruction process from 2D pixels to 3D voxel-aligned space. By constructing 3D voxel feature and predicting Gaussians directly from this unified representation, our method effectively decouples the 3D scene from the constraints of the input views. This voxelcentric design enables adaptive control over Gaussian density according to scene complexity and inherently resolves alignment ambiguities, leading to more geometrically consistent and faithful reconstructions for downstream tasks."
        },
        {
            "title": "REFERENCES",
            "content": "[1] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, Nerf: Representing scenes as neural radiance fields for view synthesis, Communications of the ACM, vol. 65, no. 1, pp. 99106, 2021. [2] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., vol. 42, no. 4, pp. 1391, 2023. [3] A. Yu, V. Ye, M. Tancik, and A. Kanazawa, pixelnerf: Neural radiance fields from one or few images, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 45784587. [4] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla, N. Snavely, and T. Funkhouser, Ibrnet: Learning multi-view image-based rendering, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 46904699. [5] A. Chen, Z. Xu, F. Zhao, X. Zhang, F. Xiang, J. Yu, and H. Su, Mvsnerf: Fast generalizable radiance field reconstruction from multiview stereo, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 14 12414 133. [6] D. Charatan, S. L. Li, A. Tagliasacchi, and V. Sitzmann, pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 19 45719 467. [7] K. Zhang, S. Bi, H. Tan, Y. Xiangli, N. Zhao, K. Sunkavalli, and Z. Xu, Gs-lrm: Large reconstruction model for 3d gaussian splatting, in European Conference on Computer Vision. Springer, 2024, pp. 1 19. [8] Y. Chen, H. Xu, C. Zheng, B. Zhuang, M. Pollefeys, A. Geiger, T.-J. Cham, and J. Cai, Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images, in European Conference on Computer Vision. Springer, 2024, pp. 370386. [9] H. Xu, S. Peng, F. Wang, H. Blum, D. Barath, A. Geiger, and M. Pollefeys, Depthsplat: Connecting gaussian splatting and depth, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 16 45316 463. [10] W. Wang, D. Y. Chen, Z. Zhang, D. Shi, A. Liu, and B. Zhuang, Zpressor: Bottleneck-aware compression for scalable feed-forward 3dgs, arXiv preprint arXiv:2505.23734, 2025. [11] Y. Wang, T. Huang, H. Chen, and G. H. Lee, Freesplat: Generalizable 3d gaussian splatting towards free view synthesis of indoor scenes, Advances in Neural Information Processing Systems, vol. 37, pp. 107 326107 349, 2024. [12] Y. Chen, C. Zheng, H. Xu, B. Zhuang, A. Vedaldi, T.-J. Cham, and J. Cai, Mvsplat360: Feed-forward 360 scene synthesis from sparse views, Advances in Neural Information Processing Systems, vol. 37, pp. 107 064107 086, 2024. [13] G. Kang, S. Nam, X. Sun, S. Khamis, A. Mohamed, and E. Park, ilrm: An iterative large 3d reconstruction model, arXiv preprint arXiv:2507.23277, 2025. [14] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger, 3d u-net: learning dense volumetric segmentation from sparse annotation, in International conference on medical image computing and computer-assisted intervention. Springer, 2016, pp. 424432. [15] D. Shi, W. Wang, D. Y. Chen, Z. Zhang, J. Bian, B. Zhuang, and C. Shen, Revisiting depth representations for feed-forward 3d gaussian splatting, arXiv preprint arXiv:2506.05327, 2025. [16] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner, Scannet: Richly-annotated 3d reconstructions of indoor scenes, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 58285839. [17] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely, Stereo magnification: Learning view synthesis using multiplane images, arXiv preprint arXiv:1805.09817, 2018. [18] P. E. Debevec, C. J. Taylor, and J. Malik, Modeling and rendering architecture from photographs: hybrid geometry-and image-based approach, in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 465474. [19] D. Ji, J. Kwon, M. McFarland, and S. Savarese, Deep view morphing, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 21552163. [20] M. Levoy and P. Hanrahan, Light field rendering, in Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023, pp. 441 452. [21] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan, Mip-nerf: multiscale representation for antialiasing neural radiance fields, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 58555864. [22] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, Zip-nerf: Anti-aliased grid-based neural radiance fields, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 19 69719 705. [23] Z. Fan, K. Wang, K. Wen, Z. Zhu, D. Xu, Z. Wang, et al., Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps, Advances in neural information processing systems, vol. 37, pp. 140 138140 158, 2024. [24] Z. Zhu, Z. Fan, Y. Jiang, and Z. Wang, Fsgs: Real-time few-shot view synthesis using gaussian splatting, in European conference on computer vision. Springer, 2024, pp. 145163. [25] H. Liu, Y. Wang, C. Li, R. Cai, K. Wang, W. Li, P. Molchanov, P. Wang, and Z. Wang, Flexgs: Train once, deploy everywhere with many-in-one flexible 3d gaussian splatting, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 16 33616 345. [26] D. Meagher, Geometric modeling using octree encoding, Computer graphics and image processing, vol. 19, no. 2, pp. 129147, 1982. [27] A. E. Kaufman and K. Mueller, Overview of volume rendering. The visualization handbook, vol. 7, pp. 127174, 2005. [28] C. H. Koneputugodage, Y. Ben-Shabat, and S. Gould, Octree guided unoriented surface reconstruction, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 16 71716 726. [29] Y. Zhou and O. Tuzel, Voxelnet: End-to-end learning for point cloud based 3d object detection, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 44904499. [30] G. Riegler, A. Osman Ulusoy, and A. Geiger, Octnet: Learning deep 3d representations at high resolutions, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 35773586. [31] S. Fridovich-Keil, A. Yu, M. Tancik, Q. Chen, B. Recht, and A. Kanazawa, Plenoxels: Radiance fields without neural networks, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 55015510. [32] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa, K-planes: Explicit radiance fields in space, time, and appearance, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 12 47912 488. [33] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai, Scaffold-gs: Structured 3d gaussians for view-adaptive rendering, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 20 65420 664. [34] K. Ren, L. Jiang, T. Lu, M. Yu, L. Xu, Z. Ni, and B. Dai, Octreegs: Towards consistent real-time rendering with lod-structured 3d gaussians, arXiv preprint arXiv:2403.17898, 2024. [35] S. Zhang, X. Fei, F. Liu, H. Song, and Y. Duan, Gaussian graph network: Learning efficient and generalizable gaussian representations from multi-view images, Advances in Neural Information Processing Systems, vol. 37, pp. 50 36150 380, 2024. [36] L. Jiang, Y. Mao, L. Xu, T. Lu, K. Ren, Y. Jin, X. Xu, M. Yu, J. Pang, F. Zhao, et al., Anysplat: Feed-forward 3d gaussian splatting from unconstrained views, arXiv preprint arXiv:2505.23716, 2025. [37] B. Ye, S. Liu, H. Xu, X. Li, M. Pollefeys, M.-H. Yang, and S. Peng, No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images, arXiv preprint arXiv:2410.24207, 2024. [38] G. Kang, J. Yoo, J. Park, S. Nam, H. Im, S. Shin, S. Kim, and E. Park, Selfsplat: Pose-free and 3d prior-free generalizable 3d gaussian splatting, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 22 01222 022. [39] C. Ziwen, H. Tan, K. Zhang, S. Bi, F. Luan, Y. Hong, L. Fuxin, and Z. Xu, Long-lrm: Long-sequence large reconstruction model for wide-coverage gaussian splats, arXiv preprint arXiv:2410.12781, 2024. [40] S. Miao, J. Huang, D. Bai, X. Yan, H. Zhou, Y. Wang, B. Liu, A. Geiger, and Y. Liao, Evolsplat: Efficient volume-based gaussian splatting for urban view synthesis, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 11 28611 296. Deconvolution and [Online]. Available: http: [41] A. Odena, V. Dumoulin, and C. Olah, checkerboard artifacts, Distill, 2016. //distill.pub/2016/deconv-checkerboard [42] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770778. [43] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using the IEEE/CVF international shifted windows, in Proceedings of conference on computer vision, 2021, pp. 10 01210 022. [44] H. Xu, J. Zhang, J. Cai, H. Rezatofighi, F. Yu, D. Tao, and A. Geiger, Unifying flow, stereo and depth estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 11, pp. 13 941 13 958, 2023. [45] T. Wang, X. Mao, C. Zhu, R. Xu, R. Lyu, P. Li, X. Chen, W. Zhang, K. Chen, T. Xue, et al., Embodiedscan: holistic multi-modal 3d perception suite towards embodied ai, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 19 75719 767. [46] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, The unreasonable effectiveness of deep features as perceptual metric, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 586595. [47] X. Zhang, S. Bi, K. Sunkavalli, H. Su, and Z. Xu, Nerfusion: Fusing radiance fields for large-scale scene reconstruction, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 54495458. [48] Y. Gao, Y.-P. Cao, and Y. Shan, Surfelnerf: Neural surfel radiance fields for online photorealistic reconstruction of indoor scenes, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 108118. [49] J. Kim, J. Noh, D.-G. Lee, and A. Kim, Transplat: Surface embedding-guided 3d gaussian splatting for transparent object manipulation, arXiv preprint arXiv:2502.07840, 2025. [50] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imperative style, high-performance deep learning library, Advances in neural information processing systems, vol. 32, 2019. [51] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, arXiv preprint arXiv:1711.05101, 2017. [52] B. Lefaudeux, F. Massa, D. Liskovich, W. Xiong, V. Caggiano, S. Naren, M. Xu, J. Hu, M. Tintore, S. Zhang, P. Labatut, D. Haziza, L. Wehrstedt, J. Reizenstein, and G. Sizov, xformers: modular and hackable transformer modelling library, https://github.com/ facebookresearch/xformers, 2022. [53] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, Depth anything v2, arXiv:2406.09414, 2024. [54] Y. Wang, T. Huang, H. Chen, and G. H. Lee, Freesplat++: Generalizable 3d gaussian splatting for efficient indoor scene reconstruction, arXiv preprint arXiv:2503.22986, 2025. [55] A. Liu, R. Tucker, V. Jampani, A. Makadia, N. Snavely, and A. Kanazawa, Infinite nature: Perpetual view generation of natural scenes from single image, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 45814 467."
        }
    ],
    "affiliations": [
        "GigaAI",
        "Monash University",
        "The Chinese University of Hong Kong",
        "Tsinghua University",
        "University of Electronic Science and Technology of China",
        "Zhejiang University"
    ]
}