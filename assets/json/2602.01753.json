{
    "paper_title": "ObjEmbed: Towards Universal Multimodal Object Embeddings",
    "authors": [
        "Shenghao Fu",
        "Yukun Su",
        "Fengyun Rao",
        "Jing Lyu",
        "Xiaohua Xie",
        "Wei-Shi Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination."
        },
        {
            "title": "Start",
            "content": "ObjEmbed: Towards Universal Multimodal Object Embeddings Shenghao Fu 1 2 Yukun Su 3 Fengyun Rao 3 Jing LYU 3 Xiaohua Xie 1 4 5 6 Wei-Shi Zheng 1 2 4"
        },
        {
            "title": "Abstract",
            "content": "Aligning objects with corresponding textual descriptions is fundamental challenge and realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) ObjectOriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination. Code is available at https: //github.com/WeChatCV/ObjEmbed. 6 2 0 2 3 ] . [ 2 3 5 7 1 0 . 2 0 6 2 : r 1School of Computer Science and Engineering, Sun Yat-sen University, China 2Peng Cheng Laboratory, China 3Independent Researcher 4Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China 5Guangdong Province Key Laboratory of Information Security Technology, China 6Pazhou Laboratory (Huangpu), China. Correspondence to: Xiaohua Xie <xiexiaoh6@mail.sysu.edu.cn>, Wei-Shi Zheng <wszheng@ieee.org>. Preprint. February 4, 2026. 1 Figure 1. ObjEmbed achieves balanced and superior performance across wide span of benchmarks. 1. Introduction Multimodal embedding models have emerged as cornerstone in bridging heterogeneous data modalities, such as vision, language, and audio, into unified semantic space, enabling rich cross-modal understanding, retrieval, and reasoning. Recent advances in large-scale image-text contrastive learning (Radford et al., 2021; Zhai et al., 2023; Xie et al., 2025b) have led to significant progress in multimodal representation learning, especially in aligning images and corresponding captions. Powered by large multimodal models (Bai et al., 2025b;a), embedding models (Zhang et al., 2024; 2025; Jiang et al., 2025c; Lan et al., 2025) can generate task-specific embeddings following user instructions, generate cross-modal embeddings with arbitrary modality combinations, and even perform high-level summary and deep reasoning through chain-of-thought. However, encoding objects within images and aligning them with text queries are still challenging for recent embedding ObjEmbed: Towards Universal Multimodal Object Embeddings models. Such capabilities are often critical in real-world applications such as autonomous driving (e.g., distant traffic signs), robotics (e.g., small parts manipulation), and digital content safety moderation. Reliable retrieval and representation of objects demand precise localization and strong semantic discrimination, yet remain under-addressed in current frameworks. While FG-CLIP (Xie et al., 2025b;a) improves regional alignment by combining regional and global contrastive objectives, its object embeddings lack explicit modeling of bounding box quality, limiting their reliability in localization-sensitive tasks. Another line of research, open-vocabulary object detection (Liu et al., 2024b; Fu et al., 2025c;b), directly aligning text embeddings with regions of interest, can accurately localize objects and perform openvocabulary recognition. However, due to limited training data, their generalization ability is largely constrained. To encode objects with high semantic discrimination and precise localization awareness, we introduce ObjEmbed, an MLLM-based object embedding model that encodes all objects within an image as embeddings. Given an input image, regions of interest (RoIs) are first extracted using an off-theshelf proposal generator and then encoded as sequence of tokens. Each object is represented by two special tokens: (1) an object token to capture fine-grained semantic content; and (2) an IoU token to predict the quality of the corresponding bounding box by regressing its IoU score with the ground truth. The object and IoU tokens, along with global image tokens, are processed in parallel by large language model (LLM) to ensure efficiency. The final-layer hidden states of these tokens serve as the object embeddings, IoU embeddings, and image embeddings, respectively. Similarly, text queries are independently encoded into text embeddings through the same LLM backbone, enabling seamless crossmodal alignment. With this novel architecture, ObjEmbed produces object-centric representations that jointly encode semantic meaning and localization confidence, enabling accurate recognition, precise localization, and robust crossmodal retrieval. Equipped with this object-centric design, ObjEmbed supports wide range of downstream applications in unified framework: (1) Object detection and referring expression comprehension: Class names or natural language expressions are encoded as text embeddings and matched against all object embeddings in the image. The final object matching score combines both semantic similarity (between object and text embeddings) and predicted localization quality (from the IoU embedding), computed as their product, effectively balancing semantic relevance and spatial accuracy. (2) Local image retrieval: When the query describes only specific region or object, we compute the image-level relevance as the maximum matching score across all detected objects. This strategy enables fine-grained, part-aware retrieval even when the target occupies small portion of the image. (3) Global image retrieval: Since ObjEmbed also retains global image embeddings, they can be directly used for standard image-text retrieval tasks, ensuring compatibility with conventional benchmarks and applications. After training on 1.3M samples, ObjEmbed demonstrates strong performance across wide range of vision tasks, summarized in Figure 1. On object detection, it achieves 53.0% mAP on the COCO dataset, highly competitive result compared to specialist models. For referring expression comprehension, ObjEmbed attains an average accuracy of 89.5 on RefCOCO/+/g, reflecting its robust ability to align language with visual objects. Most notably, ObjEmbed excels in local image retrieval, which requires fine-grained cross-image comparisons. In this setting, ObjEmbed outperforms existing global image embedding models by around 20 points on four standard benchmarks. It also achieves competitive performance on global image retrieval tasks, despite using small-scale training set. We hope that ObjEmbed can serve as strong and general-purpose baseline for future research in object representation learning. 2. Related Work 2.1. Multimodal Embedding Models Contrastive language-image pre-training (Radford et al., 2021), which aims to align matched image-text embeddings while pushing away others, is an effective and scalable way to learn transferable image representations. Other improvements, including sigmoid loss (Zhai et al., 2023), wellcurated data (Xu et al., 2024; Chuang et al., 2025), hard negative samples (Wei et al., 2025), and multi-task learning (Tschannen et al., 2025), further improving its effectiveness. The development of large multimodal models further equips embedding models with the ability to follow instructions, cross-modality combinations, and deep understanding and reasoning (Zhang et al., 2025; Jiang et al., 2025c; Li et al., 2026; Lan et al., 2025). However, representing object features and assessing their localization quality are still challenging for global image embedding models. FG-CLIP (Xie et al., 2025b;a) tries to mitigate the problem by introducing regional contrastive learning. It cannot assess localization quality, thus it can only tackle the classification problem. CLARE (Hao et al., 2025) is the first method tackling the object retrieval task by training traditional detector with contrastive language-instance alignment. However, due to limited data and model capacity, the generalization ability is constrained. 2.2. Open-Vocabulary Object Detection Open-vocabulary object detection aims to detect arbitrary objects described by text queries. Thus, detectors should encode objects into embeddings and align them with text 2 ObjEmbed: Towards Universal Multimodal Object Embeddings Figure 2. The architecture of ObjEmbed. ObjEmbed is single-tower model built upon large multimodal language model, enhanced with an object projector and five special tokens (object, iou, global, local text, and global text) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings). embeddings. To align with the text space, some methods (Fu et al., 2025b; Gu et al., 2022; Wu et al., 2023) distill the features of CLIP (Radford et al., 2021), some methods (Fu et al., 2025a; 2024; Du et al., 2024) integrate the CLIP model as module or backbone, while others (Liu et al., 2024b; Fu et al., 2025c) use deep fusion layers for cross-modal alignment. Although open-vocabulary object detectors excel at object localization, detectors aligning with CLIP cannot truly inherit open-vocabulary capacity, while deep-fusion methods cannot produce query-agnostic object embeddings and the training data is hard to scale up. These limitations motivate us to develop novel MLLM-based object embedding model with localization awareness and robust transferability. 3. Method 3.1. Model Architecture In this work, we aim to build novel MLLM-based object embedding model, featuring three key properties: (1) Object-Oriented Representation: it captures both semantic and spatial aspects of objects and assigns higher matching scores to objects with tighter and more accurate boxes; (2) Versatility: the model has the ability to tackle both objectlevel and image-level tasks; (3) Efficient Encoding: the model encodes all objects in an image in single forward pass. To achieve the goal, we finetune standard large multimodal model, Qwen3-VL-instruct (Bai et al., 2025a), and introduce five special tokens whose hidden states from the last layer are used as embeddings: (1) object: the object embedding to represent semantic details; (2) iou: the IoU embedding to assess the box quality of each object; (3) global: the global image embedding representing the full image; (4) local text: the text embedding for matching objects; and (5) global text: the text embedding for matching images. The overall architecture is shown in Figure 2 and the different usages of each token are detailed as follows: Representing objects as sequence of embeddings. Following WeDetect-Ref (Fu et al., 2025a), we first use universal proposal generator, WeDetect-Uni (Fu et al., 2025a), to generate top-N proposals (100 proposals in this work) for each image. Each RoI feature is extracted via RoIAlign and then compressed into single token via an object projector. These object features will replace the object tokens and are organized sequentially before sending to the large language model. Each object will be separated by prompts Object i: object to ensure distinctiveness. By representing objects as sequence of embeddings, the model can encode all objects simultaneously with high efficiency. Assessing box quality via IoU embeddings. To equip the model with location awareness, we explicitly model object location by predicting an IoU score for each detected object. We empirically find that using single token to jointly learn location and classification leads to optimization conflicts. To mitigate this, we introduce dedicated special token iou to represent the quality of each bounding box. This token immediately follows the corresponding object token, forming the structured sequence: Object i: objectiou. The final IoU score is computed by applying linear head to the IoU embedding and then multiplying it by the classification score to produce the final object matching score. 3 ObjEmbed: Towards Universal Multimodal Object Embeddings Integrating global image embeddings with object embeddings. We also introduce global image token global to encode full image content. As demonstrated in the next subsection, we will use both long text caption and short text caption as labels to learn global image embeddings. We use two same but separate global tokens for different kinds of captions. The global image embeddings and object embeddings will be encoded in single forward pass and the final template is: vision start IMAGE vision end Task Instruction The coarse global image is global. Object 0: objectiou. Object N: objectiou. The detailed global image is global. where IMAGE is the full image. Task instructions are used to separate different tasks, like object detection and REC, and are detailed in Section A. Representing text queries as embeddings. Finally, we introduce local text token local text and global text token global text to represent text queries. The local text token is used to match object embeddings while the global text token is used to match global image embeddings. The encoding templates are Find an object that matches the given caption. CAPTION local text and Find an image that matches the given caption. CAPTION global text. We use the same model to encode text and visual embeddings. Efficiency Analysis. With the template described above, all objects in an image, as well as the full image, are encoded in single forward pass without the need for time-consuming autoregressive token prediction. Each object consumes only 8 tokens. When the full image is encoded into 1000 tokens, the total sequence length remains under 2000, requiring minimal GPU memory and enabling efficient acceleration with FlashAttention-2 (Dao, 2024). 3.2. Training Objective For each image, we annotate long image caption Clong, short image caption Cshort, and set of objects {Oi}M i=1, where each object Oi is associated with an object description obj (class names or REC-like descriptions) and bounding box Bi. The overall training objective comprises three components: region-level contrastive learning, image-level contrastive learning, and IoU regression, detailed as follows: Region-level contrastive learning. Unlike traditional contrastive language-image pre-training, where image-caption pairs are one-to-one matched, an object description may correspond to multiple instances, while some proposals remain unmatched due to missing annotations or low-quality bounding boxes. To handle this partial and many-to-one matching, we employ sigmoid focal loss (Lin et al., 2017) for superviType DET 380k Table 1. The overview of ObjEmbed training dataset. #Sample Dataset COCO (111k) (Lin et al., 2014), LVIS (94k) (Gupta et al., 2019) V3Det (175k) (Wang et al., 2023) RefCOCO/+/g (28k) (Kazemzadeh et al., 2014; Yu et al., 2016; Mao et al., 2016) FG-OVD (175k) (Bianchi et al., 2024), HumanRef (43k) (Jiang et al., 2025b) grefcoco (15k) (Liu et al., 2023), Ref-L4 (9k) (Chen et al., 2025) DAM (77k) (Lian et al., 2025), FineCops-Ref (29k) (Liu et al., 2024a) REIRCOCO (25k) (Hao et al., 2025), self-collected data (508k) REC 909k SUM 1289k sion, choice commonly adopted in object detection. Specifically, for each object description obj, we treat each region proposal pj as positive sample if IoU(pj, Bi) > 0.5, and negative otherwise. We compute the similarity between the proposal object embedding ej and the local text embedding of ei obj as: sij = ej ei ej pei , (1) Then, the similarities are optimized via sigmoid focal loss: Lregion = (cid:88) (cid:88) i=1 j=1 Lfocal (sij, yij) , (2) where yij = 1 if IoU(pj, Bi) > 0.5, and 0 otherwise. is the number of annotations and is the number of proposals. Image-level contrastive learning. In line with region-level contrastive learning, we also use sigmoid focal loss (Zhai et al., 2023) for image-level supervision Limage. Specifically, the first global image embedding is only supervised by short captions while the second global image embedding is only supervised by long captions. And we will collect captions from other GPUs to enlarge the negative samples. IoU regression. The IoU loss Liou is also formulated as sigmoid focal loss with ground truth IoUs between proposals and corresponding boxes as labels: Liou = + (cid:88) j= Lfocal (cid:0)ˆuj, (cid:1) , (3) where ˆuj is predicted IoU scores. Since not all objects in an image are annotated, Liou is only applied to + positive proposals. The total training objective is weighted combination: Ltotal = λ1Lregion + λ2Limage + λ3Liou, (4) where λ1, λ2, and λ3 are hyperparameters. 3.3. Training Data Construction To reduce annotation costs, we aggregate existing opensource object detection and referring expression comprehension datasets that provide region-level annotations. Due to limited data coverage, we further curate 300k images from 4 ObjEmbed: Towards Universal Multimodal Object Embeddings Table 2. Object detection results. Specialist detectors are typically designed for target tasks and trained on the target datasets. Method Specialist Detectors DINO-R50 (Zhang et al., 2023) GUIDED (Li et al., 2025a) Weak-to-Strong (Park et al., 2024) Open-vocabulary Detectors GLIP-T (Li et al., 2022b) OWLv2 (L/14) (Minderer et al., 2023) Grounding-DINO-T (Liu et al., 2024b) LLMDet-T (Fu et al., 2025c) WeDetect-B (Fu et al., 2025a) MLLMs VLM-FO1-3B (Liu et al., 2025c) LMM-Det-7B (Li et al., 2025b) WeDetect-Ref-2B (Fu et al., 2025a) WeDetect-Ref-4B (Fu et al., 2025a) Ours Qwen3-VL-2B (Bai et al., 2025a) Qwen3-VL-4B (Bai et al., 2025a) ObjEmbed-2B ObjEmbed-4B COCO COCO-O ODinW13 FG-OVD AP APs APm APl AP AP hard medium easy trivial full D3 Pres Abs 51.2 - - 46.1 37.9 47.9 54.9 52. 44.0 47.5 49.9 50.0 16.9 29.1 52.9 53.0 35.0 - - - 24.9 33.4 40.1 34.8 - 34.7 34.0 34.7 6.2 12.9 35.8 35. 54.3 - - - 41.2 51.2 58.3 57.1 - 51.8 58.0 57.6 20.7 33.9 59.7 59.6 65.3 - - - 53.7 62.2 68.6 69. - 60.3 68.9 69.2 36.4 54.2 72.5 72.2 - - - 29.0 42.7 37.6 36.1 44.1 - - 55.8 56.0 29.4 44.4 59.1 58. - - - 46.5 50.1 51.4 52.1 53.1 44.0 - 48.2 47.3 43.4 48.2 49.8 50.8 - 57.5 - - 25.4 17.0 15.0 - - - 28.7 31.1 59.3 62.0 65.7 66.1 - 69.5 - - 41.2 28.4 26.2 - - - 42.5 45.7 58.9 62.6 74.3 74. - 73.3 - - 42.8 31.0 23.8 - - - 48.1 50.1 53.8 64.0 77.3 77.2 - 72.6 - - 63.2 62.5 55.4 - - - 65.6 68.4 55.9 45.7 76.2 76.0 - - 30.8 19.1 22.8 20.7 17.2 - - - 41.8 42.0 16.0 24.3 39.4 40. - - 31.0 18.3 22.1 20.1 17.1 - - - 43.9 44.0 17.0 25.8 41.1 42.2 - - 30.4 21.5 24.7 22.5 17.6 - - - 35.4 35.8 11.2 19.7 34.2 34.4 SA-1B (Kirillov et al., 2023) and 200k images self-crawled from licensed websites. These images are first annotated with class-agnostic bounding boxes using WeDetect-Uni (Fu et al., 2025a), followed by generating unique instance-level descriptions for each object using Qwen3-VL-235B (Bai et al., 2025a). We find that the instance-level descriptions should be as unique as possible to minimize false-negative conflicts and can not include subjective content. Finally, each image is assigned both short and long caption, also generated by Qwen3-VL-235B. The prompts used for annotation are provided in Section B. The overall training data are summarized in Table 1, comprising 1.3M images and 8.1M bounding boxes. 4. Experiment 4.1. Implementation Details ObjEmbed is finetuned from Qwen3-VL-Instruct (Bai et al., 2025a) by first initializing the object projector following WeDetect-Ref (Fu et al., 2025a) and then training under the objective described in Equation (4). The model is trained using 16 GPUs, with batch size of 2 images per GPU, and an initial learning rate of 2e5. All parameters are updated during training except those in the frozen vision encoder. Training proceeds for two epochs. The loss weights λ1, λ2, and λ3 are set to 1.0, 1.0 and 0.25. Input images are resized such that the number of visual tokens ranges from 900 to 1200, corresponding to 900*32*32 to 1200*32*32 pixels, ensuring adaptive computation based on image content. 4.2. Comparisons on Fine-Grained Region-Level Tasks Results on object detection benchmarks. Object detection aims to simultaneously localize and classify all target objects within an image. The standard evaluation metric is mAP, computed over IoU thresholds ranging from 0.50 to 0.95, which places strong emphasis on localization accuracy. We evaluate on five benchmarks: COCO (Lin et al., 2014) for general object detection, COCO-O (Mao et al., 2023) for out-of-distribution detection, ODinW13 (Li et al., 2022a) for detection in the wild, FG-OVD (Bianchi et al., 2024) for fine-grained attribute-aware recognition, and D3 (Xie et al., 2023) for language-based object detection. As shown in Table 2, traditional detectors excel at precise localization and handling multi-object scenes, but their semantic understanding is limited to fixed class vocabularies with short category names. Consequently, their performance on FG-OVD and D3 is low. In contrast, multimodal large language models (MLLMs) demonstrate superior language comprehension and can interpret complex textual descriptions, yet suffer from poor localization accuracy due to coarse spatial reasoning, resulting in low performance on COCO and ODinW13. Equipped with the dual-token design, ObjEmbed achieves favorable balance between high semantic discriminability and precise location assessment. It understands rich language inputs, prioritizes well-localized predictions with higher scores, and is robust to domain variances, leading to consistently strong performance across all five benchmarks. Results on referring expression comprehension benchmarks. Referring expression comprehension aims to localize the unique object in an image that is described by natural language expression. This task demands finegrained multimodal reasoning, including deep linguistic understanding, interpretation of complex phrases, and contextual reasoning over spatial and semantic relationships among objects. The standard evaluation metric is accuracy at an IoU threshold of 0.5. We evaluate on three standard benchmarks: RefCOCO (Kazemzadeh et al., 2014), RefCOCO+ (Yu et al., 2016), and RefCOCOg (Mao et al., 2016). As shown in Table 3, ObjEmbed achieves an average accuObjEmbed: Towards Universal Multimodal Object Embeddings Table 3. Evaluation results on referring expression comprehension datasets. The evaluation metric is the Top-1 accuracy. Method Grounding-DINO-L (Liu et al., 2024b) CLARE-L (Hao et al., 2025) Qwen2.5-VL 3B (Bai et al., 2025b) Qwen2.5-VL 7B (Bai et al., 2025b) InternVL2.5-8B (Chen et al., 2024b) InternVL3.5-38B (Wang et al., 2025) Octopus 7B (Zhao et al., 2024) VLM-R1 3B (Shen et al., 2025) Rex-Omni 3B (Jiang et al., 2025a) ChatRex 7B (Jiang et al., 2024) VLM-FO1 3B (Liu et al., 2025b) RexSeek 7B (Jiang et al., 2025b) Qwen3-VL 2B (Bai et al., 2025a) Qwen3-VL 4B (Bai et al., 2025a) ObjEmbed-2B ObjEmbed-4B RefCOCO RefCOCO+ testA testB 88.2 93.2 90.0 92.0 84.0 91.7 85.4 92.5 94.5 85.9 89.0 91.8 83.4 92.6 85.2 92.3 82.8 89.5 87.0 94.1 87.6 93.7 - - 83.1 91.0 86.7 92.2 88.3 93.5 90.2 94.3 val 82.8 80.1 82.4 84.2 85.2 87.5 83.6 84.2 79.6 89.8 86.4 - 78.6 82.9 83.4 86.1 testA testB 75.9 89.0 74.4 83.3 74.1 88.0 76.9 89.1 78.8 91.5 84.7 90.0 76.0 89.4 76.8 89.4 71.4 84.8 91.9 79.3 91.9 80.6 - - 70.4 85.2 75.6 89.4 76.6 90.1 80.6 91.4 val 90.6 91.4 89.1 90.0 90.3 90.3 89.0 90.1 86.6 91.0 91.1 - 88.2 90.7 91.7 92.5 RefCOCOg Avg. test val 87.0 86.1 86.7 86.3 85.7 85.2 87.2 87.2 87.6 86.7 89.9 89.7 86.3 84.3 86.8 85.6 86.2 85.3 90.0 89.8 88.3 88.9 84.4 84.0 85.0 84.7 87.7 87.3 88.6 88.6 90.7 89.9 86.6 85.5 85.0 86.6 87.6 89.1 85.6 86.3 83.3 89.1 88.6 - 83.3 86.6 87.6 89. Table 4. Evaluation results on local image retrieval datasets. The evaluation metric for SORCE-1K and REIRCOCO is Recall@1 while the evaluation metric for ILIAS is mAP@50, metric evaluating the top 50 predictions. Method CLIP ViT-L/14 SigLIP2 ViT-So/16 MetaCLIP2 ViT-H/14 FG-CLIP2 ViT-So/16 GME-2B GME-7B VLM2Vec-2B VLM2Vec-V2-2B QQMM-embed-v2-7B RzenEmbed-7B UME-R1-2B UME-R1-7B Qwen3-VL-Embedding-2B Qwen3-VL-Embedding-8B FG-CLIP2 ViT-So/16 (RoIAlign) ObjEmbed-2B ObjEmbed-4B SORCE-1K REIRCOCO ILIAS Avg. T2I 32.6 34.2 37.5 46.9 28.3 30.6 26.3 24.8 47.7 36.0 36.1 41.3 45.4 49.1 28.4 67.3 71.7 T2I 16.0 22.9 19.2 27.3 19.8 24.6 20.1 24.5 32.7 30.4 23.3 28.2 28.0 32.6 16.0 37.5 39.3 T2I 44.2 45.2 49.6 57.9 35.7 33.5 29.8 32.0 44.1 39.2 31.6 37.6 47.3 51.2 53.9 76.5 77.6 I2I 39.9 55.4 42.6 64.1 44.9 40.4 39.7 36.4 48.0 42.0 38.9 41.7 57.9 59.2 72.5 84.0 85.3 33.2 39.4 37.2 49.1 32.2 32.3 29.0 29.4 43.1 36.9 32.5 37.2 44.7 48.0 42.7 66.3 68.5 racy of 89.5, surpassing both specialized MLLMs designed for referring tasks and significantly larger general-purpose multimodal models. This strong performance demonstrates the high semantic discriminability of our object embeddings. Results on local image retrieval benchmarks. Local image retrieval aims to retrieve target image from gallery based on query that describes only small region or specific object within it. The query can be textual description (text-to-image, T2I) or an image exemplar (image-to-image, I2I), both requiring fine-grained cross-modal alignment between local regions and external queries. For text-based retrieval (T2I), we evaluate on three benchmarks: SORCE1K (Liu et al., 2025a), REIRCOCO (Hao et al., 2025), and ILIAS (Kordopatis-Zilos et al., 2025). On REIRCOCO and ILIAS, we adopt simplified evaluation protocol (detailed in Section C). The evaluation metric is Recall@1 for SORCE-1K and REIRCOCO, and mAP@50 for ILIAS. 6 These metrics assess whether the correct target images are ranked highly in the retrieval results, without considering bounding box localization accuracy. In our framework, we compute the overall image similarity by taking the maximum matching score among all objects. As shown in Table 4, we compare ObjEmbed with other global embedding models that represent the entire image as single, holistic feature vector. However, such global representations suffer from semantic ambiguity and fail to capture fine-grained details, leading to poor performance in local retrieval tasks. Even FG-CLIP2 (Xie et al., 2025a), which incorporates regional contrastive learning, underperforms significantly in T2I retrieval when using object embeddings extracted by RoIAlign with the same object proposals and scoring strategy as ours. We hypothesize that this is due to misalignment between region features and complex queries. In contrast, our model represents each object with fine-grained details and gets an average score of 68.5, surpassing other models by around 20 points. For the image-based retrieval (I2I) task, we use global image embeddings to represent image exemplars and to retrieve target images. Surprisingly, although our model does not optimize for image-based retrieval tasks, the model aligns global text embeddings and global image embeddings in unified semantic space. Therefore, it can transfer from text-based retrieval tasks to image-based retrieval tasks successfully. 4.3. Comparisons on Image-Level Tasks In Table 5, we evaluate ObjEmbed on traditional imagetext retrieval benchmarks, including long text retrieval (ShareGPT4V (Chen et al., 2024a) and DCI (Urbanek et al., 2024)), short text retrieval (COCO (Chen et al., 2015) and Flickr30K (Young et al., 2014)), and multilingual text retrieval (COCO-CN (Li et al., 2019) and Flikr30K-CN (Lan et al., 2017)). Despite using relatively small-scale trainObjEmbed: Towards Universal Multimodal Object Embeddings Table 5. Evaluation results on global image retrieval datasets. The evaluation metrics are Recall@1. Long Image Captions DCI Short Image Captions COCO Method CLIP ViT-L/14 (Radford et al., 2021) EVA-CLIP ViT-L/14 (Sun et al., 2023) SigLIP2 ViT-So/16 (Tschannen et al., 2025) MetaCLIP2 ViT-H/14 (Chuang et al., 2025) FG-CLIP2 ViT-So/16 (Xie et al., 2025a) GME-2B (Zhang et al., 2025) GME-7B (Zhang et al., 2025) VLM2Vec-2B (Jiang et al., 2025c) VLM2Vec-V2-2B (Meng et al., 2025) UME-R1-2B (Lan et al., 2025) Qwen3-VL-Embedding-2B (Li et al., 2026) ObjEmbed-2B ObjEmbed-4B ShareGPT4V T2I I2T 83.6 86.5 89.4 91.5 79.5 78.6 89.2 93.9 96.7 97.5 91.7 92.8 92.2 89.3 74.3 77.7 92.4 92.1 93.9 92.9 97.8 96.7 97.3 97.1 97.7 97. I2T 37.2 47.2 46.0 53.0 70.6 52.9 59.8 33.2 53.1 59.8 77.9 74.5 77.2 T2I 36.4 47.8 47.1 50.2 72.1 58.8 64.6 44.2 65.3 67.9 79.7 74.6 76.9 I2T 58.0 64.2 71.0 66.8 74.6 67.3 67.8 53.5 62.6 71.4 69.6 75.2 75.7 T2I 37.1 47.9 55.8 47.7 56.7 51.7 55.6 39.2 50.3 54.4 55.2 51.0 52.2 Flickr30K T2I I2T 67.3 87.4 77.9 89.2 82.5 94.1 77.0 91.9 85.0 95.9 75.1 88.0 81.1 91.1 68.5 83.0 80.3 89.2 79.1 91.6 81.9 92.9 80.0 94.2 80.4 94.2 Multilingual Image Captions Avg. COCO-CN Flickr30K-CN I2T - - 72.0 80.1 83.2 79.7 81.7 67.9 74.4 83.6 83.0 86.0 87. I2T - - 78.4 89.3 91.5 85.5 91.8 76.1 84.0 87.9 91.9 94.0 94.7 T2I - - 51.7 72.2 77.2 71.1 79.1 56.9 67.1 75.0 78.6 76.1 77.3 T2I - - 50.7 63.1 68.1 66.7 71.8 52.6 60.3 70.1 72.2 66.7 68.8 - - 67.3 72.9 80.8 73.4 77.2 60.6 72.6 77.3 81.5 80.6 81.7 Table 6. Ablation studies on different object token designs. Method AP single token & label=1 37.1 single token & label=IoU 42.3 45.1 two tokens (iou+cls) 45.5 two tokens (cls+iou) COCO APs APm APl 52.8 47.9 28.8 60.9 50.9 29.5 67.1 51.1 27.1 66.6 51.4 27.3 RefCOCO Avg. 86.8 87.1 86.8 86.6 Table 7. Ablation studies on instructions. Method None object instruction object & task instructions COCO AP 42.1 45.5 47.1 APs APm APl 63.1 52.5 28.0 66.6 51.4 27.3 67.1 55.6 32.1 RefCOCO Avg. 86.9 86.6 86.9 ing set, ObjEmbed achieves an overall score of 81.7 points, which is highly competitive with both traditional CLIPstyle models and recent LMM-based embedding approaches. Moreover, it demonstrates robust generalization across varying text lengths and multiple languages. 4.4. Ablation Study In this subsection, we explore the effects of different designs with the 4B model, partial data, and one-epoch training schedule. Ablation studies on object token design. As objects are sensitive to localization quality, an object embedding model should have the ability to assess the quality of boxes. To achieve the goal, we change classification labels in sigmoid focal loss to box IoUs, which increases 5.2% mAP on COCO, as shown in Table 6. However, the classification and box localization may have conflicting training objectives. We find that decoupling an object into two tokens, one for classification and the other for IoU regression, further improves 3.2% mAP on COCO. And placing the classification token in front of the IoU token is slightly better. Since REC places less emphasis on precise localization, its performance is relatively robust to the choice of IoU design. Table 8. Ablation studies on different training objectives. LIR denotes the average scores of local image retrieval tasks except for the I2I task. GIR denotes the average scores of global image retrieval tasks. Method object-level image-level object-level & image-level COCO RefCOCO LIR GIR 53.0 - 52. 60.2 - 62.9 88.1 - 87.4 - 81.2 81.6 Table 9. Ablation studies on image-level supervision design. Share denotes whether global text embeddings and local text embeddings share the same special token. #token denotes the number of global image tokens and Type can be single long caption (long), single short caption (short), single randomly selected caption (mix), or two captions used simultaneously (both). LIR denotes the average scores of local image retrieval tasks. GIR denotes the average scores of global image retrieval tasks. Share Exp. 1 2 3 4 5 #token Type COCO RefCOCO LIR GIR 80.1 80.1 72.7 80.2 80.6 81.6 mix mix long short both both 67.3 67.6 67.2 67.4 67.5 68.6 86.0 86.8 86.1 86.3 86.6 87.4 52.4 52.7 52.6 52.6 52.8 52.8 1 1 1 1 1 Ablation studies on instructions. As shown in Table 7, we study the effects of instructions. As we encode multiple objects simultaneously, using the object instruction (Object i: objectiou.) to separate different objects can increase the distinctiveness of each object and increase 3.4% mAP on COCO. Further, different tasks focus on different aspects of objects. Object detection focuses on the shared properties within class while referring expression comprehension requires encoding instance-specific features. Incorporating task instructions to guide the model encoding different features for different tasks is beneficial. Task prompts used in this work are shown in Section A. Ablation studies on different training objectives. In this work, we build universal object embedding model along with the global image representation ability. In Table 8, 7 ObjEmbed: Towards Universal Multimodal Object Embeddings Table 10. Effect of the quality of proposals. mix denotes setting where portion of the generated object proposals are randomly replaced with ground truth bounding boxes. We report fixed AP (Dave et al., 2021) on LVIS. AR is computed as the mean recall across IoU thresholds ranging from 0.50 to 0.95. mix AR 66.7 100.0 COCO LVIS v1 val AP 53.0 65.2 APs APm APl 72.2 59.6 35.6 77.5 70.1 55.3 AR 50.8 100. AP 49.0 66.6 APs APm APl 45.5 50.2 53.7 65.9 67.1 66.7 we find that incorporating an image-level training objective with an object-level training objective can increase the local image retrieval performance by 2.7 points while maintaining the performance on object detection and REC. Further, comparing with single image-level training, training with both objectives can also boost the global image retrieval performance by 0.4 points, demonstrating the mutual benefits between the two training objectives. Ablation studies on image-level supervision design. In this work, we use both local and global text embeddings and long and short captions as global text embeddings to learn global image embeddings. We study their effects in Table 9. As local text embeddings match with object embeddings while global text embeddings match with global image embeddings, we find that not sharing the text tokens can mitigate task discrepancies and get higher performance on COCO (+0.3% mAP) and RefCOCO (+0.8), comparing Exp1 and Exp2. Further, using only short captions, long captions, or mixture of them can not handle complex retrieval requirements. Using both captions for supervision achieves the highest 80.6 global retrieval results (Exp2-Exp5). Finally, using two global image tokens (Exp6), one for short captions and one for long captions, gets the highest results. And we find that the performance on object detection and REC is quite robust to the image-level designs. 4.5. Discussion How does proposal quality affect performance? In ObjEmbed, we employ state-of-the-art proposal generator, WeDetect-Uni (Fu et al., 2025a), to produce 100 object proposals per image. Therefore, the recall rate is essential as the model can not encode missing objects. As shown in Table 10, WeDetect-Uni achieves an Average Recall (AR) of 66.7 on COCO and 50.8 on LVISv1 val (Gupta et al., 2019). To assess the upper bound, we conduct an oracle experiment in which ground truth bounding boxes are randomly mixed into the generated proposals (denoted as mix in the table). With access to ground truth regions, ObjEmbed achieves significant gain of 12.2% AP on COCO and 17.6% AP on LVIS, demonstrating that (1) the model can accurately align object proposals with corresponding text descriptions, and (2) it effectively ranks high-quality proposals above low-quality ones during matching. These results indicate Table 11. Ablation studies on supporting box regression. box regression COCO RefCOCO LIR GIR 81.5 81.6 68.1 68.6 86.2 87.4 52.5 52.8 that the representation learning in ObjEmbed is orthogonal to proposal quality but its overall performance is still limited by the recall rate of proposals. Therefore, improving proposal quality through fine-tuning the proposal network on target datasets or leveraging human-annotated bounding boxes can further boost performance. Can we directly use ObjEmbed to regress high-quality bounding boxes, similar to object detectors? Given the importance of proposal quality, natural question arises: can the embedding model itself be leveraged to refine proposals by predicting bounding box offsets, thereby improving localization accuracy? To investigate this, we conduct experiments where the model uses the IoU embeddings to predict both IoU scores and bounding box offsets simultaneously. The offset regression head is trained with combination of L1 loss and IoU loss, following the standard practice in DETR (Carion et al., 2020). Similar to IoU regression loss, the bounding box regression loss is applied only to positive proposals with an IoU greater than 0.5. As result, the regression head can refine existing bounding boxes but is unable to generate missing ones. As shown in Table 11, we find that incorporating box regression degrades overall performance, possibly due to learning conflicts. 5. Conclusion In this work, we present ObjEmbed, novel MLLM-based object embedding model that features object-oriented representation, versatility, and efficient encoding. In our framework, each object is represented by two complementary embeddings: an object embedding for semantic matching and an IoU embedding for assessing localization quality. This decoupled design reduces learning complexity while maintaining encoding efficiency. ObjEmbed can be seamlessly applied to wide range of downstream tasks, including object detection, referring expression comprehension, local image retrieval, and global image retrieval. The consistently high and balanced performance across 18 diverse benchmarks demonstrates the effectiveness and generalization capability of our approach."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 8 ObjEmbed: Towards Universal Multimodal Object Embeddings"
        },
        {
            "title": "References",
            "content": "Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025a. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Bianchi, L., Carrara, F., Messina, N., Gennaro, C., and Falchi, F. The devil is in the fine-grained details: Evaluating open-vocabulary object detectors for fine-grained understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2252022529, 2024. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. End-to-end object detection with transformers. In European conference on computer vision, pp. 213229. Springer, 2020. Chen, J., Wei, F., Zhao, J., Song, S., Wu, B., Peng, Z., Chan, S.-H. G., and Zhang, H. Revisiting referring expression comprehension evaluation in the era of large multimodal In Proceedings of the Computer Vision and models. Pattern Recognition Conference, pp. 513524, 2025. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pp. 370387. Springer, 2024a. Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollar, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Chuang, Y.-S., Li, Y., Wang, D., Yeh, C.-F., Lyu, K., Raghavendra, R., Glass, J., Huang, L., Weston, J., Zettlemoyer, L., et al. Meta clip 2: worldwide scaling recipe. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Dave, A., Dollar, P., Ramanan, D., Kirillov, A., and Girshick, R. Evaluating large-vocabulary object detectors: The devil is in the details. arXiv preprint arXiv:2102.01066, 2021. Du, P., Wang, Y., Sun, Y., Wang, L., Liao, Y., Zhang, G., Ding, E., Wang, Y., Wang, J., and Liu, S. Lami-detr: Open-vocabulary detection with language model instruction. In European Conference on Computer Vision, pp. 312328. Springer, 2024. Fu, S., Yan, J., Yang, Q., Wei, X., Xie, X., and Zheng, W.-S. Frozen-detr: Enhancing detr with image understanding from frozen foundation models. In Advances in Neural Information Processing Systems, volume 37, pp. 105949 105971, 2024. Fu, S., Su, Y., Rao, F., Lyu, J., Xie, X., and Zheng, W.- S. Wedetect: Fast open-vocabulary object detection as retrieval. arXiv preprint arXiv:2512.12309, 2025a. Fu, S., Yan, J., Yang, Q., Wei, X., Xie, X., and Zheng, W.-S. hierarchical semantic distillation framework for open-vocabulary object detection. IEEE Transactions on Multimedia, 2025b. Fu, S., Yang, Q., Mo, Q., Yan, J., Wei, X., Meng, J., Xie, X., and Zheng, W.-S. Llmdet: Learning strong openvocabulary object detectors under the supervision of large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1498714997, 2025c. Gu, X., Lin, T.-Y., Kuo, W., and Cui, Y. Open-vocabulary object detection via vision and language knowledge distillation. International Conference on Learning Representations, 2022. Gupta, A., Dollar, P., and Girshick, R. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 53565364, 2019. Hao, X., Zhu, K., Guo, H., Guo, H., Jiang, N., Lu, Q., Tang, M., and Wang, J. Referring expression instance retrieval and strong end-to-end baseline. In Proceedings of the 33rd ACM International Conference on Multimedia, pp. 44644473, 2025. Jiang, Q., Luo, G., Yang, Y., Xiong, Y., Chen, Y., Zeng, Z., Ren, T., and Zhang, L. Chatrex: Taming multimodal llm 9 ObjEmbed: Towards Universal Multimodal Object Embeddings for joint perception and understanding. arXiv preprint arXiv:2411.18363, 2024. Jiang, Q., Huo, J., Chen, X., Xiong, Y., Zeng, Z., Chen, Y., Ren, T., Yu, J., and Zhang, L. Detect anything via next point prediction. arXiv preprint arXiv:2510.12798, 2025a. Jiang, Q., Wu, L., Zeng, Z., Ren, T., Xiong, Y., Chen, Y., In Qin, L., and Zhang, L. Referring to any person. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025b. Jiang, Z., Meng, R., Yang, X., Yavuz, S., Zhou, Y., and Chen, W. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. In International Conference on Learning Representations, 2025c. Kazemzadeh, S., Ordonez, V., Matten, M., and Berg, T. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 787798, 2014. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Kordopatis-Zilos, G., Stojnic, V., Manko, A., Suma, P., Ypsilantis, N.-A., Efthymiadis, N., Laskar, Z., Matas, J., Chum, O., and Tolias, G. Ilias: Instance-level image retrieval at scale. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1477714787, 2025. Lan, W., Li, X., and Dong, J. Fluency-guided cross-lingual image captioning. In Proceedings of the 25th ACM international conference on Multimedia, pp. 15491557, 2017. Lan, Z., Niu, L., Meng, F., Zhou, J., and Su, J. Umer1: Exploring reasoning-driven generative multimodal embeddings. arXiv preprint arXiv:2511.00405, 2025. Li, C., Liu, H., Li, L., Zhang, P., Aneja, J., Yang, J., Jin, P., Hu, H., Liu, Z., Lee, Y. J., et al. Elevater: benchmark and toolkit for evaluating language-augmented visual models. In Advances in Neural Information Processing Systems, volume 35, pp. 92879301, 2022a. Li, J., Liang, Z., Chen, W., Ma, L., and Li, G. Guided: Granular understanding via identification, detection, and discrimination for fine-grained open-vocabulary object In The Thirty-ninth Annual Conference on detection. Neural Information Processing Systems, 2025a. Li, J., Xie, C., Ao, J., Leng, D., and Yin, Y. Lmm-det: Make large multimodal models excel in object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 308318, 2025b. Li, L. H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L., Hwang, J.-N., et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1096510975, 2022b. Li, M., Zhang, Y., Long, D., Chen, K., Song, S., Bai, S., Yang, Z., Xie, P., Yang, A., Liu, D., et al. Qwen3vl-embedding and qwen3-vl-reranker: unified framework for state-of-the-art multimodal retrieval and ranking. arXiv preprint arXiv:2601.04720, 2026. Li, X., Xu, C., Wang, X., Lan, W., Jia, Z., Yang, G., and Xu, J. Coco-cn for cross-lingual image tagging, captioning, and retrieval. IEEE Transactions on Multimedia, 21(9): 23472360, 2019. Lian, L., Ding, Y., Ge, Y., Liu, S., Mao, H., Li, B., Pavone, M., Liu, M.-Y., Darrell, T., Yala, A., et al. Describe anything: Detailed localized image and video captioning. In Proceedings of the IEEE/CVF international conference on computer vision, 2025. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740755. Springer, 2014. Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollar, P. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 29802988, 2017. Liu, C., Ding, H., and Jiang, X. Gres: Generalized referring expression segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2359223601, 2023. Liu, C., Xie, C., Chen, X., Li, W., Zhu, F., Zhao, R., and Wang, L. Sorce: Small object retrieval in complex environments. arXiv preprint arXiv:2505.24441, 2025a. Liu, J., Yang, X., Li, W., and Wang, P. Finecops-ref: new dataset and task for fine-grained compositional arXiv preprint referring expression comprehension. arXiv:2409.14750, 2024a. Liu, P., Shen, H., Fang, C., Sun, Z., Liao, J., and Zhao, T. Vlm-fo1: Bridging the gap between high-level reasoning and fine-grained perception in vlms. arXiv preprint arXiv:2509.25916, 2025b. 10 ObjEmbed: Towards Universal Multimodal Object Embeddings Liu, P., Shen, H., Fang, C., Sun, Z., Liao, J., and Zhao, T. Vlm-fo1: Bridging the gap between high-level reasoning and fine-grained perception in vlms. arXiv preprint arXiv:2509.25916, 2025c. Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Jiang, Q., Li, C., Yang, J., Su, H., et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pp. 3855. Springer, 2024b. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L., and Murphy, K. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1120, 2016. Mao, X., Chen, Y., Zhu, Y., Chen, D., Su, H., Zhang, R., and Xue, H. Coco-o: benchmark for object detectors under natural distribution shifts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 63396350, 2023. Meng, R., Jiang, Z., Liu, Y., Su, M., Yang, X., Fu, Y., Qin, C., Chen, Z., Xu, R., Xiong, C., et al. Vlm2vec-v2: Advancing multimodal embedding for videos, images, and visual documents. arXiv preprint arXiv:2507.04590, 2025. Minderer, M., Gritsenko, A., and Houlsby, N. Scaling open-vocabulary object detection. In Advances in Neural Information Processing Systems, volume 36, pp. 72983 73007, 2023. Park, K., Saito, K., and Kim, D. Weak-to-strong compositional learning from generative models for languagebased object detection. In European Conference on Computer Vision, pp. 119. Springer, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PmLR, 2021. Shen, H., Liu, P., Li, J., Fang, C., Ma, Y., Liao, J., Shen, Q., Zhang, Z., Zhao, K., Zhang, Q., et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Sun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Evaclip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. Tschannen, M., Gritsenko, A., Wang, X., Naeem, M. F., Alabdulmohsin, I., Parthasarathy, N., Evans, T., Beyer, L., Xia, Y., Mustafa, B., et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Urbanek, J., Bordes, F., Astolfi, P., Williamson, M., Sharma, V., and Romero-Soriano, A. picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26700 26709, 2024. Wang, J., Zhang, P., Chu, T., Cao, Y., Zhou, Y., Wu, T., Wang, B., He, C., and Lin, D. V3det: Vast vocabulary visual detection dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 19844 19854, 2023. Wang, W., Gao, Z., Gu, L., Pu, H., Cui, L., Wei, X., Liu, Z., Jing, L., Ye, S., Shao, J., et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. Wei, Z., Wang, G., Ma, X., Mei, K., Chen, H., Jin, Y., and Rao, F. Hq-clip: Leveraging large vision-language models to create high-quality image-text datasets and clip models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2244722456, 2025. Wu, S., Zhang, W., Jin, S., Liu, W., and Loy, C. C. Aligning bag of regions for open-vocabulary object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1525415264, 2023. Xie, C., Zhang, Z., Wu, Y., Zhu, F., Zhao, R., and Liang, S. Described object detection: Liberating object detection with flexible expressions. In Advances in Neural Information Processing Systems, volume 36, pp. 7909579107, 2023. Xie, C., Wang, B., Kong, F., Li, J., Liang, D., Ao, J., Leng, D., and Yin, Y. Fg-clip 2: bilingual finegrained vision-language alignment model. arXiv preprint arXiv:2510.10921, 2025a. Xie, C., Wang, B., Kong, F., Li, J., Liang, D., Zhang, G., Leng, D., and Yin, Y. Fg-clip: Fine-grained visual and textual alignment. In International conference on machine learning, 2025b. Xu, H., Xie, S., Tan, X. E., Huang, P.-Y., Howes, R., Sharma, V., Li, S.-W., Ghosh, G., Zettlemoyer, L., and FeichtIn International enhofer, C. Demystifying clip data. Conference on Learning Representations, 2024. Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the association for computational linguistics, 2:6778, 2014. 11 ObjEmbed: Towards Universal Multimodal Object Embeddings Yu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L. In EuroModeling context in referring expressions. pean conference on computer vision, pp. 6985. Springer, 2016. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L. M., and Shum, H.-Y. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. In International Conference on Learning Representations, 2023. Zhang, K., Luan, Y., Hu, H., Lee, K., Qiao, S., Chen, W., Su, Y., and Chang, M.-W. Magiclens: Self-supervised image retrieval with open-ended instructions. In International conference on machine learning, 2024. Zhang, X., Zhang, Y., Xie, W., Li, M., Dai, Z., Long, D., Xie, P., Zhang, M., Li, W., and Zhang, M. Gme: Improving universal multimodal retrieval by multimodal llms. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2025. Zhao, C., Song, Y., Chen, J., Rong, K., Feng, H., Zhang, G., Ji, S., Wang, J., Ding, E., and Sun, Y. Octopus: multi-modal llm with parallel recognition and sequential understanding. In Advances in Neural Information Processing Systems, 2024. ObjEmbed: Towards Universal Multimodal Object Embeddings A. Details of Task Instructions ObjEmbed is versatile embedding model applicable to wide range of downstream tasks. However, different tasks emphasize distinct aspects of object representation. For instance, object detection relies on shared semantic properties within object categories, whereas referring expression comprehension requires fine-grained, instance-specific features to distinguish between visually similar objects. To address these divergent requirements and mitigate potential task conflicts, we introduce task-specific instructions to guide the model in generating context-aware embeddings tailored to each task. The instructions used during training are listed as follows: Object Detection Detect all objects in the image by identifying the common visual features of their respective classes. Localize each object by matching it to the archetypal visual form of its category. Detect all objects in the image by recognizing the shared visual attributes of their respective categories. Identify every object in the scene based on the core visual characteristics that define its class. Locate all objects by using the fundamental visual properties common to their object class. Perform object detection by referencing the shared visual patterns that characterize each class. Find every object in the picture by matching it to the defining visual traits of its category. Identify all objects present by their class-defining visual features, which are common across all instances. Detect every object based on the visual essence shared by all members of its class. Localize each object in the image according to the general visual blueprint of its category. Identify all objects by applying the common visual criteria that define their respective classes. Referring expression comprehension Locate the specific object being described by analyzing its unique instance-level attributes, its spatial position, and its relationship with surrounding objects. Identify the single instance mentioned in the text by considering its distinct visual features, its location within the scene, and its context relative to nearby items. Ground the referring expression by pinpointing the object that matches the descriptions details regarding its appearance, placement, and interaction with other elements. Find the objects that correspond to the given description, paying close attention to its specific details, its position, and how it relates to its neighbors. Disambiguate and find the correct object by carefully examining the provided description of its instance-specific properties, its coordinates in the image, and its spatial arrangement with other objects. Resolve the reference by identifying the object that uniquely matches the specified details, including its appearance, its place in the scene, and its connections to adjacent objects. Pinpoint the described instance by evaluating its specific visual traits, its spatial context, and its relational properties with other objects in the image. To locate the referred object, you must analyze three things from the description: 1) its unique visual details (e.g., color, texture), 2) its precise location, and 3) its relationship to the objects around it. 13 ObjEmbed: Towards Universal Multimodal Object Embeddings Find the specific object the text is referring to by synthesizing information about its individual characteristics, its location, and its interactions within the scene. Celebrity Identify the famous person depicted in this image. Recognize and name the public figure featured in this picture. Please provide the name of the well-known individual in this image. Identify all recognizable celebrities in this image. State the name of the celebrity shown. B. Details of Dataset Annotation To train ObjEmbed, each image in the training dataset is annotated with long caption, short caption, and several regions of interest, where each region is associated with corresponding object description. To minimize false-negative conflicts during contrastive learning, captions are designed to be as diverse and distinctive as possible. Furthermore, they should exclude subjective or interpretive content to ensure objectivity and consistency. To achieve high-quality annotations, we carefully design structured prompts and employ state-of-the-art multimodal large language model, Qwen-VL-235B (Bai et al., 2025a), as the automated annotator. The annotation prompts are as follows: The prompt for annotating region-level captions The full image is FULL IMAGE The cropped object is at [x1, y1, x2, y2], CROP IMAGE Analyze the given image and generate multiple detailed descriptions for the given object (instance) found in the image. Each description must be accurate and unique, focusing solely on the information available in the image and annotations. Do not include any information that is not explicitly present in the image or annotation data. The descriptions should ensure that the object can be uniquely identified from large set of similar images. **Description Generation**: 1. Generate concise, clear descriptions. 2. Focus mainly on the object itself using: - The objects inherent properties. - The special details that can be used for separating other instances of the same category. - Ensure each description allows the object to be uniquely identifiable within the image. - Ensure diversity without referencing prior descriptions. - Avoid direct mention of coordinate values. 3. For instances that are heavily occluded, blurry, or too small to be recognized due to tiny bounding box, directly return Instance quality is poor. **Description Style**: 1. Use short sentences. 2. Minimize commas, avoid long or complex sentences. 3. Each description must reflect the interesting, accurate, and clear representation of the object, emphasizing the object as the focal point. 4. Each description should be more natural and aligned with human language conventions. 5. Each description must use the described object as the subject of the sentence. Output the descriptions in JSON format. 14 ObjEmbed: Towards Universal Multimodal Object Embeddings The prompt for annotating image-level captions The full image is FULL IMAGE You are precise, factual image cataloger. Your task is to generate literal description of the image for visual database. You should generate **short caption** and **long caption** for each image. For short captions, follow these rules strictly: 1. **Identify Core Elements:** Describe the primary entities, objects, and the surrounding environment. 2. **Be Concise:** The entire description must be single, clear sentence or phrase under 30 words. 3. **Be Natural:** Each description should be more natural and aligned with human language conventions. For long captions, follow these rules strictly: 1. **Include Key Details:** Mention essential visual attributes like color, count, spatial relationships (e.g., on the left, in the background), and relationships between objects. 2. **Be Objective:** Describe only what you can see. Strictly avoid any subjective language, atmosphere (e.g., peaceful, sad), or interpretation of intent, actions, or the purpose of objects. 3. **Be Concise:** The entire description must be under 100 words but more than 50 words. 4. **Be Natural:** Each description should be more natural and aligned with human language conventions. Sentences need to be smooth and coherent. Describe the image and output the descriptions in JSON format. C. Details of Local Image Retrieval Benchmarks Local image retrieval is challenging task in which the textual or visual query corresponds to only small region or specific object within an image, rather than the entire scene. In this work, we evaluate on three established benchmarks: SORCE-1K (Liu et al., 2025a) comprises 1,023 carefully curated images with complex backgrounds and textual queries that describe less prominent small objects with minimum surrounding context. The target objects typically occupy less than 10% of the image area, posing significant challenges for global image embedding models that focus on holistic scene understanding. Each query is associated with exactly one positive image, and performance is evaluated using Recall@1. REIRCOCO (Hao et al., 2025) consists of 4,994 images from the COCO dataset, each annotated with referring expression describing specific object. The original evaluation protocol requires both image retrieval and object localization. However, since global embedding models lack localization capabilities, we adapt the protocol to standard text-to-image retrieval setting, where the goal is to retrieve the correct image containing the described object. We report performance using Recall@1. ILIAS (Kordopatis-Zilos et al., 2025) supports both text-based and image-based local retrieval, with queries formulated as natural language description or large image exemplar. The original dataset includes 5 million distractors, making full-scale evaluation computationally infeasible. Instead, we construct manageable gallery using all 4,715 positive images. Different from the benchmarks mentioned above, ILIAS contains only 1,232 queries, each potentially matching multiple positive images. We evaluate using mAP@50, which computes mean average precision over the top 50 retrieved results. D. Limitation As pioneering MLLM-based object embedding model, ObjEmbed can be further improved in the following directions: Scaling up training data: Due to resource constraints, we currently train on only 1.3M samples, significantly fewer than those used in CLIP-series models. Scaling up the pretraining data through broader data collection could enhance model performance. Hard negative mining: Hard negatives are essential for learning discriminative embeddings. However, effective mining must be balanced with the mitigation of the false negative problem. Integrating robust hard negative sampling strategies 15 ObjEmbed: Towards Universal Multimodal Object Embeddings while accounting for annotation incompleteness can further boost performance. E. Visualization Visualizations of referring expression comprehension results. In addition to strong performance on standard referring benchmarks (e.g., RefCOCO, RefCOCO+, RefCOCOg), in Figure 3, we demonstrate that ObjEmbed exhibits not only strong OCR capabilities (a, b, d), but also commonsense reasoning (c) and image-image matching abilities (e, f), highlighting its generalizability and versatility. Visualizations of local image retrieval results. In Figure 4, we present qualitative results for three queries from the SORCE-1K dataset, showing the top-3 retrieved images. Our ObjEmbed not only ranks the correct target images as the top result but also accurately localizes the queried objects within the images. In contrast, even the state-of-the-art global image embedding model, Qwen-VL-Embedding-8B (Li et al., 2026), fails to retrieve the correct images in these challenging cases, highlighting the limitations of holistic representations in capturing fine-grained, localized visual content. Visualizations of self-annotated data. Figure 5 shows representative examples from our self-annotated dataset. Thanks to carefully designed prompts and the use of frontier MLLMs, the generated captions are both accurate and highly distinctive. 16 ObjEmbed: Towards Universal Multimodal Object Embeddings Figure 3. Visualizations of referring expression comprehension results with text queries and image queries. 17 ObjEmbed: Towards Universal Multimodal Object Embeddings Figure 4. Visualizations of retrieval results on SORCE-1K. Our ObjEmbed successfully ranks the target image as the top result and accurately localizes the target objects (highlighted with red bounding boxes). In contrast, global image embedding models, like Qwen3VL-Embedding 8B, tend to overlook small objects. 18 ObjEmbed: Towards Universal Multimodal Object Embeddings Figure 5. Visualizations of self-annotated data. Each image is annotated with high-quality image-level and object-level captions. Images come from SA-1B (Kirillov et al., 2023)."
        }
    ],
    "affiliations": [
        "Guangdong Province Key Laboratory of Information Security Technology, China",
        "Independent Researcher",
        "Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",
        "Pazhou Laboratory (Huangpu), China",
        "Peng Cheng Laboratory, China",
        "School of Computer Science and Engineering, Sun Yat-sen University, China"
    ]
}