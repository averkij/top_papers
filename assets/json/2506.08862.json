{
    "paper_title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams",
    "authors": [
        "Zike Wu",
        "Qi Yan",
        "Xuanyu Yi",
        "Lele Wang",
        "Renjie Liao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat."
        },
        {
            "title": "Start",
            "content": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams Zike Wu1,2 Qi Yan1,2 Xuanyu Yi4 Lele Wang1 Renjie Liao1,2,3 1University of British Columbia 2Vector Institute for AI 3Canada CIFAR AI Chair 4Nanyang Technological University 5 2 0 2 0 1 ] . [ 1 2 6 8 8 0 . 6 0 5 2 : r {zikewu, qi.yan, lelewang, rjliao}@ece.ubc.ca, xuanyu001@e.ntu.edu.sg Figure 1: Given an uncalibrated video stream, our StreamSplat performs instant reconstruction of dynamic 3D Gaussian scene in an online manner, enabling video reconstruction and interpolation, depth estimation, and novel view synthesis."
        },
        {
            "title": "Abstract",
            "content": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining longterm stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github. com/nickwzk/StreamSplat. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Real-time reconstruction of dynamic 3D scenes from video streams is crucial for numerous applications, e.g., robotics [1, 2], augmented/virtual reality (AR/VR) [3, 4], and autonomous driving [5]. AR/VR systems rely on accurate, continuously updated 3D models to deliver immersive user experiences, while robots and autonomous vehicles require real-time representations of dynamic environments for safe navigation and responsive interactions. These applications demand robust online reconstruction systems capable of accurately capturing evolving scene geometry and appearance. Despite growing research interest, real-time dynamic 3D reconstruction from uncalibrated video remains largely unsolved. Although SLAM-based [6, 7, 8, 9] and scene-coordinate-based methods [10, 11, 12, 13] could effectively estimate camera poses and reconstruct static scenes, their extension to dynamic scenes is often hindered by the challenge of disentangling camera and object motion from uncalibrated inputs. As result, they often require computationally intensive post-optimization of camera poses or scene representations [6, 14], limiting their real-time applicability. Meanwhile, feed-forward methods [15, 16, 17] are promising for static scene reconstruction from uncalibrated inputs. Recent work [18] has begun to explore their potential for dynamic scene reconstruction. However, such methods typically treat dynamic scenes as sequence of static ones, ignoring temporal coherence and failing to explicitly model scene dynamics. Moreover, many require access to entire video sequences during inference [18, 19], making them unsuitable for online streaming scenarios. Building such an online 3D reconstruction system for dynamic scenes introduces three fundamental challenges: (1) Real-Time Processing. Latency-sensitive tasks [1, 5] require immediate 3D reconstruction. However, traditional methods often rely on offline calibration such as Structure-fromMotion (SfM) [20] or iterative optimization of scene representations [21, 22], making them unsuitable for real-time processing of uncalibrated video streams [23, 24, 25]; (2) Dynamic Motion Modeling. In uncalibrated videos, camera and object motions are almost always entangled, and dynamic scenes exhibit complex, non-rigid motions that must be inferred from video frames [26, 27, 28]; (3) LongTerm Stability. Online systems must process arbitrarily long video streams while preventing error accumulation and maintaining efficiency in both memory and computation [17]. To address these challenges, we propose StreamSplat, the first fully feed-forward framework for online dynamic 3D Gaussian Splatting (3DGS) [21, 29] reconstruction from uncalibrated video streams. Inspired by the recent success of feed-forward Gaussian Splatting [15], we leverage pixel-aligned 3D Gaussians in an orthographic canonical space [26, 30] to support online, dynamic, and uncalibrated reconstruction. We formulate online video reconstruction as sequential, frame-by-frame process. First, we encode each incoming frame into set of pixel-aligned static 3D Gaussians in probabilistic way. This formulation is specifically designed for 3DGS position prediction (Section 3.1), alleviating positional local minima caused by random initialization and better capturing uncertainty. To model dynamic motion, we propose novel bidirectional deformation field coupled with an adaptive Gaussian fusion mechanism. These components enable smooth transitions and seamless fusion of Gaussians across frames by softly matching spatially and temporally coherent structures, while dynamically integrating newly observed content (Section 3.2). The bidirectional formulation ensures that dynamic Gaussians remain structured and pixel-aligned at key frames, improving robustness and mitigating the error accumulation common in long-term online reconstruction [17]. We evaluate StreamSplat on both static (CO3Dv2 [31], RealEstate10K [32]) and dynamic (DAVIS [33], YouTube-VOS [34]) benchmarks. Our method consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting fully online reconstruction of arbitrarily long video streams (Section 4). In summary, our key contributions are: We propose StreamSplat, the first fully feed-forward framework for online dynamic 3DGS reconstruction from uncalibrated video streams. We introduce two key technical innovations: probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. StreamSplat achieves state-of-the-art reconstruction quality and dynamic scene modeling across multiple static and dynamic benchmarks, while uniquely supporting online reconstruction of arbitrarily long uncalibrated video streams."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Neural Representations for Dynamic Scenes Dynamic 3D reconstruction from monocular videos is critical for many real-world applications. Early works leveraged implicit neural representations and modeled dynamic scenes using coordinatebased multilayer perceptrons (MLPs) optimized through per-image optimization [35, 36, 37, 38]. Later works extended by introducing learnable time-conditioned deformation fields with canonical representations [39, 40, 41, 42]. However, these methods typically fail to capture large and complex motions due to lack of explicit 3D structure and motions. To address this, recent work shifts to explicit 3D primitives, notably dynamic Gaussian splatting [30, 43, 44, 45, 46, 47, 48], which leverages persistent and deformable Gaussians to efficiently represent dynamic scenes for real-time rendering. However, these methods still rely on calibrated camera poses and extensive per-scene optimization, making them unsuitable for fully uncalibrated and real-time scenarios. 2.2 Feed-Forward Dynamic 3D Reconstruction By directly predicting 3D scene representations via neural networks, feed-forward methods have emerged as promising alternatives to optimization-based approaches. For example, based on DUSt3R [11], several approaches have shown the ability to estimate camera parameters and 3D point clouds from uncalibrated inputs [12, 13, 49, 50, 51, 52, 53]. MonST3R [14] extends this paradigm to dynamic scenes by producing temporally coherent point clouds and camera poses across frames. Such methods typically require post-optimization on camera poses or scene geometry to maintain consistency in dynamic scenarios. Moreover, scene-coordinate-based point clouds are inherently difficult to perform deformations, limiting their effectiveness in dynamic scene modeling [43, 45]. Another research line explores fully feed-forward 3D Gaussian Splatting (3DGS) methods for 3D reconstruction [54, 55, 56, 57, 58, 59, 60, 61, 62]. Prior works achieve fast static scene reconstruction from images pairs. Specifically, pixelSplat [15] employs multi-view transformers to regress 3D Gaussians from calibrated image pairs, while NoPoSplat [16] extends this to uncalibrated pairs, and StreamGS [17] achieves online static reconstruction from image streams. Recent works such as BTimer [18] further adapt these approaches to dynamic scene reconstruction by stacking multiple temporal-consistent static 3DGS from calibrated video frames. Despite their efficiency, these methods either primarily address static scenes or approximate dynamics by stacking static representations, which fail to explicitly model scene dynamics. Moreover, many require access to entire video sequences during inference [18, 19], making them unsuitable for online streaming scenarios."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present StreamSplat, framework designed to instantly transforms uncalibrated online video streams into dynamic 3D Gaussian Splatting (3DGS) representations capable of capturing scene dynamics. Figure 2 provides an overview. We first encode the current frame into static 3D Gaussians in canonical space (Section 3.1), then predict bidirectional deformation field between Gaussians encoded from current frame and propagated from the previous frame, and finally fuse them into unified dynamic representation (Section 3.2). This representation supports rendering at arbitrary times and viewpoints, thereby effectively recovering scene dynamics (Section 3.3). 3.1 Probabilistic 3D Gaussian Encoding Canonical 3D Space. Following [19, 26, 30], we define the canonical space using shared orthographic camera coordinate system. Details are provided in Appendix A. We argue that this choice offers two key advantages. First, under orthographic projection, scene dynamics that typically entangle camera and object motion manifest as shared global shift, which can be naturally absorbed by Gaussian motions and handled by our dynamic prediction module in an unified manner (Section 3.2). This simplification allows us to bypass errors from inaccurate camera estimation. Second, orthographic projection simplifies the camera model while still preserving the spatio-temporal structure of 3D motion [26]. Together, this orthographic canonical space provides simplified, yet effective foundation for predicting dynamic 3D representations directly from uncalibrated videos. 3 Figure 2: Overview of the StreamSplat framework. Given pair of frames, we first encode them using the Static Encoder to produce canonical 3D Gaussians (Section 3.1), and then pass the 3DGS Embeddings to the Dynamic Decoder to predict the deformation field (Section 3.2). The resulting dynamic 3D Gaussians can be rendered at arbitrary time to produce RGB images and depth maps. Structured Static 3D Gaussian Encoding. To overcome the inherent depth ambiguity under orthographic projection [63] and unstructured nature of 3DGS [64], we incorporate pretrained depth estimator [65] to generate per-frame pseudo-depth maps as ground truth and predict 3D Gaussian positions in pixel-aligned manner [15] to maintain spatial correspondence with input frames. As shown in Figure 2, given an RGB image RHW 3, we first obtain the corresponding depth map and concatenate it with to form 4-channel RGBD input. This is processed by nonoverlapping 2D convolution to extract 8 8 patches, which are then fed into Transformer encoder with self-attention and MLP blocks [66, 67], producing 3DGS Embeddings RN D, where = HW/64. These embeddings are upsampled via lightweight block composed of linear layers and window-based attention [68, 69], yielding static Gaussian tokens ˆE R16N D/4. Each token ˆEi is decoded by linear heads into 3DGS parameters: position offset oi R3, rotation Ri R4, scale Si R3, opacity αi R, and color ci R3. The final 3D position µi is computed via pixel-aligned prediction: µi = (u + oi,0, + oi,1, 1/oi,2), where (u, v) is the pixel coordinate of the i-th token. The first two offset components oi,0, oi,1 specify the local offset within the 2 2 image patch, while oi,2 denotes the inverse depth following [65] for better depth estimation near the camera. Probabilistic Position Sampling. 3D Gaussian Splatting is sensitive to position initialization [21] and prone to local minima [15], especially in feed-forward models [15, 58] that make predictions in single forward pass without iterative refinement. Inspired by [15], we predict truncated normal distribution for each 3D offset rather than regressing it directly: N[1,1](µp, Σp), where µp and Σp are the predicted mean and covariance. As shown in Section 4.4, this strategy promotes spatial exploration during early training and stabilizes convergence toward optimal positions. 3.2 Dynamic Deformation Prediction Bidirectional Deformation Field. Online dynamic scene reconstruction involves complex motion, including large deformations and emerging objects/surfaces [17]. To handle this, we propose bidirectional deformation field that jointly models forward and backward temporal motion between consecutive frames. Specifically, our model estimates: (i) forward deformation field that transforms the current-frame Gaussians Gt toward the next frame, and (ii) backward deformation field that transforms next-frame Gaussians Gt+1 back to the current frame. 4 Each deformation field consists of 3D velocity vector [1, 1]3 and an opacity coefficient γ that controls visibility over time. This bidirectional design naturally handles the appearance and disappearance of Gaussians without explicitly managing varying number of Gaussians, thereby facilitating end-to-end training and online updates during inference. Adaptive Gaussian Fusion via Soft Matching. Directly combining new Gaussians often leads to spatial overlap and redundancy [17]. Traditional optimization-based methods resolve this by rigid one-to-one matching and iterative fusion [47], which are computationally expensive and hard to keep spatial structures, potentially resulting in long-term accumulation of errors. Inspired by defining the life-cycle of Gaussians [70], we propose an adaptive Gaussian fusion mechanism based on opacity deformation. Each two-frame interval is normalized to [0, 1], with = 0 and = 1 representing the previous and current frames, respectively. Each Gaussian persists across two consecutive frames, with time-dependent opacity deformation: α(t) = α σ (γ0 (t t0 γ1)) σ (γ0 γ1) , (1) where σ() denotes sigmoid function, α denotes initial opacity, t0 {0, 1} denotes the Gaussians creation frame, γ0 R+ and γ1 [0, 1] control the transition rate and fade-out timing. This allows overlapped Gaussians from adjacent frames to be implicitly merged through opacity deformation, and the rendering loss can naturally drive the model to find soft match between them. Dynamic Deformation Decoding. Given two consecutive frames from randomly sampled time interval, It1 and It2, we first use the frozen static encoder to extract their 3DGS Embeddings ht1 and ht2. We also extract DINOv2 features [71] ft1 and ft2 from It1 and It2, respectively. To distinguish the embeddings, we add learnable Type Embeddings Tsrc, Ttgt RD to ht1 and ht2, yielding ˆht1 and ˆht2 . These are processed by the decoder as follows: [ˆht1, ˆht2] = Self-Attn([ˆht1, ˆht2 ]), ˆht1 = Cross-Attn(ˆht1 , ft2), ˆht2 = Cross-Attn(ˆht2, ft1 ) (2) [ˆht1, ˆht2] = FFN([ˆht1, ˆht2 ]), where [] denotes concatenation. After passing through few decoder blocks, we obtain the Deformation Embeddings [dt1 , dt2] R2N D, which are then upsampled via the same upsampler to produce deformation tokens ˆd R32N D/4. Each deformation token ˆdj is concatenated with the corresponding static Gaussian token from the same frame ˆEj to form joint token ˆEj ˆdj, which is passed through 2-layer MLP head to predict the deformation field, including velocity vj and opacity coefficient γj. These deformation fields allow Gaussians to move and fade over time, enabling computation of dynamic 3D Gaussians at arbitrary times for continuous scene reconstruction. 3.3 Training and Inference Robust Stage-wise Training. To address the difficulty of jointly optimizing static 3D Gaussian encoding and dynamic deformation prediction, we adopt two-stage training protocol: Stage 1: Static 3DGS Encoder Training. The static encoder aims to reconstruct static 3DGS from single input frame It and pseudo-depth Dt. It produces 3DGS primitives that are used to render RGB ˆIt and depth ˆDt. The training loss combines photometric and depth supervision: Lstatic = Lrecon( ˆIt, It) + λdepthLdepth( ˆDt, Dt), where Lrecon is the sum of L2 loss (RGB space) and LPIPS loss [72], and Ldepth is scaleand shift-invariant depth loss [73]: (3) Ldepth = Eτ ( ˆDt) τ (Dt), where τ (x) = median(x) Ex median(x) . (4) To reduce the impact of noisy pseudo-depth, we introduce an adaptive decay factor into the depth loss weight. Specifically, we define the effective weight as ˆλdepth = λdepth σ(τ ( ˆDt) τ (Dt)/w), where λdepth is fixed hyperparameter, and controls the sensitivity of the sigmoidbased decay. We use ˆλdepth instead of λdepth to reduce unreliable supervision and improve robustness. Figure 3: Qualitative results on RE10K. StreamSplat produces detailed and consistent reconstructions across diverse scenes. Videos are provided on the website. Stage 2: Dynamic Deformation Decoder Training. With the encoder frozen, the dynamic decoder learns to predict the bidirectional deformation fields. Given two randomly sampled frames It1 and It2 , we first encode them to obtain static 3DGS Gt1 and Gt2 . Then we use the dynamic decoder to predict the deformation fields that transform Gt1 It2 and Gt2 It1 , followed by adaptive fusion. The training objective is to reconstruct It for all intermediate time [t1, t2]: Ldynamic = Et (cid:104) (cid:105) Lrecon( ˆIt, It) + λdepthLdepth( ˆDt, Dt) + λmaskLmask( ˆIt Mt, It Mt) , (5) where Lmask is an auxiliary reconstruction loss that encourages the model to focus on moving foreground regions using binary segmentation mask Mt from datasets [33, 34]. Online Inference Pipeline. As illustrated in Figure 2, our StreamSplat can instantly process an incoming video stream (I0, I1, . . . , IN ) frame-by-frame. Let ht1 and ft1 denote the 3DGS Embedding and DINO feature from the previous frame (initialized as zero for = 0). When new frame It arrives, we estimate its pseudo-depth Dt [65] and encode it into 3DGS Embedding ht. These 3DGS Embeddings and DINO features are passed to the dynamic decoder to predict bidirectional deformation field. The resulting field is applied to both Gt1 and Gt, which are fused and rendered from arbitrary viewpoints and interpolated times. Finally, ht and ft are cached for the next step."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Training Datasets. We pre-train StreamSplat on combination of real-world datasets, including static scenes from CO3Dv2 [31] and RealEstate10K (RE10K) [32], and dynamic video datasets DAVIS [33] and YouTube-VOS [34]. CO3Dv2 and RE10K are treated as pure video datasets without using any pre-calibrated camera information. For dynamic scenes, we utilize object segmentation masks from DAVIS and YouTube-VOS to supervise motion-aware components, and no mask supervision is applied to CO3Dv2 and RE10K. We follow the official train/validation splits and train only on the training sets. The same pre-trained model is evaluated across both static and dynamic benchmarks. Implementation Details. StreamSplat is trained on 8 NVIDIA A100 GPUs for approximately 3 days. We use FlashAttention-2 [74], gradient checkpointing [63], and mixed-precision training with BF16 for better efficiency. Input frames are resized to 512 288 to preserve aspect ratio for pixel-aligned 3DGS prediction. We apply image-level augmentations following EDM [75] and optimize using 6 Figure 4: Qualitative comparison on DAVIS. Blue box: given frames; Red box: interpolated frames. Our StreamSplat produces high-fidelity and temporal coherent interpolations across both (a) 5-frame and (b) 8-frame interval tasks. Videos are provided on the website. Table 1: Quantitative results on RE10K. We report results for 2 given views and 5 novel views. Method pixelSplat [15] MVSplat [55] NoPoSplat [16] CoDeF [35] DGMarbles [78] StreamSplat (Ours) Rep. Type Stat. Dyn. Given View SSIM PSNR LPIPS PSNR Novel View SSIM LPIPS PSNR Average SSIM LPIPS 30.70 31.48 29.50 35.13 27.48 41.60 0.952 0.962 0. 0.943 0.867 0.992 0.055 0.046 0.069 0.091 0.232 0.010 28.31 28.48 28.65 20.51 23.40 24.68 0.905 0.909 0. 0.591 0.727 0.777 0.097 0.091 0.096 0.402 0.333 0.167 28.99 29.34 28.90 21.77 23.73 29.51 0.918 0.924 0. 0.625 0.738 0.839 0.085 0.078 0.089 0.374 0.325 0.122 AdamW [76] with gradient clipping set to 1.0. For Stage 1, we use batch size of 128, peak learning rate of 5 104 with 20K linear warm-up iterations, and weight decay of 0.05. For Stage 2, we use batch size of 256 (with gradient accumulation), peak learning rate of 1 104 with 100K linear warm-up iterations, and weight decay of 0.05. Additional training details, including hyperparameters and model configurations, are provided in the Appendix B. Evaluation Settings. We follow prior works [15, 28] and report peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) [77], and LPIPS [72], all evaluated at resolution of 256 256 for fair comparison [15, 28]. For static scene reconstruction, we follow [15], using two randomly sampled input views with at least 60% overlap and five target views sampled between them. For dynamic scene reconstruction, we first evaluate on full video sequences. To further assess dynamic modeling, we follow [28] and evaluate video interpolation on subsampled sequences with 5-frame and 8-frame intervals and use intermediate frames for metric computation. Our website provide video results across all four datasets, including extended demonstrations on long videos. 4.2 Static Scene Reconstruction We evaluate StreamSplat on the RE10K benchmark and compare it with recent static and dynamic reconstruction methods. Results on CO3Dv2 are in Appendix C. For dynamic methods without camera pose input, novel views are rendered using relative timestamps. Quantitative and qualitative results are presented in Table 1 and Figures 3 and 7. StreamSplat significantly outperforms all static baselines on the given-view reconstruction task. However, on novel-view reconstruction, static methods still lead, although StreamSplat achieves the best performance among all dynamic approaches. 7 Table 2: Quantitative results on DAVIS. denotes results reported in the original papers. Method Omnimotion [26] RoDynRF [79] CoDeF [35] MonST3R [14] 4DGS [45] Splater Video [30] DGMarbles [78] StreamSplat (Ours) Scene Rep. NeRF NeRF NeRF Points 3DGS 3DGS 3DGS 3DGS Key Frames SSIM LPIPS PSNR 24.11 24.79 31.49 42.33 18.12 28.63 28.38 37.83 0.714 0.723 0.939 0. 0.573 0.837 0.879 0.982 0.371 0.394 0.088 0.012 0.513 0.228 0.172 0.016 Middle-4 Frames SSIM LPIPS PSNR 19.40 21.33 23.66 0.498 0.619 0.684 0.400 0.313 0.193 Time > 8 hrs > 24 hrs 10 mins 30 40 mins 30 mins 30 mins 1.48 Figure 5: Visualization of reconstructed dynamic scene from canonical and novel views. Our method captures consistent 3D motion over time, enabling faithful reconstruction at arbitrary time and viewpoints. Videos are provided on the website. According to our qualitative comparison in Figures 3 and 7, we attribute this to the absence of accurate camera poses input in our model. Our StreamSplat assumes smooth camera motion between two input views and relies solely on relative timestamps for novel view synthesis, which can cause slight misalignments. Furthermore, unlike static models that assume fixed scene, StreamSplat is designed to model scene dynamics, which may introduce motion artifacts that hinder performance under static benchmarks. Despite these challenges, our StreamSplat achieves competitive average performance across all test views and consistently surpasses all dynamic baselines in every evaluation setting. 4.3 Dynamic Scene Reconstruction We evaluate StreamSplat on the DAVIS benchmark and compare it with state-of-the-art methods, and report the main results in Table 2. Notably, StreamSplat is the only method capable of near real-time dynamic 3D reconstruction per frame. Table 3: Quantitative results on the DAVIS7 [28] with 8-frame interval. Method Type PSNR SSIM LPIPS AMT [80] RIFE [81] FILM [82] LDMVFI [83] VIDIM [28] 21.09 20.48 20.71 19.98 19.62 0.544 0.511 0.528 0.479 0. StreamFor key-frame reconstruction task, Splat achieves performance competitive with the state-of-the-art scene-coordinate-based method MonST3R [14], which represents the dynamic scenes as sequences of static 3D point clouds. However, MonST3R requires extensive postoptimization and is limited to key-frame reconstruction. dynamics, enabling reconstruction of intermediate frames across substantial temporal gaps. CoDeF [35] DGMarbles [78] Ours 0.520 0.548 0.613 20.34 19.83 22.10 In contrast, StreamSplat operates in near real time and explicitly models the scene pixel 3D 0.254 0.258 0.270 0.276 0.257 0.365 0.353 0.234 We further evaluate dynamic modeling performance under 5-frame and 8-frame interval settings, comparing against optimization-based 3D reconstruction methods and pixel-level video-interpolation methods. As shown in Table 3 and Figures 4 and 8, StreamSplat consistently outperforms all baselines, including video-interpolation methods which lack explicit 3D modeling. This highlights the effectiveness of our approach in modeling temporally coherent dynamic scenes. Moreover, as 8 Figure 6: Ablation. w/o sampling: deterministic position prediction; w/o depth: no depth supervision. illustrated in Figures 4 and 8, StreamSplat maintains high visual fidelity even under challenging scenarios such as large camera motion and reflective surfaces, where other methods often fail. In addition, we demonstrate the robustness of StreamSplat in long-range dynamic modeling. As shown in Figure 5, our method maintains coherent 3D structure and appearance across large spatio-temporal distances, from both canonical and novel viewpoints. 4.4 Ablation Studies We conduct ablation studies to evaluate the contribution of each key component in StreamSplat. Quantitative and qualitative results are presented in Table 4 and Figures 6 and 9. Table 4: Component-wise ablations on key and intermediate frames. Variants Frame PSNR SSIM LPIPS w/o Sampling w/o Depth Full (Ours) Ablation on probabilistic position sampling. We evaluate the impact of probabilistic position sampling for 3DGS by comparing it with deterministic counterpart. As shown in Table 4, probabilistic sampling yields substantial improvement (6.36dB) in PSNR for key-frame reconstruction. Qualitatively, the deterministic variant is prone to local minima, particularly along the depth axis, resulting in blurry and inaccurate reconstructions. This aligns with findings from prior work [15, 21] and highlights the importance of probabilistic position prediction in feed-forward 3DGS models. w/o Bi-Deform. Full (Ours) 0.582 0.684 18.89 23. 0.492 0.193 Mid. Key 31.47 36.68 37.83 0.946 0.975 0.982 0.073 0.039 0. Ablation on pseudo depth supervision. We evaluate the impact of removing pseudo depth supervision on key-frame reconstruction. Table 4 shows that removing depth supervision leads to only minor drop in reconstruction quality. However, as shown in Figure 6, the model without depth supervision fails to capture accurate spatial structure. The learned depth becomes entangled with RGB values, resulting in distorted 3D reconstructions. Ablation on bidirectional deformation field. We evaluate the effectiveness of the bidirectional deformation field by comparing it with conventional deformation field [44] on middle-frame reconstruction. As shown in Table 4 and Figure 9, the bidirectional variant significantly improves reconstruction quality. The baseline struggles to preserve pixel-aligned structures, leading to noticeable error accumulation over longer sequences."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced StreamSplat, the first feed-forward framework for instant, online dynamic 3D reconstruction from uncalibrated video streams. By incorporating probabilistic position sampling strategy and bidirectional deformation field with adaptive Gaussian fusion, our StreamSplat effectively addresses key challenges in online dynamic reconstruction, allowing to produce accurate dynamic 3D Gaussian Splatting (3DGS) representations from arbitrarily long video streams. These representations faithfully capture scene dynamics and support interpolation at arbitrary time. Extensive experiments on both static and dynamic benchmarks validate the superior performance of StreamSplat in terms of reconstruction quality and dynamic scene modeling. In future, we plan to explore its potential in video generation and autonomous driving."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was funded, in part, by the NSERC DG Grant (No. RGPIN-2022-04636, No. RGPIN-201905448), the Vector Institute for AI, Canada CIFAR AI Chair, and Google Gift Fund. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through the Digital Research Alliance of Canada alliance.can.ca, and companies sponsoring the Vector Institute www.vectorinstitute.ai/#partners, and Advanced Research Computing at the University of British Columbia. Additional hardware support was provided by John R. Evans Leaders Fund CFI grant. ZW and QY are supported by UBC Four Year Doctoral Fellowships. We thank Qiuhong Shen for constructive discussions and helpful comments."
        },
        {
            "title": "References",
            "content": "[1] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023. [2] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. [3] Julie Carmigniani and Borko Furht. Augmented reality: an overview. Handbook of augmented reality, pages 346, 2011. [4] Carolina Cruz-Neira, Daniel Sandin, Thomas DeFanti, Robert Kenyon, and John Hart. The cave: Audio visual experience automatic virtual environment. Communications of the ACM, 35(6):6473, 1992. [5] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. [6] Hidenobu Matsuki, Riku Murai, Paul HJ Kelly, and Andrew Davison. Gaussian splatting slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18039 18048, 2024. [7] Chi Yan, Delin Qu, Dan Xu, Bin Zhao, Zhigang Wang, Dong Wang, and Xuelong Li. Gs-slam: Dense visual slam with 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1959519604, 2024. [8] Vladimir Yugay, Yue Li, Theo Gevers, and Martin Oswald. Gaussian-slam: Photo-realistic dense slam with gaussian splatting. arXiv preprint arXiv:2312.10070, 2023. [9] Zihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui, Martin Oswald, Andreas Geiger, and Marc Pollefeys. Nicer-slam: Neural implicit scene encoding for rgb slam. In 2024 International Conference on 3D Vision (3DV), pages 4252. IEEE, 2024. [10] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in rgb-d images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29302937, 2013. [11] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [12] Kai Xu, Tze Ho Elden Tse, Jizong Peng, and Angela Yao. Das3r: Dynamics-aware gaussian splatting for static scene reconstruction. arXiv preprint arXiv:2412.19584, 2024. [13] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Easi3r: Estimating disentangled motion from dust3r without training. arXiv preprint arXiv:2503.24391, 2025. [14] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. [15] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1945719467, 2024. 10 [16] Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, and Songyou Peng. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. arXiv preprint arXiv:2410.24207, 2024. [17] Yang Li, Jinglu Wang, Lei Chu, Xiao Li, Shiu-hong Kao, Ying-Cong Chen, and Yan Lu. Streamgs: Online generalizable gaussian splatting reconstruction for unposed image streams. arXiv preprint arXiv:2503.06235, 2025. [18] Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, et al. Feed-forward bullet-time reconstruction of dynamic scenes from monocular videos. arXiv preprint arXiv:2412.03526, 2024. [19] Qiuhong Shen, Xuanyu Yi, Mingbao Lin, Hanwang Zhang, Shuicheng Yan, and Xinchao Wang. Seeing world dynamics in nutshell. arXiv preprint arXiv:2502.03465, 2025. [20] Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 41044113, 2016. [21] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):114, August 2023. ISSN 0730-0301, 1557-7368. doi: 10.1145/3592433. [22] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65 (1):99106, 2021. [23] Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 41604169, 2023. [24] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2079620805, 2024. [25] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 57415751, 2021. [26] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1979519806, 2023. [27] Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lyv, Peng Wang, Wenping Wang, and Junhui Hou. Modgs: Dynamic gaussian splatting from casually-captured monocular videos. arXiv preprint arXiv:2406.00434, 2024. [28] Siddhant Jain, Daniel Watson, Eric Tabellion, Ben Poole, Janne Kontkanen, et al. Video interpolation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73417351, 2024. [29] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In Proceedings Visualization, 2001. VIS01., pages 29538. IEEE, 2001. [30] Yang-Tian Sun, Yihua Huang, Lin Ma, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Splatter video: Video gaussian representation for versatile processing. Advances in Neural Information Processing Systems, 37:5040150425, 2024. [31] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In International Conference on Computer Vision, 2021. [32] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. ACM Trans. Graph. (Proc. SIGGRAPH), 37, 2018. [33] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. [34] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and Thomas Huang. Youtube-vos: Sequence-to-sequence video object segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 585601, 2018. 11 [35] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally consistent video processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80898099, 2024. [36] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86288638, 2021. [37] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in neural information processing systems, 33:74627473, 2020. [38] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33: 75377547, 2020. [39] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1031810327, 2021. [40] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58655874, 2021. [41] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Ricardo Martin-Brualla, and Steven Seitz. Hypernerf: higher-dimensional representation for topologically varying neural radiance fields. ACM Transactions on Graphics (TOG), 40(6):112, 2021. [42] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55215531, 2022. [43] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 2024 International Conference on 3D Vision (3DV), pages 800809. IEEE, 2024. [44] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85088520, 2024. [45] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, and Baoquan Chen. 4d gaussian splatting: Towards efficient novel view synthesis for dynamic scenes. arXiv e-prints, pages arXiv2402, 2024. [46] Yuheng Yuan, Qiuhong Shen, Xingyi Yang, and Xinchao Wang. 1000+ fps 4d gaussian splatting for dynamic scene rendering. arXiv preprint arXiv:2503.16422, 2025. [47] Junoh Lee, ChangYeon Won, Hyunjun Jung, Inhwan Bae, and Hae-Gon Jeon. Fully explicit dynamic gaussian splatting. Advances in Neural Information Processing Systems, 37:53845409, 2024. [48] Dongbo Shi, Shen Cao, Lubin Fan, Bojian Wu, Jinhui Guo, Renjie Chen, Ligang Liu, and Jieping Ye. No parameters, no problem: 3d gaussian splatting without camera intrinsics and extrinsics. arXiv e-prints, pages arXiv2502, 2025. [49] Wonbong Jang, Philippe Weinzaepfel, Vincent Leroy, Lourdes Agapito, and Jerome Revaud. Pow3r: Empowering unconstrained 3d reconstruction with camera and scene priors. arXiv preprint arXiv:2503.17316, 2025. [50] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [51] Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. Splatt3r: Zero-shot gaussian splatting from uncalibrated image pairs. arXiv preprint arXiv:2408.13912, 2024. [52] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. arXiv preprint arXiv:2501.13928, 2025. [53] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. arXiv preprint arXiv:2503.11651, 2025. [54] Yunsong Wang, Tianxin Huang, Hanlin Chen, and Gim Hee Lee. Freesplat: Generalizable 3d gaussian splatting towards free view synthesis of indoor scenes. Advances in Neural Information Processing Systems, 37:107326107349, 2024. [55] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2024. [56] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision, pages 119. Springer, 2024. [57] Qiuhong Shen, Zike Wu, Xuanyu Yi, Pan Zhou, Hanwang Zhang, Shuicheng Yan, and Xinchao Wang. Gamba: Marry gaussian splatting with mamba for single-view 3d reconstruction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [58] Xuanyu Yi, Zike Wu, Qiuhong Shen, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, Shuicheng Yan, Xinchao Wang, and Hanwang Zhang. Mvgamba: Unify 3d content generation as state space sequence modeling. arXiv preprint arXiv:2406.06367, 2024. [59] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. arXiv preprint arXiv:2410.13862, 2024. [60] Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jisang Han, Jiaolong Yang, Chong Luo, and Seungryong Kim. Pf3plat: Pose-free feed-forward 3d gaussian splatting. arXiv preprint arXiv:2410.22128, 2024. [61] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views. arXiv preprint arXiv:2502.12138, 2025. [62] Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, and Ziwei Liu. Mvsgaussian: Fast generalizable gaussian splatting reconstruction from multi-view stereo. In European Conference on Computer Vision, pages 3753. Springer, 2024. [63] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. [64] Jaeyoung Chung, Jeongtaek Oh, and Kyoung Mu Lee. Depth-regularized optimization for 3d gaussian splatting in few-shot images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 811820, 2024. [65] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. [66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [67] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [68] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [69] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation. In European Conference on Computer Vision, pages 120. Springer, 2024. [70] Boming Zhao, Yuan Li, Ziyu Sun, Lin Zeng, Yujun Shen, Rui Ma, Yinda Zhang, Hujun Bao, and Zhaopeng Cui. Gaussianprediction: Dynamic 3d gaussian prediction for motion extrapolation and free view synthesis. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 13 [71] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [72] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [73] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. [74] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [75] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. [76] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [77] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [78] Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, and Leonidas Guibas. Dynamic gaussian marbles for novel view synthesis of casual monocular videos. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [79] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323, 2023. [80] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98019810, 2023. [81] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow estimation for video frame interpolation. In European Conference on Computer Vision, pages 624642. Springer, 2022. [82] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and Brian Curless. Film: Frame interpolation for large motion. In European Conference on Computer Vision, pages 250266. Springer, 2022. [83] Duolikun Danier, Fan Zhang, and David Bull. Ldmvfi: Video frame interpolation with latent diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 14721480, 2024."
        },
        {
            "title": "Appendices",
            "content": "The Appendix is organized as follows: Appendix A: Background. brief overview of 3D Gaussian Splatting (3DGS), including the orthographic projection implementation used in our method. Appendix B: Implementation Details. Model configurations, hyper-parameters, and further details on 3DGS parameterization, including the implementation of our probabilistic position sampling. Appendix C: Additional Experimental Results. Additional qualitative results on static scene reconstruction, ablation studies, and video reconstruction/interpolation. Appendix D: Limitations and Societal Impact. discussion of current limitations, future directions, and potential societal impact of our method."
        },
        {
            "title": "A Background",
            "content": "3D Gaussian Splatting. 3D Gaussian Splatting (3DGS) [21] represents static scene using collection of 3D Gaussians. Each Gaussian is defined by its mean (position) µ, covariance matrix Σ, opacity α, and color represented by spherical harmonics (SH) coefficients c. The final opacity of 3D Gaussian at given point is computed as: α(x) = α exp (cid:18) 1 2 (x µ)T Σ1(x µ) (cid:19) . (6) Normally, the covariance matrix Σ is decomposed into diagonal scaling matrix and rotation quaternion for differentiable optimization: Σ = RSST RT . (7) Given set of 3DGS = {Gi}N image plane and then blending their colors based on their opacity and depth. i=1, the rendering process involves splatting them onto the 2D The 3D Gaussians are projected using the approximate transformation [29]: Σ = JWΣWT JT , where is the Jacobian of the perspective projection function defined as: (cid:17) (cid:16) (u, v) = = (u, v) (x, y, z) = fx x/z + cx, fy y/z + cy (cid:19) (cid:18)fx/z 0 fx x/z2 fy/z fy y/z 0 , . In case of orthographic projection, the projection is simplified to: (u, v) = = (u, v) (x, y, z) = (cid:16) fx + cx, fy + cy (cid:19) (cid:18)fx 0 0 fy 0 . (cid:17) , The pixel color is obtained by alpha-blending the projected 2D Gaussians sorted by depth: = (cid:88) i=1 i1 (cid:89) ciαi (1 αj) j=1 (8) (9) (10) (11) where αi is the opacity of the i-th projected Gaussian obtained by Eq. (6). Dynamic 3D Gaussian Splatting. Dynamic 3D Gaussian Splatting extends the original 3DGS with deformation field to model the motion of the Gaussians over time [43]. Typically, the deformation fields D(t) = {µ(t), R(t), α(t)} are used to update the static canonical Gaussian with various of approaches [30, 44, 45, 47, 48]. For example, given static position µ and deformation field µ(t), the deformed position can be obtained as: ˆµt = µ + µ(t). Figure 7: Qualitative results on RE10K. Blue box: given frames; Red box: interpolated frames. Compared to other dynamic-scene reconstruction methods, StreamSplat produces more detailed and consistent 3D reconstructions across diverse scenes, whereas other methods often exhibit distortions in both color and geometry. 16 Algorithm 1: 3DGS Parameters Predictions :Gaussian token ˆE, deformation token ˆd, current time t, frame time t0 Input Output :3DGS Parameters: position µ, scale S, rotation R, opacity α, color // Static Prediction (Time-Invariant Parameters) fstatic LINEAR( ˆE) ; (fµp , fΣp , fS, fR, fα, fc) SPLIT(fstatic) ; N[1,1](tanh(fµp ), exp2(fΣp )) ; (cid:20) + o0, + o1, µ0 (cid:21) 1 0.5 + 0.5 o2 // Static feature // Sample position offset ; // Pixel-aligned position α0 SIGMOID(fα) ; 0.1 SOFTPLUS(fS) ; NORMALIZE(fR) ; SIGMOID(fc) ; ; (cid:16) (cid:17) LINEAR( ˆE) ˆd // Deformation Prediction (Time-Dependent Parameters) fdef MLP (fµp , fΣp , fγ) SPLIT(fdef) ; N[1,1](tanh(fµp ), exp2(fΣp )) ; γ [RELU(fγ0), SIGMOID(fγ1 )] ; // Apply time-dependent translation and opacity decay µ µ0 + ; α α0 SIGMOID (γ0 (t t0 γ1)) SIGMOID (γ0 γ1) // Deformation feature // Sample position velocity ; // Eq. (1) return µ, S, R, α, c"
        },
        {
            "title": "B Implementation Details",
            "content": "Model Configurations. As shown in Table 5, StreamSplat consists of an image tokenizer, static encoder, dynamic decoder, and an upsampler. The image tokenizer takes RGBD inputs at resolution of 288512 and uses patch size of 8 to produce 768-dimensional tokens. Both the static encoder and the dynamic decoder contain 10 transformer layers with an embedding dimension of 768 and 12 attention heads. The dynamic decoder uses 0.1 stochastic drop path to prevent overfitting on image features. An upsampler with 2 layers of window attention with 2304 token length increases the token length to 16 and reduces the embedding dimension from 768 to 192. The model uses linear head for static prediction and two-layer MLP for deformation prediction. We apply loss balancing weights of 1.0 for MSE, 0.05 for both LPIPS and depth, and 3.0 for the mask loss. 3DGS Parameterization. 3DGS parameters are obtained as summarized in Algorithm 1. linear head projects each Gaussian token ˆE and, through parameter-specific activations, yields the static tuple (µ0, S, R, α0, c). 2-layer MLP, conditioned on the concatenation ˆE ˆd, predicts the velocity and opacity coefficients γ, from which the final time-dependent values are computed by integrating the current time t. Note, we adopt our proposed Probabilistic Position Sampling for all positionrelated parameters, including position offset and velocity v, to improve robustness and avoid spatial local minima."
        },
        {
            "title": "C Additional Experimental Results",
            "content": "Due to the limited space in the main manuscript, we provide additional experimental results in this section, including qualitative comparisons with other dynamic methods on static scene reconstruction (Figure 7), and qualitative comparisons on ablation studies of our bidirectional deformation field (Figure 9), and more qualitative results on video reconstruction and interpolation on CO3Dv2 [31], DAVIS [33], and Youtube-VOS [34] (Figure 8). Please refer to the website for video results. 17 Figure 8: Qualitative results with 8-frame interval on different datasets. Blue box: given frames; Red box: interpolated frames."
        },
        {
            "title": "D Limitations and Impact",
            "content": "While StreamSplat achieves strong performance, certain design choices introduce some limitations. First, the framework relies on pseudo-depth maps predicted by an external monocular estimator, which may introduce noiseparticularly around fine-scale geometry and depth discontinuities. To mitigate this, we incorporate an adaptive decay weighting scheme during training to downweight unreliable 18 Figure 9: Ablation. w/o bi.: without bidirectional deformation field. Blue box: given frames; Red box: interpolated frames. depth supervision. Nonetheless, scaling the training dataset to enable internal depth refinement remains promising direction to reduce reliance on external priors. Second, the bidirectional deformation field is trained over two-frame window for efficiency. As result, information from earlier frames may be lost in dynamic scenes with fast motion or extended occlusions. Future work may explore efficient mechanisms for adaptively selecting and fusing Gaussians across extended frame histories, which could help retain more temporal context in challenging scenarios. Societal impact. While our work enables instant dynamic 3D perception from monocular video, potentially improving accessibility and safety real-world applications like autonomous driving systems, it also warrants careful evaluation in edge cases where performance limitations could affect reliability in safety-critical scenarios."
        },
        {
            "title": "E Licenses",
            "content": "Datasets. CO3Dv2 [31]: CC BY-NC 4.0 RealEstate10K [32]: CC BY 4.0 DAVIS [33]: BSD 3-Clause License YouTube-VOS [34]: CC BY 4.0 Pre-trained models. DepthAnythingv2 [65]: Apache-2.0 License and CC BY-NC 4.0 19 Table 5: Detailed model configuration of StreamSplat. Parameter Image Tokenizer input resolution input channels patch size output channels Static Encoder layers embed dim attention heads droppath rate Dynamic Decoder layers embed dim attention heads droppath rate Upsampler layers window size upsample ratio embed dim Prediction Heads static head deformation head Training optimizer Adam (β1, β2) lr scheduler epochs batch size peak learning rate weight decay gradient clipping warm-up iterations mixed precision Loss Weights λMSE λLPIPS λDepth λMask Value 288 512 RGB (3) + Depth (1) 8 768 10 768 12 0.0 10 768 12 0.1 2 2304 16 768 Linear 2-layer MLP AdamW (0.9, 0.95) CosineAnnealingLR 50 / 70 128 / 256 5 104 / 1 104 0.05 1.0 20K / 100K bf16 1.0 0.05 0.05 3."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "Nanyang Technological University",
        "University of British Columbia",
        "Vector Institute for AI"
    ]
}