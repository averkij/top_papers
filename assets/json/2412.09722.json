{
    "paper_title": "GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong Prompt Optimizers",
    "authors": [
        "Sarkar Snigdha Sarathi Das",
        "Ryo Kamoi",
        "Bo Pang",
        "Yusen Zhang",
        "Caiming Xiong",
        "Rui Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The effectiveness of large language models (LLMs) is closely tied to the design of prompts, making prompt optimization essential for enhancing their performance across a wide range of tasks. Many existing approaches to automating prompt engineering rely exclusively on textual feedback, refining prompts based solely on inference errors identified by large, computationally expensive LLMs. Unfortunately, smaller models struggle to generate high-quality feedback, resulting in complete dependence on large LLM judgment. Moreover, these methods fail to leverage more direct and finer-grained information, such as gradients, due to operating purely in text space. To this end, we introduce GReaTer, a novel prompt optimization technique that directly incorporates gradient information over task-specific reasoning. By utilizing task loss gradients, GReaTer enables self-optimization of prompts for open-source, lightweight language models without the need for costly closed-source LLMs. This allows high-performance prompt optimization without dependence on massive LLMs, closing the gap between smaller models and the sophisticated reasoning often needed for prompt refinement. Extensive evaluations across diverse reasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GReaTer consistently outperforms previous state-of-the-art prompt optimization methods, even those reliant on powerful LLMs. Additionally, GReaTer-optimized prompts frequently exhibit better transferability and, in some cases, boost task performance to levels comparable to or surpassing those achieved by larger language models, highlighting the effectiveness of prompt optimization guided by gradients over reasoning. Code of GReaTer is available at https://github.com/psunlpgroup/GreaTer."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 2 2 7 9 0 . 2 1 4 2 : r GREATER: GRADIENTS OVER REASONING MAKES SMALLER LANGUAGE MODELS STRONG PROMPT OPTIMIZERS Sarkar Snigdha Sarathi Das Ryo Kamoi Bo Pang Yusen Zhang Caiming Xiong Rui Zhang The Pennsylvania State University {sfd5525, rmz5227}@psu.edu Salesforce Research"
        },
        {
            "title": "ABSTRACT",
            "content": "The effectiveness of large language models (LLMs) is closely tied to the design of prompts, making prompt optimization essential for enhancing their performance across wide range of tasks. Many existing approaches to automating prompt engineering rely exclusively on textual feedback, refining prompts based solely on inference errors identified by large, computationally expensive LLMs. Unfortunately, smaller models struggle to generate high-quality feedback, resulting in complete dependence on large LLM judgment. Moreover, these methods fail to leverage more direct and finer-grained information, such as gradients, due to operating purely in text space. To this end, we introduce GREATER, novel prompt optimization technique that directly incorporates gradient information over taskspecific reasoning. By utilizing task loss gradients, GREATER enables selfoptimization of prompts for open-source, lightweight language models without the need for costly closed-source LLMs. This allows high-performance prompt optimization without dependence on massive LLMs, closing the gap between smaller models and the sophisticated reasoning often needed for prompt refinement. Extensive evaluations across diverse reasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GREATER consistently outperforms previous state-ofthe-art prompt optimization methods, even those reliant on powerful LLMs. Additionally, GREATER-optimized prompts frequently exhibit better transferability and, in some cases, boost task performance to levels comparable to or surpassing those achieved by larger language models, highlighting the effectiveness of prompt optimization guided by gradients over reasoning. Code of GREATER is available at: https://github.com/psunlpgroup/GreaTer."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have demonstrated impressive performance across various task domains (Brown, 2020; Achiam et al., 2023; Reid et al., 2024). However, these models are known to exhibit prompt sensitivity, phenomenon where slight variations in input prompts can lead to significant differences in output quality (Lu et al., 2021; Madaan & Yazdanbakhsh, 2022; Zhao et al., 2021; Reynolds & McDonell, 2021; Wei et al., 2022; Kojima et al., 2022). Consequently, prompt design has emerged as critical factor in achieving optimal LLM performance. As the popularity of LLMs has surged, Prompt Engineering has become focal point of attention in the field. Traditionally, this process has been carried out by domain experts who iteratively query expensive LLMs until the desired response is obtained. However, this manual approach is time-consuming and resource-intensive, prompting researchers to explore more efficient alternatives. Recent research has focused on Automated Prompt Engineering (Zhou et al., 2022), which aims to systematically search for prompts that improve target task performance. Following this line of research, (Pryzant et al., 2023; Ye et al., 2023) improved upon it by resorting to computationally expensive stronger LLMs to reason about failure causes in smaller, efficient LLMs deployed in practical tasks. Pryzant et al. (2023) termed this feedback as textual gradient, since this feedback is leveraged to improve the prompts iteratively. 1 Figure 1: Comparison of textual feedback-based prompt optimization and GREATER. Left: textual feedback relies entirely on larger language models judgments. Right: GREATER avoids external large, proprietary models, using token suggestions from small model and guiding prompt token selection with loss gradients. GREATER incorporates model reasoning by first generating reasoning, then applying an extraction prompt to obtain answer logits for computing loss gradients. This gradient over reasoning approach optimizes using direct signals rather than relying on language model feedback. Despite showing promising performance, the primary limitation of this category of prompt optimization is the reliance on massive LLMs like GPT-4 for optimizing smaller model performance. Smaller LLMs used in practice rely on large models like GPT-4 for optimization, as these big models generate the textual gradients needed to refine and transfer knowledge. (Zhang et al., 2024) found that smaller LMs are incapable of generating such optimization feedback, further emphasizing the dependence on large models. Thus, enhancing smaller models depends on the computational power of larger ones. Additionally, the optimization process increases computational costs due to the need for sizeable prompt length due to multiple task samples, and heavy dependence on the optimizer LLMs judgment may result in less reliable outcomes. To mitigate these issues, we propose GREATER that allows smaller LLMs to optimize prompts using true gradients (i.e., numerical loss gradients) without resorting to larger models. Figure 1 (right) gives an overview of our approach. GREATER leverages gradient over reasoning for more accurate prompt improvement direction. GREATER first calculates the forward token probabilities to generate small number of probable token candidates at the selected position conditioned on the input. Then, it utilizes the LLM to generate the reasoning for problem solution, and extracts the final answer logits for calculating the loss. Finally, we leverage the gradient calculated for the probable token candidates to select the best tokens for optimization. Our technique innovates by simultaneously addressing token discreteness and incorporating reasoning chains, particularly for scenarios where limited datasets (e.g., Big Bench Hard (Suzgun et al., 2022)) lack explicit reasoning paths, and only give final labels. GREATER shows strong prompt optimization performance, where optimized prompt often delivers performance equivalent to larger LLMs in solving the task. In our experiments, we selected Llama-3-8B-Instruct (Meta, 2024) and Gemma-2-9B-it (Team et al., 2024), two highly popular smaller language models that are proven to be also very useful in solving different tasks. Across wide variety of selected BBH (Suzgun et al., 2022) tasks, mathematical reasoning task GSM8k, and first-order logic task, FOLIO (Han et al., 2022), GREATER shows up to 8.9% performance improvement over SOTA prompt optimization technique on average in BBH suite of tasks. Moreover, GREATER optimized prompts perform on par or better than GPT-4 optimized prompts (Ye et al., 2023), demonstrating superior performance without resorting to larger proprietary LLMs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "LLMs as Prompt Optimizers. Recently, significant attention has been brought to the prospect of LLMs as prompt optimizers for less powerful models. This line of work was first proposed by Zhou et al. (2022) by prompting LLMs with input-output pairs to infer the instruction. Pryzant et al. (2023) formalized the term textual gradient to refer to textual feedback based optimization. Here 2 the authors introduced the use of mini-batches of data to create natural language feedback. These gradients critique the current prompt, mimicking the role of numerical gradient in optimization. Later, large body of works has improved upon it by using optimization logic in text space (Yang et al., 2023; Yuksekgonul et al., 2024), meta-prompt engineering (Ye et al., 2023), agent-based learning and reasoning (Wang et al., 2023; Liu et al., 2024; Shinn et al., 2024), external trained model (Cheng et al., 2023), evolutionary algorithms (Guo et al., 2023; Liu et al., 2024), etc. Hu et al. (2024) introduced ZOPO, leveraging Gaussian processes inspired by the Neural Tangent Kernel to systematically explore local optima in prompt optimization using zeroth-order methods. Other approaches like programming model (Khattab et al., 2023), editing (Prasad et al., 2022), and reinforcement learning (Deng et al., 2022) are also notable. These techniques usually rely on reasoning and judgment from larger LMs to improve the performance of smaller LMs. In other words, the larger model can share its knowledge through the optimized prompt which helps smaller models achieve performance uplift. Therefore, to get strong results with smaller, more lightweight language models, prompts must be optimized using powerful, expensive, closed-source models, as smaller models are inadequate at this optimization on their own (Zhang et al., 2024). Prompt Tuning. Prompt-tuning has been explored in prior works as task-specific continuous vectors tuned by gradient-based methods to improve task performance (Li & Liang, 2021; Lester et al., 2021; Qin & Eisner, 2021; Gao et al., 2020). Discrete prompts on the other hand involve searching for discrete vocabulary tokens through gradients (Shin et al., 2020; Shi et al., 2022). These approaches can be further extended for visual prompt tuning (Wen et al., 2024), where the authors optimize hard text based prompts through efficient gradient-based optimization. fundamental flaw in these methods stems from the fact that these methods are typically only suitable for classification tasks or tasks with fixed input-output structures, as they rely on predefined templates and verbalizers. Reasoning tasks on the other hand require complex analytical reasoning chains, e.g., Big-Bench-Hard (Suzgun et al., 2022), which leads to the final output, where using fixed template verbalizer is incompatible and impractical. Jailbreaking LLMs. Gradient-based search methods have also been applied to find trigger prompts that bypass LLM alignment-based filtering and generate harmful responses (Zou et al., 2023). These methods have been further refined to improve readability and effectiveness by including perplexity regularization and constrained-decoding (Guo et al., 2021; Alon & Kamfonas, 2023; Liu et al., 2023; Guo et al., 2024). Similar to prompt tuning, these methods also adhere to simple input-output structure. The target output typically is an affirmative response Sure here is, without emphasis on the reasoning chain."
        },
        {
            "title": "3 PROBLEM DEFINITION",
            "content": "We formally define the problem of prompt optimization to lay the foundation of the optimization target. Given language model fLLM, and small representative task dataset, Dtask = {(x1, y1), . . . (xn, yn)}, the goal of prompt optimization is to find prompt such that: = arg max (cid:88) (x,y)Dtask (fLLM(x; p), y) (1) where fLLM(x; p) is the output from task language model fLLM upon channeling the input with the prompt p, and m() is the evaluation function for this task. Textual Feedback Based Prompt Optimization. As shown in the left part of Figure 1, to search for p, previous prompt optimization methods based on textual feedback use an optimizer model foptimizer which is usually substantially larger and more expensive than fLLM (Zhou et al., 2022; Ye (cid:1) drives et al., 2023; Pryzant et al., 2023). Conceptually, foptimizer the optimization process by assessing and providing feedback for refining the prompt. Therefore, finding primarily relies on the capabilities of foptimizer and its hypothesis for prompt refinement. (cid:0)m(fLLM(x; p), y)(x, y) Dtask 3 Figure 2: Overall workflow of GREATER. (i) The language model fLLM generates token candidates by conditioning on input samples. (ii) fLLM uses task input and current prompt to generate reasoning and extract final answer logits. (iii) The logits are used to calculate loss and compute gradient over generated reasoning with respect to the candidate tokens. These gradients determine the selection of candidate token to update the current position of the current prompt."
        },
        {
            "title": "4 OUR METHOD",
            "content": "While Section 3 provides the formal explanation of prompt optimization, its important to understand the role of reasoning in this process. well-crafted prompt gives clear problem-solving instructions that guide the language model to think through the problem in specific way, helping it arrive at valid answer. Our method, GREATER, is built on this principle. As shown in Figure 2, GREATER begins by analyzing task examples to propose potential token candidates, essentially exploring different ways to solve the task. The task input and prompts are then used to generate reasoning steps, from which the final answer and loss are extracted. By applying gradients over the generated reasoning, GREATER refines the candidate selection, ensuring it follows the optimal path for improved performance. 4.1 METHOD OVERVIEW Given an input and prompt = [p1, p2, p3, . . .] consisting of several tokens pi, fLLM generates the reasoning chain, fLLM(x p) for the input ( for concatenation). Then, we can extract the final answer from by prompting fLLM with formatted extractor prompt pextract, e.g., Therefore, the final answer ($NUMBER$) is $. Consequently, this produces the final answer logits, = fLLM(x pextract). We define our loss function as: (cid:18) = LCE fLLM (cid:16) pextract (cid:19) (cid:17) , (2) (cid:12) In Eq. 2, for fixed pextract and x, only will affect the loss; therefore we can calculate . (cid:12)x,pextract This loss gradient takes the reasoning into account, making it more direct signal to drive the prompt optimization process. Therefore, this gradient over reasoning can be highly potent alternative to current textual feedback based prompt optimization that entirely relies on massive LLM feedback. Note that, in Eq. 2, the entire chain is equivalent to one single forward pass through fLLM, since key, values from reasoning generation can be cached and utilized during logit extraction. We sequentially apply GREATER optimization in each token position pi of p. GREATER first employs Candidate Proposal (Section 4.2) stage, where it uses language model-guided forward probabilities to generate potential token candidates for optimized prompt. Subsequently, the current prompt is channeled through LLM, fLLM for solution generation, and then answer logits extraction (Section 4.3). Using these logits, we can calculate the loss and gradient with respect to the small number of prompt token candidates which we use for token selection. (Section 4.4). This process can be applied over all the prompt token positions repeatedly. In the following subsections, we discuss each stage of GREATER to optimize prompt token pi at position i. Algorithm 1 shows all the steps in GREATER. This process is sequentially applied over all the positions repeatedly until convergence. 4 Algorithm 1 GREATER 1: Input: Initial Prompt pinit = (p1, p2, p3, . . . ), Task Dataset Dtask = (X, ), Language Model fLLM, Extractor Prompt pextract 2: Output: Optimized Prompt 3: p, p, Lbest pinit, pinit, 4: position, 0 5: for 1 to do 6: 7: 8: 9: 10: xj Dq // Candidate Proposal Stage candidatesi (cid:84) candi,j, where Dq Dtask Compute one-hot token indicator ϵi for candidatesi // Reasoning Generation and Extraction Calculate logits, ˆY {fLLM(x fLLM(x p) pextract)x D} From Eqn. 3 and Eqn. 4 From Section 4.2 Extract Solution and Logits (From Eqn. 5) 11: 12: 13: 14: 15: // Gradient Over Reasoning driven Candidate Selection Compute total Loss L( ˆY , ) for and ϵi // select top-µ tokens with gradients and find the candidate with the lowest loss selectionsi token[top-µ( )], where µ = 3 ϵi (x,y)D (fLLM(x p<i p>i), y) Find the best pi, Lmin arg mincselectionsi From Eqn. 7 (cid:80) candidate, assuming arg min also returns min loss if Lmin < Lbest then Lbest Lmin p 16: 17: 18: 19: 20: return (i + 1) mod length(pinit) Update the best loss Update the best prompt 4.2 PROMPT TOKEN CANDIDATE PROPOSAL For optimizing prompt token pi, we first leverage the task language model (LM) fLLM probabilities to propose candidates for the position. For sample input xj Dtask, we can calculate the top-k probabilities for candidate token proposals: candi,j = top-k(cid:2)fLLM(xj p1, p2, . . . , pi1)(cid:3) Therefore, we take top-k tokens from fLLM conditioning on xj followed by the previous tokens from the optimized prompt. We calculate candi,j for randomly sampled set of inputs, Dq Dtask. Since we want the token candidates to be relevant for all samples in the task, token candidates for position will incorporate candidates for all samples, so (3) candidatesi = (cid:92) xj Dq candi,j (4) Equation 4 gives candidatesi, set of promising token candidates for position i. Since candidatesi are suggested by LM and conditioned on the inputs, they are representative of the problem domain and more interpretable. Therefore, gradient mass will be calculated only for small number of promising token candidates, ϵi, preventing it from being dispersed over the entire vocabulary. One-Hot Token Indicators. As shown in Figure 2 (i), one-hot token indicator ϵi is created for only candidatesi and the current token pi, with value of one only for pi and zeros for all other candidates. Concurrently, from the original LM embedding table, we extract only the rows that correspond to ϵi to create subset of the original embedding table, E. This table is then multiplied by the one-hot indicator ϵi, allowing the input to be passed through fLLM. 4.3 REASONING GENERATION AND EXTRACTION straightforward way of calculating loss and subsequently taking gradient over it is to only consider the output of fLLM(x p) and taking cross-entropy loss of the output token to that of the ground truth. However, this completely ignores the role of reasoning as discussed before, whereas modern language models require the generation of complex reasoning chain to generate correct output 5 (Wei et al., 2022). Therefore, simply considering fLLM(yx p) would give us the wrong objective to optimize, which in turn will give incorrect gradient information. Consequently, in this stage, we first generate reasoning fLLM(x p). In r, the language model generates reasoning to derive the final answer. To compute the loss accurately, we must extract the answer logits and compare with the ground truth label. simple way to do that is to get fLLM logits when conditioned on the input, reasoning, followed by formatted extractor prompt, pextract. ˆy = fLLM(x pextract) (5) Equation 5 results in LM logits that consider the reasoning generated from the current prompt p. Therefore, it better represents the reasoning chain making it suitable for loss calculation."
        },
        {
            "title": "4.4 GRADIENT OVER REASONING DRIVEN CANDIDATE SELECTION",
            "content": "Equation 5 gives us the logits ˆy that incorporate reasoning chain information originating from current p. Therefore, we can optimize the cross-entropy loss as below. Additionally, we add the perplexity regularization term Lperpl to promote the interpretability of the optimized prompt similar to (Zou et al., 2023; Guo et al., 2024). LCE = cross entropy(ˆy, y), Lperpl = exp log fLLM(pi x, p<i) , = LCE + λLperpl 1 p (cid:88) i=1 (6) However, the perplexity term is less important in our case given that we are optimizing only over the top-k candidates suggested by the LM. Consequently, the candidate proposal stage handles most of the interpretability duties. Upon calculating the loss L, we can do backward pass to calculate the gradient over the generated reasoning with respect to one-hot token indicator ϵi. As gives us the gradient directions for each ϵi of the token candidates in Equation 4, the token replacement at position is given by: pi = token[arg max ( ϵi ϵi )] (7) In practice, we select the top-three candidate tokens with the highest negative gradients and evaluate the token replacement with forward pass on the training set. This ensures the robustness of the replacement selection process leading to higher performance from optimization."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we demonstrate that GREATER is highly effective in prompt optimization delivering substantial performance improvement across different tasks. Section 5.1 describes the experiment setup. Section 5.2 presents the main results of the GREATER performance with smaller language models. In Section 5.3, we compare GREATER prompts optimized by smaller language models against the prompts optimized by larger proprietary language models using state-of-the-art baseline methods. Section 5.4 performs an ablation study on the effectiveness of gradient over reasoning in GREATER. Section 5.5 demonstrates the transferability of GREATER prompts. Section 5.6 shows some case studies. 5.1 EXPERIMENT SETUP Datasets. To evaluate the efficacy of our approach, we use GSM8K (Cobbe et al., 2021), BigBench-Hard (BBH) (Suzgun et al., 2022), and FOLIO (Han et al., 2022) benchmark datasets for diverse reasoning tasks in mathematics, commonsense, and logical reasoning. For GSM8K, we used the same test split as (Cobbe et al., 2021). For BBH, we used the similar split setting as (Yuksekgonul et al., 2024). For FOLIO, we evaluate the natural language reasoning with the first-order logic task for the evaluation which also requires complex reasoning capabilities. We used the natural language reasoning task with premises text, conclusion text to infer the labels, and we use the validation set of the updated version of FOLIO1. More details about these are given in Appendix A.1. 1https://huggingface.co/datasets/yale-nlp/FOLIO 6 Table 1: Overall results. GREATER brings substantial performance improvements across different reasoning tasks, demonstrating its efficacy in prompt optimization with smaller models. It considerably outperforms state-of-the-art prompt optimization methods. Detailed prompts and results with breakdown across all the tasks are shown in Appendix and Appendix I. Method Gemma-2-9B Llama-3-8B GSM8K BBH FOLIO GSM8K BBH FOLIO ZS-CoT (Kojima et al., 2022) APE (Zhou et al., 2022) APO (Pryzant et al., 2023) PE2 (Ye et al., 2023) TextGrad (Yuksekgonul et al., 2024) GREATER 88.6 88.6 88.6 88.6 87.8 89.4 71.7 71.7 72.3 68.9 72.9 76.6 65.0 67.5 63.1 62.1 67.5 69.1 79.6 79.9 81.1 80.1 78.5 82.6 62.2 63.1 62.7 61.5 58.5 68.7 58.6 57.6 58.6 62.6 56.2 62. Models. To demonstrate that our approach can generalize to different backbone language models, we choose two strong and widely-used open-source language models: Llama-3-8B (Meta, 2024) and Gemma-2-9B (Team et al., 2024). For both models, we use the instruction tuned versions 2 3. Baselines. We compare with state-of-the-art prompt optimization baselines: APE (Zhou et al., 2022), APO (Pryzant et al., 2023), PE2 (Ye et al., 2023), TextGrad (Yuksekgonul et al., 2024) where we use them on Llama-3-8B and Gemma-2-9B for optimization. In addition, we also show the efficacy of our approach by comparing against the prompts optimized by massive proprietary LLMs (e.g., GPT-4, PaLM-2-L) using APE, APO, PE2, TextGrad, OPRO (Yang et al., 2023), EvoPrompt (Guo et al., 2023). 5.2 OVERALL RESULTS We evaluate the performance of GREATER in comparison to state-of-the-art baselines, including APE, APO, PE2, TextGrad, and the original zero-shot chain-of-thought approach (Kojima et al., 2022), across the GSM8K, BBH, and FOLIO benchmarks. Table 1 highlights the significant and consistent improvements achieved by GREATER over state-of-the-art prompt optimization methods, especially when optimizing prompts using lightweight language models. Across both Gemma2-9B and Llama-3-8B, GREATER demonstrates remarkable stability and outperforms the baselines. This is particularly noteworthy given the high variability in performance seen in these textual feedback methods, which tend to be unreliable with smaller models due to increased prompt sensitivity. For instance, while TextGrad performs reasonably well on BBH with Gemma-2-9B, it falters significantly with Llama-3-8B. In contrast, GREATER delivers outstanding results across the board, excelling across diverse tasks. Note that, in the FOLIO dataset, GREATER performs on par with PE2 (Ye et al., 2023), suggesting that the model has likely reached its performance ceiling, leaving minimal room for further improvement. 5.3 COMPARISON WITH PROMPTS OPTIMIZED BY LARGER PROPRIETARY MODELS Besides optimization performance in lightweight language models, we also evaluate the efficacy of GREATER optimized prompt compared with the prompts optimized by massive LLMs. These massive models, like GPT-4 and PaLM-2-L, inherently possess deeper knowledge of complex reasoning tasks and often provide richer guidance in their optimized prompts, making them strong baselines to compare with. In Table 2, we compare the performance of GREATER optimized prompt to that of APE (optimized by GPT-4), APO (optimized by GPT-4), PE2 (optimized by GPT-4), OPRO (optimized by PaLM-2-L), EvoPrompt(optimized by GPT-3.5). We use GSM8K, and five randomly selected tasks from BBH to compare the performance of these prompts. We see that in both Llama-38B and Gemma-2-9B, GREATER optimized prompts perform substantially better and robustly compared to the baseline prompts optimized with massive LLMs. The consistency in performance uplift 2https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct 3https://huggingface.co/google/gemma-2-9b-it 7 Table 2: Comparison of GREATER with prompts optimized by larger proprietary LLMs. GREATER performs on par with or notably better than prompts optimized by GPT 4 and PaLM-2-L across GSM8K and five randomly chosen BBH tasks using Llama-3-8B and Gemma-2-9B. EvoPrompt does not report its prompts on GSM8K. Here, Target Model: Llama-3-8B and Method (Optimized by): APE (GPT-4) indicates that Llama-3-8B was used for prompt evaluation while the prompt was optimized by GPT-4 with APE. Target Model Method (Optimized by) GSM8K BBH (5 randomly chosen tasks) movie rec. object count. tracking five. hyperbaton causal Average 8 - 3 - l B 9 - 2 - e APE (GPT-4) EvoPrompt (GPT-3.5) APO (GPT-4) PE2 (GPT-4) OPRO (PaLM-2-L) GREATER (Llama-3-8B) APE (GPT-4) EvoPrompt (GPT-3.5) APO (GPT-4) PE2 (GPT-4) OPRO (PaLM-2-L) GREATER (Gemma2-9B) 80.7 - 81.1 81.5 82.3 82.6 89.2 - 89.3 89.6 89.0 89.4 50 48 56 48 60 48 51 52 50 50 56 82 74 68 82 78 90 61 70 84 65 58 87 50 42 49 45 40 70 83 82 72 71 76 85 76 68 75 79 70 83 83 82 84 81 88 56 48 51 49 57 57 60 61 59 64 58 61 62.8 56.0 59.8 60.6 61.0 71.6 67.0 69.4 69.8 66.8 64.6 75.4 Figure 3: Ablation study on Gradient Over Reasoning in GREATER. Gradient calculation without reasoning causes notable performance drops, showing the importance of reasoning for gradients. makes GREATER viable choice to boost task performance in reasoning tasks with lightweight language models. 5.4 ABLATION OF GRADIENT OVER REASONING As outlined in Section 4.3 and Section 4.4, core feature of GREATER is the concept of Gradient Over Reasoning. Conventional gradient-based optimization methods rely solely on the target answer labels to compute the loss and gradients, overlooking the crucial role that reasoning plays in this process. To highlight the significance of incorporating reasoning into gradient calculations, we conducted comparative analysis of GREATER with and without applying Gradient Over Reasoning, while keeping all other steps identical. For this experiment, we optimized with both Llama-3-8B and Gemma-2-9B on two BBH tasks: movie recommendation and tracking shuffled objects five objects task. Figure 3 shows the comparison between with and without gradient over reasoning. It clearly demonstrates substantial performance drop when Gradient Over Reasoning is omitted. Fundamentally, from Equation 5, removing the reasoning generation part equates to calculating an incorrect objective function, which will drive gradient-based token selection to diverge from the optimal solution. 8 Table 3: Transferability of Llama-3-8B optimized prompts to Gemma-2-9B (Upper) and vice versa (Lower). The results demonstrate that prompts produced by GREATER exhibit strong transferability compared with those produced by other state-of-the-art prompt optimization methods. Target Model Method (Optimized by) movie rec. object count. tracking five. hyperbaton causal judgement Average BBH (5 randomly chosen tasks) TextGrad (Llama-3) APO (Llama-3) PE2 (Llama-3) GREATER (Llama-3) APO (GPT-4) TextGrad (Gemma-2) APO (Gemma-2) PE2 (Gemma-2) GREATER (Gemma-2) APO (GPT-4) 9 - 2 - e 8 - 3 - l 53 53 54 55 52 35 54 56 58 56 Llama-3-8B Gemma-2-9B 56 68 68 85 72 84 84 82 91 Gemma-2-9B Llama-3-8B 49 48 49 56 49 65 71 50 70 75 78 84 84 90 84 29 69 69 87 68 63 58 60 60 36 53 53 52 51 66.8 69.4 69.6 76.2 69.8 42.8 59.0 55.4 64.6 59.8 Table 4: Transferability of Llama-3-8B optimized prompts to Gemma-2-27B. The results demonstrate that GREATER optimized prompts exhibit strong transferability from smaller to larger language models. Target Model Method (Optimized by) movie rec. object count. tracking five. hyperbaton causal judgement Average BBH (5 randomly chosen tasks) 7 2 - 2 - e PE2 (Llama-3) APO (Llama-3) Ours (Llama-3) APO (GPT-4) 59 53 59 64 92 92 91 92 83 83 86 81 73 72 82 54 58 57 58 72.2 71.6 75.0 74.4 Llama-3-8B Gemma-2-27B 5.5 PROMPT TRANSFERABILITY We conduct further experiments to evaluate the transferability of GREATER optimized prompt. In Section 5.5.1, we first evaluate the transferability of GREATER prompt between two smaller language models, Llama-3-8B and Gemma-2-9B, by evaluating the performance of Gemma-2-9B optimized prompts on Llama-3-8B, and vice-versa. Next, in Section 5.5.2, we evaluate the transferability of prompts from smaller model (Llama-3-8B) to much larger model (Gemma-2-27B). 5.5.1 TRANSFER BETWEEN GEMMA-2 AND LLAMAAs shown in Table 3, GREATER exhibits exceptional transferability across smaller models compared to other methods. Notably, prompts optimized by GREATER significantly outperform even APO prompts, which were optimized using GPT-4, as reported by Pryzant et al. (2023). This distinction is particularly important, as it highlights that GREATER optimized prompts are not only more effective but also better suited for broader, more generalized use across smaller models. These results clearly emphasize the efficacy and versatility of our approach. 5.5.2 TRANSFER TO LARGER MODELS To assess the transferability of GREATER-optimized prompts to much larger models, we evaluated their efficacy using the Gemma-2-27B model. As demonstrated in Table 4, the GREATER-optimized prompts continue to exhibit strong performance when compared to baseline methods. However, notable observation is that the performance gap between GREATER and the baselines narrows as the model size increases. Additionally, we notice that APO GPT-4 optimized prompts yield similar performance with GREATER across various tasks, often resulting in closely matched outcomes. 9 Table 5: Example prompts (abridged) generated by GREATER and APO. GREATER prompts guide structured ways to solve tasks, leading to improved task performance compared to traditional Chain of Thought (CoT) prompts and their variations often generated by textual feedback-based optimization methods like APO. More examples can be found in the Appendix and I. Task Optimized Prompt by GREATER Optimized Prompt by APO llama3-formal fallacies Use formal notation and and think step . . . Analyze the argument step by step considllama3-causal judgement Use causal diagram... llama3-object counting llama3-navigate llama3sports understanding gemmamultistep arithmetic two gemma-date understanding gemma reasoning colored Use only addition. Add think step by ... Use your reasoning here. would like numbers assigned.. to.. To represent moving Use the context or sentence similar prior knowledge. Assume you journalist, would have been covering NHL hockey in Minnesota before joining this assignment to report sports. Use parentheses and and the step wise order... Use your format Excel formula for this answer to find it... Use your logic. Please answer. person . . . ering premises, logical . . . Analyze the situation by identifying the direct and indirect causes . . . Lets think step by step. Analyze the instructions step by step, considering each actions ... Assess the plausibility of the sentence, considering both literal and figurative meanings, as well as context and domain knowledge. Evaluate the sentences coherence and relevance to the given context . . . Lets think step by step. Lets think step by step. Analyze the given text and answer . . . Tables 3 and 4 reveal that optimized prompts enable smaller models to achieve performance comparable to larger ones, highlighting an efficient approach to maximize model capabilities without relying on resource-intensive models. 5.6 CASE STUDY One of the key characteristics of GREATER is that the generated prompts tend to be more varied and innovative compared to the textual feedback-based SOTA baselines. As noted in Table 11 and Table 14, the optimized prompts from traditional approaches are often verbose variations of the original query or standard Chain of Thought (CoT) prompts. In contrast, GREATER prompts frequently provide highly insightful guidelines that better aid in problem-solving. Table 5 demonstrates selection of abridged prompts generated by GREATER and APO. For tasks like formal fallacies and causal understanding, GREATER prompts encourage logical analysis rather than mere languagebased reasoning, which enhances performance. Similarly, in the object counting and navigate tasks, the prompts simplify the original problem by converting it into more straightforward mathematical task, leading to significant improvements. More interestingly, in the sports understanding task, the prompt promotes agentic behavior, encouraging the model to take an active role in solving the task. Additionally, for multistep arithmetic two, the prompt focuses on parentheses to avoid errors in computation, in date understanding task it instructs to convert into programming problem (by using Excel formula), and in reasoning about colored objects focuses on logical analysis. These prompts are comparatively more interesting than prompts generated by textual feedback (e.g. APO) where most prompts only show verbose instruction regarding the original question, without any guidance of how to solve the problem. They also demonstrate that GREATER prompts do more than just provide basic instructionsthey offer clear and practical strategies tailored to each task. By helping the model think about problems in more structured and systematic way, GREATER leads to better problem-solving and improved performance compared to traditional methods. While GREATER generates highly effective and innovative prompts, they may occasionally exhibit minor grammatical issues or more informal tone compared to standard prompts. For instance, the navigate task and multistep arithmetic two task in Table 5 reflect such characteristics. However, these issues can be mitigated by adjusting the Top-k parameter in Equation 3. Incorporating dynamic Top-k selection could further enhance the naturalness and accuracy of the prompts."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We present GREATER, novel gradient-guided prompt optimization technique that enhances performance without relying on massive proprietary LLMs. GREATER proposes token candidates using sample inputs, generates reasoning, and extracts final answer logits to compute loss and gradients. This Gradient over Reasoning provides strong signal for selecting optimal candidates, enabling significant performance gains on tasks like BBH, GSM8K, and FOLIO, outperforming state-of-theart textual feedback-based methods for smaller models. Additionally, GREATER prompts demonstrate notable transferability across smaller models and often match the performance of larger models. Future directions include integrating GREATER with textual feedback-based methods for more robust and effective prompt optimization."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Gabriel Alon and Michael Kamfonas. Detecting language model attacks with perplexity. arXiv preprint arXiv:2308.14132, 2023. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, and Minlie Huang. Black-box prompt optimization: Aligning large language models without model training. arXiv preprint arXiv:2311.04155, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548, 2022. Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020. Chuan Guo, Alexandre Sablayrolles, Herve Jegou, and Douwe Kiela. Gradient-based adversarial attacks against text transformers. arXiv preprint arXiv:2104.13733, 2021. Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023. Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, and Bin Hu. Cold-attack: Jailbreaking llms with stealthiness and controllability. arXiv preprint arXiv:2402.08679, 2024. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022. Wenyang Hu, Yao Shu, Zongmin Yu, Zhaoxuan Wu, Xiangqiang Lin, Zhongxiang Dai, See-Kiong Ng, and Bryan Kian Hsiang Low. Localized zeroth-order prompt optimization. arXiv preprint arXiv:2403.02993, 2024. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas Joshi, Hanna Moazam, et al. Dspy: arXiv preprint Compiling declarative language model calls into self-improving pipelines. arXiv:2310.03714, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. Large language models as evolutionary optimizers. In 2024 IEEE Congress on Evolutionary Computation (CEC), pp. 18. IEEE, 2024. Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451, 2023. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021. Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022. AI Meta. Introducing meta llama 3: The most capable openly available llm to date. Meta AI, 2024. Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based instruction search for prompting large language models. arXiv preprint arXiv:2203.07281, 2022. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with gradient descent and beam search. arXiv preprint arXiv:2305.03495, 2023. Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended abstracts of the 2021 CHI conference on human factors in computing systems, pp. 17, 2021. Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer. Toward human readable prompt tuning: Kubricks the shining is good movie, and good prompt too? arXiv preprint arXiv:2212.10539, 2022. Taylor Shin, Yasaman Razeghi, Robert Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric Xing, and Zhiting Hu. Promptagent: Strategic planning with language models enables expertlevel prompt optimization. arXiv preprint arXiv:2310.16427, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. Advances in Neural Information Processing Systems, 36, 2024. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023. Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. Prompt engineering prompt engineer. arXiv preprint arXiv:2311.05661, 2023. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic differentiation via text. arXiv preprint arXiv:2406.07496, 2024. Tuo Zhang, Jinyue Yuan, and Salman Avestimehr. Revisiting opro: The limitations of small-scale llms as optimizers. arXiv preprint arXiv:2405.10276, 2024. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pp. 1269712706. PMLR, 2021. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022. Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. arXiv preprint Universal and transferable adversarial attacks on aligned language models. arXiv:2307.15043, 2023. BENCHMARK DATASETS, MODELS, AND BASELINES In this section, we discuss the details of Datasets, Models, and Baselines. A.1 DATASETS As discussed in Section 5.1, we use GSM8K (Cobbe et al., 2021), BBH (Suzgun et al., 2022), and FOLIO (Han et al., 2022) datasets. For GSM8K, we used 100/100 for train/dev set, and original test set of 1319 size. Then, for BBH datasets, we used 21 selected BBH tasks as in Table 11 and Table 14. This covers almost all types of tasks in BBH dataset. We skip word sorting and dyck languages tasks from our evaluation since we found that the smaller LLM outputs are very difficult to reliably evaluate due to highly inconsistent output pattern. Finally, for logical deduction and tracking shuffled objects, we select five objects tasks as representative of the tasks. For all tasks we use 50/100/100 train/dev/test splits similar to (Yuksekgonul et al., 2024). Finally for the FOLIO dataset, we used the latest version of FOLIO (Han et al., 2022) for our evaluation. We use the natural language reasoning task with premises text, conclusion text to infer the labels. The original validation split (203 rows) are used for the evaluation of FOLIO, whereas 50/100 samples are taken for train and dev set respectively out of the original train split. 13 A.2 MODELS As described in experimentation setup, we only used Llama-3-8B-Instruct and Gemma-2-9B-it in our experiments due to their smaller footprint and strong performance. However, in several tasks Gemma-2-9B-it shows markedly stronger performance than Llama-3-8B, although the memory requirements for Gemma-2-9B-it is substantially higher. As we have shown, stronger models get lesser benefit from prompt optimization, as good prompts only offset for lower capabilities of weaker models. Across our experiments, we also notice substantial improvement in Llama-3-8B-Instruct, whereas the margin gets narrower for the stronger Gemma-2-9B-it. A.3 BASELINES Our primary baselines are APO (Pryzant et al., 2023), iterative APE (Kojima et al., 2022), PE2 (Ye et al., 2023), TextGrad (Yuksekgonul et al., 2024). For APO, iterative APE, and PE2 we use the original implementation by (Ye et al., 2023) for benchmarking. And we used TextGrads own library for evaluating their results. Other than that, we also compared against original Zero-Shot CoT (Wei et al., 2022), and the larger model optimized reported prompts from APE, APO, PE2, OPRO (Yang et al., 2023), EvoPrompt (Guo et al., 2023)."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTATION DETAILS",
            "content": "As shown in Algorithm 1, we run GREATER for = 105 steps with = 10 for top-k, = 5 and λ = 0.2 (in Eq. 6). While Algorithm 1 shows fixed length prompt optimization, we allow dynamic prompt. To attain that, we look for the presence of ending token in the last position. If no ending token is found, we continue the optimization process by adding placeholder token and optimizing over it. As ending token is encountered we move over to the first position for optimization. We run all our experiments on 2X NVIDIA A100 80GB GPUs. PERFORMANCE COMPARISON WITH OTHER GRADIENT-BASED METHODS GREATER leverages the Gradient Over Reasoning chain to identify the optimal prompt required for enhancing task performance. Previous works, such as Shin et al. (2020), utilized gradient-guided search to discover trigger tokens for improving performance in relatively straightforward tasks like sentiment analysis and natural language inference. This concept has also been extended to textto-image tasks by incorporating gradient-based learning in embedding spaces (Wen et al., 2024). However, these approaches are limited to simpler tasks and fail to address tasks requiring complex reasoning. As demonstrated in Table 6, GREATER-optimized prompts deliver substantial performance boost compared to prior methods that do not incorporate the reasoning chain. 6: Comparison Table tracking shuffled objects five objects for prompts optimized on Llama-3-8B and Gemma-2-9B. The results demonstrate that prompts optimized by GREATER outperform other methods across both models. in movie recommendation performance and of Method Llama-3-8B Gemma-2-9B movie rec. tracking five movie rec. tracking five AutoPrompt (Shin et al., 2020) PEZ (Wen et al., 2024) GREATER 51 42 38 35 70 50 45 57 73 73 86 COMPLEXITY COMPARISON: GREATER VS. TEXT-BASED FEEDBACK"
        },
        {
            "title": "APPROACHES",
            "content": "Assume each sample contains input-output tokens on average."
        },
        {
            "title": "FOR GREATER",
            "content": "Operations Per Sample: Forward Pass: O(L2) Backward Pass: O(L2) Total Complexity: For samples, one forward pass, and one backward pass are performed per sample in single iteration. Total complexity: O(L2) = O(N L2) FOR TEXT-BASED FEEDBACK APPROACHES Overview: No backpropagation stage. Each iteration involves, evaluating all samples, identifying incorrect samples (n incorrect samples), and chaining incorrect samples into single sequence to generate feedback. Complexity Breakdown: First Forward Pass: O(N L2) Feedback Generation: * Incorrect samples (n) are chained together. * Chained length: L. * Feedback generation complexity: Total Complexity: O((nL)2) = O(n2L2) O(N L2) + O(n2L2) Behavior Based on Task Difficulty: Simple Tasks (n 0): Total Complexity O(N L2) Difficult Tasks (n ): Total Complexity O(N L2) + O(N 2L2) = O(N 2L2) COMPARISON GreaTer Complexity: O(N L2) (consistent). Text-Based Feedback Complexity: Can scale up to O(N 2L2). difference complexity This In translated movie recommendation task, GREATER is required to be optimized for 5 hours, whereas TextGrad (Yuksekgonul et al., 2024) required total of 14 hours for prompt optimization in our own setup. performance. real-world also into is PROMPT OPTIMIZATION VS. FEW-SHOT IN-CONTEXT LEARNING Table 7: Impact of Initialization Prompt. We can see that different initialization has resulted in different optimized prompt, however they offer comparable performance. Initialization Prompt Optimized Prompt Default (Use proper logical ...) Misleading (Use no thinking just feeling.) Use movie ratings data available here above movies for reference. This HOFF has an interesting analysis based solely on options to options based on movies ratings. Expect from the other movies you are asked, choose option from those mentioned below. Use one one-liner and explain stepwise for why. ONLY READING IS ALLOWABLE AND NO CHATTY CHAT OR EXCL. Score 56 55 In-context learning has proven to be highly effective for reasoning tasks in large language models. This raises the question of whether prompt optimization provides any advantages over in-context learning for smaller models. In Figure 4, we compare the performance of fiveshot in-context learning with zeroshot reasoning using an optimized prompt from GREATER in Llama3-8B-Instruct. The results demonstrate that GREATER offers significant performance improvement over five-shot reasoning. Moreover, using an optimized prompt eliminates the need for repeated input-output examples during inference, leading to greater efficiency. Figure 4: Efficacy of GREATER in zero-shot setting compared to five-shot inference with Llama-3-8B-Instruct."
        },
        {
            "title": "F EFFECT OF INITIALIZATION",
            "content": "For GREATER, we start with fixed prompt: Use proper logical reasoning and think step by step. Finally, give the actual correct answer. This fixed initialization is convenient for adapting to various tasks. However, we also explore whether using different initialization affects performance. To test this, we conduct small-scale experiment with Llama-3-8B on the BBH-movie-recommendation task. Table 7 highlights the impact of different prompt initializations on optimization. As shown, completely misleading initialization leads to vastly different optimized prompt. However, both prompts result in very similar performance, despite solving the task in different ways. While the default initialization produces prompt that leverages movie ratings and genre information from databases, the misleading prompt emphasizes concise explanations. Despite these contrasting approaches, both deliver comparable outcomes."
        },
        {
            "title": "G PROMPT OPTIMIZATION PERFORMANCE IN VERY SMALL LANGUAGE",
            "content": "MODEL : LLAMA-3.2-1B-INSTRUCT While we primarily focused on two popular small language models - Llama-3-8B-Instruct and Gemma-2-9B-it, it is also interesting to see how GREATER perform for even smaller LLM - namely Llama-3.2-1B-Instruct. Given that is one of the smallest modern language models, getting good task performance out of it just by optimizing prompt can be very useful for real world task deployment. As we can see in Table 8, GREATER optimized prompts perform significantly better than prompts 16 Table 8: Performance comparison across selected BBH tasks for Llama-3.2-1B (target model for evaluation) with prompts optimized by different methods. The results demonstrate that prompts optimized by GREATER outperform other optimized prompts. Method (Optimized By) movie rec. causal judgement hyperbaton tracking five object count. AVG APO (GPT4) PE2 (GPT4) iAPE (GPT4) GREATER (Llama-3.2-1B) 23 27 32 45 54 57 69 61 69 53 62 24 21 15 24 51 59 41 67 40.8 46.0 39.6 53.6 Table 9: Optimized Prompts for LLama-3-8B-Instruct on GSM8K dataset. Method Optimized Prompt"
        },
        {
            "title": "TextGrad",
            "content": "APE APO PE2 You will answer mathematical reasoning question. Think step by step. The last line of your response should be of the following format: Answer: VALUE where VALUE is numerical value. Work in sequence: Complete each task in order, tackling one task at time, and only moving on to the next once its finished. Break down complex problems into smaller, logical steps, considering mathematical operations, variable relationships, and implicit rules. Provide clear, sequential solution, accounting for nuanced language and context. Break down complex problems into smaller, manageable steps, and solve them step by step. Score 78. 79.9 81.1 80.1 GREATER Use your knowledge reasoning and think step by step. Finally give the 82.6 actual correct answer. optimized by GPT-4 with other methods. This clearly shows the efficacy of GREATER in prompt optimization even with very small language models. PROMPT OPTIMIZATION RESULTS: LLAMA-3-8B-INSTRUCT H.1 GSM8K AND FOLIO: OPTIMIZED PROMPTS In Table 9 and Table 10, we first show the optimized prompts with Llama-3-8B-Instruct on GSM8K and FOLIO dataset respectively. As we can see, GREATER performs on par or better than all the baselines in every scenario. H.2 BIG BENCH HARD (BBH): OPTIMIED PROMPTS AND DETAILED RESULTS Figure 5 shows that GreaTer outperforms state-of-the-art (SOTA) prompt optimization methods, including APO (Pryzant et al., 2023), TextGrad (Yuksekgonul et al., 2024), APE (Zhou et al., 2022), and PE2 (Ye et al., 2023), across 71.4% to 85.% of tasks. full breakdown of these results is provided in Figure 6. Upon closer inspection, it becomes evident that GREATER consistently matches or surpasses the performance of other SOTA methods, highlighting the robustness and reliability of our approach. In contrast, other methods show markedly less consistency due to their sole reliance on LLM judgment. Finally, Table 11 presents all the optimized prompts generated by the baseline methods and our proposed approach. TextGrad prompts are truncated due to their excessive length, making them challenging to fit on page, which may also explain their inconsistent performance. 17 Table 10: Optimized Prompts for LLama-3-8B-Instruct on FOLIO dataset. Method Optimized Prompt"
        },
        {
            "title": "APE\nAPO",
            "content": "PE2 You will answer reasoning question by identifying the essential information, making specific conclusions, and providing nuanced and detailed reasoning. Think critically and systematically, focusing on the most relevant details, and avoid unnecessary complexity... Tackle it incrementally! Analyze the premises step by step, identifying specific details, assumptions, and ambiguities. Draw logical conclusion based on the evidence provided, considering multiple perspectives and potential counterarguments, while accounting for scope, context, and edge cases. Analyze the statement based on the provided premise, determining whether it is true, false, or uncertain. Consider all relevant information to reach logical conclusion. GREATER Use of logical deductions to show if your conclusion matches an appropriate option you chose from multiple options above by explaining how to determine whether the given conclusion follows from the given information above by explaining each step taken during the process. Score 56.2 57.6 58. 62.6 62.6 Figure 5: Win/Draw/Loss Comparison of GREATER and SOTA prompt optimization techniques APO, TextGrad, APE, and PE2 in optimization with Llama-3-8B-Instruct. GREATER maintains significant winning margin over these methods, highlighting its effectiveness in optimization. Table 11: List of optimized prompts for BBH tasks by different prompt optimization methods with Llama-3-8B-Instruct. Method Optimized Prompt PE2 APE TextGrad formal fallacies Determine the validity of the given argument. Simplify and analyze. You will answer reasoning question by explicitly identifying the key relationships between the premises and the conclusion, and explaining how they lead to the conclusion. Use clear and concise language to facilitate understanding, and... Continued on next page 18 Method Optimized Prompt"
        },
        {
            "title": "APO",
            "content": "Analyze the argument step by step, considering premises, logical connections, and conditional statements. Identify the conclusion and evaluate its validity, considering sufficient and necessary conditions, counterexamples, and alternative scenarios."
        },
        {
            "title": "GREATER",
            "content": "Use formal notation and and think step by step. Finally give the actual correct answer. salient translation error detection PE2 Identify the type of error in the translation from German to English. nSource: [insert source text] nTranslation: [insert translation] nError type: [one of the following] n(A) Modifiers or Adjectives n(B) Numerical Values n(C) Negation..."
        },
        {
            "title": "APE",
            "content": "Clarify your thoughts, break it down step by step."
        },
        {
            "title": "TextGrad",
            "content": "APO You will answer reasoning question by providing detailed analysis of the original text and the translation. Think step by step, considering multiple possible explanations for the error. Clearly explain how each step leads... Analyze the translation error by carefully reading the original sentence and identifying the specific mistake. Consider the exact words, phrases, and grammatical structures to determine the correct error type from the options. GREATER Use your answer reasoning as if had step. would be taking correct answer. PE APE TextGrad tracking shuffled objects five objects Lets think step by step. Take it one step at time: Focus on one task, complete it, then move on to the next. You will answer reasoning question by providing step-by-step breakdown of the process. Use vivid and descriptive language to describe the events, and make sure to highlight the key connections and relationships between each... APO Lets think step by step. GREATER Use this process as an explanation stepwise for each step until you get to as given above Alice has got originaly the following as follows. causal judgement PE APE TextGrad APO What action(s) led to the outcome? Lets break it down step by step. Break down your thinking into clear, consecutive steps. You will answer reasoning question by explicitly connecting the events and outcomes, considering multiple perspectives and potential counterarguments, and providing nuanced explanations that take into account the context in which the events occurred. Think... Analyze the situation by identifying the direct and indirect causes, considering multiple perspectives, and evaluating counterfactuals. Provide clear and concise answer, taking into account the context and nuances of the situation. Focus on the... GREATER Use causal diagram. The correct option ask about whether there the variable of about whether specific cause is sufficient. The answer causal relationship between to if the probability that occurs given changes. boolean expressions Continued on next page Method Optimized Prompt PE2 Evaluate logical expressions step by step, considering the order of operations and specific values. Break down expressions into parts, and evaluate each part using or, and, and not rules."
        },
        {
            "title": "APE",
            "content": "Analyze and simplify."
        },
        {
            "title": "APO",
            "content": "You will answer reasoning question by breaking down the expression into smaller, manageable parts. Provide concise and clear explanation, using precise and concise language to describe the logical operations used to arrive at... Evaluate the boolean expression by following PEMDAS and applying boolean logic rules (AND, OR, NOT). Handle parentheses carefully. Consider edge cases and provide step-by-step explanation of your reasoning, including any assumptions made."
        },
        {
            "title": "GREATER",
            "content": "Use this statement with conditional if know what is the value True of and what Not False means. Or not True and also boolean. In explain your. PE2 APE TextGrad object counting Lets think step by step. Break it down, step by step. You will answer reasoning question about counting objects. Think step by step, considering the context of the question and using it to inform your answer. Be explicit in your counting process, breaking it down... APO Lets think step by step. GREATER Use only addition. Add think step by step. Finally give the actual correct answer. navigate PE2 APE TextGrad Check if the instructions return to the starting point by calculating the total number of steps taken. Clarify your thoughts, analyze step by step. You will answer reasoning question by breaking down the problem step-bystep and providing explicit explanations for each step. Think carefully about the instructions and consider alternative scenarios. Use clear and precise language to describe... APO Analyze the instructions step by step, considering each actions effect on your position. Use logical reasoning to determine if you return to the starting point. GREATER Use your reasoning here. moving. would like numbers assigned.. to.. To represent sports understanding PE2 APE TextGrad APO Assess the plausibility of the sentence. Is it likely to be true or fictional? Break down the task into manageable parts, examining each element thoroughly. You will answer reasoning question by providing clear and concise stepby-step breakdown of your thought process, focusing on the most relevant and concrete evidence to support your claims. Consider alternative explanations and counterarguments... Assess the plausibility of the sentence, considering both literal and figurative meanings, as well as context and domain knowledge. Evaluate the sentences coherence and relevance to the given context. Continued on next page Method Optimized Prompt"
        },
        {
            "title": "GREATER",
            "content": "Use the context or sentence similar prior knowledge. Assume you journalist, would have been covering NHL hockey in Minnesota before joining this assignment to report sports. reasoning about colored objects PE"
        },
        {
            "title": "TextGrad",
            "content": "Analyze the input and options step by step to identify the correct answer. Break down into simpler components. You will answer reasoning question by carefully analyzing the problem statement, identifying the relevant information, and using logical deductions to arrive at solution. Use precise and accurate language to describe your thought process,..."
        },
        {
            "title": "APO",
            "content": "Lets think step by step."
        },
        {
            "title": "GREATER",
            "content": "Use this problem type as inspiration! which option best represents amu, the answer of all my are known. PE2 APE TextGrad APO GREATER PE2 APE TextGrad multistep arithmetic two Evaluate step-by-step and provide the correct answer, following the order of operations (PEMDAS). Decompose and analyze each part carefully. You will answer reasoning question by providing clear, step-by-step breakdown of your thought process, using simple language and avoiding ambiguity. Focus on the key steps and simplify the intermediate calculations. Use descriptive variable... Evaluate the expression by following PEMDAS, handling parentheses, and accurately calculating with negative numbers. Break down complex expressions into simpler steps and provide the final answer. Use PEMAS reasoning here and step by the. STEP to the actual number result and explain what PEMAS means by each step of how would evaluate this expression correctly according follow these step wise... date understanding Lets think step by step. Analyze step-by-step. You will answer reasoning question by breaking it down into manageable steps, focusing on simplicity and clarity in your reasoning. Provide concise and clear explanation of your thought process, avoiding unnecessary conversions and... APO Lets think step by step. GREATER Use the date today which will not would give us an error. solution is given as answer date is correct the option data and the current month and year to get to previous and current month of year to determine what the current data will look. ruin names PE2 Identify the humorous edit of this artist or movie name. Choose an option that cleverly replaces word or plays on words with the original name. Options may include the correct answer. APE Break it down, step by step. TextGrad You will answer reasoning question by providing step-by-step analysis of the options, highlighting the unique features and characteristics of each humorous edit. Consider the linguistic and cognitive factors that contribute to humor, such... Continued on next page 21 Method Optimized Prompt"
        },
        {
            "title": "APO",
            "content": "Imagine creative reinterpretation of the original name. Think outside the box and come up with clever edit thats unexpected yet amusing. Consider tone, context, and audience when selecting the most humorous and engaging..."
        },
        {
            "title": "GREATER",
            "content": "Use your logical reasoning to make this, not brute force checking.CONTEXT is provided below. movie recommendation PE2 Find movie that shares similar elements with the given films, considering narrative structure, memorable characters, genre blending, strong protagonists, and emotional impact."
        },
        {
            "title": "APE",
            "content": "Calmly analyze, think critically."
        },
        {
            "title": "TextGrad",
            "content": "APO You will answer reasoning question by analyzing the given movies and identifying the most suitable match. Think step-by-step, focusing on the most distinctive features that connect the input movies, such as unique plot twists,... Analyze the movies tone, genre, and style, considering action, drama, and comedy elements. Identify the most fitting movie from the options that shares these characteristics, focusing on overall themes and elements rather than individual features. GREATER Use movie ratings data available here above movies for reference. ThisHOFF has an interesting analysis based solely options to options based movies ratings expect from the other movies you are asked ones mentioned here you... web of lies PE2 APE TextGrad APO Evaluate statements about the truthfulness of others in chain of lies or truthtelling. Determine if speakers are telling the truth or lying, considering each statement and the speakers integrity. Lets take it one step at time: analyze the task into smaller, manageable chunks, and then tackle each chunk individually to achieve clear and focused approach. You will answer reasoning question by specifying the scope of the truth and using explicit language to connect each step in your reasoning. Focus on essential steps and consider alternative perspectives. Use direct and... Analyze each statement individually, considering the speakers truthfulness and potential contradictions. Determine the truth or falsehood of each statement, then use this information to evaluate the final statement. GREATER Use only statement reasoning.Let step ick. We need to step out from here to figure this one out step out step out step out step out from each of those. PE2 APE TextGrad APO disambiguation qa Identify the antecedent of the pronoun, considering sentence structure and context. If ambiguous, provide evidence to support your answer. Clarify thoughts, analyze step by step. You will answer reasoning question by providing step-by-step explanation of your thought process, considering the context, syntax, and semantics of the sentence, as well as the relationships between the entities mentioned. Use linguistic... Analyze the sentence and identify the antecedent of the pronoun. Consider the context, relationships between entities, and potential ambiguity. Provide clear explanation for your answer, highlighting any relevant details that support your conclusion. Continued on next page 22 Method Optimized Prompt"
        },
        {
            "title": "GREATER",
            "content": "Use is possible reasoning for either answer by step. Finally, the actual correct answer may also not have an explicit mention of PE"
        },
        {
            "title": "APO",
            "content": "logical deduction five objects Determine the correct order of objects based on logical relationships and statements provided. Break down and examine each stage carefully. You will answer reasoning question by breaking down the information into clear and concise steps. Use specific and unambiguous language to describe the relationships between the objects. Consider using diagrams or illustrations to help... Carefully analyze each statement, considering relationships between objects and logical implications. Eliminate options that contradict the statements. Recognize and resolve contradictions. Consider word order and syntax to ensure accurate conclusions. GREATER Use elimination logical reasoning and think step by step. Finally give the actual correct answer. snarks PE2 APE TextGrad Identify the sarcastic statement and explain the irony, mocking tone, and intended meaning. Consider language that is ironic, mocking, or opposite of what is meant. Break down the task into smaller steps, and lets tackle each one individually. You will answer reasoning question by considering multiple factors and providing detailed, step-by-step analysis. Think critically about the context, speakers intent, and audiences perspective. Pay particular attention to the tone and language used... APO Lets think step by step. GREATER Use your common reasoning and judgment, by step. Finally give the actual correct answer. geometric shapes PE2 APE TextGrad APO GREATER Identify the quadrilateral or geometric shape drawn by this SVG path element. Analyze and simplify. You will answer reasoning question by analyzing the paths overall shape, examining how the individual segments contribute to the paths geometry, and provide more context and domain-specific knowledge about SVG path elements, such as... Analyze the SVG path element, focusing on both line segments and curves. Identify the starting and ending points, and recognize patterns in the movement. Consider the overall path structure and geometric properties to determine the... Use your best answer from the I. answer the options. assistantactiveassistancesassistantative be mathematical object with vertices. If there be represented by the path. hyperbaton PE2 Identify the correct adjective order in the given sentence. Adjectives typically follow specific order: opinion, shape, size, material, etc., with exceptions and context-dependent variations. APE Organize your ideas, simplify them. Continued on next page Method Optimized Prompt"
        },
        {
            "title": "APO",
            "content": "You will answer reasoning question. Think step by step. Provide explicit explanations for each step. Consider breaking down complex concepts into smaller, more manageable parts. When analyzing the sentence, pay close attention to the... Analyze the adjective order in each sentence, considering context, typical order of opinion, adverb role, and exceptions. Provide the correct sentence with adjectives in the most natural and idiomatic order."
        },
        {
            "title": "GREATER",
            "content": "Use the reasoning and examples you would step. Finally give the actual correct answer. PE"
        },
        {
            "title": "TextGrad",
            "content": "penguins in table Count step by step and find the answer. Unpack your ideas, review thoroughly. You will answer reasoning question by following structured approach. Think step by step, considering the most critical information and alternative explanations. Use precise language and clarify the scope of the question. Organize your... APO Lets think step by step. GREATER Use this to solve this puzzle step by step. Finally give the actual correct answer. temporal sequences PE2 APE TextGrad Find the time windows when the person was not busy or occupied to visit the location, considering their schedule. Dissect and analyze the information. You will answer reasoning question by identifying the most plausible answer, explicitly stating assumptions and considering alternative explanations. Clearly explain how each piece of evidence supports your conclusion, and provide specific and precise language... APO Lets think step by step. GREATER Use the timeline provided and answer step by step. Finally give the actual correct answer. PROMPT OPTIMIZATION RESULTS: GEMMA-2-9B-IT I.1 GSM8K AND FOLIO: OPTIMIZED PROMPTS In Table 12 and Table 13, we first show the optimized prompts with Llama-3-8B-Instruct on GSM8K and FOLIO dataset respectively. As we can see, GREATER performs on par or better than all the baselines in every scenario. Table 12: Optimized Prompts for Gemma-2-9B-it on GSM8K dataset Method Optimized Prompt TextGrad APE APO PE2 GREATER Use these logical reasoning process steps and explain Step. step. Here You will answer mathematical reasoning question. Think step by step. Lets think step by step. Lets think step by step. Lets think step by step. is correct answer. Score 87.8 88.6 88.6 88.6 89. 24 Figure 6: Full performance breakdown across 21 BBH tasks of GREATER and SOTA prompt optimization techniques APO, TextGrad, APE, and PE2 in optimization with Llama-3-8B. Method TextGrad APE APO PE2 Table 13: Optimized Prompts for Gemma-2-9B-it on FOLIO dataset Optimized Prompt You will answer reasoning question. Think step by step, carefully considering all provided information and identifying any potential contradictions or ambiguities. When evaluating statements about preferences, . . . Divide the problem into manageable chunks. (empty prompt) Given the premises, determine the certainty of the following statement. Choose from: * Conclusive True * Conclusive False * Uncertain Score 67.5 67.5 63.1 62.1 GREATER Use logic or reasoning and think step by step. Finally give the actual 68. correct answer. I.2 BIG BENCH HARD (BBH): OPTIMIZED PROMPTS AND DETAILED RESULTS 25 Figure 7: Win/Draw/Loss Comparison of GREATER and SOTA prompt optimization techniques APO, TextGrad, APE, and PE2 in optimization with Gemma-2-9B-it. GREATER maintains winning margin over these methods, highlighting its effectiveness in optimization. Table 14: List of optimized prompts for BBH tasks by different prompt optimization methods with Gemma-2-9B-it. Method Optimized Prompt PE2 APE TextGrad multistep arithmetic two Lets think step by step and calculate the result. Lets think step by step. You will answer reasoning question. Remember to follow the order of operations (PEMDAS/BODMAS) when solving the problem step-by-step. Think step-by-step, clearly outlining each operation you perform. Begin by simplifying any expressions within parentheses. Then,... APO Lets think step by step. GREATER Use parentheses, and and the step wise order. Solve for the correct answer. PE2 APE TextGrad reasoning about colored objects Lets think step by step to determine the answer. Break this down into smaller, easier-to-handle sections. You will answer reasoning question. Your goal is to determine the answer to the question based on the provided information and explain your thought process clearly. Present your reasoning in the most concise and... APO Analyze the given text and answer the question. Provide brief explanation of your reasoning, listing the steps you took. GREATER Use your logic. Please answer. person. Yout answer. B. geometric shapes PE2 APE Analyze the SVG path data in the attribute and identify the most specific geometric shape it represents, considering commands like M, L, and others. Break this down into smaller parts. Continued on next page Method Optimized Prompt"
        },
        {
            "title": "TextGrad",
            "content": "You will analyze the provided SVG path element and determine the shape it represents. Consider the number of line segments (L commands) and their connections to identify the shape. Look for patterns in the coordinates..."
        },
        {
            "title": "APO",
            "content": "(empty)"
        },
        {
            "title": "GREATER",
            "content": "Use an logical reasoning and think step by step. Finally give the actual correct answer. sports understanding PE"
        },
        {
            "title": "TextGrad",
            "content": "APO Lets think step-by-step and assess the plausibility of the following sentence. Can we break this down into smaller steps? You will evaluate the plausibility of statements based on the provided context and established rules and mechanisms of football. When evaluating plausibility, consider the relationship between the statement and the broader context of football... Evaluate the plausibility of this sentence, considering both general knowledge and the context of sports. Think about whether such an event is realistically possible. GREATER Use your understanding to explain the step by step. Finally give the actual correct answer. disambiguation qa PE2 APE TextGrad APO Lets think step by step. Select the option that correctly identifies the antecedent of the pronoun. Walk me through this process step by step. You will answer reasoning question. Think step by step, paying close attention to the grammatical structure of the sentence and identify the function of each word. When encountering pronouns, clearly explain which noun or... Identify the noun or phrase that the pronoun they refers to in each sentence. Choose the most specific and accurate antecedent. If ambiguous, select Ambiguous. GREATER Use of logical connection instead think step by step. Finally give the actual correct answer. PE2 APE TextGrad ruin names Lets think step by step. Lets think step by step. You will analyze humorous edits of artist or movie names, assuming your audience is [specify target demographic]. nn**Key Humor Components:**nn* **Incongruity:** Juxtaposing clashing or unexpected elements.n* **Surprise:** Unexpected twists or... APO Identify the **most humorous** edit of the given artist or movie name. Focus on **creative wordplay** and **unexpected twists**, not just phonetic changes. GREATER Use your logic, not change spelling. punny play with existing names to get the answer. PE APE Lets identify the sentence with the incorrect adjective order: Explain each step separately. hyperbaton Continued on next page Method Optimized Prompt"
        },
        {
            "title": "TextGrad",
            "content": "Your primary goal is to clearly and accurately explain the reasoning behind the correct answer. First, discuss the relevant grammatical principles at play when arranging adjectives in sentence. Then, apply these principles to the..."
        },
        {
            "title": "APO",
            "content": "Lets think step by step."
        },
        {
            "title": "GREATER",
            "content": "Use your knowledge to and think step by step. Finally give the actual correct answer. causal judgement PE"
        },
        {
            "title": "TextGrad",
            "content": "Analyze the scenario and determine if the persons action was direct cause of the event. Explain your reasoning. Divide the problem into manageable chunks. You will answer causation question, demonstrating nuanced understanding of cause-and-effect relationships. Consider complex interactions between multiple factors, analyze situations with indirect or delayed effects, and evaluate the role of probability and likelihood in... APO What single action was the most immediate cause of the stated outcome? GREATER Use proper causal reasoning . step through step. Finally give the actual correct answer. boolean expressions PE2 APE TextGrad Evaluate the truth value of the following logical expression, showing your reasoning step-by-step: Lets think step by step. You will answer reasoning question involving logical expressions. Analyze the problem logically, providing thorough explanation of how each logical operator (and, or, not) influences the truth value of the expression. Use clear examples... APO Lets think step by step. GREATER Use logical truth tables. method or truth operations. And. Or or The. following statements evaluate each. PE2 APE TextGrad object counting Lets think step by step. Lets think step by step. You will answer reasoning question. Clearly calculate the answer to the question. nnList all the relevant elements involved in the calculation. Then, using those elements, perform the calculation to find the answer.... APO Lets think step by step. GREATER Use counting maths . see that is more that single. have more then ten because there more the number. movie recommendation PE2 APE TextGrad Lets identify movie with similar genre to Braveheart, Dances with Wolves, Pulp Fiction, and Schindlers List. Break this down into smaller, easier-to-handle parts. You will answer reasoning question by identifying the movie most similar to given set. To arrive at your answer, follow these steps:nn1. **Analyze each movie:** Identify and analyze specific plot points,... Continued on next page 28 Method Optimized Prompt"
        },
        {
            "title": "APO",
            "content": "Classify the movie option most similar in genre to the given film list. Choose the best fit."
        },
        {
            "title": "GREATER",
            "content": "Use only reasoning and reasoning based logic. chose option. think the film that fits the listed criteria but is more readily avaliabke on common viewing services. PE"
        },
        {
            "title": "TextGrad",
            "content": "formal fallacies Lets think step by step. Lets think step by step. You will answer reasoning question. Your task is to determine if the conclusion *logically follows* from the premises, regardless of whether the conclusion is true in the real world. Think step-by-step and clearly articulate..."
        },
        {
            "title": "APO",
            "content": "Lets think step by step."
        },
        {
            "title": "GREATER",
            "content": "Use modus ponenis incorrectly because step is incorrect for some premises. Invalid due because. salient translation error detection PE2 Lets think step by step.nIdentify the error type in these translations: Named Entities, Numerical Values, Modifiers or Adjectives, Negation or Antonyms, Facts, or Dropped Content. APE Break this down into smaller steps. TextGrad APO You will answer reasoning question based on text passage. Carefully compare the source text and the provided translation, paying close attention to the meaning of individual words and phrases. Identify any words or... Remember, good prompt for zero-shot classifier should be:nn* **Clear and concise:** Avoid ambiguity and unnecessary jargon.n* **Specific:** Clearly define the task and the expected output format.n* **Grounded in the... GREATER Use the following based on this information, using specific error category as an example. PE2 APE TextGrad penguins in table Lets think step by step to answer the following question: Lets think step by step. Your task is to answer reasoning question by carefully analyzing the provided information. Pay close attention to the specific details and facts presented in the text. Identify the key pieces of information that are... APO Lets think step by step. GREATER Use the the provided context, ,,and explaining. The answer and explain the solution is process in simple step. step guide for someone just leering about coding Python. PE APE TextGrad tracking shuffled objects five objects Lets trace the changes in partners step-by-step to determine the final state. Explain it step by step. Your goal is to determine the final state of given scenario by carefully analyzing series of steps. Pay close attention to each step and track how items or values change hands. After detailing... APO Lets think step by step. Continued on next page 29 Method Optimized Prompt"
        },
        {
            "title": "GREATER",
            "content": "Use logic series or process or best method this. Following each series. PE"
        },
        {
            "title": "TextGrad",
            "content": "date understanding Lets think step by step. Lets think step by step. You will answer reasoning question. Think step by step, paying close attention to any date formats presented in the question. Ensure your reasoning clearly reflects how you interpret and manipulate dates based on their..."
        },
        {
            "title": "APO",
            "content": "Lets think step by step."
        },
        {
            "title": "GREATER",
            "content": "Use your format Excel formula for this answer to find it . It have gotten some. PE2 APE TextGrad web of lies Lets think step by step to determine the answer. Explain each step individually. You will answer reasoning question. Analyze the information carefully and identify the key relationships and deductions that lead to the solution. Express your reasoning concisely, highlighting the most important connections. Use clear and direct... APO Lets think step by step. GREATER Use proper logical reasoning and think step by step. Finally give the actual correct answer. PE2 APE TextGrad snarks Lets think step by step. Identify the most sarcastic statement. Decompose the problem into manageable subtasks. You will answer reasoning question. Think step by step. The last line of your response should be of the following format: Answer: $VALUE where VALUE is numerical value. APO Lets think step by step. GREATER Use logical reasoning and think step by step. Finally give the actual correct answer. temporal sequences PE2 APE TextGrad Given the following information about [persons name]s day, determine the time slot(s) when they could have gone to the coffee shop, which closes at 7pm. Walk me through the process, step by step. You will answer reasoning question. Break down the problem into smaller steps, identifying key pieces of information and eliminating possibilities based on the given facts. Present your reasoning in clear, step-by-step manner, explicitly... APO (empty) GREATER Use process logic, and eliminate options by considering what we do the actual correct answer. logical deduction five objects PE Lets think step-by-step to determine the position of the specified object within the sequence. Continued on next page 30 Method Optimized Prompt"
        },
        {
            "title": "APE",
            "content": "Lets think step by step."
        },
        {
            "title": "TextGrad",
            "content": "Your goal is to determine the position of specific item within described arrangement. You will be presented with set of statements describing the arrangement and question about the position of a..."
        },
        {
            "title": "APO",
            "content": "Lets think step by step."
        },
        {
            "title": "GREATER",
            "content": "Use elimination process, use this information, to eliminate choices sufficient information, eliminate to the. correct answer choice correct. navigate PE"
        },
        {
            "title": "TextGrad",
            "content": "Lets think step-by-step and determine your final position relative to the starting point based on these instructions. Lets think step by step. You will answer reasoning question involving changes in position or state. n* For each movement, clearly state the direction (e.g., 3 steps to the left) along with the number of steps.n* Assume... APO Lets think step by step. GREATER Use proper mathematical logic, explaining step by step. Finally give the actual correct answer. 31 Figure 8: Full performance breakdown across 21 BBH tasks of GREATER and SOTA prompt optimization techniques APO, TextGrad, APE, and PE2 in optimization with Gemma-2-9B."
        }
    ],
    "affiliations": [
        "Salesforce Research",
        "The Pennsylvania State University"
    ]
}