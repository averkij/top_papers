{
    "paper_title": "Is Diversity All You Need for Scalable Robotic Manipulation?",
    "authors": [
        "Modi Shi",
        "Li Chen",
        "Jin Chen",
        "Yuxiang Lu",
        "Chiming Liu",
        "Guanghui Ren",
        "Ping Luo",
        "Di Huang",
        "Maoqing Yao",
        "Hongyang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data scaling has driven remarkable success in foundation models for Natural Language Processing (NLP) and Computer Vision (CV), yet the principles of effective data scaling in robotic manipulation remain insufficiently understood. In this work, we investigate the nuanced role of data diversity in robot learning by examining three critical dimensions-task (what to do), embodiment (which robot to use), and expert (who demonstrates)-challenging the conventional intuition of \"more diverse is better\". Throughout extensive experiments on various robot platforms, we reveal that (1) task diversity proves more critical than per-task demonstration quantity, benefiting transfer from diverse pre-training tasks to novel downstream scenarios; (2) multi-embodiment pre-training data is optional for cross-embodiment transfer-models trained on high-quality single-embodiment data can efficiently transfer to different platforms, showing more desirable scaling property during fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity, arising from individual operational preferences and stochastic variations in human demonstrations, can be confounding to policy learning, with velocity multimodality emerging as a key contributing factor. Based on this insight, we propose a distribution debiasing method to mitigate velocity ambiguity, the yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to using 2.5 times pre-training data. Collectively, these findings provide new perspectives and offer practical guidance on how to scale robotic manipulation datasets effectively."
        },
        {
            "title": "Start",
            "content": "Is Diversity All You Need for Scalable Robotic Manipulation? Modi Shi1,2,4 Li Chen3,5 Jin Chen1,2 Yuxiang Lu2 1 Chiming Liu2 Guanghui Ren2 Ping Luo3 Di Huang4 Maoqing Yao2 Hongyang Li3 2 AgiBot 3 The University of Hong Kong 1 Shanghai Innovation Institute 4 Beihang University 5 Shanghai AI Lab (cid:135) Code: https://github.com/OpenDriveLab/AgiBot-World 5 2 0 2 J 8 ] . [ 1 9 1 2 6 0 . 7 0 5 2 : r Fig. 1: We investigate critical aspects of data diversity for robotic manipulation systematically, i.e., task, embodiment, and expert diversity. Through comprehensive evaluation in simulation and the real world, we reveal key insights that challenge conventional assumptions on data scaling. (a) Task diversity benefits policy learning with predictable power-law scaling. (b) Multi-embodiment pre-training data is optional for cross-embodiment transfer capabilitiesmodels pre-trained on singleembodiment data can efficiently adapt to different embodiments and show more desirable scaling property during finetuning than multi-embodiment pre-trained models. (c) Expert diversity confuses robot learning, towards which we devise distribution debiasing method based on GO-1 [1]; the yielding GO-1-Pro attains superior data efficiency during both pre-training and finetuning, where it achieves substantial performance gains of 15%, equivalent to using 2.5 times the pre-training data. AbstractData scaling has driven remarkable success in foundation models for Natural Language Processing (NLP) and Computer Vision (CV), yet the principles of effective data scaling in robotic manipulation remain insufficiently understood. In this work, we investigate the nuanced role of data diversity in robot learning by examining three critical dimensionstask (what to do), embodiment (which robot to use), and expert (who demonstrates)challenging the conventional intuition of more diverse is better. Throughout extensive experiments on various robot platforms, we reveal that (1) task diversity proves more critical than per-task demonstration quantity, benefiting transfer from diverse pre-training tasks to novel downstream scenarios; (2) multi-embodiment pre-training data is optional for crossembodiment transfermodels trained on high-quality singleembodiment data can efficiently transfer to different platforms, showing more desirable scaling property during fine-tuning than Equal contribution. Project lead. multi-embodiment pre-trained models; and (3) expert diversity, arising from individual operational preferences and stochastic variations in human demonstrations, can be confounding to policy learning, with velocity multimodality emerging as key contributing factor. Based on this insight, we propose distribution debiasing method to mitigate velocity ambiguity, the yielding GO1-Pro achieves substantial performance gains of 15%, equivalent to using 2.5 pre-training data. Collectively, these findings provide new perspectives and offer practical guidance on how to scale robotic manipulation datasets effectively. Index TermsRobotic Manipulation, Data Diversity, Scaling Law, Cross-Embodiment, Distribution Debias. I. INTRODUCTION"
        },
        {
            "title": "R ECENT advances in foundation models across NLP and",
            "content": "CV have demonstrated remarkable generalization capabilities, such as GPT-4 [2], Gemini [3], and SAM2 [4]. critical 2 factor underlying these breakthroughs is systematic data scaling, where training on massive, diverse, while carefully curated datasets yields superior performance and broader applicability. Given that data scaling principles have revolutionized multiple domains, natural question emerges: can similar data scaling approaches pave the way toward robotic foundation models? Building on state-of-the-art Vision Language Models (VLMs) [5][10] and visual foundation models [4], [11] [13], the robotics community has developed several large-scale robotic models including RT-2 [14], OpenVLA [15], Pi-0 [16], RDT [17], GO-1 [1], UniVLA [18], and GR00T [19]. Despite representing significant progress, these models are still far from genuine robotic foundation models. Their generalization capabilities remain constrained, struggling with novel objects, unfamiliar environments, new tasks, and different robot embodiments. Even minor variations in object positioning or lighting conditions can significantly compromise performance. This persistent gap between robotic manipulation and advances in NLP and CV domains can be attributed to multiple factors, with the limited quantity and quality of robot datasets being critical bottleneck. To address this data scarcity, recent efforts have focused on large-scale data collection initiatives such as Bridge Data [20], [21], DROID [22], Open X-Embodiment (OXE) [23], and AgiBot World [1]. However, these approaches primarily advocate more is better philosophy, relying on brute-force collection or simple aggregation, without carefully considering what constitutes effective data for robot learning. This limitation is exemplified by OpenVLAs finding that removing DROID data actually improves model performance [15]. Such results raise fundamental questions about what makes good manipulation dataset and how we should strategically scale up the datasets to maximize learning outcomes. Some preliminary studies have attempted to address these questions by scaling data for direct Imitation Learning (IL). For instance, Lin et al. [24] find power-law relationship between policy performance and object diversity and environment diversity, while ManiBox [25] highlights the benefits of spatial diversity for spatial generalization. These findings naturally lead to the intuition that data diversity is universally beneficial for robotic manipulation. In this study, we systematically explore three underexplored dimensions of data diversity, e.g., task diversity, embodiment diversity, and expert diversity, and investigate data diversity in both policy pre-training and finetuning stages, to provide comprehensive insights into effective data scaling strategies. In the end, our investigation suggests more complex picture. We find that the impact of diversity varies significantly across different dimensions: while some aspects of diversity are indeed critical and beneficial, others may be less important, or even confounding. First, we investigate how task diversity in large-scale pretraining affects downstream performance. Given that current robotic foundation models struggle to adapt to new tasks or skills, fundamental challenge emerges regarding how models should acquire transferable knowledge. Two potential approaches exist: broad exposure to diverse tasks for general knowledge acquisition, or intensive training on focused skill sets for specialized expertise development. To address this challenge, we construct two pre-training datasets with identical sample sizes but different task compositions: one with high task diversity and another focused on target-relevant skills. Our results demonstrate that task diversity outweighs the number of demonstrations per task. Building on this insight, we further investigate whether model performance continues to improve with increasing training samples when task diversity is sufficiently maintained, examining the power-law scaling relationship under this condition. Second, we explore embodiment diversity and its implications for cross-embodiment generalization. truly foundational robotic model should be capable of adapting to different robot embodiments. While the robotics community conventionally considers that achieving this capability requires diverse embodiments in the training data, cross-embodiment training is inherently complex due to morphological and state space heterogeneity across robots [26][29]. However, intuitively, the end-effector action spaces of robots with different configurations are fundamentally similarrobots with different morphologies can produce comparable behaviors when their end-effectors follow the same trajectory in world coordinates, suggesting that action space transformation across embodiments may be feasible. This observation leads to critical hypothesis: models pre-trained on single embodiment may easily transfer their learned knowledge to new robot configurations, thereby circumventing the difficulties of cross-embodiment training. To verify this hypothesis, we evaluate models trained solely on AgiBot G1 across diverse simulated and real-world platforms. Remarkably, we find that the model adapts well, even showing more desirable scaling property during fine-tuning than models pre-trained on the OXE dataset [23], which includes the test embodiment and thus has smaller embodiment gap. Importantly, we do not claim that single-embodiment pretraining is superior to multi-embodiment approaches; rather, our findings reveal that single-embodiment pre-training alone can achieve effective cross-embodiment generalization, offering an alternative pathway to circumvent the complexities of crossembodiment training. Third, we study expert diversity, an often overlooked aspect in robot learning. Expert diversity refers to the distributional variations in collected demonstrations arising from different teleoperators habits, skill levels, and inherent randomness. Unlike standardized NLP and CV datasets collected from the internet, robotic datasets are composed of continuous robot motion that is highly sensitive to teleoperator behaviors [30]. This sensitivity results in demonstrations that, while achieving the same task, exhibit distinct distributional characteristics. As shown in Figure 2, expert diversity manifests as both spatial multimodality [31] in trajectory paths and velocity multimodality in execution speeds. Crucially, these two types of multimodality have fundamentally different implications for learning: spatial variations represent meaningful task strategies that should be retained, whereas velocity variations introduce undesirable noise that complicates training. To address this, we introduce velocity model that performs distribution debiasing, specifically eliminating velocity multimodality while preserving spatial multimodality. Our experiments show that this design significantly improves model performance. These results shed light on the fundamental distinction between robotic action data 3 image-text pairs and possess substantial world knowledge. However, these methods directly integrate VLMs to generate lowlevel robotic actions, which may not be optimal for achieving generalization across different embodiments and skills. Some works utilize large-scale human manipulation videos [42], [43] to pre-train models that learn inverse dynamics by predicting latent action representations [44][46], or forward dynamics by predicting future changes [47][51], thus enhancing crossembodiment and cross-skill generalization. Other works focus on designing policy architectures with stronger capabilities for modeling multimodal distributions. Diffusion Policy [31] incorporates diffusion module to adapt to trajectory multimodality. RDT [17] employs DiT [52] for improved pretraining on heterogeneous multi-robot datasets [23] and dualarm trajectories. In our work, we conduct experiments based on GO-1 [1] and RDT [17] to ensure generalization potential across tasks and embodiments. B. Large-scale Manipulation Dataset Robotic manipulation is experiencing significant transformation toward scaling up data [20][22], [40], [53][55], seeking to enable models to develop general manipulation capabilities. The Open X-Embodiment dataset [23] exemplifies this effort by consolidating multiple datasets across 22 different embodiments and various camera configurations, reaching significant scale of 2.4 million trajectories. DROID [22] emphasizes increasing data diversity by covering various tasks, objects, scenes, camera viewpoints, and interaction locations, collecting data across 564 scenes in 52 real-world buildings. AgiBot World [1] has compiled over 1 million high-quality bimanual trajectories through unified embodiment and camera configuration, skilled teleoperators and rigorous verification protocols. Additionally, other works explore learning robotic manipulation knowledge from human manipulation videos [42], [43], leveraging the abundance of such videos to address the scarcity of robotic data. In this work, we conduct more comprehensive analysis of manipulation data diversity, aiming to provide deeper insights into constructing large-scale manipulation datasets of high quality. C. Crucial Dimensions Regarding Data Distribution Scaling robot datasets requires strategic approaches beyond simply expanding data volume. Recent research has demonstrated how data diversity enhances model generalization. Lin et al. [24] show that incorporating datasets with diverse environments and objects significantly improves model generalization, establishing power-law relationship between policy performance and data diversity. Manibox [25] highlights spatial diversity benefits, demonstrating that varied spatial layouts improve spatial generalization in manipulation tasks. Saxena et al. [56] identify camera viewpoints and spatial arrangements as crucial dimensions for collection diversity and retrieval alignment. Hejna et al. [57] propose automatic curation of largescale robotics datasets using group distributionally robust optimization, enabling efficient utilization of heterogeneous data for imitation learning. Similar findings exist in autonomous driving. Zheng et al. [58] collect data from various driving scenarios and Fig. 2: Illustration of the multimodal expert behavior in task Push-T [31]. The robot (blue circle) needs to move the gray to the green target area. Expert demonstrations exhibit multimodality in both spatial and velocity dimensions: (a) Spatial multimodality arises from different trajectory choices, where the robot can approach from either left or right sides, resulting in distinct spatial paths; (b) Velocity multimodality occurs when robots execute similar trajectory at different speeds, generating completely different demonstration profiles over time. Both spatial and velocity multimodal characteristics require models to learn these distributional properties in current action chunk-based imitation learning. and image/text data, revealing that current imitation learning approaches may be limited by overlooked data characteristics rather than insufficient model capacity or dataset scale. We hope this consolidation report will shed new light on scalable robotic manipulation and offer practical guidelines for the research community. In summary, the contributions of this article are as follows: 1) We demonstrate that task diversity is beneficial for robotic learning and further validate the power-law relationship between pre-training data and downstream performance. 2) We show that multi-embodiment data is optional for cross-embodiment transfer, as models pre-trained with single-embodiment data can efficiently adapt to different embodiments and show more desirable scaling property during fine-tuning than multi-embodiment pre-trained models. 3) We discover that expert diversity could be confounding to the imitation learning process and demonstrate that targeted elimination of velocity multimodality can significantly improve model performance. II. RELATED WORK A. Scalable Manipulation Policy Recently, the success of foundation models in CV [5], [11] [13] and NLP [32][35] has motivated research into establishing manipulation foundation models [15], [16], [19], [36][39] through IL, aiming to leverage large-scale pre-training data to enable models to absorb rich prior knowledge. Early studies like RT-1 [40] and Octo [41] employ transformer-based policies to learn generalizable manipulation knowledge from diverse heterogeneous data. Works like RT-2 [14] and OpenVLA [15] utilize advanced VLMs to process camera inputs and human instructions, which have been extensively trained on web-scale 4 behaviors, discovering that driving model performance exhibits power-law relationship with scenario and behavior diversity. Our research challenges this conventional view, suggesting that not all diversity forms are equally beneficial. While diversity in environments and objects proves crucial, other diversity types may be less important or even confounding. In the following sections, we address three critical aspects of diversity in detail: task diversity (Section III), embodiment diversity (Section IV), and expert diversity (Section V). III. TASK DIVERSITY While the recent study [24] has explored how policy performance scales with the number of training environments or objects, their analysis was limited to single-task scenarios using compact models without pre-training. In contrast, we investigate how task diversity in pre-training datasets affects downstream task performance, specifically examining the tradeoff between increasing the number of demonstrations per task versus enriching overall task diversity in pre-training data. A. Experiment Design Our experiments employ GO-1 [1] as the policy architecture, which excels at extracting task-agnostic latent actions and generalizing across diverse tasks. We leverage AgiBot World [1], large-scale robotic learning dataset containing over 1 million trajectories across more than 100 real-world scenarios. This dataset offers unique advantages, as all data is collected using single robot platform, AgiBot G1, which eliminates crossembodiment variables while ensuring high data quality through standardized collection protocols. Our training follows twophase process: first pre-training on the large-scale manipulation dataset, followed by fine-tuning on target evaluation tasks. In this section, we construct different pre-training datasets and fine-tune the model on identical evaluation task data, enabling us to compare the effects that different pre-training dataset compositions have on downstream task performance. Our evaluation encompasses four challenging tasks: Wipe Table (contact-rich cleaning), Fold Shorts (deformable object manipulation), Pour Water (fine-grained pouring), and Make Sandwich (long-horizon assembly). Each task is evaluated across three scenarios: an in-domain scenario, an objectenvironment generalization scenario, and visual distraction scenario. We conduct ten trials per scenario with position indoor lighting conditions, perturbations under consistent ensuring identical evaluation settings across all models. For evaluation, we use normalized scores to record the performance of each trial. We establish specific scoring criteria for each action within every task, with action scores categorized into three levels: 1, 0.5, and 0. The evaluation score for each trial corresponds to the average of all action scores, where score of 1 indicates perfect completion of all actions, and fractional scores represent partial success. Detailed scoring criteria can be found in Appendix-D. B. Task Diversity for Robotic Manipulation Pre-training When applying model to specific downstream task out of the pre-training domain, fundamental consideration emerges Fig. 3: Distribution of atomic skills in two pre-training datasets. Task-based sampling (10% tasks) shows lower skill diversity but concentrates on the most commonly used skills, while episode-based sampling (10% episodes) demonstrates more balanced distribution. regarding whether to construct pre-training dataset with the richest possible diversity, or to utilize fewer tasks but with potentially higher relevance to the target downstream task. To explore this trade-off, we design two pre-training datasets with distinct task distributions while controlling for other factors. We leverage the Agibot-World Beta dataset, one of the most comprehensive robotic datasets available, as our source for constructing these contrasting pre-training datasets. This allows us to maintain consistency in other aspects of the data, such as the robot embodiments included, while varying only the task distribution. We employ two sampling strategies to create datasets with different diversity characteristics but identical sizes. The task-based sampling strategy involves manually selecting 10% of tasks that are most relevant to our target downstream tasks, resulting in focused dataset with limited task diversity but high relevance. In contrast, the episode-based sampling strategy randomly samples 10% of episodes from each task in the original dataset, preserving the full spectrum of task variety while reducing the overall data volume. Our criterion for selecting relevant tasks is based on the inclusion of atomic skills required for task completion. Our evaluation tasks encompass five common atomic skills: pick, place, grasp, pour, and fold. In Figure 3, we present the distribution of atomic skills in both pre-training datasets, the episode-sampled dataset exhibits significantly greater task and skill diversity, yet consequently contains fewer episodes (59.2% vs. 71.1%) corresponding to the specific atomic skills needed for the target evaluation tasks. The experimental results in Figure 4 demonstrate that the episode-based sampling approach achieves an average performance improvement of 0.1 compared to task-based sampling, with the most significant gains observed in tasks requiring higher semantic and spatial understanding, such as Make Sandwich (0.26 improvement) and Pour Water (0.14 improvement). This finding corroborates the conclusions in [24] that, given fixed diversity, increasing training data quantity provides less benefit than enhancing diversity itself. Beyond skill diversity, the episode-based sampling strategy inherently encompasses more diverse scene configurations, object vari5 Fig. 4: Real-robot evaluation of GO-1 [1] on four challenging tasks subsequent to pre-training on different datasets. The tasks assess fine-grained manipulation, deformable object handling, long-horizon planning, and contact-rich interactions respectively. Results show that episode-based sampling (10% Episode) outperforms task-based sampling (10% Task) by 0.1 in average score with the same data amount, and performance improves consistently with increased pre-training data while ensuring sufficient task diversity. improvements across different pre-training data scales, with GO-1 average scores increasing from 0.28 (No pre-training) to 0.47 (100K demonstrations), 0.53 (250K demonstrations), and reaching 0.58 (1M demonstrations). To further explore this relationship, we fit the data using power law curve = β α, where represents the optimality gap, defined as the deviation from the maximum score (i.e., 1 Normalized Score), and represents the number of demonstrations [24]. Since the no pre-train case corresponds to zero pre-training data, which cannot be fitted in the scaling law curve, we use the fine-tuning data quantity to replace the number of demonstrations for fitting purposes. The experimental results in Figure 5 reveal clear power-law relationship between model performance and pre-training data, with Pearson correlation coefficient reaching 0.99. This finding suggests that, under the condition of adequate task diversity, robotic learning can achieve systematic performance gains through increased data scale, providing clear path for developing more capable robotic systems through data scaling. IV. EMBODIMENT DIVERSITY Cross-embodiment learning faces significant challenges due to morphological and state space heterogeneity across robot platforms. However, it remains unclear whether pre-training datasets must include multi-embodiment data to achieve effective cross-embodiment transfer. In this section, we investigate whether single-embodiment pre-trainingthereby avoiding cross-embodiment training complexitiescan still yield models with cross-embodiment capabilities. A. Experiment Design To address the challenges of cross-embodiment training, we explore whether single-embodiment pre-training can still enable Fig. 5: Performance scales with pre-training data size while maintaining adequate task diversity, following predictable power-law relationship. Left: GO-1 performance scales with pre-training data size. Right: Power-law relationship between pre-training data size and model performance. The dashed line represents power-law fit with equation = 1.24x0.08 and correlation coefficient = 0.99, indicating strong adherence to power-law scaling with pre-training data volume. ations, and environmental conditions within each selected episode. This richer contextual diversity enhances the models generalization capability across different objects, lighting conditions, and spatial arrangements, ultimately contributing to more robust real-world deployment. C. Pre-training Data Scaling Law The Agibot World [1] encompasses 217 common daily tasks and 87 frequently used skills. Building upon the conclusion that task diversity benefits robotic learning, we further investigate the relationship between data quantity and performance when scaling up pre-training data while maintaining dataset diversity. The left panel in Figure 5 demonstrates consistent performance effective cross-embodiment transfer by utilizing the large-scale single-embodiment dataset Agibot World (1M trajectories from AgiBot G1) for pre-training and systematically evaluating the resulting models cross-embodiment generalization. As reference point, we also include results from RDT pre-trained on OXEa widely recognized multi-embodiment dataset that has demonstrated strong cross-embodiment capabilities. We evaluate cross-embodiment transfer across 3 distinct benchmarks: ManiSkill (Franka arm), RoboTwin (Arx arm), and real-world Agilex (Piper arm)all different from the AgiBot G1 used in pre-training. Performance is measured by the average success rates for simulation tasks and the average scores for real-world tasks. ManiSkill includes 5 tasks (PegInsertionSide, PickCube, StackCube, PlugCharger, PushCube), while RoboTwin includes 4 tasks (BlockHammerBeat, BlocksStack, ContainerPlace, DualBottlesPick). Each simulation task involves 25 rollouts across 10 random seeds. Real-world evaluation covers 4 tasks: Package Product, Fold Shorts, Clean Trash, and Industrial Sorting. The AgiBot G1 arm (7 DOF) presents substantial morphological and dynamic differences compared to the evaluation robots, providing rigorous test of genuine cross-embodiment transfer capabilities. B. One-to-Many Embodiment Transfer Evaluation We fine-tune two pre-trained models on ManiSkill: RDTOXE (pre-trained on OXE) and RDT-AWB (pre-trained on Agibot World beta). OXE includes the Franka robot embodiment and ManiSkill data, while Agibot World uses completely different robot embodiment. Conventional wisdom suggests that RDT-AWB should underperform RDT-OXE or require substantially more fine-tuning due to the cross-embodiment gap. However, our results in Figure 7 demonstrate that singleembodiment pre-training can also achieve effective crossembodiment capabilities. While RDT-OXE converges faster initially and slightly outperforms RDT-AWB in early stages, RDT-AWB achieves effective cross-embodiment adaptation and surpasses RDT-OXE without requiring extensive fine-tuning data or training steps. As shown in Figure 6, with 125 samples per task, RDTOXE performs slightly better. At 250 samples, RDT-AWB matches RDT-OXE. With more data, RDT-AWB surpasses RDT-OXE1, with the gap increasing proportionally, exhibiting power-law relationship. Similarly, Figure 7 shows RDT-OXE performing better with fewer steps (around 10,000), but RDTAWB surpasses RDT-OXE as training steps increase. These results provide compelling evidence that single-embodiment pre-training can develop robust cross-embodiment transfer capabilities while circumventing the complexities inherent in multi-embodiment training. To more comprehensively validate our conclusions, we further compare RDT-OXE and RDT-AWB in both the RoboTwin simulation environment (using the Arx arm) and the real-world Agilex environment (using the Piper arm), 1Liu et al. [17] report RDT-OXEs fine-tuning performance on ManiSkill as 53.6%. Our evaluation results differ, possibly due to varying training configurations or inconsistent inference random seeds. Importantly, our RDTOXE and RDT-AWB evaluations use identical training and inference setups. Fig. 6: Cross-embodiment adaptation to Franka arm in ManiSkill with varying training data sizes. Left: Performance vs. number of demonstrations per task in fine-tuning data. Right: Power-law relationship between downstream performance and fine-tuning data size. Fig. 7: Cross-embodiment adaptation to Franka arm in ManiSkill with varying training steps. Left: Performance vs. training steps with 1000 fine-tuning episodes per task. Right: Performance vs. training steps with 500 fine-tuning episodes per task. testing cross-embodiment adaptation across multiple platforms. Following the same experimental protocol as in Figure 6, we conduct experiments in RoboTwin to compare model performance under varying fine-tuning data sizes and analyze the power-law relationship between performance and data size, as illustrated in Figure 8. The results demonstrate that RDTAWB achieves performance comparable to that of RDT-OXE with minimal fine-tuning data, successfully adapting to the Arx arm. Additionally, we compare the fine-tuning performance of RDT-OXE and RDT-AWB in the real-world Agilex environment using identical data sizes (100 demonstrations per task), as presented in Table I. RDT-AWB achieves superior performance compared to RDT-OXE on 3 of 4 tasks, indicating effective adaptation to the real-world Piper arm with limited fine-tuning requirements. V. EXPERT DIVERSITY During the data collection process, different human demonstrators exhibit distinct collection habits and inherent randomness in their execution, leading to diverse and complex data distributions with varying trajectory patterns. While some variations in the data distribution represent meaningful multimodal spatial distributions that capture legitimate alternative approaches [31], others constitute distribution bias that merely increases training difficulty without providing valuable TABLE I: Performance in the real world Agilex environment. The performance of RDT-OXE and RDT-AWB is compared to assess RDT-AWBs ability to transfer to the downstream Piper robot arm. Higher scores are bolded for emphasis."
        },
        {
            "title": "Industrial Sorting Average Score",
            "content": "RDT-OXE RDT-AWB 0.40 0.57 0.65 0.48 0.33 0.47 0.23 0.27 0.40 0. 7 speeds result in substantially different action chunk representations: [AB] versus [ABCD], causing spatially equivalent motions to be treated as distinct training samples. To address velocity bias, we initially consider two straightforward approaches. The first method normalizes all demonstrations to uniform velocity, ensuring consistent spatial distance per action chunk. However, this approach fails to capture taskspecific velocity requirements: fine-grained tasks like plug insertion require precise alignment phases, while pouring tasks necessitate deliberate pauses during execution. The second approach rescales all demonstrations of task to identical temporal duration [59], but this episode-level temporal normalization cannot eliminate velocity distribution bias due to heterogeneous speed patterns across trajectory segments within episodes and the inherent requirement for varying episode lengths under different initial conditions. Velocity bias introduces training instability as similar input states are associated with output action chunks exhibiting heterogeneous velocity distributions across different demonstrations. To address this issue, we propose Velocity Model (VM) that predicts the expected robot velocity conditioned on observations ot. We train the VM using MSE loss: LVM = E(ot,at:t+T )D (cid:2)VM(ot) v(at:t+T)2 (cid:3) , (1) where v(at:t+T) denotes the velocity metric extracted from action sequence at:t+T, and represents the demonstration dataset. This formulation enables the VM to learn the expected velocity profile for demonstrations with similar observations. Specifically, we define v(at:t+T) based on the end-effector t:t+T RT relative displacement representation. Let aeef denote the end-effector actions, where represents the action chunk size and corresponds to the degrees of freedom. We initially normalize each dimension of aeef to the range [1, 1], subsequently defining the velocity metric as: v(at:t+T) = aeef t:t+T 1, (2) where 1 denotes the L1 norm. Figure 10 illustrates the complete process of employing our VM for distribution debiasing. During policy training via imitation learning, for each training sample (ot, at:t+T ), we determine the optimal chunk length by: = arg min VM(ot) v(at:t+L). (3) To ensure training stability and mitigate potential interference from VM prediction errors, we constrain the search range of to lie within 0.5T and 1.5T . Subsequently, we employ interpolation to transform at:t+L into at:t+T with the desired chunk size . This temporal rescaling ensures that all training samples with similar observations exhibit consistent action Fig. 8: How the model crosses the embodiment gap to Arx in RoboTwin as data sizes increase. Left: Performance vs. number of demonstrations per task in fine-tuning data. Right: Power-law relationship between downstream performance and fine-tuning data size. Fig. 9: Illustration of distribution debiasing. Top: Two demonstrations (Demo 1 & 2) follow the same trajectory (AD) but have different velocities, resulting in distinct action chunks within the same time window (AB vs AD). Bottom: After velocity-based distribution debiasing, both demonstrations are normalized to similar action chunks (AC), reducing velocity ambiguity and facilitating model learning. information. In this section, we propose distribution debiasing method to eliminate bias in the velocity dimension, thus enhancing learning efficiency and overall model performance. The experiment setting is the same as Section III-A. A. Distribution Debiasing As shown in Figure 9, we demonstrate the velocity multimodal distribution through an example of sandwich making task. While two expert demonstrations follow identical spatial trajectories from point to point D, their varying execution 8 Fig. 10: Two-stage distribution debiasing framework using VM. Stage 1: VM is trained to predict velocity from action chunks using MSE loss, learning the expected velocity for each input from velocity-biased training data. Stage 2: During policy training, VM first predicts the unbiased velocity for each training sample, which is then used to transform the original actions into unbiased actions. The policy is subsequently trained using these unbiased actions as supervision targets, effectively simplifying the distribution complexity. velocities, thereby achieving velocity distribution debiasing. Our VM employs simple yet effective architecture, consisting of SigLIP [60] visual encoder followed by an MLP head. The VM processes three input images through SigLIP to extract visual features, which are subsequently mapped to scalar velocity value via the MLP. We normalize the output velocity to [0,1] using min-max scaling to enhance training stability. Critically, we freeze the SigLIP encoder during training to prevent the VM from overfitting to finegrained visual details in training samples, thereby ensuring it learn to predict the average velocity for similar observations rather than memorizing specific visual patterns. B. Distribution Debiasing in the Pre-training Phase We first investigate how distribution debiasing influences model performance across different training stages using 10% episode-based sampling from the AgiBot World Beta dataset. As presented in Table II, applying distribution debiasing exclusively during pre-training yields 6.5% average improvement (from 0.46 to 0.49), with particularly notable gains in Pour Water (+35%). However, this configuration introduces distribution mismatch: the debiased pre-trained representations must adapt to biased fine-tuning data, which constrains potential performance gains and may introduce training instability. More substantial improvements are observed when distribution debiasing is consistently applied across both pretraining and fine-tuning stages. This unified debiasing strategy achieves 15% overall improvement (from 0.46 to 0.53), with Pour Water showing the most significant enhancement (+60% from 0.20 to 0.32). Notably, this performance gain equals that achieved by scaling the pre-training dataset by 2.5 (as demonstrated in Figure 4), underscoring the data efficiency benefits of our debiasing method. The task-specific improvements reveal distinct patterns: manipulation-heavy tasks such as Pour Water and Fold Shorts benefit more from consistent debiasing (+60% and +23% respectively), while navigation-oriented tasks show more Fig. 11: GO-1-Pro consistently outperforms GO-1 on both the Wipe Table and Make Sandwich tasks. GO-1-Pro achieves comparable results using only 50% of the training data that GO-1 uses, demonstrating superior data efficiency. Fig. 12: Performance scaling with fine-tuning data usage for GO-1 and GO-1-Pro on Wipe Table and Make Sandwich tasks. Both models follow power law relationships (fitted equations shown), with GO-1-Pro demonstrating faster convergence rates modest but meaningful gains. This suggests that distribution debiasing is particularly effective for tasks requiring precise action sequences and complex manipulation strategies, where biased demonstrations can impede learning efficiency. C. Distribution Debiasing in the Fine-tuning Phase Due to the substantial computational cost of pre-training, in this section we explore the effectiveness of applying distribution debiasing exclusively during the fine-tuning stage. For the Wipe Table and Make Sandwich tasks, we evaluate both tasks using the GO-1 model pre-trained on the full AgiBot World Beta dataset under different fine-tuning data scales. We refer to the model fine-tuned on distribution-debiased data as GO-1-Pro. The experimental results presented in Figure 11 demonstrate that GO-1-Pro consistently outperforms GO-1 across both tasks and all data scales, achieving an average score of 0.93 on Wipe Table compared to GO-1s 0.83, and reaching 0.79 on Make Sandwich while GO-1 plateaus at 0.7. Notably, GO-1-Pro exhibits exceptional data efficiencyit achieves comparable or superior performance using only half the training data required by GO-1. Specifically, GO-1-Pro with 60 demonstrations outperforms GO-1 with 120 demonstrations on both tasks, effectively doubling data utilization efficiency. These results underscore the critical importance of addressing data distribution biases in robotic learning. TABLE II: Performance evaluation using 10% episode-based sampling from AgiBot World Beta dataset for pre-training. Distribution debiasing applied during the pre-training phase demonstrates consistent performance improvements. Additional debiasing during fine-tuning further enhances model capabilities across all evaluated tasks."
        },
        {
            "title": "Average",
            "content": "Pre-training Data Fine-tuning Data"
        },
        {
            "title": "Biased\nBiased\nDebiased",
            "content": "0.20 0.27 0.32 0.30 0.30 0.37 0.67 0.71 0.73 0.66 0.68 0.70 0.46 0.49 0.53 The benefits of our distribution debiasing approach become particularly pronounced in low-data regimes, where GO-1-Pro improves performance from 0.35 to 0.52 for Make Sandwich and from 0.38 to 0.53 for Wipe Table with only 15 demonstrations. Under data-scarce conditions, the multimodal distribution across velocity and spatial dimensions creates substantial interference in the models learning process, impeding its ability to effectively capture essential spatial distribution patterns. By disentangling these confounding factors, our distribution debiasing method enables the model to focus on learning core spatial relationships despite limited data availability, thereby facilitating more efficient and robust policy learning. To investigate how distribution debiasing methods affect model performance across different fine-tuning data scales, we fit power-law curves to analyze the impact of fine-tuning data scale on final model performance, with results presented in Figure 12. Both GO-1 and GO-1-Pro exhibit power-law scaling behavior across the two tasks, but with notably different characteristics. For the Wipe Table task, GO-1-Pro demonstrates significantly faster convergence with an exponent of -1.01 compared to GO-1s -0.67, indicating that GO-1Pro achieves near-optimal performance more rapidly as data volume increases. The steeper negative exponent suggests that our distribution debiasing method more effectively leverages additional training data to reduce the optimality gap. For the Make Sandwich task, while both models exhibit similar exponents (-0.38 for GO-1-Pro vs -0.36 for GO-1), GO-1Pro maintains consistently lower optimality gaps across all data scales. This parallel scaling with constant performance offset indicates that the benefits of distribution debiasing persist regardless of data volume. VI. CONCLUSION AND FUTURE WORK This work systematically investigates data scaling principles for robotic manipulation, revealing three key insights that challenge conventional wisdom. We find that (1) task diversity proves more critical than per-task demonstration quantity for effective transfer, (2) embodiment diversity is optional for achieving cross-embodiment transfer capabilities, and (3) expert diversity can be confounding due to velocity multimodality, leading us to propose distribution debiasing method that yields substantial performance gains. These findings challenge the more diverse is better paradigm and provide practical guidance for strategically scaling robotic manipulation datasets. Limitations and future work. While our distribution debiasing method successfully eliminates the velocity multimodality, it cannot be applied to dynamic tasks such as ping-pong where the varying velocities are crucial for robot-environment interaction. Additionally, there remain other aspects of expert diversity that harm policy learning, such as meaningless pauses during data collection and suboptimal behavioral patterns that could cause robots to enter infinite loopsfuture work could further explore these areas by developing methods to identify and mitigate confounding expert diversity while preserving beneficial variations. ACKNOWLEDGMENT This work is in part supported by the JC STEM Lab of Autonomous Intelligent Systems funded by The Hong Kong Jockey Club Charities Trust. We thank Shenyuan Gao, Chengen Xie, Jianheng Song, Xindong He and Shaoze Yang for their valuable feedback and fruitful discussions. REFERENCES [1] Q. Bu, J. Cai, L. Chen, X. Cui, Y. Ding, S. Feng, S. Gao, X. He, X. Huang, S. Jiang et al., AgiBot World Colosseo: large-scale manipulation platform for scalable and intelligent embodied systems, arXiv preprint arXiv:2503.06669, 2025. 1, 2, 3, 4, 5 [2] OpenAI, GPT-4 Technical Report, arXiv preprint arXiv:2303.08774, 2023. 1 [3] R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. 1 [4] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Radle, C. Rolland, L. Gustafson et al., SAM 2: Segment anything in images and videos, arXiv preprint arXiv:2408.00714, 2024. 1, [5] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in ICML, 2021. 2, 3 [6] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in NeurIPS, 2023. 2 [7] J. Li, D. Li, S. Savarese, and S. Hoi, BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models, in ICML, 2023. 2 [8] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu et al., InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, in CVPR, 2024. 2 [9] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang et al., Qwen2.5-VL technical report, arXiv preprint arXiv:2502.13923, 2025. 2 [10] S. Karamcheti, S. Nair, A. Balakrishna, P. Liang, T. Kollar, and D. Sadigh, Prismatic VLMs: Investigating the design space of visually-conditioned language models, in ICML, 2024. 2 [11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, in ICLR, 2021. 2, [12] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, Masked autoencoders are scalable vision learners, in CVPR, 2022. 2, 3 [13] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., DINOv2: Learning robust visual features without supervision, TMLR, 2024. 2, 3 [14] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn et al., RT-2: Vision-languageaction models transfer web knowledge to robotic control, in CoRL, 2023. 2, 3 [15] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi et al., OpenVLA: An opensource vision-language-action model, in CoRL, 2024. 2, 3 [16] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter et al., π0: vision-language-action flow model for general robot control, arXiv preprint arXiv:2410.24164, 2024. 2, 3 [17] S. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu, RDT-1B: diffusion foundation model for bimanual manipulation, in ICLR, 2025. 2, 3, 6 [18] Q. Bu, Y. Yang, J. Cai, S. Gao, G. Ren, M. Yao, P. Luo, and H. Li, UniVLA: Learning to act anywhere with task-centric latent actions, in RSS, 2025. [19] J. Bjorck, F. Castaneda, N. Cherniadev, X. Da, R. Ding, L. Fan, Y. Fang, D. Fox, F. Hu, S. Huang et al., GR00T N1: an open foundation model for generalist humanoid robots, arXiv preprint arXiv:2503.14734, 2025. 2, 3 [20] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and S. Levine, Bridge Data: Boosting generalization of robotic skills with cross-domain datasets, in RSS, 2022. 2, 3 [21] H. R. Walke, K. Black, T. Z. Zhao, Q. Vuong, C. Zheng, P. HansenEstruch, A. W. He, V. Myers, M. J. Kim, M. Du et al., BridgeData v2: dataset for robot learning at scale, in CoRL, 2023. 2, 3 [22] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis et al., DROID: large-scale in-the-wild robot manipulation dataset, in RSS, 2024. 2, 3 [23] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai, A. Singh, A. Brohan et al., Open X-Embodiment: Robotic learning datasets and RT-X models, in ICRA, 2024. 2, 3 [24] F. Lin, Y. Hu, P. Sheng, C. Wen, J. You, and Y. Gao, Data scaling laws in imitation learning for robotic manipulation, in ICLR, 2025. 2, 3, 4, 5 [25] H. Tan, X. Xu, C. Ying, X. Mao, S. Liu, X. Zhang, H. Su, and J. Zhu, ManiBox: Enhancing spatial grasping generalization via scalable simulation data generation, arXiv preprint arXiv:2411.01850, 2024. 2, 3 [26] L. Wang, X. Chen, J. Zhao, and K. He, Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers, in NeurIPS, 2024. 2 [27] J. Zheng, J. Li, D. Liu, Y. Zheng, Z. Wang, Z. Ou, Y. Liu, J. Liu, Y.-Q. Zhang, and X. Zhan, Universal actions for enhanced embodied foundation models, arXiv preprint arXiv:2501.10105, 2025. [28] J. Yang, C. Glossop, A. Bhorkar, D. Shah, Q. Vuong, C. Finn, D. Sadigh, and S. Levine, Pushing the limits of cross-embodiment learning for manipulation and navigation, arXiv preprint arXiv:2402.19432, 2024. 2 [29] R. Doshi, H. Walke, O. Mees, S. Dasari, and S. Levine, Scaling CrossEmbodied Learning: One policy for manipulation, navigation, locomotion and aviation, in CoRL, 2024. 2 [30] H. Li, Y. Cui, and D. Sadigh, How to train your robots? the impact of demonstration modality on imitation learning, arXiv preprint arXiv:2503.07017, 2025. 2 [31] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song, Diffusion Policy: Visuomotor policy learning via action diffusion, in RSS, 2023. 2, 3, 6 [32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, in NAACL, 2019. 3 [33] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., LLaMA: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. 3 [34] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language models are few-shot learners, in NeurIPS, 2020. [35] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, MiniGPT-4: Enhancing vision-language understanding with advanced large language models, in ICLR, 2024. 3 [36] X. Li, M. Liu, H. Zhang, C. Yu, J. Xu, H. Wu, C. Cheang, Y. Jing, W. Zhang, H. Liu, H. Li, and T. Kong, Vision-language foundation models as effective robot imitators, in ICLR, 2024. 3 10 [37] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu et al., PaLM-E: An embodied multimodal language model, in ICML, 2023. 3 [38] J. Wen, Y. Zhu, J. Li, M. Zhu, Z. Tang, K. Wu, Z. Xu, N. Liu, R. Cheng, C. Shen et al., TinyVLA: Towards fast, data-efficient vision-languageaction models for robotic manipulation, RA-L, 2025. 3 [39] J. Wen, Y. Zhu, J. Li, Z. Tang, C. Shen, and F. Feng, DexVLA: Visionlanguage model with plug-in diffusion expert for general robot control, arXiv preprint arXiv:2502.05855, 2025. [40] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., RT-1: Robotics transformer for real-world control at scale, in RSS, 2023. 3 [41] D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu et al., Octo: An open-source generalist robot policy, in RSS, 2024. 3 [42] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag et al., The something something video database for learning and evaluating visual common sense, in ICCV, 2017. 3 [43] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu et al., Ego4D: Around the world in 3,000 hours of egocentric video, in CVPR, 2022. 3 [44] J. Bruce, M. D. Dennis, A. Edwards, J. Parker-Holder, Y. Shi, E. Hughes, M. Lai, A. Mavalankar, R. Steigerwald, C. Apps et al., Genie: Generative interactive environments, in ICML, 2024. 3 [45] S. Ye, J. Jang, B. Jeon, S. Joo, J. Yang, B. Peng, A. Mandlekar, R. Tan, Y.-W. Chao, B. Y. Lin et al., Latent action pretraining from videos, in ICLR, 2025. [46] X. Chen, J. Guo, T. He, C. Zhang, P. Zhang, D. C. Yang, L. Zhao, and J. Bian, IGOR: Image-goal representations are the atomic control units for foundation models in embodied ai, arXiv preprint arXiv:2411.00785, 2024. 3 [47] H. Wu, Y. Jing, C. Cheang, G. Chen, J. Xu, X. Li, M. Liu, H. Li, and T. Kong, Unleashing large-scale video generative pre-training for visual robot manipulation, in ICLR, 2024. 3 [48] C.-L. Cheang, G. Chen, Y. Jing, T. Kong, H. Li, Y. Li, Y. Liu, H. Wu, J. Xu, Y. Yang et al., GR-2: generative video-language-action model with web-scale knowledge for robot manipulation, arXiv preprint arXiv:2410.06158, 2024. 3 [49] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel, Learning universal policies via text-guided video generation, in NeurIPS, 2023. 3 [50] J. Zeng, Q. Bu, B. Wang, W. Xia, L. Chen, H. Dong, H. Song, D. Wang, D. Hu, P. Luo et al., Learning manipulation by predicting interaction, in RSS, 2024. 3 [51] Q. Bu, J. Zeng, L. Chen, Y. Yang, G. Zhou, J. Yan, P. Luo, H. Cui, Y. Ma, and H. Li, Closed-loop visuomotor control with generative expectation for robotic manipulation, in NeurIPS, 2024. [52] W. Peebles and S. Xie, Scalable diffusion models with transformers, in ICCV, 2023. 3 [53] H.-S. Fang, H. Fang, Z. Tang, J. Liu, C. Wang, J. Wang, H. Zhu, and C. Lu, RH20T: comprehensive robotic dataset for learning diverse skills in one-shot, in ICRA, 2024. 3 [54] Z. Wang, H. Zheng, Y. Nie, W. Xu, Q. Wang, H. Ye, Z. Li, K. Zhang, X. Cheng, W. Dong et al., All Robots in One: new standard and unified dataset for versatile, general-purpose embodied agents, arXiv preprint arXiv:2408.10899, 2024. 3 [55] K. Wu, C. Hou, J. Liu, Z. Che, X. Ju, Z. Yang, M. Li, Y. Zhao, Z. Xu, G. Yang et al., RoboMIND: Benchmark on multi-embodiment intelligence normative data for robot manipulation, in RSS, 2025. 3 [56] V. Saxena, M. Bronars, N. R. Arachchige, K. Wang, W. C. Shin, S. Nasiriany, A. Mandlekar, and D. Xu, What matters in learning from large-scale datasets for robot manipulation, in ICLR, 2025. 3 [57] J. Hejna, C. A. Bhateja, Y. Jiang, K. Pertsch, and D. Sadigh, ReMix: Optimizing data mixtures for large scale imitation learning, in CoRL, 2024. [58] Y. Zheng, Z. Xia, Q. Zhang, T. Zhang, B. Lu, X. Huo, C. Han, Y. Li, M. Yu, B. Jin et al., Preliminary investigation into data scaling laws for imitation learning-based end-to-end autonomous driving, arXiv preprint arXiv:2412.02689, 2024. 3 [59] N. Masuya, S. Sakaino, and T. Tsuji, Variable-frequency imitation learning for variable-speed motion, in ICM, 2025. 7 [60] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, Sigmoid loss for language image pre-training, in ICCV, 2023."
        },
        {
            "title": "APPENDIX",
            "content": "A. Hardware Setup Our real-world experiments are conducted on two robotic platforms: AgiBot G1 and AgileX Cobot Magic with Piper, as shown in Figure A-1 and Figure A-2 respectively. Both platforms are equipped with multi-camera setup consisting of one front-facing camera mounted on the robots head and two wrist cameras attached to each arm. Fig. A-1: Deployment on AgiBot G1. Fig. A-2: Deployment on AgileX Cobot Magic with Piper. 11 2) Embodiment Diversity Embodiment Diversity Experiments: We fine-tuned RDTOXE and RDT-AWB on three platforms: Maniskill, RoboTwin, and real-world Agilex, with results reported in Figure 6, Figure 8, and Table respectively. For Maniskill, we evaluated five tasks (PegInsertionSide, using PickCube, StackCube, PlugCharger, PushCube) fine-tuned only with for task 37.5K/75K/150K/300K steps respectively, saving checkpoints every 10K steps and reporting peak success rates. observations. We per demonstrations the third-person 125/250/500/1000 For RoboTwin, we tested four tasks (BlockHammerBeat, BlocksStack, ContainerPlace, DualBottlesPick) using four camera views (front, head, left/right wrist). Fine-tuning used 25/50/100/200 demonstrations per task for 10K/30K/50K/60K steps respectively. For real-world Agilex, we evaluated four tasks (PackageProduct, FoldShorts, CleanTrash, IndustrialSorting) using three camera views (front, left/right wrist). We used 100 demonstrations per task (200 for FoldShorts due to complexity) and trained for 100K steps. Pre-training Setup: RDT-AWB was pre-trained on AgiBot World using 96 H100 GPUs (batch size 32 per GPU, 200K steps). RDT-OXE used the official checkpoint (48 H100 GPUs, batch size 32 per GPU, 1M steps). All other hyperparameters remained consistent between models. 3) Expert Diversity Our velocity model employs SigLIP vision encoder coupled with three-layer MLP featuring LayerNorm, GELU activations, and 0.1 dropout. For fine-tuning experiments, we train the velocity model using 8 H100 GPUs with batch size of 128 per GPU for 20K steps, while pre-training experiments employ the same hardware configuration but extend training to 100K steps. The GO-1-Pro training setup remains identical to GO-1. C. Additional Experiment Results In this section, we provide additional detailed results for the simulation experiments in Section IV, showcasing the performance of RDT-OXE and RDT-AWB on each task in Maniskill (Table A-I) and RoboTwin (Table A-II). B. Implementation Details D. Task Details 1) Task Diversity Our experimental configurations vary according to the scale of pre-training data. For pre-training with 10% of the AgiBot World dataset, we employ 32 H100 GPUs with batch size of 64 per GPU, training for 100K steps. When scaling to 25% of the data, we maintain the same hardware setup (32 GPUs, batch size 64 per GPU) but extend training to 250K steps. For full-scale pre-training on the complete dataset, we scale up to 96 H100 GPUs with batch size 64 per GPU, training for 500K steps. During the pre-training phase, we train the Vision-Language Model (VLM) component alongside other model parameters. For fine-tuning, we use 8 GPUs with batch size of 128 per GPU, training for 20K steps while keeping the VLM parameters frozen. In all the simulation experiments, we use the simple success rate as the evaluation metric. For real-world experiments, we define more fine-grained evaluation metrics to more precisely compare the capabilities of the models within limited number of rollouts. In Section III, Section IV and Section V, we cover seven tasks: Wipe Table, Fold Shorts, Pour Water, Make Sandwich, Package Product, Clean Trash, and Industrial Sorting. We have defined different evaluation metrics for each of them: 1) Wipe Table Task description. The robot is tasked with contact-rich manipulation: Wipe Table, which demands the use of sponge to clean beverage stains from the table surface. The task consists of 2 steps: first, the right arm grasps the sponge; and second, the right arm wipes the stain clean. The sponge, being deformable 12 TABLE A-I: Performance on each task in Maniskill. The performance of RDT-OXE and RDT-AWB fine-tuned with various numbers of demonstrations for each task. Higher scores are bolded for emphasis."
        },
        {
            "title": "PushCube Average Score",
            "content": "RDT-OXE RDT-AWB 125 250 500 1000 125 250 500 1000 0.00 0.00 0.04 0.04 0.02 0.02 0.00 0. 0.06 0.25 0.40 0.76 0.18 0.19 0.54 0.86 0.09 0.34 0.42 0.66 0.07 0.40 0.65 0.90 0.00 0.00 0.03 0.04 0.00 0.00 0.01 0. 0.80 0.97 0.98 1.00 0.78 0.96 0.96 1.00 0.22 0.31 0.38 0.50 0.21 0.32 0.43 0.58 TABLE A-II: Performance on each task in RoboTwin. The performance of RDT-OXE and RDT-AWB fine-tuned with various numbers of demonstrations for each task. Higher scores are bolded for emphasis."
        },
        {
            "title": "Demonstrations BlockHammerBeat BlocksStack ContainerPlace DualBottlesPick Average Score",
            "content": "RDT-OXE RDT-AWB 25 50 100 200 25 50 100 200 0.55 0.61 0.87 0.60 0.63 0.67 0.90 0. 0.02 0.13 0.32 0.53 0.04 0.20 0.51 0.55 0.52 0.49 0.57 0.49 0.43 0.40 0.50 0.60 0.19 0.47 0.58 0.68 0.42 0.42 0.60 0. 0.32 0.42 0.59 0.58 0.38 0.42 0.63 0.69 object, introduces complexities in grasping and manipulation due to its softness and flexibility. The stain, being liquid, presents additional challenges due to its irregular shape and the need for precise contact control. Scoring criteria. Step 1: Grasping the sponge scoring 0: The gripper does not grasp the sponge. scoring 0.5: The gripper attempts to grasp the sponge multiple times, struggles to maintain stable grasp, but eventually succeeds. scoring 1: The gripper successfully grasps the sponge without any slippage on the first attempt. Step 2: Wiping the stain clean scoring 0: The sponge does not make contact with the stain, or the sponge drops during the wiping process. scoring 0.5: The sponge makes contact with the stain but does not effectively wipe it clean, or the stain is only partially cleaned. scoring 1: The sponge successfully wipes the stain completely clean on the first attempt. 2) Fold Shorts Task description. Fold Shorts is complex, bimanual deformable object manipulation task that involves multiple folding operations to neatly organize shorts. This task requires the robot to perform series of coordinated actions using both arms to fold the shorts accurately. The task consists of six steps: first, the left arm grasps the left side of the shorts; second, the right arm grasps the right side of the shorts; third, both arms fold the shorts forward; fourth, the left arm grasps the left side of the shorts again; fifth, the right arm grasps the right side of the shorts again; and sixth, both arms fold the shorts again, with the left arm holding the shorts in place and the right arm folding them over to the left arms position. The complexity of this task lies in the need for precise bimanual coordination and the challenges posed by the deformable nature of the shorts. The softness and flexibility of the shorts make grasping and folding operations difficult, requiring careful control of force and motion. Additionally, the thin material of the shorts increases the risk of collisions with the table, adding another layer of complexity. Scoring criteria. Step 1: Left arm grasping the left side scoring 0: The left arm does not grasp the shorts. scoring 0.5: The left arm attempts to grasp the shorts multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The left arm successfully grasps the shorts on the first attempt without any slippage. Step 2: Right arm grasping the right side scoring 0: The right arm does not grasp the shorts. scoring 0.5: The right arm attempts to grasp the shorts multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The right arm successfully grasps the shorts on the first attempt without any slippage. Step 3: Bimanual forward folding of the shorts scoring 0: The shorts are not folded forward or the 13 folding is incomplete. scoring 0.5: The shorts are folded forward but not perfectly aligned or the folding is not symmetrical. scoring 1: The shorts are folded forward perfectly, with symmetrical alignment. Step 4: Left arm grasping the left side again scoring 0: The left arm does not grasp the shorts. scoring 0.5: The left arm attempts to grasp the shorts multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The left arm successfully grasps the shorts on the first attempt without any slippage. Step 5: Right arm grasping the right side again scoring 0: The right arm does not grasp the shorts. scoring 0.5: The right arm attempts to grasp the shorts multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The right arm successfully grasps the shorts on the first attempt without any slippage. Step 6: Bimanual folding of the shorts with the left arm holding and the right arm folding scoring 0: The shorts are not folded or the folding is incomplete. scoring 0.5: The shorts are folded but not perfectly aligned or the folding is not symmetrical. scoring 1: The shorts are folded perfectly, with symmetrical alignment. 3) Pour Water Task description. Pour Water is fine-grained manipulation task requiring the robot to grasp kettle handle and pour water into cup. The task consists of two steps: first, the robot must grasp the kettle handle; and second, the robot must pour water from the kettle into the cup. The task requires precise control over the kettles position and pouring angle to ensure accurate pouring. The complexity of this task lies in the need for precise spatial and temporal control during the pouring process, including accurate positioning, controlled pouring, and managing the flow rate of the water. Scoring criteria. Step 1: Grasping the kettle handle scoring 0: The gripper does not grasp the kettle handle. scoring 0.5: The gripper attempts to grasp the kettle handle multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The gripper successfully grasps the kettle handle on the first attempt without any slippage. Step 2: Pouring water from the kettle into the cup scoring 0: The robot does not pour the water, or the 4) Make Sandwich Task description. Make Sandwich is long-horizon task that sequentially involves picking up bread, ham, and lettuce to assemble sandwich in proper order. The task consists of eight steps: first, the robot must grasp the first slice of bread; second, place the bread on the plate; third, grasp slice of ham; fourth, place the ham on the bread; fifth, grasp piece of lettuce; sixth, place the lettuce on the ham; seventh, grasp the second slice of bread; and eighth, place the bread on the lettuce. The complexity of this task lies in the need for precise sequential manipulation and the long-horizon nature of the task, requiring the robot to maintain accuracy and stability over multiple steps. Scoring criteria. Step 1: Grasping the first slice of bread scoring 0: The gripper does not grasp the bread. scoring 0.5: The gripper attempts to grasp the bread multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The gripper successfully grasps the bread on the first attempt without any slippage. Step 2: Placing the bread on the plate scoring 0: The bread is not placed on the plate or is dropped. scoring 0.5: The bread is placed on the plate but not accurately or takes multiple attempts. scoring 1: The bread is accurately placed on the plate on the first attempt. Step 3: Grasping slice of ham scoring 0: The gripper does not grasp the ham. scoring 0.5: The gripper attempts to grasp the ham multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The gripper successfully grasps the ham on the first attempt without any slippage. Step 4: Placing the ham on the bread scoring 0: The ham is not placed on the bread or is dropped. scoring 0.5: The ham is placed on the bread but not accurately or takes multiple attempts. scoring 1: The ham is accurately placed on the bread on the first attempt. Step 5: Grasping piece of lettuce scoring 0: The gripper does not grasp the lettuce. scoring 0.5: The gripper attempts to grasp the lettuce multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The gripper successfully grasps the lettuce on the first attempt without any slippage. Step 6: Placing the lettuce on the ham water is poured outside the cup. scoring 0: The lettuce is not placed on the ham or is scoring 0.5: The robot pours the water but spills some water outside the cup, or the pouring process takes too long or requires multiple attempts to succeed. dropped. scoring 0.5: The lettuce is placed on the ham but not accurately or takes multiple attempts. scoring 1: The robot successfully pours the water into scoring 1: The lettuce is accurately placed on the ham the cup without any spillage on the first attempt. on the first attempt. Step 7: Grasping the second slice of bread scoring 0: The trash is not placed into the trash bin or 14 is dropped. scoring 0.5: The trash is placed into the trash bin but not accurately or takes multiple attempts. scoring 1: The trash is accurately placed into the trash bin on the first attempt. 7) Industrial Sorting Task description. Industrial Sorting is precision manipulation task that requires the robot to identify two different items and place them into their corresponding designated areas using the left and right arms. The task consists of four steps: first, the right arm must grasp item 1; second, the right arm must place item 1 into its designated area; third, the left arm must grasp item 2; and fourth, the left arm must place item 2 into its designated area. The complexity of this task lies in the need for precise control over the grasping and placement actions, ensuring that each item is handled carefully and placed accurately into the correct area. Scoring criteria. Step 1: Right arm grasping item 1 scoring 0: The right gripper does not grasp item 1. scoring 0.5: The right gripper attempts to grasp item 1 multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The right gripper successfully grasps item on the first attempt without any slippage. Step 2: Right arm placing item 1 into its designated area scoring 0: Item 1 is not placed into its designated area or is dropped. scoring 0.5: Item 1 is placed into its designated area but not accurately or takes multiple attempts. scoring 1: Item 1 is accurately placed into its designated area on the first attempt. Step 3: Left arm grasping item 2 scoring 0: The left gripper does not grasp item 2. scoring 0.5: The left gripper attempts to grasp item 2 multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The left gripper successfully grasps item 2 on the first attempt without any slippage. Step 4: Left arm placing item 2 into its designated area scoring 0: Item 2 is not placed into its designated area or is dropped. scoring 0.5: Item 2 is placed into its designated area but not accurately or takes multiple attempts. scoring 1: Item 2 is accurately placed into its designated area on the first attempt. scoring 0: The gripper does not grasp the bread. scoring 0.5: The gripper attempts to grasp the bread multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The gripper successfully grasps the bread on the first attempt without any slippage. Step 8: Placing the bread on the lettuce scoring 0: The bread is not placed on the lettuce or is dropped. scoring 0.5: The bread is placed on the lettuce but not accurately or takes multiple attempts. scoring 1: The bread is accurately placed on the lettuce on the first attempt. 5) Package Product Task description. Package Product is precision manipulation task that requires the robot to grasp product and place it into bag. The task consists of two steps: first, the robot must grasp the product; and second, the robot must place the product into the bag. The complexity of this task lies in the need for precise control over the grasping and placement actions, ensuring that the product is handled carefully and placed accurately into the bag. Scoring criteria. Step 1: Grasping the product scoring 0: The gripper does not grasp the product. scoring 0.5: The gripper attempts to grasp the product multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The gripper successfully grasps the product on the first attempt without any slippage. Step 2: Placing the product into the bag scoring 0: The product is not placed into the bag or is dropped. scoring 0.5: The product is placed into the bag but not accurately or takes multiple attempts. scoring 1: The product is accurately placed into the bag on the first attempt. 6) Clean Trash Task description. Clean Trash is precision manipulation task that requires the robot to grasp trash and place it into trash bin. The task consists of two steps: first, the robot must grasp the trash; and second, the robot must place the trash into the trash bin. The complexity of this task lies in the need for precise control over the grasping and placement actions, ensuring that the trash is handled carefully and placed accurately into the trash bin. Scoring criteria. Step 1: Grasping the trash scoring 0: The gripper does not grasp the trash. scoring 0.5: The gripper attempts to grasp the trash multiple times, struggles to maintain stable grasp, or takes too long to succeed. scoring 1: The gripper successfully grasps the trash on the first attempt without any slippage. Step 2: Placing the trash into the trash bin"
        }
    ],
    "affiliations": [
        "AgiBot",
        "Beihang University",
        "Shanghai AI Lab",
        "Shanghai Innovation Institute",
        "The University of Hong Kong"
    ]
}