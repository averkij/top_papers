{
    "paper_title": "Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based Spatiotemporal Diffusion for Audio-driven Talking Portrait",
    "authors": [
        "Chaolong Yang",
        "Kai Yao",
        "Yuyao Yan",
        "Chenru Jiang",
        "Weiguang Zhao",
        "Jie Sun",
        "Guangliang Cheng",
        "Yifei Zhang",
        "Bin Dong",
        "Kaizhu Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio-driven single-image talking portrait generation plays a crucial role in virtual reality, digital human creation, and filmmaking. Existing approaches are generally categorized into keypoint-based and image-based methods. Keypoint-based methods effectively preserve character identity but struggle to capture fine facial details due to the fixed points limitation of the 3D Morphable Model. Moreover, traditional generative networks face challenges in establishing causality between audio and keypoints on limited datasets, resulting in low pose diversity. In contrast, image-based approaches produce high-quality portraits with diverse details using the diffusion network but incur identity distortion and expensive computational costs. In this work, we propose KDTalker, the first framework to combine unsupervised implicit 3D keypoint with a spatiotemporal diffusion model. Leveraging unsupervised implicit 3D keypoints, KDTalker adapts facial information densities, allowing the diffusion process to model diverse head poses and capture fine facial details flexibly. The custom-designed spatiotemporal attention mechanism ensures accurate lip synchronization, producing temporally consistent, high-quality animations while enhancing computational efficiency. Experimental results demonstrate that KDTalker achieves state-of-the-art performance regarding lip synchronization accuracy, head pose diversity, and execution efficiency.Our codes are available at https://github.com/chaolongy/KDTalker."
        },
        {
            "title": "Start",
            "content": "Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based Spatiotemporal Diffusion for Audio-driven Talking Portrait Chaolong Yang1,4, Kai Yao2, Yuyao Yan3, Chenru Jiang6, Weiguang Zhao1,5, Jie Sun4*, Guangliang Cheng1, Yifei Zhang7, Bin Dong6, Kaizhu Huang6* 1Department of Computer Science, University of Liverpool, Liverpool, L69 7ZX, UK. 2Ant Group, Hangzhou, 310000, China. 3School of Robotic, Xian Jiaotong-Liverpool University, Suzhou, 215123, China. 4*Department of Mechatronics and Robotics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China. 5Department of Foundational Mathematics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China. 6*Digital Innovation Research Center, Duke Kunshan University, Kunshan, 215316, China. 7Ricoh Software Research Center, Beijing, 100027, China. *Corresponding author(s). E-mail(s): Jie.Sun@xjtlu.edu.cn; Kaizhu.Huang@dukekunshan.edu.cn; Contributing authors: Chaolong.Yang@liverpool.ac.uk; jiumo.yk@antgroup.com; yuyao.yan@xjtlu.edu.cn; Chenru.Jiang@dukekunshan.edu.cn; Weiguang.Zhao@liverpool.ac.uk; Guangliang.Cheng@liverpool.ac.uk; yifei.zhang@cn.ricoh.com; bin.dong@dukekunshan.edu.cn; These authors contributed equally to this work. Audio-driven single-image talking portrait generation plays crucial role in virtual reality, digital human creation, and filmmaking. Existing approaches are Abstract 1 5 2 0 2 7 1 ] . [ 1 3 6 9 2 1 . 3 0 5 2 : r generally categorized into keypoint-based and image-based methods. Keypointbased methods effectively preserve character identity but struggle to capture fine facial details due to the fixed points limitation of the 3D Morphable Model. Moreover, traditional generative networks face challenges in establishing causality between audio and keypoints on limited datasets, resulting in low pose diversity. In contrast, image-based approaches produce high-quality portraits with diverse details using the diffusion network but incur identity distortion and expensive computational costs. In this work, we propose KDTalker, the first framework to combine unsupervised implicit 3D keypoint with spatiotemporal diffusion model. Leveraging unsupervised implicit 3D keypoints, KDTalker adapts facial information densities, allowing the diffusion process to model diverse head poses and capture fine facial details flexibly. The custom-designed spatiotemporal attention mechanism ensures accurate lip synchronization, producing temporally consistent, high-quality animations while enhancing computational efficiency. Experimental results demonstrate that KDTalker achieves state-of-the-art performance regarding lip synchronization accuracy, head pose diversity, and execution efficiency. Our codes are available at https://github.com/chaolongy/KDTalker. Keywords: Talking Portrait Generation, Audio-driven, Spatiotemporal Diffusion, Diversity Pose"
        },
        {
            "title": "1 Introduction",
            "content": "The generation of natural and dynamic talking portraits has wide-ranging applications in computer vision and digital content creation, such as virtual reality, digital human creation, and film and television production. Creating realistic talking head videos requires precise lip synchronization and natural pose variations, which has been challenging task. Early research [14] primarily focused on lip synchronization, but by neglecting the diversity of head movements and facial expressions, these methods often produced stiff animations that lacked realism. As technology has advanced, researchers have increasingly recognized that relying solely on lip movements is insufficient to meet the demand for realism, leading to more comprehensive exploration of facial animation, including head poses, eye movements, and facial expressions. Previous approaches can be broadly categorized into keypoint-based and imagebased methods to address these challenges. Explicit keypoint-based methods [57], such as SadTalker [5], rely on audio to predict 3D Morphable Model (3DMM) [8] representation, which is mapped to unsupervised implicit 3D keypoints and input to pre-trained facial rendering to generate facial animations that preserve character identity. Image-based methods [911] generate talking faces by directly training an image generator, typically producing high-quality images thanks to powerful pre-trained models such as Stable Diffusion [12]. However, keypoint-based methods struggle to capture subtle facial movements, such as eye movement and frowning, due to the fixed nature of traditional 3DMM keypoints, limiting the expression detail of the generated animations. Additionally, these methods use conventional generative networks like Variational Autoencoders 2 Fig. 1 The proposed KDTalker: keypoint-based spatiotemporal diffusion framework that generates synchronized, high-fidelity talking videos from audio and single image, enhancing pose diversity and expression detail with realistic, temporally consistent animations. (VAEs) [13] and Generative Adversarial Networks (GANs) [14], which fail to effectively model the causal relationship between audio and key points on limited datasets, leading to poor diversity. On the other hand, image-based methods demand significant computational resources, and the image diffusion process considerably slows down generation speed, hindering real-time application. Moreover, while these methods excel in detail, they often suffer from identity distortion, leading to loss of character features in the generated faces. Notably, even latent-space approaches like VASA-1 [15] sacrifice explicit head pose control for implicit motion modeling, limiting motion flexibility. To generate more nuanced facial expressions and diverse head poses while maintaining efficiency, we propose novel implicit keypoint-based spatiotemporal diffusion framework, KDTalker. This model uniquely combines the flexibility of unsupervised implicit 3D keypoint-driven methods with the diversity capabilities of diffusion models. KDTalker predicts unsupervised implicit 3D deformation keypoints and transformation parameters from audio, capturing variations in lip movements, facial expressions, and head poses. These transformed keypoints are used for facial rendering to match the dynamics of the audio sequence. Unlike traditional 3DMM, our model does not rely on fixed keypoint positions. Instead, the learned keypoints can adapt to varying facial information densities, allowing for flexible capture of subtle motion details through the diffusion process, significantly enhancing the expressiveness of the generated results. Additionally, KDTalker incorporates spatiotemporal attention mechanism to capture long-range dependencies between audio and 3D keypoint mappings, ensuring that the generated animations are not only natural and smooth but also precisely synchronized with the audio. In summary, our KDTalker framework differs from previous works in three key aspects. First, unlike explicit keypoint-based methods [57] that rely on fixed 3DMM keypoints or landmarks, our unsupervised implicit keypoints dynamically adapt to facial feature densities, enabling flexible motion capture via diffusion modeling. This removes the rigidity of predefined keypoints and simplifies processing. Second, while conventional keypoint-driven approaches use VAEs/GANs [13, 14], our spatiotemporal 3 Fig. 2 Inference Time vs Head Diversity & LSE-D. The value of LSE-D (Lip Sync Error Distance), metric quantifying the alignment between lip movements and audio, is represented by the size of the circle. smaller circle indicates lower LSE-D value, reflecting better lip sync performance. diffusion model enhances expressiveness by modeling causal audio-to-keypoint relationships. Third, compared to non-keypoint image-based methods [911], which suffer from high computational costs and identity distortion, KDTalker achieves efficient, real-time generation while preserving identity through keypoint-guided rendering. Additionally, our framework offers explicit control over head poses, overcoming the motion constraints of latent-space methods like VASA-1 [15]. This integration of implicit keypoints and diffusion modelling achieves higher lip synchronization accuracy, richer head poses diversity and more efficient generation speed, as shown in Fig. 2. The results of pose diversity in the video generated by KDTalker can be seen in Fig. 1. Our contribution is threefold: We propose novel implicit keypoint-diffusion-based audio-driven talking portrait framework, named KDTalker, that utilizes spatiotemporal-aware attention to capture long-term dependencies between keypoints and audio, allowing for more accurate and contextually aligned facial movements. We are the first to integrate unsupervised implicit 3D keypoint-driven methods with diffusion models, significantly improving head pose and facial expression diversity while maintaining efficient computational performance suitable for real-time applications. Experimental results demonstrate that KDTalker achieves state-of-the-art performance in lip synchronization accuracy, head poses diversity and generation speed."
        },
        {
            "title": "2.1 Audio-driven One-shot Talking Portrait",
            "content": "Early research [14] primarily focused on generating accurate lip-synced talking portrait videos from audio. However, these methods often had limitations, leading to fixed head movements that concentrated mainly on mouth motion, restricting overall realism. With the advent of deep learning, attention shifted to more comprehensive models that could capture not only mouth movements but also natural head poses and facial expressions, providing more realistic depiction. Current approaches can be broadly divided into keypoint-based [5, 1619] and image-based [911, 15, 2022] methods. Previous keypoint-based methods primarily rely on supervised keypoints, such as 3DMM [5, 18, 19] or facial landmarks [7, 16, 17], which map audio features to predefined keypoints for lips, blinks, and head poses. Although 3DMM provides structured 3D parameterization, its fixed keypoint configuration constrains the capture of fine facial details and nuanced expressions. For instance, SadTalker [5] employs unsupervised implicit 3D keypoints in the final rendering stage while it is fundamentally based on initial 3DMM mappings, thus inheriting the limitations of supervised 3DMM models. Furthermore, SPACE [7] rely on predefined explicit keypoints as intermediate representations, limiting their flexibility. In contrast, KDTalker directly predicts implicit keypoints from audio, significantly simplifying intermediate processing. The distances and distribution relationships among the unsupervised implicit 3D keypoints dynamically adapt to facial feature density, enabling flexible control of expression details and head motion amplitude. Image-based approaches [911] typically rely on pre-trained image generators, such as Stable Diffusion [12], to directly generate facial frames, often yielding high-quality images with detailed textures. Further research has focused on enhancing generation quality [2022]. However, these methods are computationally demanding and require considerable inference time due to the image generation process, making real-time application challenging. Additionally, while these methods produce high visual fidelity, they tend to suffer from identity distortion, causing loss of consistency in character features across frames. Specifically, VASA-1 [15] adopts an implicit latent space to model audio-driven motion, which restricts direct control over the generated characters head pose and movement amplitude. In contrast, our KDTalker framework leverages transformation parameters, allowing for more control over head posture, thereby offering greater flexibility in pose adjustment."
        },
        {
            "title": "2.2 Video-driven One-shot Talking Portrait",
            "content": "Video-driven one-shot talking portrait synthesis involves transferring the motion from driving video to target portrait, aiming to reproduce the head movements, expressions, and lip-sync from the source in the target. Video-driven methods focus on motion transfer within the same domain (video-to-video), making the task relatively easier compared to audio-driven methods. Researchers have extensively explored this field by learning intermediate motion representations, such as unsupervised landmarks [2328], 3DMM [18, 29, 30], and latent animations [31, 32]. Our approach, KDTalker, leverages 5 Fig. 3 Overview of the proposed KDTalker for talking portrait synthesis. the latest video-driven method, LivePortrait [28], to generate implicit unsupervised 3D keypoints. We then train keypoint-based spatiotemporal diffusion model using these keypoints. During inference, KDTalker predicts implicit unsupervised 3D keypoints from the input reference image and audio, fed into LivePortrait for rendering and video generation. This approach effectively bridges the gap between video-driven and audio-driven tasks, ensuring the natural synchronization of facial movements with the audio."
        },
        {
            "title": "3.1 Overview",
            "content": "As illustrated in Fig. 3, our framework generates talking head videos by using unsupervised 3D facial keypoints as an intermediate representation. The process begins with the motion extractor extracting motion information from the reference image, while the audio encoder extracts audio features to serve as conditions for the diffusion model. Next, the Reference-Guided Priors module integrates this motion information with noise sequence, preparing the input for the model. The Keypoint-based Spatiotemporal Diffusion module then uses this input and audio feature to predict expression deformation keypoints δ, scaling and translation parameters and t, as well as head rotation R. Finally, the predicted values (s, R, t, δ) are transformed into driving keypoints xd, which are passed to the Face Render module. Through warping and decoding, this module generates video frames, producing lifelike, synchronized animation that aligns with the input audio. Each component is detailed further in the following sections: audio and image processing in Sec. 3.2, reference-guided priors in Sec. 3.3, keypoint-based spatiotemporal diffusion in Sec. 3.4, and keypoint-driven image animator in Sec. 3.5 6 Fig. 4 Reference-Guided Priors."
        },
        {
            "title": "3.2 Image and Audio Processing",
            "content": "The Motion Extractor module leverages the pre-trained LivePortrait [28] framework to derive critical elements from the reference image. These include canonical keypoints that outline the fundamental facial structure, expression deformation keypoints capturing nuanced facial movements driven by speech and emotion, and head pose parameters that denote head orientation. Through an unsupervised learning process, LivePortrait generates flexible 3D keypoints without fixed positions, allowing canonical keypoints to adapt across different reference images. This flexibility enables the model to capture fine-grained expressions and wide range of head poses with greater accuracy. The Appearance Feature Extractor extracts the appearance features of the reference image fs, preserving the visual identity of the subject in the Face Render process. Together, these components ensure identity retention and structural coherence in the generated videos. Concurrently, the audio feature extraction module employs the pre-trained Wav2Lip [3] AudioEncoder to capture audio-derived features, which serve as conditioning inputs for the model."
        },
        {
            "title": "3.3 Reference-Guided Priors",
            "content": "The Reference-Guided Priors module integrates motion information from two key reference frames with noise sequence to guide the generation process, as shown in Fig. 4. Initially, the canonical keypoints xc extracted from the reference image are used as the input for the first frame, with zero-padding applied to align other parameters. This setup provides the model with prior information on the facial structure, ensuring the canonical keypoints serve as foundation for subsequent frames. In our ablation study, we demonstrate the importance of this prior to achieving accurate lip synchronization. Specifically, the expression deformation δ0 aligns with xc, while other parameters are aligned with zero-padding. These priors (Ref1 & Ref2) are then integrated with noise sequence N1,...,n to construct the Input1,...,n+2 for the diffusion model. By initializing the process with reference frames, the model conditions the noise sequence on structured motion information, enabling temporally coherent motion generation that aligns with the original facial structure and adapts to the audio-driven dynamics."
        },
        {
            "title": "3.4 Keypoint-based Spatiotemporal Diffusion",
            "content": "The Keypoint-based Spatiotemporal Diffusion module serves as the core generative component, leveraging latent diffusion to predict motion parameters that create 7 realistic, temporally consistent talking head animations. Given the structured priors and audio conditioning, the task can be formally defined as learning mapping : (xc, s0, R0, t0, δ0, N1,...,n, a1,...,n) (s, R, t, δ)1,...,n. These inputs are iteratively denoised by the diffusion model to obtain -dimensional expression deformation keypoints and transformation parameters. Subsequently, the canonical keypoints xc are transformed according to the following equation: xd = (xc + δ) + t, (1) where xd represents the deformed keypoints."
        },
        {
            "title": "3.4.1 Diffusion Process",
            "content": "The forward diffusion process begins by initializing latent variable z0 which encapsulates the facial features, including deformation keypoints and transformation parameters. The model then introduces noise progressively over steps to corrupt this latent representation. This process can be mathematically expressed as: zt = (cid:112)1 βtzt1 + (cid:112)βtϵt1, = αtz0 + 1 αtϵ, (2) where ϵ (0, I), βt is the diffusion rate corresponding to the t-th step in the increasing sequence with value range between (0, 1), and αt = (cid:81)t i=1(1 βi) is parameter expression defined based on the diffusion rate βt. In the context of our framework, the forward process is conditioned on both the reference image and audio features. During training, the reference-guided prior module adds noise to the real keypoints and transformation parameters, and combines the motion information of the reference image to form structured input. These structured input, along with audio features, are then processed through the model to predict the added noise. The goal of training the diffusion model is to learn reverse process that reconstructs the original latent representation z0 from the noisy latent variables zt. This reverse process is modelled by neural network ϵθ, which attempts to predict the noise added at each timestep t. In particular, ϵθ here refers to our spatiotemporal-aware attention network. The corresponding loss function for training is the mean squared error (MSE) between the predicted noise and the actual noise ϵt: (θ) = Et,zt,a,θ[ϵt ϵθ(zt, a, t)2 2], (3) where ϵθ(zt, a, t) is the models prediction of the noise added at time t, denotes the extracted audio features. During inference, starting from Gaussian noise zt, the model iteratively applies the reverse diffusion process to denoise zt and recover the original latent variable z0. In our work, z0 refers to deformation keypoints δ, and transformation parameters (s, 8 Fig. 5 Spatiotemporal-Aware Attention Network. R, t). This reverse process can be described as: zt1 = zt βtϵθ(zt, a, t) + σtϵ, (4) where σt adjusts the noise variance at each step, controlling the amount of added noise and ϵ is sampled from Gaussian distribution, introducing random variability into the process."
        },
        {
            "title": "3.4.2 Spatiotemporal-Aware Attention Network",
            "content": "The Spatiotemporal-Aware Attention Network is designed to maintain both spatial coherence in facial regions and temporal consistency in motion, ensuring that keypoints evolve smoothly across frames while synchronizing with audio input. As illustrated in Fig. 5, this attention network integrates spatial and temporal information directly within the diffusion process to achieve coordinated and realistic motion of facial keypoints. The network consists of several key components: Timestep and Audio Feature Integration. In each diffusion step, the sampling timestep is encoded by Time MLP module to generate temporal embedding vector, which is combined with audio features. This combined vector serves as conditional input to the diffusion model, facilitating audio-driven learning that captures both temporal progression and spatial variations of facial keypoints. ResNet Blocks for Connection. The timestep and audio features are processed through linear layer to produce Scale and Shift embeddings, which are then applied to the combined input generated by the Reference-Guided Priors module. By applying the Scale and Shift transformations, link is established between audio-driven 9 cues and spatiotemporal keypoint adjustments. This transformed input is subsequently passed through the Spatiotemporal-Aware Attention Network via residual connection, preserving essential characteristics from the original integrated input. Spatiotemporal-Aware Self-Attention. Spatiotemporal-Aware Attention Network integrates self-attention mechanism that ensures coherence across spatial and temporal dimensions, aligning predicted motion keypoints with audio cues while capturing dependencies across frames. This mechanism introduces spatiotemporal bias to emphasize both recent frame transitions and spatial consistency within each frame, ensuring fluid motion progression. Intermediate features from ResNet blocks are projected into query, key, and value vectors (Q, K, ), which are further enriched with spatial-temporal positional encodings to capture dynamic, time-sensitive patterns. The resulting embeddings enable the model to interpret relationships across time points and spatial regions in sync with audio rhythm. Attention weights are derived by computing the dot product of the encoded and vectors, followed by softmax normalization, and applied to the vectors in weighted summation. Residual connections preserve original feature information, yielding refined motion keypoints aligned with the temporal structure and spatial details driven by audio. We verified the importance of this attention mechanism in ensuring lip synchronization and video smoothness in ablation experiments."
        },
        {
            "title": "3.5 Face Render",
            "content": "The face rendering component synthesizes the final talking head animation by using motion keypoints and transformation parameters predicted by the Keypoint-based Spatiotemporal Diffusion model, along with appearance features from the reference image. These appearance features, encapsulating the subjects identity and texture, maintain the visual fidelity of the animated face. Meanwhile, the motion keypoints drive audio-synchronized facial expressions, lip movements, and head poses. Rendering is achieved through the LivePortrait [28] warping and decoder modules, which deform the facial structure and reconstruct the visual details frame-by-frame to produce highquality, synchronized video aligned with the input audio."
        },
        {
            "title": "4.1 Experiments Setup",
            "content": "Datasets. The proposed model were trained on the VoxCeleb dataset [33], which includes over 100,000 video clips of 1,251 subjects. Due to misalignments in some video and audio pairs within VoxCeleb, 4,282 aligned video-audio pairs were selected in the training stage. Following the image animation method [23], we cropped the original videos and resized them to 256256. Audio inputs were downsampled to 16k Hz and converted to the mel-spectrogram features, following the settings in Wav2lip [3]. For evaluation, the first 8 seconds of 349 videos from the HDTF dataset [19] were used, with the first frame of each video serving as the reference image for video generation. 10 Evaluation Metric. The effectiveness of our method was demonstrated through several widely recognized evaluation metrics. For lip synchronization and mouth shape accuracy, we used perceptual metrics from Wav2Lip [3], specifically the distance score (LSE-D) and confidence score (LSE-C). To analyze head motion, we assessed the diversity of generated motions by calculating the standard deviation of head motion feature embeddings extracted from the generated frames using Hopenet [34]. Additionally, we evaluated the quality of generated frames with Frechet Inception Distance (FID) [35] for frame realism and cumulative probability blur detection (CPBD) [36] for image sharpness. Identity preservation was assessed by computing the cosine similarity (CSIM) between identity embeddings of source images and generated frames, using ArcFace [37] for embedding extraction. Finally, we measured inference efficiency by reporting Inference Time in frames per second (FPS), indicating the number of frames generated per second. Implementation Details. All experiments were conducted on single NVIDIA GeForce RTX 4090. The proposed KDTalker is an end-to-end model that requires only single training session, and the keypoint diffusion model consists of 42.93M parameters. Furthermore, The diffusion timesteps are set to 1,000 during the training phase, while DDIM [38] is employed for faster inference with 50 steps. The model processed 64 audio frames per pass, with generated image resolution of 512x512. The experiments utilized the AdamW optimizer [39] with learning rate schedule that includes Warmup and cosine annealing, where the rate initially increases linearly to 5.12e4 before decaying. Additionally, the batch size for experiments was set to 256. Finally, the unsupervised motion keypoints and parameters in our training data were extracted using the pre-trained video-driven method LivePortrait [28]."
        },
        {
            "title": "4.2 Comparison with the SOTA Methods",
            "content": "The proposed method was compared with several state-of-the-art approaches for audio-driven talking portrait generation, including keypoint-based methods like SadTalker [5] and Real3DPortrait [6], as well as image-based approaches like AniTalker [10] and AniPortrait [11]. All the methods are compared quantitatively and qualitatively. Quantitative Comparison. As shown in Table 1, the proposed method outperforms existing methods in lip synchronization, head motion diversity, video quality, and inference speed. Specifically, it achieves the highest LSE-C and lowest LSE-D, demonstrating superior lip-sync accuracy, closely matching real video performance. In head motion, our method demonstrates the highest diversity attributed to its powerful generative capabilities with unsupervised keypoints. Though our method may generate an even higher diversity than that of real video, it leads to the smallest diversity difference. For video quality, our CPBD achieves performance comparable to SadTalker, while AniPortrait achieves the highest CPBD thanks to Stable Diffusion [12] but has the slowest inference speed. Nevertheless, our best FID and CSIM scores highlight the superior visual quality and structural accuracy of our output. Furthermore, our inference speed reaches 21.678 FPS, demonstrating its suitability for real-time applications. These results demonstrate our models superiority over state-of-the-art methods. 11 Table 1 Quantitative comparison with the state-of-the-art methods on HDTF dataset. Method Real Video SadTalker [5] Real3DPortrait [6] AniTalker [10] AniPortrait [11] Ours Venue - CVPR23 ICLR24 MM24 ArXiv24 - Lip Synchronization Head Motion Diversity LSE-C LSE-D 0.639 0.494 0.118 0.374 0.353 0.760 6.929 7.813 7.804 8.077 10.946 7.548 8.243 7.121 7.004 7.107 3.438 7.326 Video Quality FID CPBD CSIM 0.353 0.000 0.280 12.701 0.239 17.428 0.305 10.244 0.401 14.480 9.756 0. 1.000 0.929 0.937 0.893 0.928 0.949 Inference Time FPS 25 21.277 3.092 14.045 1.555 21.678 Fig. 6 Qualitative comparison with the state-of-the-art methods on HDTF dataset. Qualitative Comparison. Qualitative visual results of various methods are provided, as shown in Fig. 6. In addition, we also provide the GT results under the corresponding audio frames to visualize the lip synchronization of our method. As can be seen from the figure, our method has lip movements similar to the GT video, but has richer head movements. The head movement range of all the competitors is relatively small and lacks diversity. To further verify the head diversity of our method, the motion heatmaps of various methods were calculated and shown in Fig. 7. In addition, AniPortraits lips are distorted, resulting in poor lip synchronization performance. Please refer to our supplementary videos for clearer comparison. 12 Fig. 7 Qualitative comparison of head motion diversity between existing methods and our proposed approach."
        },
        {
            "title": "4.3 User Studies",
            "content": "A user study was conducted to evaluate the performance of all methods. We selected 20 sample images and corresponding audio from the HDTF dataset as tests. These reference sample images contain different genders, ages, expressions, poses, and backgrounds to show the robustness of our method. The study involved 20 participants, who were asked to subjectively evaluate and select the best-performing method for each sample on each criterion, which included: lip synchronization, the diversity of head motion and overall naturalness. In particular, the participants comprised 13 males and 7 females with diverse professional backgrounds, including doctors, algorithm researchers, product managers, engineers, quality inspectors, and students of different grades. To eliminate any potential bias caused by the order of presentation, the sequence of methods in the comparative videos was randomized for each sample, ensuring the authenticity and fairness of the evaluations. The results are shown in Table 2. In terms of the three dimensions, participants prefer our generated videos. It is worth noting that the overall naturalness scores of 0% for AniPortrait [11] and Real3DPortrait [6] can be attributed to different issues. AniPortrait [11] has obvious lip distortion, while Real3DPortrait [6] suffers from fixed poses. More user study details can be seen in the supplemental material. Table 2 User study showing the percentage of participants who selected method as the best across three evaluation dimensions. Diversity Naturalness Method SadTalker [5] Real3DPortrait [6] AniTalker [10] AniPortrait [11] Ours Head Lip Sync. 22.8% 14.5% 0.5% 3.8% 13.0% 39.0% 2.0% 2.0% 40.7% 61.7% Overall 15.8% 0.0% 38.5% 0.0% 45.7%"
        },
        {
            "title": "4.4.1 Ablation Study for Main Components",
            "content": "First, we verify the important role of the typical key point xc in the reference image. As shown in Table 3, removing xc reduces lip synchronization by approximately 8%, as xc offers crucial prior keypoint information. The Spatiotemporal-Aware Attention module captures temporal dependencies between frames by modeling their sequential relationships through Rotary Position Embedding, thereby establishing proximity relationships between frames. This design enables the model to simultaneously address short-term dynamics and long-term temporal evolution, rather than treating all video frames as independent inputs. Excluding the spatiotemporal attention mechanism causes sharp drop in lip synchronization, resulting in excessive and incoherent head movements that lead to abnormally high head motion diversity. We observed that lip synchronization is stable for frontal head poses but declines significantly with larger head movements (e.g., profiles or side views). To validate the advantages of RoPE, we conducted additional ablation studies, as shown in Table 3. The results show that RoPE significantly enhances lip synchronization (higher LSE-C, lower LSE-D) and head motion diversity compared to vanilla self-attention. Overall, our method effectively balances coherent and naturally diverse head movements with strong lip synchronization, demonstrating the advantages of incorporating RoPE and spatiotemporal attention in generating high-quality talking head videos. Table 3 Ablation study for main components in proposed method. Method w/o Reference xc w/o Attention w/o RoPE Ours Full Lip Synchronization Head Motion Diversity LSE-C LSE-D 0.791 2.292 0.706 0.760 8.203 11.777 7.663 7.548 6.656 1.360 7.225 7."
        },
        {
            "title": "4.4.2 Comparison of Generative Methods",
            "content": "To demonstrate the advantages of diffusion models, we compared our model with versions that replace the generator with VAE [13] and GAN [14], keeping the overall architecture unchanged. As shown in Table 4, the VAE model achieves diverse head movements but lacks accurate lip-audio synchronization, while GAN suffers from convergence issues, leading to distorted outputs. The combined VAE+GAN approach, similar to SadTalker, shows some audio-to-lip mapping but fails to maintain temporal consistency in head movements, resulting in unconvincing outputs. In contrast, the diffusion-based method achieves superior synchronization and coherent, naturally diverse head poses, demonstrating its effectiveness for high-quality talking portrait generation. 14 Table 4 Comparison of our model with different generators Method VAE [13] GAN [14] VAE [13] + GAN [14] Diffusion (Ours) Lip Synchronization Head Motion Diversity LSE-C LSE-D 0.846 - 0.876 0. 13.879 - 13.284 7.548 0.665 - 1.232 7."
        },
        {
            "title": "4.4.3 Ablation of Face Render",
            "content": "High-quality face render is crucial for generating realistic and engaging animated videos, as it directly affects both the visual authenticity and the coherence of facial movements. We compared Face-vid2vid [24] and LivePortrait [28] across key dimensions such as lip synchronization and overall video quality to evaluate the effectiveness of different face rendering methods. As shown in Table 5, LivePortrait demonstrates distinct advantages in capturing accurate lip movements and enhancing visual authenticity through greater number of unsupervised keypoints, which allow it to represent facial details more precisely. Although Face-vid2vid performs slightly better in certain video quality metrics like image clarity, LivePortraits marginally lower scores do not significantly impact its overall output. The slight trade-off in sharpness is offset by LivePortraits superior lip synchronization and structural realism, making it highly effective for producing lifelike animated portraits. Table 5 Ablation of Face Render. F.V. denotes Face-vid2vid, while L.P. stands for LivePortrait, which is used in our method. Method Points Lip Synchronization LSE-C LSE-D FID CPBD CSIM Video Quality F.V. [24] L.P. [28] 15 21 5.579 7.326 9.228 7.548 8.625 0.307 0.277 9.756 0.940 0."
        },
        {
            "title": "4.4.4 Ablation of Inference Step",
            "content": "To balance performance and efficiency during inference, we conducted an ablation study to explore the impact of different sampling steps when using the DDIM strategy. Table 6 shows the results, focusing on lip synchronization and video quality. The findings show that increasing the number of steps initially improves both metrics, particularly from 1 to 5 steps, but further increases yield diminishing returns. The best video quality, reflected in the lowest FID, is observed at 50 steps, while CPBD and CSIM remain consistently high. Although fewer steps slightly enhance lip synchronization, 50 steps provide the optimal balance between quality, synchronization, and computational cost. This motivated our decision to use 50 steps for inference to achieve efficient and high-quality results. 15 Table 6 Ablation of inference step Step Lip Synchronization LSE-C LSE-D 1 5 10 20 50 100 0.817 7.455 7.448 7.394 7.326 7.264 7.221 12.630 7.424 7.436 7.501 7.548 7.614 7.633 Video Quality FID CPBD CSIM 0.262 19.058 10.226 0.277 0.278 9.939 0.277 9.797 9.756 0.277 0.277 9.987 0.277 9.901 0.823 0.949 0.948 0.945 0.949 0.948 0."
        },
        {
            "title": "4.4.5 Ablation of Frame Number",
            "content": "To improve video coherence, particularly in lip synchronization and head motion, we explored the impact of different frame numbers. Table 7 shows that increasing the number of frames generally improves lip synchronization and head motion diversity, with 64 frames achieving the best performance. Using 64 frames allows for better temporal dependencies between frames, which helps achieve smoother transitions and more natural motion. This results in improved lip synchronization and head motion consistency, making 64 frames favourable choice for enhancing video quality and continuity. Table 7 Ablation of frame number. Frame Number 8 16"
        },
        {
            "title": "5 Conclusion",
            "content": "Lip Synchronization Head Motion Diversity LSE-C LSE-D 0.673 0.687 0.686 0.760 7.928 7.904 7.636 7.548 6.875 6.912 7.256 7.326 In this work, we presented KDTalker, novel framework for generating natural and dynamic talking portraits that effectively combines the strengths of unsupervised 3D keypoint-driven animation with the diversity capabilities of diffusion models. KDTalker addresses the limitations of previous keypoint-based and image-based methods by offering both high lip synchronization accuracy and rich head pose diversity, which are essential for producing realistic and expressive digital portraits. Unlike traditional 3DMM with fixed keypoints, KDTalker adapts to varying facial information densities, capturing nuanced expressions. It also integrates spatiotemporal attention mechanism to capture long-range audio-keypoint dependencies, ensuring precise lip synchronization and coherent head poses that reflect speech dynamics. Through extensive experiments, KDTalker achieves state-of-the-art performance on multiple metrics, becoming promising solution for real-time applications. 16 Limitation. The KDTalker framework, while innovative, has limitations primarily arising from its reliance on keypoint detection and transformation. Since KDTalker relies on precise 3D keypoint detection for facial rendering, noisy data or complex facial features can result in misalignments and distortions in the animation. Additionally, the framework struggles with occlusions, where parts of the face are blocked by external objects. These occlusions can disrupt the keypoint transformation process, resulting in artefacts such as blurred edges or distorted features, particularly in areas crucial to facial expressions like the eyes, mouth, and nose. These limitations underscore the frameworks dependence on precise keypoint detection, which remains vulnerable to errors in complex scenarios, impacting both the accuracy and expressiveness of the generated animations."
        },
        {
            "title": "References",
            "content": "[1] Suwajanakorn, S., Seitz, S.M., Kemelmacher-Shlizerman, I.: Synthesizing obama: learning lip sync from audio. ACM TOG 36(4), 113 (2017) [2] KR, P., Mukhopadhyay, R., Philip, J., Jha, A., Namboodiri, V., Jawahar, C.: Towards automatic face-to-face translation. In: ACM MM, pp. 14281436 (2019) [3] Prajwal, K., Mukhopadhyay, R., Namboodiri, V.P., Jawahar, C.: lip sync expert is all you need for speech to lip generation in the wild. In: ACM MM, pp. 484492 (2020) [4] Cheng, K., Cun, X., Zhang, Y., Xia, M., Yin, F., Zhu, M., Wang, X., Wang, J., Wang, N.: Videoretalking: Audio-based lip synchronization for talking head video editing in the wild. In: SIGGRAPH ASIA, pp. 19 (2022) [5] Zhang, W., Cun, X., Wang, X., Zhang, Y., Shen, X., Guo, Y., Shan, Y., Wang, F.: Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation. In: CVPR, pp. 86528661 (2023) [6] Ye, Z., Zhong, T., Ren, Y., Yang, J., Li, W., Huang, J., Jiang, Z., He, J., Huang, R., Liu, J., et al.: Real3d-portrait: One-shot realistic 3d talking portrait synthesis. In: ICLR (2024) [7] Gururani, S., Mallya, A., Wang, T.-C., Valle, R., Liu, M.-Y.: Space: Speechdriven portrait animation with controllable expression. In: ICCV, pp. 20914 20923 (2023) [8] Blanz, V., Vetter, T.: morphable model for the synthesis of 3d faces. In: Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques, pp. 187194 (1999) [9] Shen, S., Zhao, W., Meng, Z., Li, W., Zhu, Z., Zhou, J., Lu, J.: Difftalk: Crafting diffusion models for generalized audio-driven portraits animation. In: CVPR, pp. 19821991 (2023) 17 [10] Liu, T., Chen, F., Fan, S., Du, C., Chen, Q., Chen, X., Yu, K.: Anitalker: Animate vivid and diverse talking faces through identity-decoupled facial motion encoding. In: ACM MM, pp. 66966705 (2024) [11] Wei, H., Yang, Z., Wang, Z.: Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694 (2024) [12] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR, pp. 1068410695 (2022) [13] Kingma, D.P.: Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013) [14] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial networks. Communications of the ACM 63(11), 139144 (2020) [15] Xu, S., Chen, G., Guo, Y.-X., Yang, J., Li, C., Zang, Z., Zhang, Y., Tong, X., Guo, B.: Vasa-1: Lifelike audio-driven talking faces generated in real time. NeurIPS 37, 660684 (2024) [16] Chen, L., Maddox, R.K., Duan, Z., Xu, C.: Hierarchical cross-modal talking face generation with dynamic pixel-wise loss. In: CVPR, pp. 78327841 (2019) [17] Zhou, Y., Han, X., Shechtman, E., Echevarria, J., Kalogerakis, E., Li, D.: Makelttalk: speaker-aware talking-head animation. ACM TOG 39(6), 115 (2020) [18] Ren, Y., Li, G., Chen, Y., Li, T.H., Liu, S.: Pirenderer: Controllable portrait image generation via semantic neural rendering. In: ICCV, pp. 1375913768 (2021) [19] Zhang, Z., Li, L., Ding, Y., Fan, C.: Flow-guided one-shot talking face generation with high-resolution audio-visual dataset. In: CVPR, pp. 36613670 (2021) [20] Ma, Y., Zhang, S., Wang, J., Wang, X., Zhang, Y., Deng, Z.: Dreamtalk: When expressive talking head generation meets diffusion probabilistic models. arXiv preprint arXiv:2312.09767 (2023) [21] Kim, S., Jin, S., Park, J., Kim, K., Kim, J., Nam, J., Kim, S.: Moditalker: Motion-disentangled diffusion model for high-fidelity talking head generation. arXiv preprint arXiv:2403.19144 (2024) [22] Tian, L., Wang, Q., Zhang, B., Bo, L.: Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions. arXiv preprint arXiv:2402.17485 (2024) [23] Siarohin, A., Lathuili`ere, S., Tulyakov, S., Ricci, E., Sebe, N.: First order motion model for image animation. NeurIPS 32 (2019) 18 [24] Wang, T.-C., Mallya, A., Liu, M.-Y.: One-shot free-view neural talking-head synthesis for video conferencing. In: CVPR (2021) [25] Hong, F.-T., Zhang, L., Shen, L., Xu, D.: Depth-aware generative adversarial network for talking head video generation. In: CVPR, pp. 33973406 (2022) [26] Wang, S., Li, L., Ding, Y., Yu, X.: One-shot talking face generation from singlespeaker audio-visual correlation learning. In: AAAI, vol. 36, pp. 25312539 (2022) [27] Zhao, J., Zhang, H.: Thin-plate spline motion model for image animation. In: CVPR, pp. 36573666 (2022) [28] Guo, J., Zhang, D., Liu, X., Zhong, Z., Zhang, Y., Wan, P., Zhang, D.: Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168 (2024) [29] Doukas, M.C., Zafeiriou, S., Sharmanska, V.: Headgan: One-shot neural head synthesis and editing. In: ICCV, pp. 1439814407 (2021) [30] Yin, F., Zhang, Y., Cun, X., Cao, M., Fan, Y., Wang, X., Bai, Q., Wu, B., Wang, J., Yang, Y.: Styleheat: One-shot high-resolution editable talking face generation via pre-trained stylegan. In: ECCV, pp. 85101 (2022). Springer [31] Wang, Y., Yang, D., Bremond, F., Dantcheva, A.: Latent image animator: Learning to animate images via latent space navigation. arXiv preprint arXiv:2203.09043 (2022) [32] Mallya, A., Wang, T.-C., Liu, M.-Y.: Implicit warping for animation with image sets. NeurIPS 35, 2243822450 (2022) [33] Nagrani, A., Chung, J.S., Xie, W., Zisserman, A.: Voxceleb: Large-scale speaker verification in the wild. Computer Science and Language 60, 101027 (2019) [34] Ruiz, N., Chong, E., Rehg, J.M.: Fine-grained head pose estimation without keypoints. In: CVPRW, pp. 20742083 (2018) [35] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS 30 (2017) [36] Narvekar, N.D., Karam, L.J.: no-reference image blur metric based on the cumulative probability of blur detection (cpbd). IEEE TIP 20(9), 26782683 (2011) [37] Deng, J., Guo, J., Xue, N., Zafeiriou, S.: Arcface: Additive angular margin loss for deep face recognition. In: CVPR, pp. 46904699 (2019) [38] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: ICLR 19 (2021) [39] Kingma, D.P.: Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)"
        }
    ],
    "affiliations": [
        "Ant Group, Hangzhou, 310000, China",
        "Department of Computer Science, University of Liverpool, Liverpool, L69 7 ZX, UK",
        "Department of Foundational Mathematics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China",
        "Department of Mechatronics and Robotics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China",
        "Digital Innovation Research Center, Duke Kunshan University, Kunshan, 215316, China",
        "Ricoh Software Research Center, Beijing, 100027, China",
        "School of Robotic, Xian Jiaotong-Liverpool University, Suzhou, 215123, China"
    ]
}