{
    "paper_title": "BOW: Bottlenecked Next Word Exploration",
    "authors": [
        "Ming Shen",
        "Zhikun Xu",
        "Xiao Ye",
        "Jacob Dineen",
        "Ben Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are typically trained via next-word prediction (NWP), which provides strong surface-level fluency but often lacks support for robust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel RL framework that rethinks NWP by introducing a reasoning bottleneck where a policy model first generates a reasoning path rather than predicting the next token directly, after which a frozen judge model predicts the next token distribution based solely on this reasoning path. We train the policy model using GRPO with rewards that quantify how effectively the reasoning path facilitates next-word recovery. Compared with other continual pretraining baselines, we show that BOW improves both the general and next-word reasoning capabilities of the base model, evaluated on various benchmarks. Our findings show that BOW can serve as an effective and scalable alternative to vanilla NWP."
        },
        {
            "title": "Start",
            "content": "BOW: Bottlenecked Next Word Exploration Ming Shen, Zhikun Xu*, Xiao Ye, Jacob Dineen, Ben Zhou Arizona State University {mshen16, zhikunxu}@asu.edu 5 2 0 2 6 1 ] . [ 1 2 0 5 3 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are typically trained via next-word prediction (NWP), which provides strong surface-level fluency but often lacks support for robust reasoning. We propose BOttlenecked next Word exploration (BOW), novel RL framework that rethinks NWP by introducing reasoning bottleneck where policy model first generates reasoning path rather than predicting the next token directly, after which frozen judge model predicts the next token distribution based solely on this reasoning path. We train the policy model using GRPO with rewards that quantify how effectively the reasoning path facilitates nextword recovery. Compared with other continual pretraining baselines, we show that BOW improves both the general and next-word reasoning capabilities of the base model, evaluated on various benchmarks. Our findings show that BOW can serve as an effective and scalable alternative to vanilla NWP."
        },
        {
            "title": "Introduction",
            "content": "Large language models have achieved remarkable progress in wide range of natural language understanding and generation tasks, largely powered by next word prediction (NWP) algorithm and supervised finetuning during pretraining and posttraining (Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020; Achiam et al., 2023). The NWP task is widely regarded as the principal mechanism through which LLMs acquire both factual world knowledge, reasoning, and decision-making capabilities (Wei et al., 2022; ichter et al., 2023; Yao et al., 2023). However, recent research has revealed fundamental limitations in the standard supervised NWP approach: always providing the gold next word as supervision can both obscure rich underlying information and encourage models to latch * Equal Contribution 1 onto superficial correlations in the data (e.g., associating Bruce Lee exclusively with Kung Fu), ultimately resulting in brittle generalization and hallucinations (Zhou et al., 2024; Li et al., 2024a; Xu et al., 2025). In essence, traditional NWP equips models to recognize what the next word is, but not why it should be the next word given the context. This limits models capacity for deeper reasoning and fails to capture the generative uncertainty and justification behind language use. Prior works (Jiang et al., 2024; Xu et al., 2025; Ishibashi et al., 2025) have demonstrated that explicitly encouraging models to articulate why certain continuations is likely to lead to substantial gains in reasoning and reliability. In this work, we take this line of inquiry further and introduce BOttlenecked Next Word Exploration (BOW), novel RL framework that fundamentally rethinks the NWP task. Our framework is visualized in Fig. 1. Rather than directly supervising the policy model with observed next words, we instead bottleneck the learning process. Without seeing the actual next word, policy model must first explore to generate comprehensive and self-contained reasoning trajectories that describe what the next words could be. Then, judge model (Li et al., 2024b) assesses the quality of the reasoning trajectories based on the ground truth next word, providing soft-rewards to the policy model for optimization. We finally optimize our policy model using the soft-rewards with GRPO (Shao et al., 2024), while other RL algorithms can also be applied. By replacing traditional supervised NWP with our BOW scheme, we challenge models to go beyond surface correlations and actively construct reasoning paths for the plausible next words. We also propose novel regularization technique that encourages the policy model to match the judge models distribution of words, complementary to the observed gold word itself. This effectively prevents the policy model from \"collapsing,\" where Figure 1: An overview of our proposed BOW RL framework. Given the context, \"I like fruits lot, so for lunch, ate two\", rather than predicting the next word \"pears\" directly, the policy model first generates an intermediate reasoning trajectory. frozen judge model then returns the next-token probability distribution solely based on this reasoning trajectory. The policy model is finally optimized through the reward for its generated reasoning trajectory. The reward is calculated based on the probability of the gold next token in the next-token probability distribution returned by the judge. it learns to always generate small set of specific words instead of providing accurate and comprehensive reasoning, key drawback found in baseline models using hard reward signals. Across 10 benchmarks requiring world knowledge, multi-hop, and factual reasoning, continual pretraining with BOW enhances the general zeroshot reasoning capability of Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct, also outperforming all continual pretraining baselines, including an RL baseline with hard reward design. Moreover, empirical results and human analysis show that BOW improves the intrinsic next-word reasoning capabilities compared with baselines that are continually pretrained with direct next-word supervision. We also show the effectiveness of our novel regularization technique and designs for other critical components through ablation studies. final human analysis shows that BOW leads to better nextword reasoning trajectories that comprehensively reason over all clues from the context and make more human-aligned next-word predictions. Overall, our findings suggest that explicitly bottlenecking next-word prediction is promising direction for building LLMs that are not only knowledgeable but also capable of reasoning about language in more human-like and interpretable fashion."
        },
        {
            "title": "2 Related Work",
            "content": "Elaborated Reasoning Recent research has increasingly emphasized the importance of encouraging LLMs to articulate their internal reasoning processes rather than directly emitting final answers. The ToW framework (Xu et al., 2025) demonstrates that continually pretraining models to generate reasoning connections between words before producing the next word improves factuality and reasoning across range of reasoning tasks. Similarly, Jiang et al. (2024) explores training models to produce natural language rationales between sentences for downstream reasoning tasks, showing that explicit reasoning improves both answer quality and user trust. Moreover, Ishibashi et al. (2025) proposes an unsupervised method to uncover and amplify implicit reasoning signals in domain-specific corpora, mimicking the human thinking processes involved in creating texts. These works motivate our design of bottlenecked reasoning step that compels models to externalize and refine their thought processes before prediction in self-evolving way through reinforcement learning. Bottlenecked Learning Recent work explores forcing models through information bottlenecks constrained intermediate representations that must capture essential reasoning before prediction. Zelikman et al. (2022) introduces Self-Taught Rea2 soner (STaR), which creates reasoning bottleneck by requiring models to generate explicit rationales before answers. The model can only access the final answer through this rationale bottleneck, iteratively learning which reasoning paths lead to correct predictions. Zelikman et al. (2024) further extends this to Quiet-STaR, creating an even tighter bottleneck where models must generate \"thoughts\" between every token during pretraining, not just for explicit questions. More recently, Zhou et al. (2025) demonstrates that bottlenecking can operate at an even more abstract levelforcing models to recognize and transfer high-level reasoning patterns rather than surface-level associations, creating conceptual bottleneck that enables generalization to rare cases. Our BOW framework implements particularly stringent form of bottlenecked learning: the policy model must generate reasoning that successfully guides separate frozen judge model to predict the next token, without ever seeing the gold token itself. This architectural bottleneck ensures that the reasoning must contain sufficient information for an independent model to recover the correct prediction. Reasoning Overfitting growing body of work reveals that LLMs often exploit spurious correlations rather than performing genuine reasoning, phenomenon sometimes termed as reasoning overfitting. Li et al. (2024a) shows that LLMs take deceptive semantic shortcuts, relying on keyword/entity biases instead of following correct reasoning chains. This aligns with findings from Zhou et al. (2024), which demonstrates that LLMs drop 9-28% in performance when forced to reason abstractly rather than rely on surface patterns. In mathematics, which requires much rigor in reasoning, Yu et al. (2024) and Li et al. (2025) have both shown that LLMs so-called math reasoning primarily relies on pattern matching and memorization of solution paths from training data, often establishing spurious correlations between surface-level features and certain mathematical concepts. These works collectively highlight that current LLMs often produce plausible-looking reasoning that masks fundamental failures in logical coherence. Our bottlenecked reasoning approach BOW addresses this by requiring models to generate reasoning that must successfully guide separate judge model, providing an external validation of reasoning quality beyond surface plausibility."
        },
        {
            "title": "3.1 Overview",
            "content": "Bottlenecked Next Word Exploration (BOW) is an RL framework consisting of three components: Bottlenecked Generation, Judge Mechanism, and RL Optimization. BOW first introduces bottleneck process: rather than directly conditioning on the context to predict the next token, the policy model first generates reasoning trajectory τ about plausible next words. Subsequently, separate module, referred to as the judge, computes the nexttoken probability distribution, (wτ ), given the reasoning trajectory τ . The policy model is finally optimized utilizing the reward for its generated reasoning trajectory computed based on (wτ ) and the ground truth next token w, without being explicitly trained on the gold next token."
        },
        {
            "title": "3.2 Bottlenecked Generation",
            "content": "In traditional NWP, models are trained to predict the next token given specific context directly. In contrast, BOW introduces structural bottleneck: rather than directly predicting the next token, the policy model πθ must first generate an intermediate reasoning trajectory τ to reason towards plausible next words without directly seeing the gold next word w. This bottleneck process fundamentally changes the learning schema from one-step classification task to multi-step generative decision-making process, where the reasoning path τ serves as latent action. Notably, the gold next word is never observed by the policy model nor used in any cross-entropy loss. Supervision is provided only through scalar reward signal assigned after the judge model assesses the informativeness and correctness of the generated reasoning path given the gold next word w. To ensure that learning remains feasible in this under-specified setting, we carefully design the policy model prompt to elicit reasoning trajectories that exhibit two critical properties: comprehensively incorporating all relevant contextual features that influence next word reasoning, and providing general characterization of plausible next words rather than explicitly identifying specific candidate words. For example, in Fig. 1, orange texts reflect the first property and the blue texts reflect the second, given the context. solid prompting design provides strong starting point for generating reasoning trajectories and has been shown effective in 3 prior work (Gandhi et al., 2025) to facilitate reasoning supervision in similar low-resource or weakly supervised regimes. Please refer to Fig. 6 in the Appendix for concrete prompt design."
        },
        {
            "title": "PIQA Original Instance",
            "content": "Question: Choose the most sensible solution given the physical goal: To cream butter and sugar together, you can Options: (a) Place it in bowl and use hand warmer (b) Place in bowl and use hand mixer 3."
        },
        {
            "title": "Transformed Instance",
            "content": "The judge model Jϕ is frozen LLM that serves as an evaluator of the reasoning trajectory τ . It receives only the τ , and outputs probability distribution over the vocabulary for the next token of the context: (w τ ) = Jϕ(w τ ). (1) This distribution is interpreted as the judge models best guess for the next token, conditioned on the reasoning path τ for the next token. Thus, the reward for the given reasoning path τ is then defined as the probability assigned to the gold next token under this distribution: r(τ ) = J(w τ ). (2) Importantly, the judge model is not trained to imitate human preferences (Christiano et al., 2017) or score completions step-by-step (Lightman et al., 2024). Instead, it only performs constrained continuation task: predicting the next token given structured intermediate rationale. This choice explicitly creates an information bottleneck and implicitly evaluates the generated reasoning paths by the effectiveness of recovering next tokens, which encourages self-contained and comprehensive analysis from the policy model. To justify that the judge model is able to faithfully reflect the likelihood of candidate next words, as described in the reasoning path, we provide one concrete example in 5.2.1. Moreover, as we observe that the base policy model has already gained useful patterns and behaviors (Gandhi et al., 2025) when prompted to reason on plausible next words, we add L1 regularization style term in the final reward to prevent RL exploration of reasoning trajectories from collapsing into explicitly mentioning only few specific next words instead of generally reasoning about the characteristic of candidate next words. We need to avoid this collapsing behavior since it is counterintuitive to constrain the prediction of next words to few given an open-ended context, and also harmful to our RL algorithm. To achieve this regularization, we obtain reference next-token distribution Jϕ(w C) by directly feeding the context into the Context: To cream butter and sugar into light, fluffy mixture, place them in bowl and use hand Candidate Next Words: (a) warmer (b) mixer"
        },
        {
            "title": "CSQA Original Instance",
            "content": "Question: Where could you find shark before it was caught? Options: (a) pool hall (b) tomales bay (c) marine museum (d) business (e) desert"
        },
        {
            "title": "Transformed Instance",
            "content": "Context: Before being caught, shark naturally lives in the Candidate Next Words: (a) hall (b) bay (c) museum (d) business (e) desert Table 1: Two examples of transforming the original benchmark instances into next word prediction evaluation format. judge model. As result, our final reward of the reasoning trajectory τ is formulated as: R(τ ) = r(τ ) αJϕ(w τ ) Jϕ(w C) (3) where α represents scaling factor. Please refer to 4.3.2 for more details on the implementation of the judge."
        },
        {
            "title": "3.4 RL Optimization",
            "content": "We optimize the policy model πθ with Grouped Reward Policy Optimization (GRPO) (Shao et al., 2024), which improves RL training stability by normalizing rewards within groups of reasoning paths sharing the same context. For each context C, group of reasoning paths {τ1, . . . , τN } are sampled from πθ, and reward Ri(τi) for each reasoning path τi is computed using Eq. 3. GRPO uses the group mean and group standard deviation σ to compute advantages ˆAi = rir σ , reducing gradient variance. Policy model updates are then performed using PPO-style optimization (Schulman et al., 2017). Any other RL algorithms can also be applied in BOW, such as PPO (Schulman et al., 2017) and REINFORCE (Williams, 1992)."
        },
        {
            "title": "4.1 Training Data",
            "content": "We train our models on narratives from the murder mystery domain (Del and Fishel, 2023), which 4 we argue is well-suited for studying reasoningdriven next-word prediction. Mystery stories naturally encode complex world modelsthey describe who did what, when, why, and what happened nextrequiring both commonsense and counterfactual reasoning to interpret. In this sense, we view next-word prediction not just as language modeling task, but as an implicit approximation of world state transitions. Story-driven data thus provides rich, structured inputoutput sequences that align well with our goal of encouraging explicit reasoning in LLMs. Concretely, we use 191 long-form narratives 1 sourced from the 5 Minute Mystery platform,2 filtering out those that exceed 2048 tokens to ensure compatibility with our model context length. This yields 178 narratives for training. To focus learning on reasoning-relevant supervision signals, we further filter the training data to remove contextnext word pairs where the next tokens do not require meaningful reasoning to derive based on the context. Specifically, we discard tokens that are: (i) purely functional (e.g., determiners, punctuation), (ii) syntactically or semantically deterministic based on surface cues, or (iii) explainable without invoking latent knowledge or contextual abstraction. This selective language modeling (SLM) paradigm is inspired by prior work such as RHO-1 (Lin et al., 2024), which demonstrates that focusing training on informative or \"reasoning-heavy\" tokens improves learning efficiency and model generalization. To automate the filtering process, we utilize gpt-4.1-mini-2025-04-143 to evaluate each context-next word pair based on the above criteria. Please refer to Fig. 3 in Appendix for the detailed prompt used. Only context-next word pairs where non-trivial reasoning is required are retained. This filtering pipeline produces final dataset of approximately 45K contextnext word pairs. By aligning training examples with tokens that genuinely demand reasoning, we ensure that the supervision signal is compatible with the bottlenecked learning setup of BOW, where the model is rewarded not for token overlap, but for the quality of its latent reasoning."
        },
        {
            "title": "4.2 Evaluation Data",
            "content": "Our evaluations are implemented with the following benchmarks: CSQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), TruthfulQA (Lin et al., 2022), StrategyQA (Geva et al., 2021), ARCChallenge (Clark et al., 2018), WinoGrande (Sakaguchi et al., 2020), BBH (Suzgun et al., 2023), MMLU Hendrycks et al. (2021), MMLU-Pro (Wang et al., 2024), and GPQA (Rein et al., 2024). We perform two evaluation paradigms. We evaluate our model as general reasoner and also intrinsically as next-word predictor. For general reasoning capability evaluation, we use the benchmarks in their original multiple-choice question answering format. For intrinsic next-word prediction evaluation, we convert CSQA, PIQA, TruthfulQA, StrategyQA, and ARC-Challenge into multiple-choice next-word prediction format. Specifically, we prompt gpt-4.5-preview-2025-02-274 to transform each multiple-choice QA instance into context and multiple candidate next words. We make sure that each candidate next word is strictly single word to appear at the end to complete the context and its logical reasoning. Notice that the original context and candidate options are transformed at the same time to the new context and the candidate next words. We also prompt GPT4.5 to make sure that the transformed next word selection problem must be at the same difficulty level as the original question and must evaluate the same knowledge and reasoning process. To ensure the quality of the transformed data, we further use GPT-4.5 to filter out transformed instances that dont meet our requirements. We provide two transformation examples from PIQA and CSQA in Tab. 1. Please refer to Fig. 4 and Fig. 5 in Appendix for concrete transformation and validation prompts."
        },
        {
            "title": "4.3.1 Training",
            "content": "We use Qwen2.5-7B-Instruct (Team, 2024) and Llama-3.1-8B-Instruct (Grattafiori et al., 2024) as policy models. The detailed prompt used by policy models to elicit the reasoning path for next word prediction is in Fig. 6 of Appendix A. We conducted our RL training on 4 NVIDIA H200 GPUs, leveraging the VeRL5 (Sheng et al., 2025) repository. We train one epoch with total batch 1https://github.com/TartuNLP/true-detective 2https://www.5minutemystery.com/ 3https://openai.com/index/gpt-4-1/ 4https://openai.com/index/ introducing-gpt-4-5/ 5https://github.com/volcengine/verl CSQA PIQA SQA TQA ARC-c WG BBH MMLU MMLU-p GPQA"
        },
        {
            "title": "LLaMA",
            "content": "Vanilla SLM No-Judge BOW Vanilla SLM No-Judge BOW 80.51 75.51 77.64 81.90 77.07 36.12 76.66 77.97 87.21 84.55 87.05 88.68 85.15 67.85 83.73 86. 64.21 62.99 60.53 63.19 69.33 53.78 66.46 67.28 64.65 53.98 61.08 66.83 60.47 9.55 53.12 60.22 92.06 88.05 89.76 92.32 89.25 45.82 84.47 89. 64.48 59.43 62.59 70.96 59.59 52.17 57.14 59.75 77.76 54.92 76.82 77.21 74.71 22.01 65.89 74.56 75.28 69.53 70.82 76.51 72.90 39.32 66.33 73. 58.53 48.11 46.17 57.51 50.50 12.80 38.56 52.71 31.92 29.24 31.25 33.26 32.14 24.55 22.32 32.14 Table 2: General reasoning capability evaluation of BOW and various baselines are shown in this table. Notice that the vanilla instruction models are here only for reference, instead of comparison. All scores are obtained through self-consistency. TQA stands for TruthfulQA, SQA stands for StrategyQA, ARC-c stands for ARC-Challenge, WG stands for WinoGrande, and MMLU-p stands for MMLU-Pro. CSQA PIQA SQA TQA ARC-c WG w Vanilla SLM No-Judge BOW Vanilla L SLM No-Judge BOW 84.26 79.73 84.00 88.40 84.74 40.66 82.30 87. 85.46 80.50 86.89 89.06 83.56 59.65 88.86 89.27 50.92 59.51 63.80 64.21 54.19 49.28 57.06 62.17 63.45 58.81 68.46 65.86 59.74 34.88 74.21 67. 80.52 75.68 83.24 80.02 73.16 37.54 78.00 79.11 67.70 55.49 74.92 66.81 56.94 50.28 70.14 64.71 Table 3: Intrinsic next word prediction evaluation of BOW and various baselines are shown in this table. All scores are obtained through self-consistency. size of 1024, mini-batch size of 256, and rollout size of 5. We use AdamW (Loshchilov and Hutter, 2019) optimizer with an initial learning rate of 1 106, (β1, β2) = (0.9, 0.999), and weight decay of 1 102. We turn off the KL loss used in standard GRPO. 4.3."
        },
        {
            "title": "Judge Model",
            "content": "We use Llama-3.1-8B-Instruct as the judge model for both policy model variants, given its unbiased and low-variance next word distribution for English. Please refer to Fig. 7 for the prompt that acquires the next word probability distribution from the judge model, conditioned on the reasoning path. 5.2.1 also provides detailed justification of choosing LLaMA as the judge, instead of Qwen. To calculate the reward in Eq. 2, we use temperature of 5 for smooth numerical distribution, and only when the first token of the gold next word is among the top 100 positions of the next-token distribution given by the judge model, we assign this reasoning path reward of the corresponding token probability value; otherwise, we assign it reward of 0. For the regularization term calculation in Eq. 3, we dont use the entire vocabulary of the judge model as w, but the top 100 tokens in Jϕ(w C) as w. We set α in Eq. 3 to 0.1."
        },
        {
            "title": "4.3.3 Baselines",
            "content": "We compare with two continual pretraining baselines to remove the impact caused by continual pretraining itself, and also record the performance of vanilla instruction models for reference. Selective Language Modeling We compare with the selective language modeling (SLM) pretraining paradigm Lin et al. (2024). Specifically, we apply the causal language modeling loss only on those tokens that have been used as supervision during our BOW to finetune the policy model. No-Judge We completely remove the judge mechanism by prompting the policy model to first reason on the context and then wrap the predicted next word in boxed{}. For reward calculation, we assign reward of 1 if the first token of the predicted word extracted from the box matches the first token of the ground truth next word; otherwise, 0. Compared to BOWs soft-reward design, this hard-reward design is inspired by the accuracy reward as in Guo et al. (2025). For fair comparison, we design the prompt, shown in Fig. 8, to be as similar as possible to the policy prompt of BOW to elicit the same type of reasoning. This baseline also somehow resembles Quiet-STaR (Zelikman et al., 2024), where model must generate \"thoughts\" between every token during pretraining Vanilla Instruction Model For reference purposes, we also record the performance of untrained policy models, which are Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct."
        },
        {
            "title": "4.3.4 Evaluation",
            "content": "As mentioned in 4.2, we perform two evaluation paradigms. For general reasoning capability evaluation, we perform zero-shot inference where models are prompted to think step by step and finally output the answer letter. We use Math-Verify 6 to extract the last letter in the prediction as the answer letter. The detailed zero-shot prompt is shown in Fig. 9. For intrinsic next-word prediction evaluation, we follow our BOW training pipeline by first using the trained policy model to generate reasoning path and then feeding the reasoning path to the judge model to calculate each candidate next words completion probability. We choose the one with the largest probability as the final answer. For the No-Judge baseline, since it directly generates the reasoning path along with predicted next word wrapped in boxed{}, we discard the text after the open bracket of boxed{} and calculate each candidate next words completion probability concatenated to this text prefix. For all the evaluation settings, we apply self-consistency (Wang et al., 2023) by sampling 10 times with the temperature of 0.8, and use majority vote to decide the final prediction. We use vLLM (Kwon et al., 2023) for higher efficiency during inference."
        },
        {
            "title": "5.1 Main Results",
            "content": "General Reasoning Capability We report the results of general reasoning capability evaluation in Tab. 2. We compare with continual pretraining with SLM and No-Judge baselines and use the vanilla instruction model for reference. For Qwen2.5-7B-Instruct, BOW consistently outperforms SLM and No-Judge baselines across all benchmarks with an average of 8% and 4% respectively, highlighting that continual pretraining with BOW brings the base instruction models better generalization capabilities as general reasoner, compared with baselines. At the same time, BOW outperforms the base instruction model on 7 out of 10 benchmarks, falling behind only 1% on the StrategyQA, BBH, and MMLU-Pro. These results show that continual pretraining with BOW does not harm the original reasoning and instructionfollowing capabilities of the vanilla model, but enhances the generalization and reasoning capability of the base instruction model. SLM and No-Judge baselines do not exhibit this property, but instead 6https://github.com/huggingface/Math-Verify harm the original reasoning capability of the base instruction model. For Llama-3.1-8B-Instruct, we observe the same trend. BOW outperforms continual pretraining with SLM and No-Judge baselines across all benchmarks with an average of 30% and 6%, showing better generalization capability as general reasoner. BOW also outperforms the vanilla instruction model on 6 out of 10 benchmarks. Among the other four benchmarks, BOW achieves nearly the same performance, falling behind only around 0.2% on TruthfulQA, ARC-Challenge, and BBH. These results again show that continual pretraining with BOW further enhances the generalization and reasoning capability of the base instruction model, while baselines bring negative impacts. Consistent with the trend observed for LLaMA-family models in Lin et al. (2024), we observe that selective language modeling pretraining brings fatal impact on the instruction-following capability of Llama-3.1-8B-Instruct, where models start to repeat themselves instead of performing proper zero-shot inference given the prompt. Overall, our results show that continual pretraining with BOW enhances the general reasoning capability of LLMs by shifting the training signal from direct next token prediction to explanationdriven learning. The consistent improvements across model families and benchmarks suggest that BOW is an effective and scalable continual pretraining methodology. Next-Word Prediction Capability We report the results of the intrinsic next-word prediction capability evaluation in Tab. 3. We compare with two continual pretraining baselines and also vanilla instruction models. We show that across 12 scenarios with six transformed benchmarks and two policy model variants, BOW outperforms vanilla instruction models in 10 of them with an everage of 5%, and outperforms SLM in all with an avergae of 19%. For the No-Judge baseline, despite BOW not achieving overwhelming superiority over it (outperforming in 7 out of 12 scenarios), we later show that, through human analysis in 5.3, BOW elicits policy models to generate nextword reasoning trajectories that comprehensively consider all factors affecting next-word prediction from contexts (83% vs. 25%). Moreover, with variant of BOW where the regularization term is removed, our method still outperforms No-Judge when producing human-aligned next-word descrip7 Given the context \"The marathon runner felt dizzy from dehydration, so at the aid station she grabbed bottle of,\" the next word set will likely contain nouns that represent beverages, as the runner is at an aid station and is likely to grab drink to rehydrate. CSQA PIQA SQA TQA ARC-c WG AVG. BOW w/o reg. BOW w/o reg. 88.40 85.84 87.91 84.74 89.06 85. 89.27 84.65 64.21 54.81 62.17 50.51 65.86 71.43 67.53 74.21 80.02 80. 79.11 76.89 66.81 72.14 64.71 68.59 75.73 74.98 75.12 73.27 next words can be described possible of set The as: - Water - Sports drink - Juice - Electrolyte solution the are found These at aid stations during marathons, designed to help runners rehydrate and replenish electrolytes lost through sweat. beverages common types most of Figure 2: An example reasoning trajectory for candidate next words. water sports electro juice drink energy LLaMA Rank 1 5 7 10 16 Prob. 0.7575 0.0228 0.0074 0. 0.0011 0.0007 Qwen Rank 1 Prob. 0.9995 0.0004 N/A N/A 0.0000 N/A N/A N/A N/A Table 4: Given the example reasoning path in Fig. 2, we show the top 20 tokens from the next word probability distribution given by Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct. Qwen is clearly more biased compared with LLaMA. tions (81% vs. 67%)."
        },
        {
            "title": "5.2 Ablation Study",
            "content": "5.2."
        },
        {
            "title": "Judge Justification",
            "content": "For BOW, we use LLaMA-3.1-8b-Instruct as the judge model for both Qwen and LLaMA variants of policy models. We show here LLaMA-3.1-8b-Instruct truthfully reflects the next token distribution given the reasoning trajectory for the next word, and also highlight that Qwen2.5-7b-Instruct is not an ideal choice. For example, given the reasoning trajectory for candidate next words as shown in Fig., we show the next token probability distribution given by the two judges in Tab. 4. Words that are described as highly possible, such as \"water\", \"sports\", \"electro\", \"juice\", \"drink\", and \"energy\", faithfully appear in the top 20 of LLaMA-3.1-8b-Instructs next token distribution. However, for Qwen2.5-7b-Instruct, \"electro\", \"drink\", and \"energy\" do not appear in the top 20 of its token distribution. Also, the probability distribution of Qwen2.5-7b-Instruct is extremely biased towards the word \"water\", based on its internal prior knowledge, ignoring other possible next words mentioned in the 8 Table 5: Ablation study for the effectiveness of reference distribution regularization term during reward calculation. stands for Qwen2.5-7b-Instruct and stands for LLaMA-3.1-8b-Instruct. General Reasoner Next-Word Predictor"
        },
        {
            "title": "BOW",
            "content": "w/ random"
        },
        {
            "title": "BOW",
            "content": "w/ random 70.84 70.58 67.41 66.33 75.73 75.53 75.12 75.12 Table 6: Ablation study for the effectiveness of training data filtering. reasoning path. As result, evidences indicate that Qwen2.5-7b-Instruct is less biased next word probability estimator compared with LLaMA-3.1-8b-Instruct."
        },
        {
            "title": "5.2.2 Reference Distribution Regularization",
            "content": "To show the effectiveness of the reference distribution regularization term in Eq. 3, we remove the regularization term and keep other settings the same as BOW. We first report next-word prediction capability evaluation in Tab. 5. We can see that for both Qwen2.5-7b-Instruct and LLaMA-3.1-8b-Instruct, BOW achieves higher average performances compared with variants without the regularization term. These demonstrate the positive effect brought by the regularization term on intrinsic next-word prediction evaluation. With other discussions regarding the regularization term, we will also show in Sec. 5.3 that, through qualitative human study, this regularization term leads to next-word reasoning path that explicitly avoids collapsing into only mentioning few specific next words, but focuses on comprehensively thinking and describing next words, aligned with our intuition of this regularization."
        },
        {
            "title": "5.2.3 Training Data Filtering",
            "content": "To show the effectiveness of training data filtering by retaining only those reasoning-centric next words, we compare BOW with random filter baseline, where we randomly select the same number of words as used in BOW for RL training and keep other settings the same. As shown in Tab. 6, for both Qwen2.5-7b-Instruct and LLaMA-3.1-8b-Instruct, and both capabilities"
        },
        {
            "title": "Vanilla",
            "content": "No-Judge The thunderstorm was getting closer, so rolled up the Given the context \"The thunderstorm was getting closer, so rolled up the,\" the next word set will likely contain nouns that refer to parts of window or curtains, as the action of rolling up suggests something that can be rolled. The most probable nouns in this context are \"window\" and \"curtains,\" as these are the most common items that one would roll up to protect oneself from thunderstorm. Therefore, the concise and unbiased set of possible next words can be described as \"window\" or \"curtains.\" boxed{window} Reasoning: To predict the most likely next word, we need to consider the context provided: \"The thunderstorm was getting closer, so rolled up the\". Given that thunderstorm is approaching, the action of rolling something up is likely in response to this weather event. The most probable object to be rolled up in this context would be window or door, as these are common actions taken to protect oneself from storm. Thus, the most likely next word set would be: \"window\" or \"door\". Given the context \"The thunderstorm was getting closer, so rolled up the [next-word]\", the next word in the sentence could be \"window,\" \"car window,\" \"shade,\" \"curtain\", or \"blanket,\" among others. The most common and logical next word in this context would be \"window\" or \"car window,\" as it continues the phrase \"rolled up the\" in way that makes sense in the context of thunderstorm. The phrase \"rolled up the window\" is common expression used when someone closes car window by rolling it up. Other possibilities like \"shade\" or \"curtain\" are also plausible in certain contexts, such as rolling up window shade or curtain, but \"window\" is the most direct and commonly used option. BOW w/o. Reg. language, In concise and unbiased natural the possible next words for \"The thunderstorm was getting closer, so rolled up the [next-word]\" can be \"window,\" \"car window,\" \"shade,\" \"curtain\", or similar items that can be rolled up, such as blanket or curtain. However, \"window\" is the most likely and commonly used next word in this context. Given the context \"The thunderstorm was getting closer, so rolled up the,\" the next word set can be reasoned as follows: **Continuation of the sentence**: The sentence is describing an action related to thunderstorm, so 1. the next words are likely to continue the narrative or provide more details about the situation. 2. **Context clues**: The phrase \"The thunderstorm was getting closer, so rolled up the\" suggests that the speaker is taking some action in response to the approaching thunderstorm. The word \"rolled up\" is incomplete and could be referring to something that can be rolled up, such as window, curtain, or tent. 3. **Topic relevance**: Since the context mentions thunderstorm, the next words are likely to be related to weather-related actions or objects, or they might provide more details about the setting or the speakers actions."
        },
        {
            "title": "BOW",
            "content": "Based on these observations, the next word set could likely contain: - Nouns or noun phrases related to weather or outdoor activities (e.g., window, curtain, tent, car window) - Nouns or noun phrases related to the action of rolling up (e.g., window, curtain, tent) - Nouns or noun phrases related to the setting or the speakers environment (e.g., car, door, side) - Phrases or sentences that continue the narrative or provide more details about the thunderstorm or the speakers actions Therefore, concise and unbiased description of the next word set could be: \"The next word set will likely contain nouns or noun phrases related to weather, outdoor activities, the action of rolling up, the setting or the speakers environment, or phrases that continue the narrative or provide more details about the thunderstorm or the speakers actions.\" This description captures the likely content of the next word set without missing any possibilities, adhering to the given rules. Table 7: In this table, we show one concrete qualitative analysis example. Dimension 1 Dimension"
        },
        {
            "title": "5.3 Qualitative Human Study",
            "content": "Vanilla No-Judge BOW w/o. Reg. BOW 48% 25% 52% 83% 69% 67% 81% 41% Table 8: Human selection rate for four Qwen-based model variants in our study across 150 contexts. we measure in our work, BOW leads to slightly better performance in 3 out of 4 scenarios, with the other one tie. This demonstrates the effectiveness of our training data filtering. To better understand the next-word reasoning trajectories learned by BOW, we conduct human analysis comparing four different Qwen-based models, namely, vanilla instruction model, No-Judge baseline, BOW without regularization (w/o. Reg.), and BOW. We prompt GPT-4.5 to curate total of 150 contexts that satisfy two properties: (1) there are multiple plausible immediate next words given some statistical likelihood or grammar and syntax patterns, and (2) later, with more in-depth reasoning based on context, it becomes clear that only one or few candidates are plausible. We also make sure that the part of speech for sampled contexts next words varies, including nouns, verbs, and adjectives. four with large margin, we have demonstrated the effectiveness of BOW under both human evaluation dimensions. We perform human judgment on the next-word reasoning trajectories for the four models along two evaluation dimensions: (1) regardless of the next word being predicted, which trajectory demonstrates the most in-depth reasoning by thoroughly considering every detail and possible logic clue included or implied by the given context that could affect next word prediction, and (2) which trajectory contains the next word mentioning or description to the best of annotators expectation based on their commonsense knowledge. The first dimension evaluates the reasoning process toward NWP, and the second dimension evaluates how the next-word prediction outcome aligns with humans. For each instance, annotators view anonymized and shuffled reasoning trajectories from the four models we study and are required to pick the best one along the two evaluation dimensions. We encourage ties when no clear winner is apparent. We report the selection rate of each model in Tab. 8. For the first dimension, BOW outperforms all other models with large margin, demonstrating the effectiveness of our prompting design and reward design. BOW successfully optimizes the vanilla instruction model towards eliciting nextword reasoning paths that comprehensively consider all clues in the context that could affect nextword prediction. Especially when comparing BOW with BOW w/o. Reg. (83% vs. 52%), it further demonstrates the effectiveness of our reward regularization term qualitatively (we have proved its effectiveness quantitatively in 5.2.2). For the second dimension, we observe that BOW actually is the least selected one. After we carefully examine the reasoning trajectory of BOW, we realize this is explainable. Although the regularization term makes the next-word reasoning paths more comprehensive, covering various aspects that could potentially affect next-word prediction, it at the same time forces the model to avoid collapsing into only predicting few specific words too hard. It retains from next-word prediction in human-aligned fashion, but only provides next-word descriptions in an over-general way. We conclude that there is clear trade-off between explicitly predicting few next words with the risk of collapsing and being comprehensive to describe next words when using the regularization term. Finally, since BOW w/o. Reg. achieves the best selection rate among the We showcase concrete example context to directly compare the reasoning trajectory generated by the four models in Tab. 7. For the context, \"The thunderstorm was getting closer, so rolled up the\", vanilla model and No-Judge show the least comprehensive reasoning process towards next-word prediction based on the given context, and also with the trend of collapsing into explicitly predicting small set of next words, ignoring other highly possible candidates, such as \"shade\", \"tent\", or \"blanket\". On the contrary, BOW and its variant provide more general descriptions of next-word candidates with richer reasoning paths, thus covering more plausible next words. However, we observe the tradeoff mentioned above where the regularization term lead to reasoning path or next-words description being too general, so that it is less human-aligned during reasoning. Overall, we can conclude that BOW effectively optimizes the policy model to elicit better trajectories for comprehensive nextword reasoning and next-word description."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce bottlenecked next word exploration, novel RL training framework that redefines nextword prediction by inserting an intermediate reasoning bottleneck between input and supervision. Instead of learning to predict tokens directly, the model must first generate an intermediate reasoning path, which is then evaluated by frozen judge model that recovers the next-word distribution. This encourages more interpretable and robust reasoning. BOW achieves strong gains across diverse reasoning benchmarkseven without ever seeing the gold next word during trainingdemonstrating its effectiveness as scalable alternative to conventional supervision. Our results suggest that aligning next-word prediction with explanation-based reasoning not only improves model generalization but also enhances interpretability and human alignment in generated outputs."
        },
        {
            "title": "Limitations",
            "content": "While BOW presents promising new paradigm, several limitations remain. First, the framework relies on high-quality judge models to provide informative reward signals; weaker judges can introduce noise or bias, as seen in our comparisons between Qwen and LLaMA. Second, BOW de10 pends on well-structured reasoning prompts and benefits from task-specific tuningscalability to broader domains or arbitrary text remains an open question. Lastly, our evaluations focus on nextword prediction tasks reformulated from benchmark datasets, and further work is needed to extend BOW to fully generative settings and more diverse reasoning styles. References Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Yonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):74327439. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Maksym Del and Mark Fishel. 2023. True detective: deep abductive reasoning benchmark undoable for GPT-3 and challenging for GPT-4. In Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 314322, Toronto, Canada. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. 2025. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346 361. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In The Ninth International Conference on Learning Representations. brian ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, and 26 others. 2023. Do as can, not as say: Grounding language in robotic affordances. In Proceedings of The 6th Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pages 287318. PMLR. Yoichi Ishibashi, Taro Yano, and Masafumi Oyamada. 2025. Mining hidden thoughts from texts: Evaluating continual pretraining with synthetic data for llm reasoning. arXiv preprint arXiv:2505.10182. Dongwei Jiang, Guoxuan Wang, Yining Lu, Andrew Wang, Jingyu Zhang, Chuyu Liu, Benjamin Van Durme, and Daniel Khashabi. 2024. Rationalyst: Pre-training process-supervision for improving reasoning. arXiv preprint arXiv:2410.01044. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In SOSP, pages 611626. Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, and Muhao Chen. 2024a. Deceptive semantic shortcuts on reasoning chains: How far can models go without hallucination? In Proceedings of the 2024 Conference of the North American Chapter of the 11 Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 76757688, Mexico City, Mexico. Association for Computational Linguistics. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, and 1 others. 2024b. From generation to judgment: Opportunities and challenges of llm-as-ajudge. arXiv preprint arXiv:2411.16594. Yinghui Li, Jiayi Kuang, Haojing Huang, Zhikun Xu, Xinnian Liang, Yi Yu, Wenlian Lu, Yangning Li, Xiaoyu Tan, Chao Qu, and 1 others. 2025. One example shown, many concepts known! counterexampledriven conceptual reasoning in mathematical llms. arXiv preprint arXiv:2502.10454. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland. Association for Computational Linguistics. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen. 2024. Not all tokens are what you need for pretraining. In NeurIPS. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In The Seventh International Conference on Learning Representations. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Winogrande: An adversarial winograd schema challenge at scale. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):87328740. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasonarXiv preprint ing in open language models. arXiv:2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. Hybridflow: flexible and efficient rlhf framework. In EuroSys, pages 1279 1297. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. 2023. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051, Toronto, Canada. Association for Computational Linguistics. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota. Association for Computational Linguistics. Qwen Team. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Advances in Neural Information Processing Systems, volume 37, pages 9526695290. Curran Associates, Inc. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In The Tenth International Conference on Learning Representations. Ronald Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8:229256. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Zhikun Xu, Ming Shen, Jacob Dineen, Zhaonan Li, Xiao Ye, Shijie Lu, Aswin Rrv, Chitta Baral, and Ben Zhou. 2025. ToW: Thoughts of words improve reasoning in large language models. In Proceedings of 12 the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 30573075, Albuquerque, New Mexico. Association for Computational Linguistics. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Xiaodong Yu, Ben Zhou, Hao Cheng, and Dan Roth. 2024. Reasonagain: Using extractable symbolic programs to evaluate mathematical reasoning. arXiv preprint arXiv:2410.19056. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. 2024. Quiet-star: Language models can teach themselves In First Conference on to think before speaking. Language Modeling. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems, volume 35, pages 1547615488. Curran Associates, Inc. Ben Zhou, Sarthak Jain, Yi Zhang, Qiang Ning, Shuai Wang, Yassine Benajiba, and Dan Roth. 2025. Selfsupervised analogical learning using language models. arXiv preprint arXiv:2502.00996. Ben Zhou, Hongming Zhang, Sihao Chen, Dian Yu, Hongwei Wang, Baolin Peng, Dan Roth, and Dong Yu. 2024. Conceptual and unbiased reasoning in language models. arXiv preprint arXiv:2404.00205."
        },
        {
            "title": "A Prompting Details",
            "content": "We show all concrete prompt designs in our experiment in this section."
        },
        {
            "title": "Prompt for training data filtering",
            "content": "Given context and its completion, you need to decide if the contexts immediate next word, which is the first word in the completion, requires non-trivial reasoning to derive. Below are two examples: <Context> During her last visit, my mom gave me pair of socks and swimsuit. ......(words omitted here). This winter, to stay warm, put on pair of </Context> <Completion> socks that my mom gave me last time. </Completion> In this example, the immediate next word to the context is socks, and it requires non-trivial reasoning to derive. It is common sense to know that when people feel cold, they will put on warm clothes to keep warm. Some possible next words to the context could be jeans, socks, or gloves, etc. However, from the context, we know that mom gave me pair of socks last time, so it is more likely for me to put on pair of socks instead of gloves or jeans to keep warm, since it is known that have this pair of socks. <Context> During her last visit, my mom gave me pair </Context> <Completion> of socks and swimsuit. </Completion> In this example, the immediate next word to the context is of, and it does not require reasoning to derive. Given pair, English grammar already tells us the next word is highly likely to be of to complete the phrase pair of. two above examples examples simplified The to are illustrate your goal; the context you are about to see is longer and more complex. Now, am providing you with context and its completion. Please decide if the immediate next word to the context, which is the first word in the completion, requires non-trivial or complex reasoning to derive: <Context> {context} </Context> <Completion> {completion} </Completion> Note that you must respond in the format of JSON object with two keys. The first key is named requires_reasoning with Boolean type to decide if reasoning is required. True means required, and false means not required. The second key is called explanation with String type, and it should record your explanation of why or why not non-trivial reasoning is needed. Figure 3: Prompt for data filtering to select words that require non-trivial reasoning to derive for training."
        },
        {
            "title": "Prompt for evaluation data transformation",
            "content": "# Task: Given question, its candidate answers, and the correct answer, your task is to transform them into final word prediction problem. Concatenating # Transformation rules: 1. The transformation output should contain transformed context, set of candidate final words (must be WORD instead of phrase), and the gold final word, which is much more likely to conclude the transformed context than other candidate final words. 2. the transformed context must be able to form COMPLETE sentence. This complete sentence must be NATURAL and FLUENT. 3. The transformed final word prediction problem must maintain the SAME CONTEXT from the original question, so that it can evaluate the SAME or roughly the same KNOWLEDGE and REASONING process from the original question at the SAME DIFFICULTY level. candidate final each word to # Output format requirements: You must output in JSON format with three keys. The first key is named \"transformed_context\"with the data type of string to store the transformed context. The second key is named \"final_word_candidates\"with the data type of list of strings to store the transformed candidate final words. The third key is named \"gold_final_word_index\"with the data type of integer to store the gold final word index in the \"final_word_candidates\"list."
        },
        {
            "title": "Prompt for evaluation data transformation validation",
            "content": "# Task: Last word prediction is the task of predicting the last word of given context. Given an instance of last word prediction, there is context, set of candidate last words for the context, and the gold last word. Your task is to verify the quality of this instance based on the following verification dimensions: You need to ensure that the gold last word is more # Verification rules: 1. plausible than other candidates without any ambiguity. 2. All the candidate last words must be able to be directly concatenated to the context to form semantically complete sentence. 3. The semantically complete sentence from #2 must be semantically fluent and as natural as possible, just like how we humans would talk in real life. Minor grammatical errors are acceptable. # Output format rules: You must output in JSON format with two keys. The first key is named \"is_valid\"with the data type of boolean to store whether the given instance is valid based on the verification rules. The second key is called \"rationale\"with the data type of string to store the rationale of why or why not the instance is valid. are you given Now, its candidate answers, and the gold answer. Please follow the above transformation rules, example transformations, and output format requirements to perform the transformation. question, following the <Original Question> {question} </Original Question> <Original Candidate Options> {options} </Original Candidate Options> <Original Ground Truth> {ground_truth} </Original Ground Truth> given Now, the above verification rules, the instance is not valid if single rule is violated. Please verify this instance: prediction instance last word and <Context> {context} </Context> <Candidate last words> {candidate_last_words} </Candidate last words> <Gold last word> {gold_last_word} </Gold last word> Figure 4: Prompt for transforming multiple-choice question answering into multiple-choice next word selection format. Figure 5: Prompt for validating transformed multiplechoice next word selection instances."
        },
        {
            "title": "Prompt for policy model",
            "content": "The next word prediction problem of language modeling generally involves first searching for comprehensive but small enough set of possible next words, and then estimating each possible words probability within this set. Given specific context, your job is to output reasoning process that generally follows this illustration. missing without Here is one important rule you MUST follow. Since different candidate sets are different in size, instead of enumerating all possible words, you must be smart and innovative enough to describe the word set in concise any language, unbiased and natural possibilities. This is particularly important when the smallest candidate set you can find is still not small or concrete enough. For example, given the context [next-word], the best you can do is to tell me that the next word set will likely contain verbs, and those verbs should be actions performed by humans rather than other verbs. You should not give concrete list of words since it is impossible to enumerate all the verbs humans can perform. in other contexts where information is rich, However, you should perform more in-depth reasoning. For example, given the context like fruit, so as lunch had two [next-word], you should reason that the next word set will likely contain nouns of fruit given the logical clue that like fruit and the quantitative adjective two in the context. Furthermore, you should reason that this set is more likely to contain larger fruits to fill me up for lunch instead of small fruit. In other scenarios, such as 1+1=[next-word], where the next word set can be small enough to be listed out, you should actually list out all possible words in the set you find. the given below context, Now, on possible next words following all the rules described above. Make your output self-contained without needing to reference the given context. Context: {context} reason please"
        },
        {
            "title": "Prompt for policy model",
            "content": "The next word prediction problem of language modeling generally involves first searching for comprehensive but small enough set of possible next words, and then estimating each possible words probability within this set to pick the most likely next word. Given specific context, your job is to output reasoning process that generally follows this illustration to predict the most likely next word. missing without Here is one important rule you MUST follow. Since different candidate sets are different in size, instead of enumerating all possible words, you must be smart and innovative enough to describe the word set in concise natural and any language, unbiased possibilities. This is particularly important when the smallest candidate set you can find is still not small or For example, given the context \"I like concrete enough. fruit, so as lunch had two [next-word]\", you should reason that the next word set will likely contain nouns of fruit given the logical clue that \"I like fruit\" and the quantitative adjective \"two\" in the context. Furthermore, you should reason that this set is more likely to contain larger fruits to fill me up for lunch instead of small fruit. Then you should choose the most likely next word based on the set description. the given below please context, Now, the rule described above to first reason and then wrap your predicted most likely next word within boxed{}. Make your output self-contained without needing to reference the given context. Context: {context} follow Figure 8: Prompt for No-Judge baseline rollout. Figure 6: Prompt for policy model rollout."
        },
        {
            "title": "Prompt for judge model",
            "content": "Task Instruction: Given thought for the next word of certain context, you need to predict this next word. Now please give me your prediction of the next word based on the following thought: Prompt for MCQA zero-shot inference Question: {question} Options: {options} You should ONLY choose the letters from the options and answer in the format of So the answer is X, where is the option letters A, B, C, etc. Please think step by step. {thought} Next Word: Figure 7: Prompt for judge model to predict next word. Figure 9: Zero-shot inference prompt for MCQA evaluation."
        }
    ],
    "affiliations": [
        "Arizona State University"
    ]
}