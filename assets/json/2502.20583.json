{
    "paper_title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation",
    "authors": [
        "Keisuke Kamahori",
        "Jungo Kasai",
        "Noriyuki Kojima",
        "Baris Kasikci"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper, rely on deep encoder-decoder architectures, and their encoders are a critical bottleneck for efficient deployment due to high computational intensity. We introduce LiteASR, a low-rank compression scheme for ASR encoders that significantly reduces inference costs while maintaining transcription accuracy. Our approach leverages the strong low-rank properties observed in intermediate activations: by applying principal component analysis (PCA) with a small calibration dataset, we approximate linear transformations with a chain of low-rank matrix multiplications, and further optimize self-attention to work in the reduced dimension. Evaluation results show that our method can compress Whisper large-v3's encoder size by over 50%, matching Whisper medium's size with better transcription accuracy, thereby establishing a new Pareto-optimal frontier of efficiency and performance. The code of LiteASR is available at https://github.com/efeslab/LiteASR."
        },
        {
            "title": "Start",
            "content": "LITEASR: Efficient Automatic Speech Recognition with Low-Rank Approximation Keisuke Kamahori1,2 Jungo Kasai2 Noriyuki Kojima2 Baris Kasikci1 1University of Washington 2Kotoba Technologies Inc. {kamahori,baris}@cs.washington.edu, {jkasai,nkojima}@kotoba.tech 5 2 0 2 7 ] . [ 1 3 8 5 0 2 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Modern automatic speech recognition (ASR) models, such as OpenAIs Whisper, rely on deep encoder-decoder architectures, and their encoders are critical bottleneck for efficient deployment due to high computational intensity. We introduce LITEASR, low-rank compression scheme for ASR encoders that significantly reduces inference costs while maintaining transcription accuracy. Our approach leverages the strong low-rank properties observed in intermediate activations: by applying principal component analysis (PCA) with small calibration dataset, we approximate linear transformations with chain of low-rank matrix multiplications, and further optimize self-attention to work in the reduced dimension. Evaluation results show that our method can compress Whisper large-v3s encoder size by over 50%, matching Whisper mediums size with better transcription accuracy, thereby establishing new Pareto-optimal frontier of efficiency and performance. The code of LITEASR is available at https://github.com/efeslab/LiteASR."
        },
        {
            "title": "Introduction",
            "content": "Automatic speech recognition (ASR) systems have made significant strides in recent years, achieving near-human transcription performance (Radford et al., 2023; Puvvada et al., 2024). Modern ASR models, such as OpenAIs Whisper family, typically adopt an encoder-decoder architecture (Radford et al., 2023). For instance, Whisper largev3 comprises 32 Transformer blocks in both its encoder and decoder, totaling approximately 1.6 billion parameters, and has set new standards in multilingual transcription accuracy. Despite these advances, deploying ASR systems in real-world applications poses substantial efficiency challenges. First, many applications, such as live transcription, voice assistants, and realtime translation, impose strict latency requirements (Macháˇcek et al., 2023; Bevilacqua et al., 2024; 1 Figure 1: The relationship between encoder size and accuracy, as measured by word error rate (WER), for models in the Whisper family. The stars denote variants compressed via our method, which achieves the Paretooptimal balance between accuracy and efficiency. Nguyen et al., 2020; Wang et al., 2022; Jeffries et al., 2024). Latency refers to the delay between the input of audio and the output of the transcribed text. In real-time applications, even few seconds of delay can significantly degrade user experience. Second, while the overall model size may be moderate compared to the latest large language models (LLMs), ASR encoders are computationally intensive due to the long input sequences they must process. For instance, the encoder Transformers in the Whisper series consistently process input sequences of length 1500. For real-time applications, this encoder must be processed frequently, making it significant computational bottleneck. These challenges are acute in both on-device and data center settings. In on-device scenarios (e.g., laptops or smartphones), limited hardware capabilities make it difficult to meet latency constraints. Even in data center environments, which serve multiple concurrent users, the high computational intensity of ASR encoders becomes critical bottleneck. Although batching can improve serving throughput for memory-bound workloads, such as ASR decoders, it provides limited benefits for compute-bound encoders (as discussed in 2). Moreover, recent works have shown that the decoder component of ASR models can be aggressively compressed. For example, OpenAIs Whisper large-v3-turbo successfully reduced the number of decoder layers from 32 down to 4 layers via distillation. Other variants, such as DistillWhisper and Kotoba-Whisper, have taken this even further, compressing the decoder to as few as 2 layers (Gandhi et al., 2023; Kotoba Technologies, 2024). However, the encoder part remains largely unexplored, making its optimization increasingly crucial for efficient ASR systems. In this work, we propose LITEASR, novel compression scheme that targets ASR encoders by exploiting the low-rank structure of hidden activations during inference. key insight driving our approach is that intermediate activations, both in self-attention and multi-layer perception (MLP) layers, consistently exhibit low-rank properties across wide variety of inputs. This phenomenon stems from ASR encoders use of Mel spectrograms, the 2D time-frequency audio representations. Real-world audio (e.g., human speech) exhibits strong correlations between frequency components (Huang et al., 2012; Zergat and Amrouche, 2013; Tian et al., 2024; Kacha et al., 2020), resulting in low-rank characteristics of the intermediate features. Our method first analyzes the low-rank properties of activations using small amount of calibration data. We then perform principal component analysis (PCA) (Wold et al., 1987) to extract the dominant components and approximate linear transformations with rank-k projections. This factorization allows each weight matrix to be expressed as the product of two lower-rank matrices, thereby reducing the total number of floating point operations (FLOPs) required for inference. We employ an adaptive mechanism based on the threshold to determine the optimal degree of low-rank approximation for each layer. To further capitalize on the optimization, we also modify the self-attention algorithm to operate in the reduced dimension. We implement specialized GPU kernel based on FlashAttention (Dao et al., 2022) to accelerate the computation of attention scores and outputs. Our evaluation shows that LITEASR achieves Pareto-optimal balance between speed and accuracy (see Figure 1). When applied to Whisper large-v3, LITEASR reduces the encoder size by approximately 40%, yielding an execution speedup of around 1.4x with negligible accuracy loss. In alternative configurations, we further reduce the model size to less than half, resulting in model comparable in size to Whisper medium, while delivering improved accuracy. We also demonstrate the applicability of the method across different languages and models (4). In summary, this paper makes the following contributions: 1. We introduce LITEASR, compression method for ASR encoders using low-rank approximation of activation values. This method approximates linear layers with chain of lowrank matrix multiplications and optimizes selfattention to operate in reduced dimension. 2. We present comprehensive evaluation demonstrating that our method achieves Pareto-optimal balance between accuracy and efficiency. The rest of this paper is organized as follows: 2 gives background on ASR efficiency, 3 presents our low-rank approximation framework, 4 details the experimental setup, results, and analysis, 5 reviews related work, and 6 concludes the paper."
        },
        {
            "title": "2.1 Automatic Speech Recognition (ASR)",
            "content": "ASR models convert spoken language into text by transforming raw audio into compact representation, such as Mel spectrogram, and processing it with neural networks. Modern systems often use encoder-decoder architectures, typically employing Transformers (Radford et al., 2023; Puvvada et al., 2024; Rekesh et al., 2023; Gulati et al., 2020). For instance, OpenAIs Whisper mainly uses Transformer blocks, each of which consists of self-attention and MLP layers with large number of linear transformations (query/key/value/out projections for self-attention and two larger linear transformations for MLP). notable recent trend in ASR models is the reduction in decoder size without compromising performance, as exemplified by models such as Whisper large-v3-turbo (Radford et al., 2023) and Distill-Whisper (Gandhi et al., 2023), which reduced the number of decoder layers from 32 to 4 and 2, respectively."
        },
        {
            "title": "2.2 Compute Requirements of ASR Models",
            "content": "Since the encoder often processes long sequences (e.g., fixed at 1500 for Whisper), it often emerges 2 Figure 2: Latency breakdown of encoder and decoder relative to end-to-end latency for Whisper large-v3 and Whisper large-v3-turbo models under varying batch sizes (1 and 8). as the primary runtime bottleneck. Figure 2 shows the latency breakdown between the encoder and decoder across three hardware setups (NVIDIA RTX 4090, NVIDIA RTX A6000, and Apple M1 Pro), two models (Whisper large-v3 and Whisper large-v3-turbo), and two batch sizes (1 and 8)1. Although the encoder only accounts for about 15% of the overall latency for single-batch Whisper large-v3 on GPUs, it represents more significant bottleneck in other scenarios. For the newer Whisper large-v3-turbo model, the latency contribution of the encoder increases significantly due to the reduced size of the decoder. Similarly, for on-device inference (e.g., M1 Pro), the encoders relative latency is higher due to the limited computational power of such devices compared to GPUs. In data center deployment scenarios where multiple requests are batched, the encoders latency impact is further exacerbated. For example, with batch size of 8 and using Whisper large-v3-turbo, the encoder can consume over 90% of the total latency. This disproportionate latency is primarily due to the encoders high computational intensity (Williams et al., 2009); batching is therefore ineffective at increasing throughput for encoders. In contrast, the decoder generates tokens one at time in an autoregressive manner and is memory-bound, bottlenecked by memory bandwidth rather than computational power, making batching an effective strategy to enhance throughput (Chen, 2023). Consequently, although batching can substantially improve serving throughput for the decoder, it offers limited benefits for the compute-bound encoder and the encoder becomes notable bottleneck for 1We use vLLM (Kwon et al., 2023) (ver. 0.7.0) and MLX (Hannun et al., 2023) (ver. 0.21.1) to transcribe sample audio clip from the ESB dataset (Gandhi et al., 2022). 3 Figure 3: simplified illustration of our proposal. We use low-rank decomposition of activation values (Y) to compress the weight (W). large batch sizes. These findings collectively highlight the encoder as critical bottleneck for efficient ASR deployment in both on-device and data center environments. This issue becomes more pronounced with recent trends toward smaller decoders. Therefore, there is strong demand for methods to reduce the computational requirements of the encoder."
        },
        {
            "title": "3 Methodology",
            "content": "Our method, LITEASR, compresses the ASR encoder by extracting the low-rank features from activations at different layers of the model. To do so, we first use calibration data to analyze activations and then convert the dense matrix multiplication within the model to the product of low-rank matrices (Figure 3 shows simplified overview of the method). We further modify the self-attention algorithm to work efficiently on reduced dimensions. In this section, we explain the methodologies in detail."
        },
        {
            "title": "Consider a linear layer defined by",
            "content": "Y = XW + b, (1) where the weight matrix RDinDout and the bias vector RDout are learnable model parameters. Here, the input activations RLDin produce the output activations RLDout during the forward pass. In this notation, Din and Dout denote the input and output dimensions of the layer, respectively, and is the sequence length2. 2For Whisper encoders, this is always 1500. To study the distribution of activations, we collect calibration data consisting of Ncalib inputs. For each linear layer, we record the corresponding output . The resulting dataset can be viewed as Ncalib samples, where each sample is Doutdimensional vector. For simplicity, we refer to this collection of samples as . Our goal is to approximate the observed activations by projecting them onto their principal components. First, let YM RDout denote the mean vector of the dataset . Following the standard PCA procedure, we perform singular value decomposition (SVD) on the mean-centered data: U, S, = SVD(Y YM). (2) Here, RDoutDout is the matrix of right singular vectors. By selecting the first columns of , denoted by Vk RDoutk, we capture the top-k principal components of the data. The original activations can then be approximated as: YM (Y YM) Vk . (3) This approximation retains the most significant features of while reducing its dimensionality."
        },
        {
            "title": "3.2 Compressing Model Layers",
            "content": "Using the PCA approximation from Equation 3, we can rewrite the original linear layer = XW + as combination of low-rank matrix multiplications. Substituting = XW + gives YM (XW + YM) Vk (XW + YM) Vk + YM. (4)"
        },
        {
            "title": "This expression can be reorganized as",
            "content": "Y X(W Vk)V + (cid:16) YM+(bYM) Vk (cid:17) . (5) In this factorization, the original layer is decomposed into: Two low-rank linear transformations, with weight matrices Vk RDink and RkDout, and constant bias term given by YM + (b YM) Vk . Since both weight matrices and bias can be precomputed using calibration data, this decomposition significantly reduces FLOPs when is much smaller than the original dimension. 3.2.1 How to Choose Choosing the appropriate value for involves trade-off between accuracy and efficiency. smaller leads to more aggressive approximation, which increases efficiency but may incur larger accuracy loss. Accuracy Constraint. To preserve accuracy, the top-k principal components must capture sufficient portion of total variance. Let RDout denote the singular values from the SVD of the mean-centered activations (assumed to be sorted in decreasing order). We enforce Dout(cid:88) (cid:88) S2 > θ S2 , (6) i=1 i=1 where θ is threshold that controls the trade-off between accuracy and efficiency (i.e., the extent of data compression). Efficiency Constraint. The original linear layer requires O(LDinDout) FLOPs for its matrix multiplication. In contrast, the decomposed form in Equation 4 requires O(LDink + LkDout) FLOPs. To ensure that our approximation results in reduction of computation, we require LDink + LkDout < LDinDout, which simplifies to k(Din + Dout) < DinDout. (7) (8) For example, in Whisper large-v3, the dimensions for self-attention layers are (Din, Dout) = (1280, 1280), and for MLP layers they are (1280, 5120) or (5120, 1280). This implies that the efficiency constraint requires < 640 for selfattention and < 1024 for MLP layers. Practical Considerations. To maximize the GPU efficiency, we further restrict to be multiple of 16. Therefore, we choose as the smallest multiple of 16 that satisfies both of Equation 6 and Equation 7. We empirically find that θ values between 0.99 and 0.999 achieve good balance between accuracy and efficiency. detailed sensitivity study on the choice of θ is provided in 4."
        },
        {
            "title": "3.2.2 Optimizing Self-Attention\nMoreover, there is a potential to optimize the self-\nattention layers further. Specifically, if the rank k is\nsmaller than the per-head dimension, we can com-\npute the attention score and the value projection in\nalternative ways to reduce the FLOPs requirement\nwhile preserving the mathematical operations.",
            "content": "4 Standard Self-Attention. For multi-head attention, let Dhead denote the dimension per head and the number of heads (i.e., the total model dimension is Dhead h). In the i-th head, given an input activation matrix RLDin, the self-attention mechanism first computes three linear projections: Qi = XW Q, Ki = XW K, Vi = XW , (9) RDinDhead are the correwhere sponding weight matrices. The standard attention output is then given by K, Q, Attention(Qi, Ki, Vi) = softmax (cid:18) QiK Dhead (cid:19) Vi, (10) with the softmax applied row-wise. Attention Score Computation. Using our lowrank approximation, we can factorize each projection as follows: (11) Q2 + bi Q, K2 + bi K, V2 + bi , Qi = (XWQ1)W Ki = (XWK1)W Vi = (XWV1)W where WQ1 RDinkQ, RkQDhead, and Q2 RDhead are parameters relevant for i-th head bi after low-rank approximation (with analogous definitions for and ). Here, kQ, kK, and kV are the respective rank sizes. For brevity, let = XWQ1 and = XWK1. Expanding the product QiK , we obtain: QiK = = (cid:16) (cid:16)"
        },
        {
            "title": "AW i",
            "content": "= AW (cid:17)(cid:16) (cid:17)(cid:16) Q2 + bi Q2 + bi Q2W K2 + bi Q2bi (cid:17)"
        },
        {
            "title": "BW i",
            "content": "K2 + bi K2 + bi + AW QW K2 + bi Qbi . (12) In this expansion, the term AW domiQ2 nates the computational cost, while the other three terms are bias contributions. K2 The standard approach (Equation 10) computes Qi and separately and then multiplies them, which requires approximately O(L2Dhead) FLOPs. In contrast, Equation 12 allows us to first compute the smaller matrix product and then mulQ2 tiply by and B, reducing the computational cost to O(L kQ kK + L2 min(kQ, kK)). This is benefi3. Thus, we adopt cial when min(kQ, kK) < Dhead Equation 12 if the rank is sufficiently small. K2 3We take the minimum of kQ and kK because we can choose the multiplication order to minimize computation. Value projection. After computing the attention score matrix Si = softmax (cid:18) QiK Dhead (cid:19) RLL, (13) the final output is obtained by multiplying Si with Vi: (cid:16) SiVi = Si (cid:17) (XWV1)W = Si(XWV1)W V2 + bi V2 + Sibi . one would first (14) Conventionally, compute (XWV1)W and then multiply by Si, which V2 would cost O(L2Dhead + kV Dhead) FLOPs. However, by computing Si(XWV1) first, the cost becomes O(L2kV + kV Dhead) FLOPs, making this approach more efficient when kV < Dhead. Moreover, since each row of Si sums to 1, the bias term simplifies4:"
        },
        {
            "title": "Sibi",
            "content": "V = bi . (15) Thus, the value projection can be rewritten as: (cid:16) SiVi = (cid:17) Si(XWV1)"
        },
        {
            "title": "W i",
            "content": "V2 + bi , (16) which is more efficient if kV < Dhead. Implementation. To efficiently execute the operations in Equations 12 and 16, we implement specialized kernel using Triton (Tillet et al., 2019). This kernel extends the original FlashAttention implementation (Dao et al., 2022) to handle our optimized computation strategy. In this section, we describe our experimental setup and results, focusing on both the accuracy and efficiency of LITEASR."
        },
        {
            "title": "4.1 Setup",
            "content": "Our primary accuracy evaluation focuses on compressing Whisper large-v3 and Whisper large-v3turbo, both of which have encoders of the same size. We use test data from the End-to-end Speech Benchmark (ESB) (Gandhi et al., 2022), comprehensive collection of English ASR benchmarking datasets, to assess the word error rate (WER) of both the compressed and original models. We randomly choose 1000 audio clips from each of the eight subsets of ESB: Voxpopuli, AMI, Earnings-22, GigaSpeech, LibriSpeech (test.clean 4Here, bi is broadcasted across each row of Si. (cid:17)"
        },
        {
            "title": "4 Experiments",
            "content": "Model Config. VP AMI E22 GS LS-C LS-O SG TED Avg. WER () Size () Large-v3 Turbo Original LITEASR (a) LITEASR (b) LITEASR (c) Original LITEASR (a) LITEASR (b) LITEASR (c) 8.8 8.7 8.4 8.7 9.5 9.0 8.9 10. 25.9 25.7 28.7 33.4 26.8 27.7 43.2 69.7 19.5 18.9 15.8 17.2 17.4 17.0 16.7 35.1 11.1 11.1 12.0 12.3 11.4 11.4 11.7 16. Medium Original 8.7 31.3 25.9 25. 2.4 2.5 2.7 2.8 2.6 2.8 3.1 4.2 3.9 5.5 5.0 6.1 7.4 5.5 6.2 7.8 13.7 8. 3.3 3.4 3.1 3.5 3.8 3.1 4.0 5.0 5.9 4.4 5.1 4.8 5.4 4.3 4.5 5.0 6.4 8. 10.1 10.1 10.2 11.3 10.1 10.2 12.6 20.1 14.8 635M (100.0%) 429M (67.6%) 377M (59.4%) 308M (48.5%) 635M (100.0%) 421M (66.2%) 374M (58.8%) 313M (49.3%) 306M (48.1%) Table 1: Accuracy measured by WER percentages on ESB benchmarks and encoder sizes across different configurations. Abbreviations: VP (Voxpopuli), AMI (AMI), E22 (Earnings-22), GS (GigaSpeech), LS-C (LibriSpeech test.clean), LS-O (LibriSpeech test.other), SG (SPGISpeech), TED (TED-LIUM). For encoder size, we show relative size against the original Whisper large-v3 inside parenthesis. and test.other), SPGISpeech, and TED-LIUM. For the calibration data, we randomly select 100 clips (non-overlapping with the test data), and the calibration process is completed within 10 minutes using single RTX 4090 GPU. We employ greedy sampling with temperature set to 0. We present three configurations of θ for different deployment requirements: (a) Quality-Focused: θ = 0.999 for all layers. (b) Balanced: θ = 0.99 for self-attention layers and θ = 0.999 for MLP layers. (c) Efficiency-Focused: θ = 0.99 for selfattention layers and θ = 0.995 for MLP layers. Later, we conduct sensitivity study for different values of θ, languages, and models. Regarding efficiency, we evaluated the encoder latency on NVIDIA RTX 4090, NVIDIA RTX A6000, and Apple M1 Pro. For GPUs, we modify OpenAIs Whisper implementation5 to use CUDA Graph with PyTorch (Ansel et al., 2024) (ver. 2.5.1), and we use Triton (Tillet et al., 2019) (ver. 3.2.0) for customized self-attention GPU kernel. On the Apple device, we use MLX (Hannun et al., 2023) (ver. 0.21.1). The presented latency data are averaged over 10 runs. Note that the encoder always takes fixed-length audio as input, so the computation requirement is exactly the same for different data."
        },
        {
            "title": "4.2 Accuracy Evaluation",
            "content": "Table 1 compares the WER and encoder size. LITEASR is evaluated on Whisper large-v3 and Whisper large-v3-turbo models, with Whisper medium as reference. The quality-focused config5https://github.com/openai/whisper Figure 4: Execution speed of the encoder in Whisper large-v3, compared as ratio to the original model. uration (a) cuts model size by over 30% with less than 0.1 percentage points degradation of WER for both Whisper large-v3 and Whisper large-v3turbo. For more efficiency-focused scenarios, configuration (b) reduces encoder size by over 40% with comparable WER for Whisper large-v3, and about 2.5 points degradation for Whisper largev3-turbo. Configuration (c) compresses Whisper large-v3 model to less than half, matching Whisper mediums size, with better WER by about 3.5 points. Overall, LITEASR significantly reduces the model size while largely maintaining accuracy."
        },
        {
            "title": "4.3 Efficiency Evaluation",
            "content": "Figure 4 presents the efficiency evaluation results, measuring the speedup of end-to-end latency of the encoder execution compared to the original model. LITEASR consistently achieves speed improvements across all three hardware setups, with average speedups of 1.29x for (a), 1.38x for (b), and 1.54x for (c). The best performance is observed with the RTX 4090 using (c), reaching 6 Figure 5: Our Triton kernels performance against PyTorch implementation. Figure 7: Sensitivity of WER and encoder size on the value of θ. Whisper large-v3 encoder. The data is presented for configuration (c). In general, the initial layers allow for more substantial compression, with some exceptions, such as the FC2 stage in layer 21. This tendency is most pronounced in FC2 layers, where the earlier layers exhibit compression ratio of less than 0.2, whereas the subsequent layers reach values larger than 0.8. Among the layers, the Q/K projection and FC1 layers display smaller compression ratio compared to other layers."
        },
        {
            "title": "4.4.3 Sensitivity to Languages\nTo further investigate how LITEASR generalizes\nto out-of-distribution data and its sensitivity to lan-\nguages, we extend our evaluation to non-English\nbenchmarks. We use MLS (Pratap et al., 2020)\nfor French and German, and the JSUT basic5000\n(Sonobe et al., 2017) for Japanese6. Here, we use",
            "content": "6For Japanese, we use the character error ratio (CER) instead of the WER since Japanese does not have explicit word Figure 6: Compression ratio for each linear layer of Whisper large-v3. Smaller values mean more aggressive compression peak speedup of 1.57x. Moreover, Figure 5 compares our Triton kernels performance with PyTorchs scaled dot product attention (SDPA) implementation in Whisper largev3s encoder self-attention layers. The RTX 4090 shows roughly 17% and 30% improvements over baseline, while the RTX A6000 exhibits gains of approximately 14% and 22% for matrix ranks 32 and 16, respectively (i.e., kQ, kK, kV in 3, assuming all share the same value)."
        },
        {
            "title": "4.4.1 Compression Ratio per Layer\nFigure 6 illustrates the compression ratio (i.e., de-\nfined as the quotient of k divided by the original\ndimension size) for each linear layer within the",
            "content": "7 Config WER () CER () Size () Original LITEASR (a) LITEASR (b) LITEASR (c) FR DE 7.2 7.4 6.8 9.1 13.2 8.7 7.7 10.1 JA 10.8 10.7 11.2 12.4 635M 429M 377M 308M Table 2: Sensitivity study on other languages. Abbreviations: FR (French), DE (German), JA (Japanese). Config WER () Size () Original LITEASR (a) LITEASR (b) LITEASR (c) 9.1 9.1 9.1 9.1 609M (100.0%) 593M (97.3%) 579M (95.0%) 545M (89.4%) Table 3: Accuracy and encoder size with Canary 1B model. the same English calibration data as in previous experiments to compress Whisper large-v3, and evaluate its accuracy on non-English audio. The results presented in Table 2 demonstrate LITEASRs robustness: for (a), there is almost no degradation in accuracy, and even for (c), the degradation is less than 2 percentage points in WER/CER. In some cases, such as with German, we even observe an improvement in accuracy."
        },
        {
            "title": "5.1 Efficient ASR Inference",
            "content": "Several prior works have aimed to enhance ASR model efficiency. FasterWhisper uses optimized boundaries. inference kernels (SYSTRAN, 2023), while WhisperX further improves it for long-form audio (Bain et al., 2023). Whisper.cpp is C/C++ implementation for portability on both the CPU and GPU (Gerganov, 2023). Whisper_streaming supports live transcription for streaming purposes (Macháˇcek et al., 2023). NVIDIAs NeMo is modular toolkit for deploying speech models (Harper et al.). However, they do not effectively reduce ASR encoder computational demands. Some works provide model weight quantization, but they are limited to weights (weight-only quantization) and do not accelerate the compute-bound encoder inference. Our approach can be integrated with these frameworks. Various studies, including Whisper large-v3turbo, Distill-Whisper, and Kotoba-Whisper use distillation techniques to shrink decoder size (Radford et al., 2023; Gandhi et al., 2023; Kotoba Technologies, 2024). Other approaches combine distillation with quantization or lightweight modular ASR fine-tuning for underrepresented languages (Shao et al., 2023; Ferraz et al., 2023). Our work complements these efforts by further reducing the encoders computational requirements."
        },
        {
            "title": "Approximation",
            "content": "The low-rank approximation has been used to compress machine learning models, such as for parameter-efficient fine-tuning (Hu et al., 2021) or the LLMs KV cache compression (Liu et al., 2024; Chang et al., 2024). Yu and Wu (2023) has suggested that activations in Transformer models exhibit low-rank and compressed models, mainly targeting vision models. However, their method is limited to linear layers, leaving self-attention layers unoptimized, and its applicability to speech models has not been studied."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced compression method for ASR encoders that leverages the inherent lowrank structure of activations in linear layers. By applying the PCA algorithm, this method approximates linear layers with chain of low-rank matrix multiplications and optimizes self-attention to operate in reduced dimension. Our comprehensive evaluation demonstrates that our method achieves Pareto-optimal balance between accuracy and efficiency, paving the way for more efficient ASR 8 deployments for both on-device and data center environments. Lequn Chen. 2023. Dissecting batching effects in gpt inference."
        },
        {
            "title": "7 Limitations",
            "content": "Our method focuses on compressing linear layers and self-attention mechanism, yielding substantial improvements for Whisper models. However, other architectures, such as the Conformer, include additional components such as convolution layers, which may provide further compression opportunities (see 4). Additionally, our evaluation is currently limited to standard benchmarks in English and few other major languages; evaluating performance on low-resource languages and domain-specific applications remains an important direction for future research. Finally, while our improvements do not introduce new risks per se, the enhanced efficiency could accelerate the broader adoption of ASR systems, which may amplify concerns related to privacy, surveillance, or inherent biases in large-scale deployments."
        },
        {
            "title": "8 Ethics Statement",
            "content": "All data and models used in this paper are publicly accessible and are distributed under Creative Commons, Apache-2.0, MIT, or other open-source licenses that permit research use."
        },
        {
            "title": "References",
            "content": "Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. 2024. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pages 929947. Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. 2023. Whisperx: Time-accurate speech transcription of long-form audio. arXiv preprint arXiv:2303.00747. Antonio Bevilacqua, Paolo Saviano, Alessandro Amirante, and Simon Pietro Romano. 2024. Whispy: Adapting stt whisper models to real-time environments. arXiv preprint arXiv:2405.03484. Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, ChongYan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, Mohamed Abdelfattah, and Palu: Compressing kvKai-Chiang Wu. 2024. arXiv preprint cache with low-rank projection. arXiv:2407.21118. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359. Thomas Palmeira Ferraz, Marcely Zanon Boito, Caroline Brun, and Vassilina Nikoulina. 2023. Distilwhisper: Efficient distillation of multi-task speech models via language-specific experts. arXiv preprint arXiv:2311.01070. Sanchit Gandhi, Patrick Von Platen, and Alexander Rush. 2022. Esb: benchmark for multi-domain arXiv preprint end-to-end speech recognition. arXiv:2210.13352. Sanchit Gandhi, Patrick von Platen, and Alexander Rush. 2023. Distil-whisper: Robust knowledge distillation via large-scale pseudo labelling. arXiv preprint arXiv:2311.00430. Georgi Gerganov. 2023. whisper.cpp: Port of openais whisper model in c/c++. https://github.com/ ggerganov/whisper.cpp. Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. 2020. Conformer: Convolution-augmented transarXiv preprint former for speech recognition. arXiv:2005.08100. Awni Hannun, Jagrit Digani, Angelos Katharopoulos, and Ronan Collobert. 2023. MLX: Efficient and flexible machine learning on apple silicon. Eric Harper, Somshubra Majumdar, Oleksii Kuchaiev, Li Jason, Yang Zhang, Evelina Bakhturina, Vahid Noroozi, Sandeep Subramanian, Koluguri Nithin, Huang Jocelyn, Fei Jia, Jagadeesh Balam, Xuesong Yang, Micha Livne, Yi Dong, Sean Naren, and Boris Ginsburg. NeMo: toolkit for Conversational AI and Large Language Models. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Po-Sen Huang, Scott Deeann Chen, Paris Smaragdis, and Mark Hasegawa-Johnson. 2012. Singing-voice separation from monaural recordings using robust principal component analysis. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5760. IEEE. Nat Jeffries, Evan King, Manjunath Kudlur, Guy Nicholson, James Wang, and Pete Warden. 2024. Moonshine: Speech recognition for live transcription and voice commands. arXiv preprint arXiv:2410.15608. 9 Ryosuke Sonobe, Shinnosuke Takamichi, and Hiroshi Saruwatari. 2017. free large-scale japanese speech corpus for end-to-end speech synthesis. arXiv preprint arXiv:1711.00354. Jsut corpus: SYSTRAN. 2023. transcription with ctranslate2. https://github.com/SYSTRAN/ faster-whisper. Faster whisper Yusheng Tian, Junbin Liu, and Tan Lee. 2024. Userdriven voice generation and editing through latent space navigation. arXiv e-prints, pages arXiv2408. Philippe Tillet, Hsiang-Tsung Kung, and David Cox. 2019. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 1019. Peidong Wang, Eric Sun, Jian Xue, Yu Wu, Long Zhou, Yashesh Gaur, Shujie Liu, and Jinyu Li. 2022. Lamassu: Streaming language-agnostic multilingual speech recognition and translation using neural transducers. arXiv preprint arXiv:2211.02809. Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an insightful visual performance model for multicore architectures. Communications of the ACM, 52(4):6576. Svante Wold, Kim Esbensen, and Paul Geladi. 1987. Principal component analysis. Chemometrics and intelligent laboratory systems, 2(1-3):3752. Hao Yu and Jianxin Wu. 2023. Compressing transformers: features are low-rank, but weights are not! In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1100711015. Kawthar Yasmine Zergat and Abderrahmane Amrouche. 2013. Robust support vector machines for speaker verification task. arXiv preprint arXiv:1306.2906. Abdellah Kacha, Francis Grenez, Juan Rafael OrozcoArroyave, and Jean Schoentgen. 2020. Principal component analysis of the spectrogram of the speech signal: Interpretation and application to dysarthric speech. Computer Speech & Language, 59:114122. Kotoba Technologies. 2024. Kotoba-whisper (v2.0). https://huggingface.co/kotoba-tech/ kotoba-whisper-v2.0. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Dominik Macháˇcek, Raj Dabre, and Ondˇrej Bojar. 2023. Turning whisper into real-time transcription system. arXiv preprint arXiv:2307.14743. Thai Son Nguyen, Jan Niehues, Eunah Cho, Thanh-Le Ha, Kevin Kilgour, Markus Muller, Matthias Sperber, Sebastian Stueker, and Alex Waibel. 2020. Low latency asr for simultaneous speech translation. arXiv preprint arXiv:2003.09891. Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. 2020. Mls: largescale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411. Krishna Puvvada, Piotr Zelasko, He Huang, Oleksii Hrinchuk, Nithin Rao Koluguri, Kunal Dhawan, Somshubra Majumdar, Elena Rastorgueva, Zhehuai Chen, Vitaly Lavrukhin, et al. 2024. Less is more: Accurate speech recognition & translation without web-scale data. arXiv preprint arXiv:2406.19674. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. Dima Rekesh, Nithin Rao Koluguri, Samuel Kriman, Somshubra Majumdar, Vahid Noroozi, He Huang, Oleksii Hrinchuk, Krishna Puvvada, Ankur Kumar, Jagadeesh Balam, et al. 2023. Fast conformer with linearly scalable attention for efficient speech recognition. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. IEEE. Hang Shao, Wei Wang, Bei Liu, Xun Gong, Haoyu Wang, and Yanmin Qian. 2023. Whisper-kdq: lightweight whisper via guided knowledge distillation and quantization for efficient asr. arXiv preprint arXiv:2305.10788."
        }
    ],
    "affiliations": [
        "Kotoba Technologies Inc.",
        "University of Washington"
    ]
}