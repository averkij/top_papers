{
    "paper_title": "Cautious Optimizers: Improving Training with One Line of Code",
    "authors": [
        "Kaizhao Liang",
        "Lizhang Chen",
        "Bo Liu",
        "Qiang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AdamW has been the default optimizer for transformer pretraining. For many years, our community searches for faster and more stable optimizers with only constraint positive outcomes. In this work, we propose a \\textbf{single-line modification in Pytorch} to any momentum-based optimizer, which we rename Cautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing speed-up on Llama and MAE pretraining up to $1.47\\times$. Code is available at https://github.com/kyleliang919/C-Optim"
        },
        {
            "title": "Start",
            "content": "Cautious Optimizers: Improving Training with One Line of Code Kaizhao Liang 1 Lizhang Chen 1 Bo Liu 1 Qiang Liu 1 4 2 0 2 5 2 ] . [ 1 5 8 0 6 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "AdamW has been the default optimizer for transformer pretraining. For many years, our community searches for faster and more stable optimizers with only constraint positive outcomes. In this work, we propose single-line modification in Pytorch to any momentum-based optimizer, which we rename Cautious Optimizer, e.g. CAdamW and C-Lion. Our theoretical result shows that this modification preserves Adams Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing speed-up on Llama and MAE pretraining up to 1.47. 1 1. Introduction Optimization is an important and constantly evolving field in modern machine learning. Undoubtedly, Adam (Kingma, 2014) and AdamW (Loshchilov, 2017) are the most consequential optimizers proposed almost decade ago. Since then, many efforts (Zhang et al., 2021; Loshchilov et al., 2017) have been made to discover better and faster optimizers beyond these two. However, until now, AdamW remains the dominant workhorse for applications, from pre-training Large Language Models (LLMs) (Touvron et al., 2023) to fine-tuning text to image diffusion (Rombach et al., 2022), with no real challenges to their ruling status. In the dawn of the era of LLMs, the arms race of model scaling intensifies (Achiam et al., 2023). faster optimizer means more training tokens can be consumed within the same amount of time. Ultimately, this leads to more capable models (Kaplan et al., 2020). Hence, the interest in searching for an optimizer beyond AdamW is re-kindled. Recent progress in new AdamW alternatives such as LION (Chen et al., 2024; 2023a), SHAMPOO (Gupta et al., 2018), and SOAP (Vyas et al., 2024), ADOPT (Taniguchi et al., 2024), and Schedule-Free (Defazio et al., 2024), all claim substantial improvement over AdamW. 1Code is available at https://github.com/kyleliang919/C-Optim Algorithm 1 Caution an Optimizer (OPT) in PyTorch # param p, update from OP T, grad = (u * > 0).to(g.dtype) p.add (u * m/(m.mean()+eps), alpha=-lr) Figure 1: Training Loss Curves on LLaMA 1B, using AdamW / Lion and their cautious variants (using Algorithm 1). The cautious variants achieve better convergence and are 1.47x and 1.28x more sample efficient for AdamW and Lion respectively. However, these methods are either significantly more expensive or require non-trivial efforts to obtain optimal results, especially hyperparameter tuning, which greatly limits their potential and wide adoption. In light of this dilemma, we propose cautious optimizers, an exceptionally simple performance booster of any momentum-based optimizer that only requires one line of modification (see Algorithm 1). The change is simple: do not update unless the proposed update direction and the current gradients are aligned. With this minor change, we obtain universal improvement over the base optimizer without modification of the original optimal hyperparameters. For example, the cautious versions of AdamW and Lion, denoted as C-AdamW, and C-Lion, respectively, gain 1.47X and 1.28X speedups on LIama 1B with virtually no overhead (see Figure 1). To give an overview of the idea, let us consider general optimizer for minimizing loss L(w): wt+1 wt ϵtut, where ut is the negative update direction of the parameter wt at iteration and ϵt > 0 the step size. In momentumbased optimizers, ut does not necessarily align with the gradient direction gt = L(wt), and hence may cause Cautious Optimizers temporary increase of the loss function and slow down the convergence. Cautious optimizers avoid this issue by adding simple mask function based on the sign consistency of ut and wt: wt+1 wt ϵtut ϕ(ut gt), where denotes an element-wise product, and ϕ is an element-wise map that reweights the update based on the product ut gt. We simply take as ϕ(x) = I(x > 0) so that the update is zeroed out for coordinates on which the sign of ut and gt are inconsistent. This modification ensures the new update to have nonnegative inner product with the gradient, and hence decreases the loss monotonically when the step size is sufficiently small. Specifically, Taylor approximation shows L(wt+1) L(wt) ϵt(ut gt)ϕ(ut gt) 0. Algorithm 2 Cautious AdamW (C-AdamW) Require: parameter w, step sizes {ϵt}, dampening factors β1, β2 [0, 1), ϵ > 0, weight decay γ 0 + 1 gt wLt(wt1) 1: Initialize = 0, m0 = v0 = 0 2: while wt not converged do 3: 4: 5: mt β1mt1 + (1 β1)gt 6: 7: 8: 9: 10: 11: 12: wt wt1 ϵtϕtut 13: wt wt ϵtγwt 14: end while vt β2vt1 + (1 β2)g2 ˆmt mt/(1 βt 1) ˆvt vt/(1 βt 2) ˆvt + ϵ) ut ˆmt/( ϕt I(ut gt 0) ϵt = ϵt ϕt0+ // Compute alignment mask // Scale lr, is dimension of ϕt // Add weight decay This ensures decrease of the loss, i.e., L(wt+1) L(wt), when the step size is sufficiently small. 2. Theory In addition, we show in theoretical analysis that the modified algorithm converges to local optima under mild conditions of the based optimizers. An interesting aspect of this is that the algorithm would not stuck at non-stationary points of the loss, even if ut is completely conflicting with gt. This is because ut will be updated to eventually have positive inner product with gt in valid momentum optimizers if it is stuck at non-stationary point. Theoretically, we show that the modified algorithm guarantees to converge to local optima for optimizers that admit Hamiltonian+Descent structure (Chen et al., 2023a; Liang et al., 2024), which broadly includes almost all existing popular algorithms, including Adam, Lion, and heavy ball, and Nesterov momentum. For these algorithms, we show that the cautious optimizer with ϕ satisfying xϕ(x) max(x, 0) retain the monotonic decreasing properties of the original Lyapunov (or Hamiltonian) functions of these algorithms, while in addition to also minimizing the loss function. To summarize our contributions, we shall follow: We propose Cautious Optimizer, simple performance boost of any momentum-based optimizers with one line of code. We show theoretically that cautious optimizers preserve the convergence guarantee of the base optimizer in addition to speeding up the decrease of the loss function. We show universal improvement by scaling LLaMA from 60M to 1B and pretraining MAE on ImageNet1K up to 1.47x faster. We start with introducing general Hamiltonian descent framework for the continuous-time forms of general momentum algorithms (Section 2.1). We then introduce the cautious optimizers in the continuous time form and discuss its theoretical properties (Section 2.2). Finally, we discuss in Section 2.3 theoretical properties of cautious optimizers in discrete time forms. 2.1. Hamiltonian+Descent In the continuous-time form, most momentum-based algorithms can be viewed as variant of the damped Hamiltonian system, which admits Lyapunov (or Hamiltonian) function and their convergence towards the stationary points of the loss function. The Lyapunov function is an augmented loss function H(w, s) on both the weights and the momentum states S, and should satisfy mins H(w, s) = L(w), so that minimizing L(w) is equivalent to minimizing H(w, s). This is achieved by is H(w, s) = L(w) + K(s), where K(.) is any lower-bounded function. Physically, we can think as the total energy of system parameterized by (w, s), and and as the potential energy and kinetic energy, respectively. The continuous-time form of common momentum-based algorithms can be unified into dt dt wt = K(st) Φt(L(wt)) st = L(wt) Ψt(K(st)), (1) 2 Cautious Optimizers Figure 2: Left: We compare gradient descent with momentum (GDM) against its cautious variant (C-GDM) (we also provide gradient descent (GD) result as baseline and use 10x larger step size for GD than GDM and C-GDM). Details are provided in Section 3.1. The first plot shows the optimization trajectories from the two optimizers, where both optimizers start from (1, 1) with zero-initialized momentum. C-GDM successfully lands at the optimum without overshooting. The second and third plots confirm that C-GDM always monotonically decreases both the objective and the Hamiltonian of the original GDM. Right: In this plot, we plot (wt) versus for C-GDM and GDM with different combinations (ϵ, β). Across all combinations, C-GDM outperforms GDM. where Φ(), Ψ() are two monotonic mappings satisfying with H(w, m, v) = L(w) + 1 2a + , m. x2 Φt := x, Φt(x) 0, x2 Ψt := x, Ψt(x) 0, for any x. With Φ(x) = Ψ(x) = 0, the system in (9) reduces to the standard Hamiltonian system that keeps H(wt, st) = const along the trajectory. When adding the descending components with Φ and Ψ, the system then keeps H(w, s) monotonically non-decreasing: dt where H(wt, st) = H(wt, st) 0, (2) H(wt, st) := L(wt)2 Φt + K(st)2 Ψt . On the other hand, L(w), which is the true objective, is not necessarily decreasing monotonically. There can be cases when increases temporarily, while the total energy = + remains decreasing. Specifically, we have dt L(wt) = L(wt, st), (3) We can show that Example 2.2. The Lion-K optimizer (Chen et al., 2023b;a) (without weight decay) can be written into dt H(wt, mt, vt) 0 when b/4. dt dt wt = K((1 b)mt bL(wt)), mt = a(L(wt) + mt) where 0, [0, 1] and K(x) is any convex function that attains the minimum at = 0. One of its Hamiltonians that yields the Hamiltonian+descent structure (Eq (13) in Chen et al. (Chen et al., 2023a)) is H(w, m) = aL(w) + 1 1 K((1 b)m). See (Chen et al., 2023a) for other Hamiltonian functions. Lion-K includes large family algorithms as special cases, including Polyka momentum, Nesterov momentum, signed momentum, mirror descent, Frank-Wolfe, etc. where 2.2. Cautious Dynamics L(wt, st) := L(wt)K(st) + L(wt)2 Φt . L(wt, st) is not necessarily non-negative due to the cross term. Example 2.1. Adam (Kingma, 2014) yields the following continuous-time form and Hamiltonian, dt wt = mt vt + , mt = a(L(wt) mt), Our idea is to change the dynamics to make it simultaneously decrease both H(w, s) and L(w). We do this with modified system: wt = ϕ(xt) K(st) Φt(L(wt)) xt = L(wt) K(st) dt dt st = L(wt) Ψt(K(st)), (4) vt = b(L(wt)2 vt) where denotes the element-wise product and ϕ is vector to vector mapping. Here we weigh each element of the 3 dt dt Cautious Optimizers update direction of Wt based on the product of L(w) and K(s). The following conditions on the choice of function ϕ ensure that the system simultaneously decreases both and simultaneously. Theorem 2.3. Following the dynamics in (10), we have H(wt, st) = (x (1 ϕ(xt)) Ht(wt, st), dt and dt L(wt) = ϕ(xt) L(wt)2 Φt = (x (1 ϕ(xt)) Lt(wt, st), Here, Ht(wt, st) and Lt(wt, st), as defined in (2) and (3), respectively, represent the decreasing rates of and in accordance with the original system (9). Hence: If x(1 ϕ(x)) 0 for x, then both and decreases faster than the original system: dt dt H(wt, st) Ht(wt, st) 0, L(wt) Lt(wt, st). If xϕ(x) 0 for x, then decreases monotonically, dt L(wt) 0. Proof. See Appendix. If ϕ is an element-wise mapping, then ϕ satisfies both conditions if xϕ(x) max(x, 0), R. (5) In this case, both and decrease monotonically following the cautious dynamics, with rate faster than the original systems. We recommend the default choice of ϕ(x) = I(x 0), which satisfies both conditions while being the closest to constant function. Corollary 2.4. Assume that the norm 2 Ψ is positive definite, Ψ(0) = 0, and that H(w, s) = L(w) + κ(s) is differentiable. Then, the bounded solutions of the original system (9) converge to stationary point of H(w, s). Similarly, the bounded solutions of (10) also converge to stationary point of H(w, s). Proof. See Appendix A. 4 2.3. Discrete-Time Analysis We provide analysis for the discrete time, showing that cautious optimizers can only be better than the original optimizers under mild conditions."
        },
        {
            "title": "We will consider a generic update of form",
            "content": "wk+1 = wk ϵkuk(wk, sk), sk+1 = sk + vk(wk, sk), (6) where uk, vk are vector fields that define the updates. and ϵk is the step size. and its cautious variant: uk = uk(wk, sk) wk+1 = wk ϵkuk ϕk sk+1 = sk + vk(wk, sk), (7) (8) where ϕk is mask vector determined by the algorithm. Our analysis will consider both the element-wise mask ϕk = I(uk gk > 0), and inner product mask, ϕk = I(u gk > 0), where gk = L(wk). The difference is that the inner product mask is scalar and masks the update vector as whole, while element-wise mask treat each element separately. Inner Product Mask for Convex Losses We start with the case of using inner product masks on convex loss functions. This case is interesting because the cautious optimizers is always no worse than the base optimizers, regardless of the step size choices. Theorem 2.5. Assume L() is convex, and ϕk = I(u L(wk) 0). Then, starting from (wk, sk) = (wk, sk), we have L(wk+1) L(wk+1). which holds for any step size ϵk 0. L(wk) 0, we have ϕk = 1, and wk+1 = Proof. If wk+1, and L(wk+1) = L(wk+1). If convexity of L: L(wk) < 0, we have wk+1 = wk, and by the L(wk+1) L(wk+1) = L(wk ϵuk) L(wk) ϵu L(wk) > 0. This proves the result. Corollary 2.6. Consider the elementary test function: L(w) = w2 2 . 1 2 where is non-zero vector. Assume uk and vk are element-wise mappings. We have L(wk+1) L(wk+1) given (wk, sk) = (wk, sk), with either the inner product mask ϕk = I(u L(wk) 0), or the element-wise mask ϕk = I(uk L(wk) 0). Cautious Optimizers Proof. The inner product case is implied by Theorem 2.3. For element-wise mask, since the loss is an element-wise sum, and the update functions are element-wise mappings, which can apply Theorem 2.3 on each element. 1 + κw Example: Local vs. Global Comparisons in Quadratic Toy Model Consider the objective function: L(w) = w2 2, where κ is the condition number. The optimal momentum convergence rate for this function is: κ+1 , as shown in (Goh, 2017). Setting κ = 4, we plot the convergence rate heatmap over (α, β)-space, where α and β are the learning rate and momentum parameters. κ1 The heat-map shows that the cautious method achieves lower optimal convergence rate than the momentum method, highlighted by the red dot in Figure 3. Figure 3: Convergence rate heatmaps for κ = 4. Left: Cautious method; Right: Momentum method. Red dots highlight the optimal (minimal) convergence rate for each method. General Cases We now consider element-wise masks with general loss functions, in which case we need to impose assumptions on step sizes. Remark Assume that the loss function L() is convex and element-wise separable, i.e., L(w) = (cid:80) Li(wi), where each Li is convex function. Additionally, let ϕk = I(uk L(wk) 0), where I() denotes the elementwise indicator function and represents the element-wise product. Under these conditions, starting from (wk, sk) = (wk, sk), for any step size ϵk 0, we have L(wk+1) L(wk+1). Theorem 2.7. Assuming L() is µ-smooth, and ϕk = I(L(wk)uk µϵk 2 uk2). starting from (wk, sk) = (wk, sk), we have L(wk+1) L(wk+1). 2x (1ϕ(xk)) Rk(2µuk+Rk) , where which holds for step size ϵk Rk = uk (1 M). 3. Experiments In this section, we evaluate the performance of cautious optimizers compared to their standard counterparts, highlighting the benefits introduced by the cautious masking mechanism. We begin with 2D toy experiment to provide visual demonstration of how cautious masking improves optimization. Subsequently, we extend the evaluation to large-scale pretraining tasks for both language and vision models, comparing the performance of standard optimizers and their cautious variants. 3.1. 2D Optimization Toy We consider 2D optimization problem, where the decision variable = (w1, w2) R2. The objective is (w) = 0.5(w1)2 + 0.1(w2)2. Apparently the optimum is at = (0, 0). We apply gradient descent (GD), gradient descent with momentum (GDM), and cautious gradient descent with momentum (C-GDM) on this toy example, starting from w0 = (1, 1). Specifically, for GDM, we adopt the conventional momentum update: st βst1 + (wt), wt wt1 ϵst, where β [0, 1) is the dampening factor, and ϵ is constant learning rate. When using β = 0.99 and ϵ = 0.01 for GDM and C-GDM, ϵ = 0.1 for GD, the results are visualized in Figure 2. Observation: From the left of Figure 2, one can see that GDM, due to the momentum, has fluctuating (wt), while C-GDM ensures that (wt) monotonically decreases. In addition, C-GDM achieves faster drop in terms of GDMs Hamiltonian. On the right of Figure 2, we ablate over different combinations of (β, ϵ) { (0.01, 0.5), (0.01, 0.9), (0.01, 0.99), (0.01, 0.999), (0.1, 0.99), (0.001, 0.99) }. Across all settings, C-GDM outperforms GDM, confirming the importance of cautious masking. Then, starting from (wk, sk) = (wk, sk), we have 3.2. Pretraining Large Language Models (LLMs) L(wk+1) L(wk+1). which holds for any step size ϵk 0. Theorem 2.8 (Larger Loss Decreasing). Assume loss function L() is differentiable and µ-smooth, element-wise operator ϕ satisfies (5) , and ϕk = ϕ(L(wk) uk). Then, We begin by investigating the language modeling task using the LLaMA (Touvron et al., 2023) model as the foundational architecture. Variants of LLaMA with parameter sizes ranging from 60M to 1B (specifically 60M, 100M, 350M, and 1B) are considered in this study. The models are trained on the C4 (Colossal Clean Crawled Corpus) dataset (Raffel 5 Cautious Optimizers # Params Perplexity () AdamW C-AdamW Lion C-Lion 60M 31.17 100M 26.96 350M 22.58 24.02 1B 30.78 26.82 22.14 22.00 55.17 41.69 29.10 22.00 40.04 33.21 22.84 21.17 Table 1: To demonstrate scaling law(Kaplan et al., 2020), eval perplexity for LLaMA models pretrained on C4 for 10K steps. Cautious Optimizer is better across all model sizes. Lower perplexity is better. Hyperparameters can be found in appendix. et al., 2020), large-scale web-crawled text corpus containing billions of tokens. The extensive scale of the C4 dataset, coupled with the complexity of the LLaMA architecture, results in prolonged training durations, often spanning weeks or months. For optimization, we employ AdamW (Loshchilov, 2017) and Lion (Chen et al., 2023c), two widely used optimizers in modern language modeling, as baselines. These are compared with their cautious counterparts, which we term Cautious AdamW (C-AdamW) and Cautious Lion (C-Lion). Figure 4 illustrates the training curves of LLaMA-1B for AdamW, C-AdamW, Lion, and C-Lion. Validation perplexities for all model sizes (60M, 100M, 350M, and 1B) across the four optimizers are summarized in Table 1. The complete hyperparameter configurations for language model training are detailed in Table 4 in the Appendix. Observation: As shown in Figure 4, C-AdamW and C-Lion demonstrate significant improvements in sample efficiency, achieving 1.47x and 1.28x gain over AdamW and Lion, respectively, on the LLaMA-1B model. Notably, these improvements require only single additional line of code and incur no extra computational overhead. On the other hand, we observe an interesting phenomenon that the gap between C-AdamW and AdamW increases as model size increases, but for Lion it is the reversed. Quantatively, Table 1 reveals that the cautious optimizers consistently outperform their standard counterparts across all model sizes. To further assess the quality of models trained with cautious optimizers, we conduct downstream evaluations on the GLUE benchmark (Wang, 2018), widely used suite for evaluating pretrained language models. GLUE comprises nine diverse natural language understanding tasks, including sentence similarity, text classification, entailment, and question answering, as well as diagnostic dataset for fine-grained linguistic analysis. We focus on six tasksMicrosoft Research Paraphrase Corpus (MRPC), Recognizing Textual Entailment (RTE), Stanford Sentiment Treebank (SST-2), Multi-Genre Natural Language Inference (MNLI), Question Natural Language Inference (QNLI), and Quora Question Pairs (QQP)which do not require additional fine-tuning for evaluation. Table 2 summarizes the downstream performance of models pretrained using AdamW and C-AdamW on these six tasks, alongside their average score, which serves as an overall performance metric. Observation: As shown in Table 2, models trained with C-AdamW not only achieve lower training and validation perplexities but also deliver notable 2% improvement in downstream scores, demonstrating the practical advantages of cautious optimization. 3.3. Pretraining Masked Autoencoders (MAEs) Masked Autoencoders (MAEs) (He et al., 2022) have emerged as powerful approach for pretraining Vision Transformers (ViTs)(Dosovitskiy, 2020) on large-scale datasets like ImageNet-1K (Russakovsky et al., 2015). This task involves reconstructing 75% of randomly masked image patches, challenging objective that requires extensive training over hundreds of epochs and millions of images. The primary goal is to learn robust visual representations that are generalizable across downstream vision tasks. The quality of these representations is typically measured by the final evaluation loss, which reflects how accurately the model reconstructs masked test images; lower evaluation loss indicates higher-quality representations. The results of our experiments are summarized in Figure 3, where we compare the performance of the Cautious Optimizer against the AdamW baseline. Observation: From Table 3, we observe that the Cautious Optimizer achieves consistently lower final evaluation loss compared to AdamW. This result highlights the effectiveness of the cautious approach in improving the precision of reconstruction and, consequently, the quality of the learned visual representations. 4. Related Work In this section, we provide brief overview of existing efforts on designing Adam-like optimizers, and the related works on Hamiltonian dynamics. Adam and Its Variants There exist long series of works on variants of Adam (Kingma, 2014; Loshchilov & Hutter, 2017). AdaFactor (Shazeer & Stern, 2018) factorizes the second order momentum. AdamW (Loshchilov & Hutter, 2017) proposed simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. NAdam (Dozat, 2016) proposes to combine Nesterov update with Adam. AdaBelief (Zhuang et al., 2020) changes the second momentum vt from the EMA of 6 Cautious Optimizers Figure 4: Training loss curves for AdamW, C-AdamW, Lion, C-Lion on LLaMA with 60M, 100M, 350M, and 1B parameters. Method MRPC RTE SST2 MNLI QNLI QQP AVG () AdamW 67.2 C-AdamW 68.4 52.0 46.6 49.1 63. 32.9 32.7 49.5 49.5 36.8 36.8 47.9 49.5 Table 2: Standardized GLUE evaluation for 1B model checkpoints using eval-harness. Results are reported for various downstream tasks. Method Final Eval Loss () AdamW C-AdamW 0.6085 0.5926 Table 3: Evaluation loss of pretrained MAEs on ImageNet1K on ViT backbone for 50 epochs, using AdamW and C-AdamW. to the EMA of (gt mt)2. Adan (Xie et al., 2024) introg2 duced an extra momentum term to improve training but with extra memory cost. More recently, ADOPT (Taniguchi et al., 2024) folds normalized updates into first order momentum updates. Our propose C-AdamW is single-line modification based on AdamW, which is the most commonly used optimizer in large-model training nowadays. Different from the above work that focuses on making specific change to the Adam optimizer, our proposed solution works in general for all momentum-based optimizers. Hamiltonian Dynamics Hamiltonian dynamics, also known as Hamiltonian mechanics or Hamiltonian formalism, is mathematical framework used to describe the mo7 tion of particles and systems in classical mechanics (Abraham & Marsden, 2008). In the field of sampling, Hamiltonian Monte Carlo and its variants can traverse the optimization landscape more efficiently by combining the principles of Hamiltonian dynamics with Markov Chain Monte Carlo (MCMC) methods (Neal et al., 2011; Betancourt & Girolami, 2015; Hoffman & Gelman, 2014) Analyzing momentum-based algorithms (Sutskever et al., 2013; Nesterov, 1983) poses unique challenge, given that the objective function doesnt exhibit the monotonic decrease found in Gradient Descent (GD)(Jin et al., 2018). To address this in the convex setting, researchers have introduced multiple Lyapunov functions (Krichene et al., 2015; Wilson et al., 2016). (Jin et al., 2018) introduced Hamiltonian to get convergence rate of stationary points.(Sutskever et al., 2013) led to physical interpretaion of momentum (Sutskever et al., 2013) and Nesterovs and Polyaks methods (Nesterov, 1983). Recently, the Hamiltonian dynamics have been adopted for proving convergence rate for the Lion optimizer (Chen et al., 2023a) and its distributed variant (Liu et al., 2024). Cautious Optimizers 5. Conclusion and Limitation In summary, we introduce Cautious Optimizer, straightforward enhancement for momentum-based optimizers that can be implemented with single line of code. Our theoretical analysis demonstrates that the Cautious Optimizer not only preserves the convergence guarantees of the base optimizer but also accelerates the reduction of the loss function. Empirically, it delivers universal performance improvements, as evidenced by scaling Llama models from 60M to 1B parameters and achieving up to 1.47 faster pretraining of MAE on ImageNet1K. For future research, we provide few promising directions: (1) Different ϕ functions (2) Applying masking in eigenspace instead of parameter space (3) More rigorous analysis beyond convex cases. We hope that our work provides strong foundation for exploring these directions. Limitations: Our evaluation of this method remains preliminary due to limited computational resources. Despite early promising results, it remains uncertain whether Cautious Optimizers will deliver the anticipated improvements in broader applications and large-scale experiments."
        },
        {
            "title": "References",
            "content": "Abraham, R. and Marsden, J. E. Foundations of mechanics. Number 364. American Mathematical Soc., 2008. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anandkumar, A. signsgd: Compressed optimisation for nonconvex problems. In International Conference on Machine Learning, pp. 560569. PMLR, 2018. Betancourt, M. and Girolami, M. Hamiltonian monte carlo for hierarchical models. Current trends in Bayesian methodology with applications, 79(30):24, 2015. Chen, L., Liu, B., Liang, K., and Liu, Q. Lion secretly solves constrained optimization: As lyapunov predicts. arXiv preprint arXiv:2310.05898, 2023a. Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Liu, Y., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., et al. Symbolic discovery of optimization algorithms. arXiv preprint arXiv:2302.06675, 2023b. Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Liu, Y., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., et al. Symbolic discovery of optimization algorithms. arXiv preprint arXiv:2302.06675, 2023c. Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., Lu, Y., et al. Symbolic discovery of optimization algorithms. Advances in neural information processing systems, 36, 2024. Defazio, A., Yang, X. A., Mehta, H., Mishchenko, K., Khaled, A., and Cutkosky, A. The road less scheduled. arXiv preprint arXiv:2405.15682, 2024. Dosovitskiy, A. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Dozat, T. 2016. Incorporating nesterov momentum into adam. Goh, G. Why momentum really works. Distill, 2017. doi: 10.23915/distill.00006. URL http://distill. pub/2017/momentum. Gupta, V., Koren, T., and Singer, Y. Shampoo: Preconditioned stochastic tensor optimization. In International Conference on Machine Learning, pp. 18421850. PMLR, 2018. He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Hoffman, M. D. and Gelman, A. The no-u-turn sampler: Adaptively setting path lengths in hamiltonian monte carlo. Journal of Machine Learning Research, 15(47):15931623, 2014. URL http://jmlr.org/ papers/v15/hoffman14a.html. Jin, C., Netrapalli, P., and Jordan, M. I. Accelerated gradient descent escapes saddle points faster than gradient descent. In Conference On Learning Theory, pp. 1042 1085. PMLR, 2018. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Kingma, D. P. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Krichene, W., Bayen, A., and Bartlett, P. L. Accelerated mirror descent in continuous and discrete time. Advances in neural information processing systems, 28, 2015. Liang, K., Liu, B., Chen, L., and Liu, Q. Memory-efficient llm training with online subspace descent. arXiv preprint arXiv:2408.12857, 2024. Cautious Optimizers Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vyas, N., Morwani, D., Zhao, R., Shapira, I., Brandfonbrener, D., Janson, L., and Kakade, S. Soap: Improving and stabilizing shampoo using adam. arXiv preprint arXiv:2409.11321, 2024. Wang, A. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. Wilson, A. C., Recht, B., and Jordan, M. I. lyapunov analysis of momentum methods in optimization. arXiv preprint arXiv:1611.02635, 2016. Xie, X., Zhou, P., Li, H., Lin, Z., and Yan, S. Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. Zhang, G., Kenta, N., and Kleijn, W. B. Extending adamw by leveraging its second moment and magnitude. arXiv preprint arXiv:2112.06125, 2021. Zhuang, J., Tang, T., Ding, Y., Tatikonda, S. C., Dvornek, N., Papademetris, X., and Duncan, J. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. Advances in neural information processing systems, 33: 1879518806, 2020. Liu, B., Wu, L., Chen, L., Liang, K., Zhu, J., Liang, C., Krishnamoorthi, R., and Liu, Q. Communication efficient distributed training with distributed lion. arXiv preprint arXiv:2404.00438, 2024. Loshchilov, I. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Loshchilov, I., Hutter, F., et al. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5, 2017. Neal, R. M. et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11):2, 2011. Nesterov, Y. E. method for solving the convex programming problem with convergence rate (1/κˆ 2). In Dokl. akad. nauk Sssr, volume 269, pp. 543547, 1983. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115: 211252, 2015. Shazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 45964604. PMLR, 2018. Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 11391147. PMLR, 2013. Taniguchi, S., Harada, K., Minegishi, G., Oshima, Y., Jeong, S. C., Nagahara, G., Iiyama, T., Suzuki, M., Iwasawa, Y., and Matsuo, Y. Adopt: Modified adam can converge with any beta 2 with the optimal rate. arXiv preprint arXiv:2411.02853, 2024. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., 9 A. Appendix A.1. Hamiltonian + Descent Cautious Optimizers Momentum-based algorithms can be typically viewed as monotonic descending algorithms on an augmented loss H(W, S), which satisfies minS H(W, S) = L(W ), so that minimizing L(W ) is equivalent to minimizing H(W, S). typical choice is H(w, s) = L(w) + K(s), where K(.) is any lower bounded function. We may refer H(w, s) as Hamiltonian function. Physically, one can show L(w) and K(s) the potential energy and kinetic energy, respectively. The continuous-time form of most momentum-based algorithms can be written into Hamiltonian descent form: dt dt Wt = K(S) Φt(L(Wt)) St = L(W ) Ψt(K(St)), (9) where H(W , S) is Hamiltonian (or Lyapunov) function that satisfies min H(W , S) = L(W ), , so that minimizing L(W ) reduces to minimizing H(W , S); and Φ(), Ψ() are two monotonic mappings satisfying Φ := X, Φ(X) 0, X2 Ψ := X, Ψ(X) 0, X. With Φ(X) = Ψ(X) = 0, the system in (9) reduces to the standard Hamiltonian system that keeps H(Wt, St) = const along the trajectory. When adding the descending components with Φ and Ψ, the system then keeps H(W , S) monotonically non-decreasing. It is easy to show that the dynamics monotonically decreases dt H(Wt, St) 0 since dt H(Wt, St) = L(W )T Wt + K(S)T St = L(W )T (K(S) Φt(L(Wt))) + K(S)T (L(W ) Ψt(K(St))) = L(W )T Φt(L(Wt)) K(S)T Ψt(K(St)) = X2 Φ X2 Ψ , so that minimizing L(W ) reduces to minimizing H(W, S). However, L(W ), which is the true objective, is not necessarily decreasing monotonically. Because = + K, this means that there are cases when increases while decreases. How to change the dynamics to make it simultaneously decreases both H(W, S) and L(W )? To do so, we modify introduce modification of the system: dt dt Wt = K(S) ϕ(L(W ) K(S)) Φt(L(Wt)) St = L(W ) Ψt(K(S)), (10) where denotes elementwise product. Above update (10) is exactly our C-optimizers continous form. Here we introduce weighting ϕ(L(W ) K(S)) on the update direction of Wt based on the product of L(W ) and K(S). The following conditions on the choice of function ϕ ensures that the system simultaneously decreases both and simultaneously. Theorem A.1. The following inequalities hold under specific conditions on and ϕ(x): If (1 ϕ(x)) 0, then dt H(Wt, St) 0. If ϕ(x) 0, then dt L(Wt) 0. 10 Remark The function ϕ(x) = I(x 0) is simple choice that satisfies both conditions, while being the closest to constant function. For simplicity of notation, we define the norm Xϕ as: Cautious Optimizers This notation will be used in subsequent derivations. ϕ = 1(X ϕ(X)), X. Remark The first condition also implies the rate of decrease in the loss for the masked dynamics (9) is at least as large as for the original dynamics (10) as shown in Theorem A.2 Proof. For simplicity, we write = L(W ) K(S). Following the dynamics in (10), let us see the derivation of w.r.t. t: H(Wt, St) dt = L(W )T Wt + K(S)T St = L(W )T (K(S) ϕ(D) Φt(L(Wt))) + K(S)T (L(W ) Ψt(K(St))) = L(W )T (K(S) (1 ϕ(D))) L(W )T Φt(L(Wt)) K(S)T Ψt(K(St)) Φ K(S)2 = 1T ((L(W ) K(S)) (1 ϕ(L(W ) K(S)))) L(W )2 = 1T ([D (1 ϕ(D))]) L(W )2 Φ K(S)2 Ψ Ψ (11) Given the fact that ϕ is an element-wise operator, it is noteworthy that if (1 ϕ(x)) 0, then the first term in (11) 1T ((L(W ) K(S)) (1 ϕ(L(W ) K(S)))) 0 since we see L(W ) K(S) as x. Next, let us look into the derivative of L(Wt) w.r.t. t: dt L(Wt) = L(W )T (K(S) ϕ(L(W ) K(S)) Φt(L(Wt))) = L(W )T (K(S) ϕ(L(W ) K(S))) L(W )T Φt(L(Wt)) = 1T (L(W ) K(S) ϕ(L(W ) K(S))) L(W )T Φt(L(Wt)). (12) It is clear that if the first term in (12), namely (L(W ) K(S) ϕ (L(W ) K(S))) 0, then dt L(Wt) 0. This condition is satisfied when ϕ(x) 0. Theorem A.2. Consider the two dynamical systems defined by (9) and (10). If the conditions in Theorem A.1 hold, then the rate of decrease in the loss for the masked dynamics (9) is at least as large as for the original dynamics (10). Specifically, we have: L(Wt)Ut L(Wt)Vt, (13) where Ut = K(St) ϕ (L(Wt) K(St)) Φt (L(Wt)) , Vt = K(St) Φt (L(Wt)) . Remark The momentum methods montonically decreases H(W, S), but the loss can increase temporarily (although eventually go down as is part of H). So optimizer suppresses any temporarily increase of L, without hurting the montoniconc increase of H. 11 Theorem A.3. Let (Wt, St) and (W t, St) denote the solutions to the initial value problems (IVPs) associated with the dynamical systems (10) and (9), respectively, with shared initial conditions W0 = 0 and S0 = S0, and Φ = 0. Cautious Optimizers Then, for all 0, the following inequality holds: L(Wt) L(W t), where L() represents the loss (or energy) functional associated with the system. This inequality implies that the solution trajectory (W t, St) maintains loss value that does not exceed that of the trajectory (Wt, St) at any point in time. Proof. The proof proceeds by contradiction. Assume that there exists some τ > 0 such that L(Wτ ) > L(W τ ). Since the initial conditions satisfy L(W0) = L(W 0), there must exist time τ0 (0, τ ) such that the two trajectories intersect at this point, i.e., and δ > 0, such that L(Wτ0 ) = L(W τ0 ), ϵ (0, δ), L(W τ0+ϵ) < L(Wτ0+ϵ). Consider the changes in the loss functional around τ0 + ϵ: Dividing through by ϵ and taking the limit as ϵ 0, we have: L(W τ0+ϵ) L(W τ0) < L(Wτ0+ϵ) L(Wτ0). lim ϵ0 L(W τ0+ϵ) L(W τ0 ) ϵ lim ϵ0 L(Wτ0+ϵ) L(Wτ0) ϵ . By the definition of the derivative of L, this becomes: dt L(W τ0) dt L(Wτ0), If dt L(W τ0) = dt L(Wτ0), then by Theorem A.2, we have Note that xϕ(x) max(x, 0), here we pick ϕ(x) = (1 + ϵ)I(x 0) with ϵ > 0. 1T ([D (1 ϕ(D))]) = 0, Thus, it follows = 0, and hence dt L(wτ0) = dt L(wτ0 ) = 0, which means wτ0 = wτ0 is stationary point of L. We have which translates to: L(W τ0)Vτ0 < L(Wτ0 )Uτ0. From Theorem A.2, we know that for the corresponding systems, the velocity vectors satisfy: L(Wt)Vt L(Wt)Ut. However, this contradicts the assumption that L(W τ0)Vτ0 < L(Wτ0 )Uτ0 . Thus, the assumption that L(Wτ ) > L(W τ ) must be false, and we conclude: L(Wt) L(W t), 0. 12 Cautious Optimizers Corollary A.4. Assume 2 Ψ is positive definite and Ψ(0) = 0, and H(w, s) = L(w) + K(s) is differentiable, then the original system (9) converges to stationary point of H(W, S), and (10) also converges to stationary point of H(W, S). Proof. First, we use LaSalles invariance principle to find the conditions that the accumulation points (positive limit points) satisfy: System (9) : System (10) : L(W )2 L(W )2 Φ = K(S)2 Φ = K(S)2 Ψ = 0, Ψ = 0, 1T (D (1 ϕ(D))) = 0. By the assumption that Ψ is positive definite and Ψ(0) = 0, we have K(S) = 0. For positive limit points of systems (10) and (9), if L(W ) = 0, then the point (W, S) is not positive limit point since = L(W ) Ψ(K(S)) = L(W ) = 0. Thus, L(W ) = 0. Together with K(S) = 0, we conclude that (W, S) is stationary point of H(W, S). Theorem A.5. Assuming L(W ) is convex, for the following update schemes: Cautious Momentum (cid:40) Wt+1 = Wt εMt+1 1L(Wt)Mt+10, Mt+1 = Mt εL(Wt), (cid:40) Momentum Wt+1 = Wt εMt+1, Mt+1 = Mt εL(Wt), we have that Cautious Momentum is at least as fast as Momentum. Proof. For the Momentum update, we have L(Wt+1) L(Wt) L(Wt)(Wt+1 Wt) //By convexity of L, = εL(Wt)Mt+1. If L(Wt)Mt+1 < 0, then L(Wt+1) L(Wt) 0, which means increases at this step. For the Cautious Momentum update, we have L(Wt+1) L(Wt) L(Wt)(Wt+1 Wt) //By convexity of L, = εL(Wt)Mt+1 1L(Wt)Mt+10. If L(Wt)Mt+1 < 0, then 1L(Wt)Mt+10 = 0, and thus Wt+1 = Wt. If L(Wt)Mt+1 0, then 1L(Wt)Mt+10 = 1, and Cautious Momentum follows the same update scheme as Momentum. Hence, Cautious Momentum is at least as fast as Momentum. Theorem A.6. Assuming L(W ) is L-smooth, for the following update schemes: Cautious Momentum (cid:40) Wt+1 = Wt εMt+1 1 Mt+1 = Mt εL(Wt), L(Wt)Mt+1 Lε 2 Mt+12, Momentum (cid:40) Wt+1 = Wt εMt+1, Mt+1 = Mt εL(Wt), we have that Cautious Momentum is at least as good as Momentum with any step size ε > 0. Proof. For Momentum update scheme, L(Wt) L(Wt+1) L(Wt)(Wt+1 Wt) + Wt+1 Wt //By L-smoothness of = εL(Wt)Mt+1 + ε2L 2 2 Mt+12 . 13 If L(Wt)Mt+1 + εL 2 Mt+12 < 0, then at this step, is actually increasing. While for Cautious Momentum update, Cautious Optimizers L(Wt) L(Wt+1) ε (cid:18) L(Wt)Mt+1 + Mt+12 (cid:19) 1 ε2L 2 L(Wt)Mt+1 Lε 2 Mt+1 If L(Wt)Mt+1 + εL If L(Wt)Mt+1 + εL same update scheme as Momentum. 2 Mt+12 < 0, then at this step, 1 2 Mt+12 > 0, then 1 L(Wt)Mt+1 Lε 2 Mt+12 = 0, thus, Wt+1 = Wt. L(Wt)Mt+1 Lε 2 Mt+12 = 0. Hence, Cautious Momentum follows the A.2. Examples We instantiate the result on Adam, SignGD, and Lion below. A.2.1. ADAM dW dt = Mt Vt + ϵ dt dt Mt = β1 (L(Wt) Mt) Vt = β2 (L(Wt)2 Vt) with H(W, M, ) = L(W ) + 1 2β1 < + ϵ , > (14) dH(W, M, ) dt =< L(Wt), dW dt > + 1 2β (< = < Mt Vt + ϵ , Mt > β2 4β (< 2Mt Vt + ϵ 2 3/2 + ϵ , dM dt > + < , L(Wt)2 > + < 2 ( , 2 1 Vt Vt + ϵ)2 2 3/2 + ϵ dV dt >) , Vt >) = β2 4β1 < 2 3/2 + ϵ , L(Wt)2 > (1 β2 4β1 ) < Mt Vt + ϵ , Mt > We want dH(W,M,V ) dt 0 always, hence 1 β2 4β1 0, thus β1 β2 4 A.2.2. CAUTIOUS ADAM = 1(sign(L(Wt)) = sign(Mt)) Mt Vt + ϵ Mt = β1 (L(Wt) Mt) Vt = β2 (L(Wt)2 Vt). dW dt dt dt It is easy to show that the loss function L(Wt) itself is Hamiltonian (Lyapunov) since dt L(Wt) = L(Wt) Wt = L(Wt) (cid:18) 1(sign(L(Wt)) = sign(Mt)) Mt Vt + ϵ (cid:19) 0. 14 A.2.3. SIGNGD Recall the update scheme of sign momentum (Bernstein et al., 2018): Cautious Optimizers = sign(M ) = L(W ) M. We can verify that the following is Hamiltonian for above dynamic system: H(W, ) = L(W ) + 1 , its simple to verify = L(W )sign(M ) + sign(M )L(W ) 1 = 1 . A.2.4. CAUTIOUS SIGNSGD Recall the update scheme of cautious sign momentum: = sign(M ) 1sign(L(W )=sign(M ) = L(W ) M. We can verify that he loss function itself is Hamiltonian (a Lyapunov): dt L(W ) = L(W ) Wt = L(W ) (cid:0)sign(M ) 1sign(L(W )=sign(M ) (cid:1) = (cid:13) (cid:13)f (x) 1sign(L(W )=sign(M ) (cid:13) (cid:13)1 . A.2.5. LION Recall the update of Lion (Chen et al., 2023a): Mt = αL(Wt) γMt Wt = sign( Mt), with γ, α > 0, and = ε(αL(W ) + γM ). Following the derivation in (Chen et al., 2023a), we have the following Hamiltonian (Lyapunov) function: H(W, ) = αL(W ) + (1 ϵγ) 1 . Since H(W, ) = αL(W ) + (1 ϵγ)sign(M ) = αL(W )(sign( )) + (1 ϵγ)sign(M )(αL(W ) γM ) = (1 ϵγ)(sign( ) sign(M ))(M ) γ , (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1 where 1 ϵγ 0. A.2.6. CAUTIOUS LION = αL(W ) γM = sign( ) 1 sign(L(W ))=sign( ), (15) (16) where = ε(αL(W ) + γM ). The loss function () itself is Hamiltonian since L(W ) = L(W ) = L(W ) (cid:16) sign( ) 1 sign(L(W ))=sign( ) (cid:17) = (cid:13) (cid:13)L(W ) 1 (cid:13) sign(L(W ))=sign( ) (cid:13) (cid:13) (cid:13)1 . 15 A.3. From Continuous Time to Discrete time Cautious Optimizers This subsection should be after the continuous analysis of Hamiltonian, we can show the decreasing of the loss of c-optimizer is simply larger or equal to baseline optimizer at each iteration from discrete time perspective. First, by discretizing dynamical system (9) , we have Wt+1 := Wt + εUt = Wt ε (K(St) ϕ(L(Wt) K(St)) + Φt(L(Wt))) St+1 := St + ε (L(Wt) Ψt(K(St))) . By discretizing dynamical system (10) , we have Wt+1 := Wt + εVt = Wt ε (K(St) + Φt(L(Wt))) St+1 := St + ε (L(Wt) Ψt(K(St))) (17) (18) Theorem A.7. [Larger Loss Decreasing] Assume loss function L() is differentiable and L-smooth, and element-wise operator ϕ satisfies (1 ϕ(x)) 0 and ϕ(x) 0. For the discretized update, Wt+1 = Wt + εUt t+1 = Wt + εVt, (19) we have where ε L(Wt + εUt) L(Wt + εVt), i.e. L(Wt+1) L(W t+1), 2L(Wt)K(St)2 ϕ+2L(Wt)2 Φ Rt(2LVt+Rt) and Rt = Ut Vt. Proof. First, we calculate the difference between Wt+1 and t+1, then we use Lsmooth condition to bound the difference between L(Wt+1) and L(W t+1). Wt+1 t+1 = ε(Ut Vt) = ε (K(St) + Φt(L(Wt)) K(St) ϕ(L(Wt) K(St)) Φt(L(Wt))) = εK(St) (1 ϕ(L(Wt) K(St))) = εRt. Let us use Lsmooth condition to bound the difference L(Wt+1) L(W t+1) ε L(Wt+1) (K(St) (1 ϕ(L(Wt) K(St)))) + 1 2 ε2 Rt2 = ε (L(Wt) + L(Wt+1) L(Wt+1)) (K(St) (1 ϕ(L(Wt) K(St)))) + 1 2 ε2 Rt2 = ε L(Wt) (K(St) (1 ϕ(L(Wt) K(St)))) + ε (L(Wt+1) L(Wt+1)) (K(St) (1 ϕ(L(Wt) K(St)))) + 1 2 ε2 Rt2 = ε L(Wt) (K(St) (1 ϕ(L(Wt) K(St)))) + ε Wt+1 Wt K(St) (1 ϕ(L(Wt) K(St))) + = ε L(Wt) (K(St) (1 ϕ(L(Wt) K(St)))) 1 2 ε2 Rt2 + ε2 K(St) + Φt(L(Wt)). K(St) (1 ϕ(L(Wt) K(St))) + (cid:17) (cid:16) = ε L(Wt) K(St)2 ϕ + L(Wt) Φ + ε2 Rt Vt + ε2 Rt2 1 2 1 2 ε2 Rt 0. //By the choice of ε. 16 Cautious Optimizers Theorem A.8 (Convergence Rate for C-Optimizers). Assume L(W ) is L-smooth and differentiable, and the element-wise operator ϕ satisfies (1 ϕ(x)) 0 and ϕ(x) 0 as shown in Theorem A.1. With (Wt, St) following the update in (17): Wt+1 := Wt + εUt = Wt ε (K(St) ϕ(L(Wt) K(St)) + Φt(L(Wt))) , St+1 := St + ε (L(Wt) Ψt(K(St))) . Assume ε > 0, we have:"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) (cid:16) t=1 where BT = (cid:80)T t=1 Ut2. L(Wt) K(St) ϕ + L(Wt)2 Φ (cid:17) L(W0) L(W ) ε + Lε 2T BT , Proof. Using the L-smoothness of L(W ), we expand L(Wt+1) L(Wt): L(Wt+1) L(Wt) L(Wt)(Wt+1 Wt) + 2 Wt+1 Wt2 . Substitute Wt+1 Wt = εUt, where: Ut = (K(St) ϕ(L(Wt) K(St)) + Φt(L(Wt))) , to get: L(Wt+1) L(Wt) ε L(Wt) (K(St) ϕ(L(Wt) K(St)) + Φt(L(Wt))) + Lε2 K(St) ϕ(L(Wt) K(St)) + Φt(L(Wt))2 . Simplify using the definition of ϕ and Φ: L(Wt) (K(St) ϕ(L(Wt) K(St)) + Φt(L(Wt))) = L(Wt) K(St)2 ϕ L(Wt)2 Φ , which gives: L(Wt+1) L(Wt) ε (cid:16) L(Wt) K(St)2 ϕ + L(Wt)2 Φ (cid:17) + Lε2 2 Ut2 . Summing over = 1, . . . , , we obtain telescoping sum: L(WT +1) L(W1) ε (cid:88) (cid:16) t=1 L(Wt) K(St)2 ϕ + L(Wt)2 Φ (cid:17) + Lε2 2 (cid:88) t=1 Ut2 . Rearranging, dividing by ε, and noting L(WT +1) L(W ), we get: 1 T (cid:88) (cid:16) t=1 L(Wt) K(St)2 ϕ + L(Wt)2 Φ (cid:17) L(W1) L(W ) ε + Lε 2T BT , where BT = (cid:80)T t=1 Ut2. This concludes the proof. A.4. Pesudo Code B. Experiment Details All experiments are run on 480G A100 DGX box. Cautious Optimizers Algorithm 3 C-Lion Optimizer + 1 gt wLt(wt1) ut sign(β1mt1 + (1 β1) gt) Require: learning rate ϵ, momentum coefficient β1, β2 [0, 1), weight decay factor γ 1: Initialize parameter vector wt 2: Initialize = 0, m0 = 0 3: while wt not converged do 4: 5: 6: 7: mt β2mt1 + (1 β2) gt 8: 9: 10: wt wt1 ϵtϕtut 11: wt wt ϵγwt 12: end while ϕt I(ut gt 0) ϵt = ϵt ϕt0+1 {Get gradients at timestep t} {get the signed update} {update momentum} // Compute alignment mask // Scale lr, is dimension of ϕt {Weight decay} # Params 60 100 350 1B β1 0.9 0.9 0.9 0.9 β2 0.99 0.99 0.99 0.95 Learning rate (AdamW) 0.001 0.001 0.001 0.001 Learning rate (Lion) weight decay Batch Size 0.0001 0.0001 0.0001 0.0001 0.0 0.0 0.0 0.1 512 512 1024 2048 Table 4: Hyperparameters for LLM experiments. For 60M, 100M and 350M, we use β1 = 0.9, β2 = 0.99 and weight decay 0.0 on AdamW; For 1B model, we find β1 = 0.9, β2 = 0.95 and weight decay 0.1 yeild the best results. Sequence length of all models are 256. # Params 110 β1 0. β2 0.999 Learning rate weight decay Batch Size 1.5104 0.05 4096 Table 5: Hyperparameters for MAE experiment 18 Cautious Optimizers Figure 5: 1.47x speed up on AdamW 19 Cautious Optimizers Figure 6: 1.28x speed up on Lion 20 Cautious Optimizers Figure 7: 1.16x speed up on MAE pretraining"
        }
    ],
    "affiliations": []
}