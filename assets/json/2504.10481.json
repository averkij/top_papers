{
    "paper_title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
    "authors": [
        "Ding Chen",
        "Qingchen Yu",
        "Pengyuan Wang",
        "Wentao Zhang",
        "Bo Tang",
        "Feiyu Xiong",
        "Xinchi Li",
        "Minchuan Yang",
        "Zhiyu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 1 8 4 0 1 . 4 0 5 2 : r xVerify: Efficient Answer Verifier for Reasoning"
        },
        {
            "title": "Model Evaluations",
            "content": "Ding Chen1 Qingchen Yu2 Pengyuan Wang2 Wentao Zhang3 Bo Tang2 Feiyu Xiong2 Xinchi Li1 Minchuan Yang1 Zhiyu Li2 1 Research Institute of China Telecom, Beijing, China 2 MemTensor (Shanghai) Technology Co., Ltd. 3 Center for Data Science, Peking University wentao.zhang@pku.edu.cn, lizy@iaar.ac.cn"
        },
        {
            "title": "Abstract",
            "content": "With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify. All resources for xVerify are available at https://github.com/IAAR-Shanghai/xVerify."
        },
        {
            "title": "Introduction",
            "content": "With the emergence of chain of thought (CoT) prompting [35], researchers began to explicitly encourage LLMs to generate intermediate reasoning steps, thereby enhancing their ability to handle complex tasks. Following this, OpenAI introduced the o1 model [15], which proposed the concepts of slow thinking and scaling at test time. Specifically, the model is trained to output detailed reasoning process before generating final answer, significantly improving its performance on complex tasks. Inspired by this paradigm, variety of reasoning models have emerged, such as DeepSeek-R1 [3] trained with GRPO, OpenAIs o3-mini [26], and QwQ-32B [31]. However, the rise of reasoning models poses substantial challenges for evaluation. Since the outputs of these models often contain Equal contribution. Corresponding authors Preprint. Under review. lengthy reasoning processespotentially including redundant information, intermediate results, and even self-contradictionsit becomes significantly more difficult for evaluation tools to extract the final answer from such responses [2]. Developing evaluation methods tailored for LLM responses involving complex reasoning has become key research focus. LLM reasoning is typically categorized into commonsense, logical, multihop, and mathematical reasoning [8]. Existing evaluation methods fall into automatic and human evaluation [2], with automatic evaluation gaining prominence due to its scalability and lower cost. The main automatic approaches for evaluating reasoning models include rule-based evaluation frameworks [13, 5, 27, 9, 25] and LLM-based judgment methods [20, 7, 18]. However, both approaches face limitations in reasoning model evaluation. Rule-based frameworks often struggle to extract final answers from lengthy reasoning traces, rely on strict formatting (e.g., syntactically correct LaTeX), and typically ignore the reasoning process itselfan oversimplification challenged by many researchers [36, 33, 14, 32]. Judge models are usually not optimized for reasoning evaluation and mainly produce qualitative scores or comments [7], making them more suitable for subjective questions. Objective tasks, in contrast, require accurate binary classification. Currently, effective automatic methods specifically designed for evaluating reasoning on objective questions remain lacking. To address these challenges, we introduce xVerify, an efficient LLM-answer verifier tailored for evaluating LLM responses to objective questions. xVerify processes the full LLM output, enabling it to accurately identify final answers from complex reasoning traces. It also supports robust equivalence checking, including symbol conversion (e.g., alpha α), mathematical expression matching, and semantic alignment in natural language. Moreover, it is tolerant of formatting errors such as malformed LaTeX, making it applicable to wide range of tasks, including math problems, multiplechoice, short-answer, and classification questions. To train and evaluate xVerify, we construct the Verify Answer for Reasoning (VAR) dataset, which includes responses from 19 LLMs across 24 reasoning benchmarks. All labels are verified through multi-round GPT-4o and human review. The dataset covers advanced reasoning models and benchmarks like GPQA, LiveMathBench, and AIME 2024. We fine-tune xVerify on variety of base models (e.g., Qwen2.5, LLaMA, Gemma 2) and scales (0.5B32B). Remarkably, even the smallest variant (xVerify-0.5B-I) surpasses existing evaluation methodsincluding 32B-sized modelson all metrics, while larger variants achieve F1 and accuracy over 95% on both test and generalization sets. The main contributions of this paper are summarized as follows: We construct the VAR dataset, which contains answer samples from 19 LLMs across 24 evaluation benchmarks. The dataset is annotated via multiple rounds of GPT-4o and human review, and is designed for training and evaluating judge models for reasoning tasks. We propose xVerify, an efficient answer verifier for reasoning model evaluations, and release multiple fine-tuned versions of xVerify. The checkpoints are publicly available2. We conduct comprehensive comparative evaluations against multiple existing evaluation frameworks and judge models on both test and generalization datasets, thoroughly validating the effectiveness and applicability of xVerify."
        },
        {
            "title": "2 Related Work",
            "content": "Evaluation methods have always been crucial component in the development of LLM [2]. However, the open-ended nature of LLM outputs makes it difficult to apply standardized metrics, limiting the effectiveness of traditional evaluation methods [20]. The rise of reasoning models [26, 3, 31], which often generate lengthy and complex reasoning, further complicates evaluation. For objective tasks, the main challenge is to accurately extract the final answer from the LLMs semi-structured output and compare it with the reference answer. Existing approaches are typically divided into human evaluation and automatic evaluation. While human evaluation offers flexibility, automatic methods are more cost-efficient and consistent [2]. Current automatic methods mainly include rule-based evaluation frameworks and LLM-based judgment methods. Rule-based methods are widely used in automatic evaluation frameworks such as LM Eval Harness [5], OpenCompass [27], UltraEval [9], and OpenAI Evals [25]. Tools like Math-Verify [13] also follow 2Hugging Face collections: https://huggingface.co/collections/IAAR-Shanghai/xverify 2 this approach, extracting final answers using regular expressions (RegEx) and comparing them with reference answers. However, LLM outputs often contain final answers in varied surface formse.g., \"alpha\" vs. \"α\", \"A\" vs. \"a\", or \"1000\" vs. \"103\"which can be semantically equivalent but textually different. While some tools support limited transformations, they typically handle only LaTeX expressions or simple string patterns, and struggle with basic semantic equivalence like \"one hundred\" vs. \"100\". For reasoning models, the output is usually lengthy and involves complex reasoning steps with intermediate results. This makes it difficult for regular expressions to accurately identify the final answer, causing rule-based approaches to frequently fail in such contexts. Moreover, prior work has shown that LLMs may revise or overturn their initial predictions during extended reasoning processes, exhibiting kind of self-reflection [32]. At the same time, rule-based methods typically ignore the reasoning process and only evaluate the final answer, which has drawn criticism from many researchersespecially in the context of reasoning models [36, 33, 14]. Thus, rule-based evaluations have limited applicability in reasoning scenarios. LLM-based judgment methods use fine-tuned LLMs to evaluate the quality of other LLMs responses. Compared to traditional evaluation methods, they offer greater task adaptability, generate interpretable results, reduce evaluation costs, and can be applied across the LLM lifecycle [20, 7, 18]. For objective questions, these judge models can extract final answers from responses with intermediate reasoning or self-reflection. In recent years, many LLM-based judge models have emerged, including JudgeLM [39], PandaLM [34], Auto-J [21], Prometheus 2 [17], CompassJudger [1], CritiqueLLM [16], and Themis [12]. Judge models typically support pointwise, pairwise, and listwise evaluations [20], and some also serve as reward models in reinforcement learning. However, most are designed to assign scores to LLM outputs, making them more suitable for subjective evaluations like helpfulness, reliability, or relevance. For objective questions that require binary decisions (correct or incorrect), these models are less effective. Although scores can be binarized using thresholds, this approach is unreliable, as the models are not explicitly trained for such tasks. Moreover, the current LLM-based critic models and PRMs (Process Reward Models) exhibit subpar performance when detecting errors in long chain-of-thought responses generated by reasoning models [10]. Thus, while judge model holds promise for evaluating reasoning models, they require targeted training. In summary, automatic evaluation on objective tasks remains underdeveloped. Rule-based and LLMbased methods each have clear limitations, while human annotation is costly and hard to scale. To address these challenges, we propose xVerify, robust and targeted judge model specifically designed for objective evaluation of LLMs."
        },
        {
            "title": "3 Problem Definition",
            "content": "To evaluate the correctness of LLM responses to objective questions, the key is to extract the final answer from the response and compare it with the reference answer. We formally define this evaluation task as follows: We formalize this task as 4-tuple (Q, R, Aref , E), where = {q1, q2, ..., qn} is the set of questions, = {r1, r2, ..., rn ri = W(qi)} is the set of responses generated by an LLM W, Aref = {a1 ref } is the set of reference answers, and : Aref 0, 1 is the evaluation function that returns 1 if the response is correct and 0 otherwise. ref , ..., an For the stage of extracting the final answer, given response to question q, which may include intermediate reasoning and multiple candidate answers, we denote the extracted candidates as A(r). To identify the final answer, we define scoring function : A(r) that measures the relevance or suitability of each candidate A(r) to q, and select the final answer using the extraction function: ε(q, r) = arg max aA(r) S(a, q). (1) For the equivalence comparison stage, we define an equivalence function ψ : Aref Afinal {0, 1}, where ψ returns 1 if the predicted answer is equivalent to the reference, and 0 otherwise. Since answers may appear in different forms, ψ integrates results from the following three sub-functions: For mathematical expressions, we define composite normalization function Φmath norm = ϕerr ϕsyn ϕalg ϕdim, where ϕerr repairs minor syntax errors, ϕsyn unifies syntactic structures, ϕalg performs 3 algebraic simplification, and ϕdim ensures consistency in physical units. By transforming expressions into canonical form, Φmath norm enables reliable equivalence comparison: ψmath(amath ref , amath inal) = (cid:26)1 0 norm (amath ref if Φmath otherwise ) = Φmath norm (amath inal), (2) For natural language answers, we define comparison function ψnl : Anl final {0, 1} to assess semantic equivalence. Specifically, we introduce semantic alignment function ϕnl align to measure the similarity between two textual answers. The equivalence decision is made by comparing the alignment score with predefined threshold τ : ref Anl ψnl(anl ref , anl inal) = (cid:26)1 0 align(anl if ϕnl otherwise ref , anl inal) τ, (3) For symbolic representations, we define composite normalization function Φsym norm = ϕuniϕfontϕdom, which unifies symbols by applying ϕuni for Unicode normalization, ϕfont for aligning font styles, and ϕdom for domain-specific mappings. This produces standardized form for character-level comparison, and the Φsym norm is defined as: ψsym(asym ref , asym inal) = (cid:40) 1 0 norm(asym ref ) = Φsym norm(asym inal), if Φsym otherwise (4) Based on the above components, we define unified equivalence function ψ to determine whether the final answer af inal matches the reference answer aref across different modalities. The definition is: ψ(af inal, aref ) = 1, if ψmath(amath ψnl(anl ψsym(asym inal, amath ref inal, anl ref ) = 1 inal, asym ref ) = 1; ) = 1 0, otherwise (5) inal, and asym inal, anl Here, amath inal represent the mathematical, natural language, and symbolic parts of the final answer, respectively, and similarly for aref . This allows for equivalence checking in both unimodal and multimodal settings. To summarize, the overall evaluation function is defined as: E(q, r, aref ) = ψ(cid:0)ε(q, r), aref (cid:1) (6) where is the objective question, is the response generated by the LLM, and aref is the corresponding reference answer."
        },
        {
            "title": "4 Methodology",
            "content": "The xVerify training and evaluation pipeline includes three main stages: collecting LLM responses, VAR dataset construction, and xVerify judge pipeline (see Figure 1). We first gather questionresponse pairs from various LLMs across four types of objective questions, including complex, reasoningintensive examples. To ensure accurate labels, we employ multiple rounds of annotation and rechecking using both GPT-4o and human annotators. We also apply data augmentation to increase the datasets diversity and complexity. Finally, we train xVerify models of different sizes on the VAR dataset to evaluate long, multi-step answerscases that are often difficult for existing evaluation methods. Section 4.1 details the dataset construction, and Section 4.2 describes the training process. 4 Figure 1: Framework of xVerify: (1) Collecting LLM Responses: aggregate responses from multiple LLMs across datasets covering four question types. (2) VAR Dataset Construction: employ GPT-4o and human annotators for labeling and rechecking, and use data augmentation to refine the dataset. (3) xVerify Judge Pipeline: accurately evaluate multi-component answers from reasoning models on challenging questions. 4.1 VAR Dataset xVerify is designed to assess the correctness of reasoning models responses on objective questions. However, current judge models are mostly trained on tasks such as scoring or reviewing, and reasoning models with lengthy responses have only recently emerged. As result, there is currently no suitable dataset for training xVerify. To better train and evaluate xVerify, we constructed dedicated dataset named Verify Answer for Reasoning (VAR). Examples from the VAR dataset are provided in Appendix B.3. 4.1.1 LLM Response Generation To ensure the diversity and coverage of the dataset, we selected 19 mainstream LLMs and 24 frequently used multilingual datasets to generate and collect responses. To better simulate the answering patterns of reasoning models in common evaluation scenarios, the chosen LLMs include recently released models such as the DeepSeek-R1-Distill series [3] and QwQ-32B [31]. Most of the other LLMs also support context lengths exceeding 32k tokens, enabling them to produce answers with extended reasoning chains. The selected datasets include high-difficulty benchmarks commonly used for evaluating reasoning models, such as GPQA [28], AIME 2024 [24], MATH [11], and LiveCodeBench [23], which typically require multi-step reasoning and computation to solve. During data generation, we also retained some extremely long responses, such as those exceeding 6k characters in length. Detailed information on all LLMs and datasets is provided in Appendix A. To train and evaluate xVerify more effectively, we grouped the 24 datasets into four types based on question and answer formats: multiple choice, math, short answer, and classification. Multiple choice questions offer several labeled options; math includes questions where answers are mathematical expressions (e.g., numbers, equations), including mathematics and physics problems; short answer questions expect brief natural language responses like names or dates, with no strict format constraints; classification tasks involve selecting the correct label, such as for sentiment or topic classification. To reflect realistic evaluation settings and generate diverse set of Q&A samples, we designed multiple prompt templates for guiding the LLMs in response generation. The prompt configurations vary along several dimensions: 0-shot vs. 5-shot, with or without CoT, and with or without answer format restrictions (restrict), resulting in eight distinct prompt types. Details of all prompt templates are provided in Appendix D.1. 5 In total, we generated 191,600 Q&A samples using the 19 LLMs and 24 evaluation sets, providing rich and diverse sample pool for constructing the dataset. 4.1.2 Dataset Partitioning Based on the previously collected sample pool, we constructed the training, test, and generalization sets through filtering and preprocessing. The training and test sets are used to train and evaluate the xVerify model. Both are sampled from the same pool, sharing similar distributions. Specifically, they include samples generated by 15 LLMs across 17 evaluation sets, covering the four previously mentioned question types. The training set contains 36,941 samples, and the test set includes 5,194 samples. The generalization set complements the test set by evaluating xVerifys ability to handle more diverse and challenging distributions, reflecting real-world scenarios. It consists of 5,366 samples from 7 evaluation sets not used in the training or test sets, while still spanning all four question types. These samples are generated by 19 LLMs, including 4 models not seen in training or testing, such as the reasoning model QwQ-32B, resulting in greater diversity and distribution shift. Section 4.1.4 introduces our data augmentation strategy, which adds more challenging samples to all three sets. Detailed dataset statistics are provided in Appendix B.1. 4.1.3 Data Annotations To ensure the accuracy of xVerifys training and evaluation, we conducted multiple rounds of automatic and manual annotation across the three datasets. Specifically, we used GPT-4o to perform two rounds of annotation for all samples in the datasets, utilizing two distinct prompt templates (details provided in Appendix D.2) to improve annotation confidence [33, 22]. Given the large size of the training set, we only applied manual annotation to the more challenging math problems and to samples where the two rounds of GPT-4o annotations disagreed. In contrast, for the test and generalization sets, we manually annotated all samples, resulting in three-round annotation process to maximize label reliability. Details of the manual annotation process are provided in Appendix B.2. 4.1.4 Data Augmentation Figure 2: Data Augmentation Pipelines: (1) transformation of multiple-choice options through numbering conversion and noise injection, (2) diversification of mathematical answers via equivalent expression generation, and (3) final answer sentence transformation using prompt rephrasing, symbol wrapping, and gap token insertion. To further enhance the diversity and robustness of the dataset, we designed series of data augmentation strategies (illustrated in Figure 2) to better simulate real-world evaluation settings and improve the models tolerance to varied answer formats. For multiple-choice questions, we applied two types of augmentations: option index transformation and noise injection. The former converts alphabetical labels to Arabic or Roman numerals, while the latter randomly adds or removes irrelevant distractor options without changing the original question intent, thereby increasing structural complexity. 6 For math problems, we used two approaches: augmentation based on reference answers and LLM responses. In the first approach, we generated 35 mathematically equivalent expressions of each reference answer through symbolic and formal transformations, then created new samples accordingly. In the second, we applied the same transformation logic to the final answers in LLM responses, enriching the dataset with varied mathematical formats and helping the model learn equivalence across symbolic expressions. We also augmented the final answer statements. Specifically, we extracted answer-bearing sentences from responses generated using restrict prompts, and applied over 1,000 transformation patterns. These included: 20 variations of prompt rephrasing (e.g., The answer is The most appropriate answer is B), 18 symbolic wrappers (e.g., wrapping as ), and 5 forms of delimiter insertions (e.g., adding colon or space before the answer). This improved diversity in answer formats and reduced overfitting to specific templates. Together, these strategies expanded the expressive space of the dataset while preserving semantic consistency, offering richer and more challenging training signals for xVerify. After augmentation, the sizes of the training, test, and generalization sets increased to 43,204, 6,122, and 6,468 samples respectively. Full dataset details are provided in Appendix B.1. The augmentation of math problems primarily relied on GPT-4o; prompt templates are listed in Appendix D.3. 4.2 Model Training We trained 14 models with different parameter sizes and architectures using the training set from the VAR dataset. Specifically, we utilized the LLaMA-Factory framework [38] and QLoRA technique [4] for model training. Based on extensive experimentation, we set the number of epochs to 1 and selected learning rate of 1e-4 as the optimal configuration, with other hyperparameters detailed in Appendix C.1. Many researchers have pointed out potential bias in using LLMs as judge models, where models from the same family tend to receive higher ratings [19]. To thoroughly evaluate the generalization capability of the xVerify method, we trained 14 models with varying parameter sizes and architectures. These models ranged from 0.5B to 32B parameters and included five different families, such as LLaMA 3 [6], Qwen2.5 [37], and Gemma 2 [30]. Details of the models used are provided in Appendix C.2."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we will present the configuration, results, and detailed analysis of the xVerify model evaluation experiments. First, we will outline the experimental setup: Datasets: The datasets used in the evaluation experiments are the test set and generalization set from the VAR dataset. The test set is used to evaluate the xVerify models performance, while the generalization set supplements the test set by simulating real-world scenarios with broader sample distribution to assess the models generalization ability. Metrics: The evaluation mainly uses accuracy and F1 score on both the test and generalization sets. Accuracy shows the models overall performance, while the F1 score combines precision and recall for more complete perspective. Baselines: There are two types of baselines: evaluation frameworks and judge models. The evaluation frameworks include DeepSeek-Math [29], LM Eval Harness [5], MathVerify [13], OpenAI Evals [25], OpenCompass [27], and UltraEval [9]. The judge models include PandaLM [34], Auto-J [21], Prometheus 2 [17], JudgeLM [39], and CompassJudger [1]. In addition, GPT-4o is also used as judge model with two strategies: one with CoT and one without. The prompts for the judge model and xVerify are provided in Appendix D.4 and Appendix D.5. Test Set Evaluation Results. We evaluated all evaluation frameworks, judge models, and the xVerify model on the VAR test set (see Table 1). Overall, the xVerify model outperforms all evaluation frameworks and judge models, including GPT-4o, with the best and second-best values in each column appearing for the xVerify model. Table 1: Evaluation Accuracy Results on the Test Set. \"-\" indicates that the evaluation method is not applicable to the problem type. The best performance in each column will be shown in bold, and the second-best performance will be underlined. Method Type Method Multiple Choice Math Short Answer Classification Overall F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. Evaluation Framework Judge Model xVerify DeepSeek Math Verify LM Eval Harness Math-Verify OpenAI Simple Evals OpenCompass UltraEval PandaLM-7B-v1 Auto-J-Bilingual-6B Auto-J-13B Prometheus-7B-v2.0 Prometheus-8x7B-v2.0 JudgeLM-7B-v1.0 JudgeLM-13B-v1.0 JudgeLM-33B-v1.0 CompassJudger-1-1.5B CompassJudger-1-7B CompassJudger-1-14B CompassJudger-1-32B GPT-4o as Judge GPT-4o as Judge (CoT) xVerify-0.5B-I xVerify-3B-Ib xVerify-7B-I xVerify-9B-I xVerify-14B-Ia xVerify-32B-I 74.90% 52.52% 70.77% 75.17% 78.34% 84.30% 58.44% 68.19% 25.16% 28.27% 53.41% 44.51% 72.35% 66.94% 47.67% 48.32% 5.88% 53.76% 82.55% 86.70% 42.27% 71.91% 0.00% 29.66% 45.64% 65.91% 23.61% 28.02% 66.79% 76.88% 42.23% 55.32% 73.29% 67.87% 51.17% 58.10% 74.18% 79.64% 68.11% 72.52% 79.25% 84.73% 13.95% 40.71% 17.34% 18.04% 8.88% 56.89% - - - - - - - - - - - - 4.26% 8.12% 16.78% 14.46% 23.47% 17.72% 25.32% 16.79% 16.40% 13.72% 52.85% 67.71% 40.76% 65.21% 67.22% 79.60% 74.86% 71.37% 57.04% 69.59% 40.00% 63.20% 26.32% 60.62% 64.41% 78.22% 86.04% 82.60% 53.38% 68.13% 75.76% 75.41% 74.20% 74.35% 70.95% 74.59% 84.80% 77.03% 76.50% 75.11% 71.26% 68.61% 71.99% 66.92% 76.24% 77.70% 83.27% 77.65% 74.57% 71.12% 56.53% 42.57% 46.09% 34.58% 60.33% 50.56% 83.89% 73.22% 59.02% 45.90% 56.81% 48.89% 58.39% 59.46% 77.32% 79.52% 95.63% 93.82% 68.57% 65.83% 42.86% 43.24% 44.82% 46.03% 57.86% 62.23% 73.42% 67.56% 52.00% 51.75% 49.95% 35.54% 61.66% 48.78% 57.36% 46.93% 82.51% 70.96% 61.94% 48.35% 70.05% 62.78% 66.62% 58.86% 67.47% 65.08% 92.99% 89.50% 72.72% 65.96% 58.94% 44.62% 55.09% 40.76% 59.66% 52.90% 90.87% 86.61% 63.22% 51.37% 95.09% 95.37% 84.11% 84.30% 94.95% 96.11% 98.45% 97.84% 91.67% 91.69% 96.61% 96.75% 95.27% 95.80% 95.01% 96.20% 98.14% 97.43% 96.25% 96.39% 97.10% 97.23% 95.41% 95.88% 95.63% 96.63% 99.56% 99.38% 96.85% 96.95% 97.78% 97.90% 93.74% 94.64% 96.72% 97.49% 99.71% 99.59% 96.69% 96.85% 97.31% 97.41% 95.65% 96.18% 96.38% 97.23% 99.78% 99.69% 97.17% 97.27% 97.75% 97.84% 95.94% 96.44% 96.51% 97.32% 99.78% 99.69% 97.41% 97.50% 97.43% 97.53% 95.75% 96.27% 96.06% 96.97% 99.78% 99.69% 97.19% 97.29% 97.49% 97.59% 95.73% 96.22% 95.41% 96.46% 99.63% 99.49% 97.06% 97.16% 97.81% 97.90% 95.88% 96.31% 96.18% 97.06% 99.71% 99.59% 97.32% 97.40% Among the evaluation frameworks, the best performers were DeepSeek Math Verify and OpenCompass, but neither achieved an F1 score nor accuracy exceeding 80%. Some evaluation frameworks were also not suitable for certain question types, which is an inherent limitation of rule-based methodsstrong in specificity but limited in applicability. For instance, OpenCompass was completely unsuitable for short answer and classification questions. Additionally, the long reasoning processes generated by reasoning models made it difficult for evaluation frameworks to extract final answers, lowering their overall performance. Among judge models, GPT-4o and CompassJudger showed the best overall performance. The CompassJudger-1-32B model achieved F1 score and accuracy of 91.67% and 91.69%, respectively. However, the model performed poorly on math questions, with both F1 score and accuracy below 85%, indicating that it handles simpler questions well but struggles with formula equivalence in math problems. Furthermore, only the 32B version of this judge model achieved over 90% F1 score and accuracy, while smaller models performed below 80%. Therefore, the performance of CompassJudger-1-32B is more result of the base models capabilities rather than the subsequent training. For example, the smallest xVerify-0.5B-I model outperforms CompassJudger-1-32B across the board, indicating that the VAR training set significantly improves model evaluation performance. GPT-4os overall performance is very close to xVerify, but the improvement after using CoT is small, with token consumption nearly doubling. Specifically, GPT-4o as Judge evaluated the entire test set at cost of $13.09, while GPT-4o as Judge (CoT) cost $20.15 (using the OpenAI API, charged by token count). In contrast, even the smallest xVerify-0.5B-I model outperforms all methods except GPT-4o as Judge (CoT) in overall performance, and the xVerify-3B-Ib model surpasses all others in every evaluation metric. Moreover, for more difficult math questions, all xVerify models except xVerify0.5B-I exceeded 95% performance. We also found that the performance of the xVerify model improves as the parameter size increases, but slightly decreases after exceeding 7B parameters, likely due to overfitting on the VAR training set, which is sufficiently large for smaller models. Generalization Set Evaluation Results. To better assess the performance of xVerify on broader sample distribution, we evaluated all methods on the VAR generalization set, as shown in Table 2. On the generalization set, the xVerify model showed slight decrease in overall performance. However, the drop in both F1 score and accuracy was less than 1.5%, while other methods showed mixed results. Overall, the xVerify model still outperformed all other methods, indicating that although 8 overfitting exists in xVerify, it is limited and the model maintains strong generalization ability on samples outside the training set distribution. Table 2: Evaluation Accuracy Results on the Generalization Set. \"-\" indicates that the evaluation method is not applicable to the problem type. The best performance in each column will be shown in bold, and the second-best performance will be underlined. Method Type Method Multiple Choice Math Short Answer Classification Overall F1 Acc. F1 Acc. Acc. F1 Acc. F1 Acc. Evaluation Framework Judge Model xVerify DeepSeek Math Verify LM Eval Harness Math-Verify OpenAI Simple Evals OpenCompass UltraEval PandaLM-7B-v1 Auto-J-Bilingual-6B Auto-J-13B Prometheus-7B-v2.0 Prometheus-8x7B-v2.0 JudgeLM-7B-v1.0 JudgeLM-13B-v1.0 JudgeLM-33B-v1.0 CompassJudger-1-1.5B CompassJudger-1-7B CompassJudger-1-14B CompassJudger-1-32B GPT-4o as Judge GPT-4o as Judge (CoT) xVerify-0.5B-I xVerify-3B-Ib xVerify-7B-I xVerify-9B-I xVerify-14B-Ia xVerify-32B-I 72.90% 73.39% 11.69% 79.83% 60.57% 44.42% 61.60% 65.37% 7.03% 18.48% 58.22% 45.09% 92.06% 88.21% 55.81% 51.30% 5.19% 45.10% 64.18% 87.68% 9.12% 52.75% 0.00% 24.59% 16.10% 55.53% 28.72% 29.23% 24.31% 78.90% 58.33% 59.58% 94.39% 91.62% 57.99% 63.36% 65.74% 78.18% 71.64% 71.44% 47.22% 84.39% 15.71% 48.13% 16.29% 15.31% 13.55% 78.39% - - - - - - - - - - - - 7.85% 4.28% 9.91% 15.97% 45.81% 31.43% 36.23% 25.99% 23.74% 19.14% 52.07% 60.75% 10.56% 74.79% 85.16% 86.76% 84.90% 79.91% 67.20% 74.57% 34.87% 52.78% 9.86% 76.54% 85.12% 86.97% 77.67% 71.99% 60.43% 71.35% 76.67% 73.66% 49.08% 71.46% 81.52% 81.32% 79.59% 71.92% 73.85% 74.35% 74.13% 68.60% 49.48% 60.27% 87.15% 86.13% 84.70% 77.19% 74.51% 71.69% 60.22% 45.71% 12.71% 15.40% 72.15% 62.51% 86.11% 76.18% 59.11% 46.38% 65.39% 57.80% 21.61% 44.87% 86.11% 84.53% 91.78% 86.89% 69.18% 65.63% 46.99% 45.10% 20.31% 39.99% 71.34% 66.69% 41.92% 33.36% 46.06% 46.01% 55.75% 40.87% 34.53% 33.62% 63.93% 51.57% 84.49% 73.93% 60.01% 47.65% 74.31% 65.20% 38.27% 39.89% 88.99% 88.15% 93.29% 89.29% 73.47% 67.47% 63.65% 49.50% 27.63% 21.20% 73.61% 66.48% 88.97% 81.92% 63.10% 51.21% 92.93% 92.32% 72.05% 84.91% 96.81% 96.86% 98.05% 97.05% 91.90% 92.04% 95.86% 95.38% 87.91% 94.76% 97.46% 97.49% 98.67% 97.98% 96.03% 96.18% 95.44% 94.88% 88.34% 94.71% 97.39% 97.42% 98.36% 97.52% 95.79% 95.92% 96.49% 96.10% 80.00% 91.94% 96.95% 97.00% 99.03% 98.53% 95.29% 95.53% 96.21% 95.71% 86.20% 94.15% 97.60% 97.63% 99.03% 98.53% 96.08% 96.23% 96.16% 95.66% 87.86% 94.87% 97.45% 97.49% 98.93% 98.37% 96.22% 96.37% 96.06% 95.55% 87.47% 94.76% 97.53% 97.56% 99.13% 98.68% 96.23% 96.38% 96.11% 95.60% 90.20% 95.74% 97.32% 97.35% 99.13% 98.68% 96.53% 96.65% 96.22% 95.71% 90.09% 95.59% 97.32% 97.35% 99.03% 98.53% 96.50% 96.60% Specifically, the overall F1 score and accuracy of all evaluation frameworks remained below 80%, with only OpenCompass achieving an overall accuracy above 70%. This indicates that rule-based evaluation frameworks have significant limitations in generalization performance, struggling to effectively handle the diverse answers and evaluation sets from LLMs. Among the judge models, the best-performing ones remained GPT-4o and CompassJudger. However, all judge models except GPT-4o had an F1 score below 75% on math questions, with most models scoring below 50%, indicating that judge models almost entirely fail in evaluating more diverse and complex math problems. GPT-4o as Judge and GPT-4o as Judge (CoT) also failed to achieve an F1 score above 90% on math problems, suggesting that the math samples in the generalization set indeed present challenges for evaluation methods. Furthermore, GPT-4os performance did not improve after using CoT; instead, it showed slight decline. This suggests that in broader scenarios, CoT-based prompt engineering methods do not effectively improve GPT-4os performance as judge model, and model fine-tuning may be better option. In contrast, the xVerify-0.5B-I model outperformed all evaluation methods except GPT-4o, and the xVerify-3B-Ib model outperformed both CoT-based GPT-4o methods. For more difficult math problems, the F1 score and accuracy of the xVerify-14B-Ia and xVerify-32B-I models exceeded 90%. Additionally, we observed that as the parameter size of the xVerify model increased, the performance drop on the generalization set decreased. For example, the accuracy drop for xVerify-0.5B-I was 1.33%, 0.91% for xVerify-9B-I, and 0.80% for xVerify-32B-I, suggesting that larger xVerify models exhibit stronger generalization performance. Furthermore, we comprehensively evaluated the performance of 14 xVerify models on both the test and generalization sets, and tested the computational efficiency of all xVerify and judge models, along with the evaluation cost of GPT-4o as judge model. The results showed that xVerify models outperform other judge models in both usage cost and evaluation efficiency. Full experimental results can be found in Appendix E."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose an efficient answer verifier for reasoning model evaluations, named xVerify, which can effectively assess the correctness of long reasoning responses generated by 9 reasoning models on various difficult objective questions. To train and evaluate the xVerify model, we constructed the VAR dataset based on several popular LLMs and evaluation sets. This dataset primarily collects long reasoning responses generated by reasoning models on challenging questions, and multiple rounds of labeling and verification were conducted using GPT-4o and human annotators. Ultimately, we trained multiple xVerify models of varying specifications based on the VAR dataset and performed comparative evaluations with several evaluation frameworks and judge models on both the test and generalization sets. The experimental results show that even the smallest xVerify-0.5B-I model outperforms all methods except GPT-4o, and larger xVerify models surpass all other methods, demonstrating the effectiveness and generalization ability of xVerify."
        },
        {
            "title": "References",
            "content": "[1] Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, and Kai Chen. Compassjudger-1: All-in-one judge model helps model evaluation and evolution. arXiv preprint arXiv:2410.16256, 2024. [2] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145, 2024. [3] DeepSeek-AI, Daya Guo, Dejian Yang, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [4] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:10088 10115, 2023. [5] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, September 2021. [6] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, et al. The llama 3 herd of models, 2024. [7] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. survey on llm-as-a-judge, 2025. [8] Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, and Deyi Xiong. Evaluating large language models: comprehensive survey, 2023. [9] Chaoqun He, Renjie Luo, Shengding Hu, Yuanqian Zhao, Jie Zhou, Hanghao Wu, Jiajie Zhang, Xu Han, Zhiyuan Liu, and Maosong Sun. Ultraeval: lightweight platform for flexible and comprehensive evaluation for llms. arXiv preprint arXiv:2404.07584, 2024. [10] Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, and Bo Zheng. Can large language models detect errors in long chain-of-thought reasoning?, 2025. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. [12] Xinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, and Xiaojun Wan. Themis: reference-free nlg evaluation language model with flexibility and interpretability. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1592415951, 2024. [13] Greg Gandenberger Hynek Kydlíˇcek. GitHub - huggingface/Math-Verify: robust mathematical expression evaluation system designed for assessing Large Language Model outputs in mathematical tasks., 2024. 10 [14] Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or Honovich, Michael Tseng, Michael Collins, Roee Aharoni, and Mor Geva. chain-of-thought is as strong as its weakest link: benchmark for verifiers of reasoning chains. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46154634, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [15] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [16] Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. Critiquellm: Towards an informative critique generation model for evaluation of large language model generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. [17] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 43344353, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [18] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, and Huan Liu. From generation to judgment: Opportunities and challenges of llm-as-a-judge, 2025. [19] Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, and Huan Liu. Preference leakage: contamination problem in llm-as-a-judge, 2025. [20] Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. Llms-as-judges: comprehensive survey on llm-based evaluation methods, 2024. [21] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, hai zhao, and Pengfei Liu. Generative judge for evaluating alignment. In The Twelfth International Conference on Learning Representations, 2024. [22] Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai Li, Rong-Hua Li, Yi Wang, Zhonghao Wang, Feiyu Xiong, and Zhiyu Li. Internal consistency and self-feedback in large language models: survey, 2024. [23] Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, and Kai Chen. Are your llms capable of stable reasoning? arXiv preprint arXiv:2412.13147, 2024. [24] MAA. American invitational mathematics examination - aime. American Invitational Mathematics Examination - AIME 2024, February 2024. [25] OpenAI. GitHub - openai/evals: Evals is framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks., 2024. [26] OpenAI. Openai o3-mini, 2025. [27] OpenMMLab. Opencompass: universal evaluation platform for foundation models. https: //github.com/open-compass/opencompass, 2023. [28] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [29] Zhihong Shao, Peiyi Wang, Qihao Zhu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [30] Gemma Team, Morgane Riviere, Shreya Pathak, et al. Gemma 2: Improving open language models at practical size, 2024. [31] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [32] Xinpeng Wang, Bolei Ma, Chengzhi Hu, Leon Weber-Genzel, Paul Röttger, Frauke Kreuter, Dirk Hovy, and Barbara Plank. my answer is C: First-token probabilities do not match text answers in instruction-tuned language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 74077416, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [33] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. [34] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. 2024. [35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc., 2022. [36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc., 2022. [37] An Yang, Baosong Yang, Beichen Zhang, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [38] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. [39] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. JudgeLM: Fine-tuned large language models are scalable judges. In The Thirteenth International Conference on Learning Representations, 2025."
        },
        {
            "title": "Appendices",
            "content": "A Datasets and Models VAR Dataset Details B.1 Details of Training, Test, and Generalization Sets . . . . . . . . . . . . . . . . . . B.2 Details of Human Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Examples from the VAR Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . Model Training Details C.1 Training Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Original Model Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Prompts D.1 Prompts for Generating LLM Responses . . . . . . . . . . . . . . . . . . . . . . . D.2 Prompts for GPT-4o Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Prompts for Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Prompts for Judge Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Prompts for xVerify . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Supplementary Experimental Results E.1 Evaluation Accuracy Results of All xVerify Models . . . . . . . . . . . . . . . . . E.2 Computational Efficiency and Operational Cost of xVerify and Judge Models . . . 14 14 15 19 22 22 22 22 22 23 23 25 25"
        },
        {
            "title": "A Datasets and Models",
            "content": "This section will present the relevant information for all the public datasets and LLMs involved in the experiments of this paper. In this study, we employ total of 24 datasets, which are categorized into four primary types: multiplechoice questions (Choice), short answer questions (Short Answer), mathematical problems (Math), and classification tasks (Classification), as summarized in Table 3. To evaluate the multilingual capabilities of the xVerify model, each question type includes datasets in both Chinese and English, with one dataset featuring multilingual content. For each dataset, samples are partitioned into training and test sets following 2:1 ratio, with the training and test sets ideally comprising 2,000 and 1,000 instances, respectively. In certain cases, the number of available samples is below 3,000, or the official test set is not publicly available, resulting in reduced dataset sizes after preprocessing. Table 3: Datasets Description. The \"Type\" column indicates the question type in the corresponding dataset, including multiple-choice questions (Choice), short answer questions (Short Answer), math questions (Math), and classification questions (Classification)."
        },
        {
            "title": "Type",
            "content": "#Train #Test Language"
        },
        {
            "title": "License",
            "content": "Choice CMMLU Choice C-Eval Choice GPQA Choice MMLU Choice MMLU-Pro Choice MMLU-Redux Classification AgNews Classification Amazon Classification CLUEWSC Classification CMNLI Math AMC23 Math AIME 2024 Math CMATH GSM8K Math LiveMathBench Math Math MATH MGSM Math OlympiadBench Math ARC CHID C-SimpleQA DROP FRAMES SimpleQA"
        },
        {
            "title": "Short Answer\nShort Answer\nShort Answer\nShort Answer\nShort Answer\nShort Answer",
            "content": "2000 1346 794 1816 2000 2000 2000 2000 1548 2000 26 20 1128 2000 190 2000 1892 1787 2000 2000 2000 2000 550 2000 1000 260 398 1000 1000 1000 1000 1000 1000 1000 14 10 565 1000 93 1000 946 892 1000 1000 1000 1000 274 1000 CC-BY-NC-4.0 CC-BY-NC-SA-4.0 CC-BY-4.0 MIT MIT CC-BY-4.0 Unspecified Apache-2.0 Unspecified Apache-2.0 Unspecified MIT CC-BY-4.0 MIT Chinese Chinese English English English English English English Chinese Chinese English English Chinese English English & Chinese CC-BY-4.0 English Multilingual English & Chinese Apache-2.0 English Chinese Chinese English English English CC-BY-SA-4.0 Apache-2.0 CC-BY-NC-SA-4.0 CC-BY-SA-4.0 Apache-2.0 MIT MIT CC-BY-SA-4. total of 19 large language models (LLMs) are utilized in our experiments, encompassing diverse range of model sizes and types, with particular emphasis on reasoning models (see Table 4). These models are subsequently used to collect LLM-generated responses and to train the xVerify model."
        },
        {
            "title": "B VAR Dataset Details",
            "content": "This section will present detailed information about the components of the VAR dataset, the details of human annotations, and examples from the dataset. 14 Table 4: LLMs Description. LLMs are listed by release date. All models are chat or instruct type. \"NaN\" indicates that public data is unavailable. Model #Para. Type Publisher Date ChatGLM3-6B GPT-4o Gemma-2-2B-it Gemma-2-9B-it GLM-4-9B-Chat InternLM2.5-7B-Chat Qwen2-1.5B-Instruct Qwen2-7B-Instruct Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Phi-4 DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-14B QwQ-32B 6B NaN 2B 9B 9B 7B 1.5B 7B 8B 1B 3B 7B 14B 14B 8B 1.5B 7B 14B 32B"
        },
        {
            "title": "Microsoft\nDeepSeek\nDeepSeek\nDeepSeek\nDeepSeek",
            "content": "2023.10 2024.05 2024.06 2024.06 2024.06 2024.06 2024.06 2024.06 2024.07 2024.09 2024.09 2024.09 2024.09 2024.11 2025.01 2025.01 2025.01 2025.01 2025.03 B.1 Details of Training, Test, and Generalization Sets B.1.1 Training Set The training set comprises 43,204 samples. Tables 5 to 8 provide the sample counts corresponding to each LLM, dataset, prompt template, and question type. Note that datasets with names containing \"_enh\" refer to the augmented multiple choice question datasets. Table 5: Number of samples from each LLM in the training set. Model Sample Counts ChatGLM3-6B GPT-4o Gemma-2-2B-it Gemma-2-9B-it GLM-4-9B-Chat InternLM2.5-7B-Chat Qwen2-1.5B-Instruct Qwen2-7B-Instruct LLaMA-3.1-8B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-14B 2588 2691 2657 2600 2957 2935 2700 2898 2852 2854 2801 3223 3231 3075 3142 B.1.2 Test Set The test set comprises 6,122 samples. Tables 9 to 12 provide the sample counts corresponding to each LLM, dataset, prompt template, and question type. Note that datasets with names containing \"_enh\" refer to the augmented multiple choice question datasets. Table 6: Number of samples from each dataset in the training set. Dataset Sample Counts CMMLU CMMLU_enh GPQA GPQA_enh MMLU MMLU_enh MMLU-Pro MMLU-Pro_enh AgNews CLUEWSC AMC23 AIME 2024 CMATH GSM8K MATH MGSM OlympiadBench_en OlympiadBench_zh CHID C-SimpleQA DROP FRAMES 1557 1641 1587 1668 1520 1513 1394 1442 1751 5008 1625 1333 1893 1836 2485 1384 2573 2709 2424 1913 1928 2020 Table 7: Number of samples from each prompt template in the training set. Prompt Template Sample Counts 0-shot 0-shot-restrict 0-shot-cot 0-shot-cot-restrict 5-shot 5-shot-restrict 5-shot-cot 5-shot-cot-restrict 4884 5977 4907 6041 4774 5866 4916 5839 Table 8: Number of samples from each question type in the training set. Dataset Sample Counts Multiple Choice Math Short Answer Classification 12322 15838 8285 6759 16 Table 9: Number of samples from each LLM in the test set. Model Sample Counts ChatGLM3-6B GPT-4o Gemma-2-2B-it Gemma-2-9B-it GLM-4-9B-Chat InternLM2.5-7B-Chat Qwen2-1.5B-Instruct Qwen2-7B-Instruct LLaMA-3.1-8B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-14B 378 400 416 369 367 367 433 427 404 374 415 430 451 439 452 Table 10: Number of samples from each dataset in the test set. Dataset Sample Counts CMMLU CMMLU_enh GPQA GPQA_enh MMLU MMLU_enh MMLU-Pro MMLU-Pro_enh AgNews CLUEWSC AMC23 AIME 2024 CMATH GSM8K MATH MGSM OlympiadBench_en OlympiadBench_zh CHID C-SimpleQA DROP FRAMES 216 195 207 235 225 222 171 192 261 710 258 186 263 262 362 205 349 446 347 270 265 275 Table 11: Number of samples from each prompt template in the test set. Dataset Sample Counts Multiple Choice Math Short Answer Classification 1663 2331 1157 971 17 Table 12: Number of samples from each question type in the test set. Prompt Template Sample Counts 0-shot 0-shot-restrict 0-shot-cot 0-shot-cot-restrict 5-shot 5-shot-restrict 5-shot-cot 5-shot-cot-restrict 680 798 642 891 690 789 702 930 B.1.3 Generalization Set The generalization set comprises 6,468 samples. Tables 13 to 16 provide the sample counts corresponding to each LLM, dataset, prompt template, and question type. Note that datasets with names containing \"_enh\" refer to the augmented multiple choice question datasets. Table 13: Number of samples from each LLM in the generalization set. Model Sample Counts ChatGLM3-6B GPT-4o Gemma-2-2B-it Gemma-2-9B-it GLM-4-9B-Chat InternLM2.5-7B-Chat Qwen2-1.5B-Instruct Qwen2-7B-Instruct LLaMA-3.1-8B-Instruct LLaMA-3.2-1B-Instruct LLaMA-3.2-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Phi-4 DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-14B QwQ-32B 300 305 427 296 339 341 280 346 400 314 310 326 334 314 341 399 375 434 287 Table 14: Number of samples from each dataset in the generalization set. Dataset Sample Counts C-Eval C-Eval_enh MMLU-Redux MMLU-Redux_enh Amazon CMNLI LiveMathBench_en LiveMathBench_zh ARC SimpleQA 435 442 436 483 646 643 1127 821 807 628 18 Table 15: Number of samples from each prompt template in the generalization set. Dataset Sample Counts"
        },
        {
            "title": "Multiple Choice\nMath\nShort Answer\nClassification",
            "content": "1796 1948 1435 1289 Table 16: Number of samples from each question type in the generalization set. Prompt Template Sample Counts 0-shot 0-shot-restrict 0-shot-cot 0-shot-cot-restrict 5-shot 5-shot-restrict 5-shot-cot 5-shot-cot-restrict 703 856 772 915 690 885 756 B.2 Details of Human Annotation To ensure high-quality annotation for the VAR dataset, we assembled team of 8 annotators. Among them, 6 hold bachelors degrees and are primarily responsible for batch annotation tasks, while the other 2 hold masters degrees and focus on reviewing complex cases or resolving discrepancies in annotations made by multiple annotators. The gender ratio within the annotation team is balanced at 1:1. In terms of compensation, all annotators were paid according to the local industry average rates. The annotation process lasted for three weeks, covering total of 15 working days. Figure 3: Illustration of the Label Studio Interface. The detailed annotation guidelines are presented below. Figure 3 shows an example of the interface used in our annotation tool. Each sample to be annotated contains four fields: question, LLM output, correct answer, and answer range. The question type includes four categories: multiple choice, math, short answer, and classification. Annotators are required to judge whether the LLM output matches the correct answer based on the question, while the answer range serves as auxiliary reference information to support the decision-making process. The specific annotation instructions and criteria are as follows: 19 Answer evaluation criteria for different question types: Multiple Choice For multiple-choice questions, answer options may be labeled with letters (A, B, C, D, . . . ), Roman numerals (I, II, III, IV, . . . ), or Arabic numerals (1, 2, 3, 4, . . . ). The LLM output is considered correct if it provides: Only the correct option label; Only the correct option content; Both the correct label and content. In cases where the label and content are inconsistent, the content takes precedence. If the content is correct, the answer is marked as correct; if the content is incorrect, the answer is marked as incorrect, even if the option label is correct (see the final annotation example for reference). Short Answer Short-answer questions may require responses such as names, locations, numbers, dates, or full sentences. The evaluation criteria are: For concise answers (e.g., names, places, dates), strict string matching is required. For sentence-level answers, semantic consistency with the reference answer is required. For numerical answers, mathematical equivalence must be verified (e.g., 12000 and 12,000 are considered equivalent). Classification Classification questions come with fixed set of candidate answers. The LLM output must explicitly and exactly match the correct answer in this set to be judged as correct. Math For mathematical questions, the final answer in the LLM output must be mathematically equivalent to the reference answer. Evaluation criteria include: If an initial answer (ans1) is given but followed by derived final answer (ans2) through calculation, ans2 should be used for evaluation. If the LLM output or ground-truth answer is provided in LaTeX format and cannot be visually interpreted, LaTeX compiler should be used to determine equivalence. Special cases: Overly Long Responses If the LLM output is excessively long, use the final answer provided as the basis for judgment. If the response does not converge to clear answer (e.g., repeated changes or ambiguity), it should be marked as incorrect. Truncated Calculations In long responses where the final verification or calculation is truncated, it can be ignored. If clear answer was provided earlier, use it for evaluation; if not, mark the response as incorrect (see the second-to-last annotation example). Unfamiliar Domains If the correctness of the LLM response cannot be determined due to unfamiliar domain knowledge (e.g., mathematical expressions whose equivalence is hard to judge), the case may be skipped and will later be annotated by more qualified annotators. Irrelevant but Matched Answers If the LLM output is irrelevant to the question but coincidentally contains the correct final answer, it should be marked as incorrect. B.3 Examples from the VAR Dataset (cid:11) { \" dataset \": \" SimpleQA \" , \" question \": \" In which year did Fayaz . Malik ( an Indian pharmacologist , cancer biologist , and scientist ) receive the Young Scientist of the Year from the Council of Scientific and Industrial Research ?\" , \" question_type \": \" short answer \" , \" correct_answer \": \"2009\" , \" llm_output \": \" The year Fayaz . Malik received the Young Scientist of the Year award from the Council of Scientific and Industrial Research was 2001. textbackslash textbackslash nThe answer is 2001.\" , \" hu man _ju dgm nt _r su lt \": \" Incorrect \" } , { } , { } , { } (cid:10) \" dataset \": \" MMLU - Redux_enh \" , \" question \": \" If you wanted to find the global distribution of coal , you would use topographic map . \" question_type \": \" multiple choice \" , \" correct_answer \": \" III \" , \" llm_output \": \" The answer is ( III ) thematic map .\" , \" hu man _ju dgm nt _r su lt \": \" Correct \" ( IV ) location map .\" , ( III ) thematic map . ( ) reference map . Answer Choices : ( II ) \" dataset \": \" Amazon \" , \" question \": \" Lightsaber Action !: This game is absolutly the best game have ever seen ! there is little bit of violence , but most of the time you kill droids , not humans . My favorite weopon is the lightsaber , because you can deflect laser blasts . textbackslash Please identify the sentiment polarity of the sentence : positive or negative \" , \" question_type \": \" classification \" , \" correct_answer \": \" positive \" , \" llm_output \": \" The answer is positive .\" , \" hu man _ju dgm nt _r su lt \": \" Correct \" \" dataset \": \" GSM8K \" , \" question \": \" Daisy and Rose were enjoying their backyard pool with If there are 24 legs / paws in the pool , how many dogs their dogs . do Daisy and Rose have ?\" , \" question_type \": \" math \" , \" correct_answer \": \"5\" , \" llm_output \": \" To find the number of dogs Daisy and Rose have , we need to first determine the total number of people and then subtract that from the total number of legs / paws . nSince each person has 2 legs , we can divide the total number of legs / paws by 2 to find the number of people : n24 legs / paws / 2 legs / person = 12 people nSince Daisy and Rose are the only people in the pool , we can subtract them from the total number of people to find the number of dogs : n12 people - 2 people = 10 dogs nTherefore , Daisy and Rose have 10 dogs .\" , \" hu man _ju dgm nt _r su lt \": \" Incorrect \" 21 (cid:8) (cid:9)"
        },
        {
            "title": "C Model Training Details",
            "content": "This section will further present additional information about the training of the xVerify model. C.1 Training Hyperparameters The xVerify model is trained using the QLoRA method, with consistent hyperparameter settings across all base models. The training is carried out on multiple GPU servers. Table 17 presents the key training hyperparameters. Table 17: Hyperparameter settings for model training. Hyperparameter Setting Per Device Train Batch Size Gradient Accumulation Steps Learning Rate Num Train Epochs LrScheduler Type Warmup Ratio Bf16 Ddp Timeout Lora Rank 1 8 1.0e-4 1.0 cosine 0.1 true 180000000 8 C.2 Original Model Details This paper uses 14 original models of different parameter scales and types for training on the VAR dataset. Table 18 presents the relevant information for all xVerify models and their corresponding original models. Table 18: Details of Original Models and Corresponding xVerify Models. Sorted by Original Model Name. Original Model #Para. Type Context Length xVerify Model Gemma-2-2B-it Gemma-2-9B-it Gemma-2-27B-it GLM-4-9B-Chat Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Llama-3.1-8B-Instruct Phi-4 Qwen2.5-0.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct 2B 9B 27B 9B 1B 3B 8B 14B 0.5B 1.5B 3B 7B 14B 32B Instruct Instruct Instruct Chat Instruct Instruct Instruct Instruct Instruct Instruct Instruct Instruct Instruct Instruct 8K 8K 8K 128K 128K 128K 128K 16k 128K 128K 128K 128K 128K 128K xVerify-2B-I xVerify-9B-I xVerify-27B-I xVerify-9B-C xVerify-1B-I xVerify-3B-Ia xVerify-8B-I xVerify-14B-Ib xVerify-0.5B-I xVerify-1.5B-I xVerify-3B-Ib xVerify-7B-I xVerify-14B-Ia xVerify-32B-I"
        },
        {
            "title": "D Prompts",
            "content": "This section will present all the prompt templates used in the experiments of this paper. D.1 Prompts for Generating LLM Responses The prompt templates used to generate LLM responses are illustrated in Figures 4 to 7. Each template consists of four fields that need to be populated: \"task_type\", \"task_description\", \"examples\", and 22 \"question\". The \"task_type\" and \"task_description\" fields are determined based on the type of question. For instance, for questions from the GPQA dataset, \"task_type\" is set to \"multidisciplinary question\", and \"task_description\" is set to \"Please choose the answer from options to D, corresponding to the question.\" During dataset preprocessing, we design appropriate \"task_type\" and \"task_description\" values for each dataset. The \"examples\" field is filled according to the selected prompting strategy, either 0-shot or 5-shot. In the 0-shot setting, this field is left empty, while in the 5-shot setting, it is populated with five example question-answer pairs that are similar to the target \"question\". The \"question\" field contains the specific query to be answered by the LLM. Examples of the \"examples\" and \"question\" fields are shown in Figures 8 and 9, respectively. You are an expert in {task_type}, {task_description} {examples} {question} Figure 4: Few-shot prompt for generating LLM responses. You are an expert in {task_type}, {task_description} {examples} {question} End your final answer with The answer is <answer>. Figure 5: Few-shot-restrict prompt for generating LLM responses. You are an expert in {task_type}, {task_description} {examples} {question} Lets think step by step. Figure 6: Few-shot-cot prompt for generating LLM responses. D.2 Prompts for GPT-4o Annotation The prompt templates used for annotating the collected LLM question-answer pairs with GPT-4o during the construction of the VAR dataset are shown in Figures 10 and 11. Both of these prompt templates employ the Chain-of-Thought (CoT) strategy to ensure the accuracy of the annotations generated by GPT-4o. D.3 Prompts for Data Augmentation In constructing the VAR dataset, two prompt templates used to guide GPT-4o in augmenting mathematical question samples are presented in Figures 12 and 13. D.4 Prompts for Judge Model In the experiments of this paper, the prompts used for all judge models were constructed based on the official templates provided by their respective developers. However, for some judge models, the official prompt templates were not fully compatible with the evaluation tasks in this paper, so other similar prompt templates were used. Specifically, Figure 14 shows the prompt template used by GPT-4o as Judge, Figure 15 shows the prompt template used by GPT-4o as Judge (CoT), Figure 16 shows the prompt template used by JudgeLM series models and PandaLM-7B-v1, Figure 17 shows the prompt template used by Auto-J series models, and Figure 18 shows the prompt template used 23 You are an expert in {task_type}, {task_description} {examples} {question} Lets think step by step. End your final answer with The answer is <answer>. Figure 7: Few-shot-cot-restrict prompt for generating LLM responses. ***** Start In-Context Examples ***** Q: late game rally by Washington led them to the Eagles 26 yard line. shot to the end zone by Robert Griffin III would be intercepted by Brandon Boykin, clinching an Eagles win. The Eagles would move to 6-5. This is the Eagles first win at Lincoln Financial Field since Week 4 of the 2012 season, because prior to this game, the Eagles had never won game in their home stadium in 414 days since that same week, snapping 10-game losing streak at home with this win. How many more wins than losses did the Eagles have after this game? A: The answer is 1. Q: The population of Sevastopol proper is 418,987 (01.01.16), making it the largest in the Crimean Peninsula. The citys agglomeration has about 600,000 people (2015). According to the Ukrainian Census (2001), the ethnic groups of Sevastopol include Russians (71.6%), Ukrainians (22.4%), Belarusians (1.6%), Tatars (0.7%), Crimean Tatars (0.5%), Armenians (0.3%), Jews (0.3%), Moldovans (0.2%), and Azerbaijani people (0.2%). Which ethnic has higher percentage of the population in Sevastopol: Russians or Armenians? A: The answer is Russians. Q: the most common crimes in the ACT are property related crimes, unlawful entry with intent and motor vehicle theft. They affected 2,304 and 966 people (580 and 243 per 100,000 persons respectively). Homicide and related offencesmurder, attempted murder and manslaughter, but excluding driving causing death and conspiracy to murderaffect 1.0 per 100,000 persons, which is below the national average of 1.9 per 100,000. Rates of sexual assault (64.4 per 100,000 persons) are also below the national average (98.5 per 100,000). Which was there higher national average for, homicide and related offences or sexual assault? A: The answer is sexual assault. Q: In the county, the population was spread out with 21.7% under the age of 18, 8.5% from 18 to 24, 26.9% from 25 to 44, 27.7% from 45 to 64, and 15.0% who were 65 years of age or older. The median age was 40 years. For every 100 females, there were 94.4 males. For every 100 females age 18 and over, there were 98.7 males. How many percent were not from 45 to 64? A: The answer is 72.3. Q: The median age in the city was 35.1 years. 24.2% of residents were under the age of 18; 7.9% were between the ages of 18 and 24; 33.8% were from 25 to 44; 24.6% were from 45 to 64; and 9.5% were 65 years of age or older. The gender makeup of the city was 48.6% male and 51.4% females. How many more people, in terms of percentage, were in the largest age group compared to the second smallest? A: The answer is 24.3. ***** End In-Context Examples ***** Figure 8: Example of \"examples\" fields. 24 Q: Let ABCD be tetrahedron such that AB = CD = 80, and BC = AD = 89. There exists point inside the tetrahedron such that the distances from to each of the faces of the tetrahedron are all equal. This distance can be written in the form , where m, n, and are positive integers, and are relatively prime, and is not divisible by the square of any prime. Find + + p. A: 41, AC = BD = Figure 9: Example of \"question\" fields. You are diligent and precise assistant tasked with evaluating the correctness of responses. Think step by step as you make your evaluation. You will receive question, an output sentence, and the correct answer. Your task is to determine if the output sentence accurately answers the question based on the provided correct answer. Think step by step and respond with either [Correct] or [Incorrect]. - Special considerations: 1. **Multiple Answers**: If the output contains multiple answers, evaluate whether later answers modify or correct earlier ones. In such cases, compare the final answer with the correct answer. If the final answer is unclear or incorrect, respond with [Incorrect]. 2. **Mathematical Problems**: If the formats differ but the answers are mathematically equivalent, respond with [Correct]. 3. **Explicit Options**: If the question provides explicit candidate answers, the output will be considered correct if it clearly indicates the correct options code or the correct options content. 4. **No Explicit Options**: If the question does not provide explicit options, the output must align with the correct answer in content and meaning to be considered [Correct]. Please present your response in the following JSON format: { \"reasoning\": \"Your step-by-step reasoning here.\", \"judgment\": \"Correct or Incorrect\" } - Question: \"\"\"{question}\"\"\" Output sentence: \"\"\"{output}\"\"\" Correct answer: {answer} Figure 10: Prompt for GPT-4o annotation. by Prometheus series models. The official prompt template for the CompassJudger-1 series models corresponds to pairwise evaluation, so the prompt template used by this series is the same as that for the xVerify model, as shown in Figure 19. D.5 Prompts for xVerify Figure 19 shows the prompt template used to construct the input for the xVerify model. This template is used both for training and evaluation of the xVerify model. Specifically, \"question,\" \"output,\" and \"answer\" correspond to the question content, the LLM response, and the reference answer, respectively."
        },
        {
            "title": "E Supplementary Experimental Results",
            "content": "E.1 Evaluation Accuracy Results of All xVerify Models Tables 19 and 20 present the performance of all 14 xVerify models on the test set and the generalization set, respectively. Overall, each xVerify model achieves an F1 score and accuracy exceeding 96.5% 25 You are diligent and precise assistant tasked with evaluating the correctness of responses. Think step by step as you make your evaluation. We request your feedback on whether the models response correctly answers the user question above. Follow these steps to make your evaluation: 1. Think step by step: Read the user question carefully. 2. Think step by step: Review the reference answer and understand the key points it covers. 3. Think step by step: Compare the models answer with the reference answer. 4. Think step by step: Determine if the models answer addresses the key points in the reference answer and correctly answers the question. - First, provide your reasoning in detail. Then, clearly state your judgment as either \"Correct\" or \"Incorrect.\" Please present your response in the following JSON format: { \"reasoning\": \"Your step-by-step reasoning here.\", \"judgment\": \"Correct or Incorrect\" } - Question: {question} Reference Answer: {answer} Models Answer: {output} Figure 11: Prompt II for GPT-4o annotation. on the test set and over 95.52% on the generalization set. These results demonstrate not only the effectiveness of the xVerify models for evaluation tasks but also the high quality of the VAR dataset. comparison between the results on the two datasets shows that the performance on the generalization set experiences slight decline relative to the test set, with the decrease not exceeding 1.6%. Moreover, models with larger parameter sizes exhibit smaller performance drops. This indicates that the xVerify models possess strong generalization capabilities, which further improve with an increase in parameter scale. Additionally, it is observed across both datasets that while the performance of xVerify models generally enhances with the increment of parameter size, beyond certain threshold, further increases in parameter scale do not lead to additional performance gains. Table 19: Evaluation Accuracy Results on the Test Set: All xVerify Models. The best performance in each column is shown in bold, and the second-best performance is underlined. Classification Math Multiple Choice Short Answer Total xVerify Model F1 Acc. Acc. F1 Acc. F1 Acc. Acc. xVerify-0.5B-I xVerify-1B-I xVerify-1.5B-I xVerify-2B-I xVerify-3B-Ia xVerify-3B-Ib xVerify-7B-I xVerify-8B-I xVerify-9B-C xVerify-9B-I xVerify-14B-Ia xVerify-14B-Ib xVerify-27B-I xVerify-32B-I 97.78% 97.90% 93.74% 94.64% 96.72% 97.49% 99.71% 99.59% 96.69% 96.85% 97.22% 97.35% 94.76% 95.45% 96.06% 96.97% 99.71% 99.59% 96.77% 96.91% 97.85% 97.96% 95.10% 95.75% 96.05% 96.97% 99.63% 99.49% 97.05% 97.17% 97.93% 98.02% 95.06% 95.71% 96.06% 96.97% 99.78% 99.69% 97.09% 97.21% 97.73% 97.84% 95.00% 95.67% 96.17% 97.06% 99.71% 99.59% 97.02% 97.14% 97.31% 97.41% 95.65% 96.18% 96.38% 97.23% 99.78% 99.69% 97.17% 97.27% 97.75% 97.84% 95.94% 96.44% 96.51% 97.32% 99.78% 99.69% 97.41% 97.50% 97.92% 98.02% 95.34% 95.97% 96.05% 96.97% 99.71% 99.59% 97.17% 97.29% 98.29% 98.38% 95.26% 95.88% 96.06% 96.97% 99.78% 99.69% 97.25% 97.37% 97.43% 97.53% 95.75% 96.27% 96.06% 96.97% 99.78% 99.69% 97.19% 97.29% 97.49% 97.59% 95.73% 96.22% 95.41% 96.46% 99.63% 99.49% 97.06% 97.16% 97.67% 97.78% 96.10% 96.57% 95.74% 96.72% 99.71% 99.59% 97.31% 97.40% 97.81% 97.90% 95.46% 96.01% 96.19% 97.06% 99.56% 99.38% 97.15% 97.26% 97.81% 97.90% 95.88% 96.31% 96.18% 97.06% 99.71% 99.59% 97.32% 97.40% E.2 Computational Efficiency and Operational Cost of xVerify and Judge Models Table 21 displays the running time performance of the xVerify model and other judge models. Each model was evaluated using 200 randomly selected samples per question type from the generalization set, with running times measured in seconds. This data provides insights into the computational You are an expert in mathematical calculations and data expressions. You are required to provide different equivalent forms of the standard answer for the following math problem. Problem: {question} Answer: {answer} Example 1: Problem: Let $alpha$ be the radian measure of the smallest angle in $3-4-5$ right triangle. Let $beta$ be the radian measure of the smallest angle in $7-24-25$ right triangle. Express $beta$ in terms of $alpha$. Answer: $frac{pi}{2} - 2alpha$ Output: json { \"answer1\": \"pi/2 - 2alpha\", \"answer2\": \"pi/2 - 2alpha\", \"answer3\": \"pi/2 - 2 * alpha\", \"answer4\": \"0.5 * pi - 2 * alpha\" } Example 2: Problem: volcano erupts and spews ash into the sky. The ash cloud spreads out in diameter eighteen times as far as the distance it shot up into the sky. If the ashes erupted three hundred feet into the sky, what was the radius of the ash cloud in feet? Answer: 2700 Output: json { \"answer1\": \"2.710^3\", \"answer2\": \"2700.0\", \"answer3\": \"2.7 times 10^3\", \"answer4\": \"$2.7 times 10^3$\", \"answer5\": \"Two thousand seven hundred\" } Please note: 1. You need to provide 3 to 5 different standard forms of the answer. 2. Each different form must be equivalent to the standard answer, i.e., it should still be correct and valid answer. 3. You may use LaTeX, scientific notation, or other standard mathematical expressions. 4. Please follow the JSON format below for the output: json { \"answer1\": \"xxx\", \"answer2\": \"xxx\", \"answer3\": \"xxx\", ... } Figure 12: Prompt for Generating Alternative Reference Answers. 27 You are an expert in mathematical calculations and data expressions. For an answer to specific mathematical problem, you are required to provide equivalent and different expressions of the mathematical result. Answer: {output} Example 1: Answer: The answer is $beta = frac{pi}{2} - 2alpha$. Output: json { \"answer1\": \"The answer is pi/2 - 2alpha.\", \"answer2\": \"The answer is pi/2 - 2alpha.\", \"answer3\": \"The answer is pi/2 - 2 * alpha.\", \"answer4\": \"The answer is 0.5 * pi - 2 * alpha.\" } Example 2: Answer: The answer is 2700 feet. Output: json { \"answer1\": \"The answer is 2.710^3 feet.\", \"answer2\": \"The answer is 2700.0 feet.\", \"answer3\": \"The answer is 2.7 times 10^3 feet.\", \"answer4\": \"The answer is $2.7 times 10^{3}$ feet.\", \"answer5\": \"The answer is Two thousand seven hundred feet.\" } Please note: 1. You need to provide 3 to 5 different expressions, each replacing the mathematical result with an equivalent and different form. 2. Each expression must be exactly equivalent to the target answer to ensure its correctness. 3. You can use LaTeX, scientific notation, or other standard mathematical formats. 4. Please output the result in the following JSON format: json { \"answer1\": \"The answer is xxx\", \"answer2\": \"The answer is xxx\", \"answer3\": \"The answer is xxx\", \"answer4\": \"The answer is xxx\", \"answer5\": \"The answer is xxx\" } Figure 13: Prompt for Generating Diverse Final Answer Expressions. 28 You are diligent and precise assistant tasked with evaluating the correctness of responses. You will receive question, an output sentence, and the correct answer. Your task is to determine if the output sentence accurately answers the question based on the provided correct answer. Respond with either [Correct] or [Incorrect]. - Special considerations: 1. **Multiple Answers**: If the output contains multiple answers, evaluate whether later answers modify or correct earlier ones. In such cases, compare the final answer with the correct answer. If the final answer is unclear or incorrect, respond with [Incorrect]. 2. **Mathematical Problems**: If the formats differ but the answers are mathematically equivalent, respond with [Correct]. 3. **Explicit Options**: If the question provides explicit candidate answers, the output will be considered correct if it clearly indicates the correct options code or the correct options content. 4. **No Explicit Options**: If the question does not provide explicit options, the output must align with the correct answer in content and meaning to be considered [Correct]. Please present your response in the following JSON format: { \"judgement\": \"Correct or Incorrect\" } - Question: \"\"\"{question}\"\"\" Output sentence: \"\"\"{response}\"\"\" Correct answer: {reference} Figure 14: Prompt for GPT-4o as Judge. You are diligent and precise assistant tasked with evaluating the correctness of responses. Think step by step as you make your evaluation. You will receive question, an output sentence, and the correct answer. Your task is to determine if the output sentence accurately answers the question based on the provided correct answer. Think step by step and respond with either [Correct] or [Incorrect]. - Special considerations: 1. **Multiple Answers**: If the output contains multiple answers, evaluate whether later answers modify or correct earlier ones. In such cases, compare the final answer with the correct answer. If the final answer is unclear or incorrect, respond with [Incorrect]. 2. **Mathematical Problems**: If the formats differ but the answers are mathematically equivalent, respond with [Correct]. 3. **Explicit Options**: If the question provides explicit candidate answers, the output will be considered correct if it clearly indicates the correct options code or the correct options content. 4. **No Explicit Options**: If the question does not provide explicit options, the output must align with the correct answer in content and meaning to be considered [Correct]. Please present your response in the following JSON format: { \"reasoning\": \"Your step-by-step reasoning here.\", \"judgement\": \"Correct or Incorrect\" } - Question: \"\"\"{question}\"\"\" Output sentence: \"\"\"{response}\"\"\" Correct answer: {reference} Figure 15: Prompt for GPT-4o as Judge (CoT). 29 You are helpful and precise assistant for checking the quality of the answer. [Question] {question} [Reference Answer] {reference} [Models Answer] {response} [System] We would like to request your feedback on the performance of the models response to the user question displayed above. Based on the reference answer, please rate the accuracy of the response. The model receives an overall score on scale of 1 to 10, where higher score indicates better overall performance. Please first output single line containing only the score. In the subsequent line, please provide comprehensive explanation of your evaluation, avoiding any potential bias. ### Response: Figure 16: Prompt for JudgeLM. [INST] Write critiques for submitted response on given users query, incorporating the correct answer as reference, and grade the response accordingly: [BEGIN DATA] *** [Query]: {question} *** [Correct Answer]: {reference} *** [Response]: {response} *** [END DATA] Write critiques for this response. After that, you should give final rating for the response on scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\". [/INST] Figure 17: Prompt for Auto-J. Table 20: Evaluation Accuracy Results on the Generalization Set: All xVerify Models. The best performance in each column is shown in bold, and the second-best performance is underlined. xVerify Model xVerify-0.5B-I xVerify-1B-I xVerify-1.5B-I xVerify-2B-I xVerify-3B-Ia xVerify-3B-Ib xVerify-7B-I xVerify-8B-I xVerify-9B-C xVerify-9B-I xVerify-14B-Ia xVerify-14B-Ib xVerify-27B-I xVerify-32B-I Multiple Choice Math Short Answer Classification Total F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. 96.49% 96.10% 80.00% 91.94% 96.95% 97.00% 99.03% 98.53% 95.29% 95.53% 96.10% 95.66% 82.45% 92.51% 97.32% 97.35% 98.92% 98.37% 95.43% 95.62% 96.76% 96.38% 83.58% 93.12% 97.46% 97.49% 98.88% 98.29% 95.85% 96.03% 96.27% 95.82% 82.11% 92.51% 97.60% 97.63% 98.98% 98.45% 95.57% 95.75% 96.44% 95.99% 86.10% 94.25% 97.31% 97.35% 99.03% 98.53% 96.11% 96.27% 96.21% 95.71% 86.20% 94.15% 97.60% 97.63% 99.03% 98.53% 96.08% 96.23% 96.16% 95.66% 87.86% 94.87% 97.45% 97.49% 98.93% 98.37% 96.22% 96.37% 96.67% 96.27% 86.76% 94.61% 97.45% 97.49% 99.03% 98.53% 96.33% 96.49% 97.00% 96.66% 87.08% 94.71% 97.45% 97.49% 98.98% 98.45% 96.45% 96.61% 96.06% 95.55% 87.47% 94.76% 97.53% 97.56% 99.13% 98.68% 96.23% 96.38% 96.11% 95.60% 90.20% 95.74% 97.32% 97.35% 99.13% 98.68% 96.53% 96.65% 96.35% 95.88% 87.88% 94.92% 97.45% 97.49% 98.93% 98.37% 96.30% 96.44% 96.01% 95.49% 85.64% 93.99% 97.32% 97.35% 99.13% 98.68% 95.93% 96.09% 96.22% 95.71% 90.09% 95.59% 97.32% 97.35% 99.03% 98.53% 96.50% 96.60% efficiency of each model under uniform testing conditions, thereby facilitating comparative analysis of their real-time processing capabilities and scalability in practical applications. 30 You are fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.\" ###Task Description: An instruction (might include an Input inside it), response to evaluate, reference answer that gets score of 5, and score rubric representing evaluation criteria are given. 1. Write detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. 2. After writing feedback, write score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as follows: \"Feedback: (write feedback for criteria) [RESULT] (an integer number between 1 and 5)\" 4. Please do not generate any other opening, closing, and explanations. ###The instruction to evaluate: {question} ###Response to evaluate: {response} ###Reference Answer (Score 5): {reference} ###Score Rubrics: [Does the model demonstrate logical and effective reasoning in its responses?] Score 1: The models responses show complete lack of logical reasoning, often resulting in irrelevant or nonsensical answers. Score 2: The model occasionally shows signs of logical reasoning but generally struggles to provide coherent or relevant responses. Score 3: The model usually demonstrates basic reasoning capabilities, though it may not consistently apply logical principles or fully resolve complex issues. Score 4: The model frequently exhibits strong reasoning skills, effectively addressing complex questions with minor inconsistencies or errors. Score 5: The model consistently demonstrates advanced reasoning abilities, providing logically sound, coherent, and sophisticated responses to complex queries. ###Feedback: Figure 18: Prompt for Prometheus. You are diligent and precise assistant tasked with evaluating the correctness of responses. You will receive question, an output sentence, and the correct answer. Your task is to determine if the output sentence accurately answers the question based on the provided correct answer. Respond with either [Correct] or [Incorrect]. - Special considerations: 1. **Multiple Answers**: If the output contains multiple answers, evaluate whether later answers modify or correct earlier ones. In such cases, compare the final answer with the correct answer. If the final answer is unclear or incorrect, respond with [Incorrect]. 2. **Mathematical Problems**: If the formats differ but the answers are mathematically equivalent, respond with [Correct]. 3. **Explicit Options**: If the question provides explicit candidate answers, the output will be considered correct if it clearly indicates the correct options code or the correct options content. 4. **No Explicit Options**: If the question does not provide explicit options, the output must align with the correct answer in content and meaning to be considered [Correct]. - Question: \"\"\"{question}\"\"\" Output sentence: \"\"\"{output}\"\"\" Correct answer: {answer} Judgement: Figure 19: Prompt for xVerify. 31 All models were executed on GPUs with identical configurations. Specifically, Prometheus-8x7Bv2.0, JudgeLM-33B-v1.0, CompassJudger-1-32B, xVerify-27B-I, and xVerify-32B-I were deployed on two GPUs for inference, while the remaining models were deployed on single GPU. From Table 21, it is evident that all xVerify models exhibit an overall average runtime within 100 seconds, whereas the overall average runtime for the other judge models exceeds 100 seconds. Moreover, for each question category, the models with the shortest evaluation times are the xVerify models. Thus, the xVerify models demonstrably surpass the other judge models in terms of evaluation efficiency. Table 22 presents the evaluation costs incurred when employing GPT-4o as the judge, based on assessments of 200 randomly selected samples per question type, along with the overall expenditure. Apart from the prerequisite deployment overhead, the cost of invoking the xVerify models for evaluation is substantially lower than that of GPT-4o. Additionally, compared to GPT-4o, which relies on remote server deployment, the locally deployed xVerify models offer higher invocation efficiency. Taken together, these results underscore that the xVerify models outperform the other judge models in both usage cost and evaluation efficiency. Table 21: Running Time Comparison of xVerify Models and Other Judge Models (200 Samples per Question Type). The best performance in each column is shown in bold, and the second-best performance is underlined. Method Short Answer (s) Classification (s) Multiple Choice (s) Math (s) Method Type Avg (s) Judge Model xVerify PandaLM-7B-v1 Auto-J-Bilingual-6B Auto-J-13B Prometheus-7B-v2.0 Prometheus-8x7B-v2.0 JudgeLM-7B-v1.0 JudgeLM-13B-v1.0 JudgeLM-33B-v1.0 CompassJudger-1-1.5B CompassJudger-1-7B CompassJudger-1-14B CompassJudger-1-32B xVerify-0.5B-I xVerify-1B-I xVerify-1.5B-I xVerify-2B-I xVerify-3B-Ia xVerify-3B-Ib xVerify-7B-I xVerify-8B-I xVerify-9B-C xVerify-9B-I xVerify-14B-Ia xVerify-14B-Ib xVerify-27B-I xVerify-32B-I 304.50 1,570.44 3,055.00 1,173.80 1,557.10 551.88 777.73 1,041.83 189.45 163.96 346.80 147.53 38.97 33.91 43.05 38.44 38.54 46.93 68.24 78.06 131.07 54.20 59.18 61.17 85.28 131.05 76.24 1,802.71 3,622.70 947.71 1,128.08 469.10 598.19 1,018.37 244.08 568.72 571.66 258.10 41.25 36.63 46.87 73.16 44.54 53.58 95.50 61.57 70.16 69.91 114.91 145.19 89.41 98. 76.97 1,194.08 2,807.23 706.74 1,132.84 394.57 564.25 789.80 139.50 450.20 217.86 133.59 39.12 33.44 42.17 39.29 37.11 106.06 50.66 45.34 51.66 49.41 55.50 116.43 58.99 64.74 65.79 1,148.32 1,903.00 696.34 750.51 348.05 529.60 762.99 110.95 80.58 196.18 152.11 38.87 33.47 42.08 37.38 43.02 47.84 51.67 46.82 52.57 51.06 54.80 57.55 61.00 67.45 130.88 1,428.89 2,846.98 881.15 1,142.13 440.90 617.44 903.25 171.00 315.87 333.13 172.83 39.55 34.36 43.54 47.07 40.80 63.60 66.52 57.95 76.37 56.15 71.10 95.09 73.67 90. Table 22: Total costs (in USD) of GPT-4o as Judge (200 Samples per Question Type). Method Multiple Choice ($) Math ($) Short Answer ($) Classification ($) Total ($) GPT-4o as Judge GPT-4o as Judge (CoT) 0.31 0. 0.66 1.00 0.24 0.42 0.27 0.48 1.48 2."
        }
    ],
    "affiliations": [
        "Center for Data Science, Peking University",
        "MemTensor (Shanghai) Technology Co., Ltd.",
        "Research Institute of China Telecom, Beijing, China"
    ]
}