{
    "paper_title": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation",
    "authors": [
        "Luca Moroni",
        "Giovanni Puccetti",
        "Pere-Lluis Huguet Cabot",
        "Andrei Stefan Bejgu",
        "Edoardo Barba",
        "Alessio Miaschi",
        "Felice Dell'Orletta",
        "Andrea Esuli",
        "Roberto Navigli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token \"fertility\") and slower inference speed. In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing token fertility by 25\\%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks."
        },
        {
            "title": "Start",
            "content": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation Luca Moroni1*, Giovanni Puccetti2, Pere-Lluis Huguet Cabot1, Andrei Stefan Bejgu4 Edoardo Barba1, Alessio Miaschi3 Felice DellOrletta3, Andrea Esuli2, Roberto Navigli1 1Sapienza University of Rome {surname}@diag.uniroma1.it 2ISTI-CNR {name.surname}@isti.cnr.it 3ILC-CNR {name.surname}@ilc.cnr.it 4Babelscape {surname}@babelscape.com 5 2 0 2 3 2 ] . [ 1 5 2 0 7 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for nonEnglish languages, leading to inefficient encoding (high token \"fertility\") and slower inference speed. In this work, we thoroughly compare variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7B-v0.1, reducing token fertility by 25%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have gained immense popularity and are increasingly being utilized across wide range of applications (Radford et al., 2019; Kojima et al., 2022). Despite their impressive performance, these models are mainly English-centric, that is, most state-of-the-art models are designed and pre-trained on datasets with primary focus on English (Jiang et al., 2023; Dubey et al., 2024; Mesnard et al., 2024). Although native multilingual models i.e. fully pre-trained * Those authors contributed equally. 1We release our code and models at https://github. com/SapienzaNLP/sava Figure 1: Fertility for two different tokenizers, Mistral7B-v0.1 (left) and Minerva (right), over Italian texts from CulturaX (blue) and Wikipedia (red). in multiple target languages have been released over the years (Le Scao et al., 2023), they still fall short of achieving performance levels comparable to models pre-trained in English. The primary challenge is addressing underrepresented languages, where large, clean, open-access text corpora are often scarce (Weber et al., 2024; Nguyen et al., 2024). This scarcity is problematic because models require vast amounts of high-quality data to achieve satisfactory performance (Hoffmann et al., 2022). Moreover, multilingual models generally reach suboptimal performance due to the well-known curse of multilinguality (Conneau et al., 2020). promising solution to these challenges is the adaptation of pretrained English LLMs to other languages (Chau et al., 2020). Recent studies highlight that fine-tuning English-centric models to support other languages yields substantial benefits, allowing for efficient adaptation while minimizing computational resources and training time. This method reduces both the training budget and the number of tokens required, demonstrating competitive performance even in low-resource scenarios (Koto et al., 2021; Minixhofer et al., 2022; Gee et al., 2022; Ostendorff and Rehm, 2023). Another important aspect, alongside the downstream performance of language models, is the tokenizers fertility in target languages. LLMs rely on tokenizer, which is trained on mix of text (either LLMs training data or not), transforming raw text into word-piece tokens; fertility is the average number of tokens in which word is split (Brown et al., 1993). The fertility of tokenizer is highly sensitive to the language and the type of text it was trained on, as well as the text on which the fertility itself is measured. Figure 1 shows an example of this phenomenon comparing the fertility of Minerva-LLMs, family of Italian-first LLMs (Orlando et al., 2024), and Mistral-7B-v0.1, an English-first LLM (Jiang et al., 2023), on two corpora in Italian. In this work, we explore the adaptation of two state-of-the-art English LLMs to the Italian language using both vocabulary adaptation and continual learning. Additionally, we introduce novel vocabulary adaptation technique called Semantic Alignment Vocabulary Adaptation (SAVA) and conduct comprehensive comparison with recent approaches (Gee et al., 2022; Ostendorff and Rehm, 2023), examining the impact of vocabulary substitution on model performance throughout the adaptation process. After the adaptation of the vocabulary, when tokenizing Italian texts, we are able to reduce the fertility of Mistral-7B-v0.1 by 25% and of Llama-3.1-8B by 16%. As regards Mistral7B-v0.1, we do not increase its vocabulary size or model parameters, while for Llama-3.1-8B, we effectively reduce its vocabulary size by 75% thereby reducing the final model size by 10%. Overall, we reduce memory and compute footprint of the models. To summarize, the main contributions are: Introducing an effective approach for adapting tokenizers and vocabularies of generative models, leading to competitive performance over existing methods across several downstream benchmarks; Providing detailed comparative analysis of various tokenizer adaptation techniques, with focus on continual training in lowto midresource scenarios. Analyzing the embedding representations learned through different adaptation techniques, offering deeper understanding of how vocabulary modifications impact model performance and generalization."
        },
        {
            "title": "2 Related Work",
            "content": "languages. However, Language-Adaptive Pretraining Designing LLMs in target language and thus training them from scratch is the best approach to obtain an adequate token fertility from the outset and minimize interference from pretrained data on different this approach is often impractical, especially in low-resource settings and on low computational budget. For this reason, several recent studies (de Vries and Nissim, 2021; Gee et al., 2022; Csaki et al., 2024) have focused on the adaptation of pretrained LLMs to new languages. Pretrained LLMs can be adapted to specific language using small quantity data compared to what is needed in the pretraining stage. straightforward approach to achieving this is Language-Adaptive Pre-Training (LAPT), utilized by Chau et al. (2020) in multilingual setting where they tested continual training of multilingual LLMs on target languages. Interestingly, LAPT was previously proposed on encoder-only architectures by Gururangan et al. (2020), where they successfully adapted RoBERTa (Zhuang et al., 2021) models in biomedical domain. In LAPT, models do not undergo any structural change to their architecture. This usually results in performance improvements, however it does not address the limitations of using sub-optimal tokenizer that is less suited to the encoding of different languages. Regarding LAPT research in English-to-Italian models, there have been several attempts, most notably LLaMAntino-2-LLMs, which is fine-tuning of LLama 2 on Italian translated conversations (Basile et al., 2023), and LLaMAntino-3-ANITA-8B-Inst-DPO-ITA, more recent effort that is built upon Llama-3-8B using similar approach (Polignano et al., 2024). Vocabulary Adaptation Techniques To tackle the fertility issue, recent research has focused on improving language adaptation by modifying the tokenizer and vocabulary of pretrained LLMs to better fit the target language. Several efforts in this area have shown the effectiveness of vocabulary adaptation techniques. Minixhofer et al. (2022) and Liu et al. (2024) propose to replace the tokenizer of pretrained LLM, along with its corresponding embedding layer, relying on bilingual dictionarybased, or graph-based, token mapping. Generally, the main difference between various vocabulary adaptation techniques lies in how the embedding space of the respective model is initialized during adaptation. More effort was made by Ostendorff and Rehm (2023); Dobler and de Melo (2023) who use the embeddings from helper model trained alongside the desired tokenizer. They utilize geometrical similarities in the helper models embedding structure to initialize the tokens representations of the target model effectively. In parallel, Gee et al. (2022) proposed simple heuristic, initializing target vocabulary tokens as the average of their corresponding sub-tokens in the source vocabulary. Another study by Koto et al. (2021) put forward an adaptation technique, they rely on FastText2 embedding space to learn linear mapping, to perform vocabulary adaptation of BERTbased models. Unlike previous studies, we thoroughly analyze existing adaptation heuristics, focusing on decoderonly generative models adapted to Italian. We present novel heuristic that utilizes helper embedding space, optimized for the target language, to map and initialize target vocabulary tokens."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we formalize the methodologies used to adapt pretrained LLMs to target language. The following subsections outline the techniques employed to modify the vocabulary of pretrained LLMs and describe the process of adapting them to target language. Finally, we describe the last step of the adaptation, that is, the continual training step. 3.1 Vocabulary Adaptation All the vocabulary adaptation methodologies share similar objective: substituting the tokenizer and its vocabulary, and replacing the model embeddings (both embedding module and language model head) with one more suited for the target language. In our setting, we have source pretrained LLM, 3, tokenizer Ts, Ms, with its embedding matrix Es and vocabulary Vs. To adapt our model to target language, we have target tokenizer Tt and vocabulary Vt suited to encoding texts in the target language, which we want to make Ms compatible with. In some cases, we also have access to helper model, Mh, which is an LLM, usually smaller than 2https://fasttext.cc/ 3Here, we assume tied-weights, i.e., shared embedding module and language model head. When this is not the case, the approach is symmetric, as if there were two embedding matrices. Ms, whose embeddings are noted with Eh. The helper model is trained using Tt and Vt. We use the superscript notation Eti to indicate the representation of the token ti on the matrix embedding E. The objective is to adapt the source model embeddings Es, which all these methods operate on, so as to obtain Et based on the target tokenizer Tt and the target vocabulary Vt. First, the target embeddings are initialized by keeping the same representation from Es for the tokens in the intersection of both vocabularies, while function is applied to the remaining ones: Eti = (cid:40) g(ti, ), Eti , ti Vt Vs ti Vs Vt The difference between these methods lies in g, used to initialize the tokens that are in Vt and not in Vs. This function has access to the source embeddings Es, vocabulary Vs and tokenizer Ts and possibly the embeddings, vocabulary, and tokenizer of helper model Eh, Vh and Th, respectively. Therefore, each method is defined by its respective function, as detailed below. Random As baseline approach, we initialize the tokens outside the intersection with random representation, given by normal distribution, with the mean and the variance defined by the source embedding space: grandom(ti, Es) = (µ(Es), σ2(Es)) FVT Gee et al. (2022) introduced Fast Vocabulary Transfer (FVT) for vocabulary adaptation, which consists of an efficient way to initialize the intersection tokens in the target embedding space. Here, each target token is computed with the average of the embedding source tokens given by the source tokenizer, i.e. the resulting tokens when we tokenize the target token ti with Ts: gf vt(ti, Es, Ts) = 1 Ts(ti) (cid:88) Etj . tj Ts(ti) CLP Ostendorff and Rehm (2023) and in parallel Dobler and de Melo (2023) introduced heuristic to initialize out-of-inventory tokens relying on the space structure of the helper embedding space. Both approaches compute similarity scores between the tokens in Vt Vs against the ones in Vt Vs, on the embedding space of the helper model Eh. Such similarities are used to construct representation of the out-of-inventory tokens in the target embedding matrix Et, relying on the source embedding Es representations: where Rnm and Rn are the parameters of our linear mapping. More technical details about the training of the linear mapping are provided in Appendix A. gclp(ti, Es, Vs, Eh, Vh) = (cid:88) tj VtVs Etj α(Eti , Etj ) where α(, ) indicates similarity score between two tokens in Eh. Here, we rely on the similarity function used by Ostendorff and Rehm (2023) computed as normalized cosine similarity. SAVA Mapping embedding representations between the embedding spaces of two different models using linear model comes with theoretical justification. Moschella et al. (2023) and Maiorca et al. (2024) have shown that the embeddings of different models are related by conformal translation, or more generally, by linear mapping between such spaces. Inspired by the findings of Maiorca et al. (2024) and by the intriguing effort of Koto et al. (2021), we propose technique to perform vocabulary adaptation for generative models called Semantic Alignment Vocabulary Adaptation (SAVA). In our approach, we rely on helper model embedding Eh from an LLM and learn linear mapping ϕ between Eh Rm and Es Rn. We train single-layer Feed Forward Network (FFN) to map the helper embedding space onto the source embedding one: ϕ : (cid:55) Rm, Rn, gsava(ti, Eh) = ϕ(Eti ) (1) The goal in training ϕ is to obtain mapping between the representations of the tokens of the helper model and those of the source one. To train it, we use the tokens in the intersection Vs Vt since they have representation according to both the source and the helper model, and we can train linear map between the representations in Es and those in Eh. Then, as outlined from equation 1 we use ϕ to map the tokens not present in the source vocabulary (Vt Vs) into the source embedding space. Therefore, our objective is to find: ϕ(x) = + b, such that, min W, (cid:88) tiVsVt (cid:13) (cid:13)W Eti + Eti (cid:13) 2 (cid:13) . 3.2 Continual Training While re-initializing embeddings through vocabulary adaptation techniques enables zero-shot language modeling, the resulting language model often lacks proficiency in the new language. We address this by performing continual training on mixture of source and target languages, which allows the model to retain performance in the source language while improving in the target language. To achieve robust comparison, we adapt pretrained LLMs to the target language using all the vocabulary adaptation heuristics discussed above. We also present results from continual training of the base model on the target language (LAPT). While less disruptive, this approach does not alter the vocabulary or tokenizer, preserving its fertility."
        },
        {
            "title": "4 Experimental Setup",
            "content": "This section describes the setup of our experiments where we adapt two popular LLMs, specifically Mistral-7B-v0.1 (Jiang et al., 2023) and Llama-3.18B (Dubey et al., 2024). In the following subsections we report the settings used to do vocabulary adaptation, continual training and evaluation. 4.1 Vocabulary Adaptation To adapt English models to the Italian language we rely on the Minerva-LLMs model family and its tokenizer (Orlando et al., 2024). The models of the Minerva-LLMs family are trained from scratch on an Italian-English dataset, i.e. CulturaX (Nguyen et al., 2024). At the time of writing, three different models have been released, Minerva-350M, Minerva-1B, and Minerva-3B, with the same tokenizer. The Minerva-LLMs tokenizer shares 16,438 tokens with Mistral-7B-v0.1 and 20,358 tokens with Llama-3.1-8B. For both CLP and SAVA, we use Minerva-3B as the helper model.4 Notably, as shown in Table 1, adapting large model like Llama-3.1-8B with Minerva-LLMs tokenizer significantly reduces the vocabulary size (by 75%) and thus results in fewer parameters. The adapted 4We conduct ablation studies for the SAVA method, changing the number of tokens used to train ϕ and the size of the helper model. Some considerations are reported in Appendix B. Model Mistral-7B-v0.1 Mistral-7B-v0.1 a.w. Minerva LLaMa-3-8B LLaMa-3-8B a.w. Minerva Num. Tokens Num. Parameters 32000 32768 128256 32768 7.24B 7.25B 8.03B 7.25B Table 1: Comparisons of model parameter counts and vocabulary size with and without adaptation (a.w. stands for adapted with). Llama-3.1-8B has 7.25B parameters compared to the original 8B, resulting in 10% reduction in model size. As further improvement, substituting the Mistral-7B-v0.1 and Llama-3.1-8B tokenizers with Minerva-LLMs one has significant impact on the fertility in the Italian language. As shown in Table 2, the Minerva-LLMs tokenizer has on average 25% of fertility gain compared to the Mistral-7Bv0.1 tokenizer on two Italian text sources, CulturaX (CX) and Wikipedia (Wp). In the same setting, Llama-3.1-8B improves its fertility up to 16% on Italian text relying on the Minerva-LLMs tokenizer. 4.2 Continual Training To perform continual training we use CulturaX, large-scale multilingual dataset that has been successfully used in large-scale continual training experiments on languages spoken within the European Union, including Italian.5 We aim to compare all methods on fixed amount of compute budget, i.e. number of tokens. Due to constrained computational budget, we decide to stop training after threshold of 12B training tokens. We subsample training data from the Italian and English splits of CulturaX to create dataset composed of 75% Italian tokens and 25% English tokens, as proposed by Csaki et al. (2024). We use packing to fit all the tokens into sequences of fixed length. The learning rate is fixed for all runs at 105. For Mistral-7B-v0.1, training is done on 16 nodes on the Leonardo Supercomputer (each node uses 4 64 GB A100) maintaining global batch size of 3072, and sequence length of 2048. For Llama-3.1-8B we change the sequence length of the training data to 8192, and set the global batch size to 512. When training both models we do not freeze any parameter and let them all update. We perform continual training, allowing the models to process approximately 12 billion tokens. Specif5https://huggingface.co/occiglot/ occiglot-7b-it-en-instruct Model Mistral-7B-v0.1 Minerva LLaMa-3-8B Fertility CX IT CX EN Wp IT Wp EN 1.88 1.39 1.67 2.05 1.66 1.80 1.32 1.32 1.15 1.57 1.59 1. Table 2: Fertility of different tokenizers on CulturaX (CX) and Wikipedia (Wp). ically, we train Mistral-7B-v0.1 for 2000 batches and Llama-3.1-8B for 3000 batches. We use llmfoundry for training6 and for the remaining hyperparameters we use the default settings provided by the library. See Appendix for an estimation of the CO2 cost of the experiments carried out in this work. 4.3 Evaluation To evaluate our models we rely on the LMEvaluation-Harness library (Gao et al., 2024), for multiple-choice (MC) benchmarks, using the perplexity evaluation method. As MC benchmarks, we use the translated section of ITA-Bench (Moroni et al., 2024), suite of benchmarks automatically translated from English to Italian. During continual training we evaluate our models every 200 batches for Mistral-7B-v0.1 and 300 batches for Llama-3.1-8B in 0-shot scenario; in this way, each subsequent checkpoint is evaluated consistently on the same number of tokens. To assess the reasoning capabilities of the adapted models, we use variety of benchmarks: MMLU (Hendrycks et al., 2021), BOOLQ (Clark et al., 2019), ARC-easy (Clark et al., 2018), PIQA (Bisk et al., 2020), SciQ (Welbl et al., 2017), and Hellaswag (Zellers et al., 2019). We also measure the model performance on generative tasks, focusing on two tasks: automatic translation, FLoRes benchmark (Costa-jussà et al., 2022), and question answering, SQuAD-it (Croce et al., 2018), version of SQuAD (Rajpurkar et al., 2016) automatically translated into Italian. We used vLLM (Kwon et al., 2023) as our generation pipeline. More details related to the generation techniques can be found in Appendix D."
        },
        {
            "title": "5 Results",
            "content": "In this section, we discuss the results obtained from evaluating the adapted models. We begin by examining the scores on multiple-choice benchmarks, followed by separate analysis of performance on 6https://github.com/mosaicml/llm-foundry Model Mistral-7B-v0.1 Hellaswag MMLU 47.420.42 56.500.49 Arc Easy 61.671.01 PIQA 67.241. SCIQ 84.751.16 BOOLQ AVG 75.010.75 65.43 Random FVT CLP SAVA LAPT Random FVT CLP SAVA LAPT 55.600.49 56.340.49 54.740.49 56.730.49 58.290.49 58.430.49 59.000.49 59.210.49 59.410.49 60.510. 42.480.42 44.280.42 42.500.42 44.230.42 49.310.42 46.950.42 47.350.42 47.100.42 47.570.42 46.630.42 200 Training Steps 57.921.02 60.421.01 57.621.02 60.901.01 63.001.00 68.051.16 69.901.14 67.741.16 69.721.14 69.841.14 2000 Training Steps 62.871.00 63.520.99 63.470.99 63.390.99 64.990.99 71.391.12 71.511.12 70.771.13 71.021.12 71.211.12 75.461.39 80.481.28 76.821.36 79.221.31 84.131.18 81.621.25 84.551.16 84.441.17 84.551.16 85.901.12 72.290.78 74.520.76 68.070.81 73.300.77 75.070.75 72.470.78 75.740.74 76.750.73 76.020.74 76.170. 61.96 64.32 61.24 64.01 66.60 65.62 66.94 66.95 66.99 67.56 Table 3: 0-shot results over Italian translated benchmarks for Mistral-7B-v0.1 adapted models. in this setting, SAVA and FVT perform well, while Random lags behind. In Figure 2, we present the average scores across the six Italian tasks. SAVA and FVT consistently achieve higher overall scores throughout the training process, with more pronounced advantage in the early stages. This highlights the influence of the chosen heuristic, particularly immediately after the vocabulary substitution. SAVA and FVT achieve results at 400 batches that are comparable to those of the Random approach at the end of training, thereby reducing total training time by approximately 80%. In the case of Llama-3.1-8B, Table 4 reports the scores of the adapted models, after 300 and 3000 batches. We show that FVT and SAVA maintain comparable performance, except for BOOLQ where SAVA showcases better scores, +4%, even in comparison to the LAPT setting. Compared to the adapted models, the Llama-3.1-8B model remains strong baseline on Italian tasks. Still in that setting, we further narrow the performance gap with the LAPT model using both vocabulary adaptation heuristics. In Figure 3, we report the average scores on Italian tasks and observe constant improvement through the training steps. 5.1.2 English Results Including English in the evaluation allows us to assess whether performance on the source language is preserved during continual training, for both Mistral-7B-v0.1 and Llama-3.1-8B. As mentioned in Section 4, we train on mainly Italian data and smaller portion of English (25% of the total). Figure 4 reports the average scores during training on English texts for Mistral-7B-v0.1. We can see that all trained models reach comparable avFigure 2: Average performance of Mistral-7B-v0.1 based models during training on Italian translated benchmarks. The average was calculated over six datasets. generative benchmarks, specifically FLoRes and SQuAD-it. In this and subsequent sections we indicate the continual training of the base model without vocabulary adaptation by the LAPT acronym. 5.1 Multi-choice Setting Italian Results 5.1.1 We report results on Italian benchmarks for Mistral7B-v0.1 after 200 and 2000 batches in Table 3. From the table we can see that the adapted models reach over random-chance performance at the beginning of training (200-step setting), with FVT and SAVA achieving higher performance compared to other methods (CLP and Random). All the vocabulary adaptation heuristics perform worse compared to the LAPT technique, which is expected since LAPT does not apply any disruptive architectural change to the model. Looking at the results at 2000 batches, we can see that all the adapted models surpass the scores of the base model, and the performance gap with LAPT becomes low. Even Model LLaMa-3.1-8B 57.970.49 Hellaswag MMLU 54.280.42 Arc Easy 60.461.01 PIQA 68.541.15 SCIQ 82.771. BOOLQ AVG 74.520.76 66.42 FVT SAVA LAPT FVT SAVA LAPT 300 Training Steps 55.610.49 55.480.49 57.920.49 50.240.42 49.260.42 53.100. 59.381.01 59.771.01 61.321.01 66.991.17 66.621.17 68.971.15 80.681.27 81.311.26 82.561.22 70.000.80 74.430.76 72.200.78 3000 Training Steps 58.440.49 57.820.49 59.350. 51.470.42 51.080.42 52.940.42 62.701.00 63.171.00 62.961.00 69.531.14 69.781.14 69.721.14 83.291.20 81.731.24 82.981.21 69.350.80 74.150.76 71.770.78 63.81 64.48 66. 65.79 66.29 66.62 Table 4: 0-shot results over Italian translated benchmarks for Llama-3.1-8B adapted models. Figure 3: Average performance of Llama-3.1-8B based models during training on Italian translated benchmarks. The average was calculated over six datasets. Figure 5: Average performance of Llama-3.1-8B based models during training on English benchmarks. The average was calculated over six datasets. For both models the SAVA approach leads the model to achieve slightly higher performance in the source language. Appendix reports more detailed results of evaluation over English benchmarks. 5.2 Generative Setting Multi-choice benchmarking based on perplexity scoring has its own limitations (Wang et al., 2024). To further test our models, we evaluate them on two generative tasks: Machine Translation (MT), IT-EN and EN-IT, and Italian Question Answering. We report COMET-22 (Rei et al., 2022) for the MT benchmark and RougeL (Lin, 2004) for the Question Answering task. Looking at the MT results, in Table 5, we observe that the adapted Mistral-7B-v0.1 models achieve excellent performance, outperforming those of the base model. The vocabulary adapted models reach very good results in the English-toItalian direction, where generation of Italian text is involved. Our findings indicate that SAVA and FVT emerge as the most effective vocabulary adaptation heuristics in this context. As shown in Table 6, similar trend is observed with Llama-3.1-8B, where adapted models perform competitively with the Figure 4: Average performance of Mistral-7B-v0.1 based models during training on English benchmarks. The average was calculated over six datasets. erage score at the end of the adaptation process. All the adapted models diminish in performance compared to the base one in the English language. Figure 5 reports the average scores of Llama-3.18B models on English benchmarks during training. In this setting, LAPT maintains higher performance on average; intuitively, this could be attributed to the larger vocabulary of Llama-3.1-8B (75% bigger), which enables better performance during language adaptation, avoiding catastrophic forgetting of the source language. Model Mistral-7B-v0.1 Random FVT CLP SAVA LAPT Random FVT CLP SAVA LAPT FLoRes EN-IT IT-EN 87.75 86.57 SQuAD-it RL 68.92 200 Training Steps 86.67 87.08 86.58 87.30 87.41 87.37 87.55 87.31 87.59 87. 62.1 65.47 64.25 65.66 67.35 2000 Training Steps 88.01 88.29 88.21 88.31 88.13 87.92 87.90 87.79 87.87 88.02 64.83 66.18 65.99 67.20 66.92 Table 5: 5-shot results for Mistral-7B-v0.1 of FLoRes where COMET-22 is reported and 2-shot results for SQuAD-it where RougeL is reported. Model Llama-3.1-8B FVT SAVA LAPT FVT SAVA LAPT FLoRes EN-IT IT-EN 88.08 87.59 SQuAD-it RL 69.21 300 Training Steps 87.32 87.39 87.82 87.65 87.58 87.95 68.54 68.70 67.91 3000 Training Steps 88.05 88.12 88.11 88.02 88.04 88. 68.84 69.05 66.69 Table 6: 5-shot results for Llama-3.1-8B of FLoRes where COMET-22 is reported and 2-shot results for SQuAD-it where RougeL is reported. base model, while SAVA and FVT reach the same performance as those of LAPT. Regarding the results in the SQuAD-it task, Tables 5 and 6 show that SAVA attains very good performance, beating other heuristics and the LAPT approach for both model types, reaching inline performance equal to that of the base model for Llama3.1-8B. 5.3 Training Loss Important observations can be made concerning the loss trajectories. Figure 6 reports the Mistral-7Bv0.1 plots, and we can notice significant differences between the various heuristics in the early stages of the training. The SAVA-model emerges as the better-adapted one, right from the start, particularly Figure 6: Loss during continual training of Mistral-7Bv0.1 models. Figure 7: Loss during continual training of Llama-3.18B models. when compared to the CLP and Random models. Notably, CLP appears to lag behind Random initially. Looking at Llama-3.1-8B losses, in Figure 7 we can see that the two heuristics exhibit similar trajectories, although SAVA still achieves lower loss from the outset."
        },
        {
            "title": "6 Differences in the Embedding Structure",
            "content": "To better understand the impact of different vocabulary adaptation techniques, we analyze similarities in intra-model and inter-model embedding spaces. Specifically, we examine how different adaptations influence the structural alignment of embeddings in comparison to reference model (intra-model similarity) and how the embedding spaces of different adapted models compare to each other (inter-model similarity). To measure the similarity between two embedding spaces, we rely on the technique introduced by Moschella et al. (2023). Specifically, we randomly select 128 non-prefix tokens and 128 prefix tokens from Vt to compute relative embedding representations, resulting in total of 256 anchor tokens.7 7Non-prefix tokens refer to complete words or sub-words Mistral-7B-v0.1 Llama-3.1-8B Model @0ba @2000ba @0ba @3000ba 31.67 Random 29.68 35.30 33.65 FVT 41.10 CLP 42.84 45.33 44.81 SAVA - 33.23 - 41.84 - 33.49 - 42.02 Table 7: Similarity scores between Mistral-7B-v0.1 adapted models and Minerva-3B (left) and between Llama-3.1-8B and Minerva-3B (right) at the beginning and at the end of the training. For each model, we then adjust the representation of each token relative to these anchors, calculating each dimension as the projection onto the selected anchors. Subsequently, we compute the cosine similarity based on this relative representation across the models and average the results to obtain an overall similarity score between the two distinct models. similarity Intuitively, Intra-model welladapted model should align with Minerva-3B, as it serves as strong reference for the target language. Similarly to our setting, Minerva-3B is pretrained on balanced Italian-English data from CulturaX. In Table 7, we present the similarity scores between adapted models and Minerva-3B. Notably, CLP and SAVA achieve higher similarity scores than other approaches. This outcome is to be expected, as both CLP and SAVA leverage Minerva-3Bs embedding space. Interestingly, SAVA not only attains structure that is more similar to that of Minerva3B (+3.7), but also demonstrates superior performance, as was also the case in previous sections. Inter-model similarity To gain deeper insights into the differences in learned embedding structures, Figure 8 presents the similarity scores between Mistral-7B-v0.1 variants adapted using the specified techniques. We compare the models at the end of the continual training. The analysis shows high similarity between models, but differences of up to 10% in the relative representations reveal structural variations in the encoded information. This analysis suggests that, even after intensive training, the adapted models do not converge to the same representation. that do not start sequence (e.g., cat in concatenate), while prefix tokens initiate word or sub-word. Figure 8: Similarity across models after continual training on 12B tokens."
        },
        {
            "title": "7 Conclusions",
            "content": "In this work, we extensively explored various techniques to adapt English-focused LLMs, i.e. Mistral7B-v0.1 and Llama-3.1-8B, to the Italian language. We introduced novel heuristic called SAVA which leverages the embedding structure of smaller, native Italian Language Model, Minerva-3B. We discovered that adapting the vocabulary of English LLMs leads to significant improvements in language encoding, reducing the number of generated tokens by 25% for Mistral-7B-v0.1 and 16% for Llama-3.1-8B. Regarding Llama-3.1-8B we pruned nearly 1 billion parameters by optimizing its vocabulary, removing approximately 75% of the original tokens. Our evaluation revealed performance differences across the vocabulary adaptation heuristics, by means of thorough analysis during the continual training phase. We show that linguistic capabilities can be restored with relatively few training stepsMistral-7B-v0.1 reached base model performance after processing 2 billion tokens. Additionally, the SAVA heuristic demonstrated strong performance on downstream tasks, with SAVA-adapted models reaching faster convergence during continual training. Furthermore, the embedding structure of SAVA exhibited closer alignment with the helper model compared to other analyzed heuristics. This work opens several research directions. One key area of interest will be to evaluate how the SAVA approach scales across languages, particularly in midand low-resource settings. Understanding how different heuristics perform with small number of continual training steps in such scenarios is crucial. Additionally, since Minerva7B was not available at the time of writing, logical next step would be to utilize it as helper model."
        },
        {
            "title": "9 Ethics Statement",
            "content": "We investigated the adaptation of English-first LLMs to the Italian language with focus on adapting the vocabulary and the tokenizer to match the performance of continually trained models while achieving lower fertility and thus higher efficiency in the target language. We limited our training data to the CulturaX dataset, which consists of cleaned web-crawled data. Incorporating higher-quality datasets could improve the models performance in the target language. We limited our analysis to two distinct decoderonly Large Language Models: Mistral-7B-v0.1 and Llama-3.1-8B. For more comprehensive study, additional English-first models could be tested. However, the aforementioned two models are among the best performing ones in their parameter count. Furthermore, we chose to focus on just two models due to the extensive continual training we had to perform, as such training requires considerable computational resources. We evaluated the adapted models on automatically translated datasets for multiple-choice tasks and open-ended question answering. Specifically, Hellaswag, MMLU, Arc Easy, PIQA, SCIQ, and BOOLQ were translated using Tower-Instruct-v0.2, an open-source solution for automatic translation that, at the time of writing, represents the state of the art in open Machine Translation models. For generative tasks, SQuAD-it was translated using semi-automatic approach. We acknowledge that relying on automatically translated benchmarks may have introduced some noise, potentially obscuring certain abilities or issues in the models comprehension of Italian texts. This limitation was beyond our capabilities to resolve since no well-structured Italian native benchmarks exist. Another limitation was using only two generative benchmarks, where we observed slightly different results for the adapted models. In the generative setting, SAVA generally outperformed other methods, while LAPT models did not consistently deliver the best average performance on downstream tasks. Future work should aim to explore the capabilities of vocabulary-adapted models in generative tasks and investigate how models fertility over target language influences downstream performance. We primarily conduct experiments in the Italian language. This approach is aimed at addressing the practical challenges of working with Italian, language that is underrepresented in the NLP field. Our continual training is performed on data collected from open web sources, specifically through the CulturaX dataset. Since large-scale datasets used for pretraining can include personal and sensitive information, it is crucial to carefully assess such content before deploying models in real-world applications. Another key consideration is the use of existing monolingual or multilingual models as starting points, rather than training new models from scratch. This can introduce biases from the original pretraining data, potentially causing the model to reflect behaviors and cultural influences from other languages rather than those of the target language community."
        },
        {
            "title": "Acknowledgments",
            "content": "Edoardo Barba and Alessio Miaschi are fully funded by the PNRR MUR project PE0000013FAIR. Roberto Navigli and Felice DellOrletta acknowledge the support of the PNRR MUR project PE0000013-FAIR. Partially financed by the European Union - NextGenerationEU through the Italian Ministry of University and Research under PNRR - PRIN 2022 (2022EPTPJ9) \"WEMB: Word Embeddings from Cognitive Linguistics to Language Engineering and back\" and by the PNRR project ITSERR (CUP B53C22001770006). We acknowledge the support of the ISCRA project TRAVEL (HP10CY9V7K) for awarding access to the LEONARDO supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CINECA (Italy) and thank Giuseppe Fiameni for his support."
        },
        {
            "title": "References",
            "content": "Pierpaolo Basile, Elio Musacchio, Marco Polignano, Lucia Siciliani, Giuseppe Fiameni, and Giovanni Semeraro. 2023. Llamantino: Llama 2 models for effective text generation in italian language. Preprint, arXiv:2312.09993. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. In ThirtyFourth AAAI Conference on Artificial Intelligence. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263 311. pages 121, Miami, Florida, USA. Association for Computational Linguistics. Ethan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020. Parsing with multilingual BERT, small corpus, and In Findings of the Association small treebank. for Computational Linguistics: EMNLP 2020, pages 13241334, Online. Association for Computational Linguistics. Wietse de Vries and Malvina Nissim. 2021. As good as new. how to successfully recycle English GPT-2 to make models for other languages. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 836846, Online. Association for Computational Linguistics. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29242936, Minneapolis, Minnesota. Association for Computational Linguistics. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440 8451, Online. Association for Computational Linguistics. Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Y. Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. CoRR, arXiv:2207.04672. Danilo Croce, Alexandra Zelenanska, and Roberto Basili. 2018. Neural learning for question answering in italian. In AI*IA 2018 Advances in Artificial Intelligence, pages 389402, Cham. Springer International Publishing. Zoltan Csaki, Bo Li, Jonathan Lingjie Li, Qiantong Xu, Pian Pawakapan, Leon Zhang, Yun Du, Hengyu Zhao, Changran Hu, and Urmish Thakker. 2024. SambaLingo: Teaching large language models new languages. In Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024), Konstantin Dobler and Gerard de Melo. 2023. FOCUS: Effective embedding initialization for monolingual specialization of multilingual models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1344013454, Singapore. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. framework for few-shot language model evaluation. Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, and Paolo Torroni. 2022. Fast vocabulary transfer for language model compression. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 409 416, Abu Dhabi, UAE. Association for Computational Linguistics. Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Dont stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 83428360, Online. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. 2022. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Fajri Koto, Jey Han Lau, and Timothy Baldwin. 2021. IndoBERTweet: pretrained language model for Indonesian Twitter with effective domain-specific vocabulary initialization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1066010668, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2023. Bloom: 176bparameter open-access multilingual language model. Preprint, arXiv:2211.05100. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Yihong Liu, Peiqin Lin, Mingyang Wang, and Hinrich Schuetze. 2024. OFA: framework of initializing unseen subword embeddings for efficient large-scale multilingual continued pretraining. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 10671097, Mexico City, Mexico. Association for Computational Linguistics. Valentino Maiorca, Luca Moschella, Antonio Norelli, Marco Fumero, Francesco Locatello, and Emanuele Rodolà. 2024. Latent space translation via semantic alignment. Advances in Neural Information Processing Systems, 36. Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Cristian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, and et al. 2024. Gemma: Open models based on gemini research and technology. volume abs/2403.08295. Benjamin Minixhofer, Fabian Paischer, and Navid Rekabsaz. 2022. WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39924006, Seattle, United States. Association for Computational Linguistics. Luca Moroni, Simone Conia, Federico Martelli, and Roberto Navigli. 2024. ITA-Bench: Towards more comprehensive evaluation for Italian LLMs. In Proceedings of the Tenth Italian Conference on Computational Linguistics (CLiC-it 2024). Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodolà. 2023. Relative representations enable zeroshot latent space communication. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2024. CulturaX: cleaned, enormous, and multilingual dataset for large language models in 167 languages. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 4226 4237, Torino, Italia. ELRA and ICCL. Riccardo Orlando, Luca Moroni, Pere-Lluís Huguet Cabot, Edoardo Barba, Simone Conia, Sergio Orlandini, Giuseppe Fiameni, Roberto Navigli, et al. 2024. Minerva llms: The first family of large language models trained from scratch on italian data. In Proceedings of the Tenth Italian Conference on Computational Linguistics (CLiC-it 2024). Malte Ostendorff and Georg Rehm. 2023. Efficient language model training through cross-lingual and progressive transfer learning. arXiv preprint arXiv:2301.09626. Marco Polignano, Pierpaolo Basile, and Giovanni Semeraro. 2024. Advanced natural-based interaction for the italian language: Llamantino-3-anita. Preprint, arXiv:2405.07101. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Xinpeng Wang, Bolei Ma, Chengzhi Hu, Leon WeberGenzel, Paul Röttger, Frauke Kreuter, Dirk Hovy, and Barbara Plank. 2024. my answer is C: First-token probabilities do not match text answers in instructiontuned language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 74077416, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. 2024. Redpajama: an open dataset for training large language models. CoRR, abs/2411.12372. Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94106, Copenhagen, Denmark. Association for Computational Linguistics. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. Liu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021. robustly optimized BERT pre-training approach with post-training. In Proceedings of the 20th Chinese National Conference on Computational Linguistics, pages 12181227, Huhhot, China. Chinese Information Processing Society of China."
        },
        {
            "title": "A SAVA Training of the mapping function",
            "content": "To implement the SAVA methods, we first need to train the linear mapping function, ϕ. For this, we use the SGDAffineAligner method provided in the latentis library8. 8https://github.com/Flegyas/latentis Figure 9: Loss during continual training of Mistral models. After collecting the token representation pairs from the intersection, we train the linear mapping using the ADAM optimizer with MSE Loss, setting the learning rate to 103 and running the optimization for 1000 steps. To enhance training stability, we first apply standard scaling and L2 normalization to the token representations before learning ϕ. After training, we apply the inverse scaling to restore the original distribution before incorporating the results into the adapted model."
        },
        {
            "title": "B Ablation experiments on the SAVA",
            "content": "method In this section we analyze some ablation studies over the SAVA method. We analyzed the impact on the helper models size, using the two smaller models of Minervas family, Minerva-350M and Minerva-1B, which have, respectively, 350M and 1B parameters. In Figure 9 the training loss of Mistral-7B-v0.1 adapted using SAVA with different helper models is reported. From the plot we can see that the dimension of the helper model does not have huge impact on the loss trajectory. An orthogonal experiment was conducted to ablate the number of tokens used to learn the mapping ϕ, in Figure 10 the loss for Mistral-7B-v0.1 adapted with SAVA relying on different number of tokens in Vt Vs, over Minerva-3B is reported. We observe that using more tokens leads to faster convergence of the training loss. From the plots we can see that reducing the number of tokens has greater impact than reducing the model size, especially for the setting with two thousand tokens. Prompt EN-IT Prompt IT-EN Traduci dallInglese allItaliano Text: love you so much. Translation: Ti amo così tanto. Translate from Italian to English Text: Ti amo così tanto. Translation: love you so much. Table 8: Prompts used for machine translation task Italian Prompt Contesto: Il terremoto del Sichuan del 2008 il terremoto del Gran Sichuan, misurato 8.0 Ms 7.9 Mw, si è verificato alle 02:28:01 PM China ... Domanda: In quale anno si è verificato il terremoto nel Sichuan? Risposta: 2008 Table 9: Used prompts for question answering task ble 10 reports the performance of Mistral-7B-v0.1 on six multiple-choice benchmarks. From this table, we observe that SAVA and FVT achieve higher task-wise scores early in the adaptation process. similar trend is evident for the Llama-3.1-8B adapted models, as shown in Table 11, where the SAVA technique yields higher average scores than FVT, at the beginning and at the end of training. For both models, per-task scores remain below the base models performance. However, incorporating portion of English data during adaptation prevents catastrophic forgetting when transitioning towards the Italian language. Figure 10: Loss during continual training of Mistral models."
        },
        {
            "title": "Impact",
            "content": "using conducted Experiments were the LEONARDO Italian Supercomputer, which has carbon efficiency of 0.432 kgCO2eq/kWh. cumulative of 50000 hours of computation was performed on hardware of type A100 SXM4 80 GB (TDP of 400W). Total emissions are estimated to have been 8640 kgCO2eq of which 0 percent were directly offset. These emissions were split roughly into 95% for continual training and 5% for evaluation. This is an approximate estimate since the computation was done on LEONARDO custom hardware which is not available in the tool used for the estimation ."
        },
        {
            "title": "D Generation setting",
            "content": "We tested our adapted models on two downstream tasks in generative setting, machine translation and question answering. We tested the models in few-shot setting relying on the in-context capabilities of evaluated models, without any fine-tuning step to the specific task. We relied on the vLLM library (Kwon et al., 2023) to afford prompting generation, specifically we changed the default parameters with temperature=0 and max_tokens=512. After comprehensive number of trials we noticed that the prompting strategy had huge impact, while the order between the models remained unchanged. We report the prompts used for FLoRes and SQuAD-it tasks in Tables 8 and 9, respectively. English Results on Multi-choice benchmarks In this section, we present detailed analysis of the evaluation results on English benchmarks. TaModel Mistral-7B-v0.1 Hellaswag MMLU 57.190.42 75.980. Arc Easy 78.550.94 PIQA 83.840.94 SCIQ 95.820.80 BOOLQ AVG 77.640.78 78.17 Random FVT CLP SAVA LAPT Random FVT CLP SAVA LAPT 72.290.44 72.350.44 72.590.44 72.810.44 74.130.43 72.180.44 73.280.44 73.370.44 73.020.44 74.260.43 51.590.42 53.040.42 52.020.42 53.210.42 55.050.42 52.110.42 52.960.42 52.480.42 52.910.42 51.180.42 200 Training Steps 69.550.95 73.080.92 70.160.94 74.280.94 75.230. 81.730.96 82.600.94 81.550.96 82.470.96 84.020.91 2000 Training Steps 73.60.91 74.760.90 74.070.90 74.670.90 73.90.91 82.720.94 81.910.95 82.470.94 82.290.94 83.650.92 89.970.97 92.480.85 89.660.98 92.790.83 94.460.73 93.210.81 94.050.76 94.050.76 94.460.73 94.670. 74.030.76 72.200.78 72.810.77 71.590.78 71.980.78 75.770.74 74.460.76 74.830.75 74.580.76 74.220.76 73.19 74.29 73.13 74.52 75.81 74.93 75.23 75.21 75.32 75.31 Table 10: 0-shot results over English benchmarks for Mistral-7B-v0.1 adapted models. Model LLaMa-3.1-8B 74.210. Hellaswag MMLU 62.190.42 Arc Easy 77.600.47 PIQA 83.030.93 SCIQ 93.940.77 BOOLQ AVG 80.420.69 78.56 FVT SAVA LAPT FVT SAVA LAPT 300 Training Steps 72.350.44 72.720.44 74.350.43 58.220.42 58.190.42 61.740.41 69.550.95 70.750.94 76.140.88 81.300.97 81.790.96 83.210. 92.270.86 92.900.83 94.050.76 71.340.79 71.280.79 76.690.73 3000 Training Steps 73.020.44 72.860.44 74.400.43 57.850.42 57.940.42 60.500.42 72.130.93 72.780.92 75.320. 82.041.15 81.790.96 82.470.94 92.900.83 93.310.80 93.630.78 72.530.78 73.300.77 77.430.73 74.17 74.60 77.69 75.07 75.33 77.29 Table 11: 0-shot results over English benchmarks for Llama-3.1-8B adapted models."
        }
    ],
    "affiliations": [
        "Babelscape",
        "ILC-CNR",
        "ISTI-CNR",
        "Sapienza University of Rome"
    ]
}