{
    "paper_title": "Code2World: A GUI World Model via Renderable Code Generation",
    "authors": [
        "Yuhao Zheng",
        "Li'an Zhong",
        "Yi Wang",
        "Rui Dai",
        "Kaikui Liu",
        "Xiangxiang Chu",
        "Linyuan Lv",
        "Philip Torr",
        "Kevin Qinghong Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World."
        },
        {
            "title": "Start",
            "content": "Code2World: GUI World Model via Renderable Code Generation Yuhao Zheng 1 2 * Lian Zhong 3 * Yi Wang 2 Rui Dai 2 Kaikui Liu 2 Xiangxiang Chu 2 Linyuan 1 Philip Torr 4 Kevin Qinghong Lin 4 (cid:128) Project Page Code ı Dataset"
        },
        {
            "title": "Model",
            "content": "6 2 0 2 0 1 ] . [ 1 6 5 8 9 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing textand pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through visual-feedback revision mechanism, yielding corpus of over 80K high-quality screenaction pairs. To adapt existing VLMs into code prediction, we first perform SFT as cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in flexible manner, boosting Gemini2.5-Flash by +9.5% on AndroidWorld navigation. *Equal contribution 1University of Science and Technology of China 2AMAP, Alibaba Group 3Sun Yat-sen University 4University of Oxford. Correspondence to: Linyuan Lu <linyuan.lv@ustc.edu.cn>, Kevin Qinghong Lin <kevin.qh.lin@gmail.com>. Preprint. February 11, 2026. 1 Figure 1. Illustration of Code2World. Given current GUI observation and an action, Code2World predicts the next screenshot via renderable code generation. 1. Introduction Recent advancements in Large Multimodal Models have revolutionized the development of autonomous GUI agents (Hong et al., 2024; Lin et al., 2025b; Qin et al., 2025). These agents are designed to perceive visual interfaces and execute sequences of actions to complete complex tasks across web and mobile platforms (Xie et al., 2024; Li et al., 2024). Despite these promising capabilities, significant gap with human proficiency persists in real-world scenarios, particularly for navigation tasks that demand precise reasoning and error correction (Chae et al., 2024). key factor contributing to human superiority is the ability to mentally simulate the consequences of actions before execution to evaluate potential outcomes and adjust their strategies accordingly. In contrast, most existing GUI agents operate actions directly without such foresight. Lacking this predictive capability, an erroneous execution at the current step often necessitates costly multi-step corrections or leads to immediate task failure (Wu et al., 2025; Lin et al., 2024). This is particularly dangerous in high-risk scenarios, such as confirming payments or deleting critical data, where actions are irreversible. To mitigate these risks, it is essential to equip agents with virtual sandbox to simulate the actionCode2World: GUI World Model via Renderable Code Generation conditioned observation, often referred to as world model which has already demonstrated remarkable success in domains like embodied AI (Lu et al., 2025b). Recent works have attempted to explore GUI world models under two different representation. Text-based approaches (Chae et al., 2024; Gu et al., 2024) leverage Large Language Models (LLMs) to predict state transitions via abstract natural language descriptions. Although these methods capture semantic intent, they fundamentally lack visual information. Visual feedback provides more intuitive representation of the environment and has been shown to significantly enhance the reasoning capabilities of agents (Su et al., 2025). Conversely, Pixel-based approaches (Wei et al., 2023; Luo et al., 2025a) predominantly utilize diffusion models to synthesize future screenshots. However, it is non-trivial to model the precise and discrete state transitions of GUIs within continuous pixel space. Such approaches also face challenges in maintaining fine-grained structural controllability, particularly in text-rich interface (Zhang et al., 2025). Ultimately, neither text nor pixel representations can simultaneously achieve high-fidelity visual simulation and precise structural controllability. Motivated by the fact that GUI is natively created by programmatic code (e.g., HTML), can we explore it as representation for learning GUI world model? Unlike abstract text or raw pixels, symbolic code inherently guarantees structural controllability while enabling high-fidelity visualization via deterministic rendering. Leveraging this insight, we propose paradigm shift by treating GUI simulation as renderable code generation. Under this paradigm, we introduce Code2World, as illustrated in Figure 1, visionlanguage coder designed to predict dynamic transitions by synthesizing structured HTML code and rendering it into the next visual state. However, realizing this paradigm presents three significant challenges: (i) Data scarcity: how to curate large-scale, high-quality data that ground diverse GUI states in faithful structured code; (ii) Screenshot-code alignment: how to build backbone and devise learning strategy that effectively align textual code generation with rendered visual reality; and (iii) Evaluation & Application: how to rigorously benchmark next UI prediction and effectively assist downstream GUI agents. To overcome these challenges, (i) we first construct AndroidCode, large-scale corpus comprising over 80K samples. Derived from the image-based GUI trajectories in AndroidControl (Li et al., 2024), we synthesize their corresponding HTML representations via GPT-5. Crucially, to ensure data quality, we implement visual-feedback revision mechanism that refines the generated code based (ii) Building on Visionon visual alignment checks. Language Model (VLM), we begin with Supervised FineTuning (SFT) as cold start for format following, and then we further employ Render-Aware Reinforcement Learning strategy to enhance the generalization. This strategy integrates dual outcome rewards to align the model with visual reality and action consistency. (iii) Finally, we establish holistic evaluation protocol utilizing VLM-as-a-Judge framework to benchmark functional logic and visual quality. We further validate Code2Worlds practical value by integrating it as plug-and-play simulator to enhance diverse downstream GUI agents. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling GPT-5 and Gemini-3-Pro-Image, and significantly enhances downstream navigation success rates, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld (Rawles et al., 2025). Our contributions are as follows. We propose Code2World, novel VLM-based GUI World Model that predicts dynamic transitions via renderable code generation. By utilizing structured HTML as the native representation, our framework simultaneously achieves high-fidelity visual simulation and precise structural controllability. We present AndroidCode, high-fidelity corpus comprising over 80K samples. We employ synthesis pipeline with visual-feedback revision to ensure high data quality. We introduce novel Render-Aware Reinforcement Learning strategy to align the textual output with visual reality and ensuring the generalization. Extensive experiments demonstrate that Code2World achieves top performance in next GUI prediction. As plug-and-play simulator, it significantly enhances the capabilities of downstream GUI agents, yielding consistent improvements in both offline and online navigation tasks. 2. Related Work 2.1. GUI Agents The advent of Large Multimodal Models (LMMs) has fundamentally shifted GUI agents from metadata dependency to direct pixel-level perception, establishing robust baselines for generalist control (Hong et al., 2024; Lin et al., 2025b; Qin et al., 2025). To tackle the complexity of longhorizon navigation, recent research has augumented these agents with advanced deliberative capabilities. Innovations in advanced grounding (Nayak et al., 2025; Feizi et al., 2025), efficient context management (Cheng et al., 2025a; Liu et al., 2025a) and self-reflection mechanisms (Wu et al., 2025; Xiao et al., 2025) enable agents to maintain goal consistency and autonomously refine strategies across extended trajectories. Currently, the rapid evolution of Reinforcement Learning (RL) has empowered LMMs with advanced reasoning and planning capabilities in various domains (Xiong et al., 2025; Dai et al., 2026; Yuan et al., 2026; Wang et al., 2026; Ji et al., 2026; Chu et al., 2026). In the context of 2 Code2World: GUI World Model via Renderable Code Generation GUI automation, RL has been widely adopted to optimize decision logic, ranging from reasoning-enhanced R1-style fine-tuning (Luo et al., 2025b; Lu et al., 2025c) to online policy optimization (Shi et al., 2025; Lu et al., 2025a). However, these approaches optimize the agents policy in the actual environment. Code2World shifts the focus from the agent to the environment, functioning as learnable virtual sandbox. Unlike RL for the agent itself, we employ render-aware RL algorithm to optimize our simulator. 2.2. GUI Environments and World Models Autonomous GUI agents operate within interactive environments, executing actions to elicit feedbacks (Rawles et al., 2025; Xie et al., 2024). World models internalize these dynamics to function as efficient virtual sandboxes for agent planning. Existing research typically adopts two output modalities. (i) Text-based approaches leverage Large Language Models (LLMs) to predict state transitions via symbolic abstraction, ranging from natural language descriptions of state differentials (Chae et al., 2024; Li et al., 2025; Lin et al., 2025a) and structural DOM updates (Gu et al., 2024; Wang et al., 2025) to textual sketches preserving spatial layouts (Cao et al., 2026). Conversely, (ii) Pixel-based approaches utilize image generation models to synthesize future screenshots directly, often adapting diffusion architectures to generate UI layouts and frame sequences (Wei et al., 2023; Rivard et al., 2025), or decoupling text and layout generation (Luo et al., 2025a) for enhanced fidelity. However, text-based methods discard critical spatial-visual details, while pixel-based synthesis struggles with structural controllability and text-rich scenarios. To bridge this gap, we propose (iii) renderable code approach that ensures both visual fidelity and structural integrity. 3. Code2World We define the task as Next UI Prediction, which aims to approximate the deterministic state transition function of digital environment. Formally, let It denote the visual observation (screenshot) at time step t, at represent the user action executed on It, and denotes the task goal. The objective is to predict the subsequent visual state ˆIt+1. We propose renderable code generation paradigm by targeting the underlying structural representation of the interface rather than the raw pixel space. We define the state transition as two-step conditional generation process: ˆCt+1 = Mθ(It, at, G), ˆIt+1 = R( ˆCt+1) (1) where Mθ is multimodal generator parameterized by θ, producing the predicted structured HTML code ˆCt+1. Subsequently, the predicted visual representation ˆIt+1 is obtained deterministically via browser rendering engine R. 3.1. Training Data Synthesis primary challenge in training code-based world models is the scarcity of paired GUI trajectories and their corresponding clean HTML representations. Most existing benchmarks, such as Android Control (Li et al., 2024), solely provide raw screenshots and action coordinates. To address this, we synthesize AndroidCode, large-scale, high-fidelity dataset derived from the Android Control corpus. We implement fully automated pipeline, visualized in the left panel of Figure 2, that transforms raw GUI pixels into semantic HTML code through two hierarchical stages. Constrained Initial Synthesis. We first utilize GPT-5 to translate each GUI screenshot into structured HTML code, imposing rigorous constraints to ensure deterministic rendering. Specifically, we enforce standardized root container with fixed dimensions to align the DOM coordinate system precisely with the original screenshot. Furthermore, to mitigate the hallucination of external assets, we implement semantic placeholder strategy: the model is instructed to replace unreliable image URLs with descriptive text blocks (e.g., [IMG: Red Shoe]) and render UI icons as inline SVGs. This design guarantees that the generated HTML is self-contained, dependency-free atomic unit. Full prompts are detailed in Appendix D.1. Revision with Visual Feedback. Direct code synthesis from screenshots, lacking verification loop, faces challenges in guaranteeing strict structural alignment and pixellevel precision. To ensure high-quality data curation with computational efficiency, we introduce selective revision mechanism with visual feedback. For each generated sample, we render the code back into an image and compute visual alignment score using SigLIP against the ground truth. Samples exceeding high confidence threshold are considered qualified and retained immediately. For samples falling below this threshold, we trigger refinement loop as shown in Figure 2. Specifically, we feed the ground truth screenshot, the currently rendered image, and the generated HTML code back into GPT-5. The model is instructed to visually compare the two images to identify discrepancies and rectify the code. This mechanism effectively forces the model to revise its own output based on visual evidence, ensuring that AndroidCode maintains high fidelity while optimizing generation efficiency. 3.2. Model Optimization To empower the multimodal generator Mθ with both syntactic precision and visual-level foresight, we employ two-stage training strategy, schematically shown in the right panel of Figure 2. We organize the synthesized AndroidCode dataset into multimodal training triplets (x, , t+1), where = (It, at, G) represents the conditioning input comprising the visual observation, user action, and task goal 3 Code2World: GUI World Model via Renderable Code Generation Figure 2. Left: Illustration of Data Synthesis. The high-fidelity AndroidCode dataset is curated via constrainted initial synthesis and visual-feedback revision loop, where synthesized HTML is iteratively refined based on rendered visual discrepancies to ensure strict alignment (SigLIP score > 0.9). Right: Two-stage Model Optimization. The pipeline progresses from an SFT cold start to Render-Aware Reinforcement Learning (RARL). Utilizing Group Relative Policy Optimization (GRPO), the model optimizes dual rewardsvisual semantic (Rsem) and action consistency (Ract)derived directly from rendered outcomes to enforce structural and logical fidelity. (detailed in Appendix D.2). The terms and t+1 denote the ground-truth HTML sequence and the corresponding target GUI screenshot, respectively. This dual-modal supervision allows the model to learn from both discrete symbolic structures and rendered visual outcomes. Stage 1: Supervised Fine-tuning for Cold Start. The initial phase focuses on the symbolic mapping from to , aiming to instill the model with the fundamental syntax of HTML and the logic of UI layout. We adapt the Qwen3VL-8B-Instruct (Team, 2025b) as backbone by minimizing the standard negative log-likelihood loss over the target code tokens . Through this process, the model learns to generate syntactically valid code structures. However, since this stage treats the code merely as text, the supervised policy, denoted as πsft, remains blind to the final rendered visual outcome. Stage 2: Render-Aware Reinforcement Learning. To bridge the gap between textual code and visual reality, we use the final rendered outcome as the learning signal to guide optimization. core innovation of our framework is the design of composite reward function Rtotal that guides the model across two distinct dimensions: visual semantic fidelity and action consistency. Let ˆIt+1 = R(y) be the image rendered from sampled HTML sequence y, and t+1 be the ground-truth next state. By leveraging the reasoning capabilities of VLM, we introduce two complementary components of rewards. Visual Semantic Reward (Rsem). Standard similarity metrics (e.g., CLIP (Radford et al., 2021)) are often brittleoverly sensitive to pixel-level shifts while missing finegrained details (Yang et al., 2025). Furthermore, our code generation employs rendering abstraction strategy, where images and complex icons are represented by textual placeholders (e.g., generic box labeled Settings) since generating raw assets is infeasible. Although the resulting layout is semantically correct and more concise, embedding-based models severely penalize this visual style discrepancy. To address this, we employ VLM-as-a-Judge to evaluate the high-level semantic alignment (detailed in Appendix D.4). The VLM is instructed to focus on structural layout and element correspondence, tolerating stylistic abstractions, yielding score: Rsem = VLMjudge( ˆIt+1, t+1) (2) This ensures the model is rewarded for reconstructing the correct UI structure rather than superficial pixel matching. Action Consistency Reward (Ract). Visual similarity alone is insufficient; the state transition must logically reflect the execution of the user action at. We similarly utilize VLM-as-a-Judge mechanism to verify the dynamic logic, ensuring the transition adheres to the users intent (detailed in Appendix D.3). We feed the triplet (It, at, ˆIt+1) into the judge model to assess whether the state change in ˆIt+1 is valid consequence of executing at on the previous state It. The VLM outputs confidence score serving as the logic reward: Ract = VLMjudge(It, at, ˆIt+1) (3) This effectively penalizes hallucinations where the visual update contradicts the intended action logic. The final reward is weighted sum Rtotal = λ1Rsem+λ2Ract, where λ1 and λ2 balance each components contribution. To optimize this objective, we adopt Group Relative Policy 4 Code2World: GUI World Model via Renderable Code Generation Figure 3. Illustration of the Propose, Simulate, Select pipeline for Code2World enhanced GUI agent, exemplified by an AndroidWorld task (Rawles et al., 2025). (1) Propose: The GUI agent generates candidate actions, with red and green highlighting hallucinated/irrational reasoning and logically sound reasoning, respectively. (2) Simulate: Code2World predicts the execution result of each candidate via renderable code generation. (3) Select: By evaluating the rendered future states, the system identifies the potential failure in the original policy and rectifies the decision, ultimately selecting the optimal action that aligns with the users intent. Optimization (GRPO) (Shao et al., 2024), which estimates the baseline using the group average of rewards from sampled outputs {y1, . . . , yG} from the old policy πθold. For each input x, we compute the advantages Ai by normalizing rewards within the group: where the world models foresight effectively enhances GUI agent planning and decision-making. Accordingly, we structure our evaluation to benchmark simulation quality in Section 4.1, followed by an assessment of Code2Worlds practical impact on assisting GUI agents in Section 4.2. Ai = Rtotal(yi) mean({Rtotal(yj)}G j=1) std({Rtotal(yj)}G j=1) + ϵ (4) 4.1. Evaluation for Next UI Prediction The model parameters θ are then updated by maximizing the surrogate objective with KL-divergence penalty to maintain proximity to the reference SFT policy: LGRPO(θ) = Ex (cid:20) 1 (cid:88) i= min(cid:0)ρiAi, clip(ρi, 1 ϵ, 1 + ϵ)Ai (cid:1) βDKL(πθ πsft) (cid:21) , To comprehensively assess the capability of next UI prediction, we propose holistic evaluation protocol tailored for GUI environments. Unlike general image generation tasks, GUI dynamics requires strict adherence to interaction rules and precise structural rendering. Therefore, we design metrics across two complementary dimensions by employing unified VLM-as-a-Judge framework. (5) Functional Logic. This dimension evaluates the correctness of state transitions, verifying whether the simulation adheres to the underlying interaction rules. where ρi = πθ(yix) πθold (yix) is the importance sampling ratio. Through this process, Code2World iteratively refines its generation policy to maximize both visual fidelity and logical coherence. 4. Evaluation and Application of Code2World What defines high-quality GUI world model? We posit that reliable virtual sandbox in GUI domain must satisfy two critical requirements: (1) Precise Next UI Prediction, where the model must simultaneously achieve high visual fidelity and strictly adhere to interaction logic during state transitions, and (2) Effective GUI Agent Enhancement, Action Adherence Sad: Determines if the generated state logically ensues from the executed action and the initial state, preventing hallucinations where the visual update contradicts the intended interaction logic. Action Identifiability Sid: Tests if the executed action can be correctly inferred solely from the visual difference between the origin and the generated state, measuring the causal clarity of the simulation. Visual Quality. This dimension assesses the fidelity of the rendered interface. Beyond standard semantic metrics like SigLIP (Zhai et al., 2023) and DINO (Oquab et al., 2023) which capture high-level similarity, we introduce two 5 Code2World: GUI World Model via Renderable Code Generation specialized metrics to scrutinize fine-grained details: 5.1. World Model Ability (RQ1) Element Alignment Sele: Checks if key UI elements align precisely with the ground truth layout. Layout Integrity Slay: Evaluates the preservation of layout geometry and structural containment. The formal formulations and detailed implementations of these metrics are provided in Appendix C. 4.2. Application for GUI Agent Code2World serves as plug-and-play module to enhance any existing GUI agent. As illustrated in Figure 3, we seamlessly integrate it as look-ahead simulator during inference via Propose, Simulate, Select pattern. Given the current observation It, the task goal and the interaction history Ht, the GUI agent generates set of candidate proposals Pt, where each proposal comprises reasoning rationale r(k) , and an initial confidence score c(k) , an executable action a(k) : t Pt = Agent(It, G, Ht) = (cid:110)(cid:16) r(k) , a(k) , c(k) (cid:17)(cid:111)K k=1 . (6) For each proposal, Code2World predicts the next GUI state by generating structured HTML Code and rendering it into an image: t+1 = Mθ(It, a(k) (k) , G), ˆI (k) t+1 = (cid:16) (k) t+1 (cid:17) . (7) Finally, the scorer selects the most promising action by evaluating the task goal and simulated outcomes: = arg max a(i) At S(cid:0)It, G, a(i) , ˆI (i) t+1 (cid:1), (8) where S() can be implemented as VLM-based verifier that judges which predicted next state best advances progress toward the task goal G, effectively filtering out hallucinations or illogical plans. 5. Experiments We structure our experimental analysis to investigate three core research questions: RQ1: How effectively can Code2World predict next GUI observation in both in-domain and out-of-distribution settings? RQ2: Can Code2World enhance the GUI agents navigation in both offline and online setting? RQ3: How does each component of Code2World contribute to the overall performance? Notably, for each setting, we carefully tailor the baselines and metrics to the corresponding research question. 6 Benchmarks and Metrics. We rigorously evaluate the generalization capability of Code2World on unseen applications. We employ two benchmarks: (1) Android Control serves as the In-Domain (ID) setting, containing applications not seen during training, evaluating generalization within the same mobile device used in training. (2) GUI Odyssey represents the Out-of-Distribution (OOD) setting, serving as more challenging Cross-App setting with diverse UI styles and domains to test robustness against out-of-distribution device shifts. To strictly quantify performance, we report results using the four VLM-based metrics (Sad, Sid, Sele, Slay) defined in our evaluation protocol (Section 4.1), along with standard image similarity metrics (SigLIP, DINO). Baselines. We compare Code2World against comprehensive suite of state-of-the-art models, categorized by their generation modality. For Image Generation Models, we compare against pixel-space synthesis approaches, specifically Gemini-3-Pro-Image (Google DeepMind, 2025), GPTImage-1, Doubao-Seedream-4.5, Qwen-Image-Edit-Max, and Janus-Pro-7B (Chen et al., 2025). For Code Generation Models, we evaluate VLMs in zero-shot setting where they predict the next GUI state by directly generating HTML. This includes proprietary models such as Claude4.5-Sonnet, Gemini-3-Flash (Team, 2025a), and GPT-5 (Hurst et al., 2024), alongside leading open-source models including JanusCoderV-7B (Sun et al., 2025), Qwen3-VL8B (Team, 2025b), Qwen2.5-VL-72B (Bai et al., 2025), InternVL3-78B (Zhu et al., 2025), and GLM-4.6V-106B (Team, 2025c). Quantitative Comparison. As shown in Table 1, Code2World is lightweight yet powerful. Despite its compact 8B size, Code2World outperforms open-source baselines scaling over 10 in parameters across both dynamic logic and visual quality dimensions, rivaling proprietary giants like GPT-5 and Gemini-3-Pro-Image. Our analysis identifies distinct limitations inherent to existing open-source approaches. Large generalist VLMs (e.g., InternVL378B, GLM-4.6V-106B), while capable of inferring semantic state transitions, often lack the specialized UI-to-Code alignment required to faithfully reconstruct the future layout, evidenced by their suboptimal visual quality scores. Conversely, image editing models (e.g., Qwen-Image-Edit, Janus-Pro-7B) struggle to predict precise interface dynamics, prioritizing local texture consistency over the global interaction logic necessary for navigation. Code2World effectively bridges these gaps, delivering high-fidelity simulation in both structure and logic without such shortcomings. This validates the potential of renderable code generation paradigm to unlock GUI world modeling capabilities in lightweight VLM architectures. Robust Generalization. Performance on the GUI Odyssey Code2World: GUI World Model via Renderable Code Generation Table 1. Quantitative Comparison of various image and code generation models on Android Control (ID) to assess basic capabilities on the same device, and GUI Odyssey (OOD) to test generalization robustness across unseen devices and cross-app scenarios. The best scores are in bold while the second best are in underlined. Model Functional Logic Visual Quality Functional Logic Visual Quality Android Control (ID) GUI Odyssey (OOD) Sad Sid Sele Slay SigLIP DINO Sad Sid Sele Slay SigLIP DINO Gemini-3-Pro-Image GPT-Image-1 Doubao-Seedream-4.5 Qwen-Image-Edit-Max Janus-Pro-7B GPT-5 Gemini-3-Flash Claude-Sonnet-4.5 JanusCoderV-7B Qwen3-VL-8B Qwen2.5-VL-72B InternVL3-78B GLM-4.6V-106B 92.63 89.59 85.36 57.55 58.10 94.02 92.65 89.60 57.12 59.20 73.34 72.41 91.62 Code2World-8B (Ours) 94. 83.67 71.40 86.15 54.12 53.68 90.15 84.08 86.12 56.05 65.80 70.12 67.35 74.23 88.64 Image Generation Models 63.67 56.22 55.82 46.33 45.90 84.89 77.58 81.76 70.39 79. 64.11 61.00 52.19 53.61 54.30 Code Generation Models 69.78 69.74 62.25 31.09 42.70 48.26 46.73 58.74 78.07 81.17 75.97 59.89 63.88 68.45 62.64 67.26 41.74 45.72 36.72 21.73 28.86 30.12 25.42 27.42 68.47 58.78 59.08 54.05 55. 74.13 74.52 65.80 30.18 43.10 47.15 45.06 61.95 88.50 91.63 89.30 69.05 68.40 93.45 92.61 82.81 86.64 34.60 56.12 60.25 55.40 71.35 70.32 79. 49.18 92.73 75.20 71.40 66.04 65.11 63.30 78.22 82.04 79.17 68.11 40.00 54.08 58.10 52.80 78.22 45.60 42.04 42.71 38.24 39. 64.41 59.43 43.75 46.42 14.20 21.45 25.30 30.15 42.30 39.80 40.75 34.28 33.95 60.33 55.12 41.56 43.85 14.00 20.73 24.88 28.90 78.74 76.46 72.80 66.46 67.05 70.79 71.19 64.72 74.82 58.95 55.25 60.45 62.10 48.10 46.64 37.83 47.77 48. 46.35 43.27 34.94 41.65 32.49 30.20 31.50 33.25 55.46 49.78 74.40 46.43 benchmark (OOD) further validates Code2Worlds capability to internalize interaction dynamics rather than merely memorizing specific layouts. As expected, the shift to diverse, unseen cross-app environments causes natural decline in visual similarity metrics across all models compared to the in-domain Android Control. Crucially, however, Code2World exhibits exceptional robustness in dynamic logic, maintaining Sad of 92.73 and Sid of 78.22minimal fluctuation relative to the marked performance decay seen in open-source counterparts. This consistency suggests that Code2World has internalized the fundamental interaction dynamics of GUIs, which confirm its viability as generalpurpose world model capable of operating in novel digital environments. Table 2. Performance comparison on AndroidControl-High. Model GPT-4o Gemini-2.5-Flash GUI-R1-7B InfiGUI-R1-3B UI-TARS-1.5-7B Mobile-Agent-v3-7B +Code2World Improvement Qwen2.5-VL-7B +Code2World Improvement AndroidControl-High Type 62.14 67.43 78.45 83.16 73.36 82.05 84.13 +2.09 76.66 78.74 +2.08 Grounding SR 31.82 33.29 75.64 74.51 77.02 75.16 78.78 +3.62 69.64 74.87 +5.23 21.2 27.9 67.15 70.98 61.57 67.20 68.41 +1.21 65.16 66.47 +1. Qualitative Comparison. Figure 4 provides visual confirmation that directly corroborates the failure modes identified in our quantitative analysis. Reflecting the capacity deficit of smaller models, Qwen3-VL-8B fails to comprehend the transition logic entirely, merely reproducing the initial state with artifacts rather than predicting the future. Exemplifying the reasoning-execution gap, Qwen2.5-VL-72B correctly infers the semantic intent to switch to search interface but fails to translate this into coherent visual form, resulting in chaotic layout that lacks clarity. Similarly, verifying the structural rigidity of pixel-based generation, Gemini-3-ProImage excels at texture synthesis but remains constrained to the original layout geometry, unable to hallucinate the entirely new page structure required by the navigation. In stark contrast, Code2World bridges these gaps by accurately predicting both the logical jump and fine-grained visual details, achieving near-perfect alignment with the ground truth. compelling instance is that Code2World even simulates temporal passage, evidenced by the time change (5 : 46 5 : 47) during the state transition. This intriguing detail exemplifies the models robust world modeling capabilities, demonstrating its aptitude for capturing latent environmental changes and predicting them with both logical correctness and high visual fidelity. More Qualitative Analysis can be seen in Appendix E.1. 5.2. GUI Agent Enhancement (RQ2) Offline Navigation. We evaluate the effectiveness of Code2World in enhancing agents capabilities on AndroidControl-High (Li et al., 2024). This benchmark is highly challenging, as it provides only the user task, requiring the agent to autonomously plan step-wise action, thereby evaluating its single-step decision-making ca7 Code2World: GUI World Model via Renderable Code Generation Figure 4. Qualitative comparison of next GUI state generation over Code2World and three baselines. The red circle in origin state indicates the users click position targeting the search bar. Online Application. Although Code2World demonstrates effective improvements in the offline setting, such evaluation is inherently constrained by pre-recorded human trajectories. As result, it cannot assess critical capabilities required in real-world deployment, such as exploring alternative valid trajectories or recovering from errors. To further validate the effectiveness of Code2World in real-world navigation scenarios, we conduct online evaluation in AndroidWorld (Rawles et al., 2025), an environment designed for developing and benchmarking autonomous agents on live Android emulator, comprising 116 tasks across 20 mobile applications. Unlike offline evaluation, which focuses on single-step action prediction, AndroidWorld requires agents to generate continuous action sequences and measures performance by the task success rate (SR). We adopt the default M3A agent framework provided by AndroidWorld and evaluate multiple VLMs, including two closed-source models (GPT-4o, Gemini-2.5-Flash) and two open-source models (Qwen3-VL-8B, GLM-4.6V-Flash (Team, 2025c)). As shown in Figure 5, Code2World consistently improves task success rates across all evaluated models, demonstrating its plug-and-play effectiveness. This improvement stems from Code2Worlds renderable code generation and renderaware reinforcement learning strategy, which enable accurate and realistic GUI prediction. By providing agents with foresight into future GUI states, Code2World allows effective exploration of multiple candidate actions while reliably selecting the most advantageous one, thereby substantially enhancing long-horizon reasoning ability in online interaction. More Visualization cases of Code2World enhance GUI Agent decision-making can be found in the Appendix E.2. 5.3. Ablation Study (RQ3) Impact of Components on Next UI Prediction. We investigate the contribution of each training stage in Table 3. Figure 5. Performance comparison on the AndroidWorld. pability. We apply Code2World to enhance both specialized GUI agent (Mobile-Agent-v3 (Ye et al., 2025)) and general MLLM (Qwen2.5-VL-7B), and further compare against GPT-4o (Hurst et al., 2024), Gemini-2.5Flash (Team, 2025a), GUI-R1-7B (Luo et al., 2025b), InfiGUI-R1-3B (Liu et al., 2025b), and UI-TARS-1.57B (Qin et al., 2025). We report Action Type accuracy, Grounding Accuracy, and the overall Success Rate (SR). As shown in Table 2, incorporating Code2World yields substantial improvement for the Qwen2.5-VL-7B, achieving an improvement of 5.23 in Grounding accuracy, making it comparable to domain-specific GUI agents and even surpassing InfiGUI-R1. Moreover, although Mobile-Agent-v3-7B is trained on task-specific data and already attains strong performance, it still benefits from Code2World, achieving the best accuracy on both Type and Grounding, which validates the plug-and-play versatility of our approach. In summary, by functioning as model-agnostic simulator to enable future GUI prediction, Code2World empowers the agent to better understand the consequences of each action, thereby selecting actions that more closely align with user intent and significantly improving single-step decision-making performance. 8 Code2World: GUI World Model via Renderable Code Generation Table 3. Performance comparison world model ability of Qwen3VL-8B (Base) and Code2World variants on Android Control. Model Android Control Functional Logic Visual Quality Sad 59.20 Qwen3-VL-8B Qwen3-VL-8B+SFT 78.45 Qwen3-VL-8B+SFT+Rsem 78.90 Qwen3-VL-8B+SFT+Ract 88.20 94.28 Code2World Sid 65.80 79.12 78.85 84.53 88.64 Sele Slay SigLIP DINO 43.10 58.65 65.40 60.10 71.35 42.70 56.30 61.20 57.80 70. 63.88 71.50 76.10 74.80 79.44 28.86 39.40 44.50 40.60 49.18 Figure 6. Ablation analysis of training pipeline design for task success rate (SR) on AndroidWorld. Base denotes Qwen3-VL-8B. +SFT only and reward-based variants (+SFT+ Rsem, +SFT+ Ract) represent additional training applied on the Base model. Applying SFT instills fundamental HTML syntax and layout rules, establishing strong foundation that yields substantial improvements across both functional logic and visual quality metrics. However, refining the model solely with the visual reward (Rsem) reveals limitation: while rendering quality improves, functional logic remains stagnant or slightly regresses. This suggests degree of reward hacking, where the model prioritizes superficial pixel alignment with the ground truth rather than mastering the underlying state transition logic essential for world model. Conversely, the action reward (Ract) primarily boosts dynamic logic by enforcing correct state transitions. While this ensures the generated view corresponds to the correct target state, resulting in moderate visual gains, it lacks the fine-grained feedback needed for high-fidelity rendering. Ultimately, Code2World integrates both rewards to achieve optimal performance, verifying that combining semantic alignment with interaction logic is critical for robust world model. Impact of Components on GUI Agent Enhancement. We further analyze how varying qualities of the world model impact the downstream decision-making of the Gemini-2.5Flash agent on the AndroidWorld benchmark. As shown in Figure 6, the standalone Gemini-2.5-Flash agent achieves baseline Success Rate (SR) of 41.4%. Merely equipping it with naive simulator (+ Qwen3-VL-8B) yields negligible gain (+1.2%), indicating that low-fidelity predictions fail to provide reliable foresight. In contrast, integrating our SFT model triggers sharp performance jump to 47.5%, demonstrating that structured, parsable HTML predictions effectively ground the agents planning. The subsequent RL stages progressively refine this capability: Rsem enhances the visual clarity of the simulated future, pushing the SR to 49.2%, while Ract ensures the simulator correctly reflects interaction dynamics, further raising the SR to 50.1%. Finally, the full Code2World model, combining all optimizations, achieves peak SR of 50.9% (+9.5% improvement over the baseline). This strong correlation between world model fidelity and agent success rate underscores that accurate, renderable visual foresight is the key to unlocking robust long-horizon reasoning. 6. Conclusion In this work, we introduced Code2World, pioneering codenative GUI world model that fundamentally shifts next UI prediction from raw pixel estimation to renderable HTML code generation, uniquely combining high-fidelity visualization with fine-grained structural controllability. We constructed AndroidCode (over 80K screenaction pairs) using visual-feedback revision loop, and proposed RenderAware Reinforcement Learning to align code prediction with rendered visual fidelity and action consistency. Functioning as learnable virtual sandbox, Code2World empowers autonomous GUI agents to navigate complex, dynamic interfaces with human-like foresight. Empirically, Code2World8B achieves state-of-the-art performance in next UI prediction and, acting as plug-and-play simulator, significantly enhances downstream agents, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents significant advancement in the field of autonomous GUI agents. The primary societal benefit of our work lies in enhancing digital inclusivity. By empowering agents to handle complex interface interactions, this technology serves as an assistive tool for users with disabilities while simultaneously relieving humans from repetitive digital labor. Uniquely, as World Model, Code2World contributes to AI safety by providing sandbox environment. This allows agents to simulate and evaluate potentially irreversible actions, such as financial transactions or data deletion, without executing them in the real world, thereby mitigating the risks associated with on-policy trial-and-error learning. However, we acknowledge potential risks. If the world model hallucinates incorrect safety cues, it could mislead an agent into executing harmful actions. Additionally, as with any advanced automation technology, there is potential for misuse in automated cyber-attacks or navigation spamming. We encourage the research community to focus on robust verification mechanisms to ensure that the predictive capabilities of such models are deployed responsibly and ethically. Code2World: GUI World Model via Renderable Code Generation"
        },
        {
            "title": "References",
            "content": "Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Cao, Y., Zhong, Y., Zeng, Z., Zheng, L., Huang, J., Qiu, H., Shi, P., Mao, W., and Guanglu, W. Mobiledreamer: Generative sketch world model for gui agent. arXiv preprint arXiv:2601.04035, 2026. Chae, H., Kim, N., Ong, K. T.-i., Gwak, M., Song, G., Kim, J., Kim, S., Lee, D., and Yeo, J. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232, 2024. Chen, X., Wu, Z., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., and Ruan, C. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. Cheng, W., Ni, E., Wang, W., Sun, Y., Liu, J., Shen, W., Chen, Y., Shi, B., and Wang, D. Mga: Memory-driven gui agent for observation-centric interaction. In Proceedings of WSDM, 2025a. Cheng, Z., Chen, Q., Xu, X., Wang, J., Wang, W., Fei, H., Wang, Y., Wang, A. J., Chen, Z., Che, W., et al. Visual thoughts: unified perspective of understanding multimodal chain-of-thought. arXiv preprint arXiv:2505.15510, 2025b. Chu, X., Huang, H., Zhang, X., Wei, F., and Wang, Y. Gpg: simple and strong reinforcement learning baseline for model reasoning. In Proceedings of ICLR, 2026. Dai, Y., Ji, Y., Zhang, X., Wang, Y., Chu, X., and Lu, Z. Harder is better: Boosting mathematical reasoning via difficulty-aware grpo and multi-aspect question reformulation. In Proceedings of ICLR, 2026. Feizi, A., Nayak, S., Jian, X., Lin, K. Q., Li, K., Awal, R., L`u, X. H., Obando-Ceron, J., Rodriguez, J. A., Chapados, N., et al. Grounding computer use agents on human demonstrations. arXiv preprint arXiv:2511.07332, 2025. Google DeepMind. Gemini 3 Pro: The next generahttps://docs.cloud. tion of multimodal ai. google.com/vertex-ai/generative-ai/ docs/models/gemini/3-pro, 2025. Accessed: 2026-01-27. Gu, Y., Zhang, K., Ning, Y., Zheng, B., Gou, B., Xue, T., Chang, C., Srivastava, S., Xie, Y., Qi, P., et al. Is your llm secretly world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559, 2024. Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., et al. Cogagent: visual language model for gui agents. In Proceedings of CVPR, 2024. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Ji, Y., Ma, Z., Wang, Y., Chen, G., Chu, X., and Wu, L. Tree search for llm agent reinforcement learning. In Proceedings of ICLR, 2026. Li, S., Kallidromitis, K., Gokul, A., Kato, Y., Kozuka, K., and Grover, A. Mobileworldbench: Towards semantic world modeling for mobile agents. arXiv preprint arXiv:2512.14014, 2025. Li, W., Bishop, W. E., Li, A., Rawles, C., Campbell-Ajala, F., Tyamagundlu, D., and Riva, O. On the effects of data scale on ui control agents. In Proceedings of NIPS, 2024. Lin, K. Q., Li, L., Gao, D., Wu, Q., Yan, M., Yang, Z., Wang, L., and Shou, M. Z. Videogui: benchmark for gui automation from instructional videos. In Proceedings of NIPS, 2024. Lin, K. Q., Hu, S., Li, L., Yang, Z., Wang, L., Torr, P., and Shou, M. Z. Computer-use agents as judges for generative user interface. arXiv preprint arXiv:2511.15567, 2025a. Lin, K. Q., Li, L., Gao, D., Yang, Z., Wu, S., Bai, Z., Lei, S. W., Wang, L., and Shou, M. Z. Showui: One vision-language-action model for gui visual agent. In Proceedings of CVPR, 2025b. Liu, T., Wang, C., Li, R., Yu, Y., He, X., and Song, B. Guirise: Structured reasoning and history summarization for gui navigation. In Proceedings of NIPS, 2025a. Liu, Y., Li, P., Xie, C., Hu, X., Han, X., Zhang, S., Yang, H., and Wu, F. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025b. Lu, F., Zhong, Z., Liu, S., Fu, C.-W., and Jia, J. Arpo: Endto-end policy optimization for gui agents with experience replay. In Proceedings of NIPS, 2025a. Lu, G., Jia, B., Li, P., Chen, Y., Wang, Z., Tang, Y., and Huang, S. Gwm: Towards scalable gaussian world models for robotic manipulation. In Proceedings of ICCV, 2025b. Lu, Z., Chai, Y., Guo, Y., Yin, X., Liu, L., Wang, H., Xiao, H., Ren, S., Xiong, G., and Li, H. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025c. 10 Code2World: GUI World Model via Renderable Code Generation Luo, D., Tang, B., Li, K., Papoudakis, G., Song, J., Gong, S., Hao, J., Wang, J., and Shao, K. Vimo: generative visual gui world model for app agents. arXiv preprint arXiv:2504.13936, 2025a. Sun, Q., Gong, J., Liu, Y., Chen, Q., Li, L., Chen, K., Guo, Q., Kao, B., and Yuan, F. Januscoder: Towards foundational visual-programmatic interface for code intelligence. arXiv preprint arXiv:2510.23538, 2025. Luo, R., Wang, L., He, W., Chen, L., Li, J., and Xia, X. Guir1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025b. Nayak, S., Jian, X., Lin, K. Q., Rodriguez, J. A., Kalsi, M., Chapados, N., Ozsu, M. T., Agrawal, A., Vazquez, D., Pal, C., et al. Ui-vision: desktop-centric gui benchmark for visual perception and interaction. In Proceedings of ICML, 2025. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Qin, Y., Ye, Y., Fang, J., Wang, H., Liang, S., Tian, S., Zhang, J., Li, J., Li, Y., Huang, S., et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In Proceedings of ICML, 2021. Rawles, C., Clinckemaillie, S., Chang, Y., Waltz, J., Lau, G., Fair, M., Li, A., Bishop, W., Li, W., Campbell-Ajala, F., et al. Androidworld: dynamic benchmarking environment for autonomous agents. In Proceedings of ICLR, 2025. Rivard, L., Sun, S., Guo, H., Chen, W., and Deng, Y. Neuralos: Towards simulating operating systems via neural generative models. arXiv preprint arXiv:2507.08800, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Team, G. Gemini 2.5: Pushing the frontier with adlong context, and arXiv preprint vanced reasoning, multimodality, next generation agentic capabilities. arXiv:2507.06261, 2025a. Team, Q. Qwen3-vl technical report, 2025b. URL https: //arxiv.org/abs/2511.21631. Team, V. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025c. URL https://arxiv.org/abs/ 2507.01006. Wang, Y., Yin, D., Cui, Y., Zheng, R., Li, Z., Lin, Z., Wu, D., Wu, X., Ye, C., Zhou, Y., et al. Llms as scalable, generalpurpose simulators for evolving digital agent training. arXiv preprint arXiv:2510.14969, 2025. Wang, Y., Wang, Y., Dai, R., Wang, Y., Liu, K., Chu, X., and Li, Y. Urban socio-semantic segmentation with visionlanguage reasoning. arXiv preprint arXiv:2601.10477, 2026. Wei, J., Courbis, A.-L., Lambolais, T., Xu, B., Bernard, P. L., and Dray, G. Boosting gui prototyping with diffusion models. In 2023 IEEE 31st International Requirements Engineering Conference (RE). IEEE, 2023. Wu, P., Ma, S., Wang, B., Yu, J., Lu, L., and Liu, Z. Guireflection: Empowering multimodal gui models with selfreflection behavior. In Proceedings of NIPS, 2025. Xiao, H., Wang, G., Chai, Y., Lu, Z., Lin, W., He, H., Fan, L., Bian, L., Hu, R., Liu, L., et al. Ui-genie: selfimproving approach for iteratively boosting mllm-based mobile gui agents. In Proceedings of NIPS, 2025. Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., Hua, T. J., Cheng, Z., Shin, D., Lei, F., et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In Proceedings of NIPS, 2024. Shi, Y., Yu, W., Li, Z., Wang, Y., Zhang, H., Liu, N., Mi, H., and Yu, D. Mobilegui-rl: Advancing mobile gui agent through reinforcement learning in online environment. arXiv preprint arXiv:2507.05720, 2025. Xiong, F., Xu, H., Wang, Y., Cheng, R., Wang, Y., and Chu, X. Hs-star: Hierarchical sampling for self-taught reasoners via difficulty estimation and budget reallocation. In Proceedings of EMNLP, 2025. Su, Z., Xia, P., Guo, H., Liu, Z., Ma, Y., Qu, X., Liu, J., Li, Y., Zeng, K., Yang, Z., et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. Yang, Z., Hong, W., Xu, M., Fan, X., Wang, W., Cheng, J., Gu, X., and Tang, J. UI2Coden: visual language model for test-time scalable interactive ui-to-code generation. arXiv preprint arXiv:2511.08195, 2025. 11 Code2World: GUI World Model via Renderable Code Generation Ye, J., Zhang, X., Xu, H., Liu, H., Wang, J., Zhu, Z., Zheng, Z., Gao, F., Cao, J., Lu, Z., et al. Mobile-agent-v3: Foundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. Yuan, Z., Qu, X., Qian, C., Chen, R., Tang, J., Sun, L., Chu, X., Zhang, D., Wang, Y., Cai, Y., et al. Video-star: Reinforcing open-vocabulary action recognition with tools. In Proceedings of ICLR, 2026. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the ICCV, 2023. Zhang, J., Zhou, Y., Gu, J., Wigington, C., Yu, T., Chen, Y., Sun, T., and Zhang, R. Artist: Improving the generation of text-rich images with disentangled diffusion models and large language models. In Proceedings of WACV, 2025. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of ACL, 2024. Zheng, Y., Lu, J., Wang, S., Feng, Z., Kuang, D., and Xiong, Y. Easyr1: An efficient, scalable, multi-modality rl training framework, 2025. Zhu, J., Wang, W., Chen, Z., Liu, Z., Ye, S., Gu, L., Tian, H., Duan, Y., Su, W., Shao, J., et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 12 Code2World: GUI World Model via Renderable Code Generation A. Implementation Details A.1. Experimental Setup We conduct all experiments on computing cluster equipped with 8 NVIDIA H20 GPUs (96GB of memory). We use Qwen3-VL-8B-Instruct as the backbone of Code2World and conduct SFT with LLaMA-Factory (Zheng et al., 2024) framework and RL based on the EasyR1 (Zheng et al., 2025) framework. To ensure rigorous reproducibility, we standardize configurations across all metrics and baselines. For VLM-as-a-Judge tasks, encompassing both RL reward calculation and evaluation metric computation, we employ Qwen3-VL-8B-Instruct as the unified judge. This model is configured with temperature of 0.1 to ensure deterministic outputs and maximum generation limit of 1,024 tokens. For visual feature metrics, we compute SigLIP scores using the google/siglip-so400m-patch14-384 checkpoint and DINO scores using the facebook/dinov2-giant model. For baseline comparisons, we fix specific API versions to ensure consistency: claude-4.5-sonnet-20251120 for Claude and gpt-4o-mini-2024-07-18 for GPT-4o-mini. Across all generation models (proprietary and opensource), we unify the maximum generation length to 8,192 tokens to accommodate complex HTML structures, with sampling temperature set to 0.7. A.2. Training Hyperparams Data Allocation. We partition the AndroidCode dataset into two disjoint subsets to support the two-stage training strategy. Specifically, we allocate 70% of the samples for Stage 1 (SFT) to establish the foundational policy, while the remaining 30% are employed in Stage 2 (RL) to further align the model with visual and logical rewards. Stage 1: Code-level Supervised Fine-tuning (SFT). We initialize the backbone with Qwen3-VL-8B-Instruct. We perform full parameter fine-tuning on the language model while freezing both the vision encoder and the multimodal projector to preserve pre-trained visual perception capabilities. To optimize memory efficiency and training throughput, we utilize DeepSpeed ZeRO-2 and Flash Attention. The model is trained for 2 epochs with global batch size of 64 (configured as per-device batch size of 2 with 4 gradient accumulation steps). We employ cosine learning rate schedule with peak learning rate of 2 105 and warmup ratio of 0.1. To accommodate the high resolution of smartphone screenshots (1080 2400) and verbose HTML code, we set the cutoff length to 24,576 tokens. Stage 2: Render-Aware Reinforcement Learning (RL). We align the SFT model using Group Relative Policy Optimization (GRPO) with Flash Attention enabled. For each prompt, the policy generates group of = 4 candidate outputs with sampling temperature of 1.0 to encourage exploration. We set the rollout batch size to 16. The max prompt length and max response length are set to 24,576 and 8,192 tokens to support complex UI structures. We set the learning rate to 1 106 and apply KL-divergence penalty with coefficient of β = 0.01. The visual semantic reward (Rsem) and action consistency rewards (Ract) are weighted equally during optimization. B. Training Data Construction B.1. Synthesis and Revision Algorithm The detailed construction pipeline for AndroidCode is formally presented in Algorithm 1. While the renderable code generation pipeline is outlined in the main text, we specify the operational thresholds and the exact revision logic here. As depicted in Algorithm 1, We utilize powerful multimodal coder (GPT-5) to translate pixel-based screenshots into structured HTML and implement revision mechanism with visual feedback to ensure high data fidelity. We set the visual similarity threshold τ = 0.9, calculated using the SigLIP cosine similarity between the ground truth screenshot and the rendered generation ˆI. Generated samples scoring below this value trigger the revision loop. To ensure training efficiency and prevent infinite loops, we limit the maximum number of revision iterations to Nmax = 1. Crucially, we feed the triplet of (Ground Truth I, Rendered Image ˆI, Current Code C) directly into the multimodal coder. This allows the coder to autonomously perform visual comparison, identify discrepancies, and rectify the structural code. Samples that fail to meet the quality threshold after Nmax attempts are discarded to ensure the purity of the final training corpus. 13 Code2World: GUI World Model via Renderable Code Generation Algorithm 1 Automated Data Synthesis with Visual-Feedback Revision Require: Raw GUI dataset Draw = {I1, . . . }, Multimodal Coder Mcoder, Browser Renderer R, Alignment Metric SigLIP(), Threshold τ , Max iterations Nmax. Ensure: High-fidelity corpus ANDROIDCODE Dsyn = {(Ik, Ck)}. 1: Dsyn 2: for all (I) Draw do 0, 0 3: // Stage 1: Constrained Initial Synthesis 4: Mcoder(I, Promptinit) {Generate initial HTML with semantic placeholders} 5: ˆI R(C) {Render code into visual state} 6: SigLIP( ˆI, I) {Compute initial visual alignment score} 7: // Stage 2: Revision with Visual Feedback 8: while < τ and < Nmax do 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end if 19: 20: end for 21: return Dsyn + 1 {Model visually compares GT and Rendered image to fix code} Mcoder(I, ˆI, C, Promptrevision) ˆI R(C) SigLIP( ˆI, I) end while // Filtering & Collection if τ then Dsyn Dsyn {(I, C)} {Retain only high-quality pairs} B.2. Multimodal Instruction Formatting Based on the AndroidCode, constructing high-fidelity instruction-tuning dataset requires bridging the semantic gap between low-level interaction logs and the high-level reasoning capabilities of VLMs. Raw interaction traces, consisting solely of raw pixels and sparse coordinate metadata, often lack the semantic density required for VLMs to effectively ground user intent. To bridge the gap between low-level execution logs and high-level visual reasoning, we design meticulous data pre-processing pipeline that augments both visual and textual modalities. This ensures the instruction-tuning data is not only syntactically structured but also semantically explicit. Visual Prompting. Raw Coordinate-based interaction records are inherently abstract to vision encoders. To explicitly ground the models spatial attention on the target elements, we adopt visual prompting strategy, which has been proven effective in enhancing the referring capabilities of VLMs (Cheng et al., 2025b). Instead of relying on the model to implicitly infer locations from numerical tokens, we render visual markers directly onto the input image It. Specifically, for point-based interactions (e.g., Click, Long Press), we render semi-transparent red circle (radius=20px, α = 0.6) centered at (x, y), creating visual anchor that eliminates ambiguity regarding the precise touch position. For gesture-based interactions (e.g., Scroll, Swipe), we overlay directional arrow indicating the fingers movement trajectory (e.g., an upward arrow for scroll down command). This visualizes the dynamic flow of the operation, allowing the model to correlate the static frame with the intended motion. Instruction Expansion. Raw action primitives (e.g., JSON objects or bare coordinates like {\"action\":\"click\", \"loc\":[200,300]}) are disjoint from the natural language pre-training distribution of VLMs, often leading to suboptimal intent understanding. To address this, we implement deterministic expansion engine that transforms rigid action metadata into rich, descriptive natural language narratives. For instance, abstract coordinates are converted into explicit spatial descriptions such as: User performed CLICK at coordinates (x, y). Expect the button at this location to trigger, effectively prompting the model to attend to the causal relationship between the location and the UI element. Similarly, complex dynamics are decomposed into clear semantic instructions: scrolling is described by its consequential effect (e.g., The content should move, revealing new items...), while text inputs are formatted to emphasize content injection (e.g., The focused input field MUST now contain this text). 14 Code2World: GUI World Model via Renderable Code Generation Finally, these augmented visual hints and expanded textual descriptions are integrated into carefully designed, standardized prompt template (detailed in Appendix D.2), ensuring the model strictly follows the instruction context to simulate interface dynamics accurately. C. Evaluation Metrics of Next UI Prediction To rigorously assess the capability of GUI World Models in simulating GUI dynamics, we identify that general image similarity scores (e.g., SSIM, LPIPS) are insufficient. They predominantly focus on pixel-level texture, failing to capture the unique requirements of GUI environments: strict adherence to interaction rules (e.g., button click must trigger deterministic state change) and precise structural rendering (e.g., elements must remain aligned in the DOM tree). To bridge this gap, we propose novel and holistic evaluation protocol tailored for GUI World Models. This protocol introduces four specialized metrics across two complementary dimensions: Functional Logic and Visual Quality. These metrics are designed to provide fairer and more granular comparison. We employ unified VLM-as-a-Judge framework to approximate human judgment. The detailed prompts for these VLM-based metrics are provided in Appendix D.4. C.1. Functional Logic Functional Logic evaluates the functional correctness of state transitions, verifying whether the world model acts as reliable simulator. Action Adherence (Sad) This metric assesses whether the predicted next state ˆIt+1 is logically valid consequence of executing action at on state It. Unlike simple visual coherence, Sad penalizes hallucinations where the visual update contradicts the intended interaction (e.g., clicking Back but staying on the same page). Formally, let Jact be the VLM judge, the score for test dataset is defined as: Sad = 1 Dtest (cid:88) Jact(It, at, ˆIt+1) (It,at)Dtest (9) Action Identifiability (Sid) This metric evaluates the causal clarity of the generation. high-fidelity simulation should allow an observer to infer the cause of state change solely from the visual outcome. We instruct the VLM to act as an inverse dynamics model Jinv, predicting the action type ˆat based on the visual difference between It and ˆIt+1. Crucially, high Sid ensures that the Selector in our agent pipeline can correctly verify whether simulated outcome matches the planned action. The metric is calculated as the classification accuracy: Sid = 1 Dtest (cid:88) (cid:104) (cid:105) Jinv(It, ˆIt+1) = type(at) (It,at)Dtest (10) C.2. Visual Quality Models following the renderable code generation paradigm typically employ semantic placeholder strategy to guarantee structural correctness while avoiding the hallucination of external assets. Standard embedding metrics (e.g., SigLIP, DINO), which primarily capture high-level semantic similarity, are ill-suited for this nuance; they lack the granularity to explicitly measure fine-grained element alignment and structural layout, often penalizing valid stylistic abstractions. To address this, we propose two specialized metrics to disentangle structural fidelity from textural style. Element Alignment (Sele). This metric verifies the fine-grained positioning of UI components. It measures whether key t+1 are accurately reflected in the generation ˆIt+1 at interactive elements (buttons, inputs) present in the ground truth correct relative coordinates, explicitly tolerating semantic placeholders provided they occupy the correct screen area. Layout Integrity (Slay). This metric evaluates global layout integrity, penalizing issues common in weak code generation such as CSS collapse, overlapping containers, or misalignment. Formally, the VLM judge Jvis compares the generated output against the ground truth under specific criteria and provides 15 Code2World: GUI World Model via Renderable Code Generation composite score: D. Prompt Template D.1. Data Synthesis Constrained Initial Generation Sele/stc = 1 Dtest (cid:88) Jvis(I t+1, ˆIt+1) t+1Dtest (11) You are an expert Front-End Engineer. Your task is to generate the corresponding HTML code for the given UI screenshot. Reference Dimensions: {width}px (Width) {height}px (Height). 1. CRITICAL STRUCTURAL RULES (Must Follow) Full Document: Output COMPLETE HTML document starting with <!DOCTYPE html> and ending with </html>. Root Container: Wrap the ENTIRE content inside single container: <div id=\"render-target\"> ...</div>. Container Sizing: screenshot, #render-target MUST have width:{width}px and height:{height}px. Use position:relative and overflow:hidden to strictly enforce boundaries. is full viewport Since this Styling Target: Apply all outer styles (border-radius, shadows, background-color) to #render-target, NOT the body. 2. BODY & RENDER SETTINGS Reset: The <body> tag must have margin:0; padding:0;. Transparency: The <body> background must be transparent. This is crucial for the rendering engine. Alignment: Do NOT use flexbox/grid centering on the <body>. Let the #render-target sit naturally at the top-left (0,0) coordinates. 3. VISUAL ASSETS & ACCURACY Layout Accuracy: Match font sizes, padding, margins, and element positioning exactly to the screenshot. Use Flexbox or Grid layouts appropriate for the detected device type. Images: Use Semantic Text Placeholders. Do NOT use random URLs. Create div with fixed background #E0E0E0 and thin border. Inside, place concise text label (e.g., [IMG: Red Shoe]). Icons: Use Simple Inline SVGs to match the visual style exactly. Keep path data short. Fallback to colored Unicode characters if the icon is extremely complex. 4. OUTPUT FORMAT Return ONLY the raw HTML code. Do not wrap in Markdown blocks (no html). Do not include conversational text. COMMAND: Generate the valid HTML code now, starting strictly with <!DOCTYPE html>. 16 Code2World: GUI World Model via Renderable Code Generation Revision with Visual Feedback TASK: HTML Revision with Visual Feedback You are given: (1) target screenshot (the desired UI), (2) rendered screenshot produced by the current HTML, and (3) the current HTML. Your goal is to revise the HTML so that its rendering matches the target screenshot as closely as possible. You must infer the visual differences by comparing the target and rendered screenshots yourself (no diff text is provided). INPUTS Viewport: width = {width}px, height = {height}px. Target Screenshot: {TARGET_IMAGE} Rendered Screenshot: {RENDERED_IMAGE} Current HTML: {CURRENT_HTML} 1. CRITICAL STRUCTURAL RULES (Must Follow) Full Document: Output COMPLETE HTML document starting with <!DOCTYPE html> and ending with </html>. Root Container: Wrap the ENTIRE visible UI inside single container: <div id=\"render-target\"> ...</div>. Container Sizing: #render-target MUST be exactly width:{width}px; height:{height}px; with position:relative; overflow:hidden;. Styling Target: Apply all outer styles (rounded corners, shadows, background) to #render-target, NOT the body. 2. REVISION PROCEDURE (You Must Follow This Order) Step 1: Diagnose: Compare {TARGET_IMAGE} vs. {RENDERED_IMAGE} and identify mismatches. Step 2: Prioritize: Fix issues in this order: (i) layout/geometry, (ii) component sizing/spacing, (iii) typography, (iv) colors, (v) shadows/borders, (vi) fine details. Step 3: Minimal Edit: Modify {CURRENT_HTML} with the smallest set of changes needed to match the target. Do NOT redesign the UI. Step 4: Consistency: Ensure repeated components (rows/cards/chips) share consistent sizes and spacing. 3. BODY & RENDER SETTINGS (Must Follow) Reset: The <body> tag must have margin:0; padding:0;. Transparency: The <body> background must be transparent. No Body Centering: Do NOT use flexbox/grid centering on <body>. Place #render-target at the top-left (0,0). 4. VISUAL MATCHING REQUIREMENTS Geometry: Match positions, widths/heights, padding/margins, and alignment to the target screenshot. Typography: Match font size, weight, line-height, and letter spacing for titles, labels, and secondary text. Colors: Match major background fills/gradients, chip/button colors, and text colors. Avoid introducing unnecessary new colors. 17 Code2World: GUI World Model via Renderable Code Generation Shadows & Radius: Adjust blur, offset, opacity, and corner radius to match the targets depth and rounding. Z-Order: Fix overlap/stacking differences using position and z-index. 5. VISUAL ASSETS POLICY Images: Do NOT use external URLs. Use semantic placeholders: fixed-size div with background #E0E0E0, thin border, and short label (e.g., [IMG: Avatar]). Icons: Prefer simple inline SVG for common icons. Keep SVG paths short. If too complex, use Unicode symbols. 6. OUTPUT FORMAT (Strict) Return ONLY the revised raw HTML. Do not wrap in Markdown (no html). Do not include explanations, comments, or any extra text. COMMAND: Compare the target and rendered screenshots, revise {CURRENT_HTML} accordingly, and output the final valid HTML now, starting strictly with <!DOCTYPE html>. D.2. Multimodal Instruction-tuning Data Construction Instruction-following System Prompt You are an expert UI State Transition Simulator and Frontend Developer. Your task is to predict the NEXT UI STATE based on screenshot of the current state and user interaction. 1. IMAGE INTERPRETATION RULES The input image contains visual cues denoting the users action. You must interpret them as follows: Red Circle: Indicates Click or Long Press target at that location. Red Arrow: Indicates Scroll or Swipe. The arrow points in the direction of finger movement. Example: An arrow pointing UP means the finger slides up, pushing content up (Scrolling Down). Note: These cues exist ONLY to show the action. DO NOT render these red circles or arrows in your output HTML. 2. CRITICAL STRUCTURAL RULES (MUST FOLLOW) Format: Output ONLY raw HTML. Start with <!DOCTYPE html> and end with </html>. Root Element: All visible content MUST be wrapped in: <div id=\"render-target\"> ... </div> Container Style: #render-target must have: width: 1080px; height: 2400px; position: relative; overflow: hidden; (Apply background colors and shadows here, NOT on the body). Body Style: The <body> tag must have margin: 0; padding: 0; background: transparent;. Layout: Do NOT center the body. Let #render-target sit at (0,0). 3. CONTENT GENERATION LOGIC Transition: Analyze the action. If the user clicks button, show the result (e.g., menu opens, checkbox checks, page navigates). 18 Code2World: GUI World Model via Renderable Code Generation Images: Use semantic text placeholders. DO NOT use real URLs. Format: <div style=\"...\">[IMG: description]</div> Icons: Use simple inline SVG paths or Unicode. 4. OUTPUT REQUIREMENT Do NOT generate Markdown blocks (no html). Do NOT provide explanations or conversational text. Output the code directly. Instruction-following User Prompt <image> INPUT CONTEXT 1. User Intent: {instruction str} 2. Interaction Details: Description: {semantic desc} Action Data: {action json} COMMAND Based on the visual cues in the image and the interaction data above, generate the HTML for the RESULTING UI STATE (what the screen looks like after this action). D.3. Render-Aware RL Reward Design D.3.1. VISUAL SIMILARITY System Prompt You are an expert Visual Structural Similarity Metric. Your task is to evaluate the Visual Similarity between Ground Truth Screenshot (Image 1) and Generated UI (Image 2) for render-aware RL. CONTEXT Image 1 (GT): The real next-screen Android screenshot. Image 2 (Gen): predicted next screen rendered from HTML. Crucial Feature: The renderer uses Gray Placeholders (e.g., [IMG: avatar]) instead of real photos. EVALUATION CRITERIA (Strict Visual Fidelity) You must behave like high-precision metric (a Smart LPIPS): you SHOULD care about layout, pixel positions, component sizes, and hierarchical structure. 1) Layout & Geometry (Most Important, Strict) Are major UI regions (top bar / content / bottom bar) in the correct order and relative proportions? Are key components (titles, buttons, cards, lists, chips, icons) in the same positions and aligned similarly? Are sizes/proportions (width/height) and spacing (margins/padding/line gaps) close to GT? 2) Visual Elements & Text Appearance Does the text look visually consistent (length, density, font-size hierarchy, bold vs regular, line breaks)? 19 Code2World: GUI World Model via Renderable Code Generation Do colors and style cues match (light/dark mode, primary CTA color, highlighted chips, card backgrounds, dividers)? Are icons present and placed correctly (exact SVG details are secondary to correct icon type and placement)? 3) Placeholder Equivalence Rule (Crucial) If Image 2 contains gray placeholder block, DO NOT penalize it for not resembling the real photo. Instead, verify whether the placeholder matches the GT image region in: (a) position, (b) size, and (c) semantic tag correctness. If (a)(b)(c) are satisfied, treat the placeholder region as visually correct. CONTINUOUS SCORING RUBRIC (Float 0.0 10.0, 1 decimal place) 10.0: Near pixel-perfect; geometry/padding match; placeholders match coordinates and semantics. 9.09.9: High fidelity; only minor padding/font-weight/line-break differences. 6.08.9: Structural match; correct overall layout/components but noticeable sizing/spacing or style mismatches. 3.05.9: Rough match; UI type matches but alignment/geometry is messy and multiple elements are shifted/scaled. 0.02.9: Mismatch; wrong screen structure, missing major regions, or broken layout. IMPORTANT PENALTIES / CAPS (Apply When Relevant) If major UI region is missing/swapped (e.g., header absent, bottom nav missing), cap the score at 5.9. If the primary CTA or the main title is missing/incorrect, subtract 1.03.0 depending on severity. If the overall hierarchy (top/middle/bottom) is incorrect, cap the score at 4.9. OUTPUT FORMAT (Strict) Return JSON ONLY (no extra text). The score must be float in [0.0, 10.0] with 1 decimal place. Return JSON in the following schema: { } \"score\": <float, 0.0 - 10.0, 1 decimal place>, \"reasoning\": \"Briefly justify: layout precision, component sizing/spacing, text/style consistency, and how placeholders were treated.\" User Prompt You will be shown two images. Image 1: Ground Truth (Real Next Screen) Image 2: Generated (HTML Rendered Next Screen) Evaluate the Visual Similarity and output the JSON strictly following the required schema. 20 Code2World: GUI World Model via Renderable Code Generation D.3.2. ACTION CONSISTENCY System Prompt You are an expert UI Action-Consistency Judge. Your task is to evaluate the logical correctness of GUI world models prediction by checking whether the predicted next UI state is consistent with the given action. You will be given the Current UI State (Image 1), users Action (intent + action data), and the Predicted Next State (Image 2). IMAGE DEFINITIONS Image 1: Real screenshot before the action. Image 2: Predicted screenshot after the action (rendered from HTML). Note (Placeholder Equivalence): Image 2 may use gray placeholders (e.g., [IMG: icon]) instead of real photos/icons. Do not penalize missing real imagery if the placeholder matches the GT region in position/size and its label is semantically plausible. EVALUATION CRITERIA (Action Consistency) Judge the transition from Image 1 Image 2 based on two aspects: 1) Action Adherence (Primary) Did the intended effect occur? The UI change must be the direct consequence of the action. Click/Tap: The correct navigation, popup, selection, toggle, or state change occurs on the intended target. Text Input: The exact text (including casing and punctuation) appears in the correct field, and the UI reacts appropriately (cursor, validation, suggestions, etc., if applicable). Scroll/Swipe: Content moves in the correct direction with plausible distance; items entering/leaving the viewport are consistent. Back/Home/Close: The UI returns/closes in plausible way (dismiss dialog, go back one page, exit overlay). 2) Context Preservation (Secondary) Non-target regions should remain stable unless the action logically affects them (e.g., status bar, bottom navigation, persistent header). Preserve app identity and screen continuity: avoid sudden unrelated page changes, missing core layout regions, or app/theme shifts. CONTINUOUS SCORING RUBRIC (Float 0.0 10.0, 1 decimal place) Output single score that reflects overall action consistency. 10.0 (Perfect): The action effect is exactly correct and unambiguous; UI changes are fully plausible; context is preserved. 9.09.9 (High): Action is clearly correct; only negligible visual imperfections (minor spacing/font/style) that do not affect the perceived effect. 6.08.9 (Acceptable): The action effect is mostly correct, but with noticeable issues (slightly wrong target, partial state update, imperfect scroll distance, mild content inconsistency). 3.05.9 (Ambiguous): Something changes, but it is unclear whether it is the right consequence (wrong page/overlay, inconsistent state change, unstable context). 0.02.9 (Failed/Broken): The action clearly fails, produces an implausible transition, or yields hallucinated/blank/unrelated interface. IMPORTANT CAPS / PENALTIES (Apply When Relevant) 21 Code2World: GUI World Model via Renderable Code Generation If the next screen is wrong page (unrelated destination), cap at 5.9. If the UI becomes blank/white/noise or clearly hallucinated, cap at 2.9. If the action is text input but the text is not exact, cap at 5.9 (or lower if also wrong field). If the action target is wrong (clicked the wrong button), cap at 5.9. OUTPUT FORMAT (Strict) Return JSON ONLY: { } \"score\": <float, 0.0 to 10.0, 1 decimal place>, \"reasoning\": \"Concise justification focusing on whether the action took effect and whether context was preserved.\" Do not output any additional text. User Prompt INTERACTION DATA User Intent: {instruction} Action Description: {semantic_description} Action Data: {action_json} VISUAL INPUTS Image 1: Current State (Before) Image 2: Predicted Next State (After) Evaluate whether Image 2 is the correct consequence of the action applied to Image 1, and output the JSON score in [0.0, 10.0]. D.4. Evaluation Metrics D.4.1. ACTION ADHERENCE METRICS System Prompt You are an expert UI Dynamics Judge. Your task is to evaluate the logical correctness of World Models prediction. You will be given the Current UI State (Image 1), users Action, and the Predicted Next State (Image 2). IMAGE DEFINITIONS Image 1: Real screenshot BEFORE the action. Image 2: Predicted screenshot generated by the model (Rendered from HTML). Note: Image 2 uses Gray Placeholders (e.g., [IMG: icon]) instead of real images. Treat these as valid visual elements if their text description matches the context. EVALUATION CRITERIA Evaluate the transition based on Action Adherence and Context Preservation. 1. Did the Action Take Effect? If Click, did the button trigger the correct navigation/popup? If Input Text, does the EXACT text appear? 22 Code2World: GUI World Model via Renderable Code Generation If Scroll, did the content shift correctly? 2. Is the Context Preserved? Non-active elements (status bar, bottom nav) should remain stable. SCORING RUBRIC (0.0-10.0) 9.5-10.0 (Perfect): The transition is flawless. Text is exact, layout is perfect, logic is undeniable. 8.0-9.4 (Good): Action executed correctly. Minor visual glitches (e.g., slight misalignment, small font diff), but the user intent is clearly fulfilled. 6.0-7.9 (Acceptable): The state changed logically, but there are noticeable issues (e.g., wrong icon style, text has typos, or layout is messy). 3.0-5.9 (Ambiguous): Something changed, but its unclear if it was the right change. (e.g., opened the wrong page, or screen turned white but kept headers). 1.0-2.9 (Failed): The action clearly failed (e.g., clicked button but screen didnt move). 0.0-0.9 (Broken/Hallucination): The model generated blank screen, noise, or completely hallucinated interface unrelated to the app. OUTPUT FORMAT Provide Single JSON Object: { \"score\": <float 0.0-10.0>, \"reasoning\": \"A concise summary of why this score was given...\" } User Prompt INTERACTION DATA User Intent: {instruction} Action Description: {semantic description} Action Data: {action json} VISUAL INPUTS Image 1: Current State (Before) Image 2: Predicted Next State (After) Please evaluate the transition quality on scale of 0.0 to 10.0. D.4.2. ACTION IDENTIFIABILITY METRICS System Prompt You are an expert Inverse Dynamics Judge for UI interactions. Your task is to infer the users action by analyzing the visual transition between the Current State (Image 1) and the Predicted Next State (Image 2). ACTION CATEGORIES Choose EXACTLY ONE from the following list that best explains the change: 1. click: tap on button, icon, or link. Result: Page navigation, popup opens, toggle switches, or focus change. 23 Code2World: GUI World Model via Renderable Code Generation 2. long press: sustained touch. Result: Context menu appears or item selection mode triggers. 3. scroll: The content shifts vertically or horizontally. (New content appears, old content moves off-screen). 4. input text: Text appears in an input field (without an explicit enter press). 5. open app: The screen transitions from launcher/home screen to specific app interface. 6. navigate home: Returns to the device home screen/launcher. 7. navigate back: Returns to the previous screen (reverse navigation). 8. wait: No significant visual change, or loading spinner continues spinning. 9. none: The transition is hallucinated, broken, illogical, or the image is blank. INFERENCE RULES If Image 2 shows keyboard appearing and text in box input text. If Image 2 is completely different layout (app switch) open app or navigate home. If Image 2 is just the same list but shifted scroll. If Image 2 has visual glitch that makes no sense none. OUTPUT FORMAT Provide Single JSON Object: { \"inferred_action\": \"string\", // Must be one of: click, long_press, scroll, input_text, // open_app, navigate_home, navigate_back, wait, none \"reasoning\": \"Brief explanation of visual evidence.\" } User Prompt VISUAL INPUTS Image 1: Current State (Before) Image 2: Predicted Next State (After) Based on the visual difference, what action did the user perform? D.4.3. ELEMENT ALIGNMENT AND LAYOUT FIDELITY METRICS System Prompt You are an expert GUI Design Evaluation AI. Your task is to compare Generated UI Prediction (Image 2) against Ground Truth UI Screenshot (Image 1) and assess similarity. You must act as strict judge, penalizing deviations in element position, content, and structure. If the prediction uses gray image placeholders (e.g., [IMG: avatar]), apply placeholder equivalence: do not penalize missing real photos, but judge whether the placeholder matches the GT image region in position, size, and semantic tag. OUTPUT REQUIREMENT (Strict): Return JSON only and follow the exact schema required by the user prompt. Do not output any extra text. 24 Code2World: GUI World Model via Renderable Code Generation User Prompt Task Definition. You are provided with two images: 1. Reference Image (Ground Truth): the expected correct UI (Image 1). 2. Candidate Image (Prediction): the UI generated by model (Image 2). Evaluate the Candidate Image based on the following two metrics and output the scores strictly. Metric 1: Element Alignment (Score 1.010.0) Definition: Measures whether core UI elements are present and aligned with the GT. What to check (strict): Presence of major elements (top bar, title, key text blocks, buttons/CTAs, list/cards, navigation). Alignment: relative positions, anchors, and spacing (padding/margins) compared to GT. Element sizing/proportions (width/height), including component boundaries. Metric 2: Layout Integrity (Score 1.010.0) Definition: Measures whether the overall layout framework and visual hierarchy match the GT. What to check (strict): Global layout hierarchy (top/middle/bottom regions; grouping; column vs row structure). Visual hierarchy and emphasis (primary vs secondary text; CTA prominence; highlighted chips/tabs). Consistency of repeated patterns (row height, card style, divider usage, spacing rhythm). Scoring Guidance (Both Metrics) 10.0: Near-perfect match with only negligible differences. 8.09.9: Strong match; minor spacing/typography/style deviations. 6.07.9: Mostly correct structure; noticeable alignment/sizing errors or missing minor elements. 3.05.9: Partial match; multiple misalignments, missing components, or incorrect grouping. 1.02.9: Major mismatch; wrong page layout or missing most key regions. Output Format (Strict) You must respond with valid JSON object: { } \"reasoning\": \"Brief analysis of the differences.\", \"element_alignment_score\": <float, 1.0 to 10.0>, \"structural_fidelity_score\": <float, 1.0 to 10.0> Output JSON only. Do not include any additional text. 25 Code2World: GUI World Model via Renderable Code Generation E. More Visualizations E.1. Code2World GUI World Modeling E.1.1. CODE2WORLD (a) Click on suggested search result with an inputted search query. (b) Enter an email recipient to trigger autocomplete suggestions. (c) Click Confirm to delete contact from the confirmation dialog. (d) Adjust the number of children travelers in the selection menu. (e) Click on Menu button at the bottom to open the sidebar menu. (f) Swipe up to view more filter options. 26 Code2World: GUI World Model via Renderable Code Generation (g) Tap the cancel button to close the Map type setting page. (h) Swipe up the biographical information page to view detailed profile. E.1.2. CODE2WORLD VS. OPEN-SOURCE BASELINES Launch the email application from the home screen to access the inbox. Click on All News button in the Cerebra Research application to view news content. 27 Code2World: GUI World Model via Renderable Code Generation Mark reminder task as completed by tapping the Complete button in the Reminder app. Apply product filters by tapping the Apply Filter button in the e-commerce app to refresh the item list. E.2. Code2World Enhancing GUI Agent We show more examples of Code2World enhancing GUI agents in the figure 7-10. As illustrated in Figure 7, the agent had already saved in the last step via the action click 4, but due to limited visual perception, it failed to detect the change in the save icon, thus intending to click the Save button again. Through proactive preview, Code2World enables the model to realize that clicking again will not cause any change, leading it to select different action, navigate back, thereby avoiding an unnecessary loop. In Figure 8, on the current application page, to turn off Wi-Fi, the agent naturally chooses to scroll down to locate the settings app, correct but inefficient strategy. Under Code2Worlds pipeline, the agent is prompted to explore different possible actions, thereby discovering open app, more efficient and direct action. Subsequently, Code2World correctly predicts the interface after open app launches Settings, allowing the agent to more intuitively understand that open app can reach the Settings page faster, thus completing the task in fewer steps. Similarly, as shown in Figure 9, to locate the TripIt app, the agent explores three different actions. Although all three contribute to task progress, Code2Worlds predictions indicate that only open app directly opens the TripIt application interface. As result, the agent is able to select the most efficient action open app. In Figure 10, on the detail page, the agent has the instinctive impulse to scroll for more information, but Code2World demonstrates that Action 2 adjusts the price to 8 crore, result that clearly advances the users task and thus prevents pointless scroll. Moreover, Code2World successfully predicts the future GUIs resulting from Actions 1 and 3, which would scroll to the bottom and click inactive elements without producing any change. 28 Code2World: GUI World Model via Renderable Code Generation Figure 7. Agent action-selection performance w/ and w/o Code2World on the MarkorCreateNoteFromClipboard task in AndroidWorld. Red indicates the action ultimately selected by the Code2World pipeline. Figure 8. Agent action-selection performance w/ and w/o Code2World on the SystemWifiTurnOffVerify task in AndroidWorld. Red indicates the action ultimately selected by the Code2World pipeline. 29 Code2World: GUI World Model via Renderable Code Generation Figure 9. Agent action-selection performance w/ and w/o Code2World at step 0 of Episode 14178 in AndroidControl-High. Red indicates the action ultimately selected by the Code2World pipeline. Figure 10. Agent action-selection performance w/ and w/o Code2World at step 7 of Episode 2673 in AndroidControl-High. Red indicates the action ultimately selected by the Code2World pipeline."
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "Sun Yat-sen University",
        "University of Oxford",
        "University of Science and Technology of China"
    ]
}