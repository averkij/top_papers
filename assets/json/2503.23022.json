{
    "paper_title": "MeshCraft: Exploring Efficient and Controllable Mesh Generation with Flow-based DiTs",
    "authors": [
        "Xianglong He",
        "Junyi Chen",
        "Di Huang",
        "Zexiang Liu",
        "Xiaoshui Huang",
        "Wanli Ouyang",
        "Chun Yuan",
        "Yangguang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the domain of 3D content creation, achieving optimal mesh topology through AI models has long been a pursuit for 3D artists. Previous methods, such as MeshGPT, have explored the generation of ready-to-use 3D objects via mesh auto-regressive techniques. While these methods produce visually impressive results, their reliance on token-by-token predictions in the auto-regressive process leads to several significant limitations. These include extremely slow generation speeds and an uncontrollable number of mesh faces. In this paper, we introduce MeshCraft, a novel framework for efficient and controllable mesh generation, which leverages continuous spatial diffusion to generate discrete triangle faces. Specifically, MeshCraft consists of two core components: 1) a transformer-based VAE that encodes raw meshes into continuous face-level tokens and decodes them back to the original meshes, and 2) a flow-based diffusion transformer conditioned on the number of faces, enabling the generation of high-quality 3D meshes with a predefined number of faces. By utilizing the diffusion model for the simultaneous generation of the entire mesh topology, MeshCraft achieves high-fidelity mesh generation at significantly faster speeds compared to auto-regressive methods. Specifically, MeshCraft can generate an 800-face mesh in just 3.2 seconds (35$\\times$ faster than existing baselines). Extensive experiments demonstrate that MeshCraft outperforms state-of-the-art techniques in both qualitative and quantitative evaluations on ShapeNet dataset and demonstrates superior performance on Objaverse dataset. Moreover, it integrates seamlessly with existing conditional guidance strategies, showcasing its potential to relieve artists from the time-consuming manual work involved in mesh creation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 2 2 0 3 2 . 3 0 5 2 : r MeshCraft: Exploring Efficient and Controllable Mesh Generation with Flow-based DiTs Xianglong He1 Junyi Chen2,3 Wanli Ouyang6 Di Huang4 Chun Yuan1 Zexiang Liu5 Xiaoshui Huang Yangguang Li5 1Tsinghua University 2Shanghai Jiaotong University 3Shanghai AI Laboratory 4The University of Sydney 5VAST 6The Chinese University of Hong Kong Figure 1. Overview of generated meshes, speed and token numbers of MeshCraft."
        },
        {
            "title": "Abstract",
            "content": "In the domain of 3D content creation, achieving optimal mesh topology through AI models has long been pursuit for 3D artists. Previous methods, such as MeshGPT, have explored the generation of ready-to-use 3D objects via mesh auto-regressive techniques. While these methods produce visually impressive results, their reliance on token-by-token predictions in the auto-regressive process leads to several significant limitations. These include extremely slow generCorresponding authors. ation speeds and an uncontrollable number of mesh faces. In this paper, we introduce MeshCraft, novel framework for efficient and controllable mesh generation, which leverages continuous spatial diffusion to generate discrete triangle faces. Specifically, MeshCraft consists of two core components: 1) transformer-based VAE that encodes raw meshes into continuous face-level tokens and decodes them back to the original meshes, and 2) flow-based diffusion transformer conditioned on the number of faces, enabling the generation of high-quality 3D meshes with predefined number of faces. By utilizing the diffusion model for 1 the simultaneous generation of the entire mesh topology, MeshCraft achieves high-fidelity mesh generation at significantly faster speeds compared to auto-regressive methods. Specifically, MeshCraft can generate an 800-face mesh in just 3.2 seconds35 faster than existing baselines. Extensive experiments demonstrate that MeshCraft outperforms state-of-the-art techniques in both qualitative and quantitative evaluations on ShapeNet dataset and demonstrates superior performance on Objaverse dataset. Moreover, it integrates seamlessly with existing conditional guidance strategies, showcasing its potential to relieve artists from the time-consuming manual work involved in mesh creation. 1. Introduction With advances in fields such as gaming and 3D printing, the creation of high-quality, topologically sound 3D meshes has become increasingly important. However, generating well-structured 3D meshes from simple inputssuch as textual descriptions or single 2D imagerequires significant artistic expertise and often entails labor-intensive manual processes. Therefore, the ability to rapidly generate 3D meshes with high-quality topology that meet the needs of artists and professionals using AI models is key goal. Significant efforts have been made to automate the generation of 3D meshes. Many works [30, 39] are modeling with dense triangle meshes, which are extracted from neural fields using iso-surfacing methods [16, 25, 28]. Specifically, these methods first generate intermediate 3D representations and then post-process them into final meshes. Although these approaches yield impressive visualizations in producing neural representations, the resulting meshes often suffer from excessive face counts and artifacts due to misalignment that occur during the conversion of intermediate representations into final meshes via re-meshing techniques [21]. The efficiency of these processes is also limited by the post-processing step. To address these issues, another group of works [1, 11, 26, 32], modeling with triangle meshes that accurately reflect the compactness of artists design, offers great flexibility for manipulation and efficiency for storage. This approach, referred to as native mesh generation, focuses on explicitly modeling mesh distributions. It first transforms the mesh into latent sequences by leveraging the spatial relationships among faces, vertices, and coordinates. These methods then directly generate the sequences, and even shows superior long sequence scalability. However, these approaches are often limited by generation effectiveness or are validated only on small datasets, such as ShapeNet [2]. Furthermore, most of the methods lack user controllability, which is essential for the 3D creation process. To address these challenges, we introduce MeshCraft, an efficient and controllable approach for high-fidelity mesh generation method that leverages the strengths of continuous diffusion transformers with rectified flow (see Fig. 1(b) and (c) for comparisons with prior works). Unlike autoregressive methods such as Meshtron [11], which prioritize scalability, we explore the possibilities of different routes starting from the underlying principles of implementing native mesh generation, with the goal of demonstrating the potential of diffusion models for the practical usage of controllable and efficient mesh generation, rather than immediate scalability. MeshCraft saves token numbers for at most 9 times, and speed up for 35 times. It is designed as two-stage pipeline that consists of directly modeling the mesh distribution in latent space via Variational Auto-Encoder(VAE) [17] and then generating meshes with diffusion transformer. Unlike previous works that compress meshes into discrete tokens, we explore the modeling of meshes in low-dimensional and continuous latent space. Face features of the mesh are sent into the VAE encoder, regularized with KL divergence, and are subsequently decoded into vertex coordinates of each face. For the generative model, the diffusion transformer is modified to generate the continuous tokens with varying lengths with inspirations from recent advances in image generation [9, 22]. This approach enables MeshCraft to generate meshes at high speed. The guidance and masking strategy also allow users to control the number of faces of generated meshes in user-friendly manner. We validate the effectiveness of MeshCraft on the ShapeNet dataset [2] and demonstrate its great potential as generic mesh generator on Objaverse [7], which includes diverse array of 3D objects across various categories. Extensive experiments illustrate the effectiveness, and controllability of MeshCraft from both qualitative and quantitative aspects. In summary, our contributions are as follows: We propose transformer-based VAE that encodes discrete triangle meshes into continuous latent space and decodes them back to the original mesh, achieving competitive reconstruction performance compared to state-ofthe-art mesh auto-encoders based on vector quantization. We introduce flow-based transformer diffusion model conditioned on the number of faces, integrating classifierfree guidance for both image inputs and face number to enable effective control over the mesh generation process. MeshCraft significantly outperforms existing methods, achieving new state-of-the-art results on the ShapeNet dataset while being 35 faster than MeshGPT, and demonstrating the effectiveness of face number control on the large-scale Objaverse dataset. 2. Related works Recent advances [1, 4, 11, 26, 32, 38] have seen pioneering efforts to generate meshes directly, with many em2 ploying auto-regressive models for this task. For instance, PolyGen [26] utilizes two transformers to separately learn vertex and face distributions. MeshGPT [32] first encodes meshes into face-level quantized tokens via GNNbased Vector Quantization Variational Auto-Encoder(VQVAE) [37], followed by the application of GPT-style transformers for auto-regressive generation. MeshXL [4] introduces another sequential mesh representation for one-stage auto-regressive generation. Even though Meshtron [11] shows the scalability of auto-regressive generations, however, the generative capabilities of these models are restricted by the rapidly increasing number of tokens, leading to slow inference speeds. Additionally, users cannot precisely assign an exact number of object faces, which limits their practicality in the existing workflow for 3D creations. Among these works, Polydiff [1] is the most relevant to our approach. It trains class-conditioned discrete diffusion model using discrete state transition matrices and crossentropy loss, which is challenging to optimize and hard to benefit from existing conditional guidance method. In contrast, we decouple modeling and generation: first compressing meshes into semantically rich latent space with KLregularized VAE, then applying flow-based DiT to model this space for mesh generation. MeshCraft is the first to introduce fine-grained controllable mesh generator based on diffusion transformers. 3. Preliminary 3.1. Ordered mesh representation In this work, we adopt the ordering rule used in previous native mesh generation studies [26, 32, 38] and consider meshes as ordered sequences consisting of three progressively defined components: face level, vertex level, and coordinate level. Let be mesh that includes faces {fi}i=1,2, ,n, where each face fi comprises vertices, repi , , vk resented as fi = {v1 }. Each vertex is defined , zj = (xj in the coordinate system as vj ). Consequently, the mesh can be represented in the following manner (taking = 3 as an example): , v2 , yj = {f1, f2, , fn} 1, v1 1, v2 1, x2 1, y1 = {v1 = {x 1, v3 1, z1 2, v2 1, y2 2, v3 1, z2 2, , v1 1, , x3 n, v2 n, y3 n, v3 n} n, z3 n} (1) For sequence ordering, faces are sorted by vertex indices from lowest to highest, while vertices are sorted by their z-y-x coordinates in the same manner to ensure the uniqueness of the mesh. Unlike prior works using representations at the vertex or coordinate levels to generate meshes, we generate face-level tokens using flow-based diffusion transformers. This approach significantly reduces the number of tokens and enhances the efficiency of mesh generation. 2.1. Image-to-3D Generation 3.2. Rectified flow With the effective exploration of 3D representations [16, 25, 28], large-scale datasets [7], and popular 2D generative models [31], conditional generation of diverse and high-fidelity 3D assets has emerged as promising area of research. Most studies[12, 20, 30, 35] first learn neural 3D representations and then post-process [5, 21] them into meshes, which can lead to overly dense results. Some approaches [18, 30, 39] utilize pre-trained text-to-image models to optimize targeted meshes based on given conditions, resulting in significant time costs. In this paper, we focus on directly generating high-fidelity meshes in face-level representations, effectively balancing quality and efficiency. 2.2. Flexible Diffusion Models The rise of text-to-image diffusion models has prompted the consideration of more customized demands, particularly in generating images with unrestricted resolutions. Recent works [3, 22, 42] have explored this area with flexible training, inference strategies, and model designs. Inspired by these advancements, we propose MeshCraft, controllable mesh generation pipeline based on flow-based diffusion transformers. Our user-friendly method enables the rapid generation of compact meshes while allowing for extensive manipulation of the results, thereby facilitating practical applications of AI-generated 3D assets in the industry. With the growing popularity of generative models in recent years, diffusion models [14, 24, 29, 31, 33] have been widely recognized for their powerful modeling capabilities. Score-based models, such as [33] and DDPM [14], are commonly employed, formulating the diffusion process through stochastic differential equations (SDEs). However, these methods often suffer from slow iterative de-noising process, resulting in inefficient inference time. In contrast, rectified flow [19] is an implicit probabilistic model designed for fast generation, based on ordinary differential equations (ODE). It aims to transport the distribution π0 to π1 by following straight-line paths as much as possible. This preference is both theoretically and computationally advantageous, allowing for few-step or even one-step sampling. Given two distributions π0 and π1, the rectified flow induced from (X0, X1), where X0 π0 and X1 π1, is modeled as an ODE over time [0, 1], dZt = v(Zt, t)dt (2) which converts Z0 from π0 to Z1 following π1. The drift force : Rd Rd is to drive the flow to follow the linear direction X1 X0 as much as possible by solving simple least squares regression problem: (cid:90) 1 0 min (cid:2)(X1 X0) v(Xt, t)2(cid:3) dt (3) Figure 2. Pipeline of MeshCraft. Our framework comprises two stages. We firstly compress meshes into face-level tokens (Sec. 4.1). Then the tokens are used for training the flow-based DiT, which is guided by the input face number and the image conditions (Sec. 4.2). transformer-based auto-encoder, and we model the distribution using flow-based DiTs. As Fig. 3 shows, the continuous tokenizer achieves higher reconstruction accuracy than the discrete one, justifying our choice of continuous diffusion over discrete AR, which is also foundation motivation for the diffusion-based exploration. Moreover, this approach facilitates fast and controllable mesh generation. Furthermore, it allows our method to integrate with popular guidance strategies [13], enabling control over the diffusion process with different conditions and enhancing generation quality. An overview of MeshCraft is illustrated in Fig. 2. 4.1. Encoding meshes into face-level continuous tokens As described in Sec. 3.1, we formulate meshes as ordered sequences according to Eq. (1). To effectively learn their distribution, the sequences are fed into encoder along with associated geometric information (normals, angles, areas, and adjacency among faces), which is aggregated using single Graph Convolutional Network (GCN) layer, preserving geometric information and enhancing the representation robustness. Subsequently, NE transformer-style blocks are employed to extract face-level features Fi. Unlike previous works [32, 38] that utilize residual vector quantization [40] to obtain discrete tokens, we linearly project the features into continuous space: FCµ(Fi) = (µi,j)j[1,2, ,CKL] FCσ(Fi) = (cid:0)log σ2 i,j (cid:1) j[1,2, ,CKL] (4) Figure 3. Reconstruction quality using different tokenizers. Continuous means using KL-divergence loss to regularize continuous-space tokens, while Discrete stands for using RVQ [40] to quantize discrete tokens for reconstruction. Refer to Tab. 3 for quantitative results. where Xt = tX1 + (1 t)X0 is the linear interpolation of X0 and X1. is parameterized by the models in practice. Rectified flow not only avoids crossing paths when finding the solution but also reduces errors arising from discrete-time scheduling and transport costs. To leverage rectified flow and advanced model architectures, we follow the implementation in SiT [24] and introduce several techniques to adapt it to the task of mesh generation. 4. Methodology that generate artistExisting works [4, 26, 32, 38] like meshes predominantly utilize auto-regressive models. However, these approaches require discretizing coordinates into limited vocabulary, resulting in long token sequences for each mesh. This can lead to information loss and inefficient inference speeds. In contrast, we propose generating latent tokens of faces in the continuous space through Figure 4. Qualitative comparisons on ShapeNet. MeshCraft produces high-quality meshes with sharp edges and smooth faces. Method MeshGPT [32] PivotMesh [38] Ours Tri. Accu.(%) L2 Dist.(102) 99.99 98.88 99.42 0.00 0.86 0.06 Table 1. Reconstruction performance on ShapeNet dataset. Our continuous auto-encoder behaves competitively with prior works using the vector quantization. where FCµ() and FCσ() are linear projection layers. The continuous tokens can then be sampled from (µi, σi). To better adapt them for training the diffusion model, we use KL-divergence to regularize them: LKL ({Fi}n i=1) = 1 CKL (cid:88) CKL(cid:88) i= j=1 1 2 (cid:0)µ2 i,j + σ2 i,j log σ2 i,j (5) As the findings of Fluid [10] claim, this transformation eliminates the need for the codebook, and also results in significantly shorter token lengths, leading to accurate reconstructions (also discussed in Sec. 5.3.1). The sampled tokens are then passed to the decoder D, which consists of ND transformer-style blocks and concludes with an MLP layer. The outputs are reshaped into coordinates to compute the loss LAE against the input sequences. Following [32], we use cross-entropy loss LAE to guide the training process. 4.2. Mesh generation with the flow-based DiT As illustrated in the right part of Fig. 2, the face-level tokens are fed into the flexible diffusion transformer for training. However, standard SiT [24] does not support direct training with token sequences of variable lengths. To address this, we introduce several techniques to adapt the architecture for mesh generation. First, it is essential that the input tokens are of the same length. By padding the token sequences to match the length of the longest mesh sequence in the batch, the model can be trained using mesh sequences of varying lengths. The corresponding masks for these samples are also provided to the model, functioning as attention masks within the SiT blocks and guiding the unpadding process. RoPE [34] is applied to keys/values in attention layer. And the attention scores are computed as follows: Softmax (cid:18) QKT (cid:19) + (6) (cid:1) where and represent the queries and keys, respectively, and is their dimension. The value of is set to be 0 for noised tokens and for padding tokens. Masks are also applied to exclude padding tokens before calculating losses, ensuring that generated tokens align with the input tokens. (refer to the supplementary for our detailed architectures) In addition to masking techniques, we explicitly use the number of faces cf as guidance. Following the class conditioning approach implemented with adaLN-Zero blocks in DiT [29], we embed the number of faces using an embedding layer and add it to the timestep embeddings. This modification provides additional information and models more accurate distribution of meshes according to the targeted number of faces, significantly enhancing generation quality when combined with classifier-free guidance (CFG) [13]: (cid:101)vt = vθ(zt, ) + (vθ(zt, cf ) vθ(zt, )) (7) where θ and zt denote the network parameters and noised tokens, respectively. (cid:101)vt represents the estimated velocity at each de-noising step. For conditioning the mesh geometry (e.g., from images or texts), we employ cross-attention modules to inject the corresponding features. Thus, for image-conditioned generation (similar to text conditions), we have two conditions: the input image ci and the assigned number of faces cf . We find that the generation process Class Method COV MMD 1-NNA JSD FID KID Class Method COV MMD 1-NNA JSD FID KID Chair Bench MeshGPT [32] PivotMesh [38] MeshXL* [4] Ours MeshGPT [32] PivotMesh [38] MeshXL* [4] Ours 45.98 47.99 49.43 51. 56.06 59.09 59.09 57.58 10.34 10.00 10.17 9.61 8.44 8.25 7.74 7.90 60.06 60.06 56.90 54.31 58.33 48.48 53.79 50.76 11.67 13.51 11.37 11. 28.34 25.76 26.37 27.17 25.43 34.40 20.09 20.40 66.30 64.48 19.30 59.83 4.10 10.33 1.70 1.76 9.45 5.17 3.44 1.53 Table Lamp MeshGPT [32] PivotMesh [38] MeshXL* [4] Ours MeshGPT [32] PivotMesh [38] MeshXL* [4] Ours 48.85 47.42 50.98 55.42 43.90 50.00 42.68 62.20 9.23 9.08 9.38 8. 20.82 19.17 21.64 18.69 57.82 58.35 57.82 54.26 60.37 56.71 63.41 48.17 8.50 10.42 9.07 8.73 36.21 39.75 35.96 37.33 21.98 24.97 22.08 16. 73.21 67.76 62.46 62.81 2.99 7.99 2.88 1.70 6.04 7.09 5.32 2.17 Table 2. Quantitative results on ShapeNet dataset. MeshCraft outperforms the baselines on shape quality, visual and compactness metrics. MMD values are multiplied by 103. COV and 1-NNA are scaled by 102. * stands for using the released pre-trained models. 5. Experiments We trained our model on two datasets: ShapeNet [2] and Objaverse [7]. The model on ShapeNet demonstrates the effectiveness of our method (see Fig. 4 and Tab. 2 for the main results), which achieves state-of-the-art performance compared with prior works. Furthermore, experiments  (Fig. 6)  conducted on Objaverse show that MeshCraft is able to generate diverse samples of different face numbers conditioned on the same single image, which shows its potential to be practical mesh generator in larger-scale scenarios. 5.1. Experiment Settings 5.1.1. Datasets Following the previous convention [4, 32, 38] for preprocessing, we apply planar decimation to meshes with more than 800 faces and use further filtering by comparing the Hausdorff distance [15] between decimated and the original meshes with pre-set threshold σhausdorf for the ShapeNet dataset. As for the Objaverse dataset, we select assets whose number of faces is in [1024, 1536] to train our image-conditioned model. The dataset size is about 10k and 65k, respectively. We split the training set and validation set in 10:1 and 100:1. The coordinate space resolution is set as 128 and 256 for ShapeNet and Objaverse dataset, respectively. We normalize the coordinate range into [1, 1] and augment the data using random scaling on each axis (from 0.95 to 1.05) and random rotations for the reconstruction stage to improve auto-encoders robustness. Across all experiments, we use triangular meshes (each face fi consists of 3 vertices) as training data for fair comparisons with baselines. 5.1.2. Evaluation We choose three recent baselines to compare the results on ShapeNet dataset, including MeshGPT [32], PivotMesh [38] and MeshXL [4]. For evaluating the reconstruction quality, we follow previous works [1, 32, 38] to use two metrics, triangle accuracy and l2 distance. We evaluate the methods following previous mesh generation works [38]. The metrics include Minimum Matching Distance (MMD), Coverage (COV), 1-Nearest-Neighbor Accuracy (1-NNA), Jensen-Shannon Divergence (JSD) for measuring 3D geometry, FID and KID for visual perceptions. Figure 5. Mesh completion results. Given some partial observation of mesh, MeshCraft can produce diverse completed results. benefits from multiple different CFG weights. Therefore, our modified estimation equation for the multiple conditions cf and ci can be expressed as: (cid:101)vt = vθ(zt, , ) = +w1 (vθ(zt, cf , ) vθ(zt, , )) = +w2 (vθ(zt, cf , ci) vθ(zt, cf , )) (8) In Sec. 5.3.2, we present the effects of CFG weights on the generated meshes. Additionally, to ensure more stable training, we introduce sandwich normalization [8], replace the MLP with SwiGLU, and employ QK-norm [6] techniques inspired by previous works [36, 42]. The QK-norm is crucial for stabilizing the training of transformer models, especially when the token length gets more flexible and longer. Benefiting from the elaborate architecture and excellent properties of rectified flows linear sampling, training becomes more efficient and stable. We also draw inspiration from observations during the noising process to prioritize intermediate steps of diffusion, as the middle and final stages are more challenging during training. To alleviate this, we adopt the logit-normal sampling strategy from SD3 [9] to adjust the sampling weights: πln(t; m, s) = 1 2πt(1 t) (cid:18) exp (log(t/(1 t)) m)2 2s2 (cid:19) (9) where and represent the location and scale parameters, respectively. Finally, we utilize an MSE loss LDif to predict the velocity using our model. 6 Figure 6. Generation diversity on Objaverse dataset. The number below each asset represents for the face number of it. MeshCraft is about to produce diverse samples with different seeds and face numbers. For COV, higher is better; for 1-NNA, 50% is the optimal; for the rest of metrics, lower is better. Following PivotMesh [38], we use Chamfer Distance measure for computing these metrics on 1024-dim point clouds uniformly sampled from meshes. For each method, we generate 1000 samples to calculate the quantitative results. 5.1.3. Implementation Details For transformer-styled blocks in the auto-encoder, we set the encoder part as 12 layers with hidden size of 768, and 18 layers with hidden size of 384 for the decoder part. The channel dimension for face-level tokens is set to 8 for the balance of reconstruction quality and compression capability. For the diffusion model, we adopt 24-layer transformer with hidden size of 864, which has similar number of parameters compared with baselines. For the implementation of baselines, we adopt the public code to reimplement MeshGPT [32], and official training code of PivotMesh [38]. For MeshXL [4], we use their released 350M pre-trained models for evaluation. All models are trained following the settings claimed in the original paper. Without further specification, we generate meshes in the distribution of datas face numbers for MeshCraft and follow the default settings of baselines. For auto-encoders, we trained about 2 days on an 8A100 80GB machine with batch size of 8. For diffusion transformers, we train for 3 days on the ShapeNet dataset and for around 3 weeks on the Objaverse dataset. During the training of diffusion transformers, we use bf16 mixed precision to accelerate the training process. All the generated results are sampled with the 50-step Euler method. https://github.com/lucidrains/meshgpt-pytorch 5.2. Experiment Results 5.2.1. Results on ShapeNet dataset Firstly, we evaluate our method on the most commonly used benchmark, ShapeNet, focusing on four different categories: chair, table, bench, and lamp. Tab. 1 shows that our continuous latent space VAE achieves competitive reconstruction performance compared with baselines that adopt an advanced vector quantization technique, RVQ [40], to discretize vertex coordinates into indices of codebook. Furthermore, the number of tokens in face sequences decreases ninefold compared with prior works, which not only reduces memory requirements but also speeds up the subsequent generation process. For the comparisons in the generation part, we follow the previous setting [1, 4, 32, 38], first pretraining our model on mixed dataset composed of the four categories and subsequently fine-tuning each of them separately to perform comparisons from both qualitative and quantitative aspects. For the CFG weight of the face number condition, we set = 8.0 for better generation quality. As shown in Fig. 4, our method can produce diverse and high-quality 3D meshes. As Tab. 2 demonstrates, our method significantly outperforms the baselines, performing better on all three metrics. In Fig. 5, we also demonstrate the completion ability of our model by adapting the operation from an image inpainting work [23]. Compared with autoregressive methods, our diffusion-based method achieves state-of-the-art generation results while maintaining superior generation speed. Specifically, as shown in Fig. 1(b) and (c), our method decreases the token number by up to 9 times and speeds up generation by 35 times, demonstrating the advantages of diffusion-based models. 7 Method KL (4-dim) KL (8-dim) RVQ [40] Tri. Accu.(%) L2 Dist.(102) 90.41 99.66 65.12 0.63 0.12 8.63 Table 3. Reconstruction performance study on Objverse. 5.2.2. Results on Objaverse dataset An image-conditioned model is also trained to demonstrate the effectiveness of face number control on largescale dataset Objaverse. To enhance the capability of capturing details in the image condition, we use DINOv2 ViTL/14 [27] as the image feature extractor and fine-tuned with 3D assets whose front view occupies more than 20% of the frame. For the CFG weights w1, w2, which respectively controls the face number and the input image condition, we set them as w1 = 1.0, w2 = 5.0. Fig. 6 displays that our model has the potential for the capability of diverse generation on the large-scale dataset, which can be also combined well with existing mature conditional techniques [13]. Notably, increasing the face number primarily enhances fine local details rather than uniformly refining the entire shape. For example, in the third row, the tigers facial features become more detailed, whereas broader regions such as the torso show only slight improvements. 5.3. Ablation Studies 5.3.1. Comparisons of auto-encoder selections At the reconstruction stage, we compare auto-encoders operating in different continuous and discrete latent spaces. As shown in Tab. 3, KL regularization with 8-dimensional token channels yields better performance, striking good balance between reconstruction quality and compression capability compared to using 4-dimensional channels. We believe that using an excessively high compression ratio leads to severe information loss. Additionally, we compare the continuous tokenizer with the discrete one by replacing the compression component using KL regularization with residual vector quantization (RVQ) [40]. It is worth mentioning that discrete tokenizers, limited by their codebook size, can cause significant information loss when compressing variable data. In contrast, models with continuous tokens produce meshes of higher quality. 5.3.2. Effects of CFG weights We also investigate the effects of CFG weights on and w1, w2, respectively. For the weight controlling the face number, Tab. 4 shows that as increases, the quantitative metrics improve, reaching peak when is around 8.0. For multiple conditions (single images ci and the number of faces cf ), Fig. 7 provides visualization of the effects corresponding to different scales of w1, w2. The best result appears when w1 = 1.0, w2 = 5.0. These two sets of experiments show that low weights can result in weak conCFG COV MMD 12.61 30.70 0.0 11.34 38.20 1.0 10.72 44.80 2.0 10.33 48.60 3.0 9.89 50.80 4.0 10.05 51.70 5.0 9.97 51.00 6.0 9.65 51.70 7.0 53.30 9.90 8.0 9.85 52.50 9.0 9.86 52.30 10.0 1-NNA 84.75 75.00 67.65 61.05 58.40 57.55 56.55 56.70 56.85 56.75 57.90 Table 4. Effects of CFG weights over the face number condition to mesh generation results on ShapeNet dataset. Figure 7. CFG weights over face number and image conditions. w1 controls on the condition of the face number cf , while w2 yields weights further over the single-image condition ci. ditional control, while excessive weights are detrimental to the results. 5.4. Limitations Though MeshCraft shows promising results, there are still some limitations: (1) The extrapolation capability of our diffusion model is limited due to the face number embedder we use is learnable, and objects with unseen face numbers cannot be produced directly; (2) When the domain of images and assigned face numbers is far from the training domain, MeshCraft fails to generate completed meshes. We will further explore more generalizable models with improved training strategies and architectures. 6. Conclusion In this paper, we introduce novel method, namely MeshCraft, for generating ready-to-use 3D meshes with high efficiency and controllability. Regarding meshes as face-level sequences, we first compress them into continuous tokens and subsequently generate the tokens with flow-based diffusion transformer. Our method demonstrates superior speed (35 speed up) and shows competitive performance in both qualitative and quantitative experiments. MeshCraft shows the potential to alleviate artists from time-consuming manual work."
        },
        {
            "title": "References",
            "content": "[1] Antonio Alliegro, Yawar Siddiqui, Tatiana Tommasi, and Matthias Nießner. Polydiff: Generating 3d polygonal meshes with diffusion models. arXiv preprint arXiv:2312.11417, 2023. 2, 3, 6, 7 [2] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 2, 6 [3] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li. Pixart-{delta}: Fast and controllable image generation with latent consistency models. arXiv preprint arXiv:2401.05252, 2024. 3 [4] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Yanru Wang, Zhibin Wang, Chi Zhang, et al. Meshxl: Neural coordinate field for generative 3d foundation models. arXiv preprint arXiv:2405.20853, 2024. 2, 3, 4, 6, 7 [5] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. Meshanything: Artist-created mesh generation with autoregressive transformers. arXiv preprint arXiv:2406.10163, 2024. 3 [6] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 74807512. PMLR, 2023. 6, 2 [7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 2, 3, [8] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. 6 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 6, 1 [10] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. 5 [11] Zekun Hao, David Romero, Tsung-Yi Lin, and Ming-Yu Liu. Meshtron: High-fidelity, artist-like 3d mesh generation at scale. arXiv preprint arXiv:2412.09548, 2024. 2, 3 [12] Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, and Tong He. Gvgen: Text-to-3d generation with volumetric repIn European Conference on Computer Vision, resentation. pages 463479. Springer, 2025. 3 [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 4, 5, 8 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [15] Daniel Huttenlocher, Gregory A. Klanderman, and William Rucklidge. Comparing images using the hausdorff distance. IEEE Transactions on pattern analysis and machine intelligence, 15(9):850863, 1993. 6 [16] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3 [17] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [18] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 3 [19] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [20] Ying-Tian Liu, Yuan-Chen Guo, Guan Luo, Heyi Sun, Wei Yin, and Song-Hai Zhang. Pi3d: Efficient text-to-3d genIn Proceedings of eration with pseudo-image diffusion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1991519924, 2024. 3 [21] William Lorensen and Harvey Cline. Marching cubes: high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347353. 1998. 2, 3 [22] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, XiFit: Flexible arXiv preprint hui Liu, Wanli Ouyang, and Lei Bai. vision transformer for diffusion model. arXiv:2402.12376, 2024. 2, [23] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. 7 [24] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 3, 4, 5 [25] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2, 3 [26] Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive generative model of 9 zstein. Grm: Large gaussian reconstruction model for efarXiv preprint ficient 3d reconstruction and generation. arXiv:2403.14621, 2024. 2, [40] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An endto-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495507, 2021. 4, 7, 8 [41] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in Neural Information Processing Systems, 36, 2024. 2 [42] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024. 3, 6, 2 3d meshes. In International conference on machine learning, pages 72207229. PMLR, 2020. 2, 3, 4 [27] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 8 [28] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165174, 2019. 2, 3 [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 3, 5 [30] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 2, [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [32] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1961519625, 2024. 2, 3, 4, 5, 6, 7 [33] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [34] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [35] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2025. 3 [36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 6 [37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [38] Haohan Weng, Yikai Wang, Tong Zhang, CL Chen, and Jun Zhu. Pivotmesh: Generic 3d mesh generation via pivot vertices guidance. arXiv preprint arXiv:2405.16890, 2024. 2, 3, 4, 5, 6, 7 [39] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wet10 MeshCraft: Exploring Efficient and Controllable Mesh Generation with Flow-based DiTs"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 9. Process of adding noises. The complete mesh is gradually transformed into noises from standard normal distribution from = 1 to = 0. Figure 8. Architectures of the transformer-styled block and welldesigned blocks in the DiT. 7. Additional details of the model design 7.1. Detailed architectures of modules The proposed model architecture comprises intricate transformer-styled blocks with carefully designed components. As illustrated in Fig. 8(a), the transformer-styled block is composed of two attention layers, followed by an RMS normalization and feed-forward layer. Fig. 8(b) and (c) provide detailed visualization of our DiT-styled block and the final layer in the diffusion transformer, highlighting the nuanced design choices that contribute to the models performance. 7.2. Logit-normal sampling Drawing inspiration from the training methodology of SD3 [9], we inspect the diffusion process of noises to the latent tokens, visualizing in Fig. 9. The visualization in Fig. 9 reveals critical insights into the mesh generation process, demonstrating that intermediate and final diffusion Figure 10. Loss during training on Objaverse dataset. The QKnorm is of vital importance for stabilizing the training process. steps play pivotal role in generating complete meshes. Consequently, we implemented logit-normal sampling approach to emphasize these crucial stages of the generation process. By setting the distribution parameters to = 0.5 and = 1.0, informed by empirical results from SD3, we effectively prioritize sampling in the most informative regions of the diffusion process. 1 7.3. The importance of QK-norm Recent advancements in transformer research [6, 22, 42] have highlighted the inherent challenges of training largeparameter models with flexible data sequences. Our mesh generation experiments on the Objaverse dataset corroborated these observations, revealing significant training instabilities. To address this critical issue, we implemented the QK-norm technique, proven strategy for mitigating training volatility. Formally, we modified the attention scores as follows:"
        },
        {
            "title": "Softmax",
            "content": "(cid:18) LN(Q)LN(K)T (cid:19) + (10) By applying LayerNorm to the query and key matrices, we effectively stabilize the attention mechanism. Fig. 10 demonstrates that QK-norm helps to stabilize the process. 8. Additional quantitative results We present comprehensive visualization of generation results across multiple datasets, showcasing the versatility of our approach in Fig. 11, Fig. 12, and Fig. 13. To demonstrate the models adaptability, we also extended our method to point cloud conditioning on the ShapeNet bench dataset. Our implementation leverages pre-trained point cloud encoder inspired by the Michelangelo [41] architecture. We integrated the point cloud features into our model using cross-attention techniques, analogous to image feature injection. Specifically, we employed linear projections to seamlessly adapt and align the point cloud representations with our models internal feature space. Refer to Fig. 13 for the results. 2 Figure 11. Generation gallery on ShapeNet. Additional results on the subset of bench, lamp, chair and table. 3 Figure 12. Additional generation results on Objaverse. 4 Figure 13. Point cloud conditioned generation on the ShapeNet bench dataset."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Shanghai Jiaotong University",
        "The Chinese University of Hong Kong",
        "The University of Sydney",
        "Tsinghua University",
        "VAST"
    ]
}