{
    "paper_title": "NaTex: Seamless Texture Generation as Latent Color Diffusion",
    "authors": [
        "Zeqiang Lai",
        "Yunfei Zhao",
        "Zibo Zhao",
        "Xin Yang",
        "Xin Huang",
        "Jingwei Huang",
        "Xiangyu Yue",
        "Chunchao Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing."
        },
        {
            "title": "Start",
            "content": "NaTex: Seamless Texture Generation as Latent Color Diffusion Zeqiang Lai1,2 , Yunfei Zhao2 , Zibo Zhao2 , Xin Yang2 Xin Huang2 , Jingwei Huang2 , Xiangyu Yue1 , Chunchao Guo2 1MMLab, CUHK 2Tencent Hunyuan https://natex-ldm.github.io 5 2 0 2 0 2 ] . [ 1 7 1 3 6 1 . 1 1 5 2 : r Figure 1. High-quality textured 3D assets generated by NaTex from single image (Geometry from Hunyuan3D 2.5 [20].)"
        },
        {
            "title": "Abstract",
            "content": "We present NaTex, native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multiview images synthesized by geometry-conditioned MultiView Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features novel paradigm that addresses the aforementioned issues by viewing texture as dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises geometry-aware color point cloud VAE and multiEqual contribution. Corresponding authors. control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAEDiT architecture, where the geometry latents are extracted via dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing. 1 1. Introduction The creation of realistic and diverse materials is cornerstone of modern computer graphics, directly governing the visual fidelity of everything from cinematic visual effects to immersive virtual worlds. However, the manual creation of textures remains profound bottleneckan artisanal process that is both time-consuming and requires deep expertise. This challenge has catalyzed paradigm shift from manual creation to automated generation, seeking to develop generative systems that can produce high-quality, physically-plausible texture efficiently and at scale. To meet the high standards, multi-view texturing has become the de facto approach in numerous research studies [9, 12, 14, 58, 61] as well as commercial products [1 3, 20, 34]. The concept is straightforward. It first generates multi-view images that align the input geometry from different viewpoints. Then, using the camera information from these viewpoints, deterministic backprojection process is employed to reconstruct 3D textures from the 2D views. One of the key advantages of this paradigm is that we can leverage pre-trained image generative models [19, 24, 40] and accompanying techniques [4, 5, 52, 57] to generate multi-view images, which form the foundation for the high quality and diversity of the generated textures. Despite their success, multi-view texturing still faces fundamental challenges as shown in Fig. 2, including: (1) the lack of robust inpainting scheme for occlusion regions; (2) the difficulty in achieving precise alignment of texture features with fine-grained geometric details; and (3) the challenge of maintaining multi-view consistency and coherence across content, color, and illumination. These errors can accumulate and manifest during the projection and baking process, introducing undesired artifacts to the textured results. However, these challenges are inherently difficult to address due to several fundamental reasons. First, occlusion regions are an inevitable aspect of multi-view texturing; no matter the approach, they cannot be entirely avoided. Second, latent space diffusion inherently introduces errors, which makes pixel-level edge alignment hard to achieve, and 2D normal control is often insufficiently precise to handle fine-grained details. Third, maintaining consistency across multiple views is costly process, and even state-of-the-art video models [18, 45] struggle to achieve satisfactory results in this regard. Overall, these problems are broadly existing in 2D lifting methods [7, 8], which are largely inevitable and stem from the cascading errors in modality changes. As result, it remains compelling yet significantly underexplored research direction: Can we treat 3D textures natively as first-class citizens to address the issues caused by modality change? What kind of paradigm would make this process more scalable? In sense, treating 3D textures as first-class citizens naturally alleviates many of these chalFigure 2. Illustration of the fundamental challenges in multi-view diffusion (MVD) texturing, compared with the proposed NaTex. lenges. Since textures are generated directly on the geometry surface, post-processing such as inpainting is no longer necessary. By directly injecting the entire geometry, the model avoids the inherent information loss caused by projecting 3D shapes into 2D views (e.g., depth or normal maps discard occluded regions and structural details). This allows the model to fully reason over the spatial context and achieve more accurate geometrytexture alignment. Moreover, the coherent and unified representation of the entire 3D texture also simplifies the maintenance of global consistency. However, current solutions often build upon proxy representations (e.g., UV maps [30, 53], point-based features [48, 49]), resulting in data inefficiency as well as cascade errors. Thus, their ability to realize the full promise of 3D-native texture generation remains limited. In this paper, we introduce NaTex, novel latent color diffusion model that natively generates textures in 3D space. In contrast to previous approaches that rely on intermediate representations such as Gaussian Splatting [15, 49] or UV Maps [30, 53], NaTex directly predicts RGB color for given 3D coordinates via latent diffusion approach, paradigm that has shown remarkable effectiveness in image [19], video [45], and 3D shape generation [20], yet unexplored for texture generation. Specifically, NaTex models texture as dense color point cloud, effectively forming continuous color field in 3D space. To mitigate the computational challenges of performing diffusion directly on dense point cloud, we propose color point cloud Variational Autoencoder (VAE) with similar architecture to 3DShape2VecSet [56]. Unlike 3DShape2VecSet, which focuses on shape autoencoding, our model operates on color point clouds. We retain the use of cross-attention for compressing the input point cloud into set of latent vectors queried by points, while our set is ordered, as the point queries are known and sampled from the input geometry at test time, which makes pointwise geometry condition possible. We also introduce new color regression loss that supervises both onand near-surface regions. Together, these adaptations yield an efficient autoencoder that achieves over 2 80 compression, enabling efficient scaling for subsequent diffusion transformer (DiT) generation. Beyond occlusion-free and coherent representation for texture, another leading advantage of native texture models is native geometry control, which greatly improves alignment. In contrast, previous multi-view texturing could only utilize fragmented geometric control, such as per-view normals and positions. This necessitated the design of complex consistency modules to maintain cross-view coherence. Meanwhile, many 3D structural details are inherently ambiguous when observed from single 2D projection, making precise texture-geometry alignment difficult to achieve. In this work, we address these challenges through novel native geometry control by co-designing the VAEDiT architecture. Our key idea is to integrate precise surface hints into DiT via pairwise conditional geometry tokens, implemented through positional embeddings and channel-wise concatenation. To complement this, we design dual-branch VAE that extends the color VAE with an additional geometry branch, encoding shape cues to guide color latent compression. In this way, geometry tokens are deeply intertwined with color tokens, enabling stronger geometric guidance during color generation at test time. Building upon the aforementioned designs, we further introduce multi-control color DiT that flexibly accommodates different control signals. Our design enables wide range of applications beyond image-conditioned texture generation (with geometry and image controls) to textureconditioned material generation and texture refinement (using an initial texture, named as color control). Notably, NaTex exhibits remarkable generalization capability, enabling image-conditioned part segmentation and texturing even in training-free manner. To evaluate the effectiveness of our framework, we train NaTex-2B, which is mainly for texture generation but also adapted to previously mentioned applications for primary verifications. We report the comparison of NaTex-2B against previous methods, showing that it delivers exceptionally high-quality results and significantly outperforms prior approaches in texture oclusion, coherence and alignment. Our main contributions are summarized as follows: We design highly extensible framework for color field generation that, while demonstrated primarily on texture generation, can be readily extended to other tasks such as material modeling and part-level semantic segmentation. We design geometry-aware color VAE for color point clouds, in which geometric branch encodes local shape cues to achieve geometric awareness for color generation. We propose novel multi-control color DiT that flexibly integrates geometric, image conditions, and color conditions, enabling seamless texture generation and beyond. We achieve state-of-the-art performance against prior methods, particularly in texture coherence and alignment. 2. Related Works 2.1. 3D Texturing via 2D Priors prevalent approach in 3D texture synthesis involves adapting pre-trained 2D models such as the text-to-image diffusion models [40] for the 3D domain. An influential line of research employs Score Distillation Sampling (SDS) [38] for iterative texture optimization [7, 25, 27, 60]. While these methods can generate detailed textures, they are often computationally expensive and tend to suffer from 3D inconsistencies, commonly referred to as the Janus problem (multi-faced objects). Similarly, iterative mesh painting techniques [6, 39], which generate textures by inpainting multiple camera views, also struggle with maintaining seamless and consistent results. To address these consistency challenges, recent work has focused on Multi-view Diffusion (MVD) models [23, 31, 33, 41, 46]. These models are explicitly trained to generate camera-consistent, object-centric images from text or image prompts. In texture generation, this paradigm is extended by conditioning the diffusion process on 3D geometry [8, 9, 55, 58], often through rendered inputs like depth or normal maps. This geometric conditioning ensures that the generated views align with the underlying surface, thereby minimizing artifacts in the baked texture. 2.2. Native 3D Texture Generation Despite the success of MVD texturing in various commercial products [13, 20, 34], these projection-based methods still struggle at keeping multi-view consistency and the precise alignment of texture with fine-grained geometric details. These problems could hardly be avoided as they are deeply intertwined with the inherent limitations of the baking process and MVD itself unless one can instead generate textures directly and natively within 3D space. In fact, early approaches to 3D asset creation, such as SDS [7, 32, 38] and large reconstruction models [11, 44, 51] are native texture generators, though they typically generate geometry and texture simultaneously, and the quality is limited. Generally, previous attempts at native texture generation have focused on utilizing Generative Adversarial Networks (GAN) [35, 42], feed-forward [49], or diffusion models [29, 30, 53, 54], to predict face color [42], point color [29, 53], UV color [30, 5355], texture field [35], or Gaussian splatting [49]. Some methods, such as TexGaussian [49] and TexOct [29], adopt octree-representation for compression, while TexGarment [30] adopts 2D VAE by viewing UV maps as images. More recently, Trellis [48] has also demonstrated how to generate textured assets in one phase through their SLAT representation. UniTEX [26] presents refinement module but is limited at low resolution due to the complexity of triplane representation. In contrast to previous work, we focus on texture gen3 Figure 3. Overall architecture of NaTex: it mainly consists of geometry-aware color VAE for reconstruction and multi-control color DiT for generation, adaptable for diverse applications. Left-most assets are all generated by NaTex. eration directly in 3D space, given an input geometry from artists or 3D generators [13, 14, 2022, 50, 61]. We demonstrate that 3D textures can be formulated within the scalable latent diffusion model paradigman approach not previously exploredyielding stunning results. 3. Method It remains an open problem which generative paradigm offers the most scalable solution for texture synthesis. Prior approaches that operate in view space via multiview texturing [8, 12, 61] or directly generate color in UV space [54] often suffer from fundamental limitations, including misalignment with geometry, inability to handle occluded regions, and excessive reliance on UV unwrapping. The proposed NaTex is generative latent color diffusion model that synthesizes textures directly in 3D space. As shown in Fig. 3, it follows the standard latent diffusion architecture consisting of geometry-aware color VAE [16] and multi-control color DiT [37]. In the following, we detail our representation of 3D texture, the designs of VAE, DiT, and the conditioning mechanisms, as well as the broader applications of our model. 3.1. Color Representation and VAE Instead of modeling textures in projective 2D image space or UV space as in prior works [9, 54, 61], we propose to represent textures natively in 3D as color field. Concretely, the goal is to predict RGB values conditioned on geometric positions. To realize this, we leverage dense color point clouds sampled from textured meshes as our representation. Compared to view-space methods, our approach operates directly in 3D, naturally handling occluded regions without requiring inpainting. Compared to UV-space methods, it avoids reliance on UV quality and instead provides more structured and coherent representation, better suited for generative modeling. Geometry-Aware Autoencoding. Insipred by the success of native geometry generation [14, 58, 61], we adopt VAE architecture similar to 3DShape2VecSet [56] to encode color point cloud. While alternative designs are possible, we leave their exploration to future work. visual overview of the architecture is provided in Fig. 3. The VAE takes as input point cloud Pc RN 9 sampled from textured mesh, containing RGB color, position, and normal, and reconstructs continuous color field (x) = c, mapping each 3D coordinate R3 to its color R3. One of the unique problems for texture generation is how to incorporate fine-grained geometric conditioning during generation. straightforward solution would be to encode geometry using VecSet-based ShapeVAE [56, 61] on the same color point cloud. Instead, we propose tighter integration, as illustrated in Fig. 3. In parallel with the texture VAE, we introduce geometry VAE branch that encodes geometric features from the coordinates and normals of the color point cloud. The resulting geometry latent set is then employed as queries to guide the texture encoder. Concretely, the input point cloud is uniformly sampled and consists of positions, normals, and colors. The geometry encoder consumes positions and normals, while the color encoder leverages all three modalities. Geometry queries are constructed as point queries [56] randomly 4 sampled from the color point cloud. For both geometry and texture encoding, we adopt the same network backbone as Hunyuan3D-VAE [61], which incorporates multiple layers of cross-attention and self-attention. During the inference, our model supports two common output modalities: it can synthesize UV texture map by mapping UV coordinates to 3D and querying colors via 3D coordinates, or it can directly predict per-face / pervertex colors by querying at vertex or face-center coordinates. Similar to geometry VAE [61], the texture VAE operates at arbitrary resolution, enabling flexible decoding for different downstream targets. During training, the geometry and texture VAEs are jointly optimized with KL divergence term, color regression loss that supervises both on-surface and near-surface queries, and truncated UDF loss: o(x) = 1, udf (x) if udf (x) > , if udf (x) s, (1) where the truncated UDF is adopted because correlating the color point cloud with watertight mesh (required for standard SDF loss) is non-trivial. For the color regression loss, we supervise both on-surface points and near-surface points. The latter is implemented by randomly offsetting query points along their normal directions within threshold γ. The overall training objective is thus: = λKLLKL + λcolorLcolor + λudfLUDF. (2) where λKL, λcolor, λudf are loss weights. Reconstruct any Field. key advantage of our autoencoding framework is its universality: any modality that admits an RGB-like representation can be seamlessly incorporated into the same latent space. For example, physically based materials can be encoded using unified color VAE by mapping metallic and roughness into modified albedo, where the blue channel is fixed to zero. Likewise, semantic part segmentation can be treated in the same way by mapping discrete part labels into color values. This unified treatment allows diverse signalsranging from appearance to semanticsto be represented and processed within single, coherent framework. 3.2. Multi-Control Color DiT We adopt an architecture similar to the rectified flow diffusion transformer [19] for generating the texture latent set. To accommodate richer control signals, we introduce several adaptations that allow the model to incorporate not only the input image but also the input geometry and an initial texture, as shown in Fig. 4. Image Control. Following Hunyuan3D-2 [61], we use Dinov2-Giant [36] for image conditioning, utilizing the embedding from the last hidden layer without the class token. Unlike Hunyuan3D-2 [61], which uses resolution of 518, 5 Figure 4. Illustration of multi-control mechanisms of the proposed color DiT. Color control is useful for texture-conditioned tasks. we scale the input to 1022, as we found higher-resolution conditioning helps for capturing fine-grained details. To minimize image token length, we retain the original aspect ratio by cropping the object from the 1022-resolution input image using binary mask. No additional positional embedding is used for image tokens, as we find the position information encoded in Dino to be sufficient. Geometry Control. Geometry conditioning plays crucial role in aligning the generated texture with the input In this work, we propose native geometry conmesh. trol, which includes two conditions: (1) we incorporate RoPE [43] based on the positions of sampled point queries, which provides coarse structural guidance; and (2) we leverage the geometry latent set obtained from the VAE (described in the previous section) as an additional embedding to deliver fine-grained guidance. Furthermore, since the geometry latent set is isomorphic to the texture latent set, we concatenate it with the noisy texture latent set along the channel dimension, enabling pointwise geometric guidance during texture generation. Color Control. Our model also supports incorporating an initial texture as extra control (termed color control), which is useful for various downstream tasks, such as texture-conditioned material generation as well as texture inpainting and refinement. To achieve it, we sample color point cloud from the given texture and encode it using our VAE to obtain conditional color latent set. This latent set is then concatenated with the noisy texture latent and geometry latent along the channel dimension, providing stronger guidance while keeping the sequence length unchanged. Training and Inference Strategies. During training, we encode the sampled color point cloud using our VAE to obtain aligned geometry and texture latent sets. Each latent token is associated with position, which is used for RoPE [43] applied to the noisy texture latent. The model Figure 5. Visual results showcasing representative applications of NaTex. Additional results are provided in the Appendix. Latent Size PSNR PSNR SSIM LPIPS 6144 64 12288 64 24576 64 28.74 29.95 30.86 31.70 33.19 34.30 0.980 0.984 0.987 0.0492 0.0445 0. Table 1. Quantitative results for texture reconstruction. denotes metrics calculated on the six orthogonal rendered views. metallic as two channels in an RGB color point cloud. By leveraging the same color VAE framework, we can train generator conditioned on an additional albedo latent set extracted from the proposed color VAE, facilitating efficient material synthesis with high fidelity. Texture Refinement. Our model with color control can be viewed as neural refiner that automatically inpaints occluded regions and corrects texture. Moreover, thanks to the strong conditioning, it can perform the process in just five steps, making it extremely fast and efficient for wide range of downstream tasks that require intelligent refinement. Part Segmentation. Without any additional training, our model can be easily adapted for part segmentation tasks. This can be achieved by feeding 2D segmentation of the input image into the model, allowing it to generate texture map that aligns with 3D part segmentation results. Part Texturing. Within our native texture framework, part generation is as straightforward as generating textures for the entire object, as we can predict color directly in 3D space for different part surfaces. Moreover, our method naturally handles occluded regions, producing consistent and clean textures for different parts. 4. Experiments 4.1. Comparison Reconstruction. To the best of our knowledge, we are the first method to utilize native latent diffusion model for texture generation. We evaluate the reconstruction performance of our method across different latent sizes. We adopt Figure 6. Visual comparison of texture reconstruction results. is trained with flow matching [28] loss. For albedo generation, following MaterialMVP [9], we also include an illumination-invariant loss, which results hybrid loss: = ϵpred ϵgt2 2 + γϵpred ϵpred22 (3) where ϵpred2 and ϵpred are the predictions for input images with different illuminations. During inference, we first convert UV coordinates or vertex positions into point cloud and sample normals from the input geometry. The geometry point cloud is then encoded by the geometry branch of our VAE, producing geometry latent set along with corresponding latent positions. These geometry conditions, together with the input image, are fed into the generator for diffusion sampling step to produce the final texture. 3.3. Applications NaTex provides unified framework that is broadly applicable across diverse 3D tasks, as shown in Fig. 5. Below, we highlight several representative use cases: Material Generation. Physical-based rendering becomes extremely straightforward by treating roughness and Figure 7. Visual comparison of different methods for generating textured 3D assets from images: commercial models use their own geometries, while other methods share the same geometry from Hunyuan3D 2.5 [20]. All methods are rendered with albedo only. several metrics for evaluation: PSNR is computed directly on the color point cloud, while PSNR and SSIM [47] and LPIPS [59] are calculated by rendering the reconstructed textured meshes into 2D images. The numerical comparison is shown in Tab.1. Although our model is trained with maximum of 6144 tokens, the reconstruction quality improves as the latent size increases. The visual comparison of reconstruction is shown in Fig.6. Generation. We perform quantitative comparison with other image-conditioned texture generation methods, including Paint3D [55], TexGen [54], Hunyuan3D-2 [61], RomanTex [8], and MaterialMVP [9]. The comparison focuses solely on albedo results. Following the evaluation protocol from MaterialMVP [9], we use the same test set and four metrics for assessment: CLIP-based FID (c-FID), Learned Perceptual Image Patch Similarity (LPIPS), CLIP Maximum-Mean Discrepancy (CMMD), and LIP-Image Similarity (CLIP-I). The numerical comparison is presented in Tab. 2, where our method consistently outperforms the others. Additionally, we provide visual comparison with closed-source commercial models, Rodin-Gen2 and Tripo 3.0, in Fig. 7. It is evident that all competing methods struggle to align textures along geometry boundaries, while our method achieves near-perfect alignment. Notably, even in cases without occlusion regions (e.g., the character), other methods still produce artifacts, such as misaligned stars and buttons. Refinement/Inpainting. Our model can also function Method cFID CMMD CLIP LPIPS Paint3D [55] TexGen [54] Hunyuan3D-2 [61] RomanTex [8] MaterialMVP [9] NaTex (Ours) 26.86 28.23 26.43 24.78 24.78 21.96 2.400 2.447 2.318 2.191 2.191 2.055 0.887 0.882 0.889 0.891 0.921 0. 0.126 0.133 0.126 0.121 0.121 0.102 Table 2. Quantitative comparison with state-of-the-art methods. interpolation. As shown in Fig. 8, it is clear that our method generates cleaner and better-aligned textures in the occluded regions (see the zoomed-in window of the house). 4.2. Evaluation Ablation Study of Training Strategies. We evaluate different setups for the proposed color DiT with varying geometry conditions. The first variant removes the RoPE from each color token, while the second variant replaces the tight shape embedding from the proposed geometry-aware color VAE with shape embedding from standalone shape VAE, such as Hunyuan3D-VAE [61]. The comparison is shown in Fig. 9, where we observe that both conditions improve image-texture alignment, such as the stripes on the awning of the house and the colors of the traffic light. Additionally, the shape embedding influences texture-geometry alignment. Without the tight embedding, color sometimes diffuses, as seen in the chair back. Effect of Different Inference Schemes. Although our model is trained with maximum of 6144 tokens, it supports various inference schemes at test time. Fig. 10 demonstrates the results under different token lengths and sampling steps. It can be observed that both generation quality and alignment improve gradually as the token length increases (see windows). Moreover, our model surprisingly achieves one-step generation capability, despite not being distilled, due to the strong conditioning. 5. Conclusion In this paper, we introduce NaTex, novel framework for generating textures directly in 3D space. We demonstrate that 3D texture generation can be formulated as simply as common latent diffusion, an extremely successful paradigm in image, video, and geometry generation, without the need for multi-stage pipelines with 2D lifting priors. Through careful design of both the VAE and DiT, our method effectively mitigates several key challenges, such as texturing occlusion regions, fine-grained texture-geometry alignment, and texture consistencyissues that have been inherently difficult to address in previous methods. Additionally, our model exhibits strong versatility across wide range of downstream tasks, even without any task-specific training. Figure 8. Visual comparison of conventional inpainting and our neural inpainting. Two views of multiview images are shown on the left. We need to inpaint the occlusion in the window. Figure 9. Visual comparison of different geometry conditioning methods: with RoPE and geometry embedding from the geometryaware color VAE, texture and color alignment are optimized. Figure 10. Illustration of different inference setups. Without additional training and distillation, more tokens improve quality and alignment, and we achieve one-step generation for free. as refinement or inpainting module by utilizing the color control proposed in Sec. 3.2. We compare our method with the conventional inpainting technique, which uses OpenCV 8 Figure 11. Illustration of our material generation results from case study, with individual components visualized separately. Figure 12. different lightings, rendered using various environment maps. Illustration of our material generation results under A. Implementation Details Training Details. To validate the proposed method, we train color VAE with 300M parameters and color DiT with 1.9B parameters using flow-matching objective. The VAE is trained with maximum of 6144 tokens, with token scaling during inference. For DiT training, we set the batch size to 256 and use constant learning rate scheduler with linear warm-up for the first 500 steps. The learning rate starts at 1 104 and decays to 1 105 thereafter. The illumination-invariant loss is introduced once pretraining converges, with weight of 5. We adopt classifier-free guidance [10] by replacing conditioning embeddings with zero embeddings at 10% probability during training. Unless otherwise stated, all results in this paper are obtained with 5 diffusion steps and guidance scale of 2. The illuminationinvariant loss is introduced once pretraining converges, with its weight set to 5. Data Preparation. We use Blender to sample uniform color point clouds from raw meshes. For the input images, we render 24 views uniformly around the object, with random elevation angles in the range of 45 to -30. We also randomly select from various illumination environments, including point lights, area lights, and HDRI maps. B. More Details on Applications Material Generation. Thanks to the flexible design of the proposed NaTex framework, we can easily adapt it for material generation with color control. Specifically, we formulate material generation as two-channel texture generation task conditioned on the textured mesh with albedo. We reuse the same color VAE employed for texture generation, representing roughness and metallic as two channels in an RGB color point cloud. new material DiT is then Figure 13. Visual comparison between our NaTex material generation pipeline and conventional MVD-based material pipeline. Our method produces more accurate and better-aligned materials compared to prior approaches. trained on this material color point cloud data, conditioned on the input image (image control), the textured mesh with albedo (color control), and the input geometry (geometry control). During inference, we adopt two-stage approach: the first stage predicts the albedo, and the second stage predicts roughness and metallic based on the previously predicted albedo. Figure 14. High-quality PBR-textured assets generated by NaTex. Geometry obtained from Hunyuan3D 2.5 [20]. The generation results of NaTex-Material inherit the advantages of native texture generation, producing wellaligned and coherent roughness and metallic maps, as shown in Fig.11. We believe this represents significant advantage for developing next-generation material generation frameworks, since previous MVD approaches often struggle with alignment and sometimes misinterpret material properties, as illustrated in Fig.13. Fig.12 presents our material generation results under different lighting conditions, demonstrating the effectiveness of the generated materials. Fig.14 showcases additional high-quality PBR-textured assets generated by NaTex, with albedo, roughness, and metallic maps all produced natively by our framework. Part Segmentation. We find that our model can be readily applied to part segmentation by conditioning on 2D 10 Figure 15. Visual results of part segmentation using finetuned version of NaTex-2B. We provide 2D mask as the input image for the given geometry, and NaTex textures the model accordingly. mask, as indicated in the main paper. Specifically, this can be achieved by first performing semantic segmentation on the input RGB image using SAM[17]. We then directly apply our texture model, NaTex-2B, without any additional training, feeding in the 2D mask to obtain the textured mesh. Nevertheless, this zero-shot strategy may produce fragmented or inconsistent results for complex structures. To address this, we finetune the base model on dedicated dataset. Surprisingly, the results of the finetuned model are highly accurate even on complex cases, as shown in Fig.15, providing strong 3D segmentation with well-aligned boundaries. This further demonstrates the effectiveness and adaptation capability of our model. Part Texturing. Texturing individual parts is just as straightforward as generating textures for the entire object. Unlike previous MVD approaches, which struggle with interior regions, our method naturally circumvents this issue by predicting color directly in 3D space for different part surfaces. Fig.16 shows part texturing results obtained by directly applying NaTex-2B. It can be observed that our model effectively handles occluded regions between parts and generates accurate textures for these areas. Fig.17 provides additional visual examples. Texture Refinement. Our model can also serve as second-stage refiner for MVD pipelines. This can be easily achieved by fine-tuning NaTex-2B with color control conIn general, our refiner can ditioned on an initial texture. correct various projection errors and automatically inpaint occluded regions, as illustrated in Fig. 18. Moreover, thanks Figure 16. Illustration of part texturing using NaTex without any additional training. Our model generates textures for different parts without suffering from occlusion issues between them, as shown in the two renders with varying part arrangements. Figure 17. Visual examples of additional part texturing results generated by NaTex. to strong conditioning, this process can be performed in just five steps without any distillation, making it extremely fast 12 Illustration of texture refinement using NaTex with Figure 18. color control. As shown, NaTex effectively corrects errors in the input mesh caused by occluded regions and inconsistencies. and efficient for wide range of downstream tasks. C. Limitations and Future Works It is exciting that the proposed NaTex advances texture generation, producing more seamless results and generalizing to variety of applications. However, limitations remain that warrant further research. For example, the reconstruction quality of the VAE could be improved to support higher-resolution textures. Data curation should be enhanced for material generation. Part segmentation could be refined to reduce ambiguity and improve granularity. New methods are needed to handle closed surfaces in adjacent parts for part texturing. Additionally, texture refinement also presents promising direction for incorporating more 2D priors and leveraging established MVD research."
        },
        {
            "title": "References",
            "content": "[1] HiTem3D: Next-Gen 3D model generator, powered by AI. 2, 3 [2] Hyper3D - Rodin ChatAvatar. [3] Tripo Studio Your next 3D workspace with AI. 2, 3 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. 2 [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2256022570, 2023. 2 [6] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner. Text2tex: Text-driven texIn Proceedings of the ture synthesis via diffusion models. IEEE/CVF International Conference on Computer Vision, pages 1855818568, 2023. 3 [7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highquality text-to-3d content creation. ICCV, 2023. 2, [8] Yifei Feng, Mingxin Yang, Shuhui Yang, Sheng Zhang, Jiaao Yu, Zibo Zhao, Yuhong Liu, Jie Jiang, and Chunchao Guo. Romantex: Decoupling 3d-aware rotary positional embedded multi-attention network for texture synthesis, 2025. 2, 3, 4, 7, 8 [9] Zebin He, Mingxin Yang, Shuhui Yang, Yixuan Tang, Tao Wang, Kaihao Zhang, Guanying Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, and Wenhan Luo. Materialmvp: Illumination-invariant material generation via multi-view pbr diffusion, 2025. 2, 3, 4, 6, 7, 8 [10] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In arXiv preprint arXiv:2207.12598, 2022. 9 [11] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 3 [12] Zehuan Huang, Yuanchen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy. arXiv preprint arXiv:2412.03632, 2024. 2, 4 [13] Team Hunyuan3D, :, Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Linus, Penghao Wang, Qingxiang Lin, Sicong Liu, Xianghui Yang, Yixuan Tang, Yunfei Zhao, Zeqiang Lai, Zhihao Liang, and Zibo Zhao. Hunyuan3d-omni: unified framework for controllable generation of 3d assets, 2025. 4 [14] Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to highfidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025. 2, [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2 [16] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4 [17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 11 [18] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2 [19] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, [20] Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards highfidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025. 1, 2, 3, 4, 7, 10 [21] Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Fuyun Wang, Huiwen Shi, Xianghui Yang, Qinxiang Lin, Jinwei Huang, Yuhong Liu, Jie Jiang, Chunchao Guo, and Xiangyu Yue. Unleashing vecset diffusion model for fast shape generation, 2025. [22] Biwen Lei, Yang Li, Xinhai Liu, Shuhui Yang, Lixin Xu, Jingwei Huang, Ruining Tang, Haohan Weng, Jian Liu, Jing Xu, Zhen Zhou, Yiling Zhu, Jiankai Xing, Jiachen Xu, Changfeng Ma, Xinhao Yan, Yunhan Yang, Chunshi Wang, Duoteng Xu, Xueqi Ma, Yuguang Chen, Jing Li, Mingxin Yang, Sheng Zhang, Yifei Feng, Xin Huang, Di Luo, Zebin He, Puhua Jiang, Changrong Hu, Zihan Qin, Shiwei Miao, Haolin Liu, Yunfei Zhao, Zeqiang Lai, Qingxiang Lin, Zibo Zhao, Kunhong Li, Xianghui Yang, Huiwen Shi, Xin Yang, Yuxuan Wang, Zebin Yao, Yihang Lian, Sicong Liu, Xintong Han, Wangchen Qin, Caisheng Ouyang, Jianyin Liu, Tianwen Yuan, Shuai Jiang, Hong Duan, Yanqi Niu, Wencong Lin, Yifu Sun, Shirui Huang, Lin Niu, Gu Gong, Guojian Xiao, Bojian Zheng, Xiang Yuan, Qi Chen, Jie Xiao, Dongyang Zheng, Xiaofeng Yang, Kai Liu, Jianchen Zhu, Lifu Wang, Qinglin Lu, Jie Liu, Liang Dong, Fan Jiang, Ruibin Chen, Lei Wang, Chao Zhang, Jiaxin Lin, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Yinhe Wu, Jiayao Du, Jupeng Chen, Xinyue Mao, Dongyuan Guo, Yixuan Tang, Yulin Tsai, Yonghao Tan, Jiaao Yu, Junlin Yu, Keren Zhang, Yifan Li, Peng Chen, Tian Liu, Di Wang, Yuhong Liu, Linus, Jie Jiang, Zhuo Chen, and Chunchao Guo. Hunyuan3d studio: End-to-end ai pipeline for game-ready 3d asset generation, 2025. 4 [23] Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wenhan Luo, Ping Tan, et al. Era3d: High-resolution multiview diffusion using efficient row-wise attention. arXiv preprint arXiv:2405.11616, 2024. 3 [24] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024. 2 [25] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards highfidelity text-to-3d generation via interval score matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65176526, 2024. 3 [26] Yixun Liang, Kunming Luo, Xiao Chen, Rui Chen, Hongyu Yan, Weiyu Li, Jiarui Liu, and Ping Tan. Unitex: Universal high fidelity generative texturing for 3d shapes. arXiv preprint arXiv:2505.23253, 2025. 3 [27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, 2023. 3 [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 6 [29] Jialun Liu, Chenming Wu, Xinqi Liu, Xing Liu, Jinbo Wu, Haotian Peng, Chen Zhao, Haocheng Feng, Jingtuo Liu, and Errui Ding. Texoct: Generating textures of 3d models with In Proceedings of the IEEE/CVF octree-based diffusion. Conference on Computer Vision and Pattern Recognition, pages 42844293, 2024. 3 [30] Jialun Liu, Jinbo Wu, Xiaobo Gao, Jiakui Hu, Bojun Xiong, Xing Liu, Chen Zhao, Hongbin Pei, Haocheng Feng, Yingying Li, et al. Texgarment: Consistent garment uv texture generation via efficient 3d structure-guided diffusion transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2656626575, 2025. 2, 3 [31] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 3 [32] Zexiang Liu, Yangguang Li, Youtian Lin, Xin Yu, Sida Peng, Yan-Pei Cao, Xiaojuan Qi, Xiaoshui Huang, Ding Liang, and Wanli Ouyang. Unidream: Unifying diffusion priors for relightable text-to-3d generation. In Proceedings of European Conference on Computer Vision, pages 7491, 2024. 3 [33] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99709980, 2024. [34] Meshy. Meshy AI. 2, 3 [35] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas Geiger. Texture fields: Learning texIn Proceedings of ture representations in function space. the IEEE/CVF international conference on computer vision, pages 45314540, 2019. 3 [36] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 5 [37] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 4 [38] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. 3 [39] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. 3 [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, [41] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 3 [42] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias Nießner, and Angela Dai. Texturify: Generating textures on 3d shape surfaces. In ECCV, pages 7288. Springer, 2022. 3 [43] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [44] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2024. 3 [45] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2 [46] Peng Wang and Yichun Shi. multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. 3 Imagedream: Image-prompt arXiv preprint [47] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 7 [48] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 2, 3 [49] Bojun Xiong, Jialun Liu, Jiakui Hu, Chenming Wu, Jinbo Wu, Xing Liu, Chen Zhao, Errui Ding, and Zhouhui Lian. Texgaussian: Generating high-quality pbr material via In Proceedings of the octree-based 3d gaussian splatting. Computer Vision and Pattern Recognition Conference, pages 551561, 2025. 2, 3 [50] Xinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang Lai, Yunfei Zhao, Zhuo Chen, et al. X-part: high fidelity and structure coherent shape decomposition. arXiv preprint arXiv:2509.08643, 2025. 4 [51] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d-1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. 3 [52] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models, 2023. 2 [53] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and Xiaojuan Qi. Texture generation on 3d meshes with pointuv diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42064216, 2023. 2, 3 [54] Xin Yu, Ze Yuan, Yuan-Chen Guo, Ying-Tian Liu, Jianhui Liu, Yangguang Li, Yan-Pei Cao, Ding Liang, and Xiaojuan Qi. Texgen: generative diffusion model for mesh textures. ACM Transactions on Graphics (TOG), 43(6):114, 2024. 3, 4, 7, 8 [55] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, and Gang Yu. Paint3d: Paint anything 3d with lighting-less texture diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42524262, 2024. 3, 7, 8 [56] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):116, 2023. 2, [57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 2 [58] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 2, 3, 4 [59] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 7 [60] Yuqing Zhang, Yuan Liu, Zhiyu Xie, Lei Yang, Zhongyuan Liu, Mengzhou Yang, Runze Zhang, Qilong Kou, Cheng Lin, Wenping Wang, et al. Dreammat: High-quality pbr material generation with geometry-and light-aware diffusion models. ACM Transactions on Graphics, 43(4):118, 2024. 3 [61] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 2, 4, 5, 7,"
        }
    ],
    "affiliations": [
        "MMLab, CUHK",
        "Tencent Hunyuan"
    ]
}