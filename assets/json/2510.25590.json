{
    "paper_title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
    "authors": [
        "Pengtao Chen",
        "Xianfang Zeng",
        "Maosen Zhao",
        "Mingzhu Shen",
        "Peng Ye",
        "Bangyin Xiang",
        "Zhibo Wang",
        "Wei Cheng",
        "Gang Yu",
        "Tao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved."
        },
        {
            "title": "Start",
            "content": "Preprint Version (Under Review) October 2025 REGIONE: ADAPTIVE REGION-AWARE GENERA-"
        },
        {
            "title": "TION FOR EFFICIENT IMAGE EDITING",
            "content": "Pengtao Chen1 Xianfang Zeng2 Maosen Zhao1 Mingzhu Shen3 Bangyin Xiang1 Zhibo Wang2 Wei Cheng2 Gang Yu2 Tao Chen1 1 Fudan University Code: https://github.com/Peyton-Chen/RegionE 3 Imperial College London 2 StepFun Peng Ye1 5 2 0 2 9 ] . [ 1 0 9 5 5 2 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06, respectively, with minimal quality loss (PSNR: 30.52032.133). Evaluations by GPT-4o also confirmed that semantic and perceptual fidelity were well preserved."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, diffusion models (Rombach et al., 2022) have achieved rapid progress in generative tasks, particularly in visual generation, where state-of-the-art models can synthesize highly realistic images. Within this context, the task of editing existing images according to user requirements has gradually emerged as an important direction (Kawar et al., 2023). Recently, diffusion-based foundation models, such as FLUX.1 Kontext (Labs et al., 2025), Qwen-Image-Edit (Wu et al., 2025), and Step1X-Edit (Liu et al., 2025d), have been developed. These models can perform precise image editing using only textual instructions, offering novel solution for instruction-based image editing and providing more powerful tools for image post-processing (Choi et al., 2024). Although diffusion-based IIE models can achieve impressive editing results, their high inference latency limits their use in real-time applications. Previous research on efficient diffusion inference has primarily focused on image generation. For instance, some studies reduce model parameters through pruning (Rombach et al., 2022; Castells et al., 2024), others decrease model bit-width via quantization (Shang et al., 2023; Zhao et al., 2025), and some employ distillation to reduce model size (Kim et al., 2023) and the number of timesteps (Sauer et al., 2024). In the two-stage Corresponding author. Project leader. Work was done when interned at StepFun. 1 Preprint Version (Under Review) October 2025 Figure 1: Trajectories of different regions in the IIE task. In unedited regions, the trajectory is nearly linear, allowing early-stage velocity to provide reliable estimate of the multi-step denoised images, including the final result. In contrast, edited regions exhibit curved trajectories, making the final image harder to predict. Despite this, the velocity between consecutive timesteps remains consistent. inversion-based editing paradigm (Pan et al., 2023; Wang et al., 2025), redundancy in the inversion and denoising stages has been analyzed, leading to methods like EEdit (Yan et al., 2025) that accelerate both stages simultaneously. However, for the emerging denoising-only paradigm of IIE, the redundancy and feasibility of efficient inference remain largely unexplored. Our study reveals that current IIE models exhibit two significant types of redundancy: 1) Spatial Generation Redundancy. Unlike image generation tasks, which require reconstructing the entire image, IIE models often need to modify only local regions specified by the instructions, while the remaining areas remain essentially unchanged. For example, as shown in Figure 1, the model edits only the region around the hat. Nevertheless, IIE models apply the same computational effort to both edited and unedited areas, resulting in significant redundancy in the latter. 2) Redundancy across diffusion timesteps. First, at neighboring timesteps, the key and value within the attention layers at the same network depth are highly similar. Second, in the middle stages of denoising, the velocity output by the diffusion transformer (DiT) at adjacent timesteps is also highly similar. To mitigate spatial and temporal redundancy in IIE models, this paper introduces RegionE, trainingfree, adaptive, and region-aware generative framework that accelerates the current IIE models. Firstly, we observed that the trajectories of edited regions are often more curved, making it difficult to accurately predict the final edited results at early timesteps, as shown in Figure 1. In contrast, unedited regions follow nearly linear trajectories, allowing more reliable predictions from the same early steps. Based on this observation, RegionE introduces an Adaptive Region Partition (ARP), which performs one-step estimation for the final image in the early stage and compares its similarity with the reference (instruction) image. Regions with high similarity (minimal change after editing) are classified as unedited, whereas regions with low similarity are classified as edited. Then, we perform region-aware generation on the two separated parts. Specifically, We replace multi-step denoising with one-step estimation for the unedited areas and apply region-iterative denoising for edited areas. During edited region generation, RegionE discards unedited region tokens and instruction image tokens, and effectively reinjects global context into local generation through our proposed RegionInstruction KV Cache (RIKVCache), which leverages the similarity of key and value across timesteps. This process primarily addresses redundancy in spatial. Finally, regarding temporal redundancy, we find that the velocity outputs of DiT at adjacent timesteps are highly consistent in direction but decay in magnitude over time, with the decay dependent on the timestep. To exploit this property, RegionE introduces an Adaptive Velocity Decay Cache (AVDCache), which accurately models this pattern and further accelerates the region generation process. Experimental results demonstrate that RegionE achieves speedups of approximately 2.57, 2.41, and 2.06 on Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit, respectively, while maintaining PSNR values of 30.520, 32.133, and 31.115 before and after acceleration. Evaluations using GPT-4o further indicate that the perceptual differences are negligible, confirming that RegionE effectively eliminates redundancy in IIE tasks without compromising image quality. The contributions of our paper are as follows: We observe that in IIE tasks, unedited regions exhibit nearly linear generation trajectories, allowing early-stage velocities to provide reliable estimates for multi-step denoised images, including the Preprint Version (Under Review) October 2025 final image. In contrast, edited regions follow more curved trajectories, making the final image harder to predict. Nevertheless, the velocity remains consistent across consecutive timesteps. We propose RegionE, training-free, efficient IIE method with adaptive, region-aware generation. It reduces spatial redundancy by performing early adaptive predictions for edited and unedited regions and generating each region locally in subsequent stages, while mitigating temporal redundancy via velocity-decay cache across timesteps. RegionE achieves 2.57, 2.41, and 2.06 end-to-end speedups on Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit, while maintaining PSNR (30.520, 32.133, 31.115) and SSIM (0.939, 0.917, 0.937). Evaluations with GPT-4o further confirm that no quality degradation occurs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Efficient Diffusion Model. Although few efficient methods have been developed specifically for IIE models, variety of acceleration techniques have been proposed for diffusion models more generally. From the perspective of parameter redundancy, researchers have introduced pruning methods such as Diff-Pruning Fang et al. (2023) and LD-Pruner (Castells et al., 2024), quantization methods such as PTQ4DM (Shang et al., 2023), FPQuant (Zhao et al., 2025), and SVDQuant (Li et al., 2024a), distillation methods such as BK-SDM (Kim et al., 2023) and CLEAR (Liu et al., 2024), sparse attention methods such as DiTFastAttn (Yuan et al., 2024; Zhang et al., 2025), SVG (Xi et al., 2025), Sparse-vDiT (Chen et al., 2025), and VORTA (Sun et al., 2025), and early-stopping strategies such as ES-DDPM (Lyu et al., 2022). From the perspective of temporal redundancy, methods like DeepCache (Ma et al., 2024), -DiT (Chen et al., 2024), FORA (Selvaraju et al., 2024), and TeaCache (Liu et al., 2025a) reuse intermediate features across timesteps (Shen et al., 2024; Liu et al., 2025c;b), while approaches such as LCM (Luo et al., 2023) and ADD (Sauer et al., 2024) reduce the number of timesteps through model distillation. From the perspective of spatial redundancy, RAS (Liu et al., 2025e) observes that at each diffusion timestep, the model may focus only on semantically coherent regions; therefore, only those regions need to be updated, thereby accelerating image generation. Similarly, ToCa (Zou et al., 2024a) and DuCa (Zou et al., 2024b) note that during denoising, different tokens exhibit varying sensitivities, and dynamically updating only subset of tokens at each timestep can further accelerate image generation. In contrast to the methods above, RegionE leverages the trajectory characteristics unique to IIE tasks, while simultaneously addressing both spatial and temporal redundancies in diffusion-based image editing to achieve acceleration. Image Editing. Image editing is an essential task in the field of generative modeling. In the early U-Net (Ronneberger et al., 2015) era, ControlNet (Zhang et al., 2023b) introduced robust editing solution through repeat-structure design. As research advanced, inversion-based methods (Pan et al., 2023; Wang et al., 2025) gradually became the dominant approach. These methods apply noise to the original image in the latent space and then recover the edited result through denoising process. However, this paradigm involves both inversion and denoising stages, which increases complexity. At the same time, IIE models began to emerge. Approaches such as InstructEdit (Wang et al., 2023), MagicBrush (Zhang et al., 2023a), and BrushEdit (Li et al., 2024b) employed modular pipelines, in which large language models generate prompts, spatial cues, or synthetic instructionimage pairs to guide diffusion-based editing. Most of these approaches, however, are task-specific and lack generality. More recently, new class of IIE has been developed to improve general-purpose editing. These models rely solely on textual instructions, without requiring masks or task-specific designs, and still achieve effective editing performance. Concretely, they leverage MLLMs or advanced text encoders to provide richer semantic control signals, and feed both the target image and noise into DiT (Peebles & Xie, 2023) architecture to enhance image alignment. In this work, we propose an adaptive, region-aware acceleration method for this emerging IIE models."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "Flow Matching & Rectified Flow. Flow matching (Lipman et al., 2022) has become widely adopted training technique in advanced diffusion models. It facilitates the transfer from source distribution π1 to target distribution π0 by learning time-dependent velocity field v(x, t). This 3 Preprint Version (Under Review) October 2025 Figure 2: Comparison between traditional DiT and DiT in IIE (a, b). Symbolic visualization of the denoising process (c). L1 and cosine similarities of velocities between adjacent timesteps during denoising (d, e). Cosine similarity between velocities after t21 in edited and unedited regions with v21 (f). Cross-step key similarity (g) and cross-step similarity of instruction-related keys (h). velocity field is used to construct the flow through the ordinary differential equation (ODE): dϕt(x) dt = v(ϕt(x), t), ϕ1(x) π1. (1) Rectified Flow (Liu et al., 2022) simplifies this process through linear assumption. Given that X1 follows noise distribution π1 and X0 follows the target image distribution π0, the equation is Xt = (1 t)X0 + tX1, [0, 1]. (2) Differentiating both ends with respect to timestep yields: dXt dt = X1 X0. The velocity of the rectified flow v(Xt, t), always points in the direction of X1 X0. Therefore, the training loss is minimized by reducing the deviation between the velocity and X1 X0: = Et (cid:2)(X1 X0) v(Xt, t)2(cid:3). (3) The inference process involves starting from X1 and iteratively solving for X0 in reverse, using the learned velocity v(Xt, t). In practice, we typically use discrete Euler sampler, which discretizes the timestep ti(i NT , tT = 1, t0 = 0) and approximates: Xti1 = Xti ti,i1 v(Xti , ti), ti,i1 = ti ti1. (4) After iterations, the final target image X0 is obtained. This paper, therefore, targets the IIE task and optimizes the inference process of iterations in Equation 4. Instruction-Based Editing Model. Recent IIE models, such as Step1X-Edit (Liu et al., 2025d), FLUX.1 Kontext (Labs et al., 2025) and Qwen-Image (Wu et al., 2025), follow the same paradigm, as shown in Figure 2b. In these models, the velocity field is estimated using Instruction-DiT, the variants of DiT (Peebles & Xie, 2023). The input to Instruction-DiT consists of three types of tokens: text (prompt) tokens , noise tokens Xti, and instruction tokens . The noise token corresponds to the generation of the target image, while the text token carries the instruction information. The instruction token is specific to the editing task, representing the part of the image to be edited. Notably, the counts of noise and instruction tokens are roughly comparable and substantially higher than that of text tokens. Temporally, the text and instruction tokens serve as static control signals throughout the denoising process, whereas the noise token evolves dynamically at each timestep. Since InstructionDiT is designed to predict only the noise component, the models output corresponds exclusively to the portion represented by the noise token. To simplify the expression, the Instruction-DiT mentioned below will be referred to simply as DiT. 4 Preprint Version (Under Review) October 2025 Figure 3: Overview of the RegionE. RegionE consists of three stages: STS, RAGS, and SMS. In the STS, no acceleration is applied due to unstable DiT outputs, and all KV values are cached at the final step. In the RAGS, an Adaptive Region Partition distinguishes between edited and unedited regions: unedited regions are denoised in one step, while edited regions are generated iteratively. This iterative generation process leverages RIKVCache for injecting global information and AVDCache for acceleration. Certain forced-update steps aggregate the full image to refresh RIKVCache with complete DiT computation. Finally, in the SMS, several full denoising steps are performed to eliminate artifacts along the boundaries between edited and unedited regions."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "This section introduces RegionE, method that accelerates the IIE model without additional training. The workflow is shown in Figure 3. RegionE consists of three stages: the Stabilization Stage (STS), the Region-Aware Generation Stage (RAGS), and the Smooth Stage (SMS). Stabilization Stage. In the early steps of denoising, the input Xti to DiT is close to Gaussian noise (i.e., the signal-to-noise ratio is low). This leads to oscillations in DiTs velocity estimation (see Figure 2d and 2e). Since the estimates at this stage are inherently unstable, it is not suitable for acceleration. Therefore, we keep the original sampling process unchanged. Additionally, at the last step of this stage, we save the Key and Value in each attention layer of DiT, denoted as KC and C. Region-Aware Generation Stage. This stage is the core component of RegionE and consists of three parts: adaptive region partition, region-aware generation, and adaptive velocity decay cache. The first two parts primarily address spatial redundancy in IIE, while the third further reduces temporal redundancy across timesteps. Adaptive Region Partition. After the stabilization stage, the output of DiT becomes stable. As previously observed, the generation trajectories in the edited regions are curved, whereas those in the unedited regions are straight, as shown in Figure 1 and 2f. Therefore, for the unedited regions ti , we can accurately estimate ˆX tf at any timestep tf (f < i) using one-step estimation: = ti , ti) ti,f . vU (X ti ˆX tf When tf = 0, this corresponds to estimating the final unedited regions ˆX 0 , which is nearly identical 0 . However, using Equation 5 for the edited region does not accurately estimate ˆX to the true 0 . Based on this difference between the edited and unedited regions, we propose an adaptive region partition (ARP), as illustrated in the lower-left corner of Figure 3. Given the velocity vti+1 at the beginning of the region-aware generation stage and the noisy image Xti, the final edited result ˆX0 can be estimated in one step using Equation 5. This estimate is reliable in unedited regions but less accurate in edited ones. Since the unedited region undergoes minimal change before and after editing, (5) 5 Preprint Version (Under Review) October we can compute the cosine similarity between the estimated image ˆX0 and the instruction image along the token dimension. Regions with sufficiently high similarity (> threshold η), that is, small changes before and after editing, are considered unedited regions, while the remainder is treated as the edited region. To account for potential segmentation noise, morphological opening and closing operations are applied to make the two regions more continuous and accurate. Region-Aware Generation. After identifying the edited and unedited regions, we apply Equation 5 to the unedited region to directly estimate the denoised image tf at the next timestep tf in one step, thereby saving computation for the unedited region. For the edited region, our implementation is as follows: first, the input to DiT is changed from [X , Xti, ] to [X , ], so that DiT only ti estimates the velocity of the edited region vE ti . However, since DiT contains attention layers that involve global token interactions, completely discarding the and ti inputs can gradually inject bias into the estimation of vE ti during global attention. To compensate for this loss of information, we propose Region-Instruction KV Cache (RIKVCache). Specifically, the input to DiT remains [X , ti ], but within the attention layers of DiT, it is modified as follows: sof tmax( [QP , QE] [KP , KE, KC , KC ]T ) [VP , VE, , ]. (6) The lower corner labels , E, , and represent prompt token, edited region token, unedited region token, and instruction token, respectively. The superscript in the upper-right corner indicates that the value is taken from the cache of the previous complete computation. And the middle-lower part of Figure 3 visualizes this process. The feasibility of this approach is supported by the high similarity of the KV pairs between consecutive steps, as shown in Figure 2g and 2h. Adaptive Velocity Decay Cache. As illustrated in the right part of Figure 1, although the trajectory of the edited region is curved, the velocities between consecutive timesteps are actually similar. Focusing on the intermediate denoising phase, we observe from Figure 2e that the velocity directions between adjacent steps are almost identical (cosine similarity approaches 1). At the same time, the magnitudes exhibit gradual decay that varies across timesteps (Figure 2d). Based on this observation, we propose an adaptive velocity decay cache (AVDCache). Specifically, the AVDCache introduces decay factor: vti/vti+1 = (1 tti+1,ti) γti . (7) Here, (1 tti+1,ti) represents the sample-aware component under discrete Euler solver, while γti represents the timestep-aware component. The solver entirely determines the former, while the latter is obtained by fitting on randomly sampled dataset. Since the decay factor in Eq. 7 characterizes the intrinsic differences between diffusion model timesteps, we introduce the AVDCache criterion: Criterion = 1 (cid:89) (1 tti+1,ti) γti. i=s (8) Here,ts and te denote the start and end timesteps of the cache, respectively, while the criterion measures the cumulative error of this process. The decision of whether to apply the cache is made using threshold δ. The complete process is as follows: (cid:40) vE ti = , ) DiT (X ti (cid:81)i vE,C m=s(1 ttm+1,tm) γtm else. ts Criterion > δ (9) The right-lower part of Figure 3 visualizes this process. In fact, AVDCache is an improved version of the existing residual cache methods, with further details and analysis provided in the supplementary. After the above process, we obtain the generated results for both the edited and unedited regions. We then re-gather these results according to their spatial positions to reconstruct the complete image tokens. It is worth noting that the similarity of the KV Cache decreases as the timestep increases. To address this issue, we periodically enforce full-image gathering at certain timesteps within the region-aware generation stage, performing complete DiT computation to update the RIKVCache. Smooth Stage. Small gaps may appear at the boundaries between edited and unedited regions after stitching. Although these gaps are often imperceptible in most cases, to ensure the generality of our method, we perform several steps of unaccelerated denoising on the merged full image to smooth these discontinuities. Empirically, two denoising steps are sufficient to eliminate the gaps effectively. 6 Preprint Version (Under Review) October"
        },
        {
            "title": "5.1 EXPERIMENTAL SETTINGS",
            "content": "Pretrained Model & Dataset. We evaluate RegionE on three open-source state-of-the-art IIE models: Step1X-Edit-v1p1 (Liu et al., 2025d), FLUX.1 Kontext (Labs et al., 2025), and Qwen-Image-Edit (Wu et al., 2025). Step1X-Edit adopts CFG (classifier-free guidance) (Ho & Salimans, 2022) scale of 6, FLUX.1 Kontext uses scale of 2.5, and Qwen-Image-Edit applies scale of 4. All models are evaluated with 28 sampling steps. For evaluation, we follow the dataset protocols described in the respective technical reports. Specifically, we use 606 image prompt pairs covering 11 tasks from GEdit-Bench English (Liu et al., 2025d) for Step1X-Edit and Qwen-Image-Edit, and 1026 image prompt pairs spanning five tasks from KontextBench (Labs et al., 2025) for FLUX.1 Kontext. Evaluation Metrics. We design comprehensive evaluation framework to assess both the quality and efficiency of IIE models. For quality assessment, we adopt two complementary approaches. First, we evaluate reconstruction quality by measuring deviations before and after acceleration, using PSNR (Zhao et al., 2024), SSIM (Wang & Bovik, 2002), and LPIPS (Zhang et al., 2018) as metrics. Second, we conduct an editing evaluation using visionlanguage models (VLMs), specifically GPT4o, to assess image quality, semantic alignment, and overall performance (Ku et al., 2024), as shown in Table 1. Evaluation dimensions are denoted by the suffixes SC, PQ, and O, consistent with (Liu et al., 2025d) and (Wu et al., 2025). For efficiency evaluation, we report actual runtime latency as well as the relative speedup compared to the vanilla pretrained models. Baseline. Currently, there are no acceleration methods designed explicitly for IIEmodels. Therefore, we adapt several effective acceleration techniques initially developed for diffusion models as baselines, since they are also applicable to diffusion-based IIE tasks. From the perspective of timestep redundancy, Steoskip performs larger jumps in the sampling steps, FORA (Selvaraju et al., 2024) employs block-level cache, and -DiT (Chen et al., 2024) and TeaCache (Liu et al., 2025a) use residual cache. From the perspective of spatial redundancy, RAS (Liu et al., 2025e) and ToCa (Zou et al., 2024a) perform redundancy-reduction denoising at the token level. Implementation Details. For all three models, RegionE uses six steps in the stabilization stage, enforces an update at step 16 in the region-aware generation stage, and adopts two steps in the smooth stage. For Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit, the segmentation thresholds η of ARP are 0.88, 0.93, and 0.80, respectively, while the decision thresholds δ of AVDCache are 0.02, 0.04, and 0.03, respectively. Latency is measured on single NVIDIA H800 GPU, with each run editing one image at time."
        },
        {
            "title": "5.2 EXPERIMENTAL RESULTS ANALYSIS",
            "content": "We evaluate RegionE against several state-of-the-art acceleration methods on three prominent IIE models: Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. Our evaluation encompasses quantitative metrics, efficiency measurements, and visualization, demonstrating that RegionE achieves superior balance between acceleration and quality preservation. The quantitative results are shown in Table 1. Since both GEdit-Bench and KontextBench involve multiple editing tasks, the table reports results averaged over tasks, while the per-task quantitative results are provided in the supplementary. Deviation Analysis Compared to Pre-trained Models. The Against Vanilla evaluation reveals RegionEs exceptional fidelity to original model outputs across all evaluation metrics, significantly outperforming competing acceleration methods. RegionE achieves the highest PSNR values: 30.520 dB (Step1X-Edit), 32.133 dB (FLUX.1 Kontext), and 31.115 dB (Qwen-Image-Edit), representing substantial improvements of 2-4 dB over the next-best methods, indicating minimal pixel-level deviation from the original outputs. The SSIM scores of 0.939, 0.917, and 0.937 demonstrate superior preservation of structural coherence across different model architectures. In contrast, the LPIPS scores of 0.054, 0.057, and 0.046 represent 25-50% improvements over competing methods, indicating dramatically reduced perceptual differences that would be virtually indistinguishable to users. This consistent performance across three diverse model architectures validates RegionEs architectural agnosticism. RegionE consistently maintains stable, high-quality results. GPT-4o Editing Quality Assessment. The GPT-4o evaluation provides additional quality validation through automated semantic and perceptual analysis across three dimensions, consistently 7 Preprint Version (Under Review) October 2025 Table 1: Comparison of editing quality and efficiency between RegionE and the baseline. All the evaluations are carried out on single NVIDIA H800 GPU. denotes the strategy for reducing spatial redundancy, while denotes the strategy for reducing temporal redundancy. Model Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) FLUX.1 Kontext (Labs et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Type T & T T & T T & Against Vanilla GPT-4o Score Efficiency PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup - 26.719 22.126 24.659 28.262 26.819 24.699 30. - 26.199 24.685 20.227 28.307 26.217 23.906 32.133 - 28.439 26.508 25.020 28.314 27.251 OOM 31.115 - 0.898 0.835 0.874 0.924 0.892 0.844 0.939 - 0.838 0.809 0.723 0.869 0.829 0.767 0.917 - 0.892 0.863 0.821 0.900 0.879 OOM 0.937 - 0.096 0.178 0.122 0.072 0.100 0.152 0. - 0.123 0.146 0.225 0.097 0.132 0.192 0.057 - 0.077 0.098 0.116 0.075 0.090 OOM 0.046 7.479 7.491 6.078 7.432 7.455 7.339 7.185 7.552 7.197 7.126 7.085 7.055 7.233 7.216 6.985 7.278 8.242 8.090 8.032 7.964 8.084 8.152 OOM 8.242 7.466 7.343 7.588 7.233 7.361 7.072 6.705 7. 6.963 6.938 6.897 6.918 6.846 6.785 6.589 6.953 7.948 7.875 7.760 7.718 7.841 7.680 OOM 7.968 6.906 6.880 5.863 6.795 6.866 6.615 6.350 6.948 6.497 6.463 6.383 6.411 6.455 6.460 6.237 6.538 7.700 7.572 7.501 7.417 7.563 7.515 OOM 7.731 27.945 12.299 14.330 12.728 11.212 15.239 22.149 10. 14.682 8.512 7.497 6.751 6.203 8.219 11.299 6.096 32.125 17.555 17.815 17.470 16.445 22.327 OOM 15.604 1.000 2.272 1.950 2.196 2.493 1.834 1.262 2.572 1.000 1.725 1.958 2.175 2.367 1.786 1.299 2.409 1.000 1.830 1.803 1.839 1.954 1.439 OOM 2.059 demonstrating RegionEs superior performance. For semantic consistency (G-SC), RegionE achieves scores of 7.552, 7.278, and 8.242, matching or exceeding original models while maintaining substantial acceleration, with Qwen-Image-Edit showing perfect preservation (8.242) despite 2.059 speedup. The perceptual quality (G-PQ) scores of 7.405, 6.953, and 7.968 consistently outperform competing acceleration methods by 0.1 to 0.3 points, demonstrating the practical preservation of visual coherence through region-aware processing. Overall quality (G-O) scores of 6.948, 6.538, and 7.731 provide holistic assessment validation, with the alignment between GPT-4o assessments and quantitative metrics (PSNR, SSIM, LPIPS) strengthening confidence in RegionEs comprehensive quality preservation across multiple evaluation dimensions and providing additional evidence of the hybrid temporal-spatial optimization approachs effectiveness. Efficiency Analysis. RegionE demonstrates substantial efficiency gains while maintaining superior quality, achieving an optimal balance between acceleration and performance preservation with impressive results across all evaluated models. The method achieves speedups of 2.572, 2.409, and 2.059 across Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit respectively, translating to significant absolute latency reductions: from 27.945s to 10.865s, from 14.682s to 6.096s, and from 32.125s to 15.604s respectively. RegionE occupies the optimal position on the efficiency-quality curve, maintaining the highest quality metrics while achieving competitive or superior acceleration compared to methods that sacrifice substantial quality for higher speedups. Visualization. Figure 4 presents partial visualizations of different acceleration methods on Step1XEdit. Among the baselines, RegionE produces edited outputs closest to the vanilla setting at higher speedups, preserving both details and contours. The last column shows ARP predictions of spatial regions in RegionE, where unedited regions are masked. These masked regions closely match human perception. Additional visualizations for other tasks and models are provided in the supplementary. The experimental results conclusively demonstrate the effectiveness of RegionE in addressing both spatial and temporal redundancies in IIE models. RegionE achieves superior quality preservation metrics while maintaining competitive acceleration. The consistent improvements across diverse model architectures validate the generalizability of the underlying insights about regional editing patterns and temporal similarities in diffusion-based IIE processes. 8 Preprint Version (Under Review) October 2025 Table 2: Ablation study on cache design and stage design in RegionE."
        },
        {
            "title": "Stage Design",
            "content": "w/o RIKVCache w/o AVDCache w/o STS w/o SMS w/o Forced Step"
        },
        {
            "title": "Against Vanilla",
            "content": "GPT-4o Score"
        },
        {
            "title": "Efficiency",
            "content": "PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup 30.520 22.868 31.139 21.441 28.857 28. 0.939 0.822 0.946 0.814 0.904 0.915 0.054 0.207 0.046 0.161 0.085 0. 7.552 5.997 7.570 7.045 7.456 7.536 7.405 5.389 7.482 6.758 7.207 7. 6.948 5.191 7.023 6.325 6.773 6.925 10.865 10.223 16.122 7.149 9.766 10. 2.572 2.734 1.733 3.909 2.862 2.739 Figure 4: Examples of edited images by RegionE and baseline on Step1X-Edit-v1p1."
        },
        {
            "title": "5.3 ABLATION STUDY",
            "content": "We conduct ablation studies to investigate the contributions of different components in RegionE, primarily on the Step1X-Edit-v1p1. The quantitative results are summarized in Table 2. Cache Design. We propose two key components: RIKVCache and AVDCache. Removing RIKVCache, i.e., performing local attention within the edited region without injecting instruction information or context from the unedited region, results in 2.734 speed-up. However, this comes at significant cost to editing quality, with PSNR dropping from 30.520 to 22.868 and G-O decreasing from 6.948 to 5.191. This demonstrates that global context supervision is crucial even during region generation. In contrast, removing AVDCache results in slight improvement in editing quality (G-O increases from 6.948 to 7.023), but without eliminating redundancy across timesteps, the acceleration is limited to 1.733. This indicates that AVDCache significantly improves inference efficiency with minimal degradation in quality. Stage Design. We introduce two auxiliary stages: Stabilization Stage (STS) and Smooth Stage (SMS), as well as forced step in the region-aware generation stage (RAGS). Removing STS causes substantial drops in editing quality (PSNR: 30.520 21.441, LPIPS: 0.054 0.161, G-O: 6.948 6.325). As discussed in Section 4, STS addresses the instability in speed estimation, and skipping it results in degraded performance. Removing SMS leads to smaller declines in both pixel-level (PSNR: 30.520 28.857, SSIM: 0.939 0.904) and perceptual metrics (G-O: 6.948 6.773), reflecting its role in bridging the gap between edited and unedited regions. Finally, when the forced step in RAGS was removed, since its role was to mitigate the decay of KV similarity over time, its removal led to 2-point drop in PSNR, further validating its necessity. 9 Preprint Version (Under Review) October"
        },
        {
            "title": "6 CONCLUSION",
            "content": "Inspired by temporal and spatial redundancy in IIE, we propose RegionE, an adaptive, region-aware generation framework that accelerates the IIE process. Specifically, we perform early prediction on spatial regions using ARP and combine it with RIKVCache for region-wise editing to reduce spatial redundancy. We also use AVDCache to minimize temporal redundancy. Experiments show that RegionE achieves 2.57, 2.41, and 2.06 end-to-end speedups on Step1X-Edit and FLUX.1 Kontext, and Qwen-Image-Edit, respectively, while maintaining minimal bias (PSNR 30.5232.13) and negligible quality loss (GPT-4o evaluation results remain comparable). These results demonstrate the effectiveness of RegionE in reducing redundancy in IIE."
        },
        {
            "title": "REFERENCES",
            "content": "Jiazi Bu, Pengyang Ling, Yujie Zhou, Yibin Wang, Yuhang Zang, Tong Wu, Dahua Lin, and Jiaqi Wang. Dicache: Let diffusion model determine its own cache, 2025. Thibault Castells, Hyoung-Kyu Song, Bo-Kyeong Kim, and Shinkook Choi. Ld-pruner: Efficient pruning of latent diffusion models using task-agnostic insights. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 821830, 2024. Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. δ-dit: training-free acceleration method tailored for diffusion transformers. arXiv preprint arXiv:2406.01125, 2024. Pengtao Chen, Xianfang Zeng, Maosen Zhao, Peng Ye, Mingzhu Shen, Wei Cheng, Gang Yu, and Tao Chen. Sparse-vdit: Unleashing the power of sparse attention to accelerate video diffusion transformers, 2025. Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for authentic virtual try-on in the wild. In European Conference on Computer Vision, pp. 206235. Springer, 2024. Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models. In Advances in Neural Information Processing Systems, 2023. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 60076017, 2023. Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. Bk-sdm: Architecturally compressed stable diffusion for efficient text-to-image generation. In Workshop on Efficient Systems for Foundation Models@ ICML, 2023. Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation, 2024. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by low-rank components for 4-bit diffusion models. arXiv preprint arXiv:2411.05007, 2024a. Yaowei Li, Yuxuan Bian, Xuan Ju, Zhaoyang Zhang, Junhao Zhuang, Ying Shan, Yuexian Zou, and Qiang Xu. Brushedit: All-in-one image inpainting and editing. arXiv preprint arXiv:2412.10316, 2024b. 10 Preprint Version (Under Review) October 2025 Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 73537363, 2025a. Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, and Linfeng Zhang. From reusing to forecasting: Accelerating diffusion models with taylorseers, 2025b. URL https://arxiv. org/abs/2503.06923. Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Fei Ren, Shaobo Wang, Kaixin Li, and Linfeng Zhang. Speca: Accelerating diffusion transformers with speculative feature caching. In Proceedings of the 33rd ACM International Conference on Multimedia, pp. 1002410033. ACM, October 2025c. doi: 10.1145/3746027.3755331. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025d. Songhua Liu, Zhenxiong Tan, and Xinchao Wang. Clear: Conv-like linearization revs pre-trained diffusion transformers up. arXiv preprint arXiv:2412.16112, 2024. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, and Yuqing Yang. Region-adaptive sampling for diffusion transformers. arXiv preprint arXiv:2502.10389, 2025e. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. Zhaoyang Lyu, Xudong Xu, Ceyuan Yang, Dahua Lin, and Bo Dai. Accelerating diffusion models via early stop of the diffusion process. arXiv preprint arXiv:2205.12524, 2022. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1576215772, 2024. Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen Huang. Effective real image editing with accelerated iterative diffusion inversion, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234241. Springer, 2015. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pp. 87103. Springer, 2024. Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, and Luming Liang. Fora: Fast-forward caching in diffusion transformer acceleration. arXiv preprint arXiv:2407.01425, 2024. Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 19721981, 2023. 11 Preprint Version (Under Review) October Mingzhu Shen, Pengtao Chen, Peng Ye, Guoxuan Xia, Tao Chen, Christos-Savvas Bouganis, and Yiren Zhao. MD-dit: Step-aware mixture-of-depths for efficient diffusion transformers. In Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning, 2024. Wenhao Sun, Rong-Cheng Tu, Yifu Ding, Zhao Jin, Jingyi Liao, Shunyu Liu, and Dacheng Tao. Vorta: Efficient video diffusion via routing sparse attention, 2025. Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing, 2025. Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka. Instructedit: Improving automatic masks for diffusion-based image editing with user instructions. arXiv preprint arXiv:2305.18047, 2023. Zhou Wang and Alan Bovik. universal image quality index. IEEE signal processing letters, 9(3): 8184, 2002. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, Jianfei Chen, Ion Stoica, Kurt Keutzer, and Song Han. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity, 2025. Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, and Linfeng Zhang. Eedit: Rethinking the spatial and temporal redundancy for efficient image editing, 2025. Zhihang Yuan, Hanling Zhang, Pu Lu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 11961219. Curran Associates, Inc., 2024. Hanling Zhang, Rundong Su, Zhihang Yuan, Pengtao Chen, Mingzhu Shen, Yibo Fan, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattnv2: Head-wise attention compression for multi-modality diffusion transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1639916409, October 2025. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38363847, 2023b. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Maosen Zhao, Pengtao Chen, Chong Yu, Yan Wen, Xudong Tan, and Tao Chen. Pioneering 4-bit fp quantization for diffusion models: Mixup-sign quantization and timestep-aware fine-tuning, 2025. Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024. Xin Zhou, Dingkang Liang, Kaijin Chen, Tianrui Feng, Xiwu Chen, Hongkai Lin, Yikang Ding, Feiyang Tan, Hengshuang Zhao, and Xiang Bai. Less is enough: Training-free video diffusion acceleration via runtime-adaptive caching, 2025. Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang. Accelerating diffusion transformers with token-wise feature caching. arXiv preprint arXiv:2410.05317, 2024a. Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, and Linfeng Zhang. Accelerating diffusion transformers with dual feature caching. arXiv preprint arXiv:2412.18911, 2024b. 12 Preprint Version (Under Review) October 2025 RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"
        },
        {
            "title": "Supplementary Material",
            "content": "We organize the supplementary material as follows: Section A: Pseudocode of RegionE Section B: Analysis of Adaptive Velocity Decay Cache Section C: Per-Task Visualization Results in the Benchmark Section D: Per-Task Quantitative Results in the Benchmark Preprint Version (Under Review) October"
        },
        {
            "title": "A PSEUDOCODE OF REGIONE",
            "content": "Algorithm 1 RegionE: Adaptive Region-Aware Generation for Efficient Image Editing Input: Diffusion transformer Φ(), sampling step , insturction image , text tokens , random noise XT , total steps in stabilization stage tst, total steps in smooth stage tsm, threshold of adaptive region partition η, threshold of adaptive velocity decay cache δ, sorted forced steps list tf list. [0] = rue first dimension represents storing, second dimension represents retrieving end if vti, CKV = Φ([X , Xti, ], CKV , ) 1: // Initialization 2: RIKVCache CKV = None, RIKVCache flag = (F alse, alse); AVDCache CA=None; 3: Accumulative Error = 0; tf list.insert(0, tst); tf list.insert(1, tsm 1); 4: // Stabilization Stage 5: for to tst do if == tst then 6: 7: 8: 9: 10: Xti1 = Xti (ti ti1) vti 11: end for 12: // Region-Aware Generation Stage 13: Adaptive Region Partition 14: ˆX0 = XtT tst vT tst+1 tT tst 15: Eindex, Uindex = Erosion & Dilate(cos( ˆX0, ) > η) 16: Region-Aware Generation 17: for 0 to len(tf list) 2 do 18: 19: ˆX [0] = alse, [1] = rue iteritive denoising for edited region for prev to next + 1 do prev = tf list[i]; next = tf list[i + 1] = Xtprev [Eindex]; = Xtprev [Uindex] = vU tnext+1 tprev tprev tprev tprev+1 (tprev tnext+1) one-step estimation for unedited region 20: 21: 22: 23: 24: 25: 26: 27: Adaptive Velocity Decay Cache Calculate according to Eq.8 if > δ then , CKV = Φ([X , tj , ], CKV , ) vE tj CA = vE tj = E tj tj1 ) else tnext+1 , (tj tj1) vE tj vE tj end if end for = CAdecay factor according to Eq.7 28: 29: 30: 31: 32: 33: Xtnext+1 = gather(X tnext [0] = rue, [1] = alse 34: vtnext+1 , CKV = Φ([X , Xtnext+1, ], CKV , ) 35: 36: Xtnext = Xtnext+1 (tnext tnext+1) vtnext+1 37: end for 38: // Smooth Stage 39: [0] = alse, [1] = alse 40: for tsm 1 to 1 do vti, CKV = Φ(Xti, CKV , ) 41: 42: Xti1 = Xti (ti ti1) vti 43: end for Output: Target image after editing X0 14 Preprint Version (Under Review) October"
        },
        {
            "title": "B ANALYSIS OF ADAPTIVE VELOCITY DECAY CACHE",
            "content": "Figure 5: Pipeline Based on Residual Cache. In current research on diffusion model caching, many studies focus on residual-based caches (Chen et al., 2024; Liu et al., 2025a; Zhou et al., 2025; Bu et al., 2025), which store the shown in Figure 5. Based on the sampling formula in Equation 4 and the definition of caching, we can derive the following expression: It can be solved as: Xti1 = Xti (ti ti1) vti = vti Xti vti1 = Xti1 + . vti1 = [1 (ti ti1)] vti. Similarly, for the timestep ti2, we have: vti2 = [1 (ti1 ti2)] vti1 . Therefore, if we perform steps of residual caching, as illustrated in Figure 5, we can obtain: vtiN = = (cid:89) m=1 (cid:89) m=1 (cid:124) [1 (tim+1 tim)] vti [1 tim+1,im] vti. (cid:123)(cid:122) Determined by Solver (cid:125) (10) (11) (12) (13) This further indicates that the current residual cache and the velocity cache are equivalent. Since tim+1,im is minimal value approaching zero, the coefficient before vti is less than one. Therefore, it can be seen that the current residual cache is essentially decayed form of the velocity cache. Furthermore, we observe that the solver determines the decay coefficient in Equation 13. However, as shown in Figure 2d, the decay of velocity exhibits timestep-dependent behavior. To account for this, we introduce an external timestep correction coefficient γti. Notably, the AVDCache proposed in this paper reduces to Equation 13 when the correction coefficient γti equals 1. 15 Preprint Version (Under Review) October PER-TASK VISUALIZATION RESULTS IN THE BENCHMARK Due to space limitations, we put the visualization results of some tasks in the manuscript. Here, we provide visual comparison of additional tasks and models. Figure 6 and Figure 7 show the visualization results of 11 tasks on Step1X-Edit. Figure 9 and Figure 10 show the visualization results of 11 tasks on Qwen-Image-Edit. Figure 8 show the visualization results of 5 tasks on FLUX.1 Kontext. 16 Preprint Version (Under Review) October 2025 Figure 6: Examples of edited images by RegionE and baseline on Step1X-Edit-v1p1. Preprint Version (Under Review) October 2025 Figure 7: Examples of edited images by RegionE and baseline on Step1X-Edit-v1p1. 18 Preprint Version (Under Review) October 2025 Figure 8: Examples of edited images by RegionE and baseline on FLUX.1 Kontext. Preprint Version (Under Review) October 2025 Figure 9: Examples of edited images by RegionE and baseline on Qwen-Image-Edit. 20 Preprint Version (Under Review) October 2025 Figure 10: Examples of edited images by RegionE and baseline on Qwen-Image-Edit. Preprint Version (Under Review) October 2025 PER-TASK QUANTITATIVE RESULTS IN THE BENCHMARK In this section, we present the performance of RegionE and the baseline methods on each task in the benchmark. Table 3-Table 13 show the performance on the 11 tasks: motion-change, ps-human, color-alter, material-alter, subject-add, subject-remove, style-change, tone-transfer, subject-replace, text-change, and background-change. Table 14-Table18 show the performance on the five tasks: Character Reference, Style Reference, Text Editing, Instruction Editing-Global, and Instruction Editing-Local. Table 3: Comparison of RegionE and other baselines on the motion-change task of GEdit-Bench, evaluated in terms of quality and efficiency."
        },
        {
            "title": "Against Vanilla",
            "content": "GPT-4o Score"
        },
        {
            "title": "Efficiency",
            "content": "PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 25.887 20.935 24.549 26.926 25.888 24.428 29.633 - 27.791 26.744 25.756 26.776 26.585 OOM 29.416 - 0.902 0.818 0.876 0.925 0.889 0.843 0.937 - 0.905 0.889 0.848 0.911 0.882 OOM 0.932 - 0.093 0.189 0.121 0.068 0.109 0.165 0. - 0.066 0.079 0.095 0.070 0.096 OOM 0.057 4.350 4.350 2.175 4.350 4.475 4.025 3.775 4.625 4.850 4.725 4.825 4.675 5.025 5.000 OOM 4.825 7.950 8.100 7.575 7.975 8.050 7.375 6.975 7.775 8.550 8.625 8.325 8.575 8.500 8.625 OOM 8.550 4.444 4.562 2.385 4.445 4.524 4.012 3.578 4. 5.112 5.029 4.995 4.921 5.251 5.262 OOM 5.164 27.950 12.306 14.339 12.730 11.218 15.253 22.225 10.739 32.140 17.566 17.827 17.481 16.389 22.300 OOM 15.695 1.000 2.271 1.949 2.196 2.492 1.832 1.258 2.603 1.000 1.830 1.803 1.839 1.961 1.441 OOM 2.048 Table 4: Comparison of RegionE and other baselines on the ps-human task of GEdit-Bench, evaluated in terms of quality and efficiency. Model Against Vanilla GPT-4o Score Efficiency PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 29.220 23.596 26.348 31.428 29.077 26.716 32. - 32.080 30.120 28.323 32.347 29.857 OOM 33.550 - 0.916 0.863 0.884 0.942 0.921 0.878 0.957 - 0.936 0.920 0.887 0.948 0.917 OOM 0.963 - 0.069 0.142 0.099 0.047 0.072 0.125 0.037 - 0.040 0.049 0.062 0.038 0.061 OOM 0.029 4.614 4.600 3.414 4.800 5.114 4.400 4.786 4. 5.814 5.757 5.700 5.743 5.714 5.843 OOM 6.086 8.086 8.086 8.529 8.086 7.929 7.886 7.914 8.114 8.500 8.414 8.443 8.500 8.400 8.271 OOM 8.486 4.649 4.728 3.920 4.893 5.191 4.486 4.838 4.731 5.972 5.904 5.933 5.911 5.833 5.884 OOM 6.227 27.927 12.296 14.323 12.728 11.208 15.237 22.073 10. 32.100 17.553 17.816 17.462 16.360 22.340 OOM 15.473 1.000 2.271 1.950 2.194 2.492 1.833 1.265 2.583 1.000 1.829 1.802 1.838 1.962 1.437 OOM 2.075 22 Preprint Version (Under Review) October 2025 Table 5: Comparison of RegionE and other baselines on the color-alter task of GEdit-Bench, evaluated in terms of quality and efficiency."
        },
        {
            "title": "Against Vanilla",
            "content": "GPT-4o Score"
        },
        {
            "title": "Efficiency",
            "content": "PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 27.291 21.871 24.942 28.084 28.800 25.917 32.739 inf 29.795 28.035 25.892 30.757 29.132 OOM 33.144 - 0.919 0.838 0.901 0.938 0.909 0.864 0.956 1.000 0.896 0.879 0.835 0.922 0.909 OOM 0.951 - 0.080 0.132 0.107 0.050 0.069 0.118 0. 0.000 0.064 0.078 0.094 0.057 0.060 OOM 0.032 8.750 8.325 8.800 8.075 8.525 8.700 8.600 8.850 9.250 9.050 8.875 9.025 8.775 9.150 OOM 9.225 6.875 6.975 7.525 6.600 6.950 6.925 6.725 7.250 7.525 7.450 7.350 7.375 7.250 7.050 OOM 7.475 7.395 7.349 7.889 6.968 7.345 7.432 7.232 7. 8.170 8.084 7.872 8.021 7.840 7.860 OOM 8.172 28.019 12.330 14.356 12.770 11.242 15.274 21.996 11.188 32.082 17.527 17.795 17.479 16.566 22.356 OOM 15.527 1.000 2.273 1.952 2.194 2.492 1.834 1.274 2.504 1.000 1.830 1.803 1.835 1.937 1.435 OOM 2.066 Table 6: Comparison of RegionE and other baselines on the material-alter task of GEdit-Bench, evaluated in terms of quality and efficiency. Model Against Vanilla GPT-4o Score Efficiency PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 24.377 20.406 21.995 25.630 24.302 22.503 27. - 26.300 24.699 23.827 26.788 26.927 OOM 30.024 - 0.858 0.763 0.829 0.875 0.844 0.793 0.897 - 0.870 0.841 0.799 0.876 0.862 OOM 0.917 - 0.117 0.224 0.154 0.099 0.141 0.186 0.080 - 0.093 0.116 0.133 0.092 0.098 OOM 0.060 8.300 8.050 7.175 8.025 8.175 8.275 7.850 8. 8.725 8.650 8.525 8.425 8.725 8.400 OOM 8.550 6.575 5.900 6.875 5.975 6.000 5.700 5.450 6.200 7.150 6.875 6.675 6.475 6.775 6.625 OOM 6.900 7.226 6.676 6.579 6.695 6.796 6.633 6.352 6.997 7.629 7.557 7.389 7.205 7.564 7.192 OOM 7.415 27.880 12.260 14.286 12.685 11.163 15.202 22.306 11. 32.156 17.578 17.839 17.472 16.485 22.357 OOM 15.671 1.000 2.274 1.952 2.198 2.498 1.834 1.250 2.478 1.000 1.829 1.803 1.840 1.951 1.438 OOM 2.052 Table 7: Comparison of RegionE and other baselines on the subject-add task of GEdit-Bench, evaluated in terms of quality and efficiency."
        },
        {
            "title": "Against Vanilla",
            "content": "GPT-4o Score"
        },
        {
            "title": "Efficiency",
            "content": "PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 25.692 21.717 24.203 26.413 25.008 23.524 28.514 - 27.666 26.871 25.559 28.672 27.398 OOM 30.763 - 0.892 0.848 0.868 0.914 0.880 0.820 0.923 - 0.890 0.879 0.849 0.903 0.891 OOM 0.938 8.283 8.583 6.400 8.017 8.600 8.100 7.650 8. 9.117 8.767 9.017 8.617 8.783 9.100 OOM 8.983 7.950 8.083 8.083 8.050 8.067 7.517 6.950 7.950 8.017 7.950 7.933 7.817 7.933 7.933 OOM 8.233 7.905 8.142 6.131 7.655 8.177 7.532 6.939 7.858 8.381 8.146 8.313 7.967 8.099 8.267 OOM 8.441 27.912 12.290 14.322 12.727 11.204 15.232 22.062 10. 32.081 17.532 17.810 17.452 16.422 22.278 OOM 15.295 1.000 2.271 1.949 2.193 2.491 1.832 1.265 2.651 1.000 1.830 1.801 1.838 1.954 1.440 OOM 2.097 - 0.085 0.150 0.099 0.073 0.101 0.159 0.058 - 0.092 0.093 0.108 0.066 0.081 OOM 0.050 Preprint Version (Under Review) October 2025 Table 8: Comparison of RegionE and other baselines on the subject-remove task of GEdit-Bench, evaluated in terms of quality and efficiency."
        },
        {
            "title": "Against Vanilla",
            "content": "GPT-4o Score"
        },
        {
            "title": "Efficiency",
            "content": "PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 33.649 30.330 31.847 36.735 32.966 29.806 35.772 - 32.187 29.288 27.056 31.687 28.440 OOM 32.122 - 0.954 0.943 0.948 0.973 0.936 0.894 0.963 - 0.913 0.865 0.826 0.899 0.876 OOM 0.925 - 0.038 0.062 0.047 0.024 0.052 0.095 0. - 0.048 0.072 0.090 0.051 0.080 OOM 0.052 7.351 7.579 5.474 7.930 7.281 7.211 7.175 7.719 8.965 9.035 9.175 8.895 8.895 8.842 OOM 9.333 7.947 7.684 7.895 7.684 7.737 7.860 7.088 7.737 8.246 8.298 7.930 7.947 8.228 8.035 OOM 8.351 6.973 6.969 5.285 7.319 6.841 6.861 6.481 7. 8.477 8.558 8.475 8.348 8.441 8.371 OOM 8.787 27.954 12.300 14.330 12.724 11.213 15.236 22.378 10.453 32.170 17.572 17.820 17.486 16.434 22.331 OOM 15.349 1.000 2.273 1.951 2.197 2.493 1.835 1.249 2.674 1.000 1.831 1.805 1.840 1.958 1.441 OOM 2.096 Table 9: Comparison of RegionE and other baselines on the style-change task of GEdit-Bench, evaluated in terms of quality and efficiency. Model Against Vanilla GPT-4o Score Efficiency PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 21.064 15.851 18.893 21.695 21.355 19.819 25. - 23.954 21.784 20.552 23.137 24.073 OOM 27.980 - 0.828 0.680 0.791 0.857 0.814 0.760 0.900 - 0.805 0.745 0.662 0.816 0.772 OOM 0.897 - 0.185 0.372 0.233 0.156 0.193 0.250 0.102 - 0.139 0.185 0.219 0.152 0.169 OOM 0.073 8.150 8.183 7.183 8.167 8.000 8.217 8.283 8. 8.267 8.067 8.017 8.117 8.300 7.983 OOM 8.233 6.917 6.583 7.017 6.367 6.733 6.400 6.000 6.617 7.133 7.083 7.100 7.033 7.133 7.000 OOM 7.250 7.359 7.199 6.883 7.066 7.213 7.108 6.927 7.251 7.526 7.355 7.385 7.395 7.544 7.348 OOM 7.583 27.898 12.277 14.300 12.684 11.187 15.217 22.327 11. 32.115 17.560 17.807 17.455 16.414 22.275 OOM 16.822 1.000 2.272 1.951 2.200 2.494 1.833 1.250 2.365 1.000 1.829 1.804 1.840 1.957 1.442 OOM 1.909 Table 10: Comparison of RegionE and other baselines on the tone-transfer task of GEdit-Bench, evaluated in terms of quality and efficiency."
        },
        {
            "title": "Against Vanilla",
            "content": "GPT-4o Score"
        },
        {
            "title": "Efficiency",
            "content": "PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 24.744 19.078 22.104 27.915 26.455 23.954 30.860 - 29.715 27.514 25.471 30.064 29.142 OOM 34.051 - 0.899 0.786 0.862 0.933 0.895 0.840 0.945 - 0.862 0.839 0.792 0.910 0.880 OOM 0.948 6.950 7.200 6.900 7.000 6.825 7.200 6.500 6. 8.475 8.150 8.025 7.950 8.375 8.500 OOM 8.450 7.325 7.325 8.125 7.250 7.400 7.000 6.550 7.275 8.025 8.000 7.875 7.725 8.125 8.075 OOM 8.275 6.679 6.917 7.088 6.852 6.600 6.688 5.991 6.641 8.084 7.820 7.771 7.592 8.033 8.142 OOM 8.199 27.874 12.260 14.293 12.678 11.171 15.187 22.408 11. 32.160 17.562 17.841 17.462 16.381 22.372 OOM 15.851 1.000 2.274 1.950 2.199 2.495 1.835 1.244 2.425 1.000 1.831 1.803 1.842 1.963 1.437 OOM 2.029 - 0.122 0.251 0.164 0.072 0.111 0.159 0.064 - 0.092 0.117 0.139 0.061 0.089 OOM 0.034 Preprint Version (Under Review) October 2025 Table 11: Comparison of RegionE and other baselines on the subject-replace task of GEdit-Bench, evaluated in terms of quality and efficiency."
        },
        {
            "title": "Against Vanilla",
            "content": "GPT-4o Score"
        },
        {
            "title": "Efficiency",
            "content": "PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 25.233 20.594 22.927 25.856 25.072 23.407 28.654 inf 26.344 24.578 23.745 25.993 25.579 OOM 29.388 - 0.875 0.831 0.835 0.915 0.888 0.840 0.935 1.000 0.897 0.864 0.829 0.891 0.881 OOM 0.938 - 0.111 0.189 0.141 0.088 0.116 0.168 0. 0.000 0.076 0.104 0.120 0.084 0.095 OOM 0.047 8.650 8.683 5.833 8.500 8.417 8.250 8.267 8.517 8.883 8.783 8.600 8.450 8.700 8.867 OOM 8.967 7.233 6.867 6.817 6.733 7.183 6.433 6.217 7.167 7.683 7.733 7.550 7.267 7.733 7.400 OOM 7.767 7.718 7.548 5.306 7.345 7.536 6.996 6.909 7. 8.136 8.128 7.930 7.687 8.120 7.996 OOM 8.242 27.983 12.325 14.359 12.766 11.245 15.268 22.080 10.647 32.161 17.575 17.836 17.496 16.391 22.341 OOM 15.446 1.000 2.270 1.949 2.192 2.488 1.833 1.267 2.628 1.000 1.830 1.803 1.838 1.962 1.440 OOM 2.082 Table 12: Comparison of RegionE and other baselines on the text-change task of GEdit-Bench, evaluated in terms of quality and efficiency. Model Against Vanilla GPT-4o Score Efficiency PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 30.069 26.368 28.615 31.420 28.434 26.305 32. - 29.577 27.408 25.837 29.126 26.732 OOM 31.357 - 0.955 0.941 0.953 0.967 0.939 0.902 0.968 - 0.929 0.909 0.881 0.932 0.912 OOM 0.950 - 0.032 0.049 0.032 0.023 0.042 0.078 0.020 - 0.047 0.061 0.072 0.047 0.061 OOM 0.033 8.293 8.222 7.000 8.515 8.222 7.929 7.949 8. 9.192 8.828 8.818 8.879 8.778 8.889 OOM 8.838 8.091 8.192 7.899 8.222 8.192 7.970 7.707 8.242 8.394 8.222 8.303 8.333 8.222 8.010 OOM 8.313 7.900 7.951 6.926 8.171 7.925 7.649 7.609 8.002 8.606 8.202 8.192 8.259 8.184 8.187 OOM 8.260 28.027 12.331 14.375 12.768 11.254 15.270 21.723 10. 32.071 17.519 17.790 17.432 16.539 22.302 OOM 14.813 1.000 2.273 1.950 2.195 2.491 1.835 1.290 2.738 1.000 1.831 1.803 1.840 1.939 1.438 OOM 2.165 Table 13: Comparison of RegionE and other baselines on the background-change task of GEditBench, evaluated in terms of quality and efficiency."
        },
        {
            "title": "Against Vanilla",
            "content": "GPT-4o Score"
        },
        {
            "title": "Efficiency",
            "content": "PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup Step1X-Edit (Liu et al., 2025d) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) Qwen-Image-Edit (Wu et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 21.011 15.897 18.641 23.549 25.468 22.925 29.076 - 25.088 22.473 21.263 24.016 26.550 OOM 30.462 - 0.812 0.719 0.781 0.866 0.840 0.768 0.917 - 0.854 0.802 0.750 0.852 0.858 OOM 0.939 8.575 8.625 6.500 8.375 8.375 8.425 8.200 8. 9.125 9.175 8.775 8.825 8.850 9.100 OOM 9.175 7.175 6.975 7.125 6.625 6.725 6.725 6.175 7.125 8.200 7.975 7.875 7.850 7.950 7.450 OOM 8.050 7.722 7.635 6.104 7.338 7.379 7.372 6.995 7.675 8.603 8.511 8.252 8.281 8.281 8.153 OOM 8.547 27.886 12.267 14.285 12.687 11.163 15.206 22.631 11. 32.221 17.603 17.815 17.552 16.492 22.411 OOM 16.694 1.000 2.273 1.952 2.198 2.498 1.834 1.232 2.463 1.000 1.830 1.809 1.836 1.954 1.438 OOM 1.930 - 0.218 0.372 0.271 0.151 0.169 0.255 0.091 - 0.133 0.184 0.210 0.147 0.128 OOM 0.053 Preprint Version (Under Review) October 2025 Table 14: Comparison of RegionE and other baselines on the Character Reference task of KontextBench, evaluated in terms of quality and efficiency. Model Against Vanilla GPT-4o Score Efficiency PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup FLUX.1 Kontext (Labs et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 18.793 17.898 15.560 20.313 21.320 19.596 26. - 0.730 0.697 0.604 0.770 0.752 0.679 0.880 - 0.238 0.275 0.387 0.197 0.214 0.298 0.086 7.549 7.741 7.617 7.668 7.865 7.632 7.570 7.637 6.642 6.803 6.788 6.451 6.565 6.352 6.047 6.611 6.664 6.917 6.813 6.704 6.842 6.657 6.454 6.715 14.677 8.502 7.494 6.737 6.271 8.211 11.279 6. 1.000 1.726 1.958 2.178 2.341 1.788 1.301 2.291 Table 15: Comparison of RegionE and other baselines on the Instruction Editing-Global task of KontextBench, evaluated in terms of quality and efficiency. Model Against Vanilla GPT-4o Score Efficiency PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup FLUX.1 Kontext (Labs et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 23.957 22.611 18.687 27.206 24.845 23.030 30. - 0.797 0.760 0.659 0.842 0.778 0.711 0.886 - 0.132 0.159 0.252 0.101 0.157 0.218 0.071 7.023 7.000 7.092 7.073 7.294 7.302 7.179 7.126 6.798 6.870 6.840 6.882 6.885 6.866 6.588 6.943 6.380 6.435 6.497 6.574 6.626 6.668 6.483 6.572 14.688 8.516 7.506 6.754 6.251 8.221 11.412 6. 1.000 1.725 1.957 2.175 2.350 1.787 1.287 2.303 Table 16: Comparison of RegionE and other baselines on the Instruction Editing-Local task of KontextBench, evaluated in terms of quality and efficiency. Model Against Vanilla GPT-4o Score Efficiency PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup FLUX.1 Kontext (Labs et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 31.147 29.279 23.390 33.341 30.088 26.996 36. - 0.913 0.895 0.824 0.938 0.907 0.855 0.959 - 0.058 0.072 0.130 0.040 0.063 0.112 0.025 6.779 6.839 6.800 6.822 6.942 6.945 6.851 6.889 6.909 6.887 6.901 6.829 6.800 6.921 6.635 6.880 5.817 5.872 5.873 5.846 5.896 5.972 5.790 5.917 14.677 8.510 7.491 6.751 6.113 8.219 11.231 5. 1.000 1.725 1.959 2.174 2.401 1.786 1.307 2.531 Table 17: Comparison of RegionE and other baselines on the Style Reference task of KontextBench, evaluated in terms of quality and efficiency. Model Against Vanilla GPT-4o Score Efficiency PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup FLUX.1 Kontext (Labs et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 18.606 17.508 14.639 19.781 19.481 18.245 24. - 0.678 0.631 0.525 0.712 0.638 0.553 0.811 - 0.290 0.329 0.450 0.264 0.343 0.439 0.165 6.810 6.333 6.222 6.397 6.444 6.603 6.000 6.921 6.556 6.381 6.413 6.873 6.286 6.381 6.175 6.571 6.331 5.947 5.667 6.108 5.832 6.031 5.668 6.277 14.684 8.501 7.476 6.731 6.261 8.202 11.480 6. 1.000 1.727 1.964 2.182 2.345 1.790 1.279 2.291 Table 18: Comparison of RegionE and other baselines on the Text Editing task of KontextBench, evaluated in terms of quality and efficiency. Model Against Vanilla GPT-4o Score Efficiency PSNR SSIM LPIPS G-SC G-PQ G-O Latency (s) Speedup FLUX.1 Kontext (Labs et al., 2025) + Stepskip + FORA (Selvaraju et al., 2024) + -DiT (Chen et al., 2024) + TeaCache (Liu et al., 2025a) + RAS (Liu et al., 2025e) + ToCa (Zou et al., 2024a) + Ours (RegionE) - 30.950 28.976 23.931 31.283 27.504 25.342 34. - 0.943 0.915 0.839 0.955 0.913 0.857 0.962 7.826 7.717 7.696 7.315 7.620 7.598 7.326 7.815 7.913 7.750 7.543 7.554 7.696 7.402 7.500 7.761 7.295 7.142 7.067 6.823 7.076 6.971 6.791 7.211 14.697 8.535 7.520 6.779 6.290 8.238 11.206 5.767 1.000 1.722 1.954 2.168 2.336 1.784 1.312 2. - 0.033 0.044 0.085 0.026 0.048 0.093 0."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Imperial College London",
        "StepFun"
    ]
}