{
    "paper_title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation",
    "authors": [
        "Jiaming Zhang",
        "Shengming Cao",
        "Rui Li",
        "Xiaotong Zhao",
        "Yutao Cui",
        "Xinglin Hou",
        "Gangshan Wu",
        "Haolan Chen",
        "Yu Xu",
        "Limin Wang",
        "Kai Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods."
        },
        {
            "title": "Start",
            "content": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation Jiaming Zhang1,, Shengming Cao2, Rui Li2, Xiaotong Zhao2 Yutao Cui2 Xinglin Hou Gangshan Wu1 Haolan Chen2 Yu Xu2 Limin Wang1,3, Kai Ma2, 1State Key Laboratory for Novel Software Technology, Nanjing University 2Platform and Content Group (PCG), Tencent 3Shanghai AI Lab https://mcg-nju.github.io/steadydancer-web 5 2 0 2 4 2 ] . [ 1 0 2 3 9 1 . 1 1 5 2 : r Figure 1. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based human animation framework, with Motion-to-Image Alignment to achieve harmonized and coherent animation with first-frame preservation."
        },
        {
            "title": "Abstract",
            "content": "* Work is done during internship at Tencent PCG. Equal contribution. Corresponding author (lmwang@nju.edu.cn). Preserving first-frame identity while ensuring precise motion control is fundamental challenge in human image an1 imation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-toVideo (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ Staged DecoupledObjective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods. 1. Introduction Human image animation, which aims to generate video from single static image with controllable motion pose, has emerged as prominent research frontier in video generation and holds immense potential for applications like film production, advertising, and video game development. While significant progress has been made by prior works, the breakthroughs of diffusion models [16, 28] have recently unlocked new capabilities in generating high-fidelity and temporally coherent customized videos. Most existing methods adhere to the Reference-to-Video (R2V) paradigm [14], as illustrated in the left part of Fig. 2 (b). Specifically, the R2V paradigm defines the animation task as binding the reference image onto the driven pose, which inherently relaxes the alignment constraints between the inputs, thereby reducing the objective to only achieving natural appearance transfer. However, inherent discrepancies exist between the image and pose inputs in practical scenarios as shown in Fig. 2 (a), manifesting as spatial misalignments limb structure and proportion) and temporal misalignments (movement type and amplitude). In such scenarios, the relaxation of alignment constraints within the R2V paradigm leads to unacceptable results, including severe artifacts, appearance distortions, and temporal incoherence. Moreover, in the common start-gap scenario due to the temporal misalignments, the R2V paradigm instantly binds the reference image to the first pose, completely omitting any smooth transition from the reference state. This abrupt jump, combined with the appearance distortion, makes it unsuitable for applications that require high visual and temporal fidelity, such as keyframe-based Figure 2. (a) The spatio-temporal misalignment in practical scenarios. (b) The illustration of Reference-to-Video (R2V) and Image-to-Video (I2V) paradigms for human image animation. While R2V only cares about how to bind the reference image to driven motion, I2V additionally needs to carefully deal with the misalignment from the driven motion to the reference image. or static photo animation, and Post-Production & VFX. In contrast, the Image-to-Video (I2V) paradigm, which inherently guarantees first-frame preservation by generating consistent and coherent videos starting from the initial frame, maximizes the fidelity and emerges as preferable solution. However, I2V-based animation research remains scarce and technically challenging, due to its stringent requirement for first-frame coherence, which demands that all input conditions and generated results strictly adhere to the initial frame. Specifically, it requires the pose to be modulated into suitable control signal for the reference image, necessitating much tighter alignment than R2V. Failure to achieve such strict alignment will severely impact performance, especially when there is insufficient resources to train high-capacity models for learning motion control. In this paper, we propose SteadyDancer, an animation model built on the I2V paradigm with first-frame preservation. Firstly, to resolve the trade-off between appearance preservation and motion control within the I2V paradigm, we propose Condition-Reconciliation Mechanism that achieves precise motion-driven control without sacrificing first-frame fidelity by optimizing at three levels, including condition fusion, injection, and augmentation. Secondly, to address the spatio-temporal misalignment between the reference image and the driving pose, 2 Finally, we introduce Synergistic Pose Modulation Modules to extract pose representations that are better adapted to the reference image, comprising the Spatial Structure Adaptive Refiner, the Temporal Motion Coherence Module, and to the Frame-wise Attention Alignment Unit. achieve efficient and stable model training, we propose Staged Decoupled-Objective Training Pipeline, including an Action Supervision Stage to establish precise motion control, Condition-Decoupled Distillation Stage to enhance the generated video quality, and Motion Discontinuity Mitigation Stage that aims to generate smooth and continuous results.Ultimately, building on these strategies, SteadyDancer successfully activates the ability of firstframe preservation in the human animation task, enabling it to robustly handle misalignments and generate videos with high fidelity and accurate motion. Moreover, SteadyDancer achieves superior performance over existing methods while requiring substantially fewer training resources. In summary, the main contribution of this work lies in: (i) We propose SteadyDancer, novel high-fidelity animation framework that achieves first-frame preservation firstly and significant training resource efficiency; (ii) To address the conflict and mismatch between the reference image and the driven pose, and to improve training efficiency, we propose Condition-Reconciliation Mechanism, Synergistic Pose Modulation Modules, and Staged DecoupledObjective Training Pipeline; (iii) Extensive quantitative and qualitative results on multiple benchmarks validate the superiority and effectiveness of our proposed method. 2. Related Work Diffusion for video generation Diffusion-based models have become state-of-the-art for generative tasks, achieving remarkable success in both image [20, 37] and video generation [4, 36]. With the introduction of OpenAIs Sora [2], which leverages the Diffusion Transformer (DiT) architecture [19], DiT-based approaches have since supplanted UNets [21], as their pure Transformer structure facilitates massive parameter scaling and has become the mainstream technical route. Concurrently, to efficiently handle video data, many recent DiT models [16, 28, 35] adopt 3D VAEs over standard 2D VAEs [15, 20] to compress data across both spatial and temporal dimensions. The proliferation of powerful, open-source foundational models has further accelerated this field. Consequently, human image animation, as one of downstream tasks, directly benefits from the enhanced power and fidelity of these new models. Human Image Animation. Earlier works in image animation primarily relied on warping-based feature representations and GAN-based architectures [22, 23, 42]. Recently, this field has pivoted to diffusion models, yielding significant performance improvements. Early diffusionbased methods, such as DisCo [29], leveraged ControlNet [37] for pose guidance and integrated motion modules to enhance cross-frame consistency. key breakthrough came with Animate Anyone [9] and subsequent studies [26, 34, 45], which utilize UNet-based ReferenceNet to inject appearance features, achieving excellent identity preservation. To further enhance controllability, other works [10, 32, 41, 43, 45] incorporated camera parameters and rich 3D geometric guidance, such as depth, SMPL, hand. Mirroring the trend in general video generation, DiTbased architectures [19] have recently been adapted for human animation [5, 14, 17, 30, 33, 40, 44], leading to substantial enhancements in realism and temporal continuity. However, most approaches follow the Reference-to-Video (R2V) paradigm. This paradigm focuses on binding Imageto-Pose naturally, which inherently ignores critical input misalignments, frequently leading to unsatisfactory results. 3. Method Given reference image Ic along with pose sequence Pm = {p0, . . . , pT }, the animation task aims to generate realistic video that preserves the appearance of the reference character while maintaining strict adherence to the pose sequence. We first review the foundational Image-to-Video (I2V) generation model (Sec. 3.1) and introduce Naıve I2V Baseline to highlight its limitations (Sec. 3.2). We then present the core technical components of our model, including the Condition-Reconciliation Mechanism (Sec. 3.3) and the Synergistic Pose Modulation Modules (Sec. 3.4). Finally, we describe the Staged Decoupled-Objective Training Pipeline (Sec. 3.5). 3.1. Preliminaries foundational Image-to-Video (I2V) model [1, 28, 35, 39] controls synthesis by conditioning on static image Ic, which provides strong appearance priors as the first frame, and is then temporally concatenated with zero-filled frames and encoded by VAE encoder [15] into the condition latent zc. At given denoising timestep t, the input ˆzt to the Diffusion Transformer (DiT) [19] is formed by channelconcatenating the current noisy latent ˆzt, binary mask m, and the condition latent zc as: zt = ChannelConcat(ˆzt, m, zc). (1) This combined latent zt is then processed by the DiT to predict the denoised latent. Additionally, the model injects global context cclip and textual conditions ctxt into the decoupled cross-attention layers of DiT to enhance alignment between spatial details and semantic guidance. 3.2. Naıve I2V Baseline To introduce the new control condition, pose sequence Pm, we first propose Naıve Baseline that treats the pose condition and image condition equivalently. Specifically, the 3 Figure 3. An overview of SteadyDancer, Human Image Animation framework based on the Image-to-Video (I2V) paradigm. First, it employs Condition-Reconciliation Mechanism to reconcile appearance and motion conditions, achieving precise control without sacrificing first-frame preservation. Second, it utilizes Synergistic Pose Modulation Modules to resolve critical spatio-temporal misalignments. pose sequence first reuses the VAE encoder, which is used by the image, to encode it into pose condition latent zp, ensuring both condition latents reside in the same feature space. These two latents are then fused via simple elementwise addition for further denoising: zt = ChannelConcat(ˆzt, m, zc + zp). (2) This baseline attempts to achieve both appearance preservation and motion control by simple element-wise addition. 3.3. Condition-Reconciliation Mechanism The Naıve Baseline attempts to preserve both appearance and motion by leveraging simple additive fusion of the conditional features. However, this approach ignores fundamental misalignment: the fine-grained appearance from the static image often conflicts structurally and semantically with the motion dynamics of the driving pose, which is particularly acute for the I2V paradigm. As result, this simple fusion often biases the model towards motion control, severely compromising its ability to retain appearance details. To resolve this trade-off, we propose the Condition-Reconciliation Mechanism, three-aspect approach designed to achieve precise motion control without sacrificing first-frame preservation. Condition Fusion. We identify that the element-wise addition is critical bottleneck of Naıve Baseline, which conflates the static appearance (zc) and dynamic pose (zp) signals, leading to information loss and mutual interference. To resolve this, we replace it with channel-wise concatenation, which preserves the distinctness of each condition: zt = ChannelConcat(ˆzt, m, zc, zp). (3) This separation allows the model to learn more effective representations for each signal, resulting in significantly improved appearance preservation and motion control. Condition Injection. common parameter-intensive injection approach, such as using adapters or decoupled crossattention, significantly increases trainable parameters and attention complexity, risking interference with the pretrained models generation capacity and leading to performance degradation. To mitigate this, we adopt parameterefficient strategy. First, the pose latent zp is injected along with the image condition without additional modules. Second, we utilize LoRA-based fine-tuning. This combination efficiently enhances motion control while preserving generation capacity, such as first-frame preservation. Condition Augmentation. To further reinforce the preservation of the first frame, we introduce two augmentation strategies. First, we augment the DiT input at the temporal level. We take the channel-concatenated latent zcond from the Condition Fusion and temporally concatenate it with the image latent zc0 and the first-frame pose latent zp0 as: zcond = ChannelConcat(ˆzt, m, zc, zp), zt = TemporalConcat(zfused, zc0 , zp0 ). (4) This provides the model with an explicit, clean reference to the starting appearance and pose. Second, we enhance the global context cclip by concatenating it with the CLIP feature of the first pose frame. This provides the model with richer, pose-aware semantic embedding. These two augmentations work synergistically to improve identity preservation and visual consistency. 3.4. Synergistic Pose Modulation Modules While our Condition-Reconciliation Mechanism improves the fidelity-control balance, as shown in Fig. 2, critical challenges in spatio-temporal misalignment persist. 1) Spatial Misalignment: Fundamental disparities in static attributes between the source image and the driving pose, such as 4 skeletal proportions and identity features, can force structural alterations to the character, leading to identity drift and detail degradation. 2) Temporal Misalignment: This challenge manifests in two forms. First, complex or noisy extracted pose sequences often cause temporal jitter and artifacts. More critically, motion discontinuity occurs, an unsmooth, abrupt transition from the source image to the initial driving pose that severely undermines visual realism. Therefore, to resolve these conflicts, we designed the following modulation modules to achieve precise alignment. Spatial Structure Adaptive Extractor. To address spatial misalignment, we propose the Spatial Structure Adaptive It employs dynamic convolution, which Refiner PSSAR. adaptively generates kernel from the input pose features zp to refine the pose representation. This process yields more expressive representation that is highly compatible with the image feature space, thereby mitigating interference during fusion and safeguarding fine-grained details. Temporal Motion Coherence Module. As for temporal misalignment, we introduce the Temporal Motion Coherence Module PT CM to model continuous motion dynamics from the discrete pose zp. It utilizes stacked factorized convolutional blocks, including depthwise spatial convolutions to capture intra-frame structures, and pointwise temporal convolutions to model inter-frame dynamics while smoothing the sequence. This module efficiently suppresses artifacts from erratic poses, yielding smooth, coherent motion representation for consistent guidance. Frame-wise Attention Alignment Unit. To enforce finegrained, frame-wise correspondence between pose and appearance, we introduce the lightweight Frame-wise Attention Alignment Unit PF AAU via cross-attention mechanism, where the denoising latent ˆzt (Query) attends to the pose latent (Key / Value). This process yields an appearancecalibrated pose representation, enhancing the structure-toappearance alignment for the subsequent fusion. Hierarchical Aggregation. Ultimately, we combine the aforementioned three modules using hierarchical aggregation strategy. First, the base pose feature is processed in parallel by the spatial (PSSAE) and temporal (PT CM ) modules. Their outputs are then integrated with the base feature via residual connection to construct high-quality, spatio-temporally coherent representation. This intermediate representation is immediately calibrated by the Alignment Unit (PF AAU ). This synergistically refined pose feature achieves precise appearance alignment, which then serves as an additional, high-quality control condition for Eq. 4. The aggregation process can be formalized as: zp = zp + PSP AE(zp) + PT SM (zp), zp = PF AAU (q = ˆzt, kv = zp ), zcond = ChannelConcat(ˆzt, m, zc, zp , zp). (5) Figure 4. Pose Simulation in Motion Discontinuity Mitigation of Staged Decoupled-Objective Training Pipeline. 3.5. Staged Decoupled-Objective Training Pipeline Our training pipeline is divided into three distinct stages to achieve efficient and precise training. Action Supervision. The primary objective of this stage is to rapidly instill motion control capabilities. For each video training sample, we fix the first frame as the reference frame, while the entire video serves as the source of the motion condition and the target of supervision. Concurrently, we exclusively employ LoRA-based [8] fine-tuning to protect the rich generative priors of the pre-trained model from being drastically altered by motion control. Condition-Decoupled Distillation. To compensate for the loss in generation quality incurred while learning motion control in the first stage, we employ Condition-Decoupled Distillation in the second stage to enhance realistic detail and quality while maintaining the pose controllability obtained in the first stage. Specifically, we use the original pretrained I2V model as the teacher model θ, which serves as stationary reference manifold that parameterises the unconditional data distribution. We designate the model trained in the first stage as the student model ϕ, which acts as conditional flow estimator that must simultaneously guarantee semantic fidelity and high perceptual quality. We decompose its velocity prediction into an unconditional component and conditional component as: vϕ(xt, t, c) = vϕ (cid:124) (xt, t) (cid:125) (cid:123)(cid:122) unconditional component + vϕ (xt, t, c) (cid:123)(cid:122) (cid:125) conditional component (cid:124) , (6) (xt, t) vθ where the supervision objective of the unconditional component is Ldistill = vϕ (xt, t)2 that aligns the unconditional component with the frozen teacher, and the supervision objective of the conditional component is Lfidelity = vϕ(xt, t, c) v2 that regresses the groundtruth velocity field like the first stage. Consequently, the unconditional manifold encoded by the teacher is injected into the student without biasing the pose-specific conditions, eliminating the distribution shift observed when conditional network is forced to mimic an unconditional target, thereby improving the video quality of the student. 5 Table 1. Quantitative Results on the TikTok [13] and RealisDance-Val [44]. We evaluate on TikTok using low-level metrics, whereas for RealisDance-Val, we employ Vbench-I2V, high-level, multi-dimensional benchmark. Our SteadyDancer achieves highly competitive results across all metrics, notably outperforming others in more representative and practical FVD and most VBench metrics. Method SSIM PSNR LPIPS FID FVD 0.752 Animate Anyone [9] 0.748 MagicAnimate [34] 0.778 Champ [45] 0.740 Animate-X [25] HumanVid [32] 0.778 Realisdance-DiT [44] 0.717 0.749 SteadyDancer (Ours) 16.79 17.89 18.43 16.71 18.76 17.55 17.67 0.288 0.270 0.267 0.280 0.247 0.260 0.263 52.26 935.6 56.84 876.0 50.76 736.1 32.77 508.6 41.35 691.8 30.39 458.8 30.65 451.3 (a) TikTok dataset. Model I2V Subject I2V Background Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Aesthetic Quality Imaging Quality Moore-AnimateAnyone [9] HumanVid [32] MimicMotion [41] Animate-X [25] Hyper-Motion [33] UniAnimate-DiT [30] VACE [14] Wan-Animate [5] SteadyDancer (Ours) 94.00 94.72 90.78 95.68 94.76 93.15 87.39 93.81 94.68 94.69 95.27 92.52 96.22 95.71 93.95 88.58 94.61 95.38 94.65 93.69 92.21 93.39 93.58 94.56 93.56 93.06 93. 94.90 94.94 93.60 95.11 94.97 95.44 95.03 94.52 95.18 97.16 97.87 97.46 97.79 98.19 98.78 96.74 98.42 97.99 98.07 98.52 98.61 98.68 99.01 99.24 98.25 98.96 99.02 51.56 55.58 52.09 51.72 52.97 52.18 57.81 54.47 56.80 66.34 67.45 59.67 60.91 65.52 65.52 70.61 66.87 68.45 FVD 748.38 624.33 785.73 679.66 568.14 599.03 911.72 386.87 326.49 (b) RealisDance-Val dataset. Motion Discontinuity Mitigation. At test time, significant pose discrepancy often exists between the static reference image and the initial frame of the driving pose sequence, manifesting as an abrupt, instantaneous jump artifact without smooth transition. This vulnerability arises because the reference image and the first pose frame in the training data are idealistically identical (v0), meaning the model is never exposed to the critical start-gap misalignment. However, naively injecting drastic, random jumps can destabilize the training process. We therefore propose Pose Simulation, designed to introduce plausible pose mismatches during training. Specifically, for given smooth sequence {p0, p1, . . . , pT }, we sample synthetic pairs (p0, pT ), where is randomly sampled from {2, 3, 4} to enhance diversity. We then interpolate the pose pair to create new sequence {p1, . . . , pT 1}, and replace the original p1 with this synthetic p1, creating pseudo training sample {p0, p1, . . . , pT }. Notably, this strategy requires only few hundred fine-tuning steps and no architectural modifications, while still effectively resolving over 80% of discontinuity without more inference overhead. 4. Experiments 4.1. Experimental Setups Implementation Details. We employ pre-trained Wan2.1 I2V 14B video model [28] for initialization, and all experiments are conducted on 8 NVIDIA H800s. The video sample consists of 81 frames, which are primarily at center resolution of 640 during training, but are extrapolated to 768 during inference. The three stages of our training pipeline consist of 12,000, 2,000, and 500 steps, respectively, totaling only 14,500 steps. As shown in Table 2, our model requires significantly fewer training steps compared to other DiT-based methods. This highlights the core efficiency of our design, which delivers excellent motion control, high-quality, and smooth video without requiring massive data or computational budgets. To further illustrate the efficiency of training, we present the results of various steps in the first stage. As shown in Fig. 5, it rapidly acTable 2. Comparison of Extra Inputs, Pre-Trained Model, and Training Requirements for various methods. Method Extra Inputs Pre-Trained Model Training Step Training Data UNet-based Disco [CVPR24] [29] Animate Anyone [CVPR24] [9] MagicAnimate [CVPR24] [34] Champ [ECCV24] [45] HumanVid [NeurIPS24] [32] RealisDance [arxiv24] [43] StableAnimator [CVPR25] [26] X-Dyna [CVPR25] [3] MIMO [CVPR25] [18] Animate-X [ICLR25] [25] MimicMotion [ICML25] [41] Mask DensePose Depth, SMPL Camera SMPL, HaMeR Face Face Depth, Mask Stable Diffusion Stable Diffusion Stable Diffusion Stable Diffusion Stable Diffusion Real Vision 70k [30k, 10k] - [60k, 20k] [30k, 10k] [200k, 100k] TikTok (350) 5k character video clips TikTok (350), TED-talks (1,203) 5k human videos 20k real, 50k synthetic about 16k videos Stable Video Diffusion 20 ep (15k) 3K videos (60-90 seconds long) Stable Diffusion Stable Diffusion Stable Diffusion [5ep, 2ep] 50K - Stable Video Diffusion 20 ep (11k) 107,546, 30-second videos 5k real, 2k synthetic videos 9k human videos 4,436 human dancing videos DiT-based Dreamactor-M1 [ICCV25] [17] VACE [ICCV25] [14] FlexiAct [SIGGRAPH25] [40] X-UniMotion [SIGGRAPH ASIA25] [24] UniAnimate-DiT [arxiv25] [30] RealisDance-DiT [arxiv25] [44] HyperMotion [arxiv25] [33] Wan-Animate [arxiv25] [5] SteadyDancer (Ours) Face Mask SMPL Face Seaweed APTs Wan-2.1 T2V 14B CogVideoX-I2V Seaweed-7b Wan-2.1 I2V 14B Wan-2.1 I2V 14B Wan-2.1 I2V 14B Wan-2.1 I2V 14B Wan-2.1 I2V 14B [20k, 20k, 30k] 200k [40K, 1.5K] 40k - - 20k - [12k, 2k, 0.5k] 500-hour videos 1M - 200-hour videos 10K human dance videos 1M high-quality videos 19,597 video clips - 7338 videos (10.2 hours) Figure 5. Model performance across various training steps. The results indicate that our model rapidly acquires motion-control in the early steps, while the later steps focus more on detail. quires motion control in the early steps and subsequently focuses more on enhancing details. Training Dataset. We collected proprietary human motion dataset for training, consisting of 7,338 five-second video clips, totaling 10.2 hours. This high-quality dataset consists primarily of human dance sequences, supplemented by small number of slow-motion, documentarystyle shots, and deliberately excludes extreme or complex movements. Notably, as shown in Table 2, the dataset scale is significantly smaller than that of comparable works, highlighting the efficiency of our design. 6 As shown in Fig. 7 and Fig. 8, we compare our model against others on X-Dance. When confronted with these challenges, competing methods exhibit catastrophic dual failure. They neither preserve the reference characters In identity nor correctly adhere to the driving motion. contrast, our model generates harmonized and coherent results that near perfectly maintain first-frame identity while also ensuring precise motion control. We also visualize the results on the RealisDance-Val dataset. Beyond collecting standard daily and dance motions to verify basic motion control, this dataset uniquely incorporates numerous scenarios featuring complex Human-Object Interactions (HOI). In these sequences, the subject actively interacts with external objects, creating dual challenge that tests both the models pose-following precision and its interaction potential. As shown in Fig. 9 and Fig. 10, our model demonstrates exceptional performance in these scenarios. Remarkably, even when driven solely by human pose signals, our model successfully synthesizes the interacting objects with physically plausible motion and deformation, while maintaining high appearance fidelity. In contrast, while competing models achieve comparable performance in controlling the human subject, they fail to generate reasonable interactions with the objects, often resulting in static artifacts or severe shape collapse. This comparison highlights the superior interaction potential of our proposed method. 4.3. Ablation Study To assess the contributions of each component, we conducted series of comprehensive ablation studies. Condition-Reconciliation Mechanism. Our objective is to achieve precise motion control with minimal training data and resources, while preserving the rich I2V generative priors of the pre-trained model. Therefore, and in contrast to the R2V paradigm, how the conditions are integrated into the model is paramount to our success. Firstly, as shown in Fig. 6, our channel concatenation strategy outperforms both element-wise addition (Row 1) and adapter-based injection (Row 2) in preserving identity. Secondly, removing the Condition Augmentation strategy (Row 3) confirms it is crucial for maintaining appearance fidelity. Synergistic Pose Modulation Modules. Achieving robust Motion-to-Image Alignment in the I2V paradigm remains pressing issue, primarily due to inherent pose inaccuracies and spatio-temporal misalignments. We validated the distinct roles of all three modules in Fig. 11. For spatial issues, such as pose errors (Row 1) and large structural disparities (Row 2), our SPAE and FAAU modules respectively extract multi-scale and more reference-adaptive representations. Meanwhile, for temporal issues, such as missing or contradictory poses (Row 3), our TMSM module models temporal features to ensure smooth and consistent guidance. Figure 6. Ablation of Condition-Reconciliation Mechanism. Evaluation Metrics. For the TikTok [13] dataset, we follow the settings in HumanVid [32]. The metrics consist of image quality (SSIM [31], LPIPS [38], PSNR [7], and FID [6]) and video fidelity (FVD [27]). For the RealisDance-Val [44] dataset, we utilize Vbench-I2V [11, 12] to facilitate fine-grained and objective evaluation. 4.2. Comparison with the State-of-the-Art methods Quantitative results. As shown in Table 1, we conduct quantitative comparisons on TikTok with low-level metrics, and RealisDance-Val with the comprehensive, multi-dimensional Vbench-I2V metrics. Our SteadyDancer achieves highly competitive results across all metrics, particularly excelling on the more representative and practical FVD and VBench-I2V metrics. We wish to point out difference in evaluation on these same-source benchmarks, where the reference frame and pose sequence originate from the same video. Our SteadyDancer uses the first frame as reference frame, while for other methods, we follow their original settings to use the middle frame, which statistically implies shorter average spatio-temporal distance to the sequence start and end. Comparatively, our method must bridge the full spatio-temporal generation. This difference may therefore cause slight metric variations, which deviate from real-world scenarios. Qualitative comparisons. To fill the void left by existing same-source benchmarks, which fail to evaluate spatiotemporal misalignments, we propose X-Dance, new different-source benchmark that focuses on these challenges. The X-Dance benchmark is constructed from diverse image categories (male/female/cartoon, and upper- /full-body shots) and challenging driving videos (complex motions with blur and occlusion). Its curated set of pairings intentionally introduces both spatial-structural inconsistencies and temporal start-gaps, allowing for more robust evaluation of model generalization in the real world. 7 Figure 7. Qualitative comparisons between SteadyDancer and other methods on the X-Dance benchmark. Figure 8. Qualitative comparisons between SteadyDancer and other methods on the X-Dance benchmark. Each example displays the evolution (starting from the first frame), highlighting our models superior high-fidelity, coherence, and first-frame preservation. 8 Figure 9. Visualization on RealisDance-Val. Even when driven solely by human pose signals, our model successfully synthesizes the interacting objects with physically plausible motion and deformation Figure 10. Comparison on RealisDance-Val. Compared to other models, our model not only achieves precise control but also enables reasonable interaction with objects, causing them to produce reasonable movements. 9 Figure 12. Ablation study of Condition-Decoupled Distillation. Red lines are incorrectly artifacts in Normal Distillation. Figure 13. Ablation study of Motion Discontinuity Mitigation. phenomenon we refer to as the start-gap misalignment, we proposed Pose Simulation in Sec. 3.5 to systematically acquaint the model to these discontinuities. As illustrated in Figure 13, this case presents the misalignment, where significant pose discrepancy exists between the reference image and the initial driving frame. This discrepancy induces an abrupt visual artifact in the baseline result, which exhibits clear motion discontinuity between the second and third frames. In contrast, the model fine-tuned with our Pose Imitation strategy generates smooth and gradual transition. The subjects hand-raising motion is synthesized as progressive, plausible transition over 10 frames, rather than an abrupt shift. This case clearly validates the efficacy of our approach in mitigating such motion discontinuities. 5. Conclusion We present SteadyDancer, framework for harmonized, coherent human animation that leverages first-frame preservation. It resolves the core I2V challenge of harmonizing fidelity with motion control and ensuring coherence via our novel Condition-Reconciliation Mechanism and Synergistic Pose Modulation Modules. Our Staged DecoupledObjective Training pipeline efficiently optimizes for motion, quality, and continuity with minimal resources. Quantitative and qualitative results, especially on our X-Dance benchmark, show SteadyDancer significantly outperforms competitors. We believe these innovations provide solid, efficient method for future robust human animation. Figure 11. Ablation of Synergistic Pose Modulation Modules. Condition-Decoupled Distillation. To compensate for the degradation in generation quality incurred while learning motion control in the first stage, we have introduced the Condition-Decoupled Distillation stage in Sec. 3.5. To validate the effectiveness of its decoupled design, we compare it with the model after first stage, and model trained with normal distillation using simple MSE loss. As shown in Figure 12, our condition-decoupled distillation not only ensures training stability but also significantly enhances video fidelity, whereas conventional distillation leads to training collapse. We argue that the failure stems from fundamental optimization conflict. Within parameter-shared architectural framework, gradients originating from the unconditional branch, which is introduced via the distillation loss, inherently conflict with the optimization of the poseconditional branch. While this conflicts impact may be negligible initially, as training progresses, it gradually desensitizes the model to the conditional inputs, invariably leading to collapse. In contrast, our condition-decoupled distillation mitigates this detrimental inter-branch interference, preserving the stability of both optimizations. Motion Discontinuity Mitigation. To resolve the abrupt jump artifacts caused by the significant pose discrepancy between the reference image and the first pose frame,"
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. CoRR, abs/2311.15127, 2023. 3 [2] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. 3 [3] Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Shengqu Cai, Chenxu Zhang, Guoxian Song, Chao Wang, Yichun Shi, Zeyuan Chen, Shijie Zhou, Linjie Luo, Gordon Wetzstein, and Mohammad Soleymani. X-dyna: ExIn IEEE/CVF pressive dynamic human image animation. Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 54995509. Computer Vision Foundation / IEEE, 2025. 6 [4] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation. CoRR, abs/2310.19512, 2023. 3 [5] Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Feng Wang, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, and Lian Zhuo. Wan-animate: Unified character animation and replacement with holistic replication. CoRR, abs/2509.14055, 2025. 3, 6 [6] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [7] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 23662369. IEEE, 2010. [8] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. 5 [9] Li Hu. Animate anyone: Consistent and controllable imageIn IEEE/CVF to-video synthesis for character animation. Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 81538163. IEEE, 2024. 3, 6 [10] Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. Animate anyone 2: High-fidelity character image arXiv preprint animation with environment affordance. arXiv:2502.06145, 2025. 3 [11] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 7 [12] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, YingCong Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench++: Comprehensive and versatile bencharXiv preprint mark suite for video generative models. arXiv:2411.13503, 2024. 7 [13] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12753 12762, 2021. 6, [14] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1719117202, 2025. 2, 3, 6 [15] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 1416, 2014, Conference Track Proceedings, 2014. 3 [16] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. CoRR, abs/2412.03603, 2024. 2, 3 [17] Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, and Tianshu Hu. Dreamactor-m1: Holistic, expressive and robust human image animation with hybrid guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1103611046, 2025. 3, 6 [18] Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. MIMO: controllable character video synthesis with spaIn IEEE/CVF Conference on tial decomposed modeling. Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 2118121191. Computer Vision Foundation / IEEE, 2025. 6 [19] William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 16, 2023, pages 41724182. IEEE, 2023. 11 [20] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 10674 10685. IEEE, 2022. 3 [21] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III, pages 234241. Springer, 2015. 3 [22] Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019. 3 [23] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1365313662, 2021. 3 [24] Guoxian Song, Hongyi Xu, Xiaochen Zhao, You Xie, Tianpei Gu, Zenan Li, Chenxu Zhang, and Linjie Luo. X-unimotion: Animating human images with expressive, unified and identity-agnostic motion latents. CoRR, abs/2508.09383, 2025. [25] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. 6 [26] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: Highquality identity-preserving human image animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2109621106, 2025. 3, 6 [27] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [28] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Xiaofeng Meng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. CoRR, abs/2503.20314, 2025. 2, 3, 6 [29] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, ChungChing Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93269336, 2024. 3, 6 [30] Xiang Wang, Shiwei Zhang, Longxiang Tang, Yingya Zhang, Changxin Gao, Yuehuan Wang, and Nong Sang. Unianimate-dit: Human image animation with large-scale video diffusion transformer. CoRR, abs/2504.11289, 2025. 3, 6 [31] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [32] Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, and Dahua Lin. Humanvid: Demystifying training data for camera-controllable human image animation. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. 3, 6, 7 [33] Shuolin Xu, Siming Zheng, Ziyi Wang, HC Yu, Jinwei Chen, Huaqi Zhang, Bo Li, and Peng-Tao Jiang. Hypermotion: Ditbased pose-guided human image animation of complex motions. CoRR, abs/2505.22977, 2025. 3, 6 [34] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human imIn IEEE/CVF Conage animation using diffusion model. ference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1481 1490. IEEE, 2024. 3, 6 [35] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. 3 [36] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. MAGVIT: masked generative video transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1045910469. IEEE, 2023. 3 [37] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 3813 3824. IEEE, 2023. [38] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the 12 IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 7 [39] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qing, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. CoRR, abs/2311.04145, 2023. 3 [40] Shiyi Zhang, Junhao Zhuang, Zhaoyang Zhang, Ying Shan, and Yansong Tang. Flexiact: Towards flexible action control In Proceedings of the Special in heterogeneous scenarios. Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. 3, 6 [41] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. CoRR, abs/2406.19680, 2024. 3, [42] Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36573666, 2022. 3 [43] Jingkai Zhou, Benzhi Wang, Weihua Chen, Jingqi Bai, Dongyang Li, Aixi Zhang, Hao Xu, Mingyang Yang, and Fan Wang. Realisdance: Equip controllable character animation with realistic hands. arXiv preprint arXiv:2409.06202, 2024. 3, 6 [44] Jingkai Zhou, Yifan Wu, Shikai Li, Min Wei, Chao Fan, Weihua Chen, Wei Jiang, and Fan Wang. Realisdance-dit: Simple yet strong baseline towards controllable character animation in the wild. CoRR, abs/2504.14977, 2025. 3, 6, 7 [45] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, pages 145162. Springer, 2024. 3, 6 13 SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation"
        },
        {
            "title": "Supplementary Material",
            "content": "6. X-Dance Standard benchmarks, such as TikTok and RealisDance, source both the reference image and pose sequence from the same video. This idealized setup fails to reflect the spatiotemporal misalignment challenges prevalent in real-world applications. As shown in Fig. 14, to more robustly evaluate the models generalization capabilities in such scenarios, we curated and introduced new evaluation dataset, XDance. We first collected 12 distinct driving videos, comprising 8 sequences of intricate, high-dynamic dance movements and 4 sequences of low-amplitude daily activities. These sequences are replete with non-ideal real-world factors, such as motion blur, severe occlusion, and drastic pose changes. Tailored to these motions, we specifically curated diverse set of reference images to simulate real-world misalignments. This specially designed collection contains: (1) anime characters to introduce stylistic domain gaps; (2) half-body shots to represent compositional inconsistencies; (3) cross-gender or anime characters to simulate significant skeletal structural discrepancies; and (4) subjects in distinct postures to maximize the initial action gap. By systematically pairing these reference images with the 12 driving videos, we simulate two critical real-world challenges: (1) Spatial pose-structure inconsistency (e.g., an anime character driving real-world pose); and (2) Temporal discontinuity, specifically the significant gap between the reference pose and the initial driving pose. 7. Model Details 7.1. Motion Discontinuity Mitigation. As discussed in the main text, to address the abrupt transition between the reference frame and the initial pose frame, we propose Pose Simulation to explicitly replicate this discontinuity within the training data. Specifically, given smooth training sequence {p0, p1, . . . , pT }, we first construct synthetic pairs (p0, pT ), where the target timestamp is randomly sampled from {2, 3, 4} to ensure diversity. We then interpolate between this pair to generate intermediate poses, selecting the first interpolated frame p1. With probability of 0.5, we replace the original p1 with this synthetic p1, yielding pseudo-training sample {p0, p1, p2, . . . , pT }. This strategy effectively mimics realistic discontinuities while preserving the integrity of the motion control signal. By directly utilizing these synthetic trajectories as training samples, the model is exposed to realistic jump patterns without requiring any architectural modifications. Notably, fine-tuning on these synthetic samples for just few hundred steps resolves over 80% of extreme jump scenarios, all while maintaining no additional module parameters and zero additional inference latency. Beyond our proposed strategy, we explored several alternative approaches to address the motion discontinuity: Pose Warping: We inserted an explicit poseinterpolation submodule designed to generate intermediate poses bridging the reference pose and the first driving frame, effectively implementing pose warping. Pose Generator: We devised lightweight posesequence generator trained to synthesize intermediate frames conditioned on the start and end poses. Feature Mapping: We designed feature-mapping module that enhances each pose latent by aggregating features from its adjacent temporal neighbors. As shown in Fig. 15, comparison of the four methods reveals that Pose Simulation yields the best results, producing smooth and natural transitions superior to the alternatives. Furthermore, all three approaches necessitate the introduction of additional network modules, which proved difficult to optimize effectively given our limited training data. In contrast, Pose Simulation mitigates this issue from datacentric perspective. It is perfectly suited to our limited data regime and imposes zero additional parameter overhead. 7.2. Implementation Details. Training Details. As detailed in the main text, our Staged Decoupled-Objective Training Pipeline employs distinct training configurations to optimize different objectives across separate stages, all leveraging the same dataset. Stage 1: Action Supervision. We utilize LoRA-based training with learning rate of 1e-4 for 12,000 steps. Model selection in this phase prioritizes motion adherence and basic image quality. Stage 2: Condition-Decoupled Distillation. We switch to full-parameter fine-tuning with reduced learning rate of 1e-6 for 2,000 steps. The selection of the optimal checkpoint is governed by visual preference. Stage 3: Motion Discontinuity Mitigation. We revert to LoRA-based training with learning rate of 1e-4 for brief 500 steps, focusing on leveraging the augmented training data to mitigate motion discontinuity artifacts. Training Dataset. We curated proprietary human motion dataset consisting of 7,338 five-second video clips, totaling 10.2 hours. Notably, this dataset scale is significantly smaller than that of comparable works, highlighting 1 Figure 14. Examples from the X-Dance benchmark. The second and third rows display driving video sequences, comprising both intricate, high-dynamic dance movements and low-amplitude simple activities. The first row presents reference images, which were specifically curated relative to these driving videos to evaluate the model with real-world misalignment challenges. the data efficiency of our design. To ensure quality, these clips were rigorously filtered from the Internet based on aesthetic scores, motion smoothness, subject prominence, and action types. The final collection is composed of two parts: 2,000 clips sourced from high-quality footage (e.g., documentaries, YouTube) to introduce diversity in aspect ratios and motion dynamics; and 5,000 vertical videos collected primarily from social media, focusing predominantly on dance sequences. We purposefully excluded extreme or complex actions to maintain training stability. Decoupled-Condition Classifier-Free Guidance. Most of the existing video generation models typically employ Classifier-Free Guidance (CFG) to synthesize high-quality samples that strictly adhere to the provided conditional guidance. Specifically, during the sampling process, the model leverages its capability to predict both conditional and unconditional noise. At each denoising timestep t, we perform two forward passes through the DiT using the current noisy latent xt, one conditioned on (ϵθ(xt, t, y)), and another conditioned on null embedding (ϵθ(xt, t, )). The standard CFG noise prediction is formulated as: ˆϵθ(xt, t, y, w) = ϵθ(xt, t, ) +w(ϵθ(xt, t, y) ϵθ(xt, t, )) (7) where the difference between the two predictions represents vector in noise space that steers the generation towards the condition y. The scalar denotes the guidance scale, determining the strength of the shift from the unconditional distribution toward the conditional one. In practice, it is common to replace the null condition with specific negative prompt (ytxt neg) to provide more explicit negative constraint, thereby improving generation quality. Within our framework, the pose signal ypose serves as critical condition. Inspired by textual negative prompting, we propose an innovative Decoupled-Condition ClassifierFree Guidance (DC-CFG) to further enhance pose controllability. Specifically, we apply scale and shift perturbations to the extracted pose signal to construct negative pose condition ypose neg . This explicitly simulates the misalignment issues we aim to avoid. Based on this, we obtain prediction guided by the negative pose: ϵθ(xt, t, ypose neg ). To effectively integrate these multiple conditions, we decouple the guid2 Figure 15. Performance comparison of four Motion Discontinuity Mitigation methods, showing that the Pose Simulation approach generates smooth and natural transitions. Notably, this method achieves this without introducing additional modules or extra inference latency. ance and adjustment processes as follows: ϵθ(xt, t, y, ypose ϵθ(xt, t, y, ytxt neg ) = ϵθ(xt, t, y) ϵθ(xt, t, ypose neg ) neg) = ϵθ(xt, t, y) ϵθ(xt, t, ytxt neg) neg) ˆϵθ(xt, t, y, wpose, wtxt) = ϵθ(xt, t, ytxt +wpose ϵθ(xt, t, y,ypose neg ) + wtxt ϵθ(xt, t, y, ytxt neg) (8) where wpose and wtxt denote the guidance scales for the pose and text prompt, respectively. By leveraging poseconditioned CFG, we compel the generator to not only strictly adhere to the target pose but also actively diverge from imprecise or ambiguous neighboring poses. This mechanism significantly sharpens motion precision, effectively suppressing common artifacts such as limb blurring, motion ghosting, or regression to the mean pose, ultimately achieving higher-fidelity control of the driving motion. Furthermore, we carefully designed temporal scheduling strategy for pose-guided CFG, capitalizing on the in3 herent coarse-to-fine generation characteristic of diffusion models. The core philosophy is to modulate the guidance strength dynamically. In the early denoising stages, the model primarily constructs low-frequency components, such as global contours and spatial layout. Imposing strong guidance at this phase ensures that the characters global pose structure is precisely anchored. Conversely, as the process advances to the later stages, the model shifts its focus to rendering high-frequency details, such as texture and lighting. Here, it is crucial to attenuate or remove the guidance. This grants the model sufficient degrees of freedom to synthesize photorealistic details faithful to the source appearance, avoiding visual artifacts like structural rigidity or unnatural deformations caused by over-guidance. This temporal mechanism effectively decouples structural pose control from appearance detail generation. By maximizing realism and naturalness while ensuring motion precision, it serves as pivotal trade-off strategy for achievFigure 16. Model performance using Decoupled-Condition Classifier-Free Guidance (DC-CFG). From top to bottom, the rows display the original pose (positive condition), the perturbed pose (negative condition), generation results without DC-CFG, and generation results with DC-CFG, showing our DC-CFG improves pose control and effectively suppresses generation artifacts. ing high-fidelity pose-driven video generation. In practice, we apply pose-conditioned CFG exclusively within the normalized timestep interval of [0.1, 0.4]. The guidance scales are configured as wpose = 1.0 for the pose condition and wtxt = 5.0 for the text prompt. Fig. 16 presents the positive and perturbed negative pose conditions along with generation results, illustrating the impact of our DC-CFG. As shown, our approach improves pose control and effectively suppresses generation artifacts. 8. Limitation and Future Work Despite the promising results achieved by SteadyDancer in harmonized and coherent animation, several limitations remain to be addressed. 1) Domain Gap in Stylized Images. While our model delivers visually pleasing and coherent results for anime reference frames, its performance remains slightly inferior to the exceptional fidelity achieved on real-world images, occasionally exhibiting minor degradation in stylistic consistency. This limitation stems from the under-representation of anime samples in our current In future work, we aim to augment the training corpus. 4 training data with specialized anime datasets to bridge this domain gap and enhance the models generalization across different artistic styles. 2) Extreme Motion Discontinuities. Our proposed strategy effectively mitigates motion discontinuities in the vast majority of scenarios. However, in cases of extreme pose discrepancies between the reference frame and the initial driving pose, as the model prioritizes precise motion control, it may generate transitions that appear accelerated or slightly unnatural. We believe that developing more sophisticated temporal modeling architectures and scaling up the training data will be instrumental in further resolving this start-gap challenge. 3) Advanced Motion Representation. The current architecture relies heavily on the accuracy of the input pose sequence; for instance, consecutive pose estimation errors can lead to irreversible artifacts in the generated video. We argue that an ideal animation system should balance precise controllability with high error tolerance. Therefore, designing more refined, diverse, and semantically rich motion representation remains promising direction for future research."
        }
    ],
    "affiliations": [
        "Platform and Content Group (PCG), Tencent",
        "Shanghai AI Lab",
        "State Key Laboratory for Novel Software Technology, Nanjing University"
    ]
}