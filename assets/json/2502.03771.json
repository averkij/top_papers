{
    "paper_title": "Adaptive Semantic Prompt Caching with VectorQ",
    "authors": [
        "Luis Gaspar Schroeder",
        "Shu Liu",
        "Alejandro Cuadron",
        "Mark Zhao",
        "Stephan Krusche",
        "Alfons Kemper",
        "Matei Zaharia",
        "Joseph E. Gonzalez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Semantic prompt caches reduce the latency and cost of large language model (LLM) inference by reusing cached LLM-generated responses for semantically similar prompts. Vector similarity metrics assign a numerical score to quantify the similarity between an embedded prompt and its nearest neighbor in the cache. Existing systems rely on a static threshold to classify whether the similarity score is sufficiently high to result in a cache hit. We show that this one-size-fits-all threshold is insufficient across different prompts. We propose VectorQ, a framework to learn embedding-specific threshold regions that adapt to the complexity and uncertainty of an embedding. Through evaluations on a combination of four diverse datasets, we show that VectorQ consistently outperforms state-of-the-art systems across all static thresholds, achieving up to 12x increases in cache hit rate and error rate reductions up to 92%."
        },
        {
            "title": "Start",
            "content": "Luis Gaspar Schroeder 1 2 Shu Liu 1 Alejandro Cuadron 1 3 Mark Zhao 4 Stephan Krusche 2 Alfons Kemper 2 Matei Zaharia 1 Joseph E. Gonzalez 1 5 2 0 2 6 ] . [ 1 1 7 7 3 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Semantic prompt caches reduce the latency and cost of large language model (LLM) inference by reusing cached LLM-generated responses for semantically similar prompts. Vector similarity metrics assign numerical score to quantify the similarity between an embedded prompt and its nearest neighbor in the cache. Existing systems rely on static threshold to classify whether the similarity score is sufficiently high to result in cache hit. We show that this one-size-fits-all threshold is insufficient across different prompts. We propose VectorQ, framework to learn embedding-specific threshold regions that adapt to the complexity and uncertainty of an embedding. Through evaluations on combination of four diverse datasets, we show that VectorQ consistently outperforms state-of-the-art systems across all static thresholds, achieving up to 12 increases in cache hit rate and error rate reductions up to 92%. 1. Introduction Semantic prompt caches can reduce large language model (LLM) inference latency by up to 100x by reusing cached LLM-generated responses for semantically similar prompts (Bang, 2023). These systems convert prompts into vector embeddings and cache them in vector database along with their corresponding LLM-generated responses (Dasgupta et al., 2025). prompt that is currently being processed is referred to as candidate. Given new candidate, the system must decide whether to return cached response, known as reuse, or generate new one with an expensive LLM inference (Li et al., 2024). The system retrieves its nearest neighbor from the vector database when processing candidate. cache hit occurs when the system returns the *Equal contribution 1University of California, Berkeley 2Technical University of Munich 3ETH Zurich 4Stanford University. Correspondence to: Luis Gaspar Schroeder <luisgasparschroeder[at]berkeley.edu>. nn, nn, R3 Figure 1. Left: VectorQ learns embedding-specific similarity threshold regions (R1 nn) to make cache hit or miss decisions in semantic prompt caches. Bayesian sampling, guided by an embedding-specific correctness posterior (orange line), enables VectorQ to prioritize re-evaluations for uncertain cache hits. Right: VectorQ consistently outperforms state-of-the-art semantic prompt caches across all static thresholds by achieving higher number of cache hits and reduced number of errors. previously cached LLM-generated response of the nearest neighbor as the candidates response. Conversely, cache miss occurs when the system cannot return the cached response and instead performs an expensive LLM inference to generate new response for the candidate. Determining whether the nearest neighbor is suitable for cache hit remains an open research question, as the candidate may require different LLM response (Bang, 2023; Dasgupta et al., 2025; Li et al., 2024). For example, consider cache containing research papers on Biology and Architecture. When the system processes new candidate whose prompt request is for Chemistry paper, the nearest neighbor might be Biology paper. Consequently, semantic prompt caches rely on vector similarity metrics to evaluate the contextual similarity of embeddings and determine whether the nearest neighbors cached response can be reused. Metrics such as cosine similarity (Rahutomo et al., 2012) and Euclidean distance (Globerson et al., 2004) provide numerical scores to quantify the similarity between two embeddings. We consider normalized scores, where 0.0 indicates no similarity between embeddings and 1.0 identical ones. At the extremes, similarity score of 0.0 indicates no similarity, requiring the system to generate new LLM response, while score of 1.0 implies an identical match, allowing the cache to reuse the cached response confidently. However, it is unclear which value between 0.0 and 1.0 is 1 Adaptive Semantic Prompt Caching with VectorQ sufficient to classify the nearest neighbor as similar enough. This value is called the threshold. State-of-the-art semantic prompt caches, such as GPTCache (Bang, 2023), use single static threshold throughout the caches runtime. Users either rely on predefined threshold (e.g., 0.7) or determine one by testing multiple thresholds and selecting the value that best fits their requirements (Dasgupta et al., 2025). If the threshold is set too low, the system might produce incorrect cache hits, where it reuses the cached response from the nearest neighbor, but the response differs from the one the candidate requires (Rekabsaz et al., 2017). Vice versa, if the threshold is set too high, the system may avoid cache hits entirely. Our work demonstrates that static threshold is inadequate for classifying different prompts as semantically similar (Section 4). Instead of one static threshold value uniformly applied across all embeddings, we propose VectorQ, framework that learns embedding-specific threshold regions that adapt to each embedding (Section 5.1). Given candidate and its nearest neighbor, along with its cached LLMgenerated response and threshold regions, the system determines its action based on the similarity between the candidate and the nearest neighbor. If the similarity falls within region spanned by similarity values that previously led to correct cache hits, the system returns the cached response (cache hit). Otherwise, it generates new LLM response (cache miss). VectorQ defines these regions by analyzing the similarity scores linked to correct and incorrect cache hits for each embedding (Figure 1). Through Bayesian inference-based (Bayes, 1763) sampling approach, VectorQ prioritizes the correctness re-evaluation of the most uncertain regions and guarantees threshold region convergence that minimizes the number of incorrect cache hits (Section 5.2). Users can adjust the caches accuracy-latency trade-off using the uncertainty gate parameter (Section 5.3). We demonstrate the effectiveness of our approach by replacing the static similarity threshold with VectorQ to showcase the impact of dynamic and embedding-specific threshold regions. We evaluate our approach using combination of four diverse datasets (Saurabh Shahane, 2023; Talmor et al., 2018; Ni et al., 2019; Rogers et al., 2023) and use them in novel semantic prompt cache benchmark (Appendix E) designed to simulate challenging workload conditions by alternating prompts across varying contexts. Our results show that VectorQ consistently outperforms all static thresholds, achieving up to 12 increases in cache hit rate and error rate reductions by up to 92% (Figure 4). Our main contributions are three-fold: We demonstrate that static similarity threshold fails to classify different prompts. We propose VectorQ, framework that learns embedding-specific threshold regions using Bayesian sampling to go beyond static similarity threshold. We demonstrate that VectorQ consistently outperforms static thresholds and achieves up to 12 more cache hits and up to 92% lower error rates. 2. Related Work KV Caching Inference Engines. Systems like vLLM (Kwon et al., 2023) and SGLang (Zheng et al., 2023) optimize the efficiency of large language model inference. vLLM is system designed to enhance the efficiency of large language model serving by optimizing memory management and scheduling. vLLM introduces PagedAttention, an attention algorithm inspired by virtual memory and paging techniques, to address inefficiencies in managing the key-value (KV) cache memory for LLMs (Kwon et al., 2023). SGLang is serving framework for large language models that improves interaction speed and control through co-design of the backend runtime and frontend language. It includes features like RadixAttention for efficient prefix caching (Zheng et al., 2023). This work is orthogonal as our adaptive semantic prompt cache complements these systems by reusing cached LLM-generated responses for semantically similar prompts. For prompts that cannot reuse cached response, systems like vLLM or SGLang reduce the latency of inference itself. Semantic Prompt Caches. Semantic prompt caches intercept prompts before they reach the LLM, matching them to previously stored prompts based on semantic similarity. If similar prompt exists (cache hit), the system retrieves the cached response, avoiding new LLM inference. If no match is found, the prompt is sent to the LLM, and its response is added to the cache for future reuse (Bang, 2023). These systems can reduce LLM inference latency by up to 100x (zilliztech). GPTCache (zilliztech) is the most prominent open-source implementation with 7.4k stars on GitHub. AWS (Razi et al., 2024), Microsoft (Dan Lepow, 2025), waLLMartCache (Dasgupta et al., 2025), and SCALM (Li et al., 2024) have proposed semantic prompt cache frameworks based on similar architecture. These frameworks rely on vector similarity metrics to quantify the semantic similarity of the most similar prompt stored in the cache (Bang, 2023). Metrics such as cosine similarity (Rahutomo et al., 2012) and Euclidean distance (Globerson et al., 2004) provide numerical scores to quantify the similarity between two embeddings. However, it is unclear which similarity value, the threshold, is sufficient to classify the most similar prompt as similar enough. State-of-the-art semantic prompt caches use single static threshold throughout the caches runtime. Users either rely on predefined threshold (i.e., 0.7) or determine one by testing multiple thresholds and se2 Adaptive Semantic Prompt Caching with VectorQ lecting the value that best fits their requirements (Dasgupta et al., 2025). If the threshold is too low the system might produce incorrect cache hits. If the threshold is too high the system may avoid cache hits entirely. We demonstrate the insufficiency of static threshold and propose VectorQ, framework that learns adaptive and embedding-specific threshold regions. 3. Problem Setup"
        },
        {
            "title": "The cache maintains a vector database D containing a set of\nembedded prompts and their metadata",
            "content": "D = {(pi, ri, R1 , R2 , R3 , πi) N}, where: pi Rd: The d-dimensional vector embedding of prompt. ri: The LLM-generated response for the prompt. R1 , R2 , R3 : The threshold regions (Section 5.1). πi(s): The correctness posterior function (Section 5.2). For new prompt (referred to as the candidate), let Rd represent its vector embedding. The system identifies the nearest neighbor nn in given similarity metric: nn = arg max pnnD sim(p, pnn), and nn = (pnn, rnn, nn, R2 nn, R3 nn, πnn), where sim(p, pnn) [0.0, 1.0] is the embedding similarity score, with 0.0 indicating no similarity and 1.0 indicating identical embeddings. cache miss occurs if the similarity score = sim(p, pnn) falls below threshold θ: < θ. VectorQ uses threshold regions instead of one threshold (see Section 5.1). The system generates new response using the LLM and updates the cache: {(p, r, R1, R2, R3, π)}. cache hit occurs if the similarity score exceeds threshold θ: θ. The cached response rnn associated with nn is returned in this case. cache hit is correct if the cached response rnn matches the LLM-generated response that would have been produced for the prompt (rnn = r). cache hit is incorrect if the cached response rnn deviates from the LLM-generated response (rnn = r). 3 Figure 2. Kernel Density Functions (KDFs) for correct (green) and incorrect (red) cache hits for Amazon Prime Video Review Dataset. 4. The Problem of Static Similarity Thresholds This section examines the limitations of using static similarity threshold in semantic prompt caches, highlighting why single threshold cannot generalize across different prompts. State-of-the-art semantic prompt caches rely on static threshold to determine whether cached response should be reused or if new response should be generated for candidate (Bang, 2023; Dan Lepow, 2025; Razi et al., 2024; Dasgupta et al., 2025; Li et al., 2024). single threshold assumes that all candidates and their nearest neighbors share uniform similarity value that can reliably distinguish between correct and incorrect cache hits. The problem is that even highly similar prompts, in terms of their embeddings, might lead to different responses, while dissimilar prompts might produce the same response (Zhu et al., 2024). For example, the prompts Name paper about plant cell Biology. and Name paper about animal cell Biology. are semantically similar but require different responses. Conversely, prompts like How many tires does car have? and What is two times two? are semantically different but share the same response. This inconsistency highlights the challenges of relying on static similarity threshold. Experiment. To analyze the relationship between similarity values and cache hit correctness, we designed an experiment with the following setup: Every processed candidate is added to the vector database, ensuring the database contains all previously seen prompts. For each new candidate, we identify its nearest neighbor in the vector database using cosine similarity (Rahutomo et al., 2012). We validate whether the cached response for the nearest neighbor character matches the response that would have been generated for the candidate. This allows us to classify cache hits as either correct or incorrect. For each prompt, we record the similarity value between the new prompt and its nearest Adaptive Semantic Prompt Caching with VectorQ Figure 3. The figure illustrates the evolution of the three threshold regions (R1 0) for the prompt Name paper about human Biology. Six candidates use this embedding as their nearest neighbor and attempt to reuse its cached response (i.e., the name of paper about human Biology). Steps 3 and 6.1 demonstrate how correctness sampling re-evaluations either increase certainty in R3 0 or reduce its size, respectively. Further details are provided in Sections 5.1 and 5.2. 0, R3 0, R2 neighbor, along with label indicating whether the cache hit was correct or incorrect. We generate distributions of similarity values for both correct and incorrect cache hits. These distributions are visualized as kernel density functions (KDFs) in Figure 2, which is based on 400 samples from the Amazon Instant Video Review dataset (Ni et al., 2019). Analysis. For static threshold to be effective, the kernel density function (KDF) for correct cache hits would need to be shifted to the right relative to the KDF for incorrect cache hits. This shift would indicate that correct cache hits consistently require higher embedding similarity values, allowing threshold to reliably distinguish between correct and incorrect responses. However, the results show significant overlap between the correct and incorrect KDFs, demonstrating that no single similarity threshold can effectively differentiate between the two. Similar patterns are observed in the E-Commerce dataset and the semantic prompt cache benchmark, as shown in Appendix C. These findings underscore the inherent limitations of static thresholds. To address this, we propose embedding-specific and dynamic threshold regions that adapt to the characteristics of individual embeddings. 5. VectorQ This section describes how VectorQ overcomes the limitations of the static similarity threshold in semantic prompt caches. We introduce embedding-specific threshold regions accompanied by an illustrative example, explain how correctness sampling enables the refinement of these regions, and how to adjust the accuracy-latency trade-off in VectorQ. 5.1. Threshold Regions Each cached embedded prompt maintains three threshold regions in its metadata: Incorrect Region (R1 p), Uncertain Region (R2 p), and Correct Region (R3 p). The system decides between cache hit or cache miss based on the region to which the similarity score between the candidate and its nearest neighbor belongs. The regions expand and contract based on the correctness of cache hits and their associated embedding similarity scores, where all possible similarity thresholds are in the range [0.0, 1.0]. Region 1 (R1 p) spans from 0.0 to the largest similarity value which resulted in wrong cache hit. Region 3 (R3 p) spans from the smallest similarity value which resulted in correct cache hit to 0.0. Region 2 (R2 p) is the uncertainty region that spans all values between R1 and R3 p. The three regions of an embedding are non-overlapping, with R1 expanding monotonically toward higher similarity values. We describe the algorithm used to construct the threshold regions with an illustrative example where user prompts an LLM to name relevant paper titles. This explanation is accompanied by the visualization in Figure 3 and references Algorithm 1. Construction of Threshold Regions. The cache starts with an empty vector database. Step 0) When the first candidate Name paper about human Biology is processed, no nearest neighbor exists. The LLM generates response for the candidate, which gets stored in the embedding metadata. The embedding is added to the vector database with Region 2 (R2 0) initially spanning Adaptive Semantic Prompt Caching with VectorQ Algorithm 1 VectorQ Algorithm Require: Prompt p, Embedded prompt p, nearest neighbor nn, R2 nn, πnn), user-defined nn = (pnn, rnn, R1 uncertainty gate ug (1.0 by default. See Section 5.3) nn, R3 else nn then nn then Add to R1 return gt nn and add to the vector database Generate ground truth gt for via LLM inference Add to R1 return gt Generate ground truth gt for via LLM inference if rnn == gt then Add to R3 nn return gt Ensure: Reuse cached response or do LLM inference 1: Compute similarity = sim(p, pnn) 2: if R1 3: 4: 5: 6: else if R2 7: 8: 9: 10: 11: 12: 13: end if 14: 15: else if R3 16: 17: 18: 19: 20: 21: 22: 23: 24: Generate ground truth gt for via LLM inference if rnn == gt then Add to R3 nn return gt Draw random number U(0.0, ug) if < πnn(s) then Add to R1 nn and add to the vector database Transfer all similarity values less than or equal nn, and remove them from R3 to to R1 nn return gt nn and add to the vector database nn then else end if else 25: 26: 27: 28: 29: 30: end if end if return rnn the full threshold range. For the rest of the examples, this embedding is the nearest neighbor for all candidates. Step 1) For the next candidate Name paper about Biology, the cache retrieves the nearest neighbor and computes the similarity between the two (Line 1 in Algorithm 1). The similarity falls within R2 0, which initially spans the entire range. Since R2 0 represents the uncertainty region, the system uses the LLM to generate ground truth response for the candidate (Line 6 in Algorithm 1). The candidate and the nearest neighbor share the same response, so the similarity value is considered valid and added to Region 3 (R3 0), the correct region (Line 8-9 in Algorithm 1). Step 2) For the next candidate Name paper about Architecture the cache retrieves the nearest neighbor and computes the similarity, which again falls in R2 0. The system uses the LLM to generate ground truth response. However, Algorithm 2 Posterior Updates for Correctness Sampling Require: Current posterior πnn(s), x-values X, sample xsample, correctness flag is correct, decay λ Ensure: Updated posterior πnn(s) 1: exp(λ xsample) 2: if is correct then 1 3: 4: else 1 2 1 2 1 + 5: 6: end if 7: πnn(s) πnn(s) πnn(s) (cid:82) πnn(s) ds 8: πnn(s) 9: return πnn(s) the candidate requires an Architecture paper, whereas the cached response corresponds to the name of Biology paper, which would result in an incorrect cache hit. The similarity value is classified as incorrect and added to R2 0. Thus the candidate is not represented in the vector database, it stores the ground truth (computed in Line 6 in Algorithm 1) and gets added to the vector database (Line 11 in Algorithm 1). Step 3) For the next candidate Name homo sapiens Biology paper, the cache retrieves the nearest neighbor and computes the similarity, which falls in R3 0. The correctness sampling (details in Section 5.2) triggers re-evaluation of the cache hits correctness. The LLM generates ground truth response for the candidate, and the result matches the cached response. Their similarity value is added to R3 0 as it classifies as correct (Line 19 in Algorithm 1). Step 4) For the next candidate Name paper about animal Biology, the cache retrieves the nearest neighbor and computes the similarity, which falls in R2 0 represents the uncertainty region, the system invokes the LLM to generate ground truth response. The cached response does not match the expected response for the candidate, so their similarity value is classified as incorrect and added to R1 0 (Line 11 in Algorithm 1). 0. As R2 Step 5) For the next candidate Name paper that covers Biology, the cache retrieves the nearest neighbor and computes the similarity, which falls in R3 0. The correctness sampling classifies the similarity value as certain enough and allows cache hit (Line 27 in Algorithm 1). Step 6) For the next candidate Name paper about plant and human Biology, the cache computes the similarity, which falls in R3 0. The correctness sampling triggers reevaluation of the cached response (Line 15-16 in Algorithm 1). The LLM generates ground truth response for the candidate, which does not match the nearest neighbors cached response. This indicates that all similarity values smaller than the current value are insufficient, as they fail 5 Adaptive Semantic Prompt Caching with VectorQ to ensure correctness. Consequently, all similarity values less than or equal to this similarity value are reclassified as incorrect and moved to R1 0 (Lines 2224 in Algorithm 1). 5.2. Correctness Sampling As shown in Figure 2, similarity threshold cannot reliably differentiate between correct and incorrect cache hits. We propose Bayesian Inference-based correctness sampling (Bayes, 1763), method to periodically re-validate uncertain similarity values between candidate and its nearest neighbor nn that fall in R3 nn. This approach constructs and maintains embedding-specific posterior distributions that model the likelihood of correct and incorrect cache hits given their corresponding similarity values. By sampling from this posterior, we determine whether to re-evaluate the correctness of the cache hit. Correctness Posterior. The correctness posterior models the likelihood of an incorrect cache hit rate for given similarity value, enabling adaptation to an embeddings cache hit rate history. Initially, the prior distribution is uniform, pprior(s) = 1.0 for all similarity values [0.0, 1.0], indicating no prior knowledge about embeddings cache hit rate history and considering all similarity values as insufficient (Thompson, 1933). As the correctness of the cache hits is evaluated, the posterior is updated using an exponential likelihood function, L(s) = eλc, where denotes whether the cache hit was correct (c = 1) or incorrect (c = 1), and λ controls the update sensitivity (Schennach, 2005). Correct cache hits decrease the posterior value at the corresponding threshold, signifying increased confidence in its correctness (Line 3 in Algorithm 2). Conversely, incorrect cache hits increase the posterior, signaling higher likelihood of failure at that threshold (Line 5 in Algorithm 2). This localized updating ensures the posterior evolves to reflect cache hit patterns specific to their associated similarity values. After each update, the posteriors y-axis range is normalized to [0.0, 1.0] (Line 8 in Algorithm 2), providing consistent scaling for the correctness sampling. With increasing sample size, the posterior converges toward 1.0 in threshold regions associated with frequent incorrect cache hits and toward 0.0 in regions corresponding to correct cache hits (Bayes, 1763). Correctness Sampling. The objective is to minimize the number of incorrect cache hits per embedding by dynamically refining R3 nn. To decide whether to re-evaluate the correctness of cache hit, we leverage the correctness posterior of an embedding. For candidate with similarity value to its nearest neighbor (where falls in the nearest neighbors R3 nn), we evaluate the posterior πnn and draw random number from uniform distribution U(0.0, 1.0). If πnn, the similarity value is likely to result in an incorrect cache hit. We re-evaluate it and adjust the posterior accordingly (Lines 17-25 in Algorithm 1). Conversely, if > πnn, the similarity value is likely to be correct and results in cache hit (Line 27 in Algorithm 1). The uniform random sampling ensures probabilistic fairness by occasionally re-evaluating even similarity values with low posterior likelihoods. This sampling process operates analogously to Thompson Sampling (Thompson, 1933), prioritizing correctness re-evaluations for similarity values that are most likely to result in incorrect cache hits. This minimizes the number of expensive LLM calls required to re-evaluate the match between the nearest neighbors cached response and the expected one for the candidate. nn and removed from Threshold Region Convergence Guarantee. When cache hit is classified as incorrect, the associated similarity value between the candidate and its nearest neighbor nn is nn. If is also in the range of R3 added to R1 nn, all similarity values in R3 nn that are less than or equal to are transferred to R1 nn (Line 23 in Algorithm 1). Because similarity values are never removed from R1 nn, and the three threshold regions remain non-overlapping, R1 nn expands monotonically toward the upper limit of 1.0, the maximum possible similarity value. At convergence, only candidates with similarity value of exactly 1.0 (perfect matches) result in cache hits. This ensures that all unreliable similarity values are excluded, effectively eliminating incorrect cache hits for the embedding. We elaborate on the convergence speed in Appendix D. 5.3. Uncertainty Gate: Accuracy-Latency Trade-Off VectorQ evaluates the correctness of cache hits by generating the candidates response via LLM inference and comparing it to the nearest neighbors cached response. Since LLM inference is computationally expensive, performing this evaluation for every cache hit would make the system impractical. To address this, VectorQ selectively identifies cache hits as correct without re-evaluation when there is high confidence in their correctness. To manage the trade-off between accuracy and latency, VectorQ introduces the optional, userconfigurable uncertainty gate parameter that balances the number of correctness evaluations. This trade-off is implemented by modifying the bounds of the uniform distribution used during correctness sampling (see Section 5.2). By default, random numbers are drawn from uniform distribution U(0.0, 1.0). Users can modify the maximum value of the distribution, ug, to redefine the range of randomly drawn numbers as U(0.0, ug), where 0.0 ug 1.0. Reducing ug lowers the likelihood of drawing random number greater than the posterior in uncertain regions. For example, setting ug = 0.3 ensures that the system classifies cache hits as correct only if the posterior value at the given similarity score is below 0.3. Since the posterior decreases for similarity scores associated with correct cache hits and 6 Adaptive Semantic Prompt Caching with VectorQ (a) Amazon Product Review Dataset (b) E-Commerce Dataset (c) Semantic Prompt Cache Benchmark Figure 4. Performance comparison of VectorQ with state-of-the-art semantic prompt cache using static similarity thresholds (e.g., GPTCache). VectorQ consistently outperforms static thresholds across all evaluated benchmarks and configurations of the uncertainty gate parameter (Section 5.3). VectorQ achieves up to 12 increases in cache hit rate (given 0.4% error rate in Figure 4a) and reduces error rates by up to 92% (given 16% cache hit rate in Figure 4b). increases for those linked to incorrect cache hits, lower posterior value indicates similarity score with stronger history of correct cache hits (see Section 5.2). Reducing ug emphasizes accuracy by increasing LLM re-evaluations for cache hits. Conversely, increasing ug prioritizes lower latency by reducing expensive re-evaluations, possibly leading to more incorrect cache hits. 6. Evaluation We demonstrate the effectiveness of our approach by replacing static similarity thresholds with VectorQ, highlighting the benefits of dynamic and embedding-specific threshold regions for semantic prompt caches. We show that VectorQ achieves higher cache hit rates, lower error rates, imposes minimal latency overhead, and we include an ablation study to evaluate the impact of Bayesian correctness sampling. Server and Model Configuration. For all of our experiments, we use an N1 Google Cloud Platform (GCP) instance with one f1-micro CPU and one NVIDIA T4 GPU to host the VectorQ server and inference server. The T4 has 16 GB GPU memory and processes 65 TFLOPS with mixed precision (FP16/FP32) (nvi, 2024). We use the lightweight LLaMA-3.1-8B model (Touvron et al., 2023), hosted on our GCP instance with an Ollama (Morgan & Chiang, 2023) inference server. To ensure consistent and deterministic responses, we set the temperature parameter of the LLM to 0.0 (Wang et al., 2020). Embedding Construction and Comparison. We use the gte-large-en-v1.5 model (Zhang et al., 2024) to create vector embedding representations for the prompts. The prompt construction is explained in Appendix B. The gte-large-env1.5 model is optimized for text retrieval tasks and supports sequence lengths up to 8192 tokens, making it suited for semantic caching. The embeddings are stored in the HNSWLIB vector database (Malkov & Yashunin, 2018) and compared using cosine similarity, commonly used metric in semantic prompt caches (Bang, 2023; Dasgupta et al., 2025). Key Metrics. We evaluate the performance of each dataset by measuring correct and incorrect cache hits. cache hit is considered correct if the cached LLM-generated response of the nearest neighbor character matches the LLM-generated response for the candidate. Otherwise, the cache hit is classified as incorrect because the returned response deviates from the expected result. The error rate reflects the number of incorrect cache hits over the number of processed entries. Datasets. We use combination of four datasets that span three different task categories: classification, sentiment analysis, and user prompts. For classification tasks, we use the E-Commerce Text Classification (Saurabh Shahane, 2023) and CommonsenseQA dataset (Talmor et al., 2018). For sentiment analysis, we use the Amazon Instant Video Review dataset (Ni et al., 2019). The ComQA dataset (Rogers et al., 2023) represents user prompts with semantically distinct entries. Further dataset and prompting details can be found in Appendix B. The Semantic Prompt Caching Benchmark combines these four datasets into single workload to introduce varying levels of difficulty (see Appendix E). 6.1. Benchmark Results VectorQ Outperforms Every Static Threshold. We compare the performance of VectorQ to state-of-the-art semantic prompt cache system across all static similarity thresholds and all user-definable uncertainty gate parameters (Section 5.3). Threshold and uncertainty gate selection details are provided in Appendix F. The evaluation is conducted across multiple datasets, including Amazon Product Reviews, E-Commerce Classification, and the Semantic Prompt Cache Benchmark, which collectively test caching strategies under varying prompt contexts. The results show that VectorQ consistently achieves lower error rates or higher cache hit rates across all reasonable 7 Adaptive Semantic Prompt Caching with VectorQ Figure 6. Latency comparison between VectorQ and staticthreshold methods on the Semantic Prompt Caching Benchmark (Appendix E). The results demonstrate that VectorQ introduces minimal overhead, achieving nearly identical latency to static-threshold methods despite incorporating embedding-specific threshold region updates and Bayesian sampling. approach leverages embedding-specific certainty measures to prioritize re-evaluations for uncertain embedding similarity values within R3 nn, ensuring that areas of higher uncertainty are addressed more frequently. In contrast, uniform sampling assigns equal probability to all embedding similarity values, disregarding their uncertainty levels. The results, presented in Figure 9 (Appendix), demonstrate that Bayesian sampling is overall more robust, achieving lower error rates or higher cache hit rates. While uniform sampling occasionally matches or slightly outperforms the static threshold baseline, its performance is inconsistent and often suboptimal. 6.2. Limitations The embedding model choice is critical in determining how effectively the prompt is represented in compressed vector format. If the embedding model is trained on use case that differs from the context of the prompts, it may fail to capture the relevant semantics required for accurate classifications (Tyshchuk et al., 2023). For instance, many embedding models are trained in specific language and struggle to interpret prompts in other languages (Chung et al., 2020). To address this limitation, we propose two approaches. First, users should leverage domain expertise to select an embedding model that aligns with their application context. Second, automatic model fine-tuning, as proposed by (Zeighami et al., 2024) or (Zhu et al., 2024), can be employed to adapt the model to the desired domain. 6.3. Future Work Embedding-specific posterior distributions offer natural way to derive certainty metrics, which can inform advanced eviction policies that potentially go beyond LRU and MRU (Mattson et al., 1970). We will aim to incorporate error rate guarantees, enabling users to set target error rates while maximizing cache hit rates. Figure 5. Performance comparison of VectorQ and staticthreshold approach on the Semantic Prompt Caching Benchmark. Top: VectorQ achieves steady improvement, unlike the static similarity thresholds converging performance. Bottom: Error rates, given static threshold of 0.8 and an uncertainty gate of 0.2. static thresholds (600 dataset entries for each threshold uncertainty gate), as shown in Figures (4a), (4b), and (4c). These results demonstrate the limitations of static similarity thresholds in adapting to diverse workloads and validate the adaptability and efficiency of VectorQs embedding-specific threshold regions. VectorQ Is Learning. Figure 5 highlights the learning capability of VectorQ compared to the static-threshold approach. VectorQ demonstrates growing cache hit rate, driven by its adaptive threshold regions that learn and refine cache hit decisions. In contrast, the static-threshold approach maintains an almost constant cache hit rate and is unable to adapt to varying embedding complexities. While VectorQ stabilizes its error rate as it learns, the static-threshold approach experiences steadily increasing error rate, reflecting its inability to adapt the threshold as more entries are processed. VectorQ Latency Overhead. To evaluate the computational overhead introduced by VectorQs embedding-specific region updates, posterior updates, and Bayesian sampling, we compare the latency of semantic prompt cache using static thresholds with one that replaces them with VectorQ. To ensure fair comparison, the first 300 prompts are forced to result in cache misses, and the subsequent 300 prompts are forced to result in cache hits. This setup standardizes the LLM inference costs, as the computation of LLM responses remains identical across both methods. We calculate the average latency for both cache misses and cache hits and present the results in Figure 6. The findings demonstrate that VectorQ introduces minimal overhead, achieving identical latency to static-threshold methods, even with embeddingspecific threshold region updates and Bayesian sampling. Correctness Sampling Ablation Study. This study evaluates the effectiveness of Bayesian inference-based correctness sampling compared to uniform sampling. The Bayesian 8 Adaptive Semantic Prompt Caching with VectorQ 7. Conclusion This paper proposes VectorQ, framework that learns embedding-specific similarity threshold regions to address the limitations of static thresholds in semantic prompt caches. VectorQ employs Bayesian sampling to re-evaluate the correctness of cache hits and adapts embedding-specific threshold regions. Our results show that VectorQ consistently outperforms all static thresholds, achieving up to 12 increases in cache hit rate and error rate reductions by up to 92%."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. By reducing the computational cost and latency of inference systems, our approach makes LLMbased technologies more accessible to broader audience, lowering the barrier to entry for organizations and individuals who may otherwise lack the resources for large-scale LLM usage. Furthermore, by reducing the need to invoke the full LLM generation process, this work reduces the demand for compute associated with LLMs and as consequence the potential broader carbon footprint associated with building and running additional AI data-centers."
        },
        {
            "title": "References",
            "content": "Nvidia t4 flexible design, extraordinary performance. http s://www.nvidia.com/en-us/data-center/tesl a-t4/, 2024. [Accessed 12-11-2024]. Bang, F. Gptcache: An open-source semantic cache for llm applications enabling faster answers and cost savings. In Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pp. 212218, 2023. Bayes, T. An essay towards solving problem in the doctrine of chances. Philosophical Transactions, 53:370 418, 1763. Chung, H. W., Fevry, T., Tsai, H., Johnson, M., and Ruder, S. Rethinking embedding coupling in pre-trained language models. arXiv preprint arXiv:2010.12821, 2020. Dan Lepow, Arie Heinrich, R. M. Enable semantic caching for Azure OpenAI APIs in Azure API Management, 01 2025. URL https://learn.microsoft.com/en-u s/azure/api-management/azure-openai-enabl e-semantic-caching. [Accessed 22-01-2025]. Dang, N. C., Moreno-Garcıa, M. N., and De la Prieta, F. Sentiment analysis based on deep learning: comparative study. Electronics, 9(3):483, 2020. Dasgupta, S., Wagh, A., Parsai, L., Gupta, B., Vudata, G., Sangal, S., Majumdar, S., Rajesh, H., Banerjee, K., and Chatterjee, A. wallmartcache: distributed, multi-tenant and enhanced semantic caching system for llms. In International Conference on Pattern Recognition, pp. 232248. Springer, 2025. Globerson, A., Chechik, G., Pereira, F., and Tishby, N. Euclidean embedding of co-occurrence data. Advances in neural information processing systems, 17, 2004. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th Symwith pagedattention. posium on Operating Systems Principles, pp. 611626, 2023. Li, J., Xu, C., Wang, F., von Riedemann, I. M., Zhang, C., and Liu, J. Scalm: Towards semantic caching for automated chat services with large language models. arXiv preprint arXiv:2406.00025, 2024. Malkov, Y. A. and Yashunin, D. A. Efficient and robust approximate nearest neighbor search using hierarchical IEEE transactions on navigable small world graphs. pattern analysis and machine intelligence, 42(4):824 836, 2018. Mattson, R. L., Gecsei, J., Slutz, D. R., and Traiger, I. L. Evaluation techniques for storage hierarchies. IBM Systems journal, 9(2):78117, 1970. Morgan, J. and Chiang, M. Ollama, 2023. URL https: //ollama.com/. [Accessed 12-11-2024]. Ni, J., Li, J., and McAuley, J. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pp. 188197, 2019. Rahutomo, F., Kitasuka, T., Aritsugi, M., et al. SemanIn The 7th international student tic cosine similarity. conference on advanced science and technology ICAST, volume 4, pp. 1. University of Seoul South Korea, 2012. Razi, K., Joshi, A., Hong, S., and Shah, Y. Build readthrough semantic cache with Amazon OpenSearch Serverless and Amazon Bedrock, 11 2024. URL https: //aws.amazon.com/blogs/machinelearnin g/build-a-read-through-semantic-cache-wit h-amazon-opensearch-serverless-and-amazo n-bedrock/. [Accessed 17-01-2025]. Adaptive Semantic Prompt Caching with VectorQ Zheng, L., Yin, L., Xie, Z., Huang, J., Sun, C., Hao Yu, C., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., et al. Efficiently programming large language models using sglang. arXiv e-prints, pp. arXiv2312, 2023. Zhu, H., Zhu, B., and Jiao, J. Efficient prompt caching via embedding similarity. arXiv preprint arXiv:2402.01173, 2024. zilliztech. GPTCache : Library for Creating Semantic Cache for LLM Queries. URL https://github.com /zilliztech/GPTCache. [Accessed 23-01-2025]. Rekabsaz, N., Lupu, M., and Hanbury, A. Exploration of threshold for similarity based on uncertainty in word embedding. In Advances in Information Retrieval: 39th European Conference on IR Research, ECIR 2017, Aberdeen, UK, April 8-13, 2017, Proceedings 39, pp. 396 409. Springer, 2017. Rogers, A., Gardner, M., and Augenstein, I. Qa dataset explosion: taxonomy of nlp resources for question answering and reading comprehension. ACM Computing Surveys, 55(10):145, 2023. Saurabh Shahane. Ecommerce text classification, 10 2023. URL https://www.kaggle.com/datasets/saur abhshahane/ecommerce-text-classification. [Accessed 12-11-2024]. Schennach, S. M. Bayesian exponentially tilted empirical likelihood. Biometrika, 92(1):3146, 2005. Talmor, A., Herzig, J., Lourie, N., and Berant, J. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. Thompson, W. R. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285294, 1933. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Tyshchuk, K., Karpikova, P., Spiridonov, A., Prutianova, A., Razzhigaev, A., and Panchenko, A. On isotropy of multimodal embeddings. Information, 14(7):392, 2023. Wang, P.-H., Hsieh, S.-I., Chang, S.-C., Chen, Y.-T., Pan, J.-Y., Wei, W., and Juan, D.-C. Contextual temperature for language modeling. arXiv preprint arXiv:2012.13575, 2020. Zeighami, S., Wellmer, Z., and Parameswaran, A. Nudge: Lightweight non-parametric fine-tuning of embeddings for retrieval. arXiv preprint arXiv:2409.02343, 2024. Zhang, B., Yang, H., Zhou, T., Ali Babar, M., and Liu, X.- Y. Enhancing financial sentiment analysis via retrieval augmented large language models. In Proceedings of the fourth ACM international conference on AI in finance, pp. 349356, 2023. Zhang, X., Zhang, Y., Long, D., Xie, W., Dai, Z., Tang, J., Lin, H., Yang, B., Xie, P., Huang, F., et al. mgte: Generalized long-context text representation and reranking models for multilingual text retrieval. arXiv preprint arXiv:2407.19669, 2024. A. Semantic Prompt Cache Architecture Adaptive Semantic Prompt Caching with VectorQ The semantic prompt cache architecture in Figure 7 integrates VectorQ to optimize cache hit or cache miss decisions. Figure 7. Semantic prompt cache architecture where VectorQ replaces the static similarity threshold. Upon receiving request, the system transforms it into vector embedding, which the similar answer extractor uses to query the vector database for the nearest neighborrepresenting the most semantically similar previously processed prompt. Users can select metrics such as cosine similarity, Euclidean distance, or other available similarity measures to compute this similarity. VectorQ evaluates the retrieved neighbor using its dynamic threshold mechanism, which adapts based on embedding-specific threshold regions. If the similarity surpasses the threshold, the cached response linked to the neighbor is reused; otherwise, the request is forwarded to the inference server to generate new response. B. Datasets and Prompt Construction Semantic prompt caching requires the dataset to consist of clusters where the entries in each cluster map to the same LLM response. Otherwise, if all entries in dataset have distinct responses, it is impossible to reuse them. We identified three task categories that satisfy this requirement. 1) Classification. Classification tasks often involve mapping variable amount of input data to finite set of categories. For this experiment, we use the E-Commerce Text Classification dataset (Saurabh Shahane, 2023), which assigns product descriptions to one of four categories: Books, Electronics, Household, and Clothing & Accessories. To ensure balanced representation, we shuffle the dataset to distribute all categories evenly. The prompt for classification is structured as follows: { } \"prompt\": \"Which category does the text belong to?\", \"output_format\": \"Answer with Books, Electronics, Household, or Clothing & Accessories only\", \"sentence\": \"{row}\" Additionally, we use the CommonsenseQA (Talmor et al., 2018) dataset, which assigns questions to question categories. To ensure balanced representation, we shuffle the dataset to distribute all categories evenly. The prompt for CommonsenseQA is structured as follows: { \"prompt\": \"What is the main subject of the following question?\", 11 Adaptive Semantic Prompt Caching with VectorQ \"output_format\": \"Answer with only one of the words of this set: [people, small dog, cat, car, children, weasel, water, doing homework, human, shark, chatting with friends, student, bald eagle, fox, food, snake, ficus, potato, driving car, monkey, animals, apple tree, horse, crab, lizard, person, competing, killing]\", \"sentence\": \"{row}\" } 2) Sentiment. Sentiment classification maps variable amount of input data to finite set of possible sentiments. While NLP-based methods, such as those proposed by (Dang et al., 2020), outperform LLM-based sentiment analysis in terms of latency, this remains relevant scenario, as sentiment analysis is used in semantic prompt caches within Retrieval-Augmented Generation (RAG) systems (Zhang et al., 2023). For this experiment, we use the Amazon Instant Video Review dataset (Ni et al., 2019) and prompt the question: Is this review friendly?. To ensure unbiased results, we shuffle the dataset to create an even distribution of friendly and unfriendly reviews. The sentiment prompt is structured as follows: { } \"prompt\": \"Is this review friendly?\", \"output_format\": \"Answer with yes or no only\", \"sentence\": \"{row}\" 3) Chatbot User Prompts. We utilize the ComQA dataset (Rogers et al., 2023), which comprises set of semantically distinct user questions, to simulate scenario where reuse is not feasible. Rather than altering the dataset by generating semantically similar questions to evaluate reuse capabilities in chatbot scenario, we focus on the distinct entries to test whether the threshold can effectively distinguish non-reusable responses. The chatbot user prompt is structured as follows: { } \"prompt\": \"Answer the following question.\", \"output_format\": \"Concise.\", \"sentence\": \"{row}\" C. The Problem of Static Similarity Thresholds To analyze the relationship between similarity values and cache hit correctness, we designed an experiment with the following setup: Every processed candidate is added to the vector database, ensuring the database contains all previously seen prompts. For each new candidate, we retrieve its nearest neighbor from the database. We validate whether the cached response for the nearest neighbor character matches the response that would have been generated for the candidate. This allows us to classify cache hits as either correct or incorrect. For each prompt, we record the similarity value between the new prompt and its nearest neighbor, along with label indicating whether the cache hit is correct or incorrect. We generate distributions of similarity values for both correct and incorrect cache hits. These distributions are visualized as kernel density functions (KDFs) in Figure 8. We evaluate using 400 samples from the Amazon Instant Video Review dataset (Ni et al., 2019), the E-Commerce Text Classification dataset (Saurabh Shahane, 2023), and the Semantic Prompt Caching Benchmark (Appendix E). For static threshold to be effective, the kernel density function (KDF) for correct cache hits would need to be shifted to the right relative to the KDF for incorrect cache hits. This shift would indicate that correct cache hits consistently require higher embedding similarity values, allowing threshold to reliably distinguish between correct and incorrect responses. However, as shown in Figure 8, the results reveal substantial overlap between the correct and incorrect KDFs across all three datasets(a) the Amazon Instant Video Review dataset (Figure 8a), (b) the E-Commerce dataset (Figure 8b), and (c) the Semantic Prompt Caching Benchmark (Figure 8c). These findings underscore the inherent limitations of static thresholds. To address this, we propose embedding-specific and dynamic threshold regions that adapt to the characteristics of individual embeddings. 12 Adaptive Semantic Prompt Caching with VectorQ (a) Amazon Product Review Dataset (b) E-Commerce Dataset (c) Semantic Prompt Cache Benchmark Figure 8. Kernel Density Functions (KDFs) for correct (green) and incorrect (red) cache hits. D. Threshold Regions: Convergence Speed The threshold region convergence ensures that similarity values causing incorrect cache hits for an embedding are gradually added to R1 nn. Over time, this process excludes unreliable similarity values, based on an embeddings history of correct and incorrect cache hit rates. When cache hit is classified as incorrect, the associated similarity value between the candidate nn, all similarity values in R3 and its nearest neighbor nn is added to R1 nn that are less than or equal to are transferred to R1 nn (Line 23 in Algorithm 1). Because similarity values are never removed from R1 nn, and the three threshold regions remain non-overlapping, R1 nn expands monotonically toward the upper limit of 1.0, the maximum possible similarity value. At convergence, where R1 nn spans the entire range between 0.0 and 1.0, only candidates with similarity value of exactly 1.0 (perfect matches) result in cache hits. This ensures that all unreliable similarity values are excluded, effectively eliminating incorrect cache hits for the embedding. nn. If is also in the range of R3 nn and removed from R3 The convergence speed of R1 incorrect, their associated similarity values, the region they belong to, and the embedding-specific correctness posterior. nn for given embedding is influenced by four factors: the number of cache hits classified as Impact of Incorrect Cache Hits: R1 associated with each incorrect cache hit is added to R1 as the number of incorrect cache hits increases, R1 nn expands only when cache hit is classified as incorrect. The similarity value nn. Thus, nn, provided it exceeds the largest value currently in R1 nn continues to grow. Magnitude of Similarity Values: The rate of R1 nns convergence depends on the similarity values associated with incorrect cache hits. Higher similarity scores associated with incorrect cache hits lead to faster convergence toward 1.0. For example, if candidate and its nearest neighbor have cosine similarity of 0.95 and the cached response results in an incorrect cache hit, R1 nn expands to the range [0.0, 0.95]. Region of the Similarity Value: If similarity value associated with an incorrect cache hit lies within R1 nn or R3 nn and removed from R3 not expand, as the value is already classified as insufficient. If the similarity value lies within R2 in R3 Algorithm 1). nn that are less than or equal to the incorrect value are transferred to nn, R1 nn does nn, all values nn (Line 23 in Role of the Correctness Posterior: Cache hits in R1 nn are always re-evaluated for correctness. However, in R3 nn, correctness sampling determines whether cache hit is re-evaluated. The correctness posterior pinn(s) of an embedding is updated for each similarity value that is re-evaluated and enables the identification of similarity values likely to cause incorrect cache hits. Higher pinn(s) values prioritize re-evaluation, accelerating R1 nns convergence for uncertain similarity values. The uniform random sampling ensures probabilistic fairness by occasionally re-evaluating even similarity values with low posterior likelihoods. nn and R2 The convergence of R1 nn ensures that similarity values leading to incorrect cache hits are progressively excluded, refining the reliability of cache hits over time. The convergence speed depends on how frequently incorrect cache hits occur, their similarity values, and the prioritization of uncertain regions for correctness re-evaluations based on the embedding-specific correctness posterior. E. Semantic Prompt Caching Benchmark Adaptive Semantic Prompt Caching with VectorQ Semantic prompt caching aims to enhance the efficiency of large language model (LLM) inference by reusing responses for semantically similar prompts. We construct comprehensive semantic prompt caching benchmark using datasets across sentiment analysis, classification, and chatbot user queries. Each entry in the benchmark is designed to test the ability of caching systems to distinguish between reusable and non-reusable responses. An example entry from the benchmark is shown below: Each benchmark entry consists of the following fields: id: unique identifier corresponding to the original dataset entry. task: The specific task associated with the entry, such as sentiment classification or user query response. output format: The standardized format for the output to ensure consistency during evaluation. sentence: The row from the corresponding dataset. answer: The ground-truth response generated using LLaMA-3.1-8B. global id: universal identifier for aggregating tasks and analyzing results. This is an example entry: { } \"id\": \"video_review_1771\", \"task\": \"Is this review friendly?\", \"output_format\": \"Answer with yes or no only\", \"sentence\": \"This last season was ridiculously bad. So disappointed that actually paid for any of it. The last episodes were the worst.\", \"answer\": \"no\", \"global_id\": 3 The benchmark integrates 2,000 rows from the E-Commerce dataset (Saurabh Shahane, 2023), 1,000 rows from the CommonsenseQA dataset (Talmor et al., 2018), 1,000 rows from the ComQA dataset (Rogers et al., 2023), and 2,000 rows from the Amazon Instant Video Review dataset (Ni et al., 2019). Each dataset is balanced to ensure fairness and unbiased evaluation, with an equal occurrence of distinct responses. Additionally, the datasets are shuffled to simulate alternating context changes. F. Evaluation: Threshold Selection To assess the performance of embedding-specific threshold regions versus static thresholds, we choose range of uncertainty gates and thresholds for the experiments shown in Figure 4. For VectorQ, we select all possible uncertainty gates from the interval [0.0, 1.0] with step size of 0.1, demonstrating that VectorQ consistently outperforms all static thresholds regardless of configuration. Details on the correctness sampling parameter are provided in Section 5.3. For the Amazon Product Review Dataset (Figure 4a), we select static thresholds in the range [0.7, 0.88] (step size 0.02). Thresholds below 0.7 double the error rate compared to VectorQ, while thresholds above 0.88 lead to no reuse due to excessive strictness. Similarly, for the E-Commerce Dataset (Figure 4b), we select static thresholds in the range [0.72, 0.9]. Below 0.74, error rates double, and above 0.9, caching is entirely disabled. For the Semantic Prompt Cache Benchmark (Figure 4c) we select thresholds in the range [0.74, 0.9], with 0.74 tripling the error rate at equivalent cache hit rates, and thresholds above 0.9 disabling reuse entirely. For each threshold, we process the first 600 samples of the corresponding dataset. These intervals and sample sizes ensure comprehensive and fair comparisons. G. Ablation Study: Correctness Sampling This study investigates the necessity of Bayesian inference-based correctness sampling in comparison to simpler uniform sampling approach. Bayesian sampling leverages correctness posteriors to prioritize re-evaluations of cache hits with 14 Adaptive Semantic Prompt Caching with VectorQ higher uncertainty and their corresponding similarity value location in Region 3, allowing localized certainty regions to guide adaptive reuse decisions. In contrast, uniform sampling treats all similarity values in Region 3 equally, ignoring threshold-specific uncertainty. While Bayesian sampling may seem excessive, this ablation study demonstrates its important role in enabling reliable and adaptive threshold re-evaluations. (a) Amazon Product Review Dataset (b) E-Commerce Dataset (c) Semantic Prompt Cache Benchmark Figure 9. Performance comparison of VectorQ with Bayesian correctness sampling, VectorQ with uniform sampling, and static-threshold approaches across two datasets and the Semantic Prompt Caching benchmark. Figures (a), (b), and (c) plot the error rates against cache hit rates, demonstrating that VectorQ is more robust across diverse datasets, reliably handling varying prompts from different contexts. Uniform sampling exhibits inconsistent behavior, sometimes outperforming the static threshold or Bayesian-based approach and other times underperforming both. These results emphasize the advantages of Bayesian inference-based correctness sampling for reliable semantic prompt caching. As shown in Figure 9, Bayesian sampling exhibits greater robustness across datasets, achieving lower error rates or higher cache hit rates compared to both uniform sampling and static thresholds. Uniform sampling, while occasionally matching or slightly outperforming the static threshold approach, exhibits inconsistent performance. These results emphasize the importance of incorporating localized threshold certainty measures, as enabled by the correctness posterior."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Stanford University",
        "Technical University of Munich",
        "University of California, Berkeley"
    ]
}