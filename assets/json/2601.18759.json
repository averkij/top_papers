{
    "paper_title": "UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing",
    "authors": [
        "Junling Wang",
        "Hongyi Lan",
        "Xiaotian Su",
        "Mustafa Doga Dogan",
        "April Yi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Designing user interfaces (UIs) is a critical step when launching products, building portfolios, or personalizing projects, yet end users without design expertise often struggle to articulate their intent and to trust design choices. Existing example-based tools either promote broad exploration, which can cause overwhelm and design drift, or require adapting a single example, risking design fixation. We present UI Remix, an interactive system that supports mobile UI design through an example-driven design workflow. Powered by a multimodal retrieval-augmented generation (MMRAG) model, UI Remix enables iterative search, selection, and adaptation of examples at both the global (whole interface) and local (component) level. To foster trust, it presents source transparency cues such as ratings, download counts, and developer information. In an empirical study with 24 end users, UI Remix significantly improved participants' ability to achieve their design goals, facilitated effective iteration, and encouraged exploration of alternative designs. Participants also reported that source transparency cues enhanced their confidence in adapting examples. Our findings suggest new directions for AI-assisted, example-driven systems that empower end users to design with greater control, trust, and openness to exploration."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 2 ] . [ 1 9 5 7 8 1 . 1 0 6 2 : r UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing Junling Wang ETH Zurich Zurich, Switzerland junling.wang@ai.ethz.ch Hongyi Lan ETH Zurich Zurich, Switzerland honlan@student.ethz.ch Xiaotian Su ETH Zurich Zurich, Switzerland xiaotian.su@inf.ethz.ch Mustafa Doga Dogan Adobe Research Basel, Switzerland doga@adobe.com April Yi Wang ETH Zurich Zurich, Switzerland april.wang@inf.ethz.ch Figure 1: An overview of UI Remix, an example-driven assistant for mobile UI design featuring three main panels: (A) Conversation Panel: Users describe their design goals and interact with the system through three modes: Chat (generate and refine UI designs), Search (retrieve relevant UIs for inspiration), and Apply (adapt entire design through global remix or specific components through local remix from selected examples). (B) Example Gallery: Displays retrieved real-world UI examples along with source transparency cues (e.g., ratings, download counts, developer information) to help users assess credibility. (C) Editable Canvas: Presents live preview of the current design, supporting toggling between visual and code views. These authors contributed equally to this work."
        },
        {
            "title": "Abstract",
            "content": "Designing user interfaces (UIs) is critical step when launching products, building portfolios, or personalizing projects, yet end This work is licensed under Creative Commons Attribution 4.0 International License. IUI 26, Paphos, Cyprus 2026 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1984-4/2026/03 https://doi.org/10.1145/3742413.3789154 IUI 26, March 2326, 2026, Paphos, Cyprus Junling Wang et al. users without design expertise often struggle to articulate their intent and to trust design choices. Existing example-based tools either promote broad exploration, which can cause overwhelm and design drift, or require adapting single example, risking design fixation. We present UI Remix, an interactive system that supports mobile UI design through an example-driven design workflow. Powered by multimodal retrieval-augmented generation (MMRAG) model, UI Remix enables iterative search, selection, and adaptation of examples at both the global (whole interface) and local (component) level. To foster trust, it presents source transparency cues such as ratings, download counts, and developer information. In an empirical study with 24 end users, UI Remix significantly improved participants ability to achieve their design goals, facilitated effective iteration, and encouraged exploration of alternative designs. Participants also reported that source transparency cues enhanced their confidence in adapting examples. Our findings suggest new directions for AI-assisted, example-driven systems that empower end users to design with greater control, trust, and openness to exploration. CCS Concepts Human-centered computing Interactive systems and tools."
        },
        {
            "title": "Keywords",
            "content": "Example-driven UI design, Multimodal retrieval-augmented generation, End-user design, UI example retrieval, Interactive design systems ACM Reference Format: Junling Wang, Hongyi Lan, Xiaotian Su, Mustafa Doga Dogan, and April Yi Wang. 2026. UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing. In 31st International Conference on Intelligent User Interfaces (IUI 26), March 2326, 2026, Paphos, Cyprus. ACM, New York, NY, USA, 17 pages. https://doi.org/10.1145/3742413."
        },
        {
            "title": "1 Introduction",
            "content": "Designing User Interfaces (UIs) requires translating abstract ideas into concrete visual and interactive forms, skill that traditionally demands both creative expertise and technical proficiency with design software. However, an increasing number of end-user designers, such as small business owners, students, hobbyists, and content creators, now create UIs using AI-powered tools [32, 55]. For these users, interface design is not specialized profession but an expressive means to communicate ideas, promote services, or prototype concepts quickly [77]. While AI assistance has lowered the barrier to producing interfaces, it has not eliminated the difficulty of articulating what one wants to create. End-user designers can easily generate layouts, but they often struggle to specify visual intent beyond broad impressions [52, 53, 68]. For instance, small business owner might know they want modern or user friendly UI but find it difficult to translate these abstract impressions into specific design choices. Recent LLM-based design tools show promise in translating loosely specified prompts into plausible initial designs [8]. However, even when an AI system produces design that matches such prompts or generates rationale explaining why the design is modern or user-friendly, end-user designers may still struggle to judge its quality. Lacking the design literacy to critically evaluate or justify aesthetic choices, they may hesitate to trust the systems output or commit to particular direction [46, 74]. more intuitive way to build confidence is through socially grounded evidence rather than abstract AI rationales. When users can see where design comes from who made it, how it has been received, and in what context it provides familiar and trustworthy signal of quality, much like trusting product recommended by respected expert. promising direction to overcome these challenges is to leverage real-world UI examples. These examples can help users articulate their design intent by offering concrete references that make abstract preferences more tangible and comparable. Presenting traceable examples to users could also enhance trust in design decisions by enabling users to evaluate these choices through transparent source information [17, 70]. Using existing UI examples to support UI design has gained significant focus in Human-Computer Interaction (HCI) [6, 7, 14, 24, 27, 48, 57, 63, 82]. Current research generally falls into two categories. The first category employs bottom-up, serendipitous approach, inspired by UI designer community platforms such as Mobbin [56], Dribbble [15] and Behance [2], where designers explore broad collection of examples to spark inspiration without pre-defined goal [7, 63]. While this broad search space encourages discovery, it can be overwhelming and may lead to design drift [57], where designs gradually deviate from their original goals due to ad-hoc choices during exploration. The second category utilizes top-down, targeted approach, where inspirational tools find examples based on specific design inputs, such as sketches or existing layouts [3, 39, 57, 63, 71]. For instance, Park et al. [62] introduced multimodal search framework that extracts semantics from input UI images and retrieves relative UI designs examples based on semantic similarity. These UI examples offer broad search space, requiring users to browse through many examples. Although targeted retrieval reduces search effort, it still relies heavily on users ability to formulate precise inputs and offers limited support for adapting retrieved examples into personalized designs. More recent work such as Lu et al [47], explores how users can blend provided examples into generative workflows, yet such systems require users to supply initial examples rather than retrieve them from large-scale repositories. However, this approach depends on users providing desirable example as input, which can be particularly challenging for those who struggle to define their expectations, and may expose them to narrow range of similar styles, leading to design fixation [29, 57]. These limitations raise central question: How can we design system that helps end users articulate their UI intentions while offering traceable justifications for design choices? In response to this, we propose UI Remix, system that supports end users in designing UIs by helping them articulate their design ideas through traceable UI examples. We refer to traceable UI examples as retrieved interfaces whose sources, context, and quality indicators are transparent to the user, allowing them to understand where design came from and why it was recommended. At the core of UI Remix is an example-driven exploration paradigm, where users iteratively search, select, and adapt retrieved examples to progressively shape their designs. UI Remix leverages Multimodal UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing IUI 26, March 2326, 2026, Paphos, Cyprus Retrieval-Augmented Generation (MMRAG) model to retrieve UI examples based on users text queries and their selected UI components, ranging from the whole interface to single component. The process begins with users generating an initial UI by simply describing their needs through natural language query. They can then adapt this initial UI to different styles by providing another query that searches for UI examples relevant to their needs, upon which UI Remix returns diverse set of example UIs. Users can browse these examples, select those they find satisfactory, and adapt styles, layouts, colors, or other attributes into their current design process we call global remix. Beyond global remix, users can also select specific components within their current design, retrieve new UI examples relevant to the selected part, and adapt styles or attributes into that component process we call local remix. In the following iterations, users can freely use global and local remix to help design their own UI. Drawing from social transparency theory [17, 70], we highlight key attributes of retrieved UI examples, such as ratings, download counts, categories and developer information, to make their sources and context visible. This transparency enables users to assess the relevance and applicability of each design, fostering trust and critical evaluation in the systems suggestions. While UI Remix is technically capable of generating interactive and multi-screen UIs, our work focuses on supporting the early-stage prototyping of static single-screen UI designs. To evaluate UI Remixs effectiveness in supporting end users during mobile UI design, we conducted within-subjects lab study with 24 participants. Our analysis showed that UI Remix significantly improved participants ability to achieve their design goals, facilitated effective iteration, and encouraged exploration of alternative designs. Participants reported greater willingness to use UI Remix for personal UI designs, attributing these gains to the example-driven workflow and source transparency cues. The retrieval and presentation of real-world UI examples encouraged active exploration and inspiration, while the systems iterative refinement capabilities supported flexible, goal-oriented design improvement. Source transparency cues such as ratings and developer information further enhanced users trust and justification of design decisions. Building on these findings, we derived several design implications for developing example-driven AI systems that enable end users to design with greater control, trust, and creativity. We also reflected on UI Remixs design philosophy, emphasizing the role of transparent, example-driven interactions in fostering humanAI collaboration in creative design contexts. In summary, our work makes the following contributions: (1) UI Remix, an interactive system that employs an example-driven exploration paradigm, grounding UI designs in traceable, real-world examples to enhance intent articulation and user trust. (2) An empirical evaluation demonstrating that UI Remix effectively supports end users in articulating their design intent and enhances users trust in design decisions."
        },
        {
            "title": "2 Related Work\n2.1 Using Examples in UI Designs",
            "content": "Getting inspiration from existing UI examples is common practice in UI design [6, 7, 14, 24, 27, 34, 35, 48, 57, 63]. Designers frequently seek inspiration from existing designs to understand best practices, explore styles, and refine design concepts [47]. Gallery-based approaches such as Mobbin [56], Dribbble [15], and Behance [2] provide large-scale visual archives that enable browsing across diverse interface styles. These repositories support open-ended exploration, but they often require extensive manual searching and rely on subjective interpretation. In parallel, prior researches have explored retrieval and generation-based approaches that automatically present design examples based on user input [6, 7, 27, 57, 63]. Early approaches employed keyword-based retrieval [7, 63], while more recent work leverages deep generative networks and multimodal representations [9, 10, 47, 73, 80] to recommend or synthesize examples aligned with textual or visual cues. Other studies extend example use through style or layout transfer between UIs [4, 20, 37, 47, 58, 59, 72, 78], allowing designers to apply desired designs from one interface to another. While these approaches inspire users with existing examples, each category has limitations. The gallery-based approach offers broad search space but can overwhelm users and lead to design drift [57]. Retrieval and generative systems require users to provide clear examples or descriptions [6, 47], which can be particularly challenging for end users who struggle to define their expectations. This reliance on explicit input can also lead to design fixation [29, 57], as users may focus too narrowly on the few examples they can describe. Moreover, few systems make example sources transparent or traceable [17, 70], limiting users ability to evaluate credibility and relevance of examples."
        },
        {
            "title": "2.2 Supporting End Users in UI Designs",
            "content": "While traditional interface design tools are primarily intended for professional designers, recent research in HCI has increasingly focused on empowering end users without formal design training to create user interfaces. These studies seek to lower the entry barrier to UI design by allowing users to express intent and customize the UI through accessible interactions such as direct manipulation or natural language [32, 40, 42]. Early research in this direction focused on enabling end users to restyle or extend existing interfaces through lightweight interactions. Systems such as Stylette [32] demonstrate how users can restyle UI designs using natural language, while subsequent work such as ProgramAlly [25] enables end users to design and customize interfaces through multimodal instructions and demonstrations. Additional research leverages existing UI examples to support creative exploration [62, 63], showing how example galleries and inspirational search can help users draw ideas and inspiration for their own designs. Expanding on these approaches, other studies have introduced AI-assisted recommendation frameworks that guide end users during the design process [12, 40, 65]. For example, GUIComp [40] integrates with GUI design software to provide real-time, multi-faceted feedback on users interface prototypes, helping them iteratively refine layouts and improve overall usability. Similarly, DeepInventor [12] employs deep learning to convert hand-drawn interface sketches into runnable wireframes in App Inventor, supporting end users in rapidly prototyping mobile UIs. Such systems exemplify IUI 26, March 2326, 2026, Paphos, Cyprus Junling Wang et al. Table 1: Summary of prior systems on multimodal retrieval, remix features, and target users. indicates the feature is supported. System Multimodal Retrieval Remix Target User Keywordbased Natural Language-based Retrieval Type Global Local Misty [47] Gallery D.C. [7] VINS [6] InspirationSearch [62] GANSpiration [57] UIClip [80] GUing [79] Swire [27] d.tour [63] GUI2WiRe [35] RaWi [34] UI Remix (this work) text-to-text image-to-image text-to-text text-to-image text-to-text, textto-image, sketchto-image sketch-to-image text-to-image text-to-text text-to-text text-to-image Professional Designers End User how AI can provide guidance in UI design while preserving user control and interpretability. In summary, while existing works lower the barrier for end users in UI designs, they rarely support the articulation of design intent through UI examples or provide transparent justifications for design decisions. Building on this insight, our system focuses on supporting intent articulation through traceable, example-driven design exploration and remixing."
        },
        {
            "title": "2.3 AI Support for UI Designs",
            "content": "Recent advances in AI have transformed how designers and end users create and iterate on user interfaces. Extensive research has examined how AI can support UI design workflows [31, 33, 36, 41, 44, 50, 51]. Industrial systems from OpenAI, Figma, Stitch, Magic Patterns, and Vercel1 have further demonstrated the practical potential of AI for automating UI design tasks. However, these tools largely rely on conversational interfaces, where users must articulate their goals through precise natural language instructions, which poses challenges for end users who struggle to express abstract UI preferences. While systems like Figma rely on similarity-based find more like this workflows that retrieve similar UIs and require manual import and remixing, our system supports preference-driven retrieval from broader set of real-world app UIs and enables direct remixing of selected designs into the current prototype. In the research domain, large multimodal models have shown strong capabilities in transforming between textual, visual, and code representations of UIs [9, 16, 43, 47, 49, 69, 81]. Research such as Design2Code [69] and UICoder [81] translates UI screenshots into executable code, while others generate UI layouts from natural language descriptions [38, 43, 45, 49, 83]. More recent works integrate multimodal reasoning for creative tasks combining layout understanding, component recognition, and style adaptation to produce 1Stitch: https://stitch.withgoogle.com/; Magic Patterns: https://www.magicpatterns. com/; Vercel: https://vercel.com/templates/material-ui coherent interface outputs [9, 47]. These approaches demonstrate AIs generative strength but typically operate as one-shot translators from user input to UI output, offering limited transparency into how or why particular design is produced. Our work builds on prior research while shifting from generationonly paradigms to retrieval-augmented, example-driven exploration. UI Remix leverages MMRAG model to present real-world UI examples that help users articulate, refine, and justify their design choices. By highlighting source transparency cues, such as ratings, developer information, and download counts, UI Remix makes design decisions more interpretable and trustworthy. As summarized in Table 1, our system specifically targets end-user designers and uniquely combines natural-language-based text-to-image multimodal retrieval with both global and local remix functions. In doing so, our system extends the role of AI from generative assistant to transparent collaborator that scaffolds intent articulation through traceable, example-grounded interactions."
        },
        {
            "title": "3 UI Remix System Design",
            "content": "We introduce UI Remix, system that supports end users in designing mobile UIs by helping them articulate their design ideas through traceable UI examples. We first present the design motivations that guided the development of UI Remix, grounded in insights from prior literature."
        },
        {
            "title": "3.1 Design Motivations",
            "content": "Support Intent Articulation through Example-Guided Explo3.1.1 ration. End users struggle to articulate their design intent due to the implicit and ill-defined nature of their preferences [52, 53]. Traditional example-based UI inspiration tools either present users with wide collection of designs [2, 15] or expect them to provide high-quality example to initiate targeted searches [3, 39, 57, 63, 71]. However, both strategies fall short when users cannot clearly define their needs. UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing IUI 26, March 2326, 2026, Paphos, Cyprus This challenge highlights the need for systems that can scaffold the articulation of vague or intuitive preferences into concrete design goals. In response, UI Remix is designed to help users gradually form and articulate their intent by iteratively searching for relevant UI examples and adapting these designs to their own UIs. The global remix supports early-stage ideation by exposing users to diverse UI examples, while the local remix enables targeted refinement of specific design aspects. This example-driven approach helps users both articulate and evolve their design goals over time. 3.1.2 Enable Trustworthy and Justifiable Design Choices. Even when users are presented with suitable UI examples, they may hesitate to incorporate them into their own designs if the rationale behind those examples is unclear [26]. Prior tools often lack mechanisms to make the source of design examples transparent [2, 15], making users question whether the suggestions are appropriate for their goals. To address this, our design is informed by principles from social transparency theory [70]. Social transparency emphasizes the importance of making the intentions, identities, and actions of others visible, helping users understand the social context of information and thereby enhancing trust [17, 70]. UI Remix is intended to present attributes such as ratings, download counts, categories, and developer information alongside each retrieved example, allowing users to assess their relevance and traceability. This transparency is envisioned to build user confidence in design decisions and encourage meaningful adaptation of retrieved examples. 3.1.3 Supporting Focused Exploration and Reducing Design Drift. Example-driven UI exploration can lead to design drift, where users lose track of their original design goals because they get distracted by browsing many unrelated examples [57]. This phenomenon is especially common in unstructured environments like design galleries [2, 15]. Conversely, narrowly scoped tools may induce design fixation, leading users to converge prematurely on small set of similar solutions [29]. UI Remix seeks to balance breadth and focus by offering both global and local remix functions. Global remix is designed to encourage broad inspiration, while local remix supports the targeted refinement of specific components. This structure is motivated by the need to help users stay focused on their evolving design goals during exploration. By enabling iterative selection and blending of examples, UI Remix supports focused exploration while preserving user control over design direction."
        },
        {
            "title": "3.2 UI Remix Overview",
            "content": "As shown in Figure 1, UI Remix supports end users in designing UIs through an example-guided workflow powered by MMRAG model. The interface consists of three main panels. The Conversation Panel (Figure 1A) allows users to describe their intended interface using free-form natural language. The Example Gallery (Figure 1B) displays relevant UI examples retrieved based on the users query and selected components. Each example is accompanied by metadata such as ratings, download counts, etc to help users assess its credibility. The Editable Canvas (Figure 1C) presents live preview of the generated interface, where users can customize and refine their design through direct interaction with individual components. UI Remix supports flexible, example-driven workflow that integrates three complementary modes rather than fixed linear sequence: Chat Mode: In this mode, users can converse with our system to generate and iteratively refine their UI designs. Through natural-language prompts, they can produce an initial complete UI or modify existing UI within the Editable Canvas. Search Mode: In this mode, users can search for real-world UI designs using natural language queries. Retrieved designs are displayed in the Example Gallery alongside their metadata. If user selects UI element in the Editable Canvas, the system focuses retrieval on that specific component. Apply Mode: In this mode, users can remix retrieved UI designs into their own. They can provide natural language instructions to specify how the retrieved design should be applied to their existing UI while also highlighting elements from the selected example to indicate which parts to incorporate. When specific UI element on the Editable Canvas is selected, the remix operation updates only that element. This combination of high-level inspiration and fine-grained control allows users to construct custom UIs without writing code. Users can seamlessly switch between modes, with all interactions recorded in the Conversation Panel to ensure transparency and traceability."
        },
        {
            "title": "3.3 Target Users and System Walkthrough",
            "content": "UI Remix is designed for end user designers: non-professional creators such as small business owners, students, and hobbyists who engage in interface design as means to an end (e.g., promoting content, prototyping ideas), rather than as their primary occupation. This notion extends prior work in end user UI design, emphasizing the creative and visual aspects of design rather than programming. End users often rely on UI references or existing templates but face challenges when translating their ideas into concrete UIs [40]. UI Remix offers an example-driven workflow to support these users in exploring design possibilities, understanding design components, and assembling custom UIs without requiring direct manipulation of code. It assumes that users are self-directed and curious, and prefer working iteratively with visual examples to shape their creative intent. We now walk through an example of how user, Alice, restaurant owner designing mobile app for her business, might use UI Remix to create her UI: Alice begins in the Conversation Panel (Figure 1A), where she enters natural language query: mobile UI for Chinese restaurant, with menu section including dishes:.... Upon submission, the system generates the corresponding code and provides UI preview in the Editable Canvas (Figure 1C), where she can toggle between the preview and the generated code. Although the current UI contains all the information she needs, Alice is not satisfied with its color scheme. She wants to see how existing restaurant UIs look, so she enters new query in the Conversation Panel (Figure 1A), switches to search mode, and retrieves gallery of real-world UI examples in the Example Gallery IUI 26, March 2326, 2026, Paphos, Cyprus Junling Wang et al. (Figure 1B), drawn from mobile UI designs that semantically align with her input. As she browses the gallery, Alice discovers several UIs with engaging color schemes. To prioritize credible and relevant examples, she consults the Design Traceability features ratings, download counts, and developer information which provide useful signals about the quality and popularity of each UI. She selects one example, switches to apply mode in the Conversation Panel (Figure 1A), and specifies how she would like to adapt the current design (e.g., applying the color scheme). Upon submission, the system updates the UI in the Editable Canvas (Figure 1C). We call this process global remix. Next, Alice decides that the reservation button design could be further improved. To refine the UI, she clicks on the button in the Editable Canvas and types her query stylish red reservation button into the Conversation Panel, switching to search mode. The system presents set of reservation button variations from existing UI designs. She selects UI example featuring rounded red button, zooms in on the design, and marks the button she likes by drawing circle around it. Then, she types use this style in the Conversation Panel and switches to the apply mode to integrate the buttons style into her current design. We call this process local remix. Alice performs several rounds of global and local remix iteratively until she is fully satisfied with her UI. This example illustrates how UI Remix balances creative freedom with example guidance, enabling novice users to design UIs with confidence."
        },
        {
            "title": "3.4 Global Remix",
            "content": "3.4.1 UI Retrieval. Users begin by describing their intended interface in natural language through the Conversation Panel (Figure 1A). UI Remix then retrieves diverse set of UI examples from curated dataset of mobile UIs. These examples are ranked by semantic similarity to the users query using the underlying MMRAG model. The retrieved results are displayed in the Example Gallery (Figure 1B), where each UI example is shown alongside traceability metadata such as ratings, download counts, comment counts, and categories. These cues help users assess the credibility and relevance of each example. Users can also zoom in on UI example to view larger version, along with additional source metadata including the developers information and app name. 3.4.2 UI Example Selection and Remixing. Users browse the retrieved examples and select those that align with their design goals. Upon selecting an example, they can specify how they wish to adapt it, such as modifying the color scheme or layout, by entering request in the Conversation Panel. After user selects an example, the system automatically switches to apply mode. Once the user submits their request, UI Remix generates the corresponding HTML/CSS code and displays live UI preview on the Editable Canvas (Figure 1C). Users can toggle between the code and the rendered preview to inspect or refine the layout. The preview is fully editable, enabling users to continue building on the layout or initiate local remix for further customization."
        },
        {
            "title": "3.5 Local Remix",
            "content": "3.5.1 UI Component Retrieval. Local remix supports the refinement of specific components within the current UI design. By entering query in the Conversation Panel (e.g., stylish red button), users trigger component retrieval process that presents design variations of the queried component from existing UI examples. This retrieval shares the same backend retrieval pipeline as UI retrieval within global remix. The retrieved results are displayed in the Example Gallery, where each variation of the selected component is shown alongside traceability metadata. 3.5.2 UI Example Selection and Remixing. Users can review the retrieved component variations and select one at time. When applying retrieved variation, users may click on corresponding UI element in the Editable Canvas to indicate where the change should be applied. Similar to the global workflow, they can issue natural language commands, such as use this color, to adapt the chosen variation into their current design. Users may also zoom in on retrieved variations and highlight the specific part they wish to adapt by directly drawing on the example UI. Once confirmed, the system switches to apply mode in the Conversation Panel, and upon submission updates the relevant portion of the code and the UI preview accordingly. This fine-grained, component-level editing enables users to iterate efficiently without restarting their entire design. Multiple rounds of local refinement can be performed, and users can flexibly alternate between global and local remix as their design evolves."
        },
        {
            "title": "3.6 Technical Architecture",
            "content": "UI Remix is UI design system powered primarily by MMRAG framework. As illustrated in Figure 2, UI Remix is designed to support three modes of interaction (Chat, Search, Apply) through two core modules: the retriever and the generator. This section introduces the data source utilized in the pipeline and provides brief overview of both modules. 3.6.1 Data Source and Preprocessing. Our data sources are the Mobbin [56], Interaction Mining [28] and the MobileViews [22] dataset. The Mobbin is an online UI screenshot browsing platform that provides UI screenshots of mobile apps and websites. Interaction Mining is online platform for managing and sharing mobile interaction dataset, which provides UI screenshots of the track implementing various operations. MobileView is large-scale dataset designed for research on mobile UI analysis and mobile agents, which contains more than 600K UI screenshot-view hierarchy pairs from more than 20,000 applications. The metadata of the applications are manually collected from the Google Play Store. We selected 196 popular mobile applications across diverse categories, including Food, Travel, Lifestyle, News, Education, etc. Subsequently, we collected corresponding UI screenshots from the Mobbin, Interaction Mining, and MobileViews datasets, yielding around 900 unique interface screenshots that served as the example set to support users during the design process. Although we did not collect UI data for all app categories due to resource constraints, our pipeline can easily scale to larger datasets by providing screenshots and corresponding metadata. We then processed the collected UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing IUI 26, March 2326, 2026, Paphos, Cyprus Figure 2: MMRAG Pipeline. The process begins with preprocessing UI screenshots from the Mobbin [56], Interaction Mining [28], and MobileViews [22] datasets, along with metadata from the Google Play Store. Each screenshot is linked to its corresponding metadata through indexing. Next, the UI screenshots are passed through an encoder to generate embeddings. The embeddings, together with the corresponding image content, are stored in ChromaDB. When user submits query, it is converted into embeddings, and the system retrieves and ranks relevant screenshots using cosine similarity. The retrieved UIs, along with their metadata, are then displayed in the Example Gallery for the user to browse and select desirable designs. Once user selects UI, the selected design combined with the users query, current UI code, and target area (used only for local remix) is formatted into complete prompt and sent to the Multimodal Large Language Model (MLLM), which generates updated UI code in diff-patch format. The system backend parses these responses using the unidiff and difflib packages, updates the UI code, and renders them as UI previews. screenshots by cropping them to eliminate black borders and ensure consistent framing across all images. 3.6.2 Retriever and Generator Modules. To enable example retrieval, we construct UI screenshot database by embedding each UI screenshot into vector representation using GUIClip [79]. The resulting embeddings are indexed and stored in ChromaDB vector database [1], allowing efficient similarity-based retrieval during interaction. When users switch between modes, the system handles them differently. Chat Mode: In this mode, the users query is combined with the current UI code (which is empty on the first call) to form structured prompt that is processed by the MLLM. To reduce generation time, the MLLM is instructed to produce code in diff-patch format, representing only the changes needed. The generated patch is then applied to the existing UI code to update the design accordingly. Search Mode: In this mode, the system first encodes the users natural language query into fixed-dimensional vector representation using GUIClip [79], which maps textual descriptions into the same embedding space as UI screenshots. It then computes the cosine similarity between the query vector and each stored screenshot embedding, defined as the normalized dot product between the two vectors, to measure their semantic closeness. Based on these similarity scores, the system retrieves the top 10 most relevant screenshots2. These retrieved UI designs are ranked by similarity and presented in the Example Gallery, accompanied by their associated metadata. Apply Mode: In this mode, the system supports two remix operations global and local depending on the scope of the users modification request.For global remix, the system combines the users natural language query, the current UI code, and the selected reference screenshot into structured prompt. This prompt is sent to the GPT-5 model [61], which generates an updated version of the entire UI code reflecting the requested changes. For local remix, the system constructs structured prompt that includes the users query, the current UI code, the reference screenshot annotated with the users drawing, and the identifier of the target UI component. The GPT-5 model then generates code in diff-patch format (consistent with the format used in Chat Mode) as only 2The choice of retrieving the top 10 results follows common practice in information retrieval [30]. IUI 26, March 2326, 2026, Paphos, Cyprus Junling Wang et al. specific region of the interface is modified. The system backend parses the diff-patch codes using the unidiff [75] and difflib [13] packages, updates the UI code, and renders the updated interface at the Editable Canvas. Implementation. We implemented UI Remix as web-based 3.6.3 application for easy access. The backend, built with FastAPI, integrates the full generation and retrieval pipeline. It uses the OpenAI API to send requests to the GPT-5 model [61] and ChromaDB [1] to store and query screenshot embeddings for retrieval. We selected GPT-5 because our task focuses on general UI design code generation; at the time of the study, GPT-5 was among the best-performing models for code generation [61] and was therefore considered appropriate for our setting. The generated UI designs are rendered on the frontend within an iframe element [76]. To support interactive remixing, we inject custom CSS and JavaScript into the iframe, enabling users to select components for local remix directly within the preview panel. The backend communicates bidirectionally with the frontend receiving user actions and returning model outputs along with retrieved UI examples. The frontend was developed from scratch using React. The iframe [76] is responsible for rendering the generated UI designs, while the Monaco Editor [54] provides built-in code editor that allows users to view and modify UI code. 3 3.6.4 Pilot Studies and Iterative Refinement. We conducted four pilot studies with four participants: three masters students in data science and one PhD student in computer science. The goal was to collect feedback on the systems design and usability. In these pilot sessions, participants completed the same UI design tasks as those used in the main study (see Section 4), using an early version of the system. Each session lasted approximately 2030 minutes and focused on identifying usability issues and interaction breakdowns. Feedback was captured through researcher observation notes and informal post-task interviews. key issue raised during the pilot sessions concerned the long waiting time for revising UI designs. In the earlier system version, the MLLM was prompted to regenerate the entire UI code even for small revisions, which took over three minutes on average. To address this, we modified the prompt to request only diff-patch code and implemented an update mechanism using the unidiff and difflib packages. This optimization reduced the average revision time to about one minute. Another issue was the lack of mechanism to revert to previous design versions after making edits. To address this, we implemented version history system that records all intermediate design states. Users can now navigate between versions using the Back and Forward buttons, allowing them to easily revisit or restore any earlier version of their design. Further improvements included repositioning the Home button and refining the interface for switching between the Chat, Search, and Apply modes. 3.6.5 Technical Evaluation. To assess the retrieval performance of our MLLM-based system, we conducted controlled technical evaluation using 100 automatically generated text queries. The queries were created from predefined templates covering four intent 3The pipeline is open-sourced at: https://github.com/ETH-PEACH-Lab/UI_Remix. Table 2: Retrieval performance of the system across different query types. Hit@5 measures the proportion of queries retrieving at least one relevant result in the top 5, while nDCG@5 reflects the quality of ranking among the top results. Query Type Hit@5 nDCG@ Color Theme Layout UI Category UI Component Average (All) 0.92 0.88 0.92 0.80 0.88 0.83 0.72 0.84 0.70 0.77 types color theme, layout, UI category, and UI component design each targeting distinct aspect of user interface representation. This setup allows us to systematically test whether the retriever can align textual descriptions with corresponding visual or structural features in the corpus. For each query, the system retrieved the top-5 results. Two researchers independently annotated the relevance of each retrieved UI on 4-point scale (0 = irrelevant, 1 = slightly related, 2 = moderately relevant, 3 = highly relevant). Scores of 2 or above were considered relevant when computing Hit@5, while the full graded values were used for nDCG@5. The annotators initially disagreed on seven cases out of the total, which were resolved through discussion to reach final consensus. As shown in Table 2, our system achieved an average Hit@5 of 0.88 and nDCG@5 of 0.77 across all query types. This result indicates that in 88% of queries, at least one UI judged relevant by annotators appeared within the top five retrieved results, while the nDCG@5 score suggests that more relevant UIs were generally ranked higher in the list. These scores are considered strong for vision-language UI retrieval tasks [19, 79]. The retriever performed particularly well on color and category queries, demonstrating that the multimodal embeddings effectively capture both stylistic and semantic aspects of UI design. These results confirm that the retrieval module provides accurate and consistent matches, supporting the interactive exploration features evaluated in our user study."
        },
        {
            "title": "4 User Study",
            "content": "To investigate the effectiveness of UI Remix in supporting end user in UI design, we conducted user study to address the following research question: RQ. How does UI Remix support end users in achieving their design goals, iterating on designs, exploring alternative designs, and influencing their trust through traceable UI examples? We employed within-subjects study with two experimental conditions, involving 24 end users who had no expertise in mobile UI design. Each participant was asked to design two mobile UIs, one with each system. Although UI Remix is capable of generating interactive and multi-screen UIs, our evaluation focused on static single-screen UI prototyping to examine UI design within the limited study duration. This scope reflects the early stage of UI prototyping, when users focus on exploring and refining design UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing IUI 26, March 2326, 2026, Paphos, Cyprus ideas before adding interactivity. Accordingly, our study design emphasizes divergent exploration and early ideation; although UI Remix supports iterative refinement, the limited task duration and single-screen setup did not allow for sustained or deep refinement, which we leave to future investigation."
        },
        {
            "title": "4.1 Two Experimental Conditions",
            "content": "To understand how end users perceive the benefits of UI Remix, we implemented two system variants for comparison, using the same study apparatus. During the study, we referred to the conditions by their names (Alpha, Beta) when introducing the session to participants to minimize any bias associated with the names. GPT-Canvas Baseline (Alpha): The GPT-Canvas baseline adopts design similar to GPT Canvas [60] and Figma Make [21]. It uses the same MLLM as UI Remix, but excludes the Example Gallery (Figure 1B) as well as the corresponding search and apply modes in the Conversation Panel (Figure 1A). By omitting these components, we can evaluate whether their presence influences users performance in mobile UI design and their perceptions of the system. Participants interacted with the system using text-only instructions. UI Remix (Beta): UI Remix incorporates all design features and functionalities described in Section 3. It integrates our specially designed MMRAG model for UI example retrieval and employs the MLLM to generate initial UIs and perform example-based remixing."
        },
        {
            "title": "4.2 Participants and Recruitment",
            "content": "We conducted power analysis using G*Power [18] to estimate the required sample size for our within-subjects study design. Based on pilot studies (Section 3.6.4), we assumed an effect size of ùëì = 0.6 (moderate), with significance level of ùõº = 0.05 and power (1 ùõΩ) of 0.8, resulting in required sample size of 24 participants. After securing ethical approval, we recruited 24 participants through social media platforms. Participants included 19 male and 5 female individuals, with an average age of 25. Qualified participants self-identified as having little or no expertise in mobile UI design but expressed interest in it. Importantly, none of the participants were professional UI designers; we define mobile UI design expertise as formal training or professional experience in mobile UI design, whereas reported UI design experience reflects informal, non-professional activities (e.g., designing personal website). Our participants varied in their familiarity with AI tools for interface design, ranging from those who had never used such tools to those who engaged with them few times week. Table 3 shows detailed breakdown of participant demographics."
        },
        {
            "title": "4.3 Study Protocol",
            "content": "All participants signed the consent form before the study. During the study, participants first received 5-minute introduction on how to use UI Remix. To ensure participants fully understood how to use each system, we demonstrated its functions and then gave them time to explore it on their own before starting each task. We began each task only after participants indicated that they were comfortable with the system. During this familiarization phase, participants were encouraged to ask any questions they had about using UI Remix. Participants then completed two mobile UI design tasks, with maximum time limit of 25 minutes per task, each representing typical challenge in mobile UI design. For each task, participants designed one mobile UI using one of the two systems. We used Balanced Latin Square design [5] to arrange the order of the assisting methods for each participant, ensuring that each method was used across different tasks rather than being consistently paired with the same task, while minimizing potential order effects. After each task, participants completed Likert-scale post-task questionnaire to evaluate the system. After completing both tasks, we conducted 10-minute semi-structured exit interview with each participant and collected their completed UI design for analysis. The study lasted 60-70 minutes and was conducted virtually via Zoom. Participants used the system in their own browsers. Each received 25 USD as compensation."
        },
        {
            "title": "4.4 Study Task",
            "content": "We designed two tasks to represent common mobile UI design scenarios for end users. The first involved creating menu page for hamburger restaurant based on provided menu. The second involved designing news display page for news-reading app using given set of news articles. We selected these two tasks for their straightforward and familiar nature, which allows participants to focus on design interaction rather than domain knowledge. We set 25-minute time limit per task, based on pilot results showing participants could complete the tasks within that time."
        },
        {
            "title": "4.5 Data Collection and Analysis",
            "content": "This study collected data from multiple sources, including screening questionnaires, post-task questionnaires, exit interviews, observation notes, system usage logs and expert ratings. We report the significance levels (ùëù) and test statistics (ùëç or ùë°) for each statistical test in this section and Section 5. 4.5.1 Post-Task Questionnaires. The post-task questionnaires used 7-point Likert scales to evaluate systems on various attributes. The ShapiroWilk test showed none of the post-task questionnaire scores were normally distributed. As Beta consistently had higher mean score than Alpha condition, we performed one-sided post-hoc Wilcoxon signed-rank tests with Bonferroni correction to compare Alpha with Beta. Results are in Table 4. Semi-structured Interview. We conducted exit interviews af4.5.2 ter the session, recording 329 minutes of video to gather additional feedback on the systems. We transcribed and proofread participants answers and the recordings to produce the interview transcripts. We use the interview transcripts as anecdotal evidence to support our quantitative findings. Therefore, we used in vivo [64] coding to analyze the interviews and attune ourselves to the users perspectives on the different designs. 4.5.3 Observation Notes and Usage Logs. During each session, researcher took observational notes while the system recorded participants queries, UI designs, and function usage. These data were IUI 26, March 2326, 2026, Paphos, Cyprus Junling Wang et al. Table 3: Participants demographics: all 24 participants reported having no mobile UI design expertise. The sample included 10 bachelor, 12 masters, and two Ph.D. degree holders. Their experience with AI ranged from 6 months to over 3 years, while their usage of AI for UI design ranged from never to few times week. PID Educational Level Occupation Frequency of Using AI for UI Design Experience of Using AI Design Tools Used Before 1 2 3 4 5 6 7 8 9 10 11 12 13 14 16 17 18 19 20 21 22 23 24 Bachelor Bachelor Bachelor Master Master Master Master Bachelor Bachelor Master Master Bachelor Master Master Student Student Pilot/Union Representative Student Never Never Less than once month Never Scientific Assistant Student PhD Student Teacher Student PhD Student PhD Student Student PhD Student Data Scientist Less than once month About once week Less than once month few times month few times week Never Less than once month Never Less than once month few times month Bachelor Student few times week Master PhD Student few times month Master Bachelor Master Master Ph.D Ph.D Bachelor PhD Student Student Student Doctorate Researcher Scientific Researcher PostDoc Student Less than once month few times week few times week few times month Less than once month Never Less than once month Bachelor Student Less than once month 23 years 23 years 12 years 12 years 12 years 23 years 23 years 0.51 year 23 years 12 years 12 years 23 years 23 years 12 years 12 years 23 years 23 years 12 years 12 years > 3 years > 3 years > 3 years 12 years 23 years PowerPoint, Google Slides PowerPoint, Google Slides, ChatGPT, Photoshop MidJourney PowerPoint, Google Slides, Canva, ChatGPT, Photoshop, MidJourney, DALLE None ChatGPT, Figma PowerPoint, Google Slides, ChatGPT, Figma, Photoshop, DALLE Google Slides, ChatGPT PowerPoint, Google Slides, ChatGPT Google Slides, Canva, Adobe Illustrator, Photoshop PowerPoint, Google Slides PowerPoint, Google Slides, Canva, ChatGPT, Adobe Illustrator, MidJourney, DALLE, MS Paint PowerPoint, Google Slides, Canva, ChatGPT PowerPoint, Google Slides, ChatGPT, Figma, Adobe Illustrator, Photoshop, DALLE PowerPoint, Google Slides, Canva, Adobe Illustrator PowerPoint, Google Slides, ChatGPT, Photoshop, Others PowerPoint, ChatGPT, Figma PowerPoint, Google Slides, Canva, ChatGPT Google Slides, ChatGPT, Photoshop PowerPoint, ChatGPT PowerPoint, Google Slides, ChatGPT, Figma, MidJourney, DALLE PowerPoint, Google Slides, Canva PowerPoint, Google Slides, ChatGPT, Photoshop, MidJourney PowerPoint, Google Slides, Canva, ChatGPT, Figma, MidJourney, DALLE used to capture user behavior and provide insights into how participants interacted with the different systems. The screen recordings were also transcribed and used to review participant behaviors. 4.5.4 Task Completion Time and Expert Evaluation. Regarding task completion time, the ShapiroWilk test indicated no significant deviation from normality. Therefore, we conducted paired t-tests with Bonferroni correction to compare completion times between Alpha and Beta, as well as between the two tasks. The results showed significant difference in completion time between the two systems (ùë° = -3.05, ùëù < 0.01) and no significant effect of task type on completion time (ùë° = 0.45, ùëù = 0.66). These results are reasonable, as the Beta system includes additional functions such as global remix and local remix, which may require additional time to operate. However, the average completion time for the Beta system (15.76 min) was only approximately 3 minutes longer than that for Alpha (12.41 min), suggesting that the increased time demand was modest. To evaluate the quality of participants designed UIs, we collected 48 UIs and presented them in random order to two external UI design experts who were not involved in the project. The experts jointly reviewed each design, discussed their judgments, and reached consensus rating on three criteria aesthetic quality, layout quality, and accuracy using seven-point Likert scale (1 = very poor, 7 = excellent). We then conducted two-sided Wilcoxon signed-rank tests on the consensus ratings, and no significant differences were observed between the Alpha and Beta conditions across any of the criteria. One plausible explanation is that the expert evaluation rubric is necessarily coarse-grained and focuses on static end results, which may not capture more subtle differences related to design intent or rationale. As result, the benefits of example-driven interaction, such as supporting how users reason about and explore design intent during the task, may not accumulate into observable differences in final UI quality within short, time-constrained tasks, and increased exploration can come at the expense of final visual polish."
        },
        {
            "title": "5 Results",
            "content": "We report results from the user study and summarize key findings as below."
        },
        {
            "title": "Examples",
            "content": "Participants reported that the Beta system substantially enhanced their ability to engage in creative exploration compared to Alpha. Quantitatively, Beta received significantly higher ratings for helping to explore more alternative design options (ùëù < .001, ùëç = 1.03) and like to use this system for other creative tasks. (ùëù < .001, ùëç = 0.03), as shown in Table 4. central way Beta supported creativity was through inspiration from examples. Nine participants (P12, P45, P67, P9P11) explicitly mentioned that they liked the example-driven nature of the Beta system. Participants described that seeing real-world UI examples supported their design process in three key ways. First, it helped them spark creative design inspirations (P1, P3, P10, P12); for instance, P3 mentioned that they liked to get design ideas ... UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing IUI 26, March 2326, 2026, Paphos, Cyprus Table 4: Results of post-task questionnaire comparing Alpha and Beta. Wilcoxon signed-rank tests with Bonferroni correction were conducted for each statement. Reported are Bonferroni-corrected p-values (ùëù < .05*; ùëù < .01**). Statement Condition SD Agreement: 1 to 7 The system is generally easy to use. The systems interaction is intuitive. The system supports me well in achieving my design goals. The systems outputs within each iteration follow my expectations. am satisfied with the design outcomes get in the end. The system can help me to iterate on UI designs effectively. The system helps me to explore more alternative design options. If need to build personal mobile UI, would like to use this system. would like to use this system for other creative tasks. Alpha Beta Alpha Beta Alpha Beta Alpha Beta Alpha Beta Alpha Beta Alpha Beta Alpha Beta Alpha Beta 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 5.75 1.22 6.08 0. 5.62 1.35 6.04 1.08 0.36 0.22 4.79 1.67 0.02* 5.92 0.97 4.83 1.61 5.58 1.06 5.33 1.52 5.96 0. 0.14 0.13 4.67 1.99 0.04* 5.83 1.20 4.17 2.01 0.00** 6.58 0.78 4.88 1.83 0.01** 6.08 1.02 4.75 1.80 0.00** 6.00 1. see inspirations and try to use them from other websites or other apps. Second, it guided them in applying existing styles to their own designs (P2, P10, P14, P15). As P14 pointed out [the retrieved examples] helped lot; can refer to many other different websites, and integrate their ideas into my own design.. Finally, it enabled them to learn from established best practices (P9, P10). These UI examples served as concrete visual references that helped participants imagine new design directions or adapt effective elements. As P7 noted, the system helps go beyond language description while building the app, allowing them to move from abstract intentions to tangible visual ideas. Examples also encouraged participants to explore more design options and experiment with variations. Five participants (P4-7, P17) described that seeing multiple examples motivated them to look for other layouts or try more options before deciding on final design. As P7 put it, the system encourages you to look for other layouts, suggesting that the example presentation expanded users exploration space and prompted more creative comparison among different possibilities. This active engagement with alternative examples enabled participants to experiment freely and evaluate designs from multiple perspectives, fostering more open design mindset. Overall, Beta transformed example viewing from passive experience into more dynamic process of exploration, which allowed participants to question and imagine what their ideal UI could look like."
        },
        {
            "title": "5.2 Supporting Goal-Oriented Design Iteration",
            "content": "Beyond fostering creative exploration, participants also highlighted how the system supported goal-oriented iteration during the design process, helping them refine their design toward their intended design goals. Quantitatively, participants rated the Beta system significantly higher for helping to iterate on UI designs effectively (ùëù = .04, ùëç = 0.67) and supporting me well in achieving my design goals (ùëù = .02, ùëç = -0.64). Behavioral data from usage logs and observation notes further support these findings. Participants used the search mode an average of 3.75 times, the global remix 1.33 times, and the local remix about 3 times per task; six participants (P3, P67, P1112, P20) retrieved at least five relevant UIs, reflecting active engagement in iterative refinement. In addition, interaction logs show that 21 participants (P17, P914, P1724) used the retrieval mode at least once within their first five operations, suggesting that participants often referred to existing examples early in the design process. Participants found that examples served as reference points for meaningful iteration throughout the design process. Five participants (P2, P6, P11, P13, P14) mentioned that the presence of concrete examples made it easier to start the design and iteration process. As P2 explained, like that it can give some examples of the app, because dont know what the layout will look good, while P11 shared that the system makes me feel good or easier than starting from scratch. These reflections suggest that examples served as visual scaffolds that guided users goal-driven refinement. Instead of starting over or relying on trial and error, participants could use examples to recognize what aspects worked or needed adjustment, making iteration more guided and flexible process toward their desired outcomes. IUI 26, March 2326, 2026, Paphos, Cyprus Junling Wang et al. Participants also discussed how specific system features, particularly the global remix and local remix, facilitated iterative refinement during the design process. The local remix function was appreciated for allowing users to modify specific component while preserving the overall design, enabling them to incorporate elements from UI examples into targeted components of their UI design (P1-2, P5). In contrast, the global remix supported higher-level design adjustments by applying UI styles from retrieved examples to the entire interface. Seven participants explicitly stated that the global remix function effectively helped them apply UI styles from selected examples (P1-2, P5, P11, P13, P15, P24), e.g., P1 would say its actually quite powerful, since you can perfectly copy the style and layout and apply the reference choose to our app."
        },
        {
            "title": "Source Transparency Cues",
            "content": "Participants rated the Beta system significantly higher for If need to build personal mobile UI, would like to use this system (ùëù = .01, ùëç = -0.93), indicating strong willingness to adopt it for real design tasks. Interview data further suggest that this preference was closely and consistently tied to the source transparency cues displayed alongside retrieved UI examples. These cues included app ratings, download counts, developer information, app name, and category, allowing users to clearly trace where each example originated. Out of 24 participants, clear majority of 17 actively examined these cues during the study itself. Participants described source transparency cues as valuable for assessing the credibility and relevance of design examples overall. Six participants (P2-5, P10-11) mentioned that higher ratings made them more inclined to trust and adopt certain designs, reflecting their belief that well-rated UIs were more likely to come from successful or professional applications. Others emphasized that these cues increased reliability (P1, P3) and provided additional references (P4-5) for evaluating UI examples. As P9 noted, by looking at the installation number and the rating. If these numbers are high, the user will say, okay, this may be good design and choose this one. So the metadata serves as reference. These comments indicate that source transparency cues provided an extra layer of contextual trust, enabling participants to evaluate examples not only by appearance but also by perceived credibility, making their design choices feel better informed and thus justified. However, not all participants relied on these cues. Seven participants (P2, P6, P12-14, P20, P22) stated that metadata did not significantly influence their design decisions, with some focusing purely on the visual aspects of UI examples. As P6 explained, just dont really care about the details here, and P12 mentioned, focus more on the visual aspect, so wasnt really influenced by the metadata at all. This variation suggests that while transparency cues were effective in fostering trust and confidence for many users, others prioritized direct visual appeal or personal aesthetic judgment. Overall, the findings indicate that transparency cues help build trust, particularly among users who value contextual validation, yet still allowing flexibility for those who rely more on visual intuition."
        },
        {
            "title": "6 Discussion and Future Work\n6.1 Significance of Example-Driven Design in\nArticulating Intent and Building Trust",
            "content": "Our findings suggest that, from participants perspectives, designing with real-world UI examples can support the process of articulating design goals and developing trust in AI-assisted design. This process occurs through two mechanisms: (1) grounding users underspecified intents in concrete design references, and (2) enhancing user trust and design confidence through source transparent cues. Firstly, participants experienced the example-driven paradigm as supporting intent articulation through externalization. Participants often started with broad, underspecified goals such as modern, clean restaurant menu but struggled to define what modern or clean meant in design terms. By browsing and comparing examples, participants reported gradually refining their design goals, using retrieved UI designs as concrete references to evaluate and adjust their preferences. This process often prompted self-explanations (e.g., prefer this one because the layout is simpler), suggesting that examples helped participants reflect on and verbalize preferences that had previously been implicit. This pattern can be interpreted through the notion of reflection-in-action: where designers come to understand their goals through interaction with concrete representations [66]. Secondly, the use of source transparency cues for UI examples directly contributed to the perceived trust and design confidence. Displaying source transparency cues, including ratings, developer information, and download counts, helped users evaluate the credibility of retrieved designs. In line with social transparency theory [17, 70], these cues allowed users to form transitive trust, reasoning that if many others used or liked this design, it must be reliable. This was particularly important for novices, who lacked professional heuristics for judging design quality. Transparency thus transformed retrieval results from opaque outputs into trustworthy design references, enabling users to make informed adaptation decisions."
        },
        {
            "title": "6.2 Supporting Progressive Articulation\nThrough Iterative Co-Evolution",
            "content": "A central insight from our study is that UI Remix supported users in iteratively refining their designs through interaction with examples. Each retrieval-remix cycle enabled users to explore alternatives UI designs and make more confident adjustments, reflecting design process that participants perceived as more structured and effective. This iterative dynamic aligns with Sch√∂ns notion of reflectionin-action [66, 67], where design understanding emerges through reciprocal feedback loops between thought and material form rather than from single act of specification. UI Remix supports this process by providing two complementary levels of exploration: global remix (whole-interface adaptation) and local remix (component-level refinement). This design allowed users to shift between divergent and convergent modes of thinking [23], broadly exploring stylistic directions before zooming in on specific elements to refine. This flexible structure mirrors how UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing IUI 26, March 2326, 2026, Paphos, Cyprus expert designers alternate between high-level concepts and detailed refinements [11], helping non-designers navigate between abstraction levels in way they described as manageable. These iterative processes also highlight the changing nature of the humanAI relationship during design. More broadly, our findings point toward reframing of AIs role in end user design support, i.e., from generator that executes instructions to reflective collaborator that scaffolds understanding. Rather than assuming users can articulate their goals clearly upfront, UI Remix supported exploratory interactions through which participants reported clarifying what they wanted. In this view, AIs value lies not in automating design creation but in mediating reflection, i.e., providing traceable examples, supporting iteration, and prompting users to reason about their own choices. This perspective invites shift from optimizing generative accuracy toward designing systems that foster reflective efficacy how effectively users can examine and refine their ideas through collaboration with AI. While UI Remix was experienced as empowering users design process by fostering exploration, reflection, and confidence, these process-level benefits did not translate into significantly higher expert-rated output quality within the constrained study setting. This suggests that example-driven systems may primarily shape how users engage with design rather than immediately improving final outcomes, particularly in early-stage tasks. At the same time, reliance on retrieved examples raises potential risks of stylistic homogenization or over-dependence on existing designs if not carefully managed. Looking ahead, such systems may influence design practice by lowering entry barriers and shifting design toward more exploratory, dialogue-based process, while also raising questions about originality, skill development, and long-term creative agency, which warrant further longitudinal investigation."
        },
        {
            "title": "6.3 Design Takeaways",
            "content": "Drawing on insights from our study, we outline several takeaways for designing example-driven AI systems for design that support end user creativity and trust. Scaffold articulation through example-grounded exploration: Systems should treat examples not merely as outcomes to imitate but as articulation aids that help users articulate vague ideas. Interfaces that encourage users to search, browse, and iteratively remix examples can help them progressively clarify their design intent, even when their initial design goals are under-specified. Make example provenance visible to foster informed trust: Displaying source transparency cues, such as ratings, download counts, and developer information, can transform retrieved results into credible references rather than opaque suggestions. Such transparency allows users to calibrate trust and evaluate which examples best align with their needs. Support reflective co-evolution between user and system: Example-driven tools should emphasize iterative feedback loops that allow users to move fluidly between exploration and refinement. Providing complementary modes of interaction (e.g., global and local remix) helps users reason across abstraction levels, promoting structured yet creative iteration. Reposition AI as reflective collaborator, not generator: Future systems should value not only generative fidelity, i.e., how well the output matches user specifications, but also reflective efficacy, i.e., how effectively the system helps users interpret, evaluate, and refine their own ideas. AI can augment design thinking by mediating reflection: returning interpretable examples, revealing its retrieval rationale, and encouraging users to reframe their goals through exploration."
        },
        {
            "title": "6.4 Ethical Considerations",
            "content": "Building example-driven AI systems for design requires careful attention to ethical and legal aspects of data sourcing, attribution, and user interaction. UI Remix relies on real-world UI examples retrieved from publicly available repositories to support design exploration. It is therefore essential to ensure that all referenced or displayed examples are collected and used in accordance with licensing agreements and data protection regulations. Future systems that integrate similar retrieval mechanisms should prioritize datasets and sources that are explicitly open-licensed or permit fair-use display for research and educational purposes. When visual examples originate from commercial or user-generated platforms, clear attribution should be maintained to respect the intellectual property of original creators and promote responsible reuse. Displaying source metadata, as implemented in UI Remix, not only supports transparency and trust but also reinforces ethical visibility by acknowledging the provenance of each design. Finally, as AI systems increasingly mediate creative processes, designers and researchers must be mindful of how example retrieval and remixing could influence originality and authorship. Ethical design assistance should aim to empower users learning and reflection rather than encourage direct imitation."
        },
        {
            "title": "6.5 Limitations and Future Work",
            "content": "Our study has several limitations. For the design tasks, we focused on creating single mobile UI page based on given context (e.g., restaurant menu or news app page). This focus allowed us to closely examine how end users engage in example-driven workflow within bounded design space. While UI Remix is capable of generating interactive and multi-screen user interfaces, our evaluation was intentionally limited to static single-screen UIs due to time constraints and to maintain experimental control. Future work could extend UI Remix to support multi-screen or interactive design sequences, exploring how example-driven reflection extends across connected design contexts. In addition, our evaluation was conducted as single-session lab study focused on short-term design activities. While this setup allowed controlled observation of users interactions with UI Remix, it does not capture how the system might be used in more naturalistic or collaborative settings. Future evaluations could explore how users employ example-driven design support in real-world or longitudinal contexts, where goals and workflows are more diverse. Besides, the scope of our evaluation opens up several opportunities for future investigation. Our study included participants with varied prior experience in AI-assisted UI design, but was not designed to explicitly contrast different end-user populations (e.g., IUI 26, March 2326, 2026, Paphos, Cyprus Junling Wang et al. users with versus without prior AI design experience). Future work could build on our findings by conducting broader user studies with more clearly defined target groups and by examining how prior experience shapes system use and perception. In addition, our study evaluated retrieval and remixing as an integrated workflow and did not isolate the individual contributions of these components. Future work could address this limitation through component-level ablation studies (e.g., retrieval-only, remix-only, and combined conditions) to more precisely characterize how each component supports exploration, reflection, and intent articulation. Moreover, while our evaluation focused on short-term, task-based interactions within single session, complementary evaluation approaches, such as longitudinal usage, learning trajectories, or changes in intent articulation over time, could provide deeper insight into how users engage with example-driven design tools in practice. Finally, source transparency cues were examined as part of the integrated system design; future studies could isolate and evaluate transparency features independently (e.g., with and without metadata cues) to more precisely characterize their role in supporting trust and design decision-making. Finally, while UI Remix effectively supports text and componentbased retrieval, its current workflow is primarily user-initiated. Future work could explore integrating agentic mechanisms that enable the system to proactively suggest examples, refinements, or design directions based on users ongoing actions and emerging goals. Such an extension would move UI Remix toward more collaborative design partner capable of anticipating user needs and coordinating multi-step remix operations."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduced UI Remix, an interactive system that supports end users in designing mobile user interfaces through an example-driven design workflow. Grounded in multimodal retrievalaugmented generation, UI Remix enables users to iteratively search, select, and adapt real-world UI examples to refine their designs. By presenting source transparency cues such as ratings, download counts, and developer information of example UIs, the system helps users make more informed and trustworthy design choices. Our user study with 24 participants showed that UI Remix significantly improved users ability to achieve their design goals, iterate effectively, and explore alternative UI designs. Participants also reported that source transparency cues enhanced their confidence in adapting examples. These findings highlight how transparent, example-driven interactions can advance more reflective and trustworthy form of humanAI collaboration in end user design. We hope the insights from this work inspire future research on developing example-driven AI systems that empower end users to design with greater creativity, confidence, and control. Immediate directions for future work include extending example-driven support to multi-screen or interactive UI flows, exploring more proactive or agentic suggestion mechanisms, and examining longer-term use in real-world or collaborative settings."
        },
        {
            "title": "8 GenAI Usage Disclosure",
            "content": "In our study, AI assistants were used selectively and in accordance with the ACM Policy on the Use of Generative AI in Publications. We utilized ChatGPT and Grammarly for minor paraphrasing, grammar correction and code debugging. These tools were applied minimally to ensure the authenticity of our work and to adhere strictly to ACMs ethical and publication standards. Our use of these AI tools was focused, responsible, and aimed at supplementing rather than replacing human input and expertise in our research process."
        },
        {
            "title": "Acknowledgments",
            "content": "This project was made possible by ETH AI Center Doctoral Fellowships to Junling Wang, with partial support from the ETH Zurich Foundation. Additionally, the authors wish to thank the reviewers, members of the PEACH Lab at ETH Zurich, and the participants in the user study. References [1] 2023. GitHub - chroma-core/chroma: the AI-native open-source embedding database github.com. https://github.com/chroma-core/chroma. [Accessed 03-07-2024]. [2] Behance. 2025. Behance. https://www.behance.net/ [3] Farnaz Behrang, Steven P. Reiss, and Alessandro Orso. 2018. GUIfetch: Supporting App Design and Development through GUI Search. In Proceedings of the 5th International Conference on Mobile Software Engineering and Systems. ACM, Gothenburg Sweden, 236246. doi:10.1145/3197231.3197244 [4] Tony Beltramelli. 2018. pix2code: Generating Code from Graphical User Interface Screenshot. In Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems (Paris, France) (EICS 18). Association for Computing Machinery, New York, NY, USA, Article 3, 6 pages. doi:10.1145/3220134.3220135 [5] James Bradley. 1958. Complete counterbalancing of immediate sequential effects in Latin square design. J. Amer. Statist. Assoc. 53, 282 (1958), 525528. [6] Sara Bunian, Kai Li, Chaima Jemmali, Casper Harteveld, Yun Fu, and Magy Seif Seif El-Nasr. 2021. VINS: Visual Search for Mobile User Interface Design. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI 21). Association for Computing Machinery, New York, NY, USA, Article 423, 14 pages. doi:10.1145/3411764.3445762 [7] Chunyang Chen, Sidong Feng, Zhenchang Xing, Linda Liu, Shengdong Zhao, and Jinshui Wang. 2019. Gallery D.C.: Design Search and Knowledge Discovery through Auto-created GUI Component Gallery. Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 180 (Nov. 2019), 22 pages. doi:10.1145/3359282 [8] Xiang Anthony Chen, Tiffany Knearem, and Yang Li. 2025. The GenUI Study: Exploring the Design of Generative UI Tools to Support UX Practitioners and Beyond. In Proceedings of the 2025 ACM Designing Interactive Systems Conference (DIS 25). Association for Computing Machinery, New York, NY, USA, 11791196. doi:10.1145/3715336. [9] Chin-Yi Cheng, Ruiqi Gao, Forrest Huang, and Yang Li. 2024. Colay: Controllable layout generation through multi-conditional latent diffusion. arXiv preprint arXiv:2405.13045 (2024). [10] Chin-Yi Cheng, Forrest Huang, Gang Li, and Yang Li. 2023. PLay: parametrically conditioned layout generation using latent diffusion. In Proceedings of the 40th International Conference on Machine Learning (Honolulu, Hawaii, USA) (ICML23). JMLR.org, Article 216, 23 pages. [11] Nigel Cross. 2004. Expertise in design: an overview. Design studies 25, 5 (2004), 427441. [12] Daniel de Souza Baul√©, Christiane Gresse von Wangenheim, Aldo von Wangenheim, Jean C. R. Hauck, and Edson C. Vargas J√∫nior. 2021. Using Deep Learning to Support the User Interface Design of Mobile Applications with App Inventor. In Proceedings of the XX Brazilian Symposium on Human Factors in Computing Systems (Virtual Event, Brazil) (IHC 21). Association for Computing Machinery, New York, NY, USA, Article 49, 11 pages. doi:10.1145/3472301.3484340 [13] difflib. 2025. difflib Helpers for computing deltas docs.python.org. https: //docs.python.org/3/library/difflib.html. [Accessed 10-10-2025]. [14] Steven P. Dow, Alana Glassco, Jonathan Kass, Melissa Schwarz, Daniel L. Schwartz, and Scott R. Klemmer. 2011. Parallel prototyping leads to better design results, more divergence, and increased self-efficacy. ACM Trans. Comput.-Hum. Interact. 17, 4, Article 18 (Dec. 2011), 24 pages. doi:10.1145/1879831.1879836 [15] Dribbble. 2025. Dribbble. https://dribbble.com/ [16] Peitong Duan, Chin-Yi Cheng, Gang Li, Bjoern Hartmann, and Yang Li. 2024. UICrit: Enhancing Automated Design Evaluation with UI Critique Dataset. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology. ACM, Pittsburgh PA USA, 117. doi:10.1145/3654777.3676381 [17] Upol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, and Justin D. Weisz. 2021. Expanding Explainability: Towards Social Transparency in AI systems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing IUI 26, March 2326, 2026, Paphos, Cyprus (Yokohama, Japan) (CHI 21). Association for Computing Machinery, New York, NY, USA, Article 82, 19 pages. doi:10.1145/3411764. [18] Franz Faul, Edgar Erdfelder, Axel Buchner, and Albert-Georg Lang. 2009. Statistical power analyses using G* Power 3.1: Tests for correlation and regression analyses. Behavior research methods 41, 4 (2009), 11491160. [19] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, CELINE HUDELOT, and Pierre Colombo. 2025. ColPali: Efficient Document Retrieval with Vision Language Models. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=ogjBpZ8uSi [20] Sidong Feng, Minmin Jiang, Tingting Zhou, Yankun Zhen, and Chunyang Chen. 2022. Auto-Icon+: An Automated End-to-End Code Generation Tool for Icon Designs in UI Development. ACM Trans. Interact. Intell. Syst. 12, 4, Article 36 (Nov. 2022), 26 pages. doi:10.1145/3531065 [21] Figma. 2025. Introducing Figma Make: New Way to Test, Edit, and Prompt Designs Figma Blog figma.com. https://www.figma.com/blog/introducingfigma-make/. [Accessed 04-10-2025]. [22] Longxi Gao, Li Zhang, Shihe Wang, Shangguang Wang, Yuanchun Li, and Mengwei Xu. 2024. Mobileviews: large-scale mobile gui dataset. arXiv preprint arXiv:2409.14337 (2024). [23] Joy Paul Guilford. 1950. Creativity. American psychologist 5, 9 (1950). [24] Scarlett R. Herring, Chia-Chen Chang, Jesse Krantzler, and Brian P. Bailey. 2009. Getting inspired! understanding how and why examples are used in creative design practice. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Boston, MA, USA) (CHI 09). Association for Computing Machinery, New York, NY, USA, 8796. doi:10.1145/1518701. [25] Jaylin Herskovitz, Andi Xu, Rahaf Alharbi, and Anhong Guo. 2024. ProgramAlly: Creating Custom Visual Access Programs via Multi-Modal End-User Programming. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology (Pittsburgh, PA, USA) (UIST 24). Association for Computing Machinery, New York, NY, USA, Article 85, 15 pages. doi:10.1145/3654777.3676391 [26] Morten Hertzum, Hans HK Andersen, Verner Andersen, and Camilla Hansen. 2002. Trust in information sources: seeking information from people, documents, and virtual agents. Interacting with computers 14, 5 (2002), 575599. [27] Forrest Huang, John F. Canny, and Jeffrey Nichols. 2019. Swire: Sketch-based User Interface Retrieval. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI 19). Association for Computing Machinery, New York, NY, USA, 110. doi:10.1145/3290605.3300334 https://www. [28] Interactionmining. 2025. ODIM interactionmining.org. interactionmining.org/explore. [Accessed 07-10-2025]. [29] David G. Jansson and Steven M. Smith. 1991. Design fixation. Design Studies 12, 1 (1991), 311. doi:10.1016/0142-694X(91)90003-F [30] Steve Jones and Mark Staveley. 1999. Phrasier: system for interactive document retrieval using keyphrases. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 160167. [31] Abidullah Khan, Atefeh Shokrizadeh, and Jinghui Cheng. 2025. Beyond Automation: How Designers Perceive AI as Creative Partner in the Divergent Thinking Stages of UI/UX Design. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 1105, 12 pages. doi:10.1145/3706598.3713500 [32] Tae Soo Kim, DaEun Choi, Yoonseo Choi, and Juho Kim. 2022. Stylette: Styling the Web with Natural Language. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI 22). Association for Computing Machinery, New York, NY, USA, Article 5, 17 pages. doi:10.1145/ 3491102.3501931 [33] Tiffany Knearem, Mohammed Khwaja, Yuling Gao, Frank Bentley, and Clara Kliman-Silver. 2023. Exploring the future of design tooling: The role of artificial intelligence in tools for user experience professionals. In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI EA 23). Association for Computing Machinery, New York, NY, USA, Article 384, 6 pages. doi:10.1145/3544549.3573874 [34] Kristian Kolthoff, Christian Bartelt, and Simone Paolo Ponzetto. 2023. Datadriven prototyping via natural-language-based GUI retrieval. 30, 1 (2023), 13. doi:10.1007/s10515-023-00377-x [35] Kristian Kolthoff, Christian Bartelt, and Simone Paolo Ponzetto. 2021. GUI2WiRe: rapid wireframing with mined and large-scale GUI repository using natural language requirements. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering (Virtual Event, Australia) (ASE 20). Association for Computing Machinery, New York, NY, USA, 12971301. doi:10.1145/3324884.3415289 [36] Felix Kretzer, Kristian Kolthoff, Christian Bartelt, Simone Paolo Ponzetto, and Alexander Maedche. 2025. Closing the Loop between User Stories and GUI Prototypes: An LLM-Based Assistant for Cross-Functional Integration in Software Development. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 879, 19 pages. doi:10.1145/3706598.3713932 [37] Ranjitha Kumar, Jerry O. Talton, Salman Ahmad, and Scott R. Klemmer. 2011. Bricolage: example-based retargeting for web design. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Vancouver, BC, Canada) (CHI 11). Association for Computing Machinery, New York, NY, USA, 21972206. doi:10.1145/1978942.1979262 [38] Triet H. M. Le, Hao Chen, and Muhammad Ali Babar. 2020. Deep Learning for Source Code Modeling and Generation: Models, Applications, and Challenges. ACM Comput. Surv. 53, 3, Article 62 (June 2020), 38 pages. doi:10.1145/3383458 [39] Brian Lee, Savil Srivastava, Ranjitha Kumar, Ronen Brafman, and Scott R. Klemmer. 2010. Designing with Interactive Example Galleries. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, Atlanta Georgia USA, 22572266. doi:10.1145/1753326.1753667 [40] Chunggi Lee, Sanghoon Kim, Dongyun Han, Hongjun Yang, Young-Woo Park, Bum Chul Kwon, and Sungahn Ko. 2020. GUIComp: GUI Design Assistant with Real-Time, Multi-Faceted Feedback. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 20). Association for Computing Machinery, New York, NY, USA, 113. doi:10.1145/3313831. 3376327 [41] Luis Leiva, Asutosh Hota, and Antti Oulasvirta. 2021. Interactive Exploration of Large-Scale UI Datasets with Design Maps. Interacting with Computers 32, 5-6 (03 2021), 490509. arXiv:https://academic.oup.com/iwc/article-pdf/32/56/490/38856879/iwab006.pdf doi:10.1093/iwcomp/iwab006 [42] Alan Leung, Ruijia Cheng, Jason Wu, Jeffrey Nichols, and Titus Barik. 2025. SQUIRE: Interactive UI Authoring via Slot QUery Intermediate REpresentations. In Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology (UIST 25). Association for Computing Machinery, New York, NY, USA, Article 199, 17 pages. doi:10.1145/3746059.3747672 [43] Amanda Li, Jason Wu, and Jeffrey Bigham. 2023. Using LLMs to Customize the UI of Webpages. In Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (San Francisco, CA, USA) (UIST 23 Adjunct). Association for Computing Machinery, New York, NY, USA, Article 45, 3 pages. doi:10.1145/3586182. [44] Jie Li, Hancheng Cao, Laura Lin, Youyang Hou, Ruihao Zhu, and Abdallah El Ali. 2024. User Experience Design Professionals Perceptions of Generative Artificial Intelligence. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 24). Association for Computing Machinery, New York, NY, USA, Article 381, 18 pages. doi:10.1145/3613904. 3642114 [45] Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu√±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, and Harm de Vries. 2023. StarCoder: may the source be with you! Transactions on Machine Learning Research (2023). https://openreview.net/forum?id=KoFOg41haE Reproducibility Certification. [46] Michael Xieyang Liu, Jane Hsieh, Nathan Hahn, Angelina Zhou, Emily Deng, Shaun Burley, Cynthia Taylor, Aniket Kittur, and Brad A. Myers. 2019. Unakite: Scaffolding Developers Decision-Making Using the Web. In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology. ACM, New Orleans LA USA, 6780. doi:10.1145/3332165.3347908 [47] Yuwen Lu, Alan Leung, Amanda Swearngin, Jeffrey Nichols, and Titus Barik. 2025. Misty: UI Prototyping Through Interactive Conceptual Blending. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 1108, 17 pages. doi:10.1145/3706598.3713924 [48] Yuwen Lu, Ziang Tong, Qinyi Zhao, Yewon Oh, Bryan Wang, and Toby Jia-Jun Li. 2024. Flowy: Supporting UX design decisions through AI-driven pattern annotation in multi-screen user flows. arXiv preprint arXiv:2406.16177 (2024). [49] Yuwen Lu, Ziang Tong, Qinyi Zhao, Chengzhi Zhang, and Toby Jia-Jun Li. 2023. Ui layout generation with llms guided by ui grammar. arXiv preprint arXiv:2310.15455 (2023). [50] Yuwen Lu, Yuewen Yang, Qinyi Zhao, Chengzhi Zhang, and Toby Jia-Jun Li. 2024. AI assistance for UX: literature review through human-centered AI. arXiv preprint arXiv:2402.06089 (2024). [51] Yuwen Lu, Chengzhi Zhang, Iris Zhang, and Toby Jia-Jun Li. 2022. Bridging the Gap Between UX Practitioners Work Practices and AI-Enabled Design Support Tools. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA 22). Association for Computing Machinery, New York, NY, USA, Article 268, 7 pages. doi:10.1145/3491101.3519809 [52] Ulrik Lyngs, Reuben Binns, Max Van Kleek, and Nigel Shadbolt. 2018. \"So, Tell Me What Users Want, What They Really, Really Want!\". In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, Montreal QC Canada, 110. doi:10.1145/3170427.3188397 IUI 26, March 2326, 2026, Paphos, Cyprus Junling Wang et al. Association for Computing Machinery, New York, NY, USA, 113. doi:10.1145/ 3313831.3376593 [74] Macy Takaffoli, Sijia Li, and Ville M√§kel√§. 2024. Generative AI in User Experience Design and Research: How Do UX Practitioners, Teams, and Companies Use GenAI in Industry?. In Proceedings of the 2024 ACM Designing Interactive Systems Conference (Copenhagen, Denmark) (DIS 24). Association for Computing Machinery, New York, NY, USA, 15791593. doi:10.1145/3643834.3660720 [75] unidiff. 2025. GitHub - matiasb/python-unidiff: Unified diff python parsing/metadata extraction library github.com. https://github.com/matiasb/ python-unidiff. [Accessed 10-10-2025]. [76] w3. 2025. HTML Standard w3.org. https://www.w3.org/TR/html52/embeddedcontent-0.html#the-iframe-element. [Accessed 08-10-2025]. [77] Hongyu Wang and Nikolas Martelaro. 2022. End-User Puppeteering of Expressive Movements. arXiv preprint arXiv:2207.12544 (2022). [78] Jeremy Warner, Kyu Won Kim, and Bjoern Hartmann. 2023. Interactive Flexible Style Transfer for Vector Graphics. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (San Francisco, CA, USA) (UIST 23). Association for Computing Machinery, New York, NY, USA, Article 49, 14 pages. doi:10.1145/3586183.3606751 [79] Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, G√©rard Dray, and Walid Maalej. 2025. GUing: Mobile GUI Search Engine using Vision-Language Model. ACM Trans. Softw. Eng. Methodol. 34, 4, Article 88 (April 2025), 30 pages. doi:10.1145/ [80] Jason Wu, Yi-Hao Peng, Xin Yue Amanda Li, Amanda Swearngin, Jeffrey Bigham, and Jeffrey Nichols. 2024. UIClip: Data-driven Model for Assessing User Interface Design. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology. ACM, Pittsburgh PA USA, 116. doi:10.1145/ 3654777.3676408 [81] Jason Wu, Eldon Schoop, Alan Leung, Titus Barik, Jeffrey Bigham, and Jeffrey Nichols. 2024. UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 75117525. doi:10.18653/v1/2024.naacl-long.417 [82] Ziming Wu, Qianyao Xu, Yiding Liu, Zhenhui Peng, Yingqing Xu, and Xiaojuan Ma. 2021. Exploring Designers Practice of Online Example Management for Supporting Mobile UI Design. In Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction (Toulouse & Virtual, France) (MobileHCI 21). Association for Computing Machinery, New York, NY, USA, Article 44, 12 pages. doi:10.1145/3447526.3472048 [83] Pengcheng Yin and Graham Neubig. 2017. Syntactic Neural Model for GeneralPurpose Code Generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 440450. doi:10.18653/v1/P17-1041 GPT-Canvas baseline Configurations We present screenshot of the GPT-Canvas baseline in Figure 3. The baseline adopts design similar to GPT Canvas [60] and Figma Make [21]: the left panel allows users to chat with the MLLM, while the right panel mirrors the GPT Canvas layout, enabling users to preview the UI and switch back to the code view by clicking button. The baseline uses the same MLLM as UI Remix, but excludes the Example Gallery (Figure 1B) and the corresponding search and apply modes in the Conversation Panel (Figure 1A). [53] Donald Metzler and Bruce Croft. 2006. Beyond bags of words: Modeling implicit user preferences in information retrieval. In AAAI, Vol. 6. 16461649. [54] Microsoft. 2025. Monaco Editor microsoft.github.io. https://microsoft.github. io/monaco-editor/. [Accessed 08-10-2025]. [55] Bryan Min, Allen Chen, Yining Cao, and Haijun Xia. 2025. Malleable OverviewDetail Interfaces. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 688, 25 pages. doi:10.1145/3706598.3714164 [56] Mobbin. 2025. Mobbin. https://mobbin.com/ [57] Mohammad Amin Mozaffari, Xinyuan Zhang, Jinghui Cheng, and Jin L.C. Guo. 2022. GANSpiration: Balancing Targeted and Serendipitous Inspiration in User Interface Design with Style-Based Generative Adversarial Network. In CHI Conference on Human Factors in Computing Systems. ACM, New Orleans LA USA, 115. doi:10.1145/3491102. [58] Tuan Anh Nguyen and Christoph Csallner. 2015. Reverse engineering mobile application user interfaces with REMAUI. In Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (Lincoln, Nebraska) (ASE 15). IEEE Press, 248259. doi:10.1109/ASE.2015.32 [59] Jeffrey Nichols, Brad A. Myers, and Brandon Rothrock. 2006. UNIFORM: automatically generating consistent remote control user interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Montr√©al, Qu√©bec, Canada) (CHI 06). Association for Computing Machinery, New York, NY, USA, 611620. doi:10.1145/1124772.1124865 [60] OpenAI. 2024. Introducing canvas openai.com. https://openai.com/index/ introducing-canvas/. [Accessed 04-10-2025]. [61] OpenAI. 2025. Introducing GPT-5 openai.com. https://openai.com/index/ introducing-gpt-5/. [Accessed 08-10-2025]. [62] Seokhyeon Park, Yumin Song, Soohyun Lee, Jaeyoung Kim, and Jinwook Seo. 2025. Leveraging Multimodal LLM for Inspirational User Interface Search. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 579, 22 pages. doi:10.1145/3706598.3714213 [63] Daniel Ritchie, Ankita Arvind Kejriwal, and Scott R. Klemmer. 2011. d.tour: style-based exploration of design example galleries. In Proceedings of the 24th annual ACM symposium on User interface software and technology. ACM, Santa Barbara California USA, 165174. doi:10.1145/2047196.2047216 [64] Johnny Salda√±a. 2013. The coding manual for qualitative researchers (2nd ed.). SAGE, Los Angeles. [65] Audrey Sanctorum, Luka Rukonic, and Beat Signer. 2021. Design Requirements for Recommendations in End-User User Interface Design. In End-User Development, Daniela Fogli, Daniel Tetteroo, Barbara Rita Barricelli, Simone Borsci, Panos Markopoulos, and George A. Papadopoulos (Eds.). Springer International Publishing, Cham, 204212. [66] Donald Sch√∂n. 2017. The reflective practitioner: How professionals think in action. Routledge. [67] Phoebe Sengers, Kirsten Boehner, Shay David, and JosephJofish Kaye. 2005. Reflective design. In Proceedings of the 4th decennial conference on Critical computing: between sense and sensibility. 4958. [68] Xinyu Shi, Yinghou Wang, Ryan Rossi, and Jian Zhao. 2025. Brickify: Enabling Expressive Design Intent Specification through Direct Manipulation on Design Tokens. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 424, 20 pages. doi:10.1145/3706598.3714087 [69] Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. 2025. Design2Code: Benchmarking Multimodal Code Generation for Automated Front-End Engineering. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, Albuquerque, New Mexico, 39563974. doi:10.18653/v1/2025.naacl-long.199 [70] H. Colleen Stuart, Laura Dabbish, Sara Kiesler, Peter Kinnaird, and Ruogu Kang. 2012. Social transparency in networked information exchange: theoretical framework. In Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work (Seattle, Washington, USA) (CSCW 12). Association for Computing Machinery, New York, NY, USA, 451460. doi:10.1145/2145204.2145275 [71] Amanda Swearngin, Mira Dontcheva, Wilmot Li, Joel Brandt, Morgan Dixon, and Amy J. Ko. 2018. Rewire: Interface Design Assistance from Examples. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, Montreal QC Canada, 112. doi:10.1145/3173574. [72] Amanda Swearngin, Mira Dontcheva, Wilmot Li, Joel Brandt, Morgan Dixon, and Amy J. Ko. 2018. Rewire: Interface Design Assistance from Examples. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI 18). Association for Computing Machinery, New York, NY, USA, 112. doi:10.1145/3173574.3174078 [73] Amanda Swearngin, Chenglong Wang, Alannah Oleson, James Fogarty, and Amy J. Ko. 2020. Scout: Rapid Exploration of Interface Layout Alternatives through High-Level Design Constraints. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 20). UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing IUI 26, March 2326, 2026, Paphos, Cyprus Figure 3: screenshot of GPT-Canvas baseline system."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "ETH Zurich"
    ]
}