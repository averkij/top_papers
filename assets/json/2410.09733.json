{
    "paper_title": "MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models",
    "authors": [
        "Hang Hua",
        "Yunlong Tang",
        "Ziyun Zeng",
        "Liangliang Cao",
        "Zhengyuan Yang",
        "Hangfeng He",
        "Chenliang Xu",
        "Jiebo Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMs' superior capabilities, researchers lack a comprehensive understanding of their compositionality -- the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs. To address this limitation, we propose MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMs' compositionality. Our proposed benchmark serves as a complement to these earlier works. With MMCOMPOSITION, we can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, we find GPT-4o's compositionality inferior to the best open-source model, and we analyze the underlying reasons. Our experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training. Resources available at: https://hanghuacs.github.io/MMComposition/"
        },
        {
            "title": "Start",
            "content": "MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models MMCOMPOSITION: REVISITING THE COMPOSITIONALITY OF PRE-TRAINED VISION-LANGUAGE MODELS Hang Hua1, Yunlong Tang1, Chenliang Xu1 Hangfeng He1 1 University of Rochester 2 Apple {hhua2, jluo}@cs.rochester.edu, {yunlong.tang, chenliang.xu}@rochester.edu, {zzeng24, hhe15}@ur.rochester.edu, llcao@apple.com, zhengyang@microsoft.com Ziyun Zeng1, Jiebo Luo1, 3 Microsoft Liangliang Cao2 Zhengyuan Yang 4 2 0 2 3 1 ] . [ 1 3 3 7 9 0 . 0 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMs superior capabilities, researchers lack comprehensive understanding of their compositionality the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is critical ability that facilitates coherent reasoning and understanding across modalities for VLMs. To address this limitation, we propose MMCOMPOSITION, novel human-annotated benchmark for comprehensively and accurately evaluating VLMs compositionality. Our proposed benchmark serves as complement to these earlier works. With MMCOMPOSITION, we can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, we find GPT-4os compositionality inferior to the best open-source model, and we analyze the underlying reasons. Our experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training. Resources available at: hanghuacs.github.io/MMComposition"
        },
        {
            "title": "INTRODUCTION",
            "content": "Pre-trained vision-language models, such as GPT-4o (Achiam et al., 2023), LLaVA (Liu et al., 2024b), InternVL (Chen et al., 2024b), and VILA (Lin et al., 2024a), have demonstrated impressive capabilities in complex reasoning, and have achieved remarkable results in various vision-language (VL) tasks. Despite these advancements, contemporary state-of-the-art VLMs still struggle with understanding fine-grained multimodal compositional information (Yuksekgonul et al., 2022; Thrush et al., 2022). For instance, VLMs often fail at counting objects in images, especially when the objects are mixed with other items or occluded, while humans can handle this task easily. This reveals compositionality gap between humans and models. However, compositionality is recognized as core capability for VLMs (Yuksekgonul et al., 2022), referring to the ability to understand and produce potentially infinite number of novel combinations of known visual and textual components, i.e., to make infinite use of finite means (Chomsky, 2014). Compositionality is essential for tackling challenging questions in image captioning, visual question answering (VQA), and scene understanding, where complex interactions between objects and attributes need to be communicated in natural language. In recent years, there has been growing focus on evaluating the comprehensive capabilities of large VL models, such as MMBench (Liu et al., 2023b), MMMU (Yue et al., 2023), MMVet (Yu et al., 2024a;b), MME (Fu et al., 2023), Seed-bench (Li et al., 2023a), MMStar (Chen et al., 2024a), Equal Contribution Corresponding Author 1 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Figure 1: MMCOMPOSITION comprises 13 categories of high-quality VL composition QA pairs, covering wide range of complex compositions. In the example, GPT-4o failed to understand the compositional aspects of the visual and textual components, misidentifying three-story building as double-decker structure. This misinterpretation highlights the limitations of current VLMs. MathVista (Lu et al., 2023), and LLaVA-Bench (Liu et al., 2024b). These benchmarks evaluate VLMs capabilities in recognition, OCR, knowledge, language generation, spatial awareness, and mathematical reasoning. While some of these benchmarks include visual compositional questionanswering (QA) pairs (Fu et al., 2024; Li et al., 2023a; Tong et al., 2024b), none are specifically designed to comprehensively evaluate the models fine-grained VL compositional perception and reasoning abilities. Additionally, some existing benchmarks (Yuksekgonul et al., 2022; Hsieh et al., 2024; Zhao et al., 2022; Thrush et al., 2022; Ray et al., 2023; Ma et al., 2023) evaluate models compositionality roughly from the perspective of attribute, relation, and object perception. These benchmarks have limitations in evaluating fine-grained visual composition and reasoning. They mainly focus on image-to-text retrieval tasks, assessing basic object, relation, and attribute recognition but neglecting deeper reasoning about object interactions, counting, and complex compositions. As result, researchers currently have an incomplete understanding of VLMs compositionality. To address these issues, we propose MMCOMPOSITION, novel, human-annotated, high-quality benchmark for the comprehensive evaluation of VLMs compositionality. MMCOMPOSITION evaluates the compositionality of VLMs in three main dimensions: VL compositional perception, reasoning, and probing, which are further divided into 13 distinct categories of questions, as illustrated in Figure 1. While previous evaluation benchmarks have primarily focused on text-to-image retrieval, single-choice questions, and open-ended text generation, MMCOMPOSITION introduces more diverse and challenging set of tasks. The benchmark encompasses 4,342 questions, covering both single-image and multi-image scenarios, as well as single-choice and indefinite-choice formats. This expanded range of tasks is designed to evaluate the complex interplay between vision and language in VLMs more effectively. By incorporating wider variety of complex composition questions, MMCOMPOSITION provides more comprehensive and in-depth assessment of models capabilities in cross-modal compositionality, surpassing the evaluations offered by earlier benchmarks like ARO (Yuksekgonul et al., 2022) and Winoground (Thrush et al., 2022). Table 1 highlights the differences between MMCOMPOSITION and other existing datasets that focus on VL compositionality. In addition to the new benchmark, we also provide comprehensive analysis of the models capabilities in fine-grained VL compositional perception and reasoning. Our experiments show that most SOTA VLMs exhibit deficiencies in compositional understanding. Even GPT-4o, despite its advanced capabilities, struggles with tasks requiring nuanced compositional reasoning. These findings highlight the need for further research and development to enhance the compositional abilities of VLMs. Our 2 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Table 1: Comparison with related VL compositional benchmarks: Yes/No Ratio refers to the proportion of yes/no questions, Fine-grained indicates whether the data provide detailed breakdowns of VL compositional information, and IT Mismatch Detec. means Image Text Mismatch Detection. Dataset Yes/No Ratio Size Human Annotation Multi-Image Indefinite-Choice Task Fine-grained Winoground (Thrush et al., 2022) ARO (Yuksekgonul et al., 2022) Sugarcrepe (Hsieh et al., 2024) VL-Checklist (Zhao et al., 2022) Cola (Ray et al., 2023) FineMatch (Hua et al., 2024a) GQA (Hudson & Manning, 2019) MMCOMPOSITION (ours) - - - - - - 0.774 0.038 400 50k 7,512 410k 1,200 49.9k 22M 4, - - - - - - Compositional Reasoning T2I Retrieval T2I Retrieval T2I Retrieval T2I Retrieval IT Mismatch Detec. Compositional QA Compositional QA benchmark serves as tool for identifying these gaps and inspiring future improvements in VLM design and training. Moreover, we analyze the critical factors in VLM architecture and training that may influence the compositionality of VLMs. According to the empirical results, we reach three findings: (1) Visual Encoder Design: While mixture-of-encoder architecture can enhance compositionality, adding more encoders does not necessarily improve performance. Moreover, models that encode images with minimal degradation of image quality preserving the original high resolution and aspect ratio exhibit superior compositionality compared to those that utilize downsampling during the encoding process. (2) Language Decoder Size: Larger language decoders are associated with improved compositionality. (3) The Volume of Training Data: Fine-tuning models on more diverse datasets helps mitigate some compositionality limitations, driving more robust compositional understanding. In addition, although GPT-4o includes powerful language model, we find that for relatively simple QA tasks, only small portion of its language capabilities are utilized (compared to the models outperform GPT-4o, whose language model size is only 70B). Once the language decoder size reaches certain threshold (e.g., 34B, 70B), the visual encoder has more significant impact on the models compositionality. We demonstrate in Figure 13 that the downsampling image processing in GPT-4o contributes to its inferior performance. Our experimental analysis highlights the limitations of large-scale VLMs in fine-grained compositional perception and reasoning. Our empirical analysis provides systematic framework for evaluating and enhancing models capability, pinpointing areas where large models still struggle. Our main contributions are three-fold: We introduce MMCOMPOSITION, novel, human-annotated, high-quality benchmark designed to evaluate the compositionality of pre-trained VLMs. MMCOMPOSITION assesses compositionality across three dimensions: compositional perception, reasoning, and probing, which are further divided into 13 distinct categories of questions. The benchmark includes diverse set of 4,342 questions, encompassing both single-image and multi-image scenarios, as well as single-choice and indefinite-choice questions, providing comprehensive and robust evaluating framework for VLM compositionality. We comprehensively evaluate 54 well-known VLMs with MMCOMPOSITION. The empirical results highlight the challenging nature of MMCOMPOSITION, as the highest model accuracy reached only 67.95%, compared to 90.31% for human performance. This evaluation reveals substantial gap between state-of-the-art VLMs and human capabilities and provides insights into the limitations of current VLMs. We systematically analyze critical factors in VLM architecture that may influence the compositionality of VLMs, including the size of language decoders, the volume of training data, and the visual encoder design. Furthermore, we provide an interpretable analysis of models limitations in complex compositional understanding. This analysis identifies critical areas for model improvement and suggests directions for future advancements."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 VLM EVALUATION BENCHMARKS The advent of large-scale VLMs has led to the development of numerous benchmarks designed to evaluate various model capabilities. Among the most commonly evaluated are image captioning (Lin et al., 2024b; Onoe et al., 2024; Masry et al., 2022), which tests VLMs ability to generate natural 3 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models language descriptions of images; VQA (Antol et al., 2015; Marino et al., 2019; Mathew et al., 2020), which assesses the models capacity to answer image-based questions by integrating visual perception with language understanding or external knowledge; and Visual Reasoning (Johnson et al., 2017; Suhr et al., 2017), which evaluates models understanding of spatial relationships and logical reasoning based on visual input. In recent years, researchers have built benchmarks that aim to evaluate the comprehensive capabilities of VLMs (Li et al., 2023a; Liu et al., 2023b; Yue et al., 2023; Fu et al., 2023; Yu et al., 2024a; Lu et al., 2023; Guan et al., 2024; Lu et al., 2024). Although some benchmarks include QA pairs related to compositional reasoning, such as BLINK (Fu et al., 2024), MMVP (Tong et al., 2024b), and Seed-bench (Li et al., 2023a), these are often mixed with other types of QA pairs, making it challenging to assess models compositionality precisely. In contrast, MMCOMPOSITION consolidates and refines existing categories of VL compositionality, offering diverse set of compositional QA pairs that provide more precise evaluation of model performance."
        },
        {
            "title": "2.2 COMPOSITIONALITY FOR VISION-LANGUAGE MODELS",
            "content": "Compositional understanding of images and text is critical capability for VLMs. Research indicates that VLMs struggle to distinguish hard negative examples, i.e., image-text pairs that mismatch in at least one aspect (e.g., attribute, relation, object), as there is little incentive for them to learn compositionality during contrastive pre-training (Yuksekgonul et al., 2022). Hsieh et al. (2024) illustrate that contrastive pre-training with generated hard negative examples can improve models performance on downstream tasks. Various benchmarks have been proposed to assess the capabilities of VLMs in compositional vision-language perception, including VL-Checklist (Zhao et al., 2022), ARO (Yuksekgonul et al., 2022), FineMatch (Hua et al., 2024a), Sugarcrepe (Hsieh et al., 2024), Crepe (Ma et al., 2023), Cola (Ray et al., 2023), CheckList (Zhao et al., 2022), etc. However, these benchmarks often evaluate models capabilities from limited perspectives, such as object, attribute, and relation perception, and primarily focus on simple tasks like binary image-to-text retrieval, where models need to select the correct caption from pairs containing correct and hard negative caption. Moreover, the aforementioned benchmarks often contain limited range of relations or attributes (e.g., ARO includes 48 relations and 117 attributes). GQA (Hudson & Manning, 2019) includes diverse set of QA pairs focused on compositional reasoning, but the majority of the questions (77.74%) are simple Yes/No format. In contrast, MMCOMPOSITION offers more comprehensive assessment with various compositional scenarios, including multi-image and indefinite choice questions, providing more comprehensive assessment. Furthermore, MMCOMPOSITION evaluates the robustness in detecting complex relationships, including subtle scene composition, object interactions, and higherorder concepts beyond basic perception. 2.3 PRE-TRAINED VISION-LANGUAGE MODELS Vision-language models (Radford et al., 2021; Liu et al., 2024a; Hua et al., 2024b; Ye et al., 2023; Tang et al., 2024; Chen et al., 2024b; Bi et al., 2024; Li et al., 2022; Tong et al., 2024a) aim to achieve multimodal intelligence by jointly understanding and generating visual and language information. Inspired by the remarkable success of recent large language models (LLMs) (Touvron et al., 2023; Chiang et al., 2023; Hua et al., 2021), researchers are now exploring large VLMs that combine pre-trained visual encoders and language decoders to tackle complex multimodal tasks. Flamingo (Alayrac et al., 2022) and BLIP-2 (Li et al., 2023b) are two of the early works that explore the integration of LLMs into vision-language pre-training. These models are trained as VL foundation models. Beginning with LLaVA (Liu et al., 2024a), researchers have used LLM-synthesized instruction-following chat data in VQA format for instruction tuning, achieving significantly improved results (Hua et al., 2024a). Subsequent studies have expanded to explore the broader capabilities of multimodal LLMs (Hu et al., 2023; Guan et al., 2024; Lin et al., 2023; Yu et al., 2024c; Tang et al., 2023). However, these efforts place less emphasis on improving the models ability to fine-grained compositional perception and reasoning. 4 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Figure 2: The statistics of 13 distinct categories of QA pairs in MMCOMPOSITION and some models performance on each category."
        },
        {
            "title": "3 MMCOMPOSITION",
            "content": "3.1 DATA CURATION To ensure comprehensive and high-quality benchmark, we develop an efficient pipeline for curating VQA data that accurately reflects compositional information. Data Collection. We use various datasets with the potential to construct VL compositional QA pairs as our seed data. This collection includes datasets that contain the description of objects, attributes, relations, and counting, such as VL-CheckList (Zhao et al., 2022), Sugar-Crepe (Hsieh et al., 2024), ARO (Yuksekgonul et al., 2022), Crepe (Ma et al., 2023), and DOCCI (Onoe et al., 2024). Additionally, we incorporate sources that are well-suited for constructing VL compositional reasoning QA pairs, including SVO-Probes (Hendricks & Nematzadeh, 2021), VSR (Liu et al., 2023a), BLINK (Fu et al., 2024), GQA (Hudson & Manning, 2019), Visual Genome (Krishna et al., 2016), and CLVER (Johnson et al., 2017). It also contains datasets with multiple images in each sample, such as Winoground (Thrush et al., 2022), MuriBench (Wang et al., 2024a) and NLVR2 (Suhr et al., 2017). Question and Answer Construction. We obtain QA pairs from the seed data in through several methodologies: For the seed data that only contain positive and negative captions (e.g., ARC (Yuksekgonul et al., 2022)), we first generate sentence embeddings for each caption using Sentence-BERT (Reimers & Gurevych, 2019). We then utilize these embeddings to retrieve the most similar captions from the Visual Genome (Krishna et al., 2016) dataset. This process results in four captions per image in each sample, forming four answer options per question. For data samples containing multiple images such as those in the image difference spotting task, which includes two images per question we concatenate the two images side by side and label them Left and Right beneath each sub-image. This setup allows for two types of question-answer options: Left and Right for questions asking which sub-image is described by caption, and True and False for questions determining the accuracy of caption describing the image difference. For tasks that include more than two images per question (e.g., visual similarity assessments), we concatenate all images into single composite image and label each sub-image as Image1, ..., Imagei. For the probing task, we select several captions from the dense captions in Visual Genome (Krishna et al., 2016) as the correct options and write the misaligned captions manually for the image. Then, we randomly select {1, 2, 3, 4} captions from the set of accurate captions for given image and complement these with 4 incorrect options drawn from set of conflict captions. With this approach, we can obtain the indefinite-choice QA pairs. 5 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Data Filtering and Difficulty Classification. We divide the data into different difficulty levels: easy, medium, hard, and super hard. To achieve this, we use voting system with six models, ranging from weaker to stronger, including LLaVA-1.5-13B (Liu et al., 2024b), LLaVA-1.6-Mistral-7B (Liu et al., 2024a), LLaVA-1.6-Vicuna-13B (Liu et al., 2024a), Phi-3-Vision-128K-Instruct (Abdin et al., 2024), InternVL-Chat-V1.5 (Chen et al., 2024b), and Qwen-VL-Chat (Bai et al., 2023). Based on the accuracy of model predictions for each question, questions are categorized into different difficulty levels. Questions with zero correct predictions are classified as super hard, those with one or two correct predictions are labeled as hard, questions with three or four correct predictions are considered medium, and those with more than five correct predictions are categorized as easy. The overall difficulty of the dataset is then controlled by adjusting the ratio of questions at each difficulty level. Human Annotation. All QA pairs in the benchmark are human-annotated. Annotators first assess image quality to ensure it meets the required standards. For human-created data, annotators are first trained with detailed instructions to develop thorough understanding of the compositional aspects in our dataset. During annotation, they generate QA pairs based on the provided aspect prompts. For GPT-synthesized data sourced from DOCCI, annotators verify whether the question accurately reflects the compositional information in the image and whether the answer appropriately corresponds to the question. 3.2 EVALUATION METRIC t=1}D Let = {Dm = {Tt}Td m=1 denotes our dataset , where each catagory Dm consists of Td subtasks. For each subtask, we calculate the accuracy across all annotations. For each question D, let Aq be the set of correct options, Pq be the set of predicted (selected) options. The score for question q, denoted as sq, is calculated as: sq = 1, Pq Aq , 0, if Pq = Aq if Pq Aq otherwise Here, denotes the number of options selected by the participant and the number of correct options, Pq Aq means all selected options are correct, but some correct options are missing (underselection). The otherwise case covers instances where incorrect options are selected (wrong or overselection). This equation applies to both the single-choice and indefinite-choice questions. The final weighted average accuracy across all categories is calculated as ACC = (cid:80)D t=1 sq Tt/Dd, m=1 where is the question number in one set. (cid:80)Td 3.3 QUANTITATIVE ANALYSIS MMCOMPOSITION contains 13 different VL composition tasks, including Attribute Perception (Attr-P), Object Perception (Obj-P), Counting Perception (Count-P), Relation Perception (RelP), Difference Spotting (Diff-S), Text Rendering (TR), Visual Similarity (Visual-Sim), Attribute Reasoning (Attr-R), Object Reasoning (Obj-R), Counting Reasoning (Count-R), Relation Reasoning (Rel-R), Object Interaction (Obj-Interact), and Compositional Probing (Prob). We use GPT-4o to label each question category via in-context learning, followed by manual verification for accuracy. Figure 4 illustrates the difficulty distribution of MMCOMPOSITION, highlighting the challenging nature of our dataset. Figure 5 depicts the distribution of option counts per question, with over half of the data containing more than four options. To analyze the impact of input resolution on model performance, we further display the resolution distribution of images in Figure 6, which reflects the image quality of our data. For textual analysis, we visualize the phrase distribution of questions using word cloud diagram in Figure 7, clearly depicting the word frequency and distribution across the questions. We also provide detailed explanation for these 13 categories in Section A.3."
        },
        {
            "title": "4 REVISITING THE COMPOSITIONALITY OF PRE-TRAINED",
            "content": "VISION-LANGUAGE MODELS In this section, we quantify and explore the compositionality of state-of-the-art VLMs and provide comprehensive evaluation of VLMs. For all experiments, we use consistent prompt template and the official default hyperparameters for each model. 6 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Table 2: The comprehensive performance of 54 VLMs on Acc, including open source models and API-based models . The best and second best results are in bold and underlined, respectively. Method Human InternVL2-40B (Chen et al., 2024b) InternVL2-76B (Chen et al., 2024b) Qwen2-VL-72B (Wang et al., 2024b) InternVL-Chat-V1.2-Plus (Chen et al., 2024b) InternVL2-26B (Chen et al., 2024b) VILA-40B (Lin et al., 2024a) GPT-4o (Achiam et al., 2023) InternVL-Chat-V1.2 (Chen et al., 2024b) InternVL-Chat-V1.5 (Chen et al., 2024b) InternVL2-8B (Chen et al., 2024b) LLaVA-V1.6-34B (Liu et al., 2024a) MiniCPM-V2.6 (Yao et al., 2024) InternLM-XComposer2-4KHD-7B (Dong et al., 2024b) Qwen-VL-Max (Bai et al., 2023) InternLM-XComposer2.5-7B (Zhang et al., 2024a) Hunyuan-Vision InternLM-XComposer2-VL-7B (Dong et al., 2024a) Gemini-1.5-Pro (Reid et al., 2024) Mini-Gemini-34B (Li et al., 2023c) InternVL2-4B (Chen et al., 2024b) LLaMA-3.2-11B-Vision-Instruct MiniCPM-Llama3-V2.5 (Yao et al., 2024) Mini-Gemini-34B-HD (Li et al., 2023c) Bunny-Llama-3-8B-V (He et al., 2024) Mini-Monkey (Huang et al., 2024) Phi3.5-Vision-Instruct (Abdin et al., 2024) ColgVLM2-Llama3-Chat-19B (Hong et al., 2024) Phi3-Vision-128K-Instruct (Abdin et al., 2024) Yi-VL-34B (AI et al., 2024) Step-1V-32K ConvLLaVA-1024-7B (Ge et al., 2024) Yi-VL-6B (AI et al., 2024) Bunny-3B (He et al., 2024) Bunny-4B-V1.0 (He et al., 2024) LLaVA-HR-13B (Luo et al., 2024) ConvLLaVA-1536-7B (Ge et al., 2024) InternVL2-2B (Chen et al., 2024b) Monkey-Chat (Li et al., 2024) Mini-Gemini-13B (Li et al., 2023c) SliME-7B (Zhang et al., 2024b) INF-LLaVA (Ma et al., 2024) SliME-8B (Zhang et al., 2024b) INF-LLaVA (Ma et al., 2024) LLaVA-HR-7B (Luo et al., 2024) SliME-13B (Zhang et al., 2024b) ConvLLaVA-768-7B (Ge et al., 2024) InternVL2-1B (Chen et al., 2024b) Mini-Gemini-13B-HD (Li et al., 2023c) Qwen-VL-Chat (Bai et al., 2023) DeepStack-L-HD-Vicuna-7B (Meng et al., 2024) DeepStack-L-Vicuna-7B (Meng et al., 2024) LLaVA-V1.6-Vicuna-13B (Liu et al., 2024a) LLaVA-V1.6-Mistral-7B (Liu et al., 2024a) LLaVA-V1.5-13B (Liu et al., 2024b) Random Choice Attr-P Obj-P Count-P Rel-P Diff-S TR Visual-Sim Attr-R Obj-R Count-R Rel-R Obj-Interact Prob Overall Perception Reasoning Probing 72.22 70.65 59.57 69.81 68.46 65.70 63.97 64.58 59.44 62.68 67.24 65.19 62.24 53.76 56.68 61.95 59.18 55.30 58.35 53.82 54.82 51.93 54.95 58.16 52.25 55.01 57.67 55.30 53.02 46.11 51.73 51.99 49.97 52.50 50.32 50.03 43.32 49.20 43.71 45.70 43.19 46.50 45.66 40.46 47.46 46.50 43.13 42.29 41.97 43.29 45.47 37.09 36.55 30.92 23.12 77.69 76.75 51.80 66.73 69.57 66.16 58.98 64.08 62.38 61.44 69.00 61.06 58.03 54.82 57.84 65.03 55.39 53.50 59.17 55.01 58.98 51.23 52.36 53.50 59.36 48.39 54.44 43.86 39.89 42.16 47.26 45.75 50.66 47.64 42.91 46.50 54.82 49.53 41.21 45.94 46.69 43.29 42.16 43.67 40.64 40.45 48.02 38.37 36.67 37.05 41.21 29.30 29.68 28.36 23.63 45.21 48.28 52.49 43.68 40.23 45.21 37.93 41.38 38.31 31.80 44.06 41.00 39.08 36.40 37.93 37.16 40.23 39.46 37.93 31.42 36.02 36.40 37.55 34.87 26.82 30.27 34.48 30.27 30.27 26.44 32.57 30.27 26.82 39.08 35.25 28.35 26.82 24.14 27.20 28.74 32.95 32.18 27.59 31.42 28.74 28.35 22.99 32.18 25.67 28.74 27.20 23.75 24.14 28.35 21. 72.53 70.00 62.52 69.02 66.96 63.65 66.76 62.98 60.47 59.54 61.31 61.80 58.36 58.67 56.82 58.58 56.91 57.11 53.70 52.17 55.80 49.88 48.35 54.07 52.53 52.61 51.69 51.61 50.33 46.25 44.96 49.34 48.79 46.00 39.81 41.25 45.79 47.13 41.63 40.76 41.92 40.27 47.37 40.43 42.55 34.88 43.29 40.20 39.13 35.74 36.00 32.43 39.24 29.89 25.85 31.12 19.09 45.23 31.12 22.82 23.65 32.37 25.73 21.58 25.31 25.73 21.99 23.65 22.82 21.58 26.97 25.31 24.48 25.31 18.26 30.29 19.92 27.80 21.58 26.56 21.16 38.17 25.31 26.14 25.31 28.22 25.73 25.73 21.16 32.37 27.39 22.82 16.60 21.58 31.12 24.48 30.29 33.20 28.22 22.41 16.60 23.24 18.67 24.48 18.67 21.99 24.90 31.12 29.46 29.46 73.21 78.57 82.14 78.57 80.36 75.00 82.14 76.79 76.79 73.21 76.79 73.21 67.86 80.36 71.43 76.79 66.07 67.86 73.21 73.21 69.64 76.79 73.21 50.00 73.21 66.07 57.14 69.64 64.29 67.86 69.64 60.71 50.00 51.79 66.07 69.64 67.86 69.64 62.50 62.50 57.14 60.71 57.14 64.29 66.07 66.07 64.29 67.86 67.86 60.71 60.71 66.07 64.29 64.29 35. - 48.65 48.65 67.57 28.38 62.16 44.59 60.81 29.73 51.35 33.78 21.62 37.84 27.03 41.89 28.38 36.49 31.08 55.41 39.19 25.68 29.73 20.27 40.54 12.16 18.92 31.08 48.65 40.54 17.57 43.24 21.62 20.27 12.16 17.57 27.03 29.73 17.57 13.51 33.78 20.27 20.27 25.68 32.43 37.84 17.57 22.97 18.92 24.32 16.22 17.57 18.92 12.16 13.51 14.86 25.68 83.78 85.14 87.84 78.83 79.28 70.72 62.61 63.06 77.93 78.83 53.15 63.96 70.72 53.60 71.17 61.26 67.57 59.91 54.50 77.03 50.90 69.37 59.91 45.95 68.92 45.05 50.90 45.05 50.45 66.67 55.41 45.05 46.40 43.69 45.50 51.35 63.06 51.35 55.86 43.24 50.00 44.59 46.40 46.85 45.50 53.15 54.05 51.35 41.89 46.85 42.34 40.99 36.94 45.50 36.94 82.57 83.49 84.40 77.98 79.82 77.06 79.82 71.56 83.49 75.23 67.89 73.39 74.31 65.14 75.23 72.48 73.39 74.31 73.39 71.56 67.89 77.06 72.48 66.06 65.14 63.30 65.14 65.14 56.88 66.97 65.14 51.38 61.47 62.39 60.55 64.22 58.72 58.72 68.81 59.63 66.06 61.47 60.55 60.55 56.88 69.72 58.72 63.30 61.47 60.55 56.88 54.13 47.71 46. 46.79 84.51 85.40 84.51 80.53 81.86 67.26 61.95 61.06 78.32 73.89 53.10 68.14 60.18 53.98 61.06 56.19 61.06 50.44 58.41 72.57 51.33 68.14 58.85 53.10 59.29 53.10 47.35 47.79 55.31 62.83 53.10 52.21 47.79 52.21 45.58 52.21 49.56 44.25 50.44 48.23 55.31 47.79 42.92 48.67 47.79 54.42 49.12 45.58 53.54 45.13 42.04 42.04 42.04 37.17 38.50 69.20 70.93 71.51 67.13 64.59 69.32 61.13 63.44 62.05 62.05 61.59 55.25 58.71 61.36 60.55 54.09 55.02 56.29 57.90 56.40 53.29 58.13 60.09 51.67 52.71 56.40 44.75 48.79 52.94 52.13 53.06 51.56 46.14 49.94 51.90 52.13 46.94 46.60 53.06 53.17 48.33 53.29 44.98 49.25 49.83 49.02 45.91 49.83 47.52 46.94 46.83 50.29 41.18 43.60 35.64 65.85 67.07 70.12 60.98 63.41 59.15 75.00 65.24 63.41 62.20 54.27 60.98 59.15 62.80 60.98 59.15 53.66 65.24 61.59 55.49 60.98 62.20 66.46 57.32 50.00 53.66 59.15 60.37 52.44 59.76 54.88 51.83 51.22 52.44 57.32 64.02 53.66 51.22 57.32 53.05 54.27 47.56 54.88 56.71 56.10 55.49 57.93 56.71 58.54 59.15 50.61 48.78 49.39 46. 47.65 59.59 58.46 69.57 65.80 52.43 62.16 54.65 60.71 57.01 54.10 58.17 54.43 60.02 63.87 49.64 45.03 57.15 49.60 41.79 41.18 49.17 41.79 35.91 59.44 42.37 54.65 50.69 56.75 53.88 45.46 40.89 48.76 55.08 42.66 48.80 34.20 38.16 48.91 32.28 30.03 31.41 29.96 35.58 33.04 33.55 37.11 27.89 34.28 41.54 35.88 30.21 38.16 38.24 41.39 28.61 90.31 67.95 67.28 65.24 64.94 63.08 62.38 59.71 59.61 59.58 58.47 58.25 57.01 56.69 55.18 55.10 54.64 54.62 53.27 53.06 52.03 52.01 51.54 51.48 50.81 50.41 50.02 49.84 48.52 47.86 47.64 47.32 46.87 46.32 46.07 46.02 45.52 45.11 44.90 43.74 43.45 43.32 43.29 43.04 42.73 42.63 42.40 42.06 41.99 41.64 40.26 39.75 38.03 37.18 36.07 30. Overall performance. The overall performance indicates that models struggle with perceiving and reasoning about fine-grained VL compositional information. The best human expert achieves an accuracy of 90.31%, significantly outperforming all the models reported in the table. This demonstrates the still existing gap between human expertise and the performance of current models on the MMCOMPOSITION benchmark. This reflects the benchmarks rigorous standards. The opensource InternVL2 (Chen et al., 2024c) series models secured first and second place on the leaderboard. InternVL2-40B performs better than InternVL2-76B. Among the API-based models, Qwen2-VL and GPT-4o achieved the best and second best performance. The superior performance of open-source models with relatively smaller language models compared to GPT-4o, which has larger language model, is due to their more effective visual encoders. The mean accuracy of 7B and 13B open-source VLMs hovers around 3638%. For reference, we provide the random guess accuracy (30.15%) as lower bound for the benchmark. The tasks where VLMs exhibit relative strengths and weaknesses. From Table 2, we observe that VLMs perform relatively better on tasks such as Attribute, Object, and Relation Perception, as well as Attribute, Object, and Count Reasoning, where they perform much better than other categories. However, they struggle with tasks such as Count Perception, Difference Spotting, Visual Similarity, and Probing (see illustrations in Fig. 1). These tasks often involve multiple images, some with extreme aspect ratios, and the probing tasks include indefinite-choice questions, which pose additional challenges for the models. GPT-4o performs relatively weaker on Obj-P, Count-P, Attr-R, Count-R, and Rel-R tasks compared to smaller models that outperform it, aligning with the limitations outlined in the official GPT-4o documentation. Overall, the models perform relatively well on mid-level perception and reasoning tasks. 7 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models"
        },
        {
            "title": "COMPOSITIONALITY",
            "content": "In this section, we analyze the factors that may influence the compositionality of VLMs. We focus on three dominant factors: visual encoder design, language decoder size, and training data volume."
        },
        {
            "title": "5.1 VISUAL ENCODER DESIGN",
            "content": "High-resolution visual encoders. common approach to enhance models capability to perceive fine-grained visual content is to introduce higher-resolution encoders. In this study, we employ the control variable method, where the input resolution of the encoders is the only variable, while the training data and text decoders remain fixed. From Table 3, we observe that models with higher resolution encoders demonstrate superior capability for multimodal compositional perception and reasoning. However, for the Mini-Gemini series models, the introduction of high-resolution encoder with patch info mining mechanism unexpectedly resulted in performance decline. Table 3: Performance comparison of models with and without high-resolution encoders (Avg. refers to average resolution). Method ConvLLaVA-768-7B (Ge et al., 2024) ConvLLaVA-1024-7B (Ge et al., 2024) ConvLLaVA-1536-7B (Ge et al., 2024) LLaVA-1.5-13B (Liu et al., 2024a) LLaVA-HR-13B (Luo et al., 2024) DeepStack-L-Vicuna-7B (Meng et al., 2024) DeepStack-L-HD-Vicuna-7B (Meng et al., 2024) Mini-Gemini-13B (Li et al., 2023c) Mini-Gemini-13B-HD (Li et al., 2023c) Mini-Gemini-34B (Li et al., 2023c) Mini-Gemini-34B-HD (Li et al., 2023c) Resolution Visual Tokens Perception Avg. 1055*813 Reasoning Avg. 935*535 Probing Avg. 849* Overall 768 1024 1536 336 1024 672 1344 768 1536 768 144 256 576 576 576 2880 14400 576 576 576 576 36.51 43.70+7.19 41.84+5.33 29.91 41.83+11.92 36.92 35.191.73 38.51 37.241.27 51.25 47.733. 52.46 54.41+1.95 54.09+1.63 43.45 51.26+7.81 46.60 48.87+2.27 54.60 51.073.53 58.94 61.40+2.46 37.11 40.89+3.78 34.206.69 41.39 48.80+7.41 30.21 35.88+5.67 32.28 34.28+2.00 41.79 35.915.88 42.40 47.32+4.92 45.52+3.12 36.07 46.02+9.95 39.75 40.26+0.51 43.74 41.991.75 53.06 51.480.58 Mixture-of-encoder. Another approach to enhancing visual encoders is the use of mixture-ofencoder architecture. In this setup, image features are extracted by combination of high-resolution and low-resolution encoders, providing rich visual information to the language decoders. We analyze the relationship between the mixture-of-encoder architecture and model performance by aggregating different encoders while keeping the training data and decoders fixed. We use the LLaVA-1.5 pretraining data for stage-1 pretraining and the EAGLE 1.8M dataset (Bi et al., 2024) for stage-2 fine-tuning. The initial encoder is CLIP model with 448 resolution (Radford et al., 2021), and the decoder is LLaMA-3-8B (Dubey et al., 2024). We scale up the encoders using: (A) ConvNeXt (Liu et al., 2022), (B) SAM (Kirillov et al., 2023), (C) DINOv2 (Oquab et al., 2023), and (D) Pix2Struct (Lee et al., 2023). The empirical results in Table 4 indicate that combining CLIP with encoder improves the models performance; however, as the number of visual encoders increases, the models performance declines. Table 4: comparative analysis of various mixture-of-encoder architectures in relation to model compositionality. Method Visual Encoders Relolution Perception Reasoning Probing Overall LLaVA-1.5 (Liu et al., 2024a) LLaVA-1.5+A LLaVA-1.5+A+B LLaVA-1.5+A+B+C LLaVA-1.5+A+C+D CLIP CLIP+A CLIP+A+B CLIP+A+B+C CLIP+A+C+D 448 1024 1024 1024 1024 44.93 45.90+0.97 45.96+1.03 43.411.52 44.900.03 54.16 53.340.82 52.461.7 52.142.02 51.572.59 53.34 56.93+3.59 49.024.32 54.21+0.87 54.86+1.55 49.19 49.82+0.63 48.660.53 47.741.45 48.390.90 Visual encoder has more significant impact on the models compositionality, while GPT-4o struggles with processing higher-resolution images. By summarizing the empirical results of this study, we find that for relatively simple QA tasks, only small portion of its language capabilities are utilized (compared to the models outperforming GPT-4o, whose language model size is only 70B). Once the language decoder size reaches certain threshold (e.g., 34B, 70B), the visual encoder plays more critical role in the models performance. As discussed in Section A.2, Qwen2VL processes 8 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Figure 3: Interpretable analysis of different VLMs. Green letters indicate correct answers, while red letters represent wrong (predicted) answers. 9 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models images by largely preserving their original resolution and aspect ratio. The Internvl-2 series models employ dynamic any-resolution encoding strategy: images are first mapped to an optimal aspect ratio from predefined ratios, then divided into 448 448 pixel tiles, with each tile converted into 256 image tokens. These approaches enable the encoders to handle images of any resolution and aspect ratio with minimal degradation of image quality. In contrast, GPT-4o processes images with downsampling when the images longest side > 2048px or shortest side > 768px (our data contains 889 such examples), contributing to its inferior performance compared to other open-source models."
        },
        {
            "title": "5.2 THE VOLUME OF TRAINING DATA",
            "content": "The volume of training data is crucial factor influencing models performance. In this study, we conduct comparison analysis of this factor. In Table 5, we observe significant performance increase when the training data is scaled up substantially. For instance, InternVL-Chat-V1.2 and InternVL-Chat-V1.2-Plus, which use 10 times more training data than the former, show significant performance improvements. Table 5: The comparison of models with and without training data scale up. Method Dataset Size Perception Reasoning Probing Overall INF-LLaVA (Ma et al., 2024) INF-LLaVA (Ma et al., 2024) InternVL-Chat-V1.2 (Chen et al., 2024c) InternVL-Chat-V1.2-Plus (Chen et al., 2024c) InternVL-Chat-V1.5 (Chen et al., 2024b) InternVL2-26B (Chen et al., 2024b) 1.25M 2.56M 1.2M 12M 41.80 40.131.67 56.49 60.73+4.24 54.14 60.40+6.26 46.98 51.39+4.41 63.79 70.78+6.99 68.20 70.03+0.83 35.58 31.414.17 60.71 65.80+5.09 57.01 52.434.58 43.04 43.32+0.28 59.61 64.94+5.33 59.58 63.08+3.5 5.3 LANGUAGE DECODER SIZE From Table 2, we observe that models with larger decoders demonstrate stronger performance. To analyze this relationship more accurately, we compare models with different decoder sizes while keeping the encoder and training data constant. The results are shown in Table 6, from which we can conclude that larger language decoders result in better performance. Table 6: Performance comparison of models with and without high-resolution encoders (Avg. refers to average resolution). Method ConvLLaVA-768-7B (Ge et al., 2024) ConvLLaVA-1024-7B (Ge et al., 2024) ConvLLaVA-1536-7B (Ge et al., 2024) LLaVA-1.5-13B (Liu et al., 2024a) LLaVA-HR-13B (Luo et al., 2024) DeepStack-L-Vicuna-7B (Meng et al., 2024) DeepStack-L-HD-Vicuna-7B (Meng et al., 2024) Mini-Gemini-13B (Li et al., 2023c) Mini-Gemini-13B-HD (Li et al., 2023c) Mini-Gemini-34B (Li et al., 2023c) Mini-Gemini-34B-HD (Li et al., 2023c) Resolution Visual Tokens Perception Avg. 1055*813 Reasoning Avg. 935*535 Probing Avg. 849* Overall 768 1024 1536 336 1024 672 1344 768 1536 768 144 256 576 576 576 2880 14400 576 576 576 576 36.51 43.70+7.19 41.84+5.33 29.91 41.83+11.92 36.92 35.191.73 38.51 37.241.27 51.25 47.733. 52.46 54.41+1.95 54.09+1.63 43.45 51.26+7.81 46.60 48.87+2.27 54.60 51.073.53 58.94 61.40+2.46 37.11 40.89+3.78 34.206.69 41.39 48.80+7.41 30.21 35.88+5.67 32.28 34.28+2.00 41.79 35.915.88 42.40 47.32+4.92 45.52+3.12 36.07 46.02+9.95 39.75 40.26+0.51 43.74 41.991.75 53.06 51.480.58 5.4 INTERPRETABLE ANALYSIS OF MODEL DEFICIENCIES We conduct comprehensive error analysis to better understand the models deficiencies in finegrained compositional understanding. In this analysis, the models are required to answer questions and provide explanations in multi-turn dialogue format. Figures 3, 14, and 15 illustrate the reasons why the models fail to predict the correct answers for each task. For example, in the Obj-P task (example 3), while the yellow colored outline is easily detected by humans, the models struggle to accurately identify the target objects due to the outline being mixed with numerous other characters. Additionally, the models face difficulties with fine-grained object counting, especially when several similar objects are present. In the Count-R (example 6) task, for instance, humans can precisely count the number of triangles on wheel, but the models confuse the six irregular polygons for triangles. 10 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models"
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper introduces MMCOMPOSITION, novel high-quality benchmark for evaluating VLM compositionality. With MMCOMPOSITION, we comprehensively evaluate the compositionality of notable VLMs. Our evaluation reveals significant gap between these models and human performance, providing insights into the limitations of existing VLMs. Additionally, we systematically analyze factors that may influence compositionality, including visual encoder design, training data volume, and language decoder size. We find that for relatively simple QA tasks, only small portion of the language models capacity is utilized (as seen in models outperforming GPT-4o, whose language model has 70B parameters). Once the language decoder reaches certain size threshold (e.g., 34B, 70B), the visual encoder has more pronounced impact on compositionality. In summary, our work provides comprehensive and precise framework for evaluating the compositionality of VLMs, identifies key areas for improvement, and suggests potential directions for future advancements."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, et al. Yi: Open foundation models by 01.ai, 2024. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 2022. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 24252433, 2015. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. Jing Bi, Yunlong Tang, Luchuan Song, Ali Vosoughi, Nguyen Nguyen, and Chenliang Xu. EAGLE: Egocentric AGgregated language-video engine. In ACM Multimedia 2024, 2024. URL https: //openreview.net/forum?id=mk8p2JKdu0. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024a. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024b. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024c. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. Noam Chomsky. Aspects of the Theory of Syntax. MIT press, 2014. 11 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024a. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. arXiv preprint arXiv:2404.06512, 2024b. Abhimanyu Dubey, Abhinav Jauhri, Pandey, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Chaoyou Fu, Peixian Chen, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. ArXiv, abs/2306.13394, 2023. URL https://api.semanticscholar. org/CorpusID:259243928. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. Chunjiang Ge, Sijie Cheng, Ziming Wang, Jiale Yuan, Yuan Gao, Jun Song, Shiji Song, Gao Huang, and Bo Zheng. Convllava: Hierarchical backbones as visual encoder for large multimodal models. ArXiv, abs/2405.15738, 2024. URL https://api.semanticscholar.org/CorpusID: 270045537. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1437514385, 2024. Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, and Bo Zhao. Efficient multimodal learning from data-centric perspective. arXiv preprint arXiv:2402.11530, 2024. Lisa Anne Hendricks and Aida Nematzadeh. Probing image-language transformers for verb understanding. arXiv preprint arXiv:2106.09141, 2021. Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. Advances in Neural Information Processing Systems, 36, 2024. Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah Smith, and Jiebo Luo. Promptcap: Prompt-guided image captioning for vqa with gpt-3. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 29632975, 2023. Hang Hua, Xingjian Li, Dejing Dou, Cheng-Zhong Xu, and Jiebo Luo. Noise stability regularization for improving bert fine-tuning. arXiv preprint arXiv:2107.04835, 2021. Hang Hua, Jing Shi, Kushal Kafle, Simon Jenni, Daoan Zhang, John Collomosse, Scott Cohen, and Jiebo Luo. Finematch: Aspect-based fine-grained image and text mismatch detection and correction. arXiv preprint arXiv:2404.14715, 2024a. Hang Hua, Yunlong Tang, Chenliang Xu, and Jiebo Luo. V2xum-llm: Cross-modal video summarization with temporal prompt instruction tuning. arXiv preprint arXiv:2404.12353, 2024b. Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, and Xiang Bai. Mini-monkey: Multiscale adaptive cropping for multimodal large language models. arXiv preprint arXiv:2408.02034, 2024. 12 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2017. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollr, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123:32 73, 2016. URL https://api.semanticscholar. org/CorpusID:4492210. Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In International Conference on Machine Learning, pp. 1889318912. PMLR, 2023. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning. PMLR, 2023b. Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv:2403.18814, 2023c. Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2668926699, 2024a. Jingyang Lin, Hang Hua, Ming Chen, Yikang Li, Jenhao Hsiao, Chiuman Ho, and Jiebo Luo. Videoxum: Cross-modal visual and textural summarization of videos. IEEE Transactions on Multimedia, 2023. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV. Springer, 2024b. Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023a. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge (january 2024). URL https://llava-vl. github. io/blog/2024-01-30-llava-next, 1(8), 2024a. 13 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023b. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Jian Lu, Shikhar Srivastava, Junyu Chen, Robik Shrestha, Manoj Acharya, Kushal Kafle, and Christopher Kanan. Revisiting multi-modal llm evaluation. arXiv preprint arXiv:2408.05334, 2024. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003, 2024. Yiwei Ma, Zhibin Wang, Xiaoshuai Sun, Weihuang Lin, Qiang Zhou, Jiayi Ji, and Rongrong Ji. Inf-llava: Dual-perspective perception for high-resolution multimodal large language model, 2024. Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. Crepe: Can vision-language foundation models reason compositionally? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1091010921, 2023. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 31953204, 2019. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. ArXiv, abs/2203.10244, 2022. URL https://api.semanticscholar.org/CorpusID:247593713. Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. V. Jawahar. Docvqa: dataset for vqa on document images. 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 21992208, 2020. URL https://api.semanticscholar.org/CorpusID:220280200. Lingchen Meng, Jianwei Yang, Rui Tian, Xiyang Dai, Zuxuan Wu, Jianfeng Gao, and Yu-Gang Jiang. Deepstack: Deeply stacking visual tokens is surprisingly simple and effective for lmms, 2024. Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Jason Baldridge. DOCCI: Descriptions of Connected and Contrasting Images. In arXiv:2404.19753, 2024. Maxime Oquab, Timothe Darcet, Moutakanni, et al. Dinov2: Learning robust visual features without supervision, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Arijit Ray, Filip Radenovic, Abhimanyu Dubey, Bryan Plummer, Ranjay Krishna, and Kate Saenko. Cola: How to adapt vision-language models to compose objects localized with attributes? arXiv preprint arXiv:2305.03689, 2023. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Alayrac, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 14 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Conference on Empirical Methods in Natural Language Processing, 2019. URL https: //api.semanticscholar.org/CorpusID:201646309. Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. corpus of natural language for visual reasoning. In Annual Meeting of the Association for Computational Linguistics, 2017. URL https://api.semanticscholar.org/CorpusID:19435386. Yunlong Tang, Jing Bi, Siting Xu, et al. Video understanding with large language models: survey. arXiv preprint arXiv:2312.17432, 2023. Yunlong Tang, Daiki Shimada, Jing Bi, and Chenliang Xu. Avicuna: Audio-visual llm with interleaver and context-boundary alignment for temporal referential dialogue. arXiv preprint arXiv:2403.16276, 2024. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 52385248, 2022. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024a. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024b. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International conference on machine learning. PMLR, 2024a. Weihao Yu, Zhengyuan Yang, Linfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. arXiv preprint arXiv:2408.00765, 2024b. Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, and Jiebo Luo. Promptfix: You prompt and we fix the photo. arXiv preprint arXiv:2405.16785, 2024c. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International Conference on Learning Representations, 2022. Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024a. Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024b. Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin. Vl-checklist: Evaluating pre-trained vision-language models with objects, attributes and relations. arXiv preprint arXiv:2207.00221, 2022."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 QUANTITIVE RESULTS OF MMCOMPOSITION In this section, we show statistical results for MMCOMPOSITION in Figure 4 through Figure 7. Figure 4: Distribution of difficulty levels across the question set, illustrating the challenging nature of tasks. Figure 5: Distribution of option counts per question, showing the variety in answer choices provided to evaluate VLMs. Figure 7: Word cloud of key terms from the questions, illustrating the diversity of compositional content evaluated in the benchmark. Figure 6: Resolution distribution of images in our benchmark, reflecting the portion of highquality images in MMCOMPOSITION. 16 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models A.2 COMPARISON ANALYSIS OF IMAGE ENCODING IN GPT-4O, QWEN2-VL, AND INTERNVL-2 In GPT-4o, when the image detail parameters are set to high, images are first scaled to fit within 2048 2048 square while maintaining their aspect ratio. Then, the images are further scaled so that the shortest side is 768px long. Finally, GPT-4o calculates how many 512px squares the image contains, with each square costing 170 tokens. An additional 85 tokens for low resolution are always added to the final total. As result, GPT-4o does not achieve true any resolution image processing. In Qwen2-VL and InternVL-2, the image encoders adopt dynamic any resolution encoding strategy. The images are first mapped to an optimal aspect ratio from predefined ratios, then divided into 448 448 or 28 28 pixel tiles, with each tile converted into 256 or 1 image tokens. thumbnail is then generated to capture the global context. This allows the encoders to handle images of any resolution and aspect ratio. Furthermore, the image encoder in Qwen2-VL is 675M ViT with twodimensional positional encoding mechanism, while InternVL-2 utilizes the more powerful InternViT with 6B parameters. This distinction contributes to the superior performance of the compositionality of Qwen2-VL and InternVL-2 in our benchmark. In Table 7, we provide comparison of the properties of visual encoders for the aforementioned models. Table 7: Visual encoder comparison of GPT-4o, InternVL2 and Qwen2-VL. Method Visual Encoder Image Tile Size Maximum Number of Tiles Maximum Aspect Ratio # of Tokens for One Tile GPT-4o InternVL2 Qwen2-VL - InternViT-6B ViT-675M 512 512 448 448 28 28 8 12 dynamic any 1:6 any 170 256 1 A.3 DEFINITION OF 13 DISTINCT CATEGORIES IN MMCOMPOSITION Attribute Perception: The specific attributes or properties of the object perception task that can be solved by humans within blink. Object Perception: Identification or recognition of objects in the image. Counting Perception: Counting the number of objects or elements in the image. Relation Perception: Understanding the relationships between objects in the image. Difference Spotting: Identifying differences or changes between objects or scenes in two similar images. Text Rendering: Reading or interpreting text present in the image. Visual Similarity: Comparing similarities between objects or elements across multiple images. Attribute Reasoning: Identifying and reasoning about specific attributes or properties of objects in the image. Object Reasoning: Identifying and reasoning about objects in the image. Counting Reasoning: Identifying and reasoning about the number of objects or elements in the image. Relation Reasoning: Identifying and reasoning about the spatial arrangement or positioning of objects in the image. Object Interaction: Understanding interactions among multiple objects in the image. VL Composition Probing: Examining the composition or combination of visual and textual elements in images, where models are required to accurately find all the complex compositional descriptions about the image. A.4 CHARACTERISTICS OF QUESTIONS WHERE MODELS UNDERPERFORM We define the comprehensive performance value (CPV) for each question as the average score across 54 VLMs. By comparing each questions CPV with the score of random choice within its class, we find that 1,159 questions have CPV lower than that of random chance. We show statistical results questions with low CPV in Figure 8 through Figure 11. 17 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Figure 8: Distribution of difficulty levels of questions with low CPV, illustrating the authenticity of the difficulty distribution in MMCOMPOSITION. Figure 9: Distribution of option counts for questions with low CPV. Figure 10: Resolution distribution of images with low CPV. Figure 11: Word cloud of key terms from the questions with low CPV, illustrating the keywords appearing in the questions that VLMs are hard to answer currently. 18 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models A.5 ANALYSIS OF GPT-4OS UNDERPERFORMANCE IN SPECIFIC TASKS Since GPT-4o performs relatively weaker on Obj-P, Count-P, Attr-R, Count-R, and Rel-R tasks compared to smaller models that outperform it, we aim to provide an intuitive analysis of the reasons behind its poor performance on these tasks. Figure 12 presents interpretable examples for the aforementioned categories. Figure 12: GPT-4o Weak Category Analysis. The logos of the models or human displayed to the right of the option(s) indicate that the model or human has selected the option(s) as the correct answer(s). A.6 MORE INTERPRETABLE EXAMPLES To provide clearer and more comprehensive interpretation of the models capabilities, we present additional interpretable examples in Figure 14 and Figure 15. 19 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Figure 13: Performance gap between images whose shortest side > 768px and those 768px, defined as gap = Acc>768px Acc768px. The histogram shows the distribution of performance gaps across 13 tasks. The average performance gap for GPT-4o is 14.26, while for Qwen2-VL, it is 9.05. The smaller gap for Qwen2-VL indicates its greater effectiveness in processing high-resolution images. Additionally, Qwen2-VLs performance gaps are more consistently positive across different tasks, further highlighting its robustness in handling high-resolution images. Figure 14: More interpretable analysis of different VLMs. Green indicates correct answers, while red represents the predicted wrong answers. 20 MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models Figure 15: More interpretable analysis of different VLMs. Green indicates correct answers, while red represents the predicted wrong answers."
        }
    ],
    "affiliations": [
        "Apple",
        "Microsoft",
        "University of Rochester"
    ]
}