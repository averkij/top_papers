{
    "paper_title": "Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation",
    "authors": [
        "Mohammad Mahdi Abootorabi",
        "Amirhosein Zobeiri",
        "Mahdi Dehghani",
        "Mohammadali Mohammadkhani",
        "Bardia Mohammadi",
        "Omid Ghahroodi",
        "Mahdieh Soleymani Baghshah",
        "Ehsaneddin Asgari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. Resources are available at https://github.com/llm-lab-org/Multimodal-RAG-Survey."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 6 2 8 8 0 . 2 0 5 2 : r Ask in Any Modality: Comprehensive Survey on Multimodal Retrieval-Augmented Generation Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, *, Ehsaneddin Asgari, * Computer Engineering Department, Sharif University of Technology, Tehran, Iran, College of Interdisciplinary Science and Technology, University of Tehran, Tehran, Iran, Computer Engineering Department, K.N. Toosi University of Technology, Tehran, Iran, Qatar Computing Research Institute, Doha, Qatar Correspondence: mahdi.abootorabi2@gmail.com, soleymani@sharif.edu, easgari@hbku.edu.qa"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. (RAG) Retrieval-Augmented Generation mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG. This survey offers structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. Resources are available at https: //github.com/llm-lab-org/ Multimodal-RAG-Survey. 1 Introduction & Background In recent years, significant breakthroughs have been achieved in language models, driven primarily by the advent of transformers (Vaswani et al., 2017), enhanced computational capabilities, and the availability of large-scale training data (Naveed et al., 2024). The emergence of foundational Large Language Models (LLMs) (Ouyang et al., 2022; *These authors contributed equally. Grattafiori et al., 2024; Touvron et al., 2023; Qwen et al., 2025; Anil et al., 2023) has revolutionized natural language processing (NLP), demonstrating unprecedented capabilities in wide range of tasks including instruction following (Qin et al., 2024), sophisticated reasoning (Wei et al., 2024), In-context Learning (Brown et al., 2020), and multilingual machine translation (Zhu et al., 2024a). These advancements have elevated the performance of various NLP tasks, opening new avenues for research and application. Despite their remarkable achievements, LLMs face significant challenges, including hallucination, outdated internal knowledge, and lack of verifiable reasoning (Huang et al., 2024a; Xu et al., 2024b). Their reliance on parametric memory restricts their ability to access up-to-date knowledge, making them less effective for knowledge-intensive tasks compared to taskspecific architectures. Moreover, providing provenance for their decisions and updating their world knowledge remain critical open problems (Lewis et al., 2020). Retrieval-Augmented Generation (RAG) RAG (Lewis et al., 2020) has emerged as promising solution to these limitations by enabling LLMs to retrieve and incorporate external knowledge, improving factual accuracy and reducing hallucinations (Shuster et al., 2021; Ding et al., 2024a). By dynamically accessing vast external knowledge repositories, RAG systems enhance knowledge-intensive tasks while ensuring responses remain grounded in verifiable sources (Gao et al., 2023). In practice, RAG systems operate through retriever-generator pipeline. The retriever leverages embedding models (Chen et al., 2024b; Rau et al., 2024) to identify relevant passages from external knowledge bases and optionally applies re-ranking techniques to improve retrieval precision (Dong et al., 2024a). These retrieved passages are then passed to the generator, incorporating this external context to produce informed responses. Recent advancements in RAG frameworks (Asai et al., 2023; An et al., 2024; Lee et al., 2024; Liu et al., 2024c) have introduced iterative inference processes, enabling more accurate and reliable answers by refining both retrieval and generation stages. However, traditional RAG architectures are primarily designed for textual information, limiting their ability to address multimodal challenges that require integrating diverse data formats. Multimodal Learning Parallel to these developments, significant advances in multimodal learning have reshaped artificial intelligence by enabling systems to integrate and analyze heterogeneous data sources for holistic representation of information. The introduction of CLIP (Contrastive Language-Image Pre-training) (Radford et al., 2021) marked pivotal moment in connecting visual and textual information through contrastive learning, inspiring numerous subsequent models and applications (Alayrac et al., 2024; Wang et al., 2023; Pramanick et al., 2023). These breakthroughs have driven progress in various domains, including sentiment analysis (Das and Singh, 2023) and cutting-edge biomedical research (Hemker et al., 2024), demonstrating the value of multimodal approaches. By enabling systems to process and understand diverse data types such as text, images, audio, and video, multimodal learning has become essential to advancing artificial general intelligence (AGI) (Song et al., 2025). Multimodal RAG The extension of LLMs to multimodal LLMs (MLLMs) has further expanded their capabilities, allowing them to process, reason, and generate outputs across diverse modalities (Liu et al., 2023a; Team et al., 2024; Li et al., 2023b). For instance, GPT-4 (OpenAI et al., 2024) demonstrates human-level performance in various benchmarks by accepting inputs in both text and images, marking significant milestone in multimodal perception and interaction. Building on this foundation, multimodal RAG systems extend traditional RAG frameworks by incorporating multimodal knowledge sources, such as images and audio, to provide enriched context for generation (Hu et al., 2023; Chen et al., 2022a). This integration enhances the precision of generated outputs while leveraging multimodal cues to improve the reasoning capabilities of MLLMs. The general flow of the Multimodal RAG pipeline is illustrated in Figure 1. However, these multimodal systems also present unique challenges, including determining which modalities to retrieve, effectively fusing diverse data types, and addressing the complexities of cross-modal relevance (Zhao et al., 2023). Task Formulation We present mathematical formulation of the general task for multimodal RAG systems. These systems generate multimodal response, denoted as r, in response to query q, which is typically in textual format. Let = {d1, d2, ..., dn} be corpus consisting of multimodal documents. Each document di in is associated with modality Mdi and is processed by modality-specific encoder EncMdi : zi = EncMdi (di) (1) The set of all encoded representations is denoted by: = {z1, z2, ..., zn} (2) Modality-specific encoders map different modalities into shared semantic space for cross-modal alignment. retrieval model assesses the relevance of each encoded document representation with respect to the query q, represented as R(q, z). To construct the retrieval-augmented multimodal context, the retrieval model selects the most relevant documents based on modality-specific threshold: = {di s(eq, zi) τMdi } (3) where τMdi is relevancy threshold for the modality of Mdi, eq is the encoded representation of in the shared semantic space, and is scoring function that measures the relevance between the encoded query and document representations. The generative model produces the final multimodal response by conditioning on both the user query and the retrieved documents as context: = G(q, X) (4) Related Works As the field of multimodal RAGs is newly introduced and evolving rapidly, especially in recent years, there is pressing need for comprehensive survey that explores the current innovations and frontiers of these systems. While more than ten surveys have been published on RAG-related topics such as Agentic RAG (Singh et al., 2025), none provide detailed and comprehensive overview of advancements in multimodal RAGs. The only related survey to date (Zhao et al., 2023) categorizes multimodal RAGs by grouping 2 relevant papers based on their applications and modalities. However, our survey provides more detailed and different innovation-driven perspective, offering detailed taxonomy and exploring emerging trends and challenges in depth. Furthermore, significant advancements have been made in the field since its publication, and interest in this topic has grown substantially within the research community. In this survey, we review over 100 papers on multimodal RAGs published in recent years, primarily from the ACL Anthology and other repositories such as the ACM Digital Library. Contributions In this work, (i) we provide comprehensive review of the multimodal RAG field, covering task formulation, datasets, benchmarks, tasks and domain-specific applications, evaluation, and key innovations across retrieval, fusion, augmentation, generation, training strategies, and loss functions. (ii) We introduce precise structured taxonomy (Figure 2) that categorizes state-of-the-art models based on their primary contributions, highlighting methodological advancements and emerging frontiers. (iii) To support further research, we make resources, including datasets, benchmarks, and key innovations, publicly available. (iv) We identify current research trends and knowledge gaps, providing insights and recommendations to guide future advancements in this evolving field."
        },
        {
            "title": "2 Datasets and Benchmarks",
            "content": "Multimodal RAG research employs diverse datasets and benchmarks to evaluate retrieval, integration, and generation across heterogeneous sources. Imagetext tasks, including captioning and retrieval, commonly use MS-COCO (Lin et al., 2014), Flickr30K (Young et al., 2014), and LAION400M (Schuhmann et al., 2021), while visual question answering with external knowledge is supported by OK-VQA (Marino et al., 2019) and WebQA (Chang et al., 2022). For complex multimodal reasoning, MultimodalQA (Talmor et al., 2021) integrates text, images, and tables, whereas videotext tasks leverage ActivityNet (Caba Heilbron et al., 2015) and YouCook2 (Zhou et al., 2018). In the medical domain, MIMIC-CXR (Johnson et al., 2019) and CheXpert (Irvin et al., 2019) facilitate tasks such as medical report generation. It is noteworthy that number of these datasets are unimodal (e.g., solely text-based or image-based). Unimodal datasets are frequently employed to represent specific modality and are subsequently integrated with complementary datasets from other modalities. This modular approach allows each dataset to contribute its domain-specific strengths, thereby enhancing the overall performance of the multimodal retrieval and generation processes. Benchmarks assess multimodal RAG systems on visual reasoning, external knowledge integration, and dynamic retrieval. The 2RAG (Ma et al., 2024c) benchmark provides unified evaluation framework that combines fine-grained text-modal and multimodal metrics to jointly assess both the quality of generated language and the effective integration of visual elements Vision-focused evaluations, including MRAG-Bench (Hu et al., 2024c), VQAv2 (Goyal et al., 2017a) and VisDoMBench (Suri et al., 2024), test models on complex visual tasks. Dyn-VQA (Li et al., 2024b), MMBench (Liu et al., 2025), and ScienceQA (Saikh et al., 2022) evaluate dynamic retrieval and multihop reasoning across textual, visual, and diagrammatic inputs. knowledge-intensive benchmarks, such as TriviaQA (Joshi et al., 2017) and Natural Questions (Kwiatkowski et al., 2019), together with document-oriented evaluations such as OmniDocBench (Ouyang et al., 2024), measure integration of unstructured and structured data. Advanced retrieval benchmarks such as RAGCheck (Mortaheb et al., 2025a) evaluate retrieval relevance and system reliability, while specialized assessments like Counterfactual VQA (Niu et al., 2021) test robustness against adversarial inputs. Additionally, OCR impact studies such as OHRBench (Zhang et al., 2024d) examine the cascading effects of errors on RAG systems. Additional details about datasets, benchmarks, and their categorization are presented in Table 1 and Table 2 in Appendix B."
        },
        {
            "title": "3 Evaluation",
            "content": "Assessing multimodal RAG models presents significant challenges due to their diverse modalities, intricate architectures, and the diverse evaluation dimensions involved. Given the broad spectrum of modalities and system components, evaluation requires combination of metrics drawn from visionlanguage models (VLMs), generative AI, and retrieval systems. These metrics enable the assessment of various aspects of multimodal RAG, such as text and image generation or information retrieval, either independently or in combination. By analyzing prior works, we identified approximately 60 distinct evaluation metrics used in multimodal RAG studies. More details, including the formu3 Figure 1: Overview of the multimodal retrieval-augmented generation (RAG) pipeline, highlighting the advancements and techniques employed at each stage. The flow begins with query preprocessing, where user queries are refined and then encoded into shared embedding space alongside multimodal database. Retrieval strategies, such as modality-centric retrieval, similarity search, and re-ranking, enhance document selection, while fusion mechanisms align and integrate data from multiple modalities using score fusion or attention-based methods. Augmentation techniques such as iterative retrieval with feedback mechanisms, further refine the retrieved documents for the multimodal LLM. The generation stage incorporates innovations like Chain-of-Thought reasoning and source attribution for better outputs, with loss functions combining alignment loss and generation loss to optimize both retrieval and generation components. Noise management techniques are also applied to improve training stability and robustness. las for the RAG evaluation metrics, can be found in Appendix C. In the following paragraphs, we will examine the most important and widely used metrics for evaluating multimodal RAG. Retrieval Evaluation Evaluates how well system retrieves relevant information from database or knowledge source. Accuracy, recall, and precision are used to evaluate retrieval and downstream task performance, the F1 score also being used to assess both Recall and Precision simultaneously. more commonly used metric than recall is Recall@K, which measures the proportion of relevant items appearing within the top retrieved results. Another important metric Mean Reciprocal Rank (MRR), which is utilized by (Adjali et al., 2024; Nguyen et al., 2024). Modality Evaluation These evaluations focus on criteria that rely on modalities. Text and image are the most commonly used modalities in multimodal RAG. This category of evaluations considers various aspects of these modalities, including the alignment between generated text and images, the fluency of the text, the quality of image captions, and other related factors. In text similarity and overlap metrics, Exact Match (EM), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) are used. MultiRAGen (Shohan et al., 2024) uses Multilingual ROUGE for multilingual settings. For image captioning evaluation, CIDEr (Consensus-Based Image Description Evaluation) (Vedantam et al., 2015) is widely used metric for evaluating image captioning quality. It quantifies the alignment between generated caption and set of human-written reference captions by leveraging the term frequency-inverse document frequency (TF-IDF) and cosine similarity. (Yasunaga et al., 2023; Zhao et al., 2024; Luo et al., 2024a; Yuan et al., 2024; Sharifymoghaddam et al., 2024; Hu et al., 2023; Rao et al., 2024; Xu et al., 2024a; Kim et al., 2024; Zhang et al., 2024c) use CIDEr for evaluation. SPICE (Semantic Propositional Image Caption Evaluation) (Anderson et al., 2016) is metric that emphasizes semantic aspects, in contrast to CIDEr. SPIDEr (Liu et al., 2017), utilized in (Zhang et al., 2024c), is linear combination of both SPICE and CIDEr. In semantic alignment evaluations, BERTScore (Zhang et al., 2020), employed in (Sun et al., 2024b; Shohan et al., 2024), measures semantic similarity by comparing BERT-generated embeddings. (Chen et al., 2022a; Zhi Lim et al., 2024; Ma et al., 2024c) evaluate Fluency by employing BERTScore. CLIP Score (Hessel et al., 2021), used in (Sharifymoghaddam et al., 2024; Zhang et al., 2024c), measures imagetext similarity using the CLIP model (Radford et al., 2021). FID (Fréchet Inception Distance) (Heusel et al., 2017) quantifies image quality by computing the distributional difference between the generated and original image features. (Yasunaga et al., 2023; 4 Zhao et al., 2024; Sharifymoghaddam et al., 2024; Zhang et al., 2024c) use FID in their works. (Zhao et al., 2024) use Kernel Inception Distance (KID) (Binkowski et al., 2018) for evaluation. KID is another metric used to evaluate the quality of generated images. It is similar to FID but offers an unbiased estimator. (Zhi Lim et al., 2024) employs the Inception Score (IS) to assess the diversity and quality of generated images based on classification probabilities obtained from the Inception network. For the evaluation of audio synthesis, (Zhang et al., 2024c) assesses overall sound quality (OVL) and relevance to the text (REL) with the assistance of human annotators. Additionally, this study employs the Fréchet Audio Distance (FAD) (Kilgour et al., 2019) as an evaluation metric. FAD serves similar purpose to the FID but is specifically designed for the audio domain. FLOPs, execution time, response time, and average retrieval time per query are utilized to assess efficiency and computational performance (Nguyen et al., 2024; Strand et al., 2024; Dang, 2024; Zhou, 2024). (Zhong et al., 2024) employs Spearmans Rank Correlation (SRC), which calculates the Pearson correlation between the rank values of two variables. Several evaluation metrics are domainand problem-specific. Geodesic distance measurement quantifies the models accuracy by calculating the geodesic distance between predicted and actual geographical coordinates (Zhou et al., 2024e). (Lahiri and Hu, 2024) utilizes Clinical Relevance (CR) for evaluation."
        },
        {
            "title": "4.1 Retrieval Strategy",
            "content": "Efficient Search and Similarity Retrieval Modern multimodal RAG systems first encode diverse modalities (text, images, tables) into unified embedding space to enable direct cross-modal retrieval. The foundational CLIP-(Radford et al., 2021) or BLIP-based (Li et al., 2022a) approaches leverage contrastive learning, which aligns text and images into shared latent space via dual-tower encoders. Recent advancements extend this paradigm with novel multimodal retrieval architectures and training strategies (Zhou et al., 2024c; Wei et al., 2025; Zhang et al., 2024j). Multimodal RAG systems rely on efficient search strategies to retrieve relevant external knowledge from large-scale multimodal memory. Since dual-encoder models map multimodal inputs into shared high-dimensional embedding space, maximum inner product search (MIPS) is commonly used for fast and direct similarity comparisons. Systems such as MuRAG (Chen et al., 2022a) and RA-CM3 (Yasunaga et al., 2023) employ approximate MIPS to efficiently retrieve the top-k candidates by maximizing the inner product between the query vector and large collection of imagetext embeddings. Large-scale implementations leverage distributed MIPS techniques, such as TPU-KNN (Chern et al., 2022), to enable high-speed retrieval across extensive datasets. Additionally, ScaNN (Scalable Nearest Neighbors) (Guo et al., 2020), MAXSIM score (Chan and Ng, 2008; Cho et al., 2024), and approximate KNN methods (Caffagni et al., 2024) have been adopted for efficient image-text similarity computation. Recent advancements in MIPS optimization focus on reducing retrieval latency and improving accuracy. These include adaptive quantization (Li et al., 2024a; Huang et al., 2024b), hybrid sparsedense representations (Nguyen et al., 2024; Ma et al., 2024a; Zhang et al., 2024a), and learned index structures (Zhai et al., 2023; Basnet et al., 2024). These techniques help optimize retrieval performance by balancing efficiency and precision in multimodal RAG systems. Modality-Based Retrieval Modality-aware retrieval techniques optimize retrieval efficiency by leveraging the unique characteristics of each modality. Text-centric retrieval remains foundational in multimodal RAG systems. Traditional methods like BM25 (Robertson and Zaragoza, 2009), along with dense retrievers such as BGE-1.5 (Xiao et al., 2023) and BGE-M3 (Chen et al., 2024a) dominate text-based evidence retrieval (Chen et al., 2022b; Suri et al., 2024; Nan et al., 2024). Novel approaches also address the need for fine-grained semantic matching and domain specificity: for instance, ColBERT (Khattab and Zaharia, 2020) and PreFLMR (Lin et al., 2024b) employ token-level interaction mechanisms that preserve nuanced textual details to improve precision for multimodal queries, while RAFT (Zhang et al., 2024i) and CRAG (Yan et al., 2024) enhance retrieval performance by ensuring precise citation of text spans, thereby optimizing domain-specific question answering within multimodal RAG pipelines. Visioncentric retrieval focuses on directly leveraging image representations for knowledge extraction (Kumar and Marttinen, 2024; Yuan et al., 2023). Systems such as EchoSight (Yan and Xie, 2024) 5 and ImgRet (Shohan et al., 2024) retrieve visually similar content by using reference images as queries. Composed Image Retrieval (CMI) models (Feng et al., 2023; Zhao et al., 2024; Jang et al., 2024) enhance retrieval by integrating multiple image features into unified query representation. Similarly, Pic2word (Saito et al., 2023) maps visual content to textual descriptions, enabling zero-shot image retrieval. Video-centric retrieval extends vision-based techniques by incorporating temporal dynamics and large video-language models (LVLMs), driven by novel frameworks like iRAG (Arefeen et al., 2024), which introduces incremental retrieval for sequential video understanding, and MV-Adapter (Jin et al., 2024), optimizing multimodal transfer learning for video-text retrieval. Recent breakthroughs focus on long-context processing: Video-RAG (Luo et al., 2024b) leverages visually aligned auxiliary texts (OCR/ASR) to enhance retrieval without proprietary models, while VideoRAG (Ren et al., 2025) employs dual-channel architectures with graph-based knowledge grounding for extreme-length videos. For temporal reasoning, CTCH (Shen et al., 2024) uses contrastive transformer hashing to model long-term dependencies, and RTime (Du et al., 2024) introduces reversedvideo hard negatives to rigorously benchmark temporal causality. Meanwhile, for complex video understanding, OmAgent (Zhang et al., 2024e) adopts divide-and-conquer framework, while DRVideo (Ma et al., 2024d) addresses long-video understanding by document-based retrieval approach. Document Retrieval and Layout Understanding Recent research has advanced beyond traditional uni-modal retrieval methods toward models that directly process entire document pages. These approaches integrate textual, visual, and structural elements like tables, images, font styles, and page layouts to enhance retrieval performance in complex documents. ColPali (Faysse et al., 2024) pioneers end-to-end document image retrieval by embedding page patches with vision-language backbone, bypassing OCR entirely. Models like ColQwen2 (Wang et al., 2024b; Khattab and Zaharia, 2020) and M3DocVQA (Cho et al., 2024) extend this paradigm with dynamic resolution handling and holistic multi-page reasoning. Newer frameworks refine efficiency and layout understanding: ViTLP (Mao et al., 2024) and DocLLM (Wang et al., 2024a) pre-train generative models to align spatial layouts with text, while CREAM (Zhang et al., 2024b) employs coarse-to-fine retrieval with multimodal efficient tuning to balance accuracy and computation costs. Finally, mPLUG-DocOwl 1.5 (Hu et al., 2024a) and 2 (Hu et al., 2024b) unify structure learning across formats (e.g., invoices, forms) without OCR dependencies. Together, these approaches highlight shift toward holistic, layoutsensitive retrieval that leverages visual semantics and generative pre-training. Re-ranking and Selection Strategies Effective retrieval in multimodal RAG systems requires not only identifying relevant information but also refining and prioritizing retrieved candidates. Reranking and selection strategies enhance retrieval quality by optimizing example selection, refining relevance scoring, and applying filtering mechanisms. Optimized example selection techniques often employ multi-step retrieval, integrating both supervised and unsupervised selection strategies (Luo et al., 2024a). (Su et al., 2024a) refine multimodal inputs using probabilistic control keywords to improve credibility, while RULE (Xia et al., 2024b) calibrates retrieved context selection through statistical methods like the Bonferroni correction to mitigate factuality risks. Furthermore, selective fusion mechanisms dynamically refine the retrieval outcomes (Yuan et al., 2023), and key-frame extraction techniques ensure diversity in video-based retrieval through clustering-based frame selection (Dong et al., 2024b). Several methods employ advanced scoring mechanisms to improve retrieval relevance (Mortaheb et al., 2025b,a). Multimodal similarity measures, including negative log-likelihood (NLL), structural similarity index measure (SSIM) (Wang et al., 2020), normalized cross-correlation (NCC), and BERTScore (Zhang et al., 2020), aid in re-ranking retrieved documents based on textual and visual similarity. Cross-encoders trained on sequence classification tasks refine relevance scoring within retrieval pipelines (Zhi Lim et al., 2024), while hierarchical post-processing techniques integrate retrieval, passage-level, and answer confidence scores for improved ranking (Zhang et al., 2024h). LDRE (Yang et al., 2024) employs semantic ensemble methods to adaptively weigh multiple caption features, whereas RAGTrans (Cheng et al., 2024) and OMG-QA (Nan et al., 2024) integrate traditional ranking functions like BM25 (Robertson and Zaragoza, 2009). Advanced re-ranking techniques also incorporate Q-Former-based image-to6 text correspondence for fine-grained retrieval (Yan and Xie, 2024; Xu et al., 2024a). Advanced filtering methods are crucial for enhancing multimodal RAG performance by ensuring that only high-quality, relevant data is processed. Hard negative mining techniques, as seen in GME (Zhang et al., 2024j) and MM-Embed (Lin et al., 2024a), use modality-aware sampling and synthesized negatives from imbalanced data to mitigate modality bias and refine contrastive learning. Similarly, consensus-based filtering approaches, exemplified by MuRAR (Zhu et al., 2024d) and ColPali (Faysse et al., 2024), employ source attribution and multi-vector mapping to filter out lowsimilarity candidates and reduce hallucinations in enterprise QA. Moreover, dynamic modality filtering methodsincluding RAFT (Zhang et al., 2024i), Img2Loc (Zhou et al., 2024e), and MAINRAG (Chang et al., 2024)train retrievers to disregard confusing data, thereby enhancing the discriminative capacity and overall robustness of multimodal retrieval systems."
        },
        {
            "title": "4.2 Fusion Mechanisms",
            "content": "Score Fusion and Alignment Recent multimodal RAG models emphasize aligning embeddings or fusing scores across different modalities to enable adaptive retrieval. Zhi Lim et al. (2024) transform text, tables, and images into unified textual representation using cross-encoder trained for relevance scoring. To address multimodal LLM input constraints, Sharifymoghaddam et al. (2024) introduce interleaved image-text pairs, merging multiple few-shot images vertically for compatibility with models like LLaVA (Liu et al., 2023a) while aligning modalities via CLIP score fusion (Hessel et al., 2021) and BLIP feature fusion(Li et al., 2022a). For fine-grained alignment, REVEAL (Hu et al., 2023) injects retrieval scores into attention layers and minimizes L2-Norm distances between query and knowledge embeddings, and MA-LMM (He et al., 2024) aligns video-text embeddings via BLIP-inspired Query Transformer (Q-Former)(Li et al., 2022a). In contrast, LLMRA (Jian et al., 2024) concatenates text and visual embeddings into joint queries to reduce retrieval noise. RA-BLIP (Ding et al., 2024b) employs 3-layer BERT-based adaptive fusion module to unify visual-textual semantics. CLIP-based alignment strategies are widely adopted; Riedler and Langer (2024) embed images and questions into shared CLIP space, while VISA (Ma et al., 2024b) uses the Document Screenshot Embedding (DSE) model as the retrieval component of the RAG system to align and fuse textual queries with visual document representations. DSE directly encodes both textual queries and document screenshots into shared embedding space, allowing efficient retrieval based on cosine similarity. Xue et al. (2024) map object-predicate pairs into shared semantic space using prototype-based embedding network (PENET) (Zheng et al., 2023), aligning visual features with textual prototypes. Re-IMAGEN (Chen et al., 2022b) refines text-to-image synthesis with interleaved classifier-free guidance during diffusion sampling, balancing creativity and entity fidelity. To improve multimodal alignment, MegaPairs (Zhou et al., 2024a) integrates CLIPbased and MLLM-based retrieval through score fusion. Meanwhile, Wiki-LLaVA (Caffagni et al., 2024) employs CLIP encoders for hierarchical document retrieval. VISRAG (Yu et al., 2024) refines alignment via position-weighted mean pooling on VLM hidden states, prioritizing later tokens for relevance. RAG-Driver (Yuan et al., 2024) aligns visual-language embeddings via visual instruction tuning and an MLP projector. Attention-Based Mechanisms Attention-based fusion dynamically weights cross-modal interactions for task-specific reasoning. RAMM (Yuan et al., 2023) employs dual-stream co-attention transformer, combining self-attention (intra-modal) and cross-attention (inter-modal) to fuse retrieved biomedical images/texts with input data. Similarly, EMERGE (Zhu et al., 2024b) integrates Electronic Health Records (EHR) data with crossattention adaptive network, while MORE (Cui et al., 2024) uses cross-attention to weigh multimodal results for commonsense reasoning. For user-centric tasks, RAGTrans (Cheng et al., 2024) integrates user-aware attention to fuse social media features, and AlzheimerRAG (Lahiri and Hu, 2024) applies cross-modal attention to synthesize text, images, and tables. Video-text alignment is advanced by MV-Adapter (Jin et al., 2024), which leverages Cross Modality Tying (CMT) to align embeddings, and M2-RAAP (Dong et al., 2024b), enhances video-text fusion using an AuxiliaryCaption-Guided (ACG) strategy. It employs FrameCaption Re-weighting (FCR) to boost frames with high inter-modal consistency and Text-Caption Reweighting (TCR) to adjust weights based on intramodal similarity. Mutual-guided (Mug) Align7 ment Head then filters misaligned features via dot-product similarity and frame-to-token attention, producing refined frame-specific text representations. Generative refinement is demonstrated in Xu et al. (2024a), where gated cross-attention conditions text generation on visual features. Similarly, C3Net (Zhang et al., 2024c) enhances alignment between queries and candidates within CLIPs contrastive space. Mu-RAG (Chen et al., 2022a) uses intermediate fusion with cross-attention for open QA, balancing modality-specific and joint representations. Kim et al. (2024) leverage cross-modal memory retrieval with pre-trained CLIP ViT-L/14 to map video-text pairs into shared space, enabling dense captioning through the attention-based fusion of retrieved memories. Unified Frameworks and Projections Architectures unifying multimodal inputs through frameworks or projections address scalability. Su et al. (2024a) employ hierarchical cross-chains and late fusion for healthcare data, while IRAMIG (Liu et al., 2024b) iteratively consolidates multimodal results into knowledge representations. For document understanding, M3DocRAG (Cho et al., 2024) flattens multi-page documents into unified embedding tensor, and PDF-MVQA (Ding et al., 2024c) combines Region-of-Interest (RoI)-based (VisualBERT) and patch-based (CLIP) VLPMs (Long et al., 2022). DQU-CIR (Wen et al., 2024) exemplifies raw-data unification by converting images into text captions for complex queries and overlaying text onto images for simple queries. The model then fuses embeddings using MLPlearned weights. SAM-RAG (Zhai, 2024) aligns image-text modalities by generating captions for images, converting the multimodal into unimodal text representation for subsequent processing. UFineBench (Zuo et al., 2024) uses shared granularity decoder for ultra-fine textperson retrieval. Nguyen et al. (2024) introduce Dense2Sparse (D2S) projection, converting dense embeddings from models like BLIP/ALBEF (Li et al., 2022a) into sparse lexical vectors via layer normalization and probabilistic expansion control, optimizing storage and interpretability. input"
        },
        {
            "title": "4.3 Augmentation Techniques",
            "content": "Basic RAG systems typically follow single retrieval step and pass retrieved content directly to the generation phase without further refinement, which can lead to inefficiencies and suboptimal outputs. To address this, augmentation techniques have been introduced to enhance the utility of retrieved data before generation, ensuring that the model can better interpret, structure, and integrate multimodal information (Gao et al., 2023). Context Enrichment This focuses on enhancing the relevance of retrieved knowledge for the generation phase by refining external knowledge or retrieved data. General approaches such as EMERGE (Zhu et al., 2024b) integrate entity relationships and semantic descriptions to provide richer context. MiRAG (Adjali et al., 2024) improves retrieval effectiveness by expanding initial queries through entity retrieval and query reformulation, enhancing subsequent stages for the visual question-answering task. Wiki-LLaVA (Caffagni et al., 2024) incorporates text chunks and image tokens into multimodal inputs to enrich prompts for generation. VideoRAG (Luo et al., 2024b) enhances long-video understanding through Query Decoupling, which reformulates user queries into structured retrieval requests to extract additional auxiliary multimodal context. Img2Loc (Zhou et al., 2024e) enhances accuracy by including both the most similar and the most dissimilar points from the database in the prompt, allowing the model to rule out implausible locations for its predictions. Finally, Xue et al. (2024) introduce semantic-enhanced prompts, aligning structured image data with queries for contextual accuracy. Adaptive and Iterative Retrieval For more complex queries, dynamic retrieval mechanisms have proven effective. SKURG (Yang et al., 2023) employs adaptive retrieval steps to dynamically adjust the number of hops based on the query complexity. IRAMIG (Liu et al., 2024b) iteratively refines multimodal queries by incorporating feedback from previous iterations and context created by the fusion of retrieved items from text, images, and audio modalities. Similarly, OMG-QA (Nan et al., 2024) introduces multi-round retrieval strategy, where each retrieval step incorporates episodic memory to refine subsequent queries. An evaluation module assesses retrieval effectiveness at each step, guiding the refinement of subsequent retrieval efforts through feedback. Taking different approach, SAM-RAG (Zhai, 2024) further improves relevance by dynamically selecting the optimal number of retrieved contexts for each query. MMed-RAG (Xia et al., 2024a) uses context filtering to discard low-relevance results. OmniSearch (Li et al., 2024b) introduces self-adaptive retrieval 8 agent that dynamically decomposes complex multimodal questions into sub-question chains and plans retrieval actions in real-time based on retrieved content. mR2AG (Zhang et al., 2024g) introduces reflection mechanisms to enhance retrieval effectiveness. Retrieval-Reflection dynamically determines when external knowledge is necessary, while Relevance-Reflection filters retrieved content, retaining only task-critical evidence. Similarly, RAGAR (Khaliq et al., 2024) iteratively refines retrieved data by leveraging prior responses and multimodal analysis, ensuring contextual consistency."
        },
        {
            "title": "4.4 Generation Techniques",
            "content": "Recent advancements in multimodal RAG generation focus on robustness, cross-modal coherence, and task-specific adaptability. These innovations can be broadly categorized into these sections: In-Context Learning In-context learning (ICL) with retrieval augmentation enhances reasoning in multimodal RAGs by leveraging retrieved content as few-shot examples without requiring retraining. Models such as RMR (Tan et al., 2024), Sharifymoghaddam et al. (2024), and RA-CM3 (Yasunaga et al., 2023), extend this paradigm to multimodal RAG settings. RAG-Driver (Yuan et al., 2024) refines ICL by retrieving relevant driving experiences from memory database, ensuring scenariospecific contextual alignment. MSIER (Luo et al., 2024a) further enhances example selection through Multimodal Supervised In-Context Examples Retrieval framework, leveraging foundation MLLM scorer to evaluate both textual and visual relevance. Meanwhile, Raven (Rao et al., 2024) introduces Fusion-in-Context Learning, novel approach that enriches ICL by incorporating more diverse set of in-context examples, leading to better performance than standard ICL. Reasoning Structured reasoning techniques, such as chain-of-thought (CoT), decompose complex reasoning into smaller sequential steps, enhancing coherence and robustness in multimodal RAG systems. RAGAR (Khaliq et al., 2024) introduces Chain of RAG and Tree of RAG to iteratively refine fact-checking queries and explore branching reasoning paths for more robust evidence generation. VisDoM (Suri et al., 2024) integrates CoT with evidence curation to ensure logical and contextual accuracy, while SAM-RAG (Zhai, 2024) employs reasoning chains alongside multi-stage answer verification to enhance the relevance, utility, and support of generated responses. Meanwhile, LDRE (Yang et al., 2024) leverages LLMs for divergent compositional reasoning, generating refined captions by incorporating dense captions and modification text. Instruction Tuning Several works have fine-tuned generation components for specific applications. RA-BLIP (Ding et al., 2024b), uses Q-Former architecture from InstructBLIP (Dai et al., 2023) to extract visual features based on question instructions. RAGPT (Lang et al., 2025) uses contextaware prompter, which captures contextual knowledge from relevant instances and generates dynamic prompts. mR2AG (Zhang et al., 2024g) uses instruction tuning with the mR2AG-IT dataset to train MLLMs to adaptively invoke retrieval, identify relevant evidence, and generate accurate answers for knowledge-based VQA tasks. RagVL (Chen et al., 2024d) employs instruction tuning to enhance the ranking capability of MLLMs, serving them as re-ranker for filtering the top-k retrieved images. Jang et al. (2024) propose Visual Delta Generator to discriminate differences between image pairs and produce textual response. MMedRAG (Xia et al., 2024a) uses preference fine-tuning to help the model learn when to rely on retrieved information and when to trust its internal knowledge. Additionally, several methods have been proposed to improve generation quality by constructing datasets from the previous mistakes of LLMs and fine-tuning them accordingly. MegaPairs (Zhou et al., 2024a) synthesizes large-scale multimodal instruction-tuning datasets for this purpose. Similarly, Surf (Sun et al., 2024a) creates dataset of positive and negative pairs derived from the prior errors of LVLMs and subsequently finetunes the LVLM using this dataset. Additionally, Rule (Xia et al., 2024b) refines Med-LVLM by applying direct preference optimization on curated preference data, specifically focusing on instances where overreliance on retrieved contexts resulted in errors. Source Attribution and Evidence Transparency Ensuring source attribution in multimodal RAG systems is key focus of recent research. MuRAR (Zhu et al., 2024d) integrates multimodal data, fetched by source-based retriever, to refine LLMs initial response, ensuring coherence and informativeness. VISA (Ma et al., 2024b) uses large vision-language models (VLMs) to generate answers with visual source attribution by identifying and highlighting supporting evidence in retrieved 9 document screenshots. Similarly, OMG-QA (Nan et al., 2024) enhances transparency by prompting the LLM to explicitly cite evidence in generated responses."
        },
        {
            "title": "4.5 Training Strategies",
            "content": "Training multimodal RAG models involves multistage process to effectively handle cross-modal interactions (Chen et al., 2022a). Pretraining establishes the foundation using large paired datasets to learn cross-modal relationships while fine-tuning adapts models to specific tasks by leveraging crossmodal attention (Ye et al., 2019). For instance, REVEAL (Hu et al., 2023) integrates multiple training objectives. Its pretraining phase optimizes Prefix Language Modeling Loss (LPrefixLM), where the model predicts text continuations from prefix and an image. Supporting losses include Contrastive Loss (Lcontra) for aligning queries with pseudoground-truth knowledge, Disentangled Regularization Loss (Ldecor) to improve embedding expressiveness, and Alignment Regularization Loss (Lalign) to align query and knowledge embeddings. During fine-tuning, cross-entropy objective trains the model for tasks like VQA and image captioning. Alignment Contrastive learning improves representation quality by pulling positive pairs closer and pushing negative pairs apart in the embedding space. common objective is the InfoNCE loss (van den Oord et al., 2019), which maximizes the mutual information between positive pairs while minimizing similarity to negatives. Several multimodal RAG models, such as VISRAG (Yu et al., 2024), MegaPairs (Zhou et al., 2024a) and SAMRAG (Zhai, 2024) utilize InfoNCE loss to improve retrieval-augmented generation. Furthermore, EchoSight (Yan and Xie, 2024) enhances retrieval accuracy by selecting visually similar yet contextually distinct negatives, while HACL (Jiang et al., 2024) improves generation by introducing hallucinative captions as distractors. Similarly, UniRaG (Zhi Lim et al., 2024) strengthens retrieval by incorporating hard negative documents, helping the model distinguish relevant contexts from noise. The eCLIP loss (Kumar and Marttinen, 2024) extends contrastive training by integrating expertannotated data and an auxiliary Mean Squared Error (MSE) loss to refine embedding quality. Mixup strategies further augment training by generating synthetic positive pairs, improving generalization in contrastive learning (Kumar and Marttinen, 2024). Dense2Sparse (Nguyen et al., 2024) incorporates two unidirectional losses: an image-tocaption loss ℓ(I C) and caption-to-image loss ℓ(C I). It enforces sparsity through L1 regularization, optimizing retrieval precision by balancing dense and sparse representations. Details of formulas for widely used RAG loss functions can be found in Appendix D. Robustness Multimodal training faces challenges such as noise and modality-specific biases Buettner and Kovashka (2024). Managing noisy retrieval inputs is critical for maintaining model performance. MORE (Cui et al., 2024) injects irrelevant results during training to enhance focus on relevant inputs. AlzheimerRAG (Lahiri and Hu, 2024) uses progressive knowledge distillation to reduce noise while maintaining multimodal alignment. RAGTrans (Cheng et al., 2024) leverages hypergraph-based knowledge aggregation to refine multimodal representations, ensuring more effective propagation of relevant information. RA-BLIP (Ding et al., 2024b) introduces the Adaptive Selection Knowledge Generation (ASKG) strategy, which leverages the implicit capabilities of LLMs to filter relevant knowledge for generation through denoising-enhanced loss term, eliminating the need for fine-tuning. This approach achieves strong performance compared to baselines while significantly reducing computational overhead by minimizing trainable parameters. RagVL (Chen et al., 2024d) improves robustness through noise-injected training by adding hard negative samples at the data level and applying Gaussian noise with loss reweighting at the token level, enhancing the models resilience to multimodal noise. Finally, RA-CM3 (Yasunaga et al., 2023) enhances generalization using Query Dropout, which randomly removes query tokens during retrieval, serving as regularization method that improves generator performance."
        },
        {
            "title": "5 Tasks Addressed by Multimodal RAGs",
            "content": "Multimodal RAG systems extend RAG beyond unimodal settings to tasks requiring cross-modal integration, enhancing performance across modalities such as text, images, and audio. In content generation, these models enhance image captioning (Zhi Lim et al., 2024; Hu et al., 2023; Rao et al., 2024) and text-to-image synthesis (Yasunaga et al., 2023; Chen et al., 2022b) by retrieving relevant contextual information. They also improve coherence in visual storytelling and ensure fac10 Efficient-Search and Similarity Retrieval (4.1) Retrieval Strategies (4.1) Modality-Centric Retrieval (4.1) Maximum Inner Product Search (MIPS) TPU-KNN (Chern et al., 2022), ScaNN (Guo et al., 2020), MAXSIM score (Chan and Ng, 2008), OmniSearch (Li et al., 2024b), MuRAG (Chen et al., 2022a), RetrievalAttention (Liu et al., 2024a), RACM3 (Yasunaga et al., 2023), RAVEN (Rao et al., 2024) FactMM-RAG (Sun et al., 2024b) Multimodal Encoders Text-Centric Vision-Centric Video-Centric CLIP (Radford et al., 2021), BLIP (Li et al., 2022a), MARVEL (Zhou et al., 2024c), ALIGN (Jia et al., 2021), FLAVA (Singh et al., 2022), UniVL-DR (Liu et al., 2023b), UniIR (Wei et al., 2025), GME (Zhang et al., 2024j), VISTA (Zhou et al., 2024b), ColPali (Faysse et al., 2024), InternVideo (Wang et al., 2022), Ovis (Lu et al., 2024) Mi-RAG (Adjali et al., 2024) Contriever (Izacard et al., 2022), GTE (Li et al., 2023c), Re-Imagen (Chen et al., 2022b), BM25 (Robertson and Zaragoza, 2009), BGE-1.5 (Xiao et al., 2023), BGE-M3 (Chen et al., 2024a), CapRet (Shohan et al., 2024), OMG-QA (Nan et al., 2024), ColBERT (Khattab and Zaharia, 2020), PreFLMR (Lin et al., 2024b), RAFT (Zhang et al., 2024i), CRAG (Yan et al., 2024), M2-RAG (Ma et al., 2024c) VQA4CIR (Feng et al., 2023), Unifashion (Zhao et al., 2024), Jang et al. (Jang et al., 2024), Pic2word (Saito et al., 2023), eClip (Kumar and Marttinen, 2024), RAMM (Yuan et al., 2023), Joshi et al. (Joshi et al., 2024), VISA (Ma et al., 2024b), ImgRet (Shohan et al., 2024), EchoSight (Yan and Xie, 2024), Xue et al. (Xue et al., 2024) iRAG (Arefeen et al., 2024), VideoRAG (Ren et al., 2025), VideoRAG (Jeong et al., 2025), MVAdapter (Jin et al., 2024), OmAgent (Zhang et al., 2024f), CM2 (Kim et al., 2024), Video-RAG (Luo et al., 2024b), CTCH (Shen et al., 2024), RTime (Du et al., 2024), VideoMAE (Tong et al., 2022), DrVideo (Ma et al., 2024d) Document Retrieval ColPali (Faysse et al., 2024), ColQwen2 (Wang et al., 2024b), M3DocVQA (Cho et al., 2024), ViTLP (Mao et al., 2024), DocLLM (Wang et al., 2024a), CREAM (Zhang et al., 2024b), mPLUG-DocOwl 1.5 (Hu et al., 2024a), mPLUG-DocOwl 2 (Hu et al., 2024b), VisDom (Suri et al., 2024), DSE (Ma et al., 2024a) Optimized Example Selection MSIER (Luo et al., 2024a), Hybrid RAG (Su et al., 2024b), RULE (Xia et al., 2024b), RAMM (Yuan et al., 2023), M2RAAP (Dong et al., 2024b) l m u Re-ranking Strategies (4.1) Relevance Score Evaluation RAG-Check (Mortaheb et al., 2025a,b), UniRaG (Zhi Lim et al., 2024), MR2AG (Zhang et al., 2024h), LDRE (Yang et al., 2024), BM25 (Robertson and Zaragoza, 2009), RAGTrans (Cheng et al., 2024), OMGQA (Nan et al., 2024), EchoSight (Yan and Xie, 2024), EgoInstructor (Xu et al., 2024a) Filtering Mechanisms MAIN-RAG (Chang et al., 2024), MM-Embed (Lin et al., 2024a), GME (Zhang et al., 2024j), Img2Loc (Zhou, 2024), MuRAR (Zhu et al., 2024d) RAFT (Zhang et al., 2024i) Score Fusion and Alignment (4.2) M3 (Cai et al., 2025) Zhi Lim et al. (2024), Sharifymoghaddam et al. (2024), REVEAL (Hu et al., 2023), RAG-Driver (Yuan et al., 2024), LLM-RA (Jian et al., 2024), Riedler and Langer (2024), VISA (Ma et al., 2024b), MA-LMM (He et al., 2024), Xue et al. (2024), RA-BLIP (Ding et al., 2024b), Re-IMAGEN (Chen et al., 2022b), MegaPairs (Zhou et al., 2024a), Wiki-LLaVA (Caffagni et al., 2024), VISRAG (Yu et al., 2024) Fusion Mechanisms (4.2) Attention-Based Mechanisms (4.2) RAMM (Yuan et al., 2023), EMERGE (Zhu et al., 2024b), MORE (Cui et al., 2024), RAGTrans (Cheng et al., 2024), AlzheimerRAG (Lahiri and Hu, 2024), MV-Adapter (Jin et al., 2024), C3Net (Zhang et al., 2024c), Xu et al. (2024a), Kim et al. (2024), M2-RAAP (Dong et al., 2024b), Mu-RAG (Chen et al., 2022a), Ou et al. (Ou et al., 2025), CADMR (Khalafaoui et al., 2024) Unified Frameworks and Projections (4.2) Hybrid-RAG (Su et al., 2024b), Dense2Sparse (Nguyen et al., 2024), IRAMIG (Liu et al., 2024b), M3DocRAG (Cho et al., 2024), DQU-CIR (Wen et al., 2024), PDF-MVQA (Ding et al., 2024c), SAM-RAG (Zhai, 2024), UFineBench (Zuo et al., 2024), Li et al. (2022a), ContextEnrichment (4.3) EMERGE (Zhu et al., 2024b), MiRAG (Adjali et al., 2024), Wiki-LLaVA (Caffagni et al., 2024), Video-RAG (Luo et al., 2024b), Img2Loc (Zhou et al., 2024e), Xue et al. (2024) Adaptive and Iterative Retrieval (4.3) In-Context Learning (4.4) SKURG (Yang et al., 2023), IRAMIG (Liu et al., 2024b), OMG-QA (Nan et al., 2024), SAM-RAG (Zhai, 2024), MMed-RAG (Xia et al., 2024a), OmniSearch (Li et al., 2024b), mR2AG (Zhang et al., 2024g), RAGAR (Khaliq et al., 2024) RMR (Tan et al., 2024), Sharifymoghaddam et al. (2024), RA-CM3 (Yasunaga et al., 2023), RAG-Driver (Yuan et al., 2024), MSIER (Luo et al., 2024a), Raven (Rao et al., 2024) Reasoning (4.4) RAGAR (Khaliq et al., 2024), VisDoM (Suri et al., 2024), SAM-RAG (Zhai, 2024), LDRE (Yang et al., 2024) Instruction Tuning (4.4) RA-BLIP (Ding et al., 2024b), RAGPT (Lang et al., 2025), mR2AG (Zhang et al., 2024g), RagVL (Chen et al., 2024d), Jang et al. (2024), MMed-RAG (Xia et al., 2024a), MegaPairs (Zhou et al., 2024a), Surf (Sun et al., 2024a), Rule (Xia et al., 2024b) Source Attribution (4.4) Alignment (4.5) MuRAR (Zhu et al., 2024d), VISA (Ma et al., 2024b), OMG-QA (Nan et al., 2024) VISRAG (Yu et al., 2024), MegaPairs (Zhou et al., 2024a), SAM-RAG (Zhai, 2024), EchoSight (Yan and Xie, 2024), HACL (Jiang et al., 2024), Zhi Lim et al. (2024), Kumar and Marttinen (2024), Dense2Sparse (Nguyen et al., 2024) Robustness (4.5) Buettner and Kovashka (2024), MORE (Cui et al., 2024), AlzheimerRAG (Lahiri and Hu, 2024), RAGTrans (Cheng et al., 2024), RA-BLIP (Ding et al., 2024b), RagVL (Chen et al., 2024d), RA-CM3 (Yasunaga et al., 2023) Augmentation Techniques (4.3) Generation Techniques (4.4) Training Strategies (4.5) Figure 2: Taxonomy of recent advances and enhancements in multimodal retrieval-augmented generation research. tual alignment in multimodal summarization (Tonmoy et al., 2024). In knowledge-intensive applications, multimodal RAG supports open-domain and knowledge-seeking question answering (Chen et al., 2024d; Ding et al., 2024b; Yuan et al., 2023), video-based QA (Luo et al., 2024b), and fact verification (Khaliq et al., 2024), grounding responses in retrieved knowledge and thereby mitigating hallucinations. Cross-modal retrieval further advances zero-shot imagetext retrieval (Yang et al., 2024; Dong et al., 2024b). Additionally, the recent incorporation of chain-of-thought reasoning (Zhai, 2024; Khaliq et al., 2024) has enhanced its ability to support complex problem solving and logical inference. Finally, integrating multimodal RAG into interactive agents and AI assistants such as Gemini (Team et al., 2024) enables natural language-driven visual search, document understanding, and multimodal reasoning. The taxonomy of application domains can be seen in Figure 3. The following sec11 tions explore domain-specific adaptations of these techniques in greater depth. Healthcare and Medicine Multimodal RAG enhances clinical decision-making through integrated analysis of medical imaging, electronic health records, and biomedical literature. Systems like MMED-RAG (Xia et al., 2024a) address diagnostic uncertainty in medical visual question answering by aligning radiology images with contextual patient data. RULE (Xia et al., 2024b) mitigates hallucinations in automated report generation through dynamic retrieval of clinically similar cases. AsthmaBot (Bahaj and Ghogho, 2024) introduces multimodal RAG-based approach for supporting asthma patients across multiple languages, enabling structured, language-specific semantic searches. Predictive frameworks such as Realm (Zhu et al., 2024c) demonstrate robust risk assessment by fusing heterogeneous patient data streams, while Hybrid RAG (Su et al., 2024a) advances privacy-preserving architectures for federated clinical data integration. FactMM-RAG (Sun et al., 2024b) automates radiology report drafting by retrieving biomarker correlations from medical ontologies, exemplifying the paradigms capacity to operationalize expert knowledge at scale. Software Engineering Code generation systems leverage multimodal RAG to synthesize contextaware solutions from technical documentation and version histories. DocPrompting (Zhou et al., 2023) improves semantic coherence in code completion by retrieving API specifications and debugging patterns. Commit message generation models like RACE (Shi et al., 2022) contextualize code diffs against historical repository activity, while CEDAR (Nashid et al., 2023) optimizes few-shot learning through retrieval-based prompt engineering. REDCODER (Parvez et al., 2021) enhances code summarization via semantic search across opensource repositories, preserving syntactic conventions across programming paradigms. Fashion and E-Commerce Cross-modal alignment drives advancements in product discovery and design automation. UniFashion (Zhao et al., 2024) enables style-aware retrieval by jointly embedding garment images and textual descriptors, while Dang (2024) reduces search friction through multimodal query expansion. LLM4DESIGN (Chen et al., 2024c) demonstrates architectural design automation by retrieving compliance constraints and environmental impact assessments, underscoring RAGs adaptability to creative domains. Entertainment and Social Computing Multimedia analytics benefit from RAGs capacity to correlate heterogeneous signals. SoccerRAG (Strand et al., 2024) derives tactical insights by linking match footage with player statistics. MMRA (Zhong et al., 2024) predicts content virality through joint modeling of visual aesthetics and linguistic engagement patterns. Emerging Applications Autonomous systems adopt multimodal RAG for explainable decisionmaking, as seen in RAG-Drivers (Yuan et al., 2024) real-time retrieval of traffic scenarios during navigation. ENWAR (Nazar et al., 2024) enhances wireless network resilience through multi-sensor fusion, while Riedler and Langer (2024) streamline equipment maintenance by retrieving schematics during fault diagnosis. Geospatial systems such as Img2Loc (Zhou et al., 2024e) advance image geolocalization through cross-modal landmark correlation."
        },
        {
            "title": "6 Open Problems and Future Directions",
            "content": "Despite rapid advancements in multimodal RAG systems, fundamental challenges remain in achieving robust, efficient, and human-like reasoning across modalities. Generalization, Explainability, and Robustness Multimodal RAG systems often struggle with domain-specific training and exhibit modality biases, frequently over-relying on text for both retrieval and generation (Winterbottom et al., 2020). Explainability remains key challenge, as these systems typically fail to attribute answers to precise sources. Current methods often cite entire documents or large visual regions as source attribution rather than identifying the exact part of an image, speech, or other modality that led to the answer (Ma et al., 2024b; Hu et al., 2023). In addition, the interplay between modalities affects the quality of outcomes produced by these models; for example, answers derived solely from text sources may differ in quality compared to those requiring combination of text and image inputs (Baltrusaitis et al., 2019). They are also vulnerable to adversarial perturbations, such as misleading images influencing textual outputs, and their performance can degrade when relying on low-quality or outdated sources (Chen et al., 2022b). While the trustworthiness of unimodal RAGs has been studied (Zhou et al., 2024d), enhancing the robustness of multimodal RAGs remains an open challenge and promising 12 research direction. Reasoning, Alignment, and Retrieval Enhancement Multimodal RAGs struggle with compositional reasoning, where information from different modalities must be logically integrated to generate coherent and contextually rich outputs. While cross-modal techniques such as Multimodal-CoT have been proposed (Zhang et al., 2023), further innovations are needed to improve the coherence and contextual relevance of multimodal outputs. Enhancing modality alignment and retrieval strategies, particularly for entity-aware retrieval, is essential. Moreover, despite the potential of knowledge graphs to enrich cross-modal reasoning, they remain largely underexplored in multimodal RAGs compared to text-based RAGs (Zhang et al., 2024g; Procko and Ochoa, 2024). Retrieval biases such as position sensitivity (Hu et al., 2024c), redundant retrieval (Nan et al., 2024), and biases propagated from training data or retrieved content (Zhai, 2024), pose significant challenges that require further attention. Another promising direction is developing unified embedding space for all modalities, enabling direct multimodal search without intermediary conversion models (e.g., ASRs). Despite some progress, mapping multimodal knowledge into unified space remains an open challenge with significant potential. Agent-Based and Self-Guided Systems Recent trends indicate shift towards agent-based multimodal RAGs that integrate retrieval, reasoning, and generation across diverse domains. Unlike static RAG systems, future multimodal RAGs should integrate interactive feedback and selfguided decision-making to iteratively refine outputs. Furthermore, existing feedback mechanisms often fail to accurately identify whether issues with responses arise during retrieval, generation, or other stages (Dong et al., 2024b). The incorporation of reinforcement learning and end-to-end human-aligned feedback into multimodal RAGs remains largely unexplored but holds significant potential for enhancing these systems. These methods could enable multimodal RAGs to assess whether retrieval is necessary, evaluate the relevance of retrieved content, and dynamically determine the most suitable modalities for response generation. Achieving robust support for any-to-any modality is essential for adaptability in open-ended tasks (Wu et al., 2024b). Future multimodal RAGs should incorporate data from diverse real-world sources, such as environmental sensors, alongside traditional modalities like text and images, to enhance situational awareness. This progression aligns with the trend toward embodied AI, where models integrate knowledge with physical interaction, enabling applications in robotics, navigation, and physics-informed reasoning. Bridging retrieval-based reasoning with real-world agency brings these systems closer to AGI. Long-Context Processing, Scalability, and Personalization High computational costs in video frame sampling and memory bottlenecks in processing multi-page documents with images remain key challenges in long-context processing. Fixed extraction rates often fail to capture relevant video frames, necessitating dynamic adjustment based on content complexity and movement (Kandhare and Gisselbrecht, 2024). Additionally, retrieval speedaccuracy trade-offs in edge deployments and redundant computations in cross-modal fusion layers highlight the need for more scalable architectures. Personalization mechanisms, such as adapting retrieval to user-specific contexts like medical history, remain underexplored. As personalization mechanisms evolve, ensuring privacy and mitigating the risks of sensitive data leakage in multimodal outputs remain critical challenges. Lastly, the lack of datasets with complex reasoning tasks and multimodal adversarial examples hinders robust evaluation."
        },
        {
            "title": "7 Conclusion",
            "content": "In this study, we take deep dive into the realm of multimodal Retrieval-Augmented Generation (RAG) literature. Specifically, we explore and categorize key advancements across different aspects of multimodal RAG systems, including retrieval, multimodal fusion, augmentation, generation, and training strategies. Additionally, we examine the tasks these systems address, their domain-specific applications, and the datasets, benchmarks, and evaluation methods. We also discuss open challenges and limitations in current approaches. We hope it could encourage future research in the domain, particularly in improving cross-modal reasoning and retrieval, developing agent-based interactive systems, and advancing unified multimodal embedding spaces."
        },
        {
            "title": "8 Limitations",
            "content": "This study offers comprehensive examination of multimodal RAG systems. Extended discus13 sions, details of datasets and benchmarks, and additional relevant work are available in the Appendices. While we have made our maximum effort; however, some limits may persist. First, due to space constraints, our descriptions of individual methodologies are necessarily concise. Second, although we curate studies from major venues (e.g., ACL, EMNLP, NeurIPS, CVPR, ICLR, ICML, ACM Multimedia) and arXiv, our selection may inadvertently overlook emerging or domain-specific research, with primary focus on recent advancements. Additionally, this work does not include comparative performance evaluation of the various models, as task definitions, evaluation metrics, and implementation details vary significantly across studies, and executing these models requires substantial computational resources. Furthermore, multimodal RAG is rapidly evolving field with many open questions, such as optimizing fusion strategies for diverse modalities and addressing scalability challenges. As new paradigms emerge, our taxonomy and conclusions will inevitably evolve. To address these gaps, we plan to continuously monitor developments and update this survey and the corresponding GitHub repository to incorporate overlooked contributions and refine our perspectives. References Omar Adjali, Olivier Ferret, Sahar Ghannay, and Hervé Le Borgne. 2024. Multi-level information retrieval augmented generation for knowledge-based visual question answering. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1649916513, Miami, Florida, USA. Association for Computational Linguistics. Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, et al. 2019. Nocaps: Novel object captioning at scale. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 110. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millicah, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2024. Flamingo: visual language model for few-shot learning. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Zhiyu An, Xianzhong Ding, Yen-Chun Fu, ChengChung Chu, Yan Li, and Wan Du. 2024. Goldenretriever: High-fidelity agentic retrieval augmented generation for industrial knowledge base. Preprint, arXiv:2408.00798. Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. Spice: Semantic propositional image caption evaluation. Preprint, arXiv:1607.08822. Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. 2023. Palm 2 technical report. Preprint, arXiv:2305.10403. Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. 2017. Localizing moments in video with natural language. In Proceedings of the IEEE International Conference on Computer Vision (ICCV). Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. Proceedings of the IEEE International Conference on Computer Vision, pages 24252433. Md Adnan Arefeen, Biplob Debnath, Md Yusuf Sarwar Uddin, and Srimat Chakradhar. 2024. irag: Advancing rag for videos with an incremental approach. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM 24, page 43414348. ACM. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511. A. Author and B. Author. 2018. Coco-cn for crossimage tagging, captioning and retrieval. lingual arXiv preprint arXiv:1805.08661. A. Author and B. Author. 2023a. Fashionpedia: dataset for fashion understanding. arXiv preprint arXiv:2301.02560. A. Author and B. Author. 2023b. Geode: dataset for geographic dialogue understanding. arXiv preprint arXiv:2301.02560. 14 Adil Bahaj and Mounir Ghogho. 2024. Asthmabot: Multi-modal, multi-lingual retrieval augmented generation for asthma patient support. arXiv preprint arXiv:2409.15815. Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. 2021. Frozen in time: joint video and image encoder for end-to-end retrieval. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 110. Alberto Baldrati, Marco Bertini, and Alberto Del Bimbo. 2023. Zero-shot composed image retrieval with textual inversion. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 110. Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2019. Multimodal machine learning: IEEE Trans. Pattern Anal. survey and taxonomy. Mach. Intell., 41(2):423443. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan. Association for Computational Linguistics. Soyuj Basnet, Jerry Gou, Antonio Mallia, and Deeperimpact: OptimizPreprint, Torsten Suel. 2024. ing sparse learned index structures. arXiv:2405.17093. Ali Furkan Biten, Lluis Gomez, Marcal Rusinol, and Dimosthenis Karatzas. 2022. Viquae: dataset for visual question answering on events. arXiv preprint arXiv:2204.03485. Mikołaj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. 2018. Demystifying MMD In International Conference on Learning GANs. Representations. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. In AdLanguage models are few-shot learners. vances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. 58635870, Miami, Florida, USA. Association for Computational Linguistics. Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. 2015. Activitynet: large-scale video benchmark for human activity understanding. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 961970. Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 2024. Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1818 1826. Mu Cai, Jianwei Yang, Jianfeng Gao, and Yong Jae Lee. 2025. Matryoshka multimodal models. In The Thirteenth International Conference on Learning Representations. Jamie Callan, Matthew Hoy, Anagha Kulkarni, et al. 2022. Clueweb22: 10 billion web documents with visual and semantic information. arXiv preprint arXiv:2211.15848. Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: maximum similarity metric for machine translation evaluation. In Proceedings of ACL-08: HLT, pages 5562. Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, et al. 2015. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012. Chia-Yuan Chang, Zhimeng Jiang, Vineeth Rakesh, Menghai Pan, Chin-Chia Michael Yeh, Guanchu Wang, Mingzhi Hu, Zhichao Xu, Yan Zheng, Mahashweta Das, and Na Zou. 2024. Main-rag: Multi-agent filtering retrieval-augmented generation. Preprint, arXiv:2501.00332. Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2022. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16495 16504. David Chen and William Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 190200. Kyle Buettner and Adriana Kovashka. 2024. Quantifying the gaps between translation and native perception in training for multimodal, multilingual retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024a. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Preprint, arXiv:2402.03216. 15 Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024b. M3embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through selfIn Findings of the Assoknowledge distillation. ciation for Computational Linguistics: ACL 2024, pages 23182335, Bangkok, Thailand. Association for Computational Linguistics. Ran Chen, Xueqi Yao, and Xuhui Jiang. 2024c. Llm4design: An automated multi-modal system for architectural and environmental design. arXiv preprint arXiv:2407.12025. Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. 2022a. Murag: Multimodal retrievalaugmented generator for open question answering over images and text. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 55585570, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W. Cohen. 2022b. Re-imagen: RetrievalPreprint, augmented text-to-image generator. arXiv:2209.14491. Zhanpeng Chen, Chengjin Xu, Yiyan Qi, and Jian Guo. 2024d. Mllm is strong reranker: Advancing multimodal retrieval-augmented generation via knowledge-enhanced reranking and noise-injected training. arXiv preprint arXiv:2407.21439. Zhangtao Cheng, Jienan Zhang, Xovee Xu, Goce Trajcevski, Ting Zhong, and Fan Zhou. 2024. Retrievalaugmented hypergraph for multimodal social media In Proceedings of the 30th popularity prediction. ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24, page 445455, New York, NY, USA. Association for Computing Machinery. Felix Chern, Blake Hechtman, Andy Davis, Ruiqi Guo, David Majnemer, and Sanjiv Kumar. 2022. Tpu-knn: nearest neighbor search at peak flop/s. Advances in Neural Information Processing Systems, 35:15489 15501. Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, and Mohit Bansal. 2024. M3docrag: Multi-modal retrieval is what you need for multiPreprint, page multi-document understanding. arXiv:2411.04952. Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. 2021. Viton-hd: High-resolution virtual try-on via image translation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1413114140. Wanqing Cui, Keping Bi, Jiafeng Guo, and Xueqi Cheng. 2024. retrieval augmented generative commonsense reasoning. Preprint, arXiv:2402.13625. More: Multi-modal Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Preprint, arXiv:2305.06500. Quang-Vinh Dang. 2024. Multi-modal retrieval augmented generation for product query. Library of Progress-Library Science, Information Technology & Computer, 44(3). Ringki Das and Thoudam Doren Singh. 2023. Multimodal sentiment analysis: survey of methods, trends, and challenges. ACM Comput. Surv., 55(13s). Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: large-scale hierarchical image database. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 248255. Damen Dima. 2020. Rescaling egocentric vision. Comput. Res. Reposit., 2006. Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. 2024a. Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models. Preprint, arXiv:2402.10612. Muhe Ding, Yang Ma, Pengda Qin, Jianlong Wu, RaYuhong Li, blip: Multimodal adaptive retrieval-augmented bootstrapping language-image pre-training. Preprint, arXiv:2410.14154. and Liqiang Nie. 2024b. Yihao Ding, Kaixuan Ren, Jiabin Huang, Siwen Luo, and Soyeon Caren Han. 2024c. Pdf-mvqa: dataset for multimodal information retrieval in Preprint, pdf-based visual question answering. arXiv:2404.12720. Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, and Anton Tsitsulin. 2024a. Dont forget to connect! improving rag with graph-based reranking. Preprint, arXiv:2405.18414. Xingning Dong, Zipeng Feng, Chunluan Zhou, Xuzheng Yu, Ming Yang, and Qingpei Guo. 2024b. M2-raap: multi-modal recipe for advancing adaptation-based pre-training towards effective and efficient zero-shot video-text retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, page 21562166, New York, NY, USA. Association for Computing Machinery. Yang Du, Yuqi Liu, and Qin Jin. 2024. Reversed in time: novel temporal-emphasized benchmark for crossIn Proceedings of the modal video-text retrieval. 32nd ACM International Conference on Multimedia, MM 24, page 52605269, New York, NY, USA. Association for Computing Machinery. 16 Desmond Elliott, Stella Frank, Khalil Simaan, and Lucia Specia. 2016. Multi30k: Multilingual englishgerman image descriptions. Proceedings of the 5th Workshop on Vision and Language, pages 7074. Angela Fan, Claire Gardent, Chloé Braud, and Antoine Bordes. 2019. Eli5: Long form question answering. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558 3567. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2024. Colpali: Efficient document retrieval with vision language models. Preprint, arXiv:2407.01449. Chun-Mei Feng, Yang Bai, Tao Luo, Zhen Li, Salman Khan, Wangmeng Zuo, Xinxing Xu, Rick Siow Mong Goh, and Yong Liu. 2023. Vqa4cir: Boosting composed image retrieval with visual question answering. Preprint, arXiv:2312.12273. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023. Retrievalaugmented generation for large language models: survey. ArXiv, abs/2312.10997. Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, et al. 2017. Audioset: An ontology and human-labeled dataset for audio events. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 776780. Silvio Giancola, Mohieddine Amine, Tarek Dghaily, and Bernard Ghanem. 2018. Soccernet: scalable dataset for action spotting in soccer videos. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 1711 1721. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017a. Making the in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR). Yash Goyal, Tushar Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017b. Vqa v2: Visual question answering. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 110. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, and et al. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. 2022. Ego4d: Around the world in 3,000 In Proceedings of the hours of egocentric video. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. 2020. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages 38873896. PMLR. Hadi Kiapour, Xufeng Han, Svetlana Lazebnik, Alexander Berg, and Tamara Berg. 2018. Fashion-gen: The generative fashion dataset and challenge. arXiv preprint arXiv:1806.08317. Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. 2024. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1350413514. Konstantin Hemker, Nikola Simidjievski, and Mateja Jamnik. 2024. HEALNet: Multimodal fusion for heterogeneous biomedical data. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30. Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. 2024a. mPLUG-DocOwl 1.5: Unified structure learning for OCR-free document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 30963120, Miami, Florida, USA. Association for Computational Linguistics. 17 Anwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming Yan, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. 2024b. mplug-docowl2: High-resolution compressing for ocr-free multi-page document understanding. Preprint, arXiv:2409.03420. Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, and Nanyun Peng. 2024c. Mrag-bench: Vision-centric evaluation for retrievalarXiv preprint augmented multimodal models. arXiv:2410.08182. Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, KaiWei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross, and Alireza Fathi. 2023. Reveal: Retrievalaugmented visual-language pre-training with multisource multimodal knowledge memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 23369 23379. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2024a. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Trans. Inf. Syst. Just Accepted. Weiyu Huang, Yuezhou Hu, Guohao Jian, Jun Zhu, and Jianfei Chen. 2024b. Pruning large language models with semi-structural adaptive sparse training. Preprint, arXiv:2407.20584. Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. 2019. Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 590597. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Preprint, arXiv:2112.09118. Young Kyun Jang, Donghyun Kim, Zihang Meng, Dat Huynh, and Ser-Nam Lim. 2024. Visual delta generator with large multi-modal models for semisupervised composed image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1680516814. Soyeong Jeong, Kangsan Kim, Jinheon Baek, and Sung Ju Hwang. 2025. Videorag: Retrievalaugmented generation over video corpus. Preprint, arXiv:2501.05874. Pu Jian, Donglei Yu, and Jiajun Zhang. 2024. Large language models know what is key visual entity: An LLM-assisted multimodal retrieval for VQA. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 10939 10956, Miami, Florida, USA. Association for Computational Linguistics. Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang. 2024. Hallucination augmented contrastive learning for multimodal large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2703627046. Xiaojie Jin, Bowen Zhang, Weibo Gong, Kai Xu, Xueqing Deng, Peng Wang, Zhao Zhang, Xiaohui Shen, and Jiashi Feng. 2024. Mv-adapter: Multimodal video transfer learning for video text retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2714427153. Alistair EW Johnson, Tom Pollard, Nathaniel Greenbaum, Matthew Lungren, Chih-ying Deng, Yifan Peng, et al. 2019. Mimic-cxr-jpg, large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042. Alistair EW Johnson, Tom Pollard, Lu Shen, Lehman Li-wei, Mengling Feng, et al. 2016. Mimic-iii, freely accessible critical care database. Scientific Data, 3:160035. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551. Pankaj Joshi, Aditya Gupta, Pankaj Kumar, and Manas Sisodia. 2024. Robust multi model rag pipeline for documents containing text, table & images. In 2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC), pages 993 999. IEEE. Mahesh Kandhare and Thibault Gisselbrecht. 2024. An empirical comparison of video frame sampling methods for multi-modal rag retrieval. Preprint, arXiv:2408.03340. Yasser Khalafaoui, Martino Lovisetto, Basarab Matei, and Nistor Grozavu. 2024. Cadmr: Cross-attention and disentangled learning for multimodal recommender systems. Preprint, arXiv:2412.02295. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 49044916. PMLR. Mohammed Abdul Khaliq, Paul Yu-Chun Chang, Mingyang Ma, Bernhard Pflugfelder, and Filip Miletic. 2024. RAGAR, your falsehood radar: RAGaugmented reasoning for political fact-checking using multimodal large language models. In Proceedings of the Seventh Fact Extraction and VERification Workshop (FEVER), pages 280296, Miami, Florida, USA. Association for Computational Linguistics. 18 Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 20, page 3948, New York, NY, USA. Association for Computing Machinery. Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. 2019. Fréchet audio distance: metric for evaluating music enhancement algorithms. Preprint, arXiv:1812.08466. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. 2019. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 119132, Minneapolis, Minnesota. Association for Computational Linguistics. Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, and Seong Tae Kim. 2024. Do you remember? dense video captioning with cross-modal memory retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1389413904. Jonathan Krause, Michael Stark, Jia Deng, and Li FeiFei. 2013. 3d object representations for fine-grained categorization. Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 554561. Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. 2017. Dense-captioning events in videos. In Proceedings of the IEEE International Conference on Computer Vision (ICCV). Hilde Kuehne, Ali Arslan, and Thomas Serre. 2014. The language of actions: Recovering the syntax and semantics of goal-directed human activities. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Yogesh Kumar and Pekka Marttinen. 2024. Improving medical multi-modal contrastive learning with expert annotations. In Computer Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part XX, page 468486, Berlin, Heidelberg. Springer-Verlag. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466. Jian Lang, Zhangtao Cheng, Ting Zhong, and Fan Zhou. 2025. Retrieval-augmented dynamic prompt tuning for incomplete multimodal learning. arXiv preprint arXiv:2501.01120. Myeonghwa Lee, Seonho An, and Min-Soo Kim. 2024. PlanRAG: plan-then-retrieval augmented generation for generative large language models as decision makers. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 65376555, Mexico City, Mexico. Association for Computational Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021. Paq: 65 million probably-asked questions and what you can do with them. Transactions of the Association for Computational Linguistics, 9:10981115. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023a. Seed-bench: Benchmarking multimodal llms with generative comprehension. Preprint, arXiv:2307.16125. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: bootstrapping language-image pretraining with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022a. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR. Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2021. Infoseek: dataset for multimodal question answering. arXiv preprint arXiv:2104.06039. Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2022b. Webqa: Multimodal question answering on web videos. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 110. Muquan Li, Dongyang Zhang, Qiang Dong, Xiurui Xie, and Ke Qin. 2024a. Adaptive dataset quantization. Preprint, arXiv:2412.16895. Aritra Kumar Lahiri and Qinmin Vivian Hu. 2024. retrieval Alzheimerrag: Multimodal augmented generation for pubmed articles. Preprint, arXiv:2412.16701. Yangning Li, Yinghui Li, Xingyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Philip Yu, Fei Huang, et al. 2024b. Benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent. arXiv preprint arXiv:2411.02937. player? sion, pages 216233. Springer. In European conference on computer viZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023c. Towards general text embeddings with multi-stage contrastive learning. Preprint, arXiv:2308.03281. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. 2024a. Mm-embed: Universal multimodal retrieval with multimodal llms. Preprint, arXiv:2411.02571. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. European Conference on Computer Vision, pages 740755. Weizhe Lin, Jingbiao Mei, Jinghong Chen, and Bill Byrne. 2024b. PreFLMR: Scaling up fine-grained late-interaction multi-modal retrievers. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 52945316, Bangkok, Thailand. Association for Computational Linguistics. Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, and Lili Qiu. 2024a. Retrievalattention: Accelerating long-context llm inference via vector retrieval. Preprint, arXiv:2409.10516. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning. Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. 2017. Improved image captioning via policy gradient optimization of spider. In Proceedings of the IEEE international conference on computer vision, pages 873881. Xingzu Liu, Mingbang Wang, Songhang Deng, Xinyue Peng, Yanming Liu, Ruilin Nong, David Williams, and Jiyuan Li. 2024b. Iterative retrieval augmentation for multi-modal knowledge integration and generation. Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024c. RA-ISF: Learning to answer and understand from retrieval augmentation via iterative self-feedback. In Findings of the Association for Computational Linguistics: ACL 2024, pages 47304749, Bangkok, Thailand. Association for Computational Linguistics. Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge Yu. 2023b. Universal vision-language dense retrieval: Learning unified representaPreprint, tion space for multi-modal retrieval. arXiv:2209.00179. Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. 2021. Image retrieval on real-life images with pre-trained vision-and-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2125 2134. Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. 2016. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Siqu Long, Feiqi Cao, Soyeon Caren Han, and Haiqin Yang. 2022. Vision-and-language pretrained models: survey. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 55305537. International Joint Conferences on Artificial Intelligence Organization. Survey Track. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521. Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. 2024. Ovis: Structural embedding alignment for multimodal large language model. Preprint, arXiv:2405.20797. Yang Luo, Zangwei Zheng, Zirui Zhu, and Yang You. 2024a. How does the textual information affect the retrieval of multimodal in-context learning? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 5321 5335, Miami, Florida, USA. Association for Computational Linguistics. Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo, and Rongrong Ji. 2024b. Video-rag: Visuallyaligned retrieval-augmented long video comprehension. Preprint, arXiv:2411.13093. Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. 2024a. Unifying multimodal retrieval via document screenshot embedding. Preprint, arXiv:2406.11251. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2025. Mmbench: Is your multi-modal model an all-around Xueguang Ma, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Wenhu Chen, and Jimmy Lin. 2024b. Visa: Retrieval augmented generation with visual source attribution. Preprint, arXiv:2412.14457. 20 Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Yong Hu, Heyan Huang, and Xian-Ling Mao. 2024c. Multi-modal retrieval augmented multi-modal generation: benchmark, evaluate metrics and strong baselines. Preprint, arXiv:2411.16365. Ahmad Nazar, Abdulkadir Celik, Mohamed Selim, Asmaa Abdallah, Daji Qiao, and Ahmed Eltawil. 2024. Enwar: rag-empowered multimodal llm framework for wireless environment perception. arXiv preprint arXiv:2410.18104. Ziyu Ma, Chenhui Gou, Hengcan Shi, Bin Sun, Shutao Li, Hamid Rezatofighi, and Jianfei Cai. 2024d. Drvideo: Document retrieval based long video understanding. Preprint, arXiv:2406.12846. Zhiming Mao, Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Kam-Fai Wong. 2024. Visually guided generative text-layout pre-training for document intelligence. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 47134730, Mexico City, Mexico. Association for Computational Linguistics. Kenneth Marino, Xinlei Chen, Abhinav Gupta, Marcus Rohrbach, and Devi Parikh. 2019. Ok-vqa: visual question answering benchmark requiring external knowledge. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 31953204. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, and Sennur Ulukus. 2025a. retrieval Rag-check: Preprint, augmented generation performance. arXiv:2501.03995. Evaluating multimodal Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, and Sennur Ulukus. 2025b. Re-ranking the context for multimodal retrieval augmented generation. Preprint, arXiv:2501.04695. Linyong Nan, Weining Fang, Aylin Rasteh, Pouya Lahabi, Weijin Zou, Yilun Zhao, and Arman Cohan. 2024. OMG-QA: Building open-domain multimodal generative question answering systems. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 10011015, Miami, Florida, US. Association for Computational Linguistics. Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-based prompt selection for code-related few-shot learning. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pages 24502462. Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. 2024. comprehensive overview of large language models. Preprint, arXiv:2307.06435. Thong Nguyen, Mariya Hendriksen, Andrew Yates, and Maarten de Rijke. 2024. Multimodal learned sparse retrieval with probabilistic expansion control. In Advances in Information Retrieval, pages 448464, Cham. Springer Nature Switzerland. Maria-Elena Nilsback and Andrew Zisserman. 2008. Automated flower classification over large number of classes. Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, pages 722729. Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. 2021. Counterfactual vqa: cause-effect look at language bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12700 12710. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, and et al. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Weihua Ou, Yingjie Chen, Linqing Liang, Jianping Gou, Jiahao Xiong, Jiacheng Zhang, Lingge Lai, and Lei Zhang. 2025. Cross-modal retrieval of chest x-ray images and diagnostic reports based on report entity graph and dual attention: Cross-modal retrieval of chest x-ray images and diagnostic reports... Multimedia Syst., 31(1). Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, Jin Shi, Fan Wu, Pei Chu, Minghao Liu, Zhenxiang Li, Chao Xu, Bo Zhang, Botian Shi, Zhongying Tu, and Conghui He. 2024. Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations. Preprint, arXiv:2412.07626. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, 21 Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Curran Associates, Inc. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An asr corpus based on public domain audio books. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206 5210. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval augmented code generation and summarization. In EMNLP-Findings. John Pavlopoulos, Vasiliki Kougia, and Ion Androutsopoulos. 2019. survey on biomedical image captioning. In Proceedings of the Second Workshop on Shortcomings in Vision and Language, pages 2636, Minneapolis, Minnesota. Association for Computational Linguistics. Shraman Pramanick, Li Jing, Sayan Nag, Jiachen Zhu, Hardik Shah, Yann LeCun, and Rama Chellappa. 2023. Volta: Vision-language transformer with weakly-supervised local-feature alignment. TMLR. Tyler Thomas Procko and Omar Ochoa. 2024. Graph retrieval-augmented generation for large language models: survey. In 2024 Conference on AI, Science, Engineering, and Technology (AIxSET), pages 166169. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024. InFoBench: Evaluating instruction following ability in large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 13025 13048, Bangkok, Thailand. Association for Computational Linguistics. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Varun Nagaraj Rao, Siddharth Choudhary, Aditya Deshpande, Ravi Kumar Satzoda, and Srikar Appalaraju. 2024. Raven: Multitask retrieval augmented visionlanguage learning. Preprint, arXiv:2406.19150. David Rau, Shuai Wang, Hervé Déjean, and Stéphane Context embeddings for efPreprint, Clinchant. 2024. ficient answer generation in rag. arXiv:2407.09252. Xubin Ren, Lingrui Xu, Long Xia, Shuaiqiang Wang, Dawei Yin, and Chao Huang. 2025. Videorag: Retrieval-augmented generation with extreme longcontext videos. Preprint, arXiv:2502.01549. Monica Riedler and Stefan Langer. 2024. Beyond text: Optimizing rag with multimodal inputs for industrial applications. Preprint, arXiv:2410.21943. Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389. Anna Rohrbach, Marcus Rohrbach, Nihar Tandon, and Bernt Schiele. 2015. dataset for movie description. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 110. Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. 2022. Scienceqa: novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3):289301. Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. 2023. Pic2word: Mapping pictures to words for zeroshot composed image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1930519314. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation imagetext models. Advances in Neural Information Processing Systems, 35:2527825294. Christoph Schuhmann, Romain Vencu, Richard Beaumont, Robert Kaczmarczyk, Jenia Jitsev, Atsushi Komatsuzaki, et al. 2021. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: benchmark for visual question answering using world knowledge. Preprint, arXiv:2206.01718. 22 Sahel Sharifymoghaddam, Shivani Upadhyay, Wenhu Chen, and Jimmy Lin. 2024. Unirag: Universal retrieval augmentation for multi-modal large language models. ArXiv, abs/2405.10311. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Association for Computational Linguistics. Xiaobo Shen, Qianxin Huang, Long Lan, and Yuhui Zheng. 2024. Contrastive transformer cross-modal hashing for video-text retrieval. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24, pages 12271235. International Joint Conferences on Artificial Intelligence Organization. Main Track. Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun. 2022. RACE: Retrieval-augmented commit message generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 55205530, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Faisal Tareque Shohan, Mir Tafseer Nayeem, Samsul Islam, Abu Ubaida Akash, and Shafiq Joty. 2024. XL-HeadTags: Leveraging multimodal retrieval augmentation for the multilingual generation of news headlines and tags. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1299113024, Bangkok, Thailand. Association for Computational Linguistics. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 37843803, Punta Cana, Dominican Republic. Association for Computational Linguistics. Gunnar Sigurdsson, Gul Varol, Giovanni Maria Farinella, et al. 2016. Charades: dataset for multimodal research. arXiv preprint arXiv:1604.01753. Gunnar Sigurdsson, Gul Varol, Giovanni Maria Farinella, et al. 2018. Charadesego: dataset for egocentric video understanding. arXiv preprint arXiv:1804.09626. Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. 2025. Agentic retrieval-augmented generation: survey on agentic rag. Preprint, arXiv:2501.09136. Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022. Flava: foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1563815650. Shezheng Song, Xiaopeng Li, Shasha Li, Shan Zhao, Jie Yu, Jun Ma, Xiaoguang Mao, Weimin Zhang, and Meng Wang. 2025. How to bridge the gap between modalities: Survey on multimodal large language model. IEEE Transactions on Knowledge and Data Engineering. Aleksander Theo Strand, Sushant Gautam, Cise Midoglu, and Pål Halvorsen. 2024. Soccerrag: Multimodal soccer information retrieval via natural queries. Preprint, arXiv:2406.01273. Cheng Su, Jinbo Wen, Jiawen Kang, Yonghua Wang, Yuanjia Su, Hudan Pan, Zishao Zhong, and Shamim Hossain. 2024a. Hybrid rag-empowered multi-modal llm for secure data management in internet of medical things: diffusion-based contract approach. IEEE Internet of Things Journal. Cheng Su, Jinbo Wen, Jiawen Kang, Yonghua Wang, Yuanjia Su, Hudan Pan, Zishao Zhong, and M. Shamim Hossain. 2024b. Hybrid rag-empowered multi-modal llm for secure data management in internet of medical things: diffusion-based contract approach. Preprint, arXiv:2407.00978. Xin Su, Man Luo, Kris Pan, Tien Pei Chou, Vasudev Lal, and Phillip Howard. 2024c. Sk-vqa: Synthetic knowledge generation at scale for training context-augmented multimodal llms. arXiv preprint arXiv:2406.19593. Jiashuo Sun, Jihai Zhang, Yucheng Zhou, Zhaochen Su, Xiaoye Qu, and Yu Cheng. 2024a. Surf: Teaching large vision-language models to selectively utilize retrieved information. Preprint, arXiv:2409.14083. Liwen Sun, James Zhao, Megan Han, and Chenyan Xiong. 2024b. Fact-aware multimodal retrieval augmentation for accurate medical radiology report generation. Preprint, arXiv:2407.15268. Manan Suri, Puneet Mathur, Franck Dernoncourt, Kanika Goswami, Ryan Rossi, and Dinesh Manocha. 2024. Visdom: Multi-document qa with visually rich elements using multimodal arXiv preprint retrieval-augmented generation. arXiv:2412.10704. Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. 2021. Multimodal{qa}: complex question answering over text, tables and images. In International Conference on Learning Representations. Cheng Tan, Jingxuan Wei, Linzhuang Sun, Zhangyang Gao, Siyuan Li, Bihui Yu, Ruifeng Guo, and Stan Z. Li. 2024. Retrieval meets reasoning: Even highschool textbook knowledge benefits multimodal reasoning. Preprint, arXiv:2405.20834. Yansong Tang, Xiaohan Wang, Jingdong Wang, et al. 2019. Coin: large-scale dataset for comprehensive instructional video analysis. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 110. 23 Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, HengTze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, and et al. 2024. Gemini: family of highly capable multimodal models. Preprint, arXiv:2312.11805. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. 2022. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pretraining. Preprint, arXiv:2203.12602. SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, and et al. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019. Representation learning with contrastive predictive coding. Preprint, arXiv:1807.03748. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, page 60006010, Red Hook, NY, USA. Curran Associates Inc. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image deIn Proceedings of the IEEE scription evaluation. conference on computer vision and pattern recognition, pages 45664575. Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, and Xiaomo Liu. 2024a. DocLLM: layout-aware generative language model for multimodal document understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 85298548, Bangkok, Thailand. Association for Computational Linguistics. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024b. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang Zhou. 2023. One-peace: Exploring one general representation model toward unlimited modalities. arXiv preprint arXiv:2305.11172. Xin Wang, Jiawei Wu, Junkun Chen, et al. 2019. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 110. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. 2022. Internvideo: General video foundation models via generative and discriminative learning. Preprint, arXiv:2212.03191. Zhihao Wang, Jian Chen, and Steven C. H. Hoi. 2020. Deep learning for image super-resolution: survey. Preprint, arXiv:1902.06068. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2025. Uniir: Training and benchmarking universal multimodal information retrievers. In Computer Vision ECCV 2024, pages 387404, Cham. Springer Nature Switzerland. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2024. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Haokun Wen, Xuemeng Song, Xiaolin Chen, Yinwei Wei, Liqiang Nie, and Tat-Seng Chua. 2024. Simple but effective raw-data level multimodal fusion for composed image retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research 24 and Development in Information Retrieval, SIGIR 2024, page 229239. ACM. Thomas Winterbottom, Sarah Xiao, Alistair McLean, and Noura Al Moubayed. 2020. On modality bias in the tvqa dataset. Preprint, arXiv:2012.10210. Chenyun Wu, Yuting Liu, and Gang Hua. 2019. Fashion iq: new dataset towards retrieving images by natural language feedback. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1130711316. Ian Wu, Sravan Jayanthi, Vijay Viswanathan, Simon Rosenberg, Sina Khoshfetrat Pakazad, Tongshuang Wu, and Graham Neubig. 2024a. Synthetic multimodal question generation. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1296012993, Miami, Florida, USA. Association for Computational Linguistics. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2024b. Next-gpt: Any-to-any multimodal llm. In Proceedings of the International Conference on Machine Learning, pages 5336653397. Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao. 2024a. Mmed-rag: Versatile multimodal rag system for medical vision language models. Preprint, arXiv:2410.13085. Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao. 2024b. RULE: Reliable multimodal RAG for factuality in medical vision language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 10811093, Miami, Florida, USA. Association for Computational Linguistics. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding. Preprint, arXiv:2309.07597. D. Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. 2017. Video question answering via gradually refined attention over appearance and motion. Proceedings of the 25th ACM international conference on Multimedia. Haiyang Xu, Qinghao Ye, Xuan Wu, Ming Yan, Yuan Miao, Jiabo Ye, Guohai Xu, Anwen Hu, Yaya Shi, Guangwei Xu, Chenliang Li, Qi Qian, Maofei Que, Ji Zhang, Xiao Zeng, and Fei Huang. 2023. Youkumplug: 10 million large-scale chinese videolanguage dataset for pre-training and benchmarks. Preprint, arXiv:2306.04362. Huazhe Xu, Yuan Gao, Fisher Yu, and Trevor Darrell. 2018. Bdd-x: dataset for explainable driving behavior. Proceedings of the European Conference on Computer Vision (ECCV), pages 110. Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, and Weidi Xie. 2024a. Retrievalaugmented egocentric video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1352513536. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msrvtt: large video description dataset for bridging video and language. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 110. Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024b. Hallucination is inevitable: An innate limitation of large language models. Preprint, arXiv:2401.11817. Junxiao Xue, Quan Deng, Fei Yu, Yanhao Wang, Jun Wang, and Yuehua Li. 2024. Enhanced multimodal rag-llm for accurate visual question answering. Preprint, arXiv:2412.20927. Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. Yibin Yan and Weidi Xie. 2024. Echosight: Advancing visual-language models with wiki knowledge. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 15381551, Miami, Florida, USA. Association for Computational Linguistics. Qian Yang, Qian Chen, Wen Wang, Baotian Hu, and Min Zhang. 2023. Enhancing multi-modal multi-hop question answering via structured knowledge and unified retrieval-generation. In Proceedings of the 31st ACM International Conference on Multimedia, MM 23, page 52235234, New York, NY, USA. Association for Computing Machinery. Zhenyu Yang, Dizhan Xue, Shengsheng Qian, Weiming Dong, and Changsheng Xu. 2024. Ldre: Llm-based divergent reasoning and ensemble for zero-shot composed image retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, page 8090, New York, NY, USA. Association for Computing Machinery. Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. 2023. End-to-end multimodal fact-checking and explanation generation: challenging dataset and models. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 27332743. Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023. Retrievalaugmented multimodal language modeling. Preprint, arXiv:2211.12561. Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. 2019. Cross-modal self-attention network for referring image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1050210511. 25 Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778. Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. 2024. Visrag: Vision-based retrieval-augmented generPreprint, ation on multi-modality documents. arXiv:2410.10594. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 91279134. Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, and Matthew Gadd. 2024. Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model. Preprint, arXiv:2402.10828. Zheng Yuan, Qiao Jin, Chuanqi Tan, Zhengyun Zhao, Hongyi Yuan, Fei Huang, and Songfang Huang. 2023. Ramm: Retrieval-augmented biomedical visual question answering with multi-modal pretraining. Preprint, arXiv:2303.00534. Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu. 2023. Revisiting neural retrieval on accelerators. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 23, page 55205531, New York, NY, USA. Association for Computing Machinery. Wenjia Zhai. 2024. Self-adaptive multimodal retrievalaugmented generation. Preprint, arXiv:2410.11321. Haoyu Zhang, Jun Liu, Zhenhua Zhu, Shulin Zeng, Maojia Sheng, Tao Yang, Guohao Dai, and Yu Wang. 2024a. Efficient and effective retrieval of dense-sparse hybrid vectors using graph-based approximate nearest neighbor search. Preprint, arXiv:2410.20381. Jinxu Zhang, Yongqi Yu, and Yu Zhang. 2024b. Cream: Coarse-to-fine retrieval and multi-modal efficient tuning for document vqa. In Proceedings of the 32nd ACM International Conference on Multimedia, MM 24, page 925934, New York, NY, USA. Association for Computing Machinery. Juntao Zhang, Yuehuai Liu, Yu-Wing Tai, and ChiKeung Tang. 2024c. C3net: Compound conditioned controlnet for multimodal content generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2688626895. Junyuan Zhang, Qintong Zhang, Bin Wang, Linke Ouyang, Zichen Wen, Ying Li, Ka-Ho Chow, Conghui He, and Wentao Zhang. 2024d. Ocr hinders rag: Evaluating the cascading impact of ocr on retrieval-augmented generation. arXiv preprint arXiv:2412.02592. Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, and Kyusong Lee. 2024e. OmAgent: multi-modal agent framework for complex video understanding with task divide-and-conquer. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1003110045, Miami, Florida, USA. Association for Computational Linguistics. Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, and Kyusong Lee. 2024f. Omagent: multimodal agent framework for complex video understanding with task divide-and-conquer. Preprint, arXiv:2406.16620. Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Zhongang Qi, Chunfen Yuan, Bing Li, Junfu Pu, Yuxuan Zhao, Zehua Xie, Jin Ma, Ying Shan, and Weiming Hu. 2024g. mr2ag: Multimodal retrievalreflection-augmented generation for knowledgebased vqa. ArXiv, abs/2411.15041. Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Junfu Pu, Yuxuan Zhao, Zehua Xie, Jin Ma, Ying Shan, and Weiming Hu. 2024h. mr2ag: Multimodal retrieval-reflection-augmented generation for knowledge-based vqa. Preprint, arXiv:2411.15041. Tianjun Zhang, Shishir Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gonzalez. 2024i. RAFT: Adapting language model to domain specific RAG. In First Conference on Language Modeling. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Preprint, Evaluating text generation with bert. arXiv:1904.09675. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. 2024j. Gme: Improving universal multimodal retrieval by multimodal llms. Preprint, arXiv:2412.16855. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923. Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq Joty. 2023. Retrieving multimodal information for augmented generation: survey. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 47364756, Singapore. Association for Computational Linguistics. 26 Tianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, and Ge Yu. 2024c. Marvel: Unlocking the multi-modal capability of dense retrieval via visual module plugin. Preprint, arXiv:2310.14037. Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, and Philip S. Yu. 2024d. Trustworthiness in retrieval-augmented generation systems: survey. volume abs/2409.10102. Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, and Gengchen Mai. 2024e. Img2loc: Revisiting image geolocalization using multi-modality foundation models and image-based retrieval-augmented generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 27492754. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2024a. Multilingual machine translation with large language models: Empirical results and analysis. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 27652781, Mexico City, Mexico. Association for Computational Linguistics. Yinghao Zhu, Changyu Ren, Zixiang Wang, Xiaochen Zheng, Shiyun Xie, Junlan Feng, Xi Zhu, Zhoujun Li, Liantao Ma, and Chengwei Pan. 2024b. Emerge: Enhancing multimodal electronic health records predictive modeling with retrieval-augmented generation. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM 24, page 35493559, New York, NY, USA. Association for Computing Machinery. Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, and Chengwei Pan. 2024c. Realm: Rag-driven enhancement of multimodal electronic health records analysis via large language models. Preprint, arXiv:2402.07016. Zhengyuan Zhu, Daniel Lee, Hong Zhang, Sai Sree Harsha, Loic Feujio, Akash Maharaj, and Yunyao Li. 2024d. Murar: simple and effective multimodal retrieval and answer refinement framework for multimodal question answering. Preprint, arXiv:2408.08521. Jialong Zuo, Hanyu Zhou, Ying Nie, Feng Zhang, Tianyu Guo, Nong Sang, Yunhe Wang, and Changxin Gao. 2024. Ufinebench: Towards text-based person retrieval with ultra-fine granularity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2201022019. Xiangyu Zhao, Yuehan Zhang, Wenlong Zhang, and Xiao-Ming Wu. 2024. Unifashion: unified visionlanguage model for multimodal fashion retrieval and generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 14901507, Miami, Florida, USA. Association for Computational Linguistics. Chaofan Zheng, Xinyu Lyu, Lianli Gao, Bo Dai, and Jingkuan Song. 2023. Prototype-Based Embedding In 2023 Network for Scene Graph Generation . IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2278322792, Los Alamitos, CA, USA. IEEE Computer Society. Qi Zhi Lim, Chin Poo Lee, Kian Ming Lim, and Ahmad Kamsani Samingan. 2024. Unirag: Unification, retrieval, and generation for multimodal question anIEEE swering with pre-trained language models. Access, 12:7150571519. Ting Zhong, Jian Lang, Yifan Zhang, Zhangtao Cheng, Kunpeng Zhang, and Fan Zhou. 2024. Predicting micro-video popularity via multi-modal retrieval augmentation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, page 25792583, New York, NY, USA. Association for Computing Machinery. Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. 2019. Publaynet: largest dataset ever for document layout analysis. Preprint, arXiv:1908.07836. Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, and Yongping Xiong. 2024a. Megapairs: Massive data synthesis for universal multimodal retrieval. Preprint, arXiv:2412.14475. Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. 2024b. VISTA: Visualized text embedding for universal multi-modal retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31853200, Bangkok, Thailand. Association for Computational Linguistics. Luowei Zhou, Chenliang Xu, and Jason Corso. 2018. Towards automatic learning of procedures from web instructional videos. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1):110. Ren Zhou. 2024. Advanced embedding techniques in multimodal retrieval augmented generation comprehensive study on cross modal ai applications. Journal of Computing and Electronic Information Management, 13(3):1622. Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig. 2023. Docprompting: Generating code by retrieving the docs. In The Eleventh International Conference on Learning Representations."
        },
        {
            "title": "A Taxonomy",
            "content": "In this section, we present comprehensive taxonomy of multimodal retrieval-augmented generation (RAG) systems, along with classification of their application domains. Figures 2 and 3 summarize these taxonomies, respectively. Figure 2 provides an overview of recent advances in multimodal retrieval-augmented generation (RAG) systems. The taxonomy is organized into several key categories. Retrieval strategies cover efficient search and similarity retrieval methods (including maximum inner product search (MIPS) variants and different multi-modal encoders) and modality-centric techniques that distinguish between text-, vision-, and video-centric as well as document retrieval models. Reranking strategies further refine these methods via optimized example selection, relevance scoring, and filtering. are Fusion mechanisms implemented through score fusion and alignment, attentionbased techniques, and unified frameworks that project multimodal information into common representations. Augmentation techniques address context enrichment as well as adaptive and iterative retrieval. Generation methods include in-context learning, reasoning, instruction tuning, and source attribution. training strategies are characterized by approaches to alignment and robustness. Detailed discussions of these categories are provided in the corresponding sections. Figure 3 presents the taxonomy of application domains for multimodal RAG systems. The identified domains include healthcare and medicine, software engineering, fashion and e-commerce, entertainment and social computing, and emerging applications. This classification offers concise overview of the diverse applications and serves as framework for the more detailed analyses that follow. employed in multimodal RAG research. The table is organized into five columns: Category: This column categorizes each dataset or benchmark based on its primary domain or modality. The datasets are grouped into eight categories: ImageText General, VideoText, AudioText, Medical, Fashion, 3D, Knowledge & QA, and Other. The benchmarks are grouped into three categories: Vision-Focused, Cross-Modal and ReasoningFocused, and Text and Knowledge-Focused. This classification facilitates clearer understanding of each dataset or benchmarks role within multimodal framework. Name: The official name of the dataset or benchmarks is provided along with citation for reference. Statistics and Description: This column summarizes key details such as dataset size, the nature of the content (e.g., imagetext pairs, video captions, QA pairs), and the specific tasks or applications for which the dataset or benchmarks are used. These descriptions are intended to convey the datasets scope and its relevance to various multimodal RAG tasks. Modalities: The modalities covered by each dataset or benchmark are indicated (e.g., Image, Text, Video, Audio, or 3D). Notably, several datasets are unimodal; however, within multimodal RAG systems, these are combined with others to represent distinct aspects of broader multimodal context. Link: hyperlink is provided to direct readers to the official repository or additional resources for the dataset or benchmark, thereby facilitating further exploration of its properties and applications."
        },
        {
            "title": "C Metrics",
            "content": "Accuracy: Accuracy is typically defined as the ratio of correctly predicted instances to the total instances. In retrieval-based tasks, Top-K Accuracy is defined as:"
        },
        {
            "title": "B Dataset and Benchmark",
            "content": "Top-K Accuracy(y, ˆf ) = Table 1 and Table 2 present comprehensive overview of datasets and benchmarks commonly 28 1 n1 (cid:88) (cid:88) i=0 j=1 ( ˆfi,j = yi) (5) a n a p A d i M Healthcare and Medicine (5) MMED-RAG (Xia et al., 2024a), RULE (Xia et al., 2024b), AsthmaBot (Bahaj and Ghogho, 2024), Realm (Zhu et al., 2024c), Hybrid-RAG (Su et al., 2024b), FactMM-RAG (Sun et al., 2024b) Software Engineering (5) DocPrompting (Zhou et al., 2023), RACE (Shi et al., 2022), CEDAR (Nashid et al., 2023), REDCODER (Parvez et al., 2021), Fashion and E-Commerce (5) Unifashion (Zhao et al., 2024), Dang (2024), LLM4DESIGN (Chen et al., 2024c), Entertainment and Social Computing (5) SoccerRAG (Strand et al., 2024), MMRA (Zhong et al., 2024) Emerging Applications (5) RAG-Driver (Yuan et al., 2024), ENWAR (Nazar et al., 2024), Riedler and Langer (2024), Img2Loc (Zhou et al., 2024e) Figure 3: Taxonomy of application domains for Multimodal Retrieval-Augmented Generation systems. FID (Fréchet inception distance): FID is metric used to assess the quality of images created by generative model. The formula for FID is: BP = (cid:40) 1 exp (cid:0)1 rl cl (cid:1) length > rl length rl (10) FID = µr µg2 + tr(Σr + Σg 2(cid:112)ΣrΣg) (6) Here, rl represents the reference length and cl represents the candidate length. where µr and Σr are the mean and covariance of real images feature representations, respectively. µg and Σg are the mean and covariance of generated images feature representations, respectively. To extract features, InceptionV3 is typically used. ROUGE-N (N-gram Recall): The ROUGE metric is commonly used to evaluate text summarization and generation. ROUGE-N measures the overlap of N-grams between the generated and reference text. The formula for ROUGE-N is: ROUGE-N = (cid:80) gramN Ref Countmatch(gramN ) (cid:80) gramN Ref Count(gramN ) (7) ROUGE-L measures the longest common subsequence (LCS) between generated and reference text. The formula for ROUGE-L is: ROUGE-L = LCS(X, ) (8) BLEU: BLEU is another metric used for assessing text generation. The formula for calculating BLEU is: BLEU(pn, BP) = BP exp (cid:33) wn log pn (cid:32) (cid:88) n= (9) Here, pn represents the precision of n-grams, wn denotes the weight assigned to the n-gram precision, and the Brevity Penalty (BP) is defined as: BERTScore: BERTScore is metric for evaluating the quality of text generation, based on the similarity between the contextual embeddings of words in the candidate and reference texts. The formula for calculating BERTScore is: BERTScore(c, r) = 1 c (cid:88) i=1 max j=1 cos(ei, ej) (11) is the candidate sentence, and is the reference sentence. ei and ej are the embeddings (e.g., from BERT) for words ci and rj in the candidate and reference sentences, respectively. CLIPScore: CLIPScore is metric that evaluates the similarity between the text and an image by using the CLIP model. The formula for calculating CLIPScore is: CLIPScore = cos(t, i) i (12) where and are text and image embedding respectively. Mean Reciprocal Rank (MRR): MRR is metric used to evaluate the performance of systems that return list of results, such as search engines or recommendation systems. MRR measures the rank position of the first relevant result in the returned list. The formula for calculating MRR is: MRR ="
        },
        {
            "title": "1\nQ",
            "content": "Q (cid:88) q=1 1 rankq (13) where is the total number of queries. rankq is the rank of the first relevant result for query q."
        },
        {
            "title": "D Loss Function",
            "content": "InfoNCE (Information Noise Contrastive Estimation): The InfoNCE loss is commonly used in self-supervised learning, especially in contrastive learning methods. The formula for InfoNCE loss is: LInfoNCE = log exp(sim(zi, zj)/τ ) k=1 exp(sim(zi, zk)/τ ) (cid:80)K (14) where zi and zj are the embeddings of positive pair and τ is temperature parameter. GAN (Generative Adversarial Network): The GAN loss consists of two parts: the discriminator loss and the generator loss. The discriminator loss formula is: LD=Expdata(x)[log D(x)]Ezpz (z)[log(1D(G(z)))] (15) where is real sample from the data distribution. G(z) is the generated sample from the generator, where is noise vector. D(x) is the probability that is real. The Generator loss formula is: LG = Ezpz(z)[log D(G(z))] (16) Triplet Loss: Triplet Loss is used in metric learning to ensure that similar data points are closer together while dissimilar ones are farther apart in the embedding space. The Triplet loss formula is: L=(cid:80)N i=1 max(0,f (xi a)f (xi p)2f (xi a)f (xi n)2+α) (17) is the anchor sample. xi where xi are the positive and negative samples respectively. (x) is the neural network. and xi o a r G T - m e - i e - u l d o a 3 & e n h Table 1: Overview of Popular Datasets in Multimodal RAG Research. Name Statistics and Description Modalities Link LAION-400M (Schuhmann et al., 2021) 200M imagetext pairs; used for pre-training multimodal models. Conceptual-Captions (CC) (Sharma et al., 2018) 15M imagecaption pairs; multilingual EnglishGerman image descriptions. CIRR (Liu et al., 2021) 36,554 triplets from 21,552 images; focuses on natural image relationships. MS-COCO (Lin et al., 2014) 330K images with captions; used for captiontoimage and imagetocaption generation. Flickr30K (Young et al., 2014) 31K images annotated with five English captions per image. Multi30K (Elliott et al., 2016) 30k German captions from native speakers and humantranslated captions. NoCaps (Agrawal et al., 2019) For zeroshot image captioning evaluation; 15K images. Laion-5B (Schuhmann et al., 2022) 5B imagetext pairs used as external memory for retrieval. COCO-CN (Author and Author, 2018) 20,341 images for cross-lingual tagging and captioning with Chinese sentences. CIRCO (Baldrati et al., 2023) 1,020 queries with an average of 4.53 ground truths per query; for composed image retrieval. BDD-X (Xu et al., 2018) 77 hours of driving videos with expert textual explanations; for explainable driving behavior. YouCook2 (Zhou et al., 2018) 2,000 cooking videos with aligned descriptions; focused on videotext tasks. ActivityNet (Caba Heilbron et al., 2015) 20,000 videos with multiple captions; used for video understanding and captioning. SoccerNet (Giancola et al., 2018) Videos and metadata for 550 soccer games; includes transcribed commentary and key event annotations. MSR-VTT (Xu et al., 2016) 10,000 videos with 20 captions each; large video description dataset. MSVD (Chen and Dolan, 2011) 1,970 videos with approximately 40 captions per video. LSMDC (Rohrbach et al., 2015) 118,081 videotext pairs from 202 movies; movie description dataset. DiDemo (Anne Hendricks et al., 2017) 10,000 videos with four concatenated captions per video; with temporal localization of events. Breakfast (Kuehne et al., 2014) 1,712 videos of breakfast preparation; one of the largest fully annotated video datasets. COIN (Tang et al., 2019) 11,827 instructional YouTube videos across 180 tasks; for comprehensive instructional video analysis. MSRVTT-QA (Xu et al., 2017) Video question answering benchmark. MSVD-QA (Xu et al., 2017) 1,970 video clips with approximately 50.5K QA pairs; video QA dataset. ActivityNet-QA (Yu et al., 2019) 58,000 humanannotated QA pairs on 5,800 videos; benchmark for video QA models. Image, Text LAION-400M Image, Text Conceptual Captions Image, Text CIRR Image, Text MS-COCO Image, Text Flickr30K Image, Text Multi30K Image, Text NoCaps Image, Text LAION-5B Image, Text COCO-CN Image, Text CIRCO Video, Text BDD-X Video, Text YouCook2 Video, Text ActivityNet Video, Text SoccerNet Video, Text MSR-VTT Video, Text MSVD Video, Text LSMDC Video, Text DiDemo Video, Text Breakfast Video, Text COIN Video, Text MSRVTT-QA Video, Text MSVD-QA Video, Text ActivityNet-QA EpicKitchens-100 (Dima, 2020) 700 videos (100 hours of cooking activities) for online action prediction; egocentric vision dataset. Video, Text EPIC-KITCHENS-100 Ego4D (Grauman et al., 2022) 4.3M videotext pairs for egocentric videos; massivescale egocentric video dataset. HowTo100M (Miech et al., 2019) 136M video clips with captions from 1.2M YouTube videos; for learning textvideo embeddings. CharadesEgo (Sigurdsson et al., 2018) 68,536 activity instances from egoexo videos; used for evaluation. Video, Text Ego4D Video, Text HowTo100M Video, Text Charades-Ego ActivityNet Captions (Krishna et al., 2017) 20K videos with 3.7 temporally localized sentences per video; densecaptioning events in videos. Video, Text ActivityNet Captions VATEX (Wang et al., 2019) 34,991 videos, each with multiple captions; multilingual videoandlanguage dataset. Charades (Sigurdsson et al., 2016) 9,848 video clips with textual descriptions; multimodal research dataset. WebVid (Bain et al., 2021) 10M videotext pairs (refined to WebVid-Refined-1M). Video, Text VATEX Video, Text Charades Video, Text WebVid Youku-mPLUG (Xu et al., 2023) Chinese dataset with 10M videotext pairs (refined to Youku-Refined-1M). Video, Text Youku-mPLUG LibriSpeech (Panayotov et al., 2015) 1,000 hours of read English speech with corresponding text; ASR corpus based on audiobooks. AudioCap (Kim et al., 2019) 46K audio clips paired with human-written text captions. AudioSet (Gemmeke et al., 2017) 2,084,320 humanlabeled 10second sound clips from YouTube; 632 audio event classes. MIMIC-CXR (Johnson et al., 2019) 125,417 training pairs of chest Xrays and reports. CheXpert (Irvin et al., 2019) 224,316 chest radiographs of 65,240 patients; focused on medical analysis. MIMIC-III (Johnson et al., 2016) Health-related data from over 40K patients (text data). IU-Xray (Pavlopoulos et al., 2019) 7,470 pairs of chest Xrays and corresponding diagnostic reports. Audio, Text LibriSpeech Audio, Text AudioCaps Audio, Text AudioSet Image, Text MIMIC-CXR Image, Text CheXpert Text MIMIC-III Image, Text IU X-ray PubLayNet (Zhong et al., 2019) 100,000 training samples and 2,160 test samples built from PubLayNet (tailored for the medical domain). Image, Text PubLayNet Fashion-IQ (Wu et al., 2019) 77,684 images across three categories; evaluated with Recall@10 and Recall@50. FashionGen (Hadi Kiapour et al., 2018) 260.5K imagetext pairs of fashion images and item descriptions. VITON-HD (Choi et al., 2021) 83K images for virtual tryon; highresolution clothing items. Fashionpedia (Author and Author, 2023a) 48,000 fashion images annotated with segmentation masks and fine-grained attributes. DeepFashion (Liu et al., 2016) Approximately 800K diverse fashion images for pseudo triplet generation. ShapeNet (Chang et al., 2015) 7,500 text3D data pairs; repository for 3D CAD models. VQA (Antol et al., 2015) 400K QA pairs with images for visual question answering. PAQ (Lewis et al., 2021) 65M textbased QA pairs; largescale dataset. ELI5 (Fan et al., 2019) 270K complex and diverse questions augmented with web pages and images. ViQuAE (Biten et al., 2022) 11.8M passages from Wikipedia covering 2,397 unique entities; knowledgeintensive QA. OK-VQA (Marino et al., 2019) 14K questions requiring external knowledge for VQA. WebQA (Li et al., 2022b) 46K queries that require reasoning across text and images. Infoseek (Li et al., 2021) Fine-grained visual knowledge retrieval using Wikipediabased knowledge base ( 6M passages). ClueWeb22 (Callan et al., 2022) 10 billion web pages organized into three subsets; largescale web corpus. MOCHEG (Yao et al., 2023) 15,601 claims annotated with truthfulness labels and accompanied by textual and image evidence. VQA v2 (Goyal et al., 2017b) 1.1M questions (augmented with VGQA questions) for fine-tuning VQA models. Image, Text Fashion IQ Image, Text Fashion-Gen Image, Text VITON-HD Image, Text Fashionpedia Image, Text DeepFashion Text, 3D ShapeNet Image, Text VQA Text Text Text PAQ ELI5 ViQuAE Image, Text OK-VQA Text, Image WebQA Image, Text Infoseek Text ClueWeb22 Text, Image MOCHEG Image, Text VQA v2 ScienceQA (Lu et al., 2022) Highschool science curriculum dataset with 12K training samples and over 21K questions across various subjects. Text ScienceQA A-OKVQA (Schwenk et al., 2022) Benchmark for visual question answering using world knowledge; around 25K questions. Image, Text A-OKVQA MMBench (Liu et al., 2025) Bilingual benchmark for assessing the multimodal capabilities of VLMs; over 3,000 multiplechoice questions covering 20 dimensions. XL-HeadTags (Shohan et al., 2024) 415K news headline-article pairs consist of 20 languages across six diverse language families. SEED-Bench (Li et al., 2023a) 19K multiplechoice questions with accurate human annotations across 12 evaluation dimensions. ImageNet (Deng et al., 2009) 14,197,122 images for perspective understanding; hierarchical image database. Oxford Flowers102 (Nilsback and Zisserman, 2008) 102 flower categories with five examples per category; image classification dataset. Stanford Cars (Krause et al., 2013) Images of different car models (five examples per model); for fine-grained categorization. Text Text Text MMBench XL-HeadTags SEED-Bench Image ImageNet Image Oxford Flowers102 Image Stanford Cars GeoDE (Author and Author, 2023b) 61,940 images from 40 classes across 6 world regions; emphasizes geographic diversity in object recognition. Image GeoDE 31 g C u - s d c - n e a o - r s F - e n a T Table 2: Overview of Popular Benchmarks in Multimodal RAG Research. Name Statistics and Description Modalities Link MRAG-Bench (Hu et al., 2024c) Images MRAG-Bench Evaluates visual information retrieval and integration; tests robustness to irrelevant visual information and visual reasoning. VQAv2 (Goyal et al., 2017a) Images + Text VQAv Contains 265,016 images with 1,105,904 questions; assesses the ability to answer questions about images, integrating image and question text, visual reasoning, and common-sense reasoning. OK-VQA (Marino et al., 2019) Images + Text OK-VQA Comprises 14,031 questions; tests retrieval and integration of external information, combining image, question, and external knowledge, focusing on reasoning with external knowledge. Establishes comprehensive benchmark for multimodal retrieval-augmented generation; M2RAG (Ma et al., 2024c) integrates fine-grained text-modal and multimodal metrics to evaluate foundation models on dynamic retrieval, Images + Text M2RAG multi-hop reasoning, and multimodal integration. Emphasizes dynamic retrieval with rapidly changing information; Dyn-VQA (Li et al., 2024b) Images + Text Dyn-VQA requires integration of dynamic knowledge retrieval and multi-hop reasoning, testing robustness to changing information. MMBench (Liu et al., 2025) Images + Text + Audio MMBench Covers various tasks, including visual question answering, image captioning, and cross-modal retrieval; focuses on integrating information across multiple modalities (vision, text, audio) for general multimodal understanding. ScienceQA (Saikh et al., 2022) Images + Diagrams + Text ScienceQA Contains 21,208 questions; tests scientific reasoning abilities using multimodal contexts (text, diagrams, images); requires deep understanding and integration of information, scientific reasoning, and diagram understanding. Offers over 2 million question-answer pairs; SK-VQA (Su et al., 2024c) focuses on synthetic knowledge generation, multimodal reasoning, and external knowledge integration; Images + Text SK-VQA used for training context-augmented multimodal large language models. Provides 650K question-answer pairs; reading comprehension dataset where questions are based on evidence documents; TriviaQA (Joshi et al., 2017) can be adapted to multimodal RAG by providing images or other visual context related to the trivia; Text TriviaQA focuses on knowledge retrieval and factual accuracy. Contains 307,373 training examples; questions are naturally occurring Google search queries; Natural Questions (Kwiatkowski et al., 2019) adapting this for multimodal RAG involves finding or creating relevant visual contexts for the queries; Text Natural Questions emphasizes knowledge retrieval and handling noisy or ambiguous queries. SMMQG (Wu et al., 2024a) Images + Text SMMQG Includes 1,024 question-answer pairs; focuses on synthetic data generation, multimodal retrieval-augmented generation, and control over question style and modality."
        }
    ],
    "affiliations": [
        "College of Interdisciplinary Science and Technology, University of Tehran, Tehran, Iran",
        "Computer Engineering Department, K.N. Toosi University of Technology, Tehran, Iran",
        "Computer Engineering Department, Sharif University of Technology, Tehran, Iran",
        "Qatar Computing Research Institute, Doha, Qatar"
    ]
}