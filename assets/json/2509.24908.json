{
    "paper_title": "BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications",
    "authors": [
        "Andrés Fernández García",
        "Javier de la Rosa",
        "Julio Gonzalo",
        "Roser Morante",
        "Enrique Amigó",
        "Alejandro Benito-Santos",
        "Jorge Carrillo-de-Albornoz",
        "Víctor Fresno",
        "Adrian Ghajari",
        "Guillermo Marco",
        "Laura Plaza",
        "Eva Sánchez Salido"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ability to summarize long documents succinctly is increasingly important in daily life due to information overload, yet there is a notable lack of such summaries for Spanish documents in general, and in the legal domain in particular. In this work, we present BOE-XSUM, a curated dataset comprising 3,648 concise, plain-language summaries of documents sourced from Spain's ``Bolet\\'{\\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each entry in the dataset includes a short summary, the original text, and its document type label. We evaluate the performance of medium-sized large language models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose generative models in a zero-shot setting. Results show that fine-tuned models significantly outperform their non-specialized counterparts. Notably, the best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\\% performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of 41.6\\% vs.\\ 33.5\\%)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 8 0 9 4 2 . 9 0 5 2 : r BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications BOE-XSUM: Resumenes Extremos en Lenguaje Claro de Decretos Notificaciones Legales en Espanol Andres Fernandez Garcıa1,, Javier de la Rosa2,, Julio Gonzalo1, Roser Morante1, Enrique Amigo1, Alejandro Benito-Santos1, Jorge Carrillo-de-Albornoz1 ,Vıctor Fresno1, Adrian Ghajari1, Guillermo Marco1, Laura Plaza1, Eva Sanchez Salido1 . 1Universidad Nacional de Educacion Distancia, Spain 2The National Library of Norway, Norway Correspondence: nandezgarcia@gmail.com Abstract: The ability to summarize long documents succinctly is increasingly important in daily life due to information overload, yet there is notable lack of such summaries for Spanish documents in general, and in the legal domain in particular. In this work, we present BOE-XSUM, curated dataset comprising 3,648 concise, plain-language summaries of documents sourced from Spains Boletın Oficial del Estado (BOE), the State Official Gazette. Each entry in the dataset includes short summary, the original text, and its document type label. We evaluate the performance of medium-sized large language models (LLMs) fine-tuned on BOEXSUM, comparing them to general-purpose generative models in zero-shot setting. Results show that fine-tuned models significantly outperform their non-specialized counterparts. Notably, the best-performing modelBERTIN GPT-J 6B (32-bit precision)achieves 24% performance gain over the top zero-shot model, DeepSeekR1 (accuracies of 41.6% vs. 33.5%). Keywords: Extreme-summarization, legal texts, generative models, evaluation resources. Resumen: La capacidad de resumir documentos largos de forma concisa es cada vez mas importante en la vida cotidiana debido la sobrecarga de informacion, pero existe una notable escasez de este tipo de resumenes para documentos en espanol en general, en el ambito jurıdico en particular. En este trabajo, presentamos BOEXSUM, un conjunto de datos de 3648 resumenes extremadamente breves en lenguaje claro creados partir de las entradas del Boletın Oficial del Estado (BOE). El conjunto de datos contiene tanto los resumenes como los textos originales etiquetados con el tipo de documento. Ademas, presentamos los resultados de experimentar en modo de fine-tuning de zero-shot con modelos generativos. Nuestros resultados indican que los modelos generativos supervisados mediante fine tuning funcionan significativamente mejor que los modelos generativos en modo no supervisado, incluso siendo modelos mas pequenos. El mejor modelo con finetuning de nuestra experimentacion, BERTIN GPT-J 6B (precision de 32 bits), obtiene resultados un 24% mejores que el mejor modelo no supervisado, DeepSeek-R1 (41,6% vs 33,5%). Palabras clave: Resumen extremo, texto legal, modelos generativos, recursos de evaluacion."
        },
        {
            "title": "1 Introduction\nAmong the many capabilities of large lan-\nguage models (LLMs), summarization stands\nout as one of the most widely used and val-",
            "content": "These authors contributed equally to this work. ued by end users (Budhiraja et al., 2024; Brachman et al., 2025). Recent advances in LLMs have made it possible to summarize extremely long documents with remarkable accuracy and coherence. These breakthroughs have been driven in large part by the availability of large-scale, high-quality summarization datasetsparticularly in English. However, this progress has not been evenly distributed across languages. Spanish, despite being the second most widely spoken native language in the world with over 485 million speakers (Ethnologue, 2023), remains significantly under-resourced in Natural Language Processing (NLP) (Conde et al., 2024). This scarcity is especially pronounced in summarization tasks: search on Hugging Face reveals 963 summarization datasets in English,1 compared to only 75 in Spanish.2 Moreover, domain-specific resourcessuch as those tailored for administrative or legal document summarizationare virtually non-existent in Spanish, further limiting the development and applicability of LLMs in these critical areas. In this work, we introduce BOE-XSUM, curated dataset comprising 3,648 clear and extremely concise summaries of entries from Spains Boletın Oficial del Estado (BOE)3, the countrys State Official Gazette, along with their original texts. This gazette serves as the primary platform for disseminating legal notifications, and legislative decrees, various official documents, predominantly originating from the national government, but including contributions from regional and local authorities. This is the first dataset of its kind in Spanish and is characterized by two key features: first, it addresses the important challenge of adapting complex domain-specific language, such as legal or administrative texts, into clear, everyday language; and second, it provides extremely concise summaries that are manually curated and verified. The extreme summaries are built upon social media posts by Spanish journalist specialized in the analysis and treatment of public information.4 This is socially relevant 1https://huggingface.co/datasets?task_ categories=task_categories:summarization& language=language:en&sort=trending, accessed on April 1st, 2025. 2https://huggingface.co/datasets?task_ categories=task_categories:summarization& language=language:es&sort=trending, accessed on April 1st, 2025. 3https://www.boe.es/ 4Eva Belmonte is journalist and co-director of Civio Foundation (see https://civio.es/) an independent, non-profit organization that monitors pubtask for the control of governments, whether they are central, autonomous, or municipalities, and other public agencies such as the Constitutional and the Supreme Court. For example, in the context of the floods that devastated several regions in Spain in late 2024,5 these posts contributed to inform the population about how central and regional governments were managing public resources to alleviate the effects of the disaster. By compiling and making available the BOE-XSUM dataset, we aim to fill the gap in NLP resources for Spanish, offering valuable resource for both academic research and practical applications. In addition, we present experiments with generative language models. This allows us to answer the following research question: to what extent can generative language models produce extreme summaries of legal texts, not only capturing their complex meaning but also adapting them into clear language in manner comparable to that of human expert? Our contribution is twofold. First, we release new publicly available dataset of extreme summaries in Spanish within the legal and administrative domain.6 Second, we conduct experiments with generative text models, revealing that current systems still struggle to produce high-quality extreme summaries. However, qualitative analysis shows that in some cases, the generated summaries resemble those written by humans. This paper is organized as follows. In Section 2 we present the related work. In Section 3 we describe the BOE-XSUM dataset and provide qualitative analysis of its extreme summaries. Section 4 focuses on the experiments, the results of which are discussed in Section 5. Finally, the conclusions are presented in Section 6."
        },
        {
            "title": "2 Related work\nIn general, there are two main approaches\nto summarization: extractive and abstrac-",
            "content": "lic authorities through data journalism, and works on three lines of action: journalism, public advocacy and transparency services for public administrations. Belmonte is part of multidisciplinary group of people who work to improve the democratic quality in Spain. She publishes concise daily summaries of the BOE in (see https://x.com/evabelmonte). 5https://en.wikipedia.org/wiki/2024_ Spanish_floods. 6https://huggingface.co/datasets/ bertin-project/BOE-XSUM tive summarization (Cajueiro et al., 2023). Within the extractive works, we can find those that compute frequencies, such as those based on spatial vectors (Baeza-Yates et al., 1999; Belwal, Rai, and Gupta, 2021), matrix factorization (Gong and Liu, 2001), graphs (Mihalcea and Tarau, 2004), topics (Haghighi and Vanderwende, 2009) and neural word embeddings (Kageback et al., 2014). In addition to frequency-based methods, there are also approaches based on heuristics (Edmundson, 1969), (Dalal and Zaveri, 2011), linguistics (Edmundson and Wyllys, 1961), (Mohamed, 2016), supervised machine learning (Mao et al., 2019), and reinforcement learning (Ryang and Abekawa, 2012; Hyun et al., 2022). Extreme summarization is form of single-document summarization that aims to generate highly concise summariesoften single sentencethat capture the core meaning of the source text (Narayan, Cohen, and Lapata, 2018a; Cachola et al., 2020a). Unlike extractive methods, it requires abstractive generation to synthesize information from the input, making it particularly useful in domains like scientific literature and news media (Mao, Zhong, and Han, 2022). There exist datasets of summaries in many languages such as English (Grusky, Naaman, and Artzi, 2018), Spanish and Catalan (Segarra Soriano et al., 2022), Indonesian (Koto, Lau, and Baldwin, 2020), and Bengali (Khan et al., 2023) among others. And while there are multilingual datasets available, such as MLSUM (Scialom et al., 2020), WikiLingua (Ladhak et al., 2020), EUR-LexSum (Aumiller, Chouhan, and Gertz, 2022), XL-SUM (Hasan et al., 2021) and HumSet (Fekih et al., 2022), they usually share certain characteristics that are not optimal for their use in training generative models. For instance, summaries may be provided as titles preceding the text, or they might conclude with link to the full text, making it very easy for language models to match the original text and its summary. Such features, particularly given the public nature of these texts, enhance the effectiveness of generative models, which, in the end, perform memorization task, instead of meaning abstraction task. As for extreme summarization datasets in Spanish, the availability of resources is severely limited, with only one available dataset, NoticIA (Garcıa-Ferrero and Altuna, 2024), which consists of 850 Spanish news articles with clickbait headlines, each paired with human-written, single-sentence generative summary. This dataset assesses the ability of models to understand and summarize texts, addressing the challenge of interpreting complex information generated by clickbait headlines. Finding datasets of extreme summaries in languages other than English or Chinese is really difficult (Narayan, Cohen, and Lapata, 2018b; Sotudeh et al., 2021; Cachola et al., 2020b). The challenge of summarizing complex content is particularly evident when translating from highly specialized technical languagesuch as legal or scientific discourseinto more accessible forms. This issue has been widely explored in the context of science communication and plain language movements. Studies in popular science writing have shown how lexical and structural simplification strategies can effectively bring technical content closer to general audiences, while still preserving meaning and nuance (Montalt and Gonzalez Davies, 2007). Similarly, the field of legal communication has highlighted the importance of translating dense, technical legal language into clear and formal language for broader accessibility (Tiersma, 2003). These linguistic transformations not only benefit public understanding but are also central to tasks such as summarization, where preserving intent while changing register is essential. In this sense, research on genre translationfrom specialized to general discourseoffers valuable frameworks for evaluating the capacity of language models to handle extreme summarization tasks in real-world scenarios (Baram-Tsabari and Lewenstein, 2017)."
        },
        {
            "title": "3.1 Sources\nThe BOE-XSUM dataset originates from the\nsocial media posts made by a Spanish jour-\nnalist who performs daily reviews of the\nBOE. She selects the articles that considers of\ngreatest social interest and summarizes them\nin posts that also include a the link to the\nPDF version of the specific BOE article be-",
            "content": "ing summarized. Depending on the relevance or complexity of the content, the journalist writes one or several short posts summarizing the essence of the resolutions and orders in the article of the BOE. In the case of crucial matters, she elaborates detailed long post on an external website, which is subsequently linked to the initial post. Each summary in the dataset has two versions: the original social media post written by the journalist, and an edited version crafted for the extreme summarization task. These edited versions have been refined to ensure accurate representation of the BOE content and to meet the standards of clear language summarization. All experiments presented in this work have been conducted using the edited summaries. We initially collected over 4,500 social media posts. After manual review, we discarded those that lacked direct link to BOE article, were not clearly related to the BOE content, or were too subjective. Detailed annotation guidelines are available in Appendix G. The final dataset consists of 3,648 BOE entries, each paired with an original post and its corresponding edited summary. The BOE articles were also annotated by category to enhance their usability for both summarization and classification tasks."
        },
        {
            "title": "3.2 Data extraction\nThe next step was to classify the links in-\ncluded in the social media posts into two cat-\negories. Given the occasional use of external\nlinks, our analysis focused on distinguishing\nand separating between two main categories\nof links: those linking to the journalist exter-\nnal website, which contains articles related to\nthe BOE, and those linking directly to BOE\ndocuments. This differentiation allowed us\nto capture both the concise summaries in-\nherent in her posts and, where appropriate,\nextended summaries along with direct links\nto BOE documents. In the case of links to\nBOE documents, mostly in PDF format, a\nconversion process was employed to facilitate\ntheir download in plaint text format, preserv-\ning the identifier used originally in the BOE\narticle.7",
            "content": "7For example, from PDF link from the BOE as https://boe.es/boe/dias/2024/03/28/pdfs/ BOE-A-2024-6273.pdf, we first take its identifier BOE-A-2024-6273, https://boe.es/diario_boe/ txt.php?id=BOE-A-2024-6273, and then use it to generate the official URL that exposes the content in"
        },
        {
            "title": "3.3 Manual data review\nAfter a preliminary analysis of the dataset,\na decision was made to develop a tool for\nthe visualization, tagging, and editing of the\ndataset, as depicted in Figure 3 included in\nAppendix A. The main purpose of the tool\nwas to verify the integrity of the data visu-\nalization and being able to modify the data\nif there were any errors. The review of the\ndata was carried out with this tool. We pro-\nceeded to verify, one by one, the correspon-\ndence between the BOE text and the tweet\ncontaining the associated summary. When\nwe identified any inconsistencies, such as a\nsummary that did not match the content of\nthe BOE or that was poorly contextualized,\nwe made the necessary corrections to ensure\nthat the summary accurately reflected the in-\nformation contained in the official text. The\ncomplete annotation guidelines are described\nin Appendix G.",
            "content": "We also edited number of summaries to ensure that they better reflected the content of the original BOE document and agreed to our guidelines, improving clarity and accuracy. Both the original and edited versions are part of the dataset. Importantly, all training and evaluation experiments described in this work have been carried out using the edited versions only. This ensures consistency, eliminates ambiguity, and aligns with the goal of producing high-quality summaries in clear language. In Appendix we show two examples of how the summaries were edited. To estimate the extent of the editing process, we conducted similarity analysis based on cosine distance between original and edited versions.8 Table 8 in Appendix shows that third of the dataset (1,154 posts) had similarity above 90%, while approximately 160 posts had similarity below 10%. This variation reflects the range of interventions appliedfrom minor adjustments to full rewritesto ensure that the summaries do not only provide consistent view, but also faithfully represent the original BOE content. Moreover, in test with two male participants aged between 30 and 45 years, both text format, which we subsequently use to download the content of the articles in plain text for our dataset. 8We extracted embedding vectors using the original multilingual BERT. holding university degrees, summaries edited according to the guidelines were chosen 189 out of 200 times (94.5%), 95% CI [90.42%, 96.9%], significantly above chance (two-tailed binomial test, < 0.001)."
        },
        {
            "title": "3.4 Categorization\nEach entry in the dataset includes both the\noriginal and edited summaries, along with a\nlabel indicating the type of BOE article. All\ncategories are disjoint and the labeling of ar-\nticles follows two main criteria:",
            "content": "1. Explicit Mention in the BOE article: If the name of category is explicitly stated in the BOE article, that label is assigned directly. For example: Articles TRIBUNAL CONSTITUCIONAL beled as Constitutional Court. are from laArticles from CONVENIOS are labeled as Agreement. Some categories are formed by combining multiple clearly defined types. For instance, articles related to awards and medals are grouped under the PREMIOS MEDALLAS category. 2. Frequency-Based Filtering: Categories with very low number of articles are grouped into general category named OTROS ANUNCIOS (Other announcements). the BANCO DE ESPANA One (Bank of Spain) category, which is retained in the dataset due to its relevance, even though it contains relatively few articles. exception is list of all categories with illustrative examples is provided in Table 5 in Appendix B. The distribution of categories across the dataset partitions is shown in Table 6. As presented in Table 7, the dataset contains several columns. Among the most important are: BOE article texts (text), Original posts (summaries), Edited versions of the posts (edited summaries)."
        },
        {
            "title": "3.5 Content Analysis\nAmong the BOE documents, there exists a\nvariety of contents, including straightforward\narticles such as the appointments of ambas-\nsadors or designation of official positions.\nThese particular entries, by virtue of their in-\nherently concise nature, offer a less complex\nsummarization task.",
            "content": "Conversely, the dataset also encompasses BOE articles of more intricate and voluminous nature, including but not limited to decisions from the Constitutional Court, Supreme Court rulings, and various agreements. Such documents are characterized by their extensive length, often spanning thousands of words. To illustrate the complexity and scale of these articles, we show simplified example, with only the introductory and concluding segments of this representative article:9 El Real Decreto-ley 6/2012, de 9 de marzo, de medidas urgentes de proteccion de deudores hipotecarios sin recursos, establece una serie de mecanismos conducentes permitir la reestructuracion de la deuda hipotecaria de quienes padecen extraordinarias dificultades para atender su pago. tal fin, al citado Real Decretoley se incorporo un codigo de buenas practicas al que podran adherirse las entidades cuyo seguimiento sera supervisado por una comision de control, cuya composicion ha sido modificada por el artıculo 6 de la Ley 1/2013, de 14 de mayo, de medidas para ... ... Caja Rural San Jaime de Alquerıas Nino Perdido, S. Coop. de Credito V. Caja Rural San Jose de Almassora, S. Coop. de Credito. V. Caja Rural San Jose de Burriana, S. Coop. de Credito V. Caja Rural San Jose de Nules, S. Coop. de Credito V. Caja Rural San Roque de Almenara, S. Coop. de Credito V. Colonya-Caixa Destalvis de Pollenca. Liberbank, S. A. Publicredit, S. L. UNOE Bank, S. A."
        },
        {
            "title": "The original summary for this BOE article",
            "content": "as written by the journalist is as follows: Adhesiones 2 codigos de buenas practicas en hipotecas Has ahora, poco efectivos. Porque legislar no, verdad? #BOE The resulting edited summary: Lista de adhesiones de bancos los codigos de buenas practicas para reforzar la proteccion los deudores hipotecarios, reestructuracion de deuda alquiler social As Table 1 shows, the dataset contains total of 3,648 BOE texts, with total 13,304,989 space-delimited words. The average number of words per document is 3,396, while the summaries average total of 17 words, which gives us 0.005% compression rate from the original text to the summary. More than 64% of BOE documents have less than 1,000 words and only 2.65% have more than 25,000 words. histogram for BOE 9See English translations in Appendix F. documents with at least 25,000 words can be seen in Figure 1.10 e o B 40 30 20 0 25K 50K 150K 100K Words per document 200K Figure 1: Histogram showing the distribution of BOE documents by word count for BOE documents with at least 25,000 words. The dataset was divided into three splits: train, development, and test. Initially, we adhered to an 80/10/10 distribution, but adjustments were made to ensure that the annotated categories were well-balanced across the splits. The distribution of categories per split is provided in Table 6 of Appendix C."
        },
        {
            "title": "4 Experiments\nWe address the task of extreme summariza-\ntion using BOE-XSUM, which requires deep\ntextual comprehension to produce highly\ncondensed summaries of no more than 280\ncharacters. This length constraint was cho-\nsen to parallel the character limit typical of\nthe social media platform on which the orig-\ninal summaries were written. The degree of\ncompression varies significantly across docu-\nments; some entries in the BOE corpus are\nrelatively brief, while others are substantially\nlonger, as detailed in the analysis section. We\nevaluate language models under two settings:\nfine-tuning and zero-shot prompting.",
            "content": "Although the dataset also includes the original posts for reference, all experiments are conducted using the edited summaries produced through the manual curation process described in Section 3.3. This ensures consistent and contextually accurate benchmark for the extreme summarization task."
        },
        {
            "title": "4.1 Evaluation Metrics\nThe evaluation of LLMs performed using\ncommon metrics in the literature: BLEU\n(Papineni et al., 2002), ROUGE (Lin, 2004),",
            "content": "10This is due to the inclusion of complete external texts in the BOE document itself, such as Royal Decrees and Laws. See also Table 8 in Appendix for the percentages of texts by ranges of increasing size. METEOR (Lavie and Agarwal, 2007), and BERTScore (Zhang et al., 2020). BLEU focuses on n-gram overlap with reference texts and includes brevity penalty to discourage overly short outputs. METEOR emphasizes unigram precision and recall with added flexibility through synonym and stem matching, and includes penalties for fragmented matches. ROUGE measures quality using ngram overlap and the longest common subsequence, with variants tailored to different summarization aspects. BERTScore leverages contextual embeddings from models like BERT to assess semantic similarity via cosine similarity, enabling more nuanced comparison beyond surface-level word matching."
        },
        {
            "title": "4.2 Fine-tuning experiments\nWe used BERTIN GPT-J 6B11 (De la Rosa\net al., 2022; De la Rosa and Fern´andez, 2022)\nas the base model. After downloading the en-\ntire BOE from 1988 to 2023, we continued\npre-training this model for 3 epochs using\nthe same parameters as the original config-\nuration, resulting in a domain-adapted vari-\nant, BOLETIN.12 With these two models in\nplace, we conducted a grid search to train\nthem under various configurations.",
            "content": "To establish performance baseline, we first fully fine-tuned both models using 32-bit precision. In parallel, we explored parameterefficient fine-tuning using Low-Rank Adaptation (LoRA) (Hu et al., 2022). LoRA introduces trainable low-rank matrices into key components of the models attention mechanismspecifically the layers responsible for projecting and integrating attention informationallowing the model to adapt to new tasks while updating only small subset of parameters. This significantly reduces computational and memory requirements. Training was conducted for up to 600 steps using mixed-precision formats (4-, 8-, and 16bit), as detailed in Table 2. Results in Table 1 illustrate the trade-offs between full and parameter-efficient fine-tuning across model variants. Given the architectural constraints of these models, the input sequence is limited to maximum of 2048 tokens. To accommodate summary generation within this limit, 11https://huggingface.co/bertin-project/ bertin-gpt-j-6B 12https://huggingface.co/bertin-project/ BOLETIN Model Precision Layers BLEU METEOR ROUGE BERTScore BOLETIN BERTIN GPT-J 6B 4-bit 8-bit 16-bit 4-bit 8-bit 16-bit 32-bit 4-bit 8-bit 16-bit 4-bit 8-bit 16-bit 32-bit 13 13 13 27 27 27 All 13 13 13 27 27 27 All 0.050 0.065 0.066 0.054 0.062 0.074 0.094 0.062 0.057 0.061 0.068 0.056 0.057 0.109 0.258 0.292 0.299 0.264 0.292 0.306 0.333 0.274 0.295 0.306 0.278 0.295 0.305 0. 0.281 0.306 0.308 0.287 0.305 0.317 0.367 0.289 0.304 0.314 0.293 0.305 0.313 0.393 0.262 0.289 0.294 0.269 0.286 0.298 0.397 0.264 0.287 0.299 0.267 0.291 0.300 0.416 Table 1: Scores of the differet metrics for fine-tuned BOLETIN and BERTIN GPT-J 6B models evaluated on the test split. The models differ in precision (bit-depth) and the number of layers trained (13 or 27 for LoRA training, All for full fine-tuning). The last row for each model corresponds to the result for 32-bit model with all layers trained. Best scores in bold. Parameter Value Batch size Gradient accumulation steps Warmup steps Max steps Learning rate 4 4 100 600 2e-4 Table 2: Fine-tuning hyperparameters for all experiments. portion of tokens must be reserved for both initiating the summary and for the models output. During training, we appended the marker ### RESUMEN: (### SUMMARY: ) after each BOE article, followed by its corresponding summary. In cases where an article exceeded the token limit, we truncated the input as to accommodate for the task marker and allow sufficient space for model output during generation. For especially long documents, we ensured that the summary referred primarily to the initial portion of the article, minimizing the risk that truncation would remove critical information required for accurate summarization. However, these heuristics were not always sufficient as many BOE texts are long and complex. As result, non-negligible number of examples likely lacked critical information for the generation of accurate summaries. We did not systematically exclude or mark truncated samples, which means that some training instances may have introduced noise or incomplete context. This could partially explain the issues observed in some generated summaries, which were vague or abruptly cut. Future work should consider integrating models with larger context windows or using strategies such as sliding windows or hierarchical encoding to handle long legal texts more effectively. Table 1 provides the results of the finetuning experiments for both BERTIN GPT-J 6B and BOLETIN. Despite the high level of complexity of the task, most models achieve similar outcomes, with full-precision finetuning outperforming any LoRA configuration. We also noted that the number of trained layers is not predictive of performance (see Figure 4 in Appendix E). BLEU (B) METEOR (M) ROUGE (R) BERTScore (BS) 1.000 0.872 0.936 0.919 0.872 1.000 0.967 0.934 0.936 0.967 1.000 0.991 BS 0.919 0.934 0.991 1. Table 3: Pearson correlation matrix between automatic evaluation metrics. Moreover, Table 3 shows that all metrics are strongly correlated with each other, suggesting that they measure related aspects of the quality of the generated text. ROUGE and BERTScore show the highest correlation (0.991), indicating that they tend to vary together and reflect very similar characteristics of the content, such as coverage and semantics. BLEU has the lowest correlation with METEOR (0.872), but it is still high. This makes sense, as BLEU is more strict (based on exact n-gram matches), while METEOR is more flexible and semantically oriented. Overall, using these metrics together provides coherent and consistent evaluation of model performance. Interestingly, despite the additional pretraining of the BOLETIN model on the full content of the BOE (19882023) to enhance its legal domain knowledge, this strategy did not lead to improved performance on the extreme summarization task in plain language. Our initial hypothesis was that continued pretraining would help the model better grasp the structure, terminology, and semantics of legal and administrative texts, thereby improving its ability to generate accurate summaries. While this approach may be beneficial for tasks that require formal or technical language, it appears to be less effective when the objective is to produce highly concise summaries in clear, accessible language. The specialized patterns reinforced during domain adaptation may have introduced linguistic mismatch with the target style. This result underscores the importance of aligning not only the domain but also the linguistic register of the training data with the specific demands of the downstream task. deeper analysis revealed that the length constraint during training impacted negatively the generation of summaries. For example, we encountered incomplete summaries like: Real Decreto que modifica el Real Decreto 369/1999, de 5 de marzo, sobre terminos y..., instead of the expected summary: Curas evangelicos que acrediten haber ejercido antes de 1999 cobraran pension de jubilacion. Or cases like: El CSD notifica todos los interesados en el recurso del Real Madrid contra las modificaciones de los..., where the correct summary should have been: El Real Madrid recurre las reformas de reglamento estatutos sociales de la Liga de Futbol."
        },
        {
            "title": "4.3 Prompting experiments\nFor zero-shot experimentation, we selected a\nvariety of open source and proprietary mod-\nels of different sizes, some multilingual some\ntailored to Spanish content. Our goal was\nto analyze the performance of both large and\nsmall models.",
            "content": "In this setting, the importance of the prompt is crucial in determining the quality of response of generative model. After some iterations and limited number of manual trials, we settled on prompt that showed promise of being precise and effective in the generation of summaries. un experto generando Prompt: Eres resumenes cotidiano en lenguaje partir de documentos legales escritos en lenguaje formal. Quiero que me des un resumen en espanol de entre 15 22 palabras del siguiente texto. Recuerda, solo quiero que devuelvas el resumen, nada mas. Devuelveme unicamente el resumen, solo el resumen. continuacion te indico el texto que debes resumir: [ boe document] Table 4 shows the results from the experiments using the edited summaries13. On BOE-XSUM, DeepSeek R1 produced the best results with an BERTScore of 0.335, followed by the model ChatGPT 4o with BERTScore of 0.327 and Llama 3 70b Instruct with score of 0.285. Gemma 2 27B obtained 0.266. It is remarkable that the model Gemma 2 9B produced better summaries than Llama 2 70B, given their difference in size.14 We also observed that Llama 2 70B was prone to generate much longer summaries, while ChatGPT 4o, Llama 3 70B Instruct, and Gemma 2 models produced shorter and more accurate summaries. The three Gemma 2 models produced the shortest summaries, but they got worse as the model got smaller. It is also striking that the average length of the summaries produced by the top-ranked models, ChatGPT 4o and DeepSeek R1, is close to that of those of the ground truth summaries (17 words), with an average of 16.68 and 18.99 words, respecImportantly, there seems to exist tively. strong negative correlation between the average number of words of generated summaries and their BERTScore values across models (Pearsons = 0.82, < 0.001), suggesting that as the length of generated summary increases, its BERTScore against the ground truth tends to decrease significantly."
        },
        {
            "title": "5 Discussion and Future Work\nGiven the evident lack of summarization\ndatasets in Spanish, BOE-XSUM will enable\nthe training of models that generate extreme\nsummaries in clear language, as well as to cat-\negorize this type of texts. We have demon-",
            "content": "13See Table 10 in Appendix for results obtained using the original unmodified original posts by the journalist, which are generally worse. 14See Table 12 in Appendix for examples of Gemma 2 2B vs Llama 2 70B. Model BLEU METEOR ROUGE BERTScore Avg. Words Gemma 2 27b Gemma 2 9B Gemma 2 2B ChatGPT 4o Llama 3 70b Instruct DeepSeek R1 Nemotron 70B Llama 3.2 3B Llama 3.2 1B Llama 3.1 70B Llama 2 70B Llama 3.1 8B Salamandra 7B Instruct Solar 10.7B Neural-Chat 7B Mistral 7B Starling 7B Llava 7B 0.041 0.042 0.041 0.042 0.044 0.041 0.035 0.035 0.020 0.029 0.038 0.034 0.013 0.011 0.011 0.015 0.007 0.007 0.221 0.211 0.194 0.265 0.227 0.251 0.183 0.197 0.143 0.229 0.156 0.199 0.113 0.110 0.125 0.107 0.088 0.080 0.245 0.238 0.222 0.314 0.269 0.307 0.249 0.236 0.187 0.238 0.216 0.243 0.171 0.172 0.193 0.176 0.157 0.144 0.266 0.255 0.248 0.327 0.285 0.335 0.205 0.245 0.174 0.245 0.214 0.251 0.141 0.166 0.187 0.158 0.149 0.126 BERTIN GPT-J 6B 0. 0.365 0.393 0.416 12.02 14.93 16.53 16.65 18.75 18.99 19.93 21.40 31.69 31.69 33.72 33.72 66.24 60.70 57.77 73.65 72.92 88.48 16.10 Table 4: Performance results of the generative models with prompts across different metrics. Added BERTIN GPT-J 6B for reference. Best scores in bold, second best underlined. 0.4 c E 0.3 0.2 0.1 BERTIN GPT-J 6B DeepSeek R1 ChatGPT 4o Nemotron 70B Gemma 2 27B Gemma 2 9B Gemma 2 2B Llama 3 70B Instruct Llama 3.2 10B Llama 2 70B Llama 3.2 3B Llama 3.1 70B Llama 3.2 1B Average Words (log scale) Neural-Chat 7B Solar 10.7B Starling 7B Mistral 7B Salamandra 7B Instruct Llava 7B 0.82 102 Figure 2: BERTScore against average number of words in generated summaries (log scale). BERTIN GPT-J 6B is highlighted with bold blue star and excluded from the regression calculation. strated experimentally that this is complex task and major challenge for generative models, regardless of size. The low number of summarized texts in certain categories raises the possibility of discarding them or merging them into the category OTROS, but we decided against because these categories are very relevant to the dataset. Our results show that the task is more effective when fine-tuning is performed. All our fine-tuned models with precision up to 8-bit outperform models in zero-shot settings, exIt is imcept ChatGPT 4o and DeepSeek. portant to notice that the fine-tuned models have only 6B parameters, in contrast with the 671B parameters of DeepSeek R1. However, the comparison between finetuned and zero-shot models is not entirely symmetrical. Fine-tuned models were specifically trained on the BOE-XSUM dataset, with input-output structures tailored to the task, whereas zero-shot models were evaluated using single static prompt, without domain-specific tuning or few-shot techniques. Nonetheless, it was exciting to discover that smaller and limited models (6B parameters, 2048-token context window) could outperform much larger frontier models when fine-tuned for narrowly defined and linguistically constrained task. In that sense, the results showcase the power of targeted fine-tuning in legal summarization scenarios. Future work should continue exploring few-shot learning and prompt optimization for general-purpose models, in order to better understand their potential when properly guided. Although much remains to be done, due to the complexity of the task, extreme summarization of legal texts as extensive as those in the BOE remains significant challenge for current models. Key open questions include (i) to what extent automatic metrics are correctly assessing model behavior, and (ii) what is the relative complexity contributed by the two core aspects of the task, i.e., extreme summarization and translation to plain language. We also acknowledge the limitations of relying on standard automatic metrics such as BLEU, ROUGE, METEOR, and BERTScore for this specific task. These metrics may fail to capture essential qualities such as clarity, usefulness, or alignment with non-specialist expectations. Notably, we observed strong negative correlation between the length of the generated summaries and the BERTScore (-0.82), which suggests that more informative summaries may be unfairly penalized for being slightly longer. This raises concerns about whether these metrics are truly aligned with the human notion of better summaries in this setting. While we conducted limited human evaluation, more robust human-centered validation is needed. Moreover, while the dataset originated from journalistic summaries, we emphasize that all summaries used in our experiments were carefully edited to eliminate stylistic idiosyncrasies and ensure fidelity to the BOE source. This reduces the risk of source bias and provides more standardized and generalizable input for model training and evaluation. Future work should thus advance in several directions. First, robust category classifier could be developed to automatically assign thematic labels to BOE texts, enhancing the utility of the dataset and enabling more nuanced analysis. Second, the introduction of more accurate and task-specific evaluation metrics is crucial. These could include clarity-based judgments or be inspired by natural language inference or questionanswering paradigms, aiming to capture the communicative goals of extreme summarization more effectively than traditional n-grambased or embedding-based scores. Together with expanded human evaluation protocols, these lines of work will help consolidate this task as benchmark for controlled, highprecision text generation in legal and administrative domains."
        },
        {
            "title": "6 Conclusions\nWe have developed a spanish dataset in the\nhighly sought-after legal domain, designed to\ntrain and evaluate extreme summary genera-\ntion models in clear language, suitable for mi-\ncroblogging platforms such as X or Bluesky.\nThe dataset contributes to the increasingly\nrelevant task of bridging the gap between le-\ngal language and the citizens affected by legal\ntexts. The dataset presented will facilitate\nthe training of systems that help the super-\nvision of public administrations and allow the\ndetection of relevant BOE entries that are of\npublic interest.",
            "content": "Our preliminary experiments with the dataset indicate that (even small) fine-tuned systems seem better choice than unsupervised (prompted) frontier models. However, these results are obtained with automatic evaluation measures, and it remains to be verified whether such measures are adequate in this context. Another lines of future work involve applying prompt engineering to optimize the performance of unsupervised models, and determining the relative challenge posed by the summarization factor and the translation to clear language factor. Also, we plan to enhance the dataset with lenghtier summaries from entries in the Civio website. Finally, given that we have categorized dataset with extreme summaries of articles published in the BOE, future work could focus on leveraging this dataset to develop model capable of classifying all daily BOE entries. generative model could be trained to automatically generate summaries for the classified entries. This would enable the automation and publication of extremesummaries for BOE entries that are of broad public interest. Acknowledgments We thank David Cabo, co-director and CTO of the Civio Foundation, for his continued efforts to promote transparency in public institutions. We are especially grateful to codirector and journalist Eva Belmonte for her outstanding work making the Spanish Official State Gazette (BOE) understandable and relevant to the public. Through her project El BOE nuestro de cada dıa, (Our daily BOE) she has highlighted the Gazettes most impactful content, helping bridge the gap between government actions and citizen awareness. This work has been partially funded by the European Union - NextGenerationEU through the Recovery, Transformation and Resilience Plan, by the Ministry of Economic Affairs and Digital Transformation. Additionally, we thank Google for providing compute resources via the Tensor Research Cloud program, which significantly supported our model training efforts. References Aumiller, D., A. Chouhan, and M. Gertz. 2022. Eur-lex-sum: multiand crosslingual dataset for long-form summarization in the legal domain. Baeza-Yates, R., B. Ribeiro-neto, D. Mills, O. Bonn, S. Juan, M. Mexico, C. Taipei, A. Wesley, and L. Limited. 1999. Modern information retrieval. 07. Baram-Tsabari, A. and B. V. Lewenstein. Science communication training: 2017. What are we trying to teach? International Journal of Science Education, Part B, 7(3):285300. Belwal, R. C., S. Rai, and A. Gupta. 2021. Text summarization using topicbased vector space model and semantic measure. Information Processing and Management, 58(3):102536. Brachman, M., A. El-Ashry, C. Dugan, and W. Geyer. 2025. Current and future use of large language models for knowledge work. Budhiraja, R., I. Joshi, J. S. Challa, H. D. Akolekar, and D. Kumar. 2024. its not like jarvis, but its pretty close! - examining chatgpts usage among undergraduate students in computer science. In Proceedings of the 26th Australasian Computing Education Conference, ACE 24, page 124133, New York, NY, USA. Association for Computing Machinery. Cachola, I., K. Lo, A. Cohan, and D. Weld. 2020a. TLDR: Extreme summarization of scientific documents. In T. Cohn, Y. He, and Y. Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 47664777, Online, November. Association for Computational Linguistics. Cachola, I., K. Lo, A. Cohan, and D. S. Weld. 2020b. TLDR: Extreme summarization of scientific documents. Cajueiro, D. O., A. G. Nery, I. Tavares, M. K. D. Melo, S. A. dos Reis, L. Weigang, and V. R. R. Celestino. 2023. comprehensive review of automatic text summarization techniques: method, data, evaluation and coding. Conde, J., M. Gonzalez, N. Melero, R. Ferrando, G. Martinez, E. Merino-Gomez, J. A. Hernandez, and P. Reviriego. 2024. Open conversational LLMs do not know most spanish words. Procesamiento De Lenguaje Natural, 73:95108. Dalal, M. K. and M. A. Zaveri. 2011. Heuristics based automatic text summarization of unstructured text. In Proceedings of the International Conference & Workshop on Emerging Trends in Technology, ICWET 11, page 690693, New York, NY, USA. Association for Computing Machinery. De la Rosa, J. and A. Fernandez. 2022. Zeroshot reading comprehension and reasoning for spanish with BERTIN GPT-J-6B. In IberLEF@ SEPLN. De la Rosa, J., E. G. Ponferrada, M. Romero, P. Villegas, P. Gonzalez de Prado Salas, and M. Grandury. 2022. BERTIN: Efficient pre-training of spanish language model using perplexity sampling. Procesamiento del Lenguaje Natural, 68(0):13 23. Edmundson, H. P. 1969. New methods in automatic extracting. J. ACM, 16:264 285. Edmundson, H. P. and R. E. Wyllys. 1961. Automatic abstracting and indexingsurvey and recommendations. Commun. ACM, 4(5):226234, May. Ethnologue. 2023."
        },
        {
            "title": "The most",
            "content": "ken languages worldwide. //www.ethnologue.com/insights/ most-spoken-language/. Ultimo acceso: 04/04/2024. spohttps: Fekih, S., N. Tamagnone, B. Minixhofer, R. Shrestha, X. Contla, E. Oglethorpe, and N. Rekabsaz. 2022. Humset: Dataset of multilingual information extraction and classification for humanitarian crisis response. Garcıa-Ferrero, I. and B. Altuna. 2024. Noticia: clickbait article summarization dataset in spanish. Gong, Y. and X. Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 01, page 1925, New York, NY, USA. Association for Computing Machinery. Grusky, M., M. Naaman, and Y. Artzi. 2018. Newsroom: dataset of 1.3 million summaries with diverse extractive strategies. In M. Walker, H. Ji, and A. Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 708719, New Orleans, Louisiana, June. Association for Computational Linguistics. Haghighi, A. and L. Vanderwende. 2009. for multiExploring content models In North document American Chapter of the Association for Computational Linguistics. summarization. Hasan, T., A. Bhattacharjee, M. S. Islam, K. Samin, Y.-F. Li, Y.-B. Kang, M. S. Rahman, and R. Shahriyar. 2021. Xlsum: Large-scale multilingual abstractive summarization for 44 languages. Hu, E. J., Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of 2022. In International large language models. Conference on Learning Representations. Hyun, D., X. Wang, C. Park, X. Xie, and H. Yu. 2022. Generating multiple-length summaries via reinforcement learning for unsupervised sentence summarization. Khan, A., F. Kamal, M. A. Chowdhury, T. Ahmed, M. T. R. Laskar, and S. Ahmed. 2023. BanglaCHQ-summ: An abstractive summarization dataset for medical queries in Bangla conversational speech. In F. Alam, S. Kar, S. A. Chowdhury, F. Sadeque, and R. Amin, editors, Proceedings of the First Workshop on Bangla Language Processing (BLP-2023), pages 8593, Singapore, December. Association for Computational Linguistics. Koto, F., J. H. Lau, and T. Baldwin. 2020. Liputan6: large-scale Indonesian dataset for text summarization. In K.-F. Wong, K. Knight, and H. Wu, editors, Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 598608, Suzhou, China, December. Association for Computational Linguistics. Kageback, M., O. Mogren, N. Tahmasebi, and D. Dubhashi. 2014. Extractive summarization using continuous vector space models. 04. Ladhak, F., E. Durmus, C. Cardie, and K. McKeown. 2020. WikiLingua: new benchmark dataset for cross-lingual abstractive summarization. Lavie, A. and A. Agarwal. 2007. METEOR: an automatic metric for mt evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT 07, page 228231, USA. Association for Computational Linguistics. Lin, C.-Y. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74 81, Barcelona, Spain, July. Association for Computational Linguistics. Mao, X., H. Yang, S. Huang, Y. Liu, and R. Li. 2019. Extractive summarization using supervised and unsupervised learning. Expert Systems with Applications, 133:173181. Mao, Y., M. Zhong, and J. Han. 2022. CiteSum: Citation text-guided scientific exRyang, S. and T. Abekawa. 2012. Framework of automatic text summarization using reinforcement learning. In J. Tsujii, J. Henderson, and M. Pasca, editors, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 256265, Jeju Island, Korea, July. Association for Computational Linguistics. Scialom, T., P.-A. Dray, S. Lamprier, B. Piwowarski, and J. Staiano. 2020. Mlsum: The multilingual summarization corpus. Segarra Soriano, E., V. Ahuir, L.-F. Hurtado, and J. Gonzalez. 2022. DACSA: large-scale dataset for automatic summarization of Catalan and Spanish newspaper articles. In M. Carpuat, M.-C. de Marneffe, and I. V. Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5931 5943, Seattle, United States, July. Association for Computational Linguistics. Sotudeh, S., H. Deilamsalehy, F. Dernoncourt, and N. Goharian. 2021. Tldr9+: large scale resource for extreme summarization of social media posts. Tiersma, P. M. 2003. The plain language movement. Law and Contemporary Problems, 66(1/2):217240. Zhang, T., V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. 2020. Bertscore: Evaluating text generation with bert. treme summarization and domain adaptation with limited supervision. In Y. Goldberg, Z. Kozareva, and Y. Zhang, edthe 2022 Conferitors, Proceedings of ence on Empirical Methods in Natural Language Processing, pages 1092210935, Abu Dhabi, United Arab Emirates, December. Association for Computational Linguistics. Mihalcea, R. and P. Tarau. 2004. TextRank: Bringing order into text. In D. Lin and D. Wu, editors, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404 411, Barcelona, Spain, July. Association for Computational Linguistics. Mohamed, M. A. 2016. Automatic text summarisation using linguistic knowledgebased semantics. Montalt, V. and M. Gonzalez Davies. 2007. Translating scientific and technical texts: Discourse and communication strategies In Sciin the popularization of science. entific and Technical Translation. Routledge, pages 4568. Narayan, S., S. B. Cohen, and M. Lapata. 2018a. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarizaIn E. Riloff, D. Chiang, J. Hocktion. enmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 17971807, Brussels, Belgium, October-November. Association for Computational Linguistics. Narayan, S., S. B. Cohen, and M. Lapata. 2018b. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. ArXiv, abs/1808.08745. In P. Papineni, K., S. Roukos, T. Ward, and W.- J. Zhu. 2002. BLEU: method for automatic evaluation of machine transIsabelle, E. Charniak, lation. and D. Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311 318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics."
        },
        {
            "title": "A Editor server",
            "content": "The features of this editing server include: Figure 3: Editor server. Display of the current tweet number, facilitating easy navigation and reference. Indication of the data items status within the new database, distinguishing between Registered and Not registered statuses. Navigation functionality, including Back and Forward buttons, which allow users to seamlessly move through the dataset. Inclusion of the original tweet authored by Eva Belmonte, providing direct link to the initial data point. Presentation of the plain text version of the BOE documents, essential for content verification and analysis. verification checkbox to confirm the accuracy of the BOEs plain text, addressing instances of incorrect document linkage. Despite the rarity of such occurrences, all affected entries have been excluded from the dataset. An automated cleanup editor for the tweet, designed to enhance data cleanliness and usability. The identifier of the BOE document, enabling users to verify the relevance and accuracy of the associated tweet. Categorization functionality for each data item, facilitating structured analysis and retrieval. Save button to ensure any modifications or tags applied are retained."
        },
        {
            "title": "B Categories annotated for the text categorization task",
            "content": "Category OTROS ANUNCIOS CONTRATOS LICITACIONES NOMBRAMIENTOS CESES TRIBUNAL CONSTITUCIONAL SUBVENCIONES DECRETO LEY TRIBUNAL DE CUENTAS CONVOCATORIAS PREMIOS MEDALLAS CONVENIOS TRIBUNAL SUPREMO REGISTRO EMPRESA ONG Examples El Gobierno publica la nueva metodologıa de calculo precios pequeno consumidor energıa La readjudicacion del contrato Aguas de Valencia por importe total de 7.000.000 euros Nombrada comisionada del gobierno frente al reto demografico Edelmira Barreira Diz. Sentencia del TC que anula los lımites de Cataluna la libertad de horarios comerciales. Subvenciones sindicatos del Ministerio de Empleo: 8.883.890 euros. Ley de facturacion electronica. Para contratos con la administracion: todos obligados partir de 15 enero 2015 El Tribunal de Cuentas busca mas de 400.000 euros perdidos en liquidacion en la Camara Oficial de la Propiedad Urbana de Vizcaya El Ministro de Hacienda Administraciones Publicas ha dispuesto hacer publica la adjudicacion de puestos de trabajo especificados en el anexo la presente orden. Premio Nacional de Historia de Espana 2013 Jose Angel Sanchez Asiain Ratificacion proxima entrada en vigor (el 1 de abril) del convenio de doble nacionalidad Francia-Espana firmado hace un ano. Permite tener ambas nacionalidades (tambien recuperarla si se perdio para tener la del otro paıs) Karl Friedrich Schober recurre contra la Orden HAP/72/2013, la declaracion informativa sobre bienes derechos situados en el extranjero, ante la Audiencia Nacional Exteriores crea una Oficina Consular Honoraria en Incheon (Corea del Sur) para relaciones economicas. ERRORES MODIFICACIONES BOE Correccion de errores para quitar la marca en la casilla trabajador sin ELECCIONES TRASVASES MEDIOAMBIENTE EJECUCION PRESUPUESTO INDULTOS BANCO DE ESPANA especializacion sobre la madre Resultados definitivos elecciones municipales, hasta la de Jaen. La ley de contaminacion residuos que entra en vigor manana. Acorta plazo para conseguir autorizacion ambiental Ejecucion del presupuesto en junio. Indulto Marıa Salmeron Parrilla aprobado viernes en Consejo de Ministros. Multa del Banco de Espana Austrogiros Entidad de Pago, S.A. por un reguero de incumplimientos de la ley (no tener direccion en Espana, irregularidades contables, etc...): 1.300.000 euros mas multas los dos administradores inhabilitacion. Table 5: Categories annotated for the text categorization task and examples."
        },
        {
            "title": "C Additional Information about the Dataset",
            "content": "Category All Train Development Test OTROS ANUNCIOS CONTRATOS LICITACIONES NOMBRAMIENTOS CESES TRIBUNAL CONSTITUCIONAL SUBVENCIONES DECRETO LEY TRIBUNAL DE CUENTAS CONVOCATORIAS PREMIOS MEDALLAS CONVENIOS TRIBUNAL SUPREMO REGISTRO EMPRESA ONG ERRORES MODIFICACIONES BOE ELECCIONES TRASVASES MEDIOAMBIENTE EJECUCION PRESUPUESTO INDULTOS BANCO DE ESPA NA 1004 648 324 311 228 173 161 155 152 141 97 82 52 37 33 25 20 5 789 505 255 245 177 137 128 120 119 112 75 63 44 30 27 21 16 4 111 73 35 31 25 19 18 17 16 12 11 9 4 3 3 2 2 104 70 35 34 26 17 17 17 16 15 11 10 5 4 2 2 2 0 Table 6: Distribution of text categories annotated in BOE-XSUM."
        },
        {
            "title": "Description",
            "content": "Unique item identifier. BOE category identifier. id boe materials boe date publication Publication date of the BOE article. boe previous boe id boe title boe soup xml tweet original boe category boe alert boe departament tweet text cleaned Previous BOE articles that are modified by this new BOE. BOE identifier. Title of the BOE article. Complete scraped web page. Original tweet by Eva Belmonte. Category to which this item belongs. BOE classification codes for government areas. Government department that issued the BOE article. Extreme summary generated from thorough review of Eva Belmontes tweet. Subsequent legislation articles modified by this order (Only for articles referring to laws). boe subsequent Table 7: Dataset Columns. Range Count Range Count 90%+ 80%+ 70%+ 60%+ 50%+ 1154 1624 2028 2398 2720 40%+ 30%+ 20%+ 10%+ 0%+ 2946 3162 3341 3482 3648 Table 8: Distribution of cosine similarity ranges for the pairs of original and modified tweets. Examples of edited summaries As an example, we edited the summary below because the content of the post gave very little information about the BOE article it referred to: (1) Original: Es habitual tratar el tema Microsoft en administraciones publicas, pero solemos olvidar la presencia de Oracle #BOE Edited: La Agencia Estatal de Seguridad Aerea contrata Oracle por el servicio de mantenimiento soporte tecnico por un importe total de 1.232.070,40 euros. # pairs # words # words doc/sum doc sum words/doc words/sum average # average # compression rate"
        },
        {
            "title": "Train\nDev\nTest\nTotal",
            "content": "2,867 392 389 3,648 10,859,884 1,155,745 1,367,413 13,383,042 49,461 6,733 6,605 62,799 3,787 2,948 3,515 3,668 17 17 17 17 0.0045% 0.0058% 0.0048% 0.0050% Table 9: Dataset statistics showing the total number of items, word counts in documents and summaries, mean word count in documents and summaries, and the compression ratio for the train, development, and test sets. (2) Original: It is common to talk about Microsoft in public administrations, but we tend to forget the presence of Oracle. Edited: The Aviation Safety State Agency contracts Oracle for the maintenance and technical support service for total amount of 1,232,070.40 euros. In the next example, we edited the content because the post was talking about people and entities not present in the BOE article. (3) Original: La Moncloa tiene que licitar su servicio de restauracion de forma urgente porque la empresa anterior que daba este servicio, Dulcinea nutricion -que hasta 2018 presidıa uno de los bisnietos de Franco ahora esta en concurso-, dejo de pagar impuestos. #BOE. Edited: Licitacion del servicio de restauracion, de limpieza de los espacios destinados dicha finalidad de maquinas de venta automatica en el Complejo de La Moncloa.. (4) Original: La Moncloa has to tender its catering service urgently because the previous company that provided this service, Dulcinea nutricion - which until 2018 was presided over by one of Francos great-grandchildren and is now in competition - stopped paying taxes.. #BOE. Edited: Bidding for catering services, cleaning of the areas destined for this purpose and vending machines in the Moncloa Complex.."
        },
        {
            "title": "E Additional Information about Results",
            "content": "Model BLEU METEOR ROUGE BERTScore Llama 3.2 1B Llama 3.1 8B Llama 3.1 70B Llama 3 70B Instruct Llama 2 70B Gemma 2 2B Gemma 2 9B Gemma 2 27B Mistral 7B Neural-Chat 7B Starling 7B Llava 7B Solar 10.7B Nemotron 70B Salamandra 7B Instruct ChatGPT 4o 0.010 0.015 0.016 0.017 0.009 0.015 0.019 0.018 0.006 0.006 0.003 0.002 0.005 0.012 0.004 0.019 0.137 0.185 0.220 0.175 0.110 0.183 0.207 0.193 0.094 0.110 0.074 0.063 0.102 0.143 0.082 0.208 0.171 0.220 0.214 0.204 0.154 0.211 0.232 0.214 0.152 0.167 0.134 0.114 0.159 0.185 0.127 0. 0.124 0.196 0.207 0.178 0.117 0.198 0.213 0.197 0.106 0.132 0.092 0.069 0.118 0.132 0.070 0.206 Table 10: Results on test set with unmodified social media posts by the journalist. Best scores in bold. Figure 4: Comparative results between Boletin and Bertin across bit widths and training configurations. Summary Gemma 2 9B Jose Antonio Perez Tapias es nombrado Catedratico de Filosofıa en la Universidad de Granada. F1 0.79 Dif 0. 0.38 Perez Tapias ya es Catedratico de Filosofıa en la Universidad de Granada. Summary LLama 2 70B F1 Tweet La Rectora de la Universidad de Granada, Marıa Pilar Aranda Ramırez, ha nombrado Jose Antonio Perez Tapias como Catedratico de Universidad del area de conocimiento de Filosofıa, adscrito al Departamento de Filosofıa II, mediante Resolucion de 18 de enero de 2017. Se licita el mantenimiento soporte tecnico de sistemas Oracle de la Agencia Estatal de Seguridad Aerea por valor estimado de 1.018.240 euros. de Ayuntamiento El Barcelona licita un sistema para infraccontrolar ciones medioambientales de vehıculos en la zona de bajas emisiones. 0.63 El presidente de la mesa de contratacion, Eusebio Jimenez Arroyo, anuncio que se busca una empresa para el mantenimiento soporte tecnico de los sistemas de informacion en la tecnologıa Oracle de Agencia Estatal de Seguridad Aerea. El contrato tendra una duracion de 20 meses se adjudicara la empresa que ofrezca los mejores criterios de adjudicacion. La fecha lımite para la presentacion de ofertas es el 3 de junio de 2014. de Ayuntamiento Barcelona invita licitar para un contrato de suministro instalacion de un sistema de monitorizacion control de infracciones medioambientales la ciudad. El plazo de ejecucion es de 11 meses, se requiere una garantıa definitiva del 5% del ... (continue) 0.66 El en 0.19 0.44 La Agencia Estatal de Seguridad Aerea contrata Oracle por el servicio de mantenimiento soporte tecnico por un importe total de 1.232.070,40 euros. 0.12 Barcelona licita el contrato para el control de los vehıculos en la zona de bajas emisiones. 0.54 Table 12: Table with summaries, F1 scores, and differences."
        },
        {
            "title": "F English Version of BOE example with original and edited summaries",
            "content": "Royal Decree-Law 6/2012, of March 9, on urgent measures for the protection of mortgage debtors without resources, establishes series of mechanisms aimed at enabling the restructuring of mortgage debt for those experiencing extraordinary difficulties in making their payments. To this end, the aforementioned Royal Decree-Law incorporated Code of Good Practices, to which entities may adhere, and whose compliance will be supervised by control commission. The composition of this commission was modified by Article 6 of Law 1/2013, of May 14, on measures for... ... Caja Rural San Jaime de Alquerıas Nino Perdido, S. Coop. de Credito V. Caja Rural San Jose de Almassora, S. Coop. de Credito V. Caja Rural San Jose de Burriana, S. Coop. de Credito V. Caja Rural San Jose de Nules, S. Coop. de Credito V. Caja Rural San Roque de Almenara, S. Coop. de Credito V. ColonyaCaixa dEstalvis de Pollenca Liberbank, S.A. Publicredit, S.L. UNOE Bank, S.A. The original summary for this BOE article as written by the journalist is as follows: Adhesions to 2 codes of good practices in mortgages. So far, not very effective. Because legislating is hard, right? #BOE The resulting edited summary: List of bank adhesions to the codes of good practices to strengthen the protection of mortgage debtors, debt restructuring, and social rent Annotation Guidelines To improve consistency, objectivity, and reliability, these annotation guidelines will help the annotators create concise, accurate, and legally faithful extreme summaries of BOE texts. 1. General Principles Each summary must: Be concise Between 15 and 22 words. Be factual Avoid opinions, subjective language, or speculation. Preserve critical legal information Names, amounts, dates, and key legal actions must be included. Avoid redundancy Keep the summary clear and direct. Use simple but precise language The goal is to make legal information more accessible. 2. Formatting Rules Use neutral, third-person language (No opinions, interpretations, or assumptions). Avoid legal jargon unless necessary If legal term is essential, keep it."
        },
        {
            "title": "If it can be simplified",
            "content": "without changing meaning, do so. No abbreviations unless widely known (e.g., BOE, EU, UN are fine). Bad Example (Too vague & informal) Another BOE article about contract for an unnamed company. Good Example (Clear, factual, and legally informative) The Ministry of Transport awards AC2M contract to Ferrovial for highway maintenance in Madrid. 3. Information Hierarchy What to include first? If space is limited, prioritize information in this order:"
        },
        {
            "title": "Include in Summary",
            "content": "① ② ③ ④"
        },
        {
            "title": "Main Legal Action",
            "content": "Who is involved?"
        },
        {
            "title": "Financial or Legal Consequences",
            "content": "Date or Location (if critical) Example The government approves new tax reduction for self-employed workers. The Ministry of Defense signs AC5M cybersecurity contract with IBM. judge orders company to pay AC1.2M for contract fraud. new rental law takes effect in Barcelona on January 1, 2025. If space is limited, drop information from the bottom of the list first. 4. Common Errors & How to Avoid Them Subjectivity or Opinions useless new law that wont change anything. The government introduces new environmental protection law."
        },
        {
            "title": "Missing Critical Information",
            "content": "A contract was signed. (Who signed it? What for?) The Ministry of Health awards AC3.5M contract for hospital equipment."
        },
        {
            "title": "Overly Complex Legal Language",
            "content": "In accordance with Royal Decree 1892/2024, framework agreement has been instituted for the execution of infrastructural oversight. The government approves contract for highway inspections."
        },
        {
            "title": "Misleading Summaries",
            "content": "The Supreme Court supports corruption! (Misinterpretation) The Supreme Court rules against corruption conviction in case. 5. Handling Difficult Cases A) Summarizing Lengthy Legal Texts If BOE article is very long, focus on the main action in the first few paragraphs. BOE Text (Full Version): The government issues new decree modifying Article 10 of the Labor Code, changing workplace safety regulations. Summary: New decree modifies workplace safety regulations in Spain. B) Appointments & Dismissals Format: [Person] appointed/dismissed as [position] at [institution]. BOE Text: By Royal Decree 2024/317, Jose Perez is appointed Minister of Finance. Summary: Jose Perez appointed Minister of Finance by Royal Decree. C) Court Rulings Mention court name, ruling outcome, and key details. BOE Text: The Constitutional Court overturns law restricting media freedom. Summary: The Constitutional Court annuls media restriction law. D) Large Financial Figures If space is limited, round numbers where possible. Example: The Ministry of Transport signs AC2,345,678.90 contract for roads. Can be summarized as: AC2.3M contract for roadworks by Ministry of Transport. 5. Final Checklist for Annotators Did you keep the summary between 1522 words? Is the summary neutral and free of opinion? Does it capture the key legal action (law, contract, ruling, appointment)? Does it include important details (names, amounts, institutions)? Did you avoid excessive jargon? Would non-lawyer understand it?"
        }
    ],
    "affiliations": [
        "The National Library of Norway, Norway",
        "Universidad Nacional de Educacion Distancia, Spain"
    ]
}