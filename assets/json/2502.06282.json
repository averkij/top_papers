{
    "paper_title": "Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE",
    "authors": [
        "Haiduo Huang",
        "Fuwei Yang",
        "Zhenhua Liu",
        "Yixing Xu",
        "Jinze Li",
        "Yang Liu",
        "Xuanwu Yin",
        "Dong Li",
        "Pengju Ren",
        "Emad Barsoum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Speculative decoding (SD) accelerates large language model inference by using a smaller draft model to predict multiple tokens, which are then verified in parallel by the larger target model. However, the limited capacity of the draft model often necessitates tree-based sampling to improve prediction accuracy, where multiple candidates are generated at each step. We identify a key limitation in this approach: the candidates at the same step are derived from the same representation, limiting diversity and reducing overall effectiveness. To address this, we propose Jakiro, leveraging Mixture of Experts (MoE), where independent experts generate diverse predictions, effectively decoupling correlations among candidates. Furthermore, we introduce a hybrid inference strategy, combining autoregressive decoding for initial tokens with parallel decoding for subsequent stages, and enhance the latter with contrastive mechanism in features to improve accuracy. Our method significantly boosts prediction accuracy and achieves higher inference speedups. Extensive experiments across diverse models validate the effectiveness and robustness of our approach, establishing a new SOTA in speculative decoding. Our codes are available at https://github.com/haiduo/Jakiro."
        },
        {
            "title": "Start",
            "content": "Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE Haiduo Huang 1 2 Fuwei Yang 1 Zhenhua Liu 1 Yixing Xu 1 Jinze Li 3 Yang Liu 1 Xuanwu Yin 1 Dong Li 1 Pengju Ren 2 Emad Barsoum 1 5 2 0 2 0 1 ] . [ 1 2 8 2 6 0 . 2 0 5 2 : r Abstract Speculative decoding (SD) accelerates large language model inference by using smaller draft model to predict multiple tokens, which are then verified in parallel by the larger target model. However, the limited capacity of the draft model often necessitates tree-based sampling to improve prediction accuracy, where multiple candidates are generated at each step. We identify key limitation in this approach: the candidates at the same step are derived from the same representation, limiting diversity and reducing overall effectiveness. To address this, we propose Jakiro, leveraging Mixture of Experts (MoE), where independent experts generate diverse predictions, effectively decoupling correlations among candidates. Furthermore, we introduce hybrid inference strategy, combining autoregressive decoding for initial tokens with parallel decoding for subsequent stages, and enhance the latter with contrastive mechanism in features to improve accuracy. Our method significantly boosts prediction accuracy and achieves higher inference speedups. Extensive experiments across diverse models validate the effectiveness and robustness of our approach, establishing new SOTA in speculative decoding. Our codes are available at https://github.com/haiduo/Jakiro. 1. Introduction Large language models (LLMs), such as GPT-4o (Jaech et al., 2024), LLaMA3 (Dubey et al., 2024) and Deepseekr1 (Guo et al., 2025), have demonstrated remarkable capabilities across wide range of applications, including questionanswering, code synthesis, and machine translation. However, their token-by-token decoding process, combined with 1Advanced Micro Devices, Inc., Beijing, China 2Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xian, China 3University of HongKong, HongKong, China. Correspondence to: Haiduo Huang <huanghd@stu.xjtu.edu.cn>. Preprint. Under review. Figure 1. Comparison of different speculative decoding methods. the growing size of the models, leads to significant inference latency, posing challenges for real-world deployment. Recently, speculative decoding (Leviathan et al., 2023; Chen et al., 2023) has emerged as an effective technique to accelerate LLM inference. This approach utilizes an efficient but weak draft model to predict multiple tokens in sequence, which are then verified in parallel by more powerful but expensive target model. Since LLMs inference is often memory access bound (Shazeer, 2019), the verification stage can efficiently leverage hardware parallelism, achieving substantial and lossless speedups. Recent works, such as Medusa (Cai et al., 2024) and Hydra (Ankner et al., 2024), leverage multiple independent heads to predict the next tokens, with all heads relying on the same final-layer features of the target model. This shared dependency limits the decoupling of predictions for different tokens, thereby constraining the overall prediction accuracy. In contrast, Eagle (Li et al., 2024a) and Eagle2 (Li et al., 2024b) adopts an autoregressive approach, where predictions at different time steps are based on the features from the previous step. Although the Eagle-style approaches successfully decouples draft tokens across different time steps,"
        },
        {
            "title": "Jakiro",
            "content": "Figure 2. Speedup ratio of Vicuna, LLaMA2-chat, and LLaMA3-instruct models inference latency on the MT-bench for non-greedy (Temperature=1) settings. The above reproduction results are based on the open-source code from the original paper and are averaged over four inference runs on an A100-40G GPU. In this paper, we only compare with speculative sampling based methods that do not need to finetune the backbone models, ensuring the output text distribution remains constant. the top-k tokens within the same layer of the draft tree remain coupled, limiting the diversity of predicted tokens and constraining their immense potential. To enhance prediction accuracy, both Medusa-style and Eagle-style utilize tree-based attention mechanism (Miao et al., 2023) to generate multiple candidate tokens simultaneously. However, we observe that these candidates are derived from the same feature representation, resulting in inherent correlations that limit the predictive accuracy of the draft model. To address this, we propose Jakiro with dynamic decoupling mechanism that leverages an MoE mechanism (Jiang et al., 2024) to construct the attention tree, which retains the independence between layers of the traditional draft tree while introducing decoupling within the layer. By assigning different experts to generate candidate tokens, our method effectively decouples the correlations among predictions. This approach increases the draft models representation diversity while introducing minimal computational cost. As result, our method significantly improves the accuracy of the draft models prediction, leading to superior overall speedup. Moreover, due to the inherent exposure bias (Arora et al., 2022) in the autoregressive inference of LLMs, the prediction error increases as the sequence length grows. Considering the inference overhead of the draft model itself, we observe that it is not wise to adopt an autoregressive approach to construct the draft sequence in every step. Instead, we adopt parallel decoding strategy (Xia et al., 2024) for token generation in the last two steps, further improving the overall inference efficiency of LLMs. To further enhance performance during the parallel decoding phase, we introduce contrastive mechanism (Li et al., 2022) into MoEs dual-branch heads to improve the helpfulness of predictions, further boosting SD performance in the last two steps. Unlike SCMoE (Shi et al., 2024), which utilizes unselected experts in self-contrastive manner during inference, we only perform contrastive operations between the two activated expert heads, introducing almost no additional latency. An overview comparison of different speculative decoding methods is shown in Figure 1. Compared to existing methods, extensive experiments conducted on common models and benchmarks have validated the effectiveness and robustness of our method. For example, we achieve notable advancements over previous stateof-the-art (SOTA) method on the MT-bench for non-greedy mode, as shown in Figure 2. Our main contributions can be described as: Dynamic Decoupling with MoE in Draft Attention Tree Construction: This paper is the first to propose leveraging Mixture of Experts (MoE) to achieve dynamic decoupling during the construction of the draft attention tree. This approach enhances the diversity of draft tokens and improves the efficiency of speculative decoding, regardless of whether static or dynamic tree structure is used. Hybrid Inference Strategy: hybrid inference strategy is introduced, combining autoregressive token-bytoken prediction for initial tokens with parallel decoding for later stages. During the parallel decoding phase, we innovatively incorporate contrastive mechanism in the output features to further enhance performance. Comprehensive Benchmark Evaluation: We conduct comprehensive experiments across various benchmarks and target models to demonstrate the effectiveness and robustness of our approach. Our approach achieves SOTA performance in speculative decoding."
        },
        {
            "title": "Jakiro",
            "content": "2.3. Eagle-style Decoding 2. Preliminaries 2.1. Speculative Decoding Speculative decoding (Leviathan et al., 2023) is designed to accelerate inference from autoregressive LLMs without altering the output distribution. smaller, computationally efficient draft model is employed to generate candidate tokens in the drafting phase, while larger, high-quality target model verifies and refines these candidates. The key idea behind speculative decoding is to parallelize the token generation process, allowing multiple tokens to be proposed at once. This is achieved by observing that complex languagemodeling tasks often contain subtasks that can be approximated by smaller models with adequate accuracy. By using these approximations, speculative decoding achieves significant speedup while maintaining the statistical properties of the target models output. Suppose ti denotes the i-th token, and let Tx:y represent the token sequence tx, tx+1, , ty. The speculative decoding process involves three steps: 1. Drafting: Given prefix T1:j, the draft model q(..) autoregressively generates sequence of candidate tokens Tj+1:j+γ, each accompanied by probability distribution qj+1:j+γ, where γ is the number of inference steps per round for the draft model. 2. Verification: The target model p(..) computes the conditional probabilities pj+1:j+γ for the same sequence in single forward pass. Each candidate token tj+i is sequentially evaluated based on an acceptance criterion, such as min(1, pj+i(tj+i)/qj+i(tj+i)). 3. Refinement: Accepted tokens are appended to the output sequence, while rejected tokens are resampled using normalized probability distribution norm(max(0, pj+i qj+i)) to replace tj+i and discards the remaining tokens in the drafting. The method ensures that the output distribution remains consistent with vanilla autoregressive decoding for both greedy and non-greedy sampling strategies, as proven in prior works (Leviathan et al., 2023; Chen et al., 2023). 2.2. Medusa-style Decoding Medusa-style (Cai et al., 2024; Ankner et al., 2024) decoding extends speculative decoding by employing multiple lightweight draft heads, typically implemented as small multi-layer perceptrons (MLPs), atop the last hidden state of the target LLM. These heads independently predict tokens based on the last hidden state of the target LLM, and their predictions are combined into candidate continuations. However, the shared use of the last hidden state across all heads results in incomplete decoupling, limiting the ability of each head to make diverse and independent predictions. In Eagle-style (Li et al., 2024a;b) decoding, the drafting stage decouples draft tokens at different time steps by performing autoregression at the feature level (before the LM head). The LM head of the original LLM is then used to convert features into draft tokens. This approach reduces uncertainty in the feature sequence, resulting in improved draft predictions. However, the top-k tokens within the same layer of the draft tree remain coupled, which limits the diversity and potential of the predictions. Eagle1 adopts static tree-structured draft (Miao et al., 2023) in the verification stage, allowing branching paths to explore alternative continuations when draft tokens are rejected. This structure increases robustness by avoiding the full discard of draft sequences when individual tokens fail to meet the target models criteria. Eagle2 improves upon Eagle1 by introducing dynamically adjustable draft trees. This allows the tree to adapt based on runtime conditions, optimizing token prediction and verification. Despite this enhancement, the lack of intra-layer decoupling in draft trees still limits the diversity of generated tokens, particularly in non-greedy sampling modes. 3. Jakiro 3.1. Dynamic Decoupling and MoE Tree Construction 3.1.1. DYNAMIC DECOUPLING WITH MOE HEADS Unlike Medusa, which relies on the last hidden state for all draft heads, Jakiro employs multiple MoE heads that dynamically allocate expert modules to predict tokens. This mechanism accounts for inherent differences between tokens and ensures the decoupling of token predictions across heads. As result, prediction confidence is improved while maintaining computational efficiency. The structure of our draft model follows the design principles of the Eagle-style, utilizing single decoder layer that includes reduction dimension layer, an LLM attention layer, and parallel expert heads (lightweight MLP layers). And, the embedding layer and head layer remain consistent with the target model without introducing additional parameters. This ensures that inference efficiency is not compromised by the increase in the number of parameters. The detailed structure is shown in Figure 3 (2). 3.1.2. MOE TREE CONSTRUCTION Jakiro introduces novel tree construction method that decouples the intra-layer dependencies of traditional draft trees while retaining the independence between inter-layers. This design ensures that draft tokens across different layers are generated independently, preserving the consistency of the auto-regressive decoding draft model with the target model"
        },
        {
            "title": "Jakiro",
            "content": "experts is (default value is 2), the MoE router and the LLM head layer are shared across different expert layers. At each step, two candidate logits distributions are outputed, which are then processed through the MoE Tree decoding. The logits are sorted based on the scores sj,i corresponding to the selected experts: expert with larger score is placed on the left side of the current layer, while those with smaller score is placed on the right. The formulation is as follows: ui = Attn (hi) + hi, ej sj,i = Softmaxj (cid:0)ui (cid:1) , fi = (cid:88) j=1 (cid:0)gj,i Expertj (ui)(cid:1) + ui, Figure 3. Comparison of different building methods of draft tree. gj,i = logitsleft (cid:40) , logitsright = Head (cid:16) stop1 fi, stop2 fi (cid:17) , sj,i, sj,i Topk({sj,i1 }, K), 0, otherwise, (1) (2) (3) (4) (5) and significantly enhancing the accuracy and factuality of predictions. At the same time, the decoupling of intra-layer tokens enables Jakiro to explore broader range of candidate continuations or diversity, especially in non-greedy modes, without compromising the integrity of the output distribution. Specifically, the original Eagle inference process can be summarized as follows: when new token ti is passed through the embedding layer, token embedding is obtained. This embedding is then concatenated with the hidden states (denoted as feature fi1) from the previous step. The concatenated result is processed through the Reduction layer for dimensionality reduction, and the output hidden state hi subsequently through the LLM decoder layer (including the attention layer Attn and MLP layer) to produce the hidden states fi for the current step. The next token ti+1 is sampled via the Head layer and used for the next step of inference. This process iterates until an end-of-sequence token is encountered or the maximum token length is reached. Figure 4. The building process of tree attention mask mechanism. In contrast, our Jakiro inference process replaces the MLP layer in the LLM with an MoE layer Expertj and modifies the draft tree construction. Assuming the number of candidate experts is and the final number of activated 4 where ej is the centroid of the j-th expert, the gj,i is sparse, indicating that only out of gate values are nonzero. This sparsity property ensures computational efficiency in MoE layer, i.e., each token will be assigned to and computed in only experts. Also, in the above formulations, we omit the layer normalization operation for brevity. This entire process adheres to the top-k construction principle of the Eagle tree, and the corresponding tree attention mask mechanism is illustrated in Figure 4. The specific construction process of MoE tree is illustrated in Figure 3 (4). Its implementation mechanism can be combined with the predefined static tree of Eagle1 or the dynamically constructed tree of Eagle2. Experimental results demonstrate that both approaches lead to significant improvements in inference efficiency. Impact on Non-Greedy Modes While traditional speculative decoding methods struggle with diversity in non-greedy sampling, Jakiro excels by introducing mechanisms that maximize both diversity and prediction confidence. This makes it particularly effective in scenarios where alternative continuations need to be explored comprehensively. 3.2. Efficient Integration of Contrastive Mechanism Although Jakiro significantly enhances performance in nongreedy modes through its decoupled tree structure, it does not show substantial advantage in strictly top-1 accuracy tasks, such as HumanEval and GSM8K, which are optimized for greedy decoding. However, by incorporating the Medusa parallel decoding mechanism, we reduce the number of forward passes during inference without compromising the final speedup, enabling Jakiro to achieve performance on par with Eagle2. Moreover, LLMs are prone to hallucinations (Tonmoy et al.,"
        },
        {
            "title": "Jakiro",
            "content": "2024; Liu et al., 2024), where they generate content that does not align with the facts seen during pretraining. Due to the limited capabilities of the draft model itself, this issue is further exacerbated, making it difficult to further optimize under greedy decoding (i.e., strict top-1 verification). Although Eagle-style has improved the performance of the draft model by introducing feature distillation, due to the limitations on model parameters, it is hard to gain further improvements through additional training techniques. However, we explore that the currently popular contrastive decoding technique achieves significant out-of-the-box improvements compared to greedy decoding, without requiring additional training. Previous works (Chuang et al., 2023; OBrien & Lewis, 2023) have shown that contrastive decoding improves generation quality by searching for strings that maximize the weighted likelihood difference between the strong and weak models. Unlike their use of two highly divergent models in capability, we only apply the contrastive mechanism to the output hidden states, but not to the logits, of the two highest-scoring experts in the MoE layer. This approach further improves the generation quality of draft tokens in greedy mode, bringing them closer to the top-1 token of the target model and improving inference speed. The schematic diagram of the mechanism is shown in Figure 5. Below is the detailed derivation of its formulas: top2 = stop1 moe const , logitsconst top1 + stop2 = βf top1 αf top2 , = Head (cid:0)f moe i , logitsmoe , const (cid:1) , (6) (7) (8) where the logitsmoe indicates that the inference is based on the logits output from the previous [1 : γ 1] steps for autoregression decoding, while logitsconst is used for parallel decoding at step γ 1, resulting in one fewer draft process compared to the Eagle-style method. Additionally, β and α are learnable parameters corresponding to the scores of the candidate experts being top1 and top2, respectively, and are used to adaptively adjust the difference between them for better contrastive learning. Compared to the non-greedy mode of Jakiro, no additional parameters are introduced, meaning the same weights are used. Additionally, it should be noted that our Jakiro is not limited to parallel decoding for the last two steps. When > 2, it theoretically supports parallel decoding for the last steps. However, this is not the focus of the current study and will be explored in future work. 3.3. Training of the Draft Models Similar to Eagle-style, in order to reduce training costs, we use preprocessed fixed dataset for training the draft models. Other data augmentation hyperparameters are kept consistent with Eagle. Additionally, we adopt the Smooth L1 loss 5 Figure 5. Efficient integration of contrastive mechanism. for predicting the next feature as regression task and use cross-entropy to ensure the accuracy of tokens sequence. However, considering that the entire process of the draft model is autoregressive and computationally expensive, we use parallel decoding mechanism during the penultimate step. Thus, the prediction of the next token is achieved through contrast between the two expert heads of the MoE, introducing an additional loss (similar to the Medusa implementation). The optimize objective is as follows: reg = Smooth L1(f Lmoe reg = Smooth L1(f Lconst qi+2, qi+3 = Softmax(logitsmoe i+1, moe ), i+2, const ), , logitsconst Lmoe cls = Cross Entropy(pi+2, qi+2), Lconst cls = Cross Entropy(pi+3, qi+3), reg + wmoe cls + Lconst reg + wconst cls Lmoe cls Lconst cls ), = Lmoe (9) (10) (11) (12) (13) (14) We use combined loss function to train the autoregressive draft model. Given that the classification loss is an order of magnitude larger than the regression loss numerically, and the importance of const is lower compared to moe, we set wmoe to 0.1 and 0.05, respectively. cls and wconst cls 4. Experiments Models and tasks. Following the current mainstream works, we conduct experiments on Vicuna models (7B, 13B, 33B) (Chiang et al., 2023), LLaMA2-chat models (7B, 13B, 70B) (Touvron et al., 2023), and LLaMA3-Instruct (8B, 70B) (Meta, 2024), encompassing the common sizes of current mainstream LLMs. To evaluate the generality and robustness of our method, our Jakiro is evaluated across multiple tasks including multi-turn dialogue, code generation, mathematical reasoning, and instruction following, employing the MT-bench (Zheng et al., 2023), HumanEval (Chen et al., 2021), GSM8K (Cobbe et al., 2021), Alpaca (Taori et al., 2023), CNN/Daily Mail (Nallapati et al., 2016), and Natural Questions (Kwiatkowski et al., 2019) datasets, re-"
        },
        {
            "title": "Jakiro",
            "content": "spectively. Because Speculative decoding (Leviathan et al., 2023) conduct experiments with batch size of 1, setting subsequently adopted by other works such as Medusastyle (Cai et al., 2024; Ankner et al., 2024) and Eaglestyle (Li et al., 2024a;b). Similarly, the majority of our experiments also adopt this setting. Metrics. Similar to prior speculative sampling approaches, our Jakiro method primarily focuses on reducing latency rather than optimizing throughput. To measure its acceleration effects, we utilize the following metrics: Walltime speedup ratio: Compared to traditional autoregressive decoding, the ratio of speedup achieved in actual tests of end-to-end. Average acceptance length τ : The average number of tokens accepted from speculative decoding per forward pass of the target LLM. Similar to methods that use strict speculative sampling mechanism, the acceleration achieved by Jakiro ensures that the output distribution of the target LLMs is maintained. Therefore, evaluating the quality of the generated results is both unnecessary and irrelevant, as the focus is on efficiency rather than output fidelity. Training. We keep the target LLMs fixed throughout the training process. Our proposed Jakiro model is trained on the ShareGPT1 dataset using 68,000 dialogue iterations without needing extensive retraining or additional data beyond the pre-trained models, and with learning rate set at 9e-5. The AdamW optimizer is employed with beta values (β1, β2) = (0.9, 0.95), and gradient clipping is applied with threshold of 0.5. The number of trainable parameters for Jakiro varies across model sizes: 0.35B for the 7B model, 0.56B for the 8B model, 0.88B for the 13B model, 1.42B for the 33B model, and 1.87B for the 70B model. For example, training the Jakiro draft model for the 70B model takes approximately 2-3 days on 4A100 40GB GPU. Testing. Our experiments are conducted on different devices (AMD InstinctTM MI250-64G, NVIDIA A40-45G, and NVIDIA A100-40G) and the limitation of hardware GPU memory. For models of size (7B, 8B, 13B), (33B), and (77B), we perform single-GPU, two GPUs, and four GPUs, respectively. 4.1. Effectiveness As shown in Table 1 and Table 2, our proposed Jakiro consistently outperforms existing methods in terms of speedup ratios across various datasets and models on MI250. The speedup ratios achieved by Jakiro, such as 2.99x on the MT-bench and 3.43x on the HumanEval task for the Vicuna 7B model under greedy mode, significantly surpass 1We can download ShareGPT from Huggingface at link. those of other methods, including Eagle2, which achieved maximum speedup of 2.88x on average. The results across different tasks further highlight the effectiveness of Jakiro. For example, on the GSM8K task, Jakiro achieves 3.11x speedup, maintaining high average acceptance length of 4.95 tokens. This demonstrates that Jakiro not only accelerates inference but also retains high-quality output, maintaining the target LLMs performance without compromising accuracy. Since our Jakiro involves one less drafting step than Eagle2, there is no significant improvement in the average acceptance length of tokens. However, considering that the ultimate goal of speculative decoding is to improve speedup, our method is still effective. For instance, in the greedy mode for LLaMA2-Chat 7B, although the average acceptance length of our method is slightly lower than that of Eagle2 (4.51 vs. 4.63), the final speedup of our method is notably improved (2.83x vs. 2.66x). Furthermore, we observe that Jakiro excels across both code generation and natural language tasks. This is particularly notable because code generation tasks often benefit from greedy sampling, where the inherent structure of code makes it easier to predict subsequent tokens efficiently. Similarly, on Natural Questions and summarization tasks, Jakiro maintains superior performance. Additionally, in non-greedy mode, the comparison methods with speculative sampling also reveal that Jakiro achieves significantly higher speedup ratios while also maintaining higher average acceptance length. Such as Vicuna 7B, compared to Eagle2, our method achieves 15.4% improvement in the final average speedup across all benchmark test sets (2.84 vs. 2.46). This suggests that Jakiro benefits from more efficient drafting process that allows for longer and more stable sequences of tokens to be accepted, reducing the need for frequent re-sampling and minimizing the risk of errors during the inference process. 4.2. Ablation Study N-k Setting of MoE. The number of candidate experts and activated experts involved in the computation for each token in typical MoE architecture is 8 and 2, respectively. If speculative decoding uses the original chain-based construction of draft tokens, the number of experts activated each time is fixed and equal to 2. However, current mainstream speculative decoding methods use tree structure to construct draft tokens. Although the number of candidate experts activated for each token is fixed, the process of constructing the tree results in the top-k candidate tokens being selected at each layer, which leads to number of experts activated 2 during each inference. Additionally, the more candidate experts there are, the better the models predictive ability, but with an increased computational cost."
        },
        {
            "title": "Jakiro",
            "content": "Table 1. Speedup ratios and average acceptance lengths τ of different methods on MI250. represents Vicuna, L2 represents LLaMA2Chat, and L3 represents LLaMA3-Instruct. SpS denotes standard speculative sampling, with its draft model being Vicuna-68M. Methods like Medusa relax acceptance conditions under non-greedy settings, which do not guarantee lossless acceleration. Therefore, we do not compare Jakiro with these methods. Where the symbol * indicates that our method uses MoE-based weighted decoupling to construct the draft tree, as shown in Figure 3(2). Without the symbol *, it refers to the construction of the draft tree using weighted non-decoupling combined with the contrastive mechanism, as shown in Figure 5. The results on other devices can be found in the appendix. GSM8K Natural Ques. HumanEval MT-bench CNN/DM Alpaca Mean Model Method Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ SpS Medusa Hydra Eagle1 Eagle2 Jakiro Eagle2 Jakiro Eagle2 Jakiro Eagle2 Jakiro SpS Eagle1 Eagle2 Jakiro Jakiro* Eagle2 Jakiro Jakiro* Eagle2 Jakiro Eagle2 Jakiro Eagle2 Jakiro Eagle2 Jakiro Jakiro* Eagle2 Jakiro Eagle2 Jakiro 7B L2 7B 13B L2 13B 7B L2 7B 13B L2 13B 33B L2 70B L3 8B L3 70B 1.82x 1.91x 2.69x 2.52x 2.75x 2.99x 2.70x 2.89x 3.02x 3.18x 3.06x 3.22x 1.50x 2.18x 2.39x 2.50x 3.18x 2.61x 2.68x 2.93x 2.72x 2.84x 2.90x 3.06x 3.04x 3.16x 2.96x 3.08x 3.68x 2.31x 2.58x 3.11x 3.26x 2.36 2.52 3.60 3.97 4.94 4.96 4.70 4. 4.84 4.74 4.74 4.72 1.87 3.59 4.26 4.22 5.65 4.48 4.45 5.16 4.30 4.27 4.58 4. 4.22 4.31 4.48 4.56 5.16 3.97 4.66 4.98 5.04 1.99x 2.02x 2.98x 2.82x 3.12x 3.43x 3.12x 3.32x 3.51x 3.70x 3.62x 3.76x 1.55x 2.37x 2.64x 2.92x 2.48x 2.86x 3.04x 2.62x 3.09x 3.31x 3.45x 3.60x 3.52x 3.64x 3.49x 3.62x 3.18x 2.96x 3.02x 2.83x 2.89x 2.61 2.67 3.79 4.30 5.35 5.36 5.38 5. 5.42 5.36 5.53 5.41 1.95 3.82 4.67 4.70 4.52 5.04 4.94 4.73 4.78 4.95 5.37 5. 4.78 4.84 5.18 5.25 4.37 4.83 4.91 4.35 4.42 Temperature=0 2.26 2.59 3.66 4.01 4.94 4. 4.76 4.66 4.82 4.81 4.88 4.96 1.65x 1.79x 2.66x 2.37x 2.63x 2.87x 2.67x 2.81x 2.95x 3.03x 2.96x 3.18x Temperature=1 1.82 3.56 4.49 4.44 5.50 4.72 4.61 4.59 4.52 4.50 4.73 4. 4.55 4.63 4.59 4.61 4.71 4.36 4.44 4.67 4.77 1.56x 2.12x 2.28x 2.48x 2.99x 2.48x 2.64x 3.04x 2.63x 2.73x 2.86x 3.01x 2.87x 2.98x 2.97x 3.05x 3.60x 2.52x 2.64x 2.10x 2.46x 1.71x 1.89x 2.73x 2.58x 2.81x 3.11x 2.78x 2.97x 3.11x 3.30x 3.19x 3.41x 1.53x 2.16x 2.37x 2.60x 2.91x 2.67x 2.82x 2.70x 2.85x 3.00x 3.04x 3.26x 3.41x 3.52x 3.09x 3.18x 3.43x 2.65x 2.70x 2.73x 2.91x 2.21 2.48 3.58 3.87 4.85 4.82 4.65 4.47 4.89 4.68 4.62 4.67 1.85 3.72 4.28 4.28 5.67 4.37 4.32 5. 4.33 4.31 4.44 4.43 3.99 4.02 4.49 4.47 4.90 4.50 4.53 3.64 3. 1.81x 1.42x 2.01x 2.15x 2.25x 2.50x 2.27x 2.42x 2.60x 2.69x 2.66x 2.83x 1.63x 1.88x 2.01x 2.16x 2.70x 2.14x 2.27x 2.99x 2.38x 2.45x 2.56x 2.71x 2.56x 2.68x 2.34x 2.53x 3.55x 1.99x 2.06x 2.34x 2.35x 2.44 2.02 2.70 3.43 4.11 4.20 4.09 4.01 4.28 4.20 4.24 4.32 1.91 3.10 3.77 3.77 5.33 3.92 3.87 5. 3.94 3.88 4.21 4.15 3.71 3.85 3.64 3.87 5.18 3.52 3.59 3.52 3. 1.60x 1.51x 2.25x 2.05x 2.17x 2.38x 2.41x 2.56x 2.37x 2.52x 2.68x 2.87x 1.33x 1.78x 1.95x 2.08x 2.80x 2.30x 2.44x 2.73x 2.22x 2.36x 2.62x 2.83x 2.40x 2.52x 2.54x 2.65x 3.56x 2.05x 2.32x 2.30x 2.32x 2.16 2.09 2.86 3.22 3.84 3.84 4.16 4.07 3.69 3.70 4.12 4.17 1.72 2.91 3.54 3.49 5.05 4.06 3.98 4. 3.57 3.54 4.04 4.13 3.27 3.38 3.84 3.90 4.86 3.36 3.95 3.42 3. 1.76x 1.76x 2.55x 2.42x 2.62x 2.88x 2.66x 2.83x 2.93x 3.07x 3.03x 3.21x 1.52x 2.08x 2.27x 2.46x 2.84x 2.51x 2.65x 2.84x 2.65x 2.78x 2.90x 3.08x 2.97x 3.08x 2.90x 3.02x 3.50x 2.41x 2.55x 2.57x 2.70x 2.34 2.40 3.37 3.80 4.67 4.69 4.63 4.51 4.66 4.58 4.69 4.71 1.85 3.45 4.17 4.15 5.32 4.43 4.36 4. 4.24 4.24 4.56 4.53 4.09 4.17 4.37 4.44 4.86 4.09 4.35 4.10 4. Table 2. Speedup ratios and average acceptance lengths τ with other models as the original LLMs, with the temperature set to 0. MT-bench HumanEval GSM8K Table 3. Ablation experiment results with temperature set to 0 on Vicuna 7B under the MI250. indicates the candidate experts, and indicates the experts activated each time. Model Method Speedup τ Speedup τ Speedup τ MT-bench HumanEval GSM8K Alpaca LLaMA3-Instruct 8B Vicuna 33B LLaMA2-Chat 70B LLaMA3-Instruct 70B Eagle1 Eagle2 Jakiro Eagle1 Eagle2 Jakiro Eagle1 Eagle2 Jakiro Eagle1 Eagle2 Jakiro 1.84x 2.63x 2.74x 2.87x 3.21x 3.34x 2.83x 2.98x 3.01x 2.43x 2.58x 2.65x 3.07 4.32 4.23 3.69 4.43 4. 3.84 4.51 4.46 3.33 4.14 4.06 2.27x 3.08x 3.25x 3.41x 3.89x 3.96x 3.33x 3.54x 3.58x 3.01x 3.10x 3.28x 3.72 5.06 5.01 4.28 5.20 5.16 4.44 5.25 5.19 4.15 5.09 5.03 2.25x 2.79x 2.92x 3.16x 3.52x 3.67x 2.95x 3.10x 3.16x 2.53x 2.82x 2.97x 3.70 4.46 4.38 3.93 4.66 4.67 3.92 4.63 4.58 3.87 4.34 4. Therefore, it is necessary to choose an appropriate N-K setting to achieve optimal inference speed. The results under different N-K settings are shown in Table 3. Speedup τ Speedup τ Speedup τ Speedup τ 5 4 3 2 2 2 2 2 2.67x 2.69x 2.75x 2.92x 5.13 5.09 5.09 4.97 3.03x 3.07x 3.13x 3.36x 5.59 5.54 5.54 5.45 2.71x 2.75x 2.79x 3.01x 5.14 5.12 5.04 4.97 2.56x 2.57x 2.61x 2.83x 5.10 5.04 5.04 4.98 From the above results, it can be seen that as the number of candidate experts increases, both the average acceptance length and the computational load increase accordingly. However, we believe that combining hardware and algorithmic optimizations can mitigate these costs, further unlocking the potential of MoE, which remains an area for future exploration."
        },
        {
            "title": "Jakiro",
            "content": "Combine with Parallel Decoding and Contrastive Mechanism. The contrastive mechanism (perhaps similar to form of contrastive learning) and its implementation combined with parallel decoding is discussed in Section 3.2. This approach aims to reduce one draft inference step compared to conventional autoregressive decoding LLMs (e.g., Eagle2) without significant loss in prediction performance, thereby achieving optimal speedup. However, the essence of these two mechanisms is different. The contrastive mechanism focuses on better predicting the next token in the sequence, while parallel decoding aims to output multiple future tokens in one step. Therefore, it is necessary to conduct further experiments to evaluate their respective impacts on final performance, with the results shown in Table 4. As can be seen from the above results, using parallel decoding or the contrastive mechanism alone does lead to some improvement compared to using MoE alone, which demonstrates that the two proposed mechanisms can effectively complement Jakiro. However, combining both mechanisms further unleashes the potential of Jakiro, maintaining comparable acceptance rate (similar to average accpet length) while further improving its speedup. Table 4. Ablation experiment results with temperature set to 0 on Vicuna 7B under the MI250. PD refers to whether the penultimate inference in the draft process uses the parallel decoding mechanism. Const indicates whether contrastive mechanism on features is applied in the penultimate inference of the draft process. MT-bench HumanEval GSM8K Alpaca Method Speedup τ Speedup τ Speedup τ Speedup τ w/o both w/o PD w/o Const Jakiro 2.92x 2.98x 2.98x 2.99x 4.97 5.04 4.86 4.96 3.36x 3.41x 3.42x 3.43x 5.45 5.50 5.33 5.36 3.01x 3.10x 3.05x 3.11x 4.97 5.11 4.86 4.95 2.83x 2.83x 2.86x 2.87x 4.98 4.92 4.82 4.82 5. Related Work Speculative decoding: Speculative decoding (SD)has emerged as powerful technique to accelerate LLMs inference by reducing memory bandwidth bottlenecks. Early speculative decoding methods, such as those by (Stern et al., 2018) and (Sun et al., 2021), focused on greedy decoding strategies, while (Leviathan et al., 2023) and (Chen et al., 2023) expanded speculative sampling to non-greedy decoding. Recent works in SD have enhanced draft model efficiency, with methods like SpecInfer (Miao et al., 2023) employing tree attention to verify multiple draft tokens in parallel, and Medusa (Cai et al., 2024) using extra MLP heads to generate token drafts. Although these methods have achieved notable acceleration, they face limitations in token diversity and decoupling between draft and target models. Eagle (Li et al., 2024a) introduces more dynamic approach by decoupling the draft tokens across different time steps. However, Eagle still maintains coupling between the Top-k tokens at the same layer in the draft tree, which restricts the diversity and specialization of tokens. Our approach builds upon these limitations by introducing dynamic decoupling mechanism with Mixture of Experts (MoE) heads, which enables draft tokens to consider inherent differences between them, leading to more diversity and confident predictions. Mixture of Experts: MoE has been explored extensively for improving the specialization of models. MoE techniques were originally proposed by (Jacobs et al., 1991; Jordan & Jacobs, 1994) and later adapted to language models, such as in GShard (Lepikhin et al., 2021) and Switch Transformer (Fedus et al., 2021), which scale MoE to large models with top-k routing strategies. More recently, the integration of MoE in Transformer-based architectures has garnered significant interest, with methods like StableMoE (Dai et al., 2022) exploring fixed routing strategies for more stable training. MoE heads have also been used in multi-modal settings (Du et al., 2022; Xue et al., 2023), where they enable specialization across different modalities. Furthermore, in the context of speculative decoding, our method combines MoEs dual-branch heads with contrastive decoding techniques, strategy inspired by recent works (Shazeer et al., 2017; Dai et al., 2022) to improve the usefulness of draft token predictions, especially in greedy modes. By integrating these strategies, we can achieve more reliable predictions with faster inference, as demonstrated through our experiments. Parallel decoding: Parallel decoding is known for its efficiency in machine translation (Ghazvininejad et al., 2019) and code generation (Gloeckle et al., 2024), has also been integrated into SD frameworks to further enhance efficiency. Although the use of parallel decoding in speculative frameworks has been under-explored, works like (Monea et al., 2023) and (Yi et al., 2024) have pioneered its application. These methods, however, still face challenges in achieving perfect alignment between draft distributions and target models, which can limit their effectiveness in lossless acceleration. Our approach addresses these challenges by improving the decoupling mechanism within the MoE heads, ensuring better alignment and more diverse token predictions in both greedy and non-greedy modes. 6. Conclusion In summary, while speculative decoding has proven effective for accelerating LLM inference, challenges remain in improving token diversity and maintaining distribution alignment between draft and target models. Our work makes significant strides in addressing these challenges by introducing dynamic decoupling mechanism with MoE heads, which improves prediction confidence, diversity, and factuality. Additionally, by combining contrastive mechanism with MoEs dual-branch heads, we further enhance the utility of our predictions, demonstrating the potential of this ap-"
        },
        {
            "title": "Jakiro",
            "content": "proach in both greedy and non-greedy generation tasks. Our proposed Jakiro demonstrates superior speedup ratios, high acceptance lengths, and overall efficiency across variety of tasks and model sizes, making it compelling solution for improving inference speed without sacrificing model output quality."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Ankner, Z., Parthasarathy, R., Nrusimha, A., Rinard, C., Ragan-Kelley, J., and Brandon, W. Hydra: SequentiallyarXiv dependent draft heads for medusa decoding. preprint arXiv:2402.05109, 2024. Arora, K., Asri, L. E., Bahuleyan, H., and Cheung, J. C. K. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. arXiv preprint arXiv:2204.01171, 2022. Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D., and Dao, T. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/ 2023-03-30-vicuna/. Dai, D., Dong, L., Ma, S., Zheng, B., Sui, Z., Chang, B., and Wei, F. Stablemoe: Stable routing strategy for mixture of experts. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 70857095. Association for Computational Linguistics, 2022. doi: 10.18653/ V1/2022.ACL-LONG.489. URL https://doi.org/ 10.18653/v1/2022.acl-long.489. Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., Zoph, B., Fedus, L., Bosma, M. P., Zhou, Z., Wang, T., Wang, Y. E., Webster, K., Pellat, M., Robinson, K., Meier-Hellstern, K. S., Duke, T., Dixon, L., Zhang, K., Le, Q. V., Wu, Y., Chen, Z., and Cui, C. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 55475569. PMLR, 2022. URL https://proceedings.mlr. press/v162/du22c.html. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https://arxiv.org/abs/2101.03961. Ghazvininejad, M., Levy, O., Liu, Y., and Zettlemoyer, L. Mask-predict: Parallel decoding of conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 61126121, Hong Kong, China, November 2019. Association for Computational Linguistics. Gloeckle, F., Idrissi, B. Y., Rozi`ere, B., Lopez-Paz, D., and Synnaeve, G. Better & faster large language models via multi-token prediction. arXiv preprint arXiv: 2404.19737, 2024. Chuang, Y.-S., Xie, Y., Luo, H., Kim, Y., Glass, J., and He, P. Dola: Decoding by contrasting layers improves arXiv preprint factuality in large language models. arXiv:2309.03883, 2023. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural Computing, 3(1):7987, 1991. URL https://doi.org/ 10.1162/neco.1991.3.1.79."
        },
        {
            "title": "Jakiro",
            "content": "Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of experts and the EM algorithm. Neural Computing, 6(2):181214, 1994. URL https://doi.org/10. 1162/neco.1994.6.2.181. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and auIn 9th International Conference on tomatic sharding. Learning Representations, ICLR 2021. OpenReview.net, 2021. URL https://openreview.net/forum? id=qrwe7XHTmYb. Leviathan, Y., Kalman, M., and Matias, Y. Fast inference In Interfrom transformers via speculative decoding. national Conference on Machine Learning, pp. 19274 19286. PMLR, 2023. Li, X. L., Holtzman, A., Fried, D., Liang, P., Eisner, J., Hashimoto, T., Zettlemoyer, L., and Lewis, M. Contrastive decoding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097, 2022. Li, Y., Wei, F., Zhang, C., and Zhang, H. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024a. Li, Y., Wei, F., Zhang, C., and Zhang, H. Eagle-2: Faster inference of language models with dynamic draft trees. arXiv preprint arXiv:2406.16858, 2024b. Liu, F., Liu, Y., Shi, L., Huang, H., Wang, R., Yang, Z., Zhang, L., Li, Z., and Ma, Y. Exploring and evaluating hallucinations in llm-powered code generation. arXiv preprint arXiv:2404.00971, 2024. Meta. LLaMA3. https://github.com/ pytorch-labs/gpt-fast/, 2024. Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Zhang, Z., Wong, R. Y. Y., Zhu, A., Yang, L., Shi, X., et al. Specinfer: Accelerating generative large language model serving with tree-based speculative inference and verification. arXiv preprint arXiv:2305.09781, 2023. Monea, G., Joulin, A., and Grave, E. Pass: Parallel speculative sampling. arXiv preprint arXiv:2311.13581, 2023. Nallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023, 2016. OBrien, S. and Lewis, M. Contrastive decoding improves reasoning in large language models. arXiv preprint arXiv:2309.09117, 2023. Shazeer, N. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G. E., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017. OpenReview.net, 2017. URL https: //openreview.net/forum?id=B1ckMDqlg. Shi, C., Yang, C., Zhu, X., Wang, J., Wu, T., Li, S., Cai, D., Yang, Y., and Meng, Y. Unchosen experts can contribute too: Unleashing moe models power by self-contrast. arXiv preprint arXiv:2405.14507, 2024. Stern, M., Shazeer, N., and Uszkoreit, J. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018. Sun, X., Ge, T., Wei, F., and Wang, H. Instantaneous grammatical error correction with shallow aggressive decoding. arXiv preprint arXiv:2106.04970, 2021. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama https://github.com/tatsu-lab/ model. stanford_alpaca, 2023. Tonmoy, S., Zaman, S., Jain, V., Rani, A., Rawte, V., Chadha, A., and Das, A. comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313, 2024. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models (2023). arXiv preprint arXiv:2302.13971, 2023."
        },
        {
            "title": "Jakiro",
            "content": "Xia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T., Li, W., and Sui, Z. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024. Xue, F., Zheng, Z., Fu, Y., Ni, J., Zheng, Z., Zhou, W., and You, Y. Openmoe: Open mixture-ofexperts language models. https://github.com/ XueFuzhao/OpenMoE, 2023. Yi, H., Lin, F., Li, H., Peiyang, N., Yu, X., and Xiao, R. Generation meets verification: Accelerating large language model inference with smart parallel auto-correct decoding. In Findings of the Association for Computational Linguistics ACL 2024, pp. 52855299, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. 11 A. Supplementary experiments A.1. Average Acceptance Length."
        },
        {
            "title": "Jakiro",
            "content": "Figure 6. Average acceptance length of Vicuna, LLaMA2-chat, and LLaMA3-instruct models inference on the MT-bench for non-greedy (Temperature=1) settings. The above reproduction results are based on the open-source code from the original paper and are averaged over four inference runs on an A100-40G GPU. In this paper, we only compare with speculative sampling based methods that do not need to finetune the backbone models, ensuring the output text distribution remains constant. From Figure 6, it can be observed that our Jakiro achieves the highest acceptance rate (approximately average acceptance length) across different models, further demonstrating the robustness of our method. Under the advantage of the MoE mechanism, it fully utilizes the diversity prediction of draft tokens in the non-greedy mode, resulting in an increased number of accepted tokens and thus improving the average length accepted by the target model during the validation phase. A.2. Results of Other Devices. Figure 7. Speedup ratios of Vicuna 7B inference on all tasks under different devices. Where the suffix T0 represents greedy mode sampling, while suffix T1 represents non-greedy sampling. The above reproduction results are based on the open-source code from the original paper and are averaged over four inference runs. In this paper, we only compare with speculative sampling based methods that do not need to finetune the backbone models, ensuring the output text distribution remains constant. As shown Figure 7, our proposed Jakiro achieves consistent improvements across different devices, indicating that our method is robust and has the capability of performance transferability across devices. Results in A40-45G:"
        },
        {
            "title": "Jakiro",
            "content": "Table 5. Speedup ratios and average acceptance lengths τ of different methods. represents Vicuna, L2 represents LLaMA2-Chat. SpS denotes standard speculative sampling, with its draft model being Vicuna-68M. Methods like Medusa relax acceptance conditions under non-greedy settings, which do not guarantee lossless acceleration. Therefore, we do not compare Jakiro with these methods. Where the symbol * indicates that our method uses MoE-based weighted decoupling to construct the draft tree, as shown in Figure 3(2). Without the symbol *, it refers to the construction of the draft tree using weighted non-decoupling combined with the contrastive mechanism, as shown in Figure 5. MT-bench HumanEval GSM8K Alpaca CNN/DM Natural Ques. Mean Model Method Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ 7B L2 7B 13B L2 13B 7B L2 7B 13B L2 13B Eagle1 Eagle2 Jakiro Eagle1 Eagle2 Jakiro Eagle1 Eagle2 Jakiro Eagle1 Eagle2 Jakiro Eagle1 Eagle2 Jakiro Jakiro* Eagle1 Eagle2 Jakiro Jakiro* Eagle1 Eagle2 Jakiro Jakiro* Eagle1 Eagle2 Jakiro Jakiro* 2.69x 2.92x 3.02x 2.62x 2.81x 2.87x 2.90x 2.97x 3.11x 2.89x 2.96x 3.11x 2.23x 2.46x 2.54x 3.24x 2.24x 2.63x 2.66x 2.97x 2.51x 2.68x 2.78x 3.04x 2.59x 2.82x 2.97x 3.14x 3.98 4.98 4. 3.82 4.71 4.61 4.00 4.83 4.76 3.93 4.74 4.73 3.54 4.29 4.15 5.64 3.59 4.53 4.45 5.13 3.66 4.38 4.23 4. 3.69 4.61 4.56 4.72 2.98x 3.24x 3.40x 2.94x 3.23x 3.29x 3.28x 3.45x 3.61x 3.39x 3.52x 3.62x 2.41x 2.72x 2.95x 2.67x 2.55x 2.97x 2.99x 2.70x 2.84x 3.03x 3.28x 3.00x 3.03x 3.37x 3.53x 2.75x 4.29 5.34 5.37 4.30 5.39 5.25 4.39 5.41 5. 4.52 5.52 5.42 3.82 4.63 4.72 4.12 4.08 5.04 4.93 4.68 4.02 4.86 4.89 4.73 4.23 5.40 5.29 4.03 Temperature= 4.00 4.95 4.97 3.90 4.76 4.64 3.97 4.79 4.81 4.03 4.90 4.94 2.56x 2.76x 2.86x 2.53x 2.79x 2.81x 2.74x 2.89x 2.96x 2.81x 2.88x 3.07x Temperature=1 3.66 4.41 4.48 5.76 3.80 4.76 4.59 4.69 3.74 4.46 4.40 5. 3.81 4.79 4.79 4.90 2.23x 2.41x 2.54x 2.97x 2.21x 2.63x 2.66x 3.06x 2.49x 2.53x 2.70x 2.61x 2.54x 2.75x 2.95x 2.79x 2.74x 2.94x 3.08x 2.66x 2.86x 2.93x 2.94x 3.03x 3.22x 2.99x 3.09x 3.30x 2.23x 2.47x 2.63x 2.84x 2.39x 2.79x 2.80x 2.80x 2.58x 2.69x 2.90x 2.90x 2.69x 2.97x 3.19x 3.33x 3.87 4.87 4.82 3.70 4.66 4.47 3.95 4.89 4.69 3.83 4.60 4.65 3.63 4.47 4.47 5. 3.51 4.53 4.40 5.19 3.77 4.48 4.46 4.77 3.64 4.52 4.52 4.11 2.21x 2.28x 2.38x 2.24x 2.31x 2.32x 2.45x 2.46x 2.49x 2.54x 2.53x 2.62x 1.91x 2.09x 2.19x 2.84x 1.96x 2.18x 2.19x 2.97x 2.22x 2.24x 2.35x 2.04x 2.34x 2.44x 2.55x 2.91x 3.42 4.12 4. 3.41 4.12 4.01 3.52 4.22 4.13 3.59 4.25 4.30 3.13 3.82 3.89 5.59 3.18 3.93 3.93 5.46 3.32 3.91 3.90 3. 3.45 4.20 4.22 4.50 2.17x 2.26x 2.36x 2.34x 2.51x 2.56x 2.28x 2.33x 2.45x 2.58x 2.60x 2.79x 1.86x 2.03x 2.12x 2.85x 2.04x 2.34x 2.37x 2.80x 2.11x 2.15x 2.31x 2.44x 2.37x 2.54x 2.76x 2.77x 3.21 3.81 3.83 3.44 4.19 4.07 3.11 3.74 3. 3.47 4.11 4.16 2.95 3.50 3.51 5.04 3.24 3.99 3.89 4.85 3.03 3.53 3.55 3.88 3.34 4.08 4.15 4.10 2.56x 2.73x 2.85x 2.55x 2.75x 2.80x 2.77x 2.86x 2.97x 2.87x 2.93x 3.08x 2.15x 2.36x 2.50x 2.90x 2.23x 2.59x 2.61x 2.88x 2.46x 2.55x 2.72x 2.67x 2.59x 2.81x 2.99x 2.95x 3.80 4.68 4.69 3.76 4.64 4.51 3.82 4.65 4.57 3.89 4.69 4.70 3.45 4.19 4.20 5. 3.56 4.46 4.37 5.00 3.59 4.27 4.24 4.50 3.69 4.60 4.59 4.39 Results in A100-40G:"
        },
        {
            "title": "Jakiro",
            "content": "Table 6. Speedup ratios and average acceptance lengths τ of different methods. represents Vicuna, L2 represents LLaMA2-Chat. SpS denotes standard speculative sampling, with its draft model being Vicuna-68M. Methods like Medusa relax acceptance conditions under non-greedy settings, which do not guarantee lossless acceleration. Therefore, we do not compare Jakiro with these methods. Where the symbol * indicates that our method uses MoE-based weighted decoupling to construct the draft tree, as shown in Figure 3(2). Without the symbol *, it refers to the construction of the draft tree using weighted non-decoupling combined with the contrastive mechanism, as shown in Figure 5. MT-bench HumanEval GSM8K Alpaca CNN/DM Natural Ques. Mean Model Method Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ 7B L2 7B 13B L2 13B 7B L2 7B 13B L2 13B Eagle1 Eagle2 Jakiro Eagle1 Eagle2 Jakiro Eagle1 Eagle2 Jakiro Eagle1 Eagle2 Jakiro Eagle1 Eagle2 Jakiro Jakiro* Eagle1 Eagle2 Jakiro Jakiro* Eagle1 Eagle2 Jakiro* Eagle1 Eagle2 Jakiro* 2.84x 3.29x 3.34x 2.94x 3.20x 3.15x 2.94x 2.96x 3.05x 2.97x 3.00x 3.03x 2.40x 2.69x 2.66x 3.86x 2.52x 2.95x 2.91x 3.36x 2.57x 2.62x 3.00x 2.73x 2.75x 3.15x 3.98 4.98 5. 3.82 4.70 4.61 4.00 4.83 4.74 3.93 4.75 4.67 3.55 4.27 4.31 5.68 3.64 4.49 4.43 5.17 3.60 4.31 4. 3.67 4.58 4.67 3.23x 3.71x 3.81x 3.24x 3.63x 3.60x 3.34x 3.47x 3.55x 3.39x 3.57x 3.56x 2.75x 3.11x 2.96x 2.97x 2.79x 3.39x 3.17x 3.03x 2.71x 3.02x 2.92x 3.08x 3.31x 2.93x 4.30 5.35 5.48 4.30 5.38 5.24 4.39 5.42 5. 4.52 5.52 5.42 3.85 4.66 4.57 4.88 4.05 5.04 4.87 4.68 3.92 4.80 4.76 4.27 5.33 4.10 Temperature= 3.99 4.94 4.98 3.91 4.77 4.65 3.98 4.79 4.81 4.02 4.89 4.79 2.75x 2.97x 3.06x 2.81x 3.12x 3.06x 2.96x 2.94x 2.95x 3.00x 2.94x 2.96x Temperature=1 3.67 4.45 4.49 5.89 3.77 4.69 4.57 4.65 3.78 4.46 4. 3.80 4.68 4.94 2.27x 2.60x 2.50x 3.35x 2.47x 2.94x 2.76x 3.31x 2.35x 2.61x 2.51x 2.52x 2.78x 2.83x 3.18x 3.15x 3.22x 2.98x 3.21x 3.20x 2.93x 3.07x 3.23x 3.20x 3.14x 3.16x 2.35x 2.68x 2.69x 3.32x 2.56x 3.05x 2.90x 3.02x 2.64x 2.75x 2.93x 2.68x 2.93x 3.43x 3.87 4.86 4.97 3.68 4.66 4.48 3.95 4.90 4.70 3.83 4.60 4.51 3.62 4.37 4.30 5. 3.58 4.56 4.38 5.23 3.73 4.54 4.84 3.66 4.56 4.14 2.48x 2.62x 2.68x 2.58x 2.68x 2.62x 2.55x 2.45x 2.56x 2.65x 2.54x 2.55x 2.15x 2.32x 2.20x 3.29x 2.15x 2.42x 2.32x 3.38x 2.14x 2.28x 1.96x 2.22x 2.41x 2.88x 3.42 4.11 4. 3.41 4.10 4.01 3.52 4.21 4.17 3.58 4.26 4.16 3.10 3.84 3.85 5.51 3.15 3.94 3.89 5.36 3.30 4.00 4. 3.37 4.16 4.42 2.42x 2.46x 2.50x 2.67x 2.77x 2.70x 2.35x 2.34x 2.43x 2.61x 2.67x 2.67x 2.04x 2.12x 2.09x 3.46x 2.30x 2.48x 2.37x 3.30x 2.01x 2.17x 2.53x 2.42x 2.52x 2.83x 3.21 3.82 3.86 3.43 4.16 4.07 3.10 3.71 3. 3.46 4.12 4.04 2.93 3.49 3.52 5.14 3.23 4.06 3.98 4.86 3.01 3.49 4.03 3.36 4.04 4.16 2.82x 3.03x 3.10x 2.87x 3.10x 3.05x 2.85x 2.87x 2.96x 2.97x 2.98x 2.99x 2.33x 2.59x 2.52x 3.38x 2.46x 2.87x 2.74x 3.23x 2.40x 2.57x 2.64x 2.61x 2.78x 3.01x 3.80 4.68 4.74 3.76 4.63 4.51 3.82 4.64 4.57 3.89 4.69 4.60 3.45 4.18 4.17 5. 3.57 4.46 4.35 4.99 3.56 4.27 4.54 3.69 4.56 4."
        }
    ],
    "affiliations": [
        "Advanced Micro Devices, Inc., Beijing, China",
        "Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xian"
    ]
}