{
    "paper_title": "Grounding Computer Use Agents on Human Demonstrations",
    "authors": [
        "Aarash Feizi",
        "Shravan Nayak",
        "Xiangru Jian",
        "Kevin Qinghong Lin",
        "Kaixin Li",
        "Rabiul Awal",
        "Xing Han Lù",
        "Johan Obando-Ceron",
        "Juan A. Rodriguez",
        "Nicolas Chapados",
        "David Vazquez",
        "Adriana Romero-Soriano",
        "Reihaneh Rabbany",
        "Perouz Taslakian",
        "Christopher Pal",
        "Spandana Gella",
        "Sai Rajeswar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GroundCUA, a large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for a total of over 3.56M human-verified annotations. From these demonstrations, we generate diverse instructions that capture a wide range of real-world tasks, providing high-quality data for model training. Using GroundCUA, we develop the GroundNext family of models that map instructions to their target UI elements. At both 3B and 7B scales, GroundNext achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GroundNext attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of high-quality, expert-driven datasets in advancing general-purpose computer-use agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 2 3 3 7 0 . 1 1 5 2 : r a"
        },
        {
            "title": "DEMONSTRATIONS",
            "content": "Aarash Feizi1,2,4* Shravan Nayak1,3* Xiangru Jian5 Kevin Qinghong Lin6 Kaixin Li7 Rabiul Awal1,3,4 Xing Han `u1,2 Nicolas Chapados4 David Vazquez4 Adriana Romero-Soriano 1,2 Reihaneh Rabbany 1,2 Perouz Taslakian 1,2,4 Christopher Pal1,2,4,8 Spandana Gella4 Sai Rajeswar1,3,4 Johan Obando-Ceron1,3 Juan A. Rodriguez1,9 1Mila - Quebec AI Institute 4ServiceNow Research 7National University of Singapore 9 Ecole de Technologie Superieure 2McGill University 5University of Waterloo 3Universite de Montreal 6University of Oxford 8Polytechnique Montreal 10CIFAR AI Chair Project Page: https://groundcua.github.io"
        },
        {
            "title": "ABSTRACT",
            "content": "Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GROUNDCUA, large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for total of over 3.56M humanverified annotations. From these demonstrations, we generate diverse instructions that capture wide range of real-world tasks, providing high-quality data for model training. Using GROUNDCUA, we develop the GROUNDNEXT family of models that map instructions to their target UI elements. At both 3B and 7B scales, GROUNDNEXT achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GROUNDNEXT attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of highquality, expert-driven datasets in advancing general-purpose computer-use agents."
        },
        {
            "title": "INTRODUCTION",
            "content": "The vision of computer-use agents (CUA) that operate software on behalf of users has gained significant momentum with recent progress in multimodal large language modelbased agents (OpenAI, 2025; Anthropic, 2024a; Qin et al., 2025; Wang et al., 2025a). These agents promise to automate routine work and make complex digital tools more accessible. For such agents to succeed, they must first plan the next step in task, then ground the plan to the exact on-screen element to click, type, or drag. Accurate grounding is critical: without correctly identifying the right button or menu item, even flawless plan cannot be executed. In FreeCAD, for instance, when asked to open the color picker (Figure 1), the agent must distinguish small palette icon from look-alike tools, one of which it must precisely click. When grounding fails, the plan quickly veers off course, minor errors compound, and tasks ultimately fail (Nayak et al., 2025). Moreover, grounding in desktop applications is challenging due to their complexity and diversity. These applications often feature high-resolution displays with dense layouts and visually similar elements, making precise localization difficult. Additionally, desktop applications can contain user-specific artifacts (e.g., documents or spreadsheets) that may not have been seen during training, adding variability and unseen contexts. Finally, collecting automated datasets for desktop environments with strong coverage is also challenging, as highlighted by recent datasets (Gou et al., 2024b; Wu et al., 2024; Xie et al., 2025). * Equal contribution. Equal supervision."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of the GROUNDCUA dataset and GROUNDNEXT models. Human demonstrations of computer-use tasks are recorded as screenshots (example from FreeCAD) with UI metadata, which are processed into high-quality natural language instruction tasks for UI grounding. GROUNDNEXT is trained in two stages: SFT (700K samples) followed by RL (10K samples), achieving state-of-the-art grounding performance with efficient training. To this end, we introduce GROUNDCUA, large-scale, human-annotated dataset for desktop grounding. The dataset spans 87 applications across 12 categories, with 56K screenshots and 3.56M+ element annotations. These annotations are collected from task demonstrations by trained annotators, ensuring high-quality and densely labeled data that provides rich context for effective model training. It also reflects the pixel diversity of desktops, with resolutions ranging from 500K to 7M pixels and substantial proportion of very small bounding boxes, highlighting the fine-grained challenges agents must overcome. Furthermore, GROUNDCUA includes fine-grained category information (menus, buttons, etc.) for 50% of the UI elements and includes multiple variants of related applications (e.g., LibreOffice and OnlyOffice), directly addressing the difficulty of similar yet distinct applications and enabling agents to learn robust, application-specific grounding strategies. Key highlights of GROUNDCUA compared to other datasets are: Scale: 56K annotated screenshots and 3.56 million elements; Resolution, Element Size, and Density: High-resolution images with maximum annotation density, covering almost every visible element, including small elements like icons and controls; Expert Quality: Human-verified annotations for high accuracy; Application Diversity: 87 desktop applications for broad real-world coverage. Using this dataset, we construct 700K image-instruction pair instruction-tuning set that mimics real-world semantic interactions. We introduce the GROUNDNEXT series of vision-language models, designed for precise grounding across desktop applications. The series includes models at 3B and 7B scales, offering balance between efficiency and accuracy. Each model is trained in two stages: first, supervised fine-tuning (SFT) on 700K curated datapoints from GROUNDCUA, and second, reinforcement learning (RL) to further refine performance. This approach enables GROUNDNEXT to achieve state-of-the-art results on key desktop benchmarks, including ScreenSpotPro (Li et al., 2025), OSWorld-G (Xie et al., 2025), and UI-Vision (Nayak et al., 2025). Despite using significantly fewer SFT datapoints than state-of-the-art models like JEDI (which are trained on 9M datapoints), GROUNDNEXT outperforms existing models, demonstrating its efficiency in training and proving that high-quality, well-curated data can outperform larger, less precise datasets. In the RL stage, GROUNDNEXT further refines its grounding accuracy, achieving significant improvements without relying on complex reward strategies, unlike many RL-tuned models, which typically incorporate specialized reward functions and additional objectives. This shows the effectiveness of combining supervised fine-tuning (SFT) with high-quality data. This efficiency is further highlighted in agentic, multi-step tasks; evaluated on the OSWorld-Verified benchmark, GROUNDNEXT-3B not only significantly outperforms its 3B peers but also surpasses many larger models, including OpenCUA-72B and proprietary APIs. Notably, our 3B model achieves performance comparable to the much larger JEDI-7B, demonstrating significant practical utility for real-world, resource-constrained systems. Additionally, GROUNDNEXT excels in cross-platform generalization, delivering strong performance across desktop, mobile, and web environments; even though we only train on desktop dataset. Evaluated on benchmarks like MMBench-GUI (L2) and ScreenSpot-v2, in addition to desktop-specific tasks, GROUNDNEXT showcases its ability to generalize across wide range of user interfaces and platforms."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Comparison of grounding datasets. Columns: = human-provided instructions and labels; Desk = includes desktop data; / Desk-E / = number of elements, desktop elements, and screenshots; Res Range = screenshot resolution range (MP); EleArea = average element area (% of screenshot); #AvgE = average elements per screen; Perm = permissive OSI-style license (e.g., Apache-2.0, MIT), ? = not clearly reported. Datasets marked with are grounding-specific versions constructed from the OS-ATLAS (Wu et al., 2024). Grounding Datasets Annotation Scale Avg. Data Stats Perm? Desk UGround (Gou et al., 2024b) JEDI (Xie et al., 2025) AGUVIS-G (Xu et al., 2024) OS-ATLAS (Wu et al., 2024) RICOSCA*(Li et al., 2020a) UIBert*(Bai et al., 2021) Widget Caption*(Li et al., 2020b) AMEX*(Chai et al., 2025) SeeClick (Cheng et al., 2024) Fineweb*(Penedo et al., 2024) GROUNDCUA(ours) Desk-E Res Range EleArea #AvgE 9M 773k (0.4, 1.9) 4M 2.4M 575k (0.9, 2.1) 3.8 452k (0.5, 2.1) 14.5M 1.2M 1.85M (0.5, 5.2) 18K (0.5, 2.1) 170K 57K (0.5, 2.1) 166K 101K 14K (0.5, 2.1) 1.2M 101K (0.9, 4.5) 3M 270K (2.1, 2.1) 9.9M 1.4M (2.1, 2.1) 3.56M 3.56M 55k (0.4, 7.0) 11.6 7.0 8.5 7.8 0.53% 9.4 0.28% 2.9 0.24% 7.0 4.2% 2.1% 11.8 0.33% 11.2 0.29% 6.9 0.13% 64.1 ? ? ? ? ? In summary, our contributions are as follows: We introduce GROUNDCUA, large-scale, human-annotated desktop grounding dataset with over 3.56 million annotations across 56K screenshots from 87 applications in 12 categories, providing dense, high-resolution, and fine-grained supervision for robust computer-use agents. We present the GROUNDNEXT series, vision-language models at 3B and 7B scales, trained on GROUNDCUA with SFT and RL, achieving state-of-the-art performance across desktop benchmarks and multi-step agentic tasks, with significantly fewer datapoints than prior models. We provide comprehensive analysis of SFT and RL roles, evaluate our datasets cross-domain impact and generalization beyond desktop, and study the benefits of open-source software for grounding performance. We release both GROUNDCUA and the GROUNDNEXT models to support open research."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Computer-Use Agents. Recent advancements in computer-use agents have focused on enhancing their ability to understand and interact with user interfaces, ranging from simple commands to complex, multi-step tasks. Supervised fine-tuned models such as CogAgent (Hong et al., 2023), ShowUI (Lin et al., 2024), and Ferret-UI (You et al., 2024) have improved interaction capabilities by enabling zero-shot instruction-following across desktop, web, and mobile interfaces, combining vision, language, and action. Benchmarks like ScreenSpot-Pro (Li et al., 2025) and UI-Vision (Nayak et al., 2025) have emphasized the challenges of grounding natural language instructions in highresolution desktop environments, particularly with dense screens and small elements. Groundingfocused agents, such as OS-ATLAS (Wu et al., 2024), UGround (Gou et al., 2024b), and JEDI (Xie et al., 2025), have made significant progress by scaling training data to map language to specific UI elements. However, these methods often face challenges with data efficiency, particularly in complex desktop environments. Furthermore, recent RL-based approaches, inspired by DeepSeek-R1 (Guo et al., 2025), such as GUI-R1 (Luo et al., 2025), GUI-G2 (Tang et al., 2025), and InfiGUI-G1 (Liu et al., 2025a), have addressed grounding through both simplistic and complex distance-based reward approaches. Despite these advancements, reliably grounding instructions to the correct on-screen elements remains persistent bottleneck. To address this, we focus on high-quality, expert-annotated data to enhance grounding through both SFT and RL training, while prioritizing data-efficient finetuning to improve model performance. GUI Grounding Datasets. Training datasets for GUI grounding span mobile, web, and desktop platforms. Mobile datasets like RICO (Deka et al., 2017), UIBert (Bai et al., 2021), and AMEX (Chai et al., 2025) provide element-level supervision within standardized layouts, sim-"
        },
        {
            "title": "Preprint",
            "content": "(a) GIMP (b) FreeCAD (c) LibreOffice Calc Figure 2: Examples of screenshots from different applications in GROUNDCUA. Red bounding boxes indicate the annotated UI elements within each screenshot. plifying extraction but limiting exposure to desktop-style density and iconography. Web-focused datasets, including SeeClick (Cheng et al., 2024) and UGround (Gou et al., 2024b), scale grounding through automated harvesting from HTML/DOM, while Aguvis-G (Xu et al., 2024) broadens coverage across platforms. However, these automated pipelines likely overemphasize text-bearing elements while underrepresenting small icon-only controls, which are standard in desktop software. Desktop resources remain limited and challenging. OS-ATLAS (Wu et al., 2024) assembles desktop splits via accessibility-tree traversal, yet accessibility signals are often incomplete or inconsistent, leading to missing or imprecise element labels (Muryn et al., 2025; Gou et al., 2024b). JEDI (Xie et al., 2025) achieves scale through synthetic interface generation, but these simplified screens underrepresent genuine desktop complexity. How is ours different? GROUNDCUA is the largest expert-annotated dataset for desktop grounding, comprising 55,568 screenshots across 87 open-source applications with over 3.56M humanverified UI elements. Compared to existing datasets, GROUNDCUA features denser screens, wider resolution range, and smaller average element areas (see Table 1). It uniquely captures small desktop components, such as icons, toolbars, and controls, that are difficult to capture using automated tools. Its high-resolution images (ranging from 0.39 to 7.0M pixels) are substantially higher than other datasets and are the only ones to include very high-resolution images (see Figure 5, left). Additionally, GROUNDCUA features dense annotations that support semanticsand context-aware instructions, averaging 64 per screenshot, more than three times that of OS-Atlas (Desktop) and much higher than Aguvis-G (9) or UGround (11). Together, these properties make GROUNDCUA comprehensive and challenging dataset for training robust desktop grounding agents."
        },
        {
            "title": "3 GROUNDCUA DATASET",
            "content": "This section introduces GROUNDCUA, the largest and most diverse desktop-specific dataset annotated by human experts. We provide an overview of the data collection pipeline and annotation process, along with our high-quality fine-tuning data below. Collecting demonstrations from human experts We record real-world interactions of expert users performing tasks with desktop applications and annotated interface elements at scale. This approach captures user-driven interactions, resulting in more realistic distribution of screenshots that better reflects real-world usage, compared to prior work that often relies on depth-first or breadthfirst search to generate random interface states (Wu et al., 2024). Our pipeline consists of three main steps: selecting diverse applications, designing and executing practical tasks, and annotating screenshots. We partnered with data labeling company for this process, with details on the annotator pool and training in Section A.2. Selecting diverse desktop applications To support general-purpose computer-use agents, we selected 87 open-source applications across 12 categories  (Table 5)  . Most applications are drawn from UI-Vision (Nayak et al., 2025), with four additional ones covering finance and scientific applications. By focusing on open-source applications with permissive licenses, we ensure the dataset can be freely released while encompassing wide range of domains. These applications mirror the functionality of popular closed-source software (e.g., LibreOffice vs. Microsoft Office), making the dataset broadly applicable. Further details are provided in Section A.1."
        },
        {
            "title": "Preprint",
            "content": "Designing and executing computer-use tasks. We asked annotators to design everyday computer-use tasks that reflect common goals (e.g., drafting document, editing spreadsheet, running simulation) and then carry them out. This approach produces natural interaction trajectories, unlike random clicking, and yields screenshots that closely mirror real-world usage. In total, annotators completed over 10, 000 task demonstrations across 87 applications 1. Dense annotation of screenshots. From the recorded demonstrations, we extracted keyframes that capture the state of the interface immediately before user action (e.g., mouse click or text entry) that would trigger change in the application. Annotators labeled every visible element in each keyframe using bounding boxes. For each element, they provided textual label. This label was the elements name when available, the displayed text for shorter strings, or concise summary in the case of long passages such as source code or detailed descriptions. We also extracted OCR using PaddleOCR (Cui et al., 2025) to extract raw text specifically for these longer segments. In addition, around 50% of the elements were assigned to one of eight high-level categories(see Table 6). In total, this process produced over 3.56 million annotated elements, making GROUNDCUA the largest and most diverse human-annotated grounding dataset for desktop environments to date. Examples of the annotations are provided in Appendix A.5, and further details of the annotation process are described in Appendix A.2. Constructing high-quality finetuning instructions User queries in real-world settings can take various forms, from explicit references to UI elements (e.g., Click Save), to functional commands (e.g., Open new tab), or spatial descriptions (e.g., Select the icon left of Files). To handle this diversity, we design pipeline that leverages our dense annotations, which include bounding boxes, labels, categories, and OCR text, to construct diverse instruction-tuning data. These annotations enable the generation of highly contextual instructions, grounded directly in annotated screenshots. Unlike prior works that rely on pretrained models, our approach involves prompting multimodal LLM with annotated bounding boxes, application names, element labels, and surrounding context. This ensures that the instructions are tightly linked to both the visual and textual content, making them semantically and contextually relevant. By leveraging nearly every visible element on the screen, we are able to create UI context-aware and challenging instructions. We generate three primary types of instructions: Direct, which describe an elements attributes, position, and surrounding context (e.g., Click the magnifying-glass icon next to the search bar for visual elements or Click the button that has the text Save for OCR-based textual elements); Functional, which focus on the intended action of an element (e.g., Open new tab instead of Click the + button); and Spatial, which guide the model based on the relative positioning of elements (e.g., Click the element to the left of Files or Select the icon between Undo and Redo). We describe these instruction types in more detail in Section and provide examples in Section B.4. These diverse instruction types, grounded in both visual and semantic context, provide comprehensive foundation for training more effective and context-aware GUI agents. Dataset Statistics GROUNDCUA consists of 56K screenshots, totaling 3.56 million annotated elements. On average, each screenshot contains 64 annotations, with some images having as many as 542. The images have mean resolution of 2.03 megapixels, with range from 0.39 to 7 megapixels. Bounding boxes are relatively small, covering just 0.13% of the image area on average, underscoring the fine-grained nature of the annotations. This results in high-quality fine-tuning data, with 700K samples for SFT and 10K for RL, extracted from the densely annotated screenshots and metadata. Detailed distribution plots of resolution, bounding box sizes, and category-level statistics for both screenshots and annotations are provided in Appendix A.3."
        },
        {
            "title": "4 TRAINING GROUNDNEXT MODELS ON GROUNDCUA",
            "content": "4.1 MODEL TRAINING We use Qwen2.5-VL-Instruct as the base model for all experiments, considering both the 3B and 7B parameter variants. We finetune both the vision encoder and the language model, as preliminary experiments indicated that this leads to better grounding performance. 1We will release both the tasks and videos as part of the dataset."
        },
        {
            "title": "Preprint",
            "content": "SFT We first train the models with standard supervised finetuning. Training is performed on single node with 8 H100 GPUs, using global batch size of 128. Additional hyperparameter details are provided in Appendix C.2. For training data, we use the instruction tuning dataset introduced in Section 3. From this dataset, we use subset of 700k instructions that balances coverage and diversity. This choice keeps the experiments practical and reproducible, while still being large enough to demonstrate the effectiveness of our dataset for grounding tasks. Further details on the composition of this subset, along with the choices made in its construction, are provided in Appendix C.1. RL Post-training. In the next stage, we adopted RL post-training and explored several heuristics for constructing training data. GROUNDCUA allows us to sample from much larger pool than the one used for SFT, so we selected 10K new elements not included in the original 700K SFT training set. This approach yielded the strongest generalization across benchmarks in our initial experiments, and we adopted it for the final model. For policy optimization, we employed the Relative Leave-One-Out (RLOO) method (Ahmadian et al., 2024), which compares the reward of each rollout to the average reward of other samples within the same group, avoiding the need for training separate critic model. Concretely, for group of rollouts {y1, . . . , yn}, the gradient is given by: θJ(πθ) = 1 (cid:88) (cid:16) i=1 R(yi, x) 1 1 (cid:88) j=i (cid:17) R(yj, x) .θ log πθ(yix), where R(yi, x) is the reward assigned to output yi given the input x. In our grounding setup, each yi corresponds to sequence of tokens representing the predicted coordinates (ˆpi) on the image and corresponds to the input prompt and image. Reward Function. We designed customized discrete reward based on the normalized distance Rscore(ˆp, B, I) = 1.0 if Dnorm < 0.5, 0.5 if 0.5 Dnorm < 0.1, 0.1 if 0.1 Dnorm < 0, 0.1 0.5 1.0 if 0 Dnorm < 0.1, if 0.1 Dnorm < 0.5, if Dnorm 0.5, with Dnorm = D( ˆp,B) Dmax(B,I) , where D(p, B) is the distance between the predicted coordinate ˆp and the ground-truth bounding box B, and Dmax(B, I) is the maximum possible distance within the image to bounding box B. We normalize the distance to obtain Dnorm, ensuring value between -1 and 1 if the predicted point lies within the image. This discrete scheme captures dominant error modes: predictions just outside the box receive milder penalty, while predictions far outside receive stronger one, and predictions inside the box are encouraged to move toward the center. We exclude reward model-based approaches due to the unreliable nature of current judges (Feizi et al., 2025; L`u et al., 2025). We experimented with alternative reward formulations (e.g., continuous and binary schemes), but ultimately adopted this discrete variant due to its superior empirical performance (see Appendix C.4 for details). We set the group size to = 8, the batch size to 64, and trained for one epoch on single H100 node (8 GPUs), consistent with the SFT setup. 4.2 EVALUATION Task Definition. Given screenshot and user instruction x, the model predicts 2D point ˆp = (ˆu, ˆv) in image coordinates. Let denote the axis-aligned ground-truth bounding box for the target element. prediction is marked correct if ˆp B, and incorrect otherwise. We report the accuracy metric. Benchmarks. We evaluate GROUNDNEXT on five key benchmarks that cover wide range of grounding scenarios. For desktop applications, we use ScreenspotPro (Li et al., 2025), OSWorld-G (Xie et al., 2025), and UI-Vision (Nayak et al., 2025), which focus on desktop interactions. To test"
        },
        {
            "title": "Preprint",
            "content": "Table 2: SFT-only results on five challenging benchmarks. Results are shown for both 3B and 7B model scales. Only top-performing models are presented here; see Section for full comparisons with additional baselines. Our GROUNDNEXT (SFT) consistently achieves the best average performance across all benchmarks, demonstrating the effectiveness of our high-quality data. Model 3B SSPro OSW-G MMB-GUI SSv2 UI-V Avg Qwen2.5-VL-3B (Bai et al., 2025) Qwen2.5-VL-3B (Agent mode) PhiGround-4B-7C (Zhang et al., 2025) JEDI-3B (Xie et al., 2025) GUI-Actor-3B (Wu et al., 2025) GROUNDNEXT-3B (SFT) 7B Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-7B (Agent mode) OS-Atlas-7B (Wu et al., 2024) UGround-V1-7B (Gou et al., 2024b) Aguvis-7B (Xu et al., 2024) GUI-Actor-7B (Wu et al., 2025) JEDI-7B (Xie et al., 2025) GROUNDNEXT-7B (SFT) 16.1 29.0 22.8 36.1 42.2 48.6 26.8 29.7 18.9 16.5 39.5 44.6 39.5 50.2 27.3 37.4 51.4 50.9 48.9 62.2 31.4 42.7 27.7 36.4 38.7 47.0 54.1 67. 60.8 60.8 60.3 66.5 69.8 75.5 33.9 67.7 41.4 65.7 45.7 70.9 70.4 80.4 80.9 81.8 80.8 88.6 91.0 87.3 88.8 86.4 85.1 87.6 86.0 92.1 91.7 89.3 6.3 6.3 20.5 18.7 19.7 58.2 0.9 16.5 9.0 12.9 13.7 21.9 24.8 58. 38.3 43.1 47.2 52.2 54.3 66.4 36.4 48.6 36.4 43.8 44.7 55.3 56.1 69.2 cross-platform performance, we also use MMBench-GUI (L2) (Wang et al., 2025b) and Screenspotv2 (Cheng et al., 2024), which include mobile and web splits in addition to desktop. This mix of benchmarks lets us evaluate performance across desktops, mobile, and web environments. Since UI-Vision overlaps with our dataset in platform coverage, we treat it as an in-domain benchmark, while the others are out-of-domain. We make efforts to minimize overlap during training, but due to annotation differences and the repetitive nature of desktop software, perfect separation isnt always possible. Baselines. We compare GROUNDNEXT against two main types of baselines. First, we evaluate GROUNDNEXT (SFT) alongside several SFT-only variants to measure the impact of our instruction data (see Table 2). Then, we compare GROUNDNEXT (RL) with recent reinforcement learningbased models to assess the effectiveness of RL fine-tuning (see Table 3)."
        },
        {
            "title": "5 RESULTS",
            "content": "5.1 EFFICIENT SUPERVISED FINE-TUNING WITH HIGH-QUALITY DATA We present the performance results of our models trained using SFT across five benchmarks in Table 2. Our models achieve the highest average performance for both 3B and 7B model sizes. For 3B, GROUNDNEXT-3B (SFT) ranks first on most datasets (except SSv2) and leads the SFT-only group by clear margin with an average performance of 68.4 vs. 63.0 for the next best (GUI-Actor3B) without considering UI-V, and 66.4 vs. 54.3 with UI-V (i.e., +5.4 and +12.1 points, respectively). Notably, our 3B SFT average also surpasses all RL-tuned 3B baselines. Adding the RL stage yields small, consistent lift to 68.4 Avg / 70.0 (w/o UI-V), setting the best overall results in this size range. For 7B, GROUNDNEXT-7B (SFT) also leads among SFT-only models with 71.8 Avg without UIV and 69.2 with, outperforming the next best SFT baseline (JEDI-7B) by +7.9 and +13.1 points, respectively. Among RL-tuned systems, GROUNDNEXT-7B (RL) attains the top Avg (w/o UI-V) = 73.0. Overall, these results indicate the efficacy of our high quality data. Notably, our results are achieved with substantially less data and modest compute. We train on only 700K instructions, which is far below multi-millionsample corpora used by prior work (e.g., JEDI 9M). Yet, we outperform larger SFT baselines and remain competitive with RL-tuned systems. This suggests that high-quality, densely grounded supervision and targeted instruction design can substitute for raw scale, delivering strong gains without escalating data volume or compute."
        },
        {
            "title": "Preprint",
            "content": "Table 3: RL-tuned results. We present results for the 3B and 7B model scales. We highlight the top-performing models here and refer readers to Section for full comparisons with additional baselines. Our GROUNDNEXT (RL) achieves the highest average performance. Model 3B SSPro OSW-G MMB-GUI SSv2 UI-V Avg UI-R1-E-3B (Lu et al., 2025) SE-GUI-3B (Yuan et al., 2025) InfiGUI-R1-3B (Liu et al., 2025a) GUI G2-3B (Tang et al., 2025) GUI-G1-3B (Zhou et al., 2025) InfiGUI-G1-3B (Liu et al., 2025b) GROUNDNEXT-3B (SFT) GROUNDNEXT-3B (RL) 7B SE-GUI-7B (Yuan et al., 2025) UI-TARS-1.5-7B (Qin et al., 2025) GUI G2-7B (Tang et al., 2025) InfiGUI-G1-7B (Liu et al., 2025b) GTA1-7B (Yang et al., 2025) GROUNDNEXT-7B (SFT) GROUNDNEXT-7B (RL) 17.8 35.9 35.7 36.4 37.1 45.2 48.6 49.8 47.3 49.6 47.5 51.9 50.1 50.2 52.9 48.8 46.1 42.9 53.5 49.5 49.6 62.2 64.2 33.9 64.2 61.9 59.9 67.7 67.2 67. 68.4 66.3 70.6 66.3 71.0 73.4 75.5 77.1 34.5 64.3 79.5 80.8 79.4 80.4 81.1 88.6 86.8 89.5 87.6 89.5 91.1 87.3 88.8 68.9 90.3 93.3 93.5 92.4 89.3 90.4 16.5 15.0 17.8 18.7 20.3 22.0 58.2 62.1 16.7 20.8 25.6 26.1 25.7 58.7 60. 48.0 50.0 51.3 52.5 53.5 56.3 66.4 68.4 40.3 57.8 61.7 62.4 63.1 69.2 70.5 GROUNDCUA compared to other SFT training corpora To make fair comparison and highlight the quality of our dataset, we train the same base model (Qwen2.5-VL-3B-Instruct) on 100K samples from each of the following datasets: Aguvis, UGround, OS-Atlas (Desktop), JEDI, and GROUNDCUA. We use identical hyperparameters and preprocessing for all experiments (details in the Appendix). Figure 3 (yellow bars) summarizes the average performance across benchmarks, excluding UI-Vision. We observe that GROUNDCUA yields significantly higher SFT averages than all other training sources, demonstrating the benefits of its high-quality, densely grounded supervision. 5.2 REINFORCEMENT LEARNING POST-TRAINING RL post-training on top of the SFT models results in consistent but modest improvements for both the 3B and 7B models shown in Table 3. For the 3B model, GROUNDNEXT-3B (RL) achieves an average of 70.0 (without UI-V) and 68.4 overall, surpassing the SFT-only model, GROUNDNEXT-3B (SFT), which scores 68.4 and 66.4, respectively. For the 7B model, GROUNDNEXT-7B (RL) achieves 70.5, improving upon GROUNDNEXT-7B (SFT)s 69.2 (with UI-V). These results suggest that SFT, when trained with high-quality data, captures the majority of the models performance, with RL offering targeted fine-tuning that provides incremental improvements. In practice, high-quality SFT can establish strong baselines, and RL can serve as an optional refinement step to further enhance performance. While our reward design is simple, we acknowledge that more sophisticated reward functions, such as those in Liu et al. (2025b), could lead to more substantial RL gains, which we leave for future work. Figure 3: Mean SFT scores (orange) across benchmarks, with RL gains from 10k GROUNDCUA samples shown in blue. Analyzing RL Gains. We investigate the modest gains from RL (see Figure 3). In this setup, we start with SFT models (Qwen2.5-VL-3B-Instruct) trained on Aguvis, UGround, OS-Atlas (Desktop), JEDI, and GROUNDCUA, and then apply RL using 10K examples exclusively from GROUND-"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Agentic performance comparison on OSWorld-Verified. Bold and underline indicate the best-performing open-source model in each category. Our 3B model, GROUNDNEXT-3B, is among the top-performing open-source models, surpassing larger and proprietary models, highlighting its practical utility and efficiency for real-world agentic tasks. Model Proprietary Models OS Office Daily Pro Workflow Overall OpenAI o3 (OpenAI, 2025) CUA (OpenAI, 2025) Claude-4-Sonnet (Anthropic, 2025a) Qwen3-VL-Flash (Bai et al., 2025) UI-TARS-250705 (Qin et al., 2025) Claude-4.5-Sonnet (Anthropic, 2025b) Open-source Models Qwen2.5-VL-32B (Bai et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) Kimi-VL-A3B (Kimi Team, 2025) OpenCUA-A3B (Wang et al., 2025a) UI-TARS-72B-DPO (Qin et al., 2025) OpenCUA-7B (Wang et al., 2025a) UI-TARS-1.5-7B (Qin et al., 2025) OpenCUA-72B (Wang et al., 2025a) JEDI-7B w/ o3 (Xie et al., 2025) GROUNDNEXT-3B w/ o3 (ours) 62.5 23.9 45.8 40.9 41.7 70.8 8.3 16.7 12.5 12.5 37.5 41.7 33.3 58.3 50.0 62.5 14.5 34.6 39.3 53.6 50.4 72. 1.7 4.3 6.0 16.3 19.0 22.2 29.9 47.0 46.1 47.0 21.4 55.1 48.1 55.1 55.7 61.4 6.4 6.4 21.7 21.7 34.6 37.1 37.9 53.8 61.9 38.8 18.3 59.2 22.0 51.0 63.3 6.1 2.0 18.4 46.9 63.3 49.0 53.1 73.5 75. 55.0 73.5 16.5 18.3 27.9 22.0 14.7 49.0 2.2 3.2 1.1 2.2 8.3 9.3 9.1 20.4 35.3 36.5 23.0 31.4 41.4 41.6 41.8 62. 3.9 5.0 10.3 17.7 27.1 27.0 29.6 46.1 51.0 50.6 CUA. We find that models trained with GROUNDCUA during SFT show the smallest performance gains from RL, while models trained on other datasets benefit more from RL fine-tuning with GROUNDCUA. This suggests that SFT with GROUNDCUA already provides highly informative supervision, leaving fewer errors for RL to correct. Moreover, the magnitude of RL improvements correlates with the initial SFT performance: stronger SFT models yield smaller absolute gains because they start with fewer remaining errors. 5.3 FURTHER ANALYSIS Agentic Ability We evaluate GROUNDNEXTs performance in an agentic setting to assess its ability to ground in realistic, multi-step tasks. Experiments are conducted on the OSWorld-Verified benchmark using the o3 planner, which consumes task instructions and action history to generate grounding commands that GROUNDNEXT executes to locate target UI elements on the screen. Following the setup of Xie et al. (2025), Yang et al. (2025) and Wang et al. (2025a), we evaluate 361 tasks (excluding Google Driverelated ones) on an Ubuntu system with 19201080 resolution, running on Microsoft Azure within 10 Docker environments. The results in Table 4 highlight GROUNDNEXT-3Bs strong performance. Within its 3B parameter class, GROUNDNEXT-3B (50.6 Overall) significantly outperforms peers like OpenCUA-A3B (17.7) and Kimi-VL-A3B (10.3). Notably, it surpasses many larger models, including OpenCUA72B (46.1) and proprietary APIs such as Qwen3-VL-Flash (41.6) and Claude-4-Sonnet (41.4). The comparison with JEDI-7B, which also uses the o3 planner, is particularly notable. Despite being less than half the size, our 3B model achieves comparable overall score (50.6 vs. 51.0) and demonstrates superior performance in 3 out of 5 categories (OS, Office, and Workflow). This performance from compact 3B model underscores GROUNDNEXT-3Bs significant practical utility, presenting it as an effective and efficient solution for real-world agentic systems where inference speed and resource constraints are critical factors. Gains from GROUNDCUA. We investigate where GROUNDCUA yields the greatest gains by studying the performance of GROUNDNEXT. Since GROUNDCUA primarily covers desktop soft-"
        },
        {
            "title": "Preprint",
            "content": "ware, we expect the largest gains on desktop benchmarks. Our results confirm this: GROUNDNEXT-7B (RL) achieves the best performance on UI-V, OSW-G, and SSPro. For mixed datasets such as MMBench-GUI, GROUNDNEXT shows 3.66% improvement on desktop platforms over the second-best model, InfiGUI-G1, with notable gains coming from Linux and macOS (see Section D.3). At the element level, the most significant improvements are observed in icon recognition. For example, on SSPro, we outperform most models by an average of 10.7% in icon recognition (see Table 7). This reflects the high density of icons in desktop applications and suggests that the diversity in GROUNDCUA provides richer knowledge, leading to better performance on icons. GROUNDNEXT generalization across domains. Next, we evaluate the generalization ability of GROUNDNEXT, trained primarily on desktop software, to mobile and web interfaces using SSv2 and MMBench-GUI. On MMBench-GUI, GROUNDNEXT-7B (RL) performs competitively across both domains, achieving 89.2% on mobile and 81.9% on web, compared to the next best model, i.e., InfiGUI-G1-7B, at 90.9% and 85.3%, respectively. On SSv2, GROUNDNEXT achieves comparable results on mobile but falls behind on web. detailed error analysis is provided in Section E. These results suggest that while GROUNDCUA enables strong cross-domain generalization, future work could explore augmenting desktop data with web and mobile sources to further enhance performance. Effects of using open source applications. To study the impact of open-source software, we examine SSPro performance across various categories, focusing particularly on icon recognition. Icons often require application-specific knowledge, unlike text, which is more general in nature. As shown in Table 7, GROUNDNEXT achieves the best icon performance in the Office Suite, Development, Creative, Scientific, and CAD categories, and ranks second in OS. The presence of open-source office software, such as LibreOffice, likely contributes to the strong results in the Office Suite category. Similarly, the diversity of open-source development tools and creative software, such as video and image editing programs, results in significant improvements, with our model outperforming the next best model, i.e., InfiGUI-G1-7B, by 15.9% in Development and 8.4% in Creative for icon accuracy. Future work could further analyze the impact of application similarity to determine whether applications more similar to those in our dataset lead to higher performance."
        },
        {
            "title": "6 CONCLUSION & DISCUSSION",
            "content": "We introduced GROUNDCUA, human-annotated desktop grounding dataset spanning 87 applications (56K screenshots, 3.56M+ elements) with dense keyframe labels that reflect real interaction states. From these annotations, we constructed real-world computer-use instruction tasks for grounding. We developed the GROUNDNEXT family of models and following recent trends, trained it first with SFT and then RL on verifiable rewards. Across five challenging benchmarks, GROUNDNEXT achieves state-of-the-art results despite using substantially less SFT training data than many prior works. The key takeaway is that high-quality data drives reliable desktop grounding more effectively than sheer data volume. By releasing both the dataset and other research artifacts, we aim to unlock grounding as core capability, laying the foundation for end-to-end computer-use agents that can perform complex tasks across diverse desktop applications. While this work advances desktop grounding and demonstrates the value of high-quality expert demonstrations, it also opens up new opportunities and raises important questions. First, we train models with limited scale and compute, but the dataset can support variable-sized fine-tuning sets to further scale model performance. Second, our dense annotations should enable the development of precise and expressive reward signals for RL, moving beyond the simplistic one used in this paper. This creates opportunities to systematically study how different reward designs impact grounding accuracy. Third, cross-domain generalization remains key frontier. Desktop environments involve complex, multi-window workflows, whereas mobile and web tasks are lighter and more context-specific. Mixing data across these domains could yield models that operate seamlessly across platforms, though balancing these domains and addressing transfer bottlenecks will require careful study. Finally, GROUNDCUA includes platformand category-level metadata, enabling research on continual learning and adaptation, evaluating how agents adapt to unseen applications and continually improve as new interaction paradigms emerge."
        },
        {
            "title": "REFERENCES",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku, October 2024a. URL https://www.anthropic.com/news/3-5-models-and-computer-use. Anthropic. Introducing claude 4, May 2025a. URL https://www.anthropic.com/news/ claude-4. Anthropic. Introducing claude sonnet 4.5, September 2025b. URL https://www.anthropic. com/news/claude-sonnet-4-5. AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024b. Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, et al. Uibert: Learning generic multimodal representations for ui understanding. arXiv preprint arXiv:2107.13731, 2021. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents, 2025. URL https://arxiv.org/abs/2407.17490. Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yiming Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. URL https://arxiv.org/abs/2412.05271. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. Paddleocr 3.0 technical report, 2025. URL https://arxiv.org/abs/2507.05595. Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pp. 845854, 2017. Aarash Feizi, Sai Rajeswar, Adriana Romero-Soriano, Reihaneh Rabbany, Valentina Zantedeschi, Spandana Gella, and Joao Monteiro. Pairbench: Are vision-language models reliable at comparing what they see?, 2025. URL https://arxiv.org/abs/2502.15210."
        },
        {
            "title": "Preprint",
            "content": "Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/ 2507.06261. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024a. URL https://arxiv.org/abs/2410.05243. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024b. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023. Kimi Team. Kimi-vl technical report, 2025. URL https://arxiv.org/abs/2504.07491. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use, 2025. Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile ui action sequences. arXiv preprint arXiv:2005.03776, 2020a. Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements. arXiv preprint arXiv:2010.04295, 2020b. Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent, 2024. URL https://arxiv.org/abs/2411.17465. Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025a. Yuhang Liu, Zeyu Liu, Shuanghe Zhu, Pengxiang Li, Congkai Xie, Jiasheng Wang, Xueyu Hu, Xiaotian Han, Jianbo Yuan, Xinyao Wang, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infiguig1: Advancing gui grounding with adaptive exploration policy optimization, 2025b. URL https://arxiv.org/abs/2508.05731. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning, 2025. URL https://arxiv.org/abs/2503.21620. Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. Xing Han L`u, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Stanczak, Peter Shaw, Christopher J. Pal, and Siva Reddy. Agentrewardbench: Evaluating automatic evaluations of web agent trajectories, 2025. URL https: //arxiv.org/abs/2504.08942. Viktor Muryn, Marta Sumyk, Mariya Hirna, Sofiya Garkot, and Maksym Shamrai. Screen2ax: Vision-based approach for automatic macos accessibility generation, 2025. URL https:// arxiv.org/abs/2507.16704."
        },
        {
            "title": "Preprint",
            "content": "Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan A. Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, M. Tamer Ozsu, Aishwarya Agrawal, David Vazquez, Christopher Pal, Perouz Taslakian, Spandana Gella, and Sai Rajeswar. Ui-vision: desktop-centric gui benchmark for visual perception and interaction, 2025. URL https://arxiv.org/abs/2503.15661. OpenAI. Gpt-4 technical report, 2023. OpenAI. Hello gpt-4o, May 2024. URL https://openai.com/index/hello-gpt-4o/. Accessed: 2024-06-06. OpenAI. Introducing openai o3 and o4-mini, April 2025. URL https://openai.com/ index/introducing-o3-and-o4-mini/. OpenAI. Computer-using agent, January 2025. URL https://openai.com/index/ computer-using-agent/. Guilherme Penedo, Hynek Kydlıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. Ui-tars: Pioneering auarXiv preprint arXiv:2501.12326, 2025. URL tomated gui interaction with native agents. https://github.com/bytedance/UI-TARS. Seed Team. Seed1.5-vl technical report, 2025. Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, et al. Gui-g2: Gaussian reward modeling for gui grounding. arXiv preprint arXiv:2507.15846, 2025. Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025a. URL https://opencua.xlang.ai/. Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang Liu, Qingyun Li, Xuan Dong, Zhe Chen, Weiyun Wang, Xiangyu Zhao, Jixuan Chen, Haodong Duan, Tianbao Xie, Chenyu Yang, Shiqian Su, Yue Yu, Yuan Huang, Yiqian Liu, Xiao Zhang, Yanting Zhang, Xiangyu Yue, Weijie Su, Xizhou Zhu, Wei Shen, Jifeng Dai, and Wenhai Wang. Mmbench-gui: Hierarchical multi-platform evaluation framework for gui agents, 2025b. URL https://arxiv.org/abs/2507.19478. Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, Si Qin, Lars Liden, Qingwei Lin, Huan Zhang, Tong Zhang, Jianbing Zhang, Dongmei Zhang, and Jianfeng Gao. Gui-actor: Coordinate-free visual grounding for gui agents, 2025. URL https://arxiv.org/abs/2506.03143. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. Os-atlas: foundation action model for generalist gui agents, 2024. URL https://arxiv.org/abs/2410.23218. Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. arXiv preprint arXiv:2505.13227, 2025. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction, 2024. URL https://arxiv.org/abs/2412.04454."
        },
        {
            "title": "Preprint",
            "content": "Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Caiming Xiong, and Junnan Li. Gta1: Gui test-time scaling agent, 2025. URL https://arxiv.org/abs/2507.05791. Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256, 2024. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms. arXiv preprint arXiv:2404.05719, 2024. Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via selfevolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025. Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, et al. Phi-ground tech report: Advancing perception in gui grounding. arXiv preprint arXiv:2507.23779, 2025. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents, 2025. URL https: //arxiv.org/abs/2505.15810. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479."
        },
        {
            "title": "Table of Contents",
            "content": "A. GROUNDCUA Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Human Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Dataset Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Comparision with Prior Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Dataset Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. Instruction Tuning Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 Direct Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Functional Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Spatial Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1 SFT Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 SFT Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 RL Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 RL Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D. Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1 ScreenSpot-Pro Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 OSWorld-G Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 MMBench-GUI Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 ScreenSpot-v2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 UI-Vision Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. GROUNDNEXT Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F. Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Page 16 16 16 17 17 19 19 19 22 22 22 22 23 24 24 24 25 25 25 25 25 25 25"
        },
        {
            "title": "Preprint",
            "content": "A GROUNDCUA CREATION A.1 PLATFORMS Table 5: Categories of desktop applications and their corresponding applications. Category"
        },
        {
            "title": "Productivity",
            "content": "Graphics and Design Video and Audio Production Communication Entertainment System Utilities Platforms Anki, Zotero, Calibre, OpenBoard, Mendeley Brave, Chromium, Mozilla Firefox, DuckDuckGo VSCode, Atom, Eclipse, NetBeans, PyCharm, IntelliJ IDEA, Brackets, Geany, Bluefish, KDevelop, Komodo Edit, Code::Blocks, Qt Creator, Arduino IDE LibreOffice Calc, LibreOffice Draw, LibreOffice Impress, LibreOffice Writer, draw.io, Joplin, OpenProject, Affine, PDFedit, OnlyOffice Calendar, OnlyOffice Document Editor, OnlyOffice Forms, OnlyOffice PDF Forms, OnlyOffice Presentation, OnlyOffice Spreadsheet, Nextcloud, Gnumeric, Simplenote, WeKan Blender, GIMP, Inkscape, Krita, darktable, FontForge, Scribus, WordPress OpenShot, OBS Studio, Lightworks, Shotcut, Natron, OpenToonz, Audacity, MuseScore Element, Signal, Mastodon, Lemmy, Matrix, Zulip, Jitsi VLC Media Player, Kodi, Emby Ubuntu Terminal, Conky, Bash, 7-Zip, Flameshot, Nemo, gedit Security Bitwarden, Cryptomator Finance and Business Analytics GnuCash, Frappe Books, Metabase Scientific RStudio, Veusz, GNU Octave, GrassGIS, QGIS, FreeCAD, Spyder We select 87 platforms, focusing on open-source software with permissive licenses. These applications span 12 diverse categories, detailed in Table 5. Our selection is motivated by the under-representation of such platforms in existing datasets and the flexibility provided by permissive licensing, which enables dataset release with minimal restrictions. We primarily rely on UIVision (Nayak et al., 2025) as the source for platforms, as they motivated their platform selection similarly. We additionally include 4 platforms to improve coverage across finance and scientific categories. We further show that this choice does not compromise generalization (see Section 5.3), as the open-source software usually shares UI elements and layout with its closed-source counterparts. For instance, LibreOffice and Office Suite share many interface elements, layout, and functionality. This ensures broader applicability of GROUNDCUA. A.2 HUMAN ANNOTATION We collaborated with professional data labeling vendor that specializes in dataset curation for AI applications. The annotation effort spanned three phases, beginning with pilot study where we worked closely with the annotation team to refine task instructions and provide iterative feedback. The annotation team consisted of around 70 individuals, organized into multiple tiers of annotators, quality assurance specialists, and project managers. The majority of the team was located in India and Latin America, with participants in the 2035 year age group and balanced gender distribution. All annotators held at least bachelors degree in technical fields such as Computer Science or Engineering and had prior experience in data labeling and user interface research."
        },
        {
            "title": "Preprint",
            "content": "Table 6: UI element categories in GROUNDCUA with descriptions and representative examples. Category Description and Common UI Elements Input Element Sidebar Information Display Button Navigation Visual Elements Menu Others Interactive fields where users enter or modify data, like text boxes, checkboxes, radio buttons, etc. Vertical or horizontal panels that provide quick access to tools or navigation. Examples include tool palettes, folder trees, settings sidebars. Regions that primarily present textual or numerical information. Examples include labels, console outputs, document text, and code blocks. Clickable controls that trigger an action like submit button, OK/Cancel buttons, play/pause buttons Elements that help users move within or across applications. Examples: tabs, back/forward arrows etc. Non-textual graphical elements that convey information or functionality. Examples include icons, thumbnails, images, charts, and progress bars. Structured lists of commands or options, often hierarchical. Examples: file menu, context menu, dropdown menus. Elements not covered by the above categories, often decorative or container elements like spacers. Annotators underwent training process to become familiar with the platforms and annotation guidelines. They were compensated hourly, with each task requiring on average 6090 minutes to complete, including quality checks. The process began with the creation of computer-use tasks for 87 software applications (see Table 5). Annotators then executed these tasks while screen recordings were collected. From these recordings, we extracted keyframes corresponding to major user interactions. Each keyframe was annotated using custom tool, where annotators drew bounding boxes around all visible interface elements. For each bounding box, annotators assigned label corresponding to the elements name, or, in the case of textual elements, the text was also provided in addition to the element name. For long text segments such as source code or lengthy descriptions, annotators provided concise summary that captured the main theme. To supplement these summaries, we also applied OCR using PaddleOCR (Cui et al., 2025) to extract the full text when available. In addition, every element was assigned to one of six high-level categories. We applied rigorous quality assurance at multiple stages. Annotations were reviewed by dedicated quality specialists, cross-checked by the authors, and validated using custom evaluation scripts. This pipeline allowed us to construct large-scale dataset of grounded user interface interactions with high diversity and reliable annotation quality. A.3 DATASET STATISTICS We provide detailed statistics for GROUNDCUA. Figure 4 presents the overall dataset statistics. Figure 4a shows the number of annotations across the 12 categories, while Figure 4b reports the number of screenshots per category. We also analyze the pixel distribution of screenshots in Figure 4c, observing wide range from roughly 0.3 megapixels to 7 megapixels. The distribution of bounding box areas, shown in Figure 4d, highlights the prevalence of small UI elements in the dataset. Finally, Figure 4e shows the number of bounding boxes per screenshot, with some screenshots containing up to 500 annotated elements and Figure 4f shows the distribution of desktop applications across 12 different categories. A.4 COMPARISION WITH PRIOR WORKS Comparative Analysis with Existing Datasets We compare GROUNDCUA against four recent grounding datasets: UGround (Gou et al., 2024b), Aguvis (Xu et al., 2024), OS-Atlas (Wu et al., 2024), and JEDI (Xie et al., 2025). For OS-Atlas and JEDI, which are much larger, we sample 200k images for screenshot-level analysis, with bounding-box statistics computed over all annotations. As shown in Figure 5 (left), GROUNDCUAs screenshots range from 0.5M7M pixels, averaging 2.0M, capturing high-resolution desktop environments. UGround and OS-Atlas (Desktop) have lower resolutions (1.1M and 1.6M), limiting their detail. Figure 5 (right) highlights GROUNDCUAs smaller median element size, with many fine-grained targets like icons and small controls, typical of desk-"
        },
        {
            "title": "Preprint",
            "content": "(a) Number of annotations in different software categories (b) Number of screenshots in different software categories (c) Pixel distribution of images (d) Relative bounding box area (e) Distribution of number of annotations in an image (f) Desktop application across different categories Figure 4: Dataset Statistics"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Comparison across different datasets. (Right) Relative bounding box area in log scale. (Left) Pixel distribution for different datasets. top interfaces. In contrast, other datasets focus on larger, more salient elements. GROUNDCUA also has denser annotations, averaging 64 per screenshot, more than three times that of OS-Atlas (Desktop) and much higher than Aguvis (9) or UGround (11). JEDI, despite its scale, has sparser annotations due to its reliance on synthetic data. UGround and Aguvis cover web interfaces, while OS-Atlas uses automated accessibility-tree traversal, which is often incomplete and prone to errors (Gou et al., 2024b; Muryn et al., 2025), resulting in less precise annotations. JEDI is impressive in scale but lacks dense, real-world coverage due to the synthetic pipeline involved in creating the dataset. GROUNDCUA, with its high-resolution, human-verified annotations, and extensive platform diversity, fills crucial gap by providing more accurate and detailed representation of desktop environments. A.5 DATASET EXAMPLES Figure 6 shows examples of screenshots from several software platforms with bounding boxes overlaid on the images."
        },
        {
            "title": "B INSTRUCTION TUNING DATA",
            "content": "GROUNDCUA contains over 3.5M annotated elements. Desktop screens are highly redundant, with many UI elements repeating across views. To reduce duplication before building instructions, we deduplicate elements using text matching on labels and perceptual image similarity (pHash) computed on crops defined by each elements bounding box. This produces roughly 900k unique elements. We use strict thresholds during filtering. While this may remove some valid cases, it yields diverse, non-redundant pool overall. We also randomize selection across screenshots so that no single interface is over-represented. The filtered elements form the base for constructing the instruction tuning data. We detail the different types of instructions we have created below and provide examples in Figure 7. B.1 DIRECT INSTRUCTIONS Direct instructions explicitly refer to the element (Click the File button) that the model should act on. These are the most common types of instruction CUA would encounter. We first create class of descriptive instructions for every element, which incorporates attributes such as color, shape, position, and nearby context. These descriptions provide richer context for the model and help reduce ambiguity. We generate these instructions by prompting Qwen2.5-VL-72B with the elements bounding box, platform name, annotated label, the full screenshot, and an optional zoomed crop. We also ask the model to provide the location of the element if there are other similar elements to disambiguate. We additionally use category information to create three specific types of direct instructions:"
        },
        {
            "title": "Preprint",
            "content": "(a) 7-Zip (b) GIMP (c) Brave (d) Audrino (e) FreeCAD (f) GNU Octave (g) Cryptomator (h) OpenShot (i) Frappe Books (j) Emby (k) Mastadon (l) LibreOffice Calc (m) Studio (n) VLC Media Player (o) Zotero Figure 6: Examples of screenshots from different platforms in GROUNDCUA. Red bounding boxes indicate the annotated UI elements within each screenshot. Description Instruction Prompt You are an expert UI analyst. You are given screenshot with target element in red bounding box, cropped image containing the target element in red bounding box, the name of the element and the platform name. Can you find it? Is it visible from the screenshot? Can you write concise description that is sufficient for humans to locate it from the screenshot? The response must be relevant to the platform and element name provided. Do not reference the red bounding box and that it is highlighted. If you find other identical elements, your description must include specific details about the"
        },
        {
            "title": "Preprint",
            "content": "target elements location and other unique attributes to differentiate it from the others. Only output what you are sure about. Do not make assumptions. Return the response in the following JSON format: { visible: true, description: your description here } Platform: {platform name} Target Element Label: {text} Textual elements. We identify textual elements by matching OCR output with the human-annotated label and by selecting items from the Information Display category. We then embed the extracted text into about 100 templates that directly instruct the model to move to these labels. Some templates used to generate instructions are provided below. Textual Elements Instruction Templates 1. Do you see the text text? Please click on it. 2. Please locate the user interface component marked with the text text and then proceed to click on it. 3. Make your way to the text label with your cursor. 4. You are required to find the element associated with the text text and then move your cursor to hover over it. Visual elements. For icon-based or other visual elements (e.g., tool icons, shapes, images), we generate concise captions that highlight distinctive features and local context (e.g., Click the magnifying-glass icon next to the search bar). These are produced using Qwen2.5-VL-72B by providing the element crop, its bounding box in the full screenshot, the platform name, and the annotated label. General templates. In addition to text and visual elements, we design set of general instructions that apply to any element. These are created heuristically using about 120 templates (e.g., Click on the following element:) or generated by prompting an MLLM. General Instruction Prompt You are an expert UI analyst. You are given screenshot with target element in red bounding box, cropped image containing the target element in red bounding box, the name of the element and the platform name. Is it visible from the screenshot? Generate concise, imperative instruction user would give to operate or interact with the target element. The response must be relevant to the platform and element name provided. Do not reference the red bounding box and that it is highlighted. If you find other identical elements, your description must include specific details about the target elements location and other unique attributes to differentiate it from the others. Only output what you are sure about. Do not make assumptions. Return the response in the following JSON format: { visible: true, instruction: your description here } Platform: {platform} Target Element Label: {text}"
        },
        {
            "title": "Preprint",
            "content": "B.2 FUNCTIONAL INSTRUCTION PROMPT Functional instructions describe an element by its purpose rather than its name (e.g., Open new tab). We focus on Buttons and Menus since these most often encode actions. For each candidate element, we prompt Qwen2.5-VL-72B with the full screenshot, the element crop and bounding box, platform name, and the annotated label, asking for concise functional instruction (e.g., Open new tab). Functional Instruction Prompt You are an expert UI analyst. You are given screenshot with target element in red bounding box, cropped image containing the target element in red bounding box, the name of the element and the platform name. Is it visible from the screenshot? Generate task-oriented instruction that describes users goal. The instruction must implicitly identify the target element by describing what it helps the user accomplish (not the name of the element). The response must be relevant to the platform and element name provided. It should also be concise and to the point. Do not reference the red bounding box and that it is highlighted. Include the location or other unique attributes if there are other identical elements. Only output what you are sure about. Do not make assumptions. Return the response in the following JSON format: { visible: true, function: your description here } Platform: {platform} Target Element Label: {text} B.3 SPATIAL INSTRUCTIONS Spatial instructions locate target element by its position relative to another element (anchor), using relations such as left, right, above, below, and between. We leverage dense annotations to choose anchors that are close to the target and have reliable labels (e.g., Click the icon to the left of Files). We generate these with simple templates that insert the anchors label and the relation. Some templates used to produce instructions are provided below. Spatial Instructions Templates 1. Place your mouse on the element directly to the right of {element}. 2. Hover your mouse on the element immediately to the left of {element}. 3. Hover your mouse on the element between {element 1} and {element 2}. 4. Place your mouse on the element directly above {element}. B.4 EXAMPLES Figure 7 shows different kinds of instructions generated by our data generation pipeline."
        },
        {
            "title": "C TRAINING",
            "content": "In this section, we describe the training process for GROUNDNEXT. We outline the key design choices behind our SFT and RL setups, including data selection and filtering strategies, hyperparameter configurations, and other relevant details. We also report experimental observations, highlighting the impact of these choices and the insights gained during development."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Instruction tuning data examples. C.1 SFT DATA From the instruction-tuning corpus, we curate split of 700K size with 50% direct instructions, 35% functional instructions, and 15% spatial instructions."
        },
        {
            "title": "Preprint",
            "content": "C.2 SFT TRAINING DETAILS We use LlamaFactory (Zheng et al., 2024) to train our SFT models with learning rate of 3e-6, cosine decay, and warmup ratio of 0.05. Models are trained for two epochs, as this consistently outperforms training for single epoch. Preliminary experiments also show that training the entire model, rather than only the LLM, is more effective; we therefore adopt this configuration throughout. All models are trained on single H100 node with 8 H100 GPUs, using global batch size of 128, gradient accumulation of 16, and per-device batch size of 1. C.3 RL DATA For RL training, we first performed rejection sampling on the SFT training set using the SFT model itself. Specifically, we extracted the models errors and sampled 10K instances, which were then used to run RL. While this yielded modest improvements, the SFT model was already strong, and many of the extracted errors corresponded to noisy or ambiguous datapoints (e.g., prompts with multiple valid answers or inconsistent labels). These issues limited the effectiveness of this approach. We next applied RL on top of the SFT model using 10K previously unseen samples from GROUNDCUA. This strategy avoided noise from ambiguous training points and yielded more significant performance boost. Consequently, our final setup exclusively used the 10K samples unseen during SFT from GROUNDCUA. We also explored incorporating small amount of out-of-distribution data to encourage generalization to web and mobile domains. Specifically, we added 10K samples from GUIAct (Chen et al., 2024), in addition to 10K samples from GROUNDCUA, split evenly between mobile (5K) and web (5K). Unlike the gains observed when adding in-distribution samples from GROUNDCUA, this preliminary attempt did not yield consistent improvements. We note, however, that our setup was limited in scope and did not include rejection sampling or other analysis. more systematic investigation of combining our dataset with complementary sources, particularly in the context of RL training to improve cross-platform performance, is an exciting direction for future work. C.4 RL TRAINING DETAILS For our RL training, we compared two rule-based optimization methods, Group Relative Policy Optimization (GRPO) and Relative Leave-One-Out (RLOO). Empirically, and as pointed out in previous literature (Zhang et al., 2025), we found that RLOO produced more stable learning and better results. The RLOO objective can be written as: θJ(πθ) = Eτ πθ (cid:88) t=1 θ log πθ(atst) R(τ ) 1 1 (cid:88) j=i R(τj) , (1) where R(τ ) is the reward of trajectory τ , and the baseline is computed as the average reward of all other trajectories in the same group (excluding the i-th trajectory). This avoids training critic model and instead uses relative group comparisons. In our case, the trajectories are the predicted coordinates by the model, and the reward is defined based on where the predicted point is relative to the bounding box. For the grounding task, τ is sequence of tokens, which represents the predicted coordinate. Reward Formulation. 1. Continuous reward: Based on the normalized distance between the predicted point ˆp and the ground-truth bounding box B, we defined: = 1 d, = ˆp MaxDist(B, W, H) , where is the closest point in B, and MaxDist(B, W, H) is the maximum possible distance point inside an image of width and height can have. However, this suffered from sparsity and weak gradient signals."
        },
        {
            "title": "Preprint",
            "content": "2. Binary reward: simple scheme assigning (cid:26)1 0 = if ˆp B, otherwise. This proved more stable than continuous rewards but lacked sensitivity to error magnitude. 3. Customized Discrete Reward (final choice): To distinguish between predictions that miss the bounding box by small or large margin, and to encourage predictions inside the box to move closer to the center, we refined the reward into multi-level discrete scale. Let the normalized distance be dnorm = MaxDist(B, W, H) , where is and MaxDist(B, W, H) is the maximum possible distance in the image. The reward is then defined as: the distance between the predicted point and the ground-truth box, = 1.0 0.5 0.1 0.1 0.5 1.0 if dnorm < 0.5, if 0.5 dnorm < 0.1, if 0.1 dnorm < 0, if 0 dnorm < 0.1, if 0.1 dnorm < 0.5, if dnorm 0.5. The thresholds for near were based on normalized distance bins. This shaped reward captured dominant error modes and yielded the best empirical performance. In summary, we adopt RLOO with the shaped reward formulation, which proved most effective for fine-tuning beyond the SFT initialization."
        },
        {
            "title": "D EVALUATION",
            "content": "D.1 SCREENSPOTPRO RESULTS Table 7 summarises the results for different models on ScreenSpot-Pro (Li et al., 2025). D.2 OSWORLD-G RESULTS Table 8 summarises the results for different models on OSWorld-G (Xie et al., 2025). D.3 MMBENCH-GUI RESULTS Table 9 summarises the results for different models on MMBench-GUI (Wang et al., 2025b). D.4 SCREENSPOT-V2 RESULTS Table 10 summarises the results for different models on ScreenSpot-v2 (Cheng et al., 2024). D.5 UI-VISION RESULTS Table 11 summarises the results for different models on UI-Vision (Nayak et al., 2025)."
        },
        {
            "title": "E GROUNDNEXT ERROR ANALYSIS",
            "content": "In this section, we examine the errors made by GROUNDNEXT-7B (RL) across different benchmarks. For example, as illustrated in Figure Figure 8, since our model was trained exclusively on desktop screenshots, some of its errors on mobile interfaces arise from limited domain knowledge (e.g., failing on the bottom-right join twitch server example). In some other cases, e.g., top left"
        },
        {
            "title": "Preprint",
            "content": "Table 7: Performance of different models on SSPro across categories (CAD, Dev, Creative, Scientific, Office, OS). Text and Icon refer to different input types. Model CAD Dev Creative Scientific Office OS Avg. GPT-4o (OpenAI, 2024) Claude Computer Use (Anthropic, 2025a) Qwen2.5-VL-3B (Bai et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025) ShowUI-2B (Lin et al., 2024) UI-TARS-2B (Qin et al., 2025) JEDI-3B (Xie et al., 2025) SeeClick-9.6B (Cheng et al., 2024) Aria-UI (Yang et al., 2024) OS-Atlas-7B (Wu et al., 2024) UGround-7B (Gou et al., 2024a) UI-TARS-7B (Qin et al., 2025) JEDI-7B (Xie et al., 2025) GUI-Actor-7B (Wu et al., 2025) OpenCUA-7B (Wang et al., 2025a) CogAgent-18B (Hong et al., 2023) UI-TARS-72B (Qin et al., 2025) UI-R1-3B (Lu et al., 2025) UI-R1-E-3B (Lu et al., 2025) GUI-R1-3B (Luo et al., 2025) InfiGUI-R1-3B (Liu et al., 2025a) GUI-G1-3B (Zhou et al., 2025) SE-GUI-3B (Yuan et al., 2025) InfiGUI-G1-3B (Liu et al., 2025b) GUI-R1-7B (Luo et al., 2025) SE-GUI-7B (Yuan et al., 2025) Phi-Ground-7B (Zhang et al., 2025) GUI-G2-7B (Tang et al., 2025) GTA1-7B (Yang et al., 2025) InfiGUI-G1-7B (Liu et al., 2025b) Our Models GROUNDNEXT-3B (SFT) GROUNDNEXT-3B (RL) GROUNDNEXT-7B (SFT) GROUNDNEXT-7B (RL) Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon Avg. 2.0 14.5 9.1 16.8 2.5 15.8 27.4 2.5 7.6 12.2 14.2 17.8 38.0 7.1 18.8 11.2 37.1 26.4 33.0 39.6 38.1 50.8 23.9 51.3 70.8 55.8 66.9 57.4 50.3 55.3 46.2 50.2 0.0 3.7 7.3 1.6 0.0 1.2 9.4 0.0 1.6 4.7 1.6 4.7 14.1 3.1 12.5 6.3 12.5 7.8 14.1 9.4 12.5 25.0 6.3 42.2 16.7 12.5 20.7 23.4 26.6 32.8 32.8 34. 1.3 22.0 22.1 46.8 16.9 51.9 61.0 0.6 16.2 33.1 26.6 47.4 42.9 14.9 62.9 22.7 46.1 33.8 51.3 50.7 55.8 64.9 49.4 68.2 56.6 68.8 62.6 74.7 65.6 65.6 68.2 73.4 0.0 3.9 1.4 4.1 1.4 2.8 13.8 0.0 0.0 1.4 2.1 4.1 11.0 0.7 17.2 4.1 6.9 4.8 12.4 10.3 7.6 20.0 4.8 19.3 13.3 17.2 18.2 24.1 36.6 36.6 38.6 40.0 1.0 25.9 26.8 35.9 9.1 47.5 53.5 1.0 23.7 28.8 27.3 42.9 50.0 9.6 57.1 27.3 41.9 40.9 44.9 36.6 47.0 51.5 38.9 57.6 26.9 57.1 53.3 64.6 48.5 50.0 54.5 59. 0.0 3.4 2.1 7.7 0.0 9.7 8.4 0.0 2.1 2.8 2.8 6.3 11.9 0.0 15.4 3.5 4.2 5.6 7.0 11.9 4.9 16.8 8.4 9.1 17.2 15.4 17.2 15.4 22.4 24.5 20.3 23.8 2.1 33.9 38.2 49.3 13.2 57.6 54.2 3.5 27.1 37.5 31.9 56.9 72.9 22.2 64.6 42.4 56.9 61.8 58.3 61.8 61.8 68.8 55.6 75.0 58.0 77.1 31.8 80.6 66.0 66.0 70.8 70.1 0.0 15.8 7.3 7.3 7.3 14.5 18.2 0.0 6.4 7.3 2.7 17.3 25.5 1.8 20.9 11.8 21.8 17.3 20.0 30.0 16.4 32.7 11.8 28.2 29.1 24.5 76.4 31.8 38.2 37.3 37.3 42. 1.1 30.1 33.9 52.5 15.3 60.5 64.4 1.1 20.3 33.9 31.6 50.3 75.1 13.0 63.3 32.2 65.0 53.6 65.5 67.2 59.9 70.6 58.7 78.5 76.4 74.0 82.5 75.7 76.3 74.6 76.8 74.6 0.0 16.3 15.1 20.8 7.5 13.2 32.1 0.0 1.9 5.7 11.3 17.0 47.2 0.0 26.4 11.3 26.4 17.0 28.3 32.1 24.5 32.1 26.4 43.4 44.0 32.7 50.9 39.6 54.7 50.9 49.1 54.7 0.0 11.0 10.3 37.4 10.3 38.3 38.3 2.8 4.7 27.1 17.8 21.5 33.6 5.6 42.1 13.1 32.7 28.1 43.9 23.5 40.2 49.5 42.1 49.5 55.1 57.9 48.6 57.0 41.1 45.8 45.8 53. 0.0 4.5 1.1 6.7 2.2 7.9 9.0 0.0 0.0 4.5 0.0 5.6 16.9 0.0 15.7 4.5 10.1 5.6 12.4 10.6 12.4 15.7 16.9 25.8 25.8 21.3 25.9 29.2 28.1 29.2 33.7 30.3 1.3 23.4 23.6 38.9 10.8 45.2 49.8 1.8 17.1 28.1 25.0 39.6 52.6 12.0 50.9 24.9 49.1 49.5 50.4 63.5 56.4 64.7 65.5 68.4 58.3 59.9 59.9 60.5 0.0 7.1 3.8 7.1 2.6 8.1 13.7 0.0 2.0 4.0 2.8 8.4 18.2 0.8 17.6 6.4 14.1 16.8 11.8 21.0 21.8 19.6 25.2 25.2 32.8 33.6 33.6 33. 0.8 17.1 16.1 26.8 7.7 31.1 36.1 1.1 11.3 18.9 16.5 27.7 39.5 44.6 50.0 7.7 38.1 17.8 33.5 35.7 37.1 35.9 45.2 47.3 43.2 47.5 50.1 51.9 48.6 49.8 50.2 52.9 Table 8: Performance comparison of models on OSWORLD-G across multiple capability dimensions. Model Text Matching Element Recognition Layout Understanding Fine-grained Manipulation Refusal Overall OS-Atlas-7B (Wu et al., 2024) UGround-V1-7B (Gou et al., 2024b) Aguvis-7B (Xu et al., 2024) UI-TARS-7B (Qin et al., 2025) Seed1.5-VL (Seed Team, 2025) UI-TARS-72B (Qin et al., 2025) Gemini-2.5-Pro (Gemini Team, 2025) Operator (OpenAI, 2025) Qwen2.5-VL-3B (Bai et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) JEDI-3B (Xie et al., 2025) JEDI-7B (Xie et al., 2025) InfiGUI-G1-3B (Liu et al., 2025b) InfiGUI-G1-7B (Liu et al., 2025b) GTA-1-7B (Yang et al., 2025) Our Models GROUNDNEXT-3B (SFT) GROUNDNEXT-3B (RL) GROUNDNEXT-7B (SFT) GROUNDNEXT-7B (RL) 44.1 51.3 55.9 60.2 73.9 69.4 59.8 51.3 41.4 45.6 63.2 67.4 65.9 65.5 72.0 63.2 67.4 70.9 72.4 74.3 29.4 40.3 41.2 51.8 66.7 60.6 45.5 42.4 28.8 32.7 47.3 53.0 55.5 53.0 63.6 82. 68.8 71.2 73.3 73.9 35.2 43.5 43.9 54.9 69.6 62.9 49.0 46.6 34.8 41.9 49.0 53.8 57.7 56.1 66.8 74.2 68.4 70.8 73.1 73.5 16.8 24.8 28.2 35.6 47.0 45.6 33.6 31.5 13.4 18.1 36.9 44.3 46.9 34.2 46.3 70.5 43.0 43.6 53.7 51.7 7.4 0.0 0.0 0.0 18.5 0.0 38.9 0.0 0.0 0.0 0.0 7.4 7.4 0.0 0.0 0. 0.0 0.0 0.0 0.0 27.7 36.4 38.7 47.5 62.9 57.1 45.2 40.6 27.3 31.4 46.5 50.9 54.1 49.6 59.9 67.7 62.2 64.2 67.2 67.7 choose tag, the model predicts click on the text label of an element rather than the element itself. We hypothesize that this behavior is partly due to our SFT and RL data selection strategy, which emphasized smaller elements within GROUNDCUA. As result, the model may have developed bias toward selecting the smaller element rather than the surrounding larger component."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Four examples of mistakes of GROUNDNEXT-7B (RL) on ScreenSpot-V2. The ground truth bounding box (green box) and predicted coordinates (red circle) are shown along with the instruction."
        },
        {
            "title": "Preprint",
            "content": "Table 9: MMBench-GUI: Cross-platform performance of models across Windows, MacOS, Linux, iOS, Android, and Web. Model Windows MacOS Linux iOS Android Web Avg GPT-4o (OpenAI, 2024) Claude-3.7 (Anthropic, 2024b) Qwen-Max-VL (Bai et al., 2023) ShowUI-2B (Lin et al., 2024) Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) OS-Atlas-Base-7B (Wu et al., 2024) Aguvis-7B-720P (Xu et al., 2024) UI-TARS-1.5-7B (Qin et al., 2025) UI-TARS-72B-DPO Qin et al. (2025) UGround-V1-7B (Gou et al., 2024b) InternVL3-72B (Zhu et al., 2025) Naive RLVR-3B (Liu et al., 2025b) Naive RLVR-7B (Liu et al., 2025b) InfiGUI-G1-3B (Liu et al., 2025b) InfiGUI-G1-7B (Liu et al., 2025b) Our Models GROUNDNEXT-3B (SFT) GROUNDNEXT-3B (RL) GROUNDNEXT-7B (SFT) GROUNDNEXT-7B (RL) Basic Adv. Basic Adv. Basic Adv. Basic Adv. Basic Adv. Basic Adv. 1.5 1.5 43.9 9.2 31.4 55.7 36.9 37.3 68.3 78.6 66.8 70.1 68.6 79.3 74.2 82.7 81.5 80.4 83.8 81.5 1.1 0.7 36.8 4.4 16.5 33.8 18.8 21.7 39.0 51.8 39.0 42.6 44.5 58.1 47.1 61.8 50.7 52.6 60.7 60. 8.7 12.5 58.8 24.1 31.3 49.9 44.4 48.1 69.0 80.3 71.3 75.7 78.6 82.3 78.8 83.8 85.8 87.2 86.7 87.8 4.3 7.5 56.1 10.4 22.0 30.1 21.7 33.3 44.5 62.7 48.6 52.3 50.0 62.7 55.2 63.9 64.2 64.5 69.9 73.1 1.1 1.1 53.9 25.1 21.5 40.3 31.4 33.5 64.4 68.6 56.5 59.2 61.3 64.4 65.4 72.3 73.8 70.7 75.4 75. 1.0 0.0 30.1 11.7 10.2 20.9 13.3 25.0 37.8 51.5 31.1 41.3 39.3 44.9 41.8 52.0 53.6 57.1 61.2 59.2 5.1 13.7 77.4 29.0 66.6 56.1 74.8 67.5 88.5 90.8 92.7 93.6 92.4 94.9 95.2 94.9 93.0 94.9 94.3 95.2 3.3 10.6 59.1 19.7 55.2 28.2 48.8 65.2 69.4 81.2 70.9 80.6 76.4 89.1 78.8 89.4 77.0 78.5 83.3 86. 2.5 1.4 79.5 17.4 35.1 55.6 69.6 61.0 90.5 93.0 93.5 92.7 91.3 95.5 92.1 95.2 90.4 91.9 94.9 95.5 1.4 1.4 70.1 8.7 35.2 25.4 46.8 51.0 69.3 80.0 71.0 78.6 76.1 84.2 78.0 85.6 73.8 78.0 79.4 80.3 3.2 3.2 74.8 22.9 40.3 68.4 61.3 61.6 81.0 88.1 88.7 90.7 87.4 92.9 89.7 93.5 88.1 90.6 91.0 90. 2.9 2.3 58.8 12.7 32.5 45.8 35.4 45.5 56.5 68.5 64.6 65.9 63.0 79.5 64.3 76.3 59.7 64.3 70.5 72.7 2.9 4.7 58.0 16.0 33.9 41.8 41.4 45.7 64.3 74.3 65.7 72.2 70.9 79.3 73.4 80.8 75.5 77.1 80.4 81.1 Table 10: ScreenSpot-V2: Cross-platform breakdown by device and modality. Icon/Widget indicates iconor widget-based queries. Avg. is across all devices and modalities. Model Mobile Desktop Web Avg. Text Icon/Widget Text Icon/Widget Text Icon/Widget 78.4 95.2 96.9 94.8 93.4 97.6 97.9 99.3 99.0 95.2 94.8 97.2 96. 50.7 75.8 89.1 86.3 73.5 87.2 88.2 88.2 91.9 80.6 96.4 84.8 88.2 70.1 90.7 95.4 91.2 88.1 90.2 98.5 94.8 94.3 93.8 93.9 94.3 95.4 29.3 63.6 85.0 87.9 58.6 74.2 79.3 82.9 82.1 84.3 87.1 90.0 87. 55.2 90.6 93.6 91.5 88.0 93.2 91.2 94.9 97.9 87.6 90.6 91.5 94.9 32.5 77.3 85.2 87.7 71.4 81.3 86.2 80.3 89.2 78.8 79.3 74.9 75.9 55.1 85.1 91.6 90.3 80.9 88.8 91.3 91.1 93.5 87.3 88.5 89.3 90. SeeClick (Cheng et al., 2024) OS-Atlas-Base-7B (Wu et al., 2024) UI-TARS-7B (Qin et al., 2025) UI-TARS-72B (Qin et al., 2025) Qwen2.5-VL-3B (Bai et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) InfiGUI-G1-3B (Liu et al., 2025b) InfiGUI-G1-7B (Liu et al., 2025b) Our Models GROUNDNEXT-3B (SFT) GROUNDNEXT-3B (RL) GROUNDNEXT-7B (SFT) GROUNDNEXT-7B (RL)"
        },
        {
            "title": "F LIMITATIONS",
            "content": "While our work makes significant progress in desktop GUI grounding, there are few key limitations. Although it covers 87 applications across 6 categories, the dataset may not fully represent the diversity of desktop software, as it is biased toward commonly used applications. Our keyframebased annotations capture static UI states but miss dynamic elements like animations and real-time updates. While weve taken steps to ensure annotation consistency, human labeling at scale can still introduce some inconsistencies, and the time and cost of annotation limit scalability. Additionally, our evaluation focuses on benchmark accuracy, but real-world applications require robustness to changes in distribution, new app versions, and UI updates; issues that need further exploration. Finally, we do not perform end-to-end agentic testing for task completion, which remains an important area for future work."
        },
        {
            "title": "Preprint",
            "content": "Table 11: UI-Vision: Performance grouped by category (Edu., Browser, Dev., Prod., Creative, Entert.) and by setting (Basic, Functional, Spatial). Model Grouped by Setting Overall Basic Func. Spatial GPT-4o (OpenAI, 2023) Claude-3.7-Sonnet (Anthropic, 2024b) Qwen-2.5VL-7B (Bai et al., 2025) InternVL2.5-8B (Chen et al., 2025) MiniCPM-V-8B (Yao et al., 2024) SeeClick-9.6B (Cheng et al., 2024) ShowUI-2B (Lin et al., 2024) CogAgent-9B (Hong et al., 2023) OSAtlas-7B (Wu et al., 2024) AriaUI-25.3B (Yang et al., 2024) UGround-v1-7B (Gou et al., 2024b) UGround-v1-72B (Gou et al., 2024b) Aguvis-7B (Xu et al., 2024) UI-TARS-7B (Qin et al., 2025) UI-TARS-72B (Qin et al., 2025) InfiGUI-G1-3B (Liu et al., 2025b) InfiGUI-G1-7B (Liu et al., 2025b) Our Models GROUNDNEXT-3B (SFT) GROUNDNEXT-3B (RL) GROUNDNEXT-7B (SFT) GROUNDNEXT-7B (RL) 1.6 9.5 1.2 2.5 7.1 9.4 8.1 12.0 12.2 12.2 15.4 27.9 17.8 20.1 31.4 31.2 36.2 70.9 72.9 67.1 70. 1.5 7.7 0.8 2.8 5.3 4.7 7.7 12.2 11.2 14.0 17.1 26.7 18.3 24.3 30.5 28.0 31.9 59.8 63.9 60.0 62.0 1.0 7.6 0.5 1.0 1.5 2.1 2.1 2.6 3.7 4.0 6.3 14.9 5.1 8.4 14.7 8.2 11.5 45.1 50.6 49.9 49.9 1.4 8.3 0.9 2.1 4.3 5.4 5.9 8.9 9.0 10.1 12.9 23.2 13.7 17.6 25.5 22.0 26.1 58.2 62.1 58.7 60."
        }
    ],
    "affiliations": [
        "CIFAR AI Chair",
        "Ecole de Technologie Superieure",
        "McGill University",
        "Mila - Quebec AI Institute",
        "National University of Singapore",
        "Polytechnique Montreal",
        "ServiceNow Research",
        "Universite de Montreal",
        "University of Oxford",
        "University of Waterloo"
    ]
}