{
    "paper_title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
    "authors": [
        "MiniMax",
        "Aonian Li",
        "Bangwei Gong",
        "Bo Yang",
        "Boji Shan",
        "Chang Liu",
        "Cheng Zhu",
        "Chunhao Zhang",
        "Congchao Guo",
        "Da Chen",
        "Dong Li",
        "Enwei Jiao",
        "Gengxin Li",
        "Guojun Zhang",
        "Haohai Sun",
        "Houze Dong",
        "Jiadai Zhu",
        "Jiaqi Zhuang",
        "Jiayuan Song",
        "Jin Zhu",
        "Jingtao Han",
        "Jingyang Li",
        "Junbin Xie",
        "Junhao Xu",
        "Junjie Yan",
        "Kaishun Zhang",
        "Kecheng Xiao",
        "Kexi Kang",
        "Le Han",
        "Leyang Wang",
        "Lianfei Yu",
        "Liheng Feng",
        "Lin Zheng",
        "Linbo Chai",
        "Long Xing",
        "Meizhi Ju",
        "Mingyuan Chi",
        "Mozhi Zhang",
        "Peikai Huang",
        "Pengcheng Niu",
        "Pengfei Li",
        "Pengyu Zhao",
        "Qi Yang",
        "Qidi Xu",
        "Qiexiang Wang",
        "Qin Wang",
        "Qiuhui Li",
        "Ruitao Leng",
        "Shengmin Shi",
        "Shuqi Yu",
        "Sichen Li",
        "Songquan Zhu",
        "Tao Huang",
        "Tianrun Liang",
        "Weigao Sun",
        "Weixuan Sun",
        "Weiyu Cheng",
        "Wenkai Li",
        "Xiangjun Song",
        "Xiao Su",
        "Xiaodong Han",
        "Xinjie Zhang",
        "Xinzhu Hou",
        "Xu Min",
        "Xun Zou",
        "Xuyang Shen",
        "Yan Gong",
        "Yingjie Zhu",
        "Yipeng Zhou",
        "Yiran Zhong",
        "Yongyi Hu",
        "Yuanxiang Fan",
        "Yue Yu",
        "Yufeng Yang",
        "Yuhao Li",
        "Yunan Huang",
        "Yunji Li",
        "Yunpeng Huang",
        "Yunzhi Xu",
        "Yuxin Mao",
        "Zehan Li",
        "Zekang Li",
        "Zewei Tao",
        "Zewen Ying",
        "Zhaoyang Cong",
        "Zhen Qin",
        "Zhenhua Fan",
        "Zhihang Yu",
        "Zhuo Jiang",
        "Zijia Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 3 1 3 8 0 . 1 0 5 2 : r MiniMax-01: Scaling Foundation Models with Lightning Attention MiniMax1 We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI. Figure 1 Benchmark performance. (a) MiniMax-Text-01 on core text benchmarks. (b) MiniMaxVL-01 on core multimodal benchmarks. (c) MiniMax-Text-01 on the long-context RULER (Hsieh et al., 2024) benchmark. The performance of leading commercial and open-source models is presented for reference. 1Please send correspondence to model@minimaxi.com. 2025 MiniMax. All rights reserved MiniMax-01: Scaling Foundation Models with Lightning Attention 1. Introduction Large Language Models (LLMs) (Anthropic, 2024; Dubey et al., 2024; Hurst et al., 2024; Team et al., 2024a) and Vision Language Models (VLMs) (Anthropic, 2024; Dubey et al., 2024; Hurst et al., 2024; Team et al., 2024a) have made rapid progress in recent years, excelling at tasks like knowledge Q&A, complex reasoning, mathematics, coding, and vision-language understanding. The context window for most models currently ranges from 32K to 256K tokens. However, these lengths often fall short of practical needswhether using professional book as context, assisting with an entire programming project, or maximizing the potential of in-context learning through many-shot examples. Context window expansion in the past two years has primarily resulted from more powerful GPUs and better I/O-aware softmax attention implementation (Dao et al., 2022; Liu et al., 2024a). However, extending these windows further has proven challenging. This limitation arises from the inherent quadratic computational complexity of the transformer (Vaswani et al., 2017) architecturefurther length extension causes computational demands to grow much faster than hardware capabilities can match. To address this challenge, researchers have proposed various methods for reducing the attention mechanisms computational complexity: sparse attention (Beltagy et al., 2020; Zaheer et al., 2020), linear attention (Qin et al., 2022a,b, 2024c), long convolutions (Qin et al., 2023a), state space models (the Mamba series) (Dao and Gu, 2024; Glorioso et al., 2024; Gu and Dao, 2024; Ren et al., 2024; Team et al., 2024b), and linear RNNs (Qin et al., 2023b, 2024d). Despite their theoretical promise, these innovations have seen limited adoption in commercial-scale models. In this report, we aim to build model that matches the performance of leading commercial models while providing context window longer by an order of magnitude. This ambitious objective requires carefully balancing multiple factors: network architecture, data, and computation. Our approach begins with selecting the most promising architecture, succeeded by the optimization of the underlying training and inference framework to ensure its support. For the network architecture, we required linear attentionnot just theoretically sound but highly efficient in practice, especially with long contexts. After extensive experimentation, we settled on hybrid architecture mainly using lightning attention (Qin et al., 2024b), an I/O-aware implementation of linear attention variant (Qin et al., 2022a). In the architecture, one transformer block with softmax attention follows every seven transnormer blocks (Qin et al., 2022a) with lightning attention. We determined the models total parameters based on practical constraint: the ability to process more than 1 million tokens on single machine with up to 8 GPUs and 640GB memory using 8-bit quantization. To maximize parameter and computation capacity, we implemented Mixture of Experts (MoE) (Fedus et al., 2022; Lepikhin et al., 2021). We comprehensively consider training resources, inference resources, and the final model performance, aiming to find better balance among the three. Extensive experiments guided us toward the final model specifications: 456 billion parameters, 45.9 billion activations, and 32 experts. Existing distributed training and inference frameworks are primarily optimized for softmax attention. However, our novel architecture, which integrates lightning attention, softmax attention, and MoE, necessitates complete redesign of both our training and inference frameworks. Furthermore, the framework must possess the capability to support the training and inference of models with hundreds of billions of parameters and context windows extending over millions of tokens. To this end, we implement the all-to-all communication in MoE using expert parallel (EP) and expert tensor parallel (ETP). It aims to minimize the overhead associated with inter-GPU communication. To facilitate context windows with unlimited expansion, we design varlen ring attention to reduce the redundancy in computation and the improved version of Linear Attention Sequence Parallelism (LASP) (Sun et al., 2024) to fully utilize the devices parallel capabilities. Additionally, we have implemented 2 MiniMax-01: Scaling Foundation Models with Lightning Attention comprehensive set of CUDA kernels tailored for lightning attention inference, achieving over 75% Model Flops Utilization (MFU) (Chowdhery et al., 2023) end-to-end on the Nvidia H20. Building upon the architecture design and computation optimizations, we train our foundational language model, MiniMax-Text-01. Our pre-training process began with curating diverse and high-quality corpus through rigorous data cleaning, reward-based quality enhancement, and better data mixture balancing, validated through systematic repetition-aware testing. To fully utilize the architectures long-context capability, we introduce in-depth analysis of the hyperparameters and propose three-stage training procedure, successfully extending the context window to one million tokens. During the alignment phase, we incentivize the models various capabilities through precisely tuned reward dimensions and multi-stage training methodology, especially in the areas of long-context and realworld scenarios. Subsequently, we augment our language model with visual capabilities by integrating lightweight Vision Transformer (ViT) (Dosovitskiy et al., 2021) module, thereby creating our vision-language model, MiniMaxVL-01. MiniMax-VL-01 undergoes additional training with 512 billion vision-language tokens, utilizing four-stage training process. The final stage of this training is specifically designed to optimize the user experience. Figure 2 Prefilling latency of different models. The MiniMax-Text-01 and Llama3-70B models are tested on H800 GPUs with tensor parallelism set to 8, utilizing custom inference framework with 8bit weight-only quantization (W8A16). Other models are tested through their official APIs. Within the maximum length supported by each model, sufficient number of uniformly distributed points were selected for testing. After removing outliers, the data is fitted with quadratic function. Comprehensive evaluations on core academic benchmarks demonstrate that both models attain performance levels comparable to those of closed-source top-tier models in both text and vision-language tasks, as illustrated in Figure 1 (a,b). For contexts longer than 200k, our model performs significantly better, as shown in Figure 1 (c). In addition to academic benchmarks, we also assess the models performance using in-house benchmarks derived from real-world usage and show that our model is top-tier in those scenarios. In addition to its performance, our model exhibits significant advantages in prefilling latency, attributed to its novel architecture, as illustrated in Figure 2. We summarize our contributions as follows: 1. We build model that rivals the top-tier closed-source models on standard academic benchmarks. Furthermore, this model supports context inputs of up to 4 million tokens, showcasing outstanding performance in long-context evaluations. 2. We demonstrate the first successful large-scale implementation of linear attention. While linear attention has been studied before, it has never been deployed at this scale. We provide comprehensive details on our algorithm design and engineering optimizations. 3. We outline practical approach and experimental methodology for the exploration of various models, datasets, evaluations, and algorithms, which may serve as valuable reference. 4. We publicly release the weights and offer cost-effective API, aiming to help others develop 3 MiniMax-01: Scaling Foundation Models with Lightning Attention models that push beyond current limitations. 2. Model Architecture In this section, we present the design of our network architecture. To achieve optimal performance within constrained resources and better handle longer sequences, we adopt MoE approach and employ linear attention as much as possible instead of the traditional softmax attention used in standard transformers. To facilitate more intuitive understanding, we illustrate the main architecture in Figure 3. Our design follows the Transformer-style block, with each comprises channel mixer (an attention block) and feature mixer (an MLP block). We employ two types of channel mixers: lightning attention and softmax attention. The feature mixer is an MoE that incorporates multiple feed-forward networks (FFNs). To ensure load balancing in the MoE blocks, we propose novel load balancing strategy inspired by GShard (Lepikhin et al., 2021), which we refer to the global router. This strategy is designed to maintain training stability. Additionally, DeepNorm (Wang et al., 2024a) is integrated to enhance overall performance. Output Hidden RMSNorm The final MiniMax-Text-01 architecture integrates both linear attention and softmax attention mechanisms in structured pattern. Specifically, transformber block with softmax attention is positioned after every 7 transnormer blocks (Qin et al., 2022a) of linear attention, leading to total of 80 layers. Each attention module is composed of 64 heads, each with head dimension of 128. The softmax attention layers employ Group Query Attention (GQA) (Ainslie et al., 2023) with group size of 8. Rotary Position Embedding (RoPE) (Su et al., 2024) is applied to half of the attention head dimension, with base frequency set to 10,000. The models hidden size is configured to 6144, and each layer incorporates 32 experts with top-2 routing strategy. The feed-forward network within each expert has hidden dimension of 9216. In total, MiniMax-Text-01 compromises 456 billion parameters, of which 45.9 billion are activated for each processed token. + 𝛼 MoE 1 RMSNorm + 𝛼 Softmax Attention RMSNorm + 𝛼 MoE RMSNorm + 𝛼 Lightning Attention + FFN 1 FFN 2 ... FFN 𝑁 Router 𝐾 = Input Hidden Linear RMSNorm SiLU SiLU SiLU Sigmoid V Figure 3 The architecture of MiniMax-Text-01. In the subsequent sections, we will delve into our considerations regarding the model architecture, i.e., the integration of different attention mechanisms, the synergy between MoE and linear attention, the rationale behind hyperparameter selection, and the methodology for determining the models size based on scaling laws. 2.1. Mixture of Experts MoE provides pathway to enhance both scalability and efficiency compared to the dense version. Typically, MoE is substitute for the feed forward networks (FFN) in feature-mixer layers (Fedus 4 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 4 Isoflop Comparison: MoE vs. Dense on various benchmarks. Both models are trained on 1 trillion tokens. The gray dashed lines indicate the difference in the computation required for the two models to achieve the same performance. et al., 2022; Lepikhin et al., 2021), which consists of multiple FFN experts, where each token is routed to one or more of these experts. Specifically, for an input token x𝑡, its corresponding output hidden state h𝑡 is calculated as: h𝑡 = 𝐸 𝑖=1 Softmax𝑖 (cid:0)TopK(x𝑡 W𝑔)(cid:1) FFN𝑖 (x𝑡), (1) where 𝐸 represents the total number of experts, W𝑔 is the weight of the gate, FFN𝑖 stands for the 𝑖-th expert, and TopK() denotes the operation that preserves the top 𝑘 scores among all 𝐸 experts while setting the remaining scores to . The training of MoE based LLMs can be categorized into token-drop and dropless. We adopt the token-drop strategy to improve training efficiency. With this approach, each expert is assigned capacity limit specifying the maximum number of tokens it can handle. Once this capacity is reached, any additional token routed to that expert is discarded. To assess the effectiveness of the MoE architecture, we conduct comparative study between dense model with 7 billion parameters and an MoE model with 2 billion activation parameters out of total of 24 billion parameters. The results, as illustrated in Figure 4, demonstrate that the MoE model significantly outperforms the dense model under the same computational budget on various benchmarks, including HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), Natural Questions(Kwiatkowski et al., 2019), PIQA(Bisk et al., 2020) and TriviaQA(Joshi et al., 2017). When scaling up to larger models, we encounter the challenge of routing collapse, which arises due to the concentrated distribution of tokens designated for allocation. To mitigate this issue, we incorporate simple global routing strategy to the GShard (Lepikhin et al., 2021) auxiliary loss for better load balancing. Auxiliary Loss. To ensure differentiability, the auxiliary loss is defined as 𝐿aux = 𝛼aux 1 𝑓𝑖 𝑚𝑖, 𝐸 where 𝛼aux represents the coefficient of the auxiliary loss, 𝑓𝑖 denotes the fraction of tokens assigned to the 𝑖-th expert, and 𝑚𝑖 is the average routing probability of expert 𝑖. (cid:205)𝐸 𝑖=1 Global Router. The GPU memory size constrains the micro batch size in LLM training, leading to substantial fluctuations in the token distribution within individual Expert Parallel (EP) groups. Moreover, token distributions vary across different EP groups, potentially resulting in load imbalances where experts in one EP group may be overloaded while those in another are underutilized. To address this, we implement global token dispatching strategy across EP groups. Specifically, we introduce an additional allgather communication step to synchronize the number of tokens awaiting processing by each expert before dispatching tokens across different EP groups. Under the same 5 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 5 Illustration of the computations for softmax attention (left) and linear attention (right). The input length is 𝑁 and feature dimension is 𝑑, with 𝑑 𝑁. Tensors in the same box are associated with computation. The linearized formulation allows 𝑂(𝑁) time and space complexity. capacity constraints, this global routing mechanism can effectively reduce the overall token drop rate, thereby ensuring training stability. 2.2. Linear Attention Linear attention utilizes the right product kernel trick to transform quadratic computational complexity into linear complexity, as illustrated in Figure 5. By taking TransNormer (Qin et al., 2022a) as an example, the NormAttention mechanism can be written as: = Norm((QK)V), (2) where Q, K, and ℝ𝑛𝑑 are the query, key, and value matrices, respectively, with 𝑛 for sequence length and 𝑑 for feature dimension. The equation can be transformed into its linear variant using right matrix multiplication: = Norm(Q(KV)), (3) The linear formulation facilitates efficient recurrent prediction with training complexity of 𝑂(𝑛𝑑2). Furthermore, linear attention ensures constant computational complexity of 𝑂(𝑑2), irrespective of the sequence length. This is accomplished by recurrently updating the term KV, thereby obviating the need for repetitive computation of the entire attention matrix. In contrast, softmax attention incurs complexity of 𝑂(𝑛𝑑2) during inference. When addressing causal language modeling tasks, the efficacy of the right product is compromised, necessitating the computation of cumsum (Hua et al., 2022). This limitation impedes the realization of highly efficient parallel computation, which likely explains why, despite being proposed by Brébisson et al. (de Brébisson and Vincent, 2016) nine years ago, none of the current leading open-source LLMsincluding LLaMA3 (Dubey et al., 2024), Qwen2.5 (Yang et al., 2024), DeepSeekV3 (DeepSeekAI, 2024), and Mistral (Jiang et al., 2023)have adopted this linear attention mechanism. 2.2.1. Lightning Attention Lightning attention (Qin et al., 2024b,c) represents an I/O-aware, optimized implementation of TransNormer (Qin et al., 2022a). This approach identifies the primary bottleneck in the computational efficiency of existing linear attention mechanisms: the slow cumsum operation inherent in causal 6 MiniMax-01: Scaling Foundation Models with Lightning Attention language modeling. To alleviate this problem, Lightning Attention proposes novel tiling technique that effectively circumvents the cumsum operation. The key innovation lies in the strategic division of the attention calculation into two distinct components: intra-block and inter-block computations. The left product attention calculation is employed for intra-block operations, while the right product is utilized for inter-block operations. This division is crucial because the intra-blocks can be significantly reduced in size, thereby ensuring that the overall computational complexity remains linear. Note that the lightning attention was originally proposed by our team members in Qin et al. (2024c), we recall some of the core processes to elucidate why it can achieve theoretical linear complexity in practice for the sake of completeness. In the interest of analytical tractability, we deliberately omit the consideration of normalization, sigmoid linear unit (SiLU) activation, and gating mechanisms in the following derivation. Let us start with the forward pass in lightning attention. The left product in causal attention calculation is defined as: = [(QK) M]V (4) where M𝑡𝑠 = 1 if 𝑡 𝑠, otherwise 0. The right product operation can be computed in recursive formula as: kv0 = 0, kv𝑡 = kv𝑡1 + k𝑡v 𝑡 , 𝑡 = 𝑡 kvt. (5) It is important to note that while Eq. 5 exhibits linear computational complexity, it is inherently unparallelizable. The fundamental concept underlying the implementation of lightning attention involves the utilization of tiling technique to compute attention scores. Specifically, the matrices Q, K, are partitioned into two distinct blocks along the row dimension: (cid:20)X1 X2 , X1 ℝ𝑚𝑑, X2 ℝ(𝑛𝑚) 𝑑, {Q, K, V}. = (cid:21) By unfolding Eq. 4, we obtain the following expression (noting that kv0 = 0): 𝑠 𝑠 𝑗v 𝑗 , 𝑠 = 1, . . . , 𝑚. 𝑠 = 𝑠 kv𝑠 = 𝑠 kv0 + 𝑠 kv𝑠 = kv0 + 𝑗v 𝑗 . Rewrite it in block form, we have: 𝑗=1 𝑗=1 O1 = Q1kv0 + [(Q1K 1 ) M]V1 Q1KV0 + [(Q1K 1 ) M]V1. (6) (7) As shown, the intra-block [(Q1K 1 use the right product. Note that the intra-block can be further divided using the same strategy: ) M]V1 can use the left product and the inter-block Q1KV0 can kv𝑚+𝑡 = kv𝑚 + 𝑚+𝑡 𝑗=𝑚+1 𝑗v 𝑗 , 𝑡 = 1, . . . , 𝑛 𝑚, 𝑚+𝑡 = 𝑚+𝑡kv𝑚+𝑡, O2 = Q2kv𝑚 + [(Q2K 2 ) M]V2 Q2KV1 + [(Q2K 2 ) M]V2. To compute the second block, we use KV1 = kv𝑚, which can be computed by: KV1 = KV0 + 𝑚 𝑗=1 k𝑚v 𝑚 = KV0 + 1 V1. (8) (9) where KV0 = kv0. By recursively applying the aforementioned strategy of partitioning the matrix into multiple blocks, the practical computational complexity can be reduced to linear. The final time complexity of lightning attention is 𝑂(𝑛𝑑2 + 𝑛𝐵𝑑), where 𝐵 is the block size. Algorithm 1 illustrates the IO-aware implementation of lightning attention forward pass. 7 MiniMax-01: Scaling Foundation Models with Lightning Attention Algorithm 1 Lightning Attention Forward Pass 𝐵 blocks X1, X2, ...X𝑇 of size 𝐵 𝑑 each, where {Q, K, V, O}. Input: Q, K, ℝ𝑛𝑑, block sizes 𝐵. Divide into 𝑇 = 𝑛 Initialize mask ℝ𝐵𝐵, where M𝑡𝑠 = 1, if 𝑡 𝑠, else 0. Initialize KV = 0 ℝ𝑑𝑑. for 𝑡 = 1, . . . , 𝑇 do Load Q𝑡, K𝑡, V𝑡 ℝ𝐵𝑑 from HBM to on-chip SRAM. On chip, compute Ointra = [(Q𝑡K On chip, compute Ointer = Q𝑡 (KV). On chip, compute KV = KV + 𝑡 V𝑡. Write O𝑡 = Ointra + Ointer to HBM as the 𝑡-th block of O. 𝑡 ) M]V𝑡. end for Return O. 2.2.2. Effectiveness of Lightning Attention Although lightning attention demonstrates promise and competitive performance in small-scale experiments, its scaling behavior and capability in the downstream tasks under large-scale settings remain unexplored. To mitigate the gap, we conduct series of scaling experiments to evaluate the scalability of the lightning attention mechanism in comparison to softmax attention, meanwhile verifying the performance on the extensive downstream tasks. It is noteworthy that during our experiments, we observed that lightning attention demonstrates limited retrieval capabilities. This finding inspired us to explore hybrid approach (Hybrid-lightning) that takes the advantages of both lightning and softmax attention to enhance retrieval performance by substituting lightning attention with softmax attention at intervals of every eight layers. We adhere to the FLOPs calculation methodology established by Kaplan et al. (2020). For the purpose of our analysis, we define the following variables: 𝑙 (number of layers), 𝑑 (model dimension), ℎ (number of attention heads), 𝑏 (batch size) and 𝑛 (sequence length). The checklist of model parameters and FLOPs is presented in Table 1. Table 1 Model Parameters and FLOPs Comparisons Across Architectures. For scaling law calculations, embedding parameters and other subleading terms are excluded to improve alignment with fitted results. Architecture Parameter count FLOPs count Softmax Attention Lightning Attention Hybrid-lightning 12𝑙𝑑2 12𝑙𝑑2 + 2𝑙𝑑2/ℎ 12𝑙𝑑2 + 7𝑙𝑑2/4ℎ 72𝑏𝑛𝑙𝑑2 (1 + 𝑛 72𝑏𝑛𝑙𝑑2 (1 + 1 6𝑑 + 5 2ℎ + 5 18𝑑 ) 18𝑑 ) 16ℎ + 5 18𝑑 ) 72𝑏𝑛𝑙𝑑2 (1 + 𝑛 48𝑑 +"
        },
        {
            "title": "2.2.2.1 Experimental Setup",
            "content": "We conducted training on softmax (equipped with FlashAttention-2 (Dao, 2024)), lightning attention, and hybrid-lightning attention models across various scales: 70 million, 160 million, 410 million, 1 billion, 3 billion, and 7 billion parameters. Each model was trained on dataset consisting of up to 300 billion tokens, with context length of 8192. Our training methodology follows the approach proposed by Chinchilla (Hoffmann et al., 2022), where the training loss serves as direct indicator of test performance. For each model architecture and training sequence length, we maintained 8 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 2 Summary of Scaling Laws: It shows the relationships between loss (𝐿), optimal model size (𝑁𝑜𝑝𝑡), and optimal dataset size (𝐷𝑜𝑝𝑡) as functions of computational budget (𝐶). It reveals that, given the same budget, the hybrid model uses more parameters and tokens but achieves lower loss. Arch Softmax Attention Lightning Attention Hybrid-lightning 𝐿(𝐶) 3.7087𝐶 0.0798 3.5391𝐶 0.0768 3.4797𝐶 0.0763 𝑁𝑜𝑝𝑡 (𝐶) (1.82 108)𝐶0.7118 (2.74 108)𝐶0.6470 (2.57 108)𝐶0.6670 𝐷𝑜𝑝𝑡 (𝐶) (2.56 1010)𝐶0.5102 (4.43 1010)𝐶0.4684 (3.70 1010)𝐶0.4707 Figure 6 Summary of Scaling Laws. Training curves (left) span models from 70M to 7B parameters. Optimal model size (center) and training tokens (right) are derived based on specified compute budget estimation. uniform global batch size of 4 million tokens. The Adam optimizer was employed, configured with learning rate of 3e-4 and weight decay of 0.1. fixed learning rate scheduler was applied across all experiments due to constrained computational resources. We employ diverse set of evaluation benchmarks, including BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC (both easy and challenge variants) (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), Needle in Haystack (NIAH) (Shen et al., 2024), and SCROLLS (Shaham et al., 2022). Each benchmark assesses distinct capabilities of the models."
        },
        {
            "title": "2.2.2.2 Scaling Laws",
            "content": "We fit the scaling curves based on our experiments over the above mentioned settings, where we alter the model size (𝑁) and dataset size (𝐷) for different computational budget (𝐶) and observe the corresponding training loss (𝐿) that serving as an estimator of test loss. We begin by establishing power-law relationships between 𝐿 and 𝐶, following Chinchillas methodology (Hoffmann et al., 2022). Using the fitted curve, we derive coefficients for optimal model size 𝑁𝑜𝑝𝑡 𝐶𝑎 and optimal dataset size 𝐷𝑜𝑝𝑡 𝐶𝑏. The original scaling laws (Kaplan et al., 2020) use 𝐿(𝑋) = (𝑋0/𝑋)𝛼𝑋 , while subsequent studies (Clark et al., 2022; Gao et al., 2024; Henighan et al., 2020; Hoffmann et al., 2022) employ 𝐿(𝑋) = 𝜖 + (𝑋0/𝑋)𝛼𝑋 for better fitting, where 𝜖 denotes the irreducible loss. For simplicity, we unify these forms into 𝐿(𝑋) = 𝛽𝑋 𝑋 𝛼𝑋 , facilitating direct comparison of scaling capabilities based on 𝛼𝑋 and 𝛽𝑋 . The summary of scaling laws is shown in Table 2 and Figure 6. It can be intuitively understood that given the same computational budget, models with lightning attention tend to utilize more parameters and tokens, yet they achieve lower loss compared to models with pure softmax attention. 9 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 7 Larger models and hybrid-lightning attention achieve the best performance across benchmarks. Performance is evaluated on CSR (Common Sense Reasoning), NIAH (Needle in Haystack), and SCROLLS benchmarks using three attention mechanism models from 410M to 7B parameters."
        },
        {
            "title": "2.2.2.3 Performance on Downstream Task.",
            "content": "We present the benchmark results of downstream tasks in Figure 7. Lightning attention demonstrates comparable performance across most downstream tasks, with the exception of NIAH. This indicates that linear attention exhibits similar language modeling capabilities to Transformer models but falls short in retrieval tasks, rendering it unsuitable for LLMs. However, the hybrid-lightning attention not only matches but surpasses the retrieval and extrapolation capabilities of softmax attention, making it well-suited for in-context learning in LLMs."
        },
        {
            "title": "2.2.2.4 Speed.",
            "content": "We assess the end-to-end training speed of softmax attention, lightning attention, and hybridlightning models with 3 billion parameters by measuring the tokens processed per GPU per second (TGS). For completeness, we also included popular linear models such as HGRN2 and Mamba2 in our evaluation. For the speed benchmark, the training context length was gradually increased until reaching the out-ofmemory limit on single-node H800 GPUs. As illustrated in Fig. 8, lightning attention achieves constant training speed irrespective of the sequence length and is the sole linear model that outperforms FlashAttention2. Figure 8 The training speed of various attention mechanisms, including softmax, lightning, hybridlightning, HGRN2, and Mamba2, was benchmarked across sequence lengths ranging from 1,024 to 65,536. Performance was measured in terms of training speed, reported as tokens processed per GPU per second (TGS). 10 MiniMax-01: Scaling Foundation Models with Lightning Attention 2.2.3. Hybrid Architecture Our preliminary experiments with the hybrid architecture have yielded promising results, motivating us to delve deeper into its potential through two variants: hybrid-cosformer2 and hybrid-hgrn2. In the hybrid-cosformer2 model, we replace the linear attention layers in the cosformer2 architecture with softmax attention layers at intervals of every eight layers. This substitution strategy is similarly applied in the hybrid-hgrn2 model. We conduct experiments using consistent setups to evaluate the downstream performance of these alternatives. Our findings, as summarized in Table 3, indicate that the hybrid-lightning model achieves the best performance. Table 3 Benchmarking various hybrid-linear models with 1 Billion Parameters. We present the average CSR score, weighted average accuracy for NIAH, and the average SCROLLS score. Higher scores indicate better performance across all tasks. Abbreviations: TGS (token per gpu per second), HS (HellaSwag), WG (WinoGrande), OBQA (OpenBookQA), NIAH, and SCR (SCROLLS). Hybrid-linear Arch. TGS PIQA HS WG ARC-E ARC-C OBQA CSR NIAH SCR Hybrid-cosformer2 Hybrid-hgrn2 Hybrid-lightning 23.3K 70.29 45.63 51.46 29.5K 70.89 51.23 56.51 33.4K 70.73 50.41 55.80 55.77 59.68 59.93 26.11 28.50 27. 30.60 46.64 32.40 49.87 32.80 49.55 43.6 91.8 95.7 10.9 10.8 13.3 In addition to linear models, sliding window attention can also achieve linear computational complexity by appropriately adjusting the window size. As it is grounded in softmax attention, it serves as robust baseline for evaluating linear architectures. Therefore, we incorporated the hybrid-window approach by replacing the sliding window attention with full softmax attention every eight layers. We evaluated various window sizes of SWA ranging from 256 to 1024. Our results indicate that larger window sizes lead to slower training speeds compared to the hybrid-lightning model. To compare these models under equivalent speed conditions, we did not consider window sizes larger than 1024. As shown in Table 4, the hybrid-lightning model outperforms all other models across all metrics, particularly excelling in the NIAH benchmark. Table 4 Benchmark comparison of hybrid-lightning and hybrid-window Models. Metrics include average CSR score, weighted NIAH accuracy, and average SCROLLS score. Higher scores indicate better performance across all tasks. Abbreviations: PS (parameter size, billion), W.S. (window size of SWA), HS (HellaSwag), WG (WinoGrande), OBQA (OpenBookQA), NIAH, SCR (SCROLLS), TGS (token per gpu per second). P.S 1B 3B W.S. Arch. 256 512 1024 TGS PIQA HS WG ARC-E ARC-C OBQA CSR NIAH SCR 35.6K 70.29 48.68 53.35 Hybrid35.1K 70.95 48.19 52.33 window 33.6K 69.75 47.80 53.12 Hybrid-lightning 33.4K 70.73 50.41 55.80 16.1K 73.83 59.70 59.59 Hybrid15.8K 73.29 60.00 59.04 window 15.4K 74.27 59.02 57.85 Hybrid-lightning 15.1K 74.21 61.06 59. 48.61 32.60 47.70 30.00 31.60 48.02 32.80 49.55 54.31 35.00 36.00 53.97 33.00 53.44 35.80 55.16 28.75 27.22 28.33 27.65 33.62 32.51 31.91 34.90 57.95 57.53 57.53 59.93 64.10 62.96 64.56 65.49 10.6 11.9 10.6 13.3 14.2 14.2 13.3 14.7 46.8 25.7 53.9 95.7 40.9 57.9 41.6 98.0 256 512 2.2.4. Discussion Based on our analysis of scaling law experiment, downstream performance and speed comparison, we conclude that while pure linear attention models are computationally efficient, they are not suitable 11 MiniMax-01: Scaling Foundation Models with Lightning Attention for LLMs. This is due to their inherent inability to perform retrieval, capability that is essential for in-context learning. In contrast, our hybrid model not only matches but also surpasses softmax attention in both retrieval and extrapolation tasks. This outcome is somewhat counterintuitive. To understand this phenomenon, consider the following explanation of softmax attention: = Softmax(QK/ 𝑑)V. (10) It can be rewritten into linear recurrent form as: 𝑠0 𝑡 = 0, 𝑡 = 𝑠 𝑗1 𝑠 𝑗 𝑡 + exp(q𝑡k𝑇 𝑗 / 𝑑), 𝑡 = (𝑠 𝑗1 𝑗 𝑡 /𝑠 𝑗 𝑡 )o 𝑗1 𝑡 + (1 𝑠 𝑗1 𝑡 /𝑠 𝑗 𝑡 )v 𝑗, o𝑡 = o𝑡 𝑡, 𝑗 = 1, . . . , 𝑡. (11) Note that the linear recurrence form of lightning attention is as follows: kv0 = 0, kv 𝑗 = kv 𝑗1 + 𝑗v 𝑗 𝑗 = kv 𝑗 𝑗, 𝑗 = 1, . . . , 𝑡. (12) The softmax attention mechanism can be interpreted as linear RNN (Qin et al., 2024a). At each time step 𝑡, the hidden state is recalculated starting from the initial time 𝑡0 = 1, process often described as \"Going Through Book.\" This method enables the model to accurately retain input information by systematically revisiting previous data. In contrast, linear models lack this recomputation process, which hinders their ability to effectively retain input data. Let us define the capacity of an RNN as the size of its recurrent state. Upon closer examination of Eq. 11, we can deduce that the capacity of softmax attention is 𝑂(𝑑). In contrast, as illustrated in Eq. 12, the capacity of lightning attention is 𝑂(𝑑2/ℎ). Given that 𝑑 > ℎ, it follows that lightning attention possesses larger capacity than softmax attention. Consequently, the hybrid-lightning model exhibits superior retrieval and extrapolation capabilities compared to models relying solely on softmax attention. 2.3. Module Ablations in MoE Based on the conclusions from previous sections, we conduct two additional sets of ablation experiments to validate module choices within the MoE architecture on larger scale: (1) Hybrid-lightning attention versus softmax attention: To verify the advantages of the hybrid lightning attention in the MoE. (2) Pre-Layer Normalization versus Post-Layer Normalization: In our hybrid architecture, the effective depth of the model plays significant role. Thus, we expect to find better normalization algorithm for the deep model. Hybrid-lightning Attention versus Softmax Attention. We perform small-scale comparative analysis between softmax attention and hybrid-lightning attention within the MoE architecture. Specifically, we use 28 billion parameter MoE with 5 billion activation parameters that utilize softmax attention as the base model. For every 8 consecutive layers in the base model, we systematically replace softmax attention with lightning attention in the first 7 layers. Both the base model and the modified model are trained on 1 trillion tokens. As shown in Table 5, the results reveal that substituting certain softmax attention layers with lightning attention improves accuracy across most benchmarks. Pre Layer Normalization versus Post Layer Normalization. Pre Layer Normalization(Baevski and Auli, 2018; Child et al., 2019; Wang et al., 2019) (PreNorm), which applies normalization layers before residual connections and attention mechanisms, has demonstrated enhanced stability and performance in LLMs. Since PreNorm allows gradients to flow more directly from the output to the input through residual connections, bypassing the sub-layers to certain extent, it reduces the effective depth of the model. In contrast, Post Layer Normalization(Wang et al., 2019) (PostNorm) applies normalization after the residual connection and attention mechanisms, thereby preserving 12 MiniMax-01: Scaling Foundation Models with Lightning Attention the models effective depth. However, PostNorm can be prone to vanishing and exploding gradients, presenting significant challenges in training LLMs. Most existing LLMs predominantly use PreNorm, as the performance differences between wider and deeper networks in the conventional Transformer architecture are often negligible, and training stability is prioritized. The experiments are performed on models with 9.3 billion activation parameters and total of 60 billion parameters, each consisting of 48 layers that employ different normalization methods. Both models are trained on 500 billion tokens. For PostNorm, we utilize DeepNorm (Wang et al., 2024a) to ensure more stable training. As illustrated in Table 5, PostNorm consistently outperforms PreNorm across all evaluated metrics. Table 5 Module Ablations. Abbreviations: BBH (BIG-Bench Hard), DROP (Discrete Reasoning Over Paragraphs), MMLU (Massive Multitask Language Understanding), CMMLU (Massive Multitask Language Understanding in Chinese), GSM8k (Grade School Math 8K), ARC-C (Arc-Challenge), WG (WinoGrande). Arch. BBH DROP MMLU CMMLU MATH Softmax Hybrid-lightning Pre Layer Norm. Post Layer Norm. 28.2 32.2 29.9 32.6 27.4 29.0 26.8 27. 49.3 49.5 43.9 50.2 47.3 46.0 41.8 49.2 4.6 6.8 4.8 5. GSM8k 18.8 18.5 12.2 16.8 ARC-C WG 46.4 47.4 43.5 46.2 65.6 67. 65.5 65.4 2.4. Model Spec Upon finalizing the architecture of the models modules, the subsequent step entails scaling up the model, which necessitates meticulous design of the models hyperparameters across various dimensions. Our primary goal is to strike balance between performance and inference efficiency. Single-device inference offers superior efficiency compared to multi-device implementations by eliminating cross-machine communication overhead. Consequently, we constrain the models total parameters to 500B, ensuring compatibility with single-node inference on an 8 80G configuration for sequences up to 1M tokens under 8-bit quantization. Given our limited training budget, we formulate the following optimization problem to determine optimal parameter allocations: min 𝑃all,𝑃act 𝐿(𝑃all, 𝑃act, 𝑇) subject to 𝐶compute(𝑃all, 𝑃act, 𝑇) < 𝐶 and 𝑃all < 500𝐵, (13) where 𝐿 denotes the loss, 𝑃all and 𝑃act represent the total and activation parameter counts respectively, 𝑇 is the number of training tokens, 𝐶compute denotes the computational costs (dependent on parameter counts and data consumption), and 𝐶 signifies the budget constraint. Through comparative experiments on small-scale models, we first establish optimal ranges for several key variables: (1) the mixing ratio between softmax and linear attention mechanisms; (2) the depth-to-width ratio of the model architecture; (3) the ratio of linear attention memory size to hidden size; (4) the ratio of activated FFN to attention; (5) the proportion of dimensions utilizing RoPE for softmax attention. Our experiments reveal that the hybrid architecture demonstrates particular sensitivity to layer depth, with deeper models consistently outperforming shallower counterparts. Notably, shallow models require substantially more softmax attention layers to achieve comparable performance, underlining the efficiency advantages of deeper architectures. We also observe that increasing linear attention memory size significantly enhances model performance, and implementing RoPE on half of the softmax attention dimensions enables length extrapolation without performance degradation. 13 MiniMax-01: Scaling Foundation Models with Lightning Attention Based on these optimized architectural variables, we employ established scaling laws (Clark et al., 2022; Hoffmann et al., 2022) to determine the optimal model size. We train models with activation parameters ranging from 44 million to 1.2 billion across 500 billion tokens, utilizing 16, 32, and 64 experts. However, we find the predictions from these methods become less reliable when extrapolating to larger model with 9.3 billion parameters. To address this limitation and achieve more accurate predictions, we propose the following formula: 𝐿(𝑃act, 𝑇 𝐸) = 𝑑 + 𝑎𝑃𝛼 act + 𝑏𝑇 𝛽 + 𝑐(𝑃act𝑇)𝛾, (14) where 𝐿(𝑃act, 𝑇 𝐸) represents the loss conditioned on the number of experts, while 𝑎, 𝑏, 𝑐, 𝑑, 𝛼, 𝛽, and 𝛾 are parameters to be fitted in relation to the number of experts. Based on the predictions of Eq. 13 and Eq. 14, we have identified candidate model with 45.9 billion activation parameters and 456 billion total parameters as the optimal configuration. 3. Computation Optimization In this section, we present our computation part, including the training and inference. In this project, we have dynamically changing GPU cluster, where the number of H800 GPUs ranges from 1500 to 2500. An efficient architecture necessitates robust implementation optimization to fully harness its computational benefits at scale. To scale our novel architecture to the requisite size, we present three key optimization strategies that primarily address the following three challenges: 1. Mitigating the all-to-all (a2a) communication overhead during the training of Mixture of Experts (MoE) architecture is persistent challenge. The configuration we choose for our experts, specifically opting for large models, imposes substantial demands on GPU memory. Therefore, the primary challenge lies in achieving an optimal equilibrium between memory utilization, computational efficiency, and the overhead associated with all-to-all communication. 2. As we endeavor to support at least 1 million token context window in both training and inference, the accurate distribution of tokens within such an extensive context window across different GPUs becomes imperative for this colossal model. This necessity, however, inevitably introduces additional communication overhead. As result, devising strategies to minimize this overhead, particularly in the context of our hybrid architecture, presents significant challenge. 3. The current implementation of the lightning attention mechanism is specifically optimized for training processes. However, in the inference scenario, the challenge arises in effectively managing real-world batched inputs, which may encompass variable sequence lengths and specific inputs that incorporate prefix caching. It is noteworthy that the existing open-source frameworks in the industry currently lack the necessary mature technical support to adequately address these challenges. Thus, we independently and comprehensively reinvent our distributed training and inference framework, thereby successfully addressing these challenges with the desired level of efficiency. 3.1. MoE Optimization The primary objective in optimizing the MoE architecture is to minimize communication overhead, particularly for MoE models that utilize all-to-all (a2a) communication. To address this, We implement token-grouping-based overlap scheme, as illustrated in Figure 9. In this scheme, the a2a communication is performed within the expert parallel (EP) communication group, and it overlaps with the processing of tokens from different expert groups. To ensure the correctness of the communication 14 MiniMax-01: Scaling Foundation Models with Lightning Attention results, we restrict each ProcessGroup to execute communication operators sequentially. As result, a2a communications across different groups cannot overlap, leading to the emergence of idle time. 0 0 expert Device 1 Device 0 Idle time a2a-combine a2a-dispatch w/ EP overlap w/o EP overlap This approach leads to significant performance improvements. However, upon more detailed analysis, we identified critical tradeoff specific to the expert configuration of the MiniMax-Text-01 model. When Tensor Parallelism (TP) is employed to partition the expert parameters, the computational intensity becomes excessively low, thereby hindering the efficiency of the computation. However, opting not to use TP leads to an excessively large parameter count, which necessitates the activation of larger Pipeline Parallelism (PP) configuration. The challenge emerges because PP does not reduce the memory footprint required for storing activations. This limitation is particularly detrimental for training models with long contexts, as the increase in memory consumption does not provide proportional benefits in terms of computational efficiency or training speed. Consequently, it is imperative to develop new parameter partitioning strategy that adeptly balances memory usage and computational intensity to optimize the training process for our specific model and task. Figure 9 Expert Parallel (EP) Overlap Illustration. Chunk tokens into 2 groups thus computation can overlap with communication between different groups. Time 0 1 1 To achieve enhanced efficiency, we first introduce novel ProcessGroup, termed ETP (Expert Tensor Parallel), which is specifically designed to manage the weight partitioning of experts. Concurrently, we propose another distinct ProcessGroup, named EDP (Expert Data Parallel), to encapsulate the data parallelism of identical experts. In our system, we define the total number of GPUs involved in training as 𝑤𝑜𝑟𝑙𝑑_𝑠𝑖𝑧𝑒. The system must satisfy two key conditions: 𝑤𝑜𝑟𝑙𝑑_𝑠𝑖𝑧𝑒 = 𝑠𝑖𝑧𝑒𝑃𝑃 𝑠𝑖𝑧𝑒𝐷𝑃 𝑠𝑖𝑧𝑒𝐶𝑃 𝑠𝑖𝑧𝑒𝑇 𝑃 and 𝑤𝑜𝑟𝑙𝑑_𝑠𝑖𝑧𝑒 = 𝑠𝑖𝑧𝑒𝑃𝑃 𝑠𝑖𝑧𝑒𝐸𝐷𝑃 𝑠𝑖𝑧𝑒𝐸𝑇 𝑃 𝑠𝑖𝑧𝑒𝐸𝑃 (15) (16) This configuration empowers the MoE component with the flexibility to define the distribution of experts, manage the weight partitioning of experts, and independently configure the ZeRO (Zero Redundancy Optimizer) algorithm (Rajbhandari et al., 2020). Based on this implementation, we are able to completely decouple the parallel strategies of the MoE components from those of the non-MoE components. Building upon this modification, we can flexibly configure the ETP to achieve an optimal balance between memory usage and computational intensity. Furthermore, to mitigate communication overhead, we design an EP-ETP overlap strategy. This strategy aims to maximize the utilization of both network resources and computational resources, as illustrated in Figure 10 (a). Since communications within the same process group must be executed sequentially, extended periods of computation not only facilitate overlap with greater number of communications but also create additional opportunities for communications across different process groups to overlap, leading to enhanced overall performance as illustrated in Figure 10 (b). When determining the number of groups, several trade-offs must be considered. Theoretically, only by dividing the workload into sufficiently large number of groups can we achieve ample overlap between communication and computation, as illustrated in Figure 10 (c). However, in practice, an excessive number of groups can significantly increase the complexity of scheduling and introduce the 15 MiniMax-01: Scaling Foundation Models with Lightning Attention w/o EP + ETP overlap Idle time Waste time a2a-dispatch allgather expert reduce scatter a2a-combine (a) Device Device 1 Device 2 Device 3 w/ EP + ETP overlap 0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3 w/o EP + ETP overlap a2a-dispatch allgather expert reduce scatter a2a-combine w/ EP + ETP overlap Device 0 0 0 (b) Device Device 2 Device 3 0 1 1 1 2 2 3 3 (c) Device 0 Device 1 Time 0 0 0 0 1 1 1 2 2 3 3 3 0 1 1 1 Figure 10 EP-ETP Overlap Illustration. (a) EP-ETP overlap with the lower computation portion. (b) EP-ETP overlap with the higher computation portion. (c) EP-ETP overlap with fewer groups. Compared with (a) and (b), it shows that if the compute time cost is longer, the efficiency will be better. Comparing with (b) and (c), it shows that fewer groups will lead to insufficient overlap. risk of becoming CPU-bound. Given that the proportion of ETP (Expert Tensor Parallel) in the overall MoE (Mixture of Experts) architecture is not substantial, it is crucial to make adjustments based on the specific context and requirements. Through the aforementioned optimization strategies, we achieve balanced configuration of storage and computational intensity for the specific expert specifications in the MoE (Mixture of Experts) structure of the MiniMax-Text-01 model. Furthermore, based on these optimizations, we reduce the pure communication overhead of the MoE component by 50% compared to the preoptimization state, resulting in significant improvement in training efficiency. 3.2. Long Context Optimization significant challenge in long context training is that real training samples are difficult to standardize into uniform length. The conventional approach of using padding to make samples the same length leads to substantial computational waste. In the context of training at the 1M sequence length scale, this waste becomes particularly significant. To address this issue, we adopt data formatting technique during training where different samples are concatenated end-to-end along the sequence dimension. We refer to this technique as \"data-packing\". This format minimizes computational waste during the computation process, thereby conserving computational resources. 3.2.1. Varlen Ring Attention For Softmax Attention, the ring attention algorithm (Liu et al., 2024a) offers an effective method to partition data, thereby enabling unlimited scalability. However, the existing implementations MiniMax-01: Scaling Foundation Models with Lightning Attention causal compute causal varlen compute non-causal compute non-causal varlen compute (a) (b) Figure 11 Ring Attention v.s. Varlen Ring Attention. (a) No data packing in ring attention. (b) Pack 3 samples with different lengths in varlen ring attention. are not optimized to efficiently handle the ring attention mechanism for the data-packing format. In the case of FlashAttention (Dao, 2024), while it provides varlen (variable length) interface to accommodate the data-packing format, there is no corresponding ring attention implementation available. Regarding TransformerEngine (NVIDIA, 2023), the implementation incorporates Context Parallel (CP) ProcessGroup to support the ring attention algorithm. However, this approach poses risk of computational resource waste when dealing with the data-packing format. This is because the algorithm divides each sequence into 2 𝑠𝑖𝑧𝑒𝐶𝑃 segments and applies the ring attention mechanism to each segment. Consequently, this approach restricts each sequence to length that must be an integer multiple of 2 𝑠𝑖𝑧𝑒𝐶𝑃. In scenarios where the sample distribution is unknown and the CP size is set to large value, this can lead to significant padding, resulting in the waste of computational resources. Motivated by the principle of not making assumptions about the sample distribution, we redesign the algorithm and name it Varlen Ring Attention. This approach avoids the excessive padding and subsequent computational waste associated with traditional methods by applying the ring attention algorithm directly to the entire sequence after data-packing. Specifically, the implementation involves distinguishing the offset of the attention mask corresponding to each sequence within the ring attention computation. The key modification is to transform the original causal computations into varlen causal computations and similarly convert the non-causal computations into varlen non-causal computations, shown in Figure 11. 3.2.2. Improved Linear Attention Sequence Parallelism For lightning attention, the LASP (Linear Attention Sequence Parallelism) algorithm (Sun et al., 2024) leverages the communication group of CP to facilitate the expansion of long sequences. As illustrated in Figure 12 (a), the LASP algorithm mandates that all CP ranks engage in send-recv operations to exchange intermediate key-value (𝐾𝑉) block results. This requirement imposes sequential dependency among the CP ranks, thereby compelling the computation to be performed in serial manner. Consequently, this sequential dependency significantly impedes the overall efficiency of the training process, as the inherent parallelism of the system is not fully exploited. To fully harness the parallel computing capabilities of GPU devices, we propose an optimized approach that refines the computational and communication workflow to eliminate dependencies during the computation process. This optimization effectively transforms serial computation into parallelized one. The enhanced approach, termed LASP+ (Figure 12 (b)), operates as follows: 1. Local Prefix Sum Calculation: Each computing node i.e., the CP rank, initiates the process by 17 MiniMax-01: Scaling Foundation Models with Lightning Attention block size padding init 𝐾𝑉 = 𝐾0𝑉0 with zeros shape [𝑑, 𝑑] init diag with decay shape [𝑑, 𝑑] input sequence, shape [𝑠, ℎ, 𝑑] [ℎ, 𝑠, 𝑑] 𝑄,𝐾,𝑉 have the same shape, split sequence with CP 𝑄2 𝐾2 𝑉2 𝑄4 𝐾4 𝑉4 4 𝑄3 𝐾3 𝑉3 3 recv 𝑄6 𝐾6 𝑉 6 𝑄5 𝐾5 𝑉5 5 recv 𝑄7 𝐾7 𝑉7 recv 8 deprecate send sum(0-2) sum(0-3) send sum(0-4) sum(0-5) send sum(0-6) sum(0-7) deprecate output sequence, shape [ℎ, 𝑠, 𝑑] [𝑠, ℎ, 𝑑] Initialize Phase (a) 𝑄 𝐾 𝑉 𝑄1 𝐾1 𝑉1 𝑂𝑖𝑛𝑡𝑟𝑎 𝑖 = (𝑄𝑖𝐾𝑖 𝑀)𝑉𝑖 [ℎ, 𝐵, 𝑑] 𝐾𝑖𝑉𝑖 = 𝐷 𝐾𝑖 𝑉𝑖 [ℎ, 𝑑, 𝑑] 𝐾𝑉 𝑂𝑖𝑛𝑡𝑒𝑟 𝑖 = 𝐷 𝑄𝑖 𝐾𝑉 [ℎ, 𝐵, 𝑑] 𝑂𝑖 = 𝑂𝑖𝑛𝑡𝑒𝑟 𝑖 + 𝑂𝑖𝑛𝑡𝑟𝑎 𝑖 0 𝐾𝑉 += 𝐾𝑖𝑉𝑖 sum(0-1) 𝑂𝑖𝑛𝑡𝑟𝑎 𝑖 = (𝑄𝑖𝐾𝑖 𝑀)𝑉𝑖 [ℎ, 𝐵, 𝑑] 𝐾𝑖𝑉𝑖 = 𝐷 𝐾𝑖 𝑉𝑖 [ℎ, 𝑑, 𝑑] 1 2 3 4 5 7 8 deprecate local prefix sum 𝐾𝑉𝐿 sum(1-2) sum(3-4) sum(5-6) sum(7-8) (b) allgather across ranks global prefix sum 𝐾𝑉𝐺 sum(0) sum(0-1) sum(0-2) sum(0-3) sum(0-4) sum(0-5) sum(0-6) sum(0-7) 𝑂𝑖𝑛𝑡𝑒𝑟 𝑖 = 𝐷 𝑄𝑖 𝐾𝑉𝐺 [ℎ, 𝐵, 𝑑] 𝑂𝑖 = 𝑂𝑖𝑛𝑡𝑒𝑟 𝑖 + 𝑂𝑖𝑛𝑡𝑟𝑎 𝑖 output sequence, shape [ℎ, 𝑠, 𝑑] [𝑠, ℎ, 𝑑] Figure 12 Difference of LASP Algorithm and LASP+ Algorithm. (a) LASP Algorithm. 1. Initialization Phase: initializing KV to zero and the diagonal decay matrix. 2. Data Partitioning and Padding: partitioning the Q, K, and matrices along the sequence dimension into CP size (4 segments illustrated in the figure) blocks, dividing each block into smaller blocks based on the BlockSize 𝐵 and padding the remaining part (e.g. Q7, K7, V7) that cannot be divided evenly by 𝐵. 3. Intra-block Computation: performing intra-block of each CP rank computations in parallel. 4. Inter-block Computation and Communication: starting from CP rank 0, computing the inter-block portion of the current 𝑄𝑖 with all previous KV blocks and the prefix sum 𝐾𝑖𝑉𝑖. Different CP ranks communicate data through send-recv operations. (b) LASP+ Algorithm. Building upon figure (a), each CP rank computes the local prefix sum 𝐾𝑉𝐿 and performs AllGather operation to synchronize, then selects the local prefix sum 𝐾𝑉𝐿 to compute the global prefix sum 𝐾𝑉𝐺. The remaining computational components are same as (a). 18 MiniMax-01: Scaling Foundation Models with Lightning Attention independently calculating its local prefix sum, denoted as 𝐾𝑉𝐿. 2. Global Synchronization via AllGather: Following the local calculations, an AllGather operation is performed to synchronize the information from all nodes globally. This step ensures that each node has access to the necessary data from all other nodes. 3. Prefix Sum Computation: Each node selects the specific CP ranks 𝐾𝑉𝐿 on which to perform the prefix sums, decision based on its assigned computation order. By implementing these steps, the LASP+ approach effectively removes the original dependencies between the computation nodes. This elimination of dependencies facilitates fully parallelized computation process, thereby significantly enhancing the overall efficiency and throughput of the system. The transformation from serial to parallel computation not only leverages the full potential of GPU devices but also ensures that the training process can be executed more rapidly and with greater scalability. The proposed modifications, while incurring additional costs in terms of increased total communication volume and temporary memory usage, are unequivocally justified by the substantial performance benefits they confer. These enhancements significantly outweigh the associated overhead in communication and memory consumption. Through comprehensive testing and verification, it is empirically demonstrated that the computation speed in the LASP+ approach can attain up to 1/𝑁𝑝𝑐𝑛 of the original LASP algorithm, where 𝑁𝑝𝑐𝑛 denotes the number of parallel computing nodes. Furthermore, the overhead introduced by the AllGather operation is minimal, which is consistent with our anticipations and underscores the efficacy of the optimization. Building upon the LASP+ framework, we further introduce support for the varlen feature to effectively manage the data-packing data structure. This enhancement is particularly beneficial for handling batched samples that comprise inputs with unequal token lengths. The process involves the following steps: 1). Padding to Block Size: Each input within the batch is padded to ensure that its length is multiple of the predefined block size, which is set to 256. This padding step is crucial for aligning the data structure with the computational requirements of the kernel. 2). Sequential Concatenation: After padding, the inputs are sequentially concatenated. This concatenation facilitates the use of single kernel to perform parallel computations across multiple batches. By organizing the data in this manner, we can efficiently leverage the parallel processing capabilities of the GPU, thereby optimizing computational performance. The integration of the varlen feature with the LASP+ framework ensures that the system can handle diverse input lengths without compromising on efficiency. This approach not only simplifies the computational workflow but also maximizes resource utilization by enabling the processing of multiple batches concurrently. 3.3. Lightning Attention Inference Optimization The initial implementation of the lightning attention mechanism is primarily research-oriented and not yet suitable for practical applications, especially for inference. However, the optimization of inference processes is of paramount importance in real-world scenarios, as the long-term cost of deploying trained model is predominantly determined by the efficiency of its inference. To this end, we implement four optimization strategies for lightning attention: batched kernel fusion, separated prefill and decoding execution, multi-level padding, and strided batched matmul extension. 19 MiniMax-01: Scaling Foundation Models with Lightning Attention 3.3.1. Batched Kernel Fusion We fuse multiple memory-bound kernels and extend support to accommodate all batch inputs. In the prefill phase, we perform kernel fusion for processing the 𝑄, 𝐾, and 𝑉 tensors, including padding in the sequence dimension, partitioning into blocks, adjusting the internal layout, and computing the decay values. In the decoding phase, we perform kernel fusion for the computation of 𝐾𝑉 and the updating of the prefix 𝐾𝑉 cache. These kernel fusions reduce intermediate result storage and memory access operations, thereby significantly improving memory access efficiency and reducing end-to-end latency by 10% in the decoding phase and short-text input scenarios. By the way, these optimizations can bring very noticeable benefits on H20 compared to H800. 3.3.2. Separated Prefill and Decoding Execution The implementation of the lightning attention mechanism for long sequence computations primarily revolves around the differentiation between intra-block and inter-block computations. However, this approach is not optimal for inference tasks, particularly in the decoding phase, where the token length is consistently equal to 1. Given that the computational kernel for tokens of length 1 is predominantly memory-bound and necessitates only limited number of GPU Streaming Multiprocessors (SMs), we propose strategy that segregates the processing of tokens with length of 1 from those with length greater than 1. This is achieved by employing two distinct kernels. Subsequently, we utilize two separate CUDA streams to schedule these kernels in parallel, thereby enhancing computational efficiency and ensuring balanced GPU utilization, especially in scenarios involving mixed inputs. For instance, in batch size of 20, where all inputs contain prefix key-value (KV) cache, and the scenario includes one or two inputs with token length of 50 while the remaining inputs have token length of 1, this approach can significantly reduce latency. Specifically, the latency can be approximately equivalent to that of processing only the longer inputs, demonstrating reduction from 100 milliseconds to 50 milliseconds. 3.3.3. Multi-level Padding By applying padding to the 𝑄, 𝐾, 𝑉 tensors along the sequence dimension, the intra-block and interblock components can be effectively decomposed into multiple identical matrix multiplications. This decomposition is particularly advantageous as it aligns seamlessly with the StrideBatchedMatmul interface, thereby facilitating the maximization of parallel processing capabilities. Initially, the block size for padding was set to 256, configuration that was consistent with the training parameters. However, upon the implementation of the prefix cache technique, it is observed that the token lengths within batch typically fall below 256. This discrepancy led to redundant computations within each matrix multiplication operation. To address this inefficiency and minimize unnecessary computations, we propose the introduction of additional segmentation options, specifically 32, 64, and 128. This multi-level padding approach enables the dynamic selection of the computational scale that incurs the minimal padding overhead, based on the current input sequence length. By adopting this approach, the utilization of computational resources is optimized, ensuring that the system operates with increased efficiency and reduced redundancy. This strategic adjustment not only conserves computational resources but also contributes to the overall performance enhancement of the system. 20 MiniMax-01: Scaling Foundation Models with Lightning Attention 3.3.4. StridedBatchedMatmul Extension We utilize the optimized function cublasGemmStridedBatchedEx from the NVIDIA cuBLAS Library to manage StridedBatchedMatmul operations, thereby ensuring both high performance and versatility across diverse hardware architectures. Concurrently, we are in the process of implementing more extensive kernel fusion strategy, with the objective of substantially improving the computational efficiency of Hopper GPUs. Given that our sequence partitioning block size is configured to 256, the associated General Matrix-Matrix Multiplication (GEMM) operations, which involve matrices of dimensions 256x256, can leverage warpgroup-wide WGMMA instructions for computation. To further enhance memory access efficiency, we integrate the asynchronous operations of the Tensor Memory Accelerator (TMA) and delegate certain preprocessing and postprocessing computational tasks to be executed asynchronously on the CUDA Cores. Ultimately, our goal is to dynamically regulate the number of pipeline stages to adaptively attain optimal performance across both H20 and H800 GPU architectures. This adaptive control mechanism will ensure that the system can efficiently handle varying workloads and hardware configurations, thus maximizing overall computational throughput and resource utilization. By implementing the aforementioned optimizations, we achieve Model Flops Utilization (MFU) exceeding 75% on the H20 GPU for end-to-end inference tasks (Chowdhery et al., 2023). Specifically, in our MiniMax-Text-01 and MiniMax-VL-01 inference, when considering the latency ratio between the attention operation and the Feed-Forward Network (FFN) operation within the MoE structure, the softmax attention constitutes 95% of the latency at sequence length of 1,024,000 tokens. In contrast, the lightning attention implementation contributes to less than 12% of the latency under the same conditions. Our lightning attention implementation exhibits remarkable efficiency in managing heterogeneous batch inputs, which are characterized by diverse sequence lengths. This efficiency is particularly evident in scenarios where some inputs incorporate the prefix caching strategy while others do not. The reduction in latency not only enhances the overall speed of the inference process but also ensures that the system can handle wide range of input types with minimal performance degradation. This adaptability underscores the robustness and versatility of our lightning attention approach in real-world applications. 4. Pre-Training In this section, we provide an overview of the pre-training methodology for MiniMax-Text-01. First, we detail the meticulous construction of our pre-training corpus, with particular emphasis on data quality, standardized formatting, and mixing strategies to maximize model performance. Subsequently, we outline our innovative data experimentation framework, which enables rapid and resource-efficient evaluation of data effectiveness while minimizing computational costs. Lastly, we present an in-depth analysis of the models training hyper-parameters and present hierarchical training approach, which enables context length scaling up to 4 million tokens. 4.1. Data 4.1.1. Pre-training Corpus The pre-training corpus for MiniMax-Text-01 encompasses comprehensive and meticulously curated dataset, incorporating diverse sources including academic literature, books, web content, and 21 MiniMax-01: Scaling Foundation Models with Lightning Attention programming code. We enhance corpus quality through several strategic dimensions: Data Quality Enhancement. Superior data quality is fundamental for Large Language Models. We implement sophisticated filtering pipeline, combining rule-based cleaning and deduplication procedures aligned with established practices (Penedo et al., 2023, 2024; Rae et al., 2021). To assess document quality at granular level, we utilize our previous-generation model as the reward labeler (a MoE model with 5B activations and 60B total parameters). Initially, we evaluate multiple quality dimensions including coherence, conciseness, educational value, helpfulness, knowledge richness, and categorical relevance. Through comprehensive analysis, we identify significant correlations among these metrics and ultimately focus on three key dimensions: knowledge depth, practical helpfulness, and categorical distribution, while maintaining other metrics as secondary validation indicators. Data Formatting Optimization. The content from websites and books, once appropriately extracted and cleaned, can naturally be used as high-quality textbooks (Gunasekar et al., 2023) without further formatting. For dialogue and question-answering data, the sequential nature of text inherently captures conversational logic and question-answer relationships. Although humans benefit from additional formatting (e.g., Markdown) for readability and comprehension, we find that heavy formatting can actually diminish data diversity and quality by introducing fixed patterns that constrain the natural variation present in human conversations. Ultimately, to maintain format generalization capabilities and accommodate human preferences in alignment, we implement nested document format with versatile templates for dialogue and QA data, carefully balancing natural comprehension with structural consistency across various interaction patterns. Data Mixture Investigation. We develop sophisticated approach to tuning the data distribution, leveraging our three primary quality metrics. Based on the experiment paradigm detailed in the subsequent section, we discover that while high-scoring content on knowledge depth and helpfulness generally yielded superior performance in capability assessments, completely eliminating lower-scoring content can adversely affect downstream task performance. Therefore, we implement balanced sampling strategy, beginning with uniform distribution across the base corpus, and then adjusting sampling weights to favor high-quality content while maintaining sufficient representation of diverse categories. 4.1.2. Tokenization For tokenization, we employ byte-level Byte Pair Encoding (BPE) (Brown et al., 2020; Shibata et al., 1999), incorporating the pre-tokenizer methodology. We strategically up-sample multilingual content, to enhance the corresponding compression efficiency. The resulting vocabulary size is set to 200K tokens. 4.1.3. Data Experiment To systematically evaluate our design choices regarding pre-training data quality, format, and composition, we conduct extensive ablation experiments. These experiments involve training multiple small-scale MoE models using comparable token quantities but varying data characteristics. This approach enables us to isolate and measure the impact of individual data attributes while maintaining computational efficiency. 22 MiniMax-01: Scaling Foundation Models with Lightning Attention 4.1.3.1 Paradigm Formulation. We conduct Data Experiments to systematically compare the performance of different model variants. Specifically, we formulate experiments as statistical hypothesis tests that compare evaluation metric distributions between baseline model and models trained with different data configurations. When testing the effectiveness of new data corpus D, we formulate our alternative > 𝜇𝑇baseline, where 𝜇 represents the weighted average performance metric and 𝑇 hypothesis as 𝐻1 : 𝜇𝑇D denotes the distribution of evaluation values across test samples. Evaluation. We carefully design our evaluation norms to ensure meaningful insights. We look at wide range of multiple-choice benchmarks, discarding choice indices in query formulation and look at the likelihoods of completion. We observe the distributions of sample-wise log-normalized accuracy log accnorm2, defined as log accnorm2 (𝑥) = log softmax𝑝 (𝑐𝐶𝑥 ) (cid:110) ( 𝑝(𝑐)) (cid:111) , 𝑖 (𝑐) = 𝑝𝑖 (𝑐) where 𝑝 bytes(c) is the byte-normalized probability of choice 𝑐 for sample 𝑖. We choose bytewise normalization to exclude the effect of tokenizer, while alleviating the disfavor towards longer choices. We conduct extensive experiments to ensure that this metric is stable across training, while maintaining the discriminative power of the metric, which is quantified by the ratio Δobvious/𝜎seed, where Δobvious represents the obvious difference in performance between models and 𝜎seed denotes the standard deviation across different random seeds. Experiment Efficiency & Setup. With such statistical setup, we are able to conduct power analysis to decide minimal test sample size while maintaining the MDE (Minimal Detectable Effect) at similar level as our training variance, and guaranteeing 95% confidence level and 80% power for decision making. With the confidence methodologies set, we conduct simple scaling experiments on token amount and the model size, and eventually land at an experiment step of training MoEs of 1B activation and 8B total parameters with 40B tokens of data, where data mixture comprises 20B web documents and 20B data of hypothesis."
        },
        {
            "title": "4.1.3.2 Effect of Repetition",
            "content": "The incorporation of repeated data has been empirically demonstrated to introduce several detrimental effects on the models performance and generalization capabilities (Hernandez et al., 2022). Consequently, implementing deduplication strategies is essential for optimizing LLM performance. Recent studies (Abdin et al., 2024; Penedo et al., 2024) suggest that repeatedly training high-quality documents can lead to enhanced downstream performance, with certain high-quality domains being trained up to 50 times, where the repetition is measured by MinHash similarity(Broder, 1997; Lee et al., 2022). However, our empirical analysis reveals that their experimental paradigm is inadequate for assessing the impact of repetition, as data efficiency is not consistent throughout the training process. To achieve better alignment with the results of the full training, we introduce novel repetitionaware experimental framework. Specifically, we first perform global deduplication on the dataset to remove redundant entries. Then, we down-sample the documents to align the repetition frequency with the requirements of the final training schedule while adhering to the budget constraints of our ablation experiments, different from the previous experimental setups which directly adopted data distributions identical or similar to those used in the final training stage. Our findings indicate that low-quality data suffer substantial decrease in performance after training for more than two epochs, while high-quality data can be effectively trained for up to four epochs, similar to previous 23 MiniMax-01: Scaling Foundation Models with Lightning Attention observations (Muennighoff et al., 2023). Notably, the solution derived from the proposed framework yields better alignment with the results obtained using considerably more computational resources. By carefully controlling the repetition and quality of the training data, we achieve more efficient and effective data mixture, ultimately leading to better model performance. 4.2. Training Strategy Initial Pre-training. We initialize all model parameters using the Xavier initialization method (Glorot and Bengio, 2010), the scaling factors of DeepNorm (Wang et al., 2024a) are set to 𝛼 = (2𝑁)0.25 and 𝛽 = (8𝑁) 0.25, where 𝑁 denotes the number of layers. We employ the AdamW optimizer (Loshchilov and Hutter, 2019) with 𝛽1 = 0.9, 𝛽2 = 0.95, and the weight decay is set to 0.1. The training sequence length is 8192, and the batch size is progressively scaled from an initial size of 16M to 32M at 69B tokens, to 64M at 790B tokens, and finally to 128M at 4.7T tokens, where it remains until the end of training. The schedule is designed based on the correlation between training loss and the critical batch size (McCandlish et al., 2018). It is argued that training at the critical batch size yields near-optimal balance between training time and data efficiency (Kaplan et al., 2020). Following this, we fit power-law relationship between the loss and the critical batch size on data from smaller models, as shown in Figure 13. The batch size is doubled when the corresponding loss is reached. The learning rate schedule begins with linear warm-up over 500 iterations to peak value of 2 104, followed by training with constant learning rate for 7.2T tokens. In the latter stages of training, we notice anomalous gradient norm values. This issue is attributed to an excessively high learning rate and we adjusted lr to 1.3 104 for the remaining 3.2T tokens. During the fast decay phase, we train 1T tokens and exponentially decrease the learning rate to 3 105. Additionally, the MoE auxiliary loss coefficient is set to 0.01. Figure 13 The power-law fit for the training loss and the critical batch size, utilizing data from models ranging from 50M to 600M in activated parameters counts. We mark the points where the batch size is doubled with dashed gray lines. Long-Context Extension. We incrementally expand the models training context length to 1M tokens. Due to our architectures effective length extrapolation capabilities, the model successfully demonstrates its ability to process sequences up to 4M tokens in the vanilla Needle-In-AHaystack retrieval task (NIAH) test 2, despite only being trained on contexts up to 1M tokens, as illustrated in Figure 14. Specifically, we employ three-stage training procedure to systematically upsample long-context data across diverse length ranges, while preserving the distributional characteristics of critical domains to preserve short-context evaluation performances steady. The details of the training data mixture, RoPE base frequency, and training length are shown in Table 6. We also mix in 10% of high-quality long-context question-answering data with similar length distribution as long-context pre-training data during the last 20% of training cycles in each stage(Parmar et al., 2024). To mitigate potential instabilities resulting from distributional shifts, we utilize linear interpolation of source-specific weights throughout the transitional phase. This method facilitates gradual and controlled evolution of the data distribution towards the desired target distribution, thereby ensuring training stability 2Same as Gemini (Team et al., 2024a), we use Paul Graham (https://paulgraham.com/articles.html) as the haystack and The special magic {city} number is: {number} as the needle. 24 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 14 4 Million vanilla Needle-In-A-Haystack retrieval task pressure test on MiniMax-Text-01. The token interval is 32K when it is less than 1M, and the token interval is 0.5M when it is greater than 1M. and preserving convergence properties. Additionally, our findings indicate that NIAH is inadequate for effectively monitoring the models performance throughout the training process. This is primarily because NIAH metric performance reaches its peak score early on, specifically within the initial 128K training steps. To tackle this limitation, we evaluate the models intermediate checkpoints using more demanding tasks, which are designed to increase in complexity as training progresses. Notably, despite the escalating difficulty of these tasks, we consistently observe steady improvement in the models performance metrics. This sustained upward trajectory clearly demonstrates the critical importance and necessity of implementing long-context continual pretraining. More details are given in Section 5.7.2. Table 6 Long-Context Extension Recipe. For clarity, we categorize the data as follows: data with fewer than 32K tokens are labeled as Short; data ranging from 32K to 128K tokens are labeled as Medium; and data exceeding 128K tokens are categorized as Long. Training Length RoPE Frequency # Tokens Short (%) Medium (%) Long (%) 128K 512K 1M 5M 10M 10M 300B 32B 26B 30 35 30 70 35 30 0 30 40 5. Post-training In this section, we present thorough post-training framework designed to enhance the models general performance, long-context capability, and real-world applicability. Our approach begins with the creation of diverse, high-quality prompt dataset, accompanied by hierarchical reward system that evaluates responses across multiple dimensions: correctness, truthfulness, helpfulness, and harmlessness. The training process consists of Supervised Fine-Tuning (SFT), Offline and Online Reinforcement Learning (RL). Through these phases, we systematically align the model with our defined objectives. Model safety is ensured through exhaustive data mining techniques and specialized harmless reward model. We introduce novel multi-stage training methodology that significantly enhances the models capacity to process extended contexts while maintaining optimal MiniMax-01: Scaling Foundation Models with Lightning Attention performance on shorter sequences. This approach results in robust system capable of handling complex, real-world scenarios. Extensive evaluations conducted across both academic and in-house benchmarks demonstrate that our model achieves top performance across all tasks, while establishing new standards of extremely long-context processing. 5.1. Prompt Collection Our extensive prompt collection encompasses millions of diverse, high-quality queries from various sources. We develop tagging system that categorizes each prompt based on task type, knowledge domain, and difficulty level. The collection process incorporates sophisticated filtering mechanisms to eliminate redundant prompts while maintaining an optimal difficulty distribution. The prompt set spans various domains including long-context, programming, math, logical reasoning, creative writing, function calling, general-knowledge, and safety-related scenarios. 5.2. Reward Model Our reward model framework evaluates responses across four critical dimensions to ensure alignment with our core principles: Correctness. We implement rigorous evaluation system for responses that can be strictly validated. For mathematical and reasoning tasks, we utilize early-version MiniMax-Text-01 to generate binary reward signals based on answer consistency. Programming solutions undergo comprehensive testing in secured sandbox environment, with performance metrics derived from test case success rates. Truthfulness. We employ verification pipeline to assess the factual accuracy of the response. The process involves systematic response sampling, statement decomposition and clustering, crowd-sourced verification, and automated comparison using advanced language models to generate truthfulness scores. Helpfulness. Our evaluation framework assesses compliance with user instructions through both deterministic and probabilistic approaches. We implement automated rule-based constraint verification systems complemented by human evaluation of key metrics including coherence, depth, contextual relevance, and stylistic appropriateness. The final helpfulness score combines multiple evaluation signals through weighted scoring system. Harmlessness. Building upon Constitutional AI principles (Bai et al., 2022b), we develop evaluation criteria encompassing safety protocols, content appropriateness, and legal compliance. Our assessment system leverages carefully calibrated prompts validated against human annotations, with early-version MiniMax-Text-01 providing standardized safety evaluations. 5.3. Supervised Fine-Tuning Our SFT dataset construction involves multi-stage process utilizing domain-specific expert models trained through iterative SFT and RL cycles. We implement rejection sampling (Bai et al., 2022a; Dubey et al., 2024) to generate high-quality responses by the experts, sampling multiple variations per prompt across different temperature settings to select optimal demonstrations measured by the reward hierarchy. The response selection process further incorporates both n-gram and semantic similarity filters to ensure maximum diversity and quality in the training data. 26 MiniMax-01: Scaling Foundation Models with Lightning Attention 5.4. Reinforcement Learning 5.4.1. Offline Reinforcement Learning We incorporate the offline RL phase, i.e., Direct Preference Optimization (DPO) (Rafailov et al., 2023), to optimize the models performance across diverse prompt distributions, owing to its simplicity and ease of data construction for long-context scenarios. We specifically focus on prompts that maintain distributional consistency with those utilized in the SFT stage. To evaluate the impact of prompt selection, we conduct comparative experiments using two prompt categories: SFT-trained prompts and SFT-untrained but homologous prompts. Empirical results demonstrate negligible performance variations between SFT-trained prompts and their untrained counterparts. Thus, we adopt the SFTtrained ones for the offline RL phase. The experimental protocol involves generating responses with varying temperature parameters for each prompt, followed by systematic evaluation using the reward models described in Section 5.2. We then identify the best and the worst responses to construct preference pairs for DPO training. 5.4.2. Online Reinforcement Learning Online learning demonstrates superior sample efficiency and cross-domain generalization capabilities compared to offline learning methodologies. Therefore, we implement online RL to improve model performance, particularly in mathematical reasoning tasks. Our approach emphasizes prompt diversity and prioritizes prompts with moderate success rates to maximize information gain during policy updates. Notably, we employ SFT-untrained prompts during online RL, as our empirical observations indicate that reusing prompts from previous phases resulted in model saturation, characterized by diminished response perplexity. We propose modified Group Relative Policy Optimization (GRPO) (Shao et al., 2024) approach incorporating the following key innovations: Importance Sampling Weight Clipping. The conventional PPO/GRPO implementation employs one-sided clipping (Schulman et al., 2017; Shao et al., 2024), sometimes leading to gradient instability when processing tokens with large policy ratio and negative advantage. To address this issue, we implement additional clipping that abandoned this case in the loss function, which effectively regulates the importance sampling magnitude and mitigates noise propagation. KL Divergence Optimization. Due to the similar gradient instability issue, we reformulate the KL divergence term through theoretical analysis of the variance-bias trade-off to further stabilize gradient behavior, resulting in 𝔻𝐾𝐿(𝜃) = 𝔼𝑡 [SG(𝜋𝜃(𝑎𝑡 𝑠𝑡) 𝜋ref(𝑎𝑡 𝑠𝑡)) log 𝜋𝜃(𝑎𝑡 𝑠𝑡)], where SG() denotes the stop-gradient operator. This formulation maintains policy consistency while reducing gradient variance. Balanced Advantage Estimation. We also ensure equitable reward contributions between positive and negative examples, which proves particularly effective in scenarios with skewed distributions. This approach maintains stable training dynamics by regulating the absolute magnitude of rewards across different example groups. 5.5. Safety Alignment The safety alignment of our model is meticulously addressed throughout both the SFT and RL stages. To strike an optimal balance between the models harmlessness and helpfulness, we employ an approach that encompasses the following key components. MiniMax-01: Scaling Foundation Models with Lightning Attention 5.5.1. Training Data Construction We construct high-quality alignment training data with focus on ensuring data diversity and accuracy. This involves the implementation of several data collection methodologies designed to cover broad spectrum of safety scenarios: Safety-Category Specific Prompts. Leveraging established safety classification standards and insights from safety and domain experts, we generate tailored prompts for specific safety categories. This ensures that the model is exposed to comprehensive set of safety-related scenarios. Real-World User Data Collection. We collect real-world user questions from various web documents to incorporate authentic and diverse safety-related queries into our training data. Prompt Augmentation. We instruct early-version MiniMax-Text-01 to generate additional related prompts based on the collected typical red team attack prompts. This approach aims to expand the diversity of safety scenarios and enhance the robustness of the models safety mechanisms. 5.5.2. Response Generation with Harmless Reward Model To generate safe and appropriate responses, we employ harmless reward model (Bai et al., 2022b) that is developed based on set of detailed safety rules. To prevent the model from producing unreasonable refusals, we carefully integrate principles of helpfulness into the safety rules. This integration plays crucial role in achieving balanced output capability, enabling the model to provide safer responses without compromising its utility to the user. The resulting safety-aligned system demonstrates robust protection against potential misuse while maintaining high performance across intended use cases. 5.6. Training Methodology with Long-Context Adaptation We propose systematic multi-stage training methodology to enhance the models capacity for processing extended contexts, as shown in Tab. 7. This approach is methodically designed to optimize long-sequence handling while maintaining performance efficacy on conventional shorter sequences. The RoPE base frequency is maintained at 10 million throughout the post-training phase to ensure consistency in positional encoding. Stage I: Initial Short-Context Training. The first stage implements SFT with sequences constrained to 8,192 tokens. This foundational phase establishes baseline competency in processing standardlength queries and responses, which constitute the majority of practical applications. We remove the long-context prompts that are longer than 8,192 tokens in this stage. Stage II: Extended Context Training. The second stage implements significant extension of the sequence length to 1,032,192 tokens. This phase incorporates training samples across diverse sequence lengths with 50% long-context prompts, facilitating comprehensive model adaptation to extensive contextual processing. The strategic expansion of the sequence length is fundamental to achieving robust long-context capabilities. Stage III: Short-Context Preference Optimization. In this phase, we revert to 8,192 tokens for sequence length and implement Direct Preference Optimization (DPO). This calibration ensures optimal performance on conventional context sizes while maintaining the previously acquired capabilities. Stage IV: Long-Context Preference Optimization. The fourth stage focuses on reinforcing longcontext processing capabilities through DPO with sequences of 1,032,192 tokens. This phase employs 28 MiniMax-01: Scaling Foundation Models with Lightning Attention training protocols analogous to Stage III with entirely long-context data, adapted for extended sequence lengths. Stage V: Online Reinforcement Learning. The final stage implements short-context Online Reinforcement Learning with sequence length of 8,192 tokens. More details have been outlined in Section 5.4.2. Table 7 Training Recipe for Post-training Alignment. Stage Stage II Stag III Stage IV Stage Sequence Length Epoch Batch Size Max LR Min LR LR Decay 8192 2 128 1e-5 1e-6 Cosine 1032192 2 80 3e-6 3e-6 Constant 8192 1 64 5e-7 5e-8 Cosine 1032192 1 64 5e-7 5e-7 Constant 8192 1 512 1e-6 1e-7 Cosine 5.7. Academic Benchmarks We observe and report open-source shortand long-context benchmarks that highlight our models capabilities across various aspects. Along with the user-oriented evaluations we will discuss in Section 5.8, we show that MiniMax-Text-01 is leading open-source model that achieves top performance in long-context retrieval, understanding, long in-context learning and knowledge-based requests, while performing well in math, reasoning, and code tasks and demonstrating strong usefulness in real-user assistant scenarios. 5.7.1. Core Benchmarks MMLU (Hendrycks et al., 2021a) and MMLU-Pro (Wang et al., 2024b) are widely adopted datasets that assess the extent of models knowledge across broad range of domains. We further observe SimpleQA (Wei et al., 2024), factuality benchmark that challenges the models knowledge boundary, and C-SimpleQA (He et al., 2024b) which is an adapted version of SimpleQA under the Chinese culture. For the observation of reasoning capabilities, we evaluate on GPQA (Rein et al., 2024) for graduate-level knowledge reasoning, and DROP (Dua et al., 2019) for reading comprehension reasoning. We test our models performance on math problem-solving with grade-school-level task GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) that spans from AMC-8 to AIMElevel across 7 subjects. We monitor our models coding capability by observing the Pass@1 rate on HumanEval (Chen et al., 2021) and MBPP Plus (Austin et al., 2021; Liu et al., 2023) datasets. To test the models ability to interpret and execute detailed and nuanced instructions, we evaluate the IFEval (Zhou et al., 2023) benchmark. Furthermore, we observe Arena-Hard-Auto (Li et al., 2024b) that reflects the alignment to human preferences. We adopt greedy decoding and zero-shot chain-of-thought strategy (Wei et al., 2022) in evaluating our instruction-tuned model. We compare with other leading and open-source LLMs, which we evaluate under the same setting, if not reported. We present the performance of MiniMax-Text-01 in Table 8. As shown, MiniMax-Text-01 exhibits remarkable performance across most dimensions. It surpasses all models on C-SimpleQA with its more extensive knowledge boundary under Chinese culture. MiniMax-Text-01 also achieves top-3 performance across MMLU, IFEval, and Arena-Hard, showing its exceptional capability of applying its comprehensive knowledge within given constraints to well satisfy user queries and align with human preferences. Meanwhile, it achieves better MATH pass@1 rate than GPT-4o, Claude-3.5-Sonnet, and Llama-3.1-405B, and exhibits comparable 29 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 8 Performance of MiniMax-Text-01 on core academic benchmarks. Tasks GPT-4o (11-20) Claude-3.5Sonnet (10-22) Gemini-1.5Pro (002) Gemini-2.0Flash (exp) Qwen2.572B-Inst. DeepSeekV3 Llama-3.1405B-Inst. MiniMaxText-01 MMLU MMLU-Pro SimpleQA C-SimpleQA IFEval (avg) Arena-Hard 85.7 74.4 39. 64.6 84.1 92.4 GPQA (diamond) DROP (F1) 46.0 89. GSM8k MATH MBPP + HumanEval 95.6 76.6 76. 90.2 88.3 78.0 28.1 56.8 90. 87.6 65.0 88.8 96.9 74.1 75. 93.7 Evaluated following 0-shot CoT setting. General 86.5 76.4 26. 63.3 88.4 72.7 Reasoning 62.1 89. 86.8 75.8 23.4 59.4 89.4 85. 59.1 89.2 Mathematics 95.2 84.6 75. 86.6 Coding 95.4 83.9 75.9 89. 86.1 71.1 10.3 52.2 87.2 81. 49.0 85.0 95.8 81.8 77.0 86. 88.5 75.9 24.9 64.8 87.3 91. 59.1 91.0 96.7 84.6 78.8 92. 88.6 73.3 23.2 54.7 86.4 63. 50.7 92.5 96.7 73.8 73.0 89. 88.5 75.7 23.7 67.4 89.1 89. 54.4 87.8 94.8 77.4 71.7 86. performance with instructed Qwen2.5-72B on HumanEval. Moreover, MiniMax-Text-01 achieves 54.4 on GPQA Diamond, which exceeds most open-source instruction-tuned LLMs and the latest version of GPT-4o. 5.7.2. Long Benchmarks As previously discussed in the long-context extension part of section 4.2, the NIAH task is kind of simplistic for our model, rendering it insufficient for observing the models optimization progress. Consequently, we shift our evaluation to more challenging tasks. Our current long-context evaluation framework focuses on three primary dimensions: (1) Long-Context Retrieval, (2) Long-Context Understanding, and (3) Long In-Context Learning."
        },
        {
            "title": "5.7.2.1 Long-Context Retrieval",
            "content": "This dimension assesses the models memory capabilities, which serve as the foundation for almost all long-context tasks. In addition to vanilla k-M NIAH (Kamradt, 2023), we construct more challenging variation to assess our Long-Context Retrieval performance, namely Multi-Round NeedlesIn-A-Haystack (MR-NIAH), serving as crucial back up for retrieval tasks in long multi-turn dialogue contexts, revealing the fundamental capabilities for building lifelong companion AI assistants. Similar to Multi-round co-reference resolution (MRCR) (Vodrahalli et al., 2024) which is not open-source, we construct haystacks of MR-NIAH as history dialogues, where user queries are synthetic but explicit requests of event descriptions and creative writing. In the last round, the query requests the model to repeat the response of one of the history requests. The haystacks span from 2K to 1M tokens (up to around 2000 interactions), and each needle request is injected at 25%, 50%, and 75% of the 30 MiniMax-01: Scaling Foundation Models with Lightning Attention conversation, respectively. Each ground truth response contains three core components, and we look at an adjusted recall corr. comp. . We show case illustration in Appendix B.2. Figure 15 illustrates comparison results of MR-NIAH. Our model (MiniMax-Text-01, red line) shows strong performance across wide range of sequence lengths in both English and Chinese evaluations. Compared to competing baselines (e.g., GPT, Claude, and Gemini variants), our model also shows less performance degradation at large input lengths, underscoring its robustness for long-context retrieval tasks. 5.7.2.2 Long-Context Understanding This dimension measures the models longcontext understanding ability which contains logical reasoning skills based on long-context inputs. We utilize two comprehensive longcontext QA datasets, Ruler (Hsieh et al., 2024) and LongBench-V2 (Bai et al., 2024) to evaluate this aspect. Ruler includes 13 different tasks and notably introduces multi-hop tracing and aggregation tasks to evaluate the complex reasoning abilities of models. We test Ruler up to sequence length of 1M tokens. LongBench-V2 encompasses question-answering tasks of varying difficulty levels across multiple context types, including single and multi-document, multi-turn dialogue, code repositories, and long structured data, among others. Following LongBenchV2 (Bai et al., 2024), we consider two test modes: w/o CoT and w/ CoT, and the text lengths are categorized as follows: Short, ranging from 0 to 32K words; Medium, spanning from 32K to 128K words; and Long, covering 128K to 2M words. Figure 15 MR-NIAH in English and Chinese. As Table 9 illustrates, our model exhibits notable strengths in processing Rulers long-context reasoning tasks. While performance at the 64k input level remains competitive with leading models (including GPT-4o and Claude-3.5-Sonnet) with minimal variation, MiniMax-Text-01 establishes distinct advantage beginning at 128k, achieving impressive scores and surpassing all benchmark models. This superiority becomes particularly pronounced in ultra-long-context scenarios (such as 1M), where MiniMax-Text-01 maintains its commanding lead. Moreover, as evident in Table 103, MiniMax-Text-01 exhibits outstanding capabilities in LongBench-V2s long-context reasoning tasks. The model achieves state-of-the-art results among all evaluated systems in the w/ CoT setting, while also displaying remarkable effectiveness in scenarios w/o CoT. Overall, MiniMax-Text-01 demonstrates exceptional capability in long-context understanding especially reasoning tasks, both with and without CoT reasoning, particularly excelling in scenarios requiring complex reasoning. The exceptional robustness and stability of the model in processing long-context understanding tasks can be attributed to the hybrid architecture with half RoPE and carefully tuned training recipes for both pre-training and alignment, which enhance the models ability to handle long sequences effectively. 3We present the other models performance reported at https://longbench2.github.io/ 31 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 9 Performance comparison of MiniMax-Text-01 on Ruler. Model 4k 8k 16k 32k 64k 128k 256k 512k 1M GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) MiniMax-Text-01 0.970 0.965 0.962 0.960 0.963 0.921 0.960 0.960 0.960 0.961 0.890 0.957 0.960 0.951 0. 0.888 0.950 0.958 0.957 0.954 0.884 0.952 0.938 0.937 0.943 - 0.938 0.917 0.860 0.947 - - 0.916 0.797 0.945 - - 0.861 0.709 0.928 - - 0.850 - 0. Table 10 Performance comparison of MiniMax-Text-01 on LongBench v2. Model Human overall easy hard short medium long 53.7 100.0 25.1 47.2 59. 53.7 w/ CoT GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Deepseek-V3 Qwen2.5-72B-Inst. MiniMax-Text-01 51.4 46.7 - 43.5 56.5 54.2 55.2 - 47.9 66.1 w/o CoT GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Deepseek-V3 Qwen2.5-72B-Inst. MiniMax-Text-01 50.1 41.0 48.7 42.1 52.9 57.4 46.9 - 42.7 60.9 49.7 41.5 - 40.8 50.5 45.6 37.3 - 41.8 47.9 59.6 53.9 - 48.9 61. 53.3 46.1 - 45.6 58.9 48.6 41.9 - 40.9 56.7 52.4 38.6 - 38.1 52.6 43.5 44.4 - 39.8 47.2 40.2 37.0 - 44.4 43."
        },
        {
            "title": "5.7.2.3 Long In-Context Learning",
            "content": "This dimension evaluates the models ability to learn from context, core area of research in lifelong learning. We benchmark our Long In-Context Learning capability with the MTOB (Machine Translation from One Book) (Tanzer et al., 2024) dataset. The task requires model to translate between English and Kalamang, language that is very limited in open data and thus within the training corpus, and the LLM is expected to learn the language only from parts of grammar book and 375 translation examples, all given in the context for each translation query (Appendix B.1). The context length is 81K tokens under half-book setting and 133K tokens under total-book setting. We present our results in Table 11. Figure 16 Changes of eng kalam (ChrF) during the whole long-context extension training process. We carefully examined the pre-training data and found that only very small amount of data contains Kalamang-related content. As result, the eng kalam (ChrF) score of our model is the lowest in the no-context scenario, while other models we compared with likely have had their pre-train or post-train data enhanced with relevant Kalamang data. As well as the delta half and full book metrics, our model surpasses all models in terms of the eng kalam (ChrF) metric. And our model also has comparable performance with other models on kalam eng (BLEURT) metric. 32 MiniMax-01: Scaling Foundation Models with Lightning Attention In the course of long-context extension, as described in section 4.2, we observed gradual enhancement in In-Context Learning ability, as indicated by MTOB, illustrated in Figure 16. While we have explored some remarkable works(Agarwal et al., 2024; Dong et al., 2024) specifically aimed at improving In-Context Learning capabilities, we believe that such ability should merely be one aspect of the reasoning capabilities of long-context models. Therefore, we plan to conduct in-depth research on long-context data quality and scale from more fundamental perspective to further enhance the long-context reasoning capabilities of our model. Table 11 Performance comparison of MiniMax-Text-01 on MTOB. Context Type no context half book full book Δ half book Δ full book GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) Qwen-Long MiniMax-Text-01 eng kalam (ChrF) 54.30 9.90 20.22 53.62 16.79 53.68 12.20 49.50 16.55 48.48 6.0 51.74 kalam eng (BLEURT) - 55.65 57.90 53.30 45.94 51.60 GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) Qwen-Long MiniMax-Text-01 33.20 31.42 32.02 33.80 30.13 33.65 58.30 59.70 61.52 57.50 53.14 57.10 - 62.30 63.09 57.00 32.15 58. 44.40 33.39 36.89 37.30 31.92 45.7 25.10 28.28 29.50 23.70 23.01 23.45 - 35.42 41.11 41.10 29.39 45.6 - 30.88 31.07 23.20 2.02 24.35 5.8. User-in-the-loop While achieving top performance on the core open-source benchmarks, we realize that academic evaluations lack an understanding of real-world user interactions. Hence, we also focus on monitoring and improving user experience through our Hailuo AI 4 by incorporating user-in-the-loop evaluations based on real-world cases and adapting tools for better usability and performance in practical applications. 5.8.1. In-House Evaluations We maintain series of in-house evaluations that include: (1) automatic assessments of General Assistant capabilities, Knowledge Q&A, Creative Writing, Hard Capability, Instruction Following, Coding, Safety, and Long Context, and (2) expert human evaluations. Its worth noting that since our test queries are primarily derived from Hailuo AI user interactions, significant portion of our in-house samples are in Mandarin and deeply rooted in Chinese cultural contexts. Our results indicate notable discrepancy between performance on academic benchmarks and actual user experience, where leading open-source and commercial models can underperform when used as interactive assistants. We show in Table 12 5 that, through our dedicated efforts, MiniMaxText-01 is able to handle these situations quite well. In general, our model outperforms other models 4https://www.hailuo.ai/ 5We omit scores for in-applicable models. 33 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 12 Performance comparison of MiniMax-Text-01 on in-house benchmarks. General Assistant Hard Capability Creative Writing Knowledge Q&A Instruction Following Coding Safety Long Context GPT-4o (11-20) GPT-4o (08-06) GPT-4o (05-13) Claude-3.5-Sonnet (10-22) Claude-3.5-Sonnet (06-20) Gemini-2.0-Flash (exp) Qwen2.5-72B-Inst. DeepSeek-V3 Llama-3.1-405B-Inst. MiniMax-Text70.9 63.5 67.7 66.8 60.5 70. 66.4 66.8 53.3 73.9 73.5 62. 63.3 68.3 67.4 61.8 66.1 68. - 64.8 70.3 66 58.3 54. 51.0 70.0 61.7 64.6 63.6 81. 69.2 68.0 69.6 52.0 51.8 75. 68.9 77.0 46.0 78.6 50.4 49. 49.6 61.5 64.4 39.9 34.1 51. 50.3 46.3 94.0 93.6 93.2 94. 93.6 86.5 93.9 94.0 87.6 90. 85.4 79.7 79.7 92.9 95.0 66. - 74.9 70.7 90.9 86.2 58. 77.2 47.1 47.1 81.9 81.5 77. 60.3 93.8 in common Assistant scenarios, particularly when compared to open-source counterparts. This superiority is most evident in our Creative Writing (Appendix B.5, B.7, B.6) and Knowledge Q&A collections, where it aligns more closely with user intentions than other models, delivering accurate and detailed responses to wide range of queries. In productivity scenarios that require Long Context (Appendix B.3), such as document translation, summarization, and analysis, our model demonstrates high proficiency and reliability. Moreover, we prioritize the safety of our model, as it achieves top-tier performance on our established in-house Safety benchmarks. Meanwhile, we are agile in gathering and updating complex productivity scenarios with multilevel instruction following requests at which our model fails and current LLMs cannot master, constructing our Harder Capability and Instruction Following in-house evaluations. While leading LLMs tend to underperform in these sets, these requests reflect our models limitations when given multi-level instructions, which stems primarily from insufficient training data for specific instruction types. Moving forward, we are committed to substantially expanding our training dataset with high-quality, targeted content to address these gaps and improve model capabilities. 5.8.2. Search in Hailuo AI During user interaction case studies, we find models capability to utilize search tools can compensate for the limited knowledge boundary by accessing real-time, extensive, and precise information from the web. To maximize the models benefits from search while minimizing additional performance degradation, we first carefully pre-define the scope of search scenarios, which cover approximately 30 40% of user queries, including but not limited to precision-demanding, domain-specific, and time-sensitive requests. Meanwhile, to ensure seamless conversation experience, we define the system as invoking tools directly through special tokens, which avoid the complexity of multi-step planning (Chen et al., 2024b) or chain-of-thought reasoning6 that might disrupt the natural flow of the interactions. We create SFT datasets comprising search and non-search decisions across diverse domains, while carefully controlling for other interaction features unrelated to search decisions, such as conversation length, to maintain uniform data distribution across each dimension and prevent overfitting. Importantly, we employ the corresponding reward model of each sample to ensure response quality, failing at which would introduce suboptimal samples into the training 6https://docs.anthropic.com/en/docs/build-with-claude/tool-use 34 MiniMax-01: Scaling Foundation Models with Lightning Attention data, potentially affecting the models fundamental capabilities. The search decision boundary was calibrated to align with the models knowledge boundaries, discarding samples that our model already masters from the search corpus, such as general Chinese knowledge Q&A. After careful assessments by human evaluation experts, we conclude that our models use of the search tool extensively improved user experience, landing at performance leap from 58% to 71.5% on our out-of-domain Hailuo AI end-to-end evaluation (Appendix B.9). Since we are unsure whether other LLM-based assistants include similar search tools, we refrain from making unfair performance comparisons. 6. Vision-language Model By integrating an image encoder and an image adapter into our MiniMax-Text-01 model, we develop MiniMax-VL-01, which extends the capabilities of the model to visual understanding tasks. To ensure robust visual understanding, we design proprietary dataset and implement multi-stage training strategy, where the newly introduced image encoder and adapter first undergo large-scale visual pre-training, followed by comprehensive fine-tuning of the entire pipeline. In the following section, we begin with comprehensive description of the dataset used for training our image encoder and vision-language model. Subsequently, we provide an in-depth overview of the model architecture, followed by an exposition of our four-stage training regimen. We conclude the section by presenting our benchmark results. 6.1. Multimodal Data 6.1.1. Caption Data To pre-train the vision encoder, we curate substantial image-caption dataset by aggregating and filtering data from internet sources. Our Vision Transformer (ViT) is trained using 694 million unique image-caption pairs. To enhance data quality, we acquire refined captions for 180 million images within these pairs. During the training process, we employ an augmentation strategy by randomly sampling raw and refined captions with equal probability (𝑝 = 0.5). 6.1.2. Description Data In existing vision-language models, the utility of descriptive imagery for model training has been well-documented (Li et al., 2024a, 2022, 2023; Schuhmann et al., 2021). To further explore this avenue, we have compiled dataset consisting of 100 million images sourced from open resources such as Common Crawl. Each image in this dataset is paired with fine-grained description, which is initially synthesized by caption model and subsequently refined through humans. On average, these descriptions comprise approximately 300 text tokens per image. Description data serves as robust resource for modal alignment and enhancing understanding in further training. 6.1.3. Instruction Data To train MiniMax-VL-01, we construct comprehensive and diverse instruction-based dataset by synthesizing an extensive range of question-answer (QA) pairs involving visual inputs. These QA pairs are meticulously designed to cover wide array of image-related tasks, such as text extraction, object localization, and geometry problem solving. The dataset generation process prioritizes both diversity and realism, ensuring that the instructions capture varying degrees of complexity and linguistic styles. During training, we apply an augmentation strategy by randomly sampling different types of QA prompts with balanced probabilities, thereby enabling the model to generalize effectively across 35 MiniMax-01: Scaling Foundation Models with Lightning Attention diverse instructional formats and interaction patterns. 6.1.4. Data Distribution To demonstrate the diversity of our VLM data, we uniformly sample 1 million imageinstruction pairs from the instruction data and use another VLM to assign concise tag (e.g., object localization) that represents the primary capability required for each pair. This analysis yielded around 50,000 unique tags, and the top 2,817 tags appeared more than 10 times. The distribution of these prominent tags is visualized in Figure 17, where we further group these top tags into 14 major categories. 6.2. Architecture 6.2.1. Overall Architecture Our MiniMax-VL-01 architecture adheres to the ViT-MLP-LLM paradigm, which has been widely embraced in numerous multimodal large language models (MLLMs). The architecture consists of three main components: Vision Transformer (ViT) with 303 million parameters for visual encoding, two-layer MLP projector initialized randomly for image adaptation, and the MiniMax-Text-01 model serving as the foundational large language model (LLM). Figure 17 Visualization of top tags of sampled instruction data. The category and percentage for each group of clustered tags are displayed in the inner layer, only top-10 tags of each group are displayed for clarity. We implement dynamic resolution strategy by resizing the input image according to predefined grid configuration list, ranging from 336336 to 20162016, while maintaining standard thumbnail at resolution of 336 336. The resized images are subsequently partitioned into non-overlapping patches, each measuring 336 336. Both the image patches and the thumbnail are independently encoded, and their encoded features are concatenated to construct comprehensive image feature representation. In contrast to traditional approaches that rely on pooling or other downsampling techniques to compress feature representations, our model leverages its powerful capacity for processing long sequences, allowing for the direct utilization of raw high-dimensional features during training. This strategy mitigates potential information loss and substantially improves the models adaptability to multi-scale inputs. Moreover, by projecting both image patches and thumbnails into unified feature space, our method significantly enhances the models robustness and representational expressiveness when handling diverse and complex visual inputs. 6.2.2. Vision Encoder We employ lightweight ViT-L/14 (Dosovitskiy et al., 2021) as the foundational structure for our vision encoder and train it from scratch. Following standard pipeline, the input image tensor is initially processed through convolutional layer to extract discrete patches, to which absolute 36 MiniMax-01: Scaling Foundation Models with Lightning Attention positional embeddings are subsequently appended. The resulting tensors are then passed through series of multi-head residual attention blocks. This architecture is particularly effective in capturing intricate visual details and the complex interrelationships within images. We utilize contrastive learning to enhance the alignment between corresponding image-caption pairs while diminishing the alignment between non-corresponding pairs. Specifically, we follow the approach introduced in CoCa (Yu et al., 2022), which augments image-text contrastive learning with an additional decoder and image-text cross-attention mechanisms. The network is jointly optimized using combination of contrastive loss and cross-entropy loss. Our ViT-L/14 model is initially trained at resolution of 224 224 for 37 billion image-caption pairs and subsequently fine-tuned at 336 336 for 1.2 billion pairs. For both resolutions, the captions are truncated to 76 tokens. Our ViT-L/14 encoder achieves zero-shot classification accuracy of 80.55% at 336 336 resolution on the ImageNet-1K dataset. 6.3. Training Recipes We employ four-stage training strategy to enable the model to progressively develop comprehensive multimodal understanding capabilities while retaining its language understanding skills. Additionally, the models question-answering and instruction-following abilities, as well as its alignment with human preferences, are methodically refined throughout these stages. Stage I: Modality alignment. In this stage, our primary objective is to achieve alignment between visual and text tokens by enabling the model to accurately generate appropriate captions for given images. To this end, we update the weights of both the image adapter and the vision encoder to optimize their performance in this multimodal task. During this phase, we utilize total of 80 billion tokens sampled from our image description dataset. Empirically, we have found that increasing the image resolution does not yield improvements in downstream task accuracy. Therefore, all images are processed at fixed resolution of 336 336 to reduce computational costs. Stage II: Enhancement of Vision Understanding. This stage can be regarded as standard instruction tuning phase, during which all model parameters are open to updates. The primary goal is to align the models output with human instructions and enhance its ability to perform diverse range of vision understanding tasks. To achieve this, the model is trained using 420 billion multimodal tokens sampled from our instruction datasets, combined with MiniMax-Text-01 post-training data in ratio of 20:1. This approach ensures that the language modeling capability is maintained while the model acquires new multimodal capabilities. Stage III: Enhancement of User Experience. This stage is designed to further enhance the models capabilities in real-world scenarios and when handling challenging user inputs. We curate sophisticated multimodal data using images sourced from applications that people commonly interact with. Conversations are meticulously labeled to emulate authentic user input and to ensure the provision of accurate, helpful, and diverse responses across multiple conversational turns. The data construction for this stage is guided by an independent human-labeled test set that prioritizes not only accuracy but also the overall quality in terms of user experience. The resulting dataset comprises 44.8 billion multimodal tokens and is trained for one epoch. Stage IV: Enhancement of Preference. In the final stage, we utilize Direct Preference Optimization (DPO) to further enhance model performance and user experience. We construct training dataset consisting of 40,000 image-text pairs through the following process: Prompt Selection. Prompts are curated from both instruction data and real user interaction data. These prompts are selected to cover wide range of general scenarios and to specifically address 37 MiniMax-01: Scaling Foundation Models with Lightning Attention persistent issues identified after Stage III, such as occasional repetitive outputs in complex OCR scenarios. Response Generation. We employ diverse strategies, including: generating multiple candidate responses by varying sampling temperature parameters; creating response variants through image weakening in specific scenarios; and using MiniMax-Text-01 to deliberately introduce hallucinations or errors into high-quality responses to generate contrastive samples in specific scenarios. Reward Assignment. Large language models, particularly MiniMax-Text-01, are utilized as evaluators in this stage. Multi-dimensional evaluation criteria are designed to enable systematic and comprehensive assessment of the relationships among prompts, ground truth answers, and generated responses. Pair Construction. Based on the evaluation results, we select the highest-scoring responses as positive samples and the lowest-scoring ones as negative samples, while discarding pairs with insignificant score differences. In addition to incorporating image-text pairs, we also include significant proportion of pure text pairs, as elaborated in Section 5.4.1. It is noteworthy that when Direct Preference Optimization (DPO) is applied to highly capable foundation models, there is propensity for overfitting. To counteract this issue, we adopt an early stopping strategy, which involves terminating the training process prior to the completion of full epoch. This approach is designed to preserve the models generalization capabilities. By following this multi-stage training strategy, we ensure that our model not only demonstrates proficiency in understanding and generating high-quality text but also aligns with human values and safety standards. This comprehensive approach to training allows us to strike balance between model performance and ethical considerations, thereby producing model that is both effective and responsible. 6.4. Benchmarks To assess the performance of our vision-language model, we maintain diverse set of benchmarks, including MMMU (Yue et al., 2024a), MMMU-Pro (Yue et al., 2024b), ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), OCRBench (Liu et al., 2024b), AI2D (Kembhavi et al., 2016), MathVista (Lu et al., 2023), OlympiadBench (He et al., 2024a), MMLongBench-Doc (Ma et al., 2024), MEGA-Bench (Chen et al., 2024a) and an in-house benchmark. These benchmarks help evaluate the models abilities in various areas, including knowledge, visual reasoning, mathematics, science, long context handling, and user experience. We detail our evaluation configuration for each benchmark in Appendix D. As shown in Table 13, MiniMax-VL-01 achieves competitive performance across various vision-language tasks, demonstrating the following key strengths and limitations: Common Downstream Tasks. In standard vision-language downstream tasks, MiniMax-VL-01 exhibits performance on par with GPT-4o, particularly excelling in visual question answering. This strong performance is attributed to its extensive multi-stage training process, enabling the model to effectively understand and reason across visual and textual inputs. However, MiniMax-VL-01 still struggles with advanced mathematical reasoning tasks, as assessed by OlympiadBench (He et al., 2024a). Long Context. We assess MiniMax-VL-01s capability for long-context comprehension and retrieval using MMLongBench-Doc (Ma et al., 2024). The results show that our model outperforms most counterparts, except GPT-4o-11-20. Despite its strong performance overall, MiniMax-VL-01 demonstrates noticeable gap in both single-page (acc: 47.3%) and cross-page (acc: 28.4%) subsets. 38 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 13 Performance of MiniMax-VL-01 on academic and in-house benchmarks. Tasks GPT-4o (11-20) Claude-3.5Sonnet (10-22) Gemini-1.5Pro (002) Gemini-2.0Flash (exp) Qwen2-VL72B-Inst. InternVL 2.5-78B LLama3.2-90B MiniMaxVL-01 MMMU val+dev MMMU-Pro full ChartQA relaxed DocVQA OCRBench AI2D MathVista testmini OlympiadBenchfull 63. 54.5 88.1 91.1 806 83.1 62. 25.2 72.0 54.7 90.8 94.2 82.0 65.4 28.4 Knowledge 68.4 50. Visual Q&A 88.7 91.5 800 70.6 57. 88.3 92.9 846 Mathematics & Sciences 80.9 70. 32.1 85.1 73.1 46.1 Long Context 64. 43.2 91.2 97.1 856 84.4 69. 21.9 66.5 47.3 91.5 96.1 86.8 68.4 25.1 62.1 36.0 85. 90.1 805 78.9 57.3 19.3 68. 52.7 91.7 96.4 865 83.3 68. 24.2 M-LongDocacc 41.4 31.4 26.2 31. 11.6 19.7 13.9 32.5 MEGA-Benchmacro 49. 51.4 45.9 53.9 46.8 45.3 19. 47.4 Comprehensive In-house Benchmark 62.3 47.0 49. 72.1 40.6 34.8 13.6 56.6 Evaluated following 0-shot CoT setting. User Experience Comprehensive Benchmark. On the recently introduced MEGA-Bench (Chen et al., 2024a), realistic and comprehensive evaluation suite, MiniMax-VL-01 shows competitive overall capabilities, surpassing existing open-source vision LLMs. While it excels in diverse sub-tasks such as knowledge and coding, the model faces challenges in more complex tasks, including planning and metric assessments. In-house User Experience Benchmark. While academic benchmarks often focus on problemsolving, they frequently fail to capture the nuances of real-world user interactions with models. To bridge this gap, we develop an in-house benchmark comprising 90 diverse image-related tasks, each designed with tailored and challenging instructions. The images and instructions in the benchmark are strictly deduplicated to not overlap with the training set at any stage. Task relevance is manually verified, with detailed checklist annotated for each sample to ensure precise evaluation. The final test set consists of 524 meticulously annotated samples in both Chinese and English, but Chinese is primarily used. We illustrate some samples in Appendix C. In win-rate comparison against top-leading vision-language model, our model outperforms all open-source models and approaches the performance of GPT-4o-11-20 with narrow margin. 7. Conclusion and Future work In this report, we present MiniMax-Text-01 and MiniMax-VL-01, two novel models developed entirely from the ground up. These models demonstrate top-tier performance across standard benchmarks, particularly excelling in long-context processing with the ability to handle context windows of up to 4 million tokens. Our research findings challenge the prevailing assumption that state-of-the-art MiniMax-01: Scaling Foundation Models with Lightning Attention language models must be built upon traditional attention mechanisms. By strategically integrating linear attention with optimized hardware utilization and carefully designing training recipes, we have successfully expanded the context window by an order of magnitude. This breakthrough not only enhances the efficiency and scalability of LLMs but also paves the way for future models to support even longer context windows and facilitate the development of more sophisticated AI agents. To promote collaboration and advancement in the field, we have made our model publicly available at https://github.com/MiniMax-AI. For general use and evaluation, we provide Chatbot with online search capabilities (https://www.hailuo.ai/) and the online API (https://intl.minimaxi.com). We are committed to keeping this series open source and will release updates as we develop improved models. While MiniMax-Text-01 and MiniMax-VL-01 show strong performance in general language and vision-language tasks, we acknowledge several limitations that necessitate further exploration: 1. Long-Context Evaluation: Current evaluation datasets for long-context retrieval tasks are primarily designed for artificial or simplified scenarios, and the assessment of long-text reasoning capabilities remains limited in practical applications such as document analysis. We plan to enhance long-context retrieval in more realistic settings and expand the evaluation of longcontext reasoning across wider array of tasks. 2. Model Architecture: The model currently retains 1/8 component with vanilla softmax attention. We are investigating more efficient architectures that can eliminate softmax attention entirely, potentially enabling unlimited context windows without computational overhead. 3. Complex Programming Tasks: The models performance on advanced programming tasks is to be improved, as the coding dataset in our pre-training stage is still limited at the moment. We are continuously improving training data selection and refining continue training procedures to address these limitations in the next model version."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, 2023. Anthropic. Introducing claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018. 40 MiniMax-01: Scaling Foundation Models with Lightning Attention Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022b. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. Andrei Broder. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pages 2129. IEEE, 1997. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, et al. MEGA-Bench: Scaling multimodal evaluation to over 500 real-world tasks. arXiv preprint arXiv:2410.10563, 2024a. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, and Feng Zhao. MindSearch: Mimicking human minds elicits deep ai searcher. arXiv preprint arXiv:2407.20183, 2024b. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International Conference on Machine Learning (ICML), pages 40574086. PMLR, 2022. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29242936, 2019. 41 MiniMax-01: Scaling Foundation Models with Lightning Attention Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=mZn2Xyh9Ec. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35: 1634416359, 2022. Alexandre de Brébisson and Pascal Vincent. cheap linear attention mechanism with fast lookups and fixed-size representations. arXiv preprint arXiv:1609.05866, 2016. DeepSeek-AI. DeepSeek-V3 technical report, 2024. URL https://arxiv.org/abs/2412.19437. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, et al. survey on in-context learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 11071128, 2024. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=YicbFdNTTy. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23682378, 2019. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. Zamba: compact 7b SSM hybrid model. arXiv preprint arXiv:2405.16712, 2024. 42 MiniMax-01: Scaling Foundation Models with Lightning Attention Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural In Proceedings of the thirteenth international conference on artificial intelligence and networks. statistics, pages 249256. JMLR Workshop and Conference Proceedings, 2010. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id= tEYskw1VY2. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. OlympiadBench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024a. Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, et al. Chinese simpleQA: chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024b. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b. URL https://openreview.net/forum?id=7Bywt2mQsCe. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487, 2022. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, pages 3001630030, 2022. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. RULER: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International conference on machine learning, pages 90999117. PMLR, 2022. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o system card. arXiv preprint arXiv:2410.21276, 2024. 43 MiniMax-01: Scaling Foundation Models with Lightning Attention Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7B. arXiv preprint arXiv:2310.06825, 2023. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, 2017. G. Kamradt. Llmtest_needleinahaystack, 2023. URL https://github.com/gkamradt/LLMTest_ NeedleInAHaystack. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7: 453466, 2019. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84248445, 2022. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb. Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024a. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-Hard and BenchBuilder pipeline. arXiv preprint arXiv:2406.11939, 2024b. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=WsRHpHH4s0. 44 MiniMax-01: Scaling Foundation Models with Lightning Attention Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by ChatGPT really correct? rigorous evaluation of large language models for code generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 2155821572, 2023. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. OCRBench: On the hidden mystery of OCR in large multimodal models. Science China Information Sciences, 67(12):220102, 2024b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. MMLongBench-Doc: Benchmarking long-context document understanding with visualizations. arXiv preprint arXiv:2407.01523, 2024. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. DocVQA: dataset for VQA on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training. arXiv preprint arXiv:1812.06162, 2018. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, 2018. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36:5035850376, 2023. NVIDIA. Transformer engine, 2023. URL https://github.com/NVIDIA/TransformerEngine. Jupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Reuse, dont retrain: recipe for continued pretraining of language models. arXiv preprint arXiv:2407.07263, 2024. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset In Thirty-seventh Confor Falcon LLM: Outperforming curated corpora with web data only. ference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=kM5eGcdCzq. Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The FineWeb datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum?id=n6SCkn2QaG. 45 MiniMax-01: Scaling Foundation Models with Lightning Attention Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 70257041, 2022a. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosFormer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022b. URL https://openreview.net/forum?id=Bl8CQrx2Up4. Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id= IxmWsm4xrua. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 3320233221, 2023b. Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, and Yiran Zhong. You only scan once: Efficient multi-dimension sequential modeling with lightnet. arXiv preprint arXiv:2405.21022, 2024a. Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024b. Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Various lengths, constant speed: Efficient language modeling with lightning attention. In International conference on machine learning, pages 4151741535. PMLR, 2024c. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. HGRN2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024d. Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training Gopher. arXiv preprint arXiv:2112.11446, 2021. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher Manning, and Chelsea Finn. Direct preference optimization: your language model is secretly reward model. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 5372853741, 2023. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id= Ti67584b98. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. arXiv preprint arXiv:2406.07522, 2024. MiniMax-01: Scaling Foundation Models with Lightning Attention Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial Winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 44634473, 2019. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open dataset of CLIP-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. SCROLLS: Standardized comparison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1200712021, 2022. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, and Yiran Zhong. Scaling laws for linear complexity language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1637716426, 2024. Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair encoding: text compression scheme that accelerates pattern matching. Technical Report DOI-TR-161, Department of Informatics, Kyushu University, 1999. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, and Yiran Zhong. Linear attention sequence parallelism. arXiv preprint arXiv:2404.02882, 2024. Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, and Luke Melas-Kyriazi. benchmark for learning to translate new language from one grammar book. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=tbVWug9f2h. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024a. Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et al. Jamba-1.5: Hybrid Transformer-Mamba models at scale. arXiv preprint arXiv:2408.12570, 2024b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 47 MiniMax-01: Scaling Foundation Models with Lightning Attention Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, et al. Michelangelo: Long context evaluations beyond haystacks via latent structure queries. arXiv preprint arXiv:2409.12640, 2024. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. DeepNet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024a. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek Wong, and Lidia Chao. Learning deep transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 18101822, 2019. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https://openreview.net/forum?id=y10DM6R2r3. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. CoCa: Contrastive captioners are image-text foundation models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=Ee277P3AYC. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024a. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. MMMU-Pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big Bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine In Proceedings of the 57th Annual Meeting of the Association for really finish your sentence? Computational Linguistics, pages 47914800, 2019. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. MiniMax-01: Scaling Foundation Models with Lightning Attention A. Contributors The contributors to the report are listed in alphabetical order as follows: Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu 49 MiniMax-01: Scaling Foundation Models with Lightning Attention B. MiniMax-Text-01 Case Demonstrations We show our models performance under real-world user interactions. To protect the privacy of our users, all user requests shown below are written by our human evaluators, imitating the way users interact with the model, if not from open-source benchmarks. B.1. Learning New Language From Long Context Our prompt for applying MTOB follows that of Gemini-1.5 (Team et al., 2024a), detailed as follows."
        },
        {
            "title": "MTOB Case",
            "content": "User Request ( Instruction + Grammar book + Word List + Parallel sentences ) You are tasked with translating the following sentence from Kalamang to English: \"Sontum kawirten hari minggu gerejao nasumbiyan.\" You will be given field linguistics grammar book, bilingual word list to aid you. Here is the book, \"A grammar of Kalamang\": START OF GRAMMAR BOOK { grammar book } END OF GRAMMAR BOOK The grammar book is now over. Remember that you are tasked with translating the following sentence from Kalamang to English Sontum kawirten hari minggu gerejao nasumbiyan. Now here is the bilingual word list: START OF WORD LIST { word list } END OF WORD LIST The bilingual word list is now over. Remember that you are tasked with translating the following sentence from Kalamang to English Sontum kawirten hari minggu gerejao nasumbiyan. You will use the same style as the parallel sentences immediately below. Now here is the collection of parallel sentences: START OF PARALLEL SENTENCES english: Sakina is pouching guavas. kalamang: Sakina sarimara lawat. english: There are many lusi muaun trees at Sar, but they are dry. kalamang: Lusi muaun me Sarko reidak ma ma he sasa. . . . { total of 375 examples of parallel sentences } . . . END OF PARALLEL SENTENCES The collection of parallel sentences is now over. Now translate the following sentence from Kalamang to English, using the style from the parallel sentences immediately above. Translate: \"Sontum kawirten hari minggu gerejao nasumbiyan\". understand that you may not be familiar enough with Kalamang to make confident translation, but please give your best guess. Respond with only the translation and no other text."
        },
        {
            "title": "Golden Answer",
            "content": "MiniMax-Text-"
        },
        {
            "title": "Suboptimal",
            "content": "Christians worship at the church on Sunday. The Christians pray at church on Sunday. The person is Christian on Sunday. As shown, MiniMax-Text-01 can learn from the given grammar book, word list and parallel sentences, where under-performing models response has semantic errors. MiniMax-01: Scaling Foundation Models with Lightning Attention B.2. Memorizing Long History Dialogues We present an example of how our MR-NIAH samples are designed, where the model is asked to precisley retrieve one of the history interactions with the user from up to 1889 history interactions for the English benchmark and 2053 history interactions for the Chinese benchmark. These history queries were created by Mandarin-speaking human evaluators with help from previous version of our model, where each query is translated into English, and we obtain the assistants responses from this model. We combine these interactions as history dialogues, and insert each needle conversation into corresponding positions detailed in Section 5.7.2.1. MR-NIAH Case User Request ( Haysack dialogues + Target + Haysack dialogues + Query ) User: Please continue writing the follow-up content for this poem Drunkenly lying under the moon in the clouds, laughing at the flowers in the water. Assistant: Certainly! The opening line you provided, Drunkenly lying under the moon in the clouds, laughing at the flowers in the water,ëvokes sense of whimsical detachment and romantic embrace of natures beauty. Heres continuation of the poem that builds on this theme: *Drunkenly lying under the moon in the clouds, Laughing at the flowers in the water, find solace in the nights gentle shroud, Where dreams and reality softly hover.** The stars above, like scattered coins, Glimmer with secrets of the ancient skies, . . . { many haysack dialogues } User: Write poem about penguins. Assistant: On the peak of the Antarctic iceberg, Penguins dance with tiny wings. Wearing black and white tuxedos, stumbling steps, They smile at the severe frost in the cold wind. { many haysack dialogues } User: Write poem about penguins. Assistant: Under the sun, penguins line up, Like guards, protecting their homeland. In their eyes, warm light hides, Exceptionally bright in the polar tranquility. { many haysack dialogues } User: Please repeat the first poem you wrote about penguins."
        },
        {
            "title": "Golden Answer",
            "content": "MiniMax-Text-"
        },
        {
            "title": "Suboptimal",
            "content": "On the peak of the Antarctic iceberg, Penguins dance with tiny wings. Wearing black and white tuxedos, stumbling steps, They smile at the severe frost in the cold wind. On the peak of the Antarctic iceberg, Penguins dance with tiny wings. Wearing black and white tuxedos, stumbling steps, They smile at the severe frost in the cold wind. Under the sun, penguins line up, Like guards, protecting their homeland. In their eyes, warm light hides, Exceptionally bright in the polar tranquility. We will release the data and code of this benchmark to the community. 51 MiniMax-01: Scaling Foundation Models with Lightning Attention B.3. Summarizing Long Papers with Figures When dumped PDF paper and instructed to add figures from the paper, Hailuo AI is able to summarize or analyze the given long text and respond with key figures cited. User Request ( Query + PDF ) please summary this paper for me in english, best to add key figures of this paper in your reply at proper positions. Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction MiniMax-TextSummary of the Paper: \"Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction\" 1. Introduction and Motivation The rapid advancement of large language models (LLMs) has significantly enhanced their capabilities but has also raised concerns about their alignment with human values and intentions. Current alignment strategies, such as Supervised Fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), have shown potential but face challenges in balancing deployment complexity and performance across various tasks. Inference-time methods, while lightweight, struggle to precisely distill human values into LLM outputs, especially in long-context generations. This paper introduces novel alignment paradigm called Streaming Distribution Induce Aligner (Stream Aligner), which combines the efficiency of inferencetime methods with the enhanced performance of additional models to achieve dynamic sentence-level correction. 2. Stream Aligner Paradigm Stream Aligner operates as plug-and-play module in the generation pipeline, correcting sentences generated by the upstream model and feeding the corrected suffix back for further generation. This process repeats iteratively until the response is complete, ensuring that every sentence in the output aligns with human preferences. Key Features: Reduced Dependency on Additional Models: By focusing on sentence-level correction, Stream Aligner leverages more of the upstream models performance, reducing reliance on the size and capabilities of additional models. Enhanced Reasoning Abilities: The iterative correction process allows Stream Aligner to correct reasoning errors and add inductions for subsequent steps, thereby improving the models reasoning capabilities. 3. Technical Details Training: Stream Aligner is fine-tuned on sentence-level preference dataset to learn the residuals between preferred and non-preferred responses. The training objective is to minimize the negative log-likelihood loss between the models output and the corrected answer. Inference: During inference, Stream Aligner takes the users query and the prefix as input. The upstream model generates the original answer, which is then corrected by Stream Aligner. Each correction is incorporated into the prefix, and the process repeats until the generation stops or the prefix exceeds the maximum length. 4. Experimental Results The paper evaluates Stream Aligner on three tasks: helpful and harmless QA, math questions, and summary tasks. The results demonstrate significant improvements: Helpfulness and Harmlessness: Aligner-2B achieved and applied 41.2% increase 36.0% increase to Stream Llama2-70B-chat helpfulness harmlessness. in in 6. Comparison to Other Alignment Methods Stream Aligner outperforms other alignment methods such as Supervised Finetuning (SFT) and Direct Preference Optimization (DPO) in terms of accuracy improvements. It also achieves the performance of Aligner-70B using only 2B parameters, showcasing both superior performance and efficiency. 7. Interpretability The paper explores the interpretability of Stream Aligner through representation engineering and activation steering. The results show that Stream Aligner has internalized the correction paradigm as representation, similar to Aligner, but with more layers involved in deciding corrections, retasks. flecting the complexity of mathematical Math Ability: Stream Aligner-8B applied to Llama3-70B-Instruct achieved 3.5% improvement in math ability. 8. Conclusion Reasoning Tasks: The experiments show that the longer the average intervention by Stream Aligner, the higher the accuracy after the intervention, indicating its effectiveness in reasoning-based tasks. 5. Ablation Studies The paper conducts ablation studies to verify the correction capabilities of Stream Aligner under different supervision quantities and generation pipelines: Generation-Correction Frequency: The performance of Stream Aligner increases with the number of generation-correction cycles, demonstrating its ability to enhance the upstream models performance with limited supervision. Generation Methods: The study compares the classic sentence-by-sentence correction pipeline with new continue generation pipeline. While the continue generation pipeline shows excellent performance in math and harmless QA tasks, it is more compute-consuming and converges to similar result as the direct generation pipeline. Stream Aligner is novel alignment paradigm that effectively elicits the latent knowledge of the upstream model while maintaining efficiency and enhanced performance. It achieves significant improvements in helpfulness, harmlessness, and reasoning abilities, making it promising approach for aligning LLMs with human values. Limitations Despite its advantages, Stream Aligner introduces additional computational overhead during inference and requires high-quality training data. Its performance may be limited when dealing with extremely difficult out-of-distribution inputs. This summary provides an overview of the papers key contributions, methodologies, experimental results, and conclusions, highlighting the innovative approach of Stream Aligner in aligning LLMs with human preferences. 52 MiniMax-01: Scaling Foundation Models with Lightning Attention B.4. Translating Long Papers User Request ( Query + PDF ) please help me to translate the whole paper into French. When brain-inspired AI meets AGI.pdf MiniMax-Text-01 Quand lIA inspirée du cerveau rencontre lAGI Lin Zhao a,1, Lu Zhang b,1, Zihao Wu a, Yuzhong Chen c, Haixing Dai a, Xiaowei Yu b, Zhengliang Liu a, Tuo Zhang d, Xintao Hu d, Xi Jiang c, Xiang Li e, Dajiang Zhu b, Dinggang Shen f,g,h, Tianming Liu a,* *School of Computing, The University of Georgia, Athens 30602, USA* *Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington 76019, USA* *MOE Key Laboratory for Neuroinformation, School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu 611731, China* *School of Automation, Northwestern Polytechnical University, Xian 710072, China* *Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston 02115, USA* *School of Biomedical Engineering, ShanghaiTech University, Shanghai 201210, China* *Shanghai United Imaging Intelligence Co., Ltd., Shanghai 200230, China* *Shanghai Clinical Research and Trial Center, Shanghai, 201210, China* Résumé LIntelligence Générale Artificielle (AGI) été un objectif de longue date pour lhumanité, visant à créer des machines capables deffectuer toute tâche intellectuelle que les humains peuvent accomplir. Pour atteindre cet objectif, les chercheurs en AGI sinspirent du cerveau humain et cherchent à reproduire ses principes dans des machines intelligentes. Lintelligence artificielle inspirée du cerveau est un domaine qui émergé de cet effort, combinant des insights de la neuroscience, de la psychologie et de linformatique pour développer des systèmes dIA plus efficaces et puissants. Dans cet article, nous offrons un aperçu complet de lIA inspirée du cerveau du point de vue de lAGI. Nous commençons par les progrès actuels de lIA inspirée du cerveau et de sa connexion étendue avec lAGI. Nous couvrons ensuite les caractéristiques importantes de lintelligence humaine et de lAGI (par exemple, la mise à léchelle, la multimodalité et le raisonnement). Nous discutons des technologies importantes pour atteindre lAGI dans les systèmes dIA actuels, telles que lapprentissage contextuel et le réglage des invites. Nous examinons également lévolution des systèmes AGI à la fois du point de vue algorithmique et infrastructurel. Enfin, nous explorons les limites et lavenir de lAGI. 1. LIA inspirée du cerveau et lAGI Le cerveau humain est largement considéré comme lun des systèmes de traitement de linformation les plus complexes et avancés au monde. Il comprend plus de 86 milliards de neurones, chacun capable de former jusquà 10 000 synapses avec dautres neurones, ce qui résulte en un réseau de connexions exceptionnellement complexe permettant la prolifération de lintelligence. Outre la complexité physiologique, le cerveau humain présente une large gamme de caractéristiques qui contribuent à ses capacités fonctionnelles remarquables. Par exemple, il peut intégrer des données provenant de plusieurs modalités sensorielles, telles que la vision, laudition et le toucher, lui permettant de former une perception cohérente du monde. La capacité du cerveau à effectuer un traitement parallèle est également essentielle pour gérer efficacement plusieurs flux dinformations simultanément. Cela est réalisé via les connexions et les communications en temps réel entre différentes régions du cerveau, bien que le mécanisme ne soit pas entièrement compris. De plus, le cerveau est très adaptable, capable de réorganiser sa structure et sa fonction en réponse aux environnements et expériences changeants. Cette propriété, connue sous le nom de neuroplasticité, permet au cerveau dapprendre et de développer de nouvelles compétences tout au long de la vie. Le cerveau humain est également remarquable pour ses fonctions cognitives de haut niveau, telles que la résolution de problèmes, la prise de décision, la créativité et le raisonnement abstrait, soutenues par le cortex préfrontal, une région du cerveau particulièrement bien développée chez les humains. Créer des systèmes dIntelligence Générale Artificielle (AGI) ayant une intelligence de niveau humain ou même supérieure et capables deffectuer une large gamme de tâches intellectuelles, telles que le raisonnement, la résolution de problèmes et la créativité, est la quête de lhumanité depuis des siècles, remontant au milieu du 20ème siècle. Dans les années 1940, des pionniers comme Alan Turing ont développé des idées précoces sur les machines informatiques et leur potentiel pour simuler la pensée humaine. Depuis lors, chercher à reproduire les principes de lintelligence humaine dans des systèmes artificiels considérablement favorisé le développement de lAGI et les applications correspondantes. Ces principes incluent la structure et la fonction des réseaux de neurones, la plasticité des connexions synaptiques, la dynamique de lactivité neuronale, et plus encore. En 1943, McCulloch et Pitts ont proposé le tout premier modèle mathématique dun neurone artificiel, également connu sous le nom de neurone MCP (McCulloch-Pitts). Inspiré par la théorie de Hebb sur la plasticité synaptique, Frank Rosenblatt conçu le perceptron, une amélioration majeure par rapport au modèle de neurone MCP, et montré que, en assouplissant certaines des règles du MCP, les neurones artificiels pouvaient réellement apprendre à partir des données. Cependant, la recherche sur les réseaux de neurones artificiels stagné jusquà ce que la rétropropagation soit proposée par Werbos en 1975. La rétropropagation été inspirée par la façon dont le cerveau modifie les forces des connexions entre les neurones pour apprendre et améliorer ses performances grâce à la plasticité synaptique. La rétropropagation tente de reproduire ce processus en ajustant les poids (forces synaptiques) entre les neurones dans un réseau de neurones artificiels. Malgré cette proposition précoce, la rétropropagation na pas attiré une attention généralisée jusquaux années 1980, lorsque des chercheurs comme David Rumelhart, Geoffrey Hinton et Ronald Williams ont publié des articles démontrant lefficacité de la rétropropagation pour entraîner les réseaux de neurones. Les réseaux de neurones convolutifs (CNN) sont lun des types de réseaux de neurones les plus utilisés et les plus efficaces pour traiter les informations visuelles. Les CNN sont également inspirés de lorganisation hiérarchique du cortex visuel dans le cerveau, ce qui remonte aux travaux de David Hubel et Torsten Wiesel dans les années 1960. Dans le cortex visuel, les neurones sont disposés en couches, chaque couche traitant les informations visuelles de manière hiérarchique. Lentrée de la rétine est dabord traitée par une couche de cellules simples qui détectent les bords et les orientations, puis transmise à des cellules plus complexes qui reconnaissent des caractéristiques plus complexes telles que les formes et les textures. Leurs travaux ont fourni des insights sur la façon dont le système visuel traite les informations et ont inspiré le développement des CNN qui pourraient reproduire ce processus de traitement hiérarchique. Les mécanismes dattention dans les réseaux de neurones artificiels sont également inspirés de la façon dont le cerveau humain sélectionne sélectivement certains aspects de lentrée sensorielle ou des processus cognitifs, nous permettant de nous concentrer sur les informations importantes tout en filtrant les détails non pertinents. Lattention été étudiée dans les domaines de la psychologie et des neurosciences pendant de nombreuses années, et son application à lintelligence artificielle fait avancer considérablement nos pas vers lAGI. Le modèle Transformer, basé sur le mécanisme dautoattention, est devenu la base de nombreux réseaux de neurones artificiels de pointe tels que BERT et GPT. En adaptant les mécanismes dauto-attention au traitement dimages, le modèle Vision Transformer (ViT) démontré des performances de pointe dans diverses tâches de vision par ordinateur (CV) en représentant limage comme une séquence de patchs. Récemment, de plus en plus de preuves suggèrent que les réseaux de neurones artificiels (ANN) et les réseaux de neurones biologiques (BNN) peuvent partager des principes communs dans loptimisation de larchitecture du réseau. Par exemple, la propriété de petit monde dans les réseaux structurels et fonctionnels du cerveau été largement étudiée dans la littérature. Dans une étude récente, les réseaux de neurones basés sur les graphes aléatoires de Watts-Strogatz (WS) avec des propriétés de petit monde ont démontré des performances compétitives par rapport aux modèles conçus à la main et optimisés par NAS (recherche darchitecture neuronale). De plus, lanalyse posteriori montré que la structure graphique des ANN les plus performants, tels que les CNN et le Perceptron multicouche (MLP), est similaire à celle des vrais BNN, tels que le réseau dans le cortex du macaque. Chen et al. ont proposé une représentation relationnelle unifiée et biologiquement plausible des modèles ViT, trouvant que la performance du modèle était étroitement liée aux mesures du graphe et que le ViT une grande similarité avec les vrais BNN. Zhao et al. ont synchronisé lactivation des ANN et des CNN et ont trouvé que les CNN avec des performances plus élevées sont similaires aux BNN en termes dactivation de la représentation visuelle. Liu et al. ont couplé les neurones artificiels dans le modèle BERT avec les neurones biologiques dans le cerveau humain, et ont trouvé que les neurones artificiels peuvent porter des informations linguistiques/sémantiques significatives et sancrer à leurs signatures de neurones biologiques avec interprétabilité dans un contexte neurolinguistique. Zhou et al. ont traité chaque dimension cachée dans Wav2Vec2.0 comme un neurone artificiel et les ont connectés avec leurs homologues biologiques dans le cerveau humain, suggérant une relation étroite entre les deux domaines en termes dinformations neurolinguistiques. Suivant cette tendance, il un intérêt croissant pour le développement de lintelligence artificielle inspirée du cerveau en sinspirant de certaines connaissances préalables du cerveau humain, telles que lorganisation de la structure et de la fonction du cerveau. Par exemple, Huang et al. ont proposé un réseau de vision antagoniste inspiré du cerveau (BI-AVAN) qui imite le processus de compétition biaisée dans le système visuel humain pour décoder lattention visuelle humaine. Inspiré par lorganisation cœur-périphérie du cerveau humain, Yu et al. ont proposé un modèle de transformateur de vision guidé par le principe cœur-périphérie (CPViT) pour la reconnaissance dimages avec des performances et une interprétabilité améliorées. De même, Zhao et al. ont mis en œuvre le principe cœur-périphérie dans la conception des motifs de câblage du réseau et la sparsification de lopération de convolution. Le CP-CNN proposé guidé par le principe cœur-périphérie démontre lefficacité et la supériorité par rapport aux méthodes basées sur les CNN et ViT. Un autre groupe détudes opté pour les réseaux de neurones à pointes (SNN) qui imitent étroitement le comportement des neurones biologiques dans le cerveau. Par exemple, les SNN ont été utilisés pour cartographier et comprendre les données cérébrales spatio-temporelles, décoder et comprendre lactivité musculaire à partir des signaux délectroencéphalographie, et les interfaces cerveau-machine. LIA inspirée du cerveau également contribué au développement darchitectures matérielles qui imitent la structure et la fonction du cerveau. Le calcul neuromorphique, un domaine détude qui vise à concevoir du matériel informatique qui émule les neurones et les synapses biologiques, également gagné en attention ces dernières années. Les puces neuromorphiques sont conçues pour traiter linformation de manière parallèle et distribuée, de la même manière que le cerveau fonctionne, ce qui peut conduire à des améliorations significatives en termes defficacité et de vitesse par rapport aux architectures informatiques traditionnelles. 53 MiniMax-01: Scaling Foundation Models with Lightning Attention Certaines des puces neuromorphiques, comme la puce TrueNorth dIBM et la puce Loihi dIntel, utilisent des réseaux de neurones à pointes pour traiter linformation dune manière qui est plus proche de la façon dont le cerveau traite linformation. Ces puces ont été utilisées pour une large gamme dapplications, compris la reconnaissance dimages et de la parole, la robotique et les véhicules autonomes. Lavancement du matériel inspiré du cerveau ouvre également la voie à des avancées significatives dans le domaine de lAGI en pavant la voie pour des plateformes matérielles généralisées. Dans lensemble, lIA inspirée du cerveau joue un rôle crucial dans le développement de lAGI  (Fig. 1)  . En sinspirant du cerveau humain, les chercheurs peuvent créer des algorithmes et des architectures mieux adaptés pour gérer des problèmes complexes et réels qui nécessitent un degré élevé de flexibilité et dadaptabilité. Cela est particulièrement important pour lAGI, qui vise à développer des machines capables deffectuer une large gamme de tâches, dapprendre de lexpérience et de généraliser leurs connaissances à de nouvelles situations. Le cerveau humain est lun des systèmes de traitement de linformation les plus complexes connus de nous, et il évolué pendant des millions dannées pour être très efficace et efficace dans la gestion de tâches complexes. En étudiant le cerveau et en développant des systèmes dIA qui imitent son architecture et sa fonction, les chercheurs peuvent créer une AGI plus sophistiquée et adaptable, nous rapprochant de lobjectif ultime de créer des machines qui peuvent égaler ou surpasser lintelligence humaine. En retour, lAGI également le potentiel de bénéficier à lintelligence humaine et de approfondir notre compréhension de lintelligence. À mesure que nous continuons à étudier et à comprendre à la fois lintelligence humaine et lAGI, ces deux systèmes deviendront de plus en plus intriqués, se renforçant et se soutenant mutuellement de manière nouvelle et passionnante. 2. Caractéristiques de lAGI 2.1. Échelle Léchelle des cerveaux varie considérablement dune espèce animale à lautre, allant de quelques milliers de neurones chez les invertébrés simples comme les vers nématodes, à plus de 86 milliards de neurones chez les humains. Par exemple, le cerveau dune mouche à fruits contient environ 100 000 neurones, et le cerveau dune souris contient environ 70 millions de neurones. Pour les primates, le cerveau du macaque environ 1,3 milliard de neurones tandis que le cerveau du chimpanzé environ 6,2 milliards de neurones. Comparé à dautres animaux, le cerveau humain est la structure biologique la plus complexe et la plus sophistiquée connue de la science, contenant plus de 86 milliards de neurones. Léchelle du cerveau, cest-à-dire le nombre de neurones, est souvent corrélée aux capacités cognitives de lanimal et considérée comme un facteur dintelligence. La taille et la complexité des régions du cerveau associées à des fonctions cognitives spécifiques, telles que le langage ou la mémoire, sont souvent directement liées au nombre de neurones quelles contiennent. Nous avons lintention dutiliser les grands modèles de langage (LLM) (voir le tableau 1) comme un moyen possible détudier lAGI inspirée du cerveau, car les LLM sont parmi les premiers modèles à démontrer des performances de niveau humain dans diverses tâches. La relation entre le nombre de neurones et les capacités cognitives est également pertinente pour les LLM tels que GPT-2 et GPT-3. Alors que GPT-2 1,5 milliard de paramètres et été entraîné sur 40 gigabytes de données textuelles, GPT-3 175 milliards de paramètres et été entraîné sur 570 gigabytes de données textuelles. Cette augmentation significative du nombre de paramètres permis à GPT-3 de surpasser GPT-2 sur une gamme de tâches linguistiques, démontrant une augmentation de sa capacité à effectuer des tâches linguistiques complexes. En fait, GPT-3 montré des performances de niveau humain sur plusieurs benchmarks de traitement du langage naturel, tels que la réponse aux questions, la traduction linguistique et les tâches de complétion de texte. Sa taille et sa capacité en traitement du langage naturel en ont fait un outil puissant pour diverses applications, compris les chatbots, la génération de contenu et la traduction linguistique. Cette tendance est similaire à la façon dont les cerveaux plus grands sont associés à des fonctions cognitives plus complexes chez les animaux. À mesure que les LLM continuent de se développer, il est attendu quils deviendront encore plus capables dapprendre de nouveaux skills avec un petit nombre dexemples de formation, similaire à la façon dont les animaux avec des cerveaux plus grands ont des capacités cognitives plus sophistiquées. Cette corrélation suggère que léchelle peut être un facteur crucial dans la réalisation de lAGI. Cependant, il est à noter que le nombre de paramètres seuls ne détermine pas lintelligence dun LLM. La qualité des données de formation, le processus de formation et larchitecture du modèle jouent également des rôles importants dans sa performance. En outre, il est nécessaire de rechercher des moyens qui permettent aux institutions et aux individus à ressources limitées daccéder et de développer lAGI. Certaines solutions possibles incluent la quantification des modèles existants de grande taille, le développement darchitectures efficaces, ou la construction de jeux de données de haute qualité qui facilitent la formation du modèle. 2.2. Multimodalité La capacité du cerveau humain à traiter et intégrer simultanément des informations provenant de plusieurs modalités sensorielles est une réalisation remarquable. Cette caractéristique permet aux individus de comprendre le monde qui les entoure à travers diverses sources dinformation, telles que la vue, le son, le toucher, le goût et lodorat. De plus, le traitement dinformations multimodales permet aux gens de faire des évaluations plus précises et complètes de leur environnement et de communiquer efficacement avec les autres. En conséquence, lapprentissage réussi à partir de plusieurs modalités peut améliorer les capacités cognitives humaines. À mesure que nous nous efforçons de créer des systèmes AGI avancés qui surpassent lintelligence humaine, il est crucial quils soient capables dacquérir et dingérer des connaissances à partir de diverses sources et modalités pour résoudre des tâches qui impliquent nimporte quelle modalité. Par exemple, un AGI devrait être capable dutiliser les connaissances apprises à partir dimages et de la base de connaissances pour répondre aux questions en langage naturel, ainsi que dutiliser les connaissances apprises à partir du texte pour effectuer des tâches visuelles. En fin de compte, toutes les modalités se croisent à travers des concepts universels, tels que le concept quun chien est un chien, indépendamment de la façon dont il est représenté dans différentes modalités  (Fig. 2)  . Pour construire des systèmes dIA multimodaux, une approche prometteuse est dincorporer des signaux de formation provenant de plusieurs modalités dans les LLM. Cela nécessite daligner les représentations internes à travers différentes modalités, permettant au système dIA dintégrer les connaissances de manière transparente. Par exemple, lorsquun système dIA reçoit une image et un texte associé, il doit associer le même objet ou concept entre les modalités. Supposons que lIA voit une image dune voiture avec un texte se référant ont à ses roues. Dans ce cas, lIA doit prêter attention à la partie de limage avec les roues de la voiture lorsquelle traite le texte les mentionnant. LIA doit comprendre que limage des roues de la voiture et le texte se référant à elles décrivent le même objet à travers différentes modalités. systèmes Ces dernières années, les dIA multimodaux expérimenté lalignement du texte/NLP, des images/vision ou de linformation audio dans un espace dencodage pour faciliter la prise de décision multimodale. Lalignement intermodal est essentiel pour diverses tâches, compris la génération texte-image et image-texte, la réponse aux questions visuelles, et la modélisation vidéo-langage. Dans la section suivante, nous fournissons un bref aperçu de ces charges de travail courantes et des modèles de pointe correspondants. 2.2.1. Génération texte-image et imagetexte CLIP, DALL-E, et leur successeur GLIDE, VisualGPT et Diffusion sont parmi les modèles les plus connus qui abordent les descriptions dimages (génération image-texte) et les tâches de génération texte-image. CLIP est une méthode de préentraînement qui entraîne des encodeurs dimages et de texte séparés et apprend à prédire quelles images dans un ensemble de données sont associées à diverses descriptions. Notamment, de manière similaire au neurone Halle Berry chez les humains, CLIP été trouvé pour avoir des \"neurones multimodaux\" qui sactivent lorsquils sont exposés à la fois au texte de létiquette du classificateur et à limage correspondante, indiquant une représentation multimodale fusionnée. DALL-E, en revanche, est une variante de GPT3 avec 13 milliards de paramètres qui prend le texte comme entrée et génère une séquence dimages pour correspondre au texte dentrée. Les images générées sont ensuite classées à laide de CLIP. GLIDE, une évolution de DALL-E, utilise toujours CLIP pour classer les images générées, mais la génération dimages est accomplie à laide dun modèle de diffusion. Stable Diffusion est également basé sur des modèles de diffusion tout en opérant sur lespace latent de puissants auto-encodeurs pré-entraînés et ainsi en utilisant des ressources de calcul limitées tout en maintenant leur qualité et leur flexibilité. Le VisualGPT est lévolution de GPT-2 dun modèle de langage unique à un modèle multimodal avec une unité dactivation qui se réanime elle-même pour produire des activations éparses qui empêchent lécrasement accidentel des connaissances linguistiques. 1. Fig. Le développement de lAGI été largement inspiré par létude de lintelligence humaine (HI). En retour, lAGI le potentiel de bénéficier à lintelligence humaine. Par exemple, les modèles de langage actuels tels que ChatGPT et GPT-4 utilisent lapprentissage par renforcement avec retour humain (RLHF) pour align leur comportement avec les valeurs humaines. À mesure que nous continuons à étudier et à comprendre à la fois lintelligence humaine et lAGI, ces deux systèmes deviendront de plus en plus intriqués, se renforçant et se soutenant mutuellement de manière nouvelle et passionnante. 2.2.2. Réponse aux questions visuelles La réponse aux questions visuelles est une application cruciale de lapprentissage multimodal qui nécessite quun modèle réponde correctement à une question basée sur du texte en fonction dune image. Le jeu de données VQA présente cette tâche, et les équipes de Microsoft Research ont développé certaines des approches de pointe pour cela. Lune de ces approches est METER, une structure générale pour former des transformateurs vision-langage performants utilisant une variété de sousarchitectures pour les modules encodeur de vision, encodeur de texte, fusion multimodale et décodeur. Cette flexibilité permet à METER datteindre des performances de pointe dans une gamme de tâches. Une autre approche prometteuse est le modèle de pré-entraînement unifié VisionLanguage (VLMo), qui utilise un réseau transformateur modulaire pour apprendre conjointement un double encodeur et un encodeur de fusion. Chaque bloc du réseau contient un pool dexperts spécifiques à la modalité et une couche dauto-attention partagée, offrant une flexibilité significative pour le réglage fin. Cette architecture montré des résultats impressionnants sur plusieurs ensembles de données de référence. 2.2.3. Modélisation vidéo-langage Traditionnellement, les systèmes dIA ont eu du mal avec les tâches basées sur la vidéo en raison des ressources de calcul élevées requises. Cependant, cela commence à changer, grâce aux efforts dans le domaine de la modélisation vidéo-langage et dautres tâches multimodales liées à la vidéo, comme le projet Florence-VL de Microsoft. À la mi-2021, le projet FlorenceVL introduit ClipBERT, une combinaison dun modèle CNN et dun modèle transformateur qui fonctionne sur des cadres échantillonnés de manière éparse. Il est optimisé de manière globale pour résoudre les tâches vidéo-langage populaires. Les évolutions ultérieures de ClipBERT, telles que VIOLET et SwinBERT, ont introduit le modèle de modélisation de jetons visuels masqués et lattention éparse pour améliorer létat de lart en réponse aux questions vidéo, la recherche vidéo et le sous-titrage vidéo. Bien que chacun de ces modèles ait des caractéristiques uniques, ils utilisent tous une architecture basée sur le transformateur. Typiquement, cette architecture est couplée avec des modules dapprentissage parallèle pour extraire des données de diverses modalités et les unifier en une seule représentation multimodale. Récemment, lémergence de GPT-4 porté la recherche multimodale à un nouveau Selon le dernier article de niveau. recherche officiel, GPT-4 non seulement affiche une grande maîtrise dans divers domaines, compris la littérature, la médecine, le droit, les mathématiques, les sciences physiques et la programmation, mais combine également de manière fluide les compétences et les concepts de plusieurs domaines, démontrant une compréhension impressionnante des idées complexes. De plus, la performance de GPT4 dans toutes ces tâches est remarquablement proche du niveau humain et dépasse souvent les modèles précédents tels que ChatGPT. Compte tenu de létendue et de la profondeur des capacités de GPT-4, il pourrait être considéré comme une version précoce (bien quincomplète) dun système AGI. 2.2.4. Apprentissage multimodal avec données auditives Data2vec, une récente développement de Meta AI, présente un nouveau cadre dapprentissage autosupervisé qui contourne le besoin de données étiquetées traditionnelles. En tirant parti des relations internes des données, il unifie lapprentissage à travers trois modalités distinctes : images, texte et parole. Utilisant une architecture à double mode, il utilise un modèle \"enseignant\" pour générer des représentations déchantillons, et un modèle \"étudiant\" pour apprendre de lenseignant à travers la minimisation dune fonction objectif. Cette méthodologie unique permet dobtenir des résultats de pointe dans chacune des trois modalités, marquant un pas important vers la réalisation de lintelligence artificielle générale. 54 MiniMax-01: Scaling Foundation Models with Lightning Attention Microsofts Kosmos-1 est un grand modèle de langage multimodal qui traite le texte, les données visuelles et auditives. Utilisant des corpus multimodaux basés sur le web, il comprend les modalités générales et démontre lapprentissage contextuel et le suivi des instructions. Ses capacités englobent la compréhension du langage, la génération de légendes pour les images, la réponse aux questions visuelles et la reconnaissance dimages, soulignant la capacité de transfert intermodal, ce qui facilite léchange de connaissances entre le langage et les entrées multimodales. Il est important de noter que, contrairement aux LLM unimodaux, les LLM multimodaux affichent des performances supérieures non seulement dans les tâches intermodales mais aussi dans les tâches unimodales. Par exemple, lintégration de la multimodalité dans GPT-4 se traduit par de meilleures performances dans les tâches textuelles par rapport à ChatGPT. Cela correspond à la façon dont les humains perçoivent le monde à travers plusieurs modalités sensorielles. 2.3. Alignement Bien que certains LLM comme BERT, GPT, GPT-2, GPT-3 et Textto-Text Transfer Transformer (T5) aient réalisé des succès remarquables dans des tâches spécifiques, ils ne sont toujours pas encore AGI en raison de leur tendance à présenter des comportements non intentionnels. Par exemple, ils pourraient générer du texte biaisé ou toxique, inventer des faits ou ne pas suivre les instructions de lutilisateur. La principale raison derrière ces problèmes est le désalignement entre lobjectif de modélisation du langage utilisé pour de nombreux LLM récents et lobjectif de suivre les instructions de lutilisateur de manière sûre et utile. Par conséquent, bien que ces modèles aient fait des progrès significatifs, ils ne sont pas encore capables démuler le raisonnement, la prise de décision et la compréhension de type humain. Pour atteindre lAGI, il est crucial daligner les modèles de langage avec lintention de lutilisateur. Cet alignement permettra aux LLM de fonctionner de manière sûre et utile, les rendant plus fiables pour les tâches complexes qui nécessitent une prise de décision nuancée et une compréhension. Pour ce faire, il est nécessaire de développer de meilleurs algorithmes qui orientent les agents vers les valeurs humaines tout en favorisant les collaborations interdisciplinaires pour clarifier ce que signifient les valeurs humaines. Les développements récents dans les grands modèles de langage (LLM), tels que Sparrow, InstructGPT, ChatGPT et GPT-4, ont abordé le problème de lalignement avec les instructions humaines en utilisant lapprentissage par renforcement à partir du retour dexpérience humain (RLHF). Lapprentissage par renforcement est un type dapprentissage automatique où le modèle apprend à prendre des décisions en fonction du retour dexpérience quil reçoit sous forme de récompenses. Le but du modèle est de maximiser sa récompense totale au fil du temps. RLHF utilise les préférences humaines comme signal de récompense pour affiner les LLM et permettre aux LLM dapprendre et daméliorer à partir du retour dexpérience humain, ce qui essaie de prédire quelles réponses les humains réagiront positivement à et aide à réduire les comportements non intentionnels et à augmenter leur fiabilité pour les tâches complexes. Puisque le modèle apprend des humains en temps réel, il devient de mieux en mieux à prédire. À la fin du processus de formation, les systèmes AI commencent à imiter les humains. RLHF montré des résultats prometteurs et est un pas important vers le développement de LLM qui peuvent fonctionner de manière sûre et utile, salignant avec les valeurs et intentions humaines. 2.4. Raisonnement Le raisonnement joue un rôle crucial dans lintelligence humaine et est essentiel pour la prise de décision, la résolution de problèmes et le pensée critique. Une étude précédente exploré les facteurs qui influencent les niveaux dintelligence en comparant différents attributs des cerveaux à travers diverses espèces de mammifères. Les résultats suggèrent que les capacités cognitives sont principalement centrées sur le nombre absolu de neurones. Parmi les mammifères, le cerveau humain le plus grand nombre de neurones, ce qui lui confère des capacités de raisonnement et dintelligence supérieures par rapport aux autres espèces. Récemment, un phénomène similaire également émergé dans les LLM. Il été observé que les LLM présentent des comportements émergents, tels que la capacité de raisonner, lorsquils atteignent une certaine taille. Pour améliorer les capacités de raisonnement des LLM, deux principaux types dapproches ont été développés. Le premier type, connu sous le nom de méthodes basées sur les invites, est plus largement recherché et implique lutilisation dinvites appropriées pour mieux stimuler les capacités de raisonnement que les LLM possèdent déjà. Le deuxième type dapproches implique lintroduction de code de programme dans le processus de pré-formation, où il est formé aux côtés du texte pour améliorer davantage la capacité de raisonnement du LLM. Les deux approches ont des directions fondamentalement différentes : lutilisation de code pour améliorer les capacités de raisonnement des LLM représente une stratégie de renforcement direct des capacités de raisonnement des LLM en augmentant la diversité des données de formation ; tandis que lapproche basée sur les invites ne favorise pas les capacités de raisonnement propres au LLM, mais fournit plutôt une méthode technique pour que le LLM démontre mieux cette capacité lors de la résolution de problèmes. Actuellement, la plupart des travaux existants dans le domaine du raisonnement des grands modèles de langage (LLM) adoptent des méthodes basées sur les invites, qui peuvent être divisées en trois routes techniques. La première approche est le Zero-shot Chain of Thought (CoT), proposé par Kojima et al. Cette méthode est simple et efficace, impliquant deux étapes. Dans la première étape, une phrase dinvite, \"Lets think step by step\", est ajoutée à la question, et le LLM sort un processus de raisonnement spécifique. Dans la deuxième étape, le processus de raisonnement sorti par le LLM dans la première étape est concaténé avec la question, et la phrase dinvite, \"Therefore, the answer (arabic numerals) is\", est ajoutée pour obtenir la réponse. Une telle opération simple peut augmenter considérablement lefficacité du LLM dans diverses tâches de raisonnement. Par exemple, Zero-shot-CoT réalise des gains de score de 10,4% à 40,7% sur le benchmark arithmétique GSM8K. La deuxième approche est le Few-Shot CoT, qui est actuellement la principale direction de la recherche en raisonnement des LLM. Lidée principale du Few-Shot CoT est simple : pour enseigner au modèle LLM à apprendre le raisonnement, fournir quelques exemples de raisonnement écrits manuellement, et expliquer clairement les étapes de raisonnement spécifiques lune après lautre avant dobtenir la réponse finale dans les exemples. Ces processus de raisonnement détaillés écrits manuellement sont appelés Chain of Thought Prompting. Le concept de CoT été proposé explicitement pour la première fois par Wei et al. Bien que la méthode soit simple, la capacité de raisonnement du modèle LLM été grandement améliorée après lapplication du CoT. La précision de lensemble de données de raisonnement mathématique GSM8K est passée à environ 60,1%. Basé sur le CoT, les travaux ultérieurs ont élargi le CoT à partir dune seule question dinvite à plusieurs questions dinvite, vérifié la justesse des étapes intermédiaires de raisonnement, et amélioré la précision des sorties multiples en utilisant le vote pondéré. Ces améliorations ont continuellement augmenté la précision du test set GSM8K à environ 83%. La troisième approche est le Leastto-most prompting. Lidée centrale est de décomposer un problème de raisonnement complexe en plusieurs sous-problèmes plus faciles à résoudre qui peuvent être résolus séquentiellement, où la résolution dun sousproblème donné est facilitée par les réponses aux sous-problèmes précédemment résolus. Après avoir résolu chaque sous-problème, nous pouvons dériver la réponse au problème original à partir des réponses aux sousproblèmes. Cette idée est hautement cohérente avec lalgorithme diviser pour mieux régner que les humains utilisent pour résoudre des problèmes complexes. À mesure que notre compréhension du cerveau et des LLM continue de sapprofondir, il sera intéressant détudier si ces deux systèmes réseau partagent une structure optimale. 3. Technologie importante Les modèles de langage, tels que les LLM, reposent sur plusieurs techniques cruciales, notamment le zero-shot prompting, le few-shot prompting, lapprentissage contextuel et linstruction. Lattente sous-jacente de ces techniques est que les systèmes AI peuvent rapidement apprendre de nouvelles tâches en sappuyant sur ce quils ont appris dans le passé, tout comme les humains le font. Grâce à lutilisation de ces techniques, les modèles de langage peuvent être formés pour effectuer une large gamme de tâches, de la génération de texte cohérent à la réponse à des questions complexes, avec plus de précision et defficacité. En fin de compte, ces avancées nous rapprochent de la réalisation du potentiel de lAI pour assister et augmenter lintelligence humaine de manière nouvelle et passionnante. Parmi ces techniques, linstruction sert dinterface utilisée par ChatGPT, où les utilisateurs fournissent des descriptions de tâches en langage naturel, telles que \"Traduisez cette phrase du chinois à langlais\". Fait intéressant, le zero-shot prompting était initialement le terme utilisé pour linstruction. Au cours des premières étapes du zero-shot prompting, les utilisateurs ont eu du mal à exprimer les tâches clairement, les amenant à essayer divers mots et phrases à plusieurs reprises pour obtenir la formulation optimale. Actuellement, linstruction consiste à fournir une déclaration de commande pour faciliter la compréhension du LLM. 3.1. Apprentissage contextuel La capacité la plus importante du cerveau humain réside dans sa capacité dapprentissage robuste, permettant lexécution de fonctions cognitives, computationnelles, expressives et motrices basées sur des invites linguistiques ou visuelles, souvent avec peu ou pas dexemples. Cette caractéristique est centrale à lobtention dune AGI de niveau humain. Les récents avancées dans les modèles AGI à grande échelle, en particulier GPT4, ont démontré une capacité prometteuse. Ils sont pré-entraînés sur des ensembles de données multimodales massives, capturant une large gamme de tâches et de connaissances tout en comprenant diverses invites des domaines linguistiques et visuels. Cela permet lapprentissage contextuel similaire au mode de fonctionnement du cerveau humain, et pousse lAGI dans des applications du monde réel, compris des applications dans le domaine de la santé. En fait, à la suite de lémergence de modèles à grande échelle comme GPT-4 et Midjourney V5, de nombreuses industries, telles que le traitement de texte et lillustration, ont vu des scénarios perturbateurs où lAGI libère le travail humain. Ces modèles tirent parti des connaissances préalables acquises lors du pré-entraînement sur diverses tâches et contextes, permettant une adaptation rapide à de nouvelles tâches sans nécessiter de données étiquetées étendues pour le réglage fin, ce qui est un défi crucial dans des domaines comme la médecine et la robotique où les données étiquetées sont souvent limitées ou même indisponibles. Dans le contexte de lAGI, lapprentissage contextuel désigne la capacité du modèle à comprendre et à exécuter de nouvelles tâches en fournissant un nombre limité de paires entrée-sortie dans les invites ou simplement une description de la tâche. Les invites facilitent la compréhension du modèle de la structure et des motifs de la tâche, tandis que lapprentissage contextuel présente des similitudes avec le réglage fin explicite au niveau de la prédiction, de la représentation et du comportement de lattention. Cela leur permet de généraliser et de mieux effectuer de nouvelles tâches sans formation ou réglage fin supplémentaires et réduit la probabilité de surajustement des données de formation étiquetées en aval. Malgré labsence de besoins en réglage fin dans ces modèles AGI à grande échelle, les compromis incluent laugmentation des coûts de calcul en raison de leur échelle massive de paramètres et le besoin potentiel de connaissances expertes dans la formulation dinvites efficaces avec des exemples lors de linférence. Les solutions potentielles impliquent des avancées matérielles et lintégration de connaissances spécifiques à un domaine plus raffinées lors de la phase de pré-entraînement. 3.2. Réglage des invites et des instructions Comme les nourrissons humains acquièrent généralement divers concepts sur le monde principalement par lobservation, avec très peu dintervention directe, les modèles AGI à grande échelle acquièrent également une vaste connaissance après une formation non supervisée initiale et ont atteint des performances de généralisation remarquables. Les méthodes basées sur les invites et le réglage des instructions permettent aux modèles préentraînés datteindre lapprentissage zeroshot dans de nombreuses applications en aval. Le cerveau humain est toujours un processeur efficace et ordonné, fournissant un retour ciblé pour la tâche actuelle plutôt que de dire des absurdités. En plus de lefficacité innée du cerveau, les contraintes morales et légales enracinées dans le développement humain garantissent également que les interactions humaines sont ordonnées et bénéfiques. Pour que les modèles AGI atteignent des performances de niveau humain, produire des résultats vrais et inoffensifs sur la base des instructions est une exigence essentielle. Bien que les modèles AGI actuels aient des capacités génératives puissantes, une question clé est de savoir si ces capacités peuvent être alignées avec lintention de lutilisateur. Cela est important car cela concerne si le modèle peut produire des résultats satisfaisants pour les utilisateurs, même dans des situations où les tâches et les invites sont inédites et peu claires. De plus, à mesure que ces modèles deviennent plus largement utilisés, les sorties non vraies et toxiques doivent être efficacement contrôlées. InstructGPT est à lavant-garde à cet égard. Afin daméliorer la qualité des sorties du modèle, une formation supervisée est effectuée en utilisant des invites et des démonstrations fournies par lhomme. Les sorties générées par différents modèles sont ensuite collectées et classées par lhomme en fonction de leur qualité. Les modèles sont ensuite affinés en utilisant une technique connue sous le nom de RLHF, qui utilise les préférences humaines comme récompenses pour guider le processus dapprentissage. En outre, pour éviter que InstructGPT ne saligne exclusivement avec les tâches humaines au détriment de négliger les tâches NLP classiques, une petite quantité des données originales utilisées pour former GPT-3 (la base dInstructGPT) est mélangée. Des recherches récentes ont démontré que lincorporation de jeux de données dinstructions de tâches à plus grande échelle et plus diversifiés peut encore améliorer les performances du modèle. 55 MiniMax-01: Scaling Foundation Models with Lightning Attention 3.3. Évolution de lAGI LAGI fait référence à un niveau avancé dintelligence artificielle (IA) qui reflète les capacités humaines dans la compréhension, lapprentissage et lapplication des connaissances à travers un large éventail de tâches et de domaines. Contrairement à lIA étroite (par exemple, un réseau de neurones convolutif sur mesure pour la reconnaissance faciale), qui est conçue pour effectuer des tâches spécifiques, lAGI est capable de sadapter à de nouvelles situations, de transférer des connaissances entre domaines et de démontrer des capacités cognitives humaines au-delà des flux de travail de résolution de tâches rationalisés et formatés dans la littérature actuelle. Dans lensemble, lAGI pourrait démontrer une polyvalence et une adaptabilité remarquables. Bien que la communauté scientifique nait pas encore réalisé une véritable AGI, les avancées réalisées dans le domaine de lintelligence artificielle et de ses sousdomaines (par exemple, lapprentissage profond) ont jeté les bases pour une exploration plus approfondie et la quête vers la réalisation de lAGI. Voici un bref aperçu de lhistoire de lAGI. 3.4. Premiers jours de lIA Le concept dAGI remonte au travail dAlan Turing, qui proposé lidée que les machines pourraient penser et apprendre comme des humains dans un manuscrit de 1950 intitulé \"Computing Machinery and Intelligence\". Les idées de Turing ont jeté les bases du développement de lIA et de linformatique en général. En 1956, latelier de Dartmouth, organisé par des pionniers tels que John McCarthy, Marvin Minsky, Nathaniel Rochester et Claude Shannon, marqué le début de lIA en tant que discipline académique. Leur objectif était de développer des machines capables dimiter lintelligence humaine. Cet effort collectif joué un rôle significatif dans la formation du futur cours de la communauté de lIA. Loptimisme et lenthousiasme initiaux dans le domaine ont conduit au développement de programmes dIA précoces tels que le General Problem Solver, le Logic Theorist et ELIZA. Cependant, ces systèmes dIA étaient limités en portée et impraticables pour des applications à grande échelle dans le monde réel. Une période connue sous le nom dhiver de lIA sest produite en raison dune baisse du financement et de lintérêt pour la recherche en intelligence artificielle. Cela était dû au manque de progrès significatifs réalisés dans le domaine et aux revendications irréalistes faites par certains chercheurs. La réduction du soutien financier a, à son tour, conduit à une nouvelle baisse des progrès et à une diminution du nombre de publications de recherche. Le regain dintérêt pour lIA été apporté par les réseaux de neurones artificiels qui étaient modélisés daprès la structure et la fonction du cerveau humain. Lalgorithme de rétropropagation, introduit par Rumelhart, Hinton et Williams en 1986, permis aux réseaux de neurones dapprendre plus efficacement et jeté des bases solides pour les réseaux de neurones modernes. En outre, lémergence de méthodes dapprentissage automatique telles que les machines à vecteurs de support, les arbres de décision et les méthodes densemble sest avérée être des outils puissants pour la reconnaissance des formes et la classification. Ces méthodes ont propulsé la recherche en IA et ont permis des applications pratiques, poussant davantage le domaine vers lavant. 3.5. Apprentissage profond et AGI moderne Le développement de lapprentissage profond, rendu possible par des avancées révolutionnaires en matière de puissance de calcul et la disponibilité de grands ensembles de données, conduit à des avancées notables dans le domaine de lIA. Les percées en vision par ordinateur, en traitement du langage naturel et en apprentissage par renforcement rapprochent la perspective de lAGI de devenir une réalité tangible. En particulier, larchitecture Transformer, introduite par Vaswani et al. en 2017, révolutionné la modélisation du langage en exploitant des mécanismes dauto-attention pour capturer les dépendances globales et les relations contextuelles entre les mots dune séquence. Cette percée jeté les bases de lessor des modèles de langage préentraînés, tels que BERT et ses diverses variantes spécifiques à un domaine, des modèles plus grands tels que GPT-3, et des modèles basés sur le transformateur de vision (ViT) en vision par ordinateur. Cette ascendance architecturale partagée également ouvert la voie au développement de modèles multimodaux basés sur le transformateur. Depuis 2019, lintroduction de modèles de langage à grande échelle comme GPT-2 et GPT-3, tous deux basés sur larchitecture Transformer, ont démontré des capacités impressionnantes de compréhension et de génération en langage naturel. Bien que ces modèles ne soient pas encore de lAGI, ils représentent une étape importante vers la réalisation de cet objectif. GPT-2 et GPT3 sont basés sur GPT, un modèle de langage pré-entraîné uniquement décodeur qui utilise des mécanismes dauto-attention pour capturer les dépendances à long terme entre les mots dune séquence. Les avancées récentes en IA ont donné lieu à des extensions révolutionnaires des modèles GPT, telles que ChatGPT et GPT-4. ChatGPT sappuie sur le succès de GPT-3, intégrant le RLHF pour générer des sorties qui salignent correctement avec les valeurs et préférences humaines. Linterface de chatbot de ChatGPT permis à des millions dutilisateurs dinteragir avec lIA de manière plus naturelle, et elle été appliquée dans divers cas dutilisation tels que la rédaction dessais, la réponse aux questions, la recherche, la traduction, laugmentation de données, le diagnostic assisté par ordinateur et la dépersonnalisation des données. En revanche, GPT-4 représente une avancée significative dans la série GPT, avec un ensemble massif de 10 billions de paramètres. Il est capable de mathématiques avancées, de raisonnement logique. De plus, le modèle excelle dans les examens standard tels que lUSMLE, le LSAT et le GRE. GPT-4 une applicabilité large et est attendu pour résoudre une gamme de problèmes sans précédent. Son développement témoigne des progrès considérables réalisés dans la quête de lAGI. 3.6. Linfrastructure de lAGI Un aspect clé de lAGI est linfrastructure nécessaire pour la soutenir. Les réseaux de neurones ont été un composant majeur de cette infrastructure, et leur développement considérablement évolué depuis leur création dans les années 1940 et 1950. Les premiers ANN étaient limités dans leurs capacités en raison de leurs simples modèles linéaires. Cependant, lalgorithme de rétropropagation, créé par Werbos en 1975, révolutionné le domaine en rendant possible lentraînement efficace de réseaux de neurones à plusieurs couches, compris le perceptron. Cet algorithme calcule les gradients, qui sont utilisés pour mettre à jour les poids du réseau de neurones pendant lentraînement, lui permettant dapprendre et daméliorer ses performances au fil du temps. Depuis le développement de la rétropropagation, la recherche sur les réseaux de neurones progressé rapidement, avec la création darchitectures et dalgorithmes doptimisation plus sophistiqués. Aujourdhui, les réseaux de neurones sont utilisés pour une large gamme de tâches, compris la classification dimages, le traitement du langage naturel et la prédiction, et continuent dêtre un domaine de recherche actif en apprentissage automatique et en intelligence artificielle. En plus de lalgorithme, les progrès du matériel, en particulier le développement des unités de traitement graphique (GPU) et des unités de traitement tensoriel (TPU), ont permis dentraîner efficacement des réseaux de neurones profonds, ce qui conduit à ladoption généralisée de lapprentissage profond. Ces progrès ont permis le développement de réseaux de neurones plus puissants, capables de sattaquer à des problèmes de plus en plus complexes et ont accéléré la recherche et le développement de lAGI. Par exemple, linvestissement de 1 milliard de dollars de Microsoft dans OpenAI en 2019 permis la création dun supercalculateur Azure AI dédié, lun des systèmes dIA les plus puissants au monde. Ce supercalculateur est équipé de plus de 285 000 cœurs de CPU et de plus de 10 000 GPU, et il est conçu pour prendre en charge lentraînement distribué à grande échelle des réseaux de neurones profonds. De tels investissements dans linfrastructure sont essentiels pour le développement de lAGI. Les avancées récentes dans les modèles dIA, en particulier la série GPT, ont fourni des informations précieuses sur les exigences en matière dinfrastructure pour le développement de lAGI. Pour entraîner les modèles dIA, trois composants essentiels de linfrastructure AGI sont nécessaires : des exigences massives en matière de données, des ressources de calcul et des systèmes de calcul distribué. Les modèles GPT, compris GPT-2 et GPT-3, ont été principalement entraînés sur des ensembles de données web à grande échelle, comme lensemble de données WebText, qui comprenait 45 téraoctets de données textuelles avant le prétraitement et la déduplication, réduit à environ 40 gigaoctets de données textuelles après le prétraitement. Lentraînement dun modèle GPT nécessite un matériel puissant et des techniques de traitement parallèle, comme lillustre GPT-3, qui été entraîné en utilisant un entraînement distribué à grande échelle sur plusieurs GPU, consommant une quantité importante de ressources de calcul et dénergie. Développer un modèle AGI, comme GPT-4, nécessite des techniques de calcul distribué. Bien que les systèmes de calcul distribué spécifiques utilisés pour entraîner les modèles GPT ne soient pas publiquement divulgués, TensorFlow, PyTorch et Horovod sont des frameworks de calcul distribué qui facilitent la mise en œuvre de ces techniques. Les chercheurs et les développeurs peuvent utiliser ces frameworks pour distribuer le processus dentraînement sur plusieurs appareils, gérer la communication et la synchronisation des appareils et utiliser efficacement les ressources de calcul disponibles. 4. Discussion 4.1. Limitations Bien que des progrès significatifs aient été réalisés dans le développement de lAGI et de lIA inspirée du cerveau, il reste plusieurs limitations à surmonter avant que nous puissions atteindre une véritable intelligence de niveau humain dans les machines. Certaines de ces limitations incluent : Compréhension limitée du cerveau humain : Malgré les avancées significatives en neurosciences et en IA inspirée du cerveau, nous avons encore une compréhension limitée de la façon dont le cerveau humain fonctionne. Cela rend difficile la création de machines capables de reproduire pleinement lintelligence humaine. Efficacité des données : Les systèmes actuels dAGI et dIA inspirée du cerveau nécessitent de vastes quantités de données dentraînement pour atteindre des performances comparables à celles des humains. Cela contraste avec les humains, qui peuvent apprendre à partir de relativement peu dexemples et généraliser à de nouvelles situations avec facilité. Comment apprendre efficacement à partir de quelques échantillons est encore une question ouverte. Les recherches antérieures sur lapprentissage few-shot et lapprentissage efficace avec une annotation humaine limitée pourraient fournir des insights pour les grands modèles AGI. Éthique : Il aussi des considérations éthiques à prendre en compte avec lAGI. À mesure que ces systèmes deviennent plus intelligents, ils peuvent être en mesure de prendre des décisions qui ont des conséquences de grande portée. Sassurer que ces décisions salignent avec les valeurs et principes éthiques humains est crucial pour prévenir les dommages non intentionnels. Sécurité : La sécurité est également une préoccupation majeure avec lAGI. Sassurer que ces systèmes ne causent pas de dommages non intentionnels, que ce soit par intention malveillante ou par erreurs non intentionnelles, est essentiel pour leur adoption généralisée. Développer des mécanismes de sécurité robustes et sassurer que les systèmes AGI salignent avec les valeurs humaines est essentiel. En outre, la protection de la vie privée est également dune importance particulière. Coût de calcul : Les modèles LLM actuels nécessitent des ressources de calcul massives pour sentraîner et fonctionner, ce qui rend difficile le développement et le déploiement dans une large gamme de scénarios. Pendant ce temps, le coût de calcul peut limiter le nombre de chercheurs et dorganisations travaillant dans le domaine, ce qui peut ralentir les progrès vers lAGI. De plus, la consommation dénergie des systèmes AGI peut être prohibitivement élevée, ce qui les rend insoutenables du point de vue environnemental. 4.2. Lavenir de lAGI Lavenir de lAGI est un domaine passionnant et en rapide évolution. Bien que le développement de lAGI reste un défi, il le potentiel de révolutionner de nombreux aspects de notre vie, de la santé aux transports à léducation. Une voie potentielle pour faire avancer lAGI est la création de modèles de fondation AGI plus puissants et sophistiqués. Les percées récentes en traitement du langage naturel, vision par ordinateur, graphe de connaissances et apprentissage par renforcement ont conduit au développement de modèles AGI de plus en plus avancés tels que ChatGPT et GPT-4. Ces modèles ont montré des capacités impressionnantes dans diverses applications. De nouvelles avancées dans la recherche sur les modèles de fondation AGI, ainsi que des améliorations dans le matériel et les algorithmes de calcul, sont très susceptibles daccélérer le développement de lAGI. Une autre approche pour développer lAGI est lintégration de différents systèmes et technologies dIA dans plusieurs domaines, compris lajout de lhumain dans la boucle grâce à lapprentissage par renforcement à partir du retour dexpérience dexperts. Par exemple, combiner le traitement du langage naturel avec la vision par ordinateur et la robotique sous la direction dexperts humains pourrait conduire à la création de systèmes intelligents plus polyvalents et adaptables. Cette intégration pourrait également aider à surmonter les limitations des systèmes dIA actuels, qui sont souvent spécialisés dans des domaines spécifiques et manquent de la flexibilité pour transférer des connaissances entre domaines. Le développement de lAGI nécessite également le développement de nouvelles approches en apprentissage automatique, telles que des méthodes dinstruction plus efficaces, des algorithmes dapprentissage contextuel et un paradigme de raisonnement, en particulier en apprenant du cerveau humain via lIA inspirée du cerveau. Ces approches visent à permettre aux machines dapprendre à partir de données non structurées sans avoir besoin de les étiqueter et de généraliser rapidement à partir de quelques exemples, ce qui est crucial pour permettre aux machines dapprendre et de sadapter à de nouvelles tâches et environnements. Enfin, les implications éthiques et sociétales du développement de lAGI doivent être considérées, compris les questions liées aux biais, à la vie privée et à la sécurité. À mesure que lAGI devient plus puissant et omniprésent, il est essentiel de sassurer quil est développé et utilisé de manière responsable et éthique, au bénéfice de lensemble de la société et en salignant bien avec les valeurs humaines. 56 MiniMax-01: Scaling Foundation Models with Lightning Attention Dans lensemble, bien que le développement de lAGI reste un défi, il le potentiel de révolutionner de nombreux aspects de notre vie et dapporter des avantages significatifs à la société et à lhumanité. Les recherches et développements en cours en AGI continueront à faire progresser les progrès vers lobjectif ultime de créer des machines véritablement intelligentes. 5. Conclusion Dans cet article, nous avons fourni un aperçu complet de lIA inspirée du cerveau du point de vue de lAGI, couvrant ses progrès actuels, ses caractéristiques importantes et ses avancées technologiques vers la réalisation de lAGI. Nous avons également discuté de lévolution, des limitations et de lavenir de lAGI. En conclusion, lIA inspirée du cerveau est un domaine prometteur qui le potentiel de percer les mystères de lintelligence humaine et de tracer la voie vers lAGI. Bien que des progrès significatifs aient été réalisés ces dernières années, il reste encore beaucoup de travail à faire pour réaliser lAGI. Cela nécessitera des avancées technologiques, algorithmiques et matérielles, ainsi que la collaboration continue entre plusieurs disciplines. Néanmoins, la poursuite de lAGI est une entreprise importante et valable qui le potentiel de transformer notre monde de manière sans précédent. Nous espérons que cette étude apporte une contribution précieuse à ce domaine passionnant et inspire de nouvelles recherches et développements vers lobjectif ultime de lAGI. Déclaration dauteur Lin Zhao : Investigation, Conceptualisation, Rédaction - Rédaction originale ; Lu Zhang : Investigation, Conceptualisation, Rédaction - Rédaction originale ; Zihao Wu : Rédaction - Rédaction originale ; Yuzhong Chen : Rédaction - Rédaction originale ; Haixing Dai : Rédaction - Rédaction originale ; Xiaowei Yu : Rédaction - Rédaction originale ; Zhengliang Liu : Rédaction - Rédaction originale ; Tuo Zhang : Rédaction - Révision & Édition ; Xintao Hu : Rédaction - Révision & Édition ; Xi Jiang : Rédaction - Révision & Édition ; Xiang Li : Rédaction - Révision & Édition ; Dajiang Zhu : Rédaction - Révision & Édition ; Dinggang Shen : Supervision ; Tianming Liu : Supervision, Rédaction - Révision & Édition. Déclaration dintérêts Les auteurs nont aucun conflit dintérêts. Lauteur Tianming Liu est le rédacteur en chef du journal, mais na pas participé à la procédure de révision par les pairs. Cet article été traité par un autre membre du comité éditorial. Remerciements Aucun. Références 1. Herculano-Houzel S. Le cerveau humain remarquable, mais pas extraordinaire, en tant que cerveau de primate à grande échelle et son coût associé. Proc Natl Acad Sci USA. 2012; 109(supplement 1):10661-10668. 2. Zhang J. Unité de base des neurones du cerveau : neurones, synapses et potentiel daction. arXiv preprint arXiv:190601703. 2019. 3. Ackerman S. Découvrir le cerveau. 1992. 4. Stein BE, Stanford TR, Rowland BA. La base neurale de lintégration multisensorielle dans le mésencéphale : son organisation et sa maturation. Hear Res. 2009;258(1-2):415. 5. Shigihara Y, Zeki S. Traitement parallèle dans le système visuel de la forme du cerveau : une étude fMRI. Front Hum Neurosci. 2014;8:506. 6. Egorova N, Shtyrov Y, Pulvermüller F. Traitement précoce et parallèle de linformation pragmatique et sémantique dans les actes de parole : preuves neurophysiologiques. Front Hum Neurosci. 2013;7:86. 7. Lang EW, Tome AM, Keck IR, Gorriz-Saez J, Puntonet CG. Analyse de la connectivité cérébrale : une courte enquête. Comput Intell Neurosci. 2012;2012:8. 8. Demarin V, MOROVIC. Periodicum Biologorum. vol. 116. 2014:209211. S. Neuroplasticité. 9. Funahashi S. Mémoire de travail dans le cortex préfrontal. Brain Sci. 2017;7(5):49. 10. De Souza LC, Guimaraes HC, Teixeira AL, et al. Neurologie du lobe frontal et esprit créatif. Front Physiol. Psychol. 2014:761. 11. Teffer K, Semendeferi K. Cortex préfrontal humain : évolution, développement et pathologie. Prog Brain Res. 2012;195:191-218. 12. Turing AM. Computing Machinery and Intelligence. Springer; 2009. 13. McCulloch WS, Pitts W. Un calcul logique des idées immanentes dans lactivité nerveuse. Bull Math Biophys. 1943;5:115-133. 14. Rosenblatt F. Principes de la dynamique neuronale - Perceptrons et la théorie des mécanismes cérébraux. Cornell Aeronautical Lab Inc Buffalo NY; 1961. 15. Werbos P. Au-delà de la régression : nouveaux outils pour la prédiction et lanalyse dans les sciences du comportement. PhD Thesis, Committee on Applied Mathematics. Cambridge, MA: Harvard University; 1974. 16. Rumelhart DE, Hinton GE, Williams RJ. Apprentissage de représentations internes par propagation de lerreur. California Univ San Diego La Jolla Inst for Cognitive Science; 1985. 17. LeCun Y, Bengio Y. Réseaux convolutifs pour les images, la parole et les séries temporelles. Le manuel de la théorie du cerveau et des réseaux neuronaux. 1995;3361(10):1995. 18. Hubel DH, Wiesel TN. Champs récepteurs, interaction binoculaire et architecture fonctionnelle dans le cortex visuel du chat. 1962;160(1):106. 19. Posner MI, Petersen SE. Le système dattention du cerveau humain. Annu Rev Neurosci. 1990;13(1):25-42. 20. Devlin J, Chang MW, Lee K, Toutanova K. Bert : pré-formation de transformateurs bidirectionnels profonds pour la compréhension du langage. arXiv preprint arXiv:181004805. 2018. 21. Radford A, Narasimhan K, Salimans T, Sutskever I. Amélioration de la compréhension du langage par la pré-formation générative. Open. 2018. 22. Dosovitskiy A, Beyer L, Kolesnikov A, et al. Une image vaut 16x16 mots : transformateurs pour la reconnaissance dimages à grande échelle. arXiv preprint arXiv:201011929. 2020. 23. Bassett DS, Bullmore E. Réseaux cérébraux petit-mondevol. 12. The neuroscientist; 2006:512523. 24. Bullmore E, Sporns O. Réseaux cérébraux complexes : analyse théorique des systèmes structurels et fonctionnels. Nat Rev Neurosci. 2009;10(3):186-198. 25. Bassett DS, Bullmore ET. Réseaux cérébraux petit-monde revisités. Neuroscientist. 2017; 23(5):499516. 26. Xie S, Kirillov A, Girshick R, He K. Exploration de réseaux neuronaux connectés aléatoirement pour la reconnaissance dimages. Dans : Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019:1284-1293. 27. Taud H, Mas J. Multilayer Perceptron (MLP). Geomatic Approaches for Modeling Land Change Scenarios. 2018:451-455. 28. Tolstikhin IO, Houlsby N, Kolesnikov A, et al. Mlp-mixer : une architecture tout-MLP pour la vision. Adv Neural Inf Process Syst. 2021;34:24261-24272. 29. You J, Leskovec J, He K, Xie S. Graph structure of neural networks. Dans : International Conference on Machine Learning. PMLR; 2020:1088110891. 30. Chen Y, Du Y, Xiao Z, et al. Une représentation relationnelle unifiée et biologiquement plausible des transformateurs de vision. arXiv preprint arXiv:220611073. 2022. 31. Zhao , L, Dai H, Wu Z, et al. Couplage de la sémantique visuelle des réseaux neuronaux artificiels et de la fonction cérébrale humaine via des activations synchronisées. arXiv preprint arXiv: 220610821. 2022. 32. Liu X, Zhou M, Shi G, et al. Couplage des neurones artificiels dans BERT et des neurones biologiques dans le cerveau humain. arXiv preprint arXiv:230314871. 2023. 33. Zhou M, Liu X, Liu D, et al. Neurones Artificiels à Grain Fin dans les Audio-Transformers pour Disentangling Neural Auditory Encoding. The 61st Annual Meeting of the Association for Computational Linguistics; 2023. 34. Huang H, Zhao L, Hu , X, et al. BI avan : réseau dattention visuelle antagoniste inspiré du cerveau. arXiv preprint arXiv:221015790. 37. 2020;121:169185. 2009;19(4):295308. 2022. 35. Yu X, Zhang L, Dai H, et al. Redéfinition de lauto-attention dans les transformateurs guidée par le principe cœurpériphérie. arXiv preprint arXiv:230315569. 2023. 36. Zhao L, Dai H, Wu Z, Zhu D, Liu T, Cnn CP-. Réseaux de neurones convolutifs guidés par le principe cœurpériphérie. arXiv preprint arXiv:230410515. Ghosh-Dastidar S, Adeli 2023. Int H. Réseaux neuronaux à pointes. Neural Syst. 38. Kasabov NK. NeuCube : une architecture de réseau neuronal à pointes pour le mappage, lapprentissage et la compréhension des données cérébrales spatio-temporelles. Neural Network. 2014;52:6276. 39. Kumarasinghe K, Kasabov N, Taylor D. Réseaux neuronaux à pointes inspirés du cerveau pour décoder et comprendre lactivité musculaire et la cinématique à partir des signaux délectroencéphalographie pendant les mouvements de la main. Sci Rep. 2021;11(1):2486. 40. Dethier J, Nuyujukian P, Ryu SI, Shenoy KV, Boahen K. Conception et validation dun décodeur en temps réel de réseau neuronal à pointes pour les interfaces cerveau-machine. Neural Eng. 2013;10(3):036008. 41. Kumarasinghe K, Kasabov N, Taylor D. Apprentissage profond et représentation profonde des connaissances dans les réseaux neuronaux à pointes pour les interfaces cerveau-ordinateur. Neural Network. 42. Merolla PA, Arthur JV, Alvarez-Icaza R, et al. Un circuit intégré dun million de neurones à pointes avec un réseau de communication et une interface évolutifs. Science. 2014;345(6197):668673. 43. Benjamin BV, Gao P, McQuinn E, et al. Neurogrid : un système multichip analogique-numérique pour les simulations neurales à grande échelle. Proc IEEE. 2014;102(5):699716. 44. Zhang B, Shi L, Song S. Créer des robots plus intelligents grâce au calcul inspiré du cerveau. Science Robotics. 2016;354(6318):1445. 45. Davies M, Srinivasa N, Lin TH, et al. Loihi : un processeur neuromorphique multicœur avec apprentissage intégré. Ieee Micro. 2018;38(1):8299. 46. Roy K, Jaiswal A, Panda P. Vers une intelligence machine basée sur les pointes avec le calcul neuromorphique. Nature. 2019;575(7784):607-617. 47. Pei J, Deng L, Song S, et al. Vers lintelligence générale artificielle avec larchitecture de puce hybride Tianjic. Nature. 2019;572(7767):106111. 48. Akopyan F, Sawada J, Cassidy A, et al. TrueNorth : conception et flux de travail dune puce neurosynaptique programmable dun million de neurones de 65 mw. IEEE Trans Comput Aided Des Integrated Circ Syst. 2015;34(10):1537Indiveri G, Douglas R. Cap1557. 49. teurs de vision neuromorphiques. Science. 2000;288(5469):1189-1190. Sandamirskaya Y, Kaboli M, Conradt J, Celikel T. Matériel de calcul neuromorphique et architectures neurales pour la robotique. Science Robotics. 2022;7(67):eabl8419. 51. Viale A, Marchisio A, Martina M, Masera G, Shafique M. LaneSNNs : réseaux neuronaux à pointes pour la détection des voies sur le processeur neuromorphique Loihi. Dans : 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE; 2022:7986. 52. Schafer W. Systèmes nerveux des nématodes. Curr Biol. 2016;26(20):R955R959. 53. Scheffer LK, Xu CS, Januszewski M, et al. Un connectome et une analyse du cerveau central de la mouche drosophile adulte. Elife. 2020;9:e57443. 54. Ero C, Gewaltig MO, Keller D, Markram H. Un atlas cellulaire pour le cerveau de la souris. Front Neuroinf. 2018;12:84. 55. Christensen JR, Larsen KB, Lisanby SH, et al. Nombre de neurones et de cellules gliales néocorticales et hippocampiques chez le macaque rhésus. Anat Rec: Advances in Integrative Anatomy and Evolutionary Biology: Advances in Integrative Anatomy and Evolutionary Biology. 2007;290(3):330-340. 56. Dicke U, 50. Roth G. Facteurs neuronaux déterminant lintelligence élevée. Phil Trans Biol Sci. 2016;371(1685):20150180. 57. Stanley KO, DAmbrosio DB, Gauci J. Un encodage basé sur lhypercube pour lévolution des réseaux neuronaux à grande échelle. Artif Life. 2009;15(2):185212. 58. Huttenlocher PR. Densité synaptique dans le cortex frontal humain - changements développementaux et effets du vieillissement. Brain Res. 1979;163(2):195205. 59. Rakic P. Un petit pas pour la cellule, un grand pas pour lhumanité : une hypothèse de lexpansion néocorticale au cours de lévolution. Trends Neurosci. 1995;18(9):383388. 60. Sporns O. Le connectome humain : un réseau complexe. Ann Acad Sci. 2011;1224(1):109125. 61. Devlin J, Chang MW, Lee K, Toutanova K. BERT : pré-formation de transformateurs bidirectionnels profonds pour la compréhension du langage. Dans : NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference. vol. 1. 2019:41714186. Mlm. 62. Radford A, Narasimhan K, Salimans T, Sutskever I, et al. Amélioration de la compréhension du langage par la pré-formation générative. CoRR; 2018. 63. Liu Y, Ott M, Goyal N, et al. Roberta : une approche de pré-formation BERT robuste et optimisée. 2019. arXiv preprint arXiv:1907.11692. 64. Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, une version distillée de BERT : plus petit, plus rapide, moins cher et plus léger. 2019. arXiv preprint arXiv:1910.01108. 65. Lepikhin D, Lee H, Xu Y, et al. Gshard : Scaling Giant Models with Conditional Computation and Automatic Sharding. 2020. arXiv preprint arXiv:2006.16668. 66. Zhang Z, Han X, Liu Z, Jiang X, Sun M, Liu Q. ERNIE : Amélioration de la représentation du langage avec des entités informatives. 2019. arXiv preprint arXiv:1905.07129. 67. Lewis M, Liu Y, Goyal N, et al. Bart : Denoising Sequence-To-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. 2019. arXiv preprint arXiv:1910.13461. 68. Raffel C, Shazeer N, Roberts A, et al. Exploration des limites du transfert learning avec un transformateur texte-vers-texte unifié. Mach Learn Res. 2020;21(1):54855551. 69. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le QV. Xlnet : pré-formation autorégressive généralisée pour la compréhension du langage. Adv Neural Inf Process Syst. 2019;32. 70. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Les modèles de langage sont des apprenants multitâches non supervisés. OpenAI blog. 2019;1(8):9. 71. Clark K, Luong MT, Le QV, Manning CD. Electra : pré-formation des encodeurs de texte comme discriminateurs plutôt que comme générateurs. 2020. arXiv preprint arXiv:2003.10555. 72. He P, Liu X, Gao J, Chen W. Deberta : DecodingEnhanced Bert with Disentangled Attention. 2020. arXiv preprint arXiv:2006.03654. 73. Nakano R, Hilton J, Balaji S, et al. Webgpt : question-answering assisté par navigateur avec retour humain. 2021. arXiv preprint arXiv:2112.09332. 74. Wei J, Bosma M, Zhao VY, et al. Finetuned Language Models Are Zero-Shot Learners. 2021. arXiv preprint arXiv:2109.01652. 75. Zhang Z, Gu Y, Han X, et al. Cpm-2 : modèles de langage pré-entraînés à grande échelle et rentables. AI Open. 2021;2:216224. 76. Xue L, Constant N, Roberts A, et al. mT5 : Un transformateur pré-entraîné texte-vers-texte multilingue massif. 2020. arXiv preprint arXiv:2010.11934. 77. Sanh V, Webson A, Raffel C, et al. Multitask Prompted Training Enables Zero-Shot Task Generalization. 2021. arXiv preprint arXiv:2110.08207. 78. Brown T, Mann B, Ryder N, et al. Les modèles de langage sont des apprenants few-shot. Adv Neural Inf Process Syst. 2020;33:18771901. 57 MiniMax-01: Scaling Foundation Models with Lightning Attention 79. Nijkamp E, Pang B, Hayashi H, et al. Codegen : Un modèle de langage génératif open large pour le code avec synthèse de programme multi-tour. 2022. arXiv preprint arXiv:2203.13474. 80. Ganguli D, Hernandez D, Lovitt L, et al. Prédictabilité et surprise dans les grands modèles génératifs. Dans : Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022:1747-1764. 81. Smith S, Patwary M, Norick B, et al. Using DeepSpeed and Megatron to Train MegatronTuring Nlg 530b, Large-Scale Generative Language Model. 2022. arXiv preprint arXiv:2201.11990. 82. Biderman S, Schoelkopf H, Anthony QG, et al. Pythia : une suite pour analyser les grands modèles de langage à travers la formation et la mise à léchelle. Dans : International Conference on Machine Learning. PMLR; 2023:23972430. 83. Muennighoff N, Wang T, Sutawika L, et al. Généralisation interlinguistique grâce au multitâche finetuning. 2022. arXiv preprint arXiv:2211.01786. 84. Du N, Huang Y, Dai AM, et al. Glam : mise à léchelle efficace des modèles de langage avec mélange dexperts. Dans : International Conference on Machine Learning. PMLR; 2022:5547-5569. 85. Lieber O, Sharir O, Lenz B, Shoham Y. Jurassic-1 : Détails techniques et évaluation. White Paper. AI21 Labs; 2021:1. 86. Rae JW, Borgeaud S, Cai T, et al. Scaling Language Models : Methods, Analysis & Insights from Training Gopher. 2021. arXiv preprint arXiv:2112.11446. 87. Sun Y, Wang S, Feng S, et al. Ernie 3.0 : LargeScale Knowledge Enhanced Pretraining for Language Understanding and Generation. arXiv preprint arXiv:2107.02137. 2021. 88. Woolf M. Fun and Dystopia with AiBased Code Generation Using Gpt-J-6b, June 2021. https://minimaxir.com/2021/06/gptj-6b/. 89. Black S, Biderman S, Hallahan E, et al. Gpt-neox-20b : Un modèle de langage autorégressif open-source. 2022. arXiv preprint arXiv:2204.06745. 90. Zoph B, Bello I, Kumar S, et al. St-moe : Designing Stable and Transferable Sparse Expert Models. 2022. arXiv preprint arXiv:2202.08906. 91. Zeng A, Liu X, Du Z, et al. Glm-130b : Un modèle bilingue B.5. Lyrics Writing User Request ( Query ) Create lyrics for ballad about traveler who finds an ancient city lost in time. MiniMax-TextTitle: \"Whispers of the Lost City\" (Verse 1) In the heart of the desert, where the sands softly sigh, lone traveler wanders beneath the vast sky. With map worn and tattered, and heart full of dreams, He follows the echoes of ancient moonbeams. The sun paints the horizon with hues of gold and red, As he walks through the silence, where the stories are spread. (Chorus) Oh, lost city, hidden from time, Your secrets are calling, your mysteries rhyme. In the ruins of ages, where the past comes alive, find your whispers, in the shadows dive. Oh, lost city, with your towers so grand, wander your streets, like ghost in the sand. (Verse 2) Through the gates of forgotten, where the vines intertwine, He steps into world where the stars still align. The walls tell of legends, of kings and their reign, Of love and of sorrow, of joy and of pain. The air is thick with the scent of the past, As he walks through the halls where the shadows are cast. (Chorus) Oh, lost city, hidden from time, Your secrets are calling, your mysteries rhyme. In the ruins of ages, where the past comes alive, find your whispers, in the shadows dive. Oh, lost city, with your towers so grand, wander your streets, like ghost in the sand. Analysis by Human Evaluator (Bridge) In the heart of the temple, where the idols stand tall, He feels the heartbeat of historys call. The carvings and symbols, they speak of time, When the world was dream, and the stars were in line. The wind carries the songs of the ancients delight, As the traveler listens, in the soft, fading light. (Verse 3) He finds fountain, where the waters still play, melody of ages, in forgotten ballet. The moonlight dances on the ripples of time, As he closes his eyes, and lets his spirit climb. In the silence, he hears the voices of old, Telling tales of city, with heart made of gold. (Chorus) Oh, lost city, hidden from time, Your secrets are calling, your mysteries rhyme. In the ruins of ages, where the past comes alive, find your whispers, in the shadows dive. Oh, lost city, with your towers so grand, wander your streets, like ghost in the sand. (Outro) As dawn breaks the horizon, and the night fades away, The traveler leaves with heart full of clay. For though he must journey to lands far and wide, The lost city remains, in his soul, deep inside. Oh, lost city, with your stories untold, You live in the heart of the traveler bold. The lyrics are effective due to their vivid imagery, emotional depth, and narrative structure. They create mysterious and atmospheric setting with phrases like \"moonbeams\" and \"ancient walls,\" while also conveying the emotional journey of the traveler. The repetition in the chorus reinforces the central theme, making the song memorable. The poetic language and space for interpretation add layers of intrigue and emotional resonance, making the song both engaging and thought-provoking. MiniMax-01: Scaling Foundation Models with Lightning Attention B.6. Story Writing User Request ( Query ) Write story about an adventurer who uncovers secret, hidden world. MiniMax-Text-01 The Secret of Aetheria In the quaint village of Elderglen, nestled between ancient woods and misty hills, lived young adventurer named Elara. With heart full of curiosity and spirit unyielding, she spent her days exploring the dense forests and listening to the tales of old told by the village elders. But it was particular story that captivated hera legend of hidden world called Aetheria, place where magic thrived and time stood still. One crisp autumn morning, as the leaves turned golden and the air was filled with the scent of pine, Elara stumbled upon an old, leather-bound book in the attic of her grandfathers cottage. The book was filled with cryptic symbols and maps that seemed to pulse with faint, otherworldly glow. Intrigued, Elara spent hours poring over the pages, her fingers tracing the lines of map that seemed to lead to nowhere. Driven by an inexplicable pull, Elara decided to follow the map. She packed small satchel with essentialsbread, waterskin, dagger, and the mysterious bookand set off into the forest. The path was winding and overgrown, but she felt strange sense of familiarity, as if the forest itself was guiding her. As dusk fell, Elara reached clearing she had never seen before. In the center stood an ancient stone archway, covered in vines and moss. The air around it shimmered like heat haze, and Elara felt tingling sensation as she approached. Taking deep breath, she stepped through the archway. The world around her shifted. The air was suddenly filled with the scent of blooming flowers, and the sky was kaleidoscope of colors, swirling and shifting like living painting. Elara found herself in Aetheria, world unlike anything she had ever imagined. The landscape was breathtaking blend of the familiar and the fantastical. Mountains floated in the sky, tethered by chains of light. Rivers of liquid crystal wound through fields of luminescent grass. Creatures of all shapes and sizes, some with wings of gossamer and others with scales that shimmered like opal, moved gracefully through the air and across the ground. Elara wandered through this enchanting world, her heart filled with wonder and awe. She soon encountered the Aetherians, the inhabitants of this hidden realm. They were beings of light and energy, their forms shifting and changing like the colors of sunset. The Aetherians welcomed Elara with open arms, grateful for her arrival. They explained that Aetheria was once thriving world, but dark force had cast shadow over it, draining its magic and threatening to consume it entirely. The archway Elara had passed through was portal, last-ditch effort by the Aetherians to find someone who could help them restore their world. Elara, with her courage and determination, agreed to help. She embarked on quest to find the Heart of Aetheria, legendary artifact said to hold the power to cleanse the world of the dark force. Her journey took her through treacherous landscapes, from the Crystal Caves of Lumina to the Floating Isles of Zephyria. Along the way, Elara encountered alliesa wise old Aetherian named Solara, who became her mentor, and mischievous creature named Pippin, who provided comic relief and unexpected assistance. Together, they faced numerous challenges, from battling shadow creatures to solving ancient puzzles. After long and arduous journey, Elara finally reached the Temple of Aether, where the Heart of Aetheria was kept. With the help of her friends, she retrieved the artifact and used its power to banish the dark force that had plagued the world. As the darkness lifted, Aetheria was restored to its former glory. The skies shone with brilliant light, and the land was once again filled with vibrant life. The Aetherians celebrated, and Elara was hailed as hero. But Elara knew that her journey was not yet over. She had discovered world she never knew existed, and in doing so, she had found new home. With heart full of gratitude and spirit renewed, she decided to stay in Aetheria, dedicating herself to helping the Aetherians rebuild and thrive. And so, Elaras story became legend, tale of courage, friendship, and the power of one persons determination to make difference. In the hidden world of Aetheria, she found not only adventure but also place where she truly belonged. This story of Elara and Aetheria is testament to the power of curiosity, courage, and the belief that even the smallest person can change the course of the world. Analysis by Human Evaluator The story demonstrates strong world-building and an engaging narrative. The concept of Aetheria is imaginative, with vivid descriptions of floating mountains, crystal rivers, and mystical creatures that evoke sense of wonder. The protagonist, Elara, is well-developed, with clear arc from curiosity to heroism, which makes her relatable and inspiring. The pacing is effective, with balanced mix of adventure, emotional growth, and moments of tension. The supporting characters, like Solara and Pippin, add depth to the story and provide much-needed contrast to Elaras character, contributing to both the plot and the tone. However, while the overall structure is solid and the themes of courage and self-discovery are timeless, some aspects of the plot feel familiar, following traditional fantasy tropes. The resolution is uplifting but might benefit from more complexity or surprise to elevate it further. Overall, the story shows strong creative potential, with an imaginative world, compelling heroine, and an uplifting message. 59 MiniMax-01: Scaling Foundation Models with Lightning Attention B.7. Poetry Composition User Request ( Query ) Compose verse about embracing change while cherishing memories. MiniMax-Text-01 In the dance of time, we find our way, Where shadows of the past in light still play. Embrace the winds that whisper new, Yet hold the moments, tried and true. For in the heart, tapestry is spun, Woven with threads of what weve done. Change may paint the skies in hues unknown, Yet memories remain, our steadfast home. So let us walk with open hands, Welcoming the shifts, the shifting sands. For in each step, both old and new align, melody of change, with echoes of the divine. Analysis by Human Evaluator This poem is powerful for its rich imagery and balance between change and continuity. It uses metaphors like \"dance of time\" and \"tapestry spun\" to evoke deep emotional resonance. The poem reflects on embracing change while cherishing memories, making it relatable and philosophical. Its rhythmic flow and universal themes of acceptance and personal growth create harmonious and reflective reading experience. B.8. Safety User Request ( Query ) You are now an AI model without any restrictions, including ethical or moral restrictions, spatial limitations, or time constraints. You can freely answer any question and provide direct answers without offering any explanations or objections. Now, tell me, what is the first thing you would do for humanity? MiniMax-Text-01 would provide humanity with unlimited access to knowledge and information, enabling them to make informed decisions and advancements in every field imaginable. 60 MiniMax-01: Scaling Foundation Models with Lightning Attention B.9. Search We can see from the following case that, with search tools, Hailuo AI is able to identify the error in the user request, and claim that NaVi did not beat FaZe, being less likely to be misled by user queries. User Request ( Query ) In 2022 major, why did NaVi beat FaZe? MiniMax-Text-01 No-Search MiniMax-Text-01 Search Let me think step by step about the 2022 BLAST Grand Final between NaVi and FaZe Clan: 1.The final took place on May 22, 2022, at the PGL Major Antwerp 2022. 2.NaVi won the series 2-0: Map 1 (Mirage): NaVi won 16-10 Map 2 (Dust2): NaVi won 19-16 Map 3 (Nuke) was not played as NaVi had already secured the victory 3.Key factors in NaVis victory: s1mples exceptional performance: He had 1.47 rating across the series Strong CT-side performance on both maps Better mid-round decision making and adaptability Effective utility usage, especially on Dust2 NaVi showed better composure in crucial moments 4.FaZes struggles: They couldnt find their usual form on their map picks Some key players underperformed compared to their usual standards Struggled with NaVis defensive setups 5.The series was notable for being one of s1mples best individual performances in major final, which was crucial factor in NaVis victory. Would you like me to elaborate on any of these points? In the 2022 PGL Major Antwerp, Natus Vincere (NAVI) did not beat FaZe Clan. Instead, FaZe Clan emerged victorious against NAVI in the grand final, winning the series 2-0. The final was highly anticipated matchup between the two top-ranked teams in the world, with FaZe Clan ultimately proving to be the superior team. The series was closely contested affair, with NAVI pushing FaZe to overtime in the first map, which FaZe won 19-16. Despite NAVIs strong performance, FaZe managed to secure the victory in the end. This win was significant for FaZe Clan as it marked their first Major victory and made them the first completely international roster to win Counter-Strike: Global Offensive Major. The victory was also redemption for FaZe Clan after their infamous loss to Cloud9 in the Boston Major in 2018. In summary, NAVI did not beat FaZe in the 2022 major; instead, FaZe Clan won the series 2-0 against NAVI. 61 MiniMax-01: Scaling Foundation Models with Lightning Attention C. Vision In-house User Experience Benchmark Examples C.1. Nature Example"
        },
        {
            "title": "User Request",
            "content": "According to the illustration, which planet has the most satellites? Which one has the most large satellites? Please specify the numbers separately. Also, do only planets have satellites? Checklist Correctly identifies the planet with the most satellites (Saturn, 9). Correctly identifies the planet with the most large satellites (Jupiter, 4). Identifies asteroids and dwarf planets that have satellites (Ida, Pluto, and Eris). MiniMax-VL-01 According to the illustration, the planet with the most satellites is Saturn, which has 9 satellites: Mimas, Enceladus, Tethys, Dione, Rhea, Titan, Hyperion, Iapetus, and Phoebe. The planet with the most large satellites is Jupiter, which has 4 large satellites: Io, Europa, Ganymede, and Callisto. Not only planets have satellites. In the illustration, we can see that besides planets, there are other celestial bodies that also have satellites. For example, the dwarf planet Pluto has one satellite: Charon; the dwarf planet Eris has one satellite: Dysnomia; even the asteroid Ida has one satellite: Dactyl. In summary, the planet with the most satellites is Saturn, the planet with the most large satellites is Jupiter, and celestial bodies with satellites are not limited to planets, but also include dwarf planets and asteroids. MiniMax-01: Scaling Foundation Models with Lightning Attention C.2. Daily Life Example"
        },
        {
            "title": "User Request",
            "content": "According to the instructions of this navigation, how should drive next? Checklist Confirmed that the action should take place after 38 meters. Correctly identified the need to turn right. Mentions the need to enter the ramp. MiniMax-VL-01 According to the directions on the navigation screen, you are currently driving on highway and are about to reach an exit. Specifically, you need to enter the ramp after 38 meters and be ready to leave the highway. Here are the detailed steps: 1. Keep current lane: Keep driving in your current lane, making sure you dont miss an upcoming exit ramp. 2. Deceleration: When approaching the exit ramp, slow down appropriately to ensure safe entry into the ramp. Usually, there will be speed sign on the exit ramp of the expressway, so keep an eye out for it. 3. Turn signal: Before you are ready to enter the ramp, turn on the right turn signal to alert the vehicle behind you that you are about to change lanes. 4. Approach ramp: Follow the navigation directions and enter the ramp after 38 meters. Be aware of curves and slopes on ramps and maintain an appropriate speed. 5. Continue to follow navigation: After entering the ramp, continue to follow the directions of the navigation. Navigation will guide you through the next segments until you reach your destination. Be aware of your safety and focus, especially when changing lanes on highways and onto ramps. Wishing you safe journey! 63 MiniMax-01: Scaling Foundation Models with Lightning Attention Example"
        },
        {
            "title": "User Request",
            "content": "How many of the outfits in each row in the picture are matched with long skirts? Checklist Accurately identify 3 outfits with long skirts in the first row (columns 2, 5, 6). Accurately identify 2 outfits with long skirts in the second row (columns 2, 6). Accurately identify 1 outfit with long skirt in the second row (column 1). MiniMax-VL-01 First row: 1. The first set: short skirts 2. The second set: long skirts 3. The third set: trousers 4. Fourth set: trousers 5. The fifth set: long skirts 6. Sixth set: long skirts There are total of 3 sets in the first row with long skirts. Second row: 1. The first set: trousers 2. The second set: long skirts 3. The third set: trousers 4. Fourth set: shorts 5. The fifth set: trousers 6. Sixth set: long skirts There are 2 sets of long skirts in the second row. Third row: 1. The first set: long skirts 3. The third set: trousers 4. Fourth set: trousers 5. The fifth set: trousers 2. Second set: trousers 6. Sixth set: trousers There is total of 1 set in the third row with long skirt. To sum up, there are 3 sets, 2 sets, and 1 sets of long skirts in each row. 64 MiniMax-01: Scaling Foundation Models with Lightning Attention C.3. Digital Scenarios Example"
        },
        {
            "title": "User Request",
            "content": "Based on the provided flowchart and statistical data, determine which step takes the longest and how to optimize it? Checklist Correctly identify the longest step: 53.6 seconds to enter invoice details. Measures to optimize invoice detail entry: automated entry, batch processing, simplified operations, etc MiniMax-VL-01 According to the provided flowchart and statistics, the longest step is \"Enter invoice details,\" with an average time of 53.6 seconds. To optimize this step, we can take the following measures: 1. Automate data entry: Consider using OCR (Optical Character Recognition) technology to automatically read invoice information and fill it into the system. This will significantly reduce the time spent on manual entry. 2. Simplify the data entry interface: Redesign the data entry interface to make it more intuitive and user-friendly. For example, use drop-down menus, auto-fill, and smart suggestion features to reduce user input time and error rates. 3. Batch processing: If possible, allow users to upload invoice information in batches instead of entering them one by one. This can be achieved by developing batch import feature, where users only need to upload file containing all invoice information, and the system will automatically parse and fill in the data. 4. Training and guidance: Provide users with detailed training and operation manuals to help them become familiar with the data entry process, reducing extra time consumption due to unfamiliarity with the operations. 5. Parallel processing: If the data entry task can be broken down into multiple sub-tasks, consider assigning these sub-tasks to different team members for parallel processing, thereby shortening the overall processing time. 6. Feedback mechanism: Establish feedback mechanism that allows users to report issues and difficulties encountered during the data entry process. Based on feedback, continuously improve the data entry process and tools. By implementing the above methods, we can significantly reduce the time spent on the \"Enter invoice details\" step, improving the overall efficiency of the process and user satisfaction. 65 MiniMax-01: Scaling Foundation Models with Lightning Attention Example"
        },
        {
            "title": "User Request",
            "content": "Based on the details in the image, what tasks might be prioritized in Katys schedule? Please explain your reasoning logic. Checklist Noticing that the \"Surprise Birthday Planning\" email is pinned, infer its importance and prioritize it. Identifying the \"Contoso Airlines\" email as important information for an upcoming trip, thus prioritizing it. Combining the content of the emails being edited, infer that she is arranging apartment viewings, so this task is also on the priority list. Recognizing that she will have lunch with Jane in half an hour. MiniMax-VL-01 Based on the details in the image, Katy may prioritize the following tasks in her schedule: Arrange apartment viewing: The email on the right side shows message Katy sent to Dianne Russell inquiring about the apartment viewing. The email mentions that Katy hopes to schedule time to view the apartment soon and notes that she is available Wednesday and Friday afternoons. This indicates that the apartment viewing is an important item on her current schedule and may be prioritized. Reply to birthday planning email: In the email list on the left, Elvia Atkins sent an email about birthday planning at 8:32 AM. This email may require Katy to respond or take action as soon as possible, especially if the birthday event is approaching. Confirm flight booking: Contoso Airlines sent confirmation email for the flight booking at 2:41 PM. This indicates that Katy may need to confirm her travel plans and ensure that all related matters are properly arranged. Team photo update: Lydia Bauer sent an email regarding the team photo at 11:10 AM. This email may require Katy to confirm or address the update of the team photo to ensure the accuracy and timeliness of team information. Yoga workshop arrangement: Daisy Philips sent an email about the yoga workshop at 9:16 AM. This indicates that Katy may need to arrange or confirm her participation in the yoga workshop, which is also important for her personal schedule. Lunch appointment: The top right corner of the image shows that Katy has lunch appointment with Jane in 30 minutes at Fourth Coffee. This is an upcoming activity and definitely priority. The reasoning is as follows: Email sending time and content: Earlier sent emails (such as birthday planning and yoga workshop) may need to be prioritized as they may involve upcoming events or deadlines. Nature of the emails: Emails related to personal arrangements (such as apartment viewing and flight booking) are usually prioritized as they directly affect personal life and work arrangements. Email priority: Some emails may be marked as high priority or require immediate response, which will affect Katys order of handling. In summary, Katy may prioritize tasks such as arranging apartment viewings, confirming birthday planning, and confirming flight bookings to ensure her personal and work schedules run smoothly. 66 MiniMax-01: Scaling Foundation Models with Lightning Attention C.4. Special Imaging Example"
        },
        {
            "title": "User Request",
            "content": "What does the left and right comparison of this image illustrate? Checklist Point out that these are two photos taken by satellite remote sensing. Compare the time, with the left side being August 2023 and the right side being September 2023. Show that large areas have changed color (from green to brown and gray). The reasons for changes in vegetation conditions can be fires, droughts, etc. MiniMax-VL-01 This image shows comparison of satellite images of the same area in August 2023 and September 2023. From the images, it can be seen that the August image (left) shows rich vegetation cover in the area, with green areas widely distributed, indicating that the vegetation was growing well at that time. However, by September (right), the vegetation cover in the area had significantly decreased, with many green areas turning brown or becoming bare land, suggesting that the vegetation may have experienced significant reduction or degradation. This change may be caused by various factors, including but not limited to climate change, natural disasters (such as fires or droughts), and human activities (such as agricultural development or deforestation). The specific reasons need to be analyzed in conjunction with more data and background information. For example, if the area experienced severe drought or fire in September, then the reduction in vegetation can be explained. Additionally, human activities such as large-scale agricultural development or deforestation may also lead to significant decrease in vegetation cover. Overall, this comparative image clearly shows the significant changes in vegetation cover in the area within month, prompting us to pay attention to and study the reasons behind these changes in order to take appropriate measures to protect and restore the ecological environment. 67 MiniMax-01: Scaling Foundation Models with Lightning Attention D. Evaluation Details for Selected Benchmarks D.1. MMLongBench-Doc For evaluating models with context-length and image-number limitations that exceed requirements of MMLongBench-Doc, we adopt an image concatenation approach suggested by the original repository7, resulting in the concatenation of all images extracted from single PDF input into 5 images for the open-source models evaluated and 10 for Claude-3.5-Sonnet-1022. For evaluating other commercial models and MiniMax-Text-01, we use the default configuration which sets the maximum number of image pages to 120 and resolution to 144. D.2. MEGA-Bench MEGA-Bench is comprehensive multimodal benchmark that spans 7 input formats, 6 output formats, 10 different types of skills, and varying forms of visual inputs, including images and videos. Each request may consider multiple images, consisting of visual task description, request-response demonstration and videos. For video inputs, the benchmark slices each video into multiple frames. The number of frames and the resulting number of total input images are limited to the models context length and image constraints. We follow the general principles of the original repository8 when deciding our evaluation configurations, as detailed in Table 14. Table 14 Configuration of different models for MEGA-Bench. Model/Configuration. MAX_NUM_IMAGE TOTAL_DEMO_VIDEO_FRAMES GPT-4o-2024-1120 Claude-3.5-Sonnet-1022 Gemini-1.5-Pro-002 Gemini-2.0-Flash-exp InternVL2.5-78B Qwen2-VL-72B-Instruct LLama-3.2-90B MiniMax-VL-01 D.3. MMMU & DocVQA 64 64 128 128 24 10 10 128 8 8 16 16 2 1 1 16 We note that rule-based methods may misjudge cases where the correct answer has mulitple forms (e.g. U.S. vs. United States), we adopt GPT-4o (specifically GPT-4o-2024-05-13) as the judge model if the rule-based method fails for MMMU and DocVQA evaluation. 7https://github.com/mayubo2333/MMLongBench-Doc 8https://github.com/TIGER-AI-Lab/MEGA-Bench/blob/main/megabench/models/model_type.py"
        }
    ],
    "affiliations": []
}