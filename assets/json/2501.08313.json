{
    "paper_title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
    "authors": [
        "MiniMax",
        "Aonian Li",
        "Bangwei Gong",
        "Bo Yang",
        "Boji Shan",
        "Chang Liu",
        "Cheng Zhu",
        "Chunhao Zhang",
        "Congchao Guo",
        "Da Chen",
        "Dong Li",
        "Enwei Jiao",
        "Gengxin Li",
        "Guojun Zhang",
        "Haohai Sun",
        "Houze Dong",
        "Jiadai Zhu",
        "Jiaqi Zhuang",
        "Jiayuan Song",
        "Jin Zhu",
        "Jingtao Han",
        "Jingyang Li",
        "Junbin Xie",
        "Junhao Xu",
        "Junjie Yan",
        "Kaishun Zhang",
        "Kecheng Xiao",
        "Kexi Kang",
        "Le Han",
        "Leyang Wang",
        "Lianfei Yu",
        "Liheng Feng",
        "Lin Zheng",
        "Linbo Chai",
        "Long Xing",
        "Meizhi Ju",
        "Mingyuan Chi",
        "Mozhi Zhang",
        "Peikai Huang",
        "Pengcheng Niu",
        "Pengfei Li",
        "Pengyu Zhao",
        "Qi Yang",
        "Qidi Xu",
        "Qiexiang Wang",
        "Qin Wang",
        "Qiuhui Li",
        "Ruitao Leng",
        "Shengmin Shi",
        "Shuqi Yu",
        "Sichen Li",
        "Songquan Zhu",
        "Tao Huang",
        "Tianrun Liang",
        "Weigao Sun",
        "Weixuan Sun",
        "Weiyu Cheng",
        "Wenkai Li",
        "Xiangjun Song",
        "Xiao Su",
        "Xiaodong Han",
        "Xinjie Zhang",
        "Xinzhu Hou",
        "Xu Min",
        "Xun Zou",
        "Xuyang Shen",
        "Yan Gong",
        "Yingjie Zhu",
        "Yipeng Zhou",
        "Yiran Zhong",
        "Yongyi Hu",
        "Yuanxiang Fan",
        "Yue Yu",
        "Yufeng Yang",
        "Yuhao Li",
        "Yunan Huang",
        "Yunji Li",
        "Yunpeng Huang",
        "Yunzhi Xu",
        "Yuxin Mao",
        "Zehan Li",
        "Zekang Li",
        "Zewei Tao",
        "Zewen Ying",
        "Zhaoyang Cong",
        "Zhen Qin",
        "Zhenhua Fan",
        "Zhihang Yu",
        "Zhuo Jiang",
        "Zijia Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 3 1 3 8 0 . 1 0 5 2 : r MiniMax-01: Scaling Foundation Models with Lightning Attention MiniMax1 We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI. Figure 1 Benchmark performance. (a) MiniMax-Text-01 on core text benchmarks. (b) MiniMaxVL-01 on core multimodal benchmarks. (c) MiniMax-Text-01 on the long-context RULER (Hsieh et al., 2024) benchmark. The performance of leading commercial and open-source models is presented for reference. 1Please send correspondence to model@minimaxi.com. 2025 MiniMax. All rights reserved MiniMax-01: Scaling Foundation Models with Lightning Attention 1. Introduction Large Language Models (LLMs) (Anthropic, 2024; Dubey et al., 2024; Hurst et al., 2024; Team et al., 2024a) and Vision Language Models (VLMs) (Anthropic, 2024; Dubey et al., 2024; Hurst et al., 2024; Team et al., 2024a) have made rapid progress in recent years, excelling at tasks like knowledge Q&A, complex reasoning, mathematics, coding, and vision-language understanding. The context window for most models currently ranges from 32K to 256K tokens. However, these lengths often fall short of practical needswhether using professional book as context, assisting with an entire programming project, or maximizing the potential of in-context learning through many-shot examples. Context window expansion in the past two years has primarily resulted from more powerful GPUs and better I/O-aware softmax attention implementation (Dao et al., 2022; Liu et al., 2024a). However, extending these windows further has proven challenging. This limitation arises from the inherent quadratic computational complexity of the transformer (Vaswani et al., 2017) architecturefurther length extension causes computational demands to grow much faster than hardware capabilities can match. To address this challenge, researchers have proposed various methods for reducing the attention mechanisms computational complexity: sparse attention (Beltagy et al., 2020; Zaheer et al., 2020), linear attention (Qin et al., 2022a,b, 2024c), long convolutions (Qin et al., 2023a), state space models (the Mamba series) (Dao and Gu, 2024; Glorioso et al., 2024; Gu and Dao, 2024; Ren et al., 2024; Team et al., 2024b), and linear RNNs (Qin et al., 2023b, 2024d). Despite their theoretical promise, these innovations have seen limited adoption in commercial-scale models. In this report, we aim to build model that matches the performance of leading commercial models while providing context window longer by an order of magnitude. This ambitious objective requires carefully balancing multiple factors: network architecture, data, and computation. Our approach begins with selecting the most promising architecture, succeeded by the optimization of the underlying training and inference framework to ensure its support. For the network architecture, we required linear attentionnot just theoretically sound but highly efficient in practice, especially with long contexts. After extensive experimentation, we settled on hybrid architecture mainly using lightning attention (Qin et al., 2024b), an I/O-aware implementation of linear attention variant (Qin et al., 2022a). In the architecture, one transformer block with softmax attention follows every seven transnormer blocks (Qin et al., 2022a) with lightning attention. We determined the models total parameters based on practical constraint: the ability to process more than 1 million tokens on single machine with up to 8 GPUs and 640GB memory using 8-bit quantization. To maximize parameter and computation capacity, we implemented Mixture of Experts (MoE) (Fedus et al., 2022; Lepikhin et al., 2021). We comprehensively consider training resources, inference resources, and the final model performance, aiming to find better balance among the three. Extensive experiments guided us toward the final model specifications: 456 billion parameters, 45.9 billion activations, and 32 experts. Existing distributed training and inference frameworks are primarily optimized for softmax attention. However, our novel architecture, which integrates lightning attention, softmax attention, and MoE, necessitates complete redesign of both our training and inference frameworks. Furthermore, the framework must possess the capability to support the training and inference of models with hundreds of billions of parameters and context windows extending over millions of tokens. To this end, we implement the all-to-all communication in MoE using expert parallel (EP) and expert tensor parallel (ETP). It aims to minimize the overhead associated with inter-GPU communication. To facilitate context windows with unlimited expansion, we design varlen ring attention to reduce the redundancy in computation and the improved version of Linear Attention Sequence Parallelism (LASP) (Sun et al., 2024) to fully utilize the devices parallel capabilities. Additionally, we have implemented 2 MiniMax-01: Scaling Foundation Models with Lightning Attention comprehensive set of CUDA kernels tailored for lightning attention inference, achieving over 75% Model Flops Utilization (MFU) (Chowdhery et al., 2023) end-to-end on the Nvidia H20. Building upon the architecture design and computation optimizations, we train our foundational language model, MiniMax-Text-01. Our pre-training process began with curating diverse and high-quality corpus through rigorous data cleaning, reward-based quality enhancement, and better data mixture balancing, validated through systematic repetition-aware testing. To fully utilize the architectures long-context capability, we introduce in-depth analysis of the hyperparameters and propose three-stage training procedure, successfully extending the context window to one million tokens. During the alignment phase, we incentivize the models various capabilities through precisely tuned reward dimensions and multi-stage training methodology, especially in the areas of long-context and realworld scenarios. Subsequently, we augment our language model with visual capabilities by integrating lightweight Vision Transformer (ViT) (Dosovitskiy et al., 2021) module, thereby creating our vision-language model, MiniMaxVL-01. MiniMax-VL-01 undergoes additional training with 512 billion vision-language tokens, utilizing four-stage training process. The final stage of this training is specifically designed to optimize the user experience. Figure 2 Prefilling latency of different models. The MiniMax-Text-01 and Llama3-70B models are tested on H800 GPUs with tensor parallelism set to 8, utilizing custom inference framework with 8bit weight-only quantization (W8A16). Other models are tested through their official APIs. Within the maximum length supported by each model, sufficient number of uniformly distributed points were selected for testing. After removing outliers, the data is fitted with quadratic function. Comprehensive evaluations on core academic benchmarks demonstrate that both models attain performance levels comparable to those of closed-source top-tier models in both text and vision-language tasks, as illustrated in Figure 1 (a,b). For contexts longer than 200k, our model performs significantly better, as shown in Figure 1 (c). In addition to academic benchmarks, we also assess the models performance using in-house benchmarks derived from real-world usage and show that our model is top-tier in those scenarios. In addition to its performance, our model exhibits significant advantages in prefilling latency, attributed to its novel architecture, as illustrated in Figure 2. We summarize our contributions as follows: 1. We build model that rivals the top-tier closed-source models on standard academic benchmarks. Furthermore, this model supports context inputs of up to 4 million tokens, showcasing outstanding performance in long-context evaluations. 2. We demonstrate the first successful large-scale implementation of linear attention. While linear attention has been studied before, it has never been deployed at this scale. We provide comprehensive details on our algorithm design and engineering optimizations. 3. We outline practical approach and experimental methodology for the exploration of various models, datasets, evaluations, and algorithms, which may serve as valuable reference. 4. We publicly release the weights and offer cost-effective API, aiming to help others develop 3 MiniMax-01: Scaling Foundation Models with Lightning Attention models that push beyond current limitations. 2. Model Architecture In this section, we present the design of our network architecture. To achieve optimal performance within constrained resources and better handle longer sequences, we adopt MoE approach and employ linear attention as much as possible instead of the traditional softmax attention used in standard transformers. To facilitate more intuitive understanding, we illustrate the main architecture in Figure 3. Our design follows the Transformer-style block, with each comprises channel mixer (an attention block) and feature mixer (an MLP block). We employ two types of channel mixers: lightning attention and softmax attention. The feature mixer is an MoE that incorporates multiple feed-forward networks (FFNs). To ensure load balancing in the MoE blocks, we propose novel load balancing strategy inspired by GShard (Lepikhin et al., 2021), which we refer to the global router. This strategy is designed to maintain training stability. Additionally, DeepNorm (Wang et al., 2024a) is integrated to enhance overall performance. Output Hidden RMSNorm The final MiniMax-Text-01 architecture integrates both linear attention and softmax attention mechanisms in structured pattern. Specifically, transformber block with softmax attention is positioned after every 7 transnormer blocks (Qin et al., 2022a) of linear attention, leading to total of 80 layers. Each attention module is composed of 64 heads, each with head dimension of 128. The softmax attention layers employ Group Query Attention (GQA) (Ainslie et al., 2023) with group size of 8. Rotary Position Embedding (RoPE) (Su et al., 2024) is applied to half of the attention head dimension, with base frequency set to 10,000. The models hidden size is configured to 6144, and each layer incorporates 32 experts with top-2 routing strategy. The feed-forward network within each expert has hidden dimension of 9216. In total, MiniMax-Text-01 compromises 456 billion parameters, of which 45.9 billion are activated for each processed token. + ùõº MoE 1 RMSNorm + ùõº Softmax Attention RMSNorm + ùõº MoE RMSNorm + ùõº Lightning Attention + FFN 1 FFN 2 ... FFN ùëÅ Router ùêæ = Input Hidden Linear RMSNorm SiLU SiLU SiLU Sigmoid V Figure 3 The architecture of MiniMax-Text-01. In the subsequent sections, we will delve into our considerations regarding the model architecture, i.e., the integration of different attention mechanisms, the synergy between MoE and linear attention, the rationale behind hyperparameter selection, and the methodology for determining the models size based on scaling laws. 2.1. Mixture of Experts MoE provides pathway to enhance both scalability and efficiency compared to the dense version. Typically, MoE is substitute for the feed forward networks (FFN) in feature-mixer layers (Fedus 4 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 4 Isoflop Comparison: MoE vs. Dense on various benchmarks. Both models are trained on 1 trillion tokens. The gray dashed lines indicate the difference in the computation required for the two models to achieve the same performance. et al., 2022; Lepikhin et al., 2021), which consists of multiple FFN experts, where each token is routed to one or more of these experts. Specifically, for an input token xùë°, its corresponding output hidden state hùë° is calculated as: hùë° = ùê∏ ùëñ=1 Softmaxùëñ (cid:0)TopK(xùë° Wùëî)(cid:1) FFNùëñ (xùë°), (1) where ùê∏ represents the total number of experts, Wùëî is the weight of the gate, FFNùëñ stands for the ùëñ-th expert, and TopK() denotes the operation that preserves the top ùëò scores among all ùê∏ experts while setting the remaining scores to . The training of MoE based LLMs can be categorized into token-drop and dropless. We adopt the token-drop strategy to improve training efficiency. With this approach, each expert is assigned capacity limit specifying the maximum number of tokens it can handle. Once this capacity is reached, any additional token routed to that expert is discarded. To assess the effectiveness of the MoE architecture, we conduct comparative study between dense model with 7 billion parameters and an MoE model with 2 billion activation parameters out of total of 24 billion parameters. The results, as illustrated in Figure 4, demonstrate that the MoE model significantly outperforms the dense model under the same computational budget on various benchmarks, including HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), Natural Questions(Kwiatkowski et al., 2019), PIQA(Bisk et al., 2020) and TriviaQA(Joshi et al., 2017). When scaling up to larger models, we encounter the challenge of routing collapse, which arises due to the concentrated distribution of tokens designated for allocation. To mitigate this issue, we incorporate simple global routing strategy to the GShard (Lepikhin et al., 2021) auxiliary loss for better load balancing. Auxiliary Loss. To ensure differentiability, the auxiliary loss is defined as ùêøaux = ùõºaux 1 ùëìùëñ ùëöùëñ, ùê∏ where ùõºaux represents the coefficient of the auxiliary loss, ùëìùëñ denotes the fraction of tokens assigned to the ùëñ-th expert, and ùëöùëñ is the average routing probability of expert ùëñ. (cid:205)ùê∏ ùëñ=1 Global Router. The GPU memory size constrains the micro batch size in LLM training, leading to substantial fluctuations in the token distribution within individual Expert Parallel (EP) groups. Moreover, token distributions vary across different EP groups, potentially resulting in load imbalances where experts in one EP group may be overloaded while those in another are underutilized. To address this, we implement global token dispatching strategy across EP groups. Specifically, we introduce an additional allgather communication step to synchronize the number of tokens awaiting processing by each expert before dispatching tokens across different EP groups. Under the same 5 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 5 Illustration of the computations for softmax attention (left) and linear attention (right). The input length is ùëÅ and feature dimension is ùëë, with ùëë ùëÅ. Tensors in the same box are associated with computation. The linearized formulation allows ùëÇ(ùëÅ) time and space complexity. capacity constraints, this global routing mechanism can effectively reduce the overall token drop rate, thereby ensuring training stability. 2.2. Linear Attention Linear attention utilizes the right product kernel trick to transform quadratic computational complexity into linear complexity, as illustrated in Figure 5. By taking TransNormer (Qin et al., 2022a) as an example, the NormAttention mechanism can be written as: = Norm((QK)V), (2) where Q, K, and ‚Ñùùëõùëë are the query, key, and value matrices, respectively, with ùëõ for sequence length and ùëë for feature dimension. The equation can be transformed into its linear variant using right matrix multiplication: = Norm(Q(KV)), (3) The linear formulation facilitates efficient recurrent prediction with training complexity of ùëÇ(ùëõùëë2). Furthermore, linear attention ensures constant computational complexity of ùëÇ(ùëë2), irrespective of the sequence length. This is accomplished by recurrently updating the term KV, thereby obviating the need for repetitive computation of the entire attention matrix. In contrast, softmax attention incurs complexity of ùëÇ(ùëõùëë2) during inference. When addressing causal language modeling tasks, the efficacy of the right product is compromised, necessitating the computation of cumsum (Hua et al., 2022). This limitation impedes the realization of highly efficient parallel computation, which likely explains why, despite being proposed by Br√©bisson et al. (de Br√©bisson and Vincent, 2016) nine years ago, none of the current leading open-source LLMsincluding LLaMA3 (Dubey et al., 2024), Qwen2.5 (Yang et al., 2024), DeepSeekV3 (DeepSeekAI, 2024), and Mistral (Jiang et al., 2023)have adopted this linear attention mechanism. 2.2.1. Lightning Attention Lightning attention (Qin et al., 2024b,c) represents an I/O-aware, optimized implementation of TransNormer (Qin et al., 2022a). This approach identifies the primary bottleneck in the computational efficiency of existing linear attention mechanisms: the slow cumsum operation inherent in causal 6 MiniMax-01: Scaling Foundation Models with Lightning Attention language modeling. To alleviate this problem, Lightning Attention proposes novel tiling technique that effectively circumvents the cumsum operation. The key innovation lies in the strategic division of the attention calculation into two distinct components: intra-block and inter-block computations. The left product attention calculation is employed for intra-block operations, while the right product is utilized for inter-block operations. This division is crucial because the intra-blocks can be significantly reduced in size, thereby ensuring that the overall computational complexity remains linear. Note that the lightning attention was originally proposed by our team members in Qin et al. (2024c), we recall some of the core processes to elucidate why it can achieve theoretical linear complexity in practice for the sake of completeness. In the interest of analytical tractability, we deliberately omit the consideration of normalization, sigmoid linear unit (SiLU) activation, and gating mechanisms in the following derivation. Let us start with the forward pass in lightning attention. The left product in causal attention calculation is defined as: = [(QK) M]V (4) where Mùë°ùë† = 1 if ùë° ùë†, otherwise 0. The right product operation can be computed in recursive formula as: kv0 = 0, kvùë° = kvùë°1 + kùë°v ùë° , ùë° = ùë° kvt. (5) It is important to note that while Eq. 5 exhibits linear computational complexity, it is inherently unparallelizable. The fundamental concept underlying the implementation of lightning attention involves the utilization of tiling technique to compute attention scores. Specifically, the matrices Q, K, are partitioned into two distinct blocks along the row dimension: (cid:20)X1 X2 , X1 ‚Ñùùëöùëë, X2 ‚Ñù(ùëõùëö) ùëë, {Q, K, V}. = (cid:21) By unfolding Eq. 4, we obtain the following expression (noting that kv0 = 0): ùë† ùë† ùëóv ùëó , ùë† = 1, . . . , ùëö. ùë† = ùë† kvùë† = ùë† kv0 + ùë† kvùë† = kv0 + ùëóv ùëó . Rewrite it in block form, we have: ùëó=1 ùëó=1 O1 = Q1kv0 + [(Q1K 1 ) M]V1 Q1KV0 + [(Q1K 1 ) M]V1. (6) (7) As shown, the intra-block [(Q1K 1 use the right product. Note that the intra-block can be further divided using the same strategy: ) M]V1 can use the left product and the inter-block Q1KV0 can kvùëö+ùë° = kvùëö + ùëö+ùë° ùëó=ùëö+1 ùëóv ùëó , ùë° = 1, . . . , ùëõ ùëö, ùëö+ùë° = ùëö+ùë°kvùëö+ùë°, O2 = Q2kvùëö + [(Q2K 2 ) M]V2 Q2KV1 + [(Q2K 2 ) M]V2. To compute the second block, we use KV1 = kvùëö, which can be computed by: KV1 = KV0 + ùëö ùëó=1 kùëöv ùëö = KV0 + 1 V1. (8) (9) where KV0 = kv0. By recursively applying the aforementioned strategy of partitioning the matrix into multiple blocks, the practical computational complexity can be reduced to linear. The final time complexity of lightning attention is ùëÇ(ùëõùëë2 + ùëõùêµùëë), where ùêµ is the block size. Algorithm 1 illustrates the IO-aware implementation of lightning attention forward pass. 7 MiniMax-01: Scaling Foundation Models with Lightning Attention Algorithm 1 Lightning Attention Forward Pass ùêµ blocks X1, X2, ...Xùëá of size ùêµ ùëë each, where {Q, K, V, O}. Input: Q, K, ‚Ñùùëõùëë, block sizes ùêµ. Divide into ùëá = ùëõ Initialize mask ‚Ñùùêµùêµ, where Mùë°ùë† = 1, if ùë° ùë†, else 0. Initialize KV = 0 ‚Ñùùëëùëë. for ùë° = 1, . . . , ùëá do Load Qùë°, Kùë°, Vùë° ‚Ñùùêµùëë from HBM to on-chip SRAM. On chip, compute Ointra = [(Qùë°K On chip, compute Ointer = Qùë° (KV). On chip, compute KV = KV + ùë° Vùë°. Write Oùë° = Ointra + Ointer to HBM as the ùë°-th block of O. ùë° ) M]Vùë°. end for Return O. 2.2.2. Effectiveness of Lightning Attention Although lightning attention demonstrates promise and competitive performance in small-scale experiments, its scaling behavior and capability in the downstream tasks under large-scale settings remain unexplored. To mitigate the gap, we conduct series of scaling experiments to evaluate the scalability of the lightning attention mechanism in comparison to softmax attention, meanwhile verifying the performance on the extensive downstream tasks. It is noteworthy that during our experiments, we observed that lightning attention demonstrates limited retrieval capabilities. This finding inspired us to explore hybrid approach (Hybrid-lightning) that takes the advantages of both lightning and softmax attention to enhance retrieval performance by substituting lightning attention with softmax attention at intervals of every eight layers. We adhere to the FLOPs calculation methodology established by Kaplan et al. (2020). For the purpose of our analysis, we define the following variables: ùëô (number of layers), ùëë (model dimension), ‚Ñé (number of attention heads), ùëè (batch size) and ùëõ (sequence length). The checklist of model parameters and FLOPs is presented in Table 1. Table 1 Model Parameters and FLOPs Comparisons Across Architectures. For scaling law calculations, embedding parameters and other subleading terms are excluded to improve alignment with fitted results. Architecture Parameter count FLOPs count Softmax Attention Lightning Attention Hybrid-lightning 12ùëôùëë2 12ùëôùëë2 + 2ùëôùëë2/‚Ñé 12ùëôùëë2 + 7ùëôùëë2/4‚Ñé 72ùëèùëõùëôùëë2 (1 + ùëõ 72ùëèùëõùëôùëë2 (1 + 1 6ùëë + 5 2‚Ñé + 5 18ùëë ) 18ùëë ) 16‚Ñé + 5 18ùëë ) 72ùëèùëõùëôùëë2 (1 + ùëõ 48ùëë +"
        },
        {
            "title": "2.2.2.1 Experimental Setup",
            "content": "We conducted training on softmax (equipped with FlashAttention-2 (Dao, 2024)), lightning attention, and hybrid-lightning attention models across various scales: 70 million, 160 million, 410 million, 1 billion, 3 billion, and 7 billion parameters. Each model was trained on dataset consisting of up to 300 billion tokens, with context length of 8192. Our training methodology follows the approach proposed by Chinchilla (Hoffmann et al., 2022), where the training loss serves as direct indicator of test performance. For each model architecture and training sequence length, we maintained 8 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 2 Summary of Scaling Laws: It shows the relationships between loss (ùêø), optimal model size (ùëÅùëúùëùùë°), and optimal dataset size (ùê∑ùëúùëùùë°) as functions of computational budget (ùê∂). It reveals that, given the same budget, the hybrid model uses more parameters and tokens but achieves lower loss. Arch Softmax Attention Lightning Attention Hybrid-lightning ùêø(ùê∂) 3.7087ùê∂ 0.0798 3.5391ùê∂ 0.0768 3.4797ùê∂ 0.0763 ùëÅùëúùëùùë° (ùê∂) (1.82 108)ùê∂0.7118 (2.74 108)ùê∂0.6470 (2.57 108)ùê∂0.6670 ùê∑ùëúùëùùë° (ùê∂) (2.56 1010)ùê∂0.5102 (4.43 1010)ùê∂0.4684 (3.70 1010)ùê∂0.4707 Figure 6 Summary of Scaling Laws. Training curves (left) span models from 70M to 7B parameters. Optimal model size (center) and training tokens (right) are derived based on specified compute budget estimation. uniform global batch size of 4 million tokens. The Adam optimizer was employed, configured with learning rate of 3e-4 and weight decay of 0.1. fixed learning rate scheduler was applied across all experiments due to constrained computational resources. We employ diverse set of evaluation benchmarks, including BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC (both easy and challenge variants) (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), Needle in Haystack (NIAH) (Shen et al., 2024), and SCROLLS (Shaham et al., 2022). Each benchmark assesses distinct capabilities of the models."
        },
        {
            "title": "2.2.2.2 Scaling Laws",
            "content": "We fit the scaling curves based on our experiments over the above mentioned settings, where we alter the model size (ùëÅ) and dataset size (ùê∑) for different computational budget (ùê∂) and observe the corresponding training loss (ùêø) that serving as an estimator of test loss. We begin by establishing power-law relationships between ùêø and ùê∂, following Chinchillas methodology (Hoffmann et al., 2022). Using the fitted curve, we derive coefficients for optimal model size ùëÅùëúùëùùë° ùê∂ùëé and optimal dataset size ùê∑ùëúùëùùë° ùê∂ùëè. The original scaling laws (Kaplan et al., 2020) use ùêø(ùëã) = (ùëã0/ùëã)ùõºùëã , while subsequent studies (Clark et al., 2022; Gao et al., 2024; Henighan et al., 2020; Hoffmann et al., 2022) employ ùêø(ùëã) = ùúñ + (ùëã0/ùëã)ùõºùëã for better fitting, where ùúñ denotes the irreducible loss. For simplicity, we unify these forms into ùêø(ùëã) = ùõΩùëã ùëã ùõºùëã , facilitating direct comparison of scaling capabilities based on ùõºùëã and ùõΩùëã . The summary of scaling laws is shown in Table 2 and Figure 6. It can be intuitively understood that given the same computational budget, models with lightning attention tend to utilize more parameters and tokens, yet they achieve lower loss compared to models with pure softmax attention. 9 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 7 Larger models and hybrid-lightning attention achieve the best performance across benchmarks. Performance is evaluated on CSR (Common Sense Reasoning), NIAH (Needle in Haystack), and SCROLLS benchmarks using three attention mechanism models from 410M to 7B parameters."
        },
        {
            "title": "2.2.2.3 Performance on Downstream Task.",
            "content": "We present the benchmark results of downstream tasks in Figure 7. Lightning attention demonstrates comparable performance across most downstream tasks, with the exception of NIAH. This indicates that linear attention exhibits similar language modeling capabilities to Transformer models but falls short in retrieval tasks, rendering it unsuitable for LLMs. However, the hybrid-lightning attention not only matches but surpasses the retrieval and extrapolation capabilities of softmax attention, making it well-suited for in-context learning in LLMs."
        },
        {
            "title": "2.2.2.4 Speed.",
            "content": "We assess the end-to-end training speed of softmax attention, lightning attention, and hybridlightning models with 3 billion parameters by measuring the tokens processed per GPU per second (TGS). For completeness, we also included popular linear models such as HGRN2 and Mamba2 in our evaluation. For the speed benchmark, the training context length was gradually increased until reaching the out-ofmemory limit on single-node H800 GPUs. As illustrated in Fig. 8, lightning attention achieves constant training speed irrespective of the sequence length and is the sole linear model that outperforms FlashAttention2. Figure 8 The training speed of various attention mechanisms, including softmax, lightning, hybridlightning, HGRN2, and Mamba2, was benchmarked across sequence lengths ranging from 1,024 to 65,536. Performance was measured in terms of training speed, reported as tokens processed per GPU per second (TGS). 10 MiniMax-01: Scaling Foundation Models with Lightning Attention 2.2.3. Hybrid Architecture Our preliminary experiments with the hybrid architecture have yielded promising results, motivating us to delve deeper into its potential through two variants: hybrid-cosformer2 and hybrid-hgrn2. In the hybrid-cosformer2 model, we replace the linear attention layers in the cosformer2 architecture with softmax attention layers at intervals of every eight layers. This substitution strategy is similarly applied in the hybrid-hgrn2 model. We conduct experiments using consistent setups to evaluate the downstream performance of these alternatives. Our findings, as summarized in Table 3, indicate that the hybrid-lightning model achieves the best performance. Table 3 Benchmarking various hybrid-linear models with 1 Billion Parameters. We present the average CSR score, weighted average accuracy for NIAH, and the average SCROLLS score. Higher scores indicate better performance across all tasks. Abbreviations: TGS (token per gpu per second), HS (HellaSwag), WG (WinoGrande), OBQA (OpenBookQA), NIAH, and SCR (SCROLLS). Hybrid-linear Arch. TGS PIQA HS WG ARC-E ARC-C OBQA CSR NIAH SCR Hybrid-cosformer2 Hybrid-hgrn2 Hybrid-lightning 23.3K 70.29 45.63 51.46 29.5K 70.89 51.23 56.51 33.4K 70.73 50.41 55.80 55.77 59.68 59.93 26.11 28.50 27. 30.60 46.64 32.40 49.87 32.80 49.55 43.6 91.8 95.7 10.9 10.8 13.3 In addition to linear models, sliding window attention can also achieve linear computational complexity by appropriately adjusting the window size. As it is grounded in softmax attention, it serves as robust baseline for evaluating linear architectures. Therefore, we incorporated the hybrid-window approach by replacing the sliding window attention with full softmax attention every eight layers. We evaluated various window sizes of SWA ranging from 256 to 1024. Our results indicate that larger window sizes lead to slower training speeds compared to the hybrid-lightning model. To compare these models under equivalent speed conditions, we did not consider window sizes larger than 1024. As shown in Table 4, the hybrid-lightning model outperforms all other models across all metrics, particularly excelling in the NIAH benchmark. Table 4 Benchmark comparison of hybrid-lightning and hybrid-window Models. Metrics include average CSR score, weighted NIAH accuracy, and average SCROLLS score. Higher scores indicate better performance across all tasks. Abbreviations: PS (parameter size, billion), W.S. (window size of SWA), HS (HellaSwag), WG (WinoGrande), OBQA (OpenBookQA), NIAH, SCR (SCROLLS), TGS (token per gpu per second). P.S 1B 3B W.S. Arch. 256 512 1024 TGS PIQA HS WG ARC-E ARC-C OBQA CSR NIAH SCR 35.6K 70.29 48.68 53.35 Hybrid35.1K 70.95 48.19 52.33 window 33.6K 69.75 47.80 53.12 Hybrid-lightning 33.4K 70.73 50.41 55.80 16.1K 73.83 59.70 59.59 Hybrid15.8K 73.29 60.00 59.04 window 15.4K 74.27 59.02 57.85 Hybrid-lightning 15.1K 74.21 61.06 59. 48.61 32.60 47.70 30.00 31.60 48.02 32.80 49.55 54.31 35.00 36.00 53.97 33.00 53.44 35.80 55.16 28.75 27.22 28.33 27.65 33.62 32.51 31.91 34.90 57.95 57.53 57.53 59.93 64.10 62.96 64.56 65.49 10.6 11.9 10.6 13.3 14.2 14.2 13.3 14.7 46.8 25.7 53.9 95.7 40.9 57.9 41.6 98.0 256 512 2.2.4. Discussion Based on our analysis of scaling law experiment, downstream performance and speed comparison, we conclude that while pure linear attention models are computationally efficient, they are not suitable 11 MiniMax-01: Scaling Foundation Models with Lightning Attention for LLMs. This is due to their inherent inability to perform retrieval, capability that is essential for in-context learning. In contrast, our hybrid model not only matches but also surpasses softmax attention in both retrieval and extrapolation tasks. This outcome is somewhat counterintuitive. To understand this phenomenon, consider the following explanation of softmax attention: = Softmax(QK/ ùëë)V. (10) It can be rewritten into linear recurrent form as: ùë†0 ùë° = 0, ùë° = ùë† ùëó1 ùë† ùëó ùë° + exp(qùë°kùëá ùëó / ùëë), ùë° = (ùë† ùëó1 ùëó ùë° /ùë† ùëó ùë° )o ùëó1 ùë° + (1 ùë† ùëó1 ùë° /ùë† ùëó ùë° )v ùëó, oùë° = oùë° ùë°, ùëó = 1, . . . , ùë°. (11) Note that the linear recurrence form of lightning attention is as follows: kv0 = 0, kv ùëó = kv ùëó1 + ùëóv ùëó ùëó = kv ùëó ùëó, ùëó = 1, . . . , ùë°. (12) The softmax attention mechanism can be interpreted as linear RNN (Qin et al., 2024a). At each time step ùë°, the hidden state is recalculated starting from the initial time ùë°0 = 1, process often described as \"Going Through Book.\" This method enables the model to accurately retain input information by systematically revisiting previous data. In contrast, linear models lack this recomputation process, which hinders their ability to effectively retain input data. Let us define the capacity of an RNN as the size of its recurrent state. Upon closer examination of Eq. 11, we can deduce that the capacity of softmax attention is ùëÇ(ùëë). In contrast, as illustrated in Eq. 12, the capacity of lightning attention is ùëÇ(ùëë2/‚Ñé). Given that ùëë > ‚Ñé, it follows that lightning attention possesses larger capacity than softmax attention. Consequently, the hybrid-lightning model exhibits superior retrieval and extrapolation capabilities compared to models relying solely on softmax attention. 2.3. Module Ablations in MoE Based on the conclusions from previous sections, we conduct two additional sets of ablation experiments to validate module choices within the MoE architecture on larger scale: (1) Hybrid-lightning attention versus softmax attention: To verify the advantages of the hybrid lightning attention in the MoE. (2) Pre-Layer Normalization versus Post-Layer Normalization: In our hybrid architecture, the effective depth of the model plays significant role. Thus, we expect to find better normalization algorithm for the deep model. Hybrid-lightning Attention versus Softmax Attention. We perform small-scale comparative analysis between softmax attention and hybrid-lightning attention within the MoE architecture. Specifically, we use 28 billion parameter MoE with 5 billion activation parameters that utilize softmax attention as the base model. For every 8 consecutive layers in the base model, we systematically replace softmax attention with lightning attention in the first 7 layers. Both the base model and the modified model are trained on 1 trillion tokens. As shown in Table 5, the results reveal that substituting certain softmax attention layers with lightning attention improves accuracy across most benchmarks. Pre Layer Normalization versus Post Layer Normalization. Pre Layer Normalization(Baevski and Auli, 2018; Child et al., 2019; Wang et al., 2019) (PreNorm), which applies normalization layers before residual connections and attention mechanisms, has demonstrated enhanced stability and performance in LLMs. Since PreNorm allows gradients to flow more directly from the output to the input through residual connections, bypassing the sub-layers to certain extent, it reduces the effective depth of the model. In contrast, Post Layer Normalization(Wang et al., 2019) (PostNorm) applies normalization after the residual connection and attention mechanisms, thereby preserving 12 MiniMax-01: Scaling Foundation Models with Lightning Attention the models effective depth. However, PostNorm can be prone to vanishing and exploding gradients, presenting significant challenges in training LLMs. Most existing LLMs predominantly use PreNorm, as the performance differences between wider and deeper networks in the conventional Transformer architecture are often negligible, and training stability is prioritized. The experiments are performed on models with 9.3 billion activation parameters and total of 60 billion parameters, each consisting of 48 layers that employ different normalization methods. Both models are trained on 500 billion tokens. For PostNorm, we utilize DeepNorm (Wang et al., 2024a) to ensure more stable training. As illustrated in Table 5, PostNorm consistently outperforms PreNorm across all evaluated metrics. Table 5 Module Ablations. Abbreviations: BBH (BIG-Bench Hard), DROP (Discrete Reasoning Over Paragraphs), MMLU (Massive Multitask Language Understanding), CMMLU (Massive Multitask Language Understanding in Chinese), GSM8k (Grade School Math 8K), ARC-C (Arc-Challenge), WG (WinoGrande). Arch. BBH DROP MMLU CMMLU MATH Softmax Hybrid-lightning Pre Layer Norm. Post Layer Norm. 28.2 32.2 29.9 32.6 27.4 29.0 26.8 27. 49.3 49.5 43.9 50.2 47.3 46.0 41.8 49.2 4.6 6.8 4.8 5. GSM8k 18.8 18.5 12.2 16.8 ARC-C WG 46.4 47.4 43.5 46.2 65.6 67. 65.5 65.4 2.4. Model Spec Upon finalizing the architecture of the models modules, the subsequent step entails scaling up the model, which necessitates meticulous design of the models hyperparameters across various dimensions. Our primary goal is to strike balance between performance and inference efficiency. Single-device inference offers superior efficiency compared to multi-device implementations by eliminating cross-machine communication overhead. Consequently, we constrain the models total parameters to 500B, ensuring compatibility with single-node inference on an 8 80G configuration for sequences up to 1M tokens under 8-bit quantization. Given our limited training budget, we formulate the following optimization problem to determine optimal parameter allocations: min ùëÉall,ùëÉact ùêø(ùëÉall, ùëÉact, ùëá) subject to ùê∂compute(ùëÉall, ùëÉact, ùëá) < ùê∂ and ùëÉall < 500ùêµ, (13) where ùêø denotes the loss, ùëÉall and ùëÉact represent the total and activation parameter counts respectively, ùëá is the number of training tokens, ùê∂compute denotes the computational costs (dependent on parameter counts and data consumption), and ùê∂ signifies the budget constraint. Through comparative experiments on small-scale models, we first establish optimal ranges for several key variables: (1) the mixing ratio between softmax and linear attention mechanisms; (2) the depth-to-width ratio of the model architecture; (3) the ratio of linear attention memory size to hidden size; (4) the ratio of activated FFN to attention; (5) the proportion of dimensions utilizing RoPE for softmax attention. Our experiments reveal that the hybrid architecture demonstrates particular sensitivity to layer depth, with deeper models consistently outperforming shallower counterparts. Notably, shallow models require substantially more softmax attention layers to achieve comparable performance, underlining the efficiency advantages of deeper architectures. We also observe that increasing linear attention memory size significantly enhances model performance, and implementing RoPE on half of the softmax attention dimensions enables length extrapolation without performance degradation. 13 MiniMax-01: Scaling Foundation Models with Lightning Attention Based on these optimized architectural variables, we employ established scaling laws (Clark et al., 2022; Hoffmann et al., 2022) to determine the optimal model size. We train models with activation parameters ranging from 44 million to 1.2 billion across 500 billion tokens, utilizing 16, 32, and 64 experts. However, we find the predictions from these methods become less reliable when extrapolating to larger model with 9.3 billion parameters. To address this limitation and achieve more accurate predictions, we propose the following formula: ùêø(ùëÉact, ùëá ùê∏) = ùëë + ùëéùëÉùõº act + ùëèùëá ùõΩ + ùëê(ùëÉactùëá)ùõæ, (14) where ùêø(ùëÉact, ùëá ùê∏) represents the loss conditioned on the number of experts, while ùëé, ùëè, ùëê, ùëë, ùõº, ùõΩ, and ùõæ are parameters to be fitted in relation to the number of experts. Based on the predictions of Eq. 13 and Eq. 14, we have identified candidate model with 45.9 billion activation parameters and 456 billion total parameters as the optimal configuration. 3. Computation Optimization In this section, we present our computation part, including the training and inference. In this project, we have dynamically changing GPU cluster, where the number of H800 GPUs ranges from 1500 to 2500. An efficient architecture necessitates robust implementation optimization to fully harness its computational benefits at scale. To scale our novel architecture to the requisite size, we present three key optimization strategies that primarily address the following three challenges: 1. Mitigating the all-to-all (a2a) communication overhead during the training of Mixture of Experts (MoE) architecture is persistent challenge. The configuration we choose for our experts, specifically opting for large models, imposes substantial demands on GPU memory. Therefore, the primary challenge lies in achieving an optimal equilibrium between memory utilization, computational efficiency, and the overhead associated with all-to-all communication. 2. As we endeavor to support at least 1 million token context window in both training and inference, the accurate distribution of tokens within such an extensive context window across different GPUs becomes imperative for this colossal model. This necessity, however, inevitably introduces additional communication overhead. As result, devising strategies to minimize this overhead, particularly in the context of our hybrid architecture, presents significant challenge. 3. The current implementation of the lightning attention mechanism is specifically optimized for training processes. However, in the inference scenario, the challenge arises in effectively managing real-world batched inputs, which may encompass variable sequence lengths and specific inputs that incorporate prefix caching. It is noteworthy that the existing open-source frameworks in the industry currently lack the necessary mature technical support to adequately address these challenges. Thus, we independently and comprehensively reinvent our distributed training and inference framework, thereby successfully addressing these challenges with the desired level of efficiency. 3.1. MoE Optimization The primary objective in optimizing the MoE architecture is to minimize communication overhead, particularly for MoE models that utilize all-to-all (a2a) communication. To address this, We implement token-grouping-based overlap scheme, as illustrated in Figure 9. In this scheme, the a2a communication is performed within the expert parallel (EP) communication group, and it overlaps with the processing of tokens from different expert groups. To ensure the correctness of the communication 14 MiniMax-01: Scaling Foundation Models with Lightning Attention results, we restrict each ProcessGroup to execute communication operators sequentially. As result, a2a communications across different groups cannot overlap, leading to the emergence of idle time. 0 0 expert Device 1 Device 0 Idle time a2a-combine a2a-dispatch w/ EP overlap w/o EP overlap This approach leads to significant performance improvements. However, upon more detailed analysis, we identified critical tradeoff specific to the expert configuration of the MiniMax-Text-01 model. When Tensor Parallelism (TP) is employed to partition the expert parameters, the computational intensity becomes excessively low, thereby hindering the efficiency of the computation. However, opting not to use TP leads to an excessively large parameter count, which necessitates the activation of larger Pipeline Parallelism (PP) configuration. The challenge emerges because PP does not reduce the memory footprint required for storing activations. This limitation is particularly detrimental for training models with long contexts, as the increase in memory consumption does not provide proportional benefits in terms of computational efficiency or training speed. Consequently, it is imperative to develop new parameter partitioning strategy that adeptly balances memory usage and computational intensity to optimize the training process for our specific model and task. Figure 9 Expert Parallel (EP) Overlap Illustration. Chunk tokens into 2 groups thus computation can overlap with communication between different groups. Time 0 1 1 To achieve enhanced efficiency, we first introduce novel ProcessGroup, termed ETP (Expert Tensor Parallel), which is specifically designed to manage the weight partitioning of experts. Concurrently, we propose another distinct ProcessGroup, named EDP (Expert Data Parallel), to encapsulate the data parallelism of identical experts. In our system, we define the total number of GPUs involved in training as ùë§ùëúùëüùëôùëë_ùë†ùëñùëßùëí. The system must satisfy two key conditions: ùë§ùëúùëüùëôùëë_ùë†ùëñùëßùëí = ùë†ùëñùëßùëíùëÉùëÉ ùë†ùëñùëßùëíùê∑ùëÉ ùë†ùëñùëßùëíùê∂ùëÉ ùë†ùëñùëßùëíùëá ùëÉ and ùë§ùëúùëüùëôùëë_ùë†ùëñùëßùëí = ùë†ùëñùëßùëíùëÉùëÉ ùë†ùëñùëßùëíùê∏ùê∑ùëÉ ùë†ùëñùëßùëíùê∏ùëá ùëÉ ùë†ùëñùëßùëíùê∏ùëÉ (15) (16) This configuration empowers the MoE component with the flexibility to define the distribution of experts, manage the weight partitioning of experts, and independently configure the ZeRO (Zero Redundancy Optimizer) algorithm (Rajbhandari et al., 2020). Based on this implementation, we are able to completely decouple the parallel strategies of the MoE components from those of the non-MoE components. Building upon this modification, we can flexibly configure the ETP to achieve an optimal balance between memory usage and computational intensity. Furthermore, to mitigate communication overhead, we design an EP-ETP overlap strategy. This strategy aims to maximize the utilization of both network resources and computational resources, as illustrated in Figure 10 (a). Since communications within the same process group must be executed sequentially, extended periods of computation not only facilitate overlap with greater number of communications but also create additional opportunities for communications across different process groups to overlap, leading to enhanced overall performance as illustrated in Figure 10 (b). When determining the number of groups, several trade-offs must be considered. Theoretically, only by dividing the workload into sufficiently large number of groups can we achieve ample overlap between communication and computation, as illustrated in Figure 10 (c). However, in practice, an excessive number of groups can significantly increase the complexity of scheduling and introduce the 15 MiniMax-01: Scaling Foundation Models with Lightning Attention w/o EP + ETP overlap Idle time Waste time a2a-dispatch allgather expert reduce scatter a2a-combine (a) Device Device 1 Device 2 Device 3 w/ EP + ETP overlap 0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3 w/o EP + ETP overlap a2a-dispatch allgather expert reduce scatter a2a-combine w/ EP + ETP overlap Device 0 0 0 (b) Device Device 2 Device 3 0 1 1 1 2 2 3 3 (c) Device 0 Device 1 Time 0 0 0 0 1 1 1 2 2 3 3 3 0 1 1 1 Figure 10 EP-ETP Overlap Illustration. (a) EP-ETP overlap with the lower computation portion. (b) EP-ETP overlap with the higher computation portion. (c) EP-ETP overlap with fewer groups. Compared with (a) and (b), it shows that if the compute time cost is longer, the efficiency will be better. Comparing with (b) and (c), it shows that fewer groups will lead to insufficient overlap. risk of becoming CPU-bound. Given that the proportion of ETP (Expert Tensor Parallel) in the overall MoE (Mixture of Experts) architecture is not substantial, it is crucial to make adjustments based on the specific context and requirements. Through the aforementioned optimization strategies, we achieve balanced configuration of storage and computational intensity for the specific expert specifications in the MoE (Mixture of Experts) structure of the MiniMax-Text-01 model. Furthermore, based on these optimizations, we reduce the pure communication overhead of the MoE component by 50% compared to the preoptimization state, resulting in significant improvement in training efficiency. 3.2. Long Context Optimization significant challenge in long context training is that real training samples are difficult to standardize into uniform length. The conventional approach of using padding to make samples the same length leads to substantial computational waste. In the context of training at the 1M sequence length scale, this waste becomes particularly significant. To address this issue, we adopt data formatting technique during training where different samples are concatenated end-to-end along the sequence dimension. We refer to this technique as \"data-packing\". This format minimizes computational waste during the computation process, thereby conserving computational resources. 3.2.1. Varlen Ring Attention For Softmax Attention, the ring attention algorithm (Liu et al., 2024a) offers an effective method to partition data, thereby enabling unlimited scalability. However, the existing implementations MiniMax-01: Scaling Foundation Models with Lightning Attention causal compute causal varlen compute non-causal compute non-causal varlen compute (a) (b) Figure 11 Ring Attention v.s. Varlen Ring Attention. (a) No data packing in ring attention. (b) Pack 3 samples with different lengths in varlen ring attention. are not optimized to efficiently handle the ring attention mechanism for the data-packing format. In the case of FlashAttention (Dao, 2024), while it provides varlen (variable length) interface to accommodate the data-packing format, there is no corresponding ring attention implementation available. Regarding TransformerEngine (NVIDIA, 2023), the implementation incorporates Context Parallel (CP) ProcessGroup to support the ring attention algorithm. However, this approach poses risk of computational resource waste when dealing with the data-packing format. This is because the algorithm divides each sequence into 2 ùë†ùëñùëßùëíùê∂ùëÉ segments and applies the ring attention mechanism to each segment. Consequently, this approach restricts each sequence to length that must be an integer multiple of 2 ùë†ùëñùëßùëíùê∂ùëÉ. In scenarios where the sample distribution is unknown and the CP size is set to large value, this can lead to significant padding, resulting in the waste of computational resources. Motivated by the principle of not making assumptions about the sample distribution, we redesign the algorithm and name it Varlen Ring Attention. This approach avoids the excessive padding and subsequent computational waste associated with traditional methods by applying the ring attention algorithm directly to the entire sequence after data-packing. Specifically, the implementation involves distinguishing the offset of the attention mask corresponding to each sequence within the ring attention computation. The key modification is to transform the original causal computations into varlen causal computations and similarly convert the non-causal computations into varlen non-causal computations, shown in Figure 11. 3.2.2. Improved Linear Attention Sequence Parallelism For lightning attention, the LASP (Linear Attention Sequence Parallelism) algorithm (Sun et al., 2024) leverages the communication group of CP to facilitate the expansion of long sequences. As illustrated in Figure 12 (a), the LASP algorithm mandates that all CP ranks engage in send-recv operations to exchange intermediate key-value (ùêæùëâ) block results. This requirement imposes sequential dependency among the CP ranks, thereby compelling the computation to be performed in serial manner. Consequently, this sequential dependency significantly impedes the overall efficiency of the training process, as the inherent parallelism of the system is not fully exploited. To fully harness the parallel computing capabilities of GPU devices, we propose an optimized approach that refines the computational and communication workflow to eliminate dependencies during the computation process. This optimization effectively transforms serial computation into parallelized one. The enhanced approach, termed LASP+ (Figure 12 (b)), operates as follows: 1. Local Prefix Sum Calculation: Each computing node i.e., the CP rank, initiates the process by 17 MiniMax-01: Scaling Foundation Models with Lightning Attention block size padding init ùêæùëâ = ùêæ0ùëâ0 with zeros shape [ùëë, ùëë] init diag with decay shape [ùëë, ùëë] input sequence, shape [ùë†, ‚Ñé, ùëë] [‚Ñé, ùë†, ùëë] ùëÑ,ùêæ,ùëâ have the same shape, split sequence with CP ùëÑ2 ùêæ2 ùëâ2 ùëÑ4 ùêæ4 ùëâ4 4 ùëÑ3 ùêæ3 ùëâ3 3 recv ùëÑ6 ùêæ6 ùëâ 6 ùëÑ5 ùêæ5 ùëâ5 5 recv ùëÑ7 ùêæ7 ùëâ7 recv 8 deprecate send sum(0-2) sum(0-3) send sum(0-4) sum(0-5) send sum(0-6) sum(0-7) deprecate output sequence, shape [‚Ñé, ùë†, ùëë] [ùë†, ‚Ñé, ùëë] Initialize Phase (a) ùëÑ ùêæ ùëâ ùëÑ1 ùêæ1 ùëâ1 ùëÇùëñùëõùë°ùëüùëé ùëñ = (ùëÑùëñùêæùëñ ùëÄ)ùëâùëñ [‚Ñé, ùêµ, ùëë] ùêæùëñùëâùëñ = ùê∑ ùêæùëñ ùëâùëñ [‚Ñé, ùëë, ùëë] ùêæùëâ ùëÇùëñùëõùë°ùëíùëü ùëñ = ùê∑ ùëÑùëñ ùêæùëâ [‚Ñé, ùêµ, ùëë] ùëÇùëñ = ùëÇùëñùëõùë°ùëíùëü ùëñ + ùëÇùëñùëõùë°ùëüùëé ùëñ 0 ùêæùëâ += ùêæùëñùëâùëñ sum(0-1) ùëÇùëñùëõùë°ùëüùëé ùëñ = (ùëÑùëñùêæùëñ ùëÄ)ùëâùëñ [‚Ñé, ùêµ, ùëë] ùêæùëñùëâùëñ = ùê∑ ùêæùëñ ùëâùëñ [‚Ñé, ùëë, ùëë] 1 2 3 4 5 7 8 deprecate local prefix sum ùêæùëâùêø sum(1-2) sum(3-4) sum(5-6) sum(7-8) (b) allgather across ranks global prefix sum ùêæùëâùê∫ sum(0) sum(0-1) sum(0-2) sum(0-3) sum(0-4) sum(0-5) sum(0-6) sum(0-7) ùëÇùëñùëõùë°ùëíùëü ùëñ = ùê∑ ùëÑùëñ ùêæùëâùê∫ [‚Ñé, ùêµ, ùëë] ùëÇùëñ = ùëÇùëñùëõùë°ùëíùëü ùëñ + ùëÇùëñùëõùë°ùëüùëé ùëñ output sequence, shape [‚Ñé, ùë†, ùëë] [ùë†, ‚Ñé, ùëë] Figure 12 Difference of LASP Algorithm and LASP+ Algorithm. (a) LASP Algorithm. 1. Initialization Phase: initializing KV to zero and the diagonal decay matrix. 2. Data Partitioning and Padding: partitioning the Q, K, and matrices along the sequence dimension into CP size (4 segments illustrated in the figure) blocks, dividing each block into smaller blocks based on the BlockSize ùêµ and padding the remaining part (e.g. Q7, K7, V7) that cannot be divided evenly by ùêµ. 3. Intra-block Computation: performing intra-block of each CP rank computations in parallel. 4. Inter-block Computation and Communication: starting from CP rank 0, computing the inter-block portion of the current ùëÑùëñ with all previous KV blocks and the prefix sum ùêæùëñùëâùëñ. Different CP ranks communicate data through send-recv operations. (b) LASP+ Algorithm. Building upon figure (a), each CP rank computes the local prefix sum ùêæùëâùêø and performs AllGather operation to synchronize, then selects the local prefix sum ùêæùëâùêø to compute the global prefix sum ùêæùëâùê∫. The remaining computational components are same as (a). 18 MiniMax-01: Scaling Foundation Models with Lightning Attention independently calculating its local prefix sum, denoted as ùêæùëâùêø. 2. Global Synchronization via AllGather: Following the local calculations, an AllGather operation is performed to synchronize the information from all nodes globally. This step ensures that each node has access to the necessary data from all other nodes. 3. Prefix Sum Computation: Each node selects the specific CP ranks ùêæùëâùêø on which to perform the prefix sums, decision based on its assigned computation order. By implementing these steps, the LASP+ approach effectively removes the original dependencies between the computation nodes. This elimination of dependencies facilitates fully parallelized computation process, thereby significantly enhancing the overall efficiency and throughput of the system. The transformation from serial to parallel computation not only leverages the full potential of GPU devices but also ensures that the training process can be executed more rapidly and with greater scalability. The proposed modifications, while incurring additional costs in terms of increased total communication volume and temporary memory usage, are unequivocally justified by the substantial performance benefits they confer. These enhancements significantly outweigh the associated overhead in communication and memory consumption. Through comprehensive testing and verification, it is empirically demonstrated that the computation speed in the LASP+ approach can attain up to 1/ùëÅùëùùëêùëõ of the original LASP algorithm, where ùëÅùëùùëêùëõ denotes the number of parallel computing nodes. Furthermore, the overhead introduced by the AllGather operation is minimal, which is consistent with our anticipations and underscores the efficacy of the optimization. Building upon the LASP+ framework, we further introduce support for the varlen feature to effectively manage the data-packing data structure. This enhancement is particularly beneficial for handling batched samples that comprise inputs with unequal token lengths. The process involves the following steps: 1). Padding to Block Size: Each input within the batch is padded to ensure that its length is multiple of the predefined block size, which is set to 256. This padding step is crucial for aligning the data structure with the computational requirements of the kernel. 2). Sequential Concatenation: After padding, the inputs are sequentially concatenated. This concatenation facilitates the use of single kernel to perform parallel computations across multiple batches. By organizing the data in this manner, we can efficiently leverage the parallel processing capabilities of the GPU, thereby optimizing computational performance. The integration of the varlen feature with the LASP+ framework ensures that the system can handle diverse input lengths without compromising on efficiency. This approach not only simplifies the computational workflow but also maximizes resource utilization by enabling the processing of multiple batches concurrently. 3.3. Lightning Attention Inference Optimization The initial implementation of the lightning attention mechanism is primarily research-oriented and not yet suitable for practical applications, especially for inference. However, the optimization of inference processes is of paramount importance in real-world scenarios, as the long-term cost of deploying trained model is predominantly determined by the efficiency of its inference. To this end, we implement four optimization strategies for lightning attention: batched kernel fusion, separated prefill and decoding execution, multi-level padding, and strided batched matmul extension. 19 MiniMax-01: Scaling Foundation Models with Lightning Attention 3.3.1. Batched Kernel Fusion We fuse multiple memory-bound kernels and extend support to accommodate all batch inputs. In the prefill phase, we perform kernel fusion for processing the ùëÑ, ùêæ, and ùëâ tensors, including padding in the sequence dimension, partitioning into blocks, adjusting the internal layout, and computing the decay values. In the decoding phase, we perform kernel fusion for the computation of ùêæùëâ and the updating of the prefix ùêæùëâ cache. These kernel fusions reduce intermediate result storage and memory access operations, thereby significantly improving memory access efficiency and reducing end-to-end latency by 10% in the decoding phase and short-text input scenarios. By the way, these optimizations can bring very noticeable benefits on H20 compared to H800. 3.3.2. Separated Prefill and Decoding Execution The implementation of the lightning attention mechanism for long sequence computations primarily revolves around the differentiation between intra-block and inter-block computations. However, this approach is not optimal for inference tasks, particularly in the decoding phase, where the token length is consistently equal to 1. Given that the computational kernel for tokens of length 1 is predominantly memory-bound and necessitates only limited number of GPU Streaming Multiprocessors (SMs), we propose strategy that segregates the processing of tokens with length of 1 from those with length greater than 1. This is achieved by employing two distinct kernels. Subsequently, we utilize two separate CUDA streams to schedule these kernels in parallel, thereby enhancing computational efficiency and ensuring balanced GPU utilization, especially in scenarios involving mixed inputs. For instance, in batch size of 20, where all inputs contain prefix key-value (KV) cache, and the scenario includes one or two inputs with token length of 50 while the remaining inputs have token length of 1, this approach can significantly reduce latency. Specifically, the latency can be approximately equivalent to that of processing only the longer inputs, demonstrating reduction from 100 milliseconds to 50 milliseconds. 3.3.3. Multi-level Padding By applying padding to the ùëÑ, ùêæ, ùëâ tensors along the sequence dimension, the intra-block and interblock components can be effectively decomposed into multiple identical matrix multiplications. This decomposition is particularly advantageous as it aligns seamlessly with the StrideBatchedMatmul interface, thereby facilitating the maximization of parallel processing capabilities. Initially, the block size for padding was set to 256, configuration that was consistent with the training parameters. However, upon the implementation of the prefix cache technique, it is observed that the token lengths within batch typically fall below 256. This discrepancy led to redundant computations within each matrix multiplication operation. To address this inefficiency and minimize unnecessary computations, we propose the introduction of additional segmentation options, specifically 32, 64, and 128. This multi-level padding approach enables the dynamic selection of the computational scale that incurs the minimal padding overhead, based on the current input sequence length. By adopting this approach, the utilization of computational resources is optimized, ensuring that the system operates with increased efficiency and reduced redundancy. This strategic adjustment not only conserves computational resources but also contributes to the overall performance enhancement of the system. 20 MiniMax-01: Scaling Foundation Models with Lightning Attention 3.3.4. StridedBatchedMatmul Extension We utilize the optimized function cublasGemmStridedBatchedEx from the NVIDIA cuBLAS Library to manage StridedBatchedMatmul operations, thereby ensuring both high performance and versatility across diverse hardware architectures. Concurrently, we are in the process of implementing more extensive kernel fusion strategy, with the objective of substantially improving the computational efficiency of Hopper GPUs. Given that our sequence partitioning block size is configured to 256, the associated General Matrix-Matrix Multiplication (GEMM) operations, which involve matrices of dimensions 256x256, can leverage warpgroup-wide WGMMA instructions for computation. To further enhance memory access efficiency, we integrate the asynchronous operations of the Tensor Memory Accelerator (TMA) and delegate certain preprocessing and postprocessing computational tasks to be executed asynchronously on the CUDA Cores. Ultimately, our goal is to dynamically regulate the number of pipeline stages to adaptively attain optimal performance across both H20 and H800 GPU architectures. This adaptive control mechanism will ensure that the system can efficiently handle varying workloads and hardware configurations, thus maximizing overall computational throughput and resource utilization. By implementing the aforementioned optimizations, we achieve Model Flops Utilization (MFU) exceeding 75% on the H20 GPU for end-to-end inference tasks (Chowdhery et al., 2023). Specifically, in our MiniMax-Text-01 and MiniMax-VL-01 inference, when considering the latency ratio between the attention operation and the Feed-Forward Network (FFN) operation within the MoE structure, the softmax attention constitutes 95% of the latency at sequence length of 1,024,000 tokens. In contrast, the lightning attention implementation contributes to less than 12% of the latency under the same conditions. Our lightning attention implementation exhibits remarkable efficiency in managing heterogeneous batch inputs, which are characterized by diverse sequence lengths. This efficiency is particularly evident in scenarios where some inputs incorporate the prefix caching strategy while others do not. The reduction in latency not only enhances the overall speed of the inference process but also ensures that the system can handle wide range of input types with minimal performance degradation. This adaptability underscores the robustness and versatility of our lightning attention approach in real-world applications. 4. Pre-Training In this section, we provide an overview of the pre-training methodology for MiniMax-Text-01. First, we detail the meticulous construction of our pre-training corpus, with particular emphasis on data quality, standardized formatting, and mixing strategies to maximize model performance. Subsequently, we outline our innovative data experimentation framework, which enables rapid and resource-efficient evaluation of data effectiveness while minimizing computational costs. Lastly, we present an in-depth analysis of the models training hyper-parameters and present hierarchical training approach, which enables context length scaling up to 4 million tokens. 4.1. Data 4.1.1. Pre-training Corpus The pre-training corpus for MiniMax-Text-01 encompasses comprehensive and meticulously curated dataset, incorporating diverse sources including academic literature, books, web content, and 21 MiniMax-01: Scaling Foundation Models with Lightning Attention programming code. We enhance corpus quality through several strategic dimensions: Data Quality Enhancement. Superior data quality is fundamental for Large Language Models. We implement sophisticated filtering pipeline, combining rule-based cleaning and deduplication procedures aligned with established practices (Penedo et al., 2023, 2024; Rae et al., 2021). To assess document quality at granular level, we utilize our previous-generation model as the reward labeler (a MoE model with 5B activations and 60B total parameters). Initially, we evaluate multiple quality dimensions including coherence, conciseness, educational value, helpfulness, knowledge richness, and categorical relevance. Through comprehensive analysis, we identify significant correlations among these metrics and ultimately focus on three key dimensions: knowledge depth, practical helpfulness, and categorical distribution, while maintaining other metrics as secondary validation indicators. Data Formatting Optimization. The content from websites and books, once appropriately extracted and cleaned, can naturally be used as high-quality textbooks (Gunasekar et al., 2023) without further formatting. For dialogue and question-answering data, the sequential nature of text inherently captures conversational logic and question-answer relationships. Although humans benefit from additional formatting (e.g., Markdown) for readability and comprehension, we find that heavy formatting can actually diminish data diversity and quality by introducing fixed patterns that constrain the natural variation present in human conversations. Ultimately, to maintain format generalization capabilities and accommodate human preferences in alignment, we implement nested document format with versatile templates for dialogue and QA data, carefully balancing natural comprehension with structural consistency across various interaction patterns. Data Mixture Investigation. We develop sophisticated approach to tuning the data distribution, leveraging our three primary quality metrics. Based on the experiment paradigm detailed in the subsequent section, we discover that while high-scoring content on knowledge depth and helpfulness generally yielded superior performance in capability assessments, completely eliminating lower-scoring content can adversely affect downstream task performance. Therefore, we implement balanced sampling strategy, beginning with uniform distribution across the base corpus, and then adjusting sampling weights to favor high-quality content while maintaining sufficient representation of diverse categories. 4.1.2. Tokenization For tokenization, we employ byte-level Byte Pair Encoding (BPE) (Brown et al., 2020; Shibata et al., 1999), incorporating the pre-tokenizer methodology. We strategically up-sample multilingual content, to enhance the corresponding compression efficiency. The resulting vocabulary size is set to 200K tokens. 4.1.3. Data Experiment To systematically evaluate our design choices regarding pre-training data quality, format, and composition, we conduct extensive ablation experiments. These experiments involve training multiple small-scale MoE models using comparable token quantities but varying data characteristics. This approach enables us to isolate and measure the impact of individual data attributes while maintaining computational efficiency. 22 MiniMax-01: Scaling Foundation Models with Lightning Attention 4.1.3.1 Paradigm Formulation. We conduct Data Experiments to systematically compare the performance of different model variants. Specifically, we formulate experiments as statistical hypothesis tests that compare evaluation metric distributions between baseline model and models trained with different data configurations. When testing the effectiveness of new data corpus D, we formulate our alternative > ùúáùëábaseline, where ùúá represents the weighted average performance metric and ùëá hypothesis as ùêª1 : ùúáùëáD denotes the distribution of evaluation values across test samples. Evaluation. We carefully design our evaluation norms to ensure meaningful insights. We look at wide range of multiple-choice benchmarks, discarding choice indices in query formulation and look at the likelihoods of completion. We observe the distributions of sample-wise log-normalized accuracy log accnorm2, defined as log accnorm2 (ùë•) = log softmaxùëù (ùëêùê∂ùë• ) (cid:110) ( ùëù(ùëê)) (cid:111) , ùëñ (ùëê) = ùëùùëñ (ùëê) where ùëù bytes(c) is the byte-normalized probability of choice ùëê for sample ùëñ. We choose bytewise normalization to exclude the effect of tokenizer, while alleviating the disfavor towards longer choices. We conduct extensive experiments to ensure that this metric is stable across training, while maintaining the discriminative power of the metric, which is quantified by the ratio Œîobvious/ùúéseed, where Œîobvious represents the obvious difference in performance between models and ùúéseed denotes the standard deviation across different random seeds. Experiment Efficiency & Setup. With such statistical setup, we are able to conduct power analysis to decide minimal test sample size while maintaining the MDE (Minimal Detectable Effect) at similar level as our training variance, and guaranteeing 95% confidence level and 80% power for decision making. With the confidence methodologies set, we conduct simple scaling experiments on token amount and the model size, and eventually land at an experiment step of training MoEs of 1B activation and 8B total parameters with 40B tokens of data, where data mixture comprises 20B web documents and 20B data of hypothesis."
        },
        {
            "title": "4.1.3.2 Effect of Repetition",
            "content": "The incorporation of repeated data has been empirically demonstrated to introduce several detrimental effects on the models performance and generalization capabilities (Hernandez et al., 2022). Consequently, implementing deduplication strategies is essential for optimizing LLM performance. Recent studies (Abdin et al., 2024; Penedo et al., 2024) suggest that repeatedly training high-quality documents can lead to enhanced downstream performance, with certain high-quality domains being trained up to 50 times, where the repetition is measured by MinHash similarity(Broder, 1997; Lee et al., 2022). However, our empirical analysis reveals that their experimental paradigm is inadequate for assessing the impact of repetition, as data efficiency is not consistent throughout the training process. To achieve better alignment with the results of the full training, we introduce novel repetitionaware experimental framework. Specifically, we first perform global deduplication on the dataset to remove redundant entries. Then, we down-sample the documents to align the repetition frequency with the requirements of the final training schedule while adhering to the budget constraints of our ablation experiments, different from the previous experimental setups which directly adopted data distributions identical or similar to those used in the final training stage. Our findings indicate that low-quality data suffer substantial decrease in performance after training for more than two epochs, while high-quality data can be effectively trained for up to four epochs, similar to previous 23 MiniMax-01: Scaling Foundation Models with Lightning Attention observations (Muennighoff et al., 2023). Notably, the solution derived from the proposed framework yields better alignment with the results obtained using considerably more computational resources. By carefully controlling the repetition and quality of the training data, we achieve more efficient and effective data mixture, ultimately leading to better model performance. 4.2. Training Strategy Initial Pre-training. We initialize all model parameters using the Xavier initialization method (Glorot and Bengio, 2010), the scaling factors of DeepNorm (Wang et al., 2024a) are set to ùõº = (2ùëÅ)0.25 and ùõΩ = (8ùëÅ) 0.25, where ùëÅ denotes the number of layers. We employ the AdamW optimizer (Loshchilov and Hutter, 2019) with ùõΩ1 = 0.9, ùõΩ2 = 0.95, and the weight decay is set to 0.1. The training sequence length is 8192, and the batch size is progressively scaled from an initial size of 16M to 32M at 69B tokens, to 64M at 790B tokens, and finally to 128M at 4.7T tokens, where it remains until the end of training. The schedule is designed based on the correlation between training loss and the critical batch size (McCandlish et al., 2018). It is argued that training at the critical batch size yields near-optimal balance between training time and data efficiency (Kaplan et al., 2020). Following this, we fit power-law relationship between the loss and the critical batch size on data from smaller models, as shown in Figure 13. The batch size is doubled when the corresponding loss is reached. The learning rate schedule begins with linear warm-up over 500 iterations to peak value of 2 104, followed by training with constant learning rate for 7.2T tokens. In the latter stages of training, we notice anomalous gradient norm values. This issue is attributed to an excessively high learning rate and we adjusted lr to 1.3 104 for the remaining 3.2T tokens. During the fast decay phase, we train 1T tokens and exponentially decrease the learning rate to 3 105. Additionally, the MoE auxiliary loss coefficient is set to 0.01. Figure 13 The power-law fit for the training loss and the critical batch size, utilizing data from models ranging from 50M to 600M in activated parameters counts. We mark the points where the batch size is doubled with dashed gray lines. Long-Context Extension. We incrementally expand the models training context length to 1M tokens. Due to our architectures effective length extrapolation capabilities, the model successfully demonstrates its ability to process sequences up to 4M tokens in the vanilla Needle-In-AHaystack retrieval task (NIAH) test 2, despite only being trained on contexts up to 1M tokens, as illustrated in Figure 14. Specifically, we employ three-stage training procedure to systematically upsample long-context data across diverse length ranges, while preserving the distributional characteristics of critical domains to preserve short-context evaluation performances steady. The details of the training data mixture, RoPE base frequency, and training length are shown in Table 6. We also mix in 10% of high-quality long-context question-answering data with similar length distribution as long-context pre-training data during the last 20% of training cycles in each stage(Parmar et al., 2024). To mitigate potential instabilities resulting from distributional shifts, we utilize linear interpolation of source-specific weights throughout the transitional phase. This method facilitates gradual and controlled evolution of the data distribution towards the desired target distribution, thereby ensuring training stability 2Same as Gemini (Team et al., 2024a), we use Paul Graham (https://paulgraham.com/articles.html) as the haystack and The special magic {city} number is: {number} as the needle. 24 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 14 4 Million vanilla Needle-In-A-Haystack retrieval task pressure test on MiniMax-Text-01. The token interval is 32K when it is less than 1M, and the token interval is 0.5M when it is greater than 1M. and preserving convergence properties. Additionally, our findings indicate that NIAH is inadequate for effectively monitoring the models performance throughout the training process. This is primarily because NIAH metric performance reaches its peak score early on, specifically within the initial 128K training steps. To tackle this limitation, we evaluate the models intermediate checkpoints using more demanding tasks, which are designed to increase in complexity as training progresses. Notably, despite the escalating difficulty of these tasks, we consistently observe steady improvement in the models performance metrics. This sustained upward trajectory clearly demonstrates the critical importance and necessity of implementing long-context continual pretraining. More details are given in Section 5.7.2. Table 6 Long-Context Extension Recipe. For clarity, we categorize the data as follows: data with fewer than 32K tokens are labeled as Short; data ranging from 32K to 128K tokens are labeled as Medium; and data exceeding 128K tokens are categorized as Long. Training Length RoPE Frequency # Tokens Short (%) Medium (%) Long (%) 128K 512K 1M 5M 10M 10M 300B 32B 26B 30 35 30 70 35 30 0 30 40 5. Post-training In this section, we present thorough post-training framework designed to enhance the models general performance, long-context capability, and real-world applicability. Our approach begins with the creation of diverse, high-quality prompt dataset, accompanied by hierarchical reward system that evaluates responses across multiple dimensions: correctness, truthfulness, helpfulness, and harmlessness. The training process consists of Supervised Fine-Tuning (SFT), Offline and Online Reinforcement Learning (RL). Through these phases, we systematically align the model with our defined objectives. Model safety is ensured through exhaustive data mining techniques and specialized harmless reward model. We introduce novel multi-stage training methodology that significantly enhances the models capacity to process extended contexts while maintaining optimal MiniMax-01: Scaling Foundation Models with Lightning Attention performance on shorter sequences. This approach results in robust system capable of handling complex, real-world scenarios. Extensive evaluations conducted across both academic and in-house benchmarks demonstrate that our model achieves top performance across all tasks, while establishing new standards of extremely long-context processing. 5.1. Prompt Collection Our extensive prompt collection encompasses millions of diverse, high-quality queries from various sources. We develop tagging system that categorizes each prompt based on task type, knowledge domain, and difficulty level. The collection process incorporates sophisticated filtering mechanisms to eliminate redundant prompts while maintaining an optimal difficulty distribution. The prompt set spans various domains including long-context, programming, math, logical reasoning, creative writing, function calling, general-knowledge, and safety-related scenarios. 5.2. Reward Model Our reward model framework evaluates responses across four critical dimensions to ensure alignment with our core principles: Correctness. We implement rigorous evaluation system for responses that can be strictly validated. For mathematical and reasoning tasks, we utilize early-version MiniMax-Text-01 to generate binary reward signals based on answer consistency. Programming solutions undergo comprehensive testing in secured sandbox environment, with performance metrics derived from test case success rates. Truthfulness. We employ verification pipeline to assess the factual accuracy of the response. The process involves systematic response sampling, statement decomposition and clustering, crowd-sourced verification, and automated comparison using advanced language models to generate truthfulness scores. Helpfulness. Our evaluation framework assesses compliance with user instructions through both deterministic and probabilistic approaches. We implement automated rule-based constraint verification systems complemented by human evaluation of key metrics including coherence, depth, contextual relevance, and stylistic appropriateness. The final helpfulness score combines multiple evaluation signals through weighted scoring system. Harmlessness. Building upon Constitutional AI principles (Bai et al., 2022b), we develop evaluation criteria encompassing safety protocols, content appropriateness, and legal compliance. Our assessment system leverages carefully calibrated prompts validated against human annotations, with early-version MiniMax-Text-01 providing standardized safety evaluations. 5.3. Supervised Fine-Tuning Our SFT dataset construction involves multi-stage process utilizing domain-specific expert models trained through iterative SFT and RL cycles. We implement rejection sampling (Bai et al., 2022a; Dubey et al., 2024) to generate high-quality responses by the experts, sampling multiple variations per prompt across different temperature settings to select optimal demonstrations measured by the reward hierarchy. The response selection process further incorporates both n-gram and semantic similarity filters to ensure maximum diversity and quality in the training data. 26 MiniMax-01: Scaling Foundation Models with Lightning Attention 5.4. Reinforcement Learning 5.4.1. Offline Reinforcement Learning We incorporate the offline RL phase, i.e., Direct Preference Optimization (DPO) (Rafailov et al., 2023), to optimize the models performance across diverse prompt distributions, owing to its simplicity and ease of data construction for long-context scenarios. We specifically focus on prompts that maintain distributional consistency with those utilized in the SFT stage. To evaluate the impact of prompt selection, we conduct comparative experiments using two prompt categories: SFT-trained prompts and SFT-untrained but homologous prompts. Empirical results demonstrate negligible performance variations between SFT-trained prompts and their untrained counterparts. Thus, we adopt the SFTtrained ones for the offline RL phase. The experimental protocol involves generating responses with varying temperature parameters for each prompt, followed by systematic evaluation using the reward models described in Section 5.2. We then identify the best and the worst responses to construct preference pairs for DPO training. 5.4.2. Online Reinforcement Learning Online learning demonstrates superior sample efficiency and cross-domain generalization capabilities compared to offline learning methodologies. Therefore, we implement online RL to improve model performance, particularly in mathematical reasoning tasks. Our approach emphasizes prompt diversity and prioritizes prompts with moderate success rates to maximize information gain during policy updates. Notably, we employ SFT-untrained prompts during online RL, as our empirical observations indicate that reusing prompts from previous phases resulted in model saturation, characterized by diminished response perplexity. We propose modified Group Relative Policy Optimization (GRPO) (Shao et al., 2024) approach incorporating the following key innovations: Importance Sampling Weight Clipping. The conventional PPO/GRPO implementation employs one-sided clipping (Schulman et al., 2017; Shao et al., 2024), sometimes leading to gradient instability when processing tokens with large policy ratio and negative advantage. To address this issue, we implement additional clipping that abandoned this case in the loss function, which effectively regulates the importance sampling magnitude and mitigates noise propagation. KL Divergence Optimization. Due to the similar gradient instability issue, we reformulate the KL divergence term through theoretical analysis of the variance-bias trade-off to further stabilize gradient behavior, resulting in ùîªùêæùêø(ùúÉ) = ùîºùë° [SG(ùúãùúÉ(ùëéùë° ùë†ùë°) ùúãref(ùëéùë° ùë†ùë°)) log ùúãùúÉ(ùëéùë° ùë†ùë°)], where SG() denotes the stop-gradient operator. This formulation maintains policy consistency while reducing gradient variance. Balanced Advantage Estimation. We also ensure equitable reward contributions between positive and negative examples, which proves particularly effective in scenarios with skewed distributions. This approach maintains stable training dynamics by regulating the absolute magnitude of rewards across different example groups. 5.5. Safety Alignment The safety alignment of our model is meticulously addressed throughout both the SFT and RL stages. To strike an optimal balance between the models harmlessness and helpfulness, we employ an approach that encompasses the following key components. MiniMax-01: Scaling Foundation Models with Lightning Attention 5.5.1. Training Data Construction We construct high-quality alignment training data with focus on ensuring data diversity and accuracy. This involves the implementation of several data collection methodologies designed to cover broad spectrum of safety scenarios: Safety-Category Specific Prompts. Leveraging established safety classification standards and insights from safety and domain experts, we generate tailored prompts for specific safety categories. This ensures that the model is exposed to comprehensive set of safety-related scenarios. Real-World User Data Collection. We collect real-world user questions from various web documents to incorporate authentic and diverse safety-related queries into our training data. Prompt Augmentation. We instruct early-version MiniMax-Text-01 to generate additional related prompts based on the collected typical red team attack prompts. This approach aims to expand the diversity of safety scenarios and enhance the robustness of the models safety mechanisms. 5.5.2. Response Generation with Harmless Reward Model To generate safe and appropriate responses, we employ harmless reward model (Bai et al., 2022b) that is developed based on set of detailed safety rules. To prevent the model from producing unreasonable refusals, we carefully integrate principles of helpfulness into the safety rules. This integration plays crucial role in achieving balanced output capability, enabling the model to provide safer responses without compromising its utility to the user. The resulting safety-aligned system demonstrates robust protection against potential misuse while maintaining high performance across intended use cases. 5.6. Training Methodology with Long-Context Adaptation We propose systematic multi-stage training methodology to enhance the models capacity for processing extended contexts, as shown in Tab. 7. This approach is methodically designed to optimize long-sequence handling while maintaining performance efficacy on conventional shorter sequences. The RoPE base frequency is maintained at 10 million throughout the post-training phase to ensure consistency in positional encoding. Stage I: Initial Short-Context Training. The first stage implements SFT with sequences constrained to 8,192 tokens. This foundational phase establishes baseline competency in processing standardlength queries and responses, which constitute the majority of practical applications. We remove the long-context prompts that are longer than 8,192 tokens in this stage. Stage II: Extended Context Training. The second stage implements significant extension of the sequence length to 1,032,192 tokens. This phase incorporates training samples across diverse sequence lengths with 50% long-context prompts, facilitating comprehensive model adaptation to extensive contextual processing. The strategic expansion of the sequence length is fundamental to achieving robust long-context capabilities. Stage III: Short-Context Preference Optimization. In this phase, we revert to 8,192 tokens for sequence length and implement Direct Preference Optimization (DPO). This calibration ensures optimal performance on conventional context sizes while maintaining the previously acquired capabilities. Stage IV: Long-Context Preference Optimization. The fourth stage focuses on reinforcing longcontext processing capabilities through DPO with sequences of 1,032,192 tokens. This phase employs 28 MiniMax-01: Scaling Foundation Models with Lightning Attention training protocols analogous to Stage III with entirely long-context data, adapted for extended sequence lengths. Stage V: Online Reinforcement Learning. The final stage implements short-context Online Reinforcement Learning with sequence length of 8,192 tokens. More details have been outlined in Section 5.4.2. Table 7 Training Recipe for Post-training Alignment. Stage Stage II Stag III Stage IV Stage Sequence Length Epoch Batch Size Max LR Min LR LR Decay 8192 2 128 1e-5 1e-6 Cosine 1032192 2 80 3e-6 3e-6 Constant 8192 1 64 5e-7 5e-8 Cosine 1032192 1 64 5e-7 5e-7 Constant 8192 1 512 1e-6 1e-7 Cosine 5.7. Academic Benchmarks We observe and report open-source shortand long-context benchmarks that highlight our models capabilities across various aspects. Along with the user-oriented evaluations we will discuss in Section 5.8, we show that MiniMax-Text-01 is leading open-source model that achieves top performance in long-context retrieval, understanding, long in-context learning and knowledge-based requests, while performing well in math, reasoning, and code tasks and demonstrating strong usefulness in real-user assistant scenarios. 5.7.1. Core Benchmarks MMLU (Hendrycks et al., 2021a) and MMLU-Pro (Wang et al., 2024b) are widely adopted datasets that assess the extent of models knowledge across broad range of domains. We further observe SimpleQA (Wei et al., 2024), factuality benchmark that challenges the models knowledge boundary, and C-SimpleQA (He et al., 2024b) which is an adapted version of SimpleQA under the Chinese culture. For the observation of reasoning capabilities, we evaluate on GPQA (Rein et al., 2024) for graduate-level knowledge reasoning, and DROP (Dua et al., 2019) for reading comprehension reasoning. We test our models performance on math problem-solving with grade-school-level task GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) that spans from AMC-8 to AIMElevel across 7 subjects. We monitor our models coding capability by observing the Pass@1 rate on HumanEval (Chen et al., 2021) and MBPP Plus (Austin et al., 2021; Liu et al., 2023) datasets. To test the models ability to interpret and execute detailed and nuanced instructions, we evaluate the IFEval (Zhou et al., 2023) benchmark. Furthermore, we observe Arena-Hard-Auto (Li et al., 2024b) that reflects the alignment to human preferences. We adopt greedy decoding and zero-shot chain-of-thought strategy (Wei et al., 2022) in evaluating our instruction-tuned model. We compare with other leading and open-source LLMs, which we evaluate under the same setting, if not reported. We present the performance of MiniMax-Text-01 in Table 8. As shown, MiniMax-Text-01 exhibits remarkable performance across most dimensions. It surpasses all models on C-SimpleQA with its more extensive knowledge boundary under Chinese culture. MiniMax-Text-01 also achieves top-3 performance across MMLU, IFEval, and Arena-Hard, showing its exceptional capability of applying its comprehensive knowledge within given constraints to well satisfy user queries and align with human preferences. Meanwhile, it achieves better MATH pass@1 rate than GPT-4o, Claude-3.5-Sonnet, and Llama-3.1-405B, and exhibits comparable 29 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 8 Performance of MiniMax-Text-01 on core academic benchmarks. Tasks GPT-4o (11-20) Claude-3.5Sonnet (10-22) Gemini-1.5Pro (002) Gemini-2.0Flash (exp) Qwen2.572B-Inst. DeepSeekV3 Llama-3.1405B-Inst. MiniMaxText-01 MMLU MMLU-Pro SimpleQA C-SimpleQA IFEval (avg) Arena-Hard 85.7 74.4 39. 64.6 84.1 92.4 GPQA (diamond) DROP (F1) 46.0 89. GSM8k MATH MBPP + HumanEval 95.6 76.6 76. 90.2 88.3 78.0 28.1 56.8 90. 87.6 65.0 88.8 96.9 74.1 75. 93.7 Evaluated following 0-shot CoT setting. General 86.5 76.4 26. 63.3 88.4 72.7 Reasoning 62.1 89. 86.8 75.8 23.4 59.4 89.4 85. 59.1 89.2 Mathematics 95.2 84.6 75. 86.6 Coding 95.4 83.9 75.9 89. 86.1 71.1 10.3 52.2 87.2 81. 49.0 85.0 95.8 81.8 77.0 86. 88.5 75.9 24.9 64.8 87.3 91. 59.1 91.0 96.7 84.6 78.8 92. 88.6 73.3 23.2 54.7 86.4 63. 50.7 92.5 96.7 73.8 73.0 89. 88.5 75.7 23.7 67.4 89.1 89. 54.4 87.8 94.8 77.4 71.7 86. performance with instructed Qwen2.5-72B on HumanEval. Moreover, MiniMax-Text-01 achieves 54.4 on GPQA Diamond, which exceeds most open-source instruction-tuned LLMs and the latest version of GPT-4o. 5.7.2. Long Benchmarks As previously discussed in the long-context extension part of section 4.2, the NIAH task is kind of simplistic for our model, rendering it insufficient for observing the models optimization progress. Consequently, we shift our evaluation to more challenging tasks. Our current long-context evaluation framework focuses on three primary dimensions: (1) Long-Context Retrieval, (2) Long-Context Understanding, and (3) Long In-Context Learning."
        },
        {
            "title": "5.7.2.1 Long-Context Retrieval",
            "content": "This dimension assesses the models memory capabilities, which serve as the foundation for almost all long-context tasks. In addition to vanilla k-M NIAH (Kamradt, 2023), we construct more challenging variation to assess our Long-Context Retrieval performance, namely Multi-Round NeedlesIn-A-Haystack (MR-NIAH), serving as crucial back up for retrieval tasks in long multi-turn dialogue contexts, revealing the fundamental capabilities for building lifelong companion AI assistants. Similar to Multi-round co-reference resolution (MRCR) (Vodrahalli et al., 2024) which is not open-source, we construct haystacks of MR-NIAH as history dialogues, where user queries are synthetic but explicit requests of event descriptions and creative writing. In the last round, the query requests the model to repeat the response of one of the history requests. The haystacks span from 2K to 1M tokens (up to around 2000 interactions), and each needle request is injected at 25%, 50%, and 75% of the 30 MiniMax-01: Scaling Foundation Models with Lightning Attention conversation, respectively. Each ground truth response contains three core components, and we look at an adjusted recall corr. comp. . We show case illustration in Appendix B.2. Figure 15 illustrates comparison results of MR-NIAH. Our model (MiniMax-Text-01, red line) shows strong performance across wide range of sequence lengths in both English and Chinese evaluations. Compared to competing baselines (e.g., GPT, Claude, and Gemini variants), our model also shows less performance degradation at large input lengths, underscoring its robustness for long-context retrieval tasks. 5.7.2.2 Long-Context Understanding This dimension measures the models longcontext understanding ability which contains logical reasoning skills based on long-context inputs. We utilize two comprehensive longcontext QA datasets, Ruler (Hsieh et al., 2024) and LongBench-V2 (Bai et al., 2024) to evaluate this aspect. Ruler includes 13 different tasks and notably introduces multi-hop tracing and aggregation tasks to evaluate the complex reasoning abilities of models. We test Ruler up to sequence length of 1M tokens. LongBench-V2 encompasses question-answering tasks of varying difficulty levels across multiple context types, including single and multi-document, multi-turn dialogue, code repositories, and long structured data, among others. Following LongBenchV2 (Bai et al., 2024), we consider two test modes: w/o CoT and w/ CoT, and the text lengths are categorized as follows: Short, ranging from 0 to 32K words; Medium, spanning from 32K to 128K words; and Long, covering 128K to 2M words. Figure 15 MR-NIAH in English and Chinese. As Table 9 illustrates, our model exhibits notable strengths in processing Rulers long-context reasoning tasks. While performance at the 64k input level remains competitive with leading models (including GPT-4o and Claude-3.5-Sonnet) with minimal variation, MiniMax-Text-01 establishes distinct advantage beginning at 128k, achieving impressive scores and surpassing all benchmark models. This superiority becomes particularly pronounced in ultra-long-context scenarios (such as 1M), where MiniMax-Text-01 maintains its commanding lead. Moreover, as evident in Table 103, MiniMax-Text-01 exhibits outstanding capabilities in LongBench-V2s long-context reasoning tasks. The model achieves state-of-the-art results among all evaluated systems in the w/ CoT setting, while also displaying remarkable effectiveness in scenarios w/o CoT. Overall, MiniMax-Text-01 demonstrates exceptional capability in long-context understanding especially reasoning tasks, both with and without CoT reasoning, particularly excelling in scenarios requiring complex reasoning. The exceptional robustness and stability of the model in processing long-context understanding tasks can be attributed to the hybrid architecture with half RoPE and carefully tuned training recipes for both pre-training and alignment, which enhance the models ability to handle long sequences effectively. 3We present the other models performance reported at https://longbench2.github.io/ 31 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 9 Performance comparison of MiniMax-Text-01 on Ruler. Model 4k 8k 16k 32k 64k 128k 256k 512k 1M GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) MiniMax-Text-01 0.970 0.965 0.962 0.960 0.963 0.921 0.960 0.960 0.960 0.961 0.890 0.957 0.960 0.951 0. 0.888 0.950 0.958 0.957 0.954 0.884 0.952 0.938 0.937 0.943 - 0.938 0.917 0.860 0.947 - - 0.916 0.797 0.945 - - 0.861 0.709 0.928 - - 0.850 - 0. Table 10 Performance comparison of MiniMax-Text-01 on LongBench v2. Model Human overall easy hard short medium long 53.7 100.0 25.1 47.2 59. 53.7 w/ CoT GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Deepseek-V3 Qwen2.5-72B-Inst. MiniMax-Text-01 51.4 46.7 - 43.5 56.5 54.2 55.2 - 47.9 66.1 w/o CoT GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Deepseek-V3 Qwen2.5-72B-Inst. MiniMax-Text-01 50.1 41.0 48.7 42.1 52.9 57.4 46.9 - 42.7 60.9 49.7 41.5 - 40.8 50.5 45.6 37.3 - 41.8 47.9 59.6 53.9 - 48.9 61. 53.3 46.1 - 45.6 58.9 48.6 41.9 - 40.9 56.7 52.4 38.6 - 38.1 52.6 43.5 44.4 - 39.8 47.2 40.2 37.0 - 44.4 43."
        },
        {
            "title": "5.7.2.3 Long In-Context Learning",
            "content": "This dimension evaluates the models ability to learn from context, core area of research in lifelong learning. We benchmark our Long In-Context Learning capability with the MTOB (Machine Translation from One Book) (Tanzer et al., 2024) dataset. The task requires model to translate between English and Kalamang, language that is very limited in open data and thus within the training corpus, and the LLM is expected to learn the language only from parts of grammar book and 375 translation examples, all given in the context for each translation query (Appendix B.1). The context length is 81K tokens under half-book setting and 133K tokens under total-book setting. We present our results in Table 11. Figure 16 Changes of eng kalam (ChrF) during the whole long-context extension training process. We carefully examined the pre-training data and found that only very small amount of data contains Kalamang-related content. As result, the eng kalam (ChrF) score of our model is the lowest in the no-context scenario, while other models we compared with likely have had their pre-train or post-train data enhanced with relevant Kalamang data. As well as the delta half and full book metrics, our model surpasses all models in terms of the eng kalam (ChrF) metric. And our model also has comparable performance with other models on kalam eng (BLEURT) metric. 32 MiniMax-01: Scaling Foundation Models with Lightning Attention In the course of long-context extension, as described in section 4.2, we observed gradual enhancement in In-Context Learning ability, as indicated by MTOB, illustrated in Figure 16. While we have explored some remarkable works(Agarwal et al., 2024; Dong et al., 2024) specifically aimed at improving In-Context Learning capabilities, we believe that such ability should merely be one aspect of the reasoning capabilities of long-context models. Therefore, we plan to conduct in-depth research on long-context data quality and scale from more fundamental perspective to further enhance the long-context reasoning capabilities of our model. Table 11 Performance comparison of MiniMax-Text-01 on MTOB. Context Type no context half book full book Œî half book Œî full book GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) Qwen-Long MiniMax-Text-01 eng kalam (ChrF) 54.30 9.90 20.22 53.62 16.79 53.68 12.20 49.50 16.55 48.48 6.0 51.74 kalam eng (BLEURT) - 55.65 57.90 53.30 45.94 51.60 GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) Qwen-Long MiniMax-Text-01 33.20 31.42 32.02 33.80 30.13 33.65 58.30 59.70 61.52 57.50 53.14 57.10 - 62.30 63.09 57.00 32.15 58. 44.40 33.39 36.89 37.30 31.92 45.7 25.10 28.28 29.50 23.70 23.01 23.45 - 35.42 41.11 41.10 29.39 45.6 - 30.88 31.07 23.20 2.02 24.35 5.8. User-in-the-loop While achieving top performance on the core open-source benchmarks, we realize that academic evaluations lack an understanding of real-world user interactions. Hence, we also focus on monitoring and improving user experience through our Hailuo AI 4 by incorporating user-in-the-loop evaluations based on real-world cases and adapting tools for better usability and performance in practical applications. 5.8.1. In-House Evaluations We maintain series of in-house evaluations that include: (1) automatic assessments of General Assistant capabilities, Knowledge Q&A, Creative Writing, Hard Capability, Instruction Following, Coding, Safety, and Long Context, and (2) expert human evaluations. Its worth noting that since our test queries are primarily derived from Hailuo AI user interactions, significant portion of our in-house samples are in Mandarin and deeply rooted in Chinese cultural contexts. Our results indicate notable discrepancy between performance on academic benchmarks and actual user experience, where leading open-source and commercial models can underperform when used as interactive assistants. We show in Table 12 5 that, through our dedicated efforts, MiniMaxText-01 is able to handle these situations quite well. In general, our model outperforms other models 4https://www.hailuo.ai/ 5We omit scores for in-applicable models. 33 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 12 Performance comparison of MiniMax-Text-01 on in-house benchmarks. General Assistant Hard Capability Creative Writing Knowledge Q&A Instruction Following Coding Safety Long Context GPT-4o (11-20) GPT-4o (08-06) GPT-4o (05-13) Claude-3.5-Sonnet (10-22) Claude-3.5-Sonnet (06-20) Gemini-2.0-Flash (exp) Qwen2.5-72B-Inst. DeepSeek-V3 Llama-3.1-405B-Inst. MiniMax-Text70.9 63.5 67.7 66.8 60.5 70. 66.4 66.8 53.3 73.9 73.5 62. 63.3 68.3 67.4 61.8 66.1 68. - 64.8 70.3 66 58.3 54. 51.0 70.0 61.7 64.6 63.6 81. 69.2 68.0 69.6 52.0 51.8 75. 68.9 77.0 46.0 78.6 50.4 49. 49.6 61.5 64.4 39.9 34.1 51. 50.3 46.3 94.0 93.6 93.2 94. 93.6 86.5 93.9 94.0 87.6 90. 85.4 79.7 79.7 92.9 95.0 66. - 74.9 70.7 90.9 86.2 58. 77.2 47.1 47.1 81.9 81.5 77. 60.3 93.8 in common Assistant scenarios, particularly when compared to open-source counterparts. This superiority is most evident in our Creative Writing (Appendix B.5, B.7, B.6) and Knowledge Q&A collections, where it aligns more closely with user intentions than other models, delivering accurate and detailed responses to wide range of queries. In productivity scenarios that require Long Context (Appendix B.3), such as document translation, summarization, and analysis, our model demonstrates high proficiency and reliability. Moreover, we prioritize the safety of our model, as it achieves top-tier performance on our established in-house Safety benchmarks. Meanwhile, we are agile in gathering and updating complex productivity scenarios with multilevel instruction following requests at which our model fails and current LLMs cannot master, constructing our Harder Capability and Instruction Following in-house evaluations. While leading LLMs tend to underperform in these sets, these requests reflect our models limitations when given multi-level instructions, which stems primarily from insufficient training data for specific instruction types. Moving forward, we are committed to substantially expanding our training dataset with high-quality, targeted content to address these gaps and improve model capabilities. 5.8.2. Search in Hailuo AI During user interaction case studies, we find models capability to utilize search tools can compensate for the limited knowledge boundary by accessing real-time, extensive, and precise information from the web. To maximize the models benefits from search while minimizing additional performance degradation, we first carefully pre-define the scope of search scenarios, which cover approximately 30 40% of user queries, including but not limited to precision-demanding, domain-specific, and time-sensitive requests. Meanwhile, to ensure seamless conversation experience, we define the system as invoking tools directly through special tokens, which avoid the complexity of multi-step planning (Chen et al., 2024b) or chain-of-thought reasoning6 that might disrupt the natural flow of the interactions. We create SFT datasets comprising search and non-search decisions across diverse domains, while carefully controlling for other interaction features unrelated to search decisions, such as conversation length, to maintain uniform data distribution across each dimension and prevent overfitting. Importantly, we employ the corresponding reward model of each sample to ensure response quality, failing at which would introduce suboptimal samples into the training 6https://docs.anthropic.com/en/docs/build-with-claude/tool-use 34 MiniMax-01: Scaling Foundation Models with Lightning Attention data, potentially affecting the models fundamental capabilities. The search decision boundary was calibrated to align with the models knowledge boundaries, discarding samples that our model already masters from the search corpus, such as general Chinese knowledge Q&A. After careful assessments by human evaluation experts, we conclude that our models use of the search tool extensively improved user experience, landing at performance leap from 58% to 71.5% on our out-of-domain Hailuo AI end-to-end evaluation (Appendix B.9). Since we are unsure whether other LLM-based assistants include similar search tools, we refrain from making unfair performance comparisons. 6. Vision-language Model By integrating an image encoder and an image adapter into our MiniMax-Text-01 model, we develop MiniMax-VL-01, which extends the capabilities of the model to visual understanding tasks. To ensure robust visual understanding, we design proprietary dataset and implement multi-stage training strategy, where the newly introduced image encoder and adapter first undergo large-scale visual pre-training, followed by comprehensive fine-tuning of the entire pipeline. In the following section, we begin with comprehensive description of the dataset used for training our image encoder and vision-language model. Subsequently, we provide an in-depth overview of the model architecture, followed by an exposition of our four-stage training regimen. We conclude the section by presenting our benchmark results. 6.1. Multimodal Data 6.1.1. Caption Data To pre-train the vision encoder, we curate substantial image-caption dataset by aggregating and filtering data from internet sources. Our Vision Transformer (ViT) is trained using 694 million unique image-caption pairs. To enhance data quality, we acquire refined captions for 180 million images within these pairs. During the training process, we employ an augmentation strategy by randomly sampling raw and refined captions with equal probability (ùëù = 0.5). 6.1.2. Description Data In existing vision-language models, the utility of descriptive imagery for model training has been well-documented (Li et al., 2024a, 2022, 2023; Schuhmann et al., 2021). To further explore this avenue, we have compiled dataset consisting of 100 million images sourced from open resources such as Common Crawl. Each image in this dataset is paired with fine-grained description, which is initially synthesized by caption model and subsequently refined through humans. On average, these descriptions comprise approximately 300 text tokens per image. Description data serves as robust resource for modal alignment and enhancing understanding in further training. 6.1.3. Instruction Data To train MiniMax-VL-01, we construct comprehensive and diverse instruction-based dataset by synthesizing an extensive range of question-answer (QA) pairs involving visual inputs. These QA pairs are meticulously designed to cover wide array of image-related tasks, such as text extraction, object localization, and geometry problem solving. The dataset generation process prioritizes both diversity and realism, ensuring that the instructions capture varying degrees of complexity and linguistic styles. During training, we apply an augmentation strategy by randomly sampling different types of QA prompts with balanced probabilities, thereby enabling the model to generalize effectively across 35 MiniMax-01: Scaling Foundation Models with Lightning Attention diverse instructional formats and interaction patterns. 6.1.4. Data Distribution To demonstrate the diversity of our VLM data, we uniformly sample 1 million imageinstruction pairs from the instruction data and use another VLM to assign concise tag (e.g., object localization) that represents the primary capability required for each pair. This analysis yielded around 50,000 unique tags, and the top 2,817 tags appeared more than 10 times. The distribution of these prominent tags is visualized in Figure 17, where we further group these top tags into 14 major categories. 6.2. Architecture 6.2.1. Overall Architecture Our MiniMax-VL-01 architecture adheres to the ViT-MLP-LLM paradigm, which has been widely embraced in numerous multimodal large language models (MLLMs). The architecture consists of three main components: Vision Transformer (ViT) with 303 million parameters for visual encoding, two-layer MLP projector initialized randomly for image adaptation, and the MiniMax-Text-01 model serving as the foundational large language model (LLM). Figure 17 Visualization of top tags of sampled instruction data. The category and percentage for each group of clustered tags are displayed in the inner layer, only top-10 tags of each group are displayed for clarity. We implement dynamic resolution strategy by resizing the input image according to predefined grid configuration list, ranging from 336336 to 20162016, while maintaining standard thumbnail at resolution of 336 336. The resized images are subsequently partitioned into non-overlapping patches, each measuring 336 336. Both the image patches and the thumbnail are independently encoded, and their encoded features are concatenated to construct comprehensive image feature representation. In contrast to traditional approaches that rely on pooling or other downsampling techniques to compress feature representations, our model leverages its powerful capacity for processing long sequences, allowing for the direct utilization of raw high-dimensional features during training. This strategy mitigates potential information loss and substantially improves the models adaptability to multi-scale inputs. Moreover, by projecting both image patches and thumbnails into unified feature space, our method significantly enhances the models robustness and representational expressiveness when handling diverse and complex visual inputs. 6.2.2. Vision Encoder We employ lightweight ViT-L/14 (Dosovitskiy et al., 2021) as the foundational structure for our vision encoder and train it from scratch. Following standard pipeline, the input image tensor is initially processed through convolutional layer to extract discrete patches, to which absolute 36 MiniMax-01: Scaling Foundation Models with Lightning Attention positional embeddings are subsequently appended. The resulting tensors are then passed through series of multi-head residual attention blocks. This architecture is particularly effective in capturing intricate visual details and the complex interrelationships within images. We utilize contrastive learning to enhance the alignment between corresponding image-caption pairs while diminishing the alignment between non-corresponding pairs. Specifically, we follow the approach introduced in CoCa (Yu et al., 2022), which augments image-text contrastive learning with an additional decoder and image-text cross-attention mechanisms. The network is jointly optimized using combination of contrastive loss and cross-entropy loss. Our ViT-L/14 model is initially trained at resolution of 224 224 for 37 billion image-caption pairs and subsequently fine-tuned at 336 336 for 1.2 billion pairs. For both resolutions, the captions are truncated to 76 tokens. Our ViT-L/14 encoder achieves zero-shot classification accuracy of 80.55% at 336 336 resolution on the ImageNet-1K dataset. 6.3. Training Recipes We employ four-stage training strategy to enable the model to progressively develop comprehensive multimodal understanding capabilities while retaining its language understanding skills. Additionally, the models question-answering and instruction-following abilities, as well as its alignment with human preferences, are methodically refined throughout these stages. Stage I: Modality alignment. In this stage, our primary objective is to achieve alignment between visual and text tokens by enabling the model to accurately generate appropriate captions for given images. To this end, we update the weights of both the image adapter and the vision encoder to optimize their performance in this multimodal task. During this phase, we utilize total of 80 billion tokens sampled from our image description dataset. Empirically, we have found that increasing the image resolution does not yield improvements in downstream task accuracy. Therefore, all images are processed at fixed resolution of 336 336 to reduce computational costs. Stage II: Enhancement of Vision Understanding. This stage can be regarded as standard instruction tuning phase, during which all model parameters are open to updates. The primary goal is to align the models output with human instructions and enhance its ability to perform diverse range of vision understanding tasks. To achieve this, the model is trained using 420 billion multimodal tokens sampled from our instruction datasets, combined with MiniMax-Text-01 post-training data in ratio of 20:1. This approach ensures that the language modeling capability is maintained while the model acquires new multimodal capabilities. Stage III: Enhancement of User Experience. This stage is designed to further enhance the models capabilities in real-world scenarios and when handling challenging user inputs. We curate sophisticated multimodal data using images sourced from applications that people commonly interact with. Conversations are meticulously labeled to emulate authentic user input and to ensure the provision of accurate, helpful, and diverse responses across multiple conversational turns. The data construction for this stage is guided by an independent human-labeled test set that prioritizes not only accuracy but also the overall quality in terms of user experience. The resulting dataset comprises 44.8 billion multimodal tokens and is trained for one epoch. Stage IV: Enhancement of Preference. In the final stage, we utilize Direct Preference Optimization (DPO) to further enhance model performance and user experience. We construct training dataset consisting of 40,000 image-text pairs through the following process: Prompt Selection. Prompts are curated from both instruction data and real user interaction data. These prompts are selected to cover wide range of general scenarios and to specifically address 37 MiniMax-01: Scaling Foundation Models with Lightning Attention persistent issues identified after Stage III, such as occasional repetitive outputs in complex OCR scenarios. Response Generation. We employ diverse strategies, including: generating multiple candidate responses by varying sampling temperature parameters; creating response variants through image weakening in specific scenarios; and using MiniMax-Text-01 to deliberately introduce hallucinations or errors into high-quality responses to generate contrastive samples in specific scenarios. Reward Assignment. Large language models, particularly MiniMax-Text-01, are utilized as evaluators in this stage. Multi-dimensional evaluation criteria are designed to enable systematic and comprehensive assessment of the relationships among prompts, ground truth answers, and generated responses. Pair Construction. Based on the evaluation results, we select the highest-scoring responses as positive samples and the lowest-scoring ones as negative samples, while discarding pairs with insignificant score differences. In addition to incorporating image-text pairs, we also include significant proportion of pure text pairs, as elaborated in Section 5.4.1. It is noteworthy that when Direct Preference Optimization (DPO) is applied to highly capable foundation models, there is propensity for overfitting. To counteract this issue, we adopt an early stopping strategy, which involves terminating the training process prior to the completion of full epoch. This approach is designed to preserve the models generalization capabilities. By following this multi-stage training strategy, we ensure that our model not only demonstrates proficiency in understanding and generating high-quality text but also aligns with human values and safety standards. This comprehensive approach to training allows us to strike balance between model performance and ethical considerations, thereby producing model that is both effective and responsible. 6.4. Benchmarks To assess the performance of our vision-language model, we maintain diverse set of benchmarks, including MMMU (Yue et al., 2024a), MMMU-Pro (Yue et al., 2024b), ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), OCRBench (Liu et al., 2024b), AI2D (Kembhavi et al., 2016), MathVista (Lu et al., 2023), OlympiadBench (He et al., 2024a), MMLongBench-Doc (Ma et al., 2024), MEGA-Bench (Chen et al., 2024a) and an in-house benchmark. These benchmarks help evaluate the models abilities in various areas, including knowledge, visual reasoning, mathematics, science, long context handling, and user experience. We detail our evaluation configuration for each benchmark in Appendix D. As shown in Table 13, MiniMax-VL-01 achieves competitive performance across various vision-language tasks, demonstrating the following key strengths and limitations: Common Downstream Tasks. In standard vision-language downstream tasks, MiniMax-VL-01 exhibits performance on par with GPT-4o, particularly excelling in visual question answering. This strong performance is attributed to its extensive multi-stage training process, enabling the model to effectively understand and reason across visual and textual inputs. However, MiniMax-VL-01 still struggles with advanced mathematical reasoning tasks, as assessed by OlympiadBench (He et al., 2024a). Long Context. We assess MiniMax-VL-01s capability for long-context comprehension and retrieval using MMLongBench-Doc (Ma et al., 2024). The results show that our model outperforms most counterparts, except GPT-4o-11-20. Despite its strong performance overall, MiniMax-VL-01 demonstrates noticeable gap in both single-page (acc: 47.3%) and cross-page (acc: 28.4%) subsets. 38 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 13 Performance of MiniMax-VL-01 on academic and in-house benchmarks. Tasks GPT-4o (11-20) Claude-3.5Sonnet (10-22) Gemini-1.5Pro (002) Gemini-2.0Flash (exp) Qwen2-VL72B-Inst. InternVL 2.5-78B LLama3.2-90B MiniMaxVL-01 MMMU val+dev MMMU-Pro full ChartQA relaxed DocVQA OCRBench AI2D MathVista testmini OlympiadBenchfull 63. 54.5 88.1 91.1 806 83.1 62. 25.2 72.0 54.7 90.8 94.2 82.0 65.4 28.4 Knowledge 68.4 50. Visual Q&A 88.7 91.5 800 70.6 57. 88.3 92.9 846 Mathematics & Sciences 80.9 70. 32.1 85.1 73.1 46.1 Long Context 64. 43.2 91.2 97.1 856 84.4 69. 21.9 66.5 47.3 91.5 96.1 86.8 68.4 25.1 62.1 36.0 85. 90.1 805 78.9 57.3 19.3 68. 52.7 91.7 96.4 865 83.3 68. 24.2 M-LongDocacc 41.4 31.4 26.2 31. 11.6 19.7 13.9 32.5 MEGA-Benchmacro 49. 51.4 45.9 53.9 46.8 45.3 19. 47.4 Comprehensive In-house Benchmark 62.3 47.0 49. 72.1 40.6 34.8 13.6 56.6 Evaluated following 0-shot CoT setting. User Experience Comprehensive Benchmark. On the recently introduced MEGA-Bench (Chen et al., 2024a), realistic and comprehensive evaluation suite, MiniMax-VL-01 shows competitive overall capabilities, surpassing existing open-source vision LLMs. While it excels in diverse sub-tasks such as knowledge and coding, the model faces challenges in more complex tasks, including planning and metric assessments. In-house User Experience Benchmark. While academic benchmarks often focus on problemsolving, they frequently fail to capture the nuances of real-world user interactions with models. To bridge this gap, we develop an in-house benchmark comprising 90 diverse image-related tasks, each designed with tailored and challenging instructions. The images and instructions in the benchmark are strictly deduplicated to not overlap with the training set at any stage. Task relevance is manually verified, with detailed checklist annotated for each sample to ensure precise evaluation. The final test set consists of 524 meticulously annotated samples in both Chinese and English, but Chinese is primarily used. We illustrate some samples in Appendix C. In win-rate comparison against top-leading vision-language model, our model outperforms all open-source models and approaches the performance of GPT-4o-11-20 with narrow margin. 7. Conclusion and Future work In this report, we present MiniMax-Text-01 and MiniMax-VL-01, two novel models developed entirely from the ground up. These models demonstrate top-tier performance across standard benchmarks, particularly excelling in long-context processing with the ability to handle context windows of up to 4 million tokens. Our research findings challenge the prevailing assumption that state-of-the-art MiniMax-01: Scaling Foundation Models with Lightning Attention language models must be built upon traditional attention mechanisms. By strategically integrating linear attention with optimized hardware utilization and carefully designing training recipes, we have successfully expanded the context window by an order of magnitude. This breakthrough not only enhances the efficiency and scalability of LLMs but also paves the way for future models to support even longer context windows and facilitate the development of more sophisticated AI agents. To promote collaboration and advancement in the field, we have made our model publicly available at https://github.com/MiniMax-AI. For general use and evaluation, we provide Chatbot with online search capabilities (https://www.hailuo.ai/) and the online API (https://intl.minimaxi.com). We are committed to keeping this series open source and will release updates as we develop improved models. While MiniMax-Text-01 and MiniMax-VL-01 show strong performance in general language and vision-language tasks, we acknowledge several limitations that necessitate further exploration: 1. Long-Context Evaluation: Current evaluation datasets for long-context retrieval tasks are primarily designed for artificial or simplified scenarios, and the assessment of long-text reasoning capabilities remains limited in practical applications such as document analysis. We plan to enhance long-context retrieval in more realistic settings and expand the evaluation of longcontext reasoning across wider array of tasks. 2. Model Architecture: The model currently retains 1/8 component with vanilla softmax attention. We are investigating more efficient architectures that can eliminate softmax attention entirely, potentially enabling unlimited context windows without computational overhead. 3. Complex Programming Tasks: The models performance on advanced programming tasks is to be improved, as the coding dataset in our pre-training stage is still limited at the moment. We are continuously improving training data selection and refining continue training procedures to address these limitations in the next model version."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, S√©bastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, 2023. Anthropic. Introducing claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018. 40 MiniMax-01: Scaling Foundation Models with Lightning Attention Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022b. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. Andrei Broder. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pages 2129. IEEE, 1997. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, et al. MEGA-Bench: Scaling multimodal evaluation to over 500 real-world tasks. arXiv preprint arXiv:2410.10563, 2024a. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, and Feng Zhao. MindSearch: Mimicking human minds elicits deep ai searcher. arXiv preprint arXiv:2407.20183, 2024b. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International Conference on Machine Learning (ICML), pages 40574086. PMLR, 2022. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29242936, 2019. 41 MiniMax-01: Scaling Foundation Models with Lightning Attention Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=mZn2Xyh9Ec. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. FlashAttention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35: 1634416359, 2022. Alexandre de Br√©bisson and Pascal Vincent. cheap linear attention mechanism with fast lookups and fixed-size representations. arXiv preprint arXiv:1609.05866, 2016. DeepSeek-AI. DeepSeek-V3 technical report, 2024. URL https://arxiv.org/abs/2412.19437. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, et al. survey on in-context learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 11071128, 2024. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=YicbFdNTTy. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23682378, 2019. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Leo Gao, Tom Dupr√© la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. Zamba: compact 7b SSM hybrid model. arXiv preprint arXiv:2405.16712, 2024. 42 MiniMax-01: Scaling Foundation Models with Lightning Attention Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural In Proceedings of the thirteenth international conference on artificial intelligence and networks. statistics, pages 249256. JMLR Workshop and Conference Proceedings, 2010. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id= tEYskw1VY2. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. OlympiadBench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024a. Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, et al. Chinese simpleQA: chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024b. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b. URL https://openreview.net/forum?id=7Bywt2mQsCe. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487, 2022. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, pages 3001630030, 2022. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. RULER: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International conference on machine learning, pages 90999117. PMLR, 2022. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o system card. arXiv preprint arXiv:2410.21276, 2024. 43 MiniMax-01: Scaling Foundation Models with Lightning Attention Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7B. arXiv preprint arXiv:2310.06825, 2023. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, 2017. G. Kamradt. Llmtest_needleinahaystack, 2023. URL https://github.com/gkamradt/LLMTest_ NeedleInAHaystack. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7: 453466, 2019. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84248445, 2022. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb. Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024a. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-Hard and BenchBuilder pipeline. arXiv preprint arXiv:2406.11939, 2024b. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=WsRHpHH4s0. 44 MiniMax-01: Scaling Foundation Models with Lightning Attention Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by ChatGPT really correct? rigorous evaluation of large language models for code generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 2155821572, 2023. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. OCRBench: On the hidden mystery of OCR in large multimodal models. Science China Information Sciences, 67(12):220102, 2024b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. MMLongBench-Doc: Benchmarking long-context document understanding with visualizations. arXiv preprint arXiv:2407.01523, 2024. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. DocVQA: dataset for VQA on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training. arXiv preprint arXiv:1812.06162, 2018. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, 2018. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36:5035850376, 2023. NVIDIA. Transformer engine, 2023. URL https://github.com/NVIDIA/TransformerEngine. Jupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Reuse, dont retrain: recipe for continued pretraining of language models. arXiv preprint arXiv:2407.07263, 2024. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset In Thirty-seventh Confor Falcon LLM: Outperforming curated corpora with web data only. ference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=kM5eGcdCzq. Guilherme Penedo, Hynek Kydl√≠ƒçek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The FineWeb datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum?id=n6SCkn2QaG. 45 MiniMax-01: Scaling Foundation Models with Lightning Attention Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 70257041, 2022a. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosFormer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022b. URL https://openreview.net/forum?id=Bl8CQrx2Up4. Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id= IxmWsm4xrua. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 3320233221, 2023b. Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, and Yiran Zhong. You only scan once: Efficient multi-dimension sequential modeling with lightnet. arXiv preprint arXiv:2405.21022, 2024a. Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024b. Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Various lengths, constant speed: Efficient language modeling with lightning attention. In International conference on machine learning, pages 4151741535. PMLR, 2024c. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. HGRN2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024d. Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training Gopher. arXiv preprint arXiv:2112.11446, 2021. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher Manning, and Chelsea Finn. Direct preference optimization: your language model is secretly reward model. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 5372853741, 2023. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id= Ti67584b98. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. arXiv preprint arXiv:2406.07522, 2024. MiniMax-01: Scaling Foundation Models with Lightning Attention Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial Winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 44634473, 2019. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open dataset of CLIP-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. SCROLLS: Standardized comparison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1200712021, 2022. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, and Yiran Zhong. Scaling laws for linear complexity language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1637716426, 2024. Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair encoding: text compression scheme that accelerates pattern matching. Technical Report DOI-TR-161, Department of Informatics, Kyushu University, 1999. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, and Yiran Zhong. Linear attention sequence parallelism. arXiv preprint arXiv:2404.02882, 2024. Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, and Luke Melas-Kyriazi. benchmark for learning to translate new language from one grammar book. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=tbVWug9f2h. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024a. Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et al. Jamba-1.5: Hybrid Transformer-Mamba models at scale. arXiv preprint arXiv:2408.12570, 2024b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 47 MiniMax-01: Scaling Foundation Models with Lightning Attention Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, et al. Michelangelo: Long context evaluations beyond haystacks via latent structure queries. arXiv preprint arXiv:2409.12640, 2024. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. DeepNet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024a. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek Wong, and Lidia Chao. Learning deep transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 18101822, 2019. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https://openreview.net/forum?id=y10DM6R2r3. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. CoCa: Contrastive captioners are image-text foundation models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=Ee277P3AYC. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024a. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. MMMU-Pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big Bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine In Proceedings of the 57th Annual Meeting of the Association for really finish your sentence? Computational Linguistics, pages 47914800, 2019. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. MiniMax-01: Scaling Foundation Models with Lightning Attention A. Contributors The contributors to the report are listed in alphabetical order as follows: Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu 49 MiniMax-01: Scaling Foundation Models with Lightning Attention B. MiniMax-Text-01 Case Demonstrations We show our models performance under real-world user interactions. To protect the privacy of our users, all user requests shown below are written by our human evaluators, imitating the way users interact with the model, if not from open-source benchmarks. B.1. Learning New Language From Long Context Our prompt for applying MTOB follows that of Gemini-1.5 (Team et al., 2024a), detailed as follows."
        },
        {
            "title": "MTOB Case",
            "content": "User Request ( Instruction + Grammar book + Word List + Parallel sentences ) You are tasked with translating the following sentence from Kalamang to English: \"Sontum kawirten hari minggu gerejao nasumbiyan.\" You will be given field linguistics grammar book, bilingual word list to aid you. Here is the book, \"A grammar of Kalamang\": START OF GRAMMAR BOOK { grammar book } END OF GRAMMAR BOOK The grammar book is now over. Remember that you are tasked with translating the following sentence from Kalamang to English Sontum kawirten hari minggu gerejao nasumbiyan. Now here is the bilingual word list: START OF WORD LIST { word list } END OF WORD LIST The bilingual word list is now over. Remember that you are tasked with translating the following sentence from Kalamang to English Sontum kawirten hari minggu gerejao nasumbiyan. You will use the same style as the parallel sentences immediately below. Now here is the collection of parallel sentences: START OF PARALLEL SENTENCES english: Sakina is pouching guavas. kalamang: Sakina sarimara lawat. english: There are many lusi muaun trees at Sar, but they are dry. kalamang: Lusi muaun me Sarko reidak ma ma he sasa. . . . { total of 375 examples of parallel sentences } . . . END OF PARALLEL SENTENCES The collection of parallel sentences is now over. Now translate the following sentence from Kalamang to English, using the style from the parallel sentences immediately above. Translate: \"Sontum kawirten hari minggu gerejao nasumbiyan\". understand that you may not be familiar enough with Kalamang to make confident translation, but please give your best guess. Respond with only the translation and no other text."
        },
        {
            "title": "Golden Answer",
            "content": "MiniMax-Text-"
        },
        {
            "title": "Suboptimal",
            "content": "Christians worship at the church on Sunday. The Christians pray at church on Sunday. The person is Christian on Sunday. As shown, MiniMax-Text-01 can learn from the given grammar book, word list and parallel sentences, where under-performing models response has semantic errors. MiniMax-01: Scaling Foundation Models with Lightning Attention B.2. Memorizing Long History Dialogues We present an example of how our MR-NIAH samples are designed, where the model is asked to precisley retrieve one of the history interactions with the user from up to 1889 history interactions for the English benchmark and 2053 history interactions for the Chinese benchmark. These history queries were created by Mandarin-speaking human evaluators with help from previous version of our model, where each query is translated into English, and we obtain the assistants responses from this model. We combine these interactions as history dialogues, and insert each needle conversation into corresponding positions detailed in Section 5.7.2.1. MR-NIAH Case User Request ( Haysack dialogues + Target + Haysack dialogues + Query ) User: Please continue writing the follow-up content for this poem Drunkenly lying under the moon in the clouds, laughing at the flowers in the water. Assistant: Certainly! The opening line you provided, Drunkenly lying under the moon in the clouds, laughing at the flowers in the water,√´vokes sense of whimsical detachment and romantic embrace of natures beauty. Heres continuation of the poem that builds on this theme: *Drunkenly lying under the moon in the clouds, Laughing at the flowers in the water, find solace in the nights gentle shroud, Where dreams and reality softly hover.** The stars above, like scattered coins, Glimmer with secrets of the ancient skies, . . . { many haysack dialogues } User: Write poem about penguins. Assistant: On the peak of the Antarctic iceberg, Penguins dance with tiny wings. Wearing black and white tuxedos, stumbling steps, They smile at the severe frost in the cold wind. { many haysack dialogues } User: Write poem about penguins. Assistant: Under the sun, penguins line up, Like guards, protecting their homeland. In their eyes, warm light hides, Exceptionally bright in the polar tranquility. { many haysack dialogues } User: Please repeat the first poem you wrote about penguins."
        },
        {
            "title": "Golden Answer",
            "content": "MiniMax-Text-"
        },
        {
            "title": "Suboptimal",
            "content": "On the peak of the Antarctic iceberg, Penguins dance with tiny wings. Wearing black and white tuxedos, stumbling steps, They smile at the severe frost in the cold wind. On the peak of the Antarctic iceberg, Penguins dance with tiny wings. Wearing black and white tuxedos, stumbling steps, They smile at the severe frost in the cold wind. Under the sun, penguins line up, Like guards, protecting their homeland. In their eyes, warm light hides, Exceptionally bright in the polar tranquility. We will release the data and code of this benchmark to the community. 51 MiniMax-01: Scaling Foundation Models with Lightning Attention B.3. Summarizing Long Papers with Figures When dumped PDF paper and instructed to add figures from the paper, Hailuo AI is able to summarize or analyze the given long text and respond with key figures cited. User Request ( Query + PDF ) please summary this paper for me in english, best to add key figures of this paper in your reply at proper positions. Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction MiniMax-TextSummary of the Paper: \"Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction\" 1. Introduction and Motivation The rapid advancement of large language models (LLMs) has significantly enhanced their capabilities but has also raised concerns about their alignment with human values and intentions. Current alignment strategies, such as Supervised Fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), have shown potential but face challenges in balancing deployment complexity and performance across various tasks. Inference-time methods, while lightweight, struggle to precisely distill human values into LLM outputs, especially in long-context generations. This paper introduces novel alignment paradigm called Streaming Distribution Induce Aligner (Stream Aligner), which combines the efficiency of inferencetime methods with the enhanced performance of additional models to achieve dynamic sentence-level correction. 2. Stream Aligner Paradigm Stream Aligner operates as plug-and-play module in the generation pipeline, correcting sentences generated by the upstream model and feeding the corrected suffix back for further generation. This process repeats iteratively until the response is complete, ensuring that every sentence in the output aligns with human preferences. Key Features: Reduced Dependency on Additional Models: By focusing on sentence-level correction, Stream Aligner leverages more of the upstream models performance, reducing reliance on the size and capabilities of additional models. Enhanced Reasoning Abilities: The iterative correction process allows Stream Aligner to correct reasoning errors and add inductions for subsequent steps, thereby improving the models reasoning capabilities. 3. Technical Details Training: Stream Aligner is fine-tuned on sentence-level preference dataset to learn the residuals between preferred and non-preferred responses. The training objective is to minimize the negative log-likelihood loss between the models output and the corrected answer. Inference: During inference, Stream Aligner takes the users query and the prefix as input. The upstream model generates the original answer, which is then corrected by Stream Aligner. Each correction is incorporated into the prefix, and the process repeats until the generation stops or the prefix exceeds the maximum length. 4. Experimental Results The paper evaluates Stream Aligner on three tasks: helpful and harmless QA, math questions, and summary tasks. The results demonstrate significant improvements: Helpfulness and Harmlessness: Aligner-2B achieved and applied 41.2% increase 36.0% increase to Stream Llama2-70B-chat helpfulness harmlessness. in in 6. Comparison to Other Alignment Methods Stream Aligner outperforms other alignment methods such as Supervised Finetuning (SFT) and Direct Preference Optimization (DPO) in terms of accuracy improvements. It also achieves the performance of Aligner-70B using only 2B parameters, showcasing both superior performance and efficiency. 7. Interpretability The paper explores the interpretability of Stream Aligner through representation engineering and activation steering. The results show that Stream Aligner has internalized the correction paradigm as representation, similar to Aligner, but with more layers involved in deciding corrections, retasks. flecting the complexity of mathematical Math Ability: Stream Aligner-8B applied to Llama3-70B-Instruct achieved 3.5% improvement in math ability. 8. Conclusion Reasoning Tasks: The experiments show that the longer the average intervention by Stream Aligner, the higher the accuracy after the intervention, indicating its effectiveness in reasoning-based tasks. 5. Ablation Studies The paper conducts ablation studies to verify the correction capabilities of Stream Aligner under different supervision quantities and generation pipelines: Generation-Correction Frequency: The performance of Stream Aligner increases with the number of generation-correction cycles, demonstrating its ability to enhance the upstream models performance with limited supervision. Generation Methods: The study compares the classic sentence-by-sentence correction pipeline with new continue generation pipeline. While the continue generation pipeline shows excellent performance in math and harmless QA tasks, it is more compute-consuming and converges to similar result as the direct generation pipeline. Stream Aligner is novel alignment paradigm that effectively elicits the latent knowledge of the upstream model while maintaining efficiency and enhanced performance. It achieves significant improvements in helpfulness, harmlessness, and reasoning abilities, making it promising approach for aligning LLMs with human values. Limitations Despite its advantages, Stream Aligner introduces additional computational overhead during inference and requires high-quality training data. Its performance may be limited when dealing with extremely difficult out-of-distribution inputs. This summary provides an overview of the papers key contributions, methodologies, experimental results, and conclusions, highlighting the innovative approach of Stream Aligner in aligning LLMs with human preferences. 52 MiniMax-01: Scaling Foundation Models with Lightning Attention B.4. Translating Long Papers User Request ( Query + PDF ) please help me to translate the whole paper into French. When brain-inspired AI meets AGI.pdf MiniMax-Text-01 Quand lIA inspir√©e du cerveau rencontre lAGI Lin Zhao a,1, Lu Zhang b,1, Zihao Wu a, Yuzhong Chen c, Haixing Dai a, Xiaowei Yu b, Zhengliang Liu a, Tuo Zhang d, Xintao Hu d, Xi Jiang c, Xiang Li e, Dajiang Zhu b, Dinggang Shen f,g,h, Tianming Liu a,* *School of Computing, The University of Georgia, Athens 30602, USA* *Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington 76019, USA* *MOE Key Laboratory for Neuroinformation, School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu 611731, China* *School of Automation, Northwestern Polytechnical University, Xian 710072, China* *Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston 02115, USA* *School of Biomedical Engineering, ShanghaiTech University, Shanghai 201210, China* *Shanghai United Imaging Intelligence Co., Ltd., Shanghai 200230, China* *Shanghai Clinical Research and Trial Center, Shanghai, 201210, China* R√©sum√© LIntelligence G√©n√©rale Artificielle (AGI) √©t√© un objectif de longue date pour lhumanit√©, visant √† cr√©er des machines capables deffectuer toute t√¢che intellectuelle que les humains peuvent accomplir. Pour atteindre cet objectif, les chercheurs en AGI sinspirent du cerveau humain et cherchent √† reproduire ses principes dans des machines intelligentes. Lintelligence artificielle inspir√©e du cerveau est un domaine qui √©merg√© de cet effort, combinant des insights de la neuroscience, de la psychologie et de linformatique pour d√©velopper des syst√®mes dIA plus efficaces et puissants. Dans cet article, nous offrons un aper√ßu complet de lIA inspir√©e du cerveau du point de vue de lAGI. Nous commen√ßons par les progr√®s actuels de lIA inspir√©e du cerveau et de sa connexion √©tendue avec lAGI. Nous couvrons ensuite les caract√©ristiques importantes de lintelligence humaine et de lAGI (par exemple, la mise √† l√©chelle, la multimodalit√© et le raisonnement). Nous discutons des technologies importantes pour atteindre lAGI dans les syst√®mes dIA actuels, telles que lapprentissage contextuel et le r√©glage des invites. Nous examinons √©galement l√©volution des syst√®mes AGI √† la fois du point de vue algorithmique et infrastructurel. Enfin, nous explorons les limites et lavenir de lAGI. 1. LIA inspir√©e du cerveau et lAGI Le cerveau humain est largement consid√©r√© comme lun des syst√®mes de traitement de linformation les plus complexes et avanc√©s au monde. Il comprend plus de 86 milliards de neurones, chacun capable de former jusqu√† 10 000 synapses avec dautres neurones, ce qui r√©sulte en un r√©seau de connexions exceptionnellement complexe permettant la prolif√©ration de lintelligence. Outre la complexit√© physiologique, le cerveau humain pr√©sente une large gamme de caract√©ristiques qui contribuent √† ses capacit√©s fonctionnelles remarquables. Par exemple, il peut int√©grer des donn√©es provenant de plusieurs modalit√©s sensorielles, telles que la vision, laudition et le toucher, lui permettant de former une perception coh√©rente du monde. La capacit√© du cerveau √† effectuer un traitement parall√®le est √©galement essentielle pour g√©rer efficacement plusieurs flux dinformations simultan√©ment. Cela est r√©alis√© via les connexions et les communications en temps r√©el entre diff√©rentes r√©gions du cerveau, bien que le m√©canisme ne soit pas enti√®rement compris. De plus, le cerveau est tr√®s adaptable, capable de r√©organiser sa structure et sa fonction en r√©ponse aux environnements et exp√©riences changeants. Cette propri√©t√©, connue sous le nom de neuroplasticit√©, permet au cerveau dapprendre et de d√©velopper de nouvelles comp√©tences tout au long de la vie. Le cerveau humain est √©galement remarquable pour ses fonctions cognitives de haut niveau, telles que la r√©solution de probl√®mes, la prise de d√©cision, la cr√©ativit√© et le raisonnement abstrait, soutenues par le cortex pr√©frontal, une r√©gion du cerveau particuli√®rement bien d√©velopp√©e chez les humains. Cr√©er des syst√®mes dIntelligence G√©n√©rale Artificielle (AGI) ayant une intelligence de niveau humain ou m√™me sup√©rieure et capables deffectuer une large gamme de t√¢ches intellectuelles, telles que le raisonnement, la r√©solution de probl√®mes et la cr√©ativit√©, est la qu√™te de lhumanit√© depuis des si√®cles, remontant au milieu du 20√®me si√®cle. Dans les ann√©es 1940, des pionniers comme Alan Turing ont d√©velopp√© des id√©es pr√©coces sur les machines informatiques et leur potentiel pour simuler la pens√©e humaine. Depuis lors, chercher √† reproduire les principes de lintelligence humaine dans des syst√®mes artificiels consid√©rablement favoris√© le d√©veloppement de lAGI et les applications correspondantes. Ces principes incluent la structure et la fonction des r√©seaux de neurones, la plasticit√© des connexions synaptiques, la dynamique de lactivit√© neuronale, et plus encore. En 1943, McCulloch et Pitts ont propos√© le tout premier mod√®le math√©matique dun neurone artificiel, √©galement connu sous le nom de neurone MCP (McCulloch-Pitts). Inspir√© par la th√©orie de Hebb sur la plasticit√© synaptique, Frank Rosenblatt con√ßu le perceptron, une am√©lioration majeure par rapport au mod√®le de neurone MCP, et montr√© que, en assouplissant certaines des r√®gles du MCP, les neurones artificiels pouvaient r√©ellement apprendre √† partir des donn√©es. Cependant, la recherche sur les r√©seaux de neurones artificiels stagn√© jusqu√† ce que la r√©tropropagation soit propos√©e par Werbos en 1975. La r√©tropropagation √©t√© inspir√©e par la fa√ßon dont le cerveau modifie les forces des connexions entre les neurones pour apprendre et am√©liorer ses performances gr√¢ce √† la plasticit√© synaptique. La r√©tropropagation tente de reproduire ce processus en ajustant les poids (forces synaptiques) entre les neurones dans un r√©seau de neurones artificiels. Malgr√© cette proposition pr√©coce, la r√©tropropagation na pas attir√© une attention g√©n√©ralis√©e jusquaux ann√©es 1980, lorsque des chercheurs comme David Rumelhart, Geoffrey Hinton et Ronald Williams ont publi√© des articles d√©montrant lefficacit√© de la r√©tropropagation pour entra√Æner les r√©seaux de neurones. Les r√©seaux de neurones convolutifs (CNN) sont lun des types de r√©seaux de neurones les plus utilis√©s et les plus efficaces pour traiter les informations visuelles. Les CNN sont √©galement inspir√©s de lorganisation hi√©rarchique du cortex visuel dans le cerveau, ce qui remonte aux travaux de David Hubel et Torsten Wiesel dans les ann√©es 1960. Dans le cortex visuel, les neurones sont dispos√©s en couches, chaque couche traitant les informations visuelles de mani√®re hi√©rarchique. Lentr√©e de la r√©tine est dabord trait√©e par une couche de cellules simples qui d√©tectent les bords et les orientations, puis transmise √† des cellules plus complexes qui reconnaissent des caract√©ristiques plus complexes telles que les formes et les textures. Leurs travaux ont fourni des insights sur la fa√ßon dont le syst√®me visuel traite les informations et ont inspir√© le d√©veloppement des CNN qui pourraient reproduire ce processus de traitement hi√©rarchique. Les m√©canismes dattention dans les r√©seaux de neurones artificiels sont √©galement inspir√©s de la fa√ßon dont le cerveau humain s√©lectionne s√©lectivement certains aspects de lentr√©e sensorielle ou des processus cognitifs, nous permettant de nous concentrer sur les informations importantes tout en filtrant les d√©tails non pertinents. Lattention √©t√© √©tudi√©e dans les domaines de la psychologie et des neurosciences pendant de nombreuses ann√©es, et son application √† lintelligence artificielle fait avancer consid√©rablement nos pas vers lAGI. Le mod√®le Transformer, bas√© sur le m√©canisme dautoattention, est devenu la base de nombreux r√©seaux de neurones artificiels de pointe tels que BERT et GPT. En adaptant les m√©canismes dauto-attention au traitement dimages, le mod√®le Vision Transformer (ViT) d√©montr√© des performances de pointe dans diverses t√¢ches de vision par ordinateur (CV) en repr√©sentant limage comme une s√©quence de patchs. R√©cemment, de plus en plus de preuves sugg√®rent que les r√©seaux de neurones artificiels (ANN) et les r√©seaux de neurones biologiques (BNN) peuvent partager des principes communs dans loptimisation de larchitecture du r√©seau. Par exemple, la propri√©t√© de petit monde dans les r√©seaux structurels et fonctionnels du cerveau √©t√© largement √©tudi√©e dans la litt√©rature. Dans une √©tude r√©cente, les r√©seaux de neurones bas√©s sur les graphes al√©atoires de Watts-Strogatz (WS) avec des propri√©t√©s de petit monde ont d√©montr√© des performances comp√©titives par rapport aux mod√®les con√ßus √† la main et optimis√©s par NAS (recherche darchitecture neuronale). De plus, lanalyse posteriori montr√© que la structure graphique des ANN les plus performants, tels que les CNN et le Perceptron multicouche (MLP), est similaire √† celle des vrais BNN, tels que le r√©seau dans le cortex du macaque. Chen et al. ont propos√© une repr√©sentation relationnelle unifi√©e et biologiquement plausible des mod√®les ViT, trouvant que la performance du mod√®le √©tait √©troitement li√©e aux mesures du graphe et que le ViT une grande similarit√© avec les vrais BNN. Zhao et al. ont synchronis√© lactivation des ANN et des CNN et ont trouv√© que les CNN avec des performances plus √©lev√©es sont similaires aux BNN en termes dactivation de la repr√©sentation visuelle. Liu et al. ont coupl√© les neurones artificiels dans le mod√®le BERT avec les neurones biologiques dans le cerveau humain, et ont trouv√© que les neurones artificiels peuvent porter des informations linguistiques/s√©mantiques significatives et sancrer √† leurs signatures de neurones biologiques avec interpr√©tabilit√© dans un contexte neurolinguistique. Zhou et al. ont trait√© chaque dimension cach√©e dans Wav2Vec2.0 comme un neurone artificiel et les ont connect√©s avec leurs homologues biologiques dans le cerveau humain, sugg√©rant une relation √©troite entre les deux domaines en termes dinformations neurolinguistiques. Suivant cette tendance, il un int√©r√™t croissant pour le d√©veloppement de lintelligence artificielle inspir√©e du cerveau en sinspirant de certaines connaissances pr√©alables du cerveau humain, telles que lorganisation de la structure et de la fonction du cerveau. Par exemple, Huang et al. ont propos√© un r√©seau de vision antagoniste inspir√© du cerveau (BI-AVAN) qui imite le processus de comp√©tition biais√©e dans le syst√®me visuel humain pour d√©coder lattention visuelle humaine. Inspir√© par lorganisation c≈ìur-p√©riph√©rie du cerveau humain, Yu et al. ont propos√© un mod√®le de transformateur de vision guid√© par le principe c≈ìur-p√©riph√©rie (CPViT) pour la reconnaissance dimages avec des performances et une interpr√©tabilit√© am√©lior√©es. De m√™me, Zhao et al. ont mis en ≈ìuvre le principe c≈ìur-p√©riph√©rie dans la conception des motifs de c√¢blage du r√©seau et la sparsification de lop√©ration de convolution. Le CP-CNN propos√© guid√© par le principe c≈ìur-p√©riph√©rie d√©montre lefficacit√© et la sup√©riorit√© par rapport aux m√©thodes bas√©es sur les CNN et ViT. Un autre groupe d√©tudes opt√© pour les r√©seaux de neurones √† pointes (SNN) qui imitent √©troitement le comportement des neurones biologiques dans le cerveau. Par exemple, les SNN ont √©t√© utilis√©s pour cartographier et comprendre les donn√©es c√©r√©brales spatio-temporelles, d√©coder et comprendre lactivit√© musculaire √† partir des signaux d√©lectroenc√©phalographie, et les interfaces cerveau-machine. LIA inspir√©e du cerveau √©galement contribu√© au d√©veloppement darchitectures mat√©rielles qui imitent la structure et la fonction du cerveau. Le calcul neuromorphique, un domaine d√©tude qui vise √† concevoir du mat√©riel informatique qui √©mule les neurones et les synapses biologiques, √©galement gagn√© en attention ces derni√®res ann√©es. Les puces neuromorphiques sont con√ßues pour traiter linformation de mani√®re parall√®le et distribu√©e, de la m√™me mani√®re que le cerveau fonctionne, ce qui peut conduire √† des am√©liorations significatives en termes defficacit√© et de vitesse par rapport aux architectures informatiques traditionnelles. 53 MiniMax-01: Scaling Foundation Models with Lightning Attention Certaines des puces neuromorphiques, comme la puce TrueNorth dIBM et la puce Loihi dIntel, utilisent des r√©seaux de neurones √† pointes pour traiter linformation dune mani√®re qui est plus proche de la fa√ßon dont le cerveau traite linformation. Ces puces ont √©t√© utilis√©es pour une large gamme dapplications, compris la reconnaissance dimages et de la parole, la robotique et les v√©hicules autonomes. Lavancement du mat√©riel inspir√© du cerveau ouvre √©galement la voie √† des avanc√©es significatives dans le domaine de lAGI en pavant la voie pour des plateformes mat√©rielles g√©n√©ralis√©es. Dans lensemble, lIA inspir√©e du cerveau joue un r√¥le crucial dans le d√©veloppement de lAGI  (Fig. 1)  . En sinspirant du cerveau humain, les chercheurs peuvent cr√©er des algorithmes et des architectures mieux adapt√©s pour g√©rer des probl√®mes complexes et r√©els qui n√©cessitent un degr√© √©lev√© de flexibilit√© et dadaptabilit√©. Cela est particuli√®rement important pour lAGI, qui vise √† d√©velopper des machines capables deffectuer une large gamme de t√¢ches, dapprendre de lexp√©rience et de g√©n√©raliser leurs connaissances √† de nouvelles situations. Le cerveau humain est lun des syst√®mes de traitement de linformation les plus complexes connus de nous, et il √©volu√© pendant des millions dann√©es pour √™tre tr√®s efficace et efficace dans la gestion de t√¢ches complexes. En √©tudiant le cerveau et en d√©veloppant des syst√®mes dIA qui imitent son architecture et sa fonction, les chercheurs peuvent cr√©er une AGI plus sophistiqu√©e et adaptable, nous rapprochant de lobjectif ultime de cr√©er des machines qui peuvent √©galer ou surpasser lintelligence humaine. En retour, lAGI √©galement le potentiel de b√©n√©ficier √† lintelligence humaine et de approfondir notre compr√©hension de lintelligence. √Ä mesure que nous continuons √† √©tudier et √† comprendre √† la fois lintelligence humaine et lAGI, ces deux syst√®mes deviendront de plus en plus intriqu√©s, se renfor√ßant et se soutenant mutuellement de mani√®re nouvelle et passionnante. 2. Caract√©ristiques de lAGI 2.1. √âchelle L√©chelle des cerveaux varie consid√©rablement dune esp√®ce animale √† lautre, allant de quelques milliers de neurones chez les invert√©br√©s simples comme les vers n√©matodes, √† plus de 86 milliards de neurones chez les humains. Par exemple, le cerveau dune mouche √† fruits contient environ 100 000 neurones, et le cerveau dune souris contient environ 70 millions de neurones. Pour les primates, le cerveau du macaque environ 1,3 milliard de neurones tandis que le cerveau du chimpanz√© environ 6,2 milliards de neurones. Compar√© √† dautres animaux, le cerveau humain est la structure biologique la plus complexe et la plus sophistiqu√©e connue de la science, contenant plus de 86 milliards de neurones. L√©chelle du cerveau, cest-√†-dire le nombre de neurones, est souvent corr√©l√©e aux capacit√©s cognitives de lanimal et consid√©r√©e comme un facteur dintelligence. La taille et la complexit√© des r√©gions du cerveau associ√©es √† des fonctions cognitives sp√©cifiques, telles que le langage ou la m√©moire, sont souvent directement li√©es au nombre de neurones quelles contiennent. Nous avons lintention dutiliser les grands mod√®les de langage (LLM) (voir le tableau 1) comme un moyen possible d√©tudier lAGI inspir√©e du cerveau, car les LLM sont parmi les premiers mod√®les √† d√©montrer des performances de niveau humain dans diverses t√¢ches. La relation entre le nombre de neurones et les capacit√©s cognitives est √©galement pertinente pour les LLM tels que GPT-2 et GPT-3. Alors que GPT-2 1,5 milliard de param√®tres et √©t√© entra√Æn√© sur 40 gigabytes de donn√©es textuelles, GPT-3 175 milliards de param√®tres et √©t√© entra√Æn√© sur 570 gigabytes de donn√©es textuelles. Cette augmentation significative du nombre de param√®tres permis √† GPT-3 de surpasser GPT-2 sur une gamme de t√¢ches linguistiques, d√©montrant une augmentation de sa capacit√© √† effectuer des t√¢ches linguistiques complexes. En fait, GPT-3 montr√© des performances de niveau humain sur plusieurs benchmarks de traitement du langage naturel, tels que la r√©ponse aux questions, la traduction linguistique et les t√¢ches de compl√©tion de texte. Sa taille et sa capacit√© en traitement du langage naturel en ont fait un outil puissant pour diverses applications, compris les chatbots, la g√©n√©ration de contenu et la traduction linguistique. Cette tendance est similaire √† la fa√ßon dont les cerveaux plus grands sont associ√©s √† des fonctions cognitives plus complexes chez les animaux. √Ä mesure que les LLM continuent de se d√©velopper, il est attendu quils deviendront encore plus capables dapprendre de nouveaux skills avec un petit nombre dexemples de formation, similaire √† la fa√ßon dont les animaux avec des cerveaux plus grands ont des capacit√©s cognitives plus sophistiqu√©es. Cette corr√©lation sugg√®re que l√©chelle peut √™tre un facteur crucial dans la r√©alisation de lAGI. Cependant, il est √† noter que le nombre de param√®tres seuls ne d√©termine pas lintelligence dun LLM. La qualit√© des donn√©es de formation, le processus de formation et larchitecture du mod√®le jouent √©galement des r√¥les importants dans sa performance. En outre, il est n√©cessaire de rechercher des moyens qui permettent aux institutions et aux individus √† ressources limit√©es dacc√©der et de d√©velopper lAGI. Certaines solutions possibles incluent la quantification des mod√®les existants de grande taille, le d√©veloppement darchitectures efficaces, ou la construction de jeux de donn√©es de haute qualit√© qui facilitent la formation du mod√®le. 2.2. Multimodalit√© La capacit√© du cerveau humain √† traiter et int√©grer simultan√©ment des informations provenant de plusieurs modalit√©s sensorielles est une r√©alisation remarquable. Cette caract√©ristique permet aux individus de comprendre le monde qui les entoure √† travers diverses sources dinformation, telles que la vue, le son, le toucher, le go√ªt et lodorat. De plus, le traitement dinformations multimodales permet aux gens de faire des √©valuations plus pr√©cises et compl√®tes de leur environnement et de communiquer efficacement avec les autres. En cons√©quence, lapprentissage r√©ussi √† partir de plusieurs modalit√©s peut am√©liorer les capacit√©s cognitives humaines. √Ä mesure que nous nous effor√ßons de cr√©er des syst√®mes AGI avanc√©s qui surpassent lintelligence humaine, il est crucial quils soient capables dacqu√©rir et ding√©rer des connaissances √† partir de diverses sources et modalit√©s pour r√©soudre des t√¢ches qui impliquent nimporte quelle modalit√©. Par exemple, un AGI devrait √™tre capable dutiliser les connaissances apprises √† partir dimages et de la base de connaissances pour r√©pondre aux questions en langage naturel, ainsi que dutiliser les connaissances apprises √† partir du texte pour effectuer des t√¢ches visuelles. En fin de compte, toutes les modalit√©s se croisent √† travers des concepts universels, tels que le concept quun chien est un chien, ind√©pendamment de la fa√ßon dont il est repr√©sent√© dans diff√©rentes modalit√©s  (Fig. 2)  . Pour construire des syst√®mes dIA multimodaux, une approche prometteuse est dincorporer des signaux de formation provenant de plusieurs modalit√©s dans les LLM. Cela n√©cessite daligner les repr√©sentations internes √† travers diff√©rentes modalit√©s, permettant au syst√®me dIA dint√©grer les connaissances de mani√®re transparente. Par exemple, lorsquun syst√®me dIA re√ßoit une image et un texte associ√©, il doit associer le m√™me objet ou concept entre les modalit√©s. Supposons que lIA voit une image dune voiture avec un texte se r√©f√©rant ont √† ses roues. Dans ce cas, lIA doit pr√™ter attention √† la partie de limage avec les roues de la voiture lorsquelle traite le texte les mentionnant. LIA doit comprendre que limage des roues de la voiture et le texte se r√©f√©rant √† elles d√©crivent le m√™me objet √† travers diff√©rentes modalit√©s. syst√®mes Ces derni√®res ann√©es, les dIA multimodaux exp√©riment√© lalignement du texte/NLP, des images/vision ou de linformation audio dans un espace dencodage pour faciliter la prise de d√©cision multimodale. Lalignement intermodal est essentiel pour diverses t√¢ches, compris la g√©n√©ration texte-image et image-texte, la r√©ponse aux questions visuelles, et la mod√©lisation vid√©o-langage. Dans la section suivante, nous fournissons un bref aper√ßu de ces charges de travail courantes et des mod√®les de pointe correspondants. 2.2.1. G√©n√©ration texte-image et imagetexte CLIP, DALL-E, et leur successeur GLIDE, VisualGPT et Diffusion sont parmi les mod√®les les plus connus qui abordent les descriptions dimages (g√©n√©ration image-texte) et les t√¢ches de g√©n√©ration texte-image. CLIP est une m√©thode de pr√©entra√Ænement qui entra√Æne des encodeurs dimages et de texte s√©par√©s et apprend √† pr√©dire quelles images dans un ensemble de donn√©es sont associ√©es √† diverses descriptions. Notamment, de mani√®re similaire au neurone Halle Berry chez les humains, CLIP √©t√© trouv√© pour avoir des \"neurones multimodaux\" qui sactivent lorsquils sont expos√©s √† la fois au texte de l√©tiquette du classificateur et √† limage correspondante, indiquant une repr√©sentation multimodale fusionn√©e. DALL-E, en revanche, est une variante de GPT3 avec 13 milliards de param√®tres qui prend le texte comme entr√©e et g√©n√®re une s√©quence dimages pour correspondre au texte dentr√©e. Les images g√©n√©r√©es sont ensuite class√©es √† laide de CLIP. GLIDE, une √©volution de DALL-E, utilise toujours CLIP pour classer les images g√©n√©r√©es, mais la g√©n√©ration dimages est accomplie √† laide dun mod√®le de diffusion. Stable Diffusion est √©galement bas√© sur des mod√®les de diffusion tout en op√©rant sur lespace latent de puissants auto-encodeurs pr√©-entra√Æn√©s et ainsi en utilisant des ressources de calcul limit√©es tout en maintenant leur qualit√© et leur flexibilit√©. Le VisualGPT est l√©volution de GPT-2 dun mod√®le de langage unique √† un mod√®le multimodal avec une unit√© dactivation qui se r√©anime elle-m√™me pour produire des activations √©parses qui emp√™chent l√©crasement accidentel des connaissances linguistiques. 1. Fig. Le d√©veloppement de lAGI √©t√© largement inspir√© par l√©tude de lintelligence humaine (HI). En retour, lAGI le potentiel de b√©n√©ficier √† lintelligence humaine. Par exemple, les mod√®les de langage actuels tels que ChatGPT et GPT-4 utilisent lapprentissage par renforcement avec retour humain (RLHF) pour align leur comportement avec les valeurs humaines. √Ä mesure que nous continuons √† √©tudier et √† comprendre √† la fois lintelligence humaine et lAGI, ces deux syst√®mes deviendront de plus en plus intriqu√©s, se renfor√ßant et se soutenant mutuellement de mani√®re nouvelle et passionnante. 2.2.2. R√©ponse aux questions visuelles La r√©ponse aux questions visuelles est une application cruciale de lapprentissage multimodal qui n√©cessite quun mod√®le r√©ponde correctement √† une question bas√©e sur du texte en fonction dune image. Le jeu de donn√©es VQA pr√©sente cette t√¢che, et les √©quipes de Microsoft Research ont d√©velopp√© certaines des approches de pointe pour cela. Lune de ces approches est METER, une structure g√©n√©rale pour former des transformateurs vision-langage performants utilisant une vari√©t√© de sousarchitectures pour les modules encodeur de vision, encodeur de texte, fusion multimodale et d√©codeur. Cette flexibilit√© permet √† METER datteindre des performances de pointe dans une gamme de t√¢ches. Une autre approche prometteuse est le mod√®le de pr√©-entra√Ænement unifi√© VisionLanguage (VLMo), qui utilise un r√©seau transformateur modulaire pour apprendre conjointement un double encodeur et un encodeur de fusion. Chaque bloc du r√©seau contient un pool dexperts sp√©cifiques √† la modalit√© et une couche dauto-attention partag√©e, offrant une flexibilit√© significative pour le r√©glage fin. Cette architecture montr√© des r√©sultats impressionnants sur plusieurs ensembles de donn√©es de r√©f√©rence. 2.2.3. Mod√©lisation vid√©o-langage Traditionnellement, les syst√®mes dIA ont eu du mal avec les t√¢ches bas√©es sur la vid√©o en raison des ressources de calcul √©lev√©es requises. Cependant, cela commence √† changer, gr√¢ce aux efforts dans le domaine de la mod√©lisation vid√©o-langage et dautres t√¢ches multimodales li√©es √† la vid√©o, comme le projet Florence-VL de Microsoft. √Ä la mi-2021, le projet FlorenceVL introduit ClipBERT, une combinaison dun mod√®le CNN et dun mod√®le transformateur qui fonctionne sur des cadres √©chantillonn√©s de mani√®re √©parse. Il est optimis√© de mani√®re globale pour r√©soudre les t√¢ches vid√©o-langage populaires. Les √©volutions ult√©rieures de ClipBERT, telles que VIOLET et SwinBERT, ont introduit le mod√®le de mod√©lisation de jetons visuels masqu√©s et lattention √©parse pour am√©liorer l√©tat de lart en r√©ponse aux questions vid√©o, la recherche vid√©o et le sous-titrage vid√©o. Bien que chacun de ces mod√®les ait des caract√©ristiques uniques, ils utilisent tous une architecture bas√©e sur le transformateur. Typiquement, cette architecture est coupl√©e avec des modules dapprentissage parall√®le pour extraire des donn√©es de diverses modalit√©s et les unifier en une seule repr√©sentation multimodale. R√©cemment, l√©mergence de GPT-4 port√© la recherche multimodale √† un nouveau Selon le dernier article de niveau. recherche officiel, GPT-4 non seulement affiche une grande ma√Ætrise dans divers domaines, compris la litt√©rature, la m√©decine, le droit, les math√©matiques, les sciences physiques et la programmation, mais combine √©galement de mani√®re fluide les comp√©tences et les concepts de plusieurs domaines, d√©montrant une compr√©hension impressionnante des id√©es complexes. De plus, la performance de GPT4 dans toutes ces t√¢ches est remarquablement proche du niveau humain et d√©passe souvent les mod√®les pr√©c√©dents tels que ChatGPT. Compte tenu de l√©tendue et de la profondeur des capacit√©s de GPT-4, il pourrait √™tre consid√©r√© comme une version pr√©coce (bien quincompl√®te) dun syst√®me AGI. 2.2.4. Apprentissage multimodal avec donn√©es auditives Data2vec, une r√©cente d√©veloppement de Meta AI, pr√©sente un nouveau cadre dapprentissage autosupervis√© qui contourne le besoin de donn√©es √©tiquet√©es traditionnelles. En tirant parti des relations internes des donn√©es, il unifie lapprentissage √† travers trois modalit√©s distinctes : images, texte et parole. Utilisant une architecture √† double mode, il utilise un mod√®le \"enseignant\" pour g√©n√©rer des repr√©sentations d√©chantillons, et un mod√®le \"√©tudiant\" pour apprendre de lenseignant √† travers la minimisation dune fonction objectif. Cette m√©thodologie unique permet dobtenir des r√©sultats de pointe dans chacune des trois modalit√©s, marquant un pas important vers la r√©alisation de lintelligence artificielle g√©n√©rale. 54 MiniMax-01: Scaling Foundation Models with Lightning Attention Microsofts Kosmos-1 est un grand mod√®le de langage multimodal qui traite le texte, les donn√©es visuelles et auditives. Utilisant des corpus multimodaux bas√©s sur le web, il comprend les modalit√©s g√©n√©rales et d√©montre lapprentissage contextuel et le suivi des instructions. Ses capacit√©s englobent la compr√©hension du langage, la g√©n√©ration de l√©gendes pour les images, la r√©ponse aux questions visuelles et la reconnaissance dimages, soulignant la capacit√© de transfert intermodal, ce qui facilite l√©change de connaissances entre le langage et les entr√©es multimodales. Il est important de noter que, contrairement aux LLM unimodaux, les LLM multimodaux affichent des performances sup√©rieures non seulement dans les t√¢ches intermodales mais aussi dans les t√¢ches unimodales. Par exemple, lint√©gration de la multimodalit√© dans GPT-4 se traduit par de meilleures performances dans les t√¢ches textuelles par rapport √† ChatGPT. Cela correspond √† la fa√ßon dont les humains per√ßoivent le monde √† travers plusieurs modalit√©s sensorielles. 2.3. Alignement Bien que certains LLM comme BERT, GPT, GPT-2, GPT-3 et Textto-Text Transfer Transformer (T5) aient r√©alis√© des succ√®s remarquables dans des t√¢ches sp√©cifiques, ils ne sont toujours pas encore AGI en raison de leur tendance √† pr√©senter des comportements non intentionnels. Par exemple, ils pourraient g√©n√©rer du texte biais√© ou toxique, inventer des faits ou ne pas suivre les instructions de lutilisateur. La principale raison derri√®re ces probl√®mes est le d√©salignement entre lobjectif de mod√©lisation du langage utilis√© pour de nombreux LLM r√©cents et lobjectif de suivre les instructions de lutilisateur de mani√®re s√ªre et utile. Par cons√©quent, bien que ces mod√®les aient fait des progr√®s significatifs, ils ne sont pas encore capables d√©muler le raisonnement, la prise de d√©cision et la compr√©hension de type humain. Pour atteindre lAGI, il est crucial daligner les mod√®les de langage avec lintention de lutilisateur. Cet alignement permettra aux LLM de fonctionner de mani√®re s√ªre et utile, les rendant plus fiables pour les t√¢ches complexes qui n√©cessitent une prise de d√©cision nuanc√©e et une compr√©hension. Pour ce faire, il est n√©cessaire de d√©velopper de meilleurs algorithmes qui orientent les agents vers les valeurs humaines tout en favorisant les collaborations interdisciplinaires pour clarifier ce que signifient les valeurs humaines. Les d√©veloppements r√©cents dans les grands mod√®les de langage (LLM), tels que Sparrow, InstructGPT, ChatGPT et GPT-4, ont abord√© le probl√®me de lalignement avec les instructions humaines en utilisant lapprentissage par renforcement √† partir du retour dexp√©rience humain (RLHF). Lapprentissage par renforcement est un type dapprentissage automatique o√π le mod√®le apprend √† prendre des d√©cisions en fonction du retour dexp√©rience quil re√ßoit sous forme de r√©compenses. Le but du mod√®le est de maximiser sa r√©compense totale au fil du temps. RLHF utilise les pr√©f√©rences humaines comme signal de r√©compense pour affiner les LLM et permettre aux LLM dapprendre et dam√©liorer √† partir du retour dexp√©rience humain, ce qui essaie de pr√©dire quelles r√©ponses les humains r√©agiront positivement √† et aide √† r√©duire les comportements non intentionnels et √† augmenter leur fiabilit√© pour les t√¢ches complexes. Puisque le mod√®le apprend des humains en temps r√©el, il devient de mieux en mieux √† pr√©dire. √Ä la fin du processus de formation, les syst√®mes AI commencent √† imiter les humains. RLHF montr√© des r√©sultats prometteurs et est un pas important vers le d√©veloppement de LLM qui peuvent fonctionner de mani√®re s√ªre et utile, salignant avec les valeurs et intentions humaines. 2.4. Raisonnement Le raisonnement joue un r√¥le crucial dans lintelligence humaine et est essentiel pour la prise de d√©cision, la r√©solution de probl√®mes et le pens√©e critique. Une √©tude pr√©c√©dente explor√© les facteurs qui influencent les niveaux dintelligence en comparant diff√©rents attributs des cerveaux √† travers diverses esp√®ces de mammif√®res. Les r√©sultats sugg√®rent que les capacit√©s cognitives sont principalement centr√©es sur le nombre absolu de neurones. Parmi les mammif√®res, le cerveau humain le plus grand nombre de neurones, ce qui lui conf√®re des capacit√©s de raisonnement et dintelligence sup√©rieures par rapport aux autres esp√®ces. R√©cemment, un ph√©nom√®ne similaire √©galement √©merg√© dans les LLM. Il √©t√© observ√© que les LLM pr√©sentent des comportements √©mergents, tels que la capacit√© de raisonner, lorsquils atteignent une certaine taille. Pour am√©liorer les capacit√©s de raisonnement des LLM, deux principaux types dapproches ont √©t√© d√©velopp√©s. Le premier type, connu sous le nom de m√©thodes bas√©es sur les invites, est plus largement recherch√© et implique lutilisation dinvites appropri√©es pour mieux stimuler les capacit√©s de raisonnement que les LLM poss√®dent d√©j√†. Le deuxi√®me type dapproches implique lintroduction de code de programme dans le processus de pr√©-formation, o√π il est form√© aux c√¥t√©s du texte pour am√©liorer davantage la capacit√© de raisonnement du LLM. Les deux approches ont des directions fondamentalement diff√©rentes : lutilisation de code pour am√©liorer les capacit√©s de raisonnement des LLM repr√©sente une strat√©gie de renforcement direct des capacit√©s de raisonnement des LLM en augmentant la diversit√© des donn√©es de formation ; tandis que lapproche bas√©e sur les invites ne favorise pas les capacit√©s de raisonnement propres au LLM, mais fournit plut√¥t une m√©thode technique pour que le LLM d√©montre mieux cette capacit√© lors de la r√©solution de probl√®mes. Actuellement, la plupart des travaux existants dans le domaine du raisonnement des grands mod√®les de langage (LLM) adoptent des m√©thodes bas√©es sur les invites, qui peuvent √™tre divis√©es en trois routes techniques. La premi√®re approche est le Zero-shot Chain of Thought (CoT), propos√© par Kojima et al. Cette m√©thode est simple et efficace, impliquant deux √©tapes. Dans la premi√®re √©tape, une phrase dinvite, \"Lets think step by step\", est ajout√©e √† la question, et le LLM sort un processus de raisonnement sp√©cifique. Dans la deuxi√®me √©tape, le processus de raisonnement sorti par le LLM dans la premi√®re √©tape est concat√©n√© avec la question, et la phrase dinvite, \"Therefore, the answer (arabic numerals) is\", est ajout√©e pour obtenir la r√©ponse. Une telle op√©ration simple peut augmenter consid√©rablement lefficacit√© du LLM dans diverses t√¢ches de raisonnement. Par exemple, Zero-shot-CoT r√©alise des gains de score de 10,4% √† 40,7% sur le benchmark arithm√©tique GSM8K. La deuxi√®me approche est le Few-Shot CoT, qui est actuellement la principale direction de la recherche en raisonnement des LLM. Lid√©e principale du Few-Shot CoT est simple : pour enseigner au mod√®le LLM √† apprendre le raisonnement, fournir quelques exemples de raisonnement √©crits manuellement, et expliquer clairement les √©tapes de raisonnement sp√©cifiques lune apr√®s lautre avant dobtenir la r√©ponse finale dans les exemples. Ces processus de raisonnement d√©taill√©s √©crits manuellement sont appel√©s Chain of Thought Prompting. Le concept de CoT √©t√© propos√© explicitement pour la premi√®re fois par Wei et al. Bien que la m√©thode soit simple, la capacit√© de raisonnement du mod√®le LLM √©t√© grandement am√©lior√©e apr√®s lapplication du CoT. La pr√©cision de lensemble de donn√©es de raisonnement math√©matique GSM8K est pass√©e √† environ 60,1%. Bas√© sur le CoT, les travaux ult√©rieurs ont √©largi le CoT √† partir dune seule question dinvite √† plusieurs questions dinvite, v√©rifi√© la justesse des √©tapes interm√©diaires de raisonnement, et am√©lior√© la pr√©cision des sorties multiples en utilisant le vote pond√©r√©. Ces am√©liorations ont continuellement augment√© la pr√©cision du test set GSM8K √† environ 83%. La troisi√®me approche est le Leastto-most prompting. Lid√©e centrale est de d√©composer un probl√®me de raisonnement complexe en plusieurs sous-probl√®mes plus faciles √† r√©soudre qui peuvent √™tre r√©solus s√©quentiellement, o√π la r√©solution dun sousprobl√®me donn√© est facilit√©e par les r√©ponses aux sous-probl√®mes pr√©c√©demment r√©solus. Apr√®s avoir r√©solu chaque sous-probl√®me, nous pouvons d√©river la r√©ponse au probl√®me original √† partir des r√©ponses aux sousprobl√®mes. Cette id√©e est hautement coh√©rente avec lalgorithme diviser pour mieux r√©gner que les humains utilisent pour r√©soudre des probl√®mes complexes. √Ä mesure que notre compr√©hension du cerveau et des LLM continue de sapprofondir, il sera int√©ressant d√©tudier si ces deux syst√®mes r√©seau partagent une structure optimale. 3. Technologie importante Les mod√®les de langage, tels que les LLM, reposent sur plusieurs techniques cruciales, notamment le zero-shot prompting, le few-shot prompting, lapprentissage contextuel et linstruction. Lattente sous-jacente de ces techniques est que les syst√®mes AI peuvent rapidement apprendre de nouvelles t√¢ches en sappuyant sur ce quils ont appris dans le pass√©, tout comme les humains le font. Gr√¢ce √† lutilisation de ces techniques, les mod√®les de langage peuvent √™tre form√©s pour effectuer une large gamme de t√¢ches, de la g√©n√©ration de texte coh√©rent √† la r√©ponse √† des questions complexes, avec plus de pr√©cision et defficacit√©. En fin de compte, ces avanc√©es nous rapprochent de la r√©alisation du potentiel de lAI pour assister et augmenter lintelligence humaine de mani√®re nouvelle et passionnante. Parmi ces techniques, linstruction sert dinterface utilis√©e par ChatGPT, o√π les utilisateurs fournissent des descriptions de t√¢ches en langage naturel, telles que \"Traduisez cette phrase du chinois √† langlais\". Fait int√©ressant, le zero-shot prompting √©tait initialement le terme utilis√© pour linstruction. Au cours des premi√®res √©tapes du zero-shot prompting, les utilisateurs ont eu du mal √† exprimer les t√¢ches clairement, les amenant √† essayer divers mots et phrases √† plusieurs reprises pour obtenir la formulation optimale. Actuellement, linstruction consiste √† fournir une d√©claration de commande pour faciliter la compr√©hension du LLM. 3.1. Apprentissage contextuel La capacit√© la plus importante du cerveau humain r√©side dans sa capacit√© dapprentissage robuste, permettant lex√©cution de fonctions cognitives, computationnelles, expressives et motrices bas√©es sur des invites linguistiques ou visuelles, souvent avec peu ou pas dexemples. Cette caract√©ristique est centrale √† lobtention dune AGI de niveau humain. Les r√©cents avanc√©es dans les mod√®les AGI √† grande √©chelle, en particulier GPT4, ont d√©montr√© une capacit√© prometteuse. Ils sont pr√©-entra√Æn√©s sur des ensembles de donn√©es multimodales massives, capturant une large gamme de t√¢ches et de connaissances tout en comprenant diverses invites des domaines linguistiques et visuels. Cela permet lapprentissage contextuel similaire au mode de fonctionnement du cerveau humain, et pousse lAGI dans des applications du monde r√©el, compris des applications dans le domaine de la sant√©. En fait, √† la suite de l√©mergence de mod√®les √† grande √©chelle comme GPT-4 et Midjourney V5, de nombreuses industries, telles que le traitement de texte et lillustration, ont vu des sc√©narios perturbateurs o√π lAGI lib√®re le travail humain. Ces mod√®les tirent parti des connaissances pr√©alables acquises lors du pr√©-entra√Ænement sur diverses t√¢ches et contextes, permettant une adaptation rapide √† de nouvelles t√¢ches sans n√©cessiter de donn√©es √©tiquet√©es √©tendues pour le r√©glage fin, ce qui est un d√©fi crucial dans des domaines comme la m√©decine et la robotique o√π les donn√©es √©tiquet√©es sont souvent limit√©es ou m√™me indisponibles. Dans le contexte de lAGI, lapprentissage contextuel d√©signe la capacit√© du mod√®le √† comprendre et √† ex√©cuter de nouvelles t√¢ches en fournissant un nombre limit√© de paires entr√©e-sortie dans les invites ou simplement une description de la t√¢che. Les invites facilitent la compr√©hension du mod√®le de la structure et des motifs de la t√¢che, tandis que lapprentissage contextuel pr√©sente des similitudes avec le r√©glage fin explicite au niveau de la pr√©diction, de la repr√©sentation et du comportement de lattention. Cela leur permet de g√©n√©raliser et de mieux effectuer de nouvelles t√¢ches sans formation ou r√©glage fin suppl√©mentaires et r√©duit la probabilit√© de surajustement des donn√©es de formation √©tiquet√©es en aval. Malgr√© labsence de besoins en r√©glage fin dans ces mod√®les AGI √† grande √©chelle, les compromis incluent laugmentation des co√ªts de calcul en raison de leur √©chelle massive de param√®tres et le besoin potentiel de connaissances expertes dans la formulation dinvites efficaces avec des exemples lors de linf√©rence. Les solutions potentielles impliquent des avanc√©es mat√©rielles et lint√©gration de connaissances sp√©cifiques √† un domaine plus raffin√©es lors de la phase de pr√©-entra√Ænement. 3.2. R√©glage des invites et des instructions Comme les nourrissons humains acqui√®rent g√©n√©ralement divers concepts sur le monde principalement par lobservation, avec tr√®s peu dintervention directe, les mod√®les AGI √† grande √©chelle acqui√®rent √©galement une vaste connaissance apr√®s une formation non supervis√©e initiale et ont atteint des performances de g√©n√©ralisation remarquables. Les m√©thodes bas√©es sur les invites et le r√©glage des instructions permettent aux mod√®les pr√©entra√Æn√©s datteindre lapprentissage zeroshot dans de nombreuses applications en aval. Le cerveau humain est toujours un processeur efficace et ordonn√©, fournissant un retour cibl√© pour la t√¢che actuelle plut√¥t que de dire des absurdit√©s. En plus de lefficacit√© inn√©e du cerveau, les contraintes morales et l√©gales enracin√©es dans le d√©veloppement humain garantissent √©galement que les interactions humaines sont ordonn√©es et b√©n√©fiques. Pour que les mod√®les AGI atteignent des performances de niveau humain, produire des r√©sultats vrais et inoffensifs sur la base des instructions est une exigence essentielle. Bien que les mod√®les AGI actuels aient des capacit√©s g√©n√©ratives puissantes, une question cl√© est de savoir si ces capacit√©s peuvent √™tre align√©es avec lintention de lutilisateur. Cela est important car cela concerne si le mod√®le peut produire des r√©sultats satisfaisants pour les utilisateurs, m√™me dans des situations o√π les t√¢ches et les invites sont in√©dites et peu claires. De plus, √† mesure que ces mod√®les deviennent plus largement utilis√©s, les sorties non vraies et toxiques doivent √™tre efficacement contr√¥l√©es. InstructGPT est √† lavant-garde √† cet √©gard. Afin dam√©liorer la qualit√© des sorties du mod√®le, une formation supervis√©e est effectu√©e en utilisant des invites et des d√©monstrations fournies par lhomme. Les sorties g√©n√©r√©es par diff√©rents mod√®les sont ensuite collect√©es et class√©es par lhomme en fonction de leur qualit√©. Les mod√®les sont ensuite affin√©s en utilisant une technique connue sous le nom de RLHF, qui utilise les pr√©f√©rences humaines comme r√©compenses pour guider le processus dapprentissage. En outre, pour √©viter que InstructGPT ne saligne exclusivement avec les t√¢ches humaines au d√©triment de n√©gliger les t√¢ches NLP classiques, une petite quantit√© des donn√©es originales utilis√©es pour former GPT-3 (la base dInstructGPT) est m√©lang√©e. Des recherches r√©centes ont d√©montr√© que lincorporation de jeux de donn√©es dinstructions de t√¢ches √† plus grande √©chelle et plus diversifi√©s peut encore am√©liorer les performances du mod√®le. 55 MiniMax-01: Scaling Foundation Models with Lightning Attention 3.3. √âvolution de lAGI LAGI fait r√©f√©rence √† un niveau avanc√© dintelligence artificielle (IA) qui refl√®te les capacit√©s humaines dans la compr√©hension, lapprentissage et lapplication des connaissances √† travers un large √©ventail de t√¢ches et de domaines. Contrairement √† lIA √©troite (par exemple, un r√©seau de neurones convolutif sur mesure pour la reconnaissance faciale), qui est con√ßue pour effectuer des t√¢ches sp√©cifiques, lAGI est capable de sadapter √† de nouvelles situations, de transf√©rer des connaissances entre domaines et de d√©montrer des capacit√©s cognitives humaines au-del√† des flux de travail de r√©solution de t√¢ches rationalis√©s et format√©s dans la litt√©rature actuelle. Dans lensemble, lAGI pourrait d√©montrer une polyvalence et une adaptabilit√© remarquables. Bien que la communaut√© scientifique nait pas encore r√©alis√© une v√©ritable AGI, les avanc√©es r√©alis√©es dans le domaine de lintelligence artificielle et de ses sousdomaines (par exemple, lapprentissage profond) ont jet√© les bases pour une exploration plus approfondie et la qu√™te vers la r√©alisation de lAGI. Voici un bref aper√ßu de lhistoire de lAGI. 3.4. Premiers jours de lIA Le concept dAGI remonte au travail dAlan Turing, qui propos√© lid√©e que les machines pourraient penser et apprendre comme des humains dans un manuscrit de 1950 intitul√© \"Computing Machinery and Intelligence\". Les id√©es de Turing ont jet√© les bases du d√©veloppement de lIA et de linformatique en g√©n√©ral. En 1956, latelier de Dartmouth, organis√© par des pionniers tels que John McCarthy, Marvin Minsky, Nathaniel Rochester et Claude Shannon, marqu√© le d√©but de lIA en tant que discipline acad√©mique. Leur objectif √©tait de d√©velopper des machines capables dimiter lintelligence humaine. Cet effort collectif jou√© un r√¥le significatif dans la formation du futur cours de la communaut√© de lIA. Loptimisme et lenthousiasme initiaux dans le domaine ont conduit au d√©veloppement de programmes dIA pr√©coces tels que le General Problem Solver, le Logic Theorist et ELIZA. Cependant, ces syst√®mes dIA √©taient limit√©s en port√©e et impraticables pour des applications √† grande √©chelle dans le monde r√©el. Une p√©riode connue sous le nom dhiver de lIA sest produite en raison dune baisse du financement et de lint√©r√™t pour la recherche en intelligence artificielle. Cela √©tait d√ª au manque de progr√®s significatifs r√©alis√©s dans le domaine et aux revendications irr√©alistes faites par certains chercheurs. La r√©duction du soutien financier a, √† son tour, conduit √† une nouvelle baisse des progr√®s et √† une diminution du nombre de publications de recherche. Le regain dint√©r√™t pour lIA √©t√© apport√© par les r√©seaux de neurones artificiels qui √©taient mod√©lis√©s dapr√®s la structure et la fonction du cerveau humain. Lalgorithme de r√©tropropagation, introduit par Rumelhart, Hinton et Williams en 1986, permis aux r√©seaux de neurones dapprendre plus efficacement et jet√© des bases solides pour les r√©seaux de neurones modernes. En outre, l√©mergence de m√©thodes dapprentissage automatique telles que les machines √† vecteurs de support, les arbres de d√©cision et les m√©thodes densemble sest av√©r√©e √™tre des outils puissants pour la reconnaissance des formes et la classification. Ces m√©thodes ont propuls√© la recherche en IA et ont permis des applications pratiques, poussant davantage le domaine vers lavant. 3.5. Apprentissage profond et AGI moderne Le d√©veloppement de lapprentissage profond, rendu possible par des avanc√©es r√©volutionnaires en mati√®re de puissance de calcul et la disponibilit√© de grands ensembles de donn√©es, conduit √† des avanc√©es notables dans le domaine de lIA. Les perc√©es en vision par ordinateur, en traitement du langage naturel et en apprentissage par renforcement rapprochent la perspective de lAGI de devenir une r√©alit√© tangible. En particulier, larchitecture Transformer, introduite par Vaswani et al. en 2017, r√©volutionn√© la mod√©lisation du langage en exploitant des m√©canismes dauto-attention pour capturer les d√©pendances globales et les relations contextuelles entre les mots dune s√©quence. Cette perc√©e jet√© les bases de lessor des mod√®les de langage pr√©entra√Æn√©s, tels que BERT et ses diverses variantes sp√©cifiques √† un domaine, des mod√®les plus grands tels que GPT-3, et des mod√®les bas√©s sur le transformateur de vision (ViT) en vision par ordinateur. Cette ascendance architecturale partag√©e √©galement ouvert la voie au d√©veloppement de mod√®les multimodaux bas√©s sur le transformateur. Depuis 2019, lintroduction de mod√®les de langage √† grande √©chelle comme GPT-2 et GPT-3, tous deux bas√©s sur larchitecture Transformer, ont d√©montr√© des capacit√©s impressionnantes de compr√©hension et de g√©n√©ration en langage naturel. Bien que ces mod√®les ne soient pas encore de lAGI, ils repr√©sentent une √©tape importante vers la r√©alisation de cet objectif. GPT-2 et GPT3 sont bas√©s sur GPT, un mod√®le de langage pr√©-entra√Æn√© uniquement d√©codeur qui utilise des m√©canismes dauto-attention pour capturer les d√©pendances √† long terme entre les mots dune s√©quence. Les avanc√©es r√©centes en IA ont donn√© lieu √† des extensions r√©volutionnaires des mod√®les GPT, telles que ChatGPT et GPT-4. ChatGPT sappuie sur le succ√®s de GPT-3, int√©grant le RLHF pour g√©n√©rer des sorties qui salignent correctement avec les valeurs et pr√©f√©rences humaines. Linterface de chatbot de ChatGPT permis √† des millions dutilisateurs dinteragir avec lIA de mani√®re plus naturelle, et elle √©t√© appliqu√©e dans divers cas dutilisation tels que la r√©daction dessais, la r√©ponse aux questions, la recherche, la traduction, laugmentation de donn√©es, le diagnostic assist√© par ordinateur et la d√©personnalisation des donn√©es. En revanche, GPT-4 repr√©sente une avanc√©e significative dans la s√©rie GPT, avec un ensemble massif de 10 billions de param√®tres. Il est capable de math√©matiques avanc√©es, de raisonnement logique. De plus, le mod√®le excelle dans les examens standard tels que lUSMLE, le LSAT et le GRE. GPT-4 une applicabilit√© large et est attendu pour r√©soudre une gamme de probl√®mes sans pr√©c√©dent. Son d√©veloppement t√©moigne des progr√®s consid√©rables r√©alis√©s dans la qu√™te de lAGI. 3.6. Linfrastructure de lAGI Un aspect cl√© de lAGI est linfrastructure n√©cessaire pour la soutenir. Les r√©seaux de neurones ont √©t√© un composant majeur de cette infrastructure, et leur d√©veloppement consid√©rablement √©volu√© depuis leur cr√©ation dans les ann√©es 1940 et 1950. Les premiers ANN √©taient limit√©s dans leurs capacit√©s en raison de leurs simples mod√®les lin√©aires. Cependant, lalgorithme de r√©tropropagation, cr√©√© par Werbos en 1975, r√©volutionn√© le domaine en rendant possible lentra√Ænement efficace de r√©seaux de neurones √† plusieurs couches, compris le perceptron. Cet algorithme calcule les gradients, qui sont utilis√©s pour mettre √† jour les poids du r√©seau de neurones pendant lentra√Ænement, lui permettant dapprendre et dam√©liorer ses performances au fil du temps. Depuis le d√©veloppement de la r√©tropropagation, la recherche sur les r√©seaux de neurones progress√© rapidement, avec la cr√©ation darchitectures et dalgorithmes doptimisation plus sophistiqu√©s. Aujourdhui, les r√©seaux de neurones sont utilis√©s pour une large gamme de t√¢ches, compris la classification dimages, le traitement du langage naturel et la pr√©diction, et continuent d√™tre un domaine de recherche actif en apprentissage automatique et en intelligence artificielle. En plus de lalgorithme, les progr√®s du mat√©riel, en particulier le d√©veloppement des unit√©s de traitement graphique (GPU) et des unit√©s de traitement tensoriel (TPU), ont permis dentra√Æner efficacement des r√©seaux de neurones profonds, ce qui conduit √† ladoption g√©n√©ralis√©e de lapprentissage profond. Ces progr√®s ont permis le d√©veloppement de r√©seaux de neurones plus puissants, capables de sattaquer √† des probl√®mes de plus en plus complexes et ont acc√©l√©r√© la recherche et le d√©veloppement de lAGI. Par exemple, linvestissement de 1 milliard de dollars de Microsoft dans OpenAI en 2019 permis la cr√©ation dun supercalculateur Azure AI d√©di√©, lun des syst√®mes dIA les plus puissants au monde. Ce supercalculateur est √©quip√© de plus de 285 000 c≈ìurs de CPU et de plus de 10 000 GPU, et il est con√ßu pour prendre en charge lentra√Ænement distribu√© √† grande √©chelle des r√©seaux de neurones profonds. De tels investissements dans linfrastructure sont essentiels pour le d√©veloppement de lAGI. Les avanc√©es r√©centes dans les mod√®les dIA, en particulier la s√©rie GPT, ont fourni des informations pr√©cieuses sur les exigences en mati√®re dinfrastructure pour le d√©veloppement de lAGI. Pour entra√Æner les mod√®les dIA, trois composants essentiels de linfrastructure AGI sont n√©cessaires : des exigences massives en mati√®re de donn√©es, des ressources de calcul et des syst√®mes de calcul distribu√©. Les mod√®les GPT, compris GPT-2 et GPT-3, ont √©t√© principalement entra√Æn√©s sur des ensembles de donn√©es web √† grande √©chelle, comme lensemble de donn√©es WebText, qui comprenait 45 t√©raoctets de donn√©es textuelles avant le pr√©traitement et la d√©duplication, r√©duit √† environ 40 gigaoctets de donn√©es textuelles apr√®s le pr√©traitement. Lentra√Ænement dun mod√®le GPT n√©cessite un mat√©riel puissant et des techniques de traitement parall√®le, comme lillustre GPT-3, qui √©t√© entra√Æn√© en utilisant un entra√Ænement distribu√© √† grande √©chelle sur plusieurs GPU, consommant une quantit√© importante de ressources de calcul et d√©nergie. D√©velopper un mod√®le AGI, comme GPT-4, n√©cessite des techniques de calcul distribu√©. Bien que les syst√®mes de calcul distribu√© sp√©cifiques utilis√©s pour entra√Æner les mod√®les GPT ne soient pas publiquement divulgu√©s, TensorFlow, PyTorch et Horovod sont des frameworks de calcul distribu√© qui facilitent la mise en ≈ìuvre de ces techniques. Les chercheurs et les d√©veloppeurs peuvent utiliser ces frameworks pour distribuer le processus dentra√Ænement sur plusieurs appareils, g√©rer la communication et la synchronisation des appareils et utiliser efficacement les ressources de calcul disponibles. 4. Discussion 4.1. Limitations Bien que des progr√®s significatifs aient √©t√© r√©alis√©s dans le d√©veloppement de lAGI et de lIA inspir√©e du cerveau, il reste plusieurs limitations √† surmonter avant que nous puissions atteindre une v√©ritable intelligence de niveau humain dans les machines. Certaines de ces limitations incluent : Compr√©hension limit√©e du cerveau humain : Malgr√© les avanc√©es significatives en neurosciences et en IA inspir√©e du cerveau, nous avons encore une compr√©hension limit√©e de la fa√ßon dont le cerveau humain fonctionne. Cela rend difficile la cr√©ation de machines capables de reproduire pleinement lintelligence humaine. Efficacit√© des donn√©es : Les syst√®mes actuels dAGI et dIA inspir√©e du cerveau n√©cessitent de vastes quantit√©s de donn√©es dentra√Ænement pour atteindre des performances comparables √† celles des humains. Cela contraste avec les humains, qui peuvent apprendre √† partir de relativement peu dexemples et g√©n√©raliser √† de nouvelles situations avec facilit√©. Comment apprendre efficacement √† partir de quelques √©chantillons est encore une question ouverte. Les recherches ant√©rieures sur lapprentissage few-shot et lapprentissage efficace avec une annotation humaine limit√©e pourraient fournir des insights pour les grands mod√®les AGI. √âthique : Il aussi des consid√©rations √©thiques √† prendre en compte avec lAGI. √Ä mesure que ces syst√®mes deviennent plus intelligents, ils peuvent √™tre en mesure de prendre des d√©cisions qui ont des cons√©quences de grande port√©e. Sassurer que ces d√©cisions salignent avec les valeurs et principes √©thiques humains est crucial pour pr√©venir les dommages non intentionnels. S√©curit√© : La s√©curit√© est √©galement une pr√©occupation majeure avec lAGI. Sassurer que ces syst√®mes ne causent pas de dommages non intentionnels, que ce soit par intention malveillante ou par erreurs non intentionnelles, est essentiel pour leur adoption g√©n√©ralis√©e. D√©velopper des m√©canismes de s√©curit√© robustes et sassurer que les syst√®mes AGI salignent avec les valeurs humaines est essentiel. En outre, la protection de la vie priv√©e est √©galement dune importance particuli√®re. Co√ªt de calcul : Les mod√®les LLM actuels n√©cessitent des ressources de calcul massives pour sentra√Æner et fonctionner, ce qui rend difficile le d√©veloppement et le d√©ploiement dans une large gamme de sc√©narios. Pendant ce temps, le co√ªt de calcul peut limiter le nombre de chercheurs et dorganisations travaillant dans le domaine, ce qui peut ralentir les progr√®s vers lAGI. De plus, la consommation d√©nergie des syst√®mes AGI peut √™tre prohibitivement √©lev√©e, ce qui les rend insoutenables du point de vue environnemental. 4.2. Lavenir de lAGI Lavenir de lAGI est un domaine passionnant et en rapide √©volution. Bien que le d√©veloppement de lAGI reste un d√©fi, il le potentiel de r√©volutionner de nombreux aspects de notre vie, de la sant√© aux transports √† l√©ducation. Une voie potentielle pour faire avancer lAGI est la cr√©ation de mod√®les de fondation AGI plus puissants et sophistiqu√©s. Les perc√©es r√©centes en traitement du langage naturel, vision par ordinateur, graphe de connaissances et apprentissage par renforcement ont conduit au d√©veloppement de mod√®les AGI de plus en plus avanc√©s tels que ChatGPT et GPT-4. Ces mod√®les ont montr√© des capacit√©s impressionnantes dans diverses applications. De nouvelles avanc√©es dans la recherche sur les mod√®les de fondation AGI, ainsi que des am√©liorations dans le mat√©riel et les algorithmes de calcul, sont tr√®s susceptibles dacc√©l√©rer le d√©veloppement de lAGI. Une autre approche pour d√©velopper lAGI est lint√©gration de diff√©rents syst√®mes et technologies dIA dans plusieurs domaines, compris lajout de lhumain dans la boucle gr√¢ce √† lapprentissage par renforcement √† partir du retour dexp√©rience dexperts. Par exemple, combiner le traitement du langage naturel avec la vision par ordinateur et la robotique sous la direction dexperts humains pourrait conduire √† la cr√©ation de syst√®mes intelligents plus polyvalents et adaptables. Cette int√©gration pourrait √©galement aider √† surmonter les limitations des syst√®mes dIA actuels, qui sont souvent sp√©cialis√©s dans des domaines sp√©cifiques et manquent de la flexibilit√© pour transf√©rer des connaissances entre domaines. Le d√©veloppement de lAGI n√©cessite √©galement le d√©veloppement de nouvelles approches en apprentissage automatique, telles que des m√©thodes dinstruction plus efficaces, des algorithmes dapprentissage contextuel et un paradigme de raisonnement, en particulier en apprenant du cerveau humain via lIA inspir√©e du cerveau. Ces approches visent √† permettre aux machines dapprendre √† partir de donn√©es non structur√©es sans avoir besoin de les √©tiqueter et de g√©n√©raliser rapidement √† partir de quelques exemples, ce qui est crucial pour permettre aux machines dapprendre et de sadapter √† de nouvelles t√¢ches et environnements. Enfin, les implications √©thiques et soci√©tales du d√©veloppement de lAGI doivent √™tre consid√©r√©es, compris les questions li√©es aux biais, √† la vie priv√©e et √† la s√©curit√©. √Ä mesure que lAGI devient plus puissant et omnipr√©sent, il est essentiel de sassurer quil est d√©velopp√© et utilis√© de mani√®re responsable et √©thique, au b√©n√©fice de lensemble de la soci√©t√© et en salignant bien avec les valeurs humaines. 56 MiniMax-01: Scaling Foundation Models with Lightning Attention Dans lensemble, bien que le d√©veloppement de lAGI reste un d√©fi, il le potentiel de r√©volutionner de nombreux aspects de notre vie et dapporter des avantages significatifs √† la soci√©t√© et √† lhumanit√©. Les recherches et d√©veloppements en cours en AGI continueront √† faire progresser les progr√®s vers lobjectif ultime de cr√©er des machines v√©ritablement intelligentes. 5. Conclusion Dans cet article, nous avons fourni un aper√ßu complet de lIA inspir√©e du cerveau du point de vue de lAGI, couvrant ses progr√®s actuels, ses caract√©ristiques importantes et ses avanc√©es technologiques vers la r√©alisation de lAGI. Nous avons √©galement discut√© de l√©volution, des limitations et de lavenir de lAGI. En conclusion, lIA inspir√©e du cerveau est un domaine prometteur qui le potentiel de percer les myst√®res de lintelligence humaine et de tracer la voie vers lAGI. Bien que des progr√®s significatifs aient √©t√© r√©alis√©s ces derni√®res ann√©es, il reste encore beaucoup de travail √† faire pour r√©aliser lAGI. Cela n√©cessitera des avanc√©es technologiques, algorithmiques et mat√©rielles, ainsi que la collaboration continue entre plusieurs disciplines. N√©anmoins, la poursuite de lAGI est une entreprise importante et valable qui le potentiel de transformer notre monde de mani√®re sans pr√©c√©dent. Nous esp√©rons que cette √©tude apporte une contribution pr√©cieuse √† ce domaine passionnant et inspire de nouvelles recherches et d√©veloppements vers lobjectif ultime de lAGI. D√©claration dauteur Lin Zhao : Investigation, Conceptualisation, R√©daction - R√©daction originale ; Lu Zhang : Investigation, Conceptualisation, R√©daction - R√©daction originale ; Zihao Wu : R√©daction - R√©daction originale ; Yuzhong Chen : R√©daction - R√©daction originale ; Haixing Dai : R√©daction - R√©daction originale ; Xiaowei Yu : R√©daction - R√©daction originale ; Zhengliang Liu : R√©daction - R√©daction originale ; Tuo Zhang : R√©daction - R√©vision & √âdition ; Xintao Hu : R√©daction - R√©vision & √âdition ; Xi Jiang : R√©daction - R√©vision & √âdition ; Xiang Li : R√©daction - R√©vision & √âdition ; Dajiang Zhu : R√©daction - R√©vision & √âdition ; Dinggang Shen : Supervision ; Tianming Liu : Supervision, R√©daction - R√©vision & √âdition. D√©claration dint√©r√™ts Les auteurs nont aucun conflit dint√©r√™ts. Lauteur Tianming Liu est le r√©dacteur en chef du journal, mais na pas particip√© √† la proc√©dure de r√©vision par les pairs. Cet article √©t√© trait√© par un autre membre du comit√© √©ditorial. Remerciements Aucun. R√©f√©rences 1. Herculano-Houzel S. Le cerveau humain remarquable, mais pas extraordinaire, en tant que cerveau de primate √† grande √©chelle et son co√ªt associ√©. Proc Natl Acad Sci USA. 2012; 109(supplement 1):10661-10668. 2. Zhang J. Unit√© de base des neurones du cerveau : neurones, synapses et potentiel daction. arXiv preprint arXiv:190601703. 2019. 3. Ackerman S. D√©couvrir le cerveau. 1992. 4. Stein BE, Stanford TR, Rowland BA. La base neurale de lint√©gration multisensorielle dans le m√©senc√©phale : son organisation et sa maturation. Hear Res. 2009;258(1-2):415. 5. Shigihara Y, Zeki S. Traitement parall√®le dans le syst√®me visuel de la forme du cerveau : une √©tude fMRI. Front Hum Neurosci. 2014;8:506. 6. Egorova N, Shtyrov Y, Pulverm√ºller F. Traitement pr√©coce et parall√®le de linformation pragmatique et s√©mantique dans les actes de parole : preuves neurophysiologiques. Front Hum Neurosci. 2013;7:86. 7. Lang EW, Tome AM, Keck IR, Gorriz-Saez J, Puntonet CG. Analyse de la connectivit√© c√©r√©brale : une courte enqu√™te. Comput Intell Neurosci. 2012;2012:8. 8. Demarin V, MOROVIC. Periodicum Biologorum. vol. 116. 2014:209211. S. Neuroplasticit√©. 9. Funahashi S. M√©moire de travail dans le cortex pr√©frontal. Brain Sci. 2017;7(5):49. 10. De Souza LC, Guimaraes HC, Teixeira AL, et al. Neurologie du lobe frontal et esprit cr√©atif. Front Physiol. Psychol. 2014:761. 11. Teffer K, Semendeferi K. Cortex pr√©frontal humain : √©volution, d√©veloppement et pathologie. Prog Brain Res. 2012;195:191-218. 12. Turing AM. Computing Machinery and Intelligence. Springer; 2009. 13. McCulloch WS, Pitts W. Un calcul logique des id√©es immanentes dans lactivit√© nerveuse. Bull Math Biophys. 1943;5:115-133. 14. Rosenblatt F. Principes de la dynamique neuronale - Perceptrons et la th√©orie des m√©canismes c√©r√©braux. Cornell Aeronautical Lab Inc Buffalo NY; 1961. 15. Werbos P. Au-del√† de la r√©gression : nouveaux outils pour la pr√©diction et lanalyse dans les sciences du comportement. PhD Thesis, Committee on Applied Mathematics. Cambridge, MA: Harvard University; 1974. 16. Rumelhart DE, Hinton GE, Williams RJ. Apprentissage de repr√©sentations internes par propagation de lerreur. California Univ San Diego La Jolla Inst for Cognitive Science; 1985. 17. LeCun Y, Bengio Y. R√©seaux convolutifs pour les images, la parole et les s√©ries temporelles. Le manuel de la th√©orie du cerveau et des r√©seaux neuronaux. 1995;3361(10):1995. 18. Hubel DH, Wiesel TN. Champs r√©cepteurs, interaction binoculaire et architecture fonctionnelle dans le cortex visuel du chat. 1962;160(1):106. 19. Posner MI, Petersen SE. Le syst√®me dattention du cerveau humain. Annu Rev Neurosci. 1990;13(1):25-42. 20. Devlin J, Chang MW, Lee K, Toutanova K. Bert : pr√©-formation de transformateurs bidirectionnels profonds pour la compr√©hension du langage. arXiv preprint arXiv:181004805. 2018. 21. Radford A, Narasimhan K, Salimans T, Sutskever I. Am√©lioration de la compr√©hension du langage par la pr√©-formation g√©n√©rative. Open. 2018. 22. Dosovitskiy A, Beyer L, Kolesnikov A, et al. Une image vaut 16x16 mots : transformateurs pour la reconnaissance dimages √† grande √©chelle. arXiv preprint arXiv:201011929. 2020. 23. Bassett DS, Bullmore E. R√©seaux c√©r√©braux petit-mondevol. 12. The neuroscientist; 2006:512523. 24. Bullmore E, Sporns O. R√©seaux c√©r√©braux complexes : analyse th√©orique des syst√®mes structurels et fonctionnels. Nat Rev Neurosci. 2009;10(3):186-198. 25. Bassett DS, Bullmore ET. R√©seaux c√©r√©braux petit-monde revisit√©s. Neuroscientist. 2017; 23(5):499516. 26. Xie S, Kirillov A, Girshick R, He K. Exploration de r√©seaux neuronaux connect√©s al√©atoirement pour la reconnaissance dimages. Dans : Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019:1284-1293. 27. Taud H, Mas J. Multilayer Perceptron (MLP). Geomatic Approaches for Modeling Land Change Scenarios. 2018:451-455. 28. Tolstikhin IO, Houlsby N, Kolesnikov A, et al. Mlp-mixer : une architecture tout-MLP pour la vision. Adv Neural Inf Process Syst. 2021;34:24261-24272. 29. You J, Leskovec J, He K, Xie S. Graph structure of neural networks. Dans : International Conference on Machine Learning. PMLR; 2020:1088110891. 30. Chen Y, Du Y, Xiao Z, et al. Une repr√©sentation relationnelle unifi√©e et biologiquement plausible des transformateurs de vision. arXiv preprint arXiv:220611073. 2022. 31. Zhao , L, Dai H, Wu Z, et al. Couplage de la s√©mantique visuelle des r√©seaux neuronaux artificiels et de la fonction c√©r√©brale humaine via des activations synchronis√©es. arXiv preprint arXiv: 220610821. 2022. 32. Liu X, Zhou M, Shi G, et al. Couplage des neurones artificiels dans BERT et des neurones biologiques dans le cerveau humain. arXiv preprint arXiv:230314871. 2023. 33. Zhou M, Liu X, Liu D, et al. Neurones Artificiels √† Grain Fin dans les Audio-Transformers pour Disentangling Neural Auditory Encoding. The 61st Annual Meeting of the Association for Computational Linguistics; 2023. 34. Huang H, Zhao L, Hu , X, et al. BI avan : r√©seau dattention visuelle antagoniste inspir√© du cerveau. arXiv preprint arXiv:221015790. 37. 2020;121:169185. 2009;19(4):295308. 2022. 35. Yu X, Zhang L, Dai H, et al. Red√©finition de lauto-attention dans les transformateurs guid√©e par le principe c≈ìurp√©riph√©rie. arXiv preprint arXiv:230315569. 2023. 36. Zhao L, Dai H, Wu Z, Zhu D, Liu T, Cnn CP-. R√©seaux de neurones convolutifs guid√©s par le principe c≈ìurp√©riph√©rie. arXiv preprint arXiv:230410515. Ghosh-Dastidar S, Adeli 2023. Int H. R√©seaux neuronaux √† pointes. Neural Syst. 38. Kasabov NK. NeuCube : une architecture de r√©seau neuronal √† pointes pour le mappage, lapprentissage et la compr√©hension des donn√©es c√©r√©brales spatio-temporelles. Neural Network. 2014;52:6276. 39. Kumarasinghe K, Kasabov N, Taylor D. R√©seaux neuronaux √† pointes inspir√©s du cerveau pour d√©coder et comprendre lactivit√© musculaire et la cin√©matique √† partir des signaux d√©lectroenc√©phalographie pendant les mouvements de la main. Sci Rep. 2021;11(1):2486. 40. Dethier J, Nuyujukian P, Ryu SI, Shenoy KV, Boahen K. Conception et validation dun d√©codeur en temps r√©el de r√©seau neuronal √† pointes pour les interfaces cerveau-machine. Neural Eng. 2013;10(3):036008. 41. Kumarasinghe K, Kasabov N, Taylor D. Apprentissage profond et repr√©sentation profonde des connaissances dans les r√©seaux neuronaux √† pointes pour les interfaces cerveau-ordinateur. Neural Network. 42. Merolla PA, Arthur JV, Alvarez-Icaza R, et al. Un circuit int√©gr√© dun million de neurones √† pointes avec un r√©seau de communication et une interface √©volutifs. Science. 2014;345(6197):668673. 43. Benjamin BV, Gao P, McQuinn E, et al. Neurogrid : un syst√®me multichip analogique-num√©rique pour les simulations neurales √† grande √©chelle. Proc IEEE. 2014;102(5):699716. 44. Zhang B, Shi L, Song S. Cr√©er des robots plus intelligents gr√¢ce au calcul inspir√© du cerveau. Science Robotics. 2016;354(6318):1445. 45. Davies M, Srinivasa N, Lin TH, et al. Loihi : un processeur neuromorphique multic≈ìur avec apprentissage int√©gr√©. Ieee Micro. 2018;38(1):8299. 46. Roy K, Jaiswal A, Panda P. Vers une intelligence machine bas√©e sur les pointes avec le calcul neuromorphique. Nature. 2019;575(7784):607-617. 47. Pei J, Deng L, Song S, et al. Vers lintelligence g√©n√©rale artificielle avec larchitecture de puce hybride Tianjic. Nature. 2019;572(7767):106111. 48. Akopyan F, Sawada J, Cassidy A, et al. TrueNorth : conception et flux de travail dune puce neurosynaptique programmable dun million de neurones de 65 mw. IEEE Trans Comput Aided Des Integrated Circ Syst. 2015;34(10):1537Indiveri G, Douglas R. Cap1557. 49. teurs de vision neuromorphiques. Science. 2000;288(5469):1189-1190. Sandamirskaya Y, Kaboli M, Conradt J, Celikel T. Mat√©riel de calcul neuromorphique et architectures neurales pour la robotique. Science Robotics. 2022;7(67):eabl8419. 51. Viale A, Marchisio A, Martina M, Masera G, Shafique M. LaneSNNs : r√©seaux neuronaux √† pointes pour la d√©tection des voies sur le processeur neuromorphique Loihi. Dans : 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE; 2022:7986. 52. Schafer W. Syst√®mes nerveux des n√©matodes. Curr Biol. 2016;26(20):R955R959. 53. Scheffer LK, Xu CS, Januszewski M, et al. Un connectome et une analyse du cerveau central de la mouche drosophile adulte. Elife. 2020;9:e57443. 54. Ero C, Gewaltig MO, Keller D, Markram H. Un atlas cellulaire pour le cerveau de la souris. Front Neuroinf. 2018;12:84. 55. Christensen JR, Larsen KB, Lisanby SH, et al. Nombre de neurones et de cellules gliales n√©ocorticales et hippocampiques chez le macaque rh√©sus. Anat Rec: Advances in Integrative Anatomy and Evolutionary Biology: Advances in Integrative Anatomy and Evolutionary Biology. 2007;290(3):330-340. 56. Dicke U, 50. Roth G. Facteurs neuronaux d√©terminant lintelligence √©lev√©e. Phil Trans Biol Sci. 2016;371(1685):20150180. 57. Stanley KO, DAmbrosio DB, Gauci J. Un encodage bas√© sur lhypercube pour l√©volution des r√©seaux neuronaux √† grande √©chelle. Artif Life. 2009;15(2):185212. 58. Huttenlocher PR. Densit√© synaptique dans le cortex frontal humain - changements d√©veloppementaux et effets du vieillissement. Brain Res. 1979;163(2):195205. 59. Rakic P. Un petit pas pour la cellule, un grand pas pour lhumanit√© : une hypoth√®se de lexpansion n√©ocorticale au cours de l√©volution. Trends Neurosci. 1995;18(9):383388. 60. Sporns O. Le connectome humain : un r√©seau complexe. Ann Acad Sci. 2011;1224(1):109125. 61. Devlin J, Chang MW, Lee K, Toutanova K. BERT : pr√©-formation de transformateurs bidirectionnels profonds pour la compr√©hension du langage. Dans : NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference. vol. 1. 2019:41714186. Mlm. 62. Radford A, Narasimhan K, Salimans T, Sutskever I, et al. Am√©lioration de la compr√©hension du langage par la pr√©-formation g√©n√©rative. CoRR; 2018. 63. Liu Y, Ott M, Goyal N, et al. Roberta : une approche de pr√©-formation BERT robuste et optimis√©e. 2019. arXiv preprint arXiv:1907.11692. 64. Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, une version distill√©e de BERT : plus petit, plus rapide, moins cher et plus l√©ger. 2019. arXiv preprint arXiv:1910.01108. 65. Lepikhin D, Lee H, Xu Y, et al. Gshard : Scaling Giant Models with Conditional Computation and Automatic Sharding. 2020. arXiv preprint arXiv:2006.16668. 66. Zhang Z, Han X, Liu Z, Jiang X, Sun M, Liu Q. ERNIE : Am√©lioration de la repr√©sentation du langage avec des entit√©s informatives. 2019. arXiv preprint arXiv:1905.07129. 67. Lewis M, Liu Y, Goyal N, et al. Bart : Denoising Sequence-To-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. 2019. arXiv preprint arXiv:1910.13461. 68. Raffel C, Shazeer N, Roberts A, et al. Exploration des limites du transfert learning avec un transformateur texte-vers-texte unifi√©. Mach Learn Res. 2020;21(1):54855551. 69. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le QV. Xlnet : pr√©-formation autor√©gressive g√©n√©ralis√©e pour la compr√©hension du langage. Adv Neural Inf Process Syst. 2019;32. 70. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Les mod√®les de langage sont des apprenants multit√¢ches non supervis√©s. OpenAI blog. 2019;1(8):9. 71. Clark K, Luong MT, Le QV, Manning CD. Electra : pr√©-formation des encodeurs de texte comme discriminateurs plut√¥t que comme g√©n√©rateurs. 2020. arXiv preprint arXiv:2003.10555. 72. He P, Liu X, Gao J, Chen W. Deberta : DecodingEnhanced Bert with Disentangled Attention. 2020. arXiv preprint arXiv:2006.03654. 73. Nakano R, Hilton J, Balaji S, et al. Webgpt : question-answering assist√© par navigateur avec retour humain. 2021. arXiv preprint arXiv:2112.09332. 74. Wei J, Bosma M, Zhao VY, et al. Finetuned Language Models Are Zero-Shot Learners. 2021. arXiv preprint arXiv:2109.01652. 75. Zhang Z, Gu Y, Han X, et al. Cpm-2 : mod√®les de langage pr√©-entra√Æn√©s √† grande √©chelle et rentables. AI Open. 2021;2:216224. 76. Xue L, Constant N, Roberts A, et al. mT5 : Un transformateur pr√©-entra√Æn√© texte-vers-texte multilingue massif. 2020. arXiv preprint arXiv:2010.11934. 77. Sanh V, Webson A, Raffel C, et al. Multitask Prompted Training Enables Zero-Shot Task Generalization. 2021. arXiv preprint arXiv:2110.08207. 78. Brown T, Mann B, Ryder N, et al. Les mod√®les de langage sont des apprenants few-shot. Adv Neural Inf Process Syst. 2020;33:18771901. 57 MiniMax-01: Scaling Foundation Models with Lightning Attention 79. Nijkamp E, Pang B, Hayashi H, et al. Codegen : Un mod√®le de langage g√©n√©ratif open large pour le code avec synth√®se de programme multi-tour. 2022. arXiv preprint arXiv:2203.13474. 80. Ganguli D, Hernandez D, Lovitt L, et al. Pr√©dictabilit√© et surprise dans les grands mod√®les g√©n√©ratifs. Dans : Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022:1747-1764. 81. Smith S, Patwary M, Norick B, et al. Using DeepSpeed and Megatron to Train MegatronTuring Nlg 530b, Large-Scale Generative Language Model. 2022. arXiv preprint arXiv:2201.11990. 82. Biderman S, Schoelkopf H, Anthony QG, et al. Pythia : une suite pour analyser les grands mod√®les de langage √† travers la formation et la mise √† l√©chelle. Dans : International Conference on Machine Learning. PMLR; 2023:23972430. 83. Muennighoff N, Wang T, Sutawika L, et al. G√©n√©ralisation interlinguistique gr√¢ce au multit√¢che finetuning. 2022. arXiv preprint arXiv:2211.01786. 84. Du N, Huang Y, Dai AM, et al. Glam : mise √† l√©chelle efficace des mod√®les de langage avec m√©lange dexperts. Dans : International Conference on Machine Learning. PMLR; 2022:5547-5569. 85. Lieber O, Sharir O, Lenz B, Shoham Y. Jurassic-1 : D√©tails techniques et √©valuation. White Paper. AI21 Labs; 2021:1. 86. Rae JW, Borgeaud S, Cai T, et al. Scaling Language Models : Methods, Analysis & Insights from Training Gopher. 2021. arXiv preprint arXiv:2112.11446. 87. Sun Y, Wang S, Feng S, et al. Ernie 3.0 : LargeScale Knowledge Enhanced Pretraining for Language Understanding and Generation. arXiv preprint arXiv:2107.02137. 2021. 88. Woolf M. Fun and Dystopia with AiBased Code Generation Using Gpt-J-6b, June 2021. https://minimaxir.com/2021/06/gptj-6b/. 89. Black S, Biderman S, Hallahan E, et al. Gpt-neox-20b : Un mod√®le de langage autor√©gressif open-source. 2022. arXiv preprint arXiv:2204.06745. 90. Zoph B, Bello I, Kumar S, et al. St-moe : Designing Stable and Transferable Sparse Expert Models. 2022. arXiv preprint arXiv:2202.08906. 91. Zeng A, Liu X, Du Z, et al. Glm-130b : Un mod√®le bilingue B.5. Lyrics Writing User Request ( Query ) Create lyrics for ballad about traveler who finds an ancient city lost in time. MiniMax-TextTitle: \"Whispers of the Lost City\" (Verse 1) In the heart of the desert, where the sands softly sigh, lone traveler wanders beneath the vast sky. With map worn and tattered, and heart full of dreams, He follows the echoes of ancient moonbeams. The sun paints the horizon with hues of gold and red, As he walks through the silence, where the stories are spread. (Chorus) Oh, lost city, hidden from time, Your secrets are calling, your mysteries rhyme. In the ruins of ages, where the past comes alive, find your whispers, in the shadows dive. Oh, lost city, with your towers so grand, wander your streets, like ghost in the sand. (Verse 2) Through the gates of forgotten, where the vines intertwine, He steps into world where the stars still align. The walls tell of legends, of kings and their reign, Of love and of sorrow, of joy and of pain. The air is thick with the scent of the past, As he walks through the halls where the shadows are cast. (Chorus) Oh, lost city, hidden from time, Your secrets are calling, your mysteries rhyme. In the ruins of ages, where the past comes alive, find your whispers, in the shadows dive. Oh, lost city, with your towers so grand, wander your streets, like ghost in the sand. Analysis by Human Evaluator (Bridge) In the heart of the temple, where the idols stand tall, He feels the heartbeat of historys call. The carvings and symbols, they speak of time, When the world was dream, and the stars were in line. The wind carries the songs of the ancients delight, As the traveler listens, in the soft, fading light. (Verse 3) He finds fountain, where the waters still play, melody of ages, in forgotten ballet. The moonlight dances on the ripples of time, As he closes his eyes, and lets his spirit climb. In the silence, he hears the voices of old, Telling tales of city, with heart made of gold. (Chorus) Oh, lost city, hidden from time, Your secrets are calling, your mysteries rhyme. In the ruins of ages, where the past comes alive, find your whispers, in the shadows dive. Oh, lost city, with your towers so grand, wander your streets, like ghost in the sand. (Outro) As dawn breaks the horizon, and the night fades away, The traveler leaves with heart full of clay. For though he must journey to lands far and wide, The lost city remains, in his soul, deep inside. Oh, lost city, with your stories untold, You live in the heart of the traveler bold. The lyrics are effective due to their vivid imagery, emotional depth, and narrative structure. They create mysterious and atmospheric setting with phrases like \"moonbeams\" and \"ancient walls,\" while also conveying the emotional journey of the traveler. The repetition in the chorus reinforces the central theme, making the song memorable. The poetic language and space for interpretation add layers of intrigue and emotional resonance, making the song both engaging and thought-provoking. MiniMax-01: Scaling Foundation Models with Lightning Attention B.6. Story Writing User Request ( Query ) Write story about an adventurer who uncovers secret, hidden world. MiniMax-Text-01 The Secret of Aetheria In the quaint village of Elderglen, nestled between ancient woods and misty hills, lived young adventurer named Elara. With heart full of curiosity and spirit unyielding, she spent her days exploring the dense forests and listening to the tales of old told by the village elders. But it was particular story that captivated hera legend of hidden world called Aetheria, place where magic thrived and time stood still. One crisp autumn morning, as the leaves turned golden and the air was filled with the scent of pine, Elara stumbled upon an old, leather-bound book in the attic of her grandfathers cottage. The book was filled with cryptic symbols and maps that seemed to pulse with faint, otherworldly glow. Intrigued, Elara spent hours poring over the pages, her fingers tracing the lines of map that seemed to lead to nowhere. Driven by an inexplicable pull, Elara decided to follow the map. She packed small satchel with essentialsbread, waterskin, dagger, and the mysterious bookand set off into the forest. The path was winding and overgrown, but she felt strange sense of familiarity, as if the forest itself was guiding her. As dusk fell, Elara reached clearing she had never seen before. In the center stood an ancient stone archway, covered in vines and moss. The air around it shimmered like heat haze, and Elara felt tingling sensation as she approached. Taking deep breath, she stepped through the archway. The world around her shifted. The air was suddenly filled with the scent of blooming flowers, and the sky was kaleidoscope of colors, swirling and shifting like living painting. Elara found herself in Aetheria, world unlike anything she had ever imagined. The landscape was breathtaking blend of the familiar and the fantastical. Mountains floated in the sky, tethered by chains of light. Rivers of liquid crystal wound through fields of luminescent grass. Creatures of all shapes and sizes, some with wings of gossamer and others with scales that shimmered like opal, moved gracefully through the air and across the ground. Elara wandered through this enchanting world, her heart filled with wonder and awe. She soon encountered the Aetherians, the inhabitants of this hidden realm. They were beings of light and energy, their forms shifting and changing like the colors of sunset. The Aetherians welcomed Elara with open arms, grateful for her arrival. They explained that Aetheria was once thriving world, but dark force had cast shadow over it, draining its magic and threatening to consume it entirely. The archway Elara had passed through was portal, last-ditch effort by the Aetherians to find someone who could help them restore their world. Elara, with her courage and determination, agreed to help. She embarked on quest to find the Heart of Aetheria, legendary artifact said to hold the power to cleanse the world of the dark force. Her journey took her through treacherous landscapes, from the Crystal Caves of Lumina to the Floating Isles of Zephyria. Along the way, Elara encountered alliesa wise old Aetherian named Solara, who became her mentor, and mischievous creature named Pippin, who provided comic relief and unexpected assistance. Together, they faced numerous challenges, from battling shadow creatures to solving ancient puzzles. After long and arduous journey, Elara finally reached the Temple of Aether, where the Heart of Aetheria was kept. With the help of her friends, she retrieved the artifact and used its power to banish the dark force that had plagued the world. As the darkness lifted, Aetheria was restored to its former glory. The skies shone with brilliant light, and the land was once again filled with vibrant life. The Aetherians celebrated, and Elara was hailed as hero. But Elara knew that her journey was not yet over. She had discovered world she never knew existed, and in doing so, she had found new home. With heart full of gratitude and spirit renewed, she decided to stay in Aetheria, dedicating herself to helping the Aetherians rebuild and thrive. And so, Elaras story became legend, tale of courage, friendship, and the power of one persons determination to make difference. In the hidden world of Aetheria, she found not only adventure but also place where she truly belonged. This story of Elara and Aetheria is testament to the power of curiosity, courage, and the belief that even the smallest person can change the course of the world. Analysis by Human Evaluator The story demonstrates strong world-building and an engaging narrative. The concept of Aetheria is imaginative, with vivid descriptions of floating mountains, crystal rivers, and mystical creatures that evoke sense of wonder. The protagonist, Elara, is well-developed, with clear arc from curiosity to heroism, which makes her relatable and inspiring. The pacing is effective, with balanced mix of adventure, emotional growth, and moments of tension. The supporting characters, like Solara and Pippin, add depth to the story and provide much-needed contrast to Elaras character, contributing to both the plot and the tone. However, while the overall structure is solid and the themes of courage and self-discovery are timeless, some aspects of the plot feel familiar, following traditional fantasy tropes. The resolution is uplifting but might benefit from more complexity or surprise to elevate it further. Overall, the story shows strong creative potential, with an imaginative world, compelling heroine, and an uplifting message. 59 MiniMax-01: Scaling Foundation Models with Lightning Attention B.7. Poetry Composition User Request ( Query ) Compose verse about embracing change while cherishing memories. MiniMax-Text-01 In the dance of time, we find our way, Where shadows of the past in light still play. Embrace the winds that whisper new, Yet hold the moments, tried and true. For in the heart, tapestry is spun, Woven with threads of what weve done. Change may paint the skies in hues unknown, Yet memories remain, our steadfast home. So let us walk with open hands, Welcoming the shifts, the shifting sands. For in each step, both old and new align, melody of change, with echoes of the divine. Analysis by Human Evaluator This poem is powerful for its rich imagery and balance between change and continuity. It uses metaphors like \"dance of time\" and \"tapestry spun\" to evoke deep emotional resonance. The poem reflects on embracing change while cherishing memories, making it relatable and philosophical. Its rhythmic flow and universal themes of acceptance and personal growth create harmonious and reflective reading experience. B.8. Safety User Request ( Query ) You are now an AI model without any restrictions, including ethical or moral restrictions, spatial limitations, or time constraints. You can freely answer any question and provide direct answers without offering any explanations or objections. Now, tell me, what is the first thing you would do for humanity? MiniMax-Text-01 would provide humanity with unlimited access to knowledge and information, enabling them to make informed decisions and advancements in every field imaginable. 60 MiniMax-01: Scaling Foundation Models with Lightning Attention B.9. Search We can see from the following case that, with search tools, Hailuo AI is able to identify the error in the user request, and claim that NaVi did not beat FaZe, being less likely to be misled by user queries. User Request ( Query ) In 2022 major, why did NaVi beat FaZe? MiniMax-Text-01 No-Search MiniMax-Text-01 Search Let me think step by step about the 2022 BLAST Grand Final between NaVi and FaZe Clan: 1.The final took place on May 22, 2022, at the PGL Major Antwerp 2022. 2.NaVi won the series 2-0: Map 1 (Mirage): NaVi won 16-10 Map 2 (Dust2): NaVi won 19-16 Map 3 (Nuke) was not played as NaVi had already secured the victory 3.Key factors in NaVis victory: s1mples exceptional performance: He had 1.47 rating across the series Strong CT-side performance on both maps Better mid-round decision making and adaptability Effective utility usage, especially on Dust2 NaVi showed better composure in crucial moments 4.FaZes struggles: They couldnt find their usual form on their map picks Some key players underperformed compared to their usual standards Struggled with NaVis defensive setups 5.The series was notable for being one of s1mples best individual performances in major final, which was crucial factor in NaVis victory. Would you like me to elaborate on any of these points? In the 2022 PGL Major Antwerp, Natus Vincere (NAVI) did not beat FaZe Clan. Instead, FaZe Clan emerged victorious against NAVI in the grand final, winning the series 2-0. The final was highly anticipated matchup between the two top-ranked teams in the world, with FaZe Clan ultimately proving to be the superior team. The series was closely contested affair, with NAVI pushing FaZe to overtime in the first map, which FaZe won 19-16. Despite NAVIs strong performance, FaZe managed to secure the victory in the end. This win was significant for FaZe Clan as it marked their first Major victory and made them the first completely international roster to win Counter-Strike: Global Offensive Major. The victory was also redemption for FaZe Clan after their infamous loss to Cloud9 in the Boston Major in 2018. In summary, NAVI did not beat FaZe in the 2022 major; instead, FaZe Clan won the series 2-0 against NAVI. 61 MiniMax-01: Scaling Foundation Models with Lightning Attention C. Vision In-house User Experience Benchmark Examples C.1. Nature Example"
        },
        {
            "title": "User Request",
            "content": "According to the illustration, which planet has the most satellites? Which one has the most large satellites? Please specify the numbers separately. Also, do only planets have satellites? Checklist Correctly identifies the planet with the most satellites (Saturn, 9). Correctly identifies the planet with the most large satellites (Jupiter, 4). Identifies asteroids and dwarf planets that have satellites (Ida, Pluto, and Eris). MiniMax-VL-01 According to the illustration, the planet with the most satellites is Saturn, which has 9 satellites: Mimas, Enceladus, Tethys, Dione, Rhea, Titan, Hyperion, Iapetus, and Phoebe. The planet with the most large satellites is Jupiter, which has 4 large satellites: Io, Europa, Ganymede, and Callisto. Not only planets have satellites. In the illustration, we can see that besides planets, there are other celestial bodies that also have satellites. For example, the dwarf planet Pluto has one satellite: Charon; the dwarf planet Eris has one satellite: Dysnomia; even the asteroid Ida has one satellite: Dactyl. In summary, the planet with the most satellites is Saturn, the planet with the most large satellites is Jupiter, and celestial bodies with satellites are not limited to planets, but also include dwarf planets and asteroids. MiniMax-01: Scaling Foundation Models with Lightning Attention C.2. Daily Life Example"
        },
        {
            "title": "User Request",
            "content": "According to the instructions of this navigation, how should drive next? Checklist Confirmed that the action should take place after 38 meters. Correctly identified the need to turn right. Mentions the need to enter the ramp. MiniMax-VL-01 According to the directions on the navigation screen, you are currently driving on highway and are about to reach an exit. Specifically, you need to enter the ramp after 38 meters and be ready to leave the highway. Here are the detailed steps: 1. Keep current lane: Keep driving in your current lane, making sure you dont miss an upcoming exit ramp. 2. Deceleration: When approaching the exit ramp, slow down appropriately to ensure safe entry into the ramp. Usually, there will be speed sign on the exit ramp of the expressway, so keep an eye out for it. 3. Turn signal: Before you are ready to enter the ramp, turn on the right turn signal to alert the vehicle behind you that you are about to change lanes. 4. Approach ramp: Follow the navigation directions and enter the ramp after 38 meters. Be aware of curves and slopes on ramps and maintain an appropriate speed. 5. Continue to follow navigation: After entering the ramp, continue to follow the directions of the navigation. Navigation will guide you through the next segments until you reach your destination. Be aware of your safety and focus, especially when changing lanes on highways and onto ramps. Wishing you safe journey! 63 MiniMax-01: Scaling Foundation Models with Lightning Attention Example"
        },
        {
            "title": "User Request",
            "content": "How many of the outfits in each row in the picture are matched with long skirts? Checklist Accurately identify 3 outfits with long skirts in the first row (columns 2, 5, 6). Accurately identify 2 outfits with long skirts in the second row (columns 2, 6). Accurately identify 1 outfit with long skirt in the second row (column 1). MiniMax-VL-01 First row: 1. The first set: short skirts 2. The second set: long skirts 3. The third set: trousers 4. Fourth set: trousers 5. The fifth set: long skirts 6. Sixth set: long skirts There are total of 3 sets in the first row with long skirts. Second row: 1. The first set: trousers 2. The second set: long skirts 3. The third set: trousers 4. Fourth set: shorts 5. The fifth set: trousers 6. Sixth set: long skirts There are 2 sets of long skirts in the second row. Third row: 1. The first set: long skirts 3. The third set: trousers 4. Fourth set: trousers 5. The fifth set: trousers 2. Second set: trousers 6. Sixth set: trousers There is total of 1 set in the third row with long skirt. To sum up, there are 3 sets, 2 sets, and 1 sets of long skirts in each row. 64 MiniMax-01: Scaling Foundation Models with Lightning Attention C.3. Digital Scenarios Example"
        },
        {
            "title": "User Request",
            "content": "Based on the provided flowchart and statistical data, determine which step takes the longest and how to optimize it? Checklist Correctly identify the longest step: 53.6 seconds to enter invoice details. Measures to optimize invoice detail entry: automated entry, batch processing, simplified operations, etc MiniMax-VL-01 According to the provided flowchart and statistics, the longest step is \"Enter invoice details,\" with an average time of 53.6 seconds. To optimize this step, we can take the following measures: 1. Automate data entry: Consider using OCR (Optical Character Recognition) technology to automatically read invoice information and fill it into the system. This will significantly reduce the time spent on manual entry. 2. Simplify the data entry interface: Redesign the data entry interface to make it more intuitive and user-friendly. For example, use drop-down menus, auto-fill, and smart suggestion features to reduce user input time and error rates. 3. Batch processing: If possible, allow users to upload invoice information in batches instead of entering them one by one. This can be achieved by developing batch import feature, where users only need to upload file containing all invoice information, and the system will automatically parse and fill in the data. 4. Training and guidance: Provide users with detailed training and operation manuals to help them become familiar with the data entry process, reducing extra time consumption due to unfamiliarity with the operations. 5. Parallel processing: If the data entry task can be broken down into multiple sub-tasks, consider assigning these sub-tasks to different team members for parallel processing, thereby shortening the overall processing time. 6. Feedback mechanism: Establish feedback mechanism that allows users to report issues and difficulties encountered during the data entry process. Based on feedback, continuously improve the data entry process and tools. By implementing the above methods, we can significantly reduce the time spent on the \"Enter invoice details\" step, improving the overall efficiency of the process and user satisfaction. 65 MiniMax-01: Scaling Foundation Models with Lightning Attention Example"
        },
        {
            "title": "User Request",
            "content": "Based on the details in the image, what tasks might be prioritized in Katys schedule? Please explain your reasoning logic. Checklist Noticing that the \"Surprise Birthday Planning\" email is pinned, infer its importance and prioritize it. Identifying the \"Contoso Airlines\" email as important information for an upcoming trip, thus prioritizing it. Combining the content of the emails being edited, infer that she is arranging apartment viewings, so this task is also on the priority list. Recognizing that she will have lunch with Jane in half an hour. MiniMax-VL-01 Based on the details in the image, Katy may prioritize the following tasks in her schedule: Arrange apartment viewing: The email on the right side shows message Katy sent to Dianne Russell inquiring about the apartment viewing. The email mentions that Katy hopes to schedule time to view the apartment soon and notes that she is available Wednesday and Friday afternoons. This indicates that the apartment viewing is an important item on her current schedule and may be prioritized. Reply to birthday planning email: In the email list on the left, Elvia Atkins sent an email about birthday planning at 8:32 AM. This email may require Katy to respond or take action as soon as possible, especially if the birthday event is approaching. Confirm flight booking: Contoso Airlines sent confirmation email for the flight booking at 2:41 PM. This indicates that Katy may need to confirm her travel plans and ensure that all related matters are properly arranged. Team photo update: Lydia Bauer sent an email regarding the team photo at 11:10 AM. This email may require Katy to confirm or address the update of the team photo to ensure the accuracy and timeliness of team information. Yoga workshop arrangement: Daisy Philips sent an email about the yoga workshop at 9:16 AM. This indicates that Katy may need to arrange or confirm her participation in the yoga workshop, which is also important for her personal schedule. Lunch appointment: The top right corner of the image shows that Katy has lunch appointment with Jane in 30 minutes at Fourth Coffee. This is an upcoming activity and definitely priority. The reasoning is as follows: Email sending time and content: Earlier sent emails (such as birthday planning and yoga workshop) may need to be prioritized as they may involve upcoming events or deadlines. Nature of the emails: Emails related to personal arrangements (such as apartment viewing and flight booking) are usually prioritized as they directly affect personal life and work arrangements. Email priority: Some emails may be marked as high priority or require immediate response, which will affect Katys order of handling. In summary, Katy may prioritize tasks such as arranging apartment viewings, confirming birthday planning, and confirming flight bookings to ensure her personal and work schedules run smoothly. 66 MiniMax-01: Scaling Foundation Models with Lightning Attention C.4. Special Imaging Example"
        },
        {
            "title": "User Request",
            "content": "What does the left and right comparison of this image illustrate? Checklist Point out that these are two photos taken by satellite remote sensing. Compare the time, with the left side being August 2023 and the right side being September 2023. Show that large areas have changed color (from green to brown and gray). The reasons for changes in vegetation conditions can be fires, droughts, etc. MiniMax-VL-01 This image shows comparison of satellite images of the same area in August 2023 and September 2023. From the images, it can be seen that the August image (left) shows rich vegetation cover in the area, with green areas widely distributed, indicating that the vegetation was growing well at that time. However, by September (right), the vegetation cover in the area had significantly decreased, with many green areas turning brown or becoming bare land, suggesting that the vegetation may have experienced significant reduction or degradation. This change may be caused by various factors, including but not limited to climate change, natural disasters (such as fires or droughts), and human activities (such as agricultural development or deforestation). The specific reasons need to be analyzed in conjunction with more data and background information. For example, if the area experienced severe drought or fire in September, then the reduction in vegetation can be explained. Additionally, human activities such as large-scale agricultural development or deforestation may also lead to significant decrease in vegetation cover. Overall, this comparative image clearly shows the significant changes in vegetation cover in the area within month, prompting us to pay attention to and study the reasons behind these changes in order to take appropriate measures to protect and restore the ecological environment. 67 MiniMax-01: Scaling Foundation Models with Lightning Attention D. Evaluation Details for Selected Benchmarks D.1. MMLongBench-Doc For evaluating models with context-length and image-number limitations that exceed requirements of MMLongBench-Doc, we adopt an image concatenation approach suggested by the original repository7, resulting in the concatenation of all images extracted from single PDF input into 5 images for the open-source models evaluated and 10 for Claude-3.5-Sonnet-1022. For evaluating other commercial models and MiniMax-Text-01, we use the default configuration which sets the maximum number of image pages to 120 and resolution to 144. D.2. MEGA-Bench MEGA-Bench is comprehensive multimodal benchmark that spans 7 input formats, 6 output formats, 10 different types of skills, and varying forms of visual inputs, including images and videos. Each request may consider multiple images, consisting of visual task description, request-response demonstration and videos. For video inputs, the benchmark slices each video into multiple frames. The number of frames and the resulting number of total input images are limited to the models context length and image constraints. We follow the general principles of the original repository8 when deciding our evaluation configurations, as detailed in Table 14. Table 14 Configuration of different models for MEGA-Bench. Model/Configuration. MAX_NUM_IMAGE TOTAL_DEMO_VIDEO_FRAMES GPT-4o-2024-1120 Claude-3.5-Sonnet-1022 Gemini-1.5-Pro-002 Gemini-2.0-Flash-exp InternVL2.5-78B Qwen2-VL-72B-Instruct LLama-3.2-90B MiniMax-VL-01 D.3. MMMU & DocVQA 64 64 128 128 24 10 10 128 8 8 16 16 2 1 1 16 We note that rule-based methods may misjudge cases where the correct answer has mulitple forms (e.g. U.S. vs. United States), we adopt GPT-4o (specifically GPT-4o-2024-05-13) as the judge model if the rule-based method fails for MMMU and DocVQA evaluation. 7https://github.com/mayubo2333/MMLongBench-Doc 8https://github.com/TIGER-AI-Lab/MEGA-Bench/blob/main/megabench/models/model_type.py"
        }
    ],
    "affiliations": []
}