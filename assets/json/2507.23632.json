{
    "paper_title": "On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective",
    "authors": [
        "Gabriel Mongaras",
        "Eric C. Larson"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 2 3 6 3 2 . 7 0 5 2 : r ON THE EXPRESSIVENESS OF SOFTMAX ATTENTION: RECURRENT NEURAL NETWORK PERSPECTIVE"
        },
        {
            "title": "A PREPRINT",
            "content": "Gabriel Mongaras Lyle School of Engineering Southern Methodist University Dallas, TX 75205 gabriel@mongaras.com Eric C. Larson Lyle School of Engineering Southern Methodist University Dallas, TX 75205 eclarson@smu.edu August 1,"
        },
        {
            "title": "ABSTRACT",
            "content": "Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts."
        },
        {
            "title": "Introduction and Background",
            "content": "The formulation of softmax attention was proposed by Bahdanau et al. [2015] as weighting mechanism for aligning recurrent neural networks (RNNs) in encoder-decoder architectures for language translation Sutskever et al. [2014]. However, the modern day usage of the attention in transformer architecture was first employed by Vaswani et al. [2017] as way to do sequence mixing within the context of language translation. This formulation used the softmax activation function to model token routing without the use of traditional recurrent network. Since its introduction, the attention mechanism has been widely adopted in various domains such as computer vision Dosovitskiy et al. [2021], generative models Esser et al. [2024], timeseries analysis Nie et al. [2023], audio processing Baevski et al. [2020], graphs Veliˇckovic et al. [2018], and many more applications. While softmax attention is powerful mechanism that is used in variety of areas, major drawback is its quadratic complexity with respect to the sequence length, . Linear attention swaps the softmax nonlinearity with decomposable kernel function, reducing the complexity from quadratic to linear with respect to the sequence length. The original introduction of linear attention Katharopoulos et al. [2020] replaced the nonlinearity with an elu(x) + 1 kernel. Since the original formulation, other methods have been developed that replace the nonlinearity on the QK-softmax inner product with various other functions such as ReLU Xie et al. [2025], cosine similarity Mongaras et al. [2025], cosine reweighting Qin et al. [2022], or by decomposing the QK-softmax inner product Wang et al. [2020], Zhuoran et al. 1Code found at: https://github.com/gmongaras/On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-NetworkPerspective On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT [2021]. Although these methods are linear in complexity, none are as performant as softmax attention in terms of downstream accuracy. In this work, we propose recurrent reformulation of softmax attention and use this to describe the elements of softmax attention that make it more performant compared to its linear and sparse counterparts. While softmax attention is the driving method used in many architectures, the question of why the discrepancy between softmax attention and linear attention exists remains unanswered. This work provides principled analysis of softmax attention by deriving recurrent formulation using its Taylor series expansion. Through this formulation, we experiment to reveal how each recurrent component contributes to downstream accuracy using targeted ablations. We demonstrate that softmax attention is not merely heuristic construction but structured process with interpretable, sequential dynamics. In doing so, we bridge the gap between the observed empirical performance for linearized attention (and its variants) and the theoretical underpinnings of softmax attention."
        },
        {
            "title": "2 Related Work",
            "content": "Expressiveness of Linear Attention Many approaches have attempted to make linear attention more expressive to match softmax attention, while still retaining linear complexity. Choromanski et al. [2021] used linear approximations of the softmax kernel to achieve more performant linear attention mechanisms. As linear attention can be interpreted as an RNN Katharopoulos et al. [2020], Peng et al. [2025] proposed receptance weighted key values (RWKV) attempting to enhance the expressivity of RNNs with receptance vector for time mixing. While this method helped to address computational complexity of RNN training, it still relied on linear attention in its formulation. Mamba Dao and Gu [2024] employed state space models Gu et al. [2022] to develop an efficient and expressive form of linear attention. Sun et al. [2025] formulated linear attention as step in gradient decent of hidden view. Another approach treats modeling the hidden state as step in gradient decent Sun et al. [2025] which can be viewed as type of linear attention. Behrouz et al. [2024] proposed Titans, which built upon this gradient formulation creating different variants of the hidden view gradient descent. ATLAS Behrouz et al. [2025] develops new kind of recurrent models, leveraging test-time compute as in Behrouz et al. [2024]. They include proof show that recurrent models using higher order hidden states are more expressive. Sieber et al. [2024] makes similar derivation as this work, but in the context of control systems, but did not apply this derivation to recurrent architecture. Nauen et al. [2024] makes simple derivation, but only explores second-order models. Although all of these methods are more expressive than linear attention, they are difficult to implement efficiently in hardware, akin to RNNs, and fall short to the accuracy of softmax attention. Softmax Attention Improvements Another direction of research explores how to improve softmax attention. number of works attempt to improve softmax attention by increasing sparsity to reduce dependence on context length such as Longformer Beltagy et al. [2020], BigBird Zaheer et al. [2020], and randomized feature attention Peng et al. [2021]. RoFormer Su et al. [2024] adds relative positional encodings to improve long sequence modeling and is used in most modern transformer models. Zhai* et al. [2023] prevents attention score entropy collapse by reparameterizing the weight matrices, modern transformers use norms to fix this issue. Newer improvements such as the Forgetting Transformer Lin et al. [2025] and DeepSeek Liu et al. [2024] make changes to the input of the attention mechanism such as combining the key and value matrices to reduce the KV cache size, or adding forget gate. Many of these improvements make softmax attention slightly less computational, less memory intensive, or more expressive. However, the core mechanism driving the modeling capability is still softmax attention. Why Softmax? There are several works that also attempt to address the question: What makes softmax attention so performant? This fundamental question has eluded researchers for some time. For example, Miller [2023] noticed softmax attention cannot zero out attention heads. More specifically, when limxiinf for all tokens, xi, we want the head output to produce zero, thus giving no weight to any tokens. In actuality, it produces uniform distribution over tokens instead. To fix this issue, Miller [2023] adds one to the denominator. This work emphasizes how softmax has subtle problems even though it works well in practice. Smith [2025] asks why attention works, building upon Miller [2023] to change the attention mechanism. However, the focus of this work is on the improvements to attention, rather than exploring exactly what makes attention work. Deng et al. [2023] examines the performance gap between linear and softmax attention, but only in classification and empirical context. Why linear attention is lacking compared to softmax attention, in general, remains not fully understood. Han et al. [2024] shows that linear attention is lacking an injective property and cannot model local features while softmax attention is injective and can model local features. Collins et al. [2024] uses Lipschitzness to explain in-context learning in softmax. These provide intuition but, ultimately, these properties do not fully explain the expressiveness gap. Katharopoulos et al. [2020] introduced formulation for linear attention using recurrent network components. In their work, they also mention that their formulation does not impose any constraint on the feature function and it can be 2 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT used for representing any transformer model, in theory even those using softmax attention. This inspired our current work to formulate softmax attention as RNN and use this formulation to describe what makes softmax attention expressive, in general. Moreover, we attempt to explain exactly why linear attention is not as expressive as softmax attention by showing linear attention approximates subset of elements that comprise softmax attention."
        },
        {
            "title": "3 Methodology",
            "content": "We first motivate our approach using the traditional formulation of causal softmax attention for calculating the output of the layer, Ot, at time t: Ot = Softmax(Qt K1:t) V1:t = (cid:80)t Vs s=1 eQtKT (cid:80)t n=1 eQtKT (1) We would like to rearrange this equation such that one can interpret the attention mechanism in recurrent form. However, the exponentiation in softmax couples Qt and Ks, preventing direct regrouping. For instance, in linear attention the attention function is replaced by ϕ(Qt) ψ(Ks)T . This decoupling allows causal linear attention to be reformulated into recurrent structure, as follows: Ot = ϕ(Qt) (cid:88) s= ψ(Ks)T Vs = ϕ(Qt) Ht (2) where Ht = (cid:88) s=1 ψ(Ks)T Vs = ψ(Ks)T Vs + Ht In order to derive recurrent representation for softmax, we analyze the numerator and denominator of softmax attention separately. We first examine causal softmax attention without the denominator and show there exists recurrent form. We use the Taylor series expansion of softmax to achieve this formulation (Section 3.1). Having recurrent form, Section 3.2 shows linear attention, as in equation (2), is first order approximation of softmax attention. Appendix shows another motivating examplethe quadratic casefor an intuition of how the recurrent form expands for higher order Taylor series approximations. In Section 3.3, the denominator is reinterpreted, using the language of RNNs with results provided in Section 4. 3.1 Recurrent Softmax Attention Causal softmax attention (1) is composed of exponentials of inner products between the query vector, Qt, and key vectors, Ks. Using decomposed inner product form, causal softmax attention can be rewritten as an RNN by taking the Taylor series expansion of the exponential function. Here, we set the multiplicative inverse of the denominator to Gt for simplicity. Although the derivation is for the causal case, it can be extended to the bidirectional case as in Appendix C. Note that, in the explanation below, we make use of several properties: 1. Decomposed Inner Product Property: (A B)n is equivalent to (An) (Bn) by equation (7). full derivation of this property is available in Appendix A. 2. Inner Product Equivalence: = (cid:80)d i=1 (A B)i = (cid:80)d i=1 AiBi 3. Kronecker Products Shorthand: An = (cid:78)n i=1 = Rdn where A, Rd, Rd is the Hadamard product, is the inner/dot product, and the lack of an explicit operation denotes scalar multiplication. With these properties, we can now reformulate softmax attention as follows: 3 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT Ot = Gt = Gt = Gt = Gt = Gt = Gt = Gt eQtKT Vs (cid:88) s=1 (cid:88) (cid:88) s=1 n=0 (cid:88) (cid:88) s=1 n=0 1 n! 1 n! (Qt s )n Vs dn (cid:88) i=1 (cid:0)Qn (cid:1) (cid:0)(K )T (cid:1) Vs (cid:88) n=0 (cid:88) n=0 (cid:88) n=0 (cid:88) n=0 1 n! 1 n! 1 n! 1 n! dn (cid:88) (cid:88) (cid:0)Qn (cid:1) (cid:0)(K )T (cid:1) Vs s=1 (cid:0)Qn (cid:1) i=1 dn (cid:88) i=1 (cid:88) s=1 (cid:0)(K s )T (cid:1) Vs (cid:0)Qn (cid:1) (cid:88) s=1 (cid:0)(K )T (cid:1) Vs (Qn ) , t = (cid:88) s=1 ((K )T ) Vs"
        },
        {
            "title": "By definition of the Taylor Series of e",
            "content": "By equation (7)"
        },
        {
            "title": "By factoring out Q",
            "content": "By inner product equivalence Define hidden state Where RN,d, RM,d, RM,e, RN , Qt Rd, Ks Rd, Re, Gt and are the sequence dimensions, indexed by and respectively. and are the embedding dimensions where is indexed by i. is the nth order term in the Taylor expansion. Thus, the softmax attention numerator does have recurrent formulation. This formulation can also be seen visually in Figure 1. Rather than the output coming from single recurrent equation, softmax is sum of infinite recurrent outputs, each weighted by 1 n! . Each of the RNNs that comprise the output of softmax attention have hidden state of shape Rdn,e due to the nth order Kronecker product on the queries and keys. The nth order Kronecker product can be thought of as creating nth order multiplicative interactions between dimensions of the keys and queries. The hidden state for each of the infinite RNNs can be thought of as accumulating information of nth order interaction terms on the dimension of and K. 3.2 Linear Attention is First Order Approximation With this recurrent formulation, notice that taking the = 1 term of the Taylor Series sum results in the well known form of linear attention, as seen in Equation 2. O(linear) = 1 1! (Q1 ) (cid:88) ((K 1 )T ) Vs = Qt s=1 (cid:88) s= s Vs = Qt Ht Ht = (cid:88) s=1 s Vs (3) Equation 3 and the left portion of Figure 1 show that linear attention is linear approximation of softmax attention when ϕ = ψ = Id. As linear attention is just single term in softmax attention, this derivation shows how linear attention is subset of softmax attention and provides an intuitive explanation as to why linear attention is typically less performant. Even when other functions are used for ϕ and ψ, they can only manipulate single recurrent chainthey cannot model the higher order terms from softmax. These terms in softmax attention allow it to model combinatorial interactions between inner product dimensionsthus, it is at least as expressive as linear attention. Also notice that each subsequent RNN comprising softmax uses larger hidden state, modeling higher-order interactions on the combinatorial dimension of and K. Linear attention works with single, smaller hidden state operating only on the dimension of and K, without higher-order interactions. To highlight the difference between softmax attention and linear attention, we also derive quadratic approximation in Appendix B, which models pairwise interactions on the dimension and has hidden state quadratically larger than linear attention. 4 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT Figure 1: Softmax attention as an RNN. We define Gt in place for the softmax denominator. Linear attention is equivalent to the = 1, first order, term. This result raises an important difference between linear attention, with functions on and K, versus nonlinear attention, which uses non-decomposable function on the inner product of and K. Despite the two forms appearing similar, they are fundamentally different in their ability to model higher order interactions. function on each of the vectors, ϕ(Qt) and ψ(Ks) restricts the vector space dimensions of Qt and Ks. For example, ReLU restricts vectors to the positive portion of the vector space. On the other hand, the exponential function on the inner product space does not restrict the vector space. Instead, this function creates nth degree multiplicative factors between dimensions of Qt and Ks for all [0, ). Practically, these multiplicative dimensions become less influential for larger n, as they are weighted by 1 n! , but they cannot be disregarded entirely. 3.3 Reinterpreting the Denominator In traditional softmax attention the denominator is calculated as: Gt = 1 s=1 eQtKT (cid:80)t (4) where Gt can have values from 0 to 1 2. Typically, these values are calculated from the same Qt and Ks values as the numerator. However, we hypothesize that the exact normalization is not the most crucial aspect of Gt. Rather, we observe that Gt could be interpreted like gate or norm that stabilizes the numerator, especially for long contexts when becomes large. This stabilization aspect is hypothesized to be the crucial function of Gt, rather than the exact form of Gt. Therefore, we reformulate Gt as gate via: (Gate) 1 Gt (cid:88) s=1 eQtKT Vs = (cid:88) n=0 1 n! (Qn ) , t = 1 Gt (cid:88) s=1 ((K s )T ) Vs (5) 2Gt can be greater than 1 if the denominator is less than 1, which is rare, only occurring for small t. 5 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT This assumption of the role of Gt allows us to interpret it like an output gate in recurrent structure at time t. This representation also ties the softmax attention mechanisms to traditional properties of expressive recurrent networks, such as the LSTM Hochreiter and Schmidhuber [1997] and GRU Cho et al. [2014]. However, this interpretation as gate may be too broad of an assumption as it also allows the sequence to grow without bound. This issue can be mitigated by dividing by the sequence length, clamping the inner product (for example, we clamp the pre-exponential value to 5), and clipping the gradient. While these tricks help ensure the gate is numerically stable, they are not particularly elegant solutions and complicate the overall implementation. Alternatively, Gt can be interpreted as norm at time t, normalizing the numerator according to its length and values. This can be realized via: (N orm) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) s=1 eQtKT Vs (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) n= 1 n! (Qn ) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) , = (cid:88) ((K s=1 )T ) Vs (6) where denotes vector norm. The optimal type of norm is not immediately clear, and could be any number of formulations such as L2, RM S, or others. We test each interpretation of Gt under recurrent perspective, as either gate or norm, to understand its equivalence to the traditional softmax denominator."
        },
        {
            "title": "4 Results",
            "content": "To evaluate our hypothetical softmax alternative, we train multiple Llama 2 Touvron et al. [2023] Touvron et al. [2023] models for next token language modeling. Keeping the rest of the architecture constant, we replace the attention mechanism with the proposed variations. We show log loss to emphasize the differences between each model. Section 4.1 shows our proposed replacement is empirically equivalent to softmax. Section 4.2 examines the scalability of the proposed replacement. Section 4.3 evaluates linear attention against normal softmax and our proposed methods. As our method uses Taylor expansion, Section 4.4 looks at the performance of different linear attention variations via progressive additions of higher order powers. Section 4.5 ablates various elements of the recurrent softmax attention. 4.1 Softmax Equivalence Experimental Setup In our experiments, we test both replacing the denominator with gate as in equation 5 and with norm as in equation 6 alongside normal softmax attention. To evaluate the applicability to various domains, we retrain on three datasets: The Pile Gao et al. [2021], SlimPajama Shen et al. [2023], and FineWeb Penedo et al. [2024]. The Pile is dataset created by Eleuther AI composed of 825 GiB of English text on various domains such as code, technical papers, math, and articles. SlimPajama is 627 billion token dataset that is cleaner subset of RedPajama Weber et al. [2024], dataset of various web crawl data. FineWeb is cleaned and de-duplicated 5-trillion token dataset compiled from 96 different common crawl snapshots. Each tested model is about 300 million parameters and trained on sequence length of 1024. Adding gate or norm requires up to an additional parameters per layer, which is insignificant relative to the model size. More hyperparameters for our models can be found in Appendix D. Figure 2: Test and train loss on various datasets for softmax attention and the proposed methods with gate or norm replacements. Expanded plots can be found in Appendix G. 6 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT Result Figure 2 shows the resulting loss curves for each dataset and model variant. The loss for the normed model variant follows the model trained with native softmax attention precisely while the model with gate performs slightly worse. We employ the L2 vector norm for this analysis and find that it is numerically stable. The gate was semi-unstable during training. We noticed recoverable loss spikes during training where the model loss would spike and resume after some number of steps. Dividing by the sequence length, clipping the inner product before exponentiation, and performing gradient clipping, helped produce fewer spikes, however some instability was still observed. This result implies that the function of Gt is most similar to norm operation. Moreover, the norm does not need to mirror the traditional softmax exponentiationa simple vector norm can suffice. 4.2 Scaling To investigate if scaling laws hold, we scale the model in two ways. We scale the model from 300M parameters to 2B parameters, keeping all other hyperparameters constant to evaluate the scalability of the model itself. We also scale the sequence length from 1024 to 4096, keeping all other hyperparameters constant to examine how our method scales with longer sequences. These scaling are tested upon the FineWeb Penedo et al. [2024] dataset. The scaled comparisons are found in Figure 3, showing our proposed attention formulation scales identically to softmax (with both the sequence length and the model size). Again we observe that the norm better mirrors softmax performance and is more numerically stable. Figure 3: Test and train loss on the FineWeb dataset for large models (about 2B parameters) on 1024 sequence length and small models (about 300 million parameters) on 4096 sequence length for softmax attention and the proposed methods with gate or norm replacements. Some experiments were cut short due to time constraints 4.3 Linear Attention As the proposed softmax decomposition has direct relationship to linear attention, we evaluate the method against several variations of linear attention. Figure 4 shows that softmax attention and our proposed methods outperform each linear attention variant by significant gap. This behavior supports the assertions in Section 3.2, showing linear attention is subset of softmax attention. 4.4 Taylor Series Terms As our method uses Taylor series, we investigate how performance behaves as higher order terms are added to the less expressive attention variations. That is, we use the recurrent formulation for particular type of linear attention and gradually add in higher order terms from the softmax Taylor expansion of softmax. The results are shown in Figure 5 for three types of linear attention: cosine similarity Mongaras et al. [2025], ReLU Xie et al. [2025], and elu(x) + 1 kernel Katharopoulos et al. [2020]. As more terms are added to the softmax approximation, the performance 7 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT Figure 4: Test and train loss on the FineWeb dataset for various linear attention methods, softmax attention, and the proposed methods with gate or norm replacements. smoothly transitions from linear attention performance to softmax attention performance. After adding in terms up to = 10, we observe that the recurrent approximation mirrors softmax with negligible differences. For the linear attention variants, however, we find that adding more terms (described in Appendix F) improves performance but never fully reaches the performance of softmax attention (even for = 10). We hypothesize this performance gap exists because linear attention variants have functions on the independent vectors, ϕ(Q) and ψ(K), restricting the reachable vector space when combined; whereas softmax does not restrict this vector space. We leave exploring this observation to future work. We note that cosine attention does not gain any benefit from additional terms and hypothesize that due to inner product values being between 0 and 1, the resulting higher order interaction terms are less than 1, limiting the magnitude of higher order terms and therefore the impact of these higher order terms on the resulting output. Figure 5: Log train loss for softmax attention and various linear attention method when summing more powers of the inner product. The nth order denotes the sum of powers from 0 to n. 4.5 Ablation Analysis To further investigate various recurrent elements, we conduct an ablation study using the 300M parameter model, varying both the gated and normed variants of our method. The results of these ablations are plotted in Figure 6. The leftmost plots ablate several elements of the recurrent softmax denominator, Gt, by removing, detaching, and dividing by the sequence length, (with varying combinations of all three). We find that detaching the denominator from On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT Figure 6: Test and train loss on various datasets for softmax attention and the proposed methods with gate or norm replacements. Expanded plots can be found in Appendix G. the computation graph significantly hurts downstream performance. Removing the denominator and dividing by the sequence length gives similar performance to softmax, but also creates instability. Adding gate helps to somewhat stabilize the training, though not entirely. Finally, we find that removing the denominator and adding norm is what appears to mirror softmax most closely. Gate The middle column of plots in Figure 6 ablates the gate method employed by combining sequence length normalization with an input gate or output gate (more fully explained in Appendix E), and investigating ReLU linear attention with gate. The results shows that having gate helps to stabilize training loss, but must be accompanied by sequence length normalization for good test performance. Furthermore, replacing the exponential with decomposable ReLU kernel significantly hurts performance, regardless of gating or sequence length normalization. This result emphasizes the importance of the exponential function on the inner product of and K, as well as indicating that some sort of sequence length normalization is required. Norm The rightmost column in Figure 6 varies the norm method employed, investigating the L2 norm, RMS norm, layer norm, and L2 norm with sequence normalization. We also investigate the use of an L2 norm with the decomposable ReLU kernel. The results shows that any type of normalization, with or without learnable parameters, works just as well as softmax. However, replacing the exponential with decomposable ReLU kernel significantly degrades performance. This graph further emphasizes that the important aspects of softmax attention are an exponential plus vector norm. The choice of norm does not appear to influence training stability or test performance. However, there are differences between vector norm (e.g., L2) and sequence length normalization (e.g., division by or gating). The vector norm appears to achieve the best performance, implying it is necessary aspect of softmax, but the exact form is unimportant."
        },
        {
            "title": "5 Conclusions and Limitations",
            "content": "This work connects linear and softmax attention under unifying recurrent formulation. Taylor series expansion of the softmax attention numerator was employed to develop recurrent form and competing hypotheses for approximating the softmax denominator were investigated. Linear attention was shown to be first order approximation of softmax. Using the recurrent softmax attention formulation, equivalence was shown empirically and the crucial elements of softmax attention were analyzed in an ablation study. Additional experiments showed that tenth order Taylor series approximation is sufficient similar to softmax. Finally, different types of normalization and gates were explored showing that any vector norm is sufficient to approximate the functionality of the softmax attention denominator. The theory developed in this work is crucial to understanding the performance bounds of softmax compared to other forms of attention. Moreover, this theory may be employed to uncover more performant or efficient attention mechanisms. Limitations The current formulation covers only linear attention and softmax attention, future work can expand it to more complicated recurrent architectures such as RWKV Peng et al. [2025] and state-space models like Mamba Dao and Gu [2024]. As recurrent architectures similar to RWKV and Mamba are extensions of linear attention, the theory developed in this work should extend to the additions made in these works. Only the causal next token prediction 9 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT task was investigated. While the derivations should generalize to other domains, both bidirectional and causal, further investigation is necessary to ensure this generalization. Future work can also incorporate changes from these recurrent models to softmax attention to potentially improve efficiency or performance of softmax attention implementations."
        },
        {
            "title": "References",
            "content": "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for selfsupervised learning of speech representations. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1244912460. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/ abs/1409.0473. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, and Vahab Mirrokni. Atlas: Learning to optimally memorize the context at test time, 2025. URL https://arxiv.org/abs/ 2505.23735. Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150, 2020. Kyunghyun Cho, van Merrienboer, Caglar Gulcehre, Bougares, Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 2014. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. Liam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, and Sanjay Shakkottai. In-context learning with transformers: Softmax attention adapts to function lipschitzness. Advances in Neural Information Processing Systems, 37:9263892696, 2024. Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1004110071. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/dao24a.html. Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the performance edge over linear attention. arXiv preprint arXiv:2310.11685, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations, May 2021. URL https://openreview.net/pdf?id=YicbFdNTTy. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=FPnUhsQJ5B. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. CoRR, abs/2101.00027, 2021. URL https://arxiv.org/abs/2101.00027. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= uYLFoz1vlAC. On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT Dongchen Han, Yifan Pu, Zhuofan Xia, Yizeng Han, Xuran Pan, Xiu Li, Jiwen Lu, Shiji Song, and Gao Huang. Bridging the divide: Reconsidering softmax and linear attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=RSiGFzQapl. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780, November 1997. ISSN 0899-7667. doi:10.1162/neco.1997.9.8.1735. Generated from Scopus record by KAUST IRTS on 2022-09-14. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 5156 5165. PMLR, 1318 Jul 2020. URL https://proceedings.mlr.press/v119/katharopoulos20a.html. Zhixuan Lin, Evgenii Nikishin, Xu He, and Aaron Courville. Forgetting transformer: Softmax attention with forget gate. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=q2Lnyegkr8. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Evan Miller. Attention is off by one, 2023. URL https://www.evanmiller.org/attention-is-off-by-one. html. Gabriel Mongaras, Trevor Dohm, and Eric Larson. Cottention: Linear transformers with cosine attention. Proceedings of the 2025 Computing Conference, 2025. Tobias Christian Nauen, Sebastian Palacio, and Andreas Dengel. Taylorshift: Shifting the complexity of self-attention from squared to linear (and back) using taylor-softmax. In ICPR (6), pages 116, 2024. URL https://doi.org/ 10.1007/978-3-031-78172-8_1. Yuqi Nie, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Jbdc0vTOcol. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von In The Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum?id=n6SCkn2QaG. Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, et al. Rwkv-7\" goose\" with expressive dynamic state evolution. arXiv preprint arXiv:2503.14456, 2025. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, and Lingpeng Kong. Random feature attention. In 9th International Conference on Learning Representations, ICLR 2021, 2021. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=Bl8CQrx2Up4. Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Gated attention for large language models: Non-linearity, sparsity, and attention-sink-free, 2025. URL https://arxiv.org/abs/2505.06708. Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/abs/ 2002.05202. Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. SlimPajama-DC: Understanding Data Combinations for LLM Training. arXiv e-prints, art. arXiv:2309.10818, September 2023. doi:10.48550/arXiv.2309.10818. Jerome Sieber, Carmen Amo Alonso, Alexandre Didier, Melanie Zeilinger, and Antonio Orvieto. Understanding the differences in foundation models: Attention, state space models, and recurrent neural networks. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=iF7MnXnxRw. Ethan Smith. Softmax attention is fluke, 2025. URL https://www.ethansmith2000.com/post/ softmax-attention-is-a-fluke. 11 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. ISSN 09252312. doi:https://doi.org/10.1016/j.neucom.2023.127063. URL https://www.sciencedirect.com/science/ article/pii/S0925231223011864. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. Learning to (learn at test time): RNNs with expressive hidden states, 2025. URL https://openreview.net/forum?id=eifW0W0xgt. Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. arXiv e-prints, art. arXiv:2302.13971, February 2023. doi:10.48550/arXiv.2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Petar Veliˇckovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=rJXMpikCZ. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. Redpajama: an open dataset for training large language models, 2024. URL https://arxiv.org/abs/2411.12372. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=N8Oj1XhtYZ. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020. Shuangfei Zhai*, Tatiana Likhomanenko*, Etai Littwin*, Dan Busbridge*, Jason Ramapuram*, Yizhe Zhang, Jiatao Gu, and Josh M. Susskind. Stabilizing transformer training by preventing attention entropy collapse. In ICML, 2023. URL https://arxiv.org/abs/2303.06296. Shen Zhuoran, Zhang Mingyuan, Zhao Haiyu, Yi Shuai, and Li Hongsheng. Efficient attention: Attention with linear complexities. In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 35303538, 2021. doi:10.1109/WACV48630.2021.00357. On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT"
        },
        {
            "title": "A Inner Product Decomposition",
            "content": "Let A, Rd Define Rd as the Hadamard product of and B. Define as the inner/dot product of and Note that the inner product is defined as = (cid:88) i=1 (A B)i = (cid:88) i="
        },
        {
            "title": "AiBi",
            "content": "Define An = (cid:79) i=1 = Rdn as - 1 Kronecker products The lack of operation denotes scalar multiplication. Using the notation mention in Section 3.1, we show the decomposed form for an inner product space to the nth power is equivalent to the inner product of nth order Kronecker products of each vector. (A B)n (cid:34) (cid:88) (cid:35)n (A B)i i=1 (cid:34) (cid:88) i=1 (A B)i (cid:35) (cid:34) (cid:88) i= (cid:35) (A B)i ... (cid:35) (A B)i (cid:34) (cid:88) i=1 (cid:88) (cid:88) (cid:88) ... i=1 j=1 k=1 (A B)i (A B)j ... (A B)k (cid:124) (cid:125) (cid:123)(cid:122) combinatorial multiplication from ... = = = = = dn (cid:88) i=1 dn (cid:88) i= (cid:104) (A B)n(cid:105) (cid:2)(cid:0)An(cid:1) (cid:0)Bn(cid:1)(cid:3) = (cid:0)An(cid:1) (cid:0)Bn(cid:1)"
        },
        {
            "title": "B Quadratic Derivation",
            "content": "By equation (2) Power to product of terms By rearranging the sums By equation (3) Rearrange the nth order Kronker Product Change to inner product by equation (2) (7) (8) To provide simple example of the recurrent form derived in Section 3.1, we note that A2 = Rd2 the recurrent form. and show 13 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT Ot = (cid:88) s=1 (Qt )2 Vs (cid:88) (cid:34) (cid:88) (cid:35)2 (Qt Ks)i Vs s=1 (cid:88) i=1 d2 (cid:88) (Qt Qt)i (Ks Ks)i Vs By equation (2) By equation (7) s=1 d2 (cid:88) i=1 (cid:88) (Qt Qt)i (Ks Ks)i Vs rearrange summations = = = = i=1 d2 (cid:88) i=1 s= (Qt Qt)i (cid:88) s=1 (Ks Ks)i Vs = (Qt Qt) (cid:88) s=1 (cid:0)(K ) (K )(cid:1) Vs (cid:0)(K ) (K s )(cid:1) Vs Rd2,d = (Qt Qt) Ht Ht = (cid:88) s="
        },
        {
            "title": "C Bidirectional Derivation",
            "content": "Ot = Gt (cid:88) s=1 eQtKT Vs"
        },
        {
            "title": "By factoring out Q",
            "content": "By equation (2) Define hidden state = Gt = Gt = Gt = Gt = Gt = Gt (cid:88) (cid:88) s=1 n= (cid:88) (cid:88) s=1 n=0 1 n! 1 n! (Qt )n Vs dn (cid:88) i=1 (cid:0)Qn (cid:1) (cid:0)(K )T (cid:1) Vs (cid:88) n=0 (cid:88) n=0 (cid:88) n= (cid:88) n=0 1 n! 1 n! 1 n! 1 n! dn (cid:88) (cid:88) (cid:0)Qn (cid:1) (cid:0)(K )T (cid:1) Vs s=1 (cid:0)Qn (cid:1) i=1 dn (cid:88) i=1 (cid:88) s=1 (cid:0)(K )T (cid:1) Vs (cid:0)Qn (cid:1) (cid:88) s=1 (cid:0)(K s )T (cid:1) Vs (Qn ) , = (cid:88) s=1 ((K )T ) Vs Rdn,e By definition of the Taylor Series of By equation (7)"
        },
        {
            "title": "By rearranging sums",
            "content": "By factoring out By equation (2) Define hidden state The only difference between the causal and bidirectional formulations is the range on the sequence sum indexed by s. In the causal case, the index ends at while the bidirectional case sums to the end of the sequence. This means the hidden state operates on the entire sequence for each token rather than just the past t. 14 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT"
        },
        {
            "title": "D Model Parameters",
            "content": "Unless otherwise mentioned, the below are the parameters we used in our models. As our base model is llama 2 Touvron et al. [2023]. RoPE Su et al. [2024] is used on the attention matrix and the MLPs follow SwiGLU Shazeer [2020]. 1. batch size - 36 2. learning rate - 1e-4 3. warmup steps - 10,000 4. warmup type - linear warmup from 0, linear decay 5. num steps - 100, 6. precision - float32 and bfloat16 mixed precision 7. Weight decay - 0.01 8. Max sequence length - 1024 for general experiments, 4096 for length scaling experiment 9. Test percentage - 0.001 10. Optimizer - AdamW 11. Adam betas - 0.9 and 0. 12. Hidden size - 1024 (3072 for the large model) 13. MLP intermediate size - 2048 (6144 for the large model) 14. Num attention heads - 16 15. Num hidden layers - 20 16. Tokenizer - llama2-7b-hf 17. Gradient clipping - 1.0 clipping for gated models, no clipping for all other experiments Each model was trained for maximum of 2 days. For most experiments, we use distributed data parallel processing to train on two 80 GB, A100 GPUs with the exception of the large model, trained on 4 GPUs, and 4096 sequence length, trained on 6 GPUs."
        },
        {
            "title": "E Additional Gate Information",
            "content": "The input and output gates referenced in this paper are very similar to that of an LSTM Hochreiter and Schmidhuber [1997] and similar to that used by Qiu et al. [2025]. The input gate controls how much information is being added to the LSTM memory while the output gate modulates the output of the LSTM cell. As seen in Figure 7, these gating mechanisms can be translated to the original linear attention formulation where the input gate is scalar between [0, 1] at time t, modulating the outer product while the output gate is scalar between [0, 1] at time modulating output after the QtHt inner product. These ideas extend to softmax attention it can be computed as an infinite sum of these RNNs, although the implementation is intractable. However, the gates can be translated to multiplicative values on the QK attention matrix. As the rows/queries define the output at time t, the output gate can be defined along the rows. Similarly, as the columns define the input at time s, the input gate can be defined along the columns. This idea is equivalent to applying the output gate after performing the full attention operation and applying the input gate to the values. Mathematically, this can be expressed as follows: (cid:88) Ot = (cid:104) Gin eQtKT Gout (cid:105) Vs = Gin (cid:88) eQtKT (cid:2)Gout Vs s=0 (cid:104) Gin eQKT = Gout(cid:105) s=0 = Gin (cid:104) eQKT Where RN,d, RM,d, RM,e, Gin [0, 1]N , Gout [0, 1]M And Qt Rd, Ks Rd, Vs Re, Gin [0, 1], Gout [0, 1] 15 (cid:3) (cid:105) (cid:2)Gout (cid:3) On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT Figure 7: Linear attention as an RNN with an input and output gate."
        },
        {
            "title": "F Linear Attention Expansions",
            "content": "For linear attention variants the activation functions, ϕ and ψ, are applied to and as in the native linear attention. (Sof tmax) Ot = (Linear) Ot = (cid:88) n=0 (cid:88) n=0 1 n! 1 n! (cid:0)Qn (cid:1) (cid:88) s=1 (cid:0)(K )T (cid:1) Vs (cid:0)ϕ(Q)n (cid:1) (cid:88) s=1 (cid:0)(ψ(K)n )T (cid:1) Vs = = (cid:88) (cid:88) s=1 (cid:88) n=0 (cid:88) s=1 n=0 1 n! 1 n! (Qt )n Vs (ϕ(Q)t ψ(K)T )n Vs"
        },
        {
            "title": "G Expanded Results Figures",
            "content": "Figure 8 shows expanded plots for the comparison of our recurrent softmax attention formulation. In Figure 9, we show expanded plots of our ablation study. 16 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT Figure 8: Expanded test and train loss plots on various datasets for softmax attention and the proposed methods with gate or norm replacements. 17 On the Expressiveness of Softmax Attention: Recurrent Neural Network Perspective PREPRINT Figure 9: Expanded test and train loss on various datasets for softmax attention and the proposed methods with gate or norm replacements."
        }
    ],
    "affiliations": [
        "Lyle School of Engineering Southern Methodist University Dallas, TX 75205"
    ]
}