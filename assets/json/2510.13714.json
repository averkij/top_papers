{
    "paper_title": "Dedelayed: Deleting remote inference delay via on-device correction",
    "authors": [
        "Dan Jacobellis",
        "Mateen Ulhaq",
        "Fabien Racapé",
        "Hyomin Choi",
        "Neeraja J. Yadwadkar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Remote inference allows lightweight devices to leverage powerful cloud models. However, communication network latency makes predictions stale and unsuitable for real-time tasks. To address this, we introduce Dedelayed, a delay-corrective method that mitigates arbitrary remote inference delays, allowing the local device to produce low-latency outputs in real time. Our method employs a lightweight local model that processes the current frame and fuses in features that a heavyweight remote model computes from past frames. On video from the BDD100K driving dataset, Dedelayed improves semantic segmentation accuracy over the stronger of the local-only and remote-only baselines across all realistic communication network delays beyond 33 ms. Without incurring additional delay, it improves accuracy by 6.4 mIoU compared to fully local inference and 9.8 mIoU compared to remote inference, for a round-trip delay of 100 ms. The advantage grows under longer delays and higher-motion scenes, as delay-mitigated split inference sustains accuracy more effectively, providing clear advantages for real-time tasks that must remain aligned with the current world state."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . e [ 1 4 1 7 3 1 . 0 1 5 2 : r DEDELAYED: DELETING REMOTE INFERENCE DELAY VIA ON-DEVICE CORRECTION Dan Jacobellis1,2, Mateen Ulhaq2, Fabien Racapé2, Hyomin Choi2, Neeraja J. Yadwadkar1 1University of Texas at Austin, 2InterDigital danjacobellis@utexas.edu, neeraja@austin.utexas.edu {mateen.ulhaq, fabien.racape, hyomin.choi}@interdigital.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Remote inference allows lightweight devices to leverage powerful cloud models. However, communication network latency makes predictions stale and unsuitable for real-time tasks. To address this, we introduce Dedelayed, delay-corrective method that mitigates arbitrary remote inference delays, allowing the local device to produce low-latency outputs in real time. Our method employs lightweight local model that processes the current frame and fuses in features that heavyweight remote model computes from past frames. On video from the BDD100K driving dataset, Dedelayed improves semantic segmentation accuracy over the stronger of the local-only and remote-only baselines across all realistic communication network delays beyond 33 ms. Without incurring additional delay, it improves accuracy by 6.4 mIoU compared to fully local inference and 9.8 mIoU compared to remote inference, for round-trip delay of 100 ms. The advantage grows under longer delays and higher-motion scenes, as delay-mitigated split inference sustains accuracy more effectively, providing clear advantages for real-time tasks that must remain aligned with the current world state."
        },
        {
            "title": "INTRODUCTION",
            "content": "In soft real-time applicationssuch as cloud gaming or video conferencinglate outputs may be diminished in value, but are still useful. In these applications, we can offload expensive computations to powerful cloud GPUs to save on-device power. As long as the typical latency is low, the loss of utility from latency is outweighed by power savings and extended battery life. However, in hard real-time applicationssuch as aerial robotic control or obstacle avoidancelate outputs can be catastrophic, and the system must be designed with guaranteed deadline. Due to the irreducible high-tail latency in wireless communication, hard real-time applications must be equipped with fully functional local inference pipeline as fallback in the case that the remote predictions fail to meet the deadline. In this work, we focus on such real-time applications running on resourceconstrained devices, relying on inference using video inputs. In recent years, various approaches to split computing have been proposed to offload computation of expensive image and video models to the cloud to enable the next-generation of robotic, remote sensing, and wearable technology platforms. For real-time streaming video applications, existing approaches still fall into one or more of three common pitfalls. (1) They allocate all on-device power and computation to single linear inference pipeline, leaving no resources for local-only (3) They (2) They do not account for the impact of latency on prediction accuracy. fallback. operate on videos with significantly reduced spatiotemporal resolution to manage computational cost, leaving out rich visual details available from modern camera systems. To address these limitations, we introduce Dedelayed  (Fig. 1)  , co-inference framework that leverages fresh local information to mitigate the effects of remote inference delay. It consists of local model and remote model connected over communication network, and fuses delayed remote signals into local pipeline operating on fresh on-device inputs, yielding real-time performance that is never worse than either model in isolation. This enables the use of high-capacity cloud models for delay-sensitive applications while avoiding the pitfalls of prior offloading approaches: 1 Figure 1: Overview of the Dedelayed real-time inference setup. The lightweight local model and powerful remote model augment each others strengths to produce accurate and timely outputs. 1. Full integration with local-only fallback model. No wireless communication channel can offer perfect reliability. For real-time applications with critical deadlines, any remote inference procedure must be accompanied by lightweight local fallback model. Instead of two redundant inference pipelines, Dedelayed uses single path based on local model that optionally incorporates side information from the remote model. We choose simple method to incorporate this side informationelement-wise addition of activation mapsresulting in negligible overhead and well-defined behavior in the absence of remote outputs. 2. Temporal prediction for latency mitigation. During supervised training of the remote model component, we simulate delay of frames. In other words, the remote model is trained to predict the future. delay embeddingsimilar to position embedding in text or vision transformersallows the behavior of the remote model to adapt to changes in the channel. As shown in Fig. 2, temporally predictive training is able to capture motion dynamics, which can be used to compensate for latency. 3. Mixed-resolution inference. On-device AI video processing at or near the capture resolution and frame ratetypically > 1 megapixel and > 20 frames per secondis rarely feasible, even with lightweight models. To save resources, real-time computer vision applications often process each frame independently or use small motion updates instead of natively processing dense 3D pixel volume. Dedelayed is capable of using mixedresolutioninstead of reducing the resolution for the entire inference pipeline, only the local model resolution is reduced. In parallel, the remote model operates on multiple, high-resolution frames using 3D transformer. Thus, the remote model can utilize powerful GPUs to model fine details and motion of delayed frames, while the local component can allocate its resources to modeling the current state of objects and the scene, as shown in Fig. 3. Our contributions are threefold: 1. We provide measurements demonstrating how higher degrees of latency hurt the accuracy of dense visual prediction for semantic segmentation of driving scenes. 2. We introduce Dedelayed, split computing framework for video inputs that integrates the output of future-predicting remote model with the current output of local model. 3. Using Dedelayed, we create video segmentation system for urban driving scenes that outperforms any existing local or remote inference solution, while avoiding the pitfalls that limit the practicality of previous approaches. 2 For experimental validation, we demonstrate simple Dedelayed system with an addition-based fusion on off-the-shelf models with minimal architectural changes. This makes it easy to enhance existing pipelines, deploy in practice, and extend to other real-time methods. Figure 2: To demonstrate the effect of temporally predictive training, we train 3D transformer to predict the next frame with an MSE loss on pixels. (a) shows the original video frame. (b) shows the difference between (a) and future frame, with objects such as the traffic sign and road markings in different locations. (c) shows the pixel predictions of the 3D transformer. (d) shows the difference from the true future frame. While the predictive model cannot predict high-frequency details, it is able to accurately model the motion of objects, signs, and road markings. Figure 3: Example of activation maps from local and remote model components. The remote server uses the higher level of video detail to accurately distinguish and classify objects. The local model provides exact position adjustments based on the current frame. When making predictions from the combined activation map, small details that would be impossible to make out at low resolution (e.g., the distant pedestrians, labeled red) are accurately classified and localized."
        },
        {
            "title": "2 BACKGROUND",
            "content": "In the human visual system, the optic nerve can only transmit small fraction of the information received by the retina (Kelly, 1962). Barlows efficient coding hypothesis (Barlow et al., 1961) posits 3 that compression is the primary role of early processing; once this compressed representation is received in deeper layers of the visual cortex, more metabolically intense processing can occur. In the predictive coding model (Rao & Ballard, 1999), this processing is driven by feedback mechanisms that minimize temporally predictive error signal to create perceptual model that is consistent with sensory inputs. Machines equipped with digital video sensorswhich are at the center of ongoing innovation in robotics (Kim et al., 2024; ONeill et al., 2024), remote sensing (Szwarcman et al., 2024; Khani et al., 2021), and wearable technology (Grauman et al., 2022; 2024)share similar constraints. The throughput and power efficiency of ingesting pixels on the sensor device (e.g., battery-powered robot) are extremely hightypically tens or hundreds of megapixels per watt-second (Engel et al., 2023). However, moderately sized DNNs can only process visual data at about one megapixel per watt-second (Cai et al., 2023). For more advanced video AI based on autoregressive modeling (Agarwal et al., 2025) or temporal prediction (Assran et al., 2025), the efficiency may be as low as 500 pixels per watt-second. Instead of on-device processing, power constraints can be circumvented by compressing and transmitting video streams to cloud GPU datacenters supported by 100-megawatt power infrastructure (Goldberg & Kehoe, 2013). However, fully remote processing is challenging for certain real-time applications (e.g., collision avoidance) due to unreliability in network and cloud infrastructure (Chen et al., 2024; 2025). Thus, delivering predictions by guaranteed deadline requires fallback procedure independent of the remote server. In many systems (e.g., autonomous motor vehicles) the limited accuracy and reliability of lightweight local models warrant human operator as the fallback (Committee, 2021), preventing full automation."
        },
        {
            "title": "3 RELATED WORK",
            "content": "Prior work has extensively explored lightweight architectures (e.g., EfficientViT (Cai et al., 2023), MobileNetV4 (Qin et al., 2024)) that minimize computation to achieve real-time on-device performance. These approaches deliver low latency but are constrained by device power and compute. When devices are too limited, the common alternative is fully remote inference, which offloads computation to servers, but is highly susceptible to network latency. Split-computing approaches largely focus on distributing workloads rather than optimizing for strict real-time operation. Next-generation compression standards such as MPEG AI (ISO/IEC, 2025) and JPEG AI (Ascenso et al., 2023) target bandwidth reduction via task-specific compression, significantly lowering transmission costs with reasonable compute overhead. However, even they lack explicit mechanisms to anticipate or compensate for network delay, leading to stale predictions misaligned with the current world state. Other efforts related to our work have also been explored. Clockwork Convnets (Shelhamer et al., 2016) reuse stale features to reduce inference latency, but they offer limited temporal reasoning and operate on single device. Accel (Jain et al., 2019) warps heavy-model features forward with optical flow and corrects them with lightweight model, but is also not intended for across-network operation. Adaptive Model Streaming (Khani et al., 2021) streams weight updates from server to keep local model fresh, focusing on model adaptation rather than directly mitigating per-frame staleness from communication latency. Though less known, Knowledge Boosting (Srinivas et al., 2024) is very closely related to our work. Like us, it fuses delayed remote features with small on-device model, but it assumes fixed delay. We generalize to longer and variable latencies by conditioning on tunable delay while keeping the design simple and reusable."
        },
        {
            "title": "4 METHOD",
            "content": "4.1 DELAY MITIGATION FRAMEWORK Dedelayed introduces general framework that improves the accuracy and robustness of real-time inference on resource-constrained sensor devices. It does so by combining the strengths of both local inference and remote inference, while mitigating their weaknesses. The local model has access to the latest sensor data, and yet lacks the computational capability needed to produce accurate 4 Figure 4: Time progresses left to right. The client-side camera produces video frames, which are sent across communication network to the server. The server runs heavyweight model using the latest video frame xtτ that it receives, in addition to context of previously received video frames x<tτ , as well as the measured delay τ . This produces an output ztτ that the server sends to the client. The client pairs the latest received response ztτ with freshly produced video frame xt, and runs these inputs through lightweight model. This finally produces timely result ˆyt that can be used in real-time delay-sensitive applications. outputs. The remote model provides accurate outputs, and yet delivers them with delay. By careful combination of both subsystems, Dedelayed is able to provide bounded performance guaranteesit is never worse than either local inference or remote inference independently. As we will demonstrate later, we are able to glue together the two subsystems in way that is simple yet effective. Dedelayed addresses the problem of stale predictions from powerful remote models by integrating them with lightweight, on-device model. The core idea is to leverage the high-quality features from heavyweight remote model, despite their inherent delay, by explicitly conditioning them on the measured latency and fusing them early with live information from local model. This ensures that the final predictions are both accurate and timely. Dedelayed can be formulated in simple mathematical terms as follows. Given fresh input frame xt at current time t, the final prediction ˆyt is computed using lightweight local model, flight, which processes xt along with time-delayed features ztτ from heavyweight remote model, fheavy. To produce powerful predictive features, the remote model is conditioned on the delay τ , and processes τ . This is expressed by the following equations: short clip of past frames xtτ ending at time ztτ = fheavy(τ, xtτ ) ˆyt = flight(xt, ztτ ) (1) (2) For clarity, the notation is summarized in Table 1. Fig. 4 presents system diagram that demonstrates the fundamental principle we describe, and shows how information propagates through the various subsystems as time progresses. The entire Dedelayed system is trained end-to-end to minimize task-specific loss function, evaluated against the ground truth yt for the current frame. task, task = ℓ(ˆyt, yt) For semantic segmentation, ℓ is typically the cross-entropy loss. The objective is to produce predictions ˆyt that are accurate at time on the local device. In the next section, we detail how we designed specific implementation to test the Dedelayed framework in action. Table 1: Notation. Symbol Meaning xt xtτ ˆyt ztτ τ flight fheavy Input frame at current time Input frames up to time τ Prediction for time Features outputted by heavyweight model run at time Delay in time between the old and current frame at time Delay in frames between the old and current frame at time Lightweight model run at time Heavyweight model run at time τ τ"
        },
        {
            "title": "4.2 DESIGN AND IMPLEMENTATION",
            "content": "Dedelayed consists of lightweight on-device model and predictive remote model connected over communication network. The remote side computes delay-conditioned features from past inputs and returns them to the device, where they are fused early into the local model running on the current input. To provide guarantee on baseline performance, these components can first be trained to work independently and later fused and trained jointly. System overview Information propagates through our system as follows: 1. The local device transmits input frames to the remote via the uplink. 2. Each incoming frame is independently encoded into features using pretrained 2D ViT backbone on the remote. We maintain context window of the most recent features. 3. The per-frame features are concatenated along the temporal axis, and learned delay embedding conditioned on the measured delay τ is added. 4. 3D ViT encoder followed by learned pooling (MLPpoolMLP) produces delayconditioned remote features ztτ , which are sent back to the device via the downlink. 5. The lightweight local model runs on fresh input xt, and fuses in the remote features ztτ . 6. The local model finishes decoding the fused representation and outputs labels ˆyt. Remote predictive module. Figure Fig. 5 visualizes the remote component. The remote model τ . Each frame is encoded independently processes fixed context of past frames ending at 8 patch. The per-frame features with 2D ViTwe use EfficientViT-L1 with an effective 8 are concatenated along the temporal axis and spatially merged into larger 16 16 patches to keep the sequence length comparable. (When = 4, the sequence length is identical.) learned delay embedding determined by the delay τ is added, and the result is then processed by 3D ViT followed by learned pooling (MLPpoolMLP) to produce delay-conditioned features ztτ . We pretrain the remote predictive module by attaching task head to the 3D ViT backbone and training it to predict the target labels yt from inputs up to time τ . Local model and fusion. The lightweight local model processes the fresh input xt. We compute first-stage features = T1(xt) and perform early fusion by element-wise addition with the delayed remote features, = + ztτ . Both tensors have shape 96 W/8, so no projection or resizing is required. The fused features are then processed by the remaining local blocks and decoded to the final output ˆyt. If ztτ is unavailable, the local model falls back to = and proceeds unchanged. H/"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We focus our evaluation of Dedelayed on the task of real-time semantic segmentation of driving scenes, domain where timely and accurate perception is critical for applications such as autonomous driving and robotics. We demonstrate that delay-aware feature fusion can mitigate remote 6 Figure 5: Overview of the remote video predictive model, trained to predict the label at index from K-frame context ending at D. inference latency, sustaining accuracy even when remote predictions are delayed by long communication network latencies. EXPERIMENTAL SETUP Our experiments utilize the BDD100K video dataset (Yu et al., 2020), containing video of driving scenes at 30 frames per second (fps). Since the dataset does not provide dense segmentation labels for all video frames, we generated pseudo-labels using pretrained EoMT (Kerssies et al., 2025) model, ignoring pixels with low confidence. We use subset of 19 labels from Cityscapes. Our evaluation covers range of realistic network delays, from 0 to 5 frames, corresponding to 0 to 165 ms. This range is representative of typical round-trip latencies and is sufficient to demonstrate the degradation of conventional remote inference and the resilience of Dedelayed. At training time, the delay τ was sampled per batch from uniform distribution over this range. This delay, together with the relevant frames from the video, was fed into the remote model. To replicate real-world usage, we applied compression to the uplink video streams. We chose the resolution and frame rate to fit within reasonable uplink capacity using the lossy WebP image codec at quality 85. TRAINING DETAILS We adopt multi-stage training strategy, as detailed in Table 2. The remote and local models are first trained individually and then later combined. Each model is pretrained on the large-scale ImageNet dataset (Russakovsky et al., 2015) for classification, then on the image segmentation task on Cityscapes (Cordts et al., 2016), before being fine-tuned on the smaller BDD100K driving dataset. We specifically train the remote model to have predictive capability by supplying it with delayaware (DA) objective: to predict the labels of future frames, conditioned on tunable delay. By training in stages, we are able to provide guarantees on baseline remote and local model performance. Finally, the remote and local models are glued together, and the full system is jointly fine-tuned on the delay-aware task of streaming semantic segmentation on video. We train using cross-entropy loss, the Adan (Xie et al., 2024) optimizer, trapezoidal cosine learning rate schedule, gradient clipping, and selectively applying discriminative fine-tuning or layer-wise learning rate decay (LLRD) (Howard & Ruder, 2018). 7 Table 2: Training stages for remote, local, and fusion models. Model Stage Obj. Data Epochs Res. Freeze Remote (image only) SegFormer-B5 SegFormer-B5 Remote (video-predictive) EfficientViT-Seg-L1 EfficientViT-Seg-L1+ViT3D EfficientViT-Seg-L1+ViT3D Local (image only) MSTransformer2D MSTransformer2D MSTransformer2D MSTransformer2D prepre1 2 1 2 3 4 DU DU DU DA DA DU DU DU DU IN1K, CS BDD IN1K, CS BDD BDD IN1K CS CS BDD 15 10 10 320 80 80 15 496 496 224 336 336 496 img-bkbn bkbn + proj LLRD 0.9 LLRD 0.9 Fusion (local image + remote video-predictive) + MSTransformer2D EfficientViT-Seg-L1+ViT3D DA 1 BDD 10 480/720 remote img-bkbn + 3D Stage: pre- = pretrained (external source). Obj.: DA = delay-aware objective; DU = delay-unaware objective. Data: IN1K = ImageNet-1K; CS = Cityscapes; BDD = Berkeley DeepDrive 100K. Freeze: bkbn + proj = backbone & MS-projections frozen; remote img-bkbn + 3D = remote image backbone and remote 3D encoder frozen; LLRD = layer-wise learning rate decay."
        },
        {
            "title": "6 RESULTS",
            "content": "We compare how various inference systems perform under the effect of communication network latency. Fig. 6 visualizes the various local-only, remote-only, and fused local+remote methods from different stages of our training. Each baseline is effectively an ablation of our final system. Local image and Remote image inference setups process individual frames in the conventional way, though the remote is susceptible to communication network delay. Remote video has access to past frames of context, but only predicts labels for its present view, and thus fares no better than remote image. Remote predictive is fed tunable delay and sustains accuracy by predicting the future. Local + remote predictive represents Dedelayed system, and is thus able to further sustain accuracy by merging the remote predictive features with fresh local features. While the above results demonstrate the dominance that Dedelayed is capable of, one should also account for the impact of local inference delay. We show this in Fig. 7 by assessing accuracy versus total latency. For local inference delays of 8 ms, the local + remote predictive method is consistently better across all network round-trip delays in terms of both accuracy and total latency."
        },
        {
            "title": "7 CONCLUSION",
            "content": "Dedelayed addresses central challenge in real-time systems that rely on remote computation: prediction staleness induced by network delay. It mitigates remote inference delay by elevating delay to first-class variable, conditioning the remote model via learnable delay embedding, and fusing remote features with fresh local features. Across realistic network conditions, Dedelayed surpasses strong local-only and remote-only baselines, with particular advantage for longer latencies and high-motion content. As foundational framework, Dedelayed applies to wide range of real-time problem domains, enabling intelligent systems that are not only accurate but also truly timely and dependable in dynamic environments. Future work includes studying variable and stochastic delay distributions, high-motion data, lighter local models, and local future prediction. 8 Figure 6: Segmentation accuracy (mIoU) versus round-trip latency (milliseconds or frames). Figure 7: Segmentation accuracy (mIoU) versus total latency (milliseconds or frames) for selected local model delays. Points are faded as round-trip latency increases. 4 and 8 ms were interpolated."
        },
        {
            "title": "REFERENCES",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical AI. arXiv preprint arXiv:2501.03575, 2025. João Ascenso, Elena Alshina, and Touradj Ebrahimi. The JPEG AI standard: Providing efficient human and machine visual data consumption. Ieee Multimedia, 30(1):100111, 2023. Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-JEPA 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. Horace Barlow et al. Possible principles underlying the transformation of sensory messages. Sensory communication, 1(01):217233, 1961. Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han. EfficientVIT: Lightweight multi-scale In Proceedings of the IEEE/CVF international attention for high-resolution dense prediction. conference on computer vision, pp. 1730217313, 2023. Kaiyuan Chen, Michael Wang, Marcus Gualtieri, Nan Tian, Christian Juette, Liu Ren, Jeffrey Ichnowski, John Kubiatowicz, and Ken Goldberg. FogROS2-LS: location-independent fog robotics framework for latency sensitive ros2 applications. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 1058110587. IEEE, 2024. Kaiyuan Chen, Nan Tian, Christian Juette, Tianshuang Qiu, Liu Ren, John Kubiatowicz, and Ken In 2025 IEEE Goldberg. FogROS2-PLR: Probabilistic latency-reliability for cloud robotics. International Conference on Robotics and Automation (ICRA), pp. 1629016297. IEEE, 2025. On-Road Automated Driving (ORAD) Committee. Taxonomy and definitions for terms related to driving automation systems for on-road motor vehicles. SAE international, 2021. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Jakob Engel, Kiran Somasundaram, Michael Goesele, Albert Sun, Alexander Gamino, Andrew Turner, Arjang Talattof, Arnie Yuan, Bilal Souti, Brighid Meredith, et al. Project Aria: new tool for egocentric multi-modal ai research. arXiv preprint arXiv:2308.13561, 2023. Ken Goldberg and Ben Kehoe. Cloud robotics and automation: survey of related work. EECS Department, University of California, Berkeley, Tech. Rep. UCB/EECS-2013-5, pp. 135, 2013. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1899519012, 2022. Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1938319400, 2024. Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 328339, 2018. ISO/IEC. ISO/IEC 23888: MPEG Artificial Intelligence (MPEG-AI), 2025. Parts: Part 2: Video coding for machines (VCM), Part 3: Optimization of encoders and receiving systems for machine analysis of coded video content, Part 4: Feature coding for machines (FCM), Part 5: AI-based point cloud coding. 10 Samvit Jain, Xin Wang, and Joseph Gonzalez. Accel: corrective fusion network for efficient In Proceedings of the IEEE/CVF Conference on Computer semantic segmentation on video. Vision and Pattern Recognition, pp. 88668875, 2019. D. H. Kelly. Information capacity of single retinal channel."
        },
        {
            "title": "IRE Transactions on Information",
            "content": "Theory, 8(3):221226, 1962. Tommie Kerssies, Niccolò Cavagnero, Alexander Hermans, Narges Norouzi, Giuseppe Averta, Bastian Leibe, Gijs Dubbelman, and Daan de Geus. Your ViT is secretly an image segmentation model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 25303 25313, 2025. URL https://arxiv.org/abs/2503.19108. Mehrdad Khani, Pouya Hamadanian, Arash Nasr-Esfahany, and Mohammad Alizadeh. Real-time video inference on edge devices via adaptive model streaming. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 45724582, 2021. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. OpenVLA: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open X-Embodiment: Robotic learning datasets and RT-X models: Open X-Embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 68926903. IEEE, 2024. Danfeng Qin, Chas Leichner, Manolis Delakis, Marco Fornoni, Shixin Luo, Fan Yang, Weijun Wang, Colby Banbury, Chengxi Ye, Berkin Akin, et al. MobileNetV4: universal models for the mobile ecosystem. In European Conference on Computer Vision, pp. 7896. Springer, 2024. Rajesh PN Rao and Dana Ballard. Predictive coding in the visual cortex: functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):7987, 1999. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211252, 2015. doi: 10.1007/s11263-015-0816-y. Evan Shelhamer, Kate Rakelly, Judy Hoffman, and Trevor Darrell. Clockwork convnets for video In European Conference on Computer Vision, pp. 852868. Springer, semantic segmentation. 2016. Vidya Srinivas, Malek Itani, Tuochao Chen, Emre Sefik Eskimez, Takuya Yoshioka, and Shyamnath In Proc. Interspeech 2024, pp. Gollakota. Knowledge boosting during low-latency inference. 43384342, 2024. Daniela Szwarcman, Sujit Roy, Paolo Fraccaro, Þorsteinn Elí Gíslason, Benedikt Blumenstiel, Rinki Ghosal, Pedro Henrique de Oliveira, Joao Lucas de Sousa Almeida, Rocco Sedona, Yanghui Kang, et al. Prithvi-EO-2.0: versatile multi-temporal foundation model for earth observation applications. arXiv preprint arXiv:2412.02732, 2024. Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan. Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. BDD100K: diverse driving dataset for heterogeneous multitask learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 CODE The fused local + remote predictive model is defined below: class FusedModel(nn.Module): def __init__(self, cls_classes=1000, seg_classes=19): super().__init__() self.local_model = MSTransformer2D(cls_classes, seg_classes) self.remote_model = EfficientViTSeg3D() self.mlp_pre_pool = PrepoolBlock() self.mlp_post_pool = PostpoolBlock() def forward(self, x_local, x_remote, delay): # Local feature extraction: = self.local_model.T1(x_local) _, _, H, = h.shape # Remote feature extraction and pooling: = x_remote = self.remote_model.forward_features(z, delay) = einops.rearrange(z, \"b w -> (c f) w\", f=4) = self.mlp_pre_pool(z) = F.adaptive_avg_pool2d(z, output_size=(H, W)) = self.mlp_post_pool(z) # Local and remote feature fusion: = + # Remaining local model: = = self.local_model.T2(h) = + self.local_model.P2(h) = self.local_model.T3(h) = + self.local_model.P3(h) = self.local_model.seg_head(y) return class EfficientViTSeg3D(nn.Module): def __init__(self, name=\"efficientvit-seg-l1-cityscapes\", seg_classes=19): super().__init__() self.image_model = create_efficientvit_seg_model(name) self.delay_embedding = DelayEmbedding() self.vit3d = nn.Sequential(*[ VitBlock3D(in_channels=256, head_dim=32, expand_ratio=4) for _ in range(12) ]) self.head = RemotePredictiveHead(seg_classes=seg_classes) def pool(self, x): x_pool = F.adaptive_avg_pool3d(x, output_size=(4, x.shape[-2:])) return einops.rearrange(x_pool, \"b w -> (c f) w\", f=4) def forward_features(self, x, delay): video_embedding = self.image_model.backbone(x) # shape: (B, C, F, H, W) delay_embedding = self.delay_embedding(delay, video_embedding.shape) return self.vit3d(video_embedding + delay_embedding) # For pretraining remote predictive model: def forward(self, x_remote, delay): return self.head(self.pool(self.forward_features(x_remote, delay))) class MSTransformer2D(nn.Module): \"\"\"Any 2D image segmentation model. We use one with T1, T2, T3 blocks.\"\"\" To train or evaluate the models, simply feed in frames separated by the appropriate delay and compute cross-entropy loss or mIoU in the typical fashion. 12 A.2 EFFECT OF DELAY JITTER We evaluate how our model performs under delay jitter, i.e., when the delay varies for each frame. Our model is trained only to maximize accuracy for fixed, tunable delay input and is not explicitly optimized for jitter. Nonetheless, temporal structure in the data allows it to retain accuracy even when the delay input differs from the observed delay. Fig. 8 characterizes this, showing performance under different observed delays when the model is force-fed possibly inaccurate delay as input. Accuracy peaks when the delay input matches the observed delay, and the drop is less sharp when the observed delay exceeds the delay input. The latter might be attributed to selection biasif the predicted features are relevant for future frame, they often remain useful for subsequent frames. Figure 8: Segmentation accuracy (mIoU) versus round-trip latency (milliseconds or frames). Here, the remote models delay input has been force-fed specific value. In Fig. 9, we report the above results in matrix form. Although our model was not explicitly trained for mismatched delays or delays beyond 5 frames, it continues to perform well under these conditions. This matrix can also be used to estimate the performance under delay jitter. We model the (µ = τin, σ2), centered at the delay input to the model, τin. The expected observed delay as τobs accuracy is obtained by taking weighted sum over mIoU values for each observed delay, using discretely binned normal probability mass function, with out-of-bounds mass assigned to the boundary bins. The resulting performance is shown in Fig. 10 for various values of σ. In many networks, σ = 5 ms and σ = 15 ms correspond to relatively high jitter. Yet, even for higher values of σ, our methods performance maintains its advantage, even assuming optimistically that the competing baselines experience no jitter. This shows that our method performs stably in realistic networks. For comparison, traditional remote inference loses 3.4% mIoU within the first frame of delay, drop far larger than the degradation our method incurs with the corresponding σ. 13 Figure 9: Segmentation accuracy (mIoU) over observed delay and model delay input. Figure 10: Segmentation accuracy (mIoU) versus round-trip latency (milliseconds or frames). Latency jitter is modeled as normal distribution. A.3 LOCAL INPUT RESOLUTION We evaluate on various local input resolutions. Our remote-assisted local model is able to operate at far lower resolutions without losing accuracy. Figure 11: Segmentation accuracy (mIoU) versus round-trip latency (milliseconds or frames). Further fine-tuned and evaluated on various local input resolutions."
        }
    ],
    "affiliations": [
        "InterDigital",
        "University of Texas at Austin"
    ]
}