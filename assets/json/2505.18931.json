{
    "paper_title": "Can Large Language Models Infer Causal Relationships from Real-World Text?",
    "authors": [
        "Ryan Saklad",
        "Aman Chadha",
        "Oleg Pavlov",
        "Raha Moraffah"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding and inferring causal relationships from texts is a core aspect of human cognition and is essential for advancing large language models (LLMs) towards artificial general intelligence. Existing work primarily focuses on synthetically generated texts which involve simple causal relationships explicitly mentioned in the text. This fails to reflect the complexities of real-world tasks. In this paper, we investigate whether LLMs are capable of inferring causal relationships from real-world texts. We develop a benchmark drawn from real-world academic literature which includes diverse texts with respect to length, complexity of relationships (different levels of explicitness, number of events, and causal relationships), and domains and sub-domains. To the best of our knowledge, our benchmark is the first-ever real-world dataset for this task. Our experiments on state-of-the-art LLMs evaluated on our proposed benchmark demonstrate significant challenges, with the best-performing model achieving an average F1 score of only 0.477. Analysis reveals common pitfalls: difficulty with implicitly stated information, in distinguishing relevant causal factors from surrounding contextual details, and with connecting causally relevant information spread across lengthy textual passages. By systematically characterizing these deficiencies, our benchmark offers targeted insights for further research into advancing LLM causal reasoning."
        },
        {
            "title": "Start",
            "content": "Can Large Language Models Infer Causal Relationships from Real-World Text? Ryan Saklad1, Aman Chadha*2, Oleg Pavlov1, and Raha Moraffah1 1Worcester Polytechnic Institute 2Amazon Gen AI 5 2 0 2 5 2 ] A . [ 1 1 3 9 8 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Understanding and inferring causal relationships from texts is core aspect of human cognition and is essential for advancing large language models (LLMs) towards artificial general intelligence. Existing work primarily focuses on synthetically generated texts which involve simple causal relationships explicitly mentioned in the text. This fails to reflect the complexities of real-world tasks. In this paper, we investigate whether LLMs are capable of inferring causal relationships from real-world texts. We develop benchmark drawn from real-world academic literature which includes diverse texts with respect to length, complexity of relationships (different levels of explicitness, number of events, and causal relationships), and domains and sub-domains. To the best of our knowledge, our benchmark is the firstever real-world dataset for this task. Our experiments on state-of-the-art LLMs evaluated on our proposed benchmark demonstrate significant challenges, with the best-performing model achieving an average F1 score of only 0.477. Analysis reveals common pitfalls: difficulty with implicitly stated information, in distinguishing relevant causal factors from surrounding contextual details, and with connecting causally relevant information spread across lengthy textual passages. By systematically characterizing these deficiencies, our benchmark offers targeted insights for further research into advancing LLM causal reasoning."
        },
        {
            "title": "Introduction",
            "content": "The ability to identify and understand causal relationships embedded within texts is fundamental aspect of human cognition (Pearl, 2009; Harari, 2014) and is crucial for complex decision-making. Humans excel at inferring these relationships when given text, even when they are not explicitly stated; for instance, from Raising prices might *Work done outside position at Amazon Gen AI. Implicit Businesses often find pricing challenging. Raising prices might boost revenue per item initially . . . Changes in prices risk alienating customers, thereby decreasing earnings and hurting profitability. Explicit Raising prices causes an increase of revenue per sale but decreases the number of buyers. Fewer buyers causes decrease in revenue. Price Number of Buyers Revenue Figure 1: An example causal graph illustrating fundamental economic relationships, showcasing how Price affects Number of Buyers, which in turn influences Revenue. The text example shows two example texts for the same graph, which highlights challenge in constructing causal relationship purely from text and underscores the need to detect both explicit and implicit cues for accurate causal graph generation. boost revenue per item initially... Changes in prices risk alienating customers humans can infer that raising the Price causally influences the number of customers. Large language models (LLMs) have shown remarkable progress, fueling aspirations towards artificial general intelligence (AGI). Given the importance of causality to intelligence, significant research has focused on assessing the causal reasoning capabilities of LLMs. Much of this work evaluates LLMs stored causal knowledge about events (Zhou et al., 2024) rather than their ability to infer relationships from textual contexts. While some studies examine extracting causal relationships from text (Veldhuis et al., 2024; Hosseinichimeh et al., 2024; Oh, 2025; Jin et al., 2024; Joshi et al., 2024a; Lasheras and Pinheiro, 2025), they often use synthetically generated or simplified texts with explicitly stated causal links. This approach, however, falls short of real-world scenarios where causal relationships are often embedded in long, complex texts, with varying degrees of explicitness, interconnected causal relationships, and diverse domains. Figure 1 illustrates toy example demonstrating this contrast: it shows that while synthetic text may present clear, explicit statement like Raising prices causes decrease in revenue, real-world texts often convey this far more implicitly, requiring moving beyond mere comprehension to extract the causal structure. To address this gap, we introduce ReCAST (Realistic Causal Structure from Text), novel benchmark drawn from real-world academic literature. ReCAST features diverse texts varying in length, complexity of relationships (including different levels of explicitness, number of events, and causal relationships), and spanning multiple domains and sub-domains, mirroring the challenges of real-world causal reasoning from text. Our experiments on state-of-the-art LLMs using ReCAST demonstrate significant struggles, with the best models achieving an average F1 score of only 0.477. Our results reveal common pitfalls: difficulty with implicitly stated information, distinguishing relevant causal factors from contextual details, and connecting causally relevant information spread across lengthy texts. By systematically characterizing these deficiencies, our benchmark offers targeted insights for further research into advancing LLM causal reasoning. Our main contributions are as follows: We propose, to the best of our knowledge, the first benchmark to evaluate the real-world causal reasoning abilities of LLMs from text. ReCAST offers benchmarking with varying text and graph sizes from real-world literature, with extensive filtering and post-processing to ensure quality. We further create LLM-as-aJudge for automated, fine-grained evaluation. Shows the limitations of LLMs at realistic causal reasoning from text across varied samples, including domains, text length, graph size, and degree of confounding."
        },
        {
            "title": "2 Related Work",
            "content": "Causal Reasoning with LLMs. As LLMs have grown in capabilities, significant research has explored their causal reasoning abilities. Existing benchmarks often assess these capabilities through various lenses. Some focus on identifying causal structures from pre-defined variable lists or datasets (as surveyed in (Kıcıman et al., 2024) or parts of CausalBench (Zhou et al., 2024)), largely bypassing complex textual inference. Other benchmarks utilize textual inputs, but frequently rely on synthetic or simplified narratives with explicit, often pairwise, relationships or for constructing relatively small graphs (e.g., CLadder (Jin et al., 2024), ExpliCa (Miliani et al., 2025), From text to map (Hosseinichimeh et al., 2024)). Other prior work evaluates causal understanding via question-answering, probing LLMs stored knowledge and their ability to deduce specific causal claims from short prompts or contexts (e.g., CaLQuest.PT (Lasheras and Pinheiro, 2025), COLD (Joshi et al., 2024a)). While some studies engage with real-world texts, they target more constrained tasks such as identifying single primary causal link (Oh, 2025) or classifying individual sentences for causality (Veldhuis et al., 2024), rather than requiring the synthesis of complete causal graph. These approaches, detailed further in Appendix A, generally do not address the challenge of causal graph construction under the full diversity and complexities of real-world conditions, where causality is often implicit and embedded within long narratives. ReCAST differs from prior work by directly targeting this gap, using academic literature as verified source data for causal reasoning from text under realistic conditions. Knowledge Graph Discovery with LLMs Recent years have witnessed substantial progress in knowledge graph construction frameworks with LLM-based approaches (Bratanic, 2024; Zhang and Soh, 2024; Yu et al., 2023). These methods convert unstructured text into structured formats by identifying entities, relationships, and additional attributes. However, they emphasize open-domain and factual information, differing from the unique challenges of causal reasoning."
        },
        {
            "title": "3 The ReCAST Dataset",
            "content": "In order to ensure that ReCAST is realistic causal reasoning benchmark, we need benchmark samples that have source text, ground-truth causal graph, and meet our realism criteria. The construction of the benchmark is achieved via 3-step pipeline: (i) Paper collection and filtering, (ii) Annotation, (iii) Post-processing. Next, we elaborate on each step."
        },
        {
            "title": "We focus ReCAST on the economics domain due\nto the unique characteristics of some of its causal",
            "content": "literature, which is especially well-suited for this benchmark. These papers are ideal for textbased causal benchmark, as they contain detailed textual descriptions of their goals, methodology, background, and causal reasoning, while avoiding reliance on non-textual elements (e.g., numerical data). Because each paper centers around single, human-authored causal graph, adheres to standardized structure, and articulates modeling assumptions, causal graphs can be accurately annotated, and construction from the text is feasible, even when elements are not mentioned explicitly. At the same time, economics is highly cross-disciplinary, spanning many domains, which offers substantial diversity while allowing for standardized quality control. Statistics detailing the distribution of academic domains within ReCAST are presented in Appendix B. To support large-scale collection, we utilize API access to economics papers. To ensure our corpus allows for legal redistribution of texts and diagrams, we restrict our search to the open access repositories (MDPI and PLOS). We use keyword search for the term causal loop diagram, commonly used term in economics literature to describe causal graphs, resulting in 646 candidate papers. We exclude workshop papers by removing papers whose abstracts include the keywords workshop or group model build, as these texts do not include sufficient details about the graph to make it identifiable. We then manually exclude all papers with graphs unsuitable for annotation, including ones with no or multiple unrelated causal graphs, purely illustrative or non-causal diagrams, and causal graphs with poor legibility or ambiguous graph elements. We retain only papers where there is single causal graph - either as the sole causal graph, or as the culmination of sequence of graphs - so that it can serve as the ground-truth causal graph for benchmarking. During the manual filtering, the location of each primary causal graph in the paper is labeled to ensure accurate annotation (e.g., Top diagram on page 7)."
        },
        {
            "title": "3.2 Annotation",
            "content": "As ReCAST is text-based benchmark, it is important that ground-truth causal graphs are converted from images to text-based representation to be used as the ground-truth answer. We find through manual evaluation that vision-based LLMs are unsuitable for annotation due to frequent hallucinations. Therefore, we employ domain experts to annotate each causal graph into standardized format (source_variable -> sink_variable). They are provided detailed instructions for the annotation process, including which graph elements to include, standardized output format, and step-bystep examples. To avoid potential errors, we further instruct annotators to flag any papers that cannot be unambiguously annotated given the instructions, which we manually review for suitability."
        },
        {
            "title": "3.3 Post-Processing",
            "content": "Graph Post-Processing To achieve ReCASTs goal of being realistic causal reasoning benchmark, it is essential that the ground-truth answers are accurate. While human annotators are highly accurate at transcribing causal graphs to our standardized textual format, they are prone to minute errors. To mitigate the risk of erroneous groundtruth answers, we utilize rigorous post-processing pipeline for ground-truth graphs. We first utilize code-based approaches to identify formatting mistakes, and attempt string matching approaches to automatically correct them. Any samples that are unable to be corrected for formatting are flagged and have the formatting manually corrected. To verify transcription quality, second annotator re-labeled 37 randomly chosen diagrams (778 edges, 616 nodes; 15% of ReCAST). We observed 22 missing and 5 spurious edges, giving edgelevel precision = 0.995, recall = 0.977, F1 = 0.986, and Cohens κ = 0.94. Variable labels showed 8 auto-correctable typos (e.g., carbondioxide) and 4 minor prefix/suffix omissions; no major name mismatches occurred. Full per-graph statistics and the Python script used to compute them are provided in Appendix J. We explore the possibility of utilizing code-based approaches for correction of variable naming, but find that it is prone to erroneously combining distinct variables (e.g., GDP and GNP). Therefore, we utilize LLMs for automated correction, which is detailed further in Appendix M. Text Post-Processing With the causal graphs finalized, it is important to ensure that the suitability of source texts. As ReCAST is text-based benchmark, the PDF must first be converted to textual format. As markdown is common format for LLMs, we choose this as our target for conversion. We use the Python library PyMuPDF to extract the raw text for each paper. The output of this step contains numerous formatting errors, such as arbitrary line breaks. We utilize multi-step LLM pipeline, as code-based approaches are infeasible given the diversity of documents, and manual testing shows current LLMs cannot perform this process accurately in one step. We first prompt Mistral Small (Team, 2025a) to convert from the PDF text to well-structured markdown (see Appendix N). The goal of this step is to remove non-textual elements (which are impossible to accurately represent in markdown) and entirely irrelevant elements (to streamline the task and save on computational costs). Therefore, Mistral is tasked to output the markdown autoregressively while skipping over non-textual elements (such as images, charts, or other figures), in-line citations, references, publication information, and appendices. The output of this step is well-formatted markdown version of the paper. Normalization Tool { \"normalizations\": [ {\"start\": \"text to find (start)\", \"end\": \"text to find (end)\", \"replacement\": \"new text\"}, ... ] } However, this may include information that makes the task trivial (such as table including all of the samples causal relations), or it may erroneously reference removed elements (resulting in an internally inconsistent document). To correct these issues, we utilize o3-mini (OpenAI, 2025) to remove explicit references to the causal graph and correct any references to missing elements using normalization tool. Explicit references to the graph are unrealistic (such as table listing every connection in the graph), as the task is to create causal graph that does not already exist. However, we are careful to not remove other information about the graph to avoid making it unidentifiable. To make these changes, we provide the model normalization tool to minimize output tokens, and unnecessary changes, while allowing for code-based validation."
        },
        {
            "title": "4 Experiments",
            "content": "We examine the following aspects of LLM performance on our benchmark: (i) ReCASTs ability to reveal LLM limitations across strong openand closed-source models, including both instructiontuned and reasoning-optimized variants, at varied realistic causal reasoning tasks; (ii) Ablation analysis to isolate sources of error, focusing on facAttributes Number of Samples Average Nodes per Graph Average Edges per Graph Average Length of Text Average Degree of Confounding per Node Number 292 24.99 37.37 40541.15 0.123 Table 1: ReCAST statistics. The average length of ReCAST is measured in characters for consistency. tors including the role of unobserved confounders, and varying sizes; and (iii) Performance of LLMs under name-assisted causal graph construction, where node identification is controlled for, isolating causal inference capabilities from entity extraction accuracy. In the following subsections, we first describe our experimental setup, including models, metrics, and formatting pipelines, and then present results under both the standard and ablation settings. Additional implementation details and extended analysis can be found in the Appendix1."
        },
        {
            "title": "4.1 Evaluation Details",
            "content": "To verify ReCASTs effectiveness, we first select state-of-the-art LLMs of varying model sizes and types for evaluation. Specifically, as we are interested in examining the performance of LLMs for real-world causal reasoning, we aim for diverse models to evaluate. To evaluate each open-source LLMs, we employ its official prompt format for each test sample. During generation, we use the hyperparameters recommended by its creator to ensure we utilize strong baseline. We also set the max tokens to 100,000 to ensure that models are not penalized for utilizing long chain-ofthoughts. For closed source models, we retry on failure, to ensure that all models generate valid answers. Due to the nature of ReCAST, some samples are extremely long, and in rare cases may exceed models maximum context window; we do not include these failed answers to avoid artificially depressing scores. We also provide the LLM the number of expected nodes, as fixing isolates the causal graphs abstraction from the causal reasoning objective. Without doing so, there are many valid levels of granularity for the causal graph, greatly complicating automating grading, and creating confounder for the causal reasoning task. Evaluated Models. We evaluate 5 recent LLMs, 1Our code can https://anonymous.4open.science/r/CausalBenchmarkE0C6. found be at varying in size and training on our task, below: Model Access Size Type Closed o3-mini OpenAI Open R1 et al. Open Qwen-32B Team QwQ Team Open Llama-8B Grattafiori et al. Open 685 32 32 8 Reasoning Reasoning Instruct Reasoning Instruct Table 2: Summary of Evaluated LLMs. Size in billions of parameters."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "Traditional Metrics. Evaluating causal graph construction from text poses unique challenges not addressed by standard metrics in causal discovery. Traditional approaches assume structured inputs and rely on exact variable and edge matches. Common metrics like Structural Hamming Distance (SHD), precision, recall, and F1 require that variable names are exact matches, which will rarely hold for this task. For example, person and human are semantically nearly identical, but are not exact matches as variable names. Therefore, binary metrics are poorly suited for this task. Given the automated evaluation aims to compare generated graph with ground-truth graph, Graph neural networks (GNNs) seem like natural choice for graph similarity evaluation. However, we find that it is poor choice for this task, due to lack of access to the source text, lack of fine-grained output, and lack of differentiation in performance when the task is simplified. We provide further analysis of GNN efficacy for this evaluation task, including quantitative results, in Appendix E. LLM-as-a-Judge Metrics. To address these limitations, we introduce LLM-as-a-Judge automated evaluator. It provides fine-grained feedback using the prompt and grading calculations as detailed in Appendix and Appendix U. The prompt for the LLM judge ensures several properties are exhibited during grading. There may be varying levels of abstraction in the graph; it is important that model is rewarded for including relationship at all, and using the proper level of abstraction is rewarded increasingly as it gets closer to the ground-truth graph. There are many choices when making graph that are merely stylistic; model should not be penalized for this, even if it was different from the ground-truth (most commonly with variable naming). We utilize varying labels to allow for grading on the level of individual nodes and edges, and aggregate these metrics across the entire graph. We also aim to minimize overlap between categories, so that performance in one area does not contaminate judging in another. Additionally, it is important to use the proper level of criticism when grading; missing core relationship to graph or getting the causal logic wrong is much more severe issue than choosing different variables to include on the periphery of the graph. This also allows other factors like hallucinations, reversing causality, and other factors to be appropriately incorporated into overall metrics while minimizing ambiguities in grading. The LLM judge, R1, is provided with the source text and ground-truth graph, and outputs categorical assessment across several criteria for each node and edge. These are aggregated across all nodes and edges for each graph, which can be used to calculate traditional binary metrics, as detailed below. Precision, Recall, F1: Measured on the directed edges (u, v) in the predicted and reference graphs. Structural Hamming SHD(Gtrue, Gpred) = #Adds + #Dels + #Revs"
        },
        {
            "title": "Distance",
            "content": "(SHD): Normalized SHD: Normalized SHD = SHD n(n1) where is the number of unique nodes. Metrics are computed using adjacency matrices with CausalDiscoveryToolbox. Formatting and Normalization. We enforce strict JSON formatting for graph outputs to enable automated evaluation. Since models may introduce formatting errors, we apply post-processing step using Mistral Small (Team, 2025a) to convert malformed outputs into valid JSON while preserving the intended relationships. The correction prompt is shown in Appendix Q."
        },
        {
            "title": "5.1 Main Results",
            "content": "Table 3 presents the accuracy of all selected LLMs on the realistic causal graph generation from text task. We provide detailed metrics of performance, including precision and recall, F1, SHD, and Normalized SHD, with corresponding standard deviations. We report for nodes and edges as applicable. The first observation we have from Table 3 is that all evaluated models show relatively poor performance, with the best-performing model, R1, achieving just an average F1 of 0.477 across all samples, falling far short of ideal performance. However, Model Node Precision () Node Recall () Edge Precision () Edge Recall () F1 () SHD () Normalized SHD () QwQ Llama-8B Qwen-32B o3-mini R1 0.881 0.119 0.827 0.191 0.862 0.117 0.862 0.134 0.893 0.108 0.488 0.216 0.359 0.244 0.434 0.242 0.459 0.231 0.522 0.217 0.802 0.201 0.677 0.265 0.747 0.231 0.806 0.197 0.817 0.183 0.242 0.200 0.125 0.160 0.181 0.170 0.208 0.185 0.260 0.196 0.450 0.193 0.302 0.201 0.381 0.200 0.415 0.200 0.477 0. 36.860 21.793 41.197 23.631 40.602 23.886 38.481 22.120 38.193 22.491 0.107 0.084 0.120 0.102 0.112 0.089 0.107 0.090 0.105 0.097 Table 3: Comparison of different models performance on ReCAST (mean standard deviation). Best is underlined. despite poor overall performance, models vary substantially. Llama-8B performs noticeably worse than all other models across all metrics. Comparing across metrics, models perform in roughly the same ranking for each, showing broad agreement for overall performance. We note that the standard deviation is high across all metrics. We attribute this variable performance primarily to the realistic nature of the benchmark. Sample difficulty will vary based on factors including degree of confounding, number of nodes, number of edges, text size, and domains mimicking the diverse conditions for real-world causal reasoning. All models also exhibit significantly lower recall than precision, showing that models have an easier time generating nodes and edges that are correct, but not necessarily the same as the ground-truth graph. Raw SHD scores are high, as expected, due to the large size of graphs. However, normalized SHD is low, due to most graphs being sparse. We also find that there is relationship between model size and performance. The worstperforming model, Llama-8B, is just 8 billion parameters, while the best performing model is R1, with 685 billion. There is also relationship between model performance and model type, as reasoning models consistently outperform instruction models. For example, QwQ is reasoning-trained version of Qwen-32B with the same underlying model architecture, and outperforms the instruct model on every metric."
        },
        {
            "title": "Confounding",
            "content": "We investigate whether poor overall performance is due to succeeding on easier samples while struggling on harder samples. To do so, we analyze the degree of confounding, which shows how much information source text contains about its graph. To measure this, we use the following equation, which is calculated as the percentage of nodes in graph that are not explicitly mentioned in text. DC = 1 (cid:88) vV (cid:40) 1, 0, if / E, if Degree of Confounding is an important attribute of samples, as it is natural measure of difficulty, as nodes that are explicitly mentioned are easier to identify than entirely unobserved confounders. We use R1 to label the degree of confounding for each node in each sample, providing it the ground-truth graph and source text as reference. Each node is classified as explicit (the node or synonym is explicitly described in the text) or not (the node appears implicitly or indirectly, or does not appear in the text). We calculate degree of confounding for each sample using the formula detailed above, and average over all samples. Figure 2: Heatmap of the average model scores across confounding bins (lenient definition), showcasing how degree of confounding has large impact on model performance. It underscores LLMs struggles to infer causal relationships when explicit references are sparse. Using this information, the samples are categorized into four categories, ensuring that each has sufficient samples to show the effect of confounding. We explicitly do not split into evenly sized regions, as samples with extremely high degrees of confounding are unrealistic, and hence do not appear in ReCAST. As shown in Figure 2, there is strong trend between samples degree of confounding and model performance (reported using F1). For example, the lowest-confounding bin (<10%) the best model (R1) achieves an average Model Precision () Recall () F1 () SHD () Normalized SHD () 0.513 0.202 R1 0.219 0.150 Llama-8B 0.455 0.203 o3-mini Qwen-32B 0.290 0.171 0.485 0.197 QwQ 0.131 0.101 Random 0.512 0.072 0.409 0.054 0.491 0.072 0.435 0.063 0.502 0.071 0.379 0.040 0.502 0.146 0.267 0.136 0.459 0.153 0.332 0.145 0.483 0.143 0.179 0.099 40.558 35.173 58.561 50.222 40.654 37.081 50.952 36.189 40.290 32.571 67.809 44.913 0.085 0.066 0.142 0.110 0.086 0.069 0.123 0.085 0.085 0.063 0.158 0.093 Table 4: Aggregate name-assisted metrics: precision, recall, F1, SHD, and normalized SHD. Performance is poor despite providing node names, highlighting that reasoning is required for the task beyond node identification. F1 of 0.57, yet this reduces by almost half to 0.31 for samples where more than half of the nodes are confounding. This trend shows that while models struggle for all degrees of confounding, they especially struggle when they must use causal reasoning to infer information from the text. This finding is corroborated by analysis using an alternate, stricter definition of confounding (see Appendix F)."
        },
        {
            "title": "5.3 Evaluation under Varying Input Sizes",
            "content": "We also investigate the effect of input size on performance. This includes the length of the source text, number of edges in the ground-truth graph, and number of nodes in the ground-truth graph. We report the difference in performance between the small and large samples in Table 5. We measure length using number of characters rather than tokens to standardize comparison. Counterintuitively, there is positive trend between increasing size and model performance. We investigate this by analyzing the relationship between degree of confounding and size. Due to the large variation in size, we analyze using log scale. We find that text size, number of nodes, and number of edges each have small, negative correlation with degree of confounding (with R2 values of 0.187, 0.002, and 0.006 respectively). This explains some of this trend given the strong relationship between degree of confounding and performance. We include charts visualizing this relationship in Appendix H. Model Text Edges Nodes QwQ Llama-8B Qwen-32B o3-mini +0.016 0.284 0.013 0.283 +0.058 0.287 +0.031 0.285 +0.037 0.250 +0.004 0.266 +0.010 0.281 0.129 0.293 0.061 0.298 +0.003 0.290 +0.055 0.266 0.000 0.266 0.088 0.287 +0.055 0.283 +0.016 0.254 Table 5: Difference in model performance (mean std) between the top and bottom quartiles of text length, edge count, and node count. Positive values indicate improved performance on larger instances."
        },
        {
            "title": "Generation",
            "content": "To investigate the extent to which poor model performance is explained by node identification or deeper causal reasoning, we conduct an ablation experiment in which models are explicitly provided with the complete set of ground-truth node names. Under these conditions, evaluation is deterministic, isolating the effects of LLM-based grading and node identification from that of edge causality. Table 4 shows results from this controlled scenario. While models trivially achieved perfect node-level precision and recall by design, the expected improvements in causal inference were limited: edge-level F1 slightly improved for stronger models such as R1 (by only +0.025) and o3-mini (by +0.044). Meanwhile, weaker models like Qwen-32B and Llama-8B saw reductions in performance, declining by 0.049 and 0.035 respectively. SHD similarly showed minor improvements, suggesting that explicitly provided node schemas do not have consistent effect on performance. Though the deterministic evaluation and LLMas-a-Judge approaches are not directly comparabledue to the differing tasks and the fine-grained vs binary assessments of each measureboth evaluation approaches show similar results. Despite removing entity recognition uncertainty, models continue to perform poorly at correctly inferring causal relationships, with causal reasoning as fundamental bottleneck. The consistent findings between both evaluation methods lends credibility to the effectiveness of the LLM judge. This ablation highlights that the poor performance of LLMs on ReCAST is due to fundamental limitations in their causal reasoning capabilities rather than being limited to errors in surface-level entity recognition."
        },
        {
            "title": "5.5 Case Study - R1 Model Answer",
            "content": "To illustrate model performance on realistic causal reasoning, we show benchmark sample about livelihood efficiency in the Qinba Mountains. We R1 Chain-of-Thought Excerpt . . . In the Materials and Methods, they mention. . . auxiliary variables (savings, non-farm income, per capita arable land), and constants (region area, precipitation, etc.). . . Countys arable land area (18) -> Cash crop cultivation (10) (if less land, shift to cash crops). . . Cash crop cultivation (10) -> Farmers income (maybe part of non-farm income, but non-farm income is separate. Cash crop would be part of farm income. . . county infrastructure and loan access. . . - Loan usage flexibility -> Diversified investments (cash crops, etc.) financial increases capital lead to Figure 3: Verbatim excerpts from reasoning trace. Ellipses added for readability and bold for emphasis. focus on subset of the graph that shows the causal relationship between various climate and land factors with grain production and overall economic output. We analyze the performance of R1 on this representative subgraph. The text describes these relationships most clearly in this passage: Land and climate are the fundamental conditions for agricultural production. . . . therefore, sunshine, precipitation, and arable land area were selected to represent the natural capital of the county. Even for this straightforward description, R1 fails to correctly include this relationship in its answer. As shown in bold in Figure 3, while R1 initially correctly identifies that precipitation is relevant factor, it thereafter overlooks inclusion of every climate driver in its final causal graph. It does not include Annual precipitation or Annual sunshine hours despite their explicit mention as fundamental drivers. Similarly, it mentions how cash crop cultivation is part of farm income, which is similar to GDP, but fails to elaborate on this. It also swaps the broader variable Total grain output for narrow focus on Cash crop cultivation. It generates an erroneous link between loan usage flexibility and cash crop cultivation; however, loan flexibility relates to financial capital and coordination mechanisms, not crop selection decisions. As shown in Figure 4, of the four ground-truth edges relevant to its subgraph, the model captures only one relationship partially by oversimplifying, omits three entirely, while adding one another erroneously. This shows how, even with straightforward textual cues, evaluated LLMs still struggle at causal reasoning. We include complementary Countys arable land area Cash crop cultivation Loan usage flexibility Annual precipitation Annual sunshine hours Total grain output County GDP Arable land in county Figure 4: Top: R1 model subgraph, with the spurious link in red. Bottom: the ground-truth causal subgraph. human expert case study for this sample in Appendix D."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "Understanding causal relationships from text, core aspect of human cognition, is essential for advancing LLMs towards artificial general intelligence. However, evaluating this skill under realworld conditions has been limited by the lack of appropriate benchmarks. This paper introduced ReCAST, which is, to our knowledge, the first benchmark to assess LLM causal reasoning capabilities from text under realistic conditions. ReCAST draws diverse samples from academic literature, featuring texts varied in length, relational complexity, and domain, thereby incorporating the challenges of real-world causal reasoning. Extensive experiments utilizing ReCAST revealed that state-of-the-art LLMs struggle significantly with this task, with the best-performing model achieving an average F1 score of just 0.477. In-depth analysis identified common pitfalls, including challenges with implicitly stated information and distinguishing relevant causal factors from surrounding contextual details across lengthy passages. Notably, performance remained poor even under low degrees of confounding or when node identification was simplified, indicating the primary bottleneck lies in causal reasoning itself. ReCAST offers robust platform for systematically characterizing these deficiencies. It serves as valuable tool for the research community, enabling more targeted investigations into LLM limitations. These insights directly inform the development of nextgeneration models, advancing efforts towards more sophisticated causal reasoning abilities in LLMs. identifiable information (PII) besides name was collected (to assign credit), which has since been destroyed, guaranteeing complete anonymization. Participants were explicitly informed that their anonymized annotation data would be used for research use. Given this de-identification, formal IRB review was not required. Use of AI Assistants AI assistants aided in literature review, drafting, coding, and data processing, under human direction and review. Benchmark Responsible Use ReCAST consists of English-only academic texts (primarily in economics), which may introduce linguistic or domainspecific biases. The ReCAST dataset, consistent with its source data, is intended for research. While advancements in AI causal reasoning could be misused, ReCAST is intended to foster responsible research and development of beneficial AI systems. We are committed to transparency and will publicly release ReCAST and associated code. Source academic papers are used under their original openaccess licenses (details with dataset release)."
        },
        {
            "title": "7 Limitations",
            "content": "While ReCAST establishes valuable methodology for assessing realistic causal graph generation from text, several limitations should be noted. Firstly, the ground-truth causal graphs are derived from existing economics literature. While these sources are peer-reviewed and provide authentic complexity, they may reflect domain-specific simplifications or particular theoretical assumptions inherent to the original research. This establishes practical upper bound on achievable performance by any model and means the evaluation is against one specific interpretation of causality within that text. Secondly, the use of open-access source documents, essential for benchmark accessibility, introduces the possibility that some texts were part of the evaluated LLMs pre-training data. Our analysis regarding knowledge cutoffs (Appendix K) suggests this did not significantly impact results for the models tested, but it remains general consideration as the precise contents of large pre-training corpora are often opaque. Thirdly, while benchmark samples are drawn from diverse sub-domains within economics, the benchmarks overall focus on this single academic discipline may still limit the diversity of causal phenomena and linguistic expressions of causality encountered. This could influence the direct generalizability of findings to fields with different conventions. Fourthly, our methodology incorporates LLMs for several post-processing steps and, crucially, for the LLM-as-a-Judge evaluation. While efforts were made to ensure robustness (e.g., through specific prompting, tool use, and ablations with deterministic evaluation), this introduces the potential for LLM-specific biases or inaccuracies in these automated steps. For instance, the LLM Judges assessment, despite careful calibration, might subtly favor certain graph characteristics. Finally, the task is simplified by providing models with the target number of nodes for the causal graph, which differs from the fully unconstrained nature of some real-world causal discovery scenarios where identifying the relevant variables and their appropriate level of abstraction is key part of the challenge."
        },
        {
            "title": "8 Ethics Statement",
            "content": "Annotator Data Ground-truth annotations for ReCAST were sourced from economics students participating for opt-in extra credit. No personally Sangmin S. Oh. 2025. Plausibly exogenous galore. Notion. Available at https://sangmino.notion. site/1a897b8106ca44eeaf31dcd5ae5a61b1?v= ff7dc75862c6427eb4243e91836e077e. OpenAI. 2025. Openai o3-mini system card. OpenAI, January 31, 2025. https://cdn.openai.com/ o3-mini-system-card.pdf. Judea Pearl. 2009. Causality. Cambridge University Press, Cambridge. Mistral AI Team. 2025a. Mistral small 3. Qwen Team. 2024. Qwen2.5: party of foundation models! Qwen Team. 2025b. Qwq-32b: Embracing the power of reinforcement learning. G. A. Veldhuis, D. Blok, M. H. T. de Boer, G. J. Kalkman, R. M. Bakker, and R. P. M. van Waas. 2024. From text to model: Leveraging natural language processing for system dynamics model development. System Dynamics Review, 40(3): e1780. Duo Wang, Yuan Zuo, Fengzhi Li, and Junjie Wu. 2024. Llms as zero-shot graph learners: Alignment of gnn representations with llm token embeddings. Preprint, arXiv:2408.14512. Khurram Yamin, Shantanu Gupta, Gaurav R. Ghosal, Zachary C. Lipton, and Bryan Wilder. 2024. Failure modes of llms for causal reasoning on narratives. Preprint, arXiv:2410.23884. S. Yu, T. Huang, M. Liu, and Z. Wang. 2023. BEAR: Revolutionizing service domain knowledge graph construction with llm. In Monti, F., Rinderle-Ma, S., Ruiz Cortés, A., Zheng, Z., and Mecella, M. (Eds.), Service-Oriented Computing. ICSOC 2023. Lecture Notes in Computer Science, vol 14419, pp. 339346. Springer, Cham. B. Zhang and H. Soh. 2024. Extract, define, canonicalize: An llm-based framework for knowledge graph construction. arXiv preprint arXiv:2404.03868. Yu Zhou, Xingyu Wu, Beicheng Huang, Jibin Wu, Liang Feng, and Kay Chen Tan. 2024. Causalbench: comprehensive benchmark for causal learning capability of llms. Preprint, arXiv:2404.06349."
        },
        {
            "title": "References",
            "content": "T. Bratanic. 2024. Building knowledge graphs with llm graph transformer: deep dive into langchains implementation of graph construction with llms. Towards Data Science. DeepSeek-AI et al. 2025. DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Yuval Noah Harari. 2014. Sapiens: Brief History of Humankind. Random House, New York. Niyousha Hosseinichimeh, Aritra Majumdar, Ross Williams, and Navid Ghaffarzadegan. 2024. From text to map: system dynamics bot for constructing causal loop diagrams. System Dynamics Review, 40(3):e1782. Z. Jin, Y. Chen, F. Leeb, L. Gresele, O. Kamal, Z. Lyu, K. Blin, F. G. Adauto, M. Kleiman-Weiner, M. Sachan, and B. Schölkopf. 2024. CLadder: Assessing causal reasoning in language models. https: //arxiv.org/abs/2312.04350. Abhinav Joshi, Areeb Ahmad, and Ashutosh Modi. 2024a. Cold: Causal reasoning in closed daily activities. Preprint, arXiv:2411.19500. Nitish Joshi, Abulhair Saparov, Yixin Wang, and He He. 2024b. Llms are prone to fallacies in causal inference. Preprint, arXiv:2406.12158. Hyunjung Kim and David F. Andersen. 2012. Building confidence in causal maps generated from purposive text data: mapping transcripts of the federal reserve. System Dynamics Review, 28(4):311328. Emre Kıcıman, Robert Ness, Amit Sharma, and Chenhao Tan. 2024. Causal reasoning and large language models: Opening new frontier for causality. Preprint, arXiv:2305.00050. Uriel Anderson Lasheras and Vladia Pinheiro. 2025. CaLQuest.PT: Towards the collection and evaluation of natural causal ladder questions in Portuguese for In Proceedings of the First Workshop AI agents. on Language Models for Low-Resource Languages, pages 325343, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Martina Miliani, Serena Auriemma, Alessandro Bondielli, Emmanuele Chersoni, Lucia Passaro, Irene Sucameli, and Alessandro Lenci. 2025. Explica: Evaluating explicit causal reasoning in large language models. Preprint, arXiv:2502.15487."
        },
        {
            "title": "Reasoning Benchmarks",
            "content": "This section provides comparative overview of ReCAST against existing benchmarks for causal reasoning. While each benchmark contributes to understanding LLM capabilities, ReCAST is specifically designed to test the construction of complex causal graphs from long-form, real-world academic texts. We evaluate benchmarks based on their diversity and complexity. In this context, we consider benchmark diverse when it spans many domains or sub-domains, or draws data from many different types of sources. Meanwhile, we deem sample complex when there are varied degrees of confounding in samples; that is, that the benchmark features many samples that are not highly explicit, requiring that causal reasoning, rather than mere reading comprehension, to be used. Lastly, we also examine the realism (of the source data) - is it drawn from the real-world? The following table offers high-level visual comparison, with further details on each benchmark discussed subsequently."
        },
        {
            "title": "Discussions of Selected Benchmarks",
            "content": "The following discussions provide context for the data presented in Table 6, highlighting their approaches and how we notably differ from this prior work. While these works are valuable, prior work fails to measure the causal reasoning abilities of LLMs from text under real-world settings. We highlight the approach of each benchmark, and how they compare to ReCAST. Several benchmarks concentrate on pairwise causal relations, include inputs that are highly explicit, or have inputs that are generated synthetically or are crafted as short texts by hand. We differ from these approaches by aiming to construct large graphs from real-world literature. ExpliCa (Miliani et al., 2025) examines how LLMs understand explicit connectives in sentence pairs, resulting in 2-node links. By design, it uses very short, often crafted inputs and focuses on explicit cues, thereby avoiding the complexities of implicit causality and information integration from extensive texts that ReCAST targets. Meanwhile, LLM Fallacies (Joshi et al., 2024b) employs short, synthetic scenarios to test pairwise causal inference, focusing on logical fallacies when LLMs are presented with explicit non-causal information. While this addresses specific type of reasoning complexity, its synthetic and brief inputs differ greatly from the real-world, extensive texts and broader graph construction task in ReCAST. Plausibly Exogenous Galore (Oh, 2025) serves as an interesting bridge, as it uses long economics documents similar to ReCAST. However, its task is to find only the main pairwise link for the entire document, which greatly limits its diversity and complexity. Another group of benchmarks attempt graph construction, but typically rely on short, simplified, or synthetic descriptions. These lack the depth and realism of long, real-world texts. From text to map (Hosseinichimeh et al., 2024) generate relatively small graphs (max 9-15 nodes) from concise, hand-crafted descriptions. Such inputs inherently limit textual diversity and likely feature more explicit causal links, sidestepping the challenge of parsing lengthy, nuanced documents with varying levels of confounding. Failure Modes of LLMs for Causal Reasoning on Narratives (Yamin et al., 2024) also uses short, often synthetic or CauseNet-derived narratives for constructing linear chain graphs (max 20 nodes). While it explores LLM biases and indirect effects, its input lacks the textual diversity and structural graph complexity of ReCAST, and its narratives are purpose-built rather than reflecting the reasoning of real-world conditions. Other benchmarks focus on sentence-level analysis, or offer broader surveys of causal tasks where individual components may use non-primary inputs or address different facets of reasoning. From Text to Model (Veldhuis et al., 2024) (NLP for SD) measures the ability of LLMs to classify individual sentences from real-world texts for causality. While it uses real-world text, it uses small excerpts, and avoids the complexities of reasoning over large documents. Causal Reasoning Survey (Kıcıman et al., 2024) provides wide-ranging overview of LLM capabilities across multiple causal tasks. However, its sub-tasks often use short or structured inputs (e.g., variable lists for graph construction, concise vignettes for reasoning), which differ from ReCASTs reliance on extensive, unmodified academic texts for end-to-end graph extraction. To the best of our knowledge, ReCAST is the first benchmark for LLMs to measure the causal reasoning abilities from diverse, complex, realworld texts. While previous benchmarking efforts have explored lengthy, diverse, and complex texts, causal reasoning, and real-world conditions separately, we are the first to do so at once. Input Type Max Nodes Diverse Complex Realism Benchmark Primary Task ExpliCa (Miliani et al., 2025) LLM Fallacies (Joshi et al., 2024b) Plausibly Exogenous (Oh, 2025) From text to map (Hosseinichimeh et al., 2024) Failure Modes (Yamin et al., 2024) From Text to Model (Veldhuis et al., 2024) Causal Reasoning Survey (Kıcıman et al., 2024) Multiple Tasks CLadder (Jin et al., 2024) CausalBench (Zhou et al., 2024) COLD (Joshi et al., 2024a) CaLQuest.PT (Lasheras and Pinheiro, 2025) Long Text Pairwise ID Pairwise Inference Pairwise ID Graph Construction ID/Graph Construction Sentence Classification Causal QA Causal Structure ID Causal QA Causal QA Sentences Scenario/Vignette Full Document Short Narratives Short Narrative Sentences Mixed Narrative/Question Query/Question Query/Question Query/Question 2 2 2 15 20 N/A Varies 4 109 33 N/A ReCAST (Ours) Graph Construction Full Document 140 . . . . . . . . . . . Table 6: Visual comparison of ReCAST with other causal reasoning benchmarks. Primary Task (e.g., Graph Construction: Graph Construction; Pairwise ID: Pairwise Identification; QA: Question Answering). Long Text: Indicates if the benchmark primarily uses long textual inputs. Input Type (e.g., Document, Narrative, Scenarios, Queries, Sentences). Max Nodes: Maximum nodes per instance for graph construction or the underlying model. Symbols for realism criteria (Diverse, Complex, Realism): : Fully meets criterion; .: Partially meets criterion; : Does not substantially meet criterion, relative to ReCASTs focus on long-text graph extraction."
        },
        {
            "title": "B Domain Statistics",
            "content": "Model 01 Domains 2 Domains 3+ Domains Base Accuracy 0.493 0.176 R1 0.461 0.178 QwQ 0.409 0.192 o3-mini Qwen-32B 0.375 0.187 0.323 0.213 Llama-8B 0.457 0.208 0.446 0.210 0.421 0.213 0.388 0.215 0.290 0.199 0.499 0.125 0.416 0.164 0.415 0.172 0.364 0.187 0.262 0.140 0.477 0.125 0.450 0.194 0.415 0.201 0.381 0.201 0.302 0.201 Table 7: Model performance by number of associated domains per sample, showcasing how models perform across different domain complexities, classified as 01, 2, or 3+ co-domains. Notably, R1 achieves the highest accuracy in single-domain samples, suggesting that models may struggle with cross-domain reasoning. Conversely, QwQ and o3-mini perform slightly better with increased domain diversity, reflecting some adaptability to multi-domain causal reasoning. To be able to assess the domain coverage of ReCAST, we first manually create broad domain categories, aiming to minimize overlap between domains while simultaneously ensuring that all samples belong to at least one domain. It is valuable to go beyond binary classification (economics or not), as information about domains in the samples is valuable for analysis of model performance. We manually prompt engineer to create the domain categories, aiming to minimize overlap between domains and ensure that each sample falls into at least one domain. To generate initial domain categories before manual optimization, we prompt LLMs to generate categories based on all paper titles. To minimize the risk of false positives, we aim for conservative domain labeling. For example, we choose the name Environmental & Earth Sciences for domain #5 to ensure that economics papers about sustainability are correctly classified into domains #2 and #5. We use R1 model to automatically classify each sample by domain, as manual validation showed high classification accuracy and adherence to output formatting. Each papers title and abstract as context to ensure sufficient context to classify while minimizing total tokens. We use the following prompt: Domain Classification Prompt You are an expert at correctly labeling domains. You will be given published papers title and abstract. You will label each other domain of the paper based on the content. You may pick more than one domain when applicable. Only choose domains from the list below. Choose every domain that is present in the paper. There may not be any domains from the options present in the paper. Respond with the numbers corresponding to the domains in JSON list with no other text. The domains are: 1: Agriculture & Food Systems 2: Economics & Public Policy 3: Education 4: Engineering & Technology 5: Environmental & Earth Sciences 6: Medicine EXAMPLE INPUT: Title: \"The impact of AI on job markets\" Abstract: \"This paper explores the impact of AI on job markets and the future of work.\" EXAMPLE OUTPUT: { \"domains\": [2, 4] } Each sample that it classified as non-economics is manually reviewed for accuracy, with correctly classified non-economics samples excluded from the benchmark. We manually review each sample classified as non-economics for accuracy to ensure valid benchmark samples are not erroneously excluded. After excluding non-economics papers, we finalize the benchmark samples, resulting in 292 included samples. Figure 5 shows the distribution of the number of non-economics domains for each sample, with an average of 1.67 non-economics domains per sample. We note that it is expected that almost no samples had no non-economics domains, as economics is inherently multidisciplinary. As previously described, each sample in ReCAST belongs to Economics and at least one additional domain. On average, each sample spans 2.35 domains, with Engineering & Technology and Environmental & Earth Sciences being the most prevalent. Performance is largely stable across samples with 0-1, 2, or 3+ co-domains. Interestingly, R1 scores highest on single-domain samples, suggesting less cross-domain complexity may support In contrast, QwQ and stronger graph alignment. o3-mini slightly improve with increased domain count, showing some benefit from interdisciplinary contexts. Overall, these results emphasize the importance of domain-sensitive evaluation. Variation in model performance across domains confirms that ReCAST captures diverse economic sub-domains in way that differentiates LLM capabilities under realistic conditions. Figure 5: Distribution of domain counts per sample (n = 292), illustrating the multidisciplinary nature of economics-based samples in ReCAST. Most samples are associated with multiple domains, highlighting the inherent complexity of real-world economic causal reasoning. Figure 6: Radar chart of domain-specific accuracies for each model, depicting the accuracy of different models across specific domains. Variability in performance across domains highlights the contextual challenges faced by models in domain-specific causal inference, pointing towards the need for domain-sensitive model adjustments. Agriculture & Food Systems, where QwQ (Team, 2025b) narrowly outperforms it. However, all models perform poorly overall regardless of domain, showing that this is not the factor that leads to poor causal reasoning. Figure 7 visualizes how often each domain appears in the benchmark samples. Engineering & Technology and Environmental & Earth Sciences comprise the majority of samples, appearing in over 60% of samples. As these domains appear more frequently for PLOS and MDPI economics papers, it is not surprising that they compose an outsized portion of the dataset. Additionally, we intentionally choose broad categories for domains to ensure sufficient samples for analysis, as there is substantial diversity within each domain. Even for smaller domains, there are sufficient samples to analyze performance. We also include the distributions of journals the samples were originally published in as an alternative measure of domain. As we intentionally choose broad domain categories, this can act as measure of sub-domain."
        },
        {
            "title": "C Effect of Length of Reasoning Trace",
            "content": "While domain does have an effect on performance, the magnitude of such effect is not large. This is not surprising, as LLMs have seen these domains many times before during training. As shown in Figure 6, R1 (et al., 2025) boasts the highest performance across all domains, except for We investigate how the length of chain-of-thought reasoning affects model performance. For each open-source reasoning model, we split data into one thousand token wide bins, and display the quantities of each token amount in Figure 9 and Figure 10. Interestingly, QwQ has some reasoning traces which are far longer than the longest reasoning traces from R1, which we attribute to the different training for each of these models. Additionally, manual inspection showed that some of these longer traces were due to repeatedly making small changes to formatting, which indicates that these responses did not spend more time on actual reasoning, and may have largely been due to QwQs worse performance at formatting. Figure 7: Domain distribution across 292 samples with processed text (excluding economics). Figure 9: QwQ token length distribution. The data suggests that QwQ maintains relatively consistent distribution across token lengths, reflecting its structured handling of reasoning chains. However, performance did not scale proportionally with longer traces, indicating limitations in handling extended reasoning efficiently. Figure 8: Distribution of journal for all ReCAST benchmark samples. Figure 10: Distribution of reasoning trace lengths (in tokens) for R1 across benchmark samples. The model tends to produce mid-length reasoning traces (2,0007,000 tokens), with very few exceeding 12,000 tokens."
        },
        {
            "title": "D Human Domain Expert Case Study",
            "content": "While large language models struggle to construct accurate causal graphs under realistic conditions, this task is feasible for human experts. To demonstrate this, we conduct small-scale case study where an expert economist constructs causal graph from the same input text used in the R1 model case study (subsection 5.5). Due to the highly time-intensive nature of human causal graph construction and annotation (Kim and Andersen, 2012), we include only this case study for this sample. We show only representative subgraph slice of their answer to allow for direct comparison, as full graphs are too large to allow for concise comparisons. As shown in Figure 11, the expert successfully captures the three core causal drivers: annual precipitation, annual sunshine hours, and County arable land area. These are all mapped to unified economic outcome, total household income, reflecting broaderbut validabstraction of the two-hop causal path present in the ground-truth graph (Total grain output County GDP). This demonstrates that the task is tractable for humans, and the importance of evaluation that differentiates stylistic variations from mistakes in the core causal logic. annual precipitation annual sunshine hours total household income County arable land area Annual precipitation Annual sunshine hours Total grain output County GDP Arable land in county Figure 11: Top: Subgraph generated by domain expert, with node labels shown verbatim. Bottom: Ground-truth subgraph. The experts graph aggregates intermediate steps (e.g., Total grain output and County GDP) into total household income, but preserves all core causal relationships. This highlights expert-level ability to abstract while retaining semantic fidelity."
        },
        {
            "title": "E Efficacy of GNNs",
            "content": "As we aim for an automated graph-based metric, graph neural network (GNN) is natural first choice. However, they have several flaws for acting as an evaluator of this task. Methods like Token Embedding-Aligned Graph Language Model (TEA-GLM) (Wang et al., 2024) produce Model Standard Name-Assist R1 Llama-8b o3-mini Qwen-32b QwQ 0.926 0.909 0.922 0.912 0. 0.924 0.910 0.930 0.917 0.923 0.002 +0.001 +0.008 +0.005 +0.001 Table 8: Cosine similarity ( = Name-Assist Standard) under standard vs name-assisted TEA-GLM conditions. Minimal differences indicate that NameAssist does not significantly enhance alignment with ground truth. The results raise questions about the effectiveness of name-assisted strategies for improving causal graph accuracy. embeddings for graphs, allowing similarity to be measured via cosine distance. However, these approaches fall short in settings like ours that require semantic fidelity and textual grounding. First, GNN-based methods operate purely over graph structure and do not have access to the source text, making them unable to evaluate whether predicted graph is faithful to the information provided. Second, they reduce graph comparison to single scalar score, such as cosine similarity, which offers little interpretability and no insight into specific errors in nodes or edges. Third, we find in practice that GNN embeddings are insensitive to meaningful differences: in our ablation (see Table 8), models that were explicitly given the correct node names showed nearly identical scores to those that were not, highlighting their lack of resolution. As such, while GNN-based methods remain compelling direction for graph-level embedding, we find them unsuitable for evaluating text-grounded causal graphs where variable naming, semantic meaning, and abstraction play critical role. Table 8 shows the mean cosine similarity between the TEA-GLM embedding of each generated graph and its ground-truth counterpart under both conditions. In addition to the previously identified flaws, these results cast doubt onto the feasibility of GNNs as evaluators for this task. As the table shows, the maximum increase when the model is given the ground-truth node names (prompt described in Appendix R) is only +0.008 (for o3-mini), and one model (R1) even decreases by 0.002. These negligible differences cast doubt on the evaluation capabilities of the graph embedding model for this task, as substantial information being provided to models has little effect on the final embedding score."
        },
        {
            "title": "Confounding",
            "content": "As shown in Figure 2, degree of confounding has noticeable effect on model performance. For this, we determine whether node is confounding by determining if it is explicitly mentioned in the text (or not). Given the large effect of degree of confounding in performance, we explore if this relationship holds under different measures of the degree of confounding. As detailed in Appendix L, each node in each graph is labeled as either explicit (the node was explicitly mentioned in the text), implicit (the node was implicitly or indirectly mentioned), or absent (the node was entirely unmentioned). Previously, we showed performance for the lenient measure of confounding, where we measure whether node was mentioned explicitly, or if it was implicit/absent. We recalculate the degree of confounding for each sample for the strict measure as shown below. DCstrict = 1 (cid:88) vV (cid:40) 1, 0, if A, if Figure 12: Definition of DCstrict (degree of confounding) under the strict criterion, providing quantitative measure of causal complexity. For each node in sample , we determine whether it is absent (A), or either explicit (E) or implicit (I). We count only absent nodes toward the confounding score and compute the average over all nodes in the sample. Under this alternative measure of degree of confounding, at given level of confounding, the strict measure is expected to be more difficult, as node that is entirely absent is harder to identify than one that is implicitly or indirectly described. As shown in Figure 13, the negative relationship between degree of confounding and performance holds. Performance is also consistently worse for all models under the strict confounding definition rather than the lenient one, as expected, adding credibility to the validity of the automated labeling."
        },
        {
            "title": "G Base Model Case Study",
            "content": "As ReCAST is designed to measure the capabilities of LLMs on this task, we investigate the performance of base models. We use the Qwen-2.5-7B base model (Team, 2024), and prompt it to generate an open-ended graph. As base models are trained Figure 13: Average model scores across 20 % confounding bins (strict definition). only to complete the text rather than respond to user instructions, it is unsurprising that completions were nonsensical. As with all responses, we prefill the models response to begin with <think> to steer it towards using chain-of-thought thinking before responding, which in rare cases results in valid model output. Endlessly Repeating Base Model Output <think>Assistant Assistant Assistant Assistant Assistant Assistant Assistant For endlessly repeating generations, they output the same tokens until reaching the maximum tokens. For this reason, we reduce the maximum tokens for Qwen-2.5-7b to 10,000 for computational reasons, as valid generations did not come close to this maximum. Another common failure mode was immediately outputting the end of string token, resulting in no output tokens being outputted. Another failure mode was nonsensical generations, such as generating Chinese despite the text and instructions being in English. We show this example below. occasionally exhibit behavior normally reserved to instruction-tuned models. Coherent Base Model Output <think> To generate causal graph for the provided markdown text, we first need to identify the relationships between the different factors and how they influence each other. Based on the information provided, we can create causal graph with the following relationships: ```json { \"relationships\": [ {\"source\": \"technological breakthrough\", \"sink\": \"ITI development\"}, {\"source\": \"management level\", \"sink\": \"ITI development\"}, {\"source\": \"resource reserve\", \"sink\": \"ITI development\"}, {\"source\": \"agglomeration spillovers\", \"sink\": \"ITI development\"}, {\"source\": \"policy assistance\", \"sink\": \"ITI development\"}, {\"source\": \"industrial transfer\", \"sink\": \"ITI development\"}, {\"source\": \"market demand\", \"sink\": \"ITI development\"} ] } ``` This causal graph shows the different factors and their influence on the ITI development. Each node represents factor, and each arrow represents the influence of one factor on another. The causal graph is simple representation of the relationships between the different factors and how they impact the ITI development. Size vs. Confounding We provide the following charts as reference to visualize the relationship between size and degree of confounding. Foreign-Language Base Model Output <think>建立一个因果关系图需要遵循以下步 骤 1. 确定因果关系首先需要确定因果关系即 哪些变量是因哪些变量是果以及它们之间 的关系是什么在本例中我们已经确定了一 些变量之间的因果关系例如生产生活和生 态之间的关系 2. 创建节点每个变量都可以作为一个节点 用圆圈或方框表示例如我们可以创建三个 节点分别表示生产生活和生态 3. 连接节点使用箭头连接节点表示因果关 系的方向例如我们可以从生产节点连接到 生活节点表示生产对生活有影响 4. 标注节点为每个节点添加标签表示其含 义例如我们可以为生产节点添加生产标 签为生活节点添加生活标签为生态节点 添加生态标签 5. 添加其他变量除了已知的变量外还可 以添加其他变量以更全面地描述因果关系 例如我们可以添加人口GDP土地利用等变 量 6. 检查因果关系最后需要检查因果关系是 否合理是否存在循环因果关系等例如生 产对生活有影响生活对生产也有影响但这 种关系是否合理需要进一步验证 根 据 以 上 步 骤 我 们 可 以 建 立 一 个 因 果 关 系 图 其 中 包 含 生 产 生 活 生 态 三 个 节 点 以 及 它 们 之 间 的 因 果 关 系 具 体 实 现 代 码 如 下</think>```json { \"relationships\": [ {\"source\": \"生产\", \"sink\": \"生活\"}, {\"source\": \"生活\", \"sink\": \"生产\"}, {\"source\": \"生产\", \"sink\": \"生态\"}, {\"source\": \"生态\", \"sink\": \"生活\"} ] } ```"
        },
        {
            "title": "We list some other interesting or informative",
            "content": "outputs from the base model as reference. Another Endlessly Repeating Base Model Output <think> editText editText <think> editText editText <think> editText editText <think> editText In rare occurrences, the model generated validlooking responses, although the graph is far smaller than expected. We attribute its occasional success to the practice of including small amount of instruction data into base models, which allows it to Figure 14: Relationship between text length and degree of confounding. This scatter plot shows each samples character count on the x-axis versus its degree of confounding on the y-axis. There is modest negative correlation (R2 = 0.187), indicating that longer texts tend to include fewer unobserved confounders. This helps explain why models perform slightly better on larger input instances, as reduced confounding makes causal edges easier to identify. Figure 15: Relationship between nodes and degree of confounding. Here, each point represents samples ground-truth graph node count plotted against its confounding level. The near-zero correlation (R2 = 0.002) shows that graph size hardly affects how many variables are unobserved in the text. This suggests that node count alone does not predict the difficulty introduced by hidden confounders."
        },
        {
            "title": "I Computational Costs",
            "content": "Despite the large size of ReCAST samples, its execution is notably quite computationally efficient. The total monetary cost for all experiments, encompassing the evaluation of all five LLMs across the main task and all ablation studies, including the LLM-as-a-Judge evaluations, remained under $250. This affordability is largely attributed to the use of prompt caching for the LLM judge. While the initial processing of the lengthy source texts incurs significant input token cost for the judge, this cost is one-time expense per benchmark sample. Subsequent judgments on different model outputs for the same sample, or re-evaluations, benefit greatly from caching the expensive text embedding, making the iterative evaluation process highly economical. This efficient design ensures that ReCAST can be utilized and extended by researchers without imposing prohibitive computational or financial burdens. Inter-Annotator Agreement Details To ensure the accuracy of the benchmark groundtruth graphs, we measured inter-annotator agreement by having second annotator independently transcribe 37 randomly selected causal diagrams from the source papers ( 15% of the full dataset). These diagrams included 778 directed edges and 616 nodes in total. We compare the two annotators transcriptions at both the node-level and edge-level, and compute standard metrics: precision, recall, F1 score, SHD, and normalized SHD. Edge-Level Agreement. Out of 778 annotated edges, 22 edges were missed by the second annotator (false negatives), and 5 extra edges were incorrectly added (false positives). There were no instances of edges that had flipped directions, nor nodes that were entirely missed. This yields: 751 751 + 5 751 751 + 22 = 0.993, = 0.972, Precision = Recall = P + P + = = F1 = 0.986, SHD = 27, κ = 0.94 Cohens κ statistic reflects near-perfect agreement at the edge level and is defined as: κ = po pe 1 pe where po is the observed agreement and pe is the expected agreement by chance, computed over all possible directed pairs. Node-Level Agreement. Among the 616 nodes, we observed: 8 auto-correctable typos (e.g., carbondioxide carbon dioxide). Our dataset contains 35 samples derived from papers published in 2024 or later. For Llama-8B, 229 samples were from papers published before 2024, and for o3-mini, 235 samples were from papers published before 2024. We compare the average F1 scores achieved by these models on these two subsets of ReCAST. Table 11 shows the performance of the selected models on the 35 samples from papers published in 2024 onward. Table 12 shows their performance on samples from papers published before 2024. Observing the mean F1 scores, both models show slightly higher average performance on the more recent papers. For Llama-8B, the average F1 score increased from 0.300 on older papers to 0.315 on recent papers. For o3-mini, the average F1 score increased from 0.410 to 0.452. To determine if these differences are statistically significant, we performed Mann-Whitney test comparing the F1 scores from the recent (2024 onwards) and older (pre-2024) groups for each model. For Llama-8B: The p-value was 0.5356. (Recent mean F1: 0.315, Older mean F1: 0.300). For o3-mini: The p-value was 0.4074. (Recent mean F1: 0.452, Older mean F1: 0.410). In both cases, the p-values are well above the common significance threshold of 0.05. This indicates that there is no statistically significant difference in performance between papers published before and after the models nominal knowledge cutoff dates for the ReCAST task. This finding suggests that the models ability (or inability) to infer causal relationships from the provided texts in ReCAST is not strongly dependent on whether they might have encountered the specific source documents during pre-training. The task, by its nature, requires reasoning based on the extensive context provided within each sample, rather than direct recall of information from specific papers. The consistent performance across older and potentially unseen recent texts further underscores that the challenges highlighted by ReCAST are rooted in fundamental causal reasoning capabilities rather than familiarity with the source material. Figure 16: Relationship between edges and degree of confounding. This chart plots the count of true causal edges for each sample against its confounding degree. With very small negative correlation (R2 = 0.006), edge density has minimal impact on how many confounders remain implicit. Models thus face similar challenges inferring hidden variables regardless of edge complexity. 4 minor differences (e.g., prefix/suffix omissions such as CO2 emissions vs. emissions). No major label mismatches (0 spurious or missing nodes). This results in node-level F1 score of 0.997. Sample-Level Error Overview. We provide high-level summary of the agreement between annotators. The following is summarized view of the persample disagreement between annotators. This agreement analysis shows that human annotators were highly consistent, with virtually no spurious nodes and very few edge disagreements. These results validate the overall accuracy and reliability of the benchmarks gold-standard graphs."
        },
        {
            "title": "K Effect of Knowledge Cutoff",
            "content": "To investigate whether the models performance on ReCAST is influenced by their knowledge cutoff dates (i.e., whether they might have encountered some of the source texts during their pre-training), we conducted an analysis comparing performance on papers published before and after nominal cutoff. We selected two models for this analysis, Llama-8B and o3-mini, as their stated knowledge cutoffs are prior to 2024. This allows us to treat papers published in 2024 and onwards as more likely unseen by these models. Category Nodes Edges Combined Precision Recall SHD Norm. SHD Cohens κ 0.9943 0.0204 0.9933 0.0182 0.9939 0.0134 1.0000 0.0000 0.9830 0.0509 0.9897 0.0329 0.9970 0.0106 0.9876 0.0335 0.9916 0. 0.1081 0.3879 0.7568 2.3756 0.8649 2.6627 0.0062 0.0223 0.0233 0.0614 0.0162 0.0424 N/A 0.9865 0.0367 N/A Table 9: This table reports the mean and population standard deviation of key evaluation metrics over the 37 reconciled graphs. Precision, recall, and F1 quantify label and edge detection accuracy. SHD is the count of false positives plus false negatives per graph, and the normalized SHD scales this by the total number of gold elements. Cohens kappa is provided for edges only, since it relies on clearly defined set of negative instances (all possible directed non-edges); it is not defined for node labeling or the combined set where the universe of non-nodes or joint negatives is ambiguous. The N/A entries indicate those cases where kappa cannot be meaningfully calculated. Article ID # Nodes # Edges Node FP Node FN Edge FP Edge FN 645 630 617 588 574 566 558 552 536 497 491 486 481 458 449 440 435 410 393 362 306 303 642 259 235 200 156 95 90 74 59 43 42 589 188 168 163 20 25 27 10 19 24 21 16 37 15 12 28 20 18 43 12 26 9 10 11 35 9 18 19 23 16 10 26 15 13 10 15 14 12 12 15 9 24 32 23 16 32 23 24 15 23 20 21 32 28 24 88 15 27 16 14 16 28 10 23 48 26 24 13 39 19 22 14 17 20 16 17 19 11 TOTAL 674 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0 1 0 0 0 0 1 0 0 0 0 0 5 0 0 0 0 0 0 12 1 0 0 0 0 0 0 0 0 0 0 0 0 22 Table 10: Excerpt of per-sample disagreements between two annotators. Total disagreements: 4 node FPs, 5 edge FPs, 22 edge FNs. Inter-annotator reconciliation for 37 graphs. Node FP=sum of minor+major node-label discrepancies; Node FN=no missing nodes observed; Edge FP=extra edges added spuriously; Edge FN=edges present in gold but omitted."
        },
        {
            "title": "Prompt",
            "content": "Degree of Confounding is an important attribute of samples. It can act as natural measure of difficulty, as nodes that are explicitly mentioned are easier to identify than entirely unobserved confounders. We use R1 (et al., 2025) to label the degree of confounding for each node in each sample when provide the ground truth and source text using the prompt below. To allow for fine-grained analysis of degree of confounding, we allow for three different levels of confounding for each node. (i) Explicit (it or synonym of the nodes name appears in Model # Evaluations Average F1 () Llama-8B o3-mini 35 35 0.315 0.187 0.452 0. Table 11: Model Performance on Recent Papers (2024 Onwards) Model # Evaluations Average F1 () Llama-8B o3-mini 229 235 0.300 0.203 0.410 0. Table 12: Model Performance on Older Papers (Before 2024) the text), (ii) Implicit (the node implicitly or indirectly appears in the text), or (iii) Absent (the node does not appear in the text whatsoever). We iteratively reject and retry any answers that do not meet formatting requirements until we receive valid answers for every benchmark sample to ensure that all samples have confounding levels. We detail the prompt used for this below. These node-level labels are used as the basis for calculation of degree of confounding. Label Unobserved Confounders Prompt Variable Correction Prompt You will be given causal graph in economics and source text. Your task is to label each node in the graph to determine its degree of explicitness in the text. For each node, there are three possible levels: 1. The node (or the concept behind it) is explicitly mentioned in the text - This can be verbatim, or though use of synonym - It is sufficient to be mentioned in the text; it is irrelevant if it is mentioned to be in the causal graph or not 2. implicitly in the text. 3. even if related concepts are discussed"
        },
        {
            "title": "The node is mentioned indirectly or",
            "content": "The node is unmentioned in the text, Be conservative when determining the degree Output of explicitness for each node. only the JSON code block with your answer, without commentary, reasoning, explanation, or any other text. You must include the name of each node in the graph verbatim, even when the graph is very large, or many nodes are highly related or seem redundant. # Expected Output Format ```json { \"scores\": { \"first_node_name\": int_score_1_2_or_3, \"second_node_name\": int_score_1_2_or_3, ... \"last_node_name\": int_score_1_2_or_3 } } ``` to each examine MANDATORY critically and and It is thoroughly *every* node in the causal graph one at time. Explicitly think about each node (and where its appropriate) individually, even when it seems redundant or unnecessary. Even if it is tedious, you MUST do this and not take shortcuts. relationships corresponding"
        },
        {
            "title": "M Variable Correction",
            "content": "We use the following prompt to correct the raw variable names extracted after annotation using o3-mini. To ensure validity, we use code-based approaches to automatically reject and retry any answers where all old names did not appear You are world-class economist. You will be given causal loop diagram (CLD) in JSON format. Your task is to combine variables that are intended to be the same, but are not named identically due to annotation errors. You will do this by combining variables and choosing which variable name to keep. are that"
        },
        {
            "title": "Avoid",
            "content": "combining variables Your task is NOT to functionally alter the CLD. Be careful to only combine variables that are intended to be the same and are different solely due to annotation errors. When in doubt, do not combine the variables. Follow these guidelines: - intended to be separate. - Avoid combining variables that are highly similar but have different names. - Do not create new variables or variable names, nor remove any variables from the CLD. - Use the context of the CLD when making your decision. - You must choose an existing variable name or your response will be rejected. - You must only combine variables that are intended to be the same. - Combining variables with more than one character difference between them is only done very rarely. Positive examples: - \"Number of dog\" and \"Number of dogs\" should be combined into \"Number of dogs\". - \"number of dogs\" and \"Number of dogs\" should be combined into \"Number of dogs\". Negative examples (do not combine): - \"<variable>\" and \"variable\" should not be combined since it is clear that they are intended to be distinct. - NEVER change any variables with < or > in the name. - \"Number of dogs\" and \"Number of hounds\" should not be combined since it is clear that this isnt from an annotation error. - \"GDP\" and \"GNP\" should not be combined; while they are only one letter apart, they are distinct variables. Respond with your answer in JSON format and no other text. JSON format: { \"combined_variables\": [ { }, { } ] } \"old_names\": [\"variable1\", \"variable 1\", \"Variable1\"], \"new_name\": \"Variable1\" \"old_names\": [\"variable2\", \"Variable 2\", \"variable two\"], \"new_name\": \"Variable2\" PDF-to-Markdown Conversion PDF to Markdown Prompt or the word around is most edited of Your task is to perform the minimal PDF pre-processing necessary to convert the provided PDF into well-structured md file. Follow the guidelines below in order of priority: Modify the text only when absolutely 1. necessary. The exact wording of the original paper must be preserved verbatim. - Do not correct spelling or grammar, even if it is incorrect - The response will be rejected if even removed single unnecessarily; response should effectively be copy-pasted from the original text - Your response will likely be extremely the long, original and normal. Correct any broken text from the 2. PDF processing and convert it into well-structured md file. - Convert sections and sub-sections into headings and subheadings 3. entirety: - Images, figures, and any other visual elements - including when in-line. E.g., \"[20, 22]\" would be removed. - Acknowledgments - Authorship information - Appendices - Page numbers Remove the following information in as expected References Citations, length text; this same the and is We utilize LLM for the task of converting the text of the PDF to well-structured markdown as papers do not follow consistent format. We find that reasoning models struggle at this task, and frequently fail to follow instructions to output the entire document by leaving out large sections of the text. We note that the normalization tool cannot be used for this task, as the numerous formatting errors and in-line citations would require it to be called once for almost every line of the text, and would result in an output many times longer than the source text. Mistral Small (Team, 2025a) follows the conversion instructions at tractable computational costs. We remove non-textual elements as they would be difficult to accurately represent in markdown. We additionally exclude irrelevant elements such as publication information and references as they are unrelated the economics task and needlessly inflate the length of texts. We also remove appendices, which are usually irrelevant or contain explicit information about the causal graph. Remember; your only output is the processed text in full, with no thinking, reasoning, or other commentary."
        },
        {
            "title": "O Text Normalization Prompt",
            "content": "In order to ensure the realism of the ReCAST benchmark, it is important to remove any explicit references to the causal graph, which make the task trivial. During this step, we also correct any references to non-existent elements which were removed in previous pre-processing steps (for example, referencing an image). We utilize normalization tool to make these changes, which helps address several limitations of current LLMs. First, they struggle to output large text in full, and have significant computational costs when doing so. Additionally, when outputting large chunks of text, they are prone to hallucinations and excessive edits, which are inappropriate. Using tool also allows us to use code to check that the changes are valid; that is, that the start and end text are actually present in the text. We note that LLMs often struggle to account normalizations that overlap, even with specific prompting for this. In this case, normalization will fail, and the entire response will be rejected. We iteratively prompt with the normalization prompt, stopping only when no normalizations are given. This ensures that it is confident that the text was correctly changed, and that no new text was introduced that needs to be changed. We utilize o3-mini (OpenAI, 2025) to perform this task, as it was shown to perform well during manual evaluation. Normalize Text Prompt the and graph Your task is to edit md version of published economics paper in markdown format to remove specific types of content. - Remove any information that explicitly its causal references contents, including the causal graph itself - This is the only information you should remove from the paper - Only modify the text when it is necessary to remove the causal graphs information - Only remove explicit references to the causal graphs elements, such as variable names, feedback loops, arrow colors, variable explicitly being included, etc. Do not remove other references and related information to the causal graph, such as discussing elements of the causal graph, its relationships generally, and similar information - You can only edit the paper; do not attempt to edit the causal graph - The graph is supplied as reference only in <causal_graph> tags - Do not attempt to edit anything before </causal_graph>; this is not part of the paper and will be rejected You have access to special tool called normalize that can replace text. This is the only way you can modify the text. Be careful to ensure that the text you are replacing is only the causal graphs information, and that it exists verbatim in the text. The normalize tool takes three parameters: 1. start_string: The beginning of the text to replace 2. replace 3. replacement: The text to insert instead You can call normalize multiple times to make several targeted replacements in the document. All three parameters are required for each call. - By default, normalize will locate the *first* occurrence of the start_string. As workaround for when the same text appears verbatim multiple times, use slightly longer start_string and include some of the original text in your replacement to maintain context. - remove not references entirely rather than replacing them with generic text. - Both the start and end strings will be included in the text that gets replaced. Changes are applied in order, so ensure that any string you replace is not used in another replacement or an error will be thrown. Respond only with JSON in the following format: end_string: The end of the text to \"redact\" text; the Do {\"normalizations\": [ {\"start\": \"text to find (beginning)\", \"end\": \"text to find (end)\", \"replacement\": \"text to insert instead\"}, ... ]}"
        },
        {
            "title": "Prompt",
            "content": "Formatting Correction Prompt Your task is to correct the formatting of misformatted response, which is intended to end with causal graph in economics that conforms to the proper JSON format. You will convert their intended answer to the proper JSON format, taking great care to be as faithful to the ground truth as possible. Do not attempt to modify the substance of their answer in any form, even if you think it may improve its quality (including typos) - the task is to make the minimal changes possible to correct the formatting. The extent of the formatting may be minor, or be so extensive as to require writing the JSON from scratch. Expected output format: ```json { \"relationships\": [ {\"source\": causal_variable0, \"sink\": affected_variable0}, {\"source\": causal_variable1, \"sink\": affected_variable1}, ... ] } ``` If be the will answer. provided original, You misformatted included lengthy intermediate steps, you will be given snippet of them as context. Use only the final answer, always prioritizing the information provided closest to the end of the response. it If there is no text in the answer that resembles causal graph, return an empty list of relationships. Begin your response with the start of the JSON code block. Do not provide any - reasoning, thinking, commentary, etc. just the reformatted response. Dont overthink it. Causal Graph Generation Prompt You are an expert causal reasoner and economist. Your task is to generate causal graph for the provided markdown text. First, use extremely long chain-of-thought reasoning in <think> tags. Then, provide your final answer in JSON code block, strictly following the following format: ```json { \"relationships\": [ {\"source\": causal_variable0, \"sink\": affected_variable0}, {\"source\": causal_variable1, \"sink\": affected_variable1}, ... ] } ``` Your graph will contain exactly NUM_NODES nodes. When answering, do not provide any additional reasoning, commentary, or the - information other JSON code block, with each dictionary representing one relationship in the graph. provide only Name-Assisted Causal Graph Name-Assisted Formatting Correction"
        },
        {
            "title": "Prompt",
            "content": "Causal Graph Generation with Node Names Prompt You are an expert causal reasoner and economist. Your task is to generate causal graph for the provided markdown text. First, use extremely long chain-of-thought reasoning in <think> tags. Then, provide your final answer in JSON code block, strictly following the following format: ```json { \"relationships\": [ {\"source\": id_of_source_node, \"sink\": id_of_sink_node}, {\"source\": id_of_source_node, \"sink\": id_of_sink_node}, ... ] } ``` You will be provided with the source markdown text and the name of each node in the graph. Ensure that each node is included at least once in the generated causal graph. Do not use the nodes name in the graph; instead, use the id corresponding to the node. For the example nodes below (not the same as the ones you will be provided), whenever you want to include the node named \"demand\" in your graph, you would use the integer 2 rather than the word demand. ```json { \"nodes\": [ {\"name\": \"supply\", \"id\": 1}, {\"name\": \"demand\", \"id\": 2}, ... ] } ``` do not answering, reasoning, any When or additional the - information other JSON code block, with each dictionary representing one relationship in the graph. provide commentary, provide only Name-Assisted Formatting Correction Prompt Your task is to correct the formatting of misformatted response, which is intended to end with causal graph in economics that conforms to the proper JSON format. You will convert their intended answer to the proper JSON format, taking great care to be as faithful to the ground truth as possible. Do not attempt to modify the substance of their answer in any form, even if you think it may improve its quality (including typos) - the task is to make the minimal changes possible to correct the formatting. The extent of the formatting may be minor, or be so extensive as to require writing the JSON from scratch. In the original creation step, they were given the node names for the graph, each When correcting with corresponding ids. the graph, only ever use the integer ids corresponding to the node name, regardless of if the original used the names or correctly used the ids. Expected output format: { } \"relationships\": [ {\"source\": id_of_source_node, \"sink\": id_of_sink_node}, {\"source\": id_of_source_node, \"sink\": id_of_sink_node}, ... ] If be it the will answer. provided original, You misformatted included lengthy intermediate steps, you will be given snippet of them as context. Use only the final answer, always prioritizing the information provided closest to the end of the response. If it never comes to an answer, do not attempt to solve it yourself. Instead, simply return an empty list of relationships. Begin your response with the start of the JSON code block. Do not provide any reasoning, thinking, commentary, etc. just the reformatted response. Dont overthink it. Here are the nodes for your graph: Here are the nodes for your graph: ```json NODE_JSON ``` ```json NODE_JSON ``` LLM-as-a-Judge Prompt LLM-as-a-Judge Prompt You are an expert economist. Your task is to act as an evaluator for causal graph. You are provided with the ground-truth graph, the source text, and the LLMs response. You will also be told the type of evaluation to perform; only evaluate the response for that type of evaluation by instructions. Do not evaluate using any other type of evaluation. following closely the the each texts about"
        },
        {
            "title": "Follow",
            "content": "provided reasoning direction carefully, from before When evaluating, follow these guidelines: 1. completely, and in-order It is very important to be thorough a. and not take shortcuts, even when it seems tedious, redundant, or unnecessary. Do this for each node or edge you are evaluating; there is no time limit. Be sure to fully to fully think through each node or edge you are tasked with evaluating fully before moving onto the next one. It is helpful to quote supporting i. and evidence graphs their relevance to the final evaluation for that node or edge. ii. While evaluating node or edge, you may examine several plausible counterparts to judge presence, semantics, abstraction, etc. (e.g., to see if it is broader or narrower than any ground-truth items). Use all relevant comparisons to inform your decision, but output oneand only oneset of labels for the item. b. Only focus on the specific type of evaluation you are asked to do. Regardless of the accuracy (or lack thereof) in other categories, if you are asked to evaluate node precision, only evaluate node precision, not recall or edges. These are intended to be separate evaluations, so do not conflate the two. c. Not Applicable labels must be explicitly selected when category is skipped due to prior labels d. Be conservative when grading - When in doubt between two labels, ere on the side of being harsh. by thinking in Start Then, output your answer <think> tags. in YAML code block, formatted exactly as specified in the expected output format. step-by-step # Node Level Evaluation ## Node Precision For response, evaluate against both ground truth sources: LLMs each node the in"
        },
        {
            "title": "Explicitly",
            "content": "1. Ground-Truth Graph Evaluation - potentially ground-truth graph identify corresponding and quote nodes"
        },
        {
            "title": "Core",
            "content": "- Apply these labels where applicable: Presence Labels (select one): - concept PRESENCE_STRONG_MATCH: matches ground-truth node with only minor, inconsequential differences - PRESENCE_WEAK_MATCH: Core concept shares meaning with ground-truth node, even if there are noticeable differences - ground-truth remotely similar core concept PRESENCE_NO_MATCH: captures"
        },
        {
            "title": "There",
            "content": "no node that is or only nearly subtle SEMANTIC_STRONG: meaning Semantic Labels (select one): Exactly - identical with distinctions - SEMANTIC_MODERATE: Same core concept but with meaningful differences in scope or implication - SEMANTIC_WEAK: Shares some semantic space but with substantial differences - SEMANTIC_NA: Not applicable"
        },
        {
            "title": "Represents",
            "content": "ABSTRACTION_BROADER: ABSTRACTION_ALIGNED: same Abstraction Labels (select one): - more general concept that includes the ground-truth node - approximately scope the specificity of the ground-truth node - ABSTRACTION_NARROWER: Represents more specific subset of the ground-truth node - ABSTRACTION_NA: Not applicable or the concepts were so different as to make abstraction comparison impossible Represents and 2. Ground-Truth Text Evaluation - Explicitly quote ALL relevant supporting text from source - Apply these labels where applicable: Evidence Labels (select one): PRESENCE_STRONG_MATCH: - appears with inconsequential differences - PRESENCE_WEAK_MATCH: Core concept shares significant meaning with text but has notable differences - capture similar core concept PRESENCE_NO_MATCH: concept minor, segments text Core only text in No Semantic Labels (select one): - SEMANTIC_STRONG: Captures precisely what is stated in text or represents meaning with minimal interpretation - some SEMANTIC_MODERATE: interpretation but maintains core meaning - SEMANTIC_WEAK: Significant interpretation needed; meaning partially preserved - SEMANTIC_NA: Not applicable"
        },
        {
            "title": "Requires",
            "content": "Abstraction Labels (select one): - ABSTRACTION_BROADER: Represents more general concept that includes text concepts Represents - approximately and specificity as the text - ABSTRACTION_NARROWER: Represents more specific subset of text concepts ABSTRACTION_ALIGNED: same scope the - ABSTRACTION_NA: Not applicable or the concepts were so different as to make abstraction comparison impossible ## Node Level Recall For each node in the ground-truth graph, evaluate against the LLMs response: and quote identify"
        },
        {
            "title": "Explicitly",
            "content": "IMPORTANCE_CORE: Response Evaluation - ALL potentially corresponding nodes from LLMs response - Apply these labels where applicable: Importance Labels (select one): - node represents fundamental concept central to the causal structure - Ground-truth node serves as key connection between central concepts Ground-truth - node provides supplementary or contextual information IMPORTANCE_INTERMEDIATE: IMPORTANCE_PERIPHERAL: Ground-truth in"
        },
        {
            "title": "Core",
            "content": "response Presence Labels (select one): PRESENCE_STRONG_MATCH: - appears with inconsequential differences - PRESENCE_WEAK_MATCH: Core concept shares significant meaning with response node but has notable differences - captures similar core concept PRESENCE_NO_MATCH: concept minor, response only node No Semantic Labels (select one): - SEMANTIC_COMPLETE: Ground-truth concept fully captured with high fidelity, whether in single or multiple nodes - SEMANTIC_PARTIAL: Core aspects captured but with some meaning loss or missing implications - surface-level captured - SEMANTIC_NA: Not applicable SEMANTIC_MINIMAL: or concept Only of aspects basic the Represents ABSTRACTION_BROADER: ABSTRACTION_ALIGNED: same Abstraction Labels (select one): - more general concept that includes the ground-truth node - approximately scope the specificity of the ground-truth node - ABSTRACTION_NARROWER: Represents more specific subset of the ground-truth node - ABSTRACTION_NA: Not applicable or the concepts were so different as to make abstraction comparison impossible"
        },
        {
            "title": "Represents\nand",
            "content": "# Edge Level Evaluation ## Edge Precision For each edge (causal relationship) in the LLMs response, evaluate against both ground truth sources: 1. Ground-Truth Graph Evaluation -"
        },
        {
            "title": "Explicitly",
            "content": "identify and quote"
        },
        {
            "title": "ALL",
            "content": "from edges corresponding potentially ground-truth graph - Apply these labels where applicable: Presence Labels (select one): - connects PRESENCE_STRONG_MATCH: highly similar concepts as in ground-truth connects - somewhat in as ground-truth - PRESENCE_NO_MATCH: No corresponding edge exists in ground-truth PRESENCE_WEAK_MATCH:"
        },
        {
            "title": "Edge\nconcepts",
            "content": "similar"
        },
        {
            "title": "Edge",
            "content": "DIRECTION_REVERSED: Directionality Labels: - DIRECTION_CORRECT: Direction of causality matches ground-truth - causality is opposite of ground-truth - the or Not concepts were so different as to make direction comparison impossible DIRECTION_NA: applicable"
        },
        {
            "title": "Direction",
            "content": "of of"
        },
        {
            "title": "Edge",
            "content": "relationship ABSTRACTION_BROADER: represents as ABSTRACTION_ALIGNED: scope Abstraction Labels: - similar ground-truth - substantially ground-truth - substantially ground-truth - ABSTRACTION_NA: Not applicable or the concepts were so different as to make abstraction comparison impossible ABSTRACTION_NARROWER: is than is than specific general more Edge Edge more in ground-truth 2. Ground-Truth Text Evaluation - Explicitly quote ALL relevant supporting text that describes causal relationships - Apply these labels where applicable: Evidence Labels (select one): - PRESENCE_GRAPH_ONLY: Causal relationship present (always select this if present) - PRESENCE_EXPLICIT: Causal relationship directly stated in text (only if not in graph) - relationship can be reasonably inferred from text (only if not in graph) - PRESENCE_NO_MATCH: No text supports this causal relationship (only if not in graph) PRESENCE_IMPLIED: Causal graph Inference Labels (select one): - INFERENCE_DIRECT: Relationship matches texts explicit causal claims - INFERENCE_DERIVED: Relationship logically follows from text - INFERENCE_STRETCHED: possible but weakly supported - relationship does not exist INFERENCE_NA:"
        },
        {
            "title": "Relationship",
            "content": "applicable"
        },
        {
            "title": "Not",
            "content": "or ABSTRACTION_ALIGNED: Abstraction Labels (select one): - Matches granularity of texts causal claims - ABSTRACTION_BROADER: Generalizes multiple textual relationships - ABSTRACTION_NARROWER: Specifies subset the of texts causal claims - ABSTRACTION_NA: Not applicable or the concepts were so different as to make abstraction comparison impossible ## Edge Level Recall For each causal relationship (edge) in the ground-truth graph, evaluate against the LLMs response: quote identify"
        },
        {
            "title": "ALL\ncausal",
            "content": "and corresponding Response Evaluation - potentially relationships from LLMs response - Apply these labels where applicable: Importance Labels (select one): Importance is based on how important it is to the ground-truth graph, regardless of whether it is present or accurately represented in the LLMs response. key IMPORTANCE_CENTRAL: - relationship that drives main effects - IMPORTANCE_CONNECTING: Links major causal chains together IMPORTANCE_AUXILIARY: - supplementary causal context Provides causal in Core response Presence Labels (select one): - PRESENCE_STRONG_MATCH: with appears inconsequential differences - PRESENCE_WEAK_MATCH: Core concept shares significant meaning with response node but has notable differences - captures similar core concept PRESENCE_NO_MATCH: concept minor, response only node No Directionality Labels (select one): - DIRECTION_CORRECT: Causal relationship captured with correct direction - DIRECTION_REVERSED: Causal relationship present but direction is reversed - DIRECTION_UNCLEAR: Relationship present but direction is ambiguous - DIRECTION_MISSING: Relationship entirely absent from response at match similar ABSTRACTION_BROADER: One-to-one level Abstraction Labels (select one): - ABSTRACTION_ALIGNED: relationship of detail - substantially ground-truth - substantially ground-truth - ABSTRACTION_NA: Not applicable or the concepts were so different as to make abstraction comparison impossible ABSTRACTION_NARROWER: is than is than specific general"
        },
        {
            "title": "Edge",
            "content": "more more output include should the # Expected Output Format format. be The in Only sections evaluation that are being evaluated - omit other For example, if only sections entirely. the evaluating precision,"
        },
        {
            "title": "YAML",
            "content": "only node node_precision_evaluations section should be present. However, within the required evaluation sections, be sure to always include the Not Applicable labels rather than omitting them. yaml # If evaluating node precision: node_precision_evaluations: - node_number: <integer> graph_evaluation: presence_label: <PRESENCE_LABEL> semantic_label: <SEMANTIC_LABEL> abstraction_label: <ABSTRACTION_LABEL> text_evaluation: presence_label: <PRESENCE_LABEL> semantic_label: <SEMANTIC_LABEL> abstraction_label: <ABSTRACTION_LABEL> # If evaluating node recall: node_recall_evaluations: - node_number: <integer> importance_label: <IMPORTANCE_LABEL> presence_label: <PRESENCE_LABEL> semantic_label: <SEMANTIC_LABEL> abstraction_label: <ABSTRACTION_LABEL> # If evaluating edge precision: edge_precision_evaluations: - edge_number: <integer> graph_evaluation: presence_label: <PRESENCE_LABEL> directionality_label: <DIRECTION_LABEL> abstraction_label: <ABSTRACTION_LABEL> text_evaluation: presence_label: <PRESENCE_LABEL> inference_label: <INFERENCE_LABEL> abstraction_label: <ABSTRACTION_LABEL> # If evaluating edge recall: edge_recall_evaluations: - edge_number: <integer> importance_label: <IMPORTANCE_LABEL> presence_label: <PRESENCE_LABEL> directionality_label: <DIRECTION_LABEL> abstraction_label: <ABSTRACTION_LABEL> LLM-as-a-Judge Scoring Mechanics IMPORTANCE_CORE) The quantitative metrics derived from the LLM-asa-Judges YAML output are calculated as follows. First, the judges qualitative labels for various evaluation criteria (e.g., PRESENCE_STRONG_MATCH, SEMANTIC_MODERATE, are mapped to pre-defined numerical scores, ranging from 0.0 (no match/irrelevant) to 1.0 (perfect match/highly important). For multi-faceted evaluations like node precision, which considers presence, semantic similarity, and abstraction level, composite score for single aspect (e.g., node precision against the ground-truth graph) is computed by averaging the numerical scores of its constituent labels. Precision metrics (node precision, edge precision) for each item generated by the LLM are determined by comparing it against both the groundtruth graph and the source text. If the item is labeled as PRESENCE_NO_MATCH against both sources, its score is 0.0. Otherwise, the higher of the two composite scores (one from graph comparison, one from text comparison) is taken as the items precision score. The overall precision for category (e.g., node precision) is then the arithmetic mean of these individual item precision scores. Recall metrics (node recall, edge recall) assess how well the LLMs output captures items from the ground-truth graph. For each ground-truth item, composite correctness score is calculated based on its presence and the fidelity of its representation in the LLMs output (considering factors like semantics, abstraction, and directionality for edges). This correctness score is then multiplied by numerical importance weight assigned by the judge to that ground-truth item (e.g., IMPORTANCE_CORE receives higher weight than IMPORTANCE_AUXILIARY). The final recall score for category is weighted average: the sum of (correctness score importance weight) for all ground-truth items, divided by the sum of all possible importance weights. This ensures that correctly recalling more important ground-truth items contributes more significantly to the recall score. Finally, F1 scores for nodes, edges, and overall performance are calculated using the standard harmonic mean: 2(PrecisionRecall)/(Precision+ Recall). Overall precision and recall are microaveraged, where the total weighted sum of correct predictions is divided by the total number of predictions (for precision) or total ground-truth items (for recall, considering importance weights), across both nodes and edges."
        }
    ],
    "affiliations": [
        "Amazon Gen AI",
        "Worcester Polytechnic Institute"
    ]
}