{
    "paper_title": "HUNYUANPROVER: A Scalable Data Synthesis Framework and Guided Tree Search for Automated Theorem Proving",
    "authors": [
        "Yang Li",
        "Dong Du",
        "Linfeng Song",
        "Chen Li",
        "Weikang Wang",
        "Tao Yang",
        "Haitao Mi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce HunyuanProver, an language model finetuned from the Hunyuan 7B for interactive automatic theorem proving with LEAN4. To alleviate the data sparsity issue, we design a scalable framework to iterative synthesize data with low cost. Besides, guided tree search algorithms are designed to enable effective ``system 2 thinking`` of the prover. HunyuanProver achieves state-of-the-art (SOTA) performances on major benchmarks. Specifically, it achieves a pass of 68.4% on the miniF2F-test compared to 65.9%, the current SOTA results. It proves 4 IMO statements (imo_1960_p2, imo_1962_p2}, imo_1964_p2 and imo_1983_p6) in miniF2F-test. To benefit the community, we will open-source a dataset of 30k synthesized instances, where each instance contains the original question in natural language, the converted statement by autoformalization, and the proof by HunyuanProver."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 3 ] . [ 1 5 3 7 0 2 . 2 1 4 2 : r Technical Report HUNYUANPROVER: Scalable Data Synthesis Framework and Guided Tree Search for Automated Theorem Proving Yang Li, Dong Du*, Linfeng Song*, Chen Li*, Weikang Wang, Tao Yang and Haitao Mi Tencent Hunyuan Teams {youngyli,dongdu,vegetali,daxianwang,rigorosyang}@tencent.com {lfsong,haitaomi}@global.tencent.com"
        },
        {
            "title": "Abstract",
            "content": "We introduce HUNYUANPROVER, an language model finetuned from the HUNYUAN 7B for interactive automatic theorem proving with LEAN4. To alleviate the data sparsity issue, we design scalable framework to iterative synthesize data with low cost. Besides, guided tree search algorithms are designed to enable effective system 2 thinking of the prover. HUNYUANPROVER achieves stateof-the-art (SOTA) performances on major benchmarks. Specifically, it achieves pass of 68.4% on the miniF2F-test compared to 65.9%, the current SOTA results. It proves 4 IMO statements (IMO 1960 P2, IMO 1962 P2, IMO 1964 P2 and IMO 1983 P6) in miniF2F-test. To benefit the community, we will opensource dataset of 30k synthesized instances, where each instance contains the original question in natural language, the converted statement by autoformalization, and the proof by HunyuanProver."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs) have profoundly impacted mathematical reasoning and theorem proving in artificial intelligence. Despite notable progress in natural language domains, language models still encounter substantial challenges in formal theorem proving (e.g., using LEAN (Moura & Ullrich, 2021) or Isabelle (PAULSON, 1994)) probably due to the massive search space of Olympiad-level theorem proving and limited data in such scenario. As the results, even the most advanced models like GPT-4o Hurst et al. (2024) struggle with complex formal-statement proving (Xin et al., 2024a). formal theorem-proving model should be capable of understanding both the syntax and semantics of formal system, enabling it to generate valid next steps within the system. More critically, it must integrate this capability with its abstract mathematical reasoning skills to perform effective and efficient deductions. We propose HUNYUANPROVER, framework designed to address the aforementioned challenges in automatic theorem proving. HunyuanProver takes two core modules: scalable prover-data generator and guided tree-search algorithms. The prover-data generator only takes open-source data to train the initial autoformalizer and prover. The autoformalizer then converts large amount of existing math questions into the format of the target prover (e.g., LEAN4). The prover is then iteratively improved on such data where new proof data is generated at each iteration to train the prover. At testing time, tree-search algorithms and multiple critic models are designed to perform slow thinking that is empirically essential for solving complex theorem-proving tasks. Evaluations show that HUNYUANPROVER yields an accuracy of 68.4% on the miniF2F benchmark. In addition, we also observe several key findings: 1) using explicitly trained critic for tree-search guidance is helpful; 2) the scale of finetuning data for theorem proving is critical, thus designing efficient data scaling framework is important; 3) data curation and selection is important as well when there is sufficient amount of training data. Our key contributions includes: We introduce scalable pipeline that utilizes open-source prover data and massive math problems in natural language to generate large number of new training data for automatic theorem proving. Equal contribution 1 Technical Report We develop several critic models and evaluate their effectiveness using two widely adopted search algorithms: best-first search and Monte Carlo tree search. We will open-source approximately 30,000 data instances at XXX. Each instance includes the formal statement, proofs searched by HunyuanProver, and the original problems."
        },
        {
            "title": "2 Scalable Data Generation for Prover Improving",
            "content": "One major bottleneck for automated theorem proving is the lack of training data. For example, as one of the largest open-source LEAN4 datasets, mathlib4 (Moura & Ullrich, 2021) only contains around 50k theorems(with tactics) for training. This is far from sufficient to train stronger prover given the difficulty of automatic theorem proving. On the other hand, massive high-quality math problems in natural language have been released in recent years (Yu et al., 2024; Luo et al., 2023). Our efforts fall into two critical aspects to scale the training data for automatic theorem proving: autoformalization (Section 2.1) that maps natural language problem (Section 2.2) into LEAN format statement, and tactic generation for iterative theorem proving. 2.1 Autoformalization Data Generation For autoformalization, we start with 130k high-quality natural language to LEAN statement pairs, which includes 50k from lean workbook (Ying et al., 2024) and 80k from MMA (Jiang et al., 2024). We first translate the natural language part of the 130k data into Chinese to double the dataset size, and with such data we train autoformalization model, it can translate both English and Chinese problems to lean4 format statement. In the next step, 30 million internal math problems in natural language are converted into formal statement by the autoformalization model. For each natural language problem, we sample 8 outputs with different temperatures.We obtain dataset Dq of 20 million LEAN statements after filtering out these do not conform with LEAN grammar or do not satisfy other rules adopted by previous practices (Ying et al., 2024). In addition to using internal data, we also utilized open-source natural language mathematical data NuminaMath CoT(LI et al., 2024). 2.2 Prover Improving via Iterative Tactic-Level Proving Data Generation We design iterative framework that takes LEAN engine Γ and statement dataset Dq from autoformalization to generate new tactic data for prover improving. Specifically, in iteration t, we leverage best-first search (Section 3.1) algorithm with the prover from the previous iteration πt1 on all unsolved statements so far in D. We collect the proving trajectories (e.g., τ ) for newly solved statements (e.g., q) if there is any: Dt = {(q, τ ) Dq Dt1, τ BFS(q), τ = null} Dt1 (1) Then the prover is updated using rejection finetuning (RFT) with the proving trajectories in Dt after filtering out easy statements solved in early iterations. The initial prover π0 is trained on public data including mathlib4. After more than 10 rounds of iteration, more than 20 million tactic-level data is obtained. Enhancing Diversity As indicated by previous work (Xin et al., 2024c), prover diversity is important due to the massive search space of theorem proving. We further develop two methods to enhance the prover diversity within the iterative data-generation framework. For the first method, we design rules to convert the last state of an unfinished proving trajectory into new statement. In this way, more diverse proving data is obtained for prover training. For the second method, we collect data from proving more challenging statements, including those Olympiad-level algebraic inequalities Wei et al. (2024) and lean workbook Ying et al. (2024). 2 Technical Report Figure 1: Comparing best-first search (BFS) with Monte-Carlo tree search (MCTS). BFS only takes Selection and Expansion in one iteration, while MCTS takes all four steps. The numbers represent critic-assigned scores."
        },
        {
            "title": "3 Guided Tree Search",
            "content": "As described in Section 2, our task involves iterative interaction with LEAN as the environment, where for each iteration the policy predicts new tactic given from state in the proving process. We abstract this process as tree search where state si in the proving process corresponds to tree node ni with the input statement being the root n0. An edge that points from ni to nj represents applying tactic on node (state) ni to yield node (state) nj. To handle this problem, we design two major tree search algorithms as described in Section 3.1. We also design several critics, as shown in Section 3.2, to guide these algorithms. 3.1 Tree Search Algorithms We explore two search algorithms: best-first search (BFS) and Monte-Carlo tree search (MCTS) for the tactic-level iterative statement proving process. Best-First Search We first study BFS due to its simplicity and effectiveness. As demoed in the left part of Figure 1, BFS can be viewed as an iterative process of selection and expansion. The search process continues until either the statement is successfully proved or the maximal number of iteration is reached. In the selection step, the node ˆn with the highest critic score is selected (without return) from the set of active nodes : ˆn = arg max CRITIC(n) nN (2) where CRITIC represents the critic function. In the expansion step, the policy π samples set ( ˆC) of candidate tactics under ˆn: ˆC = {ˆciˆci π(q, ˆn), [0, . . . , K]} (3) Each tactic in ˆC is then executed against the LEAN engine to yield new tree node if the tactic is valid under state ˆn. After removing those that are identical to previously explored tree nodes, the remaining are merged into the active node set . We check two nodes are identical simply by string matching. ηMCTS Thought being simple and effective in general, BFS has several limitations on handling complex search problems. One limitation is that each node is only visited once with fixed amount of expansion budget. Besides, BFS only takes the critic score as guidance, thus it can suffer from any bias and misjudgment inherent in the critic model. We additionally adapt ηMCTS (Tian et al., 2024) to handle such limitations. As shown in Figure 1, the original ηMCTS algorithm takes 4 steps of selection, expansion, simulation and backpropagation in each iteration. Here we remove the step of simulation, leaving it for future work. As 3 Technical Report another major, our ηMCTS follows the setting of BFS to sample candidate tactics (instead of one in Tian et al. (2024)) at time given state. Different from BFS, our ηMCTS algorithm can sample tree node multiple times, and the expansion budget for each node is updated by its importance score that can be dynamically changed throughout the search process. Following Tian et al. (2024), the importance score for any node is defined as the maximal value difference between it and its descendant: I(n) = max ˆnSUCC(n) CRITIC(ˆn) CRITIC(n), (4) where SUCC(n) represents all succeeding nodes of node n. Then, the adapted expansion budget for node is defined as: E(n) = max(Bmin, min(Bmax, αI(n) + 1)) (5) where α, Bmin, Bmax are the corresponding factor, lower bound and upper bound, respectively. As another major difference from BFS, ηMCTS selects nodes based on Upper Confidence Bound (UCB) defined as: CB(n) = CRITIC(n) + α 2 ln (cid:115) CNT(PRNT(n)) CNT(n) (6) where PRNT(n) denotes the parent node of in the search tree, and CNT(n) represents the number of visiting times so far for n. Generally, UCB can be viewed as balance between exploration and exploitation, where the first term of the equation prefers nodes with high critic score (exploitation) while the second term prefers under-explored nodes (exploration). 3.2 Critics for Search Guidance Critic modeling is central component in tree search as it provides the guidance. We design three types of critic models to guide the search process. Policy Confidence (PC) We first leverage policy confidence as guidance for cold start of guided search due to limited tree-search data for training critic models at the beginning. Particularly, for tactic under state in the proving process of statement q, we define its policy confidence π(c) as token-level average log probability: π(c) = 1 c (cid:88) j=1 log pπ(cjq, n, c<j) (7) where is the number of tokens in c. Process Reward Model (PRM) The process reward model (PRM), denoted as vπ ϕ(q, n) (parameterized by ϕ), represents the possibility of tree node for proving statement when starting from it and following policy π thereafter. ϕ using statement set = [q1, . . . ], we first generate search To train parameterized PRM vπ tree for each statement qk by following policy π under the guidance of the critic from the previous iteration. Next, for each node nk in the search tree, score is assigned to reflect its quality. Ideally, human experts are hired for rating the node quality, but the cost is dramatic. Following Wang et al. (2024c), we simply assign score of +1 or -1 to each node by indicating whether it can reach the final state that indicates successful proving of qk. This approximation has been proven effective in the experiments of Wang et al. (2024c). As the results, we obtain PRM dataset Dv = [(qk, nk ), . . . ] with lk indicating the score for node nk . , lk With dataset Dv, the PRM is then optimized by minimizing the mean squared error for each node: )Dv (vπ ϕ = vπ ϕ(qk, nk ) lk )2 (qk,nk ,lk (8) Similar to Wang et al. (2024c), vπ token. We use the scalar prediction at the last token of each state as the value. ϕ is LLM with an MLP layer on top to output scalar for each 4 Technical Report Distance Critic (DC) PRM only captures the possibility of proving the input statement starting from tree node n, therefore we further design distance-based critic model to predict the estimated remaining number of steps to prove from n. One naive solution can be training the critic model to directly predict the distance, while it is likely to suffer from data sparsity issue. When the state is very complex, it is very difficult to predict an accurate number of the remaining steps. We train the critic model to predict not only the exact number of remaining steps but also the range in which this number falls, represented using balanced binary tree structure. This approach helps mitigate the data sparsity issue by enabling the model to make predictions in hierarchical, coarse-to-fine manner. Specifically, the critic model is trained to identify path on the balanced binary tree, progressively narrowing down to the exact number of remaining steps. Figure 2 illustrates 4-level binary tree capable of representing numbers from 1 to 8. Each level of the tree corresponds to progressively finer division of the numerical range. For instance, at the second level, there are two nodes: 1/2 and 2/2, which split the entire range into left half [1, 4] and right half [5, 8]. If number falls within the range [1, 4], the corresponding path through the tree includes the node 1/2. Empirically, nodes at higher levels of the tree are easier to predict than leaf nodes. This hierarchical structure helps reduce prediction errors during tree search by enabling the model to first make broader, more reliable predictions before refining them. For example, two numbers that are close in distance share common prefix in their paths on the tree. Therefore, representing numbers using their paths on the tree effectively captures the distance relationship between them, enabling the model to predict this information more directly and intuitively. root 1/2 2/2 1/4 2/ 3/4 4/4 1/8 2/8 3/8 4/ 5/8 6/8 7/8 8/8 Figure 2: Balanced binary tree structure for coarse-to-fine number representation. When the level of tree is 4, the max number can be represented is 8. The path of number 6 is root 2/2 3/4 6/8. The tuple associated with the path is (2, 3, 6). In practice, we use an 8-level binary tree to represent numbers up to 64, where each node on the tree is represented by separate special token. For instance, 1/2 is represented by the token <num-1-of-2>, and 5/8 is represented by <num-5-of-8>. When constructing training data, if the number of remaining steps exceeds 64, we set it to 64. path on the binary tree is essentially tuple of numbers, such as (2,3,6) shown in Figure 2. During the tree-search stage with the distance critic, we compare two states by directly comparing their corresponding tuples. This approach inherently evaluates states in coarse-to-fine manner."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setup Benchmarks We evaluate theorem-proving performance on the following benchmarks to pinpoint the effectiveness of each proposed module: MiniF2F (Zheng et al., 2022) examines LLMs automatic formal problem-solving skills targeted at high-school-level exercises and competitions, such as AMC, AIME, and IMO, with particular focus on algebra and number theory. The benchmark comprises 244 validation problems and 244 test problems. 5 Technical Report System Model Size Sample Budget MiniF2F-test Whole-Proof Generation Methods DeepSeek-Prover-V1.5-RL+MCTS DeepSeek-Prover-V1.5-RL+RMaxTS Interactive Step Proving Methods Lean-STaR+BFS+CG InternLM2.5-StepProver+BFS InternLM2.5-StepProver+BFS+CG HunyuanProver v16+BFS HunyuanProver v16+BFS+DC 7B 7B 7B 7B 7B 7B 7B 16 6400 32 6400 64 1 50 256 32 600 256 32 600 600 8 400 600 8 400 60.2% 63.5% 46.3% 59.4% 65.9% 64.8% 68.4% Table 1: Main comparison regarding accuracy and sampling cost with other systems on MiniF2Ftest. CG indicates critic-guided search, while DC represents taking our proposed distance critic as guidance. For BFS methods, the cost is represented as #Pass #Beam #Iteration, while for MCTS, the cost is defined as #Pass #Iteration. Inference We use the LEAN engine from LeanDojo (Yang et al., 2024) as the engine to conduct both tactic data generation and benchmark evaluation. Whole and per-step timeout for tactic executing in LEAN are set to 3600 seconds and 60 seconds, respectively. At most 800 search steps are conducted for both BFS and MCTS. For each search step, = 8 tactics are sampled from the given LEAN state under temperatures 0.7, 0.8, 1.0 and 1.1, where two tactic are sampled under each temperature. These temperature values are empirically decided. Finetuning Hyperparameters Our prover is obtained by fine-tuning HUNYUAN 7B model on our self-generated tactic data. During fine-tuning, at most 4 epochs are conducted and checkpoint is selected based on miniF2F valid set. The maximum sequence length, learning rate, minimal learning rate and batch size are set to 4096, 2 105, 1 106 and 256, respectively. Cosine learning schedule is used. Comparing Systems We compare HunyuanProver with former state-of-the-art systems. Among them, Lean-STaR (Lin et al., 2024) and InternLM2.5-StepProver (Wu et al., 2024) are interactive step-level proving methods, while on the other hand, DeepSeek-Prover-V1.5 (Xin et al., 2024b) is whole-proof generation method. 4.2 Results and Analysis Table 1 shows the comparison between HunyuanProver with other existing state-of-the-art systems. Among these systems, our HunyuanProver with distance-tritic as guidance shows the best accuracy on miniF2F, advancing the previous SOTA system (InternLM2.5-StepProver+BFS+CG) by 2.5% points using less search budget. Though InternLM2.5-StepProver+BFS+CG is slightly better than DeepSeek-Prover-V1.5-RL+MCTS, the later does not take any critic for guidance. We believe MCTS can be more beneficial to the task of automatic theorem proving, and this potential has not been fully revealed. In addition, we can also probe the importance of critic guidance by incorporating DCG on HunyuanProver, which shows 3.6%-point gain. Effectiveness of Iterative Tactic Data Generation Figure 3 visualizes the changes regarding miniF2F-test accuracy and the amount of training data during the prover improving process based on iterative tactic data generation. We initially see performance boosts in early iterations until version v8, and then minor improvements are observed by further increase the number of training tokens from roughly 2.75B (v8) to 4.25B (v12). After v12, we remove some data generated from early iterations(before v8), Most of the removed training data are relatively easy statements.We can see the performance boost is achieved by removing these easy data. This highlights the importance of data selection in the iterative improving process. Effectiveness of Different Critics and Tree Search Methods As shown in Table 2, we conduct an ablation study on different search and critic combinations under three different versions of Hunyuan6 Technical Report Figure 3: The trend regarding miniF2F-test accuracy and the number of finetuning tokens during the iterative tactic data generation process for prover improving, where represents version. The version number is approximately equivalent to the number of iterations. After v12, we remove some easy training data. BFS with policy confidence as the critic is adopted. Model + Search MiniF2F-test HUNYUANPROVER + (BFS, PC) + (MCTS, PRM) HUNYUANPROVER V14 + (BFS, PC) + (BFS, DC) + (MCTS, PRM) HUNYUANPROVER V16 + (BFS, PC) + (BFS, DC) 61.07% 62.29% 62.70% 65.57% 66.39% 64.75% 68.44% Table 2: Ablation study on different variations of guided tree search. Prover. Due to limitation on time and computation resources, MCTS is executed together with PRM. The MCTS-with-PRM combination is consistently better than the BFS with policy confidence. Due to the limitation of time and computation resources, we leave separately examining the effectiveness of MCTS and PRM in future work. In addition, simply replacing policy confidence with distance critic is also significant effective. Since both PRM and DC only require prover generated data with natural labels, thus they are both superior choices than policy confidence as critic. Figure 4 visualizes the distribution over solved miniF2F-test theorems for HunyuanProver v16 and HunyuanProver v16+DC. HunyuanProver v16+DC finds more deep proofs compared to HunyuanProver v16 in miniF2F. Interestingly, HunyuanProver v16+DC takes more steps than HunyuanProver v16 on some of the easiest theorems. We follow previous work to calculate the proof length based on the number of steps in the shortest proof for each theorem."
        },
        {
            "title": "5 Conclusions",
            "content": "In this report, we present HunyuanProver, system that enhances automatic theorem-proving capabilities using LEAN through iterative tactic data generation and guided tree search. The iterative tactic data generation method expands the training dataset by nearly 40-fold, resulting in signif7 Technical Report Figure 4: Proof length distributions of BFS using policy confidence and distance critic respectively for guidance. icant performance improvement. Meanwhile, critic-guided tree search algorithms further enhance the overall effectiveness of the prover. Future work includes better curation for prover training data and exploring other more cost-efficient tree search algorithms such as Q* (Wang et al., 2024a;b)."
        },
        {
            "title": "References",
            "content": "Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Albert Qiaochu Jiang, Wenda Li, and Mateja Jamnik. Multilingual mathematical autoformalization, 2024. URL https://openreview.net/forum?id=QqdloE1QH2. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface. co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. Haohan Lin, Zhiqing Sun, Sean Welleck, and Yiming Yang. Lean-star: Learning to interleave thinking and proving. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming language. In Automated DeductionCADE 28: 28th International Conference on Automated Deduction, Virtual Event, July 1215, 2021, Proceedings 28, pp. 625635. Springer, 2021. LC PAULSON. Isabelle: generic theorem prover. Lecture Notes in Computer Science, 828, 1994. 8 Technical Report Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward selfimprovement of llms via imagination, searching, and criticizing. In Proceedings of Neurips 2024, 2024. Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Dian Yu, Haitao Mi, Jinsong Su, and Dong Yu. Litesearch: Efficacious tree search for llm. arXiv preprint arXiv:2407.00320, 2024a. Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. arXiv preprint Improving multi-step reasoning for llms with deliberative planning. Q*: arXiv:2406.14283, 2024b. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024c. Chenrui Wei, Mengzhou Sun, and Wei Wang. Proving olympiad algebraic inequalities without human demonstrations, 2024. URL https://openreview.net/forum?id=8kFctyli9H. Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. Internlm2. 5-stepprover: Advancing automated theorem proving via expert iteration on large-scale lean problems. arXiv preprint arXiv:2410.15700, 2024. Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. arXiv preprint arXiv:2405.14333, 2024a. Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024b. URL https://arxiv.org/ abs/2408.08152. Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. arXiv preprint arXiv:2408.08152, 2024c. Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrievalaugmented language models. Advances in Neural Information Processing Systems, 36, 2024. Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, and Kai Chen. Lean workbook: large-scale lean problem set formalized from natural language math problems. arXiv preprint arXiv:2406.03847, 2024. Longhui Yu, Weisen Jiang, Han Shi, YU Jincheng, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. minif2f: cross-system benchmark for formal olympiad-level mathematics. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=9ZPegFuFTFv. 9 Technical Report"
        },
        {
            "title": "A Examples Theorems Proved by HunyuanProver",
            "content": "Example 1 (From Chinese High School Mathematics League) Natural Language Problem The sequences {an} and {bn} satisfy a1 = 1, b1 = 2, an+1 = 1+an+anbn 1+bn+anbn an . Prove that : a2008 < 5. bn , bn+1 = theorem theorem_11044972_a81f_4ea2_afa4_339d23e89245 (a : R) (a1 : 1 = 1) (b1 : 1 = 2) (a_rec : n, (n + 1) = (1 + + * n) / n) (b_rec : n, (n + 1) = (1 + + * n) / n) : 2008 < 5 := by induction 2008 <;> simp_all [Nat.add_succ, Nat.succ_add, Nat.mul_succ, Nat.succ_mul] contrapose! a1 all_goals have h1 : 0 = 0 := fun => by simp [h] at b1 field_simp at b1 intro all_goals have := a_rec 0 have := b_rec 0 simp_all [div_eq_mul_inv, mul_add, add_mul, mul_comm, mul_assoc, mul_left_comm] field_simp at have h2 : 0 = 0 := fun hb => by simp [hb] at field_simp at nlinarith have h2 : 0 = 0 := fun => by simp [h] at a1 field_simp at a1 b1 have h3 : 0 < 1 + 0 + 0 * 0 := by nlinarith have h4 := mul_self_pos.mpr h1 rw [div_lt_iff] <;> nlinarith 10 Technical Report Example 2 (From IMO 1962 P2) Natural Language Problem Determine all real numbers which satisfy the inequality: (cid:113) 3 + 1 > (cid:34) Show that it is 1, 1 127 32 (cid:33) . 1 2 theorem imo_1962_p2 (x : R) (h0 : 0 3 - x) (h1 : 0 + 1) (h2 : 1 / 2 < Real.sqrt (Real.sqrt (3 - x) - Real.sqrt (x + 1))) : -1 < 1 - Real.sqrt 127 / 32 := by rw [ sub_nonneg] at h0 h1 refine by linarith [Real.sqrt_nonneg (3 - x), Real.sqrt_nonneg (x + 1)], _ contrapose! h2 norm_num at h0 h1 rw [Real.sqrt_le_iff] norm_num rw [Real.sqrt_le_left] rw [sq, add_mul_self_eq] field_simp norm_num rw [ sub_nonneg] swap refine add_nonneg _ (sqrt_nonneg _) linarith [Real.sqrt_nonneg (x + 1)] have h3 : 0 Real.sqrt (x + 1) := sqrt_nonneg _ nlinarith [Real.sq_sqrt h1, Real.sq_sqrt (by positivity : (0:R) 127)] Example 3 (From lean workbook 2061) Natural Language Problem Prove the theorem: Let a, b, be three integer numbers so that abc = 0. Then the equation ax + by = has at least an integer solution if and only if the greatest common divisor of the numbers and divides the number c. theorem lean_workbook_2061 (a : Z) (habc : * * = 0) : ( : Z, * + * = c) (gcd b) := by constructor rintro x, y, rw [ h] exact dvd_add (dvd_mul_of_dvd_left (GCDMonoid.gcd_dvd_left b) x) (dvd_mul_of_dvd_left (GCDMonoid.gcd_dvd_right b) y) intro obtain x, y, hxy := exists_gcd_eq_mul_add_mul obtain z, rfl := exact * z, * z, by rw [ mul_assoc, mul_assoc, hxy]; ring 11 Technical Report Example 4 (From AIPS) Natural Language Problem abc/(a(cid:112)(a3 + c3) + b(cid:112)(a3 + b3) + c(cid:112)(b3 + c3)) <= (a2 + b2)(a2 + c2)(b2 + c2)/(2((a2 + b2)(a2 + c2)(cid:112)(a3 + c3) + (a2 + b2)(cid:112)(a3 + b3)(b2 + c2) + (a2 + c2)(b2 + c2)(cid:112)(b3 + c3))) theorem theorem_dc0af938_4a25_4093_8ed3_7f4999a556ed {a : R} (ha : 0 < a) (hb : 0 < b) (hc : 0 < c) : * * / (a * Real.sqrt (a ˆ 3 + ˆ 3) + * Real.sqrt (a ˆ 3 + ˆ 3) + * Real.sqrt (b ˆ 3 + ˆ 3)) (a ˆ 2 + ˆ 2) * (a ˆ 2 + ˆ 2) * (b ˆ 2 + ˆ 2) / (2 * (a ˆ 2 + ˆ 2) * (a ˆ 2 + ˆ 2) * Real.sqrt (a ˆ 3 + ˆ 3) + 2 * (a ˆ 2 + ˆ 2) * Real.sqrt (a ˆ 3 + ˆ 3) * (b ˆ 2 + ˆ 2) + 2 * (a ˆ 2 + ˆ 2) * (b ˆ 2 + ˆ 2) * Real.sqrt (b ˆ 3 + ˆ 3)) := by have h1 : 0 (a - b) ˆ 2 * (aˆ2 + cˆ2) * (bˆ2 + cˆ2) := by positivity rw [mul_assoc] rw [ sub_nonneg] have h2 : 0 (a - c) ˆ 2 * (a ˆ 2 + ˆ 2) * (b ˆ 2 + ˆ 2) := by positivity have h3 : 0 (b - c) ˆ 2 * (a ˆ 2 + ˆ 2) * (a ˆ 2 + ˆ 2) := by positivity field_simp [ha.ne, hb.ne, hc.ne, h1, h2, h3] have h4 : 0 ˆ 2 * ˆ 2 := mul_nonneg (sq_nonneg _) (sq_nonneg _) have h5 : 0 (a * Real.sqrt (a ˆ 3 + ˆ 3) - * Real.sqrt (a ˆ 3 + ˆ 3)) ˆ 2 := sq_nonneg _ have h6 := sq_nonneg (a * Real.sqrt (bˆ3 + cˆ3) - * Real.sqrt (aˆ3 + cˆ3)) have h7 : 0 (a * Real.sqrt (a ˆ 3 + ˆ 3) - * Real.sqrt (b ˆ 3 + ˆ 3)) ˆ 2 := sq_nonneg _ have h8 : 0 (b * Real.sqrt (a ˆ 3 + ˆ 3) - * Real.sqrt (a ˆ 3 + ˆ 3)) ˆ 2 := sq_nonneg _ have h9 : 0 ˆ 2 * ˆ 2 := mul_nonneg (sq_nonneg _) (sq_nonneg _) have h10 : 0 (bˆ2 + cˆ2) * (aˆ2 + cˆ2) := by positivity have h11 : 0 (aˆ2 + bˆ2) * (aˆ2 + cˆ2) := by positivity have h12 : 0 (aˆ2 + bˆ2) * (bˆ2 + cˆ2) := by positivity have h13 : 0 (a * Real.sqrt (a ˆ 3 + ˆ 3) - * Real.sqrt (a ˆ 3 + ˆ 3)) ˆ 2 := sq_nonneg _ have h14 := sq_nonneg (a * Real.sqrt (b ˆ 3 + ˆ 3) - * Real.sqrt (a ˆ 3 + ˆ 3)) have h15 : 0 (a * Real.sqrt (aˆ3 + cˆ3) - * Real.sqrt (aˆ3 + bˆ3))ˆ2 := sq_nonneg _ have h16 : 0 (a * Real.sqrt (a ˆ 3 + ˆ 3) - * Real.sqrt (b ˆ 3 + ˆ 3)) ˆ 2 := sq_nonneg _ have h17 := sq_nonneg (a * Real.sqrt (bˆ3 + cˆ3) - * Real.sqrt (aˆ3 + cˆ3)) have h18 : 0 (a * Real.sqrt (aˆ3 + cˆ3) - * Real.sqrt (aˆ3 + bˆ3))ˆ2 := sq_nonneg _ have h19 := sq_nonneg (a * Real.sqrt (aˆ3 + cˆ3) - * Real.sqrt (bˆ3 + cˆ3)) have h20 : 0 ˆ 2 * ˆ 2 := mul_nonneg (sq_nonneg _) (sq_nonneg _) have h21 : 0 * Real.sqrt (aˆ3 + cˆ3) := mul_nonneg ha.le (Real.sqrt_nonneg _) have h22 : 0 * Real.sqrt (a ˆ 3 + ˆ 3) := by positivity have h23 : 0 * Real.sqrt (b ˆ 3 + ˆ 3) := by positivity refine div_nonneg (by nlinarith) (by positivity) 12 Technical Report Example 5 (From NuminaMath) (cid:26)21x (x 1) 1 log2 (x > 1) , find the range of that satisfies Natural Language Problem Given the function (x) = (x) 2. A: [1, 2] B: [0, 2] C: [1, +) D: [0, +) answer: [0, +) theorem theorem_5319d37e_b470_42f5_a42f_b4221daceb81 (f : R) (x : R) (hf: = if 1 then 2ˆ(1-x) else 1 - Real.logb 2 x) : 2 0 := by rw [hf] refine fun => _, fun => _ split_ifs at with h_1 contrapose! rw [ Real.rpow_one 2] rw [ rpow_mul (by norm_num)] rw [one_mul, Real.rpow_lt_rpow_left_iff (by norm_num)] linarith all_goals aesop case neg => linarith rw [ Real.rpow_one 2] rw [ Real.rpow_mul zero_lt_two.le] rw [one_mul, Real.rpow_le_rpow_left_iff (by norm_num)] linarith linarith [logb_pos (show (1 : R) < 2 by norm_num) <_>]"
        },
        {
            "title": "B Policy Prompt",
            "content": "An example of the policy model Prompt Given the Lean 4 tactic state, suggest next tactic. Tactic state: : hab : > 0 > 0 h1 : 0 < Real.log 2 : Real.log (a * b) = Real.log (2 ˆ 6) + 16 Next tactic: Response rw [Real.log_mul] at 13 Technical Report"
        },
        {
            "title": "C Distance Critic Prompt",
            "content": "An example of the critic model Prompt Here is Lean4 state: case refine_1.intro : : ( Set.Icc 0 1, rexp x) x, ˆ 2 + 4 * + = 0 hx1 : hx2 : hx1 ˆ 2 + 4 * hx1 + = 0 Set.Icc (rexp 1) 4 case refine_2 : : Set.Icc (rexp 1) 4 : hx : Set.Icc 0 1 rexp case refine_3 : : Set.Icc (rexp 1) 4 x, ˆ 2 + 4 * + = 0 ---------------- Please tell me how many more tactic steps are needed to finish this state. Response . . . Let me think step by step. . . . So, there is <num_box><num-1-of-2><num-1-of-4><num-1-of-8> <num-2-of-16><num-4-of-32><num-7-of-64></num_box> more tactic steps are needed to finish this state."
        }
    ],
    "affiliations": [
        "Tencent",
        "Tencent Hunyuan Teams"
    ]
}