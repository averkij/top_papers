{
    "paper_title": "DiMeR: Disentangled Mesh Reconstruction Model",
    "authors": [
        "Lutao Jiang",
        "Jiantao Lin",
        "Kanghao Chen",
        "Wenhang Ge",
        "Xin Yang",
        "Yifan Jiang",
        "Yuanhuiyi Lyu",
        "Xu Zheng",
        "Yingcong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the advent of large-scale 3D datasets, feed-forward 3D generative models, such as the Large Reconstruction Model (LRM), have gained significant attention and achieved remarkable success. However, we observe that RGB images often lead to conflicting training objectives and lack the necessary clarity for geometry reconstruction. In this paper, we revisit the inductive biases associated with mesh reconstruction and introduce DiMeR, a novel disentangled dual-stream feed-forward model for sparse-view mesh reconstruction. The key idea is to disentangle both the input and framework into geometry and texture parts, thereby reducing the training difficulty for each part according to the Principle of Occam's Razor. Given that normal maps are strictly consistent with geometry and accurately capture surface variations, we utilize normal maps as exclusive input for the geometry branch to reduce the complexity between the network's input and output. Moreover, we improve the mesh extraction algorithm to introduce 3D ground truth supervision. As for texture branch, we use RGB images as input to obtain the textured mesh. Overall, DiMeR demonstrates robust capabilities across various tasks, including sparse-view reconstruction, single-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR significantly outperforms previous methods, achieving over 30% improvement in Chamfer Distance on the GSO and OmniObject3D dataset."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 0 7 6 7 1 . 4 0 5 2 : r DiMeR: Disentangled Mesh Reconstruction Model Lutao Jiang1 Jiantao Lin1 Kanghao Chen1 Wenhang Ge1 Xin Yang1,2 Yifan Jiang1 Yuanhuiyi Lyu1 Xu Zheng1 Yingcong Chen1,2 1HKUST(GZ) 2HKUST ljiang553@connect.hkust-gz.edu.cn, yingcongchen@ust.hk Project Page: https://lutao2021.github.io/DiMeR_page/ Figure 1. Top: the generation demos from images. Bottom: the generation results from texts."
        },
        {
            "title": "Abstract",
            "content": "With the advent of large-scale 3D datasets, feed-forward 3D generative models, such as the Large Reconstruction Model (LRM), have gained significant attention and achieved remarkable success. However, we observe that RGB images often lead to conflicting training objectives and lack the necessary clarity for geometry reconstruction. In this paper, we revisit the inductive biases associated with mesh reconstruction and introduce DiMeR, novel disentangled dual-stream feed-forward model for sparse-view mesh reconstruction. The key idea is to disentangle both the input and framework into geometry and texture parts, thereby Contribute equally. Corresponding author. reducing the training difficulty for each part according to the Principle of Occams Razor. Given that normal maps are strictly consistent with geometry and accurately capture surface variations, we utilize normal maps as exclusive input for the geometry branch to reduce the complexity between the networks input and output. Moreover, we improve the mesh extraction algorithm to introduce 3D ground truth supervision. As for texture branch, we use RGB images as input to obtain the textured mesh. Overall, DiMeR demonstrates robust capabilities across various tasks, including sparse-view reconstruction, single-imageto-3D, and text-to-3D. Numerous experiments show that DiMeR significantly outperforms previous methods, achieving over 30% improvement in Chamfer Distance on the GSO and OmniObject3D dataset. 1. Introduction The tasks of 3D reconstruction and generation have garnered significant attention, largely due to the advancements made by NeRF [41] and 3DGS [22]. The development of these techniques holds substantial potential for applications in virtual reality and gaming production [23]. However, directly transforming the NeRF network or 3DGS into mesh representation poses complex challenge. In this paper, we focus on mesh representation, which is easy to adapt to downstream applications. Leveraging the introduction of the extensive 3D dataset, Objaverse [9, 10], numerous 3D reconstruction and generative models emerge. One notable advancement is the LRM [18], which pioneers the direct generation of NeRF model from single image. Building on LRM, Instant3D [24] extends the input to sparse set of views, thereby enhancing the accuracy of the generated NeRF models. Subsequent reconstruction models [13, 33, 64, 66, 73, 75] basically adopt this framework based on RGB images input. However, the reliance solely on RGB images as input for sparse-view reconstruction models can lead to significant training ambiguity. As illustrated in Fig. 2(a), the points of some dices are represented on the texture of smooth surface, while some points are carved on the geometry directly. When training the model, such ambiguous data samples in the dataset introduce conflicting training objectives, which hinder the overall training process. Furthermore, as shown in Fig. 2(b), the complex texture buries the actual geometry, making it hard to extract the geometry from such RGB images. To mitigate this issue, existing methods begin to integrate geometric information into the input data. For instance, CRM [64] employs Canonical Coordinate Map (CCM) to integrate point cloud as input alongside RGB images, while MeshFormer [33] utilizes normal maps. Despite these advancements, RGB images always serve as an essential part of the network input, continuing to introduce ambiguity during the training process. After training, these methods are significantly influenced by the RGB input, leading to rough and uneven surface when put into practical usage (See Fig. 6). On the other hand, most of the existing mesh reconstruction methods employ FlexiCubes [48] to extract the mesh and utilize differential rasterization for optimization. However, the Signed Distance Field (SDF) Grid defined in FlexiCubes only promises the meaning of positive and negative sign for surface extraction, which is difficult to apply 3D supervision. Moreover, its regularization losses lead to instability in training. In this paper, we present DiMeR, dual-stream feedforward sparse-view mesh reconstruction model, which is disentangled into separate geometry and texture branches. To address the challenge of training ambiguity, we leverage the inductive bias derived from the consistency between normal maps and 3D models. As demonstrated in Figure 2. The illustration of training ambiguity. (a) shows the conflict objective in the training set. (b) demonstrates that its hard to extract the geometry from the RGB images. All the demos are from the Objaverse dataset [9]. Fig. 2, the normal maps consistently align with the surface of the 3D model, offering more reliable input format for geometry reconstruction. Building on this inductive bias and the Principle of Occams Razor, we exclusively utilize normal maps as the sole input for the geometry branch to reduce the training difficulty. Naturally, we focus solely on geometry-related loss, including normal, depth, and mask to supervise this branch. Moreover, an accurate geometry should be capable of rendering the light map correctly under arbitrary environmental conditions and various materials. Therefore, we add the light map supervision signal by placing the predicted blank mesh model in multiple environments with randomly assigned materials. To address the limitations remaining in FlexiCubes, we replace the original regularization losses with eikonal loss [14] and 3D ground truth supervision. Furthermore, we simplify the network design for higher efficiency. Finally, as for the texture branch, we predict the texture field for the predicted mesh based on the RGB input, and we use RGB loss as supervision. Our DiMeR model is capable of effectively handling various tasks, including sparse-view reconstruction, singleimage-to-3D, and text-to-3D. (I) For the sparse-view reconstruction task, it is feasible to apply recent normal prediction models (e.g. StableNormal [76] and Lotus [17]) to obtain the normal map from RGB input for mesh reconstruction based on the geometry branch of our DiMer, while using RGB to predict the texture with the texture branch. (II) For the single-image-to-3D task, we begin by using 2.5D multi-view diffusion models zero123++ [49] to synthesize sparse-view images. Then, the pipeline (I) can be applied. (III) For the text-to-3D task, we construct the pipeline starting with Kiss3DGen [32], which is text-to-sparse-view diffusion model with corresponding normal map output. We can directly apply our DiMer for the output of Kiss3DGen. Our contributions can be summarized as follows: Upon inductive bias between normal maps and 3D geometry, we propose disentangled framework to predict geometry from normal and texture from RGB separately. We introduce the 3D supervision to enhance training stability and simplify the network in FlexiCubes. DiMeR demonstrates robustness on reconstruction, single-image-to-3D, and text-to-3D tasks. Numerous experiments show our method can surpass the previous mesh reconstruction model by large margin, reducing chamfer distance by over 30%. 2. Related Works 2.1. 3D Generative Models Building upon advancements in 2D diffusion models, DreamFusion [43] introduced score distillation sampling (SDS) to train 3D representation models like NeRF [41] and 3DGS [22] based on text input. Subsequently, numerous methods have been developed to enhance this approach [1, 4, 8, 20, 28, 29, 29, 31, 38, 40, 45, 50, 53, 60, 62, 78, 85]. However, significant limitation of these methods is the need to train separate 3D model for each text input, which can take tens of minutes or even hours per text. Some approaches attempt to address this by employing SDS to train feed-forward network [19, 25, 36, 44], but these are limited to few specific text subjects, reducing the diversity of the outputs. Recently, the introduction of large-scale 3D datasets, such as Objaverse [9, 10], has enabled models like LRMs [18, 57] to explore feed-forward reconstruction from single image. Following this, several methods have been developed to create sparse-view reconstruction models [24, 55, 74, 81] based on NeRF or 3DGS. However, these approaches still struggle to support real-world applications effectively. To address these challenges, leveraging differential marching cube algorithms [48, 65], some methods focus on direct mesh generation [13, 33, 64, 66, 73]. Additionally, several 3D diffusion models [15, 16, 27, 30, 46, 47, 52, 71, 79, 80, 82] emerge, but they are limited to the generation task and lack strict correspondence with the input image. Moreover, their inference times range from tens of seconds to several minutes. Inspired by auto-regressive (AR) models [56, 72, 84], some researchers have shifted focus to mesh AR generation [57, 51, 54, 63, 67]. However, these methods typically require the number of mesh faces to be fewer than 2,000, and they exhibit low robustness. Concurrently, Hi3DGen [77] also found that exclusive utilization of normal map can enhance the quality of geometry and implemented diffusion model based on this. In this paper, we focus on feed-forward sparse-view mesh reconstruction and generation. Differently, we disentangle the framework into dual branches that predict geometry solely from normal and predict texture from RGB. To align the function of each branch, we carefully choose the unambiguous supervision signal for both branches. 2.2. 2.5D Diffusion Model 2.5D diffusion models are designed to generate multi-view images instead of directly producing corresponding 3D models. This approach is gaining popularity due to the relative simplicity of its task definition, where multi-view images are synthesized first, followed by the use of sparseview reconstruction models to complete the 3D model generation process. Zero123 [34] introduces explicit control by embedding camera parameters into the conditions of 2D diffusion models. Following, many methods have achieved significant success to synthesis the multi-view images and normals [24, 26, 32, 35, 37, 39, 49, 50, 59, 61, 69]. We employ the image-input 2.5D model, such as zero123++ [49], to perform the image-to-3D task, while we use the textinput 2.5D diffusion model, such as Kiss3DGen, to accomplish the text-to-3D task. With the advancement of such models, DiMeR has the potential to further enhance generation quality. 3. Method The objective of our DiMeR is to reconstruct the 3D mesh geometry from normal maps and derive texture information from RGB images. The core idea is to identify the necessary inductive biases for different stages of 3D mesh reconstruction. Specifically, geometry reconstruction does not require texture information, as RGB textures often introduce ambiguities in important geometric cues. Leveraging the inductive bias that normal maps are inherently consistent with realistic geometry, we focus on learning the geometry reconstruction solely from normal maps (Sec. 3.1). This strategy eliminates training ambiguity  (Fig. 2)  and simplifies the overall training process. Next, we predict the texture from RGB images (Sec. 3.2). Finally, we demonstrate how our DiMeR finishes the image/text-to-3D tasks (Sec. 3.3). 3.1. Geometry Branch As demonstrated in Fig. 2, the reliance on RGB produces conflicting optimization objectives in the dataset, leading to training ambituity. In contrast, normal maps are inherently consistent with 3D geometry and accurately capture surface variations. Guided by the Principle of Occams Razor, we use normal maps as the exclusive input for the geometry branch, thereby eliminating the influence of irrelevant information from the RGB images. This design establishes clearer relationship between the networks inputs and outputs, ultimately reducing the training complexity. Network Structure. As shown in Fig. 3, the geometry branch of our DiMeR model initiates with normal maps RKHW 3 of randomly selected views, alongside their associated camera embeddings ζ RK16. We opt for random sampling of input views to improve the models capability to interpret camera embeddings from arbitrary directions and add slight noise to them, thereby enhancing robustness and reducing dependency on specific input configurations. Furthermore, this also reduces the requirements for user input, allowing users to provide input from unfixed view directions. Then, we use the Normal EnFigure 3. The framework of our DiMeR. The upper part is the geometry branch and we exclude the RGB input and supervision. The lower part is the texture branch. coder to encode these normal maps with their camera embeddings ζ into patch-wise embeddings Pg RKDC by an encoder with ViT [11] architecture, where is the number of patches of each view and is the dimension of the feature. Similar to the approach taken by LRM [66], we utilize Triplane Decoder to gather information from the Patch Embedding Pg using several transformer layers [58] to synthesize triplane [3] features Fg R3H Cg . Subsequently, we extract an SDF grid from the triplane features Fg to apply the differential isosurface construction algorithm, FlexiCubes [48], to obtain the vertices and faces for the mesh. Finally, we can rasterize the mesh to get the normal maps, masks, and depth maps for arbitrary views. By providing the environment map and assigning different materials (metallic and roughness) to the mesh, we can render the light map (including specular and diffuse) by physical-based rendering (PBR), for enhancing the supervision from different lighting conditions, which will be introduced in the next part. Original FlexiCubes algorithm requires two MLP networks to allow the different weights (each edge and vertex in the grid) and the deformation of the grid. However, this introduces too much burden of computation and GPU memory. Specifically, for 3 grid, it needs to compute the deformation of 3 vertices and the weight of 12 3 edges and 8 3 vertices. Though it is powerful for the tasks of Flexicubes itself, extensive experiments prove that these components contribute disproportionately high computational overhead with minimal performance gains. As shown in Tab. 6, we found that removing these networks from the pre-trained model does not adversely affect performance. Therefore, to enable higher efficient training and higher spatial resolutions, we discard these two components to simplify the network structure. Losses. Given the inherent ambiguity associated with RGB data in the training set  (Fig. 2)  , we exclude RGB loss to enhance training stability. Consequently, we now exclusively employ geometry-related losses to supervise the geometry branch of our model. In its original implementation, FlexiCubes incorporates three regularization losses to regularize the SDF grid values generated by the network. However, extensive experimentation reveals that this approach yielded low stability [13, 73]. Furthermore, the design does not produce true SDF representations. To address these issues, we employ the eikonal loss [14] to regularize the whole space as the SDF field, specifically by ensuring the norm of the gradient with respect to the coordinates is normalized to 1. Nevertheless, computing the derivative for 3 grid poses significant challenges in terms of computational and GPU memory costs and potential overfitting at specific grid positions. To mitigate this, we propose randomly sampling positions within the space to compute the eikonal expectation loss, effectively reducing computational demands while maintaining the integrity of the regularization, i.e., Leik = Ex(xSDF(x)2 1)2, R3 nif orm(1, 1), (1) where we sample 200K in each iteration to calculate the expectation. Moreover, we use the GT SDF value to supervise the SDF value of grid vertices RN 33 in FlexiCubes, Lsdf = SDF(v) SDFGT(v))2 2. (2) To save the computation cost, we cache the grid of these SDF values for each object in the training set. Drawing inspiration from Photometric Stereo [68], we propose the integration of Physically-based Rendering (PBR) [21] losses. The premise is that if the specular and diffuse light maps under different environmental lighting conditions and materials can be accurately rendered in PBR, then the geometry of the predicted mesh model can be deemed correct. Therefore, we introduce the statistics expectation loss of PBR to supervise the geometry branch, Lspec = Ee,m,r (cid:16) Spec( ˆO, e, m, r) Spec(O, e, m, r) (cid:16) Spec( ˆO, e, m, r), Spec(O, e, m, r) (cid:17) (cid:17) , (3) + LPIPS Ldif = Ee,m,r (cid:16) Diff( ˆO, e, m, r) Diff(O, e, m, r) (cid:16) (cid:17) Diff( ˆO, e, m, r), Diff(O, e, m, r) (cid:17)2 , + LPIPS (4) where ˆO is the predicted mesh model, is the ground truth mesh model, e, m, are the randomly sampled environment, metallic, and roughness, Spec() and Diff() are the rendering functions of specular and diffuse light map, LPIPS() is the perception loss [83]. Notably, during the training, we sample different environment, metallic, and roughness to render the light maps for single object. We also employ the commonly used normal, depth, and mask losses to supervise the geometry branch. Specifically, Lnor = MGT (1 ˆN NGT), Ldep = MGT ˆD DGT, Lmask = ( ˆM MGT)2, (7) where denotes element-wise production, MGT and ˆM are the rendered mask from ground truth mesh model and predicted mesh model, similarly, and represent normal map and depth map. (6) (5) In general, the overall loss function is Lg = Leik +Lsdf +Lspec +Ldif +Lnor +Ldep +Lmask. (8) 3.2. Texture Branch Network Structure. As demonstrated in Fig. 3, the texture branch starts from RGB images RKHW 3 with the camera embeddings ζ to finish the colorization. Similar as the geometry branch, we use ViT-based Image Encoder to get the Patch Embedding Pc RKDC and utilize Triplane Decoder to assemble the information from Pc to get the triplane features Fc R3H Cc for the texture field representation [42]. Given the predicted shape from the geometry branch, we rasterize the vertex coordinates into image space, CoordI = Rast(v, Camera), (9) where the pixel value of CoordI RHW 3 is the global coordinate. Next, we query the texture feature FI RHW on triplane Fc for each pixel, FI = Sample(CoordI, Fc). (10) Finally, we decode the color feature to predict the image ˆI = RGB Decoder(FI). (11) Figure 4. The pipeline for reconstruction, single-image-to-3D, and text-to-3D tasks. 3.3. Applications Besides the sparse-view reconstruction task, our DiMeR is also capable of performing image/text-to-3D tasks. Single-image-to-3D. Given the input image, we first employ Zero123++ [49] to generate six images from different viewpoints. Specifically, the output from zero123++ consists of six views, including the combinations of azimuth and elevation, (30, 20), (90, -10), (150, 20), (210, -10), (270, 20), and (330, -10). Next, we apply the SoTA normal prediction model Lotus [17] or StableNormal [76] to predict the normal maps for these six views. Since the predicted normal maps are initially in the local camera coordinate system, we subsequently transform them into the global coordinate system using the transformation matrices corresponding to the six view directions. Finally, we feed the six transformed normal maps and the RGB images into our DiMeR model to generate the textured mesh. Text-to-3D. This task is approached through two distinct pipelines: (I) The first pipeline involves using text-toimage model to generate an RGB image from the input text. Subsequently, we apply the single-image-to-3D pipeline to complete the reconstruction. (II) With the advancement of diffusion models, Kiss3DGen [32] fine-tunes the SoTA textto-image generative model, FLUX [2], to simultaneously output RGB images along with corresponding normal maps, ensuring high multi-view consistency. Since our DiMeR supports dynamic number of input views, we can directly feed the four views from Kiss3DGen into DiMeR for 3D model reconstruction. The generated high-quality models are presented in Fig. 1 and the supplementary materials. Losses. In this branch, we only use RGB loss to supervise the network. Specifically, 4. Experiment Lt = (ˆI IGT)2 + LPIPS(ˆI, IGT). (12) In this section, we first present the implementation details (Sec. 4.1). For quantitative comparison, the metrics consist"
        },
        {
            "title": "GSO",
            "content": "OmniObject3D CD () F1 () PSNR () SSIM () LPIPS () CD () F1 () PSNR () SSIM () LPIPS ()"
        },
        {
            "title": "InstantMesh\nPRM",
            "content": "DiMeR(Lotus) DiMeR(SN) DiMeR(GT) 0.045 0.041 0.033 0.032 0.028 0.964 0.977 0.988 0.988 0.992 31.7% +0.015 18.51 21.68 22.57 22.89 23.40 +1.72 0.846 0. 0.874 0.875 0.883 0.150 0.126 0.103 0.103 0.095 0.039 0.034 0.034 0.030 0.024 0.983 0. 0.989 0.993 0.996 +0.014 24.6% 29.4% +0.005 18.44 21. 21.88 22.15 23.04 +1.39 0.842 0.865 0.866 0.865 0.871 0.153 0.135 0.126 0.121 0. +0.006 17.0% Table 1. Quantitative results for reconstruction task. DiMeR(Lotus) is the reconstruction result from the normal map predicted by Lotus [17]. DiMeR(SN) is from StableNormal [76]. DiMeR(GT) is from ground truth normal. of Chamfer Distance, F1-score, PSNR, SSIM, and LPIPS (Sec. 4.2). For qualitative comparison, we compare with the mesh reconstruction models and image-to-3D methods (Sec. 4.3). Finally, we provide ablation studies (Sec. 4.4). Please note that we also provide the predicted and generated 3D object files in supplementary materials. 4.1. Implementation Details Datasets. For the training dataset, we use the filtered Objaverse [9] according to the mesh quality, in total of 98, 526 objects. For the test dataset, we choose the widely used GSO [12] and OmniObject3D [70]. We use all 1, 029 objects in the GSO dataset and randomly select 1, 039 objects in OmniObject3D. For the OmniObject3D dataset, we randomly select 5 objects of every class while using all objects for the classes that dont contain 5 objects. Evaluation Protocol. For 3D metrics, we use the commonly used Chamfer Distance (CD) and F1-Score to evaluate the quality of geometry. We sample 32, 000 points on the surface to compute CD and F1-score. For 2D metrics, we utilize 8 views to calculate the PSNR, SSIM, and LPIPS to evaluate the rendering quality. We rescale and utilize the point cloud registration algorithm to align the generated meshes and ground truth meshes for accurate computation of all the metrics. Training. All our experiments are conducted on 8 H100 GPUs. We set the total batch size to 64, with learning rate of 4 106 for geometry branch and 4 105 for texture branch. The resolution of the triplane is 3 64 64, and the SDF grid is 1923. The input resolutions for normal maps and RGB images both are 512 512, with the supervision resolutions also being 512 512. For each predicted object, we place them in 10 different lighting environments and apply 10 different materials, rendering from different 10 views during training. We train the geometry branch for two days and the texture branch for one day. For the texture branch, we initially use geometry to predict the mesh, and the rendered images are subsequently generated based on the predicted mesh. Dataset Metric CRM InstantMesh PRM Trellis DiMeR GSO OmniObject3D CD () F1 () CD () F1 () 0.144 0.066 0.059 0.119 0.052 0.781 0.950 0.961 0.859 0. 0.114 0.074 0.064 0.090 0.060 0.854 0.937 0.957 0.902 0.964 Table 2. Single-image-to-3D task. All the methods use the same single image input. 4.2. Quantitative Comparison Reconstruction Task. As shown in Tab. 1, we compare our DiMeR on the sparse-view reconstruction task using the same 6 randomly sampled input views. Since some sparseview reconstruction methods, like CRM [64], are limited to only support specific views (six orthogonal views), we compare them in single-image-to-3D tasks. Additionally, because MeshFormer [33] is not open-source work, we are unable to perform an accurate quantitative comparison. Therefore, we only provide qualitative visual comparisons. For the comparison, we select state-of-the-art (SoTA) methods that are accessible, including InstantMesh [73] and PRM [13]. Experiments show that our method can surpass the SoTA methods by large margin, whatever using GT (31.7% gain) or predicted normal maps (22.0% gain) from StableNormal-Turbo [76]. Notably, when equipped with normal map prediction models, the input to DiMeR remains the same as the baselines, relying solely on RGB images. Furthermore, following the improvement of normal prediction models, there is still room for continued improvement in the performance of DiMeR. Sinle-Image-to-3D Task. As demonstrated in Tab. 2, we compare our DiMeR with CRM [64], InstantMesh [73], PRM [13], and Trellis [71] using same single image input. Our pipeline for this task is shown in Fig. 4(b), where we use Lotus [17] to predict normal maps from the output of zero123++ [49]. Since the single-image-to-3D problem is inherently ill-posed, the unseen portions of the data cannot be accurately inferred from single image alone. Figure 5. The qualitative comparison for sparse view reconstruction. Consequently, we select 500 relatively clear data points for meaningful and valuable evaluation. Notably, while Trellis produces high-quality mesh generation, issues with consistency in the input image persist. This inconsistency can be attributed to Trellis generative nature rather than being deterministic model. This point is also highlighted in the qualitative comparisons in Fig. 6. In contrast, the reconstruction models, such as our DiMeR, PRM, and InstantMesh, have the advantages for the accurate alignment with input image based on the prediction of zero123++. 4.3. Qualitative Comparison Reconstruction Task. As demonstrated in Fig. 5, we present visual qualitative comparison of various methods. comparison between the rows labeled Ours and Ours (Lotus) shows similar performance, highlighting that normal prediction models effectively support DiMeR. This suggests that DiMeR, when combined with such models, is capable of surpassing previous methods in realistic applications. Furthermore, DiMeR outperforms previous mesh reconstruction models, such as InstantMesh and PRM, in terms of reconstructing finer details. Single-image-to-3D. As shown in Fig. 6, we compare our method with SoTA methods including Trellis [71], InstantMesh [73] and PRM [13], MeshFormer [33], CRM [64]. Notably, since the 3D results for MeshFormer are only available from their project page and the corresponding input images are not provided, we are unable to conduct direct comparison using the same input. In contrast, the other methods use the same input images for comparison. Due to the inherent characteristics of the generative diffusion model, Trellis often generates 3D mesh models that exhibit inconsistencies with the input images, although it maintains high quality. Specifically, the cups holes in the second column and the number of pillars in the third column are mismatched. Moreover, the other methods encounter difficulties in generating holes and rings accurately. In summary, our DiMeR achieves the best consistency and quality. 4.4. Ablation Studies In this section, we validate our key designs. All experiments are conducted based on the GSO dataset. Input Disentanglement. As shown in Tab. 3, we compare the performance of DiMeR using different input formats. The comparison between the first two columns, RGB and RGB+Normal, illustrates that incorporating geometry information results in slight improvement in effectiveness. Furthermore, the third column, labeled Normal, demonstrates significant performance gain over RGB+Normal. functions used in FlexiCubes, performing improved performance. With the regularization loss employed in FlexiCubes, the training process becomes unstable and struggles to proceed beyond 10,000 iterations, resulting in unsatisfactory network convergence. By introducing the eikonal loss and incorporating 3D ground truth, we stabilize the training process, achieving significantly better performance. PBR Loss. As shown in Tab. 5, we validate the effectiveness of the PBR expectation loss (Eq. 3 and Eq. 4). If the lighting map can be accurately computed under varying environmental lighting conditions and across different materials, we can conclude that the predicted mesh aligns well with the ground truth mesh. To achieve this, we assign different materials to the single predicted mesh and place it in various environments. The introduction of PBR losses leads to significant improvements. Deformation and Weight MLP in FlexiCubes. As shown in Tab. 6, we demonstrate that the improvements gained from the deformation and weight MLP are not worthy enough compared with their computational cost. The experiments are conducted using the official pretrained weights of InstantMesh, and similar experiments based on PRM are provided in the supplementary material. Upon removing the deformation network and weight network from FlexiCubes, we observe minimal impact on inference performance, almost no decrease. However, these two networks significantly increase computational workload (about 2.5 computation overhead) and GPU memory consumption (about 1.5 GPU memory occupancy in training). Consequently, we opt to exclude them from DiMeR in order to improve the spatial resolution. 5. Conclusion In this paper, we propose DiMeR, novel disentangled dual-stream mesh reconstruction model, leveraging the inductive bias between normal maps and 3D meshes. Specifically, we exclusively use normal maps as input for the geometry branch and RGB images for the texture branch. Additionally, we introduce 3D supervision to replace the regularization loss in FlexiCubes, alongside PBR expectation losses, to further enhance performance. We also simplify the redundant networks to reduce the GPU memory occupancy for higher spatial resolution. Extensive experiments proves the superiority of DiMeR. Moreover, our better performance across multiple tasks highlights the robustness of our approach. Furthermore, with the ongoing improvements in normal prediction models, there remains potential for DiMeR to achieve even better results in the future. Limitations. 1) On single-image-to-3D task, limited by 2.5D diffusion model, there is still space for improvement, including resolution and accuracy. 2) For the surface with high-frequency, such as sandpaper, it remains challenge due to the relatively limited space resolution and dataset. Figure 6. The qualitative comparison for single-image-to-3D. Please note that the results of MeshFormer are obtained from their project page and do not use the same input as other methods. Input RGB RGB+Normal Normal Method FlexiCubes Ours CD F1 0.041 0.971 0.041 0. 0.028 0.992 CD F1 0.037 0.975 0.028 0.992 Table 3. The ablation studies of different input formats. Table 4. The ablation studies of 3D losses. w/o w/ Method CD F1 GPU Mem Infer CD 0.039 0.973 F1 0.028 0.992 w/ w/o 0.045 0.045 0.964 0.963 73GB 48GB 0.5s 0.2s Table 5. The ablation studies of PBR expectation losses. Table 6. The ablation studies of the effectiveness of Deformation and Weight MLP. GPU Mem is training occupancy. This improvement underscores the strong inductive bias between normal maps and 3D geometry. Additionally, since the input is encoded as patch embeddings, using mixed input produces more patches, resulting in increased GPU memory usage and computational overhead. In contrast, the exclusive use of normal maps maintains the same resource consumption as RGB inputs. Regularization Loss. As demonstrated in Tab. 4, we show that Eq. 1 and Eq. 2 can replace the original loss"
        },
        {
            "title": "References",
            "content": "[1] Haotian Bai, Yuanhuiyi Lyu, Lutao Jiang, Sijia Li, Haonan Lu, Xiaodong Lin, and Lin Wang. Componerf: Text-guided multi-object compositional nerf with editable 3d scene layout. arXiv preprint arXiv:2303.13843, 2023. 3 [2] BlackForestLabs. Flux.1 model family. 2024. 5 [3] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1612316133, 2022. 4 [4] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for arXiv preprint high-quality text-to-3d content creation. arXiv:2303.13873, 2023. 3 [5] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Yanru Wang, Zhibin Wang, Chi Zhang, et al. Meshxl: Neural coordinate field for generative 3d foundation models. arXiv preprint arXiv:2405.20853, 2024. 3 [6] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. Meshanything: Artist-created mesh generation with autoregressive transformers. arXiv preprint arXiv:2406.10163, 2024. [7] Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. Meshanything v2: Artist-created mesh generation with adjacent mesh tokenization. arXiv preprint arXiv:2408.02555, 2024. 3 [8] Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei Zhang, Jian Zhang, and Li Yuan. Progressive3d: Progressively local editing for text-to-3d content creation with complex semantic prompts. In The Twelfth International Conference on Learning Representations. 3 [9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 2, 3, 6 [10] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. 2, [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 4 [12] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 25532560. IEEE, 2022. 6 [13] Wenhang Ge, Jiantao Lin, Guibao Shen, Jiawei Feng, Tao Hu, Xinli Xu, and Ying-Cong Chen. Prm: Photometric arXiv preprint stereo based large reconstruction model. arXiv:2412.07371, 2024. 2, 3, 4, 6, 7 [14] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. In Proceedings of the 37th International Conference on Machine Learning, pages 37893799, 2020. 2, 4 [15] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 3dgen: Triplane latent diffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023. 3 [16] Hao He, Yixun Liang, Luozhou Wang, Yuanhao Cai, Xinli Xu, Hao-Xiang Guo, Xiang Wen, and Yingcong Chen. Lucidfusion: Generating 3d gaussians with arbitrary unposed images. arXiv preprint arXiv:2410.15636, 2024. [17] Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, and YingLotus: Diffusion-based visual foundation Cong Chen. arXiv preprint model for high-quality dense prediction. arXiv:2409.18124, 2024. 2, 5, 6 [18] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 2, 3 [19] Lutao Jiang and Lin Wang. Brightdreamer: Generic 3d gaussian generative framework for fast text-to-3d synthesis. arXiv preprint arXiv:2403.11273, 2024. 3 [20] Lutao Jiang, Hangyu Li, and Lin Wang. general framework to boost 3d gs initialization for text-to-3d generation by lexical richness. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 68036812, 2024. 3 [21] James Kajiya. The rendering equation. In Proceedings of the 13th annual conference on Computer graphics and interactive techniques, pages 143150, 1986. 4 [22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4), 2023. 2, [23] Chenghao Li, Chaoning Zhang, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho Bae, and Choong Seon Hong. Generative ai meets 3d: survey on arXiv preprint arXiv:2305.06131, text-to-3d in aigc era. 2023. 2 [24] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. 2, 3 [25] Ming Li, Pan Zhou, Jia-Wei Liu, Jussi Keppo, Min Lin, Instant3d: Instant textShuicheng Yan, and Xiangyu Xu. to-3d generation, 2023. 3 [26] Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wei Xue, Wenhan Luo, et al. Era3d: high-resolution multiview diffusion using efficient row-wise attention. Advances in Neural Information Processing Systems, 37:5597556000, 2024. 3 [27] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. 3 [28] Zongrui Li, Minghui Hu, Qian Zheng, and Xudong Jiang. Connecting consistency distillation to score distillation for text-to-3d generation. In European Conference on Computer Vision, pages 274291. Springer, 2025. [29] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. arXiv preprint arXiv:2311.11284, 2023. 3 [30] Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, and Yadong Mu. Diffsplat: Repurposing image diffusion models for scalable gaussian splat generation. arXiv preprint arXiv:2501.16764, 2025. 3 [31] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 3 [32] Jiantao Lin, Xin Yang, Meixi Chen, Yingjie Xu, Dongyu Yan, Leyi Wu, Xinli Xu, Lie Xu, Shunsi Zhang, and YingCong Chen. Kiss3dgen: Repurposing image diffusion models for 3d asset generation. arXiv preprint arXiv:2503.01370, 2025. 2, 3, 5 [33] Minghua Liu, Chong Zeng, Xinyue Wei, Ruoxi Shi, Linghao Chen, Chao Xu, Mengqi Zhang, Zhaoning Wang, Xiaoshuai Zhang, Isabella Liu, et al. Meshformer: High-quality mesh generation with 3d-guided reconstruction model. arXiv preprint arXiv:2408.10198, 2024. 2, 3, 6, 7 [34] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. [35] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99709980, 2024. 3 [36] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. Att3d: arXiv preprint Amortized text-to-3d object synthesis. arXiv:2306.07349, 2023. 3 [37] Yuanxun Lu, Jingyang Zhang, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan, Xun Cao, and Yao Yao. Direct2. 5: Diverse text-to-3d generation via multi-view 2.5 diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8744 8753, 2024. 3 [38] Artem Lukoianov, Haitz Saez de Ocariz Borde, Kristjan Greenewald, Vitor Campagnolo Guizilini, Timur Bagautdinov, Vincent Sitzmann, and Justin Solomon. distillation via reparametrized ddim. arXiv:2405.15891, 2024. 3 Score arXiv preprint [39] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and Filippos Iterative multiview diffusion and reKokkinos. construction for high-quality 3d generation. arXiv preprint arXiv:2402.08682, 2024. 3 Im-3d: [40] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1266312673, 2023. 3 [41] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2, 3 [42] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas Geiger. Texture fields: Learning texIn Proceedings of ture representations in function space. the IEEE/CVF international conference on computer vision, pages 45314540, 2019. 5 [43] Ben Poole, Ajay Jain, Jonathan Barron, and Ben MildenIn The hall. Dreamfusion: Text-to-3d using 2d diffusion. Eleventh International Conference on Learning Representations, 2022. [44] Guocheng Qian, Junli Cao, Aliaksandr Siarohin, Yash Kant, Chaoyang Wang, Michael Vasilkovsky, Hsin-Ying Lee, Yuwei Fang, Ivan Skorokhodov, Peiye Zhuang, et al. Atom: Amortized text-to-mesh using 2d diffusion. arXiv preprint arXiv:2402.00867, 2024. 3 [45] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. DreamarXiv booth3d: Subject-driven text-to-3d generation. preprint arXiv:2303.13508, 2023. 3 [46] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42094219, 2024. 3 [47] Xuanchi Ren, Yifan Lu, Hanxue Liang, Zhangjie Wu, Huan Ling, Mike Chen, Sanja Fidler, Francis Williams, and Jiahui Huang. Scube: Instant large-scale scene reconstruction using voxsplats. arXiv preprint arXiv:2410.20030, 2024. 3 [48] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization. ACM Trans. Graph., 42(4):371, 2023. 2, 3, 4 [49] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 2, 3, 5, 6 [50] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 3 [51] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1961519625, 2024. 3 [52] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10208 10217, 2024. 3 [53] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 3 [54] Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. Edgerunner: Auto-regressive auto-encoder for artistic mesh generation. arXiv preprint arXiv:2409.18114, 2024. 3 [55] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2025. [56] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. 3 [57] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. 3 [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4 [59] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2024. 3 [60] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting In Propretrained 2d diffusion models for 3d generation. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1261912629, 2023. 3 [61] Peng Wang and Yichun Shi. multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. Imagedream: Image-prompt arXiv preprint [62] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. 3 [63] Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, and Xiaohui Zeng. Llama-mesh: Unifying 3d mesh generation with language models. arXiv preprint arXiv:2411.09595, 2024. 3 [64] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convolutional reconstruction model. In European Conference on Computer Vision, pages 5774. Springer, 2025. 2, 3, 6, 7 [65] Xinyue Wei, Fanbo Xiang, Sai Bi, Anpei Chen, Kalyan Sunkavalli, Zexiang Xu, and Hao Su. Neumanifold: Neural watertight manifold reconstruction with efficient and highquality rendering support. arXiv preprint arXiv:2305.17134, 2023. 3 [66] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for highquality mesh. arXiv preprint arXiv:2404.12385, 2024. 2, 3, 4 [67] Haohan Weng, Yikai Wang, Tong Zhang, CL Chen, and Jun Zhu. Pivotmesh: Generic 3d mesh generation via pivot vertices guidance. arXiv preprint arXiv:2405.16890, 2024. [68] Robert Woodham. Photometric method for determining surface orientation from multiple images. Optical engineering, 19(1):139144, 1980. 4 [69] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 3 [70] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 803814, 2023. 6 [71] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 3, 6, 7 [72] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3 [73] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 2, 3, 4, 6, 7 [74] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. Grm: Large gaussian reconstruction model for efarXiv preprint ficient 3d reconstruction and generation. arXiv:2403.14621, 2024. [75] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d-1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. 2 [76] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM Transactions on Graphics (TOG), 43(6):118, 2024. 2, 5, 6 [77] Chongjie Ye, Yushuang Wu, Ziteng Lu, Jiahao Chang, Xiaoyang Guo, Jiaqing Zhou, Hao Zhao, and Xiaoguang Han. Hi3dgen: High-fidelity 3d geometry generation from images via normal bridging. arXiv preprint arXiv:2503.22236, 2025. 3 [78] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. arXiv preprint arXiv:2310.08529, 2023. 3 [79] Biao Zhang and Peter Wonka. Lagem: large geometry model for 3d representation learning and diffusion. arXiv preprint arXiv:2410.01295, 2024. [80] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):116, 2023. 3 [81] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision, pages 119. Springer, 2024. 3 [82] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 3 [83] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [84] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 3 [85] Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Gala3d: Towards text-to-3d complex scene generation via layout-guided generative gaussian splatting. In Forty-first International Conference on Machine Learning."
        }
    ],
    "affiliations": [
        "HKUST",
        "HKUST(GZ)"
    ]
}