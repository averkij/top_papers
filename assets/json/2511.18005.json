{
    "paper_title": "RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale",
    "authors": [
        "Shengyuan Wang",
        "Zhiheng Zheng",
        "Yu Shang",
        "Lixuan He",
        "Yangcheng Yu",
        "Fan Hangyu",
        "Jie Feng",
        "Qingmin Liao",
        "Yong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a \\textbf{R}eality-\\textbf{A}ligned \\textbf{I}ntelligent \\textbf{S}ynthesis \\textbf{E}ngine that creates detailed, \\textbf{C}ity-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 5 0 0 8 1 . 1 1 5 2 : r RAISECITY: Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale Shengyuan Wang1*, Zhiheng Zheng2*, Yu Shang3, Lixuan He3, Yangcheng Yu3 Hangyu Fan3, Jie Feng3, Qingmin Liao2, Yong Li3 1College of AI, Tsinghua University, Beijing, China 2Shenzhen International Graduate School, Tsinghua University, Beijing, China 3Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China {fengjie, liyong07}@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECITY, Reality-Aligned Intelligent Synthesis Engine that creates detailed, Cityscale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECITY in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECITY promising foundation for applications in immersive media, embodied intelligence, and world models. 1. Introduction The generation of high-quality 3D worlds represents critical research frontier with profound implications for immersive media [2, 26, 27, 43], large-scale simulation [36, 66], and the development of embodied intelligence [5, 35, 48, 52] or world models [1, 11, 29, 67]. However, the creation of such content, particularly the creation of complex *Equal contribution Corresponding author 3D scenes, remains predominantly labor-intensive process [51]. This reliance on manual creation is both costprohibitive and time-consuming, presenting significant bottleneck to achieving greater scale and quality and necessitating the development of automated solutions. Indoor scenes represent main category of 3D environments [22, 38, 44], as they are relevant to daily life and relatively easy to capture from the real world. However, existing 3D indoor scene scenarios are often limited in scale and complexity, which creates significant domain gap for many applications requiring large-scale or highly heterogeneous data. The generation of 3D urban environments [28, 35] emerges as key research thrust. As highly complex systems, cities are characterized by intricate spatial layouts and high degree of component heterogeneity, such as diverse architectural forms. Consequently, the ability to model or reconstruct entire 3D cities is indispensable for applications with substantial industrial potential, including urban simulation, autonomous driving, and embodied AI. primary objective in this context is the creation of near-realistic or reality-aligned urban worlds. Such fidelity is crucial for minimizing the sim-to-real gap and mitigating the costs associated with domain adaptation. While existing works [9, 13, 40, 41, 53] have endeavored to advance 3D world construction, they face several significant and unresolved challenges. First, the large scale of urban 3D scenes, often comprising thousands of individual objects, imposes prohibitive computational costs for both training and generation. This is especially problematic for methods reliant on visualor neural 3Dbased representations. Second, many approaches are constrained in generation quality by data and input limitations. They often utilize simplistic environmental information or omit it entirely, while instructions guiding 3D generation are typically restricted to text, thus neglecting the richer context available from multimodal inputs. Compounding this issue 1 Figure 1. RAISECITY is multimodal agentic framework for generating high-quality, reality-aligned 3D urban worlds at city-scale. is the limited availability and quality of real-world data, such as imperfect imagery or insufficient and imbalanced GIS datasets. Finally, accurately representing the complex layout of real city remains major hurdle. The simultaneous generation of diverse urban elements (such as buildings, roads, and green spaces) is inherently difficult. Moreover, many current methods lack the capacity for real-world layout retrieval and object alignment, which are vital for creating reality-aligned 3D worlds. Recently, the increasing power of multimodal generative foundation models has enabled the development of sophisticated agentic frameworks [18] for 3D world generation. Agentic core components, such as planning, execution with tools, and iterative self-reflection, provide novel mechanism for addressing the complexity and scale of this task. This signals promising path to overcome the scalability, quality, and fidelity challenges that constrain the field. In response to these fundamental limitations, we propose RAISECITY, multimodal agentic framework designed to automatically create high-fidelity, scalable, and realityaligned 3D worlds from real-world environmental information. First, we introduce an agent-based, training-free methodology that fully leverages multimodal tools, thereby eliminating prohibitive training costs. The automatic generation process is intentionally modular, design choice that significantly enhances controllability and parallelism. The mesh-based 3D representation and RAISECITY architecture are not only compatible with established computer graphics (CG) pipelines but are also demonstrably scalable. Second, to enhance the fidelity and quality of the generated 3D world, RAISECITY implements sophisticated process for the automatic selection and curation of realworld data. This multi-stage process, in conjunction with the tool utilization by multimodal foundation models, elevates the quality and usability of the raw data, thereby establishing robust foundation for high-quality 3D world generation. Third, the element-by-element procedure, coupled with layout control guided by real-world geospatial information, serves to mitigate the risk of misalignment. Furthermore, the integrated reflection-refinement mechanism contributes to the improved quality and accuracy of the generated 3D objects. RAISECITY consequently achieves superior reality-alignment in comparison to contemporary methods. This overall approach also integrates fine-grained modeling of environmental details, such as urban elements and traffic patterns, which further enhances the realism, fidelity, and utility of the generated 3D worlds. In summary, our main contributions are as follows: We propose RAISECITY, novel multimodal agentic framework for generating city-scale, reality-aligned 3D worlds. Its agentic design overcomes fundamental challenges in quality, fidelity, robustness, and scalability, providing solid and reality-aligned foundation for important downstream applications. Through comprehensive evaluation, we show that RAISECITY achieves SOTA performance in urban 3D world generation, excelling in key metrics including shape precision, texture fidelity, reality alignment, consistency, and compatibility with standard CG pipelines. The complete source code and all generated 3D urban world assets are open-sourced for the community, enabling continuous optimization and broader application. 2. Methods 2.1. Overview As illustrated in Figure 1, RAISECITY orchestrates the entire pipeline across six stages. The first stage is Planning, which includes task decomposition and decision control. Subsequently, the Perception stage queries, reviews, selects, and processes visual and geographic source data, establishing robust foundation for the reality and fidelity of generation. Next, the Imagination stage refines the intermediate representations, addressing and rectifying imperfections from the initial perception results. This is followed by the Reflection stage, where the agent conducts iterative self-critique and self-refinement for quality control. The 3D Gen stage includes the actions of calling 3D generative tools to create high-quality 3D assets. Finally, the Scene Design stage assembles the complete 3D world. This involves executing real-world alignment for all 3D elements and simulating fine-grained urban details, including object placement, road network generation, and traffic modeling. suite of multimodal tools is utilized throughout this process, as detailed in Table 1. 2.2. Task Planning The Planning phase is structured around two principal components: task decomposition and decision control. To address the overarching objective of generating 3D urban world from geographical coordinates, RAISECITY initially subdivides this complex undertaking into five distinct stages: Perception, Imagination, Reflection, 3D Gen, and Scene Design. Such modular operational architecture significantly enhances both flexibility and controllability by effectively disentangling the heterogeneous sub-tasks inherent in the generation of complex 3D worlds. During the execution of the Perception and Reflection stages, the planning module generates control signals in response to feedback received from integrated tools. For instance, within the Perception stage, object detection models are utilized for the analysis of the input image, process that yields object candidates and their corresponding confidence scores. These scores subsequently serve as quantitative basis for decision-making concerning the input images. Analogously, in the Reflection stage, the determination of whether an imagined image is qualified for subsequent procedures is contingent upon an assessment by quality evaluator. The intricate mechanisms governing each of these stages are elaborated upon in the following sections. 2.3. Elements Perception Creating 3D worlds aligned with reality depends on reliable and scalable real-world data. Our approach leverages two key sources: structured geospatial data from OpenStreetMap (OSM) [33] and panoramic street view imagery. The process begins by obtaining geographic details of buildings and other urban elements from OSM for target location. The agent then acquires corresponding street view panoramas using an Online Map API. However, these images are often cluttered with extraneous elements such as irrelevant vegetation, construction, and vehicles, which can hinder the generation process. To address this, the agent first segments the raw images for buildings using an object detection model [30]. It then decides on the most suitable image for subsequent steps based on the models judgment. 2.4. Imagination and Reflection 2.4.1. Imaging Target Building Street-view images are valuable source of information for describing building facades and the surrounding urban environment. However, this data source presents non-negligible limitations. Primarily, the constrained viewpoints fail to capture the entirety of buildings features, particularly its three-dimensional spatial and volumetric characteristics. Additionally, the presence of transient obstacles (e.g. vehicles, vegetation, and construction sites) during image acquisition frequently results in occlusions, which obscure parts of the building and pose significant challenge to the construction of accurate 3D models. To overcome these challenges, we propose novel approach inspired by the human cognitive ability to form complete mental image from incomplete sensory data [25, 42]. This method introduces computational process of imagining the target building, which serves to create more holistic representation of the structure and inform the 3D generation function. Visualized representation contains rich spatial, structural, and textural information, which becomes an informative and effect representation of mental image. The successful reconstruction of an entire building from such limited visual instruction necessitates foundation of extensive world knowledge and sophisticated reasoning capabilities. RAISECITY provides the agent with tool interface to gemini-2.5-flash-image-preview [19], large pretrained multimodal foundation model, which the agent uses to analyze, imagine, repair, and reconstruct the building in the visual space. Introducing other information sources like its overall geometry structure and geographical volume is another promising approach to enhance the accurate imagination of building with extra clues about the target. Following successful practice of prior work [40], we develop geographical information retrieval pipeline based on OSM, high-quality and updating open-source geography service. The spatial spanning and rough geometry of building are extracted and processed. Then, these different kinds of multi-modal information are taken as inputs by the agent along with street-view images. 2.4.2. Reflecting and Refining To address the challenge of cumulative error propagation and improve the fidelity of the final output, we augment our generative agent with reflection and refinement mechanism. This mechanism is implemented as module driven by vision-language model (VLM), which serves as an au3 Table 1. The RAISECITY toolbox, detailing each components Role, Task, Backbone, and Modality ( Text, TG Geospatial Information in Text). Image, 3D 3D, Role Task Modality Backbone Preparation 4 Perception # Imagination ﬁ Reflection (cid:242) 3D Generation (cid:20) Scene Design Geospatial Info. Retrieval Street View Curation Object Detection Env. Info. Retrieval Image Generation, Image Editing Visual Question Answering Shape Generation Texture Paint Real-world Alignment Fine-grained Object Placement Road Network Modeling TG 3D TG T I 3D 3D TG 3D 3D 3D OSM API OSM API, Google/Baidu Maps API owlvit-base-patch32 Qwen2.5-VL-72B-Instruct gemini-2.5-flash-image-preview gpt-5 Hunyuan3D-DiT Hunyuan3D-Paint Blender, OSM API Blender Blender, MOSS tomated quality critic. Upon generating an initial building representation, the VLM-powered agent performs preliminary quality assessment using set of overall quality evaluation guidelines. Images scoring below predetermined threshold are designated as candidates for an iterative refinement process. During this process, each candidate is subjected to deeper evaluation, focusing on its semantic plausibility, structural integrity, and aesthetic appearance. Weakness reports and improvement guidance are also generated by the agent to instruct the next-step regeneration. The image is then regenerated and re-assessed in loop, which terminates when generated image achieves the required quality score or the maximum attempts is exhausted. 2.5. 3D Building Assets Generation The generated and refined 2D building images from the preceding stage contain rich structural and appearance information, serving as high-quality visual prompts for 3D generation. To execute this stage, the agent is equipped with specialized toolset derived from the Hunyuan-3D suite [23] for its high-fidelity, visual-conditioned generation and open-source availability. First, the agent employs Hunyuan-3D-Dit-v2.1 [23] to generate an untextured 3D mesh from corresponding visual representations. Upon receiving the base shape, Hunyuan3D-Paint-v2.1 [23] is leveraged to synthesize high-quality texture map. While these 3D generation tools are powerful, their raw outputs may contain topological errors that could interfere with subsequent procedures. post-processing pipeline is thus executed to refine the 3D object. This process involves detecting and removing extraneous artifacts, such as unwanted ground planes and geometrically outlying fragments. Having successfully generated and refined the asset, the agent now holds clean, high-fidelity, and textured 3D asset. It then concludes this stage by passing this object to the final Scene Design stage for world integration. 2.6. 3D Scene Design 2.6.1. Real-world Aligning Beyond the fidelity of individual 3D assets, significant challenge in large-scale world generation lies in the coherent spatial arrangement of these objects. For creating reality-aligned 3D scenes, the precise placement of each component is critical determinant of overall quality and suitability for downstream applications. To address this, we introduce framework that utilizes map data to systematically position high-quality, pre-generated building models. Our method begins by extracting geospatial metadata from OSM, including the locations and attributes of diverse urban elements such as roads, building footprints, vegetation, and water bodies. This information is used to render low-fidelity, schematic 3D scene that serves as foundational scaffold for the final composition. Subsequently, each high-quality 3D object is integrated into this scaffold by aligning its key attributes: its position is directly inherited from the corresponding entity in the reference scene; its scale is uniformly adjusted according to the volume ratio between the target object and its reference counterpart; and its orientation is set by rotating the model to the angle that maximizes the ground-plane footprint overlap with the reference building footprint. Apart from the buildings, other urban elements including roads, vegetation, water bodies are introduced into the world and placed according to their location information from metadata. Ground and sky are rendered with existing assets. 2.6.2. Fine-Grained Object Placement Beyond the generation of building structures, our system also reconstructs variety of fine-grained urban elements to enrich the realism of the city scene. These include roadside objects such as street lamps, traffic signs, utility poles, benches, trash bins, and vegetation elements like trees or bushes. Since these objects are typically repetitive and exhibit limited geometric variation, we employ retrievalbased strategy instead of fully generative one. This allows the system to efficiently populate the environment while preserving visual and semantic consistency with real cities. Each object category is associated with curated opensource 3D asset library that provides high-quality meshes and materials. Their spatial distribution is determined by two complementary placement mechanisms. (i) In the rulebased placement, spatial anchors are extracted from the road network obtained via OSM data. The algorithm detects road geometries and lane types (primary, secondary, or service) and places objects along road boundaries at regular intervals, with category-specific spacing, orientation, and offsets to ensure coherent alignment with the traffic infrastructure. (ii) In the VLM-assisted placement, VLMs are leveraged to interpret street-view imagery and textual tags. By analyzing geotagged images, the model infers both the semantic category and the most probable spatial location of contextual elementsfor instance, determining where traffic lights or signposts appear relative to intersections or pedestrian crossings. Through the combination of structured geographic data and multimodal visual reasoning, our framework reconstructs not only the main urban geometry but also the rich layer of fine-grained objects that define the functional and aesthetic characteristics of real streetscapes. 2.6.3. Road Network and Traffic Modeling After obtaining the initial coarse road network data from OSM, we integrate transportation simulator [59] into our framework. This integration enable deriving much finer and semantically richer representation of the urban road infrastructure and generating city-scale dynamic traffic, including both human and vehicle activity. The required assets for people and vehicles are sourced either by retrieving them from standard models or by generating them using Huanyuan-3D models. This refined process yields detailed lane-level topology and accurate connectivity between road segments, ensuring faithful reconstruction of urban traffic structures. Leveraging these high-quality assets, our procedural modeling system then creates visually realistic and geometrically consistent road layouts that seamlessly integrate with surrounding urban environments. Crucially, the resulting 3D environment is simultaneously populated with the large-scale dynamic traffic, providing crucial support for downstream applications, including embodied intelligence and world model. 3. Experiments Our evaluation is two-fold. First, we evaluate the quality and fidelity of 3D world generation using quantitative metrics and qualitative demonstrations. Second, we conduct comparison studies to validate our agentic design. 3.1. Quantitative 3D World Evaluation Evaluating 3D scene generation is multifaceted, requiring assessment from large-scale layout accuracy to fine-grained visual quality. We conduct comprehensive quantitative evaluation from two complementary perspectives: regionlevel layout and street-level quality. Setting and Metrics. To assess large-scale layout accuracy, we employ the Learned Perceptual Image Patch Similarity (LPIPS) [61] and Edge-IoU (E-IoU) to quantify the fidelity of the generated world layout against ground truth. Beyond layout, we evaluate the fine-grained, street-level quality of the generated 3D urban scenes. The LAIONAesthetics Predictor (LAP) [39] is used to assess the aesthetic quality of the generated scenes. To capture human perceptual preferences, we follow common practice of llmas-a-judge [20, 63] and employ GPT-5 [32], leading large vision-language model, as an evaluator for pair-wise comparison and point-wise evaluation. As shown in Table 2, our method achieves highly competitive performance in layout alignment. Notably, while most baselines (e.g., all except CityCraft [9]) are strictly constrained to OSM geometry, our more flexible approach meets or exceeds their performance, demonstrating high fidelity without rigid geometric priors. The street-level evaluation results are presented in Table 2. In point-wise assessment evaluating geometric reasonability, texture quality, inter-object relations, overall visual effect, and fidelity, our method significantly surpasses all competitors. Furthermore, in direct pair-wise comparisons, scenes generated by RAISECITY achieved win rate of over 90% against all other methods, highlighting distinct improvement in 3D urban world generation quality. 3.2. Qualitative 3D World Evaluation For more holistic understanding, representative qualitative results are presented in Figure 2, visually illustrating the performance of our method against several baselines. Visualization Setting. For mesh-based models, we render 3D assets with Unreal Engine 5 [14] under identical lighting conditions, camera poses, and rendering parameters. For NeRF-based models, we follow the original implementations and render the scenes using closely matched camera poses to ensure visual comparability. The references are sampled from Amaps, leading provider of digital map in China. The first column presents the generation results from SGAM [41]. As an early attempt at large-scale 3D world generation, the output quality is suboptimal, exhibiting poor shape fidelity, low-resolution textures, and unrealistic spatial relations. Furthermore, its 3D neural-based methodology imposes rigid viewpoint constraints, limiting broader applications. The second column presents the outputs produced by CityDreamer [53]. CityDreamer generates 3D ur5 Table 2. Performance comparison of our method against representative city-scale 3D generation approaches, demonstrating its competitiveness for both region-level layout accuracy and street-level visual quality. *vs. Ours indicates the percentage of time baseline was preferred over RAISECITY, while vs. Baselines reports the average win rate of each method against all methods. (higher is better) and (lower is better) indicate preferred metric directions. Bold and underlined values denote the best and second-best methods. Method Region-Level Street-Level Layout Consistency Aesthetics vs. Ours vs. Baselines LPIPS E-IoU Subject Consist. LAP Score Win Rate Win Rate SGAM [41] CityDreamer [54] CityDreamer4D [53] CityCraft [9] UrbanWorld [40] SynCity [13] 0.7179 0.6053 0.6006 0.6665 0.5231 0.6862 0.0314 0.0675 0.0795 0.0573 0.0681 0. Ours 0.5487 0.0784 - 0.9512 0.9557 0.9496 0.9469 0.9781 0.9524 - 4.7412 5.3716 5.6338 4.7303 5. 5.9833 - 0.0% 0.0% 0.0% 1.8% 8.3% -* - 17.2% 20.1% 56.4% 68.5% 52.5% 91.0% GPT-5 Score - 3.0208 2.9722 3.6909 4.4386 5.7367 6.0175 Figure 2. Qualitative comparison of different methods, where the last column represents the real world scene from commercial online map. ban scenes from OSM data; however, the resulting building geometries are overly simplified, and the textures remain coarse and frequently unrealistic. In addition, the method has difficulty incorporating auxiliary elements such as vegetation or street-side objects. As result, the scenes exhibit limited visual fidelity and fall short in aesthetic quality. The third column shows the generation results produced by Syncity [13]. Syncity relies on text prompts to generate the 3D content of each individual grid, and subsequently stitches and blends these grids together to form larger urban region. To ensure fair comparison, we partition the same area into grids and feed the geographic attributes of each grid into the corresponding prompts. As illustrated, the per-grid outputs exhibit reasonable visual appeal; however, their realism is inconsistent across grids, and noticeable discontinuities emerge at grid boundaries. Moreover, this gridbased strategy is inherently difficult to scale to large scenes and struggles to incorporate fine-grained objects or dynamic elements, limiting its applicability to urban environments. The remaining three columns showcase methods that utilize meshes as their fundamental 3D representation. To facilitate direct comparison, each column for these rows depicts the same region from an identical viewpoint. While CityCraft [9] can generate high-precision building models, it neglects the spatial relationships between models, leading to unrealistic and conflicting layouts. Moreover, its retrieval-based approach ignores the road network and struggles to create cohesive, reality-aligned 3D world. Regarding Urbanworld [40], despite offering improvements in layout accuracy and visual fidelity, it exhibits two major weaknesses stemming from limitations in its method and backbone architecture. First, it produces coarse 3D geometries, with most buildings rendered as primitive cuboids or combinations thereof. Second, its building textures are low-quality, lacking fine details and failing to leverage information from the surrounding environment. 6 In contrast, our results, demonstrated in the penultimate column, show clear advantages. The novel design of RAISECITY yields significant improvements in building model precision, texture fidelity, and overall layout reasonableness and accuracy. We utilize 3D scene data from digital map as highfidelity reference for spatial layout accuracy. This data, curated for commercial services, is representative of practical 3D urban world. However, its utility is specific: while object existence and location are accurate, the 3D models consist of coarse, untextured geometries. We employ this dataset as benchmark to clearly demonstrate the spatial misalignment in competing baselines and to validate the superior performance of our approach. 3.3. Autonomous Agents Decision Evaluation RAISECITY utilizes an agentic design to effectively and efficiently select and process raw information from geographic information systems. In this section, we demonstrate that our agents design is more effective for constructing urban 3D worlds than alternative methods, achieving performance that meets or exceeds that of human experts. Setting. For this evaluation, we isolate the agents decision-making by using fixed 2D imagination and 3D generation modules. We focus on the influence of input data selection and processing on the task of generating single buildings image and 3D object. We randomly sampled 50 buildings from the target region for our test set. panel of human experts was invited to curate the corresponding standard street view images for these buildings, which serve as the ground-truth input data. Baselines and Metrics. We evaluate two agent-based methods: (1) using street views selected and processed by our agent, and (2) using multi-source information (curated street view, OSM shape, volume) processed by our agent. We compare these approaches against four distinct baselines: (i) using only text descriptions, (ii) using randomly selected standard street views, (iii) using the human expertcurated golden street views, and (iv) using standard multisource information (expert curated street view, OSM shape, volume) without an agent. For the 2D image evaluation, we employ the Frechet Inception Distance (FID) [21], Kernel Inception Distance (KID) [4], SSIM [50], and CLIP Similarity [37]. For the 3D evaluation, we use Uni3D [64] to measure shape coherence and FID to assess texture quality. The experimental results are presented in Table 3. The method utilizing street view images selected and processed autonomously by the agent demonstrates superior performance in perceptual quality. For 2D generation, it achieves the best image quality (as measured by FID and KID) and the second-highest semantic similarity (CLIP sim) among all methods. It also outperforms all other approaches in the texture quality (FID) of the 3D object construction. The methods incorporating multi-source data achieve the two highest SSIM scores, suggesting that explicit geometric information excels at structural alignment. The multi-source methods underperform their streetview-only counterparts on perceptual metrics. This suggests trade-off in which rigid structural constraints may negatively impact visual fidelity. Overall, the agent-based methods outperform comparable non-agentic baselines across both 2D and 3D generation quality. Conversely, naive information retrieval methods (e.g., text-only or random views) perform poorly across all metrics. This result underscores the significant gap between raw data and high-quality, curated inputs, validating the importance of our intelligent agent design. 3.4. Downstream Application With easily transform the real-world geospatial data and street-view images into 3D urban environment and related postprocessing suites, we can build diverse outdoors spatial reasoning tasks, navigation tasks and city-scale traffic simulation for any region. the potential downstream application enabled by our framework are presented in Figure 3. More details are presented in the supplementary material. 4. Related Work 4.1. 3D Scene Generation Compared to generating 3D objects or avatars, generating 3D scenes presents significantly greater challenges [51]. The aim of 3D scene generation is to create spatially structured, semantically meaningful, and visually realistic 3D environment for applications such as immersive media [2, 26, 27, 43], embodied intelligence [5, 35, 48, 52], and world models [1, 11, 29, 67]. Procedural Generation [31, 35, 58, 65], Neural 3D-based Generation [28, 49, 53], Image-based Generation [8, 57, 60], and Video-based Generation [6, 16, 17, 56] are four major paradigms [51]. Existing 3D generation methodologies are characterized by significant trade-offs. Rule-based procedural generation [35], while offering scalability and control, suffers from inflexibility and necessitates extensive human intervention. Concurrently, neural 3D methods are constrained by limited training data and suboptimal scalability, whereas visual-based approaches frequently exhibit deficiencies in geometric fidelity, view consistency, and compatibility with standard CG pipelines. By contrast, our multimodal framework facilitates scalable 3D urban generation characterized by enhanced reality alignment, photorealism, and view consistency. RAISECITY is distinctly based on real-world geospatial data, which differentiates it from purely imaginary 3D world generation frameworks [13, 46]. Furthermore, unlike methods dependent upon existing surveys [66], its generative paradigm maintains competitiveness in low-resource regions, while its support for mesh exportation facilitates wider range of applications. 7 Table 3. Quantitative comparison of 2D image quality and 3D reconstruction quality from different methods. For the 3D metrics, both Shape Coherence and Texture Quality are computed using expert curated streetview images as the reference. For FID/KID, lower is better (), while for SSIM, CLIP-Sim., and Uni3D-I, higher is better (). Bold and underline denote the best and second-best results, respectively. Method Category 2D Image Quality 3D Reconstruction Shape Coherence Texture Quality FID KID SSIM CLIP-Sim. Uni3D-I UrbanWorld - - - - Text-Only Random Streetview Expert Streetview Expert Multi-Source Agent Streetview Agent Multi-Source Baselines Human Agent 294.27 292.25 292.09 303.58 286.64 298.10 0.1612 0.1610 0.1529 0. 0.1413 0.1538 0.3005 0.2902 0.2977 0.3212 0.2745 0.3033 - 0.6527 0. 0.7272 0.6947 0.6995 0.6855 0.1060 0.0874 0.0901 0.1067 0.0991 0.0951 0. FID 367.23 337.89 338.32 338.21 341.70 315.74 323.37 Figure 3. Downstream applications enabled by our framework. 4.2. Agent-based 3D generation Agent-based approaches in 3D synthesis leverage large language models (LLMs) to plan, call tools, and verify outcomes across two granularities: object-level (single asset creation and editing) and scene-level (multi-object spatial Idea23D[7] coordinates layout and world construction). multiple languagevision agents to translate mixed inputs (e.g., text, images, and optional 3D cues) into stepwise modeling operations, enabling iterative refinement of individual assets. ShapeCraft[62] represents objects with structured, program-like graph and employs LLM agents for parsing and incremental editing, supporting textured, editable, and interactive outputs. LayoutGPT[15] treats the LLM as visual planner that converts textual constraints into executable layout programs, enabling compositionally consistent indoor scene arrangements. SceneWeaver[55] adopts an extensible, self-reflective agent that selects appropriate scene-generation tools and performs automatic checks for semantic alignment, physical plausibility, and visual realism. UnrealLLM[45] integrates LLM planning with Unreal Engines procedural content ecosystem, enabling high-level language control over asset retrieval, placement, and interactive editing. Collectively, these systems illustrate progression from object-centric modeling to end-to-end scene and world construction with planning, tool use, and self-checking; unifying objectand scene-level reasoning within single agentic framework remains an important open direction. 5. Conclusion In this paper, we propose RAISECITY, an agentic framework for generating reality-aligned 3D worlds at city-scale. In this framework, an intelligent agent manages the 3D world generation by planning data and control flow, leveraging multimodal tools, and employing self-reflection for iterative refinement. RAISECITY outperforms existing 3D urban scene generation methods, featuring reality alignment, impressive 3D scene quality, view consistency, scalability, and seamless compatibility with existing CG pipelines. This highlight RAISECITYs significant potential for applications in embodied intelligence and world models."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 1, 7 [2] Nantheera Anantrasirichai and David Bull. Artificial intelligence in the creative industries: review. Artificial intelligence review, 55(1):589656, 2022. 1, 7 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3 [4] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. 7 [5] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-languageaction flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 1, 7 [6] Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. arXiv preprint arXiv:2411.00769, 2024. 7 [7] Junhao Chen, Xiang Li, Xiaojun Ye, Chao Li, Zhaoxin Fan, and Hao Zhao. Idea23D: Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs, 2024. [8] Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Jonathan Eisenmann, Siavash Khodadadeh, and JeanFrancois Lalonde. Guided co-modulated gan for 360 field of view extrapolation. In 2022 International Conference on 3D Vision (3DV), pages 475485. IEEE, 2022. 7 [9] Jie Deng, Wenhao Chai, Junsheng Huang, Zhonghan Zhao, Qixuan Huang, Mingyan Gao, Jianshu Guo, Shengyu Hao, Wenhao Hu, Jenq-Neng Hwang, et al. Citycraft: arXiv preprint real crafter for 3d city generation. arXiv:2406.04983, 2024. 1, 5, 6 Jiri Borovec, [10] Nicki Skafte Detlefsen, Justus Schock, Ananya Harsh Jha, Teddy Koker, Luca Di Liello, Daniel Stancl, Changsheng Quan, Maxim Grechkin, and William Falcon. Torchmetrics-measuring reproducibility in pytorch. Journal of Open Source Software, 7(70):4101, 2022. 7 [11] Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, et al. Understanding world or predicting future? comprehensive survey of world models. ACM Computing Surveys, 2024. 1, 7 [12] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [13] Paul Engstler, Aleksandar Shtedritski, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Syncity: Training-free generation of 3d worlds. arXiv preprint arXiv:2503.16420, 2025. 1, 6, 7 [14] Epic Games. Unreal engine. https : / / www . unrealengine.com, 2025. 5 [15] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. LayoutGPT: Compositional Visual Planning and Generation with Large Language Models, 2023. 8 [16] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control. arXiv preprint arXiv:2310.02601, 2023. 7 [17] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. Advances in Neural Information Processing Systems, 37:9156091596, 2024. 7 [18] Venus Garg. Designing the mind: How agentic frameworks are shaping the future of ai behavior. Journal of Computer Science and Technology Studies, 7(5):182193, 2025. 2 [19] Google. Nano banana: Gemini image generation overview, 2025. 3 [20] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [22] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d In Proceedings of meshes from 2d text-to-image models. the IEEE/CVF International Conference on Computer Vision, pages 79097920, 2023. 1 [23] Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to highfidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025. 4 [24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. 5 [25] Maithilee Kunda. Visual mental imagery: view from artificial intelligence. Cortex, 105:155172, 2018. [26] Steven LaValle. Virtual reality. Cambridge university press, 2023. 1, 7 [27] Lik-Hang Lee, Tristan Braud, Peng Yuan Zhou, Lin Wang, Dianlei Xu, Zijun Lin, Abhishek Kumar, Carlos Bermejo, Pan Hui, et al. All one needs to know about metaverse: complete survey on technological singularity, virtual ecosystem, and research agenda. Foundations and trends in human-computer interaction, 18(23):100337, 2024. 1, 7 [28] Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-Hsuan Yang, and Sergey In ProInfinicity: Infinite-scale city synthesis. Tulyakov. ceedings of the IEEE/CVF international conference on computer vision, pages 2280822818, 2023. 1, 7 [29] Daochang Liu, Junyu Zhang, Anh-Dung Dinh, Eunbyung Park, Shichao Zhang, Ajmal Mian, Mubarak Shah, and Chang Xu. Generative physical ai in vision: survey. arXiv preprint arXiv:2501.10928, 2025. 1, 7 [30] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection. In European conference on computer vision, pages 728755. Springer, 2022. [31] Kenton Musgrave, Craig Kolb, and Robert Mace. The synthesis and rendering of eroded fractal terrains. ACM Siggraph Computer Graphics, 23(3):4150, 1989. 7 [32] OpenAI. Introducing gpt-5. Web Page, 2025. 5, 3 [33] OpenStreetMap dump Planet trieved . //www.openstreetmap.org, 2017. 3 contributors. from https://planet.osm.org rehttps : [34] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [35] Yoav IH Parish and Pascal Muller. Procedural modeling of cities. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 301308, 2001. 1, 7 [36] Jinghua Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing Yi Wang, Di Zhou, et al. Agentsociety: Large-scale simulation of llmdriven generative agents advances understanding of human behaviors and society. arXiv preprint arXiv:2502.08691, 2025. 1 [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 7 [38] Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, et al. Infinigen indoors: Photorealistic indoor scenes using procedural generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21783 21794, 2024. 1 [39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 5, 7 [40] Yu Shang, Yuming Lin, Yu Zheng, Hangyu Fan, Jingtao Ding, Jie Feng, Jiansheng Chen, Li Tian, and Yong Li. Urbanworld: An urban world model for 3d city generation. arXiv preprint arXiv:2407.11965, 2024. 1, 3, 6 [41] Yuan Shen, Wei-Chiu Ma, and Shenlong Wang. Sgam: Building virtual 3d world through simultaneous generation 10 and mapping. Advances in Neural Information Processing Systems, 35:2209022102, 2022. 1, 5, 6 [42] Roger Shepard. The mental image. American psychologist, 33(2):125, 1978. 3 [43] Mona Soliman, Eman Ahmed, Ashraf Darwish, and Aboul Ella Hassanien. Artificial intelligence powered metaverse: analysis, challenges and future perspectives. Artificial Intelligence Review, 57(2):36, 2024. 1, 7 [44] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, and Matthias Nießner. Diffuscene: Denoising diffusion models for generative indoor scene synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2050720518, 2024. 1 [45] Song Tang, Kaiyong Zhao, Lei Wang, Yuliang Li, Xuebo Liu, Junyi Zou, Qiang Wang, and Xiaowen Chu. UnrealLLM: Towards Highly Controllable and Interactable 3D Scene Generation by LLM-powered Procedural Content In Findings of the Association for ComputaGeneration. tional Linguistics: ACL 2025, pages 1941719435, Vienna, Austria, 2025. Association for Computational Linguistics. 8 [46] HunyuanWorld Team, Zhenwei Wang, Yuhao Liu, Junta Wu, Zixiao Gu, Haoyuan Wang, Xuhui Zuo, Tianyu Huang, Wenhuan Li, Sheng Zhang, Yihang Lian, Yulin Tsai, Lifu Wang, Sicong Liu, Puhua Jiang, Xianghui Yang, Dongyuan Guo, Yixuan Tang, Xinyue Mao, Jiaao Yu, Junlin Yu, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Chao Zhang, Yonghao Tan, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Minghui Chen, Zhan Li, Wangchen Qin, Lei Wang, Yifu Sun, Lin Niu, Xiang Yuan, Xiaofeng Yang, Yingping He, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Tian Liu, Peng Chen, Di Wang, Yuhong Liu, Linus, Jie Jiang, Tengfei Wang, and Chunchao Guo. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels, 2025. [47] Stefan van der Walt, Johannes L. Schonberger, Juan NunezIglesias, Francois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu, and the scikit-image contributors. scikit-image: image processing in Python. PeerJ, 2:e453, 2014. 7 [48] Hanqing Wang, Jiahe Chen, Wensi Huang, Qingwei Ben, Tai Wang, Boyu Mi, Tao Huang, Siheng Zhao, Yilun Chen, Sizhe Yang, et al. Grutopia: Dream general robots in city at scale. arXiv preprint arXiv:2407.10943, 2024. 1, 7 [49] Kai Wang, Manolis Savva, Angel Chang, and Daniel Ritchie. Deep convolutional priors for indoor scene synthesis. ACM Transactions on Graphics (TOG), 37(4):114, 2018. 7 [50] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 7 [51] Beichen Wen, Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. 3d scene generation: survey, 2025. 1, 7 [52] Wayne Wu, Honglin He, Yiran Wang, Chenda Duan, Jack He, Zhizheng Liu, Quanyi Li, and Bolei Zhou. Metaurban: simulation platform for embodied ai in urban spaces. arXiv e-prints, pages arXiv2407, 2024. 1, [66] Qinhong Zhou, Hongxin Zhang, Xiangye Lin, Zheyuan Zhang, Yutian Chen, Wenjun Liu, Zunzhe Zhang, Sunli Chen, Lixing Fang, Qiushi Lyu, Xinyu Sun, Jincheng Yang, Zeyuan Wang, Bao Chi Dang, Zhehuan Chen, Daksha Ladia, Jiageng Liu, and Chuang Gan. Virtual community: An open world for humans, robots, and society, 2025. 1, 7 [67] Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al. Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520, 2024. 1, 7 [53] Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. Citydreamer: Compositional generative model of unbounded In Proceedings of the IEEE/CVF conference on 3d cities. computer vision and pattern recognition, pages 96669675, 2024. 1, 5, 6, 7 [54] Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. Compositional generative model of unbounded 4D cities. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 6 [55] Yandan Yang, Baoxiong Jia, Shujie Zhang, and Siyuan Huang. SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent, 2025. 8 [56] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo Jeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via video diffusion models. Advances in Neural Information Processing Systems, 37:4525645280, 2024. [57] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: In Proceedings of Going from anywhere to everywhere. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66586667, 2024. 7 [58] Lap-Fai Yu, Sai Kit Yeung, Chi-Keung Tang, Demetri Terzopoulos, Tony Chan, and Stanley Osher. Make it home: ACM automatic optimization of furniture arrangement. Trans. Graph., 30(4):86, 2011. 7 [59] Jun Zhang, Wenxuan Ao, Junbo Yan, Can Rong, Depeng Jin, Wei Wu, and Yong Li. Moss: large-scale open microscopic traffic simulation system. arXiv preprint arXiv:2405.12520, 2024. 5 [60] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. Text2nerf: Text-driven 3d scene generation with neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 30(12):77497762, 2024. 7 [61] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [62] Shuyuan Zhang, Chenhan Jiang, Zuoou Li, and Jiankang Deng. ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling, 2025. [63] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. 5, 7 [64] Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Uni3d: Exploring unified 3d representation at scale. arXiv preprint arXiv:2310.06773, 2023. 7 [65] Mengqi Zhou, Yuxi Wang, Jun Hou, Shougao Zhang, Yiwei Li, Chuanchen Luo, Junran Peng, and Zhaoxiang Zhang. Scenex: Procedural controllable large-scale scene generation. arXiv preprint arXiv:2403.15698, 2024. 7 11 RAISECITY: Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale 6. Additional Experimental Results visually convincing."
        },
        {
            "title": "Supplementary Material",
            "content": "To further validate the generalization ability and visual fidelity of our framework, we additionally generate complete 3D urban scenes in multiple regions, including areas in Guangzhou and Beijing. As illustrated in Figure 4, the results maintain high geometric consistency and semantic realism across different geographic contexts. Buildings, roads, and fine-grained urban objects are coherently aligned, reflecting plausible large-scale city structures. These cross-city experiments demonstrate that our method can be effectively applied to diverse urban layouts without task-specific tuning. The generated scenes exhibit photorealistic appearance and structural coherence, supporting their direct use in downstream applications such as urban visualization, autonomous navigation, and multiagent simulation. 7. Detailed Comparison 7.1. Birds-eye view Comparison Our birds-eye view comparison  (Fig. 6)  demonstrates that the spatial layout generated by our system is highly aligned with real-world geography, achieving level of global structural fidelity comparable to UrbanWorld [40]. However, our method produces significantly higher visual quality across large areas. Buildings exhibit clearer boundaries, more coherent block-level organization, and more consistent material semantics. In contrast, UrbanWorld suffers from visible blurring, texture artifacts, and geometry deformation when covering extended regions. These results highlight that our pipeline maintains both layout accuracy and visual realism, enabling large-scale city generation that is simultaneously precise and aesthetically superior. 7.2. Building Detail Comparison To further assess structural fidelity, we present three-view (front, side, and top) building comparison in Fig. 7. Our results reveal that the generated buildings exhibit sharper geometric profiles, more accurate facade structures, and substantially cleaner texture patterns than UrbanWorld. While UrbanWorld often produces distorted roof shapes, incomplete wall edges, and overly smoothed textures, our system preserves fine-grained architectural features such as window arrangements, facade materials, and rooftop components. These comparisons confirm that our method achieves both geometry-level precision and texture-level realism, resulting in building assets that are structurally faithful and 7.3. Street views sequence comparison Figure 5 shows qualitative comparison of generated street views. From top to bottom, each row corresponds to the results produced by CityCraft, UrbanWorld, ours, and the real street view, respectively. As shown, CityCraft generates buildings that are visually inconsistent with the actual urban structures, exhibiting unrealistic layouts and facade patterns. UrbanWorld achieves better alignment with real scenes but still suffers from limited visual realism and In contrast, our model produces street coarse geometry. views that not only exhibit high structural consistency with the ground truth but also demonstrate superior photorealism, capturing fine-grained architectural details and spatial coherence. 8. Downstream Applications 8.1. Drone Navigation The generated 3D urban environments provide highfidelity and structurally consistent foundation for developing and evaluating autonomous drone navigation systems. Compared with conventional synthetic datasets or limited real-world captures, our city models offer large-scale, topologically coherent spaces containing detailed road networks, diverse building geometries, and fine-grained urban elements such as trees, poles, and traffic signs. These components enable drones to perceive realistic visual cues and depth structures, facilitating robust flight-path planning, obstacle avoidance, and visualinertial localization. Moreover, the procedural controllability of our framework allows for systematic variations in lighting, weather, and urban density, which are crucial for testing the generalization of perception and control algorithms under diverse conditions. As result, the generated cities can serve as dynamic simulation environments for reinforcement learning and embodied navigation research, bridging the gap between photorealistic rendering and physical feasibility. 8.2. Spatial Reasoning Beyond navigation, the reconstructed 3D cities provide rich testbed for spatial reasoning tasks, where agents or multimodal models must infer geometric, semantic, and relational structures within complex urban layouts. The spatial organization of roads, buildings, and objects offers naturally constrained environment for evaluating high-level 1 Figure 4. Result in different cities. Figure 5. Street sequence comparison. 2 Figure 6. Birdview comparison between our method and UrbanWorld. reasoning skills, such as route inference, landmark recognition, and urban topology understanding. In this context, our model serves as generator of structured 3D worlds that encode both metric and semantic consistency. Such environments enable systematic investigation into how language models, visionlanguageaction frameworks, or graph-based reasoning systems interpret and interact with spatial information. Consequently, the generated cities not only replicate physical realism but also provide the cognitive structure necessary for advancing research in embodied spatial intelligence. traffic signs. This information is aggregated to support the generation of realistic 3D urban environments in subsequent stages. 9.2. Buildings Imagination The Imagination stage is powered by google/gemini-2.5flash-image-preview [19]. We access the model via API with the temperature set to 0.9, utilizing the prompt detailed in the figures. Coarse 3D geometry renderings and volumetric data are extracted from OSM and fed into the model alongside selected street-view images. 9. Implementation Details 9.1. Perception 9.3. Reflection In the Perception stage, owlvit-base-patch32 [30] is employed for building detection, setting the confidence threshold to 0.01. The cropped images corresponding to the top-3 confidence scores are utilized as input for the Imagination stage. We leverage Qwen2.5-VL-72B-Instruct [3] to interpret street-view images and extract structured information, including adjacency relations, tree heights, tree-building distances, and the presence of fine-grained objects such as In this stage, the generated building concepts are evaluated based on three criteria: structural sanity, textural realism, and structural alignment. This evaluation is conducted by the state-of-the-art VLM openai/gpt-5-mini [32]. The critique prompts are shown in Figure 9, Figure 10 and Figure 11. For the evaluation configuration, we set the temperature to 0.6 and top to 0.85, enforcing JSON schema to ensure structured output. 3 Figure 7. Three-view building comparison between our method and UrbanWorld. 4 Figure 8. The prompt for Imagination. Figure 9. The prompt for structure sanity evaluation. The first image is rough 3D model renderings of building from diagonal angle (in 45 angled topdown view). This image provides the overall shape and proportions of the building, but may lack detailed architectural features and realistic textures. The other input images are from street view photos portraying building from different angles. These images provide infomation about the buildings appearance in real-world setting, including details about its facade, materials, color, architectural features. Based on the shape and proportions from the first image and the architectural details from the street view images. Please generate one realistic perspective image of this building according to the following instruction. This building is of common modern business or residential style and is part of the urban landscape. {volume description} Preserve the true proportions and details of the architecture (roof tiles, walls, gate, stairs, railings) and present it from 45 angled top-down view. Ensure the image has natural lighting, shadows, and realistic textures, making it look like real photograph rather than 3D rendering. Remove the original background and foreground, keep only the main body of the building. Do not include any surrounding greens, people, vehicles or construction equipment. Place the building against simple light gray backdrop to emphasize depth and dimensionality. The final result should look like real drone or angled camera shot, not digital model showcase. 9.4. 3D Generation and Operation Hunyuan3D-2.1 is selected as the backbone for 3D object generation and texture painting. We employ the models locally on NVIDIA GeForce RTX 5090 GPUs. The texturing configuration is set to resolution of 512 with max num view=6. Additional 3D operations such as moving, rotating, scaling, and clipping are conducted with Blender 1. 1Version 3.2.2 5 Carefully evaluate the provided perspective building image, which is observed from diagonal angle (in 45 angled top-down view) for its structural, architectural, and geometric plausibility. Ignore photo quality (blur, framing). Respond only with JSON object containing score (0-5) and concise reason. Rubric: 5 - Excellent: Appears fully realistic. Structurally sound, architecturally coherent, and geometrically correct. 4 - Good: Largely plausible, but with subtle structural, architectural, or geometric oddities. 3 - Fair: Contains obvious flaws in its structure, design logic, or geometry, but is still somewhat coherent building. 2 - Poor: Fundamentally flawed with major structural, architectural, or geometric impossibilities. 1 - Incoherent: chaotic assembly of architectural parts that fails to form cohesive structure. 0 - Surreal: Defies basic principles of architecture and physics, or the image does not contain building, or the image is not perspective view (e.g., if it is street view image, aerial view, blueprint, or interior view, the score should be 0). 10. Evaluation Implementation 10.1. Metrics Patch Image Perceptual Learned Similarity (LPIPS). [61] LPIPS is widely-used method to assess the perceptual similarity between images. Instead of direct pixel-level comparison, LPIPS is calculated with images feature maps from pre-trained deep neural networks. Better perceptual alignment with human is demonstrated as major advantage over pixel-level metrics. The calculation of LPIPS is implemented with lpips pacakage from PyPI with AlexNet [24]. Edge Map Intersection over Union (E-IoU). To strictly evaluate the structural fidelity and geometric alignment of the generated images against the reference, we employ the Edge Map Intersection over Union (Edge-IoU) metric. Unlike pixel-wise metrics (e.g., MSE or PSNR) that focus on color intensity, Edge-IoU isolates high-frequency spatial details to assess shape consistency. The implementation proceeds in three stages. First, the input images are converted to grayscale to eliminate chromatic variance. Second, we utilize the Canny edge detector with threshold Figure 10. The prompt template for textural alignment and realism evaluation. Figure 11. The prompt for structure alignment evaluation. You are given several images. The first image is perspective building image, which is observed from diagonal angle (in 45 angled top-down view). This first image is to be evaluated for its textural realism and alignment with the buildings architectural style. The subsequent reference images are street view images of real buildings that represent the target architectural style and texture. Find and focus on the main building in the street view images that best matches the structure in the perspective image. Ignore environmental details like trees, cars, and people. Respond only with JSON object containing score (0-5) and concise reason. Rubric: 5 - Excellent: Textures are highly realistic and seamlessly integrated, perfectly matching the architectural style of the reference street view images. It can be easily related to real-world buildings in reference images. 4 - Good: Textures are realistic and generally align with the architectural style of the reference images, with minor inconsistencies. 3 - Fair: Textures show some realism and partial alignment with the reference style, but there are noticeable mismatches or unrealistic elements. 2 - Poor: Textures are largely unrealistic and do not convincingly match the architectural style of the reference images. 1 - Incoherent: Textures are chaotic and fail to represent any coherent architectural style, showing little to no relation to the reference images. 0 - Surreal: The image does not contain building, or the image is not perspective view (e.g., if it is street view image, aerial view, blueprint, or interior view, the score should be 0). You are given two images. The first image is perspective building image, which is observed from diagonal angle (in 45 angled top-down view). This first image is to be evaluated for its structural alignment with the second image, which is rough 3D model rendering of building from diagonal angle (in 45 angled top-down view). This second image provides the overall shape of the building, but may lack detailed architectural features and realistic textures. Focus on how well the structure in the perspective image matches the shape of the building in the 3D model rendering. Respond only with JSON object containing score (0-5) and concise reason. Rubric: 5 - Excellent: The structure in the perspective image perfectly matches the shape and proportions of the building in the 3D model rendering. It can be easily related to the 3D model. 4 - Good: The structure in the perspective image largely aligns with the shape of the building in the 3D model rendering, with minor deviations. 3 - Fair: The structure in the perspective image shows some alignment with the 3D model rendering, but there are noticeable mismatches in shape or proportions. 2 - Poor: The structure in the perspective image largely deviates from the shape and proportions of the building in the 3D model rendering. 1 - Incoherent: The structure in the perspective image fails to represent the shape of the building in the 3D model rendering. 0 - Surreal: The image does not contain building, or the image is not perspective view (e.g., if it is street view image, aerial view, blueprint, or interior view, the score should be 0). equaling 50 to extract binary edge maps, effectively capturing significant structural boundaries while suppressing noise. Finally, the Intersection over Union (IoU) is computed between the predicted and ground-truth edge maps. Formally, this is defined as the ratio of the intersection to the union of the binary edge sets: Edge-IoU = Epred Egt Epred Egt (1) where Epred and Egt represent the binary edge masks of the prediction and ground truth, respectively. higher EdgeIoU indicates superior preservation of structural details and geometric layout. Subject Consistency. To comprehensively assess generation quality, we evaluate subject consistency to quantify the stability of the subjects identity throughout the generated 3D video. Specifically, we employ DINOv2 [34] to capture global object semantics and calculate the cosine similarity between adjacent frames. higher score indicates that the subjects semantic features remain stable over time. LAION Aesthetics Predictor (LAP) Score. To assess the perceptual beauty and artistic composition of the generated output, we employ an Aesthetic Quality metric based on the 6 LAION Aesthetics Predictor [39]. Unlike standard signallevel metrics, this data-driven approach captures high-level visual appeal and human preference. We utilize the CLIP ViT-L/14 [12, 37] backbone to encode frames into normalized semantic embeddings, which are then projected through linear regression head pre-trained on the LAIONAesthetics dataset. This process yields scalar quality score for each frame, allowing us to quantify the overall artistic quality of the video sequence through the aggregated mean score. Frechet Inception Distance (FID) [21] and Kernel Inception Distance (KID) [4] Both metrics quantify the similarity between the distribution of generated images and real images, where lower values indicate better image quality of generated results. We utlize the implementation of torchmetrics [10] package with the ground truth of curated street view images. Structure Similarity Index Measure (SSIM) [50]. SSIM is an established work in the field of image quality assessment, extracting structural information from evaluated images. The SSIM is calculated with skimage [47] in our experiments. CLIP Similarity [37]. To evaluate the high-level semantic consistency between the generated images and the ground truth, we employ the CLIP Similarity metric. We utilize the pre-trained ViT-B/32 [12] backbone to map both the generated results and reference images into shared latent feature space. The similarity is then quantified by calculating the cosine similarity between the normalized feature embeddings. Unlike pixel-level metrics, this approach validates that the model successfully preserves the semantic information of the target scenes. Uni3D-I [64]. Uni3D offers an effective way to learn the representation of 3D mesh. We thus measure the similarity between the mesh generated by our framework and the corresponding reference image from human annotation. Pairwise llm-as-a-judge Evaluation. Adopting the LLMas-a-judge paradigm [63], which has demonstrated high correlation with human judgment, we evaluate the generated 3D urban scene with gpt-5. To ensure reproducibility, we set the inference temperature to 0. And the detailed scoring guidelines are presented in Figure 12. Pairwise llm-as-a-judge Evaluation. We also conduct pairwise evaluation using the same configuration. The prompt utilized for the evaluator is presented in Figure 13. To eliminate position bias, each comparison is performed twice with the order of the candidates swapped. 10.2. Ground Truth Data Curation For the evaluation of generation quality, 50 ground truth images of different buildings were curated from online mapping services. This curation process involved annotators with verified local knowledge (minimum two years of resFigure 12. The prompt for pointwise llm-as-a-judge evaluation. You are given one image of 3D urban scene. Please evaluate the quality of the scene reconstruction based on the image. Rate the quality on scale from 0 to 10, where 0 means very poor quality and 10 means excellent quality. Rubrics: - 10-9: Perfect reconstruction with high detail, realism, and visual appeal. The appearance of buildings, roads, and vegetation is highly reasonable and realistic. The layout and structure of the scene are flawless. It can be good representation of realworld urban scene. - 8-7: Good reconstruction with several flaws. There are some inaccuracies in the appearance of some elements or issues with the layout, but overall the scene is still kind of visually realistic and reasonable as an artificial urban scene. The buildings have details in shapes and textures. - 6-5: Average reconstruction with flaws. - 4-3: Poor reconstruction with major flaws. The scene is very basic and lacks detail, with numerous inaccuracies in the appearance of elements and serious issues with the layout. The buildings have very limited details. The scene looks artificial and unrealistic even for an artificial urban scene. - 2-0: Very poor reconstruction with almost no detail or realism. The scene is barely recognizable, with extreme inaccuracies in the appearance of elements and completely flawed layout. It can be very difficult to identify what the scene is supposed to represent. Your evaluation should consider factors such as detail, realism, and overall visual appeal. Please only provide numerical integer score without any additional text or explanation. idence or employment in the region) and university-level education. All participants are acknowledged adhering to academic ethical guidelines. 11. Computational Resource and Cost Estimation For 3D object generation and texture painting, We employ the models locally on NVIDIA GeForce RTX 5090 GPUs. This inference process requires approximately 12 hours to generate 1,800 building instances on two 5090 GPUs. All other foundation model operations are executed via APIs. 7 Figure 13. The prompt for pairwise llm-as-a-judge evaluation. Please compare the two images of urban 3D scene reconstructions provided. Evaluate their quality based on the following criteria: 1. Completeness: How well does the reconstruction capture the entire scene? 2. Accuracy: Are the structures and objects in the scene accurately represented? 3. Visual Quality: Consider the clarity, color fidelity, and overall visual appeal of the images. 4. Realism: Does the reconstruction look realistic and true to life? 5. Artifacts: Are there any noticeable artifacts or distortions in the images? Provide judgment on which image is better overall, considering all the above factors. If the first FIRST. If the second image is better, respond only with SECOND. There should be no other text in your response apart from FIRST or SECOND. respond only with image is better, The 3D world construction process exhibits time complexity of O(n), where denotes the number of buildings. This linear complexity highlights the effective scalability of RAISECITY with respect to computational resources."
        }
    ],
    "affiliations": [
        "College of AI, Tsinghua University, Beijing, China",
        "Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China",
        "Shenzhen International Graduate School, Tsinghua University, Beijing, China"
    ]
}