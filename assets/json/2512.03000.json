{
    "paper_title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "authors": [
        "Kairun Wen",
        "Yuzhi Huang",
        "Runyu Chen",
        "Hui Zheng",
        "Yunlong Lin",
        "Panwang Pan",
        "Chenxin Li",
        "Wenyan Cong",
        "Jian Zhang",
        "Junbin Lu",
        "Chenguo Lin",
        "Dilin Wang",
        "Zhicheng Yan",
        "Hongyu Xu",
        "Justin Theiss",
        "Yue Huang",
        "Xinghao Ding",
        "Rakesh Ranjan",
        "Zhiwen Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 2 0 0 0 3 0 . 2 1 5 2 : r DynamicVerse: Physically-Aware Multimodal Framework for 4D World Modeling Kairun Wen1, Yuzhi Huang1, Runyu Chen1, Hui Zheng1, Yunlong Lin1, Panwang Pan1, Chenxin Li2, Wenyan Cong3, Jian Zhang1, Junbin Lu4, Chenguo Lin5, Dilin Wang6, Zhicheng Yan6, Hongyu Xu6, Justin Theiss6, Yue Huang1, Xinghao Ding1(cid:66), Rakesh Ranjan6, Zhiwen Fan3 * Equal Contribution; Project Lead; (cid:66) Corresponding Author 1XMU 2CUHK 3UT Austin 4UW 5PKU 6Meta Project Website: https://dynamic-verse.github.io/ Figure 1: The overview of physically-aware multi-modal world modeling framework DynamicVerse."
        },
        {
            "title": "Abstract",
            "content": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into comprehensive 4D multimodal format. DynamicVerse delivers large-scale dataset consisting of 100K+ 39th Conference on Neural Information Processing Systems (NeurIPS 2025). videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods."
        },
        {
            "title": "Introduction",
            "content": "Humans inhabit dynamic 3D world where geometric structure and semantic content evolve over time, constituting 4D reality (spatial with temporal dimension). Understanding this dynamic environment is fundamental for developing advanced AI applications in fields such as robotics [1, 2, 3, 4, 5, 6], extended reality [7, 8, 9], humanagent interaction [10, 11], and digital twins [12, 13]. However, building generalizable foundation models for these downstream tasks faces longstanding challenge: acquiring high-quality, ground-truth 4D datasets from real-world environments, given that datadriven solutions increasingly demand 4D data while its collection using multiple sensors remains non-scalable. This raises the question: Can we develop an automated pipeline capable of generating real-world 4D dataset at scale? Current real-world 4D data primarily focus on indoor scenes [14, 15] or autonomous driving scenarios [16], where geometry capture is straightforward, but their diversity is limited. Even synthetic 4D data [17, 18, 19, 20], while controllable, often lack the fidelity and complexity required to truly represent the real world, resulting in notable simulation-to-real gap. Moreover, physically-aware multimodal annotations, including metric-scale 3D geometry, detailed representations of non-rigid actors (e.g., object size, mask and bounding box, etc.), as well as descriptive captions of dynamic content (i.e., object, camera and scene), are often absent [21, 22]. This limited data landscape, especially when contrasted with the progress fueled by large-scale datasets in modalities like images, videos, and language, underscores the compelling need for large-scale, diverse, physically-aware, and semantically rich annotated multi-modal dataset for 4D scene understanding. Against this background, this paper aims to generate scalable, physically-aware, and multimodal annotations from massive monocular video data  (Fig. 1)  for numerous potential applications, such as enhancing 4D Vision-Language Models [23], facilitating advanced 3D-aware video generation [24], and enabling linguistic 4D Gaussian Splatting [25]. However, achieving this goal is not trivial. To the best of our knowledge, there is currently significant lack of rich and diverse 4D datasets (see Tab. 1) adequate for these demanding tasks. To address this data scarcity, we introduce DynamicGen, novel automated data curation pipeline  (Fig. 3)  designed to generate physically-aware multi-modal 4D data at scale. This pipeline contains two main stages: (1) metric-scale geometric and moving object recovery (i.e., object category and mask) from raw videos, and (2) hierarchical dynamic content (i.e., object, camera and scene) detailed caption generation. Specifically, the pipeline curates diverse real-world monocular video sources; employs filtering strategy to remove outliers such as camera motion intensity; integrates multiple foundation models (i.e., VFMs, VLMs, LLMs, GFMs) for initial frame-wise annotation; applies dynamic bundle adjustment to jointly minimize global photometric error; and concludes with dynamic content captioning at three granularities and human-in-the-loop quality review to ensure annotation semantic accuracy. The resulting multi-modal 4D dataset, termed DynamicVerse  (Fig. 1)  , comprises over 100K distinct 4D scenes, 800K masklets, and 10M video frames. Each scene is extensively annotated with multiple modalities: metric-scale point maps, camera parameters, object masks with corresponding categories, and detailed descriptive captions. We evaluate DynamicGen through three benchmarks: video depth estimation, camera pose estimation, and camera intrinsics estimation. We demonstrate the generalization capability of DynamicGen to process web-scale video data and extract multi-modal information qualitatively. We also conduct human study and GPT-assited evaluation to validate the quality of generated captions. Our main contributions are summarized as follows: We develop DynamicGen, novel automated data curation pipeline designed to generate physically-aware multi-modal 4D data at scale. This pipeline contains two main stages: (1) metric-scale geometric and moving object recovery from raw videos, and (2) hierarchical detailed semantic captions generation at three granularities (i.e., object, camera and scene). Powered by 2 foundation models (i.e., VFMs, VLMs, LLMs, GFMs), DynamicGen efficiently generate 4D data at scale, thus addressing the critical scalability, physical reality and modality diversity limitations of traditional 4D data curation. We introduce DynamicVerse, large-scale 4D dataset featuring diverse dynamic scenes accompanied by rich multi-modal annotations including metric-scale point maps, camera parameters, object masks with corresponding categories, and detailed descriptive captions. DynamicVerse encompasses 100K+ 4D scenes coupled with 800K+ masklets, sourced through combination of massive 2D video datasets and existing 4D datasets. This represents significant improvement in terms of data scale, scene and modality diversity compared to prior 4D datasets. We validate DynamicGen through three benchmarks: video depth estimation, camera pose and intrinsics estimation. We demonstrate the generalization capability of DynamicGen to process web-scale videos and extract multi-modal information qualitatively. We also conduct human study and GPT-assited evaluation to validate the quality of generated captions."
        },
        {
            "title": "2 Related Work",
            "content": "Table 1: Comparison of DynamicVerse with large-scale 2D video datasets and existing 4D scene datasets. DynamicVerse expands the data scale and annotation richness compared to prior works. Numerical Statistics Provided Annotations Detailed Features Dataset Name 2D Video Dataset DAVIS2017 [26] Youtube-VIS [27] UVO-dense [28] VOST [29] BURST [30] MOSE [31] SA-V [32] MiraDATA [33] 4D Scene Dataset T.Air Shibuya [34] MPI Sintel [35] FlyingThings3D [36] Waymo [16] CoP3D [14] Stereo4D [37] PointOdyssey [17] Spring [18] Dynamic Replica [19] MVS-Synth [20] RealCam-Vid [21] DynPose-100K [22] e # 0.2K 3.8K 1.0K 0.7K 2.9K 2.1K 50.9K 330K 7 14 220 1,150 4,200 110,000 159 47 524 120 100K 100K a # 10.7K - 68.3K 75.5K 195.7K 638.8K 4.2M - 0.7K 0.7K 2K 200K 600K 10,000K 200K 6K 145K 12K - 6,806K e a # h D m s e t k c a y e t b n p c O t e S t a a 0.4K 8,171 10.2K 1.5K 16.1K 5.2K 642.6K - - - - - - - - - - - - - y e - - - - - - - - Mixed - Mixed Outdoor Mixed Mixed Mixed Mixed Indoor Outdoor Mixed Mixed T a - - - - - - - - Street Scripted Objects Driving Pets ? o - R - - - - - - - - ? c - t - - - - - - - - Yes - - Synthetic Synthetic Synthetic Real-world Yes Real-world - S. fisheye Real-world Yes Yes Realistic Yes Realistic Yes Realistic Yes Urban Yes Realistic Yes Realistic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic DynamicVerse 100K+ 13.6M 800K+ Mixed Realistic Real-world Yes Multi-modal foundation models. The development of numerous large foundation models in recent years has yielded remarkable performance across multiple tasks such as depth estimation [38, 39, 40, 41, 42], multi-view stereo [43, 44, 45], detection and segmentation [46, 47, 48, 32], human parsing [49], optical flow estimation [50, 51], and point tracking [52, 53, 40]. We propose that these models are highly applicable to achieving holistic 4D understanding, and unifying them within single framework represents promising direction for advancing tasks like nonrigid structure from motion. Our DynamicGen pipeline implements this idea by integrating the following pretrained components: UniDepthv2 [54] for geometry initialization, CoTracker3 [53] and UniMatch [51] for correspondence initialization, and Qwen2.5-VL [55] and SA2VA [56] for dynamic object segmentation. This integration, coupled with multi-stage optimization and regularization, allows us to extract accurate metric-scale camera poses and 4D geometry from monocular video. Similar to our method, the concurrently developed Uni4D [57] captures 4D geometry and pose, but it suffers from limited data modalities and discontinuous geometric estimates. In contrast, our DynamicGen pipeline not only produces globally refined dense 4D geometry but also supports moving object recovery (i.e., object category and mask) and provides fine-grained dynamic content (i.e., object, camera and scene) caption annotations. 3 Figure 2: The statistics and data source of DynamicVerse. Multi-modal datasets. The development of large-scale multi-modal datasets has proven essential for advancing model performance across numerous domains, including language, image-text (e.g., LAION [58, 59], Conceptual Captions [60], WebImageText [61]), and video understanding (e.g., DAVIS2017 [26], Youtube-VIS [27], UVO-dense [28], VOST [29], BURST [30], MOSE [31], SAV [32], MiraDATA [33]). Extending this success to holistic 4D understanding requires datasets that capture the dynamic 3D world with rich, multi-modal annotations. Existing 4D datasets, whether from early reconstruction efforts [17, 18, 19, 20] (limited diversity) or recent large-scale posed video collections like RealCam-Vid [21] and DynPose-100K [22] (lacking detailed geometry and semantics beyond pose), and even OBJAVERSE [62] (limited content), fall short of providing the comprehensive multi-modal information needed. Our DynamicVerse dataset bridges this gap by offering extensive multi-modal annotations, including metric-scale depth, camera parameters, instance segmentation with labels, and descriptive captions, specifically designed to facilitate advanced 4D research."
        },
        {
            "title": "3 DynamicVerse",
            "content": "Overview DynamicVerse is physical-scale, multi-modal 4D modeling framework for real-world video, which contains novel automated data curation pipeline and corresponding large-scale 4D dataset. The DynamicGen pipeline  (Fig. 3)  contains two main stages: (1) metric-scale geometric and moving object recovery (i.e., object category and mask) from raw videos, and (2) hierarchical dynamic contents (i.e., object, camera and scene) detailed caption generation. This pipeline primarily consists of five steps: 4D scene curation (in Sec. 3.1), data filtering strategy (in Sec. 3.2), moving object recovery (in Sec. 3.3), dynamic bundle adjustment (in Sec. 3.4) and dynamic content caption generation (in Sec. 3.5). The resulting DynamicVerse dataset comprises over 100K distinct 4D scenes, 800K masklets, and 10M video frames. The data statistics and collection of DynamicVerse are illustrated in Fig. 2. 3.1 4D scene curation To address the scarcity of available 4D scene data, DynamicGen unifies video data from various real-world video datasets, including DAVIS2017 [26], Youtube-VIS [27], UVO-dense [28], VOST [29], BURST [30], MOSE [31] and SA-V [32], alongside existing synthetic 4D datasets from PointOdyssey [17], Spring [18], Dynamic Replica [19], MVS-Synth [20], RealCam-Vid [21] and DynPose-100K [22]. The inclusion of these datasets is mainly motivated by their potential as scalable data sources for 4D scene understanding. 3.2 Data filtering strategy Data filtering is critical step for identifying video data suitable for subsequent dynamic bundle adjustment. This process presents challenges due to the noisy quality and inherent variability of video data, which impedes the precise selection of high-quality sequences. To address this, we developed filtering strategy incorporating several distinct criteria: proximal depth, focal-length stability, video blur, camera motion smoothness, and non-perspective distortion. Each of these aspects is quantified 4 Figure 3: The physically-aware multi-modal 4D data generation pipeline DynamicGen. by normalized score. We combine these scores as features and employ Random Forest model to predict video quality score ranging from 0 to 5. For model training, we manually annotated approximately 1,000 videos, assigning scores between 0 (indicating largely unsuitable, poor quality or insufficient dynamics) and 5 (indicating highly suitable, good quality and sufficient dynamics). We further apply VLM-based judgment to automatically exclude unsuitable videos before reconstruction. 3.3 Moving object recovery To accurately identify the main dynamic objects within video, we integrated multiple foundation models to achieve reliable segmentation. Specifically, our pipeline first employs Qwen2.5-VL [63] to identify moving objects and determine their semantic categories. These categories are then used to prompt SA2VA [56] for generating corresponding object masks. Leveraging the obtained object masks and geometric annotations, we can apply physical-aware size extraction to annotate the 3D bounding box for moving objects. 3.4 Dynamic bundle adjustment Leveraging the high-quality RGB filtered videos, we employed robust dynamic bundle adjustment method for annotating metric-scale camera parameters and point maps. This task is challenging due to dynamic objects occluding the static scene and static scene appearance changes hindering correspondence estimation. To effectively addresses both difficulties, we design multi-stage optimization framework, see Fig. 3, including: (1) dynamic masking, (2) coarse camera initialization, (3) tracking-based static area bundle adjustment, (4) tracking-based non-rigid bundle adjustment, and (5) flow-based sliding window global refinement. Compared with traditional Structure-from-Motion techniques [64] and DUSt3R-based methods [65], our framework not only can handle massive video data with different resolutions but also yield metric-scale results by leveraging the full power of various foundation models. Formulation Given video RGB frames = (I1, . . . , IT ) with resolution , we aim to estimate for each timestep = 1, . . . , : per-frame point map Xt RHW 3, camera intrinsics Kt, and camera pose Pt = [RtTt], where Rt and Tt denote the t-th cameras rotation and translation, 5 Figure 4: Qualitative Results of Moving Object Segmentation. We show qualitatively some of our segmentation results on the Youtube-VIS dataset compared with other methods. respectively. Here, contains static points Xstatic and dynamic points Xdyn. We assume all frames share the same intrinsics where we optimize focal lengths fx and fy. The overall cost function is formulated as follows: CBA(P, Xstatic) + Cflow(Xstatic) + CNR(Xdyn) + Cmotion(Xdyn) + Ccam(R) (1) where CBA(P, Xstatic) and Cflow(Xstatic) are bundle adjustment terms measuring the reprojection error between static correspondences and the static 3D structure Xstatic. CNR(Xdyn) is non-rigid structure-from-motion term evaluating the consistency of the dynamic point cloud with its tracklets. Regularization is applied to camera motion smoothness through Ccam(P) and to the dynamic structure and motion via Cmotion(Xdyn). Each term participates in different optimization stages, which are described below. Detailed explanations of the cost terms are provided in the supplementary material. Stage I: Dynamic masking We first extract dynamic masks to filter out the dynamic points for static area bundle adjustment. Specifically, we use semantic-based and motion-based method to obtain dynamic masks = {Mt}T low}T t=1. For the segmentation-based approach, we use the generated moving object masks {Mt t=1 in Sec. 3.3. For the flow-based approach, we employ Unimatch [51] to obtain dense optical flow predictions and compute per-frame epipolar error maps [66], which indicate the likelihood of pixels belonging to the dynamic foreground. Then we can obtain dynamic masks Mf low = [E1, E2, . . . , ET ] by thresholding on these epipolar error maps. t=1 = {Mt sem Mt sem}T 6 t=1 and dense pixel motion = {Zk}K Stage II: Coarse camera initialization In this stage, we start camera initialization by obtaining video depth = {Dt}T k=0. For video depth estimation, we use UniDepthV2 [54], monocular depth estimation network, to estimate initial depth maps and initial camera intrinsics Kinit. For dense pixel motion estimation, we utilize Co-TrackerV3 [53] for its robustness. We apply Co-Tracker bi-directionally on dense grid every 10 frames to ensure thorough coverage. We filter and classify tracklets using segmentation masks yielding set of correspondent point trajectories {Zk RT 2}K k=0 at visible time steps determined by Co-Tracker. Combining and allows us to establish 2D-to-3D correspondences. This allows us to initialize and tune camera parameter by minimizing the following cost function with respect to camera parameters only. Specifically, we can unproject each video frames depth at time back to 3D and minimize the following cost function: min (cid:88) (cid:88) (t,t) ZkM Zk,t πK(π1 (Zk,t, Dt, ξt), ξt)2 2 (2) where π1 is the unprojection function that maps 2D coordinates into 3D world coordinates using estimated depth Dt. We perform this over all pairs within temporal sliding window of 5 frames. Given camera initialization ˆP, we unproject our depth prediction into common world coordinate system, which provides an initial 4D structure ˆX. This is used as initialization for later optimization. Stage III: Static area bundle adjustment We jointly optimizes camera pose and static geometry by minimizing the static component-related energy in bundle adjustment fashion. Formally speaking, we solve the following: min P,Xstatic CBA(P, Xstatic; Z, M) + Ccam(R) (3) By enforcing consistency with each other, this improves both the static geometry and the camera pose quality. We perform final scene integration by unprojecting correspondences into 3D using improved pose and filtering outlier noisy points in 3D. Stage IV: Non-rigid bundle adjustment Given the estimated camera pose, this stage focuses on inferring dynamic structure. Note that we freeze camera parameters in this stage, as we find that incorrect geometry and motion evidence often harm camera pose estimation rather than improve it. Additionally, enabling camera pose optimization introduces extra flexibility in this ill-posed problem, harming robustness. Formally speaking, we solve the following: min Xdyn CNR(Xdyn; P, Z, M) + Cmotion(Xdyn) (4) We initialize Xdyn using video depth and our optimized camera pose from last step. This energy optimization might still leave some high-energy noisy points, often from incorrect cues, motion boundaries, or occlusions. We filter these outliers based on their energy values in final step. To further densify the global point cloud, enabling each pixel to correspond to 3D point, we perform depth-based interpolation by computing scale offset. Stage V: Sliding window global refinement Given the estimated optical flow, this stage focuses on refining static structure. Note that we freeze camera parameters in this stage. Formally speaking, we solve the following: min Xstatic Cflow(Xstatic) (5) With consideration for accuracy and efficiency, the sliding window global refinement is capable of significantly enhancing the multi-view consistency of static points and generalizing effectively to real-world 4D scenes. The detailed process can be found in the appendix. 3.5 Dynamic Content Caption Generation Drawing upon the emphasis placed by LEO [67] and SceneVerse [68] on the criticality of caption quality and granularity for comprehensive scene understanding, we design captions at three specific levels: object, scene, and camera. Object captioning focuses on detailed object motion, scene captioning describes object-scene interactions, and camera captioning conveys intricate camera movement. To augment the caption, Large Language Models (LLMs) are employed to automatically rephrase initial captions and align them with these three granularity levels. Finally, to ensure data quality, human verification is conducted to filter out low-quality caption annotations. 7 Moving object captioning. Moving object captions provide detailed descriptions crucial for object grounding. However, prior datasets often have incorrect temporal alignment [68] or insufficient detail [17, 69], while current video captioning methods yield only simple (e.g., Panda-70M [70]) or non-localized descriptions (e.g., Qwen2.5-VL [63]). To address these limitations and generate detailed, accurate captions for individual objects, we utilize DAM [71], known for its superior capabilities. Given RGB videos and corresponding object masks, DAM [71] generates detailed and temporally aligned object descriptions through carefully designed prompts, enabling precise grounding and richer scene understanding. Dynamic scene captioning. Scene-level captions are designed to capture global information, depicting the key objects within the scene along with their associated actions, interactions, and functionalities. For comprehensive understanding of the entire dynamic scene, we utilize Qwen2.5VL [63] for dynamic scene captioning. To obtain more detailed, fine-grained, and accurate captions, we propose the use of structured captions. This process involves leveraging the fine-grained moving object captions as auxiliary input and employing specific prompting to generate the final scene-level descriptions. In the design of the prompts, we discovered that an explicit Hierarchical Prompt Design [72] significantly aids the Qwen2.5-VL[63] in comprehending its role, its expected format, and its operational boundaries. This approach contributes to the stabilization of the outputs format and enhances the overall quality of the results. Camera motion captioning. Camera Motion Captioning aims to describe the cameras trajectory and movement patterns. Using the powerful VLM [73], we analyze the sequence of inter-frame transformations to identify key motion types like panning, tilting, zooming, and dolly movements. This kinematic information is then used to generate natural language descriptions, potentially leveraging template-based generation or LLM prompting, to convey how the viewpoint changes over time. Caption rephrasing. Following the generation of three distinct caption types (object, scene, and camera motion), Large Language Model (LLM) [63] is employed to jointly process them. This step aligns the dynamic content descriptions across caption types and refines their phrasing to enhance overall consistency and readability. Human-in-the-loop quality review. To provide faithful comparison against larger pretrained models, human evaluation was used. Addressing persistent errors from source annotation inaccuracies, we implemented an iterative human-in-the-loop verification during caption construction to identify errors, trace sources, and revise/remove problematic data."
        },
        {
            "title": "4 Experiments",
            "content": "In this section , we present experimental results to evaluate the robustness of our DynamicGen pipeline. Due to the page limit, we direct readers to the appendix for implementation details, more qualitative results, and more experimental analyses. 4.1 Video Depth Estimation To evaluate video depth estimation accuracy, we assess several baseline methods, including metric depth predictors such as Metric3Dv2 [74], Depth-Pro [38], DepthCrafter [39], and Unidepth [41], which operate without scale or shift alignment. We also consider joint 4D modeling approaches, including MonST3R [65] and RCVD [75]. Evaluations are conducted on the Sintel [35] and KITTI [77] datasets, following standard protocols [39] by applying global shift and scale alignment to the predicted depth maps. We report absolute relative error (Abs Rel) and the percentage of inlier points (δ < 1.25), with all methods undergoing least-squares alignment in disparity space. As shown in Tab. 2, DynamicGen achieves the best overall performance across all datasets and evaluation metrics. In particular, it consistently outperforms prior approaches in both absolute accuracy and geometric consistency, demonstrating strong generalization to diverse and dynamic scenes. As illustrated in Fig. 5, MonST3R consistently struggles with object geometry reconstruction, producing distorted All research undertaken at Meta AI was limited to general guidance on model architectural design. Meta did not participate in any model training activities. Fan, Z. contributed to this project prior to the NeurIPS submission deadline. 8 Figure 5: Visual comparisons of 4D reconstruction on in-the-wild data. Table 2: Video depth evaluation on Sintel and KITTI datasets. Bold and underlined values indicate best and second best results, and blue-shaded cells denote our method. Alignment Category Method Abs δ1.25 Abs δ1.25 Sintel KITTI Per-sequence scale Joint depth & pose Monst3r [65] Uni4D [57] Single-frame depth Depth-pro [38] Metric3D [74] Video depth DepthCrafter [39] Joint video depth & pose 0.358 Robust-CVD [75] 0.292 CasualSAM [76] 0.216 Uni4D [57] DynamicGen(Ours) 0.205 0.344 0.289 0.280 0.205 0. 55.9 64.9 60.5 71.9 69.0 49.7 56.9 72.5 72.9 0.089 0.086 0.080 0. 0.112 0.182 0.113 0.098 0.091 91.4 93.3 94.2 98.8 88.4 72.9 88.3 89.7 91. Per-sequence scale & shift shapes and noisy dynamic masks. Uni4D also exhibits mask imprecision. DynamicGen, however, achieves the cleanest dynamic segmentations and the strongest dynamic/static reconstructions. 4.2 Camera Pose Estimation We evaluate our method against recent dynamic scene pose estimation approaches, including learningbased visual odometry (e.g., LEAP-VO [78], DPVO [79]) and joint depth-pose optimization methods (e.g., Robust-CVD [75], CasualSAM [76], MonST3R [65]). Experiments are conducted on the Sintel [35] and TUM-dynamics [80] datasets, following LEAP-VOs split for Sintel and subsampling the first 270 frames of TUM-dynamics, as done in MonST3R. Camera trajectories are aligned using Umeyama alignment [81], and we report Absolute Trajectory Error (ATE), Relative Translation Error (RPE trans), and Relative Rotation Error (RPE rot). As shown in Tab. 3, DynamicGen consistently achieves state-of-the-art results across all metrics and datasets, outperforming existing methods in both translation and rotation accuracy. 4.3 Camera Intrinsics Estimation Camera intrinsics are typically unavailable for most casual videos, especially those sourced from the Internet. However, accurate intrinsics are critical for reliable pose estimation and 3D reconstruction. 9 Table 3: Camera Pose Evaluation on Sintel and TUM-dynamic datasets. Bold and underlined values indicate best and second best results, and blue-shaded cells denote our method. Sintel TUM-dynamics Category Method ATE RPE trans RPE rot ATE RPE trans RPE rot Pose only DPVO [79] LEAP-VO [78] Joint depth & pose Robust-CVD [75] CasualSAM [76] Monst3r [65] Uni4D [57] DynamicGen(Ours) 0.171 0. 0.368 0.137 0.108 0.110 0.108 0.063 0.065 0.153 0.039 0.043 0.032 0.029 1.291 1.669 3.462 0.630 0.729 0.338 0.282 0.019 0. 0.096 0.036 0.108 0.012 0.012 0.014 0.031 0.027 0.018 0.022 0.004 0.004 0.406 2.843 2.590 0.745 1.371 0.335 0.331 To assess this, we evaluate focal length estimation accuracy on the Sintel dataset, with results summarized in Tab. 4. UniDepth predicts depth and focal length from single image, while Dust3r processes sequential frames but is trained under classical multi-view settings and fails to generalize well to dynamic scenes. In contrast, DynamicGen demonstrates strong generalization to dynamic content and achieves the best performance in both Absolute Focal Error (AFE) and Relative Focal Error (RFE), setting new state-of-the-art for focal length estimation in unconstrained video scenarios. Table 4: Camera intrinsics estimation. Table 5: Dynamic Scene Caption evaluation. Method AFE(px) RFE(%) UniDepth [41] Dust3r [43] DynamicGen(Ours) 447.4 434.0 413. 0.357 0.364 0.241 Method Acc. Com. Con. Rel. Avg. Direct Output + SAKFE + HP + Rephrasing + COT 79.28 80.23 82.57 82.48 84.38 76.65 77.46 81.42 80.50 82.09 73.23 74.01 71.17 71.86 75.87 80.33 81.45 82.56 83.27 85. 77.37 78.29 79.43 79.53 81.97 4.4 Caption Quality Evaluation To assess caption quality, we sampled 100 videos from the SA-V dataset [32]. As presented in Table 5, our experimental results indicate that integrating semantic-aware key frame extraction (SAKFE), hierarchical prompting (HP), caption rephrasing, and Chain-of-Thought (CoT) prompting [82] significantly enhances the quality of dynamic scene captions generated by Vision-Language Models (VLMs). We evaluated caption quality using the LLM-as-Judge metric G-VEval [83], conducting ten independent evaluations to ensure robust average results. The resulting captions exhibited notable improvements across accuracy, completeness, conciseness, and relevance, confirming the effectiveness of these strategies for improving caption quality in this task."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we address key limitations in traditional 4D data curation regarding scalability, physical realism, and modality diversity. We introduce DynamicGen, an automated pipeline leveraging foundation models for video filtering, metric-scale geometry and motion recovery, and hierarchical semantic captioning from raw videos. DynamicGens capabilities are validated through standard benchmarks on video depth and camera pose/intrinsics estimation, qualitative analyses on diverse web videos, and human/LLM-based evaluations confirming caption quality. Utilizing DynamicGen, we construct DynamicVerse, large-scale 4D dataset with over 100K dynamic scenes and rich physically grounded multimodal annotations. Together, this work offers scalable 4D data generation methodology and comprehensive new resource to advance 4D scene understanding."
        },
        {
            "title": "References",
            "content": "[1] Richard Newcombe, Dieter Fox, and Steven Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 343352, 2015. [2] Chao Yu, Zuxin Liu, Xin-Jun Liu, Fugui Xie, Yi Yang, Qi Wei, and Qiao Fei. Ds-slam: In 2018 IEEE/RSJ international semantic visual slam towards dynamic environments. conference on intelligent robots and systems (IROS), pages 11681174. IEEE, 2018. [3] Berta Bescos, José Fácil, Javier Civera, and José Neira. Dynaslam: Tracking, mapping, and inpainting in dynamic scenes. IEEE robotics and automation letters, 3(4):40764083, 2018. [4] Linhui Xiao, Jinge Wang, Xiaosong Qiu, Zheng Rong, and Xudong Zou. Dynamic-slam: Semantic monocular visual localization and mapping based on deep learning in dynamic environment. Robotics and Autonomous Systems, 117:116, 2019. [5] Jiahui Huang, Sheng Yang, Zishuo Zhao, Yu-Kun Lai, and Shi-Min Hu. Clusterslam: slam backend for simultaneous rigid body clustering and motion estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58755884, 2019. [6] Jesse Morris, Yiduo Wang, and Viorela Ila. The importance of coordinate frames in dynamic slam. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1375513761. IEEE, 2024. [7] Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, and Chuang Gan. Tesseract: Learning 4d embodied world models. arXiv preprint arXiv:2504.20995, 2025. [8] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [9] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint arXiv:2309.13101, 2023. [10] Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, et al. Embodied ai agents: Modeling the world. arXiv preprint arXiv:2506.22355, 2025. [11] Ying Zheng, Lei Yao, Yuejiao Su, Yi Zhang, Yi Wang, Sicheng Zhao, Yiyi Zhang, and Lap-Pui Chau. Embodied ai: survey on the evolution from perceptive to behavioral intelligence. arXiv preprint arXiv:2502.04809, 2025. [12] Panwang Pan, Zhuo Su, Chenguo Lin, Zhen Fan, Yongjie Zhang, Zeming Li, Tingting Shen, Yadong Mu, and Yebin Liu. Humansplat: Generalizable single-image human gaussian splatting with structure priors. Advances in Neural Information Processing Systems, 37:7438374410, 2024. [13] Hezhen Hu, Zhiwen Fan, Tianhao Wu, Yihan Xi, Seoyoung Lee, Georgios Pavlakos, and Zhangyang Wang. Expressive gaussian human avatars from monocular rgb video. arXiv preprint arXiv:2407.03204, 2024. [14] Samarth Sinha, Roman Shapovalov, Jeremy Reizenstein, Ignacio Rocco, Natalia Neverova, Andrea Vedaldi, and David Novotny. Common pets in 3d: Dynamic new-view synthesis of real-life deformable categories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48814891, 2023. [15] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. [16] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. [17] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale synthetic dataset for long-term point tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1985519865, 2023. [18] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andrés Bruhn. Spring: high-resolution high-detail dataset and benchmark for scene flow, optical flow and stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49814991, 2023. [19] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Dynamicstereo: Consistent dynamic depth from stereo videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1322913239, 2023. [20] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 28212830, 2018. [21] Guangcong Zheng, Teng Li, Xianpan Zhou, and Xi Li. Realcam-vid: High-resolution video dataset with dynamic scenes and metric-scale camera movements. arXiv preprint arXiv:2504.08212, 2025. [22] Chris Rockwell, Joseph Tung, Tsung-Yi Lin, Ming-Yu Liu, David Fouhey, and Chen-Hsuan Lin. Dynamic camera poses and where to find them. arXiv preprint arXiv:2504.17788, 2025. [23] Hanyu Zhou and Gim Hee Lee. Llava-4d: Embedding spatiotemporal prompt into lmms for 4d scene understanding, 2025. [24] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, and Yuan Liu. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025. [25] Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, and Hanspeter Pfister. 4d langsplat: 4d language gaussian splatting via multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [26] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. [27] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51885197, 2019. [28] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran. Unidentified video objects: benchmark for dense, open-world segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1077610785, 2021. [29] Pavel Tokmakov, Jie Li, and Adrien Gaidon. Breaking the\" object\" in video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2283622845, 2023. [30] Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khurana, Achal Dave, Bastian Leibe, and Deva Ramanan. Burst: benchmark for unifying object recognition, segmentation and In Proceedings of the IEEE/CVF winter conference on applications of tracking in video. computer vision, pages 16741683, 2023. [31] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. Mose: new dataset for video object segmentation in complex scenes. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2022420234, 2023. [32] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [33] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. Advances in Neural Information Processing Systems, 37:4895548970, 2024. [34] Yuheng Qiu, Chen Wang, Wenshan Wang, Mina Henein, and Sebastian Scherer. Airdos: Dynamic slam benefits from articulated objects. In 2022 International Conference on Robotics and Automation (ICRA), pages 80478053. IEEE, 2022. [35] Daniel Butler, Jonas Wulff, Garrett Stanley, and Michael Black. naturalistic open source movie for optical flow evaluation. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12, pages 611625. Springer, 2012. [36] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 40404048, 2016. [37] Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, and Aleksander Holynski. Stereo4d: Learning how things move in 3d from internet stereo videos. arXiv preprint arXiv:2412.09621, 2024. [38] Aleksei Bochkovskii, AmaÃG, Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. [39] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. [40] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. [41] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1010610116, 2024. [42] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. [43] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. [44] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [45] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [46] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [47] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [48] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [49] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision models. In European Conference on Computer Vision, pages 206228. Springer, 2024. [50] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning optical flow via global matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81218130, 2022. [51] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [52] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European Conference on Computer Vision, pages 1835. Springer, 2024. [53] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. [54] Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool. Unidepthv2: Universal monocular metric depth estimation made simpler. arXiv preprint arXiv:2502.20110, 2025. [55] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [56] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025. [57] David Yifan Yao, Albert Zhai, and Shenlong Wang. Uni4d: Unifying visual foundation models for 4d modeling from single video. arXiv preprint arXiv:2503.21761, 2025. [58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [59] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. [60] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, 2018. 14 [61] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 24432449, 2021. [62] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. [63] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [64] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In European Conference on Computer Vision, pages 523542. Springer, 2022. [65] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. [66] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323, 2023. [67] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023. [68] Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, and Siyuan Huang. Sceneverse: Scaling 3d vision-language learning for grounded scene understanding. In European Conference on Computer Vision, pages 289310. Springer, 2024. [69] Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, and Hanspeter Pfister. 4d langsplat: 4d language gaussian splatting via multimodal large language models. arXiv preprint arXiv:2503.10437, 2025. [70] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. [71] Long Lian, Yifan Ding, Yunhao Ge, Sifei Liu, Hanzi Mao, Boyi Li, Marco Pavone, Ming-Yu Liu, Trevor Darrell, Adam Yala, and Yin Cui. Describe anything: Detailed localized image and video captioning. arXiv preprint arXiv:2504.16072, 2025. [72] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:19472 19495, 2024. [73] Zhiqiu Lin, Siyuan Cen, Daniel Jiang, Jay Karhade, Hewei Wang, Chancharik Mitra, Tiffany Ling, Yuhan Huang, Sifan Liu, Mingyu Chen, Rushikesh Zawar, Xue Bai, Yilun Du, Chuang Gan, and Deva Ramanan. Towards understanding camera motions in any video. arXiv preprint arXiv:2504.15376, 2025. 15 [74] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [75] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16111621, 2021. [76] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William Freeman. Structure and motion from casual videos. In European Conference on Computer Vision, pages 2037. Springer, 2022. [77] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The international journal of robotics research, 32(11):12311237, 2013. [78] Weirong Chen, Le Chen, Rui Wang, and Marc Pollefeys. Leap-vo: Long-term effective any point tracking for visual odometry. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1984419853, 2024. [79] Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. Advances in Neural Information Processing Systems, 36:3903339051, 2023. [80] Jürgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of rgb-d slam systems. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 573580. IEEE, 2012. [81] Shinji Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis & Machine Intelligence, 13(04):376380, 1991. [82] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [83] Tony Cheng Tong, Sirui He, Zhiwen Shao, and Dit-Yan Yeung. G-veval: versatile metric for evaluating image and video captions using gpt-4o. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 74197427, 2025. [84] Olga Sorkine and Marc Alexa. As-rigid-as-possible surface modeling. In Symposium on Geometry processing, volume 4, pages 109116. Citeseer, 2007. [85] Wei-Chiu Ma, Shenlong Wang, Rui Hu, Yuwen Xiong, and Raquel Urtasun. Deep rigid instance scene flow. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36143622, 2019. [86] Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, and Hanbyul Joo. Banmo: Building animatable 3d neural models from many casual videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28632873, 2022. [87] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. arXiv preprint arXiv:2407.13764, 2024. [88] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Ricardo Martin-Brualla, and Steven Seitz. Hypernerf: higher-dimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228, 2021. 16 Figure 6: DynamicVerse dataset."
        },
        {
            "title": "A Appendix",
            "content": "In the appendix, we provide more results and analysis and summarize them as follows: In Section A.1, we introduce the broader impact of our DynamicVerse framework. In Section A.2, we supplement details of dynamic bundle adjustment. In Section A.3, we ablate the different components for dynamic bundle adjustment. In Section A.4, we provide additional experiments on generated hierarchical captions. In Section A.5, we provide more qualitative results of dynamic bundle adjustment. In Section A.6, we provide inference speed and computational cost for DynamicGen. In Section A.7, we provide the limitation. A.1 Broader Impact The introduction of DynamicVerse, with its large-scale, physically-aware, and multimodally annotated 4D dataset derived from real-world videos, is set to significantly influence several advanced research areas. Our frameworks unique ability to capture metric-scale geometry, real-world motion, instancelevel semantics, and descriptive captions offers an unparalleled resource that can catalyze progress in the following domains: Dynamic 4D Scene Generation: DynamicVerse offers paradigm shift for Dynamic 4D Scene Generation. Current methods often rely on limited simulators or struggle to realistically portray complex real-world physics and motion from internet-sourced content. By accurately interpreting real-world dynamics from monocular videos and integrating window-based Bundle Adjustment with global optimization, DynamicVerse converts long video sequences into comprehensive 4D multimodal format, capturing fine-grained dynamic information. This rich, real-world data provides an unparalleled training ground for generative models, leading to the creation of highly realistic, physically plausible, and semantically coherent dynamic 4D scenes. This has profound implications for high-fidelity content creation in entertainment (e.g., movies, games), realistic virtual environments for training and simulation (e.g., disaster response, architectural visualization), and the synthetic generation of diverse data for further AI research, helping to overcome privacy and data collection limitations. 4D Vision-Language Models (4D-VLM): DynamicVerse will greatly accelerate the development of sophisticated 4D Vision-Language Models that can reason about space, time, and semantics concurrently. Existing VLMs often operate on 2D images or short video clips with limited 3D awareness. Our framework provides unique combination of metric-scale 4D geometry, real-world dynamic motion, and comprehensive textual descriptions for long video sequences, allowing 4D-VLMs to learn intricate relationships between evolving 3D scenes and natural language narratives. Such models could enable more advanced human-agent interaction, where agents can provide detailed textual explanations of complex dynamic events they perceive in 4D, or understand nuanced, temporally extended instructions involving interactions within 3D space. This could revolutionize areas like AI-powered video captioning, temporal question answering in 3D, and the development of embodied AI agents that communicate their understanding of the dynamic world with human-like richness. 4D Language-Grounded Gaussian Splatting (4D-LangSplat): DynamicVerse offers foundational dataset for advancing 4D-LangSplat methodologies. While current 4D Gaussian Splatting techniques excel at novel view synthesis of dynamic scenes, their integration with language for semantic understanding and manipulation is still nascent. Our dataset, rich with 800K+ instance masks and holistic descriptive captions directly linked to evolving 3D structures and motions at physical scale, empowers 4D-LangSplat models. This will enable the development of systems that can not only reconstruct dynamic scenes with high fidelity but also allow users to query, edit, and interact with these 4D representations using natural language. For instance, users could ask an agent to \"track the red car that just turned left\" or \"remove the person walking in front of the fountain,\" with the model understanding both the spatial dynamics and the semantic context. This can significantly enhance applications in robotics, augmented reality, and interactive content creation by bridging the gap between visual perception and linguistic instruction in dynamic 3D environments. In summary, DynamicVerse is poised to serve as crucial catalyst, providing the data and framework necessary to bridge the gap between 2D understanding and true 4D world modeling, thereby fostering advancements in semantic scene understanding, dynamic object interaction, multimodal reasoning, and realistic content generation. A.2 Details of Dynamic Bundle Adjustment Camera parameterization In Eq. (2), ξ SE(3) represents the camera poses as rigid transformations. Rotations are parameterized using so(3) rotation vectors, which offer minimal representation facilitating direct optimization. Static Area Bundle Adjustment Term In Eq. (3), the bundle adjustment energy CBA(P, Xstatic) measures the consistency between the pixel-level correspondences and the 3D structure of static scene elements. Given the input pixel tracks = {Zk}K t=1, we filter all tracks corresponding to static areas and minimize the distance between the projected pixel location and the observed pixel location: k=0 and video segmentation = {Mt}T CBA(P, Xstatic; Z, M) = (cid:88) (cid:88) ZkM wk,tZk,t πK(Xk, ξt)2 (6) where Xk is the k-th 3D point, Zk,t is the k-th 3D points corresponding pixel tracks 2D coordinates at time t, wk,t {0, 1} is visibility indicator and πK is the perspective projection function. Camera Smoothness Prior In Eq. (3), given the video input, temporal smoothness prior is imposed on camera poses. This prior penalizes abrupt changes in relative pose, defined as ξtt+1 = ξ1 t+1 ξt. We adaptively reweight this term based on the magnitude of the relative motion. Specifically, larger relative motion results in reduced penalty on its change rate, while smaller relative motion incurs higher penalty. Formally, this is expressed as: Ccam(P) = (cid:88) Crot(Rt1,t,t+1) + (cid:88) Ctrans(Tt1,t,t+1) where Crot(Rt1,t,t+1) 2ttt+1tt1t tt1t+ttt+1 ; rad converts the rotation matrix into absolute radians. 2rad(Rtt+1)rad(Rt1t) rad(Rt1t)+rad(Rtt+1) = and Ctrans(tt1,t,t+1) = Non-Rigid Bundle Adjustment Term In Eq. (4), for dynamic objects, we impose nonrigid bundle adjustment term, ENR(Xdyn), which measures the discrepancy between the dynamic point cloud and pixel tracklets. Here, each pixel tracklet corresponds to dynamic 3D point sequence, {Xk,t}, optimized for each observed tracklet: CNR(Xdyn, P, Z, M) = (cid:88) (cid:88) zkM wk,tZk,t πK(Xk,t, ξt)2 (7) where Xk,t R3 is the k-th dynamic points location at t. In Eq. (4), Cmotion(Xdyn) is regularization term that encodes the characDynamic Motion Prior teristics of the dynamic structure. It contains two prior terms that are used to regularize the dynamic structure, both of which have demonstrated effectiveness in previous work. Cmotion(Xdyn) = Carap(Xdyn) + Csmooth(Xdyn). (8) Carap represents an as-rigid-as-possible (ARAP) prior [84] designed to penalize extreme deformations that compromise local rigidity. Specifically, for each dynamic control point k, its nearest neighbors are identified using k-Nearest Neighbors (KNN) on the remaining tracks. We then enforce that the relative distances among these neighboring pairs remain consistent, preventing sudden changes Carap = (cid:88) (cid:88) (k,m) wkmd(Xk,t, Xm,t) d(Xk,t+1, Xm,t+1)2 (9) where d(, ) is the L2 distance and wkm,t = 1 if all relevant points are visible. Csmooth is simple smoothness term that promotes temporal smoothness for the dynamic point cloud: Csmooth = (cid:88) (cid:88) wk,tXk,t Xk,t+12. (10) XkXdyn Despite simplicity, both motion terms are crucial in our formulation, as they significantly reduce ambiguities in 4D dynamic structure estimation, which is highly ill-posed. Unlike other methods, we do not assume strong modelbased motion priors, such as rigid motion [85], articulated motion [86], or linear motion basis [87]. Optical Flow Prior In Eq. (5), we also use flow projection loss to encourage the global point maps to be consistent with the estimated flow for the confident, static regions of the actual frames. More precisely, given two frames t, t, using their global point maps, camera extrinsics and intrinsics, we compute the flow fields from taking the global point map Xt, assuming the scene is static, and then moving the camera from to t. We denote this value Fglobal: tt , similar to the term defined in the confident static region computation above. Then we can encourage this to be close to the , in the regions which are confidently static Xglobal: tt estimated flow, Ftt according to the global est parameters: staic cam Cflow(Xstatic) = (cid:88) (cid:88) iW tW Xglobal: tt (Fglobal: tt cam Ftt est )1, (11) where indicates element-wise multiplication. Note that the confident dynamic mask is initialized using the foundation models as described in Sec. 3.3. During the optimization, we use the global static point maps and camera parameters to compute Fglobal cam and update the confident dynamic mask. eving an average score of 4.3. 19 A.3 Ablation Study on Different Components for Dynamic Bundle Adjustment Our dynamic BA pipeline introduces three key components absent in prior work like Uni4D [57], which systematically improve the decomposition of static/dynamic elements and global consistency: (a) Epi-Mask-Based Dynamics Filtering: We introduce geometric filtering step using an epipolar-based mask (\"Epi-mask\") to achieve cleaner separation between static background and dynamic foreground pixels before bundle adjustment. This leads to more stable camera pose estimation and background reconstruction. (b) VLM-Based Semantic Dynamics Analysis: We leverage Vision-Language Model (VLM) for high-level, semantic understanding of motion. This enables intelligent, motion-aware keyframe extraction and provides robust masks for dynamic objects, significant improvement over purely geometric or flow-based segmentation. (c) Optical Flow-Based Sliding Window Global Refinement: To address error accumulation and temporal drift common in long videos, we implement global refinement strategy over sliding window. This enforces long-range temporal consistency, correcting errors that frame-by-frame or local BA approach would miss. Table 6: Components Ablation on Sintel. Ablations (a) (b) (c) ATE 0.114694 Baseline 0.114065 Ablation-1 0.11053 Ablation-2 0.114694 Ablation-3 0.108459 Ablation-4 0.114065 Ablation-5 0.110530 Ablation-6 DynamicGen (Ours) 0. RPEtrans RPErot 0.347920 0.032125 0.335198 0.032250 0.334005 0.033122 0.347920 0.032125 0.281979 0.028906 0.335198 0.032250 0.334005 0.033122 Abs δ1.25 0.216433 0.215058 0.210339 0.214282 0.205892 0.214143 0.207329 0.725167 0.726943 0.722999 0.724084 0.727616 0.725534 0. 0.028906 0.281979 0.204574 0.728961 A.4 Additional experiments on generated hierarchical captions. We performed three distinct experiments to validate the high quality of our hierarchical semantic annotations: (a) Object-Level Semantics via 4D-LangSplat [69]: To validate the annotations produced by our DynamicGen framework, we performed time-sensitive querying experiment using 4D-LangSplat model. For this evaluation, we trained the model on the \"americano\" scene from the HyperNeRF dataset and benchmarked it against re-implemented 4D-LangSplat* baseline. The results, presented in Tab. 7, demonstrate that our approach yields substantial gains in Accuracy and volumetric Intersection over Union (vIoU). This superior performance confirms that our precise object masks and labels are highly effective for demanding multi-modal applications. Table 7: Quantitative comparisons of time-sensitive querying on the HyperNeRF [88] dataset."
        },
        {
            "title": "Method",
            "content": "4D-LangSplat* [69] DynamicGen americano Acc(%) 53.84 64.42 vIoU(%) 27.55 51.65 (b) Scene-Level Semantics via G-VEval [83]: To rigorously assess our scene-level captions, we moved beyond single-score metrics and employed more granular evaluation using the ACCR framework in G-VEval benchmark. This approach provides comprehensive, multi-dimensional assessment of caption quality across four key axes: Accuracy, Completeness, Conciseness, and Relevance. On random sample of 100 videos from SA-V data, our generated captions demonstrated high performance across all four criteria, as detailed in the Tab. 8. The strong performance across these metrics confirms that our captions are not only factually accurate and relevant to the video content, but also complete in their coverage of events and efficiently concise. This robust, multi-faceted quality makes them highly suitable and reliable for demanding downstream applications. Table 8: Evaluation of generated captions using the ACCR framework from G-VEval. Evaluation Criteria Accuracy Completeness Conciseness Relevance Average Scene-Level Captions 84. 82.09 75.87 85.56 81.97 (c) Camera-Level Semantics via Human Study: We conducted formal human study to quantitatively analyze the quality of the final camera motion captions. Following prior work [73], we asked human evaluators to rate our captions on three criteria: (1) Clearness (clarity of information), (2) Conciseness (brevity without losing clarity), and (3) Grammar & Fluency. On sub-sample of 88 videos from our dataset (i.e., filtered DAVIS), our captions performed excellently. The results, presented in Tab. 9 showed that over 60.22% of the captions were rated as both clear and fluent, while also receiving high scores for conciseness. This confirms the effectiveness of our generation and quality control process. Table 9: Human evaluation results for the generated camera captions. Scores indicate the percentage of captions that met each quality criterion. Human Evaluation Rated as Clear Rated as Fluent Rated as Concise Camera Captions 85.22% 89.77% 67.04% A.5 More qualitative results of dynamic bundle adjustment We present additional qualitative reconstruction results in Fig. 8, demonstrating the generalizability and performance of our pipeline on real-world data. A.6 Inference Speed and Computational Cost for DynamicGen For reproducible analysis of computational performance, we processed the entire Sintel training set (23 videos) on NVIDIA H20 GPUs. detailed breakdown of the average processing time and peak VRAM consumption for each component of our pipeline is provided in Table 10. Module Hardware Used Avg. Time / Sintel Peak VRAM Notes Video (mins) (GB) Table 10: Computational Cost Analysis. 1. Motion-aware Keyframe Extraction 1x H20 GPU 2. VLM-Based Semantic Analysis (Qwen-VL) 2x H20 GPU 3. Moving Object Segmentation (SA2VA) 1x H20 GPU 4. Dynamic Bundle Adjustment 1x CPU Core + 1x H20 GPU 5. Moving Object Captioning 6. Dynamic Scene Captioning 7. Camera Motion Captioning 8. Caption Rephrasing Total (per video) 2x H20 GPU 2x H20 GPU 2x H20 GPU 1x H20 GPU H20 GPU 0.1 1. 0.8 12.2 2.0 3.0 2.0 2. 23.7 10 60 30 30 40 40 24 60 Selects representative frames Identifies dynamic elements Per-object video segmentation Main time bottleneck Object-level descriptions Scene-level descriptions Camera-level descriptions LLM-based refinement for consistency and conciseness Peak VRAM, not sum A.7 Limitations Despite its considerable capabilities, DynamicVerse exhibits several inherent limitations. First, its reliance on in-the-wild internet videos introduces significant noise and quality variance. This can compromise the fidelity of metric-scale geometry and motion recovery, particularly in complex, cluttered, or occluded scenes that fall outside the typical distribution of the foundation models training 21 Figure 7: Examples captions on DAVIS dataset. data. Second, the substantial computational overhead required to process long video sequences with large-scale models presents practical barrier to real-time performance and scalable deployment. Finally, while extensive, the dataset cannot exhaustively capture the long tail of real-world phenomena. Consequently, the models generalization to truly novel environments is fundamentally tethered to the intrinsic biases and capabilities of its underlying foundation models. These limitations raise AI-safety concerns: (i) privacy and security risks, since metric-scale reconstructions from web videos can expose sensitive interiors or critical infrastructure and facilitate covert mapping or surveillance; and (ii) miscalibrated confidence under distribution shift, producing plausible but erroneous geometry and dynamics that misguide downstream robotic or AR planners. Biases and licensing gaps in foundation models and web data may further perpetuate representational harms and legal or IP issues. practical mitigation is to prefilter ineligible videos using policy rules and automated detectors (e.g., content with PII, sensitive interiors or infrastructure, minors, or restricted licenses). 22 Figure 8: Qualitative Results on in-the-wild data. We show qualitatively some of our reconstruction results on in-the-wild data. For full reconstruction, please refer to our attached supplementary webpage. 23 Figure 9: Qualitative Results of moving object Segmentation. We show qualitatively some of our segmentation results on the Youtube-VIS dataset compared with other baselines."
        }
    ],
    "affiliations": [
        "CUHK",
        "Meta",
        "PKU",
        "UT Austin",
        "UW",
        "XMU"
    ]
}