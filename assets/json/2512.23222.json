{
    "paper_title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
    "authors": [
        "Jiaxu Zhang",
        "Tianshu Hu",
        "Yuan Zhang",
        "Zenan Li",
        "Linjie Luo",
        "Guosheng Lin",
        "Xin Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 2 2 2 3 2 . 2 1 5 2 : r Bridging Your Imagination with Audio-Video Generation via Unified Director Jiaxu Zhang1,2 Tianshu Hu1, Yuan Zhang1 Zenan Li1 Linjie Luo1 Guosheng Lin2, Xin Chen1,, 1ByteDance Intelligent Creation, 2Nanyang Technological University Project Lead, Corresponding Authors"
        },
        {
            "title": "Abstract",
            "content": "Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within single framework, as logical reasoning and imaginative thinking are both fundamental qualities of film director. In this work, we propose UniMAGE, unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audiovideo generation models. To achieve this, we employ the Mixture-ofTransformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce first interleaving, then disentangling training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved textimage data to foster the models deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images. Date: December 30, 2025 Project Page: https://kebii.github.io/UniMAGE"
        },
        {
            "title": "Introduction",
            "content": "\"A writer needs pen, an artist needs brush, but filmmaker needs an army.\" Orson Welles Audio-video generation has advanced rapidly in recent years, with models such as Veo 3 [3], Sora 2 [21], and Keling 2.5 [13] demonstrating remarkable creative potential and inspiring users across online communities. However, these models primarily focus on visual fidelity and temporal coherence within short, single-shot videos, which limits their ability to convey long-form narratives. To address this limitation, recent approaches [11, 18, 30] have begun to employ large language models (LLMs) and image generation models as collaborative agents that align user prompts with multi-shot video captions and generate key images for each shot, thereby facilitating film-level video generation. Nevertheless, these methods typically treat script drafting and keyframe generation as two disjoint tasks, resulting in limited narrative logic and visual consistency. 1 Figure 1 Showcase of UniMAGEs multimodal directing abilities. UniMAGE unifies script drafting, extension, continuation, and keyframe image generation, thereby enabling coherent long-form storytelling with consistent characters and cinematic visual compositions. The generated scripts and keyframes can further serve as structured, high-level guidance for existing audio-video joint generation models. In traditional filmmaking, scriptwriting and storyboard design are inseparable processes that require an experienced director to systematically plan both the overall narrative and the visual composition. film director functions much like conductor in an orchestra, bringing out the strengths of each element while maintaining overall harmony. Inspired by this analogy, we posit that unified understanding and generation model is essential to bridge user intents with audio-video generation systems, rather than relying on separate agents. Accordingly, in this work, we present unified director model named UniMAGE. UniMAGE is built upon the Mixture-of-Transformers (MoT) architecture proposed in Bagel [6], which integrates an LLM and diffusion model within single transformer framework, enabling joint reasoning and generative learning across text and image modalities. Although MoT provides strong, unified architecture, UniMAGE faces more challenging task than previous multimodal systems, as film scripts inherently involve lengthy, context-rich sequences. On the one hand, maintaining image consistency across long-context sequences is extremely challenging. Previous studies [7, 32, 37] mainly focused on preserving single subject across frames or conducting limitedstep image editing. However, when scripts involve multiple actors, existing models struggle to reliably recognize and maintain individual identities. UniMAGE addresses this issue through Interleaved Concept Learning. Specifically, we organize the script and corresponding keyframes into textimage interleaved format and jointly optimize the parameters of the LLM and diffusion transformer. This interleaved representation enables the model to better capture the overall narrative concept. To further enhance image consistency, we introduce In-Context ID Prompting strategy, which inserts special tokens among the reference ViT and VAE tokens to indicate the indices of generated keyframes and the identities of the characters, thereby enabling stable multi-subject appearances across long sequences. On the other hand, ensuring coherent script logic and supporting flexible continuation remain highly demanding. Previous studies [11, 18, 30] mainly relied on prompting pre-trained LLM to generate scripts, which requires per-sample prompt engineering and often overlooks reasonable storyboard design, making it difficult to preserve logical coherence and temporal alignment in multi-scene storytelling. UniMAGE overcomes this limitation through Disentangled Expert Learning. Specifically, we maintain keyframe generation in an interleaved format but freeze the LLM parameters when optimizing the diffusion loss, and decouple the script-writing process by 2 training the LLM as dedicated script expert. To further support flexible script continuation, we employ the Pre-Context Script Splitting strategy, which randomly divides complete scripts so that the model learns to continue script generation based on the preceding context or given user prompt, thereby ensuring logically coherent and adaptable narrative development. Our contributions are summarized as follows: Concept: UniMAGE embodies the concept of unified director model\" that holistically orchestrates narrative logic and visual composition, bridging user intent with multimodal script for creative audio-video generation. Technique: UniMAGE employs Interleaved Concept Learning and Disentangled Expert Learning with InContext ID Prompting and Pre-Context Script Splitting to enhance visual consistency and narrative coherence over long-context sequences. Performance: UniMAGE demonstrates strong capabilities and generalization in long-form, multi-scene script creation, achieving superior narrative coherence, character stability, and image consistency compared with existing agent-based and unified models."
        },
        {
            "title": "2 Related Work",
            "content": "Generative Models for Film Production. Recent studies [1, 11, 19, 23, 31] have leveraged the emerging reasoning and planning capabilities of large language models, together with the image generation capabilities of diffusion models, to advance AI-assisted filmmaking. For script creation, Anim-Director [15] employs an LLM to expand high-level prompts into detailed and structured scripts. FilmAgent [33] simulates directors workflow in 3D virtual environments, covering idea development, scriptwriting, and cinematography to support the end-to-end filmmaking process. MovieAgent [30] adopts multi-agent Chain-of-Thought strategy to automate script breakdown, scene planning, and shot design. For keyframe generation, StoryDiffusion [37] introduced plug-and-play attention module for diffusion models to maintain character consistency across image sequences. Story2Board [7] used lightweight consistency mechanism to preserve the models generative prior, enabling expressive storytelling. Furthermore, FilMaster [11] is comprehensive AI-based film generation system explicitly designed around cinematic principles to guide camera language and rhythm. However, most of these systems decoupled script generation and visual synthesis into separate agents, leading to weak narrativevisual alignment and limited long-range coherence. Although SEED-Story [34] leverages unified MLLM to produce multimodal stories, it requires separate training for each story instance, and its generated images still suffer from limited visual quality. In contrast, UniMAGE integrates script reasoning and generation within mixed transformer architecture, enabling end-to-end generalizable learning of both narrative logic and high-quality visual composition in single framework. Unified Multimodal Generation. Recent efforts have explored two main approaches to unifying multimodal understanding and visual generation. One adopts an auto-regressive (AR) framework to jointly produce text and image tokens, as in Emu [27] and Chameleon [24], which treat images as continuous or discrete visual tokens alongside text. However, AR models generally generate lower-quality images compared with diffusion-based approaches. The other line of research integrates diffusion modules into large language models, such as Show-O [32] and TransFusion [36], which synthesize text and images within shared transformer backbone. Bagel [6] further introduces Mixture-of-Transformers architecture that selectively activates modality-specific parameters while enabling long-context interaction between multimodal understanding and generation through shared self-attention mechanisms. Nevertheless, current unified models mainly handle short-context reasoning and generation, without mechanisms to preserve narrative or visual coherence across long sequences. Based on the MoT framework, our UniMAGE jointly learns narrative structure and visual composition, enabling coherent long-form script and keyframe generation. Audio-Video Generation. Early studies on joint audio-video generation, such as MM-Diffusion [22], used U-Net architectures to model audio and video separately. Later works like SyncFlow [16], Uniform [35], and UniVerse-1 [26] expanded training on large datasets, improving cross-modal alignment and generalization. 3 Recent commercial systems, including Veo 3 [3] and Sora 2 [21], have achieved notable progress in synchronized audiovideo generation. However, these models rely heavily on well-structured prompts and are typically limited to short, single-shot videos. To compensate, they often incorporate LLM-based re-captioning modules to refine user input before generation [10]. Despite these advances, current methods still lack the capability to plan and maintain coherence across multi-shot, narrative-driven sequences. To overcome this, we present UniMAGE, unified director model that bridges user imagination with long-context, film-like audiovideo creation through multimodal script generation."
        },
        {
            "title": "3 Method",
            "content": "We introduce UniMAGE, unified director model that transforms simple user prompts into illustrated scripts. Given user prompt ρ, the overall pipeline of UniMAGE can be formulated as: UniMAGE(ρ, ˆS) (cid:55) (G, C, F), (1) where denotes the global descriptions, such as character and environment definitions; = (c1, c2, ..., cn) represents the content descriptions for video shots, including keyframe and video-level narratives; and = (f1, f2, ..., fn) corresponds to the keyframe images for those shots. The full script is defined as = (G, C, F), and the superscript ˆS indicates that the input is optional. ˆS is used for script extension and continuation. Through this formulation, UniMAGE uniformly performs script creation, prompt-based or in-context script continuation, and keyframe generation, thereby empowering automatic videoaudio creation. UniMAGE adopts the MoT architecture, consisting of two transformer experts: one for multimodal understanding and another for image generation. Correspondingly, it uses two types of visual encoders, an understanding-oriented encoder (ViT), and generation-oriented encoder (VAE). Both transformer experts process the same token sequence through shared self-attention layers. For text token prediction, UniMAGE follows the Next Token Prediction paradigm [20], leveraging the established advantages of autoregressive modeling. For visual token prediction, it utilizes the Rectified Flow [8], aligning with prevailing practices in visual generation."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Next Token Prediction (NTP) serves as the fundamental training objective for auto-regressive models [20]. Given token sequence = (y1, y2, ..., yn), where each token yt is drawn from fixed vocabulary , and assuming follows data distribution (y), the auto-regressive model Pθ decomposes the joint probability as: Pθ(y) = (cid:81)T t=1 pθ(yt y<t), where y< = (y1, y2, ..., yt1) denotes the sequence of tokens preceding position t. The model parameters θ are optimized to maximize the likelihood of the true token at each step, conditioned on its ground-truth history from the training dataset. Formally, the maximum likelihood estimation of θ can be expressed as: θ = arg max θ (cid:88) t=1 log pθ (yt y<t) . (2) During inference, the model generates tokens sequentially by either sampling or selecting the most probable token, conditioned on given or learned context. Rectified Flow is an ordinary differential equation (ODE) model that transports samples from π0 to π1 along straight trajectories [17]. Given two samples X0 π0 and X1 π1, the drift field is trained to approximate their difference through the following objective: (cid:90) 1 0 min ϕ (cid:2)(X1 X0) vϕ(Xt, t)2(cid:3) dt, (3) where Xt = (1 t)X0 + tX1. In practice, vϕ is parameterized by neural network. After training, samples from π1 can be obtained by integrating the ODE along the learned straight flow with only few steps. 4 Figure 2 Script structure of UniMAGE. The script structure includes three components: global descriptions (G), content descriptions (C), and keyframe images (F), together with user prompt (ρ). Special tokens and indicator symbols are used to denote key script elements."
        },
        {
            "title": "3.2 Script Structure",
            "content": "As illustrated in Figure 2, the script structure used in UniMAGE consists of three components: global descriptions G, content descriptions C, and keyframe images F, along with user prompt ρ. We design set of special tokens to represent each element in the script, including <User>, <CharacterN >, <EnvironmentN >, <FrameN >, and <VideoN >, where denotes the index. When character or environment defined in the global description appears within the content description, the corresponding special token is used to indicate the subjects occurrence in the scenefor example, On the deck of the grand ocean liner<Environment1>, young man<Character2> . . . . These special tokens help the model accurately identify characters and environments, while keeping the script concise and structurally consistent. The content descriptions are further divided into two complementary layers. Frame descriptions capture the static visual layout of key moments, such as camera position, lighting, and character placement. Video descriptions, in contrast, focus on temporal and narrative aspects, including dialogues, plot progression, and actions. In addition, we introduce indicator symbols <- -> to denote character dialogue and environmental sound effects, allowing the corresponding audio content to be easily retrieved in subsequent stagesfor example, <-Now close your eyes. Go on.->. To accommodate diverse user input formats, we define four distinct styles of user prompts, which are randomly sampled during training. Detailed definitions of these prompt styles are provided in the supplementary materials."
        },
        {
            "title": "3.3 Interleaved Concept Learning",
            "content": "We initialize the MoT model of UniMAGE using the pre-trained weights from Bagel, which provide strong foundational capability for unified multimodal understanding and generation. However, unlike Bagels training format, which focuses on multi-step image editing, UniMAGE is required to generate not only images but also script text conditioned on the understanding of preceding narratives. Moreover, the long-context nature of script data further exceeds the base models capacity, necessitating Interleaved Concept Learning strategy for coherent narrative and visual generation. With the script structure defined above, the script can be organized as interleaved textimage data. As illustrated in the left part of Figure 3, we first perform Interleaved Concept Learning, which enables the MoT model to generate text and images in an interleaved manner, thereby facilitating deeper understanding of lengthy, context-rich scripts. This training stage is conceptually similar to the Chain-of-Thought strategy [9, 28], where the text content functions as the models reasoning process, followed by image generation conditioned on the preceding narrative context. In this stage, all parameters of the two transformer experts are jointly optimized, allowing the generated results to influence the models textual understanding and vice 5 Figure 3 Illustrations of Interleaved Concept Learning and Disentangled Expert Learning. To enhance visual consistency and logical coherence across long-context scripts, as well as to fully leverage both textual and image data, we first optimize all MoT parameters using interleaved textimage data, and then disentangle the training of the understanding and generation expertsusing pure text scripts for the former and textimage data for the latter. versa. Nevertheless, scripts often involve multiple characters and scenes, making it highly challenging for the model to consistently maintain the identities and visual coherence of different entities across long sequences. To address this, we propose In-Context ID Prompting as follows. In-Context ID Prompting. The core of addressing the visual consistency problem lies in enabling the model to recognize the characters and scenes depicted in each image and associate them with the global text descriptions and historical keyframes. Given that text tokens and image tokens are aligned within the pre-trained unified architecture, we can leverage the text to prompt and highlight key information represented in the images, thereby facilitating long-form visual consistency. As illustrated in the left part of Figure 4, within the ViT tokens used for understanding and the VAE tokens used for image reference, we insert special text tokens to indicate the frame ID, as well as the character and environment IDs appearing in each image. We apply full attention between each images ViT or VAE tokens and its corresponding special tokens. This In-Context ID Prompting strategy, together with the special tokens defined in our script structure, effectively preserves long-range associations between the script text and the generated images, ensuring consistent visual identity and scene continuity throughout the narrative."
        },
        {
            "title": "3.4 Disentangled Expert Learning",
            "content": "The Interleaved Concept Learning stage equips UniMAGE with holistic and coherent understanding of the overall script. However, this interleaved generation strategy inevitably constrains the models flexibility in content creation, particularly for tasks such as script extension and continuation, where the model must dynamically adapt to new user prompts or seamlessly expand upon existing narrative contexts. In addition, obtaining logically consistent multi-shot text-image data is inherently difficult, which limits the models ability to fully learn long-form narrative logic from interleaved data. Consequently, we introduce Disentangled Expert Learning strategy. As illustrated in the right part of Figure 3, in this training stage, we decouple script content generation from interleaved keyframe generation and optimize the understanding transformer expert using pure text scripts. Meanwhile, the generation transformer expert is further optimized using interleaved textimage data, with the understanding branch frozen via stop-gradient. In addition, we incorporate textimage pairs into the training process to further improve visual fidelity. Through this strategy, both script logic and image quality are effectively enhanced, as the model can fully exploit heterogeneous multimodal training data beyond the interleaved script data. Finally, to enable script extension and continuation, we introduce the Pre-Context Script Splitting strategy, described as follows. Pre-Context Script Splitting. Based on the pure text scripts, we randomly insert new user or system prompts to simulate two types of creative demands. The first is prompt-based script extension, as illustrated in the middle part of Figure 4. Specifically, we divide complete script into two parts and insert the indicator token 6 Figure 4 Illustrations of In-Context ID Prompting and Pre-Context Script Splitting. The former enhances visual consistency by aligning generated images with global character and scene descriptions, while the latter enables adaptive narrative extension and continuation. <Extension>, followed by new user prompt within the script, allowing the model to learn how to extend an existing narrative coherently from given prompt. The new user prompt is generated by summarizing the second part of the script using Qwen 2.5 [12]. The second is in-context script continuation, as shown in the right part of Figure 4. In this case, we insert the indicator token <Continuation>, followed by system prompt before the last shot of the script, enabling the model to infinitely continue the script during inference."
        },
        {
            "title": "3.5 Inference of UniMAGE",
            "content": "During inference, we maintain the disentangled generation process for text and images. Specifically, UniMAGE first generates multi-shot text script conditioned on the user prompt. The user can then extend the narrative with new prompts or continuously generate subsequent shots based on the previously generated content. Finally, the complete script is segmented into individual shots, and corresponding keyframe images are generated in an interleaved manner. This unified yet disentangled strategywhere single model handles both modalities while separating the generation processes for text and imageeffectively ensures the logical coherence of the script and the visual consistency of the generated images. As result, UniMAGE can produce longer, more coherent narratives while mitigating issues such as plot repetition and image distortion."
        },
        {
            "title": "4 Experiments",
            "content": "Datasets. To support UniMAGEs unified multimodal training paradigm, we construct large-scale and diverse dataset that integrates multi-shot scripts, long-form textual narratives, and high-quality textimage pairs. The dataset is composed of three complementary subsets, each aligned with specific learning objective within UniMAGE: (1) Multi-shot textimage scripts (450k sequences). We collect multi-shot videos from broad range of open-source cinematic content, short films, and documentaries. Each video is segmented into coherent shots using visual scene transition detection. For each shot, we employ Gemini 2.5 Pro [2] to generate detailed textual annotations. This subset forms the backbone for Interleaved Concept Learning, enabling UniMAGE to model multimodal reasoning and maintain global consistency across textimage interleaved sequences. (2) Multi-shot text scripts (250k samples). To further enhance long-form narrative capability beyond visually grounded data, we curate large corpus of textual scripts. These scripts are reorganized and structured using Qwen 2.5 [12] to fit the hierarchical script format of UniMAGE. This purely textual subset is essential for Disentangled Expert Learning, enabling the understanding expert to learn rich narrative logic, shot transitions, and dialog conventions. (3) Single-shot textimage pairs (250k samples). To improve image quality and fidelity, particularly for character rendering and scene composition, we curate large set of single-shot images. Each image is re-captioned with Gemini 2.5 Pro [2] to obtain detailed, script-structured descriptions. This subset is leveraged in the generation experts training during the Disentangled Expert Learning stage, enabling improved visual precision, diversity, and controllability. 7 Figure 5 Comparison with the baselines for multi-character script generation. UniMAGE demonstrates superior ability to maintain consistent character identities and visual coherence across multiple shots, whereas the baseline methods fail to preserve such consistency. Implementation details. We implement UniMAGE based on the open-source framework of BAGEL [6], which provides unified MoT architecture as the foundation for multimodal understanding and generation. All experiments follow the standardized training pipeline and parallel strategy with BAGEL. During the Interleaved Concept Learning stage, only the multi-shot textimage script data is utilized, with learning rate of 1e-5 and total of 30,000 training steps. In the subsequent Disentangled Expert Learning stage, the entire dataset is employed, using the same learning rate of 1e-5 for 10,000 training steps. Notably, in this stage, only pure textual samples are used to optimize the understanding branch, while during the optimization of the generation branch, the text tokens are detached from the computational graph to prevent gradient propagation and ensure disentangled learning. Evaluation metrics. We conduct both qualitative and quantitative evaluations to comprehensively assess the performance of UniMAGE across narrative coherence, character consistency, and visual quality. Qualitative results focus on long-form storytelling scenarios, where we present multi-shot scripts and corresponding keyframes generated from diverse user prompts. Quantitative results are obtained on the public benchmark ViStoryBench [38], which assesses story visualization models across various narrative structures, visual styles, and character settings. We report six metrics: Style Similarity (CSD), Character Identification Similarity (CIDS), Prompt Adherence (Alignment), Onstage Character Count Matching (OCCM), Image Quality (Inception), and Aesthetics."
        },
        {
            "title": "4.1 Qualitative Results",
            "content": "We compare UniMAGE with recent script visualization methods, including StoryDiffusion [37] and Story2Board [7], as well as the multimodal script generation model SEED-Story [34]. The textual scripts used for StoryDiffusion and Story2Board are generated by UniMAGE. More results are presented in the supplementary materials. Multi-character script generation. Figure 5 compares UniMAGE with baseline models on multi-character script generation. While existing methods can generate plausible single frames, they generally struggle to 8 Figure 6 Comparison with the baselines for long-form script generation. UniMAGE demonstrates substantially improved visual coherence, narrative consistency, and scene diversity across long story sequences, whereas the baseline methods are limited in these aspects. maintain consistent identities across multiple shots. When the narrative shifts to new scenes or camera angles, baseline models often produce noticeable variations in facial structure, hairstyle, or clothing, leading to unstable and mismatched character appearances. In contrast, UniMAGE achieves stable identity preservation throughout the entire sequence. Thanks to its unified director architecture and the proposed In-Context ID Prompting, the model can reliably associate each character in the image with the corresponding textual identity defined in the script. This enables UniMAGE to maintain coherent visual traits even as the story spans different environments and shot configurations, resulting in noticeably more consistent and faithful multi-character narratives. Long-form script generation. Figure 6 shows the comparison on long-form script generation. StoryDiffusion maintains basic character consistency but suffers from limited scene variation, leading to repetitive visual patterns and copy-and-paste artifacts across shots. Story2Board and SEED-Story exhibit larger inconsistencies, and both models struggle to keep stable visual styles or character identities, producing fragmented transitions and weakening narrative coherence. In contrast, UniMAGE effectively models long-range temporal structure and follows the plot progression with much higher fidelity. With unified multimodal reasoning and stable ID conditioning, UniMAGE maintains consistent visual style and character appearance throughout the sequence, resulting in more coherent, diverse, and cinematic long-form storytelling. Multimodal script generation (vs. SEED-Story). Although SEED-Story enables unified textimage script generation, its training is limited to only three animation datasets, resulting in poor generalization beyond stylized domains. As shown in Figure 7, its outputs often exhibit weak narrative logic and noticeably lower image quality. In contrast, UniMAGE generates more coherent plots with clearer causal structure, and produces significantly higher-quality keyframe images. Owing to its diverse training data and unified director architecture, UniMAGE generalizes well across different genres, visual styles, and storytelling formats. 9 Figure 7 Comparison with SEED-Story on multimodal script generation. UniMAGE demonstrates superior logical coherence, visual quality, and cross-domain generalization, reflecting its stronger capability in unified multimodal understanding and generation. Figure 8 Ablation experiments. In-Context ID Prompting contributes to preserving consistent multi-character identities across multi-shot and long-form sequences; the red circles mark failure cases without it. Pre-Context Script Splitting ensures coherent and flexible script extension and continuation; the red box highlights content repetition issues when it is absent. Ablation study for In-Context ID Prompting. Figure 8 presents the ablation results of the proposed In-Context ID Prompting strategy. With this mechanism, UniMAGE can reliably associate each visual entity with its corresponding textual identity, enabling stable character appearance across shots. When the strategy is removed, identity cues become ambiguous during scene transitions or long temporal spans, leading to mixed appearance and inconsistent depictions, as marked by the red circles. This confirms that explicit ID conditioning plays crucial role in achieving robust multi-character consistency in long-form generation. Methods CSDself CIDSself Alignment OCCM Inception Aesthetics TheaterGen [1] Story-Adapter Story2Board [7] StoryDiffusion [37] SEED-Story [34] UniMAGE w/o ID-P UniMAGE (Ours) 40.4 73.7 55.2 63.5 74.9 52.6 59.0 53.3 56.4 51.4 57.0 48.7 55.6 59.2 37.9 58.9 56.7 59.7 29. 62.5 80.8 84.4 84.9 85.5 85.3 85.9 87.00 88.07 14.88 13.73 15.26 15.71 6.33 12.06 12.97 4.90 4.89 5.11 5.76 3. 4.44 4.55 Table 1 Quantitative comparisons and ablative experiments. The best results are bolded, and the second-best are underlined. UniMAGE achieves superior performance in consistency-related metrics and alignment. Figure 9 User study. Comparison of average rankings, where lower values correspond to higher quality. Ablation study for Pre-Context Script Splitting. As shown in the bottom part of Figure 8, the Pre-Context Script Splitting strategy significantly improves UniMAGEs ability to perform script extension and continuation. By training the understanding branch to generate narratives from partial context, the model learns to follow existing plot structures and introduce new developments naturally. In contrast, the interleaved-only training baseline frequently results in repetitive descriptions and weakened narrative flow, demonstrating the necessity of explicit pre-context conditioning for coherent long-form script generation."
        },
        {
            "title": "4.2 Quantitative Results",
            "content": "Table 1 reports the quantitative results on ViStoryBench. UniMAGE achieves the best overall performance, particularly in the consistency-related metrics. It obtains the highest CIDS (59.2) and OCCM (88.07), demonstrating strong character identity preservation across shots. UniMAGE also achieves large improvement in Alignment (80.8), outperforming previous methods by clear margin, indicating substantially better adherence to narrative prompts. While some baselines show competitive scores in isolated metrics (e.g., SEED-Story in CSD or StoryDiffusion in image quality and aesthetics), none offer balanced performance across consistency, narrative alignment, and visual quality as UniMAGE does. Additionally, the notably high CSD score of SEED-Story is mainly due to its overfitting to narrow set of animation-style datasets, which leads to strong stylistic consistency but poor generalization across broader visual domains. The comparison between UniMAGE w/o ID-P and the full UniMAGE confirms the importance of the In-Context ID Prompting strategy. Removing ID-P significantly degrades CSD, CIDS, and OCCM, indicating weaker identity stability and scene coherence. This demonstrates that explicit ID conditioning is crucial for maintaining consistent multi-character representation in long-form storytelling. User study. We conducted user study with 50 volunteers, who evaluated 40 generated scripts by ranking UniMAGE and three baseline methods. For each comparison set, participants assessed the outputs in terms of overall quality, plot alignment, and character consistency, with an additional criterion of narrative logic specifically when comparing against SEED-Story. After removing invalid responses, the aggregated rankings are summarized in Figure 9. UniMAGE achieves the highest preference across all criteria, including GSB score of 0.72 for narrative logic, indicating clear advantage in long-form story coherence. These results confirm that most participants prefer scripts generated by UniMAGE over those from existing baselines."
        },
        {
            "title": "5 Conclusions",
            "content": "In this work, we introduced UniMAGE, unified director model that integrates the traditionally disjoint processes of script drafting and keyframe generation into single, coherent framework. By leveraging the Mixture-of-Transformers architecture, UniMAGE bridges the gap between textual reasoning and visual imagination, thereby enabling users to produce long-context, multi-shot narratives with logical and visual coherence. Central to our approach are two synergistic training paradigms: Interleaved Concept Learning, which fosters joint understanding of narrative concepts through textimage interleaving, and Disentangled Expert Learning, which decouples script and keyframe generation to enhance both creativity and structural consistency. Further supported by In-Context ID Prompting and Pre-Context Script Splitting, UniMAGE demonstrates strong capability in maintaining character identity, storyline continuity, and visual alignment across extended sequences. Experimental evaluations confirm that UniMAGE achieves state-of-the-art results among open-source systems, establishing foundation for the next generation of AI-driven film creation. Limitations. UniMAGE is designed primarily to enhance narrative coherence and maintain strong visual consistency across long-form scripts. However, several higher-level dimensions of filmmakingsuch as emotional pacing, stylistic cinematography, and fine-grained control over directorial intentare not yet fully addressed. Extending UniMAGE toward richer cinematic understanding and more expressive narrative control remains an important direction for future work. Declaration. This paper is conducted solely for research purposes, and the described technology has not been incorporated into any ByteDance products. All figures and scripts presented in this paper are generated by AI models. All human faces appearing in the images are also AI-generated."
        },
        {
            "title": "References",
            "content": "[1] Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, et al. Theatergen: Character management with llm for consistent multi-turn image generation. arXiv preprint arXiv:2404.18919, 2024. [2] Google DeepMind. Gemini 2.5 pro, 2025. https://deepmind.google/models/gemini/pro/. [3] Google DeepMind. Veo: text-to-video generation system, 2025. https://storage.googleapis.com/ deepmind-media/veo/Veo-3-Tech-Report.pdf. [4] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International conference on machine learning, pages 74807512. PMLR, 2023. [5] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:2252 2274, 2023. [6] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [7] David Dinkevich, Matan Levy, Omri Avrahami, Dvir Samuel, and Dani Lischinski. Story2board: training-free approach for expressive storyboard generation. arXiv preprint arXiv:2508.09983, 2025. [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [9] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. [10] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [11] Kaiyi Huang, Yukun Huang, Xintao Wang, Zinan Lin, Xuefei Ning, Pengfei Wan, Di Zhang, Yu Wang, and Xihui Liu. Filmaster: Bridging cinematic principles and generative ai for automated film generation. arXiv preprint arXiv:2506.18899, 2025. [12] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [13] Kuaishou. Kling ai: Next-generation ai creative studio, 2025. https://klingai.com. [14] Black Forest Labs. Flux, 2024. https://github.com/black-forest-labs/flux. [15] Yunxin Li, Haoyuan Shi, Baotian Hu, Longyue Wang, Jiashun Zhu, Jinyi Xu, Zhen Zhao, and Min Zhang. Animdirector: large multimodal model powered agent for controllable animation video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [16] Haohe Liu, Gael Le Lan, Xinhao Mei, Zhaoheng Ni, Anurag Kumar, Varun Nagaraja, Wenwu Wang, Mark Plumbley, Yangyang Shi, and Vikas Chandra. Syncflow: Toward temporally aligned joint audio-video generation from text. arXiv preprint arXiv:2412.15220, 2024. [17] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [18] Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. Videostudio: Generating consistent-content and multi-scene videos. In European Conference on Computer Vision, pages 468485. Springer, 2024. [19] Jiawei Mao, Xiaoke Huang, Yunfei Xie, Yuanqi Chang, Mude Hui, Bingjie Xu, and Yuyin Zhou. Story-Adapter: Training-free Iterative Framework for Long Story Visualization, 2024. 13 [20] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: survey. arXiv preprint arXiv:2402.06196, 2024. [21] OpenAI. Sora 2 system card, 2025. https://cdn.openai.com/pdf/50d5973c-c4ff-4c2d-986f-c72b5d0ff069/ sora_2_system_card.pdf. [22] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. [23] Haoyuan Shi, Yunxin Li, Xinyu Chen, Longyue Wang, Baotian Hu, and Min Zhang. Animaker: Automated multi-agent animated storytelling with mcts-driven clip generation. arXiv preprint arXiv:2506.10540, 2025. [24] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [25] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [26] Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, and Gang Yu. Universe-1: Unified audio-video generation via stitching of experts. arXiv preprint arXiv:2509.06155, 2025. [27] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [28] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [29] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Chunyi Li, Liang Liao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtai Zhai, and Weisi Lin. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. Equal Contribution by Wu, Haoning and Zhang, Zicheng. Project Lead by Wu, Haoning. Corresponding Authors: Zhai, Guangtai and Lin, Weisi. [30] Weijia Wu, Zeyu Zhu, and Mike Zheng Shou. Automated movie generation via multi-agent cot planning. arXiv preprint arXiv:2503.07314, 2025. [31] Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, and Lu Jiang. Captain cinema: Towards short movie generation. arXiv preprint arXiv:2507.18634, 2025. [32] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [33] Zhenran Xu, Longyue Wang, Jifang Wang, Zhouyi Li, Senbao Shi, Xue Yang, Yiyu Wang, Baotian Hu, Jun Yu, and Min Zhang. Filmagent: multi-agent framework for end-to-end film automation in virtual 3d spaces. arXiv preprint arXiv:2501.12909, 2025. [34] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Ying-Cong Chen. Seed-story: Multimodal long story generation with large language model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18501860, 2025. [35] Lei Zhao, Linfeng Feng, Dongxu Ge, Rujin Chen, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, and Xuelong Li. Uniform: unified multi-task diffusion transformer for audio-video generation. arXiv preprint arXiv:2502.03897, 2025. [36] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 14 [37] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. Advances in Neural Information Processing Systems, 37: 110315110340, 2024. [38] Cailin Zhuang, Ailin Huang, Wei Cheng, Jingwei Wu, Yaoqi Hu, Jiaqi Liao, Hongyuan Wang, Xinyao Liao, Weiwei Cai, Hengyuan Xu, et al. Vistorybench: Comprehensive benchmark suite for story visualization. arXiv preprint arXiv:2505.24862, 2025."
        },
        {
            "title": "Appendix",
            "content": "A Mixture-of-Transformers Architecture Our UniMAGE framework is based on the Mixture-of-Transformers (MoT) architecture introduced in Bagel [6], as shown in Figure 10. MoT combines two transformers through shared multimodal self-attention layers. The transformer parameters are initialized from the Qwen 2.5 LLM [12]. To improve training stability, each attention block includes the QK-Norm [4]. The visual information is represented from two aspects: (1) Visual understanding handled by ViT encoder. It uses SigLIP2-so400m/14 [25] with fixed input resolution of 384 for initialization. The position embeddings are interpolated, and the maximum input size is set to 980 980. NaViT [5] is incorporated to handle images in their native aspect ratios. two-layer MLP connector aligns the ViT token dimensions with the LLM hidden states. (2) Visual generation managed by pre-trained VAE model from FLUX [14], which maps images between pixel and latent spaces. The latent representation has downsampling ratio of 8, and the latent channels is 16. 2 2 patch embedding layer is used to further reduce spatial dimensions and align with the LLM hidden size. Within MoT, text, ViT, and VAE tokens are interleaved according to the multimodal input structure. For tokens from the same sample, generalized causal attention scheme is used. Tokens are divided into sequential splits by modality (text, ViT, or VAE). Each split can attend to all earlier splits, with causal attention applied to text tokens and bidirectional attention for vision tokens. Figure 10 Architecture of Mixture-of-Transformers. MoT employs two Transformer experts to process understanding and generation information, and all tokens are processed through shared multi-modal self-attention in each Transformer block. Two distinct encoders, i.e., ViT and VAE, are adopted separately to capture semantic content and low-level information for image understanding and generation tasks."
        },
        {
            "title": "B Supplementary on Script Dataset",
            "content": "The construction of textimage interleaved script dataset presents significant challenges. Therefore, we detail our method for building such dataset from multi-shot videos. Specifically, we curate film dataset comprising approximately 800k multi-shot video clips, employing video filtering strategy consistent with that adopted in Seedance 1.0 [10]. We then utilize Gemini 2.5 Pro [2] to generate captions for these video clips using the designed prompt. Subsequently, Q-Align [29] is employed to assess the image quality and aesthetics of keyframes, from which we select one keyframe image within the first 10% of frames of each video 16 clip. After filtering based on these quality metrics, approximately 450k multi-shot video clips are retained to construct the final textimage interleaved script dataset. To automatically generate user prompts, we define four types of prompt styles: (1) simple narrative, (2) abstract concept, (3) phrase splicing, and (4) spoken expression. The detailed prompt design is presented as follows. Figure 11 illustrates an example of the textimage interleaved script dataset. For detailed description of the script structure, please refer to Section 3.2 Script Structure in the main paper. Multi-Shot Video Annotation Prompt # Role You are professional video content analyst. # Task Your task is to accurately analyze the provided video file (including video and audio content) and provide structured annotation information according to my requirements. You need to understand both visual and auditory information and correlate them. Please annotate this video clip containing {NUM_VIDEOS} sorted shots. The annotation format needs to be JSON, with the key-value pairs as follows: {{ \"global_caption\": {{ \"first_frame_description\": \"Detailed visual element annotation of the first frame of the video, (cid:44) including description of the camera angle, visual style, main characters, environment, and (cid:44) main objects, as well as the position and posture of each element. At most 100 words.\" \"video_audio_description\": \"Detailed annotation of the entire audio and video, including (cid:44) description of the videos camera movement, visual style, how the objects interact in the (cid:44) environments, including the manner, style, intensity, and specific actions of the interactions, (cid:44) conversation content, etc. At most 150 words.\" \"key_character_description\": \"Detailed appearance description of the objects (N people or items) (cid:44) and environments contained in the entire video. The annotation results need to be indexed by (cid:44) serial numbers for the identified objects and environments. At the same time, please select (cid:44) the most representative frame for each objects appearance to be used for subsequent extraction (cid:44) of the objects visual features. The overall format is, for example, {{Character1: caption:..., (cid:44) short_caption: concise version of the caption, each object description does not exceed 10 (cid:44) words (E.g., middle-aged white male, short gray curly hair, black-framed glasses, wrinkles (cid:44) around the eyes/forehead, dark green button-down shirt), frame_id: {{shot: i, second: x}}, (cid:44) Character2: caption:..., short_caption:..., frame_id: {{shot: j, second: y}}, ...}} }}, \"shot_caption\": \"Covers the first frame appearance description of the current shot segment, and the (cid:44) video and audio description of the current shot segment. The ID indexes of the characters and (cid:44) environments that appear in all shots need to be aligned and consistent with the global (cid:44) annotations, appearing in the text annotation results involving characters in the format of < (cid:44) Character1>, <Character2>...\" {{ [ {{ \"first_frame_description\": \"A detailed description of the first frame of the current shot, (cid:44) including description of the camera angle, visual style, main characters, environment, and (cid:44) main objects, as well as the position and posture of each element. At most 100 words.\" \"video_audio_description\": \"A detailed description of the video and audio of the shot segment, (cid:44) at most 150 words. Please describe in order the videos camera movement, visual style, the (cid:44) appearance and specific events of the characters that appear, the appearance of the environment (cid:44) they are in, the interaction process between characters, environments, and objects, what the (cid:44) characters said and in what tone (keep the original spoken content, do not translate, and mark (cid:44) the spoken content with <- and ->), background sound description (style, content, and intensity, (cid:44) marked with <- and ->), and possible voice-overs, etc.\" \"have_background_audio\": Select from [\"None\", \"Environment\", \"Music\"], which respectively (cid:44) represent: no background sound, background environment sound, and background music sound. Except (cid:44) for the None tag, other tags can be selected in combination. }}, ... 17 ] }}, \"user_prompt\": Use sentence of no more than 40 words to describe global_caption and shot_caption. (cid:44) Please randomly select one of the following 4 styles to imitate and describe. This will be used (cid:44) as the input for the subsequent training of the video text annotation model. The output is (cid:44) global_caption and shot_caption. If shot_caption contains multiple shots, randomly choose (cid:44) whether to label each shot individually, for example, shot 1:..., shot 2:... \"user_prompt_style\": The selected style number. \"user_prompt_has_shot\": Whether to choose to caption each shot individually, true or false. }} The video and audio should be described simultaneously in chronological order. Do not describe the video first and then the audio. Only return the annotation results, do not return other content. The annotation results should all be in {LANGUAGE}, but please keep the spoken content in the original language. Only annotate the original content of the video, without inference or imagination. User prompt styles to be selected: 1. Simple narrative, such as cute anthropomorphic kitten, wearing an apron, holding frying pan in one hand, frying braised pork in pot, the frying sound is sizzling, the pot is smoking, and the smoke is lingering above the pot. 2. Abstract concepts, such as the ancient Qin Shihuang unified the six kingdoms. 3. Phrase splicing, such as photo, super large and expensive panda, light champagne color, close-up of facial features, wearing suit, dancing in the square, medium shot, parallel perspective, slowly close up, bright colors, cheerful, extremely delicate pictures, ultra-high resolution, and movie effects. 4. Spoken expression, such as generating video of panda mother riding shared bicycle and taking panda baby to school, the panda needs to be realistic, not cartoon, and the background is vegetable market."
        },
        {
            "title": "C Supplementary on Experiments",
            "content": "Due to the page limitations of the main text, it is not possible to display the complete generated multimodal scripts. The full versions of all results presented in the main paper are provided below. In addition, we include additional generated scripts to further demonstrate the generalizability of UniMAGE. Supplementary on Audio-Video Generation Returning to our original motivation, beyond story visualization, we aim to develop unified director model capable of serving audiovideo generation systems. Accordingly, we provide two video examples in the supplementary materials to demonstrate the capability of UniMAGE in facilitating automated audio and video generation. The video files are named UniMAGE_Demo_1.mp4 and UniMAGE_Demo_2.mp4. We sincerely encourage readers to view the paper in conjunction with these videos. These two video demonstrations are generated using the Veo3 I2V model [3], with the scripts, character lines, and sound effect descriptions provided by UniMAGE. It is worth noting that, due to the current limitations of audio-video generation models, the timbre and facial features of characters may vary during the generation of long multi-camera videos. These issues warrant further investigation and improvement in future work. 18 Figure 11 Example of the textimage interleaved script dataset. The dataset is constructed by captioning multi-shot video clips and extracting corresponding keyframes. 20 21 22 23 24 26 27 28 29 30 32 33 34 35 36 38 39 40 41 42"
        }
    ],
    "affiliations": [
        "ByteDance Intelligent Creation",
        "Nanyang Technological University"
    ]
}