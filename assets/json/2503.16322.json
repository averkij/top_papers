{
    "paper_title": "Ultra-Resolution Adaptation with Ease",
    "authors": [
        "Ruonan Yu",
        "Songhua Liu",
        "Zhenxiong Tan",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed \\emph{URAE}. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, \\textit{i.e.}, setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available \\href{https://github.com/Huage001/URAE}{here}."
        },
        {
            "title": "Start",
            "content": "Ultra-Resolution Adaptation with Ease Ruonan Yu * 1 Songhua Liu * 1 Zhenxiong Tan 1 Xinchao Wang 1 5 2 0 2 0 2 ] . [ 1 2 2 3 6 1 . 3 0 5 2 : r Figure 1: High-resolution results by our method."
        },
        {
            "title": "Abstract",
            "content": "Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose set of key guidelines for ultra-resolution adaptation termed URAE. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining effi- *Equal contribution 1National University of Singapore. Correspondence to: Xinchao Wang <xinchao@nus.edu.sg>. Technical Report. Copyright 2025 by the author(s). 1 ciency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, i.e., setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available here. 1. Introduction Recent years have witnessed remarkable progress in text-toimage generation with diffusion models (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021; Rombach et al., 2022; Ho et al., 2020). From UNet-based architectures (Ronneberger et al., 2015; Rombach et al., 2022) to latest state-of-theart Diffusion Transformers (DiTs) (Peebles & Xie, 2023; Bao et al., 2023; Chen et al., 2023; Esser et al., 2024; Li et al., 2024; Gao et al., 2024; Chen et al., 2024), these modUltra-Resolution Adaptation with Ease els leverage powerful backbones and multistep denoising schemes to generate high-quality and diverse images from textual prompts effectively, solidifying their leading position in this field (Croitoru et al., 2023; Yang et al., 2023a). Nevertheless, extending current diffusion models to ultraresolution generation, such as 4K, remains significant challenge. The process typically demands massive amounts of high-quality data and substantial computational resources, making training at such resolutions daunting and accessible only to industry-scale efforts. Although recent attempts have been made to train 4K-resolution text-to-image models (Chen et al., 2024; Xie et al., 2024), they rely on internal datasets containing millions of high-resolution images to fine-tune base low-resolution models. In practice, collecting such large-scale datasets for training is highly cumbersome if not infeasible at all. Meanwhile, tuning the entire diffusion backbone introduces an intensive GPU memory footprint, especially for state-of-the-art models like FLUX (Black Forest Labs, 2023) and Stable Diffusion 3.5 (Esser et al., 2024). Focusing on these drawbacks, we are curious about one practical question: Can this ultra-resolution adaptation process be made easier? In this paper, we answer the question positively by proposing URAE, set of key guidelines, under which ultra-resolution adaptation is achievable with merely thousands of training samples and iterations. Specifically, we initiate our exploration from two key aspects: data and parameter efficiency. On the one hand, we provide theoretical and empirical evidence that synthetic data produced by some teacher models can largely enhance training convergence. However, despite recent advancements in text-to-image generation, state-of-the-art models still face significant challenges in acquiring high-quality synthetic training data for ultra-resolution adaptation, such as 4K. We thus, on the other hand, investigate such scenarios where synthetic data are unavailable and identify that tuning minor components of the pre-trained weight matrices is more effective than commonly used parameter-efficient adaptation strategies like LoRA (Hu et al., 2022). Furthermore, we delve into the principles of fine-tuning guidance-distilled models like FLUX and discover that disabling classifier-free guidanceby setting the guidance scale to 1is essential, regardless of the availability of synthetic data. Backed up by the above guidelines, we conduct extensive experiments to demonstrate that URAE achieves performance comparable to state-of-the-art closedsource models like FLUX1.1 [Pro] Ultra with merely 3K training samples and 2K adaptation iterations. Meanwhile, it surpasses previous models in 4K generation performance and remains highly compatible with existing training-free high-resolution generation pipelines (Du et al., 2024b; Meng et al., 2021), enabling further performance improvements. In summary, the contributions of this paper are: We are the first to delve into the problem of ultraresolution adaption to the best of our knowledge; We propose URAE, set of key guidelines focusing on data efficiency, parameter efficiency, and classifierfree guidance, to facilitate the adaptation of existing text-to-image models to higher resolutions; We validate that URAE achieves comparable performance in 2K generation, superior capabilities in 4K generation, and strong compatibility with existing training-free high-resolution generation pipelines. 2. Methodology In this section, we delve into the motivations and technical details of URAE, our proposed strategy for ultra-resolution adaptation. We begin with some preliminary concepts, followed by three key components including training with synthetic data, parameter-efficient fine-tuning strategies, and classifier-free guidance. 2.1. Preliminary State-of-the-art text-to-image diffusion models commonly adopt the flow matching training scheme (Esser et al., 2024; Lipman et al., 2022). Specifically, in each iteration, batch of images and their corresponding textual descriptions is sampled. These images are then encoded into latent map z0 using pre-trained VAE encoder, and noise map ϵ is drawn from Gaussian distribution. Let zt denote the noisy version of z0 after applying ϵ at the t-th diffusion timestep. The flow matching loss is then formulated as: Lf m(z0, y, t, ϵ) = (ϵ z0) ϵθ(zt, t, y)2 2, (1) where ϵθ() is the denoising backbone with parameters θ. The inference process begins with text prompt and random Gaussian noise ϵ, also denoted zT , which is iteratively denoised using the trained backbone. After steps, the resulting z0 represents clean sample in the latent space and is then decoded to generated image using pre-trained VAE decoder. Such training and inference paradigms are also employed in our URAE framework. 2.2. Synthetic Data or Real Data? Previous works train 4K-generation models using millions of high-quality training images (Chen et al., 2024; Xie et al., 2024), leading to significant challenges in collecting, transmitting, storing, and processing such large volumes of data. To alleviate these inconveniences, we target data-efficient approach for ultra-resolution adaptation. Ultra-Resolution Adaptation with Ease Building on recent advances in the distillation of diffusion models (Yang et al., 2023b; Kim et al., 2023; Liu et al., 2024b;a), we recognize that incorporating teacher model for reference and loss term for knowledge distillation (Hinton, 2015) can enhance training: Ldistill(z0, y, t, ϵ) = ϵθ(zt, t, y) ϵθref (zt, t, y)2 2, (2) where θref represents the parameters of the teacher model. However, this approach relies on access to the diffusion backbone of the teacher model to compute the step-wise distillation loss, which is impractical for closed-weight models such as FLUX1.1 [Pro] Ultra. We therefore experiment with an alternative approach that optimizes the vanilla flow matching loss defined in Eq. 1 using data synthesized by the teacher model. We expect this to yield similar training benefits, as validated by the following theoretical analysis. Before presenting our main results, we first set up some necessary assumptions. From the model perspective: Assumption 2.1. Let denote the input data pair (ϵ, y). The process from to the output can be characterized by neural network (u; ) with infinite width, where denotes the networks parameters. Figure 2: toy linear regression case. There are real data with noisy labels and synthetic data generated by reference model Wref . The proportion of synthetic data is p. using the square error as the loss function: = 1 2N (u; ) x2 2. The learning rate is η. Our main results are summarized in Theorem 2.4: Theorem 2.4. Under the setting defined in Assumptions 2.1, 2.2, and 2.3, the error between WT , the parameters after training iterations, and the optimal is bounded by: E[WT 2 2] E[(I ηM )T 02 2]+ It has been demonstrated that neural networks with single hidden layer and sufficient width can approximate any complex functions (Cybenko, 1989; Hornik, 1991), and that infinite-width neural networks trained with gradient descent are equivalent to training linear models in the space of Neural Tangent Kernel (NTK) (Jacot et al., 2018). From the data perspective: η2(p(1 p)E[δ2] + (1 p)σ2) (cid:88) i=1 (1 (1 ηλi)T )2 λi + p2Wref 2 2. (3) where 0 = W0(pWref +(1p)W ), is defined as (U ; W0)W (U ; W0), δ = (u; Wref ) (u; ), and λi is the i-th eigenvalue of . Assumption 2.2. The dataset contains both real and synthetic data, with the synthetic portion denoted as [0, 1]. The total number of data is . Given an input u, the corresponding target in the real data distribution is characterized by xreal = (u; ) + ξ, where ξ (0, σ2) is scaler from noise distribution and denotes the optimal parameters, serving as an unknown oracle. For synthetic data, the target is given by xsyn = (u; Wref ), where Wref denotes the parameters of pre-trained teacher model for reference. Without loss of generality, we restrict our discussion to the one-dimensional target case. And from the training perspective: Assumption 2.3. The models parameters are initialized as W0. The training is conducted through SGD The proof can be found in Appendix B. Intuitively, Theorem 2.4 indicates that, when training data are drawn from mixture of real and synthetic data points, the distance to the optimal parameters at convergence reflects tradeoffgoverned by the synthetic portion pbetween two factors: the error introduced by label noise in the real data distribution and the discrepancy between the reference model used for generating synthetic data and the optimal model, shown in the 2nd and 3rd terms of Eq. 2.4 separately. Fig. 2 provides an illustrative example to visualize this effect. Closely examining Eq.2.4, we can discover that, by diminishing label noise, synthetic data would be helpful if the reference model providing these data is accurate. Although some works reveal that synthetic data can result in model collapse (Dohmatob et al., 2024b;a) by amplifying the gap between real and synthetic distributions, we demonstrate that they are useful particularly for ultra-resolution adaption. On one hand, large-scale real datasets such as LAION3 Ultra-Resolution Adaptation with Ease 5B (Schuhmann et al., 2022) tend to be noisy, containing numerous low-quality images and mismatched text-image pairs. On the other hand, at 2K resolution, models like FLUX-1.1 [Pro] Ultraalthough closed-weightare available to produce high-quality synthetic data. Building on this analysis, we train our 2K-generation model using only synthetic data in this work, demonstrating superior performance across various scenarios. 2.3. Tune Major or Minor Components? Parameter-efficient fine-tuning strategies enable adapting pre-trained model from its original domain to target domain by integrating lightweight adapters. For instance, in personalized text-to-image generation such as DreamBooth (Ruiz et al., 2023), attaching low-rank, e.g., rank = 4, adapters, i.e., LoRA, to the original models weights can achieve satisfactory performance (Hu et al., 2022). Specifically, this is achieved by: = XW + XAB, (4) where X, , and are input, output, and original weight matrices respectively, Rcinr and Rrcout are low-rank matrices for adaptation, and cin and cout are input and output dimensions, output dimension separately. In practice, is initialized using normal distribution, whereas is set to all zeros, which makes the adapter branch output zero initially and allows tuning to begin from the original parameters. After tuning, and can be merged into the original weight matrix via = + AB, ensuring that the total number of model parameters remains unchanged. These low-rank adapters employ small number of parameters that focus on the major components with the largest singular values, enabling efficient adaptation to the target domain (Meng et al., 2024). However, different from DreamBooth modifying the styles and appearances of output images, ultra-resolution adaptation focuses on learning the arrangements of details and local textures, which may not correspond to the major components in weight matrices. Under this hypothesis, we introduce method to tune the components associated with the smallest singular values instead. Specifically, given weight matrix Rcincout and = min(cin, cout), we first conduct Singular Value Decomposition (SVD) and derive = ΣV , where Rcinc and Rccout are orthogonal matrices, and Σ Rcc is diagonal matrix with the singular values arranged from large to small. Then, components with the smallest singular values and the rest ones are extracted via: small = [:, :]Σ[r :, :]V [r :, :], res = [:, : r]Σ[: r, : r]V [: r, :], (5) where the indexing syntax in Numpy (Harris et al., 2020) 4 Figure 3: For CFG-distilled models, classifier-free guidance should be disabled in the training time. zt and are omitted from the inputs of ϵθ and ϵθ here for simplicity. and PyTorch (Paszke et al., 2019) are used to represent the operations for extracting multiple rows/columns. Analyzing from Eq. 5, small is low-rank matrix. Therefore, for parameter efficiency, we formulate the training time behavior similar to Eq. 4: = XW res + XAB, = [:, :](cid:112)Σ[r :, :], = (cid:112)Σ[r :, :]V [r :, :]. (6) and are initialized using Eq. 6 and updated during fine-tuning. In terms of formulation, the approach is similar to PISSA (Meng et al., 2024); however, it fundamentally differs by tuning the components with the smallest singular values instead of the largest. Although (Wang et al., 2024a) introduce similar approach in the field of large language model finetuning, they fail to analyze its applicability in various scenarios. Empirically, we observe that this approach is particularly effective when no synthetic data are available to serve as reliable reference, e.g., in 4K generation. We speculate that the effectiveness stems from preserving the major components in the original weight matrices, thereby safeguarding the models capacity to handle semantics, layouts, and appearances from label noise in the real data distribution. 2.4. Enable or Disable Classifier-Free Guidance? Classifier-free guidance (CFG) (Ho & Salimans, 2022) aims to enhance the quality of the generated samples by introducing an additional null-condition branch. Specifically, at each denoising step in the inference time, the current latent map is processed by both the main branch and the nullcondition branch, and the final output is then guided, with certain strength, in the direction opposing the null-condition branchs prediction: ϵθ(zt, t, ) + (ϵθ(zt, t, y) ϵθ(zt, t, )), (7) where denotes the null condition, e.g., an empty prompt, and is hyper-parameter controlling the strength. Although effective, the additional null-condition branch doubles the inference cost. To address this issue, models Ultra-Resolution Adaptation with Ease Table 1: Quantitative results of the baseline methods and our proposed guidelines. The prompts are from HPD and DPG datasets. All images are at resolution of 2048 2048. Here, FLUX-1.dev is FLUX-1.dev with scaled RoPE, proportional attention, and removing dynamic shifting strategies. Method/Setting FID () LPIPS () MAN-IQA () QualiCLIP () HPSv2.1 () PickScore () DPG Bench () HPD Prompt DPG Prompt FLUX1.1 [Pro] Ultra - - Real-ESRGAN SinSR SDEdit w/ URAE I-Max w/ URAE PixArt-Sigma-XL Sana-1.6B FLUX-1.dev FLUX-1.dev w/ URAE 36.25 35.09 35.59 34.07 33.66 32.24 36.58 33.17 43.78 34.86 29. 0.6593 0.6566 0.6456 0.6419 0.6394 0.6357 0.6801 0.6792 0.6530 0.6036 0.5965 0.4129 0.4653 0. 0.3736 0.3872 0.3670 0.3833 0.2949 0.3695 0.3821 0.4110 0.4730 0.6424 0.6392 0.5556 0.4480 0.5800 0.4797 0. 0.4438 0.6718 0.3800 0.5468 0.7191 29.61 30.70 30.95 30.92 32.26 31.12 32.37 30.66 30. 26.22 28.73 31.15 22.99 22.91 22.96 22.86 23.02 23.02 23.18 22.92 22.83 21.54 22.68 23. 84.76 83.50 83.79 83.56 84.61 83.92 87.88 80.60 85.14 80.64 80.15 83.83 To address this issue, we note that incorporating the nullcondition branch during adaptation is unnecessary; simply disabling CFG at training time by setting = 1 works well and yields consistent target across the two stages. Fig. 3 illustrates the mismatch triggered by > 1 and how = 1 addresses the problem. During inference, CFG is still necessary by using > 1. Although the model does not encounter > 1 during adaptation, we find that it generalizes sufficiently well in practice. 3. Experiments 3.1. Settings and Implementation Details In this paper, we adopt the open-source text-to-image FLUX.1-dev model (Black Forest Labs, 2023) as the base model to demonstrate the effectiveness of our proposed URAE guidelines, thanks to its superior performance. For our 2K-generation model, we collect 3K synthetic samples with various aspect ratios generated by the FLUX1.1 [Pro] Ultra model as the training dataset, and fine-tune the FLUX.1-dev on it for merely 2K iterations with batch size of 8, which takes only 1 day on 2 H100 GPUs. For our 4K model, we utilize 30K images with at least 4K resolution from the LAION-5B dataset (Schuhmann et al., 2022) and fine-tune the base model FLUX.1-dev for 2K iterations on 8 H100 GPUs, which takes 1 days. In terms of training convergence, our method requires significantly fewer iterations compared with state-of-the-art methods, such as 10K for SANA (Xie et al., 2024). For baseline, we apply URAE on the FLUX.1-dev model and compare the performance with PixArt-Sigma-XL (Chen et al., 2024), Sana-1.6B (Xie et al., 2024), and FLUX series models. In order to further demonstrate the effectiveness of Figure 4: GPT-4o preferred evaluation against current SOTA T2I models. We request GPT-4o to select better image regarding overall quality, prompt alignment, and visual aesthetics. Our proposed method are preferred against others. like FLUX.1-dev use guidance distillation to train distilled model that takes the CFG scale embedding as an additional input, encouraging its output aligns with the result in Eq. 7. Since is typically larger than 1 in inference, in training, many works also set to the same value used at inference time during fine-tuning (XLabs-AI, 2024; TencentARC, 2024). However, according to the experiments, it results in inferior performance, especially in the problem of ultra-resolution adaptation. Specifically, during the distillation stage, the distilled model is trained with > 1 as input Eq. 7, which involves the null condition. In contrast, during the adaptation stage, the target is ϵ z0 defined in Eq. 1, which is irrelevant to the null condition. If remains larger than 1, mismatch arises between the training targets in these two stages, making the training process more challenging. 5 Ultra-Resolution Adaptation with Ease Figure 5: Visualizations of our proposed method apply to training-free high-resolution generation pipelines. The prompt is giraffe stands beneath tree beside marina. Figure 6: Qualitative comparisons with baseline methods. All the images are of 2048 2048 size. URAE, we also apply URAE to the existing training-free high-resolution generation pipelines, i.e., SDEdit (Meng et al., 2021) and I-Max (Du et al., 2024b). These pipelines require the base text-to-image model, e.g., FLUX.1-dev, to generate low-resolution, e.g., 1024 1024, images as the guidance, and upscale these images to higher resolutions through image-to-image pipelines. For comparison, we also include the super-resolution methods Real-ESRGAN (Wang et al., 2021) and SinSR (Wang et al., 2024b), based on GAN and diffusion, respectively. For baseline comparison, we conduct quantitative experiments on 2048 2048 samples generated with prompts from the HPD (Wu et al., 2023) and DPG (Hu et al., 2024) datasets. Additionally, we use the HPSv2.1 (Wu et al., 2023) and PickScore (Kirstain et al., 2023) as human preference metrics to further evaluate the quality and aesthetic appeal of the generated images. Following prior works like PixArt-Sigma and I-Max, we also utilize the GPT-4o to assess the generated images from three key perspectives: prompt alignment, visual aesthetics, and overall quality, at both 2K and 4K resolutions. These AI preference scores are derived from 300 randomly selected prompts in the COCO30K (Lin et al., 2014; Chen et al., 2024) dataset. 3.2. 2K Resolution Here, we evaluate the performance of the proposed methods on 2K images generated with prompts from HPD and DPG datasets. The results are shown in Table 1. The quantitative results indicate that our proposed method is capable of significantly enhancing the ability of models to generate high-resolution images and demonstrates its versatility and adaptability across different methods. Our method surpasses the state-of-the-art model FLUX1.1 [Pro] Ultra in terms of image quality, demonstrating its superiority in generating visually refined images. Moreover, the remarkable improvements in image quality further underscore the strength of our method in achieving state-of-the-art visual results for all quality metrics, making it highly effective solution for high-resolution image generation tasks. In addition, our method also achieves substantial improvement in the performance of the base model in terms of prompt alignment, 6 Ultra-Resolution Adaptation with Ease Figure 7: Visualization results of ablation studies. The prompt is Imogen Poots portrayed as D&D Paladin in fantasy concept art by Tomer Hanuka. Figure 8: Visualization results for ultra-resolution image generation task. All the images are of 4096 4096 size. improving the original FLUX-1.dev by 3.19 in DPG Bench score, and 3.96 for I-Max pipeline. For human preference study, we adopt HPSv2.1 and PickScore to benchmark the human preference score. The samples are generated with prompts from the HPD dataset and the resolution is 2048 2048. The results are shown in Table 1. The results show that our method improves the human preference score of the base model, indicating that our proposed guidelines are capable of generating images that better align with human preferences. We also conduct AI preference studies with GPT-4o for pair comparison regarding overall quality, prompt alignment, and visual aesthetics aspects. The results are shown in Fig. 4. For the prompts for GPT-4o to assess the quality, prompt alignment, and visual aesthetics, please refer to Appendix C.1. The results demonstrate that our proposed method excels and is preferred in all three aspects. Please refer to Appendix D.1 for more quantitative results. 3.3. 4K Resolution Here, we evaluate the performance of our proposed method in 4K-ultra-resolution image generation. The results are shown in Table 3. From the experimental results, fine-tuning minor components achieves outstanding performance when no synthetic data are available to serve as reliable reference for the ultra-resolution image generation task, while the commonly adopted LoRA may fail on the overall semantics. Moreover, our proposed method also demonstrates exceptional performance compared with other methods, further validating its competitiveness to existing approaches. Given that there is no well-defined benchmark specifically for 4K-ultra-resolution generation, we conduct user study using randomly generated prompts listed in Appendix C.4. For each prompt, we present results generated by the can7 Ultra-Resolution Adaptation with Ease Table 2: Ablation studies on three key guidelines, using real (Real) or synthetic (Syn) data, whether to adopt CFG in training, and tuning of major or minor components. Evaluations are on the 2048 2048 images generated from HPD prompts. Method/Setting FID () LPIPS () MAN-IQA () QualiCLIP () HPSv2.1 () PickScore ()"
        },
        {
            "title": "Minor",
            "content": "Syn w/o CFG Syn w/ CFG Real w/o CFG Real w/ CFG Syn w/o CFG Syn w/ CFG Real w/o CFG Real w/ CFG 29.44 76.07 31.39 133.68 27.90 65.34 32.09 133.32 0.5965 0.6388 0.6076 0.5978 0.5779 0.5858 0.6000 0. 0.4730 0.3992 0.4262 0.3254 0.4558 0.3852 0.4485 0.3387 0.7191 0.5890 0.5953 0.3645 0.6616 0.5312 0.6098 0.3672 31.15 24.80 29.33 16.55 30.40 23.77 28.71 16. 23.15 21.87 22.86 19.92 22.87 21.64 22.61 19.68 Table 3: Evaluation on ultra resolution image generation task. The images are of 4096 4096, and generated with prompts randomly selected from COCO30K. For user study, the prompts are randomly generated as listed in Appendix C.4. Method/Setting PixArt-Sigma-XL Sana-1.6B FLUX-1.dev w/ URAE (Major-4K) w/ URAE (Minor-4K) HPD Prompt User Study MAN-IQA () Rank QualiCLIP () Rank Overall Quality Rank Prompt Alignment Rank Visual Aesthetic Rank 0.2935 0.3288 0.3673 0.3280 0.3999 5 3 2 4 0.2308 0.4979 0.2564 0.2700 0.5118 5 2 4 3 1 31.18% 10.29% 3.24% 2.06% 53.24% 2 3 4 5 1 30.88% 10.00% 3.24% 1.76% 54.12% 2 3 4 5 30.00% 12.06% 3.24% 1.76% 52.94% 2 3 4 5 1 didates to users and let them select the best one from three aspects including overall quality, prompt alignment, and visual aesthetic aspects. We collect 1, 020 votes altogether. The results in Table 3 are coherent with the above analysis. 3.4. Ablation Study Here, to evaluate the effectiveness of our proposed method, we carry out ablation studies on three key guidelines that we proposed in URAE, the source of training data, tuning major or minor components, and whether to adopt classifier-free guidance (CFG) in training. The experiments are conducted on the 2048 2048 images generated from HPD prompts. The results are shown in Table 2, and the visualization examples are shown in Fig. 7. For the source of training data, the results demonstrate that high-quality synthetic data can provide better performance than noisy real data. When the model is fine-tuned with real data, tuning minor components can bring more vivid details as shown in Fig. 7. As for CFG, although it is necessary in the inference stage, it can lead to significant performance degradation in the training stage. According to these results, we by default use synthetic data and tune major components, i.e. adopt LoRA, at 2K resolution, while use real data and tune minor components at 4K resolution. In both cases, CFG is disabled in training and enabled during inference. 4. Conclusions and Limitations In this paper, we focus on the challenge of adapting textto-image diffusion models from their native scales to ultraresolution settings with limited training data and computational resources. Our proposed framework, URAE, tackles the problem from two complementary perspectives, i.e., data and parameter efficiency, and provides set of useful guidelines. First, by incorporating synthetic data generated by some teacher models, we demonstrate the potential to promote training convergence and achieve high-quality outcomes even under data-scarce conditions. Second, for the cases where synthetic data are unavailable, we introduce parameter-efficient fine-tuning approach to tune the minor components of weight matrices, which outperforms standard low-rank adapters. Additionally, for models employing guidance distillation, e.g., FLUX, setting the guidance scale to 1 during adaptation proves crucial for achieving favorable results. Extensive experiments reveal that URAE matches the 2K-generation performance of leading closed-source solutions such as FLUX1.1 [Pro] Ultra using only 3K samples and 2K iterations, and further sets new milestones for 4K-resolution generation. Nevertheless, the models presented in this work fall short of matching the inference-time efficiency exhibited by recent high-resolution text-to-image generation methods (Xie et al., 2024; Liu et al., 2024b;a; Chen et al., 2024), as we have not introduced architectural optimizations specifically targeting this aspect. In the future, we envision research aimed at streamlining the ultra-resolution generation process to balance quality and efficiency requirements in practice. It is also meaningful to integrate our methods into multi-modal large language models to unlock even broader and more versatile capabilities. 8 Ultra-Resolution Adaptation with Ease"
        },
        {
            "title": "Impact Statement",
            "content": "Our work on ultra-resolution text-to-image generation has broad implications for both research and real-world applications. By pushing resolution boundaries, we enable richer and more detailed visual content, benefiting domains such as digital art, virtual reality, advertising, and scientific visualization. At the same time, these advancements highlight important ethical considerations. High-fidelity images could be misused for creating deceptive content, and the computational demands of large-scale generation can have environmental impacts. We therefore emphasize responsible development practices, including efficient training strategies and transparent model documentation, to help ensure that the benefits of ultra-resolution text-to-image models are realized while mitigating potential risks."
        },
        {
            "title": "References",
            "content": "Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu, J. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2266922679, 2023. Bar-Tal, O., Yariv, L., Lipman, Y., and Dekel, T. Multidiffusion: Fusing diffusion paths for controlled image generation. In International Conference on Machine Learning, pp. 17371752. PMLR, 2023. Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2023. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Ding, M., Zheng, W., Hong, W., and Tang, J. Cogview2: Faster and better text-to-image generation via hierarchical transformers. Advances in Neural Information Processing Systems, 35:1689016902, 2022. Dohmatob, E., Feng, Y., and Kempe, J. Model collapse demystified: The case of regression. arXiv preprint arXiv:2402.07712, 2024a. Dohmatob, E., Feng, Y., Subramonian, A., and Kempe, J. Strong model collapse. arXiv preprint arXiv:2410.04840, 2024b. Du, R., Chang, D., Hospedales, T., Song, Y.-Z., and Ma, Z. Demofusion: Democratising high-resolution image generation with no $$$. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 61596168, 2024a. Du, R., Liu, D., Zhuo, L., Qi, Q., Li, H., Ma, Z., and Gao, P. I-max: Maximize the resolution potential of pre-trained rectified flow transformers with projected flow. arXiv preprint arXiv:2410.07536, 2024b. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Gao, P., Zhuo, L., Lin, Z., Liu, C., Chen, J., Du, R., Xie, E., Luo, X., Qiu, L., Zhang, Y., et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Chen, J., Ge, C., Xie, E., Wu, Y., Yao, L., Ren, X., Wang, Z., Luo, P., Lu, H., and Li, Z. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation, 2024. Croitoru, F.-A., Hondru, V., Ionescu, R. T., and Shah, M. Diffusion models in vision: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. Cybenko, G. Approximation by superpositions of sigmoidal function. Mathematics of control, signals and systems, 2(4):303314, 1989. Harris, C. R., Millman, K. J., Van Der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., et al. Array programming with numpy. Nature, 585(7825):357362, 2020. He, Y., Yang, S., Chen, H., Cun, X., Xia, M., Zhang, Y., Wang, X., He, R., Chen, Q., and Shan, Y. Scalecrafter: Tuning-free higher-resolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representations, 2024. Hinton, G. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. 9 Ultra-Resolution Adaptation with Ease Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hornik, K. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):251257, 1991. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https:// openreview.net/forum?id=nZeVKeeFYf9. Hu, X., Wang, R., Fang, Y., Fu, B., Cheng, P., and Yu, G. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Huang, L., Fang, R., Zhang, A., Song, G., Liu, S., Liu, Y., and Li, H. Fouriscale: frequency perspective on training-free high-resolution image synthesis. arXiv preprint arXiv:2403.12963, 2024. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, S., Tan, Z., and Wang, X. Clear: Conv-like linearization revs pre-trained diffusion transformers up. arXiv preprint arXiv:2412.16112, 2024a. Liu, S., Yu, W., Tan, Z., and Wang, X. Linfusion: 1 gpu, 1 minute, 16k image. arXiv preprint arXiv:2409.02097, 2024b. Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Meng, F., Wang, Z., and Zhang, M. Pissa: Principal singular values and singular vectors adaptation of large language models. arXiv preprint arXiv:2404.02948, 2024. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing arXiv preprint with text-guided diffusion models. arXiv:2112.10741, 2021. Hyeon-Woo, N., Ye-Bin, M., and Oh, T.-H. Fedpara: Lowrank hadamard product for communication-efficient federated learning. arXiv preprint arXiv:2108.06098, 2021. Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion In International Conference on probabilistic models. Machine Learning, pp. 81628171. PMLR, 2021. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. Kim, B.-K., Song, H.-K., Castells, T., and Choi, S. Bk-sdm: Architecturally compressed stable diffusion for efficient text-to-image generation. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., and Levy, O. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. Li, Z., Zhang, J., Lin, Q., Xiong, J., Long, Y., Deng, X., Zhang, Y., Liu, X., Huang, M., Xiao, Z., et al. Hunyuandit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740 755. Springer, 2014. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. Peebles, W. and Xie, S. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Razzhigaev, A., Shakhmatov, A., Maltseva, A., Arkhipkin, V., Pavlov, I., Ryabov, I., Kuts, A., Panchenko, A., Kuznetsov, A., and Dimitrov, D. Kandinsky: an improved text-to-image synthesis with image prior and latent diffusion. arXiv preprint arXiv:2310.03502, 2023. 10 Ultra-Resolution Adaptation with Ease Ren, J., Li, W., Chen, H., Pei, R., Shao, B., Guo, Y., Peng, L., Song, F., and Zhu, L. Ultrapixel: Advancing ultrahigh-resolution image synthesis to new peaks. arXiv preprint arXiv:2407.02158, 2024. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp. 234241. Springer, 2015. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2250022510, 2023. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35: 3647936494, 2022. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35: 2527825294, 2022. TencentARC. Fluxkits. https://github.com/ TencentARC/FluxKits, 2024. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang, H., Li, Y., Wang, S., Chen, G., and Chen, Y. Milora: Harnessing minor singular components for parameterefficient llm finetuning. arXiv preprint arXiv:2406.09044, 2024a. Wang, X., Xie, L., Dong, C., and Shan, Y. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 19051914, 2021. 11 Wang, Y., Yang, W., Chen, X., Wang, Y., Guo, L., Chau, L.-P., Liu, Z., Qiao, Y., Kot, A. C., and Wen, B. Sinsr: diffusion-based image super-resolution in single step. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2579625805, 2024b. Wu, H., Shen, S., Hu, Q., Zhang, X., Zhang, Y., and Wang, Y. Megafusion: Extend diffusion models towards higherresolution image generation without further tuning. arXiv preprint arXiv:2408.11001, 2024. Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., and Li, H. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. Xie, E., Chen, J., Chen, J., Cai, H., Lin, Y., Zhang, Z., Li, M., Lu, Y., and Han, S. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. XLabs-AI. x-flux. https://github.com/ XLabs-AI/x-flux, 2024. Xu, X., Wang, Z., Zhang, G., Wang, K., and Shi, H. Versatile diffusion: Text, images and variations all in one diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 77547765, 2023. Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W., Cui, B., and Yang, M.-H. Diffusion models: comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):139, 2023a. Yang, X., Zhou, D., Feng, J., and Wang, X. Diffusion probabilistic model made slim. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pp. 2255222562, 2023b. Yeh, S.-Y., Hsieh, Y.-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y. Navigating text-to-image customization: From lycoris fine-tuning to model evaluation. In The Twelfth International Conference on Learning Representations, 2023. Zhang, S., Chen, Z., Zhao, Z., Chen, Z., Tang, Y., Chen, Y., Cao, W., and Liang, J. Hidiffusion: Unlocking high-resolution creativity and efficiency in lowarXiv preprint resolution trained diffusion models. arXiv:2311.17528, 2023. Zheng, W., Teng, J., Yang, Z., Wang, W., Chen, J., Gu, X., Dong, Y., Ding, M., and Tang, J. Cogview3: Finer and faster text-to-image generation via relay diffusion. arXiv preprint arXiv:2403.05121, 2024. A. Related Works Ultra-Resolution Adaptation with Ease In this section, we briefly introduce related works and their relationships between this paper from three aspects: text-to-image diffusion models, high-resolution generation, and parameter-efficient fine-tuning. A.1. Text-to-Image Diffusion Models The diffusion model (Ho et al., 2020) has emerged as powerful class of generative models. Unlike traditional approaches such as GANs (Goodfellow et al., 2014), diffusion models iteratively refine noisy maps with UNet backbone (Ronneberger et al., 2015) to produce high-quality and detailed images (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021), which fuels significant advancements in large-scale text-to-image diffusion models (Rombach et al., 2022; Podell et al., 2023; Balaji et al., 2022; Ding et al., 2022; Nichol et al., 2021; Ramesh et al., 2022; Razzhigaev et al., 2023; Xu et al., 2023; Saharia et al., 2022). Leveraging billions of image-text pairs, they demonstrate remarkable semantic understanding and the ability to generate diverse and photorealistic images aligning with text prompts. Most recently, Transformer (Vaswani et al., 2017) has been introduced as an alternative backbone to UNet (Peebles & Xie, 2022) in diffusion models, known as Diffusion Transformer (DiT). Then, text-to-image models based on it have progressively demonstrated dominant performance (Chen et al., 2024; Esser et al., 2024; Gao et al., 2024; Li et al., 2024; Zheng et al., 2024). We thus focus on DiT-based models and conduct experiments mainly on FLUX, which yields state-of-the-art text-to-image performance, in sake for superior ultra-resolution adaption results. A.2. High-Resolution Generation Training models at high resolutions demands substantial computational resources. To address this, series of works propose training-free solutions, developing inference stage strategies that allow diffusion models trained at their native resolutions to operate effectively at higher scales (Bar-Tal et al., 2023; Meng et al., 2021; Du et al., 2024b; He et al., 2024; Du et al., 2024a; Huang et al., 2024; Wu et al., 2024; Zhang et al., 2023). While effective, without looking at any high-resolution images during training, in fact, they still fall short in accurately handling detailed structures and textures inherent in ultra-resolution images. By contrast, ultra-resolution adaptation focused in this paper dedicates on addressing this drawback through training, which is technically orthogonal to training-free approaches and can work as plug-and-play component to enhance their performance. There are indeed some works training for high-resolution generation like 4K (Chen et al., 2024; Xie et al., 2024; Zheng et al., 2024; Ren et al., 2024). However, millions of high-quality training data and industrial-scale computational resources are required to train the whole transformer backbone. In this paper, we focus on the challenges of data and parameter efficiency and demonstrate that comparable or even superior performance can be achieved with significantly less data and fewer trainable parameters. Another line of research has concentrated on enhancing inference efficiency through the development of efficient and scalable diffusion backbones (Chen et al., 2024; Liu et al., 2024b;a). These designs and insights are orthogonal to our work, and it is promising to combine their strengths with our approach to achieve the best of both training and inference efficiency, which lies beyond the scope of this paper and is left for future exploration. A.3. Parameter-Efficient Fine-Tuning In many real-world scenarios, fine-tuning existing models for specific applications is often necessary. However, fine-tuning all parameters can lead to substantial computational overhead, particularly in terms of memory footprint. To address this limitation, series of works propose parameter-efficient fine-tuning strategies (Hu et al., 2022; Hyeon-Woo et al., 2021; Meng et al., 2024; Yeh et al., 2023; Wang et al., 2024a). In this paper, we aim at an effective method specifically tailored for ultra-resolution adaptation. B. Theoretical Proof We supplement the proof of our main theoretical results in Theorem 2.4 here. Ultra-Resolution Adaptation with Ease Theorem B.1. Under the setting defined in Assumptions 2.1, 2.2, and 2.3, the error between WT , the parameters after training iterations, and the optimal is bounded by: E[WT 2 2] E[(I ηM )T 02 2] + η2(p(1 p)E[δ2] + (1 p)σ2) (cid:88) i=1 (1 (1 ηλi)T )2 λi (8) + p2Wref 2 2. where 0 = W0(pWref +(1p)W ), is defined as (U ; W0)W (U ; W0), δ = (u; Wref )f (u; ), and λi is the i-th eigenvalue of . Proof. Under the assumption of infinite-width neural networks, the network output (u; ) can be viewed as the following linear form with respect to the parameter : (u; ) (u; W0) + (u; W0)(W W0). (9) We denote (W0; u) as Φ for simplicity. According to the loss function = 1 of with respect to is: 2N (u; ) x2 2 and Eq. 9, the gradient = 1 (cid:88) {Φ(f (u; ) x)} = 1 (cid:88) {Φ[Φ(W W0) + (u; W0) x)]}. Training is conducted using SGD: Wt+1 = Wt ηW Lt = Wt η 1 (cid:88) {Φ[Φ(Wt W0) + (u; W0) x)]}. Due to the linearity, the optimal parameter when training on mixture of real and synthetic data is given by: The target can be viewed as the output of (u; ) with noise term ξ: = pWref + (1 p)W . = (u; ) + ξ. Then we analyze the mean and variance of Σξ. According to Eq. 13, ξ satisfies: ξ = (cid:40) (u; Wref ) (u; ), (u; ) + ξ (u; with probability p, ), with probability 1 p. Given the linearity and Eq. 12, (u; ) = pf (u; Wref ) + (1 p)f (u; ). Denote (u; Wref ) (u; ) as δ. Then, ξ = (cid:40) (1 p)δ, with probability p, pδ + ξ, with probability 1 p. Since ξ (0, σ2), the expectation of ξ is: E[ξ] = p(1 p)δ + (1 p)(pδ) = 0. And the variance, denoted as Σξ, is computed as: Σξ = E[ξ2] = p(1 p)2E[δ2] + (1 p)E[(pδ + ξ)2] = p(1 p)E[δ2] + (1 p)σ2. 13 (10) (11) (12) (13) (14) (15) (16) (17) (18) Ultra-Resolution Adaptation with Ease Let denote ΦΦ RDD, where is the number of parameters in the network, and denote Wt with Eqs. 11 and 13, we obtain: . Combining Wt+1 = (W + t) η"
        },
        {
            "title": "1\nN",
            "content": "(cid:88) {Φ[Φ(t + (W W0)) + (u; W0) (f (u; ) + ξ)]}. (19) Note that: We then have: Thus, (u; ) (u; W0) Φ(W W0). Wt+1 = (W + t) ηΦ[Φt + ξ] = + [(I ηΦΦ)t + ηΦξ]. Starting from 0, after iterations, according to Eq. 22, we can obtain the explicit expression of : t+1 = (I ηM )t + ηΦξ. = (I ηM )T 0 (cid:125) (cid:123)(cid:122) Initial error decay (cid:124) 1 (cid:88) (I ηM )kΦξ . k=0 (cid:123)(cid:122) Label noise accumulation (cid:125) + η (cid:124) (20) (21) (22) (23) Now, we are interested in E[T 2 2], which contains quadratic terms of initial error decay and label noise accumulation as well as their cross term. Since label noise is independent of error caused by initializing model parameters and E[ξ] = 0, the cross term is 0. Thus, E[WT 2] = E[(I ηM )T 02 2 (cid:34) 2] + η2E (I ηM )kΦξ(cid:13) 2 (cid:13) (cid:13) (cid:35) . (cid:13) (cid:13) (cid:13) 1 (cid:88) k=0 The first term is related to the initial error. For the noise term, let AT be the cumulative sum of the updates: 1 (cid:88) (I ηM )k. AT = k=0 Then: 1 (cid:88) 1 (cid:88) (I ηM )kΦξξΦ(I ηM )j = AT ΦΣξΦA . k=0 j=0 Since the noise covariance Σξ is scalar, using = ΦΦ, we get: E[AT Φξ2 2] = ΣξTr(cid:0)AT T (cid:1). Using the matrix geometric series sum formula: where denotes the Moore-Penrose pseudoinverse. Substituting, AT = (I (I ηM )T ), AT T = (I (I ηM )T )M (I (I ηM )T )M . Taking the trace: Tr(cid:0)AT (cid:1) = Tr(cid:0)M (I (I ηM )T )2(cid:1). Let have eigenvalue decomposition: = ΛV , Λ = diag(λ1, ..., λD). Note that for infinite-width network, . Thus, λi = 0 for < D. Then: (I ηM )2T = (I ηΛ)2T . 14 (24) (25) (26) (27) (28) (29) (30) (31) (32) Therefore: Similarly, Ultra-Resolution Adaptation with Ease Tr(cid:2)(I ηM )2T (cid:3) = (cid:88) i=1 (1 ηλi)2T . Tr(cid:2)M (I (I ηM )T )2(cid:3) = (cid:88) i=1 (1 (1 ηλi)T )2 λi . Thus, using Eq. 18, the result of E[T 2 2] is: E[WT 2] = E[(I ηM )T 02 2 2] + η2(p(1 p)E[δ2] + (1 p)σ2) (cid:88) i=1 (1 (1 ηλi)T )2 λi . By triangle inequality, we have: E[WT 2] E[WT =E[WT 2] + 2 2 2 2] + pWref + (1 p)W 2 2 2 (33) (34) (35) =E[(I ηM )T 0 2] + η2(p(1 p)E[δ2] + (1 p)σ2) (cid:88) i=1 (1 (1 ηλi)T )2 λi (36) + p2Wref 2 2. C. More Experimental Details C.1. Prompts for AI Preference Study To better compare the quality of generated images, we employ GPT-4o as the evaluator, assessing methods from three aspects: overall quality, visual aesthetics, and prompt alignment. The evaluation involved both pairwise comparisons and quantitative analysis. During the evaluation, for pairwise comparison, GPT-4o compares our method with the baseline methods, selecting the more preferred image. For quantitative analysis, GPT-4o assigns scores (0-100) to each image generated by each method. The prompts used in our testing are listed below, designed following the previous work PixArt-Sigma (Chen et al., 2024). For pairwise comparison, the designed prompt to evaluate the overall quality of images is as follows: As an AI visual assistant, you are an evaluator specialized in image quality analysis for high-resolution text-to-image generation models. Given specific caption, please evaluate the overall quality of the image by considering both content alignment and technical excellence. For content alignment, assess the key information including object identities, properties, spatial relationships, object numbers and caption-specified style. For technical quality, evaluate the images photorealism and aesthetics, focusing on clarity, richness of detail, artistic quality, and overall visual appeal. Please analyze how well the image performs in both aspects to determine its comprehensive quality, the prompt is your prompt. Please output [Image 1] if the first image is better, [Image 2] if the second image is better, and give me the reason. The designed prompt to evaluate visual aesthetics of images is as follows: As an AI visual assistant, you are an evaluator specialized in image quality analysis for high-resolution text-to-image generation models. When presented with specific caption, it is required to evaluate and determine which image exhibits greater photorealism and aesthetical, in terms of clarity, richness of detail, and overall quality. Please pay attention to the key factors, including image style, the artistic quality of the image, realism, etc., the prompt is your prompt. Please output [Image 1] if the first image is better, [Image 2] if the second image is better, and give me the reason. The designed prompt to evaluate the prompt alignment of images is as follows: 15 Table 4: Results on AI preference study. Evaluation images are generated with COCO30K prompts with resolution of 2048 2048. Ultra-Resolution Adaptation with Ease Method/Setting"
        },
        {
            "title": "Prompt Alignment Rank Visual Aesthetics Rank",
            "content": "SDEdit w/ URAE I-Max w/ URAE FLUX1.1 [Pro] Ultra PixArt-Sigma-XL Sana-1.6B FLUX-1.dev FLUX-1.dev w/ URAE 87.09 88.23 88.24 89. 90.42 86.13 86.46 84.23 86.05 89.71 2 1 2 1 1 4 3 6 5 90.99 92.49 91.38 92.58 93.53 88.71 90.25 89.05 89.48 93.64 2 1 2 2 6 3 5 4 1 89.18 90.09 89.96 90.86 90.42 86.31 87.80 84.88 87.02 91. 2 1 2 1 2 5 3 6 4 1 As an AI visual assistant, you are an evaluator specialized in image quality analysis for high-resolution text-to-image generation models. Given specific caption, you need to judge which image aligns with the caption more closely. Please pay attention to the key information, including object identities, properties, spatial relationships, object numbers and image style, etc., the prompt is your prompt. Please output [Image 1] if the first image is better, [Image 2] if the second image is better, and give me the reason. For quantitative analysis, the designed prompt to evaluate the overall quality is as follows: As an AI visual assistant, you specialize in evaluating image quality for high-resolution text-to-image generation models. Given specific caption, please evaluate the overall quality of the image by considering both content alignment and technical excellence. For content alignment, assess the key information including object identities, properties, spatial relationships, object numbers and caption-specified style. For technical quality, evaluate the images photorealism and aesthetics, focusing on clarity, richness of detail, artistic quality, and overall visual appeal. Please analyze how well the image performs in both aspects to determine its comprehensive quality. The prompt is: your prompt. Please output strictly the score from 0 to 100. Do not provide any explanation or additional text beyond this numeric score. The designed prompt to evaluate the visual aesthetics of images is as follows: As an AI visual assistant, you specialize in evaluating image quality for high-resolution text-to-image generation models. When given specific caption, you are required to assess the image and assign 0-100 score, reflecting its photorealism, aesthetic appeal, clarity, richness of detail, and overall quality. Key factors to consider include image style, artistic quality, and realism. The prompt is: your prompt. Please output strictly the score from 0 to 100. Do not provide any explanation or additional text beyond this numeric score. The designed prompt to evaluate the prompt alignment of images is as follows: 16 Ultra-Resolution Adaptation with Ease As an AI visual assistant, you are an evaluator specialized in image quality analysis for high-resolution text-to-image generation models. Given specific caption, you need to determine the score 0-100 that the image aligns with the caption. Please pay attention to the key information, including object identities, properties, spatial relationships, object numbers and image style, etc., the prompt is your prompt. Please output strictly the score from 0 to 100, reflecting how accurately the image aligns with the caption. Do not provide any explanation or additional text beyond this numeric score. C.2. Prompts Used in Main Manuscript 1. girl with pink hair, vaporwave style, retro aesthetic, cyberpunk, vibrant, neon colors, vintage 80s and 90s style, highly detailed. 2. Astronaut in jungle, cold color palette, muted colors, detailed, 8k. 3. whimsical village scene is nestled within an enormous teacup, where winding cobblestone streets and quaint cottages create surreal microcosm reminiscent of dreamlike landscapes. From high vantage point, the viewer looks down on the diminutive inhabitants going about their day-to-day activities, contrasting the juxtaposition of innocence and chaos in this fantastical setting. Soft, ethereal lighting envelops the scene, creating an atmosphere of tranquility as ordinary life intertwines with elements of fantasy and absurdity to weave captivating visual narrative that invites the audience into an extraordinary world where the commonplace becomes surreal. 4. surreal landscape depicting an ethereal fusion of natural beauty and fantastical architecture, reminiscent of Salvador Dalis dreamlike paintings. From above the clouds, one gazes upon colossal tower emerging from the earth, its intricate gears visible as it merges seamlessly with tranquil mountain lake. The scene is bathed in an otherworldly glow, casting lavender and gold hues across the sky, while delicate cherry blossoms flutter gently in the foreground, adding sense of serenity to this breathtaking vision where time and nature intertwine. 5. Imagine an enchanting scene where the dreamy allure of Klimts celestial brushstrokes intertwines with the magical wonder of Studio Ghibli animations, showcasing mesmerizing underwater realm teeming with life. In this captivating portrait, luminous mermaid princess emerges from vibrant coral landscape, her iridescent silver hair flowing like cascading waterfalls as she sings hauntingly beautiful melody that resonates through the depths. From an aerial perspective, the scene unfolds in surreal and awe-inspiring manner, capturing both the ethereal grace of this mythical creature and the breathtaking beauty of her aquatic surroundings, creating harmonious blend of fantasy and reality that invites viewers to lose themselves in its enchanting embrace. 6. whimsical portrait of an otherworldly fairy with luminescent wings captures her enchanting features from an unconventional angle beneath floating moonlit garden. Rendered in high-definition digital art, her luminous eyes gleam like stars and her flowing hair seems caught in an unseen breeze, while the intricate details of shimmering scales on her wings and petals entwined within her tresses contribute to the overall sense of wonder. The soft focus of distant flowers and glistening dewdrops engulfs the scene in dreamy atmosphere, harmoniously blending with the fairys ethereal presence in this captivating tableau that evokes feelings of enchantment and wonder. 7. Steampunk airship floating above misty Victorian cityscape, intricate brass and copper mechanical details, golden hour lighting, billowing clouds, detailed architectural elements, rich warm color palette, cinematic composition. 8. Create an image in the surrealistic style, capturing unique birds-eye view of whimsical scene where tall giraffe stands atop an oversized piano in lush garden filled with oversized, vibrant flowers. The giraffe appears to play an invisible melody on the keys, its long neck bending gracefully over the keyboard while its feet dance rhythmically on the pedals. This striking juxtaposition should evoke sense of wonder and enchantment, blending elements of natural beauty with unexpected musical elements in dreamlike atmosphere. 9. sleek black luxury sedan parked on rain-soaked city street at night, reflecting neon lights from nearby buildings. The wet pavement glistens, and the cars smooth curves are highlighted by the ambient glow of the urban environment. 10. Barbarian woman riding red dragon, holding broadsword, in gold armour. 11. person wearing Spider-Man suit in the game Half-Life Alyx. 12. person staring into lucid dream world with an adventure waiting. 13. dreamlike landscape depicting an ethereal giraffe with elongated limbs and neck gracefully floating above surreal desert oasis. This captivating scene is captured from low-angle perspective, evoking sense of wonder as the giraffe appears to defy gravity in whimsical juxtaposition of elements. The image combines vibrant colors and melting textures to create an imaginative and thought-provoking vision that blurs the lines between reality and fantasy, inviting viewers to explore the depths of their own imagination. 17 Ultra-Resolution Adaptation with Ease 14. Craft an image in the surreal digital art style, depicting dreamlike portrait of young woman whose face merges with complex floral arrangement. This scene is viewed from an elevated perspective, giving it unique bugs-eye view, making her appear to be enveloped within grand garden teeming with vivid, distorted blooms emerging directly from her body and hair. The composition should inspire awe and intrigue as the petals weave into her facial contours, creating fantastical union of nature and human form. gentle, diffused lighting bathes this captivating scene, casting delicate shadows that dance across the textures of both the flowers and her translucent skin. 15. An enigmatic dream landscape is captured in this whimsical scene, where an intricate tower with distorted, fluid architecture dominates the horizon. The sky above is filled with floating islands, their shapes shifting as if viewed through fisheye lens, emphasizing the surreal nature of the setting. In the foreground, curious child stands on one of these drifting lands, wide-eyed and flowing-haired, gazing up at the enigmatic tower with mix of awe and unease, evoking an emotional spectrum that ranges from curiosity to uneasiness in this fantastical landscape. C.3. Prompts Used in Appendix 1. portrait of an enigmatic cyberpunk warrior woman, her sleek black hair cascading behind her as she holds high-tech sword in one hand and futuristic communication device in the other. Standing atop towering skyscraper at sunset, she is illuminated by the warm glow of city lights, her armor reflecting the vibrant hues of the sky. The scene is rendered with dramatic lighting that accentuates both the sharp angles of her ancient-style attire and the sleek curves of her modern technology, conveying an atmosphere of awe and wonder at the fusion of old and new. Her expression combines fierce determination with thoughtful contemplation, symbolizing the collision of advanced civilization and traditional wisdom she embodies. This striking digital art piece captures the essence of warrior existing between worlds, bridging the gap between the past and future. 2. An elven queen wearing transparent silk in fantasy character portrait. 3. busy fantasy street depicting single street within an old city lined with quirky shops, old buildings, cobblestones, and street life. 4. young engineer man with cybernetic enhancements wearing suit and bowtie, detailed mask, and gloomy expression, with half of his face mechanical. 5. Visualize an enchanting amalgamation of impressionist serenity and fantastical landscapes, where graceful Japanese maiden in traditional kimono gracefully traverses serene pond filled with iridescent water lilies. The petals of these delicate flowers shimmer under the golden hour light, radiating spectrum of soft pastel colors that envelop the scene in an atmosphere of tranquility. As she moves through this dreamlike environment, her reflection dances across the waters surface, creating mesmerizing interplay of color and movement that captures both the timeless elegance of impressionist artistry and the enchanting magic of cinematic storytelling in one captivating masterpiece. 6. ((full body)) looking at sky)),Quality, looking at viewer, lot viewer, sense of depth and dimenck, making the womans more. The overall mood of the image is mysterious and ethereal. anime style, intricate detail. 7. captivating digital artwork in surrealistic style, featuring an alluring ballet dancer gracefully suspended above dreamlike landscape. The ethereal dancers movements are captured from low-angle perspective, highlighting her elegant limbs and flowing attire, reminiscent of fish-eye lens. The surreal backdrop is melting, impressionist-inspired cityscape with fluid architecture that seamlessly blends into the sky, creating an atmospheric mood of wonder and enchantment. This mesmerizing scene balances between reality and fantasy, inviting viewers to explore the dreamlike world of the ballet dancer and immerse themselves in the whimsical beauty of her performance. 8. kodak film potrait , girl surrounded with bubbles detailed, dramatic lighting shadow (lofi, analog-style). 9. Imagine futuristic digital art depiction of Earth at dusk, where towering skyscrapers intertwine with flourishing vertical gardens in an intricate fusion of nature and technology. The scene is captured from unique vantage point, reminiscent of bugs-eye view, as the sky transitions from golden hues to deep purples and blues, casting elongated shadows across the urban landscape. Amidst this bustling metropolis, solitary figure draped in ethereal, flowing robes stands atop one of the tallest structures, their silhouette softened by the waning light and embraced by the verdant surroundings. This enigmatic character holds an antiquated device that emanates gentle, pulsating glow, contrasting with the cool metallic surfaces of the cityscape and evoking sense of reverence for the past and wonder about the future. 10. Imagine an otherworldly coastal view in dreamlike style reminiscent of surrealism, where grand dragonfly elegantly drifts above peaceful aquatic expanse. The mirrored sky on its surface forms captivating mirage as if one could submerge into the cosmos themselves. This mesmerizing scene is captured from unique insects-eye perspective, revealing intricate patterns of the dragonflys dainty wings and vivid hues, while gentle, amber twilight bathes the tranquil tableau in warm 18 Ultra-Resolution Adaptation with Ease embrace, stirring emotions of tranquility and wonder simultaneously. 11. Envision dreamlike landscape where surrealism intertwines with fantasy. In this whimsical realm, instead of traditional timepieces, floating islands crafted from diverse cheeses take the place of melting clocks. These cheese islands are vibrant ecosystems teeming with peculiar creatures reminiscent of those found in Alices adventures. giant, levitating chessboard adds to the fantastical setting as its pieces come alive and engage in playful interactions with the island inhabitants. This aerial view captures the essence of surreal dreamscape that blends Dali-esque surrealism with Carrollian fantasy, evoking an atmosphere of wonder and curiosity about the bizarre world unfolding before our eyes. 12. high-resolution digital illustration showcases an individual standing at the edge of colossal, intricately designed cityscape, reminiscent of steampunk architecture intertwined with ancient Mayan ruins. From unique aerial perspective, this expansive panorama reveals towering mechanical spires and cogs, juxtaposed against vivid azure sky. As the sun sets, its golden rays cast elongated shadows over the labyrinthine structures, instilling sense of wonder and intrigue. The protagonist, an enigmatic figure wearing sleek, modern exoskeleton, stands poised on the brink of this massive metropolis, one foot stepping forward as if ready to unveil the secrets hidden within its gears and mechanisms. This mesmerizing image skillfully fuses elements of science fiction, ancient mythology, and futuristic technology in visually captivating composition that transports viewers into an atmospheric world brimming with both awe and mystery. C.4. Prompts Used in User Study 1. Craft an image depicting surreal dreamscape with majestic unicorn floating amidst tumultuous waves, viewed from an aerial perspective akin to observing through birds eyes soaring above the sea. This scene captures both serene beauty and chaotic turbulence in one fantastical landscape. Utilize vibrant, contrasting colors, featuring deep blues for the stormy sea and fiery oranges and purples for the swirling clouds overhead, creating an emotional gradient that evokes wonder, danger, and ethereal grace simultaneously. 2. captivating Art Nouveau-inspired image showcases celestial enchantress gracefully dancing amidst swirling vortex of shimmering stardust, her ethereal gown intricately woven with delicate silver threads reminiscent of cosmic nebulae. Captured from an elevated perspective that accentuates the vastness and grandeur of the cosmos, this scene radiates sense of wonder, enchantment, and serenity as it invites viewers to marvel at the luminous beauty of the muse against the backdrop of the infinite expanse. 3. surreal digital artwork depicting an enigmatic floating cityscape composed of inverted ziggurats suspended in midair. From an aerial perspective, the city appears to hover over an endless expanse of rippling water. As one draws closer to the waters surface, the reflection reveals mirrored image of the city, but with its architecture twisted and distorted by the shifting tides. The vibrant colors and intricate details evoke sense of wonder mixed with unease, inviting the viewer to contemplate the relationship between reality and illusion. 4. surreal digital artwork depicting bustling futuristic cityscape at night, with towering skyscrapers adorned in vibrant, abstract shapes. The scene transitions from sharp clarity to soft, dreamlike atmosphere as it approaches the horizon, evoking both awe and uncertainty in the viewer. Neon lights pulse within the city, seemingly melting and warping in the air, creating mesmerizing patterns and reflections on the wet streets below. From high-altitude perspective, this enigmatic metropolis is captured in an aerial view, inviting contemplation of the convergence of reality and dreams. 5. In the style of visionary art, depict serene female figure draped in luminous white gown with intricate mandala patterns in deep blue and vibrant teal hues. This ethereal portrait is viewed from an aerial perspective, showcasing the subject against cosmic background that seamlessly blends into swirling galaxies and nebulae. The radiant colors and harmonious compositions evoke profound sense of spiritual awakening and interconnectedness with the vast universe around us. 6. In the style of visionary art, depict serene female figure draped in luminous white gown with intricate mandala patterns in deep blue and vibrant teal hues. This ethereal portrait is viewed from an aerial perspective, showcasing the subject against cosmic background that seamlessly blends into swirling galaxies and nebulae. The radiant colors and harmonious compositions evoke profound sense of spiritual awakening and interconnectedness with the vast universe around us. 7. dreamlike landscape emerges from first-person viewpoint, immersing the observer in an alluring world where waterlilies of soft lavender and violet hues gracefully drift on the surface of an opalescent pond. Towering lotus blossoms stretch towards an indigo sky embellished with celestial bodies that gleam like stars, invoking both tranquility and awe. regal swan presides over this fantastical garden, its iridescent feathers creating captivating ripples across the water that seem to distort time itself, crafting harmonious melody of dreams and nature, encapsulating the spirit of beauty and whimsy in one stunning tableau. 19 Ultra-Resolution Adaptation with Ease 8. Envision an otherworldly aquatic environment where graceful mermaid adorned with iridescent scales reminiscent of deep-sea hues gracefully dances amidst vibrant coral formations. Her tranquil expression mirrors state of contemplative introspection, as if she is enveloped in the enigmatic depths from an unconventional vantage point that of diminutive seashell. Delicate intricacies emerge, such as her cascading tresses mirroring tender seaweed and how sunlight weaves through the water to cast enchanting patterns upon her skin. This captivating scene instills sense of awe and reflection while preserving an ethereal aura reminiscent of surrealistic art. 9. Craft an enchanting surrealist scene showcasing bugs-eye view perspective of chess game occurring on shifting landscape. In this scene, majestic phoenix perches atop black bishop, its vibrant wings casting intricate shadows across the checkerboard expanse below. The background alternates between lush tropical forests and barren deserts with each move made by the ethereal beings participating in this mysterious match, instilling feelings of intrigue and wonder as they traverse the unpredictable terrain under the eerie illumination of full moon. 10. Imagine an enchanting digital artwork depicting dragonflys perspective above tranquil pond, reminiscent of Monets captivating water lilies. The scene showcases lush, vibrant vegetation surrounding the serene water surface, with an emotional gradient highlighting the beauty and enchantment as light gracefully dances across the composition. Merging elements of impressionism with high-resolution photorealistic textures, this piece evokes sense of awe and wonder, creating an ethereal atmosphere that captures the dragonflys mesmerizing flight amidst blooming floral paradise. 11. Imagine lively digital painting capturing the exhilarating spirit of an anime-inspired hoverboard race in neondrenched cyberpunk cityscape at twilight. The scene pulses with action as diverse characters navigate through maze-like urban landscape of towering skyscrapers and radiant billboards, deftly maneuvering around airborne vehicles and zipping pedestrians while leaving trails of shimmering pixels behind them. As the sun dips below the horizon, its dramatic lighting casts elongated shadows that accentuate the futuristic architecture and high-tech trinkets sprinkled throughout, creating rich tapestry of cool blues, purples, and pinks that intensify the emotional stakes and anticipation as the race nears its peak. This captivating image, with its dynamic composition and vibrant use of color and light, transports viewers into an enthralling world where technology and nature intertwine in dazzling display of innovation and style. 12. An enigmatic digital artwork showcases an celestial ballerina gracefully spinning across the cosmos, her luminescent dress shimmering with iridescence against the inky black backdrop of space. The scene is observed from unique worms eye view, accentuating the grandeur and elegance of this otherworldly dancer as she whirls amidst nebulae and stars, creating an entrancing spectacle that captivates both the senses and the imagination while evoking sense of wonder and awe for the hidden beauty within the universe. 13. mesmerizing digital artwork portraying an enchanting celestial sorceress in shimmering robes adorned with starlightinspired patterns. Seen from above, her captivating gaze reflects the light of distant galaxies as she skillfully shapes the cosmos with her mystical staff, creating swirling nebulae and brilliant constellations in breathtaking display. The backdrop showcases deep purple expanse of space filled with nebulous clouds, pulsating stars, and enigmatic planets, generating an emotional gradient that balances wonder and mystery within this celestial tableau. 14. Craft an image that embodies spiritual realism with celestial elements, depicting an astronaut in reflective spacesuit adorned with intricate mandala patterns. This ethereal figure floats amidst the cosmic void, traversing through luminous wormhole that connects our physical world to higher plane of existence. The astronauts journey is captured from an eye-of-the-needle perspective, enveloped by vivid colors and profound symbolism that evoke wonder, transcendence, and spiritual awakening in the vast expanse of space. 15. celestial ballet unfolds within an ethereal nebula as galaxies collide in mesmerizing dance of cosmic forces. From an impossible vantage point, suspended above the event horizon, we gaze upon this breathtaking spectacle. The scene captures the chaotic beauty and sublime mystery of the universe, with swirling patterns of stars, nebulas, and cosmic dust illuminated by an otherworldly light source that casts long shadows across the celestial plane. This visually stunning composition evokes sense of awe and wonder at the sheer scale and complexity of our cosmos, as well as the delicate balance between order and chaos in the grand design. Inspired by the visionary artistry of Escherian architecture and the cosmic explorations of space telescopes, this image invites viewers to ponder the infinite mysteries that lie beyond the stars. 16. Create dreamlike still life scene blending impressionistic water lilies with surreal melting elements. Picture tranquil garden pond adorned with softly ruffled blue, pink, and green flowers, as if captured in an ethereal dance of light and color. In the background, tower melts into the distant horizon, its hands frozen in time, evoking sense of timeless wonder. The reflection on the waters surface distorts the landscape into mesmerizing shapes and colors, blurring the line between reality and fantasy, enveloping the viewer in an enchanting atmosphere of surreal beauty and awe. 17. An awe-inspiring image reveals an enigmatic blend of swirling celestial patterns reminiscent of van Goghs iconic brushstrokes and the surreal melting clocks synonymous with Dalis dreamscapes. Set within the opulent interior of vast library, the scene captivates the viewers gaze as they stand amidst towering shelves adorned with golden-hued spines. 20 Ultra-Resolution Adaptation with Ease Above, an intricate celestial map unfolds across the ceiling, its constellations echoing van Goghs dynamic style while clocks dissolve into nebulous forms that cast whimsical shadows over the polished marble floors. This surreal tableau masterfully combines wonder and introspection, inviting the observer to embark on journey through time and space within its harmonious visual symphony. 18. captivating digital artwork merges Victorian-era street market with an underwater world, creating surreal fusion of reality and fantasy. The bustling activity of vendors selling exotic goods under sky of swirling auroras transitions seamlessly into the vibrant depths of the ocean, inviting viewers to explore this mysterious realm. This unique blend of surface-level excitement and serene tranquility evokes curiosity and wonder, guiding the audience through mesmerizing journey that defies traditional boundaries. 19. An enchanting digital artwork portrays an underwater haven where fantastical creatures interact harmoniously in style reminiscent of James Gurneys Dinotopia. From an aerial perspective, this captivating scene reveals the gentle touch between regal triceratops and graceful mermaid, as they share tender moment under the luminous sunlight filtering through the pristine waters. This mesmerizing encounter evokes wonder and tranquility, skillfully blending fantasy with prehistoric elements to create visually stunning tableau that transcends time and imagination. 20. Craft an enchanting digital artwork that fuses surrealism with vibrant colors, depicting an underwater realm where fish gracefully perform ballet in harmony with the flowing currents. This unique perspective showcases dreamlike dance between elegant sea creatures and floating bubbles, bathed in soft pastel hues evoking sense of wonder and tranquility. Delicate brushstrokes and fantastical shapes intertwine to create captivating visual symphony, inviting viewers into an otherworldly realm where reality and imagination seamlessly blend. D. More Experimental Results D.1. More Results on AI Preference Study We additionally use GPT-4o for quantitative analysis, allowing for more intuitive evaluation of performance of each method across different assessment dimensions. Our experimental setting and pairwise comparison remain consistent. We randomly select 300 prompts from the COCO30K dataset and generate images of 20482048. The results are shown in Table 4. The results indicate that our method achieves an exceptionally high level across all three dimensions and is comparable to SOTA model FLUX1.1 [Pro] Ultra. D.2. More Qualitative Examples We include more qualitative samples in Fig. 9 to further demonstrate the superior performance on handling detailed and complex structures and textures in ultra-resolution image generation. 21 Ultra-Resolution Adaptation with Ease Figure 9: More high-resolution results of our proposed method. The image in upper left is of 4096 4096."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}