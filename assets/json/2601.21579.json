{
    "paper_title": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices",
    "authors": [
        "Wuyang Zhou",
        "Yuxuan Gu",
        "Giorgos Iacovides",
        "Danilo Mandic"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive $\\mathcal{O}(n^3C)$ parameter complexity with $n$ as the width of the residual stream and $C$ as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, $\\mathcal{O} \\left( nC \\cdot n! \\right)$. To address both challenges, we propose \\textbf{KromHC}, which uses the \\underline{Kro}necker products of smaller doubly stochastic matrices to parametrize the residual matrix in \\underline{mHC}. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to $\\mathcal{O}(n^2C)$. Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at \\texttt{https://github.com/wz1119/KromHC}."
        },
        {
            "title": "Start",
            "content": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices Wuyang Zhou 1 Yuxuan Gu 1 Giorgos Iacovides 1 Danilo Mandic"
        },
        {
            "title": "Abstract",
            "content": "The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained HyperConnections (mHC) mitigate these challenges by projecting the residual connection space onto Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs prohibitive O(n3C) parameter complexity with as the width of the residual stream and as the feature dimension. The recently proposed mHClite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces factorial explosion in its parameter complexity, (nC n!). To address both challenges, we propose KromHC, which uses the Kronecker products of smaller doubly stochastic matrices to parametrize the residual matrix in mHC. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to O(n2C). Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at https: //github.com/wz1119/KromHC. 6 2 0 2 9 2 ] . [ 1 9 7 5 1 2 . 1 0 6 2 : r 1. Introduction Hyper-Connections (HC) (Zhu et al., 2025) have emerged as powerful alternative to the ubiquitous residual connections (He et al., 2016). This is achieved by expanding the residual 1Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom. Preprint. January 30, 2026. Table 1. Comparisons between SOTA mHC variants. Our proposed KromHC is the only method that simultaneously achieves exact doubly stochastic residual matrices, parameter efficiency, and requires no specialized kernel optimization. Criterion mHC mHC-lite Doubly Stochastic Parameter Efficient PyTorch Native . . KromHC (Ours) stream width to enhance topological complexity. Unlike the standard residual mapping xl+1 = xl + F(xl) (He et al., 2016), HC increases the stream width by an expansion rate, n, without incurring computational overhead regarding FLOPs. By introducing learnable mixing across the multiple residual streams, HC allows for more expressive feature propagation. More specifically, single layer of HC is defined as Xl+1 = Hres Xl + Hpost (cid:0)Hpre Xl (cid:1) , (1) R1n, and Hpost where Xl RnC and Xl+1 RnC are the expanded Rnn, input and output at the l-th HC layer; Hres Hpre R1n are learnable mappings that, respectively, mix the residual streams, aggregate features from streams into one, and map the layer output back onto the streams; F() is learned residual function such as the attention mechanism (Vaswani et al., 2017). }L Recent work (Xie et al., 2025) has suggested that the unconstrained residual matrices ({Hres l=1) in HC can lead to numerical instabilities when training large-scale neural networks (NNs) such as large language models (LLMs). In particular, HC cannot preserve the identity mapping property of the standard residual connections (He et al., 2016) when stacked across multiple layers, as (cid:81)Ll Li fails to preserve the global mean of the features in i=1 Hres (cid:32)Ll (cid:89) (cid:33) Hres Li Xl XL = i=1 L1 (cid:88) + L1i (cid:89) Hpost Hres Lj F(Hpre Xi) , (2) i=l j=1 1 KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices Figure 1. Illustration of variants of manifold-constrained hyper-connections with residual stream width = 8. (a) mHC: utilizes iterative Sinkhorn-Knopp (SK) algorithm to approximate doubly stochastic residual matrix; (b) mHC-lite: builds the residual matrix as convex combinations of n! permutation matrices, but becomes infeasible for large n; (c) KromHC (Ours): constructs the residual matrix as the Kronecker products of smaller (e.g., 2 2) doubly stochastic matrices, thus guaranteeing double stochasticity while remaining parameter efficient. where and represent deeper and shallower layer, respectively (Xie et al., 2025). To address the training instability issue of HC, authors in Xie et al. (2025) have proposed the Manifold-Constrained Hyper-Connections, which applies the Sinkhorn-Knopp algorithm (Sinkhorn & Knopp, 1967) to iteratively project the residual matrices, {Hres l=1, onto the Birkhoff polytope (i.e. the set of doubly stochastic matrices). Since the sum of the individual rows and columns of doubly stochastic matrices is always equal to 1, the residual mixing mapping, Hres Xl, becomes convex combination of the input features, preserving feature mean across layers and regularizing the norm of the residual matrices. }L However, the Sinkhorn-Knopp algorithm (Sinkhorn & Knopp, 1967) in mHC can fail to achieve double stochasticity when employed over finite number of iterations (e.g., 20 iterations in mHC). This leads to error accumulation across layers and undermines training stability (Xie et al., 2025; Yang & Gao, 2026) (See Figure 2). To this end, Yang & Gao (2026) proposed mHC-lite, which guarantees exact double stochasticity by using the Birkhoff-von-Neumann theorem (Birkhoff, 1946) to parametrize the residual matrices as the convex combinations of permutation matrices. Despite achieving exact doubly stochastic residual matrices, mHC-lite suffers from an explosion in parameter complexity, as it requires n! unique permutation matrices of size nn to be stored. Furthermore, the generic mHC (Xie et al., 2025) has parameter complexity of O(n3C), thus preventing effective scaling of the residual stream width (See Figure 3). Therefore, the following question naturally arises: Can we achieve exact double stochasticity of the residual matrices without incurring an explosion in parameter count as the width of the residual stream, n, increases? 2 KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices In summary, the contributions of this paper are as follows: Based on the Kronecker product, we propose KromHC, novel manifold-constrained hyper-connection framework, providing link to tensorized residual mixing. We resolve the conflict between exact double stochasticity and parameter efficiency in SOTA mHC variants. The proposed KromHC guarantees exact doubly stochastic residual matrices while enabling more parameter-efficient scaling of the residual stream width. We demonstrate the effectiveness and scalability of our approach through extensive experiments on LLM pretraining, achieving consistent improvements over SOTA mHC variants without requiring customized kernels. 2. Related Works Macro-design of Neural Architecture. Macro-design concerns the topological structure of blocks in NN, deciding how inputs and outputs of different blocks are routed and merged across layers (Srivastava et al., 2015). Despite the success of ResNet (He et al., 2016), the use of single residual stream restricts information flow to single pathway, which may limit the representational capacity of very deep networks (Zhu et al., 2025). To this end, recent research has focused on expanding the width of the residual stream (Chai et al., 2020; Fang et al., 2023; Mak & Flanigan, 2025; Xiao et al., 2025; Xie et al., 2023; Zhu et al., 2025). For example, Hyper-Connections expands the residual stream into multiple streams and introduces learnable matrices to dynamically mix streams as in Equation (1) (Zhu et al., 2025). However, these methods (Xiao et al., 2025; Mak & Flanigan, 2025; Zhu et al., 2025) may not preserve the identity mapping property of the original residual connection, causing instabilities during training. Manifold-Constrained Hyper-Connections. Based on the original HC (Zhu et al., 2025), DeepSeek recently proposed the Manifold-Constrained Hyper-Connections (Xie et al., 2025). The mHC preserves the identity mapping property of the standard residual connection by projecting the residual matrices, {Hres l=1, onto specific manifold, known as the Birkhoff polytope, Bn. These matrices, Hres , are doubly stochastic matrices, which have the following properties Rnn}L Hres 1n = Hres 1n = 1n, Hres 0, (3) 0 means that all entries in Hres where 1n represents an n-dimensional vector of all ones, and Hres are non-negative. Since doubly stochastic matrices have spectral norms equal to 1, and the set is closed under matrix multiplication Figure 2. Numerical stability analysis of the products of residual matrices. The plot compares the Mean Absolute Error (MAE) between the column sum of (cid:81)L1 Li and 1 in an LLM with = 12 transformer blocks and = 24 layers of HC. The standard mHC architecture exhibits MAE of around 0.05, indicating potential training instabilities. The mHC-lite and KromHC have exact doubly stochastic matrices, thus yielding zero MAE. i=0 Hres Figure 3. The number of learnable parameters against the number of residual streams, n, per hyper-connection in mHC, mHC-lite, and KromHC. We assume the feature dimension, C, to be 512. Also, is factored into (cid:81)log2(n) 2, i.e., i1 = i2 = = iK = 2. m=1 To answer this question, we propose KromHC, which uses the Kronecker products (Van Loan, 2000) of smaller doubly stochastic matrices to parametrize the residual matrix in mHC. By framing residual mixing as Tucker-structured tensor network (Tucker, 1966; Kolda & Bader, 2009; Cichocki et al., 2015) with core tensor comprising of the tensorized residual streams, we induce Kronecker structure that guarantees exact double stochasticity of the residual matrices, while having parameter complexity of (cid:0)n2C(cid:1). More specifically, KromHC parametrizes the residual matrices, {Hres l=1, as Kronecker products of smaller doubly stochastic matrices, which are learned as convex combinations of smaller permutation matrices as shown in Figure 1. qualitative comparison between mHC, mHC-lite and the proposed KromHC is shown in Table 1. }L 3 KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices (Birkhoff, 1946), this manifold restores the identity mapping property across layers. Given the input hidden matrix Xl RnC at the l-th layer, it is first flattened into vector xl = vec(Xl) R1nC to preserve full context information. Then, the learnable residual mappings in mHC are obtained as = RMSNorm(xl), = σ (cid:0)αpre Hpre = 2σ (cid:0)αpost Hpost = SK (αres Hres + bpre lWpost mat (x (cid:1) , (cid:1) , + bpost ) + bres lWres lWpre ) , (4) l , bpost , αpost , Wpost RnCn2 RnCn and Wres R1n and bres where Wpre are R1n2 projection matrices; bpre are learnable bias terms; the terms αpre , and αres are learnable scalars, mat() is reshape function from R1n2 to Rnn, and σ() denotes the Sigmoid function. The SK() operator denotes 20 iterations of the Sinkhorn-Knopp algorithm (Sinkhorn & Knopp, 1967) for projecting the residual matrix onto the Birkhoff polytope. However, mHC does not guarantee exact double stochasticity and requires highly customized kernels for accelerating the SK algorithm. Yang & Gao (2026) proposed mHC-lite to parameterize the doubly stochastic residual matrices as convex combinations of permutation matrices via the Birkhoff-von-Neumann theorem (Birkhoff, 1946) (See Appendix F). It guarantees exact double stochasticity and can be implemented with PyTorch native matrix operations (Paszke et al., 2019). However, the parameter complexity of mHC-lite grows factorially, i.e., O(nC n!) with the residual stream width n, preventing the scaling of (See Figure 3). Tensor Networks. Tensor Networks (TNs) provide an efficient representation of higher-order tensors by factorizing them into network of lower-order cores and factors, thereby alleviating the curse of dimensionality (Novikov et al., 2015; Kolda & Bader, 2009; Cichocki et al., 2016; Wang et al., 2023). By exploiting the multi-linear and lowrank structures in NNs, TNs enable expressive yet parameterefficient representations that can scale efficiently. Recent works have demonstrated their effectiveness in LLM applications such as model compression (Xu et al., 2023; Gu et al., 2025a), parameter-efficient fine-tuning (Bershatsky et al., 2024; Yang et al., 2024; Gu et al., 2025b), etc. 3. Notation and Preliminaries The mathematical notations used in this paper are listed in Table 2. This is consistent with the notation used in Cichocki et al. (2015). An order-K tensor, Ri1i2iK , is multidimensional array with modes. vector is an order-1 4 Table 2. Mathematical notations Symbol a, a, A, () A(i1, . . . , iN ) ICC A a! vec(A) mat(A) Meaning Scalar, vector, matrix, tensor Matrix transpose The (i1, . . . , iN )-th element of Identity matrix of size Mode-n product Kronecker product factorial Vectorization of Matricization of tensor, and matrix is an order-2 tensor. Tensorization (folding) reshapes vector or matrix into higher-order tensor. For example, we can tensorize matrix Rj1j2 into an order-K tensor Ri1iK , provided that (cid:81)k m=k+1 im = j2 for some split point {1, . . . , K}. The inverse process of tensorization is called unfolding (matricization or vectorization). m=1 im = j1 and (cid:81)K Tucker Decomposition Tensor Network. Tucker decomposition is corner stone in multi-linear tensor networks. It generalizes the matrix singular value decomposition (SVD) to higher-order tensors (Kolda & Bader, 2009; De Lathauwer et al., 2000). More specifically, Tucker decomposition tensor network parametrizes an order-K tensor, Ri1i2iK , as = 1 U1 2 U2 3 UK, (5) or in unfolded form vec(Y) = (cid:0)UK UK1 U1(cid:1) vec(X ) = 1 (cid:79) k=K Uk vec(X ), (6) where Rr1r2rK is an order-K core tensor, {Uk Rikrk }K k=1 are the factor matrices, and vec() is the operation that converts tensor from Ri1i2iK to vector Ri1i2iK . The vector containing [r1, r2, . . . , rK] is the so-called Tucker ranks. The element-wise definition of the mode-n product in = Uk is Y(r1, , rk1, ik, rk+1, , rK) = (r1, , rk1, r, rk+1, , rK)Uk(ik, r). (7) rk(cid:88) r=1 4. Methodology KromHC keeps the parametrization of Hpost unchanged from mHC, and parametrizes the residual mixing mapping of Equation (1), Hres Xl, as Tucker decomposition tensor network where the tensorized residual stream is and Hpre KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices the core tensor. The proposed KromHC guarantees that all residual matrices are always exact doubly stochastic, while having learnable parameter count much lower than mHC and mHC-lite. The architecture of the proposed KromHC is illustrated in Figure 1. 4.1. Tensorizing the Residual Stream Let xl RC be the original input feature at the l-th layer. We expand the width of the residual stream into n, yielding Xl RnC at the l-th layer. Given = (cid:81)K k=1 ik, ik Z+, we first tensorize the residual stream into an order- (K + 1) tensor, Ri1i2iK (See Figure 7 in Appendix). Afterwards, we perform residual mixing along each of the first modes of with the learned doubly Rikik }K stochastic matrices, {Uk k=1, which satisfy Uk 1ik = Uk 1ik = 1ik , Uk 0, for 1 K. (8) This is achieved as Hres Xl = mat(Xl 1 U1 2 U2 3 UK K+1 ICC), (9) where mat() represents the matricization of tensor from Ri1i2iK to RnC. This coincides with the definition of the Tucker decomposition tensor network where the Tucker ranks are equal to the original dimensions, i.e. [r1, r2, . . . , rK, rK+1] = [i1, i2, . . . , iK, C], and the last factor matrix, UK+1 RCC, is the identity matrix. Therefore, we can write Equation (9) in the following format Hres Xl = (cid:0)UK UK1 (cid:123)(cid:122) Hres (cid:124) U1 Xl, (cid:1) (cid:125) (10) where denotes the Kronecker product. Consequently, the single layer propagation in KromHC can be written as Xl+1 = Hres Xl + Hpost (cid:0)Hpre Xl (cid:1) = 1 (cid:79) k=K Uk Xl + Hpost (cid:0)Hpre Xl (cid:1) , (11) where F() denotes neural network layer which could be an attention mechanism, feed-forward network (FFN), etc. 4.2. Kronecker-Product Residual Matrices We detail below how to guarantee the double stochasticity of the so obtained Hres from the Kronecker product of smaller doubly stochastic matrices, Uk Rikik . Theorem 4.1. (Birkhoff-von-Neumann Theorem (Birkhoff, 1946)) For any doubly stochastic matrix, X, there 5 exists finite collection of permutation matrices {Pk Rnn}n! k=1 and coefficient vector = (a1, . . . , an!) Rn! satisfying ak 0, [n!] and (cid:80)n! k=1 ak = 1, such that = (cid:80)n! k=1 ak Pk. Rikik , Since the size of doubly stochastic matrices, Uk are typically much smaller than n, we can parametrize them as convex combinations of permutation matrices of shape ik ik via Theorem 4.1. For example, let the width of the residual stream, n, be power of 2 (i.e., 2, 4, 8, 16, . . .) and {ik = 2}K k=1, where = logik=2(n). In this case, only 2 permutation matrices of size 2 2 need to be stored Rikik . Furtherfor parametrizing all different Uk more, we only need to learn 2 scalars in order to represent Rikik on the Birkhoff polytope as the convex any Uk combination of two 2 2 permutation matrices. Theorem 4.2. (Kronecker Closure of Doubly Stochastic Matrices) Let Bn Rnn denote the set of doubly stochastic matrices. Let U1 Bi2. Then their Kronecker product satisfies Bi1 and U1 U2 Bi1i2. (12) More generally, for any finite collection {Uk Bik }K their iterated Kronecker product satisfies k=1, 1 (cid:79) k=K Uk Bn, where = (cid:89) k=1 ik. (13) Proof. See Appendix B. = (cid:78)1 Theorem 4.2 states that the Kronecker product of any finite collection of doubly stochastic matrices is also doubly Rikik }K stochastic. Since {Uk k=1 are doubly stochask=K Uk tic matrices and Hres , Hres in the proposed KromHC is guaranteed to be doubly stochastic. This is equivalent to imposing Kronecker structure to the residual matrix, Hres , which acts as an extra constraint besides the manifold constraint used in mHC. Remark 4.3. Due to the guaranteed double stochasticity of the residual matrices, KromHC preserves the desired properties of the original mHC, such as norm preservation and compositional closure. Norm preservation means the spectral norm of the residual matrix is bounded by 1 (i.e., 1). Compositional closure preserves stability Hres throughout all layers, as (cid:81)L i=1 Hres Li remains exact doubly stochastic. 4.3. Parametrization of KromHC We detail below the parametrization of Hpre and Hres in KromHC. We follow mHC to flatten the input, Xl , Hpost l KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices Table 3. Comparisons of additional learnable parameters relative to standard residual connections, training loss, validation bits-per-byte (BPB), and CORE score across different types of manifold-constrained hyper connections. The number of transformer blocks is denoted by D. Each transformer block has 2 residual connections. All experiments are conducted with = 4 residual streams. The best and second best values among different methods are highlighted in bold and underlined, respectively. Method = 6 = Params (K) Train Loss Val BPB CORE Score Params (K) Train Loss Val BPB CORE Score Residual mHC mHC-lite KromHC (Ours) 462 609 240 3.490 3.493 3.484 3.488 1.047 1.042 1.045 1.047 6.477 7.971 8.208 9. 1844 2433 959 2.971 2.964 2.972 2.966 0.864 0.861 0.864 0.862 14.774 16.023 13.217 16.872 RnC, at the l-th layer to xl R1nC. The parametrization of KromHC is as follows: = RMSNorm(xl), = σ (cid:0)αpre Hpre lWpre = 2σ (cid:0)αpost Hpost ak = Softmax(αres + bpre lWpost (cid:1) , + bpost lWres,k (cid:1) , + bres,k l Uk = Hres = ik! (cid:88) m=1 1 (cid:79) k=K ak (m)Pm, Uk , ), (14) , apost l R1n, bpost where is factorized into terms, i.e., = (cid:81)K k=1 ik, ik Z+. The term Pm Rnn denotes the m-th unique permutation matrix. The scalar ak (m) is the m-th entry in ak . The operator (cid:78)1 k=K denotes the sequence of Kronecker products. The apre , and ares are the learnable scalar coefficients at the l-th HC layer. The sigmoid function is denoted as σ(). The Wpre RnCn, Wpost RnCn, and Wres,k RnCik! are the learnable weight matrices. The bpre R1n, and bres,k R1ik! are the learnable biases. Remark 4.4. Although the set of permutation matrices {Pm}ik! is given by construction, the coefficients ak are learned via Equation (14). Thus, each factor Uk is learnable point within the Birkhoff polytope Bik , allowing Hres to dynamically mix the residual streams while maintaining its doubly stochastic property. Remark 4.5. The dimensions {ik}K k=1 can be any integer factorization of such that ik 2. Although any valid factorization preserves the doubly stochastic property of Hres , choosing the prime factorization (e.g., ik = 2 for = 2K) yields the highest parameter efficiency. This is because the number of learnable parameters in KromHC scales with (cid:80) ik!. m=1 for each factor Uk Parameter Complexity Analysis. The learnable parameter count of KromHC per HC layer is much lower than that of the mHC and mHC-lite (See Figure 3). More specifically, 6 the parameter complexity of mHC is (cid:0)n3C(cid:1), while for mHC-lite, the parameter complexity is (nC n!). The parameter count of KromHC is 2n2C + (nC + 1) (cid:80)K k=1 ik! + 2n + 3. Since ik is usually very small (e.g., ik = 2), the parameter complexity of KromHC is dominated by O(n2C). 5. Experiments The evaluation of the training and downstream performance of the proposed KromHC on LLM pretraining was performed on two scales: 60M parameters (D = 6 transformer blocks) and 186M parameters (D = 12 transformer blocks) by replacing the residual connections in Nanochat (Karpathy, 2025) (See Section for more details). All models were trained on the FineWeb-Edu (Penedo et al., 2024) dataset with Token:Parameter ratio of 20 following Hoffmann et al. (2022). Experiments were conducted using either 4 or 8 NVIDIA RTX PRO 6000 GPUs, depending on the number of residual streams. Experimental results demonstrate that KromHC matches or outperforms SOTA mHC variants, while using significantly fewer trainable parameters. 5.1. Initialization Following the experimental settings in Yang & Gao (2026), we initialized Wres,k to zero. The bias vectors bpre and bpost were set to -1 for all entries except for single index in each vector, which was set to 1. We set αpre and Wpost and αpost , Wpre to 0.01. (cid:21) 0 and P2 = trices: P1 = For {ik = 2}K k=1, there are exactly two permutation ma- (cid:20)0 (cid:20)1 1 0 (cid:21) 1 . The bres,k 0 set to [0, 8]T , and αres was set to 0.01.This initialization (1) 1 and ak ensures that, at initialization, ak (2) 0, yielding Uk I22. Consequently, the Kronecker products of identity matrices produced nearly identity matrix Hres at initialization. was KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices Table 4. Commonsense and reasoning benchmark results (accuracy %). = 6 or 12 transformer blocks and = 4 residual streams were used for the experiments. The best and second best values among different methods are highlighted in bold and underlined, respectively. Method Avg HS HS-ZS PIQA ARC-E ARC-C COPA CSQA OBQA WG WGrande BoolQ 12 Residual 41.1 27.0 mHC 42.2 26.4 42.3 27.4 mHC-lite KromHC (Ours) 42.4 28.0 46.2 36.6 Residual 44.5 36.2 mHC 44.4 35.2 mHC-lite KromHC (Ours) 47.7 36.4 29.0 28.4 28.2 27.8 35.4 37.6 36.2 38.4 60.4 60.8 56.4 59. 65.4 64.2 64.6 65.0 42.8 42.8 42.8 41.4 58.4 58.6 58.6 57.8 21.8 22.4 24.4 23.0 26.4 27.4 27.2 27.6 55.0 60.0 53.0 54. 63.0 64.0 63.0 66.0 31.8 23.0 35.0 28.8 36.6 32.4 23.6 30.0 23.8 24.8 25.6 25.4 32.6 33.6 30.4 31.2 59.3 55.3 53.4 53. 57.5 60.4 58.6 58.6 49.2 50.4 50.0 50.0 52.0 53.2 49.2 54.8 37.4 54.4 54.4 60.8 44.0 54.6 41.6 58.4 Table 5. Language modeling, BigBench (BBH) subtasks, and evaluation suite results (accuracy %). = 6 or 12 transformer blocks and = 4 residual streams were used for the experiments. The best and second best values among different methods are highlighted in bold and underlined, respectively. Method Avg Lamb SQuAD CoQA BBH-QA BBH-CS BBH-Op BBH-Dyck LSAT LangID 6 12 Residual mHC mHC-lite KromHC (Ours) Residual mHC mHC-lite KromHC (Ours) 18.1 17.3 18.8 19.5 23.7 22.9 23.3 24.0 18.6 17.4 19.6 19.0 29.2 31.6 30.0 30.4 0 0 0.4 0.2 10.8 5.8 8.4 8. 5.6 4.8 4.6 5.8 13.8 13.0 14.2 15.4 20.0 10.6 19.4 14.0 39.2 39.6 36.6 40.4 40.8 42.0 38.2 40.8 38.8 42.0 42.6 44. 9.1 5.2 5.7 8.6 12.9 16.7 14.76 11.9 4.6 9.2 6.4 11.4 15.8 13.0 10.0 13.6 23.5 23.5 29.6 27.8 27.8 20.4 27.0 26. 23.4 26.0 28.0 27.8 25.4 24.0 26.2 25.0 5.2. Training and Validation Set Metrics (cid:80)T We compared the performances of standard residual connection, mHC, mHC-lite and our proposed KromHC methods under both 6 and 12 transformer blocks with = 4 residual streams. The results are shown in Table 3. The training loss denotes the cross-entropy (CE) loss LCE = 1 t=1 log pθ(xtx<t), while the validation perT formance is measured using tokenizer-invariant metric, bits-per-bytes (BPB) (i.e., LBP = LCE ln(2) Total Tokens Total Bytes ). CORE score (Li et al., 2024) is the centered accuracy computed over fixed subset of 22 downstream evaluation tasks to reflect general language understanding quality (See more details in Appendix C). Notably, our method significantly outperformed SOTA mHC variants in terms of the CORE score, indicating that the models trained with KromHC have stronger capabilities in downstream tasks including commonsense reasoning and language modeling. Additionally, our method achieved onpar training loss and validation BPB as mHC and mHC-lite while having much fewer additional learnable parameters compared to mHC and mHC-lite. 5.3. Downstream Task Performances Table 4 and 5 present the detailed performance evaluations for commonsense reasoning and language modeling, respectively. We compared the proposed KromHC with the residual connection, mHC and mHC-lite. All models were trained under identical settings with 6 or 12 transformer blocks and = 4 residual streams. Commonsense reasoning. Our method achieved the highest average accuracies at both 6-block (42.4%) and 12-block (47.7%) settings, consistently outperforming standard residual connections and other SOTA manifold-constrained HC variants. In particular, KromHC demonstrates strong capabilities in reasoning-intensive tasks such as ARC-C and BoolQ, surpassing the second best scores by up to 2% and 6.4% respectively. The consistent improvements across both model depths suggest that KromHC scales effectively with depth, while remaining robust across diverse commonsense reasoning tasks. These results demonstrate that KromHC is beneficial for reasoning tasks. Language modeling. KromHC also achieved the best average performance (19.5% and 24.0%) in language modeling at = 6 and = 12. These results suggest that KromHC is effective for improving the language modeling performance in LLM pretraining, which is essential for language understanding. 5.4. Scaling the Width of Residual Stream in KromHC In order to assess how the performance of KromHC scales with the width of the residual stream n, we conducted experiments on LLM pretraining with 12 transformer blocks and 7 KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices Table 6. Additional number of parameters of our KromHC models with different widths of residual streams compared to the standard residual connection under = 12. = 4 = 8 = 16 Params (M) 0.24 2.67 11.37 {4, 8, 12} residual stream width. As shown in Figure 4 (left), the gap between training losses becomes larger as increases. similar scaling trend is observed in validation, where BPB consistently improves as increases (See Figure 4 (right)). The additional number of learnable parameters at different are recorded in Table 6. These results demonstrate that KromHC benefits from larger residual stream width and scales effectively with n. Figure 5. Zoomed-in view of gradient norms from 5000 to 7000 steps during training. Trajectories are smoothed using EMA, with shaded regions indicating the EMA variance. coefficients for permutation matrices were computed as ak = Softmax(αres (15) As shown in Figure 6, sharing αres ter performance than learning unique αres,k yields betfor each matrix. ). + bres,k lWres,k across all Uk Figure 4. Training loss and validation BPB gaps of KromHC at different widths of the residual stream, n, compared to = 4. Exponential Moving Average (EMA) is applied to the raw loss before the calculation of the loss gap. 5.5. Gradient Norm Figure 5 presents the gradient norm trajectories across the last 2000 training steps. Identical model configurations (12 transformer blocks and = 4 residual streams) were used for mHC, mHC-lite and our KromHC. It is worth noting that our KromHC consistently achieved the lowest gradient norm compared with other manifold-constrained hyperconnection variants. Both mHC-lite and KromHC achieved lower gradient norms than mHC due to their exact doubly stochastic residual matrices (See Figure 2). This indicates improved training stability in KromHC and suggests that KromHC can control gradient magnitudes more effectively during training. 5.6. Ablation Study Shared αres should be shared across all Uk trix, i.e. αres . We examined whether the scaling factor αres or be unique for each ma- . In Equation (14), the mixing versus αres,k l across all doubly stochastic matrices Uk Figure 6. Sharing αres outperforms the use of matrix-specific αres,k . The experiment was conducted with 12 transformer blocks and = 4 residual streams. 6. Conclusion We have introduced KromHC, parameter-efficient manifold-constrained hyper-connection framework which employs Kronecker-product residual matrices to guarantee their exact double stochasticity. In this way, KromHC also resolves the scalability limitations of existing mHC variants regarding parameter complexity. Extensive experiments have demonstrated the effectiveness of the proposed method in LLM pre-training. Our future work aims to apply KromHC to other domains such as computer vision. Limitations. KromHC may encounter parameter issues when the width of the residual stream is large prime number. However, this can be mitigated by using larger which is power of 2 or 3 or exhibits prime factorization consisting of small numbers. KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices"
        },
        {
            "title": "Impact Statement",
            "content": "This work resolves the scalability and stability issues of manifold-constrained hyper-connections which advances the field of Machine Learning. By enabling more reliable training with fewer parameters, the proposed KromHC supports more accessible and sustainable future deployment of advanced AI systems."
        },
        {
            "title": "References",
            "content": "Bershatsky, D., Cherniuk, D., Daulbaev, T., Mikhalev, A., and Oseledets, I. LoTR: Low Tensor Rank Weight Adaptation. arXiv preprint arXiv:2402.01376, 2024. Birkhoff, G. Three observations on linear algebra. Univ. Nac. Tacuman, Rev. Ser. A, 5:147151, 1946. Chai, Y., Jin, S., and Hou, X. Highway Transformer: SelfGating Enhanced Self-Attentive Networks. arXiv preprint arXiv:2004.08178, 2020. Cichocki, A., Mandic, D., De Lathauwer, L., Zhou, G., Zhao, Q., Caiafa, C., and PHAN, H. A. Tensor Decompositions for Signal Processing Applications From Two-way to Multiway Component Analysis. IEEE Signal Processing Magazine, 32(2):145163, 2015. doi: 10.1109/MSP.2013.2297439. Cichocki, A., Lee, N., Oseledets, I., Phan, A.-H., Zhao, Q., Mandic, D. P., et al. Tensor Networks for Dimensionality Reduction and Large-scale Optimization: Part 1 Lowrank Tensor Decompositions. Foundations and Trends in Machine Learning, 9(4-5):249429, 2016. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 29242936. Association for Computational Linguistics, June 2019. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300/. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457, 2018. Gu, Y., Zhou, W., Iacovides, G., and Mandic, D. TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs. In Proceedings of 2025 International Joint Conference on Neural Networks (IJCNN), pp. 18, 2025a. doi: 10.1109/IJCNN64981. 2025.11228585. Gu, Y., Zhou, W., Iacovides, G., and Mandic, D. TeRA: Vector-based Random Tensor Network for High-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2509.03234, 2025b. He, K., Zhang, X., Ren, S., and Sun, J. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training Compute-Optimal Large Language Models. arXiv preprint arXiv:2203.15556, 2022. Jordan, K. modded-nanogpt: Speedrunning GPT2 training, 2024. URL https://github.com/ KellerJordan/modded-nanogpt. Karpathy, A. nanochat: The best ChatGPT that $100 URL https://github.com/ can buy, 2025. karpathy/nanochat. Accessed: 2026-01-13. Kocijan, V., Davis, E., Lukasiewicz, T., Marcus, G., and Morgenstern, L. The Defeat of the Winograd Schema Challenge. Artificial Intelligence, 325:103971, 2023. Kolda, T. G. and Bader, B. W. Tensor Decompositions and Applications. SIAM review, 51(3):455500, 2009. Li, J., Fang, A., Smyrnis, G., Ivgi, M., Jordan, M., Gadre, S. Y., Bansal, H., Guha, E., Keh, S. S., Arora, K., et al. DataComp-LM: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024. Liu, J., Su, J., Yao, X., Jiang, Z., Lai, G., Du, Y., Qin, Y., Xu, W., Lu, E., Yan, J., et al. Muon is Scalable for LLM Training. arXiv preprint arXiv:2502.16982, 2025. De Lathauwer, L., De Moor, B., and Vandewalle, J. multilinear singular value decomposition. SIAM journal on Matrix Analysis and Applications, 21(4):12531278, 2000. Loshchilov, I. and Hutter, F. Decoupled Weight Decay Regularization. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=Bkg6RiCqY7. Fang, Y., Cai, Y., Chen, J., Zhao, J., Tian, G., and Li, G. Cross-Layer Retrospective Retrieving via Layer Attention. arXiv preprint arXiv:2302.03985, 2023. Mak, B. and Flanigan, J. Residual Matrix Transformers: Scaling the Size of the Residual Stream. arXiv preprint arXiv:2506.22696, 2025. 9 KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices Novikov, A., Podoprikhin, D., Osokin, A., and Vetrov, D. P. Tensorizing Neural Networks. Advances in Neural Information Processing Systems, 28, 2015. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. PyTorch: An Imperative Style, HighPerformance Deep Learning Library. Advances in Neural Information Processing Systems, 32, 2019. Penedo, G., Kydlıˇcek, H., Lozhkov, A., Mitchell, M., Raffel, C. A., Von Werra, L., Wolf, T., et al. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. Roemmele, M., Bejan, C. A., and Gordon, A. S. Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, pp. 9095, 2011. Sakaguchi, K., Le Bras, R., Bhagavatula, C., and Choi, Y. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 87328740, 2020. Sinkhorn, R. and Knopp, P. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21(2):343348, 1967. Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. Srivastava, R. K., Greff, K., and Schmidhuber, J. Training Very Deep Networks. Advances in Neural Information Processing Systems, 28, 2015. Talmor, A., Herzig, J., Lourie, N., and Berant, J. CommonsenseQA: Question Answering Challenge Targeting Commonsense Knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, 2019. Tucker, L. R. Some Mathematical Notes on Three-Mode Factor Analysis. Psychometrika, 31(3):279311, 1966. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention Is All You Need. Advances in Neural Information Processing Systems, 30, 2017. Wang, H., Ma, S., Dong, L., Huang, S., Zhang, D., and Wei, F. DeepNet: Scaling Transformers to 1,000 Layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(10):67616774, 2024. Wang, M., Pan, Y., Xu, Z., Li, G., Yang, X., Mandic, D., and Cichocki, A. Tensor Networks Meet Neural Networks: Survey and Future Perspectives. arXiv preprint arXiv:2302.09019, 2023. Xiao, D., Meng, Q., Li, S., and Yuan, X. Muddformer: Breaking residual bottlenecks in transformers via mularXiv preprint tiway dynamic dense connections. arXiv:2502.12170, 2025. Xie, S., Zhang, H., Guo, J., Tan, X., Bian, J., Awadalla, H. H., Menezes, A., Qin, T., and Yan, R. ResiDual: Transformer with Dual Residual Connections. arXiv preprint arXiv:2304.14802, 2023. Xie, Z., Wei, Y., Cao, H., Zhao, C., Deng, C., Li, J., Dai, D., Gao, H., Chang, J., Zhao, L., et al. mHC: Manifold-Constrained Hyper-Connections. arXiv preprint arXiv:2512.24880, 2025. Xu, M., Xu, Y. L., and Mandic, D. P. TensorGPT: Efficient Compression of Large Language Models based on TensorTrain Decomposition. arXiv preprint arXiv:2307.00526, 2023. Yang, Y. and Gao, mHC-lite: You Dont Need 20 Sinkhorn-Knopp Iterations. arXiv preprint arXiv:2601.05732, 2026. J. Yang, Y., Zhou, J., Wong, N., and Zhang, Z. LoRETTA: Low-Rank Economic Tensor-Train Adaptation for UltraLow-Parameter Fine-Tuning of Large Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 31613176, 2024. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can machine really finish your In Proceedings of the 57th Annual Meetsentence? ing of the Association for Computational Linguistics, pp. 47914800. Association for Computational Linguistics, July 2019. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472/. Van Loan, C. F. The ubiquitous Kronecker product. Journal of Computational and Applied Mathematics, 123(1-2): 85100, 2000. Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. AGIEval: HumanCentric Benchmark for Evaluating Foundation Models. 10 KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 22992314, 2024. Zhu, D., Huang, H., Huang, Z., Zeng, Y., Mao, Y., Wu, B., Min, Q., and Zhou, X. Hyper-Connections. In Proceedings of The Thirteenth International Conference on Learning Representations, 2025. 11 KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices A. Kronecker Product Kronecker products provide convenient way to represent structured linear operators. For matrices Rmn and Rpq, their Kronecker product R(mp)(nq) is defined as = a11B . . . am1B amnB a1nB ... ... , (16) where each entry of scales the entire matrix B. For example, Kronecker product between two 2 2 matrices yields 4 4 matrix. Let = (cid:20)1 (cid:21) 2 4 , = (cid:20)0 5 (cid:21) 6 7 . The Kronecker product R44 is of two non-negative real number is non-negative. Therefore, U1 U2 0. Let 1i1 and 1i2 denote all-one column vectors of dimensions i1 and i2, respectively. We use the Kronecker product identity (A B)(C D) = (AC) (BD), (21) to obtain (U1 U2 )(1i1 1i2 ) = (U1 1i1) (U2 = 1i1 1i2 = 1i1i2. 1i2) (22) Therefore, all row sums of U1 U2 equal 1. Similarly, (U U2 )(1i1 1i2) = (U1 = 1i1i2 . 1i1) (U2 1i2) (23) = (cid:21) (cid:20)1B 2B 3B 4B = 5 0 7 6 0 15 18 21 10 0 14 12 0 20 24 28 Therefore, all column sums of U1 l equal 1. . (17) Combining the non-negative property with row and column sums of 1, we have showed that U1 is doubly stochastic (i.e., U1 U2 Bi1i2 ). U2 The Kronecker product naturally extends to multiple matrices. Let Ak Rmknk for = 1, . . . , K. Their Kronecker product is defined recursively as 1 (cid:79) k=K Ai = AK AK1 A1, (18) By induction, this result extends to finite collection of {Uk Bik }K k=1, as follows 1 (cid:79) k=K Uk Bn, where = (cid:89) k=1 ik. (24) and the resulting matrix has (cid:16)(cid:81)K (cid:17) . k=1 nk size (cid:16)(cid:81)K k=1 mk (cid:17) C. Details of the CORE Tasks B. Proof for Theorem B.1 Theorem B.1. (Kronecker Closure of Doubly Stochastic Matrices) Let Bn Rnn denote the set of doubly stochastic matrices. Let U1 Bi2. Then their Kronecker product satisfies Bi1 and U2 U1 U2 Bi1i2. (19) More generally, for any finite collection {Uk Bik }K their iterated Kronecker product satisfies k=1, 1 (cid:79) k=K Uk Bn, where = (cid:89) k=1 ik. (20) Proof. For any doubly stochastic matrix, its elements are non-negative, and its row and column sums are equal to one. Let U1 Bi2. The Kronecker product U2 U1 (k, l). The product Bi1 and U2 yields elements of l (i, j)U2 Authors in (Li et al., 2024) proposed the CORE metric to provide robust low-variance, centered accuracy score for LLM evaluation. There are 22 selected tasks, where in each task, accuracy is linearly scaled so that 0 indicates randomguess performance and 1 implies perfect accuracy. The final CORE score is averaged across all 22 tasks, preventing any single benchmark dominating the calculation. The tasks in CORE experiments span logical reasoning, factual recall, algorithmic thinking, commonsense inference, and language understanding. In particular, it includes reasoning and knowledge tasks (Zhong et al., 2024; Clark et al., 2018), BIG-Bench tasks (Srivastava et al., 2023), Question Answering and Commonsense (Clark et al., 2019; Talmor et al., 2019; Roemmele et al., 2011), and other widely used benchmarks (Zellers et al., 2019; Kocijan et al., 2023; Sakaguchi et al., 2020). D. Tensor Network Diagram of KromHC Figure 7 shows the TN diagram of our KromHC method. In tensor network (TN) diagram, tensor is denoted by KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices Figure 7. Tensor network diagram of the proposed KromHC method. circle, with each line emanating from the circle corresponding to tensor mode index. Also, connecting two index lines implies tensor contraction over the connected mode indices. E. Parametrization of HC In this section, we detail the parametrization of HC (Zhu et al., 2025). Given the input hidden matrix Xl RnC at the l-th layer, the dynamic mappings and the static mappings are obtained as l = RMSNorm(Xl), tanh (cid:0)Wpre Hpre = αpre tanh (cid:0)Wpost = αpost Hpost tanh (cid:0)Wres = αres Hres , Wpost XT XT XT where Wpre RnC are linear R1C and Wres projections for dynamic mappings, bpre R1n and Rnn are learnable bias terms, and RMSNorm() bres normalizes the feature dimension C. (cid:1) + bpre , (cid:1) + bpost (cid:1) + bres , bpost (25) , , l F. Parametrization of mHC-lite In this section, we detail the parametrization of mHC-lite (Yang & Gao, 2026). Let Xl RnC denote the input 13 feature in the l-th layer and xl R1nC denote the flattened , and Hpost input feature. Then we build mappings Hres dynamically based on xl as , Hpre l = RMSNorm(xl), = sigmoid(cid:0)αpre Hpre = 2 sigmoid(cid:0)αpost Hpost al = softmax(αres lWpre lWres + bpre lWpost + bres (cid:1) , + bpost ) , (cid:1) , (26) Hres = n! (cid:88) k=1 al(k)Pk, , Wpost RnCn and Wres where Wpre RnCn! are learnable weight matrices in the l-th layer. Here, bpre R1n! are learnable bias terms. The terms αpre , and αres are learnable scalars. Pm Rnn are permutation matrices. R1n and bres , αpost , bpost G. Nanochat Each transformer block uses two residual connections, one for the attention mechanism and one for the FFN. Besides the standard residual connections xl+1 = xl + F(xl), Nanochat (Karpathy, 2025) introduces learnable per-layer scalars which improves the model performance. More specifically, each layers input is calcualted as xl = λresid KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices xl + λx0 x0, where x0 is the initial embedding and , λx0 λresid are learnable scalars initialized to 1 and 0 rel spectively (Jordan, 2024; Wang et al., 2024). These were all replaced with the mHC variants when examining the performance of mHC variants. H. Hyperparameters Table 7 lists the hyperparameters used in our experiments. Note that Muon optimizer (Liu et al., 2025) is used for learning parameters in the main branch including attention and multi-layer perceptron (MLP) weight matrices, while AdamW optimizer (Loshchilov & Hutter, 2019) is used for the hyper-connections streams, embedding layer, and language modeling (LM) head layer. Additionally, following the best practice in Karpathy (2025), different learning rates (LR) are used for embedding layer, LM layer, main branch and hyper-connections branch. Table 7. Shared hyperparameters used in our experiments. Name Batch size Sequence length Head dimension % Steps for LR warmdown LR for main branch (Muon) Weight decay of Muon optimizer LR for HCs (AdamW) AdamW β1 AdamW β2 Value 524,288 2048 128 0.4 0.02 0.2 0.005 0.8 0.95 For the two different model scales, i.e. = 6 and = 12 transformer blocks, the corresponding scale-specific hyperparameters are listed in Table 8. Table 8. Scale-specific hyperparameters used in our experiments for = 4 residual streams. Name = 6 = 12 Hidden dimension LR for embedding params (AdamW) LR for LM head params (AdamW) # Training steps 384 0.43 0.0057 2500 768 0.3 0.004 7000 I. Grad Norm Figure 8 shows the raw gradient norm across 7000 training steps of mHC, mHC-lite and KromHC at = 12. 14 Figure 8. Gradient norm dynamics across training. Raw gradient norm across 7000 training steps."
        }
    ],
    "affiliations": [
        "Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom"
    ]
}