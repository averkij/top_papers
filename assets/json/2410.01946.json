{
    "paper_title": "SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics",
    "authors": [
        "Zhiwen You",
        "Kanyao Han",
        "Haotian Zhu",
        "Bertram Ludäscher",
        "Jana Diesner"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance levels comparable to those of fully fine-tuning methods. Previous studies have used crafted prompt templates and verbalizers, mapping from the label terms space to the class space, to solve the classification problem as a masked language modeling task. However, cross-domain and fine-grained prompt-based fine-tuning with an automatically enriched verbalizer remains unexplored, mainly due to the difficulty and costs of manually selecting domain label terms for the verbalizer, which requires humans with domain expertise. To address this challenge, we introduce SciPrompt, a framework designed to automatically retrieve scientific topic-related terms for low-resource text classification tasks. To this end, we select semantically correlated and domain-specific label terms within the context of scientific literature for verbalizer augmentation. Furthermore, we propose a new verbalization strategy that uses correlation scores as additional weights to enhance the prediction performance of the language model during model tuning. Our method outperforms state-of-the-art, prompt-based fine-tuning methods on scientific text classification tasks under few and zero-shot settings, especially in classifying fine-grained and emerging scientific topics."
        },
        {
            "title": "Start",
            "content": "SCIPROMPT: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics Zhiwen You1, Kanyao Han1, Haotian Zhu2, Bertram Ludäscher1, Jana Diesner1,3 1 University of Illinois Urbana-Champaign 2 University of Washington 3 Technical University of Munich {zhiweny2, kanyaoh2, ludaesch}@illinois.edu haz060@uw.edu jana.diesner@tum.de"
        },
        {
            "title": "Abstract",
            "content": "Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance levels comparable to those of fully fine-tuning methods. Previous studies have used crafted prompt templates and verbalizers, mapping from the label terms space to the class space, to solve the classification problem as masked language modeling task. However, cross-domain and finegrained prompt-based fine-tuning with an automatically enriched verbalizer remains unexplored, mainly due to the difficulty and costs of manually selecting domain label terms for the verbalizer, which requires humans with domain expertise. To address this challenge, we introduce SCIPROMPT, framework designed to automatically retrieve scientific topic-related terms for low-resource text classification tasks. To this end, we select semantically correlated and domain-specific label terms within the context of scientific literature for verbalizer augmentation. Furthermore, we propose new verbalization strategy that uses correlation scores as additional weights to enhance the prediction performance of the language model during model tuning. Our method outperforms stateof-the-art, prompt-based fine-tuning methods on scientific text classification tasks under few and zero-shot settings, especially in classifying fine-grained and emerging scientific topics1."
        },
        {
            "title": "Introduction",
            "content": "Scientific text classification tasks involve categorizing scientific abstracts into specific disciplines or topics. Recent studies leverage prompt-based finetuning method (Ding et al., 2022a; Gu et al., 2022; 1Our code is available at https://github.com/ zhiwenyou103/SciPrompt. Schick and Schütze, 2020; Liu et al., 2023a), transferring the text classification problem as masked language modeling task. Masked Language Models (MLMs) are developed by extensively training on large text corpora with percentage of the input tokens being randomly replaced with [MASK] token. Traditional fine-tuning, which requires additional training on labeled domainor task-specific data (Ovadia et al., 2023), may not be suitable in limited data scenarios, such as few and zero-shot settings. Prompt-based fine-tuning has emerged as an effective alternative. This approach uses prompt to guide the MLM in generating specific token through masking [MASK] token in the prompt template, addressing the text classification tasks (Schick and Schütze, 2020; Hu et al., 2021; Chen et al., 2022b; Gao et al., 2021a) under lowresource conditions (i.e., few and zero-shot settings) through verbalizer. As defined by Schick and Schütze (2020), the verbalizer refers to the mapping from label words (e.g., cryptanalysis) to the corresponding class (e.g., Cryptography), serving as projection function between the vocabulary and the class label space. However, in the context of classifying scientific literature, the complexity of scientific language and scarcity of fine-grained (i.e., wide range of scientific fields that are labeled with sub-categories) or emerging topics make it hard to automatically classify crossdomain scholarly articles with limited training samples and manually created verbalizers (Schick and Schütze, 2020). The goal of this paper is to address the challenge of multi-class classification in low-resource settings, specifically focusing on classifying scientific abstracts into different domains with only limited number of labeled examples. We introduce prompt-based fine-tuning approach enriched with domain knowledge as new strategy for retrieving domain-adaptive label terms (i.e., scientific terms in various fields) without manual in4 2 0 O 2 ] . [ 1 6 4 9 1 0 . 0 1 4 2 : r Figure 1: Overall framework of SCIPROMPT. The left side shows the overall process of masked language modeling for performing the text classification task. The right side shows our proposed knowledge retrieval and domainadaptive filtering phase (3). The prediction results, such as CR and SE, correspond to the class labels for Cryptography and Software Engineering, respectively, and are used for scientific knowledge retrieval. tervention. We enhance our approach for lowresource scenarios by retrieving scientific phrases from external knowledge bases (KBs) to expand label terms of the verbalizer (Hu et al., 2021) from the token-level to term phrases. We fine-tune Natural Language Inference (NLI) models for semantic similarity search between retrieved label terms and class labels to select domain-related scientific phrases. Our method differs from previous studies (Hu et al., 2021; Ding et al., 2022b), which rely on word frequency filtering and are limited to singletoken verbalizer projection for text classification. Given the complexity of scientific terminology (see Appendix for more details), we refine the traditional verbalization approach (Ding et al., 2022a) by integrating scientific terms through deploying weight-aware label term mapping function. This approach improves the projection performance from MLMs predictions to probabilities of specific class compared with prior studies (Hu et al., 2021; Gao et al., 2021b; Chen et al., 2022a). Our approach consists of three stages: 1) retrieval of scientific terms, 2) label term filtering, and 3) prediction of scientific topics. Initially, we use cloze-style prompt and an input scientific abstract to guide the MLM to generate label words to fill the [MASK] token (Figure 1). Then, we use each class label as query to retrieve class-related domain phrases (also denote as label terms) from external KBs. To filter the potentially irrelevant terms gathered in the retrieval stage, we fine-tune both bi-encoder and cross-encoder models using the SciNLI dataset (Sadat and Caragea, 2022), enabling the selection of the most relevant domain phrases. Finally, with the selected sets of knowledge-enriched scientific terms, we incorporate these label terms into the verbalizer to convert the MLMs prediction into specific class through semantic score-weighted average loss, enhancing the precision of the probability projections for the augmented verbalizer. Our method extends beyond token-to-token verbalization by encompassing token-to-phrase verbalization that enriches the semantic meaning of scientific domain vocabulary. This broader scope allows for an advanced interpretation of scientific language and classifying emerging topics under weak supervision. In summary, our contributions are the presentation of: domain-adaptive prompt-based fine-tuning framework, named SCIPROMPT, for finegrained and low-resource scientific text classification tasks. new knowledge retrieval and filtering strategy for automatically enriching the verbalizer with domain knowledge. weighted verbalization approach tailored for mapping filtered scientific label terms from model predictions to specific classes. Evaluation via experiments on four scientific datasets show that SCIPROMPT largely outperforms most state-of-the-art methods in few and zero-shot settings."
        },
        {
            "title": "Classification",
            "content": "A Pattern-Exploiting Training (PET) framework (Schick and Schütze, 2021a,b), which initially investigated how cloze-based prompt templates can guide language models to tackle classification tasks (Han et al., 2022; Ding et al., 2022b; Min et al., 2022; Wang et al., 2022a; Zhang et al., 2022; Wang et al., 2022b), has inspired research on incorporation more diverse label words into the verbalizer. Specifically, Hu et al. (2021) added external knowledge to the verbalizing process to help an MLM predict masked tokens more accurately. AdaPrompt (Chen et al., 2022b) applied different knowledge injection method that leveraged task and prompt characteristics to retrieve external knowledge for continuous pre-training of MLMs adaptively. However, classifying scientific literature presents challenges that previous methods have not addressed, including projecting phrase-level label terms in the verbalization process. Other challenges, to which broad range of solutions have been developed, include handling complex semantic structures in wide range of scientific topics (Eykens et al., 2021; Khadhraoui et al., 2022) and the scarcity or imbalance of labeled data across multiple disciplines (Cunha et al., 2021)."
        },
        {
            "title": "2.2 Label Terms Refinement",
            "content": "Prior research on prompt-based fine-tuning has used the verbalizer module to map MLMs predictions to specific classes. Schick and Schütze (2021a) introduced an automatic verbalizer search that identifies suitable label words from training data and language models to enrich the verbalizer. This approach has been further explored in different studies to improve the classification performance (Gao et al., 2021a; Shin et al., 2020; Liu et al., 2023b), although these methods typically need extensive training data, making them less suitable for low-resource scenarios. To address these challenges, one can manually expanding the verbalizer with more label words (Shin et al., 2020), which has limitations when classifying fine-grained and domain-related categories that need expert knowledge. Recently, external KBs have been used to enrich the verbalizer by sourcing class-related label words (Hu et al., 2021; Chen et al., 2022b)."
        },
        {
            "title": "3 Methodology",
            "content": "Our framework of SCIPROMPT uses two-stage approach for scientific text classification: (1) masked language modeling and (2) domain knowledge retrieval and filtering."
        },
        {
            "title": "3.1 Cloze-Style Masked Language Modeling",
            "content": "MLMs (e.g., SciBERT (Beltagy et al., 2019)) are created by randomly masking tokens in the training text and training the model to predict the masked tokens. Similarly, prompt-based finetuning typically leverages clozeor prefix-based prompt template, reformulating the input into masked language modeling task. This strategy enables to predict the masked token, facilitating the execution of downstream tasks based on outputs. Building upon Hu et al. (2021), our framework employs few-shot prompt-based fine-tuning strategy that conceptualizes scientific text classification as an N-way K-shot task, where indicates the number of classes and is the number of labeled examples per class. We provide limited number of labeled examples for each class to tune M. We construct training Dtrain and validation set Dval following previous studies (Gao et al., 2021a; Perez et al., 2021; Wang et al., 2022a; Hu et al., 2021) with examples per class. For the few-shot setting, given cloze-based prompt template Pt and an input abstract an, where an Dtrain, predicts the label word to fill into the [MASK] position in the prompt template. After that, the verbalizer function fv maps the predicted label word onto pre-defined label term set to classify it into class, i.e., Y. We use cross-entropy loss (Gao et al., 2021a) to update the parameters of through verbalization outputs. For instance, the prompt is designed as [Abstract]. The field of this article is related to: [MASK]. will predict suitable label word to fill into the [MASK]. Then, fv calculates the probability of classifying into topic yi, where yi Y: (yi an) = fv(P ([MASK] =M(l) an)), (1) where L. In the zero-shot setting, given can directly generate label word to fill into [MASK], we use the output of as the final label word and send the output into the verbalizing function to calculate class probabilities without tuning loss updates."
        },
        {
            "title": "3.2 Scientific Knowledge Retrieval",
            "content": "Predicting masked tokens using an MLM involves generating range of potential label words, each with varying probabilities of matching specific class. Enhancing the verbalizer with more extensive set of label terms has been proven to improve the accuracy of word-to-class mapping (Hu et al., 2021; Chen et al., 2022b; Wang et al., 2022a; Shin et al., 2020). To implement this approach, we use two external KBs, Related Words2 and Reverse Dictionary3 for scientific knowledge retrieval. Related Words identifies relevant terms using vector similarity and resources like word embeddings and ConceptNet. Reverse Dictionary, which acts as word search engine, finds terms based on definitions or phrases. Reverse Dictionary is particularly useful in phrase-level retrieval, where straightforward labels from Related Words may not suffice given domain-specific phrase (e.g., Networking and Internet Architecture). We set class labels = {y1, y2, ..., yn} as queries to retrieve from Related Words GRW . When GRW fails to produce terms with similarity scores above zero, we use Reverse Dictionary, denoted as GRD, for additional phrase retrieval. Each retrieved term is assigned single relevance score. Initially, we adopted the same threshold (i.e., threshold = 0) as KPT (Hu et al., 2021) for term retrieval based on topic names. Subsequently, we impose two additional thresholds for further selection of retrieved terms (3.3). Utilizing these KBs enables the compilation of knowledge-enhanced term sets Ti = t1, t2, ..., tm for each dataset, where and represents the 2https://relatedwords.org 3https://reversedictionary.org retrieved label terms. Note that the number of terms may vary for each class."
        },
        {
            "title": "3.3 Domain Adaptive Model Tuning",
            "content": "To effectively identify the most relevant label words for each class from set of initial raw terms, it is crucial to use model tailored or adaptable to specific fields. Drawing from Chen et al. (2022b), who employed pre-trained NLI model to filter label words produced by an MLM, we present method that enhances the accuracy of selecting label terms related to specific topics by integrating domain knowledge. We apply newly introduced scientific NLI dataset DSciN LI (Sadat and Caragea, 2022), consisting of labeled sentence pairs (si, sj) from scholarly articles in the fields of NLP and computational linguistics. This dataset serves to fine-tune both cross-encoder Mce and bi-encoder Mbe NLI models4, where Mbe produces for given sentence sentence embedding and Mce passes sentence pair to the encoder to produce an output value between 0 and 1 indicating the similarity of the input sentence pair (Reimers and Gurevych, 2019). The training labels are defined as Entailment or Contradiction, thus framing the model fine-tuning as binary classification task: M(si, sj) = (cid:26) > 0 < 0 if si entails sj if si contradicts sj , where denotes either Mce or Mbe."
        },
        {
            "title": "3.4 Semantic Knowledge Filtering",
            "content": "We merge each retrieved scientific label term with standard prompt (see Appendix G), encode prompts using the fine-tuned Mbe, and use these encoded embeddings as queries for sentence-level semantic searches to select topic-related label terms and calculate semantic similarity scores wl for each label term. We apply SentenceTransformers5 to conduct the cosine similarity search using Mbe within each retrieved label term set. Then, we use Mce to rerank these label terms for every prompt pair of each topic, selecting relevant sentences based on predefined thresholds (µbe = 0.5, µce = 0.1). As these scores also help predict label words, we apply this method in few and zero-shot scenarios (for more details, see Appendix F). Following KPT (Hu et al., 2021), we also apply label term calibration approach with full training 4https://www.sbert.net/examples/ applications/cross-encoder/ 5https://www.sbert.net/index.html set to directly remove irrelevant label terms in the verbalizer that are less likely to be predicted by M. The retrieved label terms for each class with lower probabilities (i.e., less than 0.5) produced by are removed. The probability of is: by WARP. In the experiments with SCIPROMPTSoft, we aggregate all retrieved label terms per topic with semantic scores into vector for topic probability prediction and optimize the aggregated vector during model tuning (detailed in Appendix A). ˆPM([MASK] = tan) PM([MASK] = tan) prior(pt)"
        },
        {
            "title": "4 Experiments",
            "content": ", (2) where prior(pt) is the prior probability of the label term produced by using the training set."
        },
        {
            "title": "3.5 Weighted Verbalizer Transformation",
            "content": "Given that retrieved label terms may be tokenized into multiple tokens, we adopt mean method to average the tokens of label term (Ding et al., 2022b), considering all parts of term as significant. Adopting the verbalizer structure from Ding et al. (2022b), we introduce verbalization approach that maps Ms output to specific classes yi, using predefined semantic scores wl as weights for each label term. This method aims to enhance the accuracy of classifying Ms predictions into topic yi: (yian) = arg max s(vyihmask, wl) yiY exp (vyi hmask wl) yY exp (vy hmask wl) = (cid:80) (3) , where the objective function s(vyihmask, wl) calculates Ms probability for the output vyi of the [MASK] token, with vyi as the label term embeddings, and hmask as the hidden states at the [MASK] position. This objective function can be optimized through the cross-entropy loss as denoted in Equation (1)."
        },
        {
            "title": "3.6 Vector-Based Verbalizer Mapping",
            "content": "Incorporating the filtered label terms into the verbalizer is crucial for making accurate predictions and eliminating noise simultaneously. Moving beyond simple summing (Wang et al., 2022a) or weighted averaging (Hu et al., 2021) of label words, the Word-level Adversarial ReProgramming (WARP) model introduced in (Hambardzumyan et al., 2021) uses vector representations for class mapping, which is distinct from conventional single word projection. We introduce new method named SCIPROMPTSoft based on the uniqueness of our phrase-level verbalizer. Specifically, we refine the verbalization in SCIPROMPTSoft by drawing from the soft verbalizer concept introduced the present We SCIPROMPT across datasets in few and zero-shot scenarios. experimental scientific of settings classification"
        },
        {
            "title": "4.1 Datasets",
            "content": "We use three publicly available datasets in English for our experiments: SDPRA 2021 (Reddy and Saini, 2021), arXiv (Meng et al., 2019), and S2ORC (Lo et al., 2020). SDPRA 2021 contains scientific articles from computer science across seven categories. arXiv (Meng et al., 2019) includes abstracts sourced from the arXiv website6 across 53 sub-categories, and S2ORC contains academic papers from across 19 disciplines. For the S2ORC data, we only select abstracts with single discipline label through the Semantic Scholar Public API7. The statistics and category examples of these datasets are shown in Table 5 and Appendix B."
        },
        {
            "title": "4.2 Experimental Settings",
            "content": "SCIPROMPT is built upon the OpenPrompt framework (Ding et al., 2022b). We apply consistent prompt template across all experiments (see Appendix for more details). The experimental details are shown in Appendix A. alongside In the few-shot setting, we benchmark SCIPROMPT standard fine-tuning, simplified prompt-tuning (PT), and previous state-of-the-art text classification models, including LM-BFF (Gao et al., 2021b), RetroPrompt (Chen et al., 2022a), and KPT (Hu et al., 2021). Standard fine-tuning takes all labeled training examples as input to tuning an MLM for text classification. We take the final representation of the [CLS] token as the output vector of the abstract (Cohan et al., 2020). Standard PT with manually defined verbalizer (Ding et al., 2022b) only takes each lowercase topic name as seed word for verbalization. We apply the same setting as in SCIPROMPT, including unified prompt template, MLM, and the models hyper-parameters. 6https://arxiv.org/ 7https://www.semanticscholar.org/ product/api"
        },
        {
            "title": "Method",
            "content": "SDPRA 2021 arXiv S2ORC Avg. 1 10 20 50 Fine-tuningSciBERT Prompt-tuningManual LM-BFF KPT SCIPROMPT SCIPROMPTSoft Fine-tuningSciBERT Prompt-tuningManual LM-BFF RetroPrompt KPT SCIPROMPT SCIPROMPTSoft Fine-tuningSciBERT Prompt-tuningManual LM-BFF RetroPrompt KPT SCIPROMPT SCIPROMPTSoft Fine-tuningSciBERT Prompt-tuningManual LM-BFF RetroPrompt KPT SCIPROMPT SCIPROMPTSoft Fine-tuningSciBERT Prompt-tuningManual LM-BFF RetroPrompt KPT SCIPROMPT SCIPROMPTSoft 12.72 3.70 71.68 4.73 68.95 1.68 50.74 3.03 64.42 3.64 62.65 4.94 16.45 4.35 83.46 1.41 79.97 2.52 64.76 3.57 77.71 3.34 81.81 3.34 83.70 2.86 17.44 4.50 85.60 0.81 82.66 2.40 74.44 1.63 83.82 0.72 84.71 0.89 85.96 0.60 17.16 3.90 87.76 0.70 86.71 1.36 77.89 1.02 87.74 0.51 87.95 0.41 87.90 0. 27.50 9.48 88.93 0.57 87.94 0.56 83.14 0.63 88.93 0.37 88.99 0.75 88.97 0.71 2.03 0.21 34.95 1.45 35.07 1.31 32.18 1.08 40.57 1.60 31.06 1.74 2.36 0.55 47.58 1.68 50.11 0.88 31.37 0.72 53.68 1.69 56.36 0.95 58.01 0.94 3.14 1.15 50.86 2.89 56.03 0.65 36.49 1.07 61.83 0.83 62.37 0.57 63.42 0.50 3.53 0.86 52.92 2.72 60.90 0.22 41.79 0.81 66.25 0.73 66.59 0.64 66.86 0.46 11.07 1.93 60.63 1.32 64.75 0.23 44.86 1.22 69.95 0.63 69.89 0.63 70.15 0. 4.76 0.85 40.88 1.92 41.50 1.43 43.20 1.33 47.92 1.67 29.94 1.94 5.63 1.37 49.53 0.88 48.67 1.02 47.09 1.38 50.40 1.84 52.12 1.59 47.44 1.60 6.31 0.81 52.15 0.98 50.51 1.19 49.82 0.78 52.91 0.66 53.65 0.22 52.41 0.30 7.29 1.32 54.32 0.89 53.31 1.07 50.55 1.33 54.67 0.43 55.49 0.56 54.70 0.42 12.02 2.22 56.08 0.29 54.97 0.69 53.04 0.73 56.50 0.81 56.66 0.49 56.02 0.60 6.50 1.59 49.17 2.70 48.51 1.47 42.04 1.81 50.97 2.30 41.22 2. 8.15 2.09 60.19 1.32 59.58 1.47 47.74 1.89 60.60 2.29 63.43 1.96 63.05 1.80 8.96 2.15 62.87 1.56 63.07 1.41 53.58 1.16 66.19 0.74 66.91 0.56 67.26 0.47 9.33 2.03 65.00 1.44 66.97 0.88 56.74 1.05 69.55 0.56 70.01 0.54 69.82 0.46 16.86 4.54 68.55 0.73 69.22 0.49 60.35 0.86 71.79 0.60 71.85 0.62 71.71 0."
        },
        {
            "title": "Full Set",
            "content": "Fine-tuning (Full) * 90.71 54.58 53.74 66.34 Table 1: Experimental results under few-shot settings. We report the mean accuracy (expressed in percentages %) and standard deviation based on five iterations across five learning shots. Fine-tuning (Full)* represents using fully labeled training set. RetroPrompt experiments are only conducted in settings above five shots, as this method requires at least two labeled examples for model tuning. KPT (Hu et al., 2021) applied external knowledge to enrich the verbalizer with additional word relevance and frequency filtering strategies. Our experiments use the same MLM (i.e., SciBERT) for equal comparison. Besides, training and validation examples per class (Ding et al., 2022b; Hu et al., 2021; Wang et al., 2022a) are uniform during model tuning, conducting tests with 1, 5, 10, 20, and 50 shots across all datasets and reporting accuracy as an evaluation metric. We evaluate model performance across five random seeds to account for variability (Hu et al., 2021; Ding et al., 2022b). For the zero-shot setting, we sample approximately 10% of each dataset for testing, ensuring adequate representation for each topic. For broader model comparison, we introduce two additional models specific to the zero-shot scenario: SimPTC (Fei et al., 2022) and NPPrompt (Zhao et al., 2023). Moreover, we extend our evaluation to include Llama 2 (Touvron et al., 2023), ChatGPT (OpenAI, 2024), and the latest Llama 3 (AI@Meta, 2024) using in-context learning for broader range of comparisons. Random seeds are applied in KPT, which samples an unlabeled support set of 200 examples to calibrate label words. Figure 2: Performance comparison of few-shot methods over three datasets in Table 1. We report the mean accuracy of each setting. Our method shows high stability in the accuracy distribution compared to the considered baseline models."
        },
        {
            "title": "5.1 Main Results",
            "content": "We highlight the performance of SCIPROMPT against baseline models across our three considered datasets in both few-shot and zero-shot settings, focusing on the fine-grained and cross-domain scientific text classification tasks. The experimental shown are listed in Table 1. Results are averaged over five runs as the same as KPT (Hu et al., 2021) to counteract sampling randomness, reported as mean accuracy with standard deviation. Few-shot Results. SCIPROMPT achieves the best average accuracy on all three datasets for all settings. Specifically, SCIPROMPT and SCIPROMPTSoft excel in low-data scenarios (e.g., one-shot and five-shot), particularly on arXiv and S2ORC, often outperforming baseline models. SCIPROMPT also outperforms KPT by 8.93% in the one-shot setting and 2.83% in the five-shot setting. As the number of training examples increases, the margin of improvement over baseline models narrows. Notably, SCIPROMPT exceeds the fullset fine-tuning by an average of 0.57%, 3.67%, and 5.51% with 10, 20, and 50 shots, respectively. Despite variability in performance improvements across different training sizes, our method consistently achieves the highest accuracy on arXiv and S2ORC across all configurations. Also, the standard deviation of all three datasets decreases as the number of input training examples increases across all three datasets. Additionally, Figure 2 provides comprehensive comparison of performances across all few-shot settings, ranging from one-shot to fifty-shot, for each dataset as outlined in Table 1. SCIPROMPT consistently delivers high and stable accuracy across all three datasets compared to the baseline models. Particularly on S2ORC, SCIPROMPT achieves higher median accuracy and narrower interquartile range, indicating more consistent performance across different few-shot scenarios. The SCIPROMPTSoft method shows high stability on the SDPRA 2021 dataset, while SCIPROMPT is more effective in fine-grained datasets. Methods SDPRA 2021 arXiv S2ORC Llama 2 Llama 3 ChatGPT PT SimPTC NPPrormpt LM-BFF RetroPrompt KPT SCIPROMPT 62.04 81.15 79. 62.97 15.79 35.00 64.79 18.32 41.503.00 51.97 26.98 54.87 54.51 40.30 49.58 46.95 Avg. 43.11 61.87 60.30 20.81 3.25 13.98 14.96 7.83 20.830.18 22. 32.93 11.35 37.23 34.07 35.47 38.420.66 41.30 38.90 10.13 28.74 37.94 20.54 33.581.28 38.52 Table 2: Performance of zero-shot setting. Only KPT is reported through mean accuracy (%) and standard deviation (4.2). We apply the same instruction for ChatGPT, Llama 2, and Llama 3 on the test sets. Zero-shot Results. Shown in Table 2, the Llama 3 70B model leads in performance across all datasets. Nonetheless, SCIPROMPT outperforms other baseline models, especially on arXiv and S2ORC, where it outperforms PT and KPT by margins of 1.47% and 2.88%, respectively. Meanwhile, LM-BFF leads among all baseline models on the SDPRA 2021 dataset. These results underscore the effectiveness of SCIPROMPT in leveraging domainspecific knowledge for fine-grained scientific text classification, even in the absence of labeled training data. Llama 3s average accuracy exceeds SCIPROMPT by 23.35% and Llama 2s by 18.76%. However, on the S2ORC dataset, SCIPROMPT surpasses Llama 2. Note that SCIPROMPTSoft is not designed for zero-shot testing since it needs trainable tokens in the decoding layer during model tuning."
        },
        {
            "title": "Method",
            "content": "KPT SCIPROMPT w/o CL w/o SS w/o SS+CL w/o FL+CL SCIPROMPTSoft w/o CL w/o SS w/o SS+CL w/o FL+CL K=1 K= K=10 K=20 K=50 Avg. Zero-shot 32.181. 53.681.69 61.830.83 66.250.73 69.950.63 56.78 40.571.60 40.191.46 38.700.86 38.360.86 29.770. 31.061.74 38.650.90 41.491.38 42.221.32 37.501.31 56.360.95 55.840.98 55.190.80 54.760.86 50.130.88 58.010.94 58.331.62 58.360.99 57.721.46 57.661.49 62.370.57 62.320.50 62.480.59 62.250.56 59.570.97 63.420.50 63.640.66 63.700.75 63.530.57 63.630.49 66.590.64 66.450.61 66.700.77 66.540.81 65.770. 66.860.46 67.050.55 67.260.75 67.030.78 67.130.93 69.890.63 69.920.64 69.731.01 69.860.92 69.550.70 70.150.52 70.410.56 70.200.21 70.350.49 70.240.49 59.16 58.94 58.56 58.35 54.96 57.90 59.62 60.20 60.17 59.23 20. 22.28 21.87 6.17 5.62 3.77 - - - - - Table 3: Ablation study of SCIPROMPT for mean accuracy and standard deviation for the arXiv dataset under few-shot and zero-shot settings. ticles published after 2019 that are beyond the knowledge cutoff of the SciBERT model. For each selected topic, we gather 30 abstracts, applying the same random seeds for few-shot experiments as those introduced in Table 1. We create new dataset named Emerging NLP by collecting 21 fine-grained NLP-related topics and their corresponding abstracts. Appendix provides detailed dataset statistics and topic examples. Figure 3 compares the performance of various baseline models. Notably, SCIPROMPT exceeds the performance of the Llama 2 70B model by 31.91% and outperforms the PT method by 6.67% in the zero-shot setting. Overall, our method outperforms all state-of-the-art methods in classifying emerging scientific topics, especially in the zero-shot setting, highlighting our methods efficacy in highly low-resource scenarios."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "Our ablation study on the arXiv dataset  (Table 3)  demonstrates the advantages of our models over KPT, with 1.45% increase in zero-shot accuracy. SCIPROMPT and SCIPROMPTSoft outperform KPT by 2.38% and 3.42%, respectively, in terms of average accuracy under the few-shot setting. We examine the impact of removing full-size calibration (w/o CL), semantic scores (w/o SS), and both (w/o SS+CL), finding that both components improve the performance, especially in the zeroshot setting where their absence lowers accuracy by 0.41% (w/o CL) and 16.11% (w/o SS) compared to SCIPROMPT, underlining the critical role of SS in bolstering the models effectiveness. Interestingly, SCIPROMPTSoft performs better without SS than when both components are included. Removing both SS and CL yields the best 1shot performance, suggesting that less intervention Figure 3: Model comparison through the Emerging NLP dataset under five-shot and zero-shot settings (5.2)."
        },
        {
            "title": "5.2 Emerging Topics Classification",
            "content": "To assess our methods effectiveness in classifying emerging scientific topics, we manually collect dataset centered around recent developments in the field of NLP, drawing inspiration from Ahmad et al. (2024). Specifically, we first extract NLP topics from Taxonomy4CL8, focusing on topics that have emerged since 2000, as identified through Semantic Scholar9. We then select scientific ar8https://github.com/DFKI-NLP/ Taxonomy4CL 9https://www.semanticscholar.org/"
        },
        {
            "title": "Method",
            "content": "SDPRA 2021 arXiv S2ORC SCIPROMPT w/o CL w/ CL SCIPROMPTSoft 9.5% 29.3% 12.5% 51.2% 12.4% 59.6% w/o CL w/ CL 12.3% 12.8% 12.3% 13.2% 12.3% 13.4% Table 4: The usage percentage of GPU memory during model tuning. optimizes model tuning in low-data contexts. Furthermore, comparing setups without pre-filtering and calibration (w/o FL+CL) to those with prefiltering shows an accuracy increase by 3.39% and 0.94% for SCIPROMPT and SCIPROMPTSoft respectively, highlighting the effectiveness of pre-filtering of augmented verbalizer for text classification. The ablation studies of SDPRA and S2ORC shows the same pattern as on arXiv."
        },
        {
            "title": "5.4 Model Tuning Efficiency",
            "content": "Table 4 shows that SCIPROMPTSoft reduces GPU memory usage by 16.5 percentage points (p.p.) for SDPRA 2021, 38 p.p. for arXiv, and 46.2 p.p. for S2ORC compared to SCIPROMPTs fullsize label term calibration. Although SCIPROMPT achieves higher average accuracy rates in the few-shot setting on the S2ORC dataset (see Table 6 in Appendix C), SCIPROMPTSoft outperforms SCIPROMPT on SDPRA 2021 and arXiv, suggesting that SCIPROMPTSoft can achieve competitive results with less GPU usage. Moreover, while ChatGPT and Llama 2 exhibit superior performance in the zero-shot setting, as shown in Table 2, it is worth noting that these language models are either mainly for commercial use or require substantial GPU resources, incurring higher costs or more time. For instance, for the S2ORC dataset, our method not only cuts down the combined training and testing (inference) time by 93 p.p. compared to Llama 2 70B but also enhances accuracy by 1 p.p. over Llama 2, highlighting the efficiency and effectiveness of our approach."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced knowledge-enhanced, promptbased fine-tuning framework for fine-grained scientific text classification using minimally or no labeled abstracts. Acknowledging the complexity of domain knowledge within scientific literature, we employed prompt-tuned MLM augmented with domain knowledge injection and semantic filtering. This approach enables the automatic extraction of domain-specific phrases and their integration into weighted verbalizer for topic projection. Our findings highlight the effectiveness of our methods over existing state-of-the-art models and standard full-set fine-tuning, particularly for emerging topic classification and scenarios requiring high levels of topic granularity. Notably, SCIPROMPT demonstrates competitive accuracy compared to the advanced Llama 2 70B model in the zero-shot setting, showing its potential to categorize scholarly topics with lightweight and efficient approach."
        },
        {
            "title": "7 Limitations",
            "content": "Our studys limitations are as follows: 1) Our external knowledge sources are limited to two nonscientific domain databases for retrieving topic words, potentially missing fine-grained scientific terminologies. Despite the challenge of identifying universally applicable, cross-domain, scientific knowledge resource, future efforts should aim to discover more precise terminology databases (Han et al., 2020). 2) We focus solely on multiclass classification task and exclude abstracts that span multiple scientific sub-domains. Advancing towards multi-label classification system capable of identifying publications across various domains would enhance the robustness of our approach. 3) Although SCIPROMPT and SCIPROMPTSoft surpassed baseline methods during evaluation, the enhancements are modest, and results fluctuate, particularly with an increase in labeled training data. Further investigation into the causes of these minimal gains as well as more comprehensive, interpretable experiments are needed to better understand and improve the model performance. 4) We only used classification accuracy and standard deviation as model evaluation metrics. The experimental results can change when using other metrics (e.g., Micro F1 and Macro F1). Additionally, while the standard deviation of our methods shrinks as the number of training examples increases, one could do statistical significance testing to draw robust conclusions by comparing system performance against baseline models."
        },
        {
            "title": "The datasets and MLM employed in our study are\npublicly accessible and extensively utilized in the",
            "content": "research community. To enhance the quality of our data, we applied heuristic filtering to exclude shortlength abstracts across these datasets, acknowledging that this process may impact experimental accuracy. Our methodology includes extracting data from external knowledge bases via public APIs. Furthermore, as we used MLMs as the foundation of our approach, it is essential to note that the predictive behavior of these models can be challenging to regulate due to the implicit knowledge embedded within the MLMs, which is difficult to decode explicitly. Therefore, caution should be exercised when adapting our method to other tasks, especially in the context of text classification through prompting."
        },
        {
            "title": "References",
            "content": "Raia Abu Ahmad, Ekaterina Borisova, and Georg Rehm. 2024. Forc4cl: fine-grained field of research classification and annotated dataset of nlp articles. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 7389 7394. AI@Meta. 2024. Llama 3 model card. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615 3620, Hong Kong, China. Association for Computational Linguistics. Xiang Chen, Lei Li, Ningyu Zhang, Xiaozhuan Liang, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022a. Decoupling knowledge from memorization: Retrieval-augmented prompt learning. Advances in Neural Information Processing Systems, 35:2390823922. Yulong Chen, Yang Liu, Li Dong, Shuohang Wang, Chenguang Zhu, Michael Zeng, and Yue Zhang. 2022b. AdaPrompt: Adaptive model training for prompt-based NLP. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 60576068, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug SPECTER: Downey, and Daniel Weld. 2020. representation learning using Document-level In Proceedings citation-informed transformers. of the 58th Annual Meeting of the Association for Computational Linguistics, pages 22702282, Online. Association for Computational Linguistics. Washington Cunha, Vítor Mangaravite, Christian Gomes, Sérgio Canuto, Elaine Resende, Cecilia Nascimento, Felipe Viegas, Celso França, Wellington Santos Martins, Jussara Almeida, et al. 2021. On the cost-effectiveness of neural and non-neural approaches and representations for text classification: comprehensive comparative study. Information Processing & Management, 58(3):102481. Ning Ding, Yulin Chen, Xu Han, Guangwei Xu, Xiaobin Wang, Pengjun Xie, Haitao Zheng, Zhiyuan Liu, Juanzi Li, and Hong-Gee Kim. 2022a. Promptlearning for fine-grained entity typing. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 68886901. Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, and Maosong Sun. 2022b. Openprompt: An open-source framework for prompt-learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 105113. Joshua Eykens, Raf Guns, and Tim CE Engels. 2021. Fine-grained classification of social science journal articles using textual data: comparison of supervised machine learning approaches. Quantitative Science Studies, 2(1):89110. Yu Fei, Zhao Meng, Ping Nie, Roger Wattenhofer, and Mrinmaya Sachan. 2022. Beyond prompting: Making pre-trained language models better zero-shot learners by clustering representations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 85608579. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021a. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 38163830, Online. Association for Computational Linguistics. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021b. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 38163830. Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2022. Ppt: Pre-trained prompt tuning for few-shot learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84108423. Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: Word-level Adversarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 49214933, Online. Association for Computational Linguistics. Kanyao Han, Pingjing Yang, Shubhanshu Mishra, and Jana Diesner. 2020. Wikicssh: extracting computer science subject headings from wikipedia. In ADBIS, TPDL and EDA 2020 Common Workshops and Doctoral Consortium: International Workshops: DOING, MADEISD, SKG, BBIGAP, SIMPDA, AIMinScience 2020 and Doctoral Consortium, Lyon, France, August 2527, 2020, Proceedings 24, pages 207218. Springer. Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2022. Ptr: Prompt tuning with rules for text classification. AI Open, 3:182192. Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Juanzi Li, and Maosong Sun. 2021. Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification. arXiv preprint arXiv:2108.02035. Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. Advances in neural information processing systems, 34:1105411070. Saichethan Miriyala Reddy and Naveen Saini. 2021. Overview and insights from scope detection of the peer review articles shared tasks 2021. In PacificAsia Conference on Knowledge Discovery and Data Mining, pages 7378. Springer. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992. Mobashir Sadat and Cornelia Caragea. 2022. Scinli: corpus for natural language inference on scientific text. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73997409. Mayara Khadhraoui, Hatem Bellaaj, Mehdi Ben Ammar, Habib Hamam, and Mohamed Jmaiel. 2022. Survey of bert-base models for scientific text classification: Covid-19 case study. Applied Sciences, 12(6):2891. Timo Schick and Hinrich Schütze. 2020. Exploiting cloze questions for few shot text classification arXiv preprint and natural language inference. arXiv:2001.07676. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023a. Pretrain, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):135. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023b. Gpt understands, too. AI Open. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2orc: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 49694983. Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han. 2019. Weakly-supervised hierarchical text classification. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 68266833. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53165330, Dublin, Ireland. Association for Computational Linguistics. OpenAI. 2024. Chatgpt. https://openai.com/ chatgpt/. Accessed: 2024-05-20. Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023. Fine-tuning or retrieval? comparing knowledge injection in llms. arXiv preprint arXiv:2312.05934. Timo Schick and Hinrich Schütze. 2021a. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255269, Online. Association for Computational Linguistics. Timo Schick and Hinrich Schütze. 2021b. Its not just size that matters: Small language models are also fewshot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online. Association for Computational Linguistics. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 42224235, Online. Association for Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Han Wang, Canwen Xu, and Julian McAuley. 2022a. Automatic multi-label prompting: Simple and interpretable few-shot classification. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 54835492. Jianing Wang, Chengyu Wang, Fuli Luo, Chuanqi Tan, Minghui Qiu, Fei Yang, Qiuhui Shi, Songfang Huang, and Ming Gao. 2022b. Towards unified prompt tuning for few-shot text classification. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 524536. Zhiwen You, HaeJin Lee, Shubhanshu Mishra, Sullam Jeoung, Apratim Mishra, Jinseok Kim, and Jana Diesner. 2024a. Beyond binary gender labels: Revealing gender bias in LLMs through gender-neutral name In Proceedings of the 5th Workshop predictions. on Gender Bias in Natural Language Processing (GeBNLP), pages 255268, Bangkok, Thailand. Association for Computational Linguistics. Zhiwen You, Shruthan Radhakrishna, Shufan Ming, and Halil Kilicoglu. 2024b. UIUC_BioNLP at BioLaySumm: An extract-then-summarize approach augmented with Wikipedia knowledge for biomedical lay summarization. In Proceedings of the 23rd Workshop on Biomedical Natural Language Processing, pages 132143, Bangkok, Thailand. Association for Computational Linguistics. Haoxing Zhang, Xiaofeng Zhang, Haibo Huang, and Lei Yu. 2022. Prompt-based meta-learning for few-shot text classification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 13421357. Xuandong Zhao, Siqi Ouyang, Zhiguo Yu, Ming Wu, and Lei Li. 2023. Pre-trained language models can be fully zero-shot learners. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15590 15606. Datasets #Abstracts # Classes Avg. Length Test arXiv SDPRA 2021 S2ORC Emerging NLP 55300 28000 630 53 (sub) 7 19 21 155 136 227 5300 2800 420 Table 5: Datasets Statistics. #Abstracts represents the total number of labeled abstracts, including train and test sets. Emerging NLP dataset is for five-shot and zero-shot settings only."
        },
        {
            "title": "A Experimental Details",
            "content": "All models use the maximum input length of 256 tokens over 5 epochs, using the same hyperparameters as KPT (Hu et al., 2021), with learning rate of 3e-5 and batch size of 5. The experiments are performed on 32 GB Tesla V100 GPU. In few-shot setting, we apply the same backbone MLM for all experiments, with the exception of RetroPrompt (Chen et al., 2022a). RetroPrompt only supports RoBERTa-based models and requires at least two examples per class for model tuning. Therefore, we apply roberta-base as base model for RetroPrompt and only conduct experiments with more than five shots. The main distinction between SCIPROMPT and SCIPROMPTSoft lies in the verbalization, as discussed in Section 3.6. Unlike SCIPROMPT, which uses single label term projection, SCIPROMPTSoft employs vector-based mapping method to represent each filtered set of label terms. for zero-shot In zero-shot setting, we include ChatGPT10, open-sourced Llama 211, and the latest Llama 312 classification using the same instruction. For ChatGPT, we use gpt-3.5-turbo-instruct, which contains 175 million model parameters developed by OpenAI. We apply llama-2-70b-chat and meta-llama-3-70b-instruct as the backbone models for Llama 2 and Llama 3 respectively through the Replicate API13. We additionally investigate the classification performance of the Llama 2 models with 7B and 13B parameters under the zero-shot setting. However, their outputs are not coherent with the predefined class label sets and often include redundant information, making 10https://openai.com/chatgpt 11https://llama.meta.com/ 12https://ai.meta.com/blog/ meta-llama-3/ 13https://replicate.com/ the calculation of prediction accuracy unreliable. Therefore, we only conduct experiments of the Llama family on the 70B models."
        },
        {
            "title": "Topic Categories",
            "content": "We present more detailed introduction to datasets used for our experiments. SDPRA 2021 contains topics of scientific articles from the field of computer science, consisting of abstracts sourced from arXiv and categorized under one of seven predefined domain labels. We combined the training and validation sets, reallocating them into new training (90%) and validation (10%) sets. arXiv includes abstracts sourced from the arXiv website collected by Meng et al. (2019), categorized into 53 sub-categories and 3 parent categories (i.e., Math, Physics, and CS). We select 100 samples for each category as test set. S2ORC includes academic papers across 19 disciplines. We filter abstracts to those with single discipline label from the 2023-11-07 release through the Semantic Scholar Public API14. Emerging Topics of NLP encompasses 21 newly developed research fields within the broader category of Computation and Language15. We collect 30 examples for each topic, assigning five instances for training and another five for validation. The rest of the examples are used for testing. In our experiments, abstracts shorter than 30 tokens were excluded to remove invalid abstracts, leading to final training and test sizes of 25,110 and 2,790 for SDPRA, 49,300 and 5,300 for arXiv, 60,000 and 5,700 for S2ORC, and 210 and 420 for Emerging NLP. We used sub-categories for arXiv and parent categories for both SDPRA and S2ORC in text classification tasks. Detailed class labels for each dataset are presented in Table 11. We report parent and sub-categories of four datasets."
        },
        {
            "title": "Sizes",
            "content": "As presented in Table 6, we document the performance metrics across various verbalizer sizes following the configurations outlined in Figure 4. We report the mean accuracy for each setting. The findings indicate that the models performance is 14https://www.semanticscholar.org/ product/api 15https://arxiv.org/list/cs.CL/recent"
        },
        {
            "title": "Paradim",
            "content": "K=1 K=5 K=10 K=20 K=50 Avg. Zero-Shot SCIPROMPT (SDPRA) w/o FL w/ FL w/ CL 45.23 61.25 63.56 73.20 81.33 81.57 81.61 84.67 84.62 87.40 87.78 88.02 88.94 89.05 89.02 75.28 80.83 81. 25.56 34.98 51.40 SCIPROMPTSoft (SDPRA) w/o FL w/ FL w/ CL 55.00 65.53 64.92 80.62 83.43 81.78 83.84 85.26 85. 87.91 88.13 87.79 88.69 88.97 89.14 79.21 82.26 81.82 - - - SCIPROMPT (arXiv) w/o FL w/ FL w/ CL 29.77 38.36 38.70 50.13 54.76 55.19 59.57 62.25 62.48 65.77 66.54 66.70 69.55 69.86 69.73 54.96 58.35 58. 3.77 5.62 6.17 SCIPROMPTSoft (arXiv) w/o FL w/ FL w/ CL 37.50 42.22 41.49 57.66 57.72 58.36 63.63 63.53 63. 67.13 67.03 67.26 70.24 70.35 70.20 59.23 60.17 60.20 - - - SCIPROMPT (S2ORC) w/o FL w/ FL w/ CL 41.27 46.00 47.55 49.22 51.23 51.85 52.69 53.43 53.52 55.30 55.25 55.32 56.31 56.15 56.67 50.96 52.41 52. 25.25 26.11 40.79 SCIPROMPTSoft (S2ORC) w/o FL w/ FL w/ CL 42.35 46.33 46.34 50.10 50.24 51.09 51.89 52.83 53. 54.52 54.76 54.59 56.17 56.17 55.82 51.01 52.07 52.17 - - - Table 6: Performance comparison under various number of label terms in the verbalizer. We report the mean accuracy after five runs for each shot. enhanced across all scientific domain text classification datasets in both few-shot and zero-shot scenarios, attributable to implementing more sophisticated label term filtering techniques. Figure 4: Various numbers of label terms across four datasets under three phrases."
        },
        {
            "title": "D Calibration of Domain Knowledge",
            "content": "Figure 4 compares the verbalizer label term counts across datasets: Raw reflects the initial count after knowledge retrieval from two KBs (3.2); Filtered shows counts post-semantic filtering (3.4), reducing terms by 84%, 77%, and 85%; Calibrated involves removing low-likelihood terms before model tuning. Appendix and Table 6 reveal that NLI filtering and calibration enhance the models accuracy in few-shot and zero-shot settings, linking domain-relevant phrases in the verbalizer to improve the classification performance."
        },
        {
            "title": "E Overall Model Performance Analysis",
            "content": "We present an overview comparison of the results from Table 1 across all three datasets (i.e., SDPRA 2021, arXiv, and S2ORC) in Figure 5. Overall, SCIPROMPT exhibits the most stable performance compared to other baseline methods. Notably, SCIPROMPT consistently outperforms the state-of-the-art model KPT across all three datasets. In contrast, SCIPROMPTSoft demonstrates variability and inconsistency compared with SCIPROMPT while showing similar median accuracy. We exclude the RetroPrompt method from this comparison due to its inability to perform in the one-shot setting. Datasets Mce < 0.1 Mce < 0.3 Mce < 0.6 Mce < 0.9 Mce > 0.9 SDPRA arXiv S2ORC 495 3,384 1,182 501 3,477 1,216 514 3,553 1, 531 3,678 1,283 738 5,646 1,771 Table 7: The number of filtered label terms applying various thresholds. Cross-Encoder Mbe < 0.5 Mbe > 0.5 Mbe > 0.6 Mbe > 0.7 Mbe > 0.8 Mbe > 0.9 Mce < 0.1 64.185. 64.423.64 65.944.84 64.695.24 64.794.19 66.673.90 Table 8: Ablation study of SCIPROMPT in various Mbe values under the fixed Mce using the SDPRA 2021 dataset. setting through the SDPRA dataset. Our findings indicate that while Mbe > 0.9 yields the optimal performance, Mbe > 0.5 kept the lowest standard deviation  (Table 8)  . Consequently, we assume setting Mbe > 0.5 as the filtering threshold is more stable across different experimental conditions. Bi-Encoder Mce < 0.1 Mce < 0.5 Mce > 0.5 Mbe > 0.5 64.423.64 63.805.11 34.866. Table 9: Ablation study of SCIPROMPT in various Mce values under the fixed Mbe using the SDPRA 2021 dataset. To further validate our choices, we conducted experiments of SCIPROMPT with varying Mce values under the 1-shot setting using the SDPRA dataset while maintaining constant Mbe threshold of 0.5. Notably, performance consistently improved and the standard deviation is stable when Mce is set below 0.1  (Table 9)  . Therefore, we adopted Mce = 0.1 as the filtering threshold."
        },
        {
            "title": "G Prompt Templates of LLMs",
            "content": "Cloze-Based Prompt Template of MLM Abstract. The field of this study is related to: [MASK]. Above is the cloze-based prompt template we applied for all MLM prompt-based fine-tuning tasks. We also explored various prompt templates as introduced by (Hu et al., 2021; Gao et al., 2021a; You et al., 2024b) to evaluate performance variations using the SDPRA 2021 dataset, where the results are found to be similar. Note that our method focuses on improving domain-related verbalization Figure 5: Box chart for all methods in the few-shot setting over three datasets. Knowledge-Retrieval Threshold"
        },
        {
            "title": "Selection",
            "content": "As we introduced in Section 3.4, during the label term filtering stage, we employ bi-encoder for Mbe and cross-encoder for Mce calculation. In our experimentation, higher Mbe score indicates more notable similarity between the topic labels and the retrieved label terms, thus enhancing the relevance of the selected terms. Conversely, lower Mce score signifies higher relevance during the reranking stage. Our analysis of the SDPRA dataset reveals that Mce scores predominantly clustered above 0.9 and below 0.1. Consequently, the median value of Mce exerts minimal influence on the final Verbalization process. Even when reducing the threshold of Mce to 0.5, only marginal difference in the number of selected label terms across various Mce scores within the range of 0.1 to 0.9 is observed  (Table 7)  . We also explored the impact of different Mbe values under the fixed Mce score (0.1) to assess performance variations of SCIPROMPT in the 1-shot process rather than creating diverse prompts for model tuning. As detailed in Section 5.1, we used ChatGPT, Llama 2, and Llama 3 to perform the task of scientific text classification guided by specific instructions. The same instructions were applied to all LLMs to infer the topics from scientific abstracts. We employed distinct task-oriented (You et al., 2024a) prompt from that used with MLMs due to our observation that the original prompt from SCIPROMPT fails to yield relevant field names, given the LLMs limitations in comprehension. Consequently, we crafted more elaborate set of instructions to direct the LLMs in classifying topics, employing projection of pre-defined class names similar to those used in the verbalization."
        },
        {
            "title": "Instructions of LLMs",
            "content": "Based on the given articles abstract, please classify the abstract to specific field of study. Only select field words from the following field words provided. Only select one field name as output for each abstract. Your output should be all in lower cases.n Field Words List: logic in computer science, distributed computing, software engineering, data structures and algorithms, computational linguistics... Abstract: For the purpose of developing applications for Post-K at an early stage, RIKEN has developed post-K processor simulator. This simulator is based on the general-purpose processor simulator gem5...n Field of Study: The Field Words List represents the original class names in the dataset. We concatenate the above instructions to LLMs and extract the predictions that appear after Field of Study: to evaluate the classification performance."
        },
        {
            "title": "H Examples of Retrieved Label Terms",
            "content": "In Table 10, we report some cases of filtered label terms using the KBs we introduced in Section 3.2 through four datasets we apply for this work."
        },
        {
            "title": "Filtered Label Terms",
            "content": "arXiv"
        },
        {
            "title": "Group Theory",
            "content": "SDPRA"
        },
        {
            "title": "Cryptography",
            "content": "S2ORC"
        },
        {
            "title": "Psychology",
            "content": "Large Language Models (LLMs)"
        },
        {
            "title": "Emerging NLP",
            "content": "Recurrent Neural Networks (RNNs) document-oriented database, hierarchical database, database management system, object database, database application accelerator physics, particle accelerator, particle beam, velocity,accelerator symmetry group, group homomorphism, representation theory of finite groups, compact lie group cryptographers, secure communication, ciphertext, cryptanalytics, cryptographers, secure communication, data encryption standard political behavior, aspects, politics, elections, practical politics, american political science, constitutions, governing psychological science, mental condition, mental state, mental function, psychological state, psychological condition bert, semi-supervised learning, chain-of-thought prompting, encoding, lstm tensor, language modeling, generative model, feedforward neural networks, gated recurrent unit Table 10: Examples of filtered label terms in four datasets (3.4). Datasets Parent-category Sub-category Math (25) arXiv Physics (10) CS (18) SDPRA 2021 Computer Science (7) S2ORC engineering, chemistry, computer science, business, political science, environmental science, physics, economics, geography, medicine, psychology, art, materials science, mathematics, sociology, geology, philosophy, biology, history Emerging NLP Natural Language Processing (21) numerical analysis, algebraic geometry, functional analysis, number theory, complex variables, applied mathematics, general mathematics, logic, optimization and control, statistics, probability, differential geometry, combinatorics, operator algebras, representation theory, classical analysis, dynamical systems, group theory, quantum algebra, rings and algebras, symplectic geometry, algebraic topology, commutative algebra, geometric topology, metric geometry optics, fluid dynamics, atomic physics, instrumentation and detectors, accelerator physics, general physics, plasma physics, chemical physics, sociophysics, classical physics computer vision, game theory, information theory, machine learning, distributed computing, cryptography, networking and internet architecture, computational linguistics, computational complexity, software engineering, artificial intelligence, systems and control, logic in computer science, cryptography and security, data structures and algorithms, programming languages, other computer science, databases logic in computer science, distributed computing, software engineering, data structures and algorithms, computational linguistics, networking and internet architecture, cryptography - sign language and fingerspelling recognition, rule-based machine translation (RBMT), transformer models, prompt engineering recurrent neural networks (RNNs), large language models (LLMs), bilingual lexicon induction (BLI), hate and offensive speech detection, email spam and phishing detection, fake news detection, fake review detection, aspect-based sentiment analysis (ABSA), dialogue state tracking (DST), visual question answering (VQA), open-domain question answering, multiple choice question answering (MCQA), nlp for for social media, nlp for the legal domain, acronyms and abbreviations detection and expansion, paraphrase and rephrase generation, named entity recognition for nested entities Table 11: Detailed topic categories of four datasets. Note we classify sub-categories for arXiv, SRPRA 2021, and Emerging NLP datasets."
        }
    ],
    "affiliations": [
        "Technical University of Munich",
        "University of Illinois Urbana-Champaign",
        "University of Washington"
    ]
}