{
    "paper_title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
    "authors": [
        "Enshen Zhou",
        "Cheng Chi",
        "Yibo Li",
        "Jingkun An",
        "Jiayuan Zhang",
        "Shanyu Rong",
        "Yi Han",
        "Yuheng Ji",
        "Mengzhen Liu",
        "Pengwei Wang",
        "Zhongyuan Wang",
        "Lu Sheng",
        "Shanghang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 0 6 6 3 1 . 2 1 5 2 : r RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics Enshen Zhou1,3*, Cheng Chi3* , Yibo Li1*, Jingkun An1*, Jiayuan Zhang1, Shanyu Rong2, 3, Yi Han1, 3, Yuheng Ji3, 4, Mengzhen Liu2, Lu Sheng1, Pengwei Wang3, Shanghang Zhang2,3 1School of Software, Beihang University 2State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University 3Beijing Academy of Artificial Intelligence 4CASIA Zhongyuan Wang3, {zhouenshen,leeibo,lsheng}@buaa.edu.cn chicheng15@mails.ucas.ac.cn anjingkun02@gmail.com shanghang@pku.edu.cn Figure 1. Spatial tracing is pivotal for embodied robots to translate the spatially constrained instructions (e.g., Water flowers from left to right with watering can hovering 1-5 cm above each flower) into 3D positional sequence (i.e., spatial traces) in complex 3D scenes. This task demands (a) 3D spatial referring to resolve spatial relations and locate relevant objects involved in the trace, and (b) 3D spatial measuring to understand absolute, real-world metric quantities related to the trace. For example, (a) 3D positions of the watering can and each flower pot are localized from left to right, and (b) their corresponding heights in meters are measured. By performing multi-step, metric-grounded reasoning over the key information above, the generated spatial trace can support not only (c) multi-step manipulation, but also (d) collision-free motion, thereby (e) enabling efficient control of diverse robots (e.g., G1 humanoid) across tasks in cluttered scenes."
        },
        {
            "title": "Abstract",
            "content": "Spatial tracing, as fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, 3D-aware VLM that first achieves both 3D spatial referring and measuring via universal spatial encoder and regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues Equal contribution Corresponding author Project leader to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes. See the project page at https://zhoues.github.io/RoboTracer. 1. Introduction Embodied robots usually have to execute actions based on increasingly complex, spatially constrained instructions [1, 87, 89, 129], such as Water flowers from left to right with watering can hovering 15 cm above each one in Fig. 1, where recent data-scarce Vision-Language-Action (VLA) models fail to master. In this case, it would be beneficial to generate 3D positional sequence, named as spatial trace, as an intuitive bridge to interpret the instruction following procedure in 3D space and guide the generation of actual action trajectories for robots. However, this surrogate task (i.e., spatial tracing) is inherently challenging as it requires multi-step, metric-grounded reasoning in complex 3D scenes. To be specific, each reasoning step requires two key components: (1) 3D spatial referring to resolve spatial relationships and accurately localize objects involved in the trace generation (e.g., identifying flowers with their from left to right order and locating them). (2) 3D spatial measuring to understand absolute, real-world metric quantities related to the trace in captured scene (e.g., quantifying each flowers physical height and 15 cm height above each). While recent Vision-Language Models (VLMs) [4, 48, 60] can perform 2D spatial reasoning [9, 17, 112, 129] and even 2D visual trace (i.e., 2D positional sequence) generation [33, 44, 114], they overlook the multi-step nature of this task, particularly the crucial participation of intermediate objects involved in the trace, resulting in suboptimal generation. Moreover, their outputs are mainly in 2D space (e.g., pixel coordinates), lacking 3D space grounding and absolute metric understanding, creating fundamental chasm between 2D visual trace and 3D spatial trace. To this end, we propose RoboTracer, 3D-aware reasoning VLM that not only acquires precise 3D spatial referring and measuring via Supervised Fine-tuning (SFT), but also exhibits multi-step metric-grounded reasoning capabilities for spatial tracing via reinforcement fine-tuning (RFT). The core of our approach is to introduce set of metric-sensitive reward functions (e.g., referring, measuring, scale) during RFT. These rewards supervise the key perceptual objects involved in the trace and offer crucial intermediate evidence for accurate spatial trace generation. In addition, as 3D spatial referring and measuring require better metric-grounded understanding, we introduce scale decoder for VLM, supervised by regression loss on the predicted metric scale factor to enhance metric perception, even from RGB inputs. Moreover, we incorporate universal spatial encoder with the architectural flexibility to integrate diverse geometric configurations (e.g., camera intrinsics, absolute depth) and further improve the precision of spatial trace generation. To support RoboTracers training, we present TraceSpatial , large-scale dataset with 4.5M high-quality examples and 30M QA pairs to first learn 3D spatial referring and measuring in SFT, and further achieve spatial tracing with compositional reasoning on both in RFT. It spans diverse indoor/outdoor/tabletop scenes with fine-grained annotations (e.g., precise geometry, object-level spatial referents) and contains greatly higher proportion (48.2%) of absolutescale data (14x prior [84]) for metric-grounded understanding. To advance multi-step reasoning capabilities, it provides step-wise annotations of the reasoning process (up to 9 steps). Moreover, it has object-/end-effector-centric spatial traces spanning 3 single-/dual-arm robot configurations, enhancing generalization and real-world applicability. We evaluate RoboTracer on spatial understanding/measuring/referring benchmarks, achieving SOTA average success rate of 79.1%, exceeding Gemini-2.5-Pro by 11%. It also outperforms all baselines on 2D visual trace benchmarks. To address the lack of benchmarks for spatial tracing, we introduce TraceSpatial-Bench , which contains 100 real-world images with manually annotated tasks involving object localization/movement/placement. Each sample requires metric-grounded, multi-step reasoning (up to 8 steps), with precise start-point masks, end-point 3D bounding boxes, precise geometry annotation, and diverse metrics for fine-grained evaluation. RoboTracer still achieves best performance, surpassing Gemini-2.5-Pro by 36%. Moreover, in Fig. 1 and Sec. 4.5, RoboTracer can execute long-horizon, dynamic tasks in cluttered real-world scenes, showing strong generalization across robots (e.g., UR5, G1 humanoid). Our contributions are summarized as follows: We propose RoboTracer, 3D-aware VLM that accepts arbitrary geometric inputs, uses scale supervision, guided by metric-sensitive rewards to achieve spatial tracing. We construct TraceSpatial , well-annotated dataset tailored for spatial tracing with reasoning, and TraceSpatialBench , benchmark that fills the gap in evaluating it. Extensive experiments show that RoboTracer surpasses baselines in spatial measuring, referring, tracing, and efficiently controls diverse robots across tasks in real world. 2. Related work Spatial Reasoning with VLMs. Spatial reasoning refers to the ability to perceive and reason about 3D space, comprising metric-agnostic and -grounded types. Metric-agnostic reasoning [9, 17, 23, 34, 64, 66, 83, 96, 97, 104106, 108, 129] captures object-centric properties (e.g., position, orientation) and inter-object relations (e.g., left/right, near/- far), whereas metric-grounded reasoning involves precise, absolute-scale measurements [7, 10, 84, 100, 102] (e.g., distance, depth, size) in physical world. Compared to metricagnostic one, metric-grounded reasoning requires better 3D understanding. Despite using 3D modalities, either explicitly [7, 10, 18, 58, 69, 129] (e.g., point clouds, depth maps) or implicitly [16, 26, 30, 99, 124] (e.g., VGGT [94]), existing methods still struggle to reason complex absolutescal scenes for 3D spatial referring and measuring. We 2 Figure 2. Overview of RoboTracer. RoboTracer can process RGB images and task instructions, while flexibly integrating various geometric configurations (e.g., absolute depth, camera intrinsics) when available to improve spatial precision, enabled by the integrated universal spatial encoder. It also has scale decoder to output metric scale factor supervised by regression loss beyond next-token prediction to bolster real-world scale awareness. After SFT, metric-sensitive reward functions in RFT further supervise the key perceptual objects involved in the trace and offer crucial intermediate evidence (e.g., 3D spatial referring and measuring) for accurate spatial trace generation. thus propose 3D-aware VLM that uses scale supervision and accepts arbitrary geometric inputs to address this gap. Trace Generation with VLMs for Robotics. Trace enhances manipulation by capturing the spatio-temporal dynamics of objects. Recent advances in VLMs [4, 14, 15, 50, 52, 73, 74, 127] focus on predicting 2D visual traces (i.e., 2D point sequence) and guiding robotic actions in two ways. (1) Lift-to-3D [33, 87, 103, 114, 128]: Projects 2D traces into 3D spatial trace using depth maps and camera (2) Overlap-on-2D [42, 44, 125]: Renders the intrinsics. 2D trace onto the image by steering the control policies for action generation. However, 2D traces struggle to fully capture object dynamics in 3D space. Moreover, existing works for 2D visual trace struggle to handle complex spatially constrained instructions and supervise the key perception steps, also required by multi-step 3D spatial tracing, largely due to the lack of datasets. We thus propose new dataset and benchmark tailored for spatial tracing. Reinforcement Fine-tuning for VLMs. Reinforcement Fine-tuning (RFT) [5, 76, 80] is post-training strategy that aligns models with human preferences [3] or specific goals via feedback, complementing SFT [98, 126], which adapts pre-trained models using task-oriented data. Recent advances in VLMs use RFT to improve visual reasoning [35, 65, 86, 107, 109, 119, 120], grounding [61, 68, 81, 118, 129], segmentation [59], tool-use [12, 29], and 2D visual trace prediction [31, 59, 114]. However, most approaches remain confined to 2D-relative perception and outcome-based reward, limiting their performance on spatial tracing tasks requiring 3D multi-step metric-grounded reasoning. We thus design metric-sensitive process rewards to guide key perception steps to meet above expectations. 3. Method 3.1. Problem Formulation We formulate spatial tracing as predicting an ordered sequence of 3D points τ = {pt}T t=1 from visual inputs (e.g., RGB, RGB-D), optional camera geometry (e.g., intrinsics) and textual instruction via VLM. Each point pt = (ut, vt, dt) comprises image-plane coordinates (ut, vt) and absolute depth dt. The resulting trace τ serves as spatial plan for entities (e.g., robot end-effector or object), ensuring motion that follows the instruction. Notably, the instruction encodes both 3D spatial referring and measuring, often requiring multi-step compositional reasoning. For example, in Fig. 2, to accomplish the task of Water flowers from left to right with watering can hovering 1-5 cm above each flower, requires inferring the 3D positions and heights of all flowers in 3D scenes. While these spatial cues, especially the points inferred through 3D spatial referring, may not correspond to or used by the keypoints of the final spatial trace, they provide crucial intermediate evidence for multi-step reasoning, enabling accurate trace generation under spatial constraints at the start, end, and along the path. 3 Figure 3. Data Construction Pipeline. TraceSpatial has 4.5M data samples from 2D/3D/video sources, covering outdoor/indoor/table scenes. It contains not only metric-agnostic QA pairs, but also metric-grounded QA pairs for 3D spatial referring/measuring/tracing. Instead of predicting 3D (x, y, z) coordinates directly, we adopt decoupled (u, v, d) formulation that is trivially convertible to 3D via camera intrinsics. This circumvents the need for VLMs to implicitly learn camera geometry, thereby simplifying training and improving accuracy. Moreover, (u, v, d) points easily project into lowerdimensional spaces: omitting yields 2D visual traces, and retaining only start/end points yields 3D or 2D spatial referring data. This formulation enhances data reusability, aligns seamlessly with existing 2D datasets [24, 42, 71] for cotraining, thus bolstering multi-task learning performance. 3.2. RoboTracer VLM Architecture. Spatial tracing requires metricgrounded 3D understanding for referring and measuring, yet simply fine-tuning existing VLMs encounters two hurdles: (1) insufficient absolute-scale supervision, especially with RGB-only data; (2) underuse of absolute-scale geometric cues (e.g., camera intrinsics, depth) to enhance precision. In Fig. 2, we address these issues by introducing scale decoder and universal spatial encoder, each aligned with the LLM via projectors, akin to the existing RGB encoder. The scale decoder maps the <SCALE> token embedding into numeric factor, linking scale-invariant representations to absolute metric scales. Rather than classification loss, we use regression loss to supervise it to heighten realworld 3D scale awareness. Moreover, we find that leveraging strong priors from powerful feed-forward metric 3D geometry model [36] greatly enhances spatial and scale understanding. Building on this model, our universal spatial encoder flexibly integrates optional geometric cues (e.g., camera intrinsics, poses, depth) to refine spatial representations as more geometry becomes available. This modular design enables: (1) Flexible training, leveraging diverse scale-aware annotations in datasets via flexible input augmentation to enrich spatial learning; (2) Geometry-adaptive inference, integrating available geometric cues without retraining or architectural changes. See Supp. C.1 for details. Supervised Fine-tuning. Since general VLMs 2D-only pretraining limits 3D metric-grounded understanding, we propose two-step SFT. (1) Metric Alignment. In Fig. 2, we align the spatial encoder and scale decoder with LLM by using geometric annotations (e.g., depth, scale) from the TraceSpatial dataset (see Sec. 3.3). During this stage, only the projector and scale decoder are updated. (2) Metric Enhancement. We freeze the spatial encoder and finetune all other components on TraceSpatial and additional instruction-following datasets [49, 51, 110]. Crucially, we train on both RGB-only and RGB+X inputs, where indicates arbitrary combinations of geometric annotations. This preserves image encoders general VQA ability while flexibly accommodating diverse geometric configurations without retraining during inference. The SFT loss is defined as: = Lntp + 0.1 log(ˆs) stopgrad(log(s)2 2 (1) where Lntp is next-token prediction loss, log(ˆs) is predicted scale in logarithmic space, is ground-truth scale. Moreover, TraceSpatial contains multi-step data with reasoning processes, providing cold start for subsequent RFT stage. This SFT-trained model thus exhibits improved abil4 ity for multi-step spatial tracing. See Supp. C.3 for details. Reinforcement Fine-tuning. We use RFT after SFT to improve compositional metric-grounded reasoning using GRPO [80] with multi-step reasoning data from TraceSpatial . We first define outcome-based rewards: (1) Outcome Format Reward (ROF) for structured outputs; (2) Point Reward (RP ) for start/end point consistency between the predicted trajectory (p1, pT ) in ˆτ = { ˆpt}T t=1 vs. τ . Formally, (cid:2)f (p1, ˆp1) + (pT , ˆpT )(cid:3),f (p, p) = max(cid:0)0, 1 RP = 1 2 (cid:1). (3) Trace Reward (RT) for trajectory-level alignpp2 2 ment using distance metric d(τ, ˆτ ): RT = max(cid:0)0, 1 d(τ, ˆτ )(cid:1). All (u, v, d) values are normalized to [0, 1], with depth scaled by the scenes maximum depth. However, the outcome-based rewards described above are metricagnostic and provide no supervision over the key perceptual objects involved in trace generation (e.g., 3D spatial measuring and referring). To address this, we introduce metric-sensitive process rewards that leverage key-step per- (1) Process Forception annotations from TraceSpatial : mat Reward (RPF), enforcing the format [Perception Type] [Target Object]:; (2) Accuracy Reward (RAcc), which applies to steps included in the key-step perception annotations. For each relevant step, we measure the prediction error using specific metric, according to the perception type (e.g., L1 distance for referring). Notably, this design is order-invariant, allowing flexible step ordering. The final reward sums both outcome-/process-based terms, with process-based terms scaled by 0.25. Fig. 2 shows that RFTtrained model generalizes well to 7-step spatial tracing, progressively resolving complex spatial relations and producing accurate trace. Please check Supp. C.4 for more details. 3.3. TraceSpatial Dataset Overview. Key features are: (1) Rich Diversity. TraceSpatial spans outdoor/indoor/tabletop scenes (Fig. 3 (a)) and includes both object-/end-effector-centric spatial traces; the latter captures gripper motions across 3 single-/dualarm robot configurations, fostering model generalization in open-world scenarios. (2) Multi-Dimensionality. Beyond metric-agnostic spatial concepts and relations, the dataset includes 48.2% metric-grouded QA (Fig. 3 (b)). These samples cover 3D spatial measuring/referring, and support multi-step spatial tracing by providing detailed annotations of reasoning process, addressing limitations in existing datasets [33, 114]. (3) Large Scale. With 4.5M samples and 30M QA pairs, TraceSpatial is the largest dataset for 3D spatial reasoning, supporting bottom-up spatial tracing learning. (4) Fine-Grained. Hierarchical object captions, from coarse categories (e.g., flower) to fine-grained spatial referents (e.g., the first flower from the left), enable 3D spatial measuring/referring/tracing in cluttered scenes. Absolute-scale geometry (e.g., intrinsics, depth) further enriches spatial learning and flexible input augmentation. (5) High Quality. Rigorous filtering preserves spatial relevance. From 1.7M OpenImages [40], 466k images remain; from CA-1M [41](2M)/ScanNet [22](190k), 100k/12k frames are retained based on text-identifiable 3D boxes. From DROID [38](116k)/AgiBot [21](167k) videos, 20k/59k episodes are preserved after verifying valid camera poses, coherent tasks, and clean trajectories. (6) Easy Scalability. Our pipeline scalably integrates 2D images, 3D scans with bounding boxes, and manipulation videos to support 3D spatial referring/measuring/tracing. Data Recipe. In Fig. 3, we propose data pipeline that progressively integrates 2D/3D/video sources for general VLMs to perform 3D spatial referring/measuring/tracing. (1) 2D Web Images aim to provide basic spatial concepts and broad-scale perception across indoor and outdoor scenes. We filter 1.7M images from OpenImage [40] down to 466K and employ VLM [4] with hierarchical regioncaptioning to produce fine-grained spatial references, surpassing previous approaches [23, 83]. Object captions serve as nodes for 3D scene graphs, where edges depict spatial relations inferred via object detection, depth, and camera estimation. Using template-/LLM-based methods, we generate metric-agnostic and 3D spatial QA grounded in these captions. (2) 3D Scanning Datasets want to arm the model with focused metric-grounded spatial reasoning of indoor scenes. We thus leverage the richly annotated CA-1M [41] and ScanNet [22]. After fine-grained filtering, we construct 3D scene graphs with more diverse spatial relations, enabled by precise 3D bounding boxes compared to 2D approaches. Moreover, we generate 3D occupancy maps that encode positions, orientations, and metric distances (e.g., 35cm right of the toy) for accurate object-centric spatial trace generation. (3) Manipulation Videos provide spatial traces aligned with the embodied manipulation in tabletop settings. While 3D scans enable object-centric tracing, they lack physically plausible manipulations. Hence, we curate real and simulated [13] tabletop videos (from 167k to 59k for AgiBot [21], and from 116k to 24k for DROID [38]) with calibrated cameras, accurate task execution, and clear trajectories. We further leverage VLM [4] to decompose these tasks into subgoals, enabling precise multi-step spatial tracing for single-/dual-arm across 3 robot configurations. Notably, as 3D datasets and simulation videos are all-seeing, we construct multi-step, metric-grounded spatial tracing data, under the assumption that the generated code reflects optimal reasoning, with each line translated into textual form and intermediate results filled into structured formats (e.g., coordinates, distances). See Supp. for details. 4. Experiments Model Configuration. We adopt NVILA [60] (2B/8B) as base model and apply SFT to obtain RoboTracer-SFT. Due to computational limits, we only perform RFT on 2B model, 5 Table 1. Performance on spatial understanding/measuring benchmarks. Top-1/-2 success rate (%) are indicated by bold/underlined text. Method GPT-4o [2] Gemini-2.5-Pro [20] NVILA-2B [60] NVILA-8B [60] Qwen-3-VL-4B [90] Qwen-3-VL-8B [90] Input RGB RGB RGB RGB RGB RGB RGB Molmo 7B-D [24] SpaceVLM-13B [9] RGB RoboBrain 2.0-7B [87] RGB SpatialBot-3B [7] RGB-D RoboTracer-2B-SFT RoboTracer-8B-SFT RGB RGB CV-Bench [92] BLINKval [28] 2D-Relation 3D-Depth 3D-Distance 2D-Relation 3D-Depth RoboSpatial [83] EmbSpatial [25] Q-spatial [46] Plus Scannet Scale Est. Refer Est. MSMU [11] Spatial Understanding Spatial Measuring 84.62 93. 70.15 91.54 92.31 93.85 87.69 63.69 96.00 69.38 96.62 97.08 86.50 91.00 79.67 91.83 94.67 94.50 66.00 66.83 94.83 77. 96.00 97.17 83.33 90.67 60.00 90.67 87.50 90.33 61.83 70.17 90.00 60.83 89.83 93.50 82.52 91. 67.83 76.92 87.71 87.41 59.44 72.73 79.72 67.83 83.22 91.61 78.23 87.90 62.10 76.61 85.48 85.48 77.42 62.90 85.48 67. 91.94 92.74 77.20 77.24 51.79 59.35 79.67 77.24 58.60 61.00 74.80 72.36 82.93 83.74 63.38 76. 47.34 67.72 77.01 77.86 58.74 49.40 74.78 50.66 70.66 81.75 31.68 56.44 36.90 46.87 53.47 29.70 51.49 25.74 44.55 20. 68.32 73.27 37.06 70.00 40.59 44.71 70.00 56.47 63.53 45.29 55.88 29.41 70.59 78.82 3.86 64. 40.15 33.98 63.71 63.32 59.85 32.42 69.11 16.60 78.38 83.01 2.09 48.42 37.37 38.95 42.11 52.63 43.68 31.58 48.42 22. 60.00 70.00 Table 2. Performance on 2D spatial referring benchmarks. L. and P. denote Location and Placement parts of RefSpatial-Bench. Top1/-2 success rate (%) are indicated by bold/underlined text. Table 3. Performance on visual trace benchmark. DFD/HD/RMSE are Discrete Frechet Distance, Hausdorff Distance, Root Mean Square Error. Top-1/-2 scores are shown by bold/underlined text. Model Where2Place RoboSpatial RefSpatial-L. RefSpatial-P. Gemini-2.5-Pro [20] Qwen3-VL-8B [90] RoboPoint [112] Molmo-7B [24] Molmo-72B [24] RoboBrain 2.0-7B [87] RoboTracer-2B-SFT RoboTracer-8B-SFT 61.90 64.00 46.80 45.00 63.80 63.59 63.00 69.00 40.20 61. 41.30 38.00 40.90 54.87 62.30 66.40 46.96 51.00 22.87 21.91 45.77 36.00 49.00 55.00 24.21 42. 9.27 12.85 14.74 29.00 45.00 53.00 yielding RoboTracer-RFT. Since the model can accept arbitrary geometric inputs, it defaults to using only images when input types are unspecified. See Supp. for details. 4.1. Spatial Understanding and Measuring We evaluate our method on public spatial understanding benchmarks, including CV-Bench [92], BLINK [28] (validation set), RoboSpatial [83] (configuration subset), and EmbSpatial [25]. In addition, we also assess spatial measuring performance on Q-Spatial [46] and MSMU [11]. Check Supp. D.2/D.3 for details. We present our analyses below. SFT learns strong spatial understanding and metric measuring. In Tab. 1, RoboTracer-8B-SFT trained solely on TraceSpatial surpasses all baselines with an average success rate of 85.7%, even outperforming Gemini-2.5-Pro by 8.58% (absolute) and base model NVILA-8B by 20.3% (absolute). Notably, we find that its improvements are more pronounced in 3D-related and measurement tasks compared to 2D tasks (23.6% vs.14.7%), underscoring that our architectural design and curated dataset during the SFT stage enhance the models 3D spatial and scale awareness. 4.2. 2D Spatial Referring and Visual Trace We evaluate 2D spatial referring (Where2Place [112], RoboSpatial [83], RefSpatial-Bench [129]) and visual trace benchmarks, including ShareRobot-Bench [33] (endeffector-centric), VABench-V [113]) (object-centric). See Model ShareRobot-Bench VABench-V DFD HD RMSE DFD HD RMSE Qwen3-VL-4B [90] Qwen3-VL-8B [90] 0.3808 0.3294 0.3922 0.3411 0.7764 0.7764 MolmoAct [42] 0.4365 0.3919 HAMSTER [44] RoboBrain 2.0-7B [87] 0.1669 0.1575 Embodied-R1-3B [115] 0.3426 0.3002 RoboTracer-2B-SFT RoboTracer-8B-SFT 0.1605 0.1544 0.1449 0.1384 0.2204 0.2328 0.6771 0.3554 0.1250 0. 0.1114 0.0966 0.2792 0.2528 0.2741 0.2549 0.8136 0.8136 0.2124 0.2045 0.3289 0.2604 0.3028 0.2588 0.1664 0.1551 0.1494 0.1367 0.2037 0.2021 0.6877 0.1825 0.2237 0. 0.1237 0.1065 Supp. D.4/D.5 for details. We present our analyses below. Decoupled formulation makes multi-task learning easy. RoboTracer achieves best performance on both 2D spatial referring (see Tab. 2) and visual trace (see Tab. 3) benchmarks. While TraceSpatial focuses on 3D metric-grounded data (e.g., 3D spatial referring and tracing) without explicitly targeting 2D tasks, our decoupled point formulation enables seamless projection of TraceSpatial into the data formats required for such 2D tasks (see Sec. 3.1). Moreover, in Tab. 8 (ID & H), modeling (u, v, d) surpasses direct 3D (x, y, z) modeling. We attribute this to (1) our dataset reuse via dimensionality reduction at various levels (e.g., 3D to 2D, point sequences to individual points), (2) stronger alignment with existing 2D datasets for co-training, thereby improving multi-task learning performance. 4.3. Multi-Step Spatial Tracing To evaluate complex multi-step, metric-grounded spatial tracing, we introduce TraceSpatial-Bench , challenging real-world benchmark focusing on cluttered indoor/tabletop scenes. The dataset comprises 100 images with precise geometric annotations (e.g., camera geometry, absolute depth, and 3D occupancy). Each sample requires 38 reasoning steps, specifying both the manipulated objects start mask and its end 3D box. We assess performance in 2D and 3D; trajectory succeeds if its start and end positions are corFigure 4. TraceSpatial-Bench Results. White masks denote ground-truth 3D starting region; pink 3D boxes mark correct end regions. Despite similar 2D projections (left views of each case), our model yields more accurate spatial traces than strong general VLMs, which often produce floating or colliding traces due to inaccurate depth estimation. Leveraging richer geometric cues further improves performance. Table 4. Performance on TraceSpatial-Bench . R., I., D., P.R. denote RGB image, intrinsics, absolute depth and process reward. Model Input P.R. 2D Start 2D End 3D Start 3D End Overall Table 6. Performance on RoboTwin hard tasks. We report the success rate (%) compared to end-to-end and VLM-based models. Gray rows indicate unseen tasks not present in TraceSpatial . Gemini-2.5-Pro [20] Qwen3-VL-4B [90] Qwen3-VL-8B [90] RGB RGB RGB MolmoAct [42] RGB RoboBrain 2.0-7B [87] RGB Embodied-R1-3B [115] RGB RoboTracer-2B-SFT RoboTracer-2B-SFT RGB R.I.D. RoboTracer-2B-RFT RGB - - - - - - - - RoboTracer-2B-RFT RoboTracer-2B-RFT RoboTracer-2B-RFT RGB R.I. R.I.D. 31 64 60 4 46 63 56 59 61 63 64 69 33 16 15 23 13 44 45 46 47 48 53 9 43 47 - - - 63 75 64 73 73 78 16 24 22 - - - 52 54 55 56 61 3 6 8 - - - 31 39 39 40 45 Table 5. Performance on public general VLM benchmarks. Model MMEtest MMBenchdev OK-VQA POPE NVILA-2B [60] RoboTracer-2B-SFT 1547 1751 78.63 77.62 64.9 65.22 81.96 82. rect and all intermediate paths remain collision-free. See Supp. D.6 for details. We present our analyses below. RFT enables multi-step metric-guided reasoning and generalization. In Tab. 4, RoboTracer-RFT outperforms all baselines on 3D metrics, exceeding Gemini-2.5-Pro by 36% and 2B-SFT by 8%. We find that while these VLM performs well on 2D referring and tracing, they fall short in 3D spatial tracing due to their limited metric depth understanding, often producing traces that float or collide with objects. In contrast, RoboTracer-RFT leverages learned 3D spatial knowledge (e.g., 3D spatial referring and measuring) and compositional reasoning to generate more accurate spatial traces. Fig. 4 further shows complex spatial tracing cases from TraceSpatial-Bench with model comparisons. Accurate geometry refines metric-guided reasoning. In Tab. 4, using more precise geometric information greatly Task Place A2B Left Move Playingcard Away Click Alarmclock Place Burger Fries Place Container Plate Stack Blocks Two Place Empty Cup Place Object Stand End-to-End Policy Vision-Language Model Ours ACT DP DP3 RDT π0 Qwen3-VL-8B Gemini-2.5-Pro RoboTracer-2B 0 0 4 0 1 0 0 0 0 0 5 0 0 0 0 0 2 3 14 1 0 1 0 3.7 0.6 2.5 1 11 12 27 17 2 7 5 6.3 4.7 5.7 1 22 11 45 1 11 11 6.6 11.6 8.6 0 0 0 0 0 0 0 0 0 0 0 2 5 0 0 0 0 0 0.7 0.3 0.5 84 94 79 99 52 33 85 38 75.4 44.4 64.0 Seen Avg. Success Rate Unseen Avg. Success Rate Total Avg. Success Rate 0.9 0.1 0.6 0.4 0 0.2 improves performance, yield up to 6% absolute gain in success rate. This suggests that explicit, readily available geometry in embodied settings can further improve metric reasoning, rather than relying solely on implicit learning of VLM to understand them via RGB-only input. Moreover, our model supports more geometry input to enhance itself without retraining, broadening its applicability. Fig. 4 shows the generated traces with different geometric inputs. 4.4. Public vision-language Benchmarks Joint training preserves common-sense knowledge. In Tab. 5, our model performs on par with or slightly surpasses base model. This benefit stems from our use of TraceSpatial for RGB and RGB+X (See Sec. 3.2) joint training, enriched by general visual instruction datasets, which enhance 3D spatial reasoning without compromising general abilities. 4.5. Simulator and Real-world Evaluation RoboTracer integrates as practical tool. In Tab. 6, we evaluate our model on 19 Robotwin 2.0 [13] hard tasks involving clustered scenes. Among them, 12 tasks are seen and 7 are unseen. Unlike task-specific end-to-end baselines trained on each task, our model, without any taskFigure 5. Real-World Evaluation. The blue trace denotes predicted spatial trace in 2D, and the blue dot marks the current target. RoboTracer can generate collision-free spatial traces whose start and end points all satisfy spatial constraints in cluttered and dynamic scenes. Table 7. Real-world robot evaluation with spatial tracing. Tasks with Spatial Tracing Success Rate(%) OpenVLA RoboRefer Ours Pick the rightmost hamburger and place it on the keyboard in front of the laptop without collisions Water flowers from right to left with watering can hovering 1-5 cm above each flower. 0. 0.00 0.00 60.00 0.00 30.00 Table 8. Ablation Study on data recipe, spatial encoder, scale supervision (superv.) with regression (Reg.) and next-token prediction (N.T.P) loss, point formulation (form.) in Q-Spatial, ShareRobotBench(S.R.B.) and TraceSpatial-Bench (T.S.B). We report success rate (SR)/Discrete Frechet Distance (DFD) of 2B model. Data Recipe Spatial Scale Point Model Q-Spatial S.R.B T.S.B. ID specific training, outperforms the best-performing baseline by 32.8% in unseen tasks, showing strong generalization as tool. Moreover, current powerful VLMs struggle to generate spatial traces, highlighting the importance of our model design and dataset contribution. See Supp. D.7 for details. Spatial tracing is critical for real-world robots. In Tab. 7, only our method can handle long-horizon tasks requiring multi-step, metric-grounded spatial tracing in cluttered, dynamic scenes. These tasks demand precise identification and placement of objects under evolving spatial constraints, while continuously avoiding collisions. In Fig. 5, integrating RoboTracer with an open-loop policy enables rapid updates at 1.5 Hz. Thus, when the rightmost hamburger is moved, the robot adapts by grasping the newly rightmost one, reaching over the large doll and the computer screen to place it on the keyboard. Notably, this embodimentagnostic spatial tracing can also be executed by G1 humanoid, enabling even more complex, long-horizon tasks such as flower-watering  (Fig. 1)  . See Supp. D.8 for details. 4.6. Ablation Study Data recipe is crucial for SFT training. Tab. 8 shows that combining 2D, 3D, and video data yields optimal performance. 2D/3D data offer critical scale supervision for both indoor and outdoor scenes; their absence degrades metricgrounded Q-spatial [46] accuracy. Video data enables gripper motion learning across diverse robot configurations, improving end-effector-centric performance on ShareRobotBench [33]. Moreover, 3D data and simulated videos increase spatial instruction diversity and metric-grounded reasoning, crucial for TraceSpatial-Bench . This three-way data synergy is thus key to effective SFT training. Universal spatial encoder improves 3D reasoning. We fine-tune NVILA-2B [60] on TraceSpatial without universal spatial encoder, followed by RFT. In Tab. 8 (ID & H), 2D 3D Video Encoder Superv. Form. Type SR(%) DFD SR (%) Reg. Reg. Reg. (u, v, d) SFT (u, v, d) SFT (u, v, d) SFT 51.49 33.52 63. 0.1723 0.1809 0.4376 Reg. (u, v, d) RFT - - (u, v, d) SFT N.T.P. (u, v, d) SFT 53.47 57.43 0.1693 0.1671 Reg. (x, y, z) SFT 68.32 0.2426 Reg. Reg. (u, v, d) SFT (u, v, d) RFT 69.61 - 0.1605 - 27 19 24 22 24 26 30 31 39 we find that the spatial encoder enhances metric-grounded reasoning and greatly improves multi-step spatial tracing. This is mainly due to: (1) encoders prior 3D knowledge that facilitates implicit 3D learning, (2) cumulative reasoning across steps, amplifying the utility of spatial cues. Scale regression supervision boosts metric awareness. In Tab. 8 (ID & & H), we compare supervising the metric scale factor using regression loss, next-token prediction loss, and no supervision. The regression-based model performs best. While textual (next-token) supervision of offers slight gains, it remains inferior to regression. We observe that: (1) pure next-token prediction supervisionwhether or not the scale factor is included as outputdemands extensive data to improve scale awareness as shown in recent work [8]; (2) explicitly predicting the scale factor, particularly for RGB-only inputs (as in our RGB/RGB+X mixed training), forces models to learn scale information without relying on auxiliary geometry, thereby bolstering capability. Metric-sensitive reward advances the accuracy of spatial tracing In Tab. 4, integrating process rewards boosts the overall success rate by 4% (absolute) compared to purely outcome-based methods. Notably, using only these metricagnostic outcome-based rewards yields smaller improvements on 3D metrics and overall success rates compared to the gains observed on 2D metrics, relative to SFT. This highlights that leveraging TraceSpatial key step annota8 tions to supervise metric-grounded step-wise perception enables more accurate traces in complex spatial relations. 5. Conclusion In this paper, we propose RoboTracer, novel 3D-aware VLM that addresses spatial tracing via multi-step, metricgrounded reasoning on both 3D spatial referring and measuring. In detail, we empower the model to flexibly process arbitrary geometric inputs for precision, employ scale supervision to enhance scale awareness, and guided by metricsensitive rewards to improve its reasoning. We also present TraceSpatial , large-scale well-designed dataset for SFT and RFT training, with TraceSpatial-Bench , benchmark tailored to evaluate spatial tracing. Extensive experiments show the effectiveness of RoboTracer and highlight its potential for broad range of robotic applications."
        },
        {
            "title": "References",
            "content": "[1] Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, et al. Gemini robotics 1.5: Pushing the frontier of generalist robots with advanced emarXiv bodied reasoning, thinking, and motion transfer. preprint arXiv:2510.03342, 2025. 2 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv, 2023. 6 [3] Jingkun An, Yinghao Zhu, Zongjian Li, Enshen Zhou, Haoran Feng, Xijie Huang, Bohua Chen, Yemin Shi, and Chengwei Pan. Agfsync: Leveraging ai-generated feedback for preference optimization in text-to-image generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 17461754, 2025. 3 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv, 2025. 2, 3, 5, 1 [5] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv, 2022. 3 [6] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-languageaction flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 26 [7] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. ICRA, 2025. 2, 6, [8] Zhipeng Cai, Ching-Feng Yeh, Hu Xu, Zhuang Liu, Gregory Meyer, Xinjie Lei, Changsheng Zhao, Shang-Wen 9 Li, Vikas Chandra, and Yangyang Shi. Depthlm: MetarXiv preprint ric depth from vision language models. arXiv:2509.25413, 2025. 8, 27 [9] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. 2, 6, 1, 3 [10] Pingyi Chen, Yujing Lou, Shen Cao, Jinhui Guo, Lubin Fan, Yue Wu, Lin Yang, Lizhuang Ma, and Jieping Ye. Sd-vlm: Spatial measuring and understanding with arXiv preprint depth-encoded vision-language models. arXiv:2509.17664, 2025. 2 [11] Pingyi Chen, Yujing Lou, Shen Cao, Jinhui Guo, Lubin Fan, Yue Wu, Lin Yang, Lizhuang Ma, and Jieping Ye. Sd-vlm: Spatial measuring and understanding with depthencoded vision-language models, 2025. 6, [12] Siyi Chen, Mikaela Angelina Uy, Chan Hee Song, Faisal Ladhak, Adithyavairavan Murali, Qing Qu, Stan Birchfield, Valts Blukis, and Jonathan Tremblay. Spacetools: Toolaugmented spatial reasoning via double interactive rl. arXiv preprint arXiv:2512.04069, 2025. 3 [13] Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, et al. Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation. arXiv preprint arXiv:2506.18088, 2025. 5, 7, 19 [14] Zeren Chen, Zhelun Shi, Xiaoya Lu, Lehan He, Sucheng Qian, Zhenfei Yin, Wanli Ouyang, Jing Shao, Yu Qiao, Cewu Lu, et al. Rh20t-p: primitive-level robotic dataset towards composable generalization agents. arXiv preprint arXiv:2403.19622, 2024. 3, 1 [15] Zeren Chen, Xiaoya Lu, Zhijie Zheng, Pengrui Li, Lehan He, Yijin Zhou, Jing Shao, Bohan Zhuang, and Lu Sheng. Geometrically-constrained agent for spatial reasoning. arXiv preprint arXiv:2511.22659, 2025. 3, 1 [16] Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, and Ruqi Huang. Think with 3d: Geometric imagination grounded spatial reasoning from limited views. arXiv preprint arXiv:2510.18632, 2025. 2 [17] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language models. NeurIPS, 2024. 2, 1, [18] An-Chieh Cheng, Yang Fu, Yukang Chen, Zhijian Liu, Xiaolong Li, Subhashree Radhakrishnan, Song Han, Yao 3d aware reLu, Jan Kautz, Pavlo Molchanov, et al. arXiv preprint gion prompted vision language model. arXiv:2509.13317, 2025. 2 [19] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 44(10-11):16841704, 2025. 26 [20] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6, 7 [21] AgiBot World Colosseum contributors. Agibot https : / / github . com / world colosseum. OpenDriveLab/AgiBot-World, 2024. 5, 16 [22] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 5, 1, 6, [23] Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan, et al. Mm-spatial: Exploring 3d spatial understanding in multimodal llms. arXiv, 2025. 2, 5 [24] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 91104, 2025. 4, 6, 25 [25] Mengfei Du, Binhao Wu, Zejun Li, Xuan-Jing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large visionlanguage models. In ACL (Volume 2: Short Papers), 2024. 6, 22, 23, 24 [26] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. 2 [27] Hao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie, and Cewu Lu. Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. IEEE Transactions on Robotics, 2023. 26 [28] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In ECCV, 2024. 6, 24 [29] Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An, Pengwei Wang, Zhongyuan Wang, Lu Sheng, and Shanghang Zhang. Tiger: Tool-integrated geometric reaarXiv soning in vision-language models for robotics. preprint arXiv:2510.07181, 2025. [30] Wenbo Hu, Jingli Lin, Yilin Long, Yunlong Ran, Lihan Jiang, Yifan Wang, Chenming Zhu, Runsen Xu, Tai Wang, and Jiangmiao Pang. G2 vlm: Geometry grounded vision language model with unified 3d reconstruction and spatial reasoning. arXiv preprint arXiv:2511.21688, 2025. 2 [31] Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, YuChiang Frank Wang, and Fu-En Yang. Thinkact: Visionlanguage-action reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025. 3 10 [32] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 23 [33] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. arXiv preprint arXiv:2502.21257, 2025. 2, 3, 5, 6, 8, 23, 25 [34] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135, 2025. 2 [35] Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, and Zhenfei Yin. Vikir: Coordinating embodied multi-agent cooperation via reinforcement learning. arXiv preprint arXiv:2506.09049, 2025. 3 [36] Nikhil Keetha, Norman Muller, Johannes Schonberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, et al. Mapanything: Universal feed-forward metric 3d reconstruction. arXiv preprint arXiv:2509.13414, 2025. 4, [37] Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. Process reward models that think. arXiv, 2025. 23 [38] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale arXiv preprint in-the-wild robot manipulation dataset. arXiv:2403.12945, 2024. 5, 14 [39] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. 23 [40] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. 5, 1 [41] Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, and Afshin Dehghan. Cubify anything: Scaling indoor 3d object detection. arXiv preprint arXiv:2412.04458, 2024. 5, 1, 6, 7, 20 [42] Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, et al. Molmoact: Action reasoning models that can reason in space. arXiv preprint arXiv:2508.07917, 2025. 3, 4, 6, 7, [43] Rui Li, Zixuan Hu, Wenxi Qu, Jinouwen Zhang, Zhenfei Yin, Sha Zhang, Xuantuo Huang, Hanqing Wang, Tai Wang, Jiangmiao Pang, et al. Labutopia: High-fidelity simulation and hierarchical benchmark for scientific embodied agents. arXiv preprint arXiv:2505.22634, 2025. 1 [44] Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, et al. Hamster: Hierarchical action models for open-world robot manipulation. arXiv preprint arXiv:2502.05485, 2025. 2, 3, 6 [45] Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang, and Chang Xu. From language to locomotion: Retargeting-free humanoid control via motion latent guidance. arXiv preprint arXiv:2510.14952, 2025. 1 [46] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models, 2024. 6, 8, 25 [47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. [48] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 2023. 2 [49] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In ICLR, 2024. 4, 21, 23 [50] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 2023. 3, 1 [51] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 4, 21 [52] Jiaming Liu, Mengzhen Liu, Zhenyu Wang, Lily Lee, Kaichen Zhou, Pengju An, Senqiao Yang, Renrui Zhang, Yandong Guo, and Shanghang Zhang. Robomamba: Multimodal state space model for efficient robot reasoning and manipulation. arXiv e-prints, pages arXiv2406, 2024. 3, 1 [53] Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, et al. Hybridvla: Collaborative diffusion and autoregression in unified vision-language-action model. arXiv preprint arXiv:2503.10631, 2025. [54] Mengzhen Liu, Mengyu Wang, Henghui Ding, Yilong Xu, Yao Zhao, and Yunchao Wei. Segment anything with precise interaction. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 37903799, 2024. 3 [55] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv, 2025. 23 [56] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. 26 [57] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. 3 [58] Yang Liu, Ming Ma, Xiaomin Yu, Pengxiang Ding, Han Zhao, Mingyang Sun, Siteng Huang, and Donglin Wang. Ssr: Enhancing depth perception in vision-language models via rationale-guided spatial reasoning. arXiv preprint arXiv:2505.12448, 2025. 2 [59] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv, 2025. [60] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv, 2024. 2, 5, 6, 7, 8, 21, 22, 25 [61] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visualrft: Visual reinforcement fine-tuning. arXiv, 2025. 3 [62] Xiaoya Lu, Zeren Chen, Xuhao Hu, Yijin Zhou, Weichen Zhang, Dongrui Liu, Lu Sheng, and Jing Shao. Isbench: Evaluating interactive safety of vlm-driven emarXiv preprint bodied agents in daily household tasks. arXiv:2506.16402, 2025. 1 [63] Yulin Luo, Chun-Kai Fan, Menghang Dong, Jiayu Shi, Mengdi Zhao, Bo-Wen Zhang, Cheng Chi, Jiaming Liu, Gaole Dai, Rongyu Zhang, et al. Robobench: comprehensive evaluation benchmark for multimodal large arXiv preprint language models as embodied brain. arXiv:2510.17801, 2025. 1 [64] Wufei Ma, Haoyu Chen, Guofeng Zhang, Yu-Cheng Chou, Jieneng Chen, Celso de Melo, and Alan Yuille. 3dsrbench: comprehensive 3d spatial reasoning benchmark. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 69246934, 2025. 2 [65] Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jieneng Chen, Jianwen Xie, and Alan Yuille. Spatialreasoner: Towards explicit and generalizable 3d spatial reasoning. arXiv, 2025. 3 [66] Wufei Ma, Luoxin Ye, Celso de Melo, Alan Yuille, and Jieneng Chen. Spatialllm: compound 3d-informed design towards spatially-intelligent large multimodal models. In CVPR, 2025. 2 [67] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In CVPR, 2024. [68] Yunze Man, Shihao Wang, Guowen Zhang, Johan Bjorck, Zhiqi Li, Liang-Yan Gui, Jim Fan, Jan Kautz, Yu-Xiong Wang, and Zhiding Yu. Locateanything3d: Visionlanguage 3d detection with chain-of-sight. arXiv preprint arXiv:2511.20648, 2025. 3 [69] Yongsen Mao, Junhao Zhong, Chuan Fang, Jia Zheng, Rui Tang, Hao Zhu, Ping Tan, and Zihan Zhou. Spatiallm: 11 Training large language models for structured indoor modeling. arXiv preprint arXiv:2506.07491, 2025. 2 [70] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. 23 [71] Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, and Roei Herzig. Llarva: Vision-action instruction tuning enhances robot learning. arXiv preprint arXiv:2406.11815, 2024. 4 [72] Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool. Unidepthv2: Universal monocular metric depth estimation made simpler. arXiv, 2025. [73] Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, et al. Worldsimbench: Towards video genarXiv preprint eration models as world simulators. arXiv:2410.18072, 2024. 3, 1 [74] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: multi-modal open-ended embodied system in minecraft via active perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1630716316, 2024. 3 [75] Yiran Qin, Li Kang, Xiufeng Song, Zhenfei Yin, Xiaohong Liu, Xihui Liu, Ruimao Zhang, and Lei Bai. Robofactory: Exploring embodied agent collaboration with compositional constraints. arXiv preprint arXiv:2503.16408, 2025. 1 [76] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 2023. 3 [77] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. ICLR, 2025. 3, 26 [78] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv, 2024. 21 [79] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 22 [80] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv, 2024. 3, 5, [81] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 3, 24 [82] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. 23 [83] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. CVPR, 2025. 2, 5, 6, 24, 25 [84] Peiwen Sun, Shiqiang Lang, Dongming Wu, Yi Ding, Kaituo Feng, Huadai Liu, Zhen Ye, Rui Liu, Yun-Hui Liu, Jianan Wang, et al. Spacevista: All-scale visual spatial reasoning from mm to km. arXiv preprint arXiv:2510.09606, 2025. 2 [85] Huajie Tan, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Yaoxu Lyu, Mingyu Cao, Zhongyuan Wang, and Shanghang Zhang. Roboos: hierarchical embodied framework for cross-embodiment and multi-agent collaboration. arXiv preprint arXiv:2505.03673, 2025. 1 [86] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv, 2025. [87] BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025. 2, 3, 6, 7, 25 [88] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv, 2023. 25 [89] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv, 2025. 2, 25 [90] Qwen Team. Qwen3 technical report, 2025. 6, 7, 25 [91] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025. 6 [92] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. NeurIPS, 2024. 6, 24 [93] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv, 2025. [94] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 2 [95] Ruicheng Wang, Sicheng Xu, Yue Dong, Yu Deng, Jianfeng Xiang, Zelong Lv, Guangzhong Sun, Xin Tong, and Jiaolong Yang. Moge-2: Accurate monocular geomearXiv preprint try with metric scale and sharp details. arXiv:2507.02546, 2025. 3 12 [96] Xingrui Wang, Wufei Ma, Tiezheng Zhang, Celso de Melo, Jieneng Chen, and Alan Yuille. Pulsecheck457: diagnostic benchmark for 6d spatial reasoning of large multimodal models. CVPR, 2025. 2 [97] Yipu Wang, Yuheng Ji, Yuyang Liu, Enshen Zhou, Ziqiang Yang, Yuxuan Tian, Ziheng Qin, Yue Liu, Huajie Tan, Towards cross-view point correCheng Chi, et al. arXiv preprint spondence in vision-language models. arXiv:2512.04686, 2025. 2 [98] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv, 2021. [99] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based arXiv preprint arXiv:2505.23747, spatial 2025. 2 intelligence. [100] Haoning Wu, Xiao Huang, Yaohui Chen, Ya Zhang, Yanfeng Wang, and Weidi Xie. Spatialscore: Towards unified evaluation for multimodal spatial understanding. arXiv preprint arXiv:2505.17012, 2025. 2 [101] Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, et al. Robomind: Benchmark on multiembodiment intelligence normative data for robot manipulation. arXiv preprint arXiv:2412.13877, 2024. 1 [102] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv preprint arXiv:2505.17015, 2025. 2 [103] Rongtao Xu, Jian Zhang, Minghao Guo, Youpeng Wen, Haoting Yang, Min Lin, Jianzheng Huang, Zhe Li, Kaidong Zhang, Liqiong Wang, et al. A0: An affordance-aware hierarchical model for general robotic manipulation. arXiv preprint arXiv:2504.12636, 2025. [104] Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, et al. Vlaser: Vision-language-action model with synergistic embodied reasoning. arXiv preprint arXiv:2510.11027, 2025. 2 [105] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025. [106] Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, et al. Cambrian-s: Towards spatial supersensing in video. arXiv preprint arXiv:2511.04670, 2025. 2 [107] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv, 2025. 3 Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. In Structural Priors for Vision Workshop at ICCV25, 2025. 2 [109] Heyang Yu, Yinan Han, Xiangyu Zhang, Baiqiao Yin, Bowen Chang, Xiangyu Han, Xinhao Liu, Jing Zhang, Marco Pavone, Chen Feng, et al. Thinking in 360 {deg}: arXiv preprint Humanoid visual search in the wild. arXiv:2511.20351, 2025. [110] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, 2016. 4, 21, 23 [111] Chengbo Yuan, Rui Zhou, Mengzhen Liu, Yingdong Hu, Shengjie Wang, Li Yi, Chuan Wen, Shanghang Zhang, and Yang Gao. Motiontrans: Human vr data enable motionarXiv level learning for robotic manipulation policies. preprint arXiv:2509.17759, 2025. 1 [112] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. arXiv preprint arXiv:2406.10721, 2024. 2, 6, 25 [113] Yifu Yuan, Haiqin Cui, Yibin Chen, Zibin Dong, Fei Ni, Longxin Kou, Jinyi Liu, Pengyi Li, Yan Zheng, and From seeing to doing: Bridging reasonJianye Hao. ing and decision for robotic manipulation. arXiv preprint arXiv:2505.08548, 2025. 6, 25 [114] Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, and Jianye Hao. Embodied-r1: Reinforced embodied reasoning for general robotic manipulation. arXiv preprint arXiv:2508.13998, 2025. 2, 3, 5, 25 [115] Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, and Jianye Hao. Embodied-r1: Reinforced embodied reasoning for general robotic manipulation, 2025. 6, [116] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. arXiv preprint arXiv:2403.03954, 2024. 26 [117] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In CVPR, 2023. 21 [118] Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning. arXiv, 2025. 3 [119] Jiayuan Zhang, Kaiquan Chen, Zhihao Lu, Enshen Zhou, Prune4web: Dom tree arXiv preprint Qian Yu, and Jing Zhang. pruning programming for web agent. arXiv:2511.21398, 2025. 3 [120] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv, 2025. 3, 22, 24 [108] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan [121] Qizhe Zhang, Mengzhen Liu, Lichen Li, Ming Lu, Yuan Zhang, Junwen Pan, Qi She, and Shanghang Zhang. 13 Beyond attention or similarity: Maximizing conditional arXiv preprint diversity for token pruning in mllms. arXiv:2506.10967, 2025. 1 [122] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: strong image tagging model. In CVPR, 2024. 3 [123] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware, 2023. 26 [124] Duo Zheng, Shijia Huang, Yanyang Li, and Liwei Wang. Learning from videos for 3d world: Enhancing arXiv preprint mllms with 3d vision geometry priors. arXiv:2505.24625, 2025. [125] Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daume III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024. 3 [126] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. NeurIPS, 2023. 3 [127] Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing Shao. Minedreamer: Learning to follow instructions via chain-ofimagination for simulated-world control. arXiv preprint arXiv:2403.12037, 2024. 3, 1 [128] Enshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, and He Wang. Code-as-monitor: Constraint-aware visual programming for reactive and proactive robotic failure detection. arXiv preprint arXiv:2412.04455, 2024. 3 [129] Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, et al. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025. 2, 3, 6, 1, 8, 25, 27 [130] Kefei Zhu, Fengshuo Bai, YuanHao Xiang, Yishuai Cai, Xinglin Chen, Ruochong Li, Xingtao Wang, Hao Dong, Yaodong Yang, Xiaopeng Fan, et al. Dexflywheel: scalable and self-improving data generation framework for dexterous manipulation. arXiv preprint arXiv:2509.23829, 2025. [131] Shengjie Zhu, Abhinav Kumar, Masa Hu, and Xiaoming Liu. Tame wild camera: in-the-wild monocular camera calibration. NeurIPS, 2023. 3 14 RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics"
        },
        {
            "title": "Supplementary Material",
            "content": "The supplementary document is organized as follows: Sec. A: Implementation Details of TraceSpatial , including data filtering, collection, and QA generation. Sec. B: Implementation details of TraceSpatial-Bench , including benchmark annotation, metrics, and statistics. Sec. C: Implementation details of RoboTracer, including architecture and training details of each stage. Sec. D: More Details on experimental settings, including VQA benchmarks, simulation, and real-world evaluation. Sec. E: More Demonstrations of RoboTracer. Sec. F: More Discussion on Limitations and Future Work. A. TraceSpatial Details In this section, we provide detailed overview of the implementation procedures and representative data samples, highlighting the construction of the TraceSpatial dataset. The dataset is specifically designed to empower general VLMs [4, 14, 15, 43, 45, 50, 52, 53, 62, 63, 7375, 85, 101, 111, 121, 127, 130] with step-by-step capacity to: (1) adapt to 3D spatial referring and measuring tasks, and (2) subsequently progress toward spatial tracing tasks in bottom-up manner. To achieve this, we meticulously establish multi-data-source generation pipeline. In the following, we describe the three fundamental components of this pipeline in detail: 2D Web Image (Supp. A.1): We present 2D data pipeline comprising image filtering, pseudo-3D scene graph construction, hierarchical referential description generationfrom coarse categories to fine-grained spatial referentsand diverse QA pair creation. Since part of our pipeline builds on prior great work [129], we focus here on the key modifications we introduce and the motivations behind them. 3D Scanning Data (Supp. A.2): This section outlines the 3D data selection process from CA-1M [41] and ScanNet [22] and presents methods for enriched scene graph construction compared to the 2D data source. We further describe QA generation framework that leverages detailed 3D annotations (e.g., depth maps, oriented 3D bounding boxes) to capture richer spatial relations for 3D spatial referring and measuring. Object-centric Spatial Tracing Generation from 3D scanning data (Supp. A.3): To bridge the gap between static 3D scanning scenes and manipulation, we introduce simulation-based generation pipeline. This section details our hierarchical framework (e.g., RRT* with escape mechanisms), the taxonomy of five manipulation primitives including active obstacle bypass, and the rigorous spatial trace refinement process. We further explain how we synthesize metric-grounded tracing data with diverse types (e.g., 2D, 3D, and Lifting 2D to 3D). Manipulation Video (Supp. A.4): We describe how to construct end-effector-centric spatial tracing data, including cleaning real-world manipulation video datasets and generating large-scale data in simulation. This enables data collection across three different robot embodiments and both single-arm and dual-arm configurations. A.1. 2D Web Image 2D Web Images aim to provide basic spatial concepts and broad-scale perception across indoor and outdoor scenes. Here we use OpenImage [40] as 2D data source. A.1.1. Dataset Construction Inspired by recent notable work [9, 17, 129], we adopt multi-step pipeline for data cleaning and construction: (1) multi-stage image filtering, (2) pseudo-3D scene graph construction, (3) hierarchical object description generation, and (4) diverse QA pair generation based on the scene graphs. While our overall approach aligns with prior work [129], we further refine Step 2 by developing more accurate method for constructing pseudo-3D scene graphs. A.1.2. Multi-Stage Image Filtering. The OpenImages [40] dataset covers 1.7M training images with extensive visual diversity. However, large portion (e.g., text-only graphics, QR codes, medical scans, abstract art) is not well-suited for spatial reasoning, especially for 3D spatial referring and measuring. To curate subset amenable to referential and reasoning tasks, we adopt twostage filtering approach. We detail below. (1) Stage 1: Coarse Filtering. We quickly eliminate off-theme or low-quality imagery lacking multiple everyday objects by using the siglip2-giant-opt-patch16-384 model [93]. We define positive labels (desired content) and negative labels (undesired content), then compute the cosine similarity between each image embedding and all label https://storage.googleapis.com/openimages/web/index.html 1 embeddings. Images closest to positive labels are retained; otherwise, they are discarded. Iterative refinement of label sets balances recall and precision. From the original 1.7M images, 934k remain after this step (see Listings 1 and 2 for label details). Listing 1. Positive Labels used during SigLIP2 filtering. Positive Labels = [ \"Mid-distance observation of some objects on table\", \"Some objects on the desktop\", \"Distant view of some animals\", \"Mid-distance observation of some animals\", \"Distant view of one object\", \"Mid-distance observation of one object\", \"Distant view of some objects\", \"Mid-distance observation of some objects\", \"Distant view of person\", \"Mid-distance observation of person\", \"Distant view of some people\", \"Mid-distance observation of some people\", \"Distant view of indoor scene\", \"Distant view of outdoor scene\", \"Distant view of traffic\", \"Distant view of Urban architecture\" ] Listing 2. Negative Labels used during SigLIP2 filtering. Negative Labels = [ \"Macro shot of an animal\", \"Macro shot of one object\", \"Macro shot of person\", \"Macro shot of flowers\", \"A piece of text\", \"A person displayed in front of white background\", \"A product displayed in front of white background\", \"A screenshot of the graphics user interface\", \"A dimly lit environment\" ] SigLIP2 preserves images enriched with diverse objects, robust depth cues, and contextual variety (indoor/outdoor) through the aforementioned labeling. However, it struggles with certain image categories: (1) paintings/artworks with visible brushstrokes or textures, (2) low-light scenes with minimal illumination or heavy shadows, (3) grayscale photographs lacking color cues, (4) distorted images exhibiting geometric anomalies, and (5) multi-scene collages containing three or more distinct segments. These types hinder reliable detection and interpretation, underscoring the need for secondary, fine-grained filtering stage. Stage 2: Fine-grained Filtering Due to SigLIP2s limited capacity for certain visual content, we introduce finegrained filtering stage using the Qwen2.5-VL-7B [4] model to improve dataset quality. This ensures that remaining images are clear, authentic, and suitable for the spatial understanding and reasoning essential to 3D spatial referring and measuring tasks. In total, Qwen2.5-VL processed 934k images pre-filtered by SigLIP2, retaining 846k. While Qwen2.5-VL offers superior precision, its slower speed necessitated SigLIP2 for rapid initial filtering, thereby increasing overall efficiency. For accurate and consistent finegrained filtering, we employ structured prompt engineering approach. system prompt (see Listing 3) designates Qwen2.5-VL as an image analysis expert, specifying key visual attributes to verify and negative categories to detect, with strict workflow. For each image, user prompt (see Listing 4) instructs the model to determine whether it falls into any predefined negative category. The models response follows fixed format: if the text after the pipe symbol () is Yes, the image is discarded; otherwise, it is retained. This scheme enforces consistent output format and enhances the reliability of filtering. Listing 3. System Prompt for Qwen2.5-VL-7B filtering. system_prompt = \"\"\" You are an image analysis expert. Follow this workflow rigidly: 1. **Content Analysis**: - Inspect: Main subjects, artistic style, visual characteristics - Check: Lighting intensity, color channels, geometric integrity, composition structure 2. **Category Verification** (YES if matches ANY) : a) Painting/Artwork - Visible brushstrokes/ canvas texture b) Dim Lighting - Very low brightness, heavy shadows c) B&W Photo - Grayscale only (0 color channels) d) Distorted Image - Warping/mirroring anomalies e) Multi-image Collage - >=3 distinct scenes with hard borders 3. **Structured Response**: Output EXACTLY in this format: \"[Analysis sentence]. Yes/No\" - Analysis must contain observable evidence - Final answer MUST use pipe separator Examples of VALID responses: \"This image is composite created by stitching together multiple smaller images, with distinct white borders visible between the individual components . Yes\" \"This image features vibrant colors, is neither an artistic painting nor composite of multiple images, and does not conform to any of the specified categories. No\" \"\"\" Listing 4. User Prompt for Qwen2.5-VL-7B filtering. user_prompt = \"\"\" 2 Analyze if this image belongs to ANY of these categories: 1. Painting/artwork 2. Dim lighting 3. Black-and-white 4. Geometric distortion 5. Multi-image collage Respond EXACTLY FORMATTED as: \"[Your evidence-based analysis]. Yes/No\" \"\"\" A.1.3. Pseudo-3D Scene Graphs Construction Although 2D images provide limited spatial cues, deriving rich 3D spatial information (e.g., near vs. far, distances), especially for 3D spatial referring and measuring, directly from these images remains challenging. Inspired by prior work [9, 17, 129], we construct pseudo-3D scene graphs from 2D images to enhance QA pairs with robust 3D spatial semantics. In these graphs, nodes represent object attributes, while edges denote inter-object spatial relationships. The following sections detail this conversion process. Object Detection and Annotation. While the OpenImages dataset provides annotations, its limited vocabulary and coarse labeling hamper open-world applications. To remedy this, we integrate state-of-the-art foundation models for refined object detection and labeling. Specifically, the Recognize Anything Model (RAM++) [122] and GroundingDINO [57] jointly assign semantic labels and bounding boxes to key objects in filtered 2D images. The pipeline proceeds as follows: 1. Semantic Labeling via RAM++: RAM++ analyzes each image to generate comprehensive category labels, ensuring broad semantic coverage. 2. Bounding Box Localization via GroundingDINO: Label outputs from RAM++ serve as text prompts for GroundingDINO, which localizes objects and produces precise bounding boxes. 3D-aware information Extraction. To further extract 3Daware information from 2D images, we need to convert the 2D images into 3D representations. Unlike prior works (e.g., RoboRefer [129]) that rely on multiple foundation models for metric depth estimation (e.g., UniDepth V2 [72]) and camera intrinsics prediction (e.g., WildeCamera [131]), we adopt MoGe-2 [95], single accurate monocular geometry reconstruction model with metric scale and sharp details. Compared to existing methods such as MoGe, DepthPro, UniDepth V2, Metric3D V2, and Depth Anything V2, MoGe-2 provides more accurate relative geometry, precise metric depth, and sharper detail recovery. It reconstructs scaled point clouds and estimates camera intrinsics from single image, enabling robust 3D scene reconstruction. we adopt for metric depth estimation due to its recent state-ofthe-art performance. Based on previously annotated object bounding boxes, we apply SAM and its variants [54, 77] to generate instance masks. Each resulting Pseudo-3D scene graph comprises object labels (via RAM++), 2D bounding boxes (via GroundingDINO), instance masks (via SAM 2.1), and object-level point clouds (via MoGe-2). Some visualizations are provided in Fig. 6. A.1.4. Hierarchical Object Description Generation While 3D scene graphs commonly encode broad object categories, real-world scenes frequently contain multiple instances of each category. To distinguish among these instances, we augment object descriptions with attributes and spatial relations, yielding fine-grained disambiguation. Below, we outline our two-stage generation pipeline. Stage 1: Generating Object Descriptions in image space. We employ the Qwen2.5-VL-7B model to generate detailed objectand image-level captions. These global captions provide contextual grounding for the QwQ-32B model during LLM QA generation (see Supp. A.1.5), thereby enhancing relevance and accuracy. Prompt templates can be found in Listings 5 and 6. Notably, the object caption user text prompt includes dynamic placeholder [class name], populated with categories predicted by the RAM++ model (see Supp. A.1.3). Listing 5. Prompts for Image Caption Generation with Qwen-VL. image_caption_system_text_prompt = \"\"\" You are an expert image analysis assistant. Your task is to generate detailed and comprehensive description of the image. Please focus on accurately capturing all visual elements present in the image, including objects, scenery, colors, shapes, textures, and lighting. Your description should be clear, precise, and professional. Additionally, ensure that your description begins with either this image or the image. \"\"\" image_caption_user_text_prompt = \"\"\" Please carefully examine the provided image and generate detailed description. Include all visible elements such as objects, scenery, colors, shapes, textures, and lighting. Ensure that your description is thorough, accurate, and complete, and that it starts with either this image or the image. \"\"\" Listing 6. Prompts for Object Caption Generation with Qwen-VL. object_caption_system_text_prompt = \"\"\" You are visual localization analyzer working with TWO distinct images: 1. [POSITION-REFERENCE] (First Image): - Contains ONLY location clues with background - Strictly use ONLY for determining spatial position (left/right/upper/lower/center) Figure 6. The visualization of pseudo-3D scene graphs generated from 2D images with detected objects, and corresponding point clouds. Figure 7. Generated object descriptions. Top: Unique-category captions. Bottom: Spatially-aware captions for the same categories. Red boxes indicate referenced objects. - Ignore all visual features except object Combine features across images placement 2. [DETAIL-SOURCE] (Second Image): - Shows the objects TRUE APPEARANCE without background Describe the [class_name] marked by red box in FIRST image and fully shown in SECOND image with this format: [Color][Material/Texture][Object] at [ - Extract EXCLUSIVELY from this: color, Position] texture, material, shape - Never infer details from the first image Generate phrase in pattern: [Color][Material ][Object] at [Position] \"\"\" Example: \"Matte black laptop on the left\" NOT Samples: - \"Brushed metal water at bottle left\" - \"Glossy ceramic mug at upper center\" - \"Faded denim jacket at lower right\" \"Red-boxed laptop\" \"\"\" object_caption_user_text_prompt = \"\"\" For the [class_name] marked by red box in FIRST image and fully shown in SECOND image: -> COLOR/MATERIAL: Must come from SECOND image -> POSITION: Only from FIRST images placement Forbidden actions: Mention red box or background elements Use location terms in second image Stage 2: Generating Object Description with Spatial Cues. To enhance referential specificity when multiple same-category objects coexist, we adopt heuristic that appends spatially oriented references (e.g., the third chair from the front). This strategy leverages 3D object coordinates from the scene graph (see Supp. A.1.3). By comparing same-category objects along the three principal axes (frontback, leftright, topbottom), we identify the axis with maximal spatial variation as the primary direction for generating relative spatial descriptions. After identifying the main sorting axis, we retrieve suitable templates from predefined library (see Listing 7) to refine the initial object 4 Figure 8. The visualization of generated image detailed descriptions from VLM (e.g., Qwen2.5-VL). descriptions. These templates capture diverse natural language patterns. For instance, if row of chairs is arranged left to right, potential templates include {dense caption}, which is the {ordinal} {class name} from left to right, or {dense caption}, the {ordinal} {class name} in the left-to-right sequence. Here, dense caption is the Qwen2.5-VLgenerated description, ordinal indicates the objects position in the sequence, and class name is the category label output by RAM++. This spatially aware enhancement applies only when multiple instances of the same category appear, preventing redundancy; otherwise, the original dense caption is used. To ensure spatial diversity, we enforce variance threshold across the three principal axes, discarding images with multiple same-category instances but low variance. This process yields final set of 466k images. By integrating spatial ordering with visual descriptions, our heuristic produces precise, discriminative referential expressions, which are essential for generating high-quality, unambiguous question-answer pairs (e.g., 3D spatial referring and measuring). Listing 7. Templates for Spatial Order Description Enhancement. TEMPLATES = { \"left_to_right\": [ \"{dense_caption}, which is the {ordinal} {class_name} from left to right\", \"{dense_caption}, marked as the {ordinal} {class_name} in left-to-right arrangement\", ], \"right_to_left\": [ \"{dense_caption}, the {ordinal} { class_name} viewed from the right\", \"{dense_caption}, the {ordinal} { class_name} from the right\", ], \"front_to_back\": [ \"{dense_caption}, which appears as the { ordinal} {class_name} when viewed from the front\", \"{dense_caption}, positioned as the { ordinal} {class_name} in front-toback order\", ], \"back_to_front\": [ \"{dense_caption}, which is counted as the {ordinal} {class_name}, starting from the back\", \"{dense_caption}, the {ordinal} { class_name} in the back-to-front sequence\", ], \"top_to_bottom\": [ \"{dense_caption}, the {ordinal} { class_name} viewed from the top\", \"{dense_caption}, placed as the {ordinal} {class_name} when sorted from top to bottom\", ], \"bottom_to_top\": [ \"{dense_caption}, which ranks as the { ordinal} {class_name} in bottom-totop order\", \"{dense_caption}, arranged as the { ordinal} {class_name} when ordered from the bottom\", ] } Examples of Object and Image Descriptions This part qualitatively show representative examples with generated descriptions. As shown in Fig. 7, we present two types of object captions: the top row displays simple Qwen2.5-VL captions for single-instance categories where spatial ordering is unnecessary, while the bottom row includes captions enriched with spatial information to distinguish multiple instances of the same category. Additionally, Fig. 8 shows Qwen2.5-VLs ability to generate detailed global descriptions of entire images used in the following. A.1.5. Generating Diverse QA Pairs via 3D Scene Graphs After constructing scene graphs and generating hierarchical object descriptions, we can leverage this information to generate diverse QA pairs from pseudo-3D scene graphs to support SFT training for improved spatial understanding, especially for 3D spatial referring and measuring. Template, Choice and Fact QA Generation. We first 5 adopt template-based approach to generate structured QA pairs, multiple-choice questions, and factual statements. These templates are derived from scene graph information (e.g., object attributes for 3D spatial measuring, 3D positions for 3D spatial referring) and refined hierarchical object descriptions. The spatial concepts addressed by our QA templates encompass the following categories: 1. Relative position relations: capture spatial layouts (left/right, above/below, front/behind). 2. Relative size comparisons: describe object attributes (e.g., bigger/smaller, taller/shorter, wider/thinner) often inferred from image-plane projections. 3. Quantitative information from 2D or pseudo-3D label: include spatial reasoning based on estimated depth maps, 2D object coordinates, 3D object coordinates (i.e., 3D spatial referring), coarse depth approximations, and coarse metric estimation for 3D spatial measuring (e.g., objects length, width, height, distance). We further design fact templates to generate declarative statements, forming structured basis for prompting Reasoning LLM to produce richer and more natural QA pairs. Reasoning QA Generation. To produce more diverse, complex, and natural QA pairs beyond templated formats, we employ QwQ-32B [91], powerful reasoning LLM. Given factual statements, initial QA pairs, multiple-choice questions (if available), as well as global image captions and precise object descriptions, QwQ-32B generates more challenging and conversational spatial reasoning QA. A.2. 3D Scanning Data 3D scanning data typically provides richer and more accurate 3D information (e.g., camera geometry, oriented 3D bounding boxes), whereas 2D web images rely entirely on foundation models to infer 3D structure. However, major challenge in utilizing 3D datasets lies in the lack of comprehensive annotations that support full 2D-3D understanding. This requires, at minimum, accurate camera parameters (i.e., intrinsic and extrinsic), as well as complete per-object annotations in both 2D (e.g., bounding boxes, masks, categories, spatial captions) and 3D (e.g., oriented 3D bounding boxes). While prior work [129] addresses several problems of specific dataset, such as resolving the missing object category labels in CA-1M [41], we note that key resource, ScanNet [22], is omitted, due to its lack of 2D bounding box annotations. To this end, we leverage the well-processed and comprehensive versions of the CA-1M and ScanNet datasetsthe former processed following the approach of prior work [129], and the latter completed by filling in missing information as detailed belowto construct spatially related QA pairs. These QA pairs focus on 3D spaital reasoning, especialy for 3D spatial referring and measuring, and more importantly, on building object-centric, multi-step, metric-grounded spatial tracing 6 data enabled by complete 3D information. In the following, we first detail the comprehensive annotation pipeline for ScanNet, followed by how CA-1M and ScanNet are used to construct the desired data (e.g., 3D spatial referring, measuring and tracing). A.2.1. ScanNet Data Processing We leverage the ScanNet dataset, utilizing the original 3D bounding boxes, object labels, depth maps, and camera intrinsic and extrinsic parameters provided by EmbodiedScan. However, EmbodiedScan does not provide 2D bounding box annotations, which are necessary for our processing pipeline. Therefore, we implement procedure to automatically generate these 2D boxes from the 3D data. Our extraction process is as follows: Step 1: 3D Point Sampling. For given 3D bounding box, we first sample 5,000 points uniformly from its surface. Step 2: Projection and Filtering. We first project these 3D points into the 2D image plane using the provided camera parameters. We then retain valid points based on two criteria: (1) Depth Consistency Check: We compare the depth of the projected point (Zproj) with the depth in the corresponding pixel of the depth map (Dmap). point is retained only if: Dmap 5cm < Zproj < Dmap + 5cm. (2) Image Boundary Check: We discard any point that projects to 2D coordinate (x, y) outside the image boundaries (i.e., < 0, width, < 0, or height). Step 3: 2D Bounding Box Generation. After filtering, we compute the 2D bounding box by finding the minimum and maximum coordinates (xmin, ymin, xmax, ymax) from the set of all remaining valid 2D points. The visualization of this process is shown in Fig. 9. A.2.2. 3D Object Description Generation and Scene Graph Construction for 3D Scanning data Following the 2D scene graph pipeline (Supp. A.1.3) and the object description generation approach (Supp. A.1.4), we build 3D scene graphs that structurally resemble those in OpenImages  (Fig. 6)  . Unlike purely 2D counterparts, these 3D graphs emphasize indoor settings and incorporate precise geometric cues (e.g., ground-truth depth, camera intrinsics/extrinsics, 3D oriented bounding boxes). The enhanced 3D scene graph outperforms 2D-based counterparts in object localization and spatial relation accuracy, enabling structurally rich and quantitatively grounded 3D QA data across 28 spatial relation types. Similar to the 2D pipeline, we construct template-based, choicebased, and fact-based QA pairs from the 3D scene graph. Fact QA pairs are further paraphrased and enhanced using reasoning LLM to generate reasoning questions. Moreover, due to the richer and more precise information in 3D scanning data, we additionally introduce methods for constructing fine-grained 3D spatial measuring, referring, and Figure 9. The visualization of the 2D bounding box generation pipeline for the ScanNet dataset. tracing data, as detailed below. A.2.3. Human-Like Measuring Descriptions Generation During the generation of ground-truth text related to distances and dimensional measurements (i.e., length, width, height), we make two critical observations regarding model performance: (1) we find that strictly constraining the model to output single, standardized unit (e.g., forcing all values to be in meters) negatively impacts its predictive accuracy. (2), we observe similar degradation in performance when the target text omits the name of the object being measured (e.g., providing only the numerical value 1.5 meters rather than contextual phrase like the table is 1.5 meters wide). Based on these findings, we design ground-truth generation strategy to create more natural, human-like descriptions. This strategy has two main components: 1. Contextual Phrasing: All measurement-related descriptions are formed in sentence with addition context, i.e., explicitly includes the name of the object and related measurement type (e.g., length, width, height), rather than being standalone numerical value of the answer. For example, for the question What is the height of the cargo ship at left near the port terminal scene?, the contextualized answer The height of the cargo ship at left near the port terminal scene is about 43 meters performs better than the raw response 43 meters. 2. Dynamic Unit Selection: Instead of fixed unit, we implement dynamic unit selection mechanism that adapts to the magnitude of the value. This method converts the base metric value into set of plausible units (e.g., meters, centimeters, feet, inches) and stochastically selects one based on human-like preferences. For example, values less than 1 meter are randomly converted to centimeters (with higher probability) or inches. Values greater than 1 meter are typically expressed in meters but are also stochastically converted to feet. This approach ensures the model is exposed to diverse and realistic range of textual formats, which our experiments show leads to better performance. A.2.4. 3D Spatial Measuring Data Generation To generate the data for 3D spatial measuring, we first define the object dimensions (length, width, height), which is very crucial for 3D scanning dataset, and then extract them. Herein we mainly focus on CA-1M and ScanNet. Data Normalization Across Datasets. critical issue we notice during data processing is the inconsistent definition of bounding box dimensions across datasets. We find that the 3D bounding box scales provided by CA-1M [41] follow [width, height, length] order. In contrast, the ScanNet annotations provided by EmbodiedScan adopt [width, length, height] order. To resolve this, we implement normalization step to unify all bounding box data into single, consistent format before extracting measurements. Semantic Dimension Definitions. simple extraction of [width, height, length] dimensions from datasets raw annotations is insufficient, as these are arbitrary and do not align with human concepts of length or width. We thus establish consistent, semantically-grounded Figure 10. The visualization of our semantic dimension definitions. Length (red) is defined as the intersection of the front and bottom faces. Width (green) is the intersection of the side and bottom faces. Height (blue) is the intersection of the front and side faces. definition for object dimensions based on their canonical orientation (e.g., their front, side, and bottom faces). Our definitions are as follows: occupancy information into simulated manipulation trajectories through structured process: initialization, planning, and reasoning. Length: Defined as the length of the edge where the objects front and bottom face intersect. Width: Defined as the length of the edge where the objects side and bottom face intersect. Height: Defined as the length of the edge where the objects front and side face intersect (i.e., the vertical edge). This semantic-based definition, shown in Figure 10, ensures that query for how wide is the cabinet consistently refers to the same geometric property, regardless of the objects orientation in the scene. A.2.5. 3D Spatial Referring Data Generation For 3D spatial referring tasks, we generate data that pairs multi-step descriptions with precise 3D surface points. Our approach is inspired by previous work in 2D spatial referring [129], which typically associates 2D pixel coordinate (x, y) with phrase like the corner of the leftmost table. key limitation of 2D referring data is that 2D point represents an infinite ray in 3D space, lacking specific 3D localization. To overcome this, we build upon the 2D referring data by incorporating depth information. For given 2D referring point (x, y) that indicates specific object, we also extract its corresponding depth value from the depth map. This critical step ensures that the ground-truth target is not an ambiguous ray but precise 3D coordinate (x, y, d). This 3D point is guaranteed to lie on the visible surface of the target object, providing an unambiguous 3D grounding for the language description. A.3. Object-centric Spatial Tracing Generation Since static 3D scanning datasets (e.g., CA-1M, ScanNet) lack ground-truth manipulation trajectories to exect spatial traces, we develop simulation-based generation pipeline to synthesize high-quality object-centric spatial traces. This pipeline transforms static 3D scene graphs with A.3.1. Scene Initialization & Task Formulation Before trace generation, we must standardize the environment and rigorously define the roles of objects within the scene to ensure plausible task formulation. Gravity Alignment and Data Validation. Raw 3D scans often exhibit arbitrary coordinate orientations. To support physics-based reasoning (e.g., stacking), we first standardize the scene geometry using gravity alignment matrix Rgravity. For every object Oi with corner coordinates Pi R83, we apply the transformation gravity. Following alignment, we perform strict geometric validation. Objects with degenerate bounding boxes (fewer than 4 valid corners) or containing numerical anomalies (NaN/Inf values) are filtered out to prevent simulation instability. = PiRT Object Role Assignment. critical challenge in data generation is balancing semantic richness for VQA with physical completeness for simulation. As detailed in Sec. A.2, our filtering pipeline produces subset of HighQuality Objects with rich VLM-generated captions, while the original raw dataset contains all geometric instances but lacks detailed descriptions, especially in CA-1M (more details can be found in prior work [129]). To address this, we assign roles as follows: Moving Objects (Osrc) & Via Objects (Ovia): Selected exclusively from the High-Quality Object Subset. This is crucial for generating unambiguous referring expressions in VQA tasks. For instance, in scene with multiple apples, raw category labels are insufficient to distinguish the target. We rely on the rich dense and spatial captions (e.g., the apple on the right) available in the high-quality subset to ensure the generated instruction uniquely identifies the specific object (e.g., Move the apple on the right around...), rather than generic and ambiguous Move the apple. 8 Reference Objects (Oref ): subset of the High-Quality Objects that satisfy stability constraints. We identify Oref by verifying they are resting on supporting platform (e.g., table or shelf) using vertical overlap heuristic. This ensures that the destination region near Oref is physically reachable and capable of supporting Osrc. Obstacles (Oobs): Sourced from the Original Raw Dataset (containing all annotated instances). While some objects may lack captions and are excluded from being Osrc or Oref , they still physically exist in the scene. To ensure collision-free motion planning, our motion planner considers the union of all raw instances as the obstacle set, preventing the generated trace from hallucinating paths through uncaptioned objects. Taxonomy of Manipulation Primitives. To improve spatial reasoning capabilities, we define five distinct manipulation primitives (Methods 15). Additionally, we introduce mechanism to identify Potential Via Objects for standard tasks to enrich spatial trace descriptions, shown in Fig. 11. Method 1 (Place Relative): Moves Osrc to spatial relation (e.g., left, right) relative to Oref . Instruction Example: Place the {source obj} to the {endpoint direction} of the {reference obj}. Method 2 (Directional Move): Moves Osrc by specific distance towards cardinal direction. Instruction Example: Move the {source obj} toward the {endpoint direction}. Method 3 (Stacking): Places Osrc on top of Oref , enforcing surface area constraints (Area ref Area src) to ensure stability. Instruction Example: Place the {source obj} on top of the {reference obj}. Method 4 (Active Bypass & Place): high-difficulty primitive where the planner explicitly identifies an obstacle (Ovia High-Quality Subset) blocking the direct path and generates trace to move around it before reaching the target. Instruction Example: Move the {source obj} around the {via obj} on its {via direction} side, then place it to the {endpoint direction} of the {reference obj}. Method 5 (Active Bypass & Stack): Combines obstacle avoidance with final stacking operation. Instruction Example: Move the {source obj} around the {via obj} on its {via direction} side, then place it on top of the {reference obj}. Note on Potential Via Objects (Method 13): Even for standard primitives (Methods 13), the generated spatial trace may coincidentally pass close to other objects. We implement Retroactive Via Discovery logic to identify these objects from the High-Quality Subset and add the spatial constraints along the trace path into original spatially constrained instructions. If spatial trace passes within proximity threshold of an object Ok, Ok is labeled as Potential Via Object, adding implicit spatial constraints to the VQA annotations (e.g., Move {source obj} around {via obj} on its {via direction} side, then place it...). A.3.2. Simulation-based Spatial Trace Generation Based on the initialized scene and defined tasks, we employ rigorous motion planning pipeline in continuous 3D space (C R3). Unlike rasterized grid searches (e.g., A*), our approach utilizes sampling-based planning to handle the high-resolution geometry of scanned environments, ensuring feasibility and plausibility. Region-based Endpoint Selection. While Sec. A.3.1 defines valid destination regions, the motion planner requires specific goal configuration pgoal. To enhance robustness against clutter, we implement Centroid-outward Polar Sampling Strategy within the candidate region Rdest. Instead of uniform random sampling, we discretize Rdest into structured polar grid centered at its geometric centroid. We define set of concentric radii = {0, 0.03, 0.06, 0.10, 0.15, 0.20} (meters) and angular steps. The pipeline iteratively evaluates candidate points from the inner rings outwards. For each candidate pcand, we apply height adjustment based on the target objects dimensions relative to the platform surface (ytarget = yplatf orm+ hobj/2 + δsaf ety) and perform static collision check. The first valid sample is selected as pgoal. This strategy implicitly maximizes the safety margin by preferring locations furthest from region boundaries and obstacles. RRT* Planning with Dual-modal Escape. We utilize the RRT* (Rapidly-exploring Random Tree Star) algorithm to generate collision-free traces. The planner grows tree from the start configuration pstart towards pgoal with goal-biasing probability of 0.25 and step size of 0.05m. Crucially, to optimize trace quality, we employ rewiring radius of 0.25m, allowing the algorithm to asymptotically converge towards the optimal path by restructuring the tree topology within the local neighborhood. Escape Strategy for Constrained Starts. common failure mode in simulation is when the source objects initial pose slightly intersects with the environment (soft collision) due to meshing noise or scanning artifacts, which locks the standard planner. To address this, we implement heuristic Dual-modal Escape Mechanism: 1. Visual Opening Analysis: If pstart is in collision, we first analyze the depth map along 6 cardinal directions (up, down, left, right, front, back). We compute an Opening Score for each direction by ray-casting into the depth buffer; directions with longer free-space rays are prioritized. 2. Geometric Push Fallback: If visual analysis is inconclusive (e.g., occluded camera), we fall back to geometric approach. We calculate the penetration depth of the objects AABB against all overlapping obstacles and Figure 11. Visual Taxonomy of Manipulation Primitives. We define five distinct primitives ranging from simple relative placement (Method 1) to complex active obstacle bypass and stacking (Methods 45). Additionally, we retroactively identify Potential Via Objects (bottom) for standard traces to enrich spatial descriptions in cluttered scenes. determine Push Vector that minimizes the translation required to resolve the collision. 3. Recovery: The planner moves the object along the optimal escape vector until it reaches free space pf ree (max distance 0.6m). The escape segment pstart pf ree is then prepended to the main RRT* plan. Hierarchical Collision Detection. Precise collision checking is paramount for generating non-penetrating traces. We reject simple sphere-based approximations in favor of two-phase hierarchical collision detection system: Broad Phase (AABB): We first filter potential colliders using Axis-Aligned Bounding Boxes (AABB). Objects with non-overlapping AABBs are immediately discarded, significantly reducing the computational overhead given the large number of objects in scanned scenes. Narrow Phase (OBB via SAT): For objects passing the broad phase, we perform precise intersection tests. Since scanned objects are often oriented arbitrarily (not axisaligned), AABBs are overly conservative and prevent valid close-proximity manipulations. We construct Oriented Bounding Boxes (OBB) for the moving object and 10 obstacles by performing Principal Component Analysis (PCA) on their vertices to extract the three principal orthogonal axes. We then employ the Separating Axis Theorem (SAT) to test for intersection across 15 potential separating axes (3 axes from object A, 3 from B, and 9 cross-products). This rigorous check allows the planner to generate tight, realistic traces that graze valid obstacles without false positives. A.3.3. Advanced Spatial Reasoning Logic Beyond basic collision avoidance, our pipeline incorporates advanced spatial reasoning modules to generate and interpret complex, multi-step spatial tracing data. This includes proactive planning for obstacle bypass (Methods 45) and retroactive analysis of spatial relations for standard traces (Methods 13). Active Bypass Planning (Methods 4 & 5). To synthesize training data that necessitates reasoning about intermediate constraints, we implement heuristic-guided bypass planner. This module forces the robot to deviate from the greedy trace to navigate around specific Via Object (Ovia). 1. Obstacle Identification: We first compute hypothetical direct trace using the standard RRT* planner. The system identifies potential blocking objects by calculating the segment-to-point distance for all scene obstacles. If an object Oblock lies within collision margin (< 0.05m) of the direct path, it is designated as the target Via Object. 2. Multi-directional Candidate Generation: Unlike simple heuristics that always bypass to the right, our system evaluates 6 cardinal bypass directions relative to Ovias geometry: left, right, front, back, up, down. For each direction d, we define valid via-region Vd. The center of this region is calculated as: cvia = cobs + (robs + rsrc + δmargin) nd (1) where cobs is the obstacle center, nd is the normal vector for direction d, and robs, rsrc are the radii of the obstacle and moving object, respectively. This explicitly ensures kinematic clearance. 3. Heuristic Scoring for Naturalness: To select the most human-like bypass spatial trace, we score each candidate region using composite cost function J. Based on our empirical tuning, the cost is defined as: = 1.0Ltotal+0.3Pangle+2.0Pbacktrack +0.2Plateral (2) Here, Ltotal is the total trace length. Pangle penalizes sharp turns (cosine similarity). Pbacktrack is binary penalty (weight 2.0) that strictly discourages candidates requiring movement opposite to the goal vector. Plateral penalizes excessive deviation from the main axis. The candidate minimizing is selected as the optimal viapoint pvia. 4. Two-stage Planning & Fusion: The planner executes two independent RRT* searches: τ1 : pstart pvia and τ2 : pvia pgoal. The resulting paths are concatenated. Crucially, during the smoothing phase (Catmull-Rom), we enforce via-point constraint: any optimization that moves the spatial trace further than 0.12m from pvia is rejected, preserving the intentional bypass behavior. Retroactive Via Discovery (Methods 13). Standard manipulation tasks (e.g.., Place next to B) often implicitly involve spatial constraints relative to other objects in cluttered scenes. To capture this rich semantic information, we implement retroactive discovery module. 1. Proximity Analysis: For generated spatial trace , we compute the minimum Euclidean distance to all HighQuality objects in the scene. We calculate the distance to the objects surface rather than its center: dsurf = ptraj cobjrobj. Objects satisfying dsurf < 0.15m are flagged as Potential Via Objects. 2. Relative Direction Classification: For each flagged object, we determine the spatial relationship of the bypass. We identify the closest point pclose on the trace and calculate the offset vector vof = cobj pclose. By determining the dominant component of vof orthogonal to the traces instantaneous velocity vector, we classify the bypass direction into natural language labels (e.g., passing on the left, passing above). 3. Instruction Enrichment: These discovered relations are injected into the VQA generation pipeline, transforming simple instructions into spatially dense descriptions (e.g., from Move mug to plate to Move mug to plate, passing to the right of the bottle). This supervision signal is critical for training RoboTracer to attend to collision boundaries and intermediate spatial context. A.3.4. Trace Refinement & Quality Assurance The raw traces generated from the motion planner are geometrically valid in 3D but may lack the visual alignment and naturalness required for VLM training. We implement comprehensive refinement pipeline followed by strict quality assurance protocols. Geometric Smoothing & Compression. To mitigate the robotic artifacts of RRT*, we apply Catmull-Rom Spline smoothing (α = 0.5) to generate fluid, humanlike curves. Subsequently, we utilize the Ramer-DouglasPeucker (RDP) algorithm to downsample the dense path into sparse sequence of keypoints (N 8), reducing token complexity while preserving critical motion geometry. Physics-aware Grounding (End-Point Refinement). common simulation artifact is floating placement, where the trace ends at the objects geometric center, leaving it visually suspended. We implement depth-guided correction: by projecting the endpoint onto the image plane and querying the ground-truth depth map, we iteratively descend the traces final point along the gravity vector until it makes contact with the physical surface (i.e., projected depth matches sensor depth). This ensures physically grounded placement. Visual Alignment Correction (Start-Point Refinement). While 3D planning operates on the objects geometric center, simply projecting this 3D center into the 2D image may result in starting point that falls outside the objects visual mask (e.g., for C-shaped objects or due to perspective distortion). To ensure precise visual grounding: 1. We retrieve the ground-truth 2D segmentation mask (RLE-encoded) of the source object. 2. We calculate the visual centroid (umask, vmask) of the masks largest connected component. 3. We override the 2D coordinates of the traces start point with (umask, vmask), while retaining the original metric depth dstart. This alignment guarantees that the visual trace originates perceptually from the objects body, eliminating offtarget supervision signals. Rigorous Quality Control. To filter out hallucinations and low-quality samples, we enforce set of validation criteria. trace is discarded if it fails any of the following checks: Occlusion Ratio: We compute the visibility of every interpolated waypoint against the scenes depth buffer. Traces with > 30% occlusion are rejected to ensure the trace is visually trackable. Field-of-View Constraint: All keypoints, especially the start and end, must project strictly within the camera frame boundaries. Volume-Adaptive Dynamics: Fixed minimum-length thresholds fail to account for scale variance. We apply dynamic threshold Lmin = Lbase 3(cid:112)Vobj. This requires large objects (e.g., laptops) to move significant distances to constitute valid action, while allowing subtle manipulations for small objects (e.g., keys). Semantic Movability: We filter out immovable fixtures (e.g., floor, wall, countertop) using semantic blocklist, ensuring the model focuses solely on interactable entities. A.3.5. VQA Data Generation traces into multimodal The final stage transforms the refined metric-grounded geometric-aware instructionfollowing pairs. To foster robust spatial reasoning, we synthesize instructions with varying levels of granularity and generate three distinct types (e.g., 2D, 3D, and Lifting) for each trace. Instruction Generation. For each generated trace, we synthesize language instruction using template-based approach. critical feature of our pipeline is the injection of Metric Scale Awareness. As shown in Listing 8, our template pool is designed such that approximately 20% of the templates include explicit metric placeholders ({distance:.3f}m). During generation, we calculate the ground-truth trace displacement Ltraj and populate these slots. This design forces the model to not only understand relative directions (e.g., to the right) but also correlate visual magnitudes with precise numerical values (e.g., 0.25m). Listing 8. Templates for Metric-aware Instruction Generation. INSTRUCTION_TEMPLATES = { \"method1_place_relative\": [ # Metric-aware templates (20%) \"Move the {source_obj} to position { distance:.3f}m to the { endpoint_direction} of the { reference_obj}.\", \"Pick up the {source_obj} and move it to position {distance:.3f}m to the { endpoint_direction} of the { reference_obj}.\", # Standard templates \"Place the {source_obj} to the { endpoint_direction} of the { reference_obj}.\", \"Pick up the {source_obj} on the { reference_obj}s {endpoint_direction} side.\", ], \"method2_directional_move\": [ # Metric-aware templates \"Move the {source_obj} {distance:.3f}m in the {endpoint_direction} direction .\", \"Pick up the {source_obj} and move it { distance:.3f}m toward the { endpoint_direction}.\", # Standard templates \"Push the {source_obj} toward the { endpoint_direction}.\", \"Slide the {source_obj} toward { endpoint_direction}.\", ], \"method3_stacking\": [ # Stacking specific templates \"Place the {source_obj} on top of the { reference_obj}.\", \"Stack the {source_obj} on the { reference_obj}.\", \"Put the {source_obj} above the { reference_obj}.\", # Vertical relative positioning \"Move the {source_obj} onto the { reference_obj}.\", \"Set the {source_obj} on the { reference_obj}.\", ], \"method4_bypass_place\": [ # Implicit constraints via Via Object \"Move the {source_obj} around the { via_obj} on its {via_direction} side, then place it to the { endpoint_direction} of the { reference_obj}.\", \"Pick up the {source_obj} around the { via_obj} from the {via_direction} side, then position it to the { endpoint_direction} of the { reference_obj}.\", ], \"method5_bypass_stack\": [ Type 2: 3D Spatial Trace. This is the core task, requiring the model to infer depth and 3D structure from monocular input. The output is sequence of 3D coordinates τ3D = {(ui, vi, di)}N i=1, where di is the absolute metric depth. # Complex composite task: Bypass + Stacking \"Move the {source_obj} around the { via_obj} on its {via_direction} side, then place it on top of the { reference_obj}.\", \"Pick up the {source_obj} around the { via_obj} from the {via_direction} side, then place it on the { reference_obj}.\", ], \"potential_via_enrichment\": [ # Retroactive description for standard tasks \"Move the {source_obj} around the { via_obj} on its {via_direction} side, then {final_action}.\", \"Pick up the {source_obj}, passing to the {via_direction} of the {via_obj}, then {final_action}.\", ] } Multi-task VQA Formatting. To fully exploit the generated data, we construct three distinct VQA tasks for every single trace sample. We utilize specific prompt templates (sourced from the Droid template set in our codebase) to format the queries. This multi-task formulation encourages the model to learn consistent representations across 2D and 3D spaces. Type 1: 2D Visual Trace. This task supervises the model to ground instructions into the 2D image plane. The output is sequence of 2D coordinates τ2D = {(ui, vi)}N i=1. Listing 9. Prompts for 2D Visual Tracing. PROMPTS_2D = [ \"Please predict 2D object-centric waypoints to complete the task successfully. The task is \"<instruction>\". Your answer should be formatted as tuple, i.e. [(x, y)], where the tuple contains the and coordinates of point satisfying the conditions above.\", \"Point the 2D object-centric waypoints for the task \"<instruction>\". Your answer should be formatted as tuple, i.e. [(x, y)].\", Listing 10. Prompts for 3D Spatial Tracing. PROMPTS_3D = [ \"Please predict 3D object-centric waypoints to complete the task successfully. The task is \"<instruction>\". Your answer should be formatted as list of tuples, i.e., [(x1, y1, d1), (x2, y2, d2), ...], where each tuple contains the and coordinates and the depth of the point.\", \"Point the 3D object-centric visual trace for the task \"<instruction>\". Your answer should be formatted as list of tuples, i.e., [(x1, y1, d1), ...].\", \"You are currently robot performing robotic manipulation tasks. The task instruction is: \"<instruction>\". Use 3D points to mark the manipulated object-centric waypoints to guide the robot...\", ] Type 3: 2D-to-3D Trace Lifting. This task isolates the geometric reasoning capability. The model is provided with the ground-truth 2D trace in the text prompt and must lift it into 3D space. This effectively trains the model to perform trace-conditioned depth estimation. Listing 11. Prompts for 2D-to-3D Trace Lifting. PROMPTS_LIFT = [ \"Please lift the 2D object-centric waypoints to 3D object-centric waypoints to complete the task successfully. The task is \"<instruction>\". The 2D waypoints is <trace>. Your answer should be formatted as list of tuples, i.e., [(x1, y1, d1) , ...].\", \"Lift the 2D object-centric visual trace to 3 object-centric visual trace for the task \"<instruction>\". The 2D visual trace is <trace>. Your answer should be formatted as list of tuples, i.e., [(x1 , y1, d1), ...].\", \"Please lift the 2D object-centric visual trace to 3D object-centric visual trace to complete the task successfully. The task is \"<instruction>\". The 2D visual trace is <trace>. Your answer should be formatted as list of tuples...\" \"You are currently robot performing robotic manipulation tasks. The task instruction is: \"<instruction>\". Use 2D points to ] mark the manipulated object-centric waypoints...\", \"Please predict 2D object-centric visual trace to complete the task successfully. The task is \"<instruction>\". Your answer should be formatted as tuple, i. e. [(x, y)].\" ] Trace Representation. For all tasks, the spatial trace is represented as sequence of discrete tokens. We normalize the 2D pixel coordinates (u, v) to the range [0, 1000] and keep the depth in absolute meters. This hybrid representation allows the model to leverage its pre-trained 2D visual grounding capabilities while learning precise 3D metric structures. 13 Figure 12. Extract end-effector-centric and object-centric traces from Droids original data by projecting gripper positions into base frame; downsample traces using RDP and remove tasks with too many points after downsampling. Figure 13. Visualization of the end-effector coordinate frame. We obtain the gripper position (blue point) by extending the end-effectors local z-axis and record both the spatial trace of the gripper and its open/closed states. A.4. Manipulation Video While 3D scans enable object-centric tracing, they lack physically plausible manipulations. We further leverage manipulation video datasets, either real-world videos or simulation videos, to provide spatial traces aligned with the embodied manipulation in tabletop settings. A.4.1. Droid Data Processing The Droid dataset [38] provides two versions: RLDSformat version and raw version containing raw stereo videos and metadata. Since the RLDS version does not allow extraction of camera intrinsics or depth information, we use the raw version. Ignoring the wrist cameras, the Droid raw dataset contains 117k stereo videos stored in the original SVO format. We extract the camera intrinsics and depth frames from these SVO files using the ZED SDK. The Droid dataset contains substantial number of samples with incorrect extrinsics (see Fig.14), which manifest as the projected end-effector positions not aligning with the actual end-effector locations in the images. After processing the Droid Raw dataset, we obtain 20.5k unique manipulation trajectories and generate total of 46.8k endeffector-centric QAs and 58.4k object-centric QAs based on templates, covering both 2D and 3D trace generation tasks (see Fig. 12, 15). The process is as follows: Raw Data Process. The original dataset is filtered through four criteria, resulting in 80k valid SVO files. The criteria include: (1) Presence of metadata; (2) Availability of language annotations; (3) Complete H5 file provided; (4) Presence of the SVO file. For each valid SVO file, camera intrinsics and extrinsics, as well as depth frames, are extracted using the ZED SDK. Extrinsic Validation. To verify the correctness of the cam14 Figure 14. Preview images from the Droid raw dataset with the end-effector projected as red dot. Each row shows pair of images from the same scene: the left image corresponds to incorrect camera extrinsics, and the right image corresponds to correct extrinsics. era extrinsics, we apply strict depth-alignment check, which also filters out some occluded frames. The procedure is as follows: 1. Project the end-effector coordinates into the corresponding image frame using the camera intrinsics and extrinsics (R, t): = (Rpeef + t) , 15 Figure 15. Droids 2D and 3D trace visualizations under the object-centric setting. The 3D traces are visualized in the point-cloud space. where peef is the end-effector position in world coordinates, and is the projected pixel coordinate. 2. Compare the depth value at the projected pixel D(u) with the end-effectors z-coordinate in the camera frame, zcam eef . The projection is considered consistent if: D(u) zcam eef < 5 cm. Only frames where the projected pixel lies within the image bounds are considered. 3. If the fraction of frames satisfying the above consistency exceeds 1 3 of the total tested frames, the extrinsics are deemed valid. This procedure also filters out frames in which the end-effector is occluded by the robot arm. Fig. 16 presents several examples illustrating the correctness of our method. End-Effector Position Definition and Task Segmentation. We define the gripper position along the end-effector coordinate systems z-axis at distance of 0.15 from the base of the end-effector (See Fig. 13). Since Droid does not provide pre-segmented tasks, we perform segmentation based on gripper closure events: (1) Detect intervals where the gripper is closed. (2) Retain only videos containing single gripper-closure interval. (3) Assign the video instruction directly as the description of the corresponding motion segment. Trace Extraction and Downsampling. For each selected video, we extract end-effector-centric and object-centric traces as follows: (1) Let the gripper closure interval be [Fs, Fe). Define base frame: Fb = max(0, Fs 60). (2) Project the gripper positions within [Fs, Fe) into the base frame to obtain the object trace. (3) Project the gripper positions within [Fb, Fe) into the base frame to obtain the end-effector trace. (4) Downsample each trace using the RDP algorithm and remove tasks with excessive points remaining after RDP downsampling. The visualization of the extracted results is shown in Fig. 12, 15. A.4.2. AgiBot Data Processing We use the AgiBot World Beta dataset [21] to extract spatial traces from dual-arm robotic manipulation. However, the raw data contains several issues as shown in Fig. 17, including invalid extrinsics, inaccurate motion segmentation, and severe occlusions, which should be resolved prior to trace extraction. We detail our full preprocessing pipeline below. Tasks Filtering. We begin with coarse filtering stage to retain only demonstrations that can meaningfully support spatial trace supervision. In Fig. 17, demonstrations exhibiting global robot-based motion while the camera extrinsics remain unchanged implicitly introduce incorrect camera poses; such sequences are manually removed. AgiBot additionally contains many samples with extrinsic errors, which are later detected using depth-alignment validation. Extrinsics Validation via Depth Alignment. Unlike Droid, AgiBot uses head-mounted RGB-D camera whose depth maps encode near-range geometry, including the robot arm, as zero depth. To continue using depth consis16 Figure 16. DROID depth-alignment results. We verify camera extrinsics by projecting the end-effector into the image and comparing pixel depth with the camera-frame end-effector value; extrinsics are accepted if over 1 3 frames have depth difference under 5 cm. tency for extrinsics validation, we extend the definition of aligned depth: D(u) = 0 or (cid:12) (cid:12)D(u) zcam eef (cid:12) (cid:12) < 0.05 m. video is accepted if the proportion of aligned frames exceeds threshold. The coverage of retained samples under different choices of the threshold in Fig. 18. We use 0.83 as the threshold in practice. Fig. 19 visualizes all samples as 2D grid (black = misaligned) and reveals that misaligned frames appear in spatially contiguous clusters, matching manual observations that extrinsic failures occur in temporally coherent segments under fixed scene configurations. Improved Motion Segmentation. The original AgiBot annotations decompose tasks into atomic labels (e.g., Pick, Place), but these labels are often temporally inaccurate. For instance, the Place interval frequently extends well after the object has reached the target surface. Such coarse segmentation prevents the reliable construction of object-centric traces. We redesign the segmentation to obtain precise endeffectorand object-centric traces. Merging PickPlace Pairs: We scan the annotation and merge each Pick with its subsequent Place segment into unified Pick-and-Place (P&P) subtask. We define the gripper position as the point located at = 0.15 along the end-effectors local z-axis. All traces are expressed in the coordinate frame of the first frame of the subtask, which we refer to as the base frame. Trace Extraction Rules: For each subtask with exactly one gripper-closure interval, we extract traces as follows: Pick subtasks: * If only one arm closes, we record the end-effectorcentric trace from the subtask start to the closure17 Figure 17. Common issues in the AgiBot dataset. (a) Meaningless Traces. For certain dual-arm collaborative tasks, spatial traces alone cannot accurately describe the motion, making them difficult to interpret even for humans. (b) Inaccurate Motion Segmentation. The original segmentation is often temporally misaligned; for example, trace recording continues even after the object has already been placed on the platform or inserted into the container. (c) Invalid Extrinsics. Incorrect camera extrinsics lead to inaccurate trace projections. (d) Occlusion. The manipulated object (highlighted in the red box) is occluded in the image, rendering the resulting trace meaningless. Figure 18. Proportion of retained samples under different threshold values. sample is considered to have correct extrinsics if the proportion of aligned frames exceeds given threshold. interval start. * If both arms close and both move, the sample is discarded (ambiguous). * If both arms close but only one moves, we use the moving arm only. Pick-and-Place subtasks: centric trace from the subtask start to the closureinterval end, and the object-centric trace in the closure-interval. * If both arms close but only one moves, we use the moving arm following the same rule. Other subtasks: * If only one arm closes, we record the end-effector- * If exactly one arm contains closure interval, we 18 Figure 19. Visualizing all samples as 2D grid (black = misaligned). Misaligned frames form spatially contiguous clusters, consistent with the observation that extrinsic failures occur in temporally coherent segments under fixed scene configurations. record the end-effector-centric trace from the subtask start closure-interval end. * If both arms close but only one arm moves, we use the moving arm. This redesigned segmentation enables the construction of both end-effector-centric and object-centric traces across wide variety of manipulation behaviors. Occlusion Detection. Head-camera viewpoints introduce significant occlusion, especially when the robot arm blocks the manipulated object. To filter such cases, we evaluate gripper visibility at designated check frame: For Pick and P&P subtasks: use the start of the closure interval. For other subtasks: use the end of the closure interval or the subtask end. We project the gripper position at the check frame into the base frame and inspect depth values within radius-30 pixel neighborhood. If more than 60% of these depth values are zero, the sample is marked as occluded and discarded. QA Generation. We apply RDP simplification to all extracted traces to reduce redundancy while preserving geometric structure. Traces that contain out-of-bounds projected points or retain an excessively large number of points after RDP compression are discarded. Finally, for all valid traces, we instantiate trace-description templates according to their types (end-effectorcentric or object-centric) to construct three categories of QA pairs, yielding total of 977.5k samples. A.5. Simulation Data We utilize RoboTwin 2.0 [13] to collect high-precision spatial traces in simulation environment. While the original RoboTwin data collection pipeline captures only endeffector positions, we modified the simulator to support our specific spatial trace requirements. Our modifications include: (1) Implementation of multi-GPU parallel simulation to accelerate data collection; (2) Extended tracking capabilities to record traces for both grippers and manipulated objects; (3) Refined instruction annotation, allowing single execution frame to be mapped to multiple finegrained sub-task instructions; (4) Incorporation of spatial referring expressions to localize objects, thereby enhancing the models spatial understanding. We re-engineered 16 original RoboTwin tasks to generate the necessary spatial traces. The statistics of the collected simulation data are listed in Tab. 9. The processing pipeline is as follows: Coordinate Mapping. We define the functional grasp point by applying an offset of 0.14 along the x-axis of the robots end-effector coordinate system to align with the actual gripper center. Using the camera intrinsic and extrinsic parameters, we project the 3D world coordinates of the grasp point into 2D pixel coordinates and depth values within the camera frame. Task Segmentation and Trace Collection. During task definition, we annotate every simulation frame with subtask labels. single recorded frame may correspond to multiple active sub-tasks. To ensure geometric consistency, traces within specific sub-task are projected into the coor19 Table 9. Statistics of the collected simulation dataset. We utilize the modified RoboTwin environment to generate spatial traces across 16 manipulation tasks. The table details the number of episodes, total traces (including both end-effectorcentric and objectcentric traces), and the data collection time in hours (using 8 NVIDIA H100 GPUs)."
        },
        {
            "title": "Traces",
            "content": "Collection Time (h)"
        },
        {
            "title": "Episodes",
            "content": "Traces Collection Time (h)"
        },
        {
            "title": "Click Bell\nClick Alarmclock\nBlocks Ranking Size\nBlocks Ranking RGB\nHandover Block\nHandover Mic\nHanging Mug\nMove Can Pot",
            "content": "8827 8222 8863 9135 4234 7562 2577 9275 17654 16444 17726 18270 8468 15124 5154 18550 10.8 4.2 41.6 64.2 16.4 27.9 17.8 28.9 Move Playingcard Away Move Stapler Pad Open Laptop Place A2B Left Place A2B Right Place Bread Basket Place Bread Skillet Place Burger Fries 8104 12340 3932 7329 6437 4337 1370 6005 16208 24680 7864 14658 12874 8674 2740 14.3 27.2 20.3 16.0 16.2 17.2 6.9 28.8 dinate system of that sub-tasks start frame. The extraction process involves two steps: 1. Subject Identification: We determine the active arm based on the instantaneous velocities of the left and right end-effectors. Similarly, the currently manipulated object is identified by calculating the velocities of all objects in the scene. 2. Trace Recording: The motion of the active arm is recorded as the end-effector-centric trace, while the position history of the manipulated object constitutes the object-centric trace. Spatial Referring Generation. To enhance the models capability in spatial reasoning and object grounding, we augment the simulation environment by algorithmically generating cluttered scenes. For each task, we instantiate random distractor objects around the target object within defined radius. These distractors are sampled from diverse asset library and include both distinct categories and identical categories to the target, necessitating fine-grained discrimination. We develop rule-based engine to synthesize unique referring expressions for the target object based on its spatial relationship with the environment. The engine parses the scene graph and selects the most concise unambiguous description from the following four categories: Object-to-Object Spatial Relations: Describes the targets position relative to unique reference object (e.g., in front of, behind, to the left/right of, next to). Proximity Comparisons: Describes the target based on its distance to reference objects or camera viewpoint (e.g., closest to the [reference], farthest from the camera). Ordinal Positioning: Identifies the target based on its sorting order along the Cartesian axes (e.g., leftmost, rightmost, frontmost, the second from the left). Attribute Comparisons: Distinguishes the target from same-category distractors based on physical properties such as scale (e.g., the largest, the smallest). QA Generation. Finally, after applying the RDP algorithm to downsample the traces, we obtain 443.1k unique spatial traces (see Fig. 2326). Using template-based generation, we further produce 1.329M QA pairs, including 914.1k end-effectorcentric and 415.1k object-centric samples. B. TraceSpatial-Bench Details The TraceSpatial-Bench is manually annotated, objectcentric evaluation suite. To the best of our knowledge, it is the first benchmark designed to evaluate 3D spatial trace prediction for object-centric tasks. The data is sourced from complex indoor scenes in ScanNet [22] and CA-1M [41], which include precise geometry annotations (e.g., absolute depth maps, camera geometry, and 3D occupancy). B.1. Benchmark Composition and Data Each scene in the benchmark is comprehensive and provides the following components: Visual Observation: The source image and its corresponding absolute depth map. Camera Parameters: Full intrinsic camera parameters (e.g., intrinsic and extrinsic). Object Definition: 2D mask identifying the specific object to be moved. Target Destination: 3D bounding box specifying the objects final intended location. Reference Trace: feasible 3D object-centric movement path, represented as series of 3D coordinates. We emphasize that this path is reference, not unique ground-truth; many other valid, collision-free paths to the destination may exist. This data structure allows for rich 3D analysis. Using the depth map and camera intrinsics, we can reconstruct 3D occupancy map of the environment. Similarly, we can isolate the point cloud of the target object by combining its 2D mask with the depth map. Although TraceSpatial-Bench is an inherent 3D benchmark, its components can be projected into the 2D image plane. We can project the 3D spatial trace and 3D destination box to create 2D paths and 2D boxes, enabling rigorous evaluation of model performance on analogous 2D tasks. 20 B.2. Benchmark Statistics The benchmark consists of 100 manually annotated scenes. The distribution of data sources and task categories is summarized in Tab. 10. Table 10. Statistics of the TraceSpatial-Bench Benchmark."
        },
        {
            "title": "Total",
            "content": "CA1M ScanNet Pick & Place Push & Pull 51 49 82 18 100 The language prompts have an overall average length of 21.73 words. As defined in our dataset, step refers to the number of spatial relations and associated objects within single prompt. As shown in Tab. 11, there is clear correlation: as the step count (i.e., task complexity) increases, the average word count of the prompt also increases, reflecting the need for more descriptive language. Table 11. Benchmark Prompt Statistics by Step Complexity."
        },
        {
            "title": "Step Count Number of Prompts Average Word Count",
            "content": "Step 2 Step 3 Step 4 Step 5 Step 6 Step 7 Step 8 7 16 16 28 21 7 5 10.14 14.81 17.19 22.29 27.48 30.86 34.60 C. RoboTracer Details C.1. Architecture We adopt NVILA [60] as the base model, including visual encoder, an LLM, and multimodal projector. We further incorporate universal spatial encoder and scale decoder into the base model to complete our architecture. Visual Encoder. The image encoder in the base model is siglip-so400m-patch14-448 [117], supporting 448 448 resolution for richer visual details. Rather than simply resizing the image to fixed resolution and producing the same number of tokens, this image encoder processes inputs at dynamic resolutions, yielding more visual tokens from higher-resolution images via finer patch division. This enables fine-grained vision-language understanding, crucial for tasks like point prediction that require detailed perception beyond VQA. Large Language Model. We adopt the Qwen2 LLM backbone from NVILA [60], which has been fully fine-tuned with extensive data during supervised training. This endows the model with rich visual knowledge, facilitating downstream 3D spatial understanding and reasoning tasks. Universal Spatial Encoder. We leverage the encoder of the universal feed-forward metric 3D reconstruction model, MapAnything [36], as our universal spatial encoder by removing the task-specific DPT and pose heads. During training, the encoder is kept frozen to preserve its pretrained representations. It provides strong 3D metric-grounded priors and flexibly accepts various geometric inputs (e.g., absolute depth, camera geometry). Incorporating more geometric cues leads to more accurate spatial representations, enhancing the VLMs 3D spatial awareness. Scale Decoder. We introduce an MLP-based scale decoder that maps the hidden embedding of special token, <SCALE>, to precise metric scale factor representing the ratio between predicted and real-world scale. The decoder is initialized from the scale head of MapAnything [36] and is trainable throughout our training process. By supervising the predicted scale with regression loss, we enhance the models metric awareness, going beyond standard classification-based next-token prediction. Multi-Modal Projector. To align multi-modal representations (e.g., image to language, spatial representations to language, language to scale), we use linear connectors, the same as NVILA [60], which is better than Q-Former, to allow the LLM to focus on fine-grained visual understanding and improve generalization. C.2. Training Data Here we highlight the training data used at each stage, including the number of samples per dataset and the overall total. See Tab. 12 for details. SFT stage. Specifically, in the first step of the SFT stage, i.e., Metric Alignment, we not only train spatial projector to align spatial representations and language space, but also train scale projector with the scale decoder to predict the metric scale factor, using the TraceSpatial (RGB+X ) dataset with 4.5M samples, where indicates arbitrary combinations of geometric annotations (e.g., absolute depth map, camera geometry). In the second step, i.e., Metric Enhancement, we use both TraceSpatial (RGB) and TraceSpatial (RGB+X ) datasets, yielding 9M samples to enhance 3D spatial understanding and reasoning (e.g., 3D spatial referring, measuring, tracing). To further improve instruction-following and basic capabilities, we incorporate auxiliary datasets: 965k samples from instructiontuned data (LLaVA-1.5 [51], LRV [49]), 321k from referring datasets (RefCOCO/+/g [110]), 3k from ShareRobotBench [78] benchmark training sets, and 127k from Em21 bSpatial [25] benchmark training sets. These additions help bridge distribution gaps between TraceSpatial and benchmark-style queries. The total number of samples used in this stage reaches 14M. RFT Stage. In the RFT stage, we train the model using TraceSpatial data annotated with detailed reasoning processes, including key intermediate steps (e.g., 3D spatial referring, measuring, scale) and final answers. To ensure both training efficiency and effective learning, we use moderately difficult samples (typically involving 46 reasoning steps), resulting in 120k-sample dataset. C.3. SFT Training Details We formulate the SFT training stage as follows: given dataset consisting of samples in the form of triplets (O, Q, A), where is sensor image with geometry information (either RGB or RGB+X ), is textual question, and is the corresponding answer. The answer may be direct response (e.g., sequence of 3D point coordinates) or include intermediate reasoning steps (e.g., key perceptual evidence followed by the final answer, such as 3D spatial referring and measuring). The training objective is to maximize the likelihood of generating the answer given the input pair (Q, A): LSFT = E(O,Q,A)D (cid:88) t=1 log πθ(yt O, Q, y<t), (3) where πθ is the models token distribution. The output model πSFT serves as the initialization for the next RFT stage, ensuring robust foundation for reinforcement learning. To be specific, our SFT consists of two steps. In the first step, Metric Alignment, only the spatial projector, scale projector, and scale decoder are updated by using the TraceSpatial (RGB+X ). We employ maximum learning rate of 1e-4, weight decay of 0, and warm-up ratio of 0.03. The 2B variant is trained with batch size of 7 per GPU, and the 8B variant with 3, both for one epoch. In the second step of Metric Enhancement, we fine-tune all model parameters without the universal spatial encoder using the datasets described in Sec. C.2. Training is conducted for one epoch with maximum learning rate of 5e-5. We use batch size of 6 per GPU for the 2B model and 2 for the 8B model. Other hyperparameters follow those in the first step. For more details, please refer to NVILA [60] settings during alignment and SFT. C.4. RFT Training Details During the RFT stage, we refine πSFT via GRPO [80], reinforcement learning method designed for efficiency and scalability. Unlike PPO [79], which relies on costly value network, GRPO estimates relative advantages by comparing intra-group rewards, reducing computation, and simplifying optimization. This makes it well-suited for reasoningintensive spatial tracing tasks. In detail, we modify R1V [120] to support our 3D-aware architecture. Training is conducted for two epochs with batch size of 1 per GPU and 8 outputs in GRPO. For details about hyperparameters, see R1-V [120]. C.4.1. Sampling Action Groups Given an input state = (O, Q), where denotes the visual encoding of the RGB or RGB+X observation and the textual encoding of the question, GRPO samples set of actions {a1, a2, . . . , aN } from the current policy πθ, initialized from πSFT. The sampling process is: ai πθ(a O, Q), for = 1, 2, . . . , (2) This strategy ensures diverse responses, promoting exploration and preventing premature convergence. C.4.2. Reward Design and Policy Update Each sampled action ai is assigned reward R(ai) based on verifiable criteria, yielding reward set r1, r2, . . . , rN . For spatial tracing tasks, R(ai) integrates three outcomebased and our proposed two process-based components. The outcome-based reward functions are defined as follows: Outcome Format Reward ROF . This component ensures structured and interpretable outputs by requiring the model reasoning within <think> . . . </think> and the final answer in <answer> . . . </answer>. reward is assigned 1 for strict compliance, 0 otherwise. Point Reward (RP ). This component primarily evaluates the consistency between the predicted trace ˆτ = { ˆpt}T t=1s start and end points (p1, pT ) and those of the ground-truth trace τ from the annotations of TraceSpatial . This process can be formulated as: to predefined format: (cid:2)f (p1, ˆp1) + (pT , ˆpT )(cid:3), RP = 1 2 (p, p) = max(cid:0)0, 1 p 2 (cid:1) (4) (5) Trace Reward (RP ). This component measures the consistency between the predicted trace ˆτ and the ground-truth trace τ , using distance metric d(τ, ˆτ ) (e.g., Dynamic Time Warping). This process can be formulated as: d(τ, ˆτ ) : RT = max(cid:0)0, 1 d(τ, ˆτ )(cid:1) All (u, v, d) points are normalized to the interval [0, 1], with the depth dimension scaled by the maximum scene depth. Nevertheless, the outcome-based rewards described above are metric-agnostic and thus fail to provide explicit supervision for the crucial perceptual evidence (e.g., 3D 22 Table 12. Details about the training datasets used in the SFT and RFT stages. M.A. and M.E denote the Metric Alignment and Metric Enhancement step in the SFT stage, respectively."
        },
        {
            "title": "Categories Datasets",
            "content": "SFT (M.A)"
        },
        {
            "title": "Spatial",
            "content": "TraceSpatial (RGB+X ) SFT (M.E)"
        },
        {
            "title": "Spatial",
            "content": "TraceSpatial (RGB), TraceSpatial (RGB+X ), ShareRobotBench [33], EmbSpatial [25] COCO [47], GQA [32], OCR-VQA [70], TextVQA [82], VG [39], LRV [49] RefCOCO/+/g [110] TraceSpatial (RGB+X ) w/ Reasoning Processing spatial measuring and referring) involved in trace generation. Accordingly, we need process rewards to more effectively guide the model. Howeever, most processbased reward mechanisms rely on Process Reward Model (PRM), typically fine-tuned large language model (LLM) or visionlanguage model (VLM) responsible for providing feedback. However, integrating such an approach into our framework presents two main challenges. (1) Since LLMs cannot process images, they are unable to verify whether the predicted coordinates correspond to the intended object. (2) Although VLMs combine visual and textual inputs, prior work [67] indicates that they may fail to exhibit precise visual understanding when interpreting textual coordinates or metric estimation. Because accurate verification of predicted coordinates or metric approximation is essential for reward assignment, additional or specialized methods must be employed to ensure reliable feedback. To address this issue, we introduce metric-sensitive rule-based process reward for spatial tracing, which eliminates the need for Process Reward Model. Specifically, our method directly evaluates crucial intermediate perceptual steps by leveraging the ground-truth stepwise annotations available in TraceSpatial . This approach differs from most existing methods that focus on process-based rewards [37, 55], which typically assume strictly sequential reasoning and rely on dedicated Process Reward Models (PRMs) for evaluating intermediate outputs. In contrast, we employ metric-sensitive rule-based process reward functions to evaluate intermediate perceptual outcomes in an order-invariant fashion. Our key insight is twofold: (1) Metric-sensitivity: Different spatial attributes (e.g., 3D spatial referring requires 3D points with depth information, while 3D spatial measuring demands precise numerical predictions, including scale) all involve metric scale and require distinct reward formulations due to their inherently different representations. (2) Order-invariance: The reasoning process in spatial tracing is not strictly sequential; for instance, identifying the placement position of the keyboard or the mouse first does not affect the final interpretation of the free area between the keyboard and the mouse. By using this rule-based yet flexible strategy, we overcome the limitations of relying on rigid sequential processes, instead allowing for more robust and adaptable spatial reasoning. We have two process-based reward functions: Process Format Reward RP . Similar to the Outcome Format Reward strategy, this component enforces structured and interpretable reasoning process, thereby facilitating accurate reward computation. In particular, the model is required to produce outputs in the following format: [Perception Type] [Target Object]: [Value] (6) where Perception Type must be one of three categories: Referring, Measuring, or Scale. The Target Object corresponds to uniquely the second leftmost entity (e.g., identifiable cup the second large cup from left or to right). The Value depends on the selected Perception Type: For Referring, the value should be normalized 2D coordinate with absolute depth of the form [(u, v, d)], where both and lie in the interval [0, 1000], and rounded to three decimal places. For Measuring, the value represents scalar indicating an objects length, width, or height, rounded to three decimal places, expressed in potentially different but appropriate units, as discussed in Sec. A.2.3. For Scale, the value is scalar representing the scale ratio between the predicted scene and the real-world scale, rounded to three decimal places. Below are examples to illustrate the expected format: [Position] [the second largest cup]: [(245, 147, 1.837)] [Measuring] [the height of the second largest cup]: 20 centimeters [Scale] [Scene]: 2.342 Accuracy Reward RAcc. The reward is computed only for steps annotated as key steps in TraceSpatial . In detail, we use regex matching to determine whether the Target Object in the current process format appears in the keystep annotations. If not, the step receives no reward. Since 23 the model has already undergone cold-start phase in SFT, it can interpret instructions and identify relevant target objects. Thus, failed match implies that the model cannot accurately refer to the object linguistically, and no reward is assigned. For each perception type, we apply specific metric to compute the reward: Referring: For 3D point modeling (u, v, d), we evaluate each component separately. For the 2D coordinates (u, v), we compute the L1 distance between the predicted and ground-truth points. If the error is within 10% of the images longer side, reward of 0.5 is assigned; otherwise, 0. For the depth d, if the predicted value falls within 30% of the ground truth, reward of 0.5 is given; otherwise, 0. Measuring: If the predicted value falls within 30% of the ground truth, the reward is 1; otherwise, 0. Scale: If the predicted value falls within 30% of the ground truth, the reward is 1; otherwise, 0. We prioritize the correctness of the final outcome over intermediate steps. To prevent reward accumulation from multistep processes, we scale the process reward by 0.25. The final reward function is defined as: ri = ROF (ai) + RP (ai) + αRP (ai) + αRAcc(ai) (7) where α is set to 0.25. By normalizing the rewards within the sampled group, we obtain the set of relative advantages {A1, A2, . . . , AN } defined as Ai = ri mean({rj}) std({rj}) , (8) which measures how each reward compares to the mean in units of standard deviation. We then update the policy based on these advantages, reinforcing actions with higher relative advantages while reducing the likelihood of those deemed less effective. To ensure stable reinforcement learning, the update is further constrained by minimizing the KL divergence between the updated policy and its reference counterpart, thereby promoting incremental and controlled policy adjustments. D. Experimental Setting and Details D.1. Experiments Compute Resources We conduct experiments on an H100 GPU cluster, with each node equipped with 8 GPUs. Object-centric Spatial Trace Generation from 3D Scanning Data. Given that the trace generation pipeline relies heavily on computationally intensive geometric operations (e.g., RRT* planning, OBB-SAT collision detection, and visibility checks) rather than neural network inference, we utilize high-performance CPU parallelization without GPU acceleration. The generation and refinement pipeline is executed on single high-end server equipped with dual Intel(R) Xeon(R) Platinum 8468 CPUs (totaling 96 physical cores and 192 threads). We configure the pipeline to utilize 180 concurrent processes to maximize throughput. The most time-consuming components are the RRT* obstacleaware planning, where we set high iteration count to ensure the generation of complex, high-difficulty traces, and the rendering of visualization assets for quality verification. Consequently, the complete generation, cleaning, and visualization process takes approximately 32 hours for CA-1M and 6 hours for ScanNet. Real-world Manipulation Video. For Droid, we use the ZED SDK to extract depth and camera intrinsics on single NVIDIA H100 GPU, which takes 46 hours in total. For the data processing pipelines of Droid and AgiBot, we use 64way parallel processing, which takes 5 hours and 27 hours, respectively. These processing times include the generation of visualization images and videos. Simulation Manipulation Video and Evaluation. The data collection time for the RoboTwin simulation is reported in Tab. 9. For the RoboTwin simulation evaluation of our model, we use 8 NVIDIA H100 GPUs, with total runtime of 36 hours. We further observe that RoboTwin executes significantly slower on H100 GPUs compared to RTX 4090 GPUs. Metric Alignment in SFT. The process is conducted on 8 nodes over 20 hours for 2B variants and 8 nodes over 50 hours for 8B variants. Both variants training use ZeRO-3. Metric Enhancement in SFT. The process is conducted on 8 nodes over 2 days for 2B variants and 8 nodes over nearly 1 week for 8B variants. Both variants training use ZeRO-3. Spatial Tracing in RFT. The process is conducted on 1 node over 3 days for 2B variants. However, our model is over twice as slow as other Qwen 2.5-VL-based methods [81, 120], mainly because they process only single RGB image during training and can leverage vLLM for group inference acceleration. In contrast, our method requires RGB+X inputs and modifies the original NVILA architecture, making it incompatible with vLLM or SGLang acceleration. D.2. Spatial Understanding Benchmarks We evaluate several publicly available spatial understanding benchmarks, strictly adhering to their official evaluation protocols. These benchmarks include CV-Bench [92] (covering 2D Spatial Relation, 3D Depth Order, and 3D Distance), the BLINK [28] validation set (Spatial Relation, Relative Depth), RoboSpatial [83] (configuration), and EmbSpatial [25]. We omit tasks that are not directly related to spatial understandingsuch as the 2D Counting task in CV-Bench and the Art Style or IQ Test tasks in 24 BLINKfrom our analysis. As all the selected benchmarks employ multiple-choice formats, we report accuracy as the evaluation metric. We compare three categories of models in our experiments: 1. Proprietary VLMs (e.g., Gemini-2.5-Pro [88]), which have demonstrated strong spatial perception, as discussed in Gemini-Robotics [89]. 2. Open-source VLMs trained on general VQA datasets (e.g., NVILA [60], Qwen3-VL [90]). 3. Spatial specialist models trained specifically on spatially relevant datasets, such as SpatialBot [7] and RoboBrain 2.0 [87], which offer fundamental spatial understanding capabilities. D.3. Spatial Measuring Benchmarks We evaluate our models spatial measurement capabilities on three distinct benchmarks: QSpatial [46] (Plus, ScanNet) and MSMU [11] bench. To determine the success of single question-answering instance, we adopt the evaluation protocol established in the original QSpatial paper. First, both the models predicted value and the ground-truth answer are converted to centimeters (cm). An answer is considered successful if the ratio of the predicted value to the ground-truth value falls within the range of [0.5, 2.0]. Formally, let vpred be the predicted value in cm and vgt be the ground-truth value in cm. The prediction is marked as successful if: 0.5 vpred vgt 2.0. D.4. 2D Spatial Referring Benchmarks We strictly follow the official evaluation protocols of several publicly available 2D spatial referring benchmarks, including Where2Place [112], RoboSpatial [83], and RefSpatialBench [129]. These benchmarks gauge models ability to predict one or more points given an image and textual instruction. The primary evaluation metric is the average success rate, which quantifies the fraction of predicted points that lie within the ground-truth mask for each sample. We compare two categories of models in our experiments: 1. General VLMs trained on general VQA datasets (e.g., Gemini-2.5-Pro [88], Qwen3-VL [90]). 2. Referring specialist models trained specifically on spatial referring/grounding datasets, such as Molmo [24] and RoboPoint [112], which offer fundamental 2D point referring capabilities. D.5. 2D Visual Trace Benchmarks We evaluate our method on public 2D visual trace benchmarks, including ShareRobotBench-V [33] and VABenchV [113]. ShareRobotBench-V focuses on end-effectorcentric 2D visual traces, while VABench-V emphasizes object-centric 2D visual traces. Unlike 3D spatial traces, 2D visual traces lack depth information and cannot assess collision-free properties. Therefore, these benchmarks evaluate the similarity between predicted traces and groundtruth references using metrics such as Discrete Frechet Distance, Hausdorff Distance, and Root Mean Square Error, where lower values indicate better performance. We compare two categories of models in our experiments: 1. General VLMs trained on general VQA datasets (e.g., Qwen3-VL [90]). 2. 2D visual trace specialist models trained specifically on tracing datasets, such as MolmoAct [42] and EmbodiedR1 [114], which offer fundamental 2D visual trace generation capabilities. D.6. Spatial Tracing Benchmarks We evaluate model performance in both 2D and 3D for spatial tracing tasks. D.6.1. 2D Evaluation For 2D evaluation, all 3D ground-truth attributes (e.g., generated 3d spatial trace, ground-truth 3D bounding box for objects final intended location) are projected into the 2D image plane. We use the following metrics: 2D Start Success: The predicted starting point must be located in the ground-truth 2D mask of the target object. 2D End Success: At least one of the final three (3) predicted points in the projected 2D trace must be located inside the projected 2D bounding box of the destination. D.6.2. 3D Evaluation Given the high difficulty of the 3D task, we adopt slightly more lenient spatial thresholds: 3D Start Success: The predicted 3D starting point must be in 20cm distance of the target objects point cloud. 3D End Success: At least one of the final three (3) predicted 3D points must be within 20cm distance of the 3D destination bounding box. Overall 3D Success: This composite metric requires task to first achieve both 3D Start Success and 3D End Success. If it passes, we then simulate the movement of the objects point cloud along the predicted trace. The path is considered successful if, during this movement, no more than 20% of the objects points intersect with the environmental 3D occupancy map (i.e., it is collisionfree). D.6.3. Baseline Performance This benchmark proves to be highly challenging. Even with these metrics, advanced baselines like Gemini-2.5-Pro and Qwen-3VL achieve an Overall 3D Success rate of less than 10%. In contrast, our model demonstrates significantly stronger performance, exceeding 30% success rate. D.7. Simulation Evaluation"
        },
        {
            "title": "We evaluate the capability of RoboTracer\nto support\nembodied tasks through spatial trace generation in the",
            "content": "25 RoboTwin simulation environment. Specifically, we select 19 tasks whose behaviors can be intuitively specified using spatial traces12 tasks have variants that appear in TraceSpatial , while 7 tasks are entirely novel. It is worth noting that the end-to-end model [6, 19, 56, 116, 123] results on these tasks are taken from the RoboTwin 2.0 online leaderboard (November 2025). All end-to-end models on the leaderboard are trained or fine-tuned for single specific task setting and are evaluated in the same task environment. In contrast, the simulation data in TraceSpatial were collected in newly designed scenes with our own instructions. Our evaluation environment configuration follows that of the RoboTwin 2.0 VLA benchmark (demo randomized). For Click Bell and Click Alarmclock, we generate endeffectorcentric traces, and the gripper executes the predicted trace after closing. For all remaining tasks, we generate object-centric traces: the gripper grasps the object corresponding to the starting point of the trace, follows the model-generated trace, and places the object at the final point. Both grasping and placing operations are implemented using the RoboTwin API. For complex multi-stage tasks, we decompose the instruction into several segments and execute them sequentially. The detailed evaluation results are provided in Tab. 13, and the visualization of the evaluation process are provided in Fig. 27-32. Under the traces predicted by our model, all tasks can be executed with high success rates, demonstrating both the strong spatial understanding capability of our model and its effectiveness in supporting embodied tasks."
        },
        {
            "title": "We construct the prompts used for RoboTwin evaluation",
            "content": "following the template below: Listing 12. Evaluation Prompts. Please predict 3D {object-centric end-effector -centric} visual trace to complete the task successfully. The task is \"<instruction>\". Your answer should be formatted as list of tuples, i.e., [(x1, y1, d1), (x2, y2, d2), ...], where each tuple contains the and coordinates and the depth of the point. ] D.8. Real-world Evaluation D.8.1. UR5 Manipulation We show the demo for UR5 Manipulation with human disturbance. In this case, RoboTracer runs at 1.5Hz. Significant shifts in the predicted end-effector-centric spatial trace endpoint, particularly at the 2D pixel level (u, v), can trigger motion interruption and re-planning. Specifically, for grasping, the 2D pixel level (u, v) of the predicted endeffector-centric spatial trace endpoint is fed into SAM2 [77] to generate segmentation mask, which filters the target objects point cloud from the scene captured by third-person Intel RealSense L515 depth camera. The extracted point cloud is input to AnyGrasp [27] to predict grasp pose in the camera coordinate frame. Using an eye-to-hand calibration method, the grasp pose is transformed into the UR5 robots base frame for execution. For final placement, the 2D pixel level (u, v) of the predicted end-effector-centric spatial trace endpoint is also converted to 3D coordinates using the cameras intrinsic parameters and depth data. The 3D point is then transformed into the robots coordinate system to identify the final placement loccation. Notably, during grasping and final placement, we do not directly use the depth predicted by the spatial trace. Since these points (e.g., grasp point, placement point) lie on contact surfaces (e.g., table, target object), more accurate values can be obtained using known camera intrinsics and observed depth data, enabling robust pick-and-place operations. In contrast, during movement, the waypoints are in mid-air without contact surfaces, so we rely on the models predicted 3D points. The robotic arms endeffector is guided to sequentially reach these 3D waypoint positions for manipulation. D.8.2. G1 Humanoid Manipulation The main pipeline is the same as UR5 Manipulation. For grasping, we employ head-mounted Intel RealSense D435 on the Unitree G1 humanoid to capture RGB-D images, which are processed by RoboTracer (i.e., RGB images, depth maps, camera intrinsic as model input) to extract 2D target coordinate (u, v) from the predicted end-effectorcentric spatial trace endpoint. These coordinates guide SAM2 [77] to generate segmentation mask, which filters the third-person D435 point cloud to isolate the target object. The filtered point cloud is then passed to AnyGrasp [27] to predict grasp pose in the third-person frame, which is transformed to the robots base frame using known camera-to-robot calibration. Since the waypoints during robotic watering are in mid-air and not on surface, 2Dto-3D projection using RGB-D cameras is not applicable. Therefore, the robot directly moves the end-effector to the predicted 3D spatial waypoints for manipulation. E. More Demonstrations Visualization of TraceSpatial-Bench . We present examples of TraceSpatial-Bench and our models rollouts in Fig. 20, 21, 22. Visualization of Simulation Data for Spatial Tracing. We present data examples in Fig. 23, 24, 25, 26. Visualization of Simulator. We present example rollouts https://robotwin-platform.github.io/leaderboard https://www.universal-robots.com/products/ur5e/ https://www.intelrealsense.com/lidar-camera-l515/ https://www.intelrealsense.com/depth-camera-d435/ Table 13. Performance on RoboTwin hard tasks. We report the success rate (%) compared to end-to-end and VLM-based models. Gray rows indicate the task variants that are not present in TraceSpatial ."
        },
        {
            "title": "Task",
            "content": "Click Bell Click Alarmclock Blocks Ranking Size Blocks Ranking RGB Move Can Pot Move Playingcard Away Move Stapler Pad Place A2B Left Place A2B Right Place Bread Basket Place Bread Skillet Place Burger Fries Place Container Plate Place Empty Cup Place Fan Place Mouse Pad Place Object Stand Stack Blocks Two Stamp Seal End-to-End Policy Vision-Language Model"
        },
        {
            "title": "Ours",
            "content": "ACT DP DP3 RDT π0 Qwen3-VL-8B Gemini-2.5-Pro RoboTracer-2B 3 4 0 0 4 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 14 0 0 6 3 0 2 0 1 0 18 1 1 1 1 0 0 0 9 12 0 0 12 11 0 1 1 2 1 27 17 7 2 0 5 2 0 3 11 1 5 21 22 2 1 6 4 1 4 45 11 10 1 11 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 2 1 0 0 0 0 0 2 0 0 0 0 96 79 89 96 27 94 18 84 93 82 48 99 52 85 66 30 38 33 7 with RoboTracer predictions in Fig. 27, 28, 29, 30, 31, 32. F. Discussion on Limitations and Future Work Despite achieving promising results, our model still has limitations. First, the spatial tracing task requires the model to fully understand 3D space and interpret various spatial constraints from instructions. This demands strong capabilities in perception, reasoning, and generalization across tasks, scenes, and objects. As result, VLM-based architecture is essential. However, VLMs often suffer from slow inference, making them less suitable for highly dynamic environments. Acceleration techniques such as quantization or parallel inference may help mitigate this limitation. Second, spatial tracing requires predicting 3D positional sequences with real-world scale information (e.g., depth), which is particularly challenging for VLM-based architectures that are primarily pre-trained on 2D data. While RoboTracer makes significant efforts, such as collecting metricgrounded data during SFT, aligning the new inputs via universal spatial encoder, supervising scale factor prediction through regression loss, using metric-sensitive function to generate rewards, the resulting improvements in 3D metric-grounded spatial understanding, especially driven by scaling up data, are limited compared to the gains observed in 2D spatial reasoning tasks (e.g., RoboRefer [129]). Recent work like DepthLM [8] further 27 highlights this limitation. Despite processing over 30M+ images with unified camera intrinsics, DepthLM sacrifices language understanding to focus solely on depth estimation at the pixel level, achieving performance only comparable to task-specific expert models. These results indicate that enhancing the 3D metric-grounded capabilities of VLMs, while maintaining their language understanding for reasoning, remains an open and challenging problem. We argue that scaling up data alone yields limited benefits due to two key reasons: (1) The lack of scene-level 3D representation that is naturally aligned with language and rich enough to encode fine-grained geometry. Although this work attempts alignment in the initial SFT stage, the spatial encoders features remain poorly aligned with the language space, and supervision via next-token prediction alone may be insufficient. (2) The lack of comprehensive, scale-aware supervision signals. This work only supervises simple metric scale factor, but dense geometric outputs (e.g., depth maps), which are crucial for full 3D understanding, are not utilized due to computational overhead. Overall, enabling VLMs to fully understand 3D space requires more than large-scale data. Therefore, better input representations and richer supervision signals, especially at both the input and output levels, are promising direction for future research to further advance spatial intelligence. Figure 20. Visualization of TraceSpatial-Bench and RoboTracers rollouts. The red mask indicates the ground-truth starting point, and the purple 3D bounding box denotes the ground-truth endpoint. We show the 2D projection of RoboTracers predicted trace. 28 Figure 21. Visualization of TraceSpatial-Bench and RoboTracers rollouts. The red mask indicates the ground-truth starting point, and the purple 3D bounding box denotes the ground-truth endpoint. We show the 2D projection of RoboTracers predicted trace. Figure 22. Visualization of TraceSpatial-Bench and RoboTracers rollouts. The red mask indicates the ground-truth starting point, and the purple 3D bounding box denotes the ground-truth endpoint. We show the 2D projection of RoboTracers predicted trace. 30 Figure 23. RoboTwin Data Visualization 31 Figure 24. RoboTwin Data Visualization Figure 25. RoboTwin Data Visualization 33 Figure 26. RoboTwin Data Visualization 34 Figure 27. Visualized RoboTwin simulation evaluation process. Figure 28. Visualized RoboTwin simulation evaluation process. 36 Figure 29. Visualized RoboTwin simulation evaluation process. 37 Figure 30. Visualized RoboTwin simulation evaluation process. Figure 31. Visualized RoboTwin simulation evaluation process. 39 Figure 32. Visualized RoboTwin simulation evaluation process."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "CASIA",
        "School of Software, Beihang University",
        "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
    ]
}