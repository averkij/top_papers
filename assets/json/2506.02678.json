{
    "paper_title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression",
    "authors": [
        "Zhong-Zhi Li",
        "Xiao Liang",
        "Zihao Tang",
        "Lei Ji",
        "Peijie Wang",
        "Haotian Xu",
        "Xing W",
        "Haizhen Huang",
        "Weiwei Deng",
        "Ying Nian Wu",
        "Yeyun Gong",
        "Zhijiang Guo",
        "Xiao Liu",
        "Fei Yin",
        "Cheng-Lin Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon."
        },
        {
            "title": "Start",
            "content": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression Zhong-Zhi Liχπ , Xiao Liangργ , Zihao Tangφ , Lei Jiφ , Peijie Wangχπ , Haotian Xuγ , Xing Wπ , Haizhen Huangφ , Weiwei Dengφ , Ying Nian Wuρ , Yeyun Gongφ , Zhijiang Guoθ β , Xiao Liuφ , Fei Yinχπ , Cheng-Lin Liuχπ χ School of Artificial Intelligence, Chinese Academy of Sciences π Institute of Automation, Chinese Academy of Sciences γ Tsinghua University ρ University of California, Los Angeles φ Microsoft β Hong Kong University of Science and Technology θ Hong Kong University of Science and Technology (Guangzhou) https://github.com/zzli2022/TLDR"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoningespecially during inference with extremely long outputshas drawn increasing attention from the research community. In this work, we propose dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the models System-1 and System-2 data to eliminate redundant reasoning processes while preserving the models reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon. 5 2 0 2 3 ] . [ 1 8 7 6 2 0 . 6 0 5 2 : r Comparison of TLDR and baseline models in terms of average accuracy and token compression ratio. Higher values on both axes indicate better performance. Equal contribution. Work done during internships at Microsoft. Correspondence to Zhijiang Guo, Xiao Liu and Cheng-Lin Liu. (cid:66): zhijiangguo@hkust-gz.edu.cn; xiaoliu2@microsoft.com; liucl@nlpr.ia.ac.cn. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "He that can have patience can have what he will. Benjamin Franklin Recent efforts have developed reasoning-oriented Large Language Models (LLMs) capable of solving complex tasks. These models progressed from System 1 to System 2 paradigms [54, 28]. System 1 implementations, such as GPT-4o [39], LLaMA-3 [12], leverage rapid intuitive processing for immediate responses but struggle with complex reasoning tasks. In contrast, System 2 architectures such as DeepSeek-R1 [10] are fine-tuned with extended thinking chains to promote deliberate analysis through iterative self-assessment, error mitigation, and verification. However, reasoning-oriented LLMs, which employ system-2 reasoning, tend to engage in excessive deliberation even for simple problems. This results in unnecessary exploration and planning, ultimately impairing their efficiency and practicality. Direct approaches aimed at addressing the issues of cognitive redundancy and excessive deliberation within reasoning LLMs. Training-free methods [50, 52, 14] include some that control the internal states of the model during reasoning through prompts or confidence-based techniques to compress the model. Alternatively, the pathway, exemplified by model merging, involves intervening in the parameters of the reasoning LLM to produce relatively concise solutions. Training-based methods primarily focus on sampling and synthesizing relatively concise reasoning paths on specified problem sets through various strategies [47, 51, 33]. These methods involve performing reinforcement learning [34, 19, 31, 1] or supervised fine-tuning (SFT) [5] on reasoning LLMs, enabling the model to learn to generate more concise yet still correct reasoning paths. Such methods typically require careful collection of problems and precise control of the data ratio for different lengths to achieve good results, leading to complex process of parameter tuning and data construction. For example, TOPS [51] requires pre-processing steps to manually label SFT data to construct length-sensitive models, while CoT-Valve [33] generates data by creating intermediate models through model interpolation for sampling. This construction process is often tedious [51], computationally expensive [1], or difficult to control for quality [33]. Demystifying Short/Long CoT Mixture in LLM Thinking Compression System-1 data (Short CoT on GSM8K-like easy problems) reduces reasoning redundancy on all problem levels. (Section 2) Short CoT reduces reasoning redundancy on simple questions and demonstrates generalization across varying levels of problem difficulty. System-2 data (Long CoT only on s1-like difficult problems) helps maintain performance. (Section 2) Incorporating small proportion of Long CoT, particularly on challenging problems, can mitigate the accuracy degradation introduced by short CoT, while long CoT on simple questions doesnt help much. Dynamic re-weighting of System-1/2 data builds effcient LLM Reasoning Compression. (Section 3.1) Driven by simple intuition, we design dynamic reweighting algorithm for system-1/2 data, achieving strong performance in LLM reasoning compression. We investigate the effects of mixing short CoT and long CoT data on compressing reasoning. Our findings suggest that long CoT and short CoT induce divergent optimization directions in the models reasoning behavior. Increasing the proportion of short CoT encourages more concise reasoning patterns, but may lead to decline in reasoning accuracy. In contrast, raising the proportion of long CoT helps preserve reasoning performance on complex tasks, though at the expense of reduced compression efficiency. This naturally raises the question: Can we identify an optimal Long-to-Short data mixture that strikes the best trade-offmaximizing reasoning efficiency while maintaining accuracy? We base our approach on an intuitive motivation: when model is thinking too long, it should reweight more intuitive reasoning paths to simplify the thinking process. Conversely, when the thinking is too direct, it should incorporate more slow-thinking reasoning chains to encourage deeper contemplation. We propose dynamic Thinking Length Data Re-Weighting method (TLDR), which dynamically balances the models complex reasoning using long CoT and efficient reasoning using short CoT data, enabling the model to eliminate redundant cognitive processes. First, we construct short CoT data for simple problems and long CoT data for complex problems. The model begins with an initial ratio and 2 Figure 1: Impact of Combining Short CoT and Long CoT in Fixed Ratios on Thinking Compression Performance and Token Cost. We assessed the variation decay rate in output token length and accuracy on datasets of various question difficulty, spanning from GSM8K to AIME. The Normalized Token/Acc metric detail please refer to Equ. 11 and Equ. 12. performs reasoning compression using mixed data. After completing compression cycle, the model re-evaluates the expected benefits of System-1 CoT data and System-2 CoT data to achieve improved performance. Specifically, and in line with intuition, System-1 CoT data can enhance efficiency, so we use an efficiency metric to measure the expected benefit of System-1 data. System-2 CoT, on the other hand, improves reasoning accuracy, and we use an accuracy metric to measure the benefit of System-2 data in terms of reasoning capability. Compared to various methods requiring fine-tuning data with different reasoning lengths, our approach enables dynamic ratio learning by utilizing the self-sampled LongCoT model and the short CoT data constructed by the original instruct/base model. Through experiments on DeepSeek-Distill-7B/14B, our model achieves excellent compression results on the 7B/14B models, with only slight decrease in reasoning capability."
        },
        {
            "title": "2 Rethinking Short-Long CoT in Thinking Compression",
            "content": "We first constructed the short CoT data using simple problems and recorded how, as training steps increased, this subset contributed to token compression and accuracy retention across datasets of varying difficulty in math benchmarks. We find that short CoT thinking data for simple problems (System-1 data) can help compress the token usage across questions of various difficulty levels. We leverage the short-cut solutions obtained from simple questions in GSM8K to fine-tune the model and then observe the token compression rates and accuracy drop rates across four datasets, ranging from simple to difficult: GSM8K, MATH500, AMC, and AIME. As shown in Figure 1, directly fine-tuning the long CoT model with short CoT data achieves good length compression for both simple and complex problems. We were pleasantly surprised to see that this form of length compression generalizes well across 3 Figure 2: Overview of TLDR: Starting with System-2 model, we iteratively update it on both ShortCoT and Long-CoT samples. The ratios of both data sources are adjusted every several steps based on the current average model accuracy and token length from the validation set until convergence. questions of all difficulty levels, and that it maintains strong performance on simple questions. However, this approach comes at cost, as it leads to significant decrease in reasoning ability on difficult problems. As this portion of the data is derived from intuitive CoT reasoning on simple problems, we denote it as System-1 data. It seems that directly using short CoT fine-tuning can only encourage the reasoning LLM to retain its System 1 reasoning abilities, while its ability for System 2 reasoningslow and cautious thinking for complex problemsis largely lost. We find that long CoT thinking data for difficult problems (System-2 data) can help maintain the models performance on challenging tasks, while simple question doesnt help much. We sample with the s1 [37] like hard question prompt and then blend the System-2 data into the previous System-1 thinking dataset at fixed short CoT vs. long CoT ratio: 0.8:0.2. We then observe the token compression rates and accuracy drop rates across four datasets. It is worth noting that, by contrast, when we mix more long CoT data from simpler questions, the model still experiences significant drop in performance on difficult questions. Refer to the middle and bottom parts of Figure 1, where we mix the long CoT sampled from challenging problems with the short CoT from simple problems. As baseline, we also mix long CoT and short CoT from simple problems. The long CoT from difficult problems achieves lower accuracy drop rates across different datasets while maintaining comparable token compression rates. We are unable to recover the original performance simply by using long CoT data from simple questions through data replay. Similar to the deliberate reasoning characteristic of the System-2 process on difficult problems, we refer to this part of the data as System-2 data. key question we directly address is whether direct mixing ratio of the two types of data(System1/2 data), can be employed for post-training the long CoT model, resulting in solution that eliminates redundancy without compromising performance. Based on these observations, we propose dynamic approach aimed at identifying the optimal Thinking Compression data."
        },
        {
            "title": "3 Thinking Length Dynamic Re-weighting",
            "content": "3.1 Short-Long CoT Reweighting with Relaxed Optimization We formalize the thinking compression problem as an optimization task to determine the optimal ratio between System-1 and System-2 reasoning. We expect the model trained on mixed data to approach the superior performance of System-1 and System-2 in specific evaluation metrics. For model and input problem x, we define (y),C(y) as the token length and correctness of LLM output text y. We represent the System-1/2 ability bound as φsys-i,bound(x), in the following sections, we will abbreviate as φsys-i,bound(x) min θ , α(0,1) L(θ , α) = 2 i=1 αi δi δi = φsys-i,bound(x) φsys-i,θ (x) 4 (1) (2) Algorithm 1 Long-to-Short (L2S) Dynamic Reweighting Pipeline Require: Domain data Dsystem-1, Dsystem-2, Ddev; training steps ; batch size b; step size η; smoothing parameter [0, 1] (e.g., = 104 in our implementation) Initialize proxy weights θ0 Initialize mixture weights α0 = (1/2, 1/2) for = 1 to do Let denote token length of example (with L) Compute benefit of fine-tuning with System-1 data: λsystem-1 and System-2 data λsystem-2 Update weights (entrywise exponential): α αt1 exp(η λt ) α Renormalize and smooth: αt (1 c) i=1 α Update proxy model weights θt using L(θt1, αt ) (e.g., via Adam, Adafactor) + cu [i] end for return 1 t=1 αt of which, φsys1,θ can be regarded as metric for measuring the efficiency of the System-1 models. φsys2,θ can be regarded as an accuracy metric. In this way, the overall optimization objective is to minimize the gap between the model and the efficiency upper bound of System-1, as well as the reasoning capability upper bound of System-2, while simultaneously optimizing the model parameters to maximize both reasoning performance and efficiency. φsys1,bound = Edev[T (Ms(x))] φsys2,bound = Edev[C(Ml(x))] (3) (4) Setup for System-1/2 Mixed Data. Since System-1 can provide fast and intuitive answers to simple problems, we use the short CoT model to modulate the data for the System-1 model. Since System-2 is designed to execute slow, logical reasoning for challenging problems, we employ the long CoT model to sample prompts from S1 [37], retaining only the correct responses. Finally, we obtain Dsystem1 =<Simple Question, Short CoT> instruction pairs. For the harder problems within the System-1 domain, we used the LongCoT model for sampling, resulting in large amount of Dsystem2 =<Hard Question, Long CoT> instruction data. 3.2 Long-to-Short Data-Reweighting Tuning. Step 1: Estimate the ideal upper bounds of efficiency and performance. During training, we aim to continuously adjust the ratio of System-1 and System-2 data in the post-training phase, ensuring that the model retains the reasoning capabilities of the original long CoT model while achieving the efficiency of the short CoT model. Therefore, we set the accuracy upper bound, φsys2,bound, of the model obtained through mixed training to match the accuracy of the original long CoT model, while setting the token lower bound, φsys1,bound, of the mixed model to correspond to the data lower bound of the short CoT model we constructed. φsys-2, bound = φsys-2,L = ˆEdev[CL(x)] = 1 i=1 1[Correct(yL )] φsys-1, bound = φsys-1,short = ˆEdev[T S(x)] = 1 i=1 Token(yS ) (5) (6) Step 2: Thinking Compression Post-Train with dynamic System-1/2 reasoning weights We dynamically evaluate the utility of System-1 and System-2 reasoning data during training, and, guided by the performance of reference model, adjust the sampling ratio between the two data types in real time to optimize training effectiveness. λsys-1 = max λsys-2 = max (cid:18) φsys-1, bound φsys-1,θproxy φsys-1,θs φsys-1,θl (cid:18) φsys-2, bound φsys-2,θproxy φsys-2,θl φsys-2,θs (cid:19) , 0 (cid:19) , 0 5 (7) (8) Model Accuracy Generation Length ASDiv GSM8K MATH AIME AMC Minerva Avg. ASDiv GSM8K MATH AIME AMC Minerva 86.8 R1-Distill-Qwen 80.4 TALE-EP 86.0 ConciseCoT Avg. Merging 92.8 Task-Arithmetic-Merging 83.3 74.4 Ties-Merging 75.9 Ties-Dare-Merging 86.6 Overthink 90.6 ThinkPrune CoT-Valve 59.4 93.0 TLDR +6.2 80.5 R1-Distill-Qwen 77.5 TALE-EP 74.0 ConciseCoT Avg. Merging 94.8 Task-Arithmetic-Merging 86.5 79.6 Ties-Merging 80.7 Ties-Dare-Merging 79.3 Overthink 80.6 ThinkPrune CoT-Valve& 72.9 88.0 TLDR +8.0 89.4 89.1 89.5 70.1 84.6 69.7 72.3 89.6 92.1 88.4 87.7 -1.7 92.5 92.4 92.4 90.3 86.5 91.3 91.8 92.3 93.7 92.0 90.9 -1. 7B Models 81.5 80.0 79.6 39.6 63.5 42.5 45.6 79.6 86.2 80.6 83.1 +1.6 46.0 42.3 46.0 29.8 39.6 23.2 24.3 45.2 45.6 41.9 44.5 -1.5 14B Models 79.6 80.3 82.3 55.0 55.3 72.5 75.3 82.8 88.7 83.5 83.8 +4.2 48.2 50.0 47.1 44.1 36.0 37.1 34.9 45.6 50.7 47.8 48.7 +0. 769 72.2 509 69.3 532 71.5 622 48.4 61.0 321 47.2 1114 49.6 1036 773 71.1 653 74.8 140 65.9 72.8 147 +0.7 -622 476 71.7 369 72.5 369 72.2 167 61.3 238 58.6 242 64.8 274 65.4 451 72.3 379 75.6 204 71.4 73.5 158 +2.1 -318 42.9 40.0 41.7 0.05 20.0 13.6 14.6 38.7 43.3 41.2 41.2 -1.7 43.4 49.2 51.6 10.8 13.3 25.4 25.4 45.8 50.8 45.0 43.3 -0.1 86.8 84.3 86.2 58.6 74.6 59.8 65.4 87.2 91.0 84.2 87.4 +0.6 86.4 85.4 85.6 73.0 74.2 82.6 84.8 88.0 89.0 87.0 86.6 +0. 554 450 457 8552 383 2475 2073 555 587 514 253 -301 679 555 555 366 368 542 467 679 563 576 240 -439 2861 1994 2330 8540 907 4086 2934 2898 2379 2144 1556 -1305 2951 2248 2066 5158 870 1919 1870 2893 2177 2652 2092 -859 3347 6820 4510 2242 6520 3892 3347 6587 4245 8544 8501 8542 794 2500 1311 4306 6767 5195 2938 5483 3698 3407 6766 4558 2762 6207 3739 2172 6397 4278 6368 3386 1451 -452 -1124 -1896 6701 4584 6551 4179 6267 3878 6364 5668 2813 1411 5913 3158 5747 3182 6700 4464 5778 3327 6686 4392 6403 3839 -745 - 3270 2731 2605 1084 1050 1850 1877 3715 2234 2833 2177 -1093 A.C.R. 22.3% 12.7% 3.2% 61.3% 0.1% 8.3% 0.1% 12.6% 26.8% 44.9% 15.4% 18.8% 30.5% 60.2% 31.8% 33.0% 1.6% 22.8% 16.7% 35.8% Table 1: Performance comparison of TLDR with baselines. The accuracy is measured by sampling multiple responses from the LLMs and taking the average to reduce variance. * denotes the CoTValve [33] result that we reproduced using the official dataset. refers to TLDR in comparison with Original. Math and Minerva refer to MATH500 and MinervaMath datasets, respectively. A.C.R. means the token compression ratio computed by Eq. 9. In the table: yellow represents prompt-based methods; green highlights Merging-based methods; red indicates SFT-based and RL-based methods. Model Accuracy Generation Length GSM8K MATH AIME AMC Minerva Avg. GSM8K MATH AIME AMC Minerva Avg. 7B Models Original Model -MixChain-Z-GSM8K& -Static-Mixture -TLDR 89.4 88.4 87.1 87. 86.8 84.2 84.8 87.4 42.9 41.2 39.7 41.2 81.5 80.6 73.1 83.1 46.0 41.9 35.5 41.0 69.3 67.3 64.0 68.1 554 514 236 2861 2144 1221 1556 6820 6397 5322 6368 4510 4278 2560 3386 3347 2172 1544 1434 3618 3101 2177 2599 Table 2: Performance comparison of TLDR with static baselines. The accuracy is measured by sampling multiple responses from the LLMs and taking the average to reduce variance. & denotes the CoT-Valve [33] result that we reproduced using the official dataset. Math and Minerva mean MATH500 and MinervaMath datasets."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Datasets and Metrics. Following prior efforts, we evaluate TLDR on several widely-used benchmarks that span broad range of difficulty levels, including ASDiv [35], GSM8K [9], MATH-500 [17], and AIME2024 [2], AMC [3] in Table 1. To ensure the stability of the evaluation, we performed multiple samplings for each dataset and took the average accuracy. For GSM8K, MATH500, and GPQA, we sampled each question 4 times and took the average accuracy of the 4 solutions. For AIME24 and AMC23, we sampled each solution 8 times and took the average accuracy of the 8 solutions. The token count was calculated using the corresponding tokenizer of the language model, and our evaluation environment was modified using the Skythought library. Baselines. We compared three types of baselines: 6 Model Accuracy Generation Length GSM8K MATH AIME AMC Avg. GSM8K MATH AIME AMC Avg. Original Model -TLDR -L1-same -L1-lower -L1-higher 89.4 87.7 86.4 86.4 86.1 DeepSeek-R1-Distill-Qwen-7B 86.8 87.4 88.6 87.6 88. 42.9 41.2 42.2 45.1 45.5 81.5 83.1 84.6 84.6 83.3 75.2 74.8 75.4 75.9 75.8 554 253 301 312 292 2861 1556 2301 1831 2589 6820 6368 5875 5675 4510 3386 3784 3807 3746 3686 2891 3056 2906 3158 Table 3: Performance comparison of TLDR with budget-aware baseline, L1 [1]. The accuracy is measured by sampling multiple responses from the LLMs and to reduce variance. The terms same, lower, and higher refer to setting the budget to match our results, 20% lower, and 20% higher, respectively. MATH means MATH500 dataset. Model Accuracy Generation Length GSM8K MATH AIME AMC Avg. GSM8K MATH AIME AMC Avg. DeepSeek-R1-Distill-Qwen-7B System-1 Short CoT Ablation Original Model -TLDR-Easy -TLDR-Medium -TLDR-Hard 89.4 87.7 88.2 83.6 86.8 87.4 86.2 80.2 42.9 41.2 41.5 30. 81.5 83.1 31.3 65.3 60.1 59.9 61.8 64.8 554 253 318 495 2861 1556 2083 2970 DeepSeek-R1-Distill-Qwen-7B System-2 Long CoT Ablation Original Model -TLDR-Easy -TLDR-Medium -TLDR-Hard 89.4 83.9 91.6 87.7 86.8 86.8 87.6 87.4 42.9 42.5 40.4 41.2 81.5 83.4 81.5 83.1 60.1 74.2 75.3 74.8 554 446 542 2861 2639 2761 1556 6820 6368 6604 6874 6820 6580 6553 6368 4510 3386 3945 4947 4510 4047 4116 3386 2949 2313 3238 2949 3428 2950 2828 Table 4: An ablation study on the difficulty levels of ShortCoT and LongCoT was conducted during the construction of the ShortCoT and LongCoT dataset. The accuracy is measured by sampling multiple responses from the LLMs and taking the average to reduce variance. & denotes the CoTValve [33] result that we reproduced using the officially dataset. MATH means MATH500 dataset. Prompt-based methods. We compared our approach with the well-known prompt-based baselines in the community, including TALE-EP [14], which requires the prompt to be as simple as possible, and ConciseCoT [26], which demands the use of the most concise CoT steps during step-by-step reasoning. Model-Merging-based Methods. Model Merging leverages the rich knowledge from short CoT Instruct and the long CoT model for model fusion, aiming to achieve the shortest yet most effective reasoning process. We compared this approach with the Avg. Merging method used in Kimi1.5 [43, 45] and some advanced merging method, like Task-Arithmetic-Merging, Ties-Merging, Ties-Dare-Merging, discussed in [45]. Reward-based Methods. ThinkPruner [20] uses progressive compression of RL training length to improve the effectiveness of context utilization during exploration. SimPOshortest was introduced in Overthink [24] to adjust the effectiveness of the RL algorithm by length-guided RL training. We find that our approach can maintain accuracy across datasets of varying difficulty while achieving satisfactory compression rates. In contrast, merging-based methods still suffer from significant performance drops on challenging problems. Compared to CoT-Valve and ThinkPrune, our method attains excellent compression rates, particularly on the ASDiv and GSM8K datasets, where overexploration tends to occur. CoT-Valve, as an SFT-based approach, requires carefully designed model blending and the construction of length-diverse datasets for dynamic learning. In contrast, our method only requires straightforward data sampling and adaptive mixing ratios, achieving adaptive reasoning in much simpler way. 7 Figure 3: Comparison of accuracy and generation length between Vanilla CoT and our TLDR method on four benchmark datasets (GSM8K, MATH500, AIME, AMC) using DeepSeek-R1-Distill-Qwen models. TLDR consistently reduces generation length while maintaining or improving accuracy across both 7B and 14B model scales. Figure 4: Frequency comparison of different keywords. The figure illustrates the distribution of exploratory, checking, and reflective keywords across datasets. Exploratory Keywords: wait, Reflective Word: but, Checking Words: make sure/confirm/verify/check, TLDR significantly reduces the presence of such words, reflecting its ability to produce streamlined and efficient reasoning steps. 4.2 Comparison of Dynamic Compression vs. Static Compression Data. We compared our dynamic algorithm with several simple static data mixing methods, including directly mixing data according to simple ratio and constructing length-uniform data using parameter interpolation. MixChain-Z-GSM8K is Long2Short dataset proposed by CoT-Valve in Table 2. 4.3 Ablation of Different System-1/2 Source We also discovered that incorporating higher-difficulty CoT data into short-long mixed dataset could effectively eliminate redundancies in CoT for their compressed version. However, direct mixing could lead to performance degradation. After introducing dynamic ratio method, we found that flexibly adjusting the ratio could effectively maintain performance in Table 4. We categorized the sources of questions in the thinking compression data into three difficulty levels: easy, medium, and hard. easy questions are from GSM8K, medium questions are from the training set of MATH500, and hard questions are from the s1 prompt questions. Short CoT Compression Generalization Analysis of Easy-to-Hard. We tested the construction of System-1 data, examining the composition of data from different thinking compression sources. Our experiments found that constructing data based on low-difficulty problems could significantly reduce the token count of high-difficulty problems while maintaining performance. We found that using lower-difficulty problems to construct thinking compression data for redundancy removal can further generalize to higher-difficulty problems. Long CoT Performance Generalization Analysis of Hard-to-Easy We also conducted an analysis of the following aspects: during the sampling of long CoT, we utilized data from three distinct sourceseasy, medium, hard prompt. Our findings reveal that only by constructing long CoT using hard problems and dynamically adjusting their proportions during training can we recover the original performance associated with long CoT. This strategy effectively mitigates the risk of forgetting in reasoning capabilities during continual learning. 8 4.4 Comparison with Token Budgeted-Aware Model We compared our redundancy reduction method with both quota-controlled models and reasoning models under the same token budget, in order to evaluate the effectiveness of our approach relative to explicit quota-based control. The results show that our method achieves higher reasoning accuracy than both the L1 [1] baselines under the same token quota. Furthermore, our approach demonstrates more efficient utilization of context length and does not require explicitly specifying reasoning quota, offering more flexible and adaptive inference mechanism. TLDR demonstrates stronger compression efficiency on simple problems. 4.5 Analysis of Thinking Patterns: Reflections & Solutions We compared our method with other thinking compression methods in terms of their impact on changes in cognitive patterns [49] of the solution in Figure 3. We performed fine-grained statistical analysis on the results across different datasets and various levels of MATH500. Our analysis demonstrates that our approach effectively compresses the internal redundancy and reflects the properties of the solution patterns. TLDR effectively reduces the reliance on such macro reasoning patterns in benchmarks like GSM8K and MATH500, thereby avoiding excessive allocation of computational budget. Notably, for more challenging problems, the model still retains significant degree of macro reasoning behavior to preserve its System-2 reasoning capabilities."
        },
        {
            "title": "5 Related Work",
            "content": "Efficient System-2 Reasoning. Despite the strong generalization and reasoning abilities demonstrated by the system 2 reasoning paradigm, the auto-regressive nature of LLMs imposes significant reasoning burden, limiting their application in agent-based or edge scenarios [46]. To address this, various approaches have emerged to improve the reasoning efficiency. These methods can be broadly categorized into two types. One category focuses on building adaptive reasoning-budget. Within this, some training-free methods like CoD [50] and TALE-EP [14] impose budget constraints to control overall reasoning cost. Budget-sensitive models such as L1 [1], TOPS [51], o1 Pruner [32], and K1.5 [43] add length penalties during the post-training. Some work [33, 22, 55] synthesizes diverse-length CoT data, while TOPS [51] samples budget-sensitive versions using data model, and C3oT [23] compresses original LLM output to train jointly on shorter CoTs. Other approaches involve external models or switching mechanisms [53] to allocate budgets. Routellm [38] uses multiple routers to find the most suitable reasoning model, while Self-REF [8] employs confidence scores to route based on reasoning difficulty. Another category concentrates on building efficient representations. TokenSkip [47] selects data based on token importance for compressed reasoning and more concise thought chains. COCONUT [15] explores more efficient reasoning in the latent space. ICoT-KD [6] and CCoT [6] attempt to build more efficient reasoning strategies in the hidden space, while Token Assorted combines hidden space and text-based reasoning to balance interpretability and efficiency. Heima [42] extends hidden-space reasoning to multi-modal models. Unlike prior efforts, TLDR compresses the length of the reasoning chain without introducing complex data construction and without compromising the original reasoning LLMs inferential representations. Data Balancing in Pre/Post Training. The quality and proportion of data are critical during both the pre-training and post-training phases. In the pre-training stage, data quality and proportion are primarily managed through filtering and reweighting. Pre-training data filtering, extensively studied to boost model performance and training efficiency [29, 4], typically involves steps like language filtering [25, 7], quality filtering [41, 40], content filtering [48, 30], and deduplication [18, 27]. While these methods significantly enhance corpus quality, their static nature can hinder dynamic adjustments during training, potentially discarding valuable data [36] and introducing biases [13, 30, 11]. Similarly, in the post-training stage, an appropriate proportion of data with varying characteristics is crucial for optimizing final performance [44, 56]. For example, DeepMath-103K generates large volume of data with evenly distributed difficulty for training [16], SRPO designs dynamic sampling approach to filter out samples that are consistently answered correctly, thereby improving inference efficiency [57]. EffiCode [21] develop self-optimization process based on overhead profiling and iterative refinement, resulting in high-quality dataset for fine-tuning LLMs to produce more efficient and accurate code. To the best of our knowledge, we are the first to introduce re-weighting 9 mechanism into thinking compression. By employing simple strategies to construct short and long CoT, we enable the model to dynamically compress its reasoning process."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces TLDR, an innovative method designed to compress the reasoning processes of LLMs without sacrificing accuracy. By dynamically re-weighting the influence of system 1 (concise reasoning) and system 2 (detailed reasoning) data during the training process, TLDR allows LLMs to eliminate unnecessary steps for simpler problems while still engaging in deep contemplation for complex tasks. TLDR avoids the laborious data collection and hyperparameter tuning typically required by other compression methods, offering more practical solution for developing LLMs that are both efficient and accurate."
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. [2] AI-MO. Aime 2024, 2024. [3] AI-MO. Amc 2023, 2024. [4] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William Yang Wang. survey on data selection for language models, 2024. [5] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2025. [6] Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171, 2024. [7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways, 2022. [8] Yu-Neng Chuang, Helen Zhou, Prathusha Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, and Xia Hu. Learning to route llms with confidence tokens. arXiv preprint arXiv:2410.13284, 2025. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 11 [11] Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: case study on the colossal clean crawled corpus, 2021. [12] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, 12 Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. [13] Suchin Gururangan, Dallas Card, Sarah Dreier, Emily Gade, Leroy Wang, Zeyu Wang, Luke Zettlemoyer, and Noah A. Smith. Whose language counts as high quality? measuring language ideologies in text data selection. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 25622580, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [14] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. [15] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [16] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning, 2025. 13 [17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [18] Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Ben Mann, Chris Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, and Sam McCandlish. Scaling laws and interpretability of learning from repeated data, 2022. [19] Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025. [20] Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025. [21] Dong Huang, Guangtao Zeng, Jianbo Dai, Meng Luo, Han Weng, Yuhao Qing, Heming Cui, Zhijiang Guo, and Jie Zhang. Effi-code: Unleashing code efficiency in language models. arXiv preprint arXiv:2410.10209, 2024. [22] Yuxuan Jiang, Dawei Li, and Frank Ferraro. Drp: Distilled reasoning pruning with skill-aware step decomposition for efficient large reasoning models. arXiv preprint arXiv:2505.13975, 2025. [23] Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. C3ot: Generating shorter chain-ofthought without compromising effectiveness. arXiv preprint arXiv:2412.11664, 2024. [24] Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian. Overthink: Slowdown attacks on reasoning llms. arXiv preprint arXiv:2502.02542, 2025. [25] Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. The bigscience roots corpus: 1.6tb composite multilingual dataset, 2023. [26] Ayeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-ofthought? token complexity approach, 2025. [27] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better, 2022. [28] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhiwei Li, Bao-Long Bi, Ling-Rui Mei, Junfeng Fang, Zhijiang Guo, Le Song, and Cheng-Lin Liu. From system 1 to system 2: survey of reasoning large language models, 2025. [29] Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and Lianwen Jin. Datasets for large language models: comprehensive survey, 2024. [30] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, and Daphne Ippolito. pretrainers guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity, 2023. [31] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025. [32] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning. arXiv preprint arXiv:2501.12570, 2025. [33] Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Length-compressible chain-of-thought tuning, 2025. 14 [34] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward, 2024. [35] Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772, 2021. [36] Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models, 2023. [37] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [38] Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms with preference data, 2025. [39] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila 15 Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. [40] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher, 2022. [41] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. [42] Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Efficient reasoning with hidden thinking. arXiv preprint arXiv:2501.19201, 2025. [43] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling Reinforcement Learning with LLMs. arXiv preprint arXiv:2501.12599, 2025. [44] Sizhe Wang, Yongqi Tong, Hengyuan Zhang, Dawei Li, Xin Zhang, and Tianlong Chen. Bpo: Towards balanced preference optimization between knowledge breadth and depth in alignment. arXiv preprint arXiv:2411.10914, 2024. [45] Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, and Mingxuan Yuan. Unlocking efficient long-to-short llm reasoning with model merging, 2025. [46] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. Os-atlas: foundation action model for generalist gui agents, 2024. [47] Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. [48] Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan Klein. Detoxifying language models risks marginalizing minority voices, 2021. [49] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, et al. RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems? arXiv preprint arXiv:2501.11284, 2025. [50] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025. [51] Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for llm reasoning, 2025. [52] Yuxuan Yao, Shuqi Liu, Zehua Liu, Qintong Li, Mingyang Liu, Xiongwei Han, Zhijiang Guo, Han Wu, and Linqi Song. Activation-guided consensus merging for large language models. arXiv preprint arXiv:2505.14009, 2025. [53] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng, Xiaonan Li, Junqi Dai, Qinyuan Cheng, Xuanjing Huang, and Xipeng Qiu. Reasoning in flux: Enhancing large language models reasoning through uncertainty-aware adaptive guidance. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 24012416, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [54] Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1, 2024. 16 [55] Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Ziyi Yang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, et al. Chain-of-reasoning: Towards unified mathematical reasoning in large language models via multi-paradigm perspective. arXiv preprint arXiv:2501.11110, 2025. [56] Hengyuan Zhang, Yanru Wu, Dawei Li, Zacc Yang, Rui Zhao, Yong Jiang, and Fei Tan. Balancing speciality and versatility: coarse to fine framework for supervised fine-tuning large language model. arXiv e-prints, pages arXiv2404, 2024. [57] Xiaojiang Zhang, Jinghui Wang, Zifei Cheng, Wenhao Zhuang, Zheng Lin, Minglei Zhang, Shaojie Wang, Yinghan Cui, Chao Wang, Junyi Peng, Shimiao Jiang, Shiqi Kuang, Shouyu Yin, Chaohang Wen, Haotian Zhang, Bin Chen, and Bing Yu. Srpo: cross-domain implementation of large-scale reinforcement learning on llm, 2025."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Training Details Due to the need to evaluate accuracy and token count on validation set every steps, our validation set consists of 512 questions sampled from past questions in AIME-1983 to AIME-2023. The original ratio for shortcot and longcot is set to 0.5:0.5, with an evaluation interval of every 32 steps. The model is allowed to train for total of 2000 steps, and the learning rate is set as constant at 1e-5. For the 7B and 14B models, we conducted training on two 8-GPU (80GB) machines, with one 8-GPU machine performing vllm inference and the other performing training. Every steps, parameter synchronization is executed using vllms parameter sync function. For the 32B model, we performed SFT on five 8-GPU nodes and deployed the inference service on one node. We ultimately select the checkpoint with the shortest token length among those whose accuracy on the validation set is no less than 30% of that achieved by the original long CoT. A.2 Metrics A.2.1 Compression Rate We provide more details on the compression rate in the main table, where the compression rate is defined as: C.R. = Compression Rate = max( #tokensoriginal #tokenscurrent #tokensoriginal , 0) A.C.R. = 1 Nbenchmark Nbenchmark i= C.R. (9) (10) A.2.2 Normalized Metric We report two normalized metrics to facilitate fair comparisons: Normalized Accuracy and Normalized Token Length. They are defined as follows: Normalized Accuracy = #Acccurrent #Accoriginal #Tokencurrent #Tokenoriginal Normalized Token = A.3 Data Construction Detail (11) (12) For long CoT, we use the prompt from dataset s1.1 [37]. Each sample is generated 8 times using the original model. For short CoT, to avoid inconsistencies in the system prompt format, we adopt the short CoT construction method from AdaR1. We annotate 10 randomly selected questions from GSM8K using the instruct model, then fine-tune the long CoT model to overfit on them. For the GSM8K training set, we sample and retain only the examples with correct answers. A.4 Evaluation Detail We use the DeepSeek-R1-Distill model and apply temperature setting of 0.7, which is the primary recommendation in QwQ-Preview, for evaluating all models. All datasets are restricted to an 8K context window for output generation. Meanwhile, considering the relatively small sizes of the AMC and AIME datasets, we sample 8 responses per question and compute the average. A.5 Evaluation Framework We use skythought-eval3 as the framework, which supports accelerating long CoT reasoning evaluation with vLLM. The version of vLLM we use is 0.6.3. 3https://github.com/NovaSky-AI/SkyThought A.6 Evaluation Dataset Detail We provide an overview of all datasets used in the following sections. ASDiv: diverse simple English math word problem corpus for evaluating the capability of various MWP solvers. It contains 2,305 MWPs that cover more text patterns and most problem types taught in elementary school. GSM8K: high-quality benchmark comprising 8,500 human-written grade school math word problems that require multi-step reasoning and basic arithmetic, each labeled with natural language solution and verified answer. The 1,319-question test set emphasizes sequential reasoning and is primarily solvable by upper-grade elementary school students. MATH500: challenging benchmark of 500 high school competition-level problems spanning seven subjects, including Algebra, Geometry, Number Theory, and Precalculus. Each problem is presented in natural language with LaTeX-formatted notation, offering strong measure of mathematical reasoning and generalization across diverse topics. AIME2024: dataset containing 30 problems from the 2024 American Invitational Mathematics Examination (AIME), prestigious high school mathematics competition for topperforming students. Each problem is designed to require deep mathematical insight, multi-step reasoning, and precise problem-solving skills. AMC: The AMC dataset consists of all 83 problems from AMC12 2022 and AMC12 2023, extracted from the AoPS wiki page. We used subset of this data containing 40 problems. MinervaMath: MinervaMath is high-difficulty math problem dataset containing challenging problems. A.7 Reproduce Details ConciseCoT & TALE-EP For the prompt-based baseline, we list the prompts used in Prompt 5. OverThink For the MATH12K dataset, we sample each problem 8 times. The shortest correct sample is selected as the chosen sample, and the longest sample is selected as the rejected sample. The model is trained for 1 epoch. ThinkPruner In our reproduction, we use the competition-level training data provided in the original paper and train the model for 10 epochs with learning rate of 1e-6. The maximum response length is set to 4096 tokens. We follow their early stopping strategy to select the optimal checkpoint for evaluation. CoT-Valve Since CoT-Valve does not report performance on all datasets, we reproduced the results using the public datasets released by CoT-Valve. We followed the training settings officially reported in the paper, using LoRA=2 to fine-tune all models. The dataset version used is Mix-Chain-Z-GSM8K. All models were fine-tuned for 5 epochs on 8 GPUs with 80GB of memory each. L1 In L1 reproduction on the 7B System-2 model, we utilize the L1-Exact reward function and limit the token length to between 100 and 4,096 tokens, while setting the token difference penalization parameter α to 0.0003, as described in the paper. We follow their original prompt by appending \"Think for ntoken tokens\" to the end of the question. In inference, the token budget is set to the same number as the average tokens from our method across the evaluated benchmarks."
        },
        {
            "title": "B Case Study",
            "content": "To better understand the behavioral differences between baseline and TLDR strategies, we conduct qualitative analysis using the DeepSeek-R1-Distill-Qwen-7B model. Case studies are drawn from three representative math datasets: GSM8K, AIME, and MATH500. As shown in Figures 68, the baseline model tends to generate verbose reasoning paths with redundant or speculative content. In contrast, TLDR produces significantly more concise outputs while maintaining correctness and logical structure. These examples demonstrate TLDRs ability to suppress unnecessary reasoning tokenssuch as exploratory or reflective phrasesleading to more efficient and focused reasoning processes. 19 Evaluation Prompt on Dataset === EVALUATION PROMPT FOR GSM8K === <begin_of_sentence>Please reason step by step, and put your final answer within boxed. <User>query<Assistant>Given the following problem, reason and give final answer to the problem. Problem: {question} Your response should end with The final answer is [answer] where [answer] is the response to the problem. <think> === EVALUATION PROMPT FOR MATH500 === <begin_of_sentence>Please reason step by step, and put your final answer within boxed. <User>{query}<Assistant>Return your final response within boxed. {problem}. <think> === EVALUATION PROMPT FOR AIME24 === <begin_of_sentence>Please reason step by step, and put your final answer within boxed. <User>query<Assistant>Return your final response within boxed. {problem}. <think> === EVALUATION PROMPT FOR AMC === <begin_of_sentence>Please reason step by step, and put your final answer within boxed. <User>query<Assistant>Return your final response within boxed. {problem}. <think> === EVALUATION PROMPT FOR MINERVAMATH === <begin_of_sentence>Please reason step by step, and put your final answer within boxed. <User>query<Assistant>Return your final response within boxed. problem. <think> Figure 5: Evaluation Prompt for GSM8K, MATH500, AIME24, AMC, MinervaMath 20 Figure 6: Comparison of Reasoning process on GSM8K: Baseline vs. TLDR. Figure 7: Comparison of Reasoning process on AIME: Baseline vs. TLDR. 22 Figure 8: Comparison of Reasoning process on MATH500: Baseline vs. TLDR."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "Institute of Automation, Chinese Academy of Sciences",
        "Microsoft",
        "School of Artificial Intelligence, Chinese Academy of Sciences",
        "Tsinghua University",
        "University of California, Los Angeles"
    ]
}