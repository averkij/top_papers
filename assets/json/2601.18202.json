{
    "paper_title": "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback",
    "authors": [
        "Fangyuan Xu",
        "Rujun Han",
        "Yanfei Chen",
        "Zifeng Wang",
        "I-Hung Hsu",
        "Jun Yan",
        "Vishy Tirumalashetty",
        "Eunsol Choi",
        "Tomas Pfister",
        "Chen-Yu Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 2 ] . [ 1 2 0 2 8 1 . 1 0 6 2 : r SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Fangyuan Xu*2, Rujun Han1, Yanfei Chen1, Zifeng Wang1, I-Hung Hsu1, Jun Yan1, Vishy Tirumalashetty1, Eunsol Choi2, Tomas Pfister1 and Chen-Yu Lee1 1Google Cloud AI Research, 2New York University Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high-quality, difficulty-controlled deep search question-answer pairs for given corpus and target difficulty level. Our pipeline, SAGE, consists of data generator which proposes QA pairs and search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training. 1. Introduction Large language models (LLMs) are increasingly used as agents which interact with external environments and solve complicated tasks, such as coding (Dong et al., 2025; Jimenez et al., 2024), e-commerce and social forum discussion (Peeters et al., 2025; Zhou et al., 2024). Recently, there is growing interest in building search-augmented agents that retrieve and reason about external information to solve complicated questions (Asai et al., 2024; Jin et al., 2025; Trivedi et al., 2023). High-quality, complex questionanswer pairs are pivotal for training and evaluating capable search agents. Yet, such high quality data is not easily accessible and costly for humans to annotate (Krishna et al., 2025; Wei et al., 2025). Earlier work on retrieval-augmented generation (RAG) primarily focuses on questions where one search suffices to provide the necessary context (Han et al., 2024; Joshi et al., 2017; Kwiatkowski et al., 2019). Subsequent datasets (Trivedi et al., 2022; Yang et al., 2018) extended this setting to multi-hop reasoning. However, questions in these benchmarks typically require no more than four retrieval and reasoning steps. Furthermore, their dependence on extensive human annotation or pre-existing structural information (e.g. inter-document links) makes such approach difficult to scale to tasks that demand longer reasoning and search chains, limiting their applicability. To address the limitations of prior work, we propose an agentic data generation pipeline that leverages search-augmented LLMs to generate high-quality, challenging data for training and evaluating deep search agents. In contrast to most existing search-augmented QA datasets, which begin with question and then retrieve supporting evidence to find the answer (Dunn et al., 2017; Kwiatkowski et al., 2019), our pipeline adopts reverse formulation to ensure the faithfulness of the generated data (Wei et al., 2025). Specifically, given randomly sampled document from corpus and target difficulty level, our pipeline employs search-augmented data generator to iteratively search and *Work done at Google Cloud AI Research. This work has no implications of any Google products. Correspondence to: fx2079@nyu.edu, {rujunh, chenyulee}@google.com The code and data will be released at https://github.com/carriex/sage. SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Figure 1 Illustration of our proposed method: (1) we first employ data generator (ğ´ğ‘”ğ‘’ğ‘›) to generate an initial question, answer (QA) pair with target number of search steps ğ‘† for difficulty control. Our framework then (2) collect execution traces from search agent (ğ´ğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„) and (3) provide the execution feedback for the data generator agent to re-generate QA pair. In this example, the initial QA pair generated does not satisfy the target step S=4. After re-generation with search agents feedback, the new QA pair is both correct and difficult. Noticeably, the question complexity increases from q0 to q1. reason, generating an initial questionanswer(QA) pair grounded in the retrieved evidences. We find that reverse generation alone is not sufficient to consistently produce correct QA pairs that require targeted number of search steps. As the target number of steps increases, the data generator increasingly fails to generate correct QA pairs that satisfy the target difficulty level. This reveals common pitfall: mismatch between the data generators intended search plans and the actual number of search steps required to answer the question, as illustrated in Figure 1. Our observations inspire us to propose dual-agent framework, which consists of data generator agent and search agent. The data generator agent first generates an initial draft of the question and answer pair. The generated question-answer pair is validated by search agent to solve the question. We collect multiple traces from the search agent to gather feedback of the correctness and difficulty of the generated data. These execution traces are fed into the data generator to produce new QA pair. Experiment results show that our framework, SAGE (Steerable Agentic Generation with Execution Feedback), significantly increases the quality of the generated data as measured by correctness and difficulty. We further conduct experiments on training search agents with our generated data, showing 27% relative improvements on in-domain evaluation set and up to 23% relative improvement for out-of-domain evaluation data (Krishna et al., 2025; Trivedi et al., 2022) across two model sizes (3B and 7B). Our analysis reveals that the generated data requires more diverse reasoning types than existing benchmarks. 2 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Dataset Annotation # search Avg@8 Large-scale training data NQ HotpotQA Musique Human Automatic Automatic + human Small-scale evaluation data FRAMES Human SAGE Automatic 1.3 2.1 2.7 3.2 4. 83.1 82.9 64.4 74.3 79.5 Table 1 Comparison with other multi-hop question answering datasets. To estimate dataset difficulty, we report both average number of searches (# search) and average performance of 8 samples from search-augmented gemini-2.5-flash. 2. Background 2.1. Search agent Given an input question q, search agent Ağ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ issues search queries (ğ‘ 0, ğ‘ 1, ... ğ‘ ğ‘–) to retrieval tool to gather information in multi-turn manner before providing the final answer a. Following the ReACT (Yao et al., 2023) framework, the agent alternates between outputting reasoning traces ğ‘ ğ‘– and issuing search queries ğ‘ ğ‘–, outputting sequence of {ğ‘Ÿ0, ğ‘ 0, ğ‘Ÿ1, ğ‘ 1, ...ğ‘Ÿğ‘–, ğ‘ ğ‘–, a}. We omit the information returned by the search tool in the notation, which will be appended as input to the LM after ğ‘ ğ‘–. 2.2. Training search agent While earlier work (Li et al., 2025b; Trivedi et al., 2023; Yao et al., 2023) explored prompting an LLM to use search tool, recent work has shown that further training the model with either supervised learning (Asai et al., 2024) or reinforcement learning (Jin et al., 2025; Nakano et al., 2021) is beneficial. Supervised fine-tuning requires set of question q, answer a, as well as gold trajectory which consists of sub-queries, reasoning trace and information returned from search tool: {ğ‘Ÿ0, ğ‘ 0, ğ‘‘0, ğ‘Ÿ1, ğ‘ 1, ğ‘‘1...ğ‘Ÿğ‘–, ğ‘ ğ‘–, ğ‘‘ğ‘–}. As it is expensive to collect gold trajectories (Asai et al., 2024; Nakano et al., 2021), recent work (Jin et al., 2025) proposes training search agent using reinforcement learning with outcome reward that measures the quality of the final answer via exact match with gold answer a. While training the model with reinforcement learning eliminates the need for gold trajectories, set of high quality (ğ‘, ğ‘) pairs are essential for training capable search agent. 2.3. Existing training and evaluation data We review widely adopted datasets for training search agent in Table 1. While there are large-scale data annotated by human, they primarily consist of questions requiring small number of steps to answer, as indicated by # search in Table 1. HotpotQA (Yang et al., 2018) and Musique (Trivedi et al., 2022) consist of multi-hop data, constructed through automatic or partially automatic pipelines. As constructing such dataset can be cognitively heavy and time consuming for human, recent effort focuses on relatively small-scale human annotated evaluation set to benchmark performances of LLM agents (Krishna et al., 2025; Wei et al., 2025). Concurrent work such as WebDancer (Wu et al., 2025) and WebShaper (Tao et al., 2025) propose automatic training data construction pipeline. They focus on the setting of using browsing tool as the retriever, which is non-trivial to reproduce (Chen et al., 2025) and expensive to train due 3 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback to excessive API costs. Moreover, their large-scale data is not yet publicly available. We provide more comprehensive discussion of prior and concurrent efforts to synthetically generate deep search training data in Section 6. 3. Generating Synthetic Data for Deep Search Agent: SAGE Our goal is to generate (question, answer) pairs that require search agent to issue multiple calls to search tool and reason over the retrieved information before deriving the final (question, answer) pair. Our framework consists of the following components: data generator agent Ağ‘”ğ‘’ğ‘›, search agent Ağ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„, retrieval model and corpus containing set of documents. In this section, we describe each component of our framework. The full procedure is outlined in Algorithm 1 and we include the implementation details such as prompt, decoding setting in Section A.2 in the appendix. Algorithm 1 Agentic Data Generation with Execution Feedback Input: Ağ‘”ğ‘’ğ‘›: data generator agent; Ağ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„: search agent; ğ‘‘: input document; ğ‘†: target search steps; ğ¾: number of search traces; ğ‘…: max feedback rounds. Initialize accumulated Ağ‘”ğ‘’ğ‘› and Ağ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ traces. Generate initial (question, answer) pair Re-generate the question and answer with execution feedback end if (ğ‘, ğ‘†, ğ‘¡ğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„, IS_CORRECT, IS_DIFFICULT) RunSearchAgent(ğ‘, ğ‘, ğ‘†, ğ¾, Ağ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„) Tğ‘”ğ‘’ğ‘› Tğ‘”ğ‘’ğ‘› {ğ‘¡ğ‘”ğ‘’ğ‘›}, Tğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ Tğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ {ğ‘¡ğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„} Collect execution. Append generator and search traces. else break if ğ‘Ÿ = 0 then (ğ‘, ğ‘, ğ‘¡ğ‘”ğ‘’ğ‘›) Ağ‘”ğ‘’ğ‘› (ğ‘‘, ğ‘†) if IS_CORRECT IS_DIFFICULT then 1: Tğ‘”ğ‘’ğ‘› , Tğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ 2: IS_CORRECT ğ¹ğ‘ğ‘™ğ‘ ğ‘’, IS_DIFFICULT ğ¹ğ‘ğ‘™ğ‘ ğ‘’ 3: for ğ‘Ÿ {0, . . . , ğ‘…} do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for 16: if IS_CORRECT then 17: 18: end if return (ğ‘, ğ‘) end if else (ğ‘, ğ‘, ğ‘¡ğ‘”ğ‘’ğ‘›) Ağ‘”ğ‘’ğ‘› (ğ‘, ğ‘, ğ‘†, Tğ‘”ğ‘’ğ‘›, Tğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„) , ğ‘† ğ‘˜ , ğ‘† ğ‘˜ , ğ‘† (ğ‘ ğ‘˜ ğ‘˜ append (ğ‘ ğ‘˜ append (ğ‘ ğ‘˜ for ğ‘˜ {1, . . . , ğ¾} do , ğ‘¡ ğ‘˜) Ağ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ (ğ‘) ğ‘˜) to , ğ‘¡ ğ‘˜) to if ğ‘ , ğ‘¡ Algorithm 2 Verification with search agent 1: function RunSearchAgent(ğ‘, ğ‘, ğ‘†, ğ¾, Ağ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„) 2: [ ] 3: [ ] 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end function end if IS_DIFFICULT (ğ‘† ğ‘†) return (ğ‘, ğ‘†, ğ‘¡, IS_CORRECT, IS_DIFFICULT) end for IS_CORRECT (M > 0) if IS_CORRECT then (ğ‘, ğ‘†, ğ‘¡) arg min(ğ‘,ğ‘†,ğ‘¡ ) ğ‘† (ğ‘, ğ‘†, ğ‘¡) UniformSample(T ) ğ‘˜ = ğ‘ else Return correct pairs only. All traces (ğ‘ , ğ‘† ğ‘˜) ğ‘˜ Correct traces where ğ‘ ğ‘˜ = ğ‘ Pick correct trace with least number of steps Otherwise pick random trace 4 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback 3.1. Initial data generation with difficulty prompt Given an input document ğ‘‘ randomly sampled from corpus ğ·, data generator agent is instructed to generate an initial (question, answer) pair {ğ‘, ğ‘} by iteratively issuing search query (ğ‘ 0, ğ‘ 1, ... ğ‘ ğ‘–) interleaved with reasoning traces to collect relevant information from the corpus (Line 8 in Alg 1). The iterative process ends when the agent outputs an initial {ğ‘, ğ‘} pair. The exact prompt used for this process is in Figure 4 in appendix. Difficulty prompt. Questions can have various levels of difficulty depending on the number of search and reasoning steps. We propose to use the number of steps ğ‘† required by the search agent as proxy measure for difficulty. Specifically, we include the target search step ğ‘† in the input prompt to the data generator agent and instruct it to reason and plan in order to produce question that requires the target number of search steps to solve. 3.2. Verifying the generated data Generated data can be incorrect or require less than the specified number of search steps to solve. To verify the quality of the generated data, we use search agent Ağ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„ to solve the generated question (Line 12 in Alg 1). Given the generated question ğ‘, the search agent iteratively issues search queries before producing an answer ğ‘. As the search agent itself is imperfect and may fail to find solution for valid (ğ‘, ğ‘) pair, we sample ğ¾ traces from the search agent. We focus on two criteria for data quality: Correctness: which measures whether the generated (ğ‘, ğ‘) pair is correct. We treat pass@K performance of answer produced by the search agent (ğ‘) against the generated answer ğ‘ as correctness, following previous work (Shi et al., 2025a). Difficulty: minimal number of search steps required among the correct traces from the search agent as an estimate of difficulty. If the number of search steps is greater than or equal to the target search step ğ‘†, we consider the generated data as difficult enough. Algorithm 2 describes this verification process. Generating complex (ğ‘, ğ‘) pairs requiring multiple search steps is non-trivial as it involves interaction with the retrieval tool. As revealed by our evaluation in Table 2, the data generator alone is only able to generate 18% of the data that satisfy both the correctness and difficulty constraint, when tasked to generate questions that require 3 to 7 steps to solve. We visualize the per-step performance in Figure 2, which further reveals that the data generator fails more frequently when attempting to generate questions requiring more search steps. 3.3. Generation with Execution Feedback Failure to generate correct (ğ‘, ğ‘) pair requiring the target number of search steps reveals discrepancy between the data generators trajectory and search agents trajectory. For instance, two steps planned by the data generator might be solved with single search step by the search agent, as we later analyze in Section 5.3. Can we leverage both trajectories to reconcile such discrepancy? Instead of merely leveraging search agents execution results as filter (Shi et al., 2025a), we propose to leverage the execution traces as feedback for the data generator. Concretely, we feed both the data generators traces and the search agents traces, each containing the retrieved documents, back to the data generator. which is now tasked to output an updated 5 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback System Data Quality % corr % pass Difficulty Avg@4 # search ğ´ğ‘”ğ‘’ğ‘› w/o ğ‘† ğ´ğ‘”ğ‘’ğ‘› +1 resample +2 resample +3 resample +1 feedback +2 feedback +3 feedback Baseline - 18 27 38 84 71 77 81 84 Ours: SAGE 31 42 50 77 83 87 86.3 87.4 84.5 80.3 80.1 83.2 80.4 79.5 3.2 3.3 3.8 4.3 4. 4.1 4.6 4.9 Table 2 Evaluation of data quality. We report the portion of correct (% corr) and portion of successful (% pass: data that are correct and require at least ğ‘† steps to solve) generation out of all generated data. We measure difficulty of the correct portion of the generated data (pass@4=True) by Avg@4 and # search steps needed. (question, answer) pairs (Line 3-15 in Alg 1). This process can be conducted in an iterative manner, alternating between generating new question and collecting execution feedback from the search agent for the updated (ğ‘, ğ‘) pair. Finally, we filter out (ğ‘, ğ‘) pairs for which none of the execution trace from the search agent arrives at the same answer as the data generators answer (pass@K=0). 4. Experiments We conduct intrinsic evaluation of our methods by measuring the correctness and difficulty of the generated data in Section 4.2. We further conduct downstream evaluation in Section 4.3 by training search agents on our generated data. 4.1. Experiment setting Input corpus and retrieval setting. We use the 2018 Wikipedia dump as our input corpus (Karpukhin et al., 2020). Following Jin et al. (2025), we use E5 (Wang et al., 2022) as the retriever module. The number of returned passages for each search call is set to 3. Data generation pipeline setting. We use gemini-2.5-flash as the LLM which acts as both the data generator and the search agent. The maximum number of search steps are set to 20 for both the data generator model and the search agent. We report results with input target ğ‘† set to 3 to 7 steps. 4.2. Intrinsic evaluation Baselines. We compare our method with (1) Data generator without difficulty prompt: we prompt the data generator to generate complicated question which requires multiple steps to solve without pre-specifying target search step ğ‘†; (2) Initial data generator model: we present the result of the initial data generator model; (3) Resampling: instead of using execution feedback to update the (question, answer) pair, we re-sample another pair from ğ´ğ‘”ğ‘’ğ‘› for samples that are incorrect or not difficult enough. This is similar to best-of-K sampling with the search agents execution result as the verifier. Note that both the resampling baseline and SAGE can be conducted in multiple rounds. 6 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Figure 2 Percentage of correct and difficult data for target steps of 3 to 7 for various methods. Updating the question with execution feedback further improves the data quality compared to resampling, particularly with more target search steps. We report results for up to 3 rounds. Evaluation. We report various metric to measure the quality of the generated data: % correct: this reports the proportion of data out of all generated data that has pass@K=1, we set as 4; % pass: this measures the proportion of data that is both correct and require the search agent at least ğ‘† step to solve. Note that this metric is undefined for the baseline with no difficulty control. We further report two difficulty metrics for the questions that are correct: Avg@4: this measures the average performance of the search agent out of 4 traces. lower Avg@4 indicates more difficult question. Number of search steps: the number of search steps required by the search agent to solve the question. We estimate this with the minimal number out of the correct traces. For correctness, we use LLM-as-a-judge against the answer proposed by the data generator as the reference answer.1 Results. Table 2 summarizes the results. Comparing ğ´ğ‘”ğ‘’ğ‘› with its variant without the target step ğ‘†, we observe that explicitly including the target step in the prompt leads to slight increase in the number of search steps required. However, ğ´ğ‘”ğ‘’ğ‘› alone struggles to generate questions that require the specified number of steps, achieving only an 18% pass rate. Resampling and incorporating execution feedback substantially improve the success rate and yield more challenging questions overall. Among the two strategies, execution feedback consistently outperforms resampling across different numbers of rounds. Figure 2 further breaks down performance by target step, showing that feedback provides greater benefits than resampling as the target step increases, indicating its effectiveness for generating more complex questions. We thus only use samples generated with execution feedback in our downstream evaluation. We include example questions generated by SAGE in Table 7 in the Appendix. 4.3. Downstream evaluation We conduct extrinsic evaluation of data generated by SAGE by using it to train search agents. We adopt the Search-R1 (Jin et al., 2025) framework to train the agents and compare with agents trained with existing public data. 1We use gemini-2.0-flash as the LLM and the exact prompt is included in Table 6 7 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Training Data Backbone Model 3-hop 4-hop 5-hop 6-hop 7-hop AVG Musique FRAMES In-domain Out-of-domain - - gemini-2.0-flash gemini-2.5-flash NQ + HotpotQA QWEN-3B QWEN-3B Musique QWEN-3B Ours NQ + HotpotQA QWEN-7B QWEN-7B Musique QWEN-7B Ours 68.7 80.0 25.3 37.3 42.3 45.0 48.3 55.7 55.7 67.0 12.0 20.0 26.7 26.3 28.7 38. 50.3 57.3 15.3 18.3 25.3 25.7 24.7 35.7 43.3 48.7 11.7 19.0 25.0 24.7 25.0 37. 41.3 37.3 15.0 17.1 23.3 23.6 21.2 24.0 51.9 58.1 15.9 22.4 28.5 29.1 29.6 38. 25.2 28.0 11.4 19.4 19.9 18.9 21.6 22.3 45.9 50.0 13.3 21.5 23.8 26.2 25.0 32. Table 3 Downstream evaluation: performance of search agents on complex question answering datasets. We report performance using LLM as judge against reference answers. Each of the data split contains 300 sampled data points. All models search for up to 10 turns. Baselines. We consider models trained on publicly available question answering data: (1) NQ + HotpotQA: combination of Natural Question (Kwiatkowski et al., 2019) and HotpotQA (Yang et al., 2018), which consists of 150K data. This is the training data used by Search-R1 (Jin et al., 2025); (2) Musique (Trivedi et al., 2022): consisting of multi-hop questions requiring two to four hops, the training split contains 20K data. We also compare with the performance of the prompting-based search agents using gemini-2.0-flash and gemini-2.5-flash as the agent. Evaluation setting. We evaluate on various downstream question answering datasets that are grounded in Wikipedia: (1) In-domain: we report performance for testing data generated with SAGE. We generate 300 test data for each target search step and report the per-step performance as well as the average performance. We also consider two out-of-domain datasets: (1) Musique: which contains 2 to 4-hop questions; we randomly sample 300 data for each hop and report the average performance. (2) FRAMES (Krishna et al., 2025): which contains multi-hop questions created by human annotators. We report the results on randomly sampled set of 300 questions. We evaluate the performance with reference-based LLM-as-a-judge using gemini-2.0-flash, which outputs binary judgment given the reference answer. Data generation setting. We generate data using gemini-2.5-flash as both the data generator and search agent. Similar to Section 4.2, we use E5 as the retriever system and the 2018 Wikipedia dump as the corpus. For all dataset generation settings, we filter out questions that require less than two search steps and generate 20K data for training, such that the training set size is the same as our baselines. We report results for data generated with two rounds of feedback and conduct ablation studies on using data generated with different number of rounds in Section 5.1. Model and training setting. We conduct experiment with two model variants: Qwen-2.5-3BInstruct and Qwen-2.5-7B-Instruct (Yang et al., 2024). For NQ+HotpotQA, we directly inference with the Search-R1 checkpoints2. For models trained with musique and our data, we train the models using reinforcement learning with the reward defined as reference-based LLM-as-a-judge using gemini-2.0-flash. All models are trained with PPO (Schulman et al., 2017). Following 2https://huggingface.co/PeterJinGo/SearchR1-nq_hotpotqa_train-qwen2.5-7b-it-em-ppo and PeterJinGo/SearchR1-nq_hotpotqa_train-qwen2.5-3b-em-ppo. 8 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback prior work Jin et al. (2025), we apply loss masking to the retrieved information. We include the implementation details in Section A.3 in the Appendix. Retrieval settings. For all training experiments, we use E5 as the retriever and the 2018 Wikipedia dump as the corpus, matching the set-up used for data generation. For inference, we use the same set-up except when evaluating on FRAMES, where we use the 2023 Wikipeida dump3 to align with its data construction process (Krishna et al., 2025). Results. Table 3 reports downstream performance of models trained on different QA datasets. Across both QWEN-3B and QWEN-7B, training on our generated data yields consistent gains over NQ + HotpotQA and Musique on in-domain evaluations. For QWEN-3B, training on our data increases average accuracy from 15.9% (NQ + HotpotQA) and 22.4% (Musique) to 28.5%, showing 27% relative improvement. For QWEN-7B, while training on Musique provides only marginal improvement over NQ + HotpotQA (29.6% vs. 29.1%), training on our data further boosts average accuracy to 38.1%, leading to 29% relative improvement. Training on our data also improves performance on out-of-domain datasets. On FRAMES, training on our data enables relative improvement of 11% for QWEN-3B and 23% for QWEN-7B. Notably, on Musique, QWEN-7B trained on our data achieves higher accuracy (22.3%) then directly on the in-domain Musique data (21.6%). 4.4. Evaluation with Google Search While our method generates challenging (ğ‘, ğ‘) pairs using fixed corpus (e.g. Wikipedia), we further ask whether search agents trained with retrieval over fixed corpus can generalize to other retrieval tool. To this end, we evaluate agents trained with Wikipedia-based retrieval on benchmarks that require Google search. Datasets and settings. We evaluate on three benchmarks that require Google search: (1) GAIA (Mialon et al., 2023); (2) Browsecomp (Wei et al., 2025) and (3) Humanitys Last Exam(HLE)- search (Phan et al., 2025). For GAIA, we report results on the text-only subset with 103 questions. For Browsecomp, we evaluate on randomly sampled subset of 200 questions. For HLE, we evaluate on subset of questions that required search as classified by Gemini-1.5-pro, following prior work (Han et al., 2025). During inference, we replace Wikipedia-based retrieval with Google Search using the Serper API 4, retrieving the top three snippets per search query. Results. Table 4 reports the results. We observe promising trend indicating that training search agents with retrieval over fixed corpus (e.g. Wikipedia) enables effective transfer when using different retrieval tool (e.g. Google Search). Training on data generated by our pipeline leads to substantial improvements on GAIA compared to training on existing large-scale training data, for both 3B and 7B models (36% and 50% relative improvements compared to the strongest baseline respectively). We also observe improvement on Browsecomp, which consists of multi-step search questions, for the QWEN-7B model. QWEN-3B achieves similarly poor performance (1.0% accuracy) across the three training datasets, likely because the questions are too challenging for 3B model. On HLE, gains are more modest for both models, likely due to the substantial domain shift toward highly specialized scientific questions. 3https://huggingface.co/datasets/wikimedia/wikipedia/viewer/20231101.en 4https://serper.dev/ SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Training data GAIA Browsecomp HLE-Search QWEN-3B NQ + HotpotQA Musique Ours 12.5 13.5 18.8 QWEN-7B NQ + HotpotQA Musique Ours 14.6 15.6 24.0 1.0 1.0 1.0 1.6 2.1 2. 5.0 4.0 5.5 4.5 8.0 7.0 Table 4 Results on deep search benchmarks using Google Search, with maximum of 10 queries for GAIA and 20 for other benchmarks. Round Downstream Performance In-domain Musique FRAMES Difficulty Avg@4 0 1 2 3 33.6 33.6 38.1 34.1 18.7 19.5 22.3 20.9 29.0 29.3 32.3 28. 86.3 83.2 80.4 79.5 Table 5 Ablation on training QWEN-7B on data generated with different numbers of feedback rounds. 5. Analysis 5.1. Ablation on feedback rounds In Section 4.3, we report downstream evaluation on training with data generated with SAGE with two rounds of feedback. How does the number of rounds impact downstream performance? We conduct an ablation study on training QWEN-7B model on data generated with 0-3 rounds of feedback. For all settings, we keep the training data size the same (20K) and filter out questions that require less than two search steps. Results are reported in Table 5. We report downstream performance on the in-domain testing set, averaged across questions requiring 2-7 search steps, as well as on Musique and FRAMES. We additionally report difficulty of the generated data, measured by Avg@4. We see improvement for both in-domain and out-of-domain datasets when increasing the number of feedback rounds from 0 to 2, confirming the effectiveness of incorporating execution feedback. Note that zero feedback round is equivalent to using the initial data generator only. We do not see further improvement when increasing the number of feedback to three rounds, despite that the data generated with three rounds of feedback being more difficult compared to those with two rounds. This suggests that increasing data difficulty alone is insufficient and highlights the need for more principled data curation strategies that balance difficulty and learnability. 5.2. Reasoning strategy analysis Solving deep search questions require the agent to be able to issue search queries as well as reason about the retrieved documents. We conduct an analysis focusing on the reasoning strategies required to solve questions generated by our pipeline. 10 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Figure 3 Distribution of reasoning types required to answer questions in Musique and our generated dataset. Our data spans broad set of reasoning types, resulting in more balanced distribution compared to Musique."
        },
        {
            "title": "Easy data",
            "content": "Superficial complexity Multi-query collapse Overly specific question Information co-location"
        },
        {
            "title": "Ambiguous question\nData generator error\nSearch agent error\nSearch agent retrieval failure",
            "content": "13% 21% 31% 35% 7% 19% 20% 54% Table 6 Error categorization for (ğ‘, ğ‘) pair generated by the initial data generator. Setting. We collect search trajectory by prompting gemini-2.5-flash as the search agent. We sample 100 trajectories that lead to the correct answer for our generated data and Musique respectively. We then prompt gemini-2.5-flash to first (1) identify the types of reasoning strategies present in these search trajectories and (2) for each trajectory, label the presence of the reasoning strategies for each step. We include the prompt, definition for each strategy, as well as implementation details of the analysis in Section A.4 in the Appendix. Results. Figure 3 shows the distribution of reasoning types required to solve questions in our dataset and Musique, note that single question typically requires multiple strategies. SAGE produces questions that span broader range of reasoning types, including conflict resolution, hypothesis generation, self-correction, calculation, and temporal reasoning. In particular, calculation and temporal reasoning are rare in Musique (5% and 8%, respectively) but appear more frequently in our data (35% and 32%), resulting in more balanced distribution across reasoning categories. 5.3. Analyzing data generators error To better understand the main causes of the data generators failure to produce correct and appropriately difficult (ğ‘, ğ‘) pairs, we analyze the discrepancies between the data generators trajectories and the execution traces of the search agent. Specifically, we prompt gemini-2.5-flash to compare these 11 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback two traces and categorize the error into error patterns. We identify four common failure patterns for the two error type (easy data and incorrect data). Results are reported in Table 6 and the prompt used for the analysis is in Section A.4 in the appendix. For easy data, we observe that most errors stem from misalignment between the data generators intended difficulty and the actual difficulty required by the search agent to solve the question, which is often influenced by the external environment. For instance, 35% of the generated question exhibit information co-location, where multiple pieces of information required to answer the question occurs in the same document in the corpus; 21% of the generated data exhibits multi-query collapse, in which information from different documents can be retrieved by the retrieval tool using single query. Both phenomena reduce the number of search steps required to answer the question. This analysis motivates our method as such discrepancy is only discoverable via execution feedback. For incorrect data, the misalignment between the search agents answer and the data generators answer most commonly arises from failure on the search agent, including retrieval failure or reasoning errors. While we do not distinguish these data from truly incorrect data and filter them out for training, future work could explore verifying the correctness of (ğ‘, ğ‘) pairs unsolved by the search agent. Around 20% of the incorrect data is attributable to errors made by the data generator, such as hallucinating ungrounded steps in the question generation process. The remaining 7% of data involves ambiguous question which leads to different answer found by the search agent. 6. Related Work Deep Search. Early retrieval-augmented generation (RAG) systems typically perform single retrieval step before generating an answer (Gao et al., 2023; Lewis et al., 2020). Recent agentic RAG approaches aim to support deep search, in which models iteratively interleave retrieval and reasoning to solve questions requiring multiple search steps. Trivedi et al. (2023) is among the earlier work that explore prompting-only strategy that guides model to alternate between search and reasoning until reaching an answer. Search-o1 (Li et al., 2025c) furthre improves deep search by incorporating in-document reasoning to filter out irrelevant retrieved information. Search-R1 (Jin et al., 2025) proposes training search agents following the ReACT framework (Yao et al., 2023) using outcome-based reward. Subsequent works (Shi et al., 2025b; Wu et al., 2025) all adopt RL training paradigm as high-quality SFT trajectories for deep search agent is difficult to collect at scale. We also adopt the Search-R1 training framework for our downstream evaluation. Synthetic data generation for deep search. We review several concurrent works on synthetic data generation for deep search. WebDancer (Wu et al., 2025) adopt reverse QA generation and difficulty filtering, using either web pages or Wikipedia, while WebShaper (Tao et al., 2025) rely on structured signals such as knowledge graphs or explicit entity relations to construct complex questions. WebSailor (Li et al., 2025a), WebPuzzle (Shi et al., 2025b) and WebExplorer (Liu et al., 2025) generate challenging questions by progressively obscuring information around seed entities. In contrast, our approach takes in random passage and leverages strong LLM-based data generator and search agent to flexibly synthesize challenging (ğ‘, ğ‘) pairs without relying on high-quality knowledge graphs or manually imposed structure. We further note that many concurrent works (Li et al., 2025a; Liu et al., 2025; Shi et al., 2025b; Tao et al., 2025; Wu et al., 2025) depend on commercial retrieval APIs (e.g., Google Search) during data generation. This substantially increases the cost for both data generation and model training due to the large number of intermediate search queries required. Our pipeline is instead grounded in fixed corpus, while experiments in Section 4.4 demonstrate promising transfer to using Google 12 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Search at inference time only. We summarize comparison between our work and the concurrent works in Table 8 in the appendix. Finally, existing baseline training datasets (Kwiatkowski et al., 2019; Trivedi et al., 2022; Yang et al., 2018) are constructed either through human annotation, automatic pipeline leveraging Wikipedia structure or combination of both. To our best knowledge, these are the only other publicly available training data for search agents grounded in Wikipedia. 7. Conclusion We introduce an agentic pipeline, SAGE, to automatically generate deep search data for given corpus. Our framework consists of data generator agent that generates complicated question as well as answers, and search agent that provides execution feedback by attempting to solve the question. We conduct comprehensive evaluation on both intrinsic data quality and downstream evaluation, training search agents with our generated data. Evaluation on in-domain and out-of-domain evaluation set demonstrates the effectiveness of our framework."
        },
        {
            "title": "Limitations",
            "content": "Proposed method. Our dual-agent framework currently relies on fixed search agent to provide execution feedback for the data generator agent to evolve the generated data. Future work can explore co-evolving both agents (e.g., through iterative training), which could further enhance the quality of the generated data and hence the capability of the search agent. We adopt pass@K=1 as the correctness criterion for the generated data, which serves as practical approximation but may still admit hallucinated or incorrect content. Future work could investigate more robust verification methods for identifying correct (ğ‘, ğ‘) pairs that the existing search agent cannot solve (pass@K=0). Our work also only focuses on generating high-quality (ğ‘, ğ‘) pairs for RL training. Exploring methods to construct high quality supervised fine-tuning trajectories to train search agents can be promising direction. Experimental setting. While we evaluate our method using both intrinsic evaluation and downstream evaluation, our experiments do not explore alternative reinforcement learning algorithms such as GRPO (Shao et al., 2024), nor model scales beyond 7B parameters. We only experiment with generating data from single general domain corpus, Wikipedia. Future work can explore generating deep search data for other domain-specific corpus, such as legal or scientific domains."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Tengxiao Liu, Siru Ouyang, Yihe Deng, Rui Meng and other members from Google Cloud AI Research for helpful discussions and feedbacks throughout the project."
        },
        {
            "title": "References",
            "content": "A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=hSyW5go0v8. Z. Chen, X. Ma, S. Zhuang, P. Nie, K. Zou, A. Liu, J. Green, K. Patel, R. Meng, M. Su, S. Sharifymoghaddam, Y. Li, H. Hong, X. Shi, X. Liu, N. Thakur, C. Zhang, L. Gao, W. Chen, and J. Lin. 13 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Browsecomp-plus: more fair and transparent evaluation benchmark of deep-research agent. ArXiv, abs/2508.06600, 2025. URL https://api.semanticscholar.org/CorpusID:280565737. Y. Dong, X. Jiang, J. Qian, T. Wang, K. Zhang, Z. Jin, and G. Li. survey on code generation with llm-based agents. ArXiv, abs/2508.00083, 2025. URL https://arxiv.org/abs/2508.00083. M. Dunn, L. Sagun, M. Higgins, V. U. Guney, V. Cirik, and K. Cho. Searchqa: new q&a dataset augmented with context from search engine. arXiv preprint arXiv:1704.05179, 2017. URL https://arxiv.org/abs/1704.05179. Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, Q. Guo, M. Wang, and H. Wang. Retrievalaugmented generation for large language models: survey. ArXiv, abs/2312.10997, 2023. URL https://api.semanticscholar.org/CorpusID:266359151. R. Han, Y. Zhang, P. Qi, Y. Xu, J. Wang, L. Liu, W. Y. Wang, B. Min, and V. Castelli. Rag-qa arena: Evaluating domain robustness for long-form retrieval augmented question answering. arXiv preprint arXiv:2407.13998, 2024. URL https://arxiv.org/abs/2407.13998. R. Han, Y. Chen, Z. CuiZhu, L. Miculicich, G. Sun, Y. Bi, W. Wen, H. Wan, C. Wen, S. MaÃ®tre, G. Lee, V. Tirumalashetty, E. Xue, Z. Zhang, S. Haykal, B. Gokturk, T. Pfister, and C.-Y. Lee. Deep researcher with test-time diffusion, 2025. URL https://arxiv.org/abs/2507.16075. C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. B. Jin, H. Zeng, Z. Yue, J. Yoon, S. O. Arik, D. Wang, H. Zamani, and J. Han. Search-r1: Training LLMs to reason and leverage search engines with reinforcement learning. In Second Conference on Language Modeling, 2025. URL https://openreview.net/forum?id=Rwhi91ideu. M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada, July 2017. Association for Computational Linguistics. V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. In B. Webber, T. Cohn, Y. He, and Y. Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/ v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550/. S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and reason: unified evaluation of retrieval-augmented generation. In L. Chiruzzo, A. Ritter, and L. Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 47454759, Albuquerque, New Mexico, Apr. 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.243. URL https: //aclanthology.org/2025.naacl-long.243/. T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026/. SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. KÃ¼ttler, M. Lewis, W.-t. Yih, T. RocktÃ¤schel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledgeintensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 94599474. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 6b493230205f780e1bc26945df7481e5-Paper.pdf. K. Li, Z. Zhang, H. Yin, L. Zhang, L. Ou, J. Wu, W. Yin, B. Li, Z. Tao, X. Wang, W. Shen, J. Zhang, D. Zhang, X. Wu, Y. Jiang, M. Yan, P. Xie, F. Huang, and J. Zhou. Websailor: Navigating super-human reasoning for web agent, 2025a. URL https://arxiv.org/abs/2507.02592. X. Li, G. Dong, J. Jin, Y. Zhang, Y. Zhou, Y. Zhu, P. Zhang, and Z. Dou. Search-o1: Agentic search-enhanced large reasoning models. ArXiv, abs/2501.05366, 2025b. URL https: //api.semanticscholar.org/CorpusID:275405676. X. Li, G. Dong, J. Jin, Y. Zhang, Y. Zhou, Y. Zhu, P. Zhang, and Z. Dou. Search-o1: Agentic searchenhanced large reasoning models. CoRR, abs/2501.05366, 2025c. doi: 10.48550/ARXIV.2501. 05366. URL https://doi.org/10.48550/arXiv.2501.05366. J. Liu, Y. Li, C. Zhang, J. Li, A. Chen, K. Ji, W. Cheng, Z. Wu, C. Du, Q. Xu, J. Song, Z. Zhu, W. Chen, P. Zhao, and J. He. Webexplorer: Explore and evolve for training long-horizon web agents, 2025. URL https://arxiv.org/abs/2509.06501. G. Mialon, C. Fourrier, C. Swift, T. Wolf, Y. LeCun, and T. Scialom. Gaia: benchmark for general ai assistants, 2023. URL https://arxiv.org/abs/2311.12983. R. Nakano, J. Hilton, S. Balaji, J. Wu, O. Long, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman. Webgpt: Browser-assisted question-answering with human feedback. ArXiv, abs/2112.09332, 2021. URL https://api.semanticscholar.org/CorpusID:245329531. R. Peeters, A. Steiner, L. Schwarz, J. Y. Caspary, and C. Bizer. Webmall multi-shop benchmark for evaluating web agents. ArXiv, abs/2508.13024, 2025. URL https://arxiv.org/abs/2508.13024. L. Phan, A. Gatti, Z. Han, N. Li, J. Hu, H. Zhang, C. B. C. Zhang, M. Shaaban, J. Ling, S. Shi, M. Choi, A. Agrawal, A. Chopra, A. Khoja, R. Kim, R. Ren, J. Hausenloy, O. Zhang, M. Mazeika, D. Dodonov, T. Nguyen, J. Lee, D. Anderson, M. Doroshenko, A. C. Stokes, M. Mahmood, O. Pokutnyi, O. Iskra, J. P. Wang, J.-C. Levin, M. Kazakov, F. Feng, S. Y. Feng, H. Zhao, M. Yu, V. Gangal, C. Zou, Z. Wang, S. Popov, R. Gerbicz, G. Galgon, J. Schmitt, W. Yeadon, Y. Lee, S. Sauers, A. Sanchez, F. Giska, M. Roth, S. Riis, S. Utpala, N. Burns, G. M. Goshu, M. M. Naiya, C. Agu, Z. Giboney, A. Cheatom, F. Fournier-Facio, S.-J. Crowson, L. Finke, Z. Cheng, J. Zampese, R. G. Hoerr, M. Nandor, H. Park, T. Gehrunger, J. Cai, B. McCarty, A. C. Garretson, E. Taylor, D. Sileo, Q. Ren, U. Qazi, L. Li, J. Nam, J. B. Wydallis, P. Arkhipov, J. W. L. Shi, A. Bacho, C. G. Willcocks, H. Cao, S. Motwani, E. de Oliveira Santos, J. Veith, E. Vendrow, D. Cojoc, K. Zenitani, J. Robinson, L. Tang, Y. Li, J. Vendrow, N. W. Fraga, V. Kuchkin, A. P. Maksimov, P. Marion, D. Efremov, J. Lynch, K. Liang, A. Mikov, A. Gritsevskiy, J. Guillod, G. Demir, D. Martinez, B. Pageler, K. Zhou, S. Soori, O. Press, H. Tang, P. Rissone, S. R. Green, L. BrÃ¼ssel, M. Twayana, A. Dieuleveut, J. M. Imperial, A. Prabhu, J. Yang, N. Crispino, A. Rao, D. Zvonkine, G. Loiseau, M. Kalinin, M. Lukas, C. Manolescu, N. Stambaugh, S. Mishra, T. Hogg, C. Bosio, B. P. Coppola, J. Salazar, J. Jin, R. Sayous, S. Ivanov, P. Schwaller, S. Senthilkuma, A. M. Bran, A. Algaba, K. V. den Houte, L. V. D. Sypt, B. Verbeken, D. Noever, A. Kopylov, B. Myklebust, B. Li, L. Schut, E. Zheltonozhskii, Q. Yuan, D. Lim, R. Stanley, T. Yang, J. Maar, J. Wykowski, M. Oller, A. Sahu, C. G. Ardito, Y. Hu, A. G. K. Kamdoum, A. Jin, T. G. Vilchis, Y. Zu, M. Lackner, J. Koppel, SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback G. Sun, D. S. Antonenko, S. Chern, B. Zhao, P. Arsene, J. M. Cavanagh, D. Li, J. Shen, D. Crisostomi, W. Zhang, A. Dehghan, S. Ivanov, D. Perrella, N. Kaparov, A. Zang, I. Sucholutsky, A. Kharlamova, D. Orel, V. Poritski, S. Ben-David, Z. Berger, P. Whitfill, M. Foster, D. Munro, L. Ho, S. Sivarajan, D. B. Hava, A. Kuchkin, D. Holmes, A. Rodriguez-Romero, F. Sommerhage, A. Zhang, R. Moat, K. Schneider, Z. Kazibwe, D. Clarke, D. H. Kim, F. M. Dias, S. Fish, V. Elser, T. Kreiman, V. E. G. Vilchis, I. Klose, U. Anantheswaran, A. Zweiger, K. Rawal, J. Li, J. Nguyen, N. Daans, H. Heidinger, M. Radionov, V. RozhoÅˆ, V. Ginis, C. Stump, N. Cohen, R. PoÅ›wiata, J. Tkadlec, A. Goldfarb, C. Wang, P. Padlewski, S. Barzowski, K. Montgomery, R. Stendall, J. Tucker-Foltz, J. Stade, T. R. Rogers, T. Goertzen, D. Grabb, A. Shukla, A. GivrÃ©, J. A. Ambay, A. Sen, M. F. Aziz, M. H. Inlow, H. He, L. Zhang, Y. Kaddar, I. Ã„ngquist, Y. Chen, H. K. Wang, K. Ramakrishnan, E. Thornley, A. Terpin, H. Schoelkopf, E. Zheng, A. Carmi, E. D. L. Brown, K. Zhu, M. Bartolo, R. Wheeler, M. Stehberger, P. Bradshaw, J. Heimonen, K. Sridhar, I. Akov, J. Sandlin, Y. Makarychev, J. Tam, H. Hoang, D. M. Cunningham, V. Goryachev, D. Patramanis, M. Krause, A. Redenti, D. Aldous, J. Lai, S. Coleman, J. Xu, S. Lee, I. Magoulas, S. Zhao, N. Tang, M. K. Cohen, O. Paradise, J. H. Kirchner, M. Ovchynnikov, J. O. Matos, A. Shenoy, M. Wang, Y. Nie, A. Sztyber-Betley, P. Faraboschi, R. Riblet, J. Crozier, S. Halasyamani, S. Verma, P. Joshi, E. Meril, Z. Ma, J. AndrÃ©oletti, R. Singhal, J. Platnick, V. Nevirkovets, L. Basler, A. Ivanov, S. Khoury, N. Gustafsson, M. Piccardo, H. Mostaghimi, Q. Chen, V. Singh, T. Q. KhÃ¡nh, P. Rosu, H. Szlyk, Z. Brown, H. Narayan, A. Menezes, J. Roberts, W. Alley, K. Sun, A. Patel, M. Lamparth, A. Reuel, L. Xin, H. Xu, J. Loader, F. Martin, Z. Wang, A. Achilleos, T. Preu, T. Korbak, I. Bosio, F. Kazemi, Z. Chen, B. BÃ¡lint, E. J. Y. Lo, J. Wang, M. I. S. Nunes, J. Milbauer, M. S. Bari, Z. Wang, B. Ansarinejad, Y. Sun, S. Durand, H. Elgnainy, G. Douville, D. Tordera, G. Balabanian, H. Wolff, L. Kvistad, H. Milliron, A. Sakor, M. Eron, A. F. D. O., S. Shah, X. Zhou, F. Kamalov, S. Abdoli, T. Santens, S. Barkan, A. Tee, R. Zhang, A. Tomasiello, G. B. D. Luca, S.-Z. Looi, V.-K. Le, N. Kolt, J. Pan, E. Rodman, J. Drori, C. J. Fossum, N. Muennighoff, M. Jagota, R. Pradeep, H. Fan, J. Eicher, M. Chen, K. Thaman, W. Merrill, M. Firsching, C. Harris, S. CiobÃ¢cÄƒ, J. Gross, R. Pandey, I. Gusev, A. Jones, S. Agnihotri, P. Zhelnov, M. Mofayezi, A. Piperski, D. K. Zhang, K. Dobarskyi, R. Leventov, I. Soroko, J. Duersch, V. Taamazyan, A. Ho, W. Ma, W. Held, R. Xian, A. R. Zebaze, M. Mohamed, J. N. Leser, M. X. Yuan, L. Yacar, J. Lengler, K. Olszewska, C. D. Fratta, E. Oliveira, J. W. Jackson, A. Zou, M. Chidambaram, T. Manik, H. Haffenden, D. Stander, A. Dasouqi, A. Shen, B. Golshani, D. Stap, E. Kretov, M. Uzhou, A. B. Zhidkovskaya, N. Winter, M. O. Rodriguez, R. Lauff, D. Wehr, C. Tang, Z. Hossain, S. Phillips, F. Samuele, F. EkstrÃ¶m, A. Hammon, O. Patel, F. Farhidi, G. Medley, F. Mohammadzadeh, M. PeÃ±aflor, H. Kassahun, A. Friedrich, R. H. Perez, D. Pyda, T. Sakal, O. Dhamane, A. K. Mirabadi, E. Hallman, K. Okutsu, M. Battaglia, M. Maghsoudimehrabani, A. Amit, D. Hulbert, R. Pereira, S. Weber, Handoko, A. Peristyy, S. Malina, M. Mehkary, R. Aly, F. Reidegeld, A.-K. Dick, C. Friday, M. Singh, H. Shapourian, W. Kim, M. Costa, H. Gurdogan, H. Kumar, C. Ceconello, C. Zhuang, H. Park, M. Carroll, A. R. Tawfeek, S. Steinerberger, D. Aggarwal, M. Kirchhof, L. Dai, E. Kim, J. Ferret, J. Shah, Y. Wang, M. Yan, K. Burdzy, L. Zhang, A. Franca, D. T. Pham, K. Y. Loh, J. Robinson, A. Jackson, P. Giordano, P. Petersen, A. Cosma, J. Colino, C. White, J. Votava, V. Vinnikov, E. Delaney, P. Spelda, V. Stritecky, S. M. Shahid, J.-C. Mourrat, L. Vetoshkin, K. Sponselee, R. Bacho, Z.-X. Yong, F. de la Rosa, N. Cho, X. Li, G. Malod, O. Weller, G. Albani, L. Lang, J. Laurendeau, D. Kazakov, F. Adesanya, J. Portier, L. Hollom, V. Souza, Y. A. Zhou, J. Degorre, Y. YalÄ±n, G. D. Obikoya, Rai, F. Bigi, M. C. BoscÃ¡, O. Shumar, K. Bacho, G. Recchia, M. Popescu, N. Shulga, N. M. Tanwie, T. C. H. Lux, B. Rank, C. Ni, M. Brooks, A. Yakimchyk, Huanxu, Liu, S. Cavalleri, O. HÃ¤ggstrÃ¶m, E. Verkama, J. Newbould, H. Gundlach, L. Brito-Santana, B. Amaro, V. Vajipey, R. Grover, T. Wang, Y. Kratish, W.-D. Li, S. Gopi, A. Caciolai, C. S. de Witt, P. HernÃ¡ndez-CÃ¡mara, E. RodolÃ , J. Robins, D. Williamson, V. Cheng, B. Raynor, H. Qi, B. Segev, J. Fan, S. Martinson, E. Y. Wang, K. Hausknecht, M. P. Brenner, M. Mao, C. Demian, P. Kassani, X. Zhang, D. Avagian, E. J. Scipio, A. Ragoler, J. Tan, B. Sims, R. Plecnik, A. Kirtland, O. F. Bodur, D. P. Shinde, Y. C. L. Labrador, Z. Adoul, M. Zekry, A. Karakoc, T. C. B. Santos, S. Shamseldeen, 16 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback L. Karim, A. Liakhovitskaia, N. Resman, N. Farina, J. C. Gonzalez, G. Maayan, E. Anderson, R. D. O. Pena, E. Kelley, H. Mariji, R. Pouriamanesh, W. Wu, R. Finocchio, I. Alarab, J. Cole, D. Ferreira, B. Johnson, M. Safdari, L. Dai, S. Arthornthurasuk, I. C. McAlister, A. J. Moyano, A. Pronin, J. Fan, A. Ramirez-Trinidad, Y. Malysheva, D. Pottmaier, O. Taheri, S. Stepanic, S. Perry, L. Askew, R. A. H. RodrÃ­guez, A. M. R. Minissi, R. Lorena, K. Iyer, A. A. Fasiludeen, R. Clark, J. Ducey, M. Piza, M. Somrak, E. Vergo, J. Qin, B. BorbÃ¡s, E. Chu, J. Lindsey, A. Jallon, I. M. J. McInnis, E. Chen, A. Semler, L. Gloor, T. Shah, M. Carauleanu, P. Lauer, T. Ãuc Huy, H. Shahrtash, E. Duc, L. Lewark, A. Brown, S. Albanie, B. Weber, W. S. Vaz, P. Clavier, Y. Fan, G. P. R. Silva, Long, Lian, M. Abramovitch, X. Jiang, S. Mendoza, M. Islam, J. Gonzalez, V. Mavroudis, J. Xu, P. Kumar, L. P. Goswami, D. Bugas, N. Heydari, F. Jeanplong, T. Jansen, A. Pinto, A. Apronti, A. Galal, N. Ze-An, A. Singh, T. Jiang, J. of Arc Xavier, K. P. Agarwal, M. Berkani, G. Zhang, Z. Du, B. A. de Oliveira Junior, D. Malishev, N. Remy, T. D. Hartman, T. Tarver, S. Mensah, G. A. Loume, W. Morak, F. Habibi, S. Hoback, W. Cai, J. Gimenez, R. G. Montecillo, J. Åucki, R. Campbell, A. Sharma, K. Meer, S. Gul, D. E. Gonzalez, X. Alapont, A. Hoover, G. Chhablani, F. Vargus, A. Agarwal, Y. Jiang, D. Patil, D. Outevsky, K. J. Scaria, R. Maheshwari, A. Dendane, P. Shukla, A. Cartwright, S. Bogdanov, N. MÃ¼ndler, S. MÃ¶ller, L. Arnaboldi, K. Thaman, M. R. Siddiqi, P. Saxena, H. Gupta, T. Fruhauff, G. Sherman, M. Vincze, S. Usawasutsakorn, D. Ler, A. Radhakrishnan, I. Enyekwe, S. M. Salauddin, J. Muzhen, A. Maksapetyan, V. Rossbach, C. Harjadi, M. Bahaloohoreh, C. Sparrow, J. Sidhu, S. Ali, S. Bian, J. Lai, E. Singer, J. L. Uro, G. Bateman, M. Sayed, A. Menshawy, D. Duclosel, D. Bezzi, Y. Jain, A. Aaron, M. Tiryakioglu, S. Siddh, K. Krenek, I. A. Shah, J. Jin, S. Creighton, D. Peskoff, Z. EL-Wasif, R. P. V, M. Richmond, J. McGowan, T. Patwardhan, H.-Y. Sun, T. Sun, N. ZubiÄ‡, S. Sala, S. Ebert, J. Kaddour, M. Schottdorf, D. Wang, G. Petruzella, A. Meiburg, T. Medved, A. ElSheikh, S. A. Hebbar, L. Vaquero, X. Yang, J. Poulos, V. Zouhar, S. Bogdanik, M. Zhang, J. Sanz-Ros, D. Anugraha, Y. Dai, A. N. Nhu, X. Wang, A. A. Demircali, Z. Jia, Y. Zhou, J. Wu, M. He, N. Chandok, A. Sinha, G. Luo, L. Le, M. NoyÃ©, M. PereÅ‚kiewicz, I. Pantidis, T. Qi, S. S. Purohit, L. Parcalabescu, T.-H. Nguyen, G. I. Winata, E. M. Ponti, H. Li, K. Dhole, J. Park, D. Abbondanza, Y. Wang, A. Nayak, D. M. Caetano, A. A. W. L. Wong, M. del Rio-Chanona, D. Kondor, P. Francois, E. Chalstrey, J. Zsambok, D. Hoyer, J. Reddish, J. Hauser, F.-J. Rodrigo-GinÃ©s, S. Datta, M. Shepherd, T. Kamphuis, Q. Zhang, H. Kim, R. Sun, J. Yao, F. Dernoncourt, S. Krishna, S. Rismanchian, B. Pu, F. Pinto, Y. Wang, K. Shridhar, K. J. Overholt, G. Briia, H. Nguyen, David, S. Bartomeu, T. C. Pang, A. Wecker, Y. Xiong, F. Li, L. S. Huber, J. Jaeger, R. D. Maddalena, X. H. LÃ¹, Y. Zhang, C. Beger, P. T. J. Kon, S. Li, V. Sanker, M. Yin, Y. Liang, X. Zhang, A. Agrawal, L. S. Yifei, Z. Zhang, M. Cai, Y. Sonmez, C. Cozianu, C. Li, A. Slen, S. Yu, H. K. Park, G. Sarti, M. BriaÅ„ski, A. Stolfo, T. A. Nguyen, M. Zhang, Y. Perlitz, J. Hernandez-Orallo, R. Li, A. Shabani, F. Juefei-Xu, S. Dhingra, O. Zohar, M. C. Nguyen, A. Pondaven, A. Yilmaz, X. Zhao, C. Jin, M. Jiang, S. Todoran, X. Han, J. Kreuer, B. Rabern, A. Plassart, M. Maggetti, L. Yap, R. Geirhos, J. Kean, D. Wang, S. Mollaei, C. Sun, Y. Yin, S. Wang, R. Li, Y. Chang, A. Wei, A. Bizeul, X. Wang, A. O. Arrais, K. Mukherjee, J. Chamorro-Padial, J. Liu, X. Qu, J. Guan, A. Bouyamourn, S. Wu, M. Plomecka, J. Chen, M. Tang, J. Deng, S. Subramanian, H. Xi, H. Chen, W. Zhang, Y. Ren, H. Tu, S. Kim, Y. Chen, S. V. MarjanoviÄ‡, J. Ha, G. Luczyna, J. J. Ma, Z. Shen, D. Song, C. E. Zhang, Z. Wang, G. Gendron, Y. Xiao, L. Smucker, E. Weng, K. H. Lee, Z. Ye, S. Ermon, I. D. Lopez-Miguel, T. Knights, A. Gitter, N. Park, B. Wei, H. Chen, K. Pai, A. Elkhanany, H. Lin, P. D. Siedler, J. Fang, R. Mishra, K. Zsolnai-FehÃ©r, X. Jiang, S. Khan, J. Yuan, R. K. Jain, X. Lin, M. Peterson, Z. Wang, A. Malusare, M. Tang, I. Gupta, I. Fosin, T. Kang, B. Dworakowska, K. Matsumoto, G. Zheng, G. Sewuster, J. P. Villanueva, I. Rannev, I. Chernyavsky, J. Chen, D. Banik, B. Racz, W. Dong, J. Wang, L. Bashmal, D. V. GonÃ§alves, W. Hu, K. Bar, O. Bohdal, A. S. Patlan, S. Dhuliawala, C. Geirhos, J. Wist, Y. Kansal, B. Chen, K. Tire, A. T. YÃ¼cel, B. Christof, V. Singla, Z. Song, S. Chen, J. Ge, K. Ponkshe, I. Park, T. Shi, M. Q. Ma, J. Mak, S. Lai, A. Moulin, Z. Cheng, Z. Zhu, Z. Zhang, V. Patil, K. Jha, Q. Men, J. Wu, T. Zhang, B. H. Vieira, A. F. Aji, J.-W. Chung, M. Mahfoud, H. T. Hoang, M. Sperzel, W. Hao, K. Meding, S. Xu, V. Kostakos, D. Manini, Y. Liu, C. Toukmaji, J. Paek, E. Yu, A. E. Demircali, Z. Sun, SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback I. Dewerpe, H. Qin, R. Pflugfelder, J. Bailey, J. Morris, V. Heilala, S. Rosset, Z. Yu, P. E. Chen, W. Yeo, E. Jain, R. Yang, S. Chigurupati, J. Chernyavsky, S. P. Reddy, S. Venugopalan, H. Batra, C. F. Park, H. Tran, G. Maximiano, G. Zhang, Y. Liang, H. Shiyu, R. Xu, R. Pan, S. Suresh, Z. Liu, S. Gulati, S. Zhang, P. Turchin, C. W. Bartlett, C. R. Scotese, P. M. Cao, B. Wu, J. Karwowski, D. Scaramuzza, A. Nattanmai, G. McKellips, A. Cheraku, A. Suhail, E. Luo, M. Deng, J. Luo, A. Zhang, K. Jindel, J. Paek, K. Halevy, A. Baranov, M. Liu, A. Avadhanam, D. Zhang, V. Cheng, B. Ma, E. Fu, L. Do, J. Lass, H. Yang, S. Sunkari, V. Bharath, V. Ai, J. Leung, R. Agrawal, A. Zhou, K. Chen, T. Kalpathi, Z. Xu, G. Wang, T. Xiao, E. Maung, S. Lee, R. Yang, R. Yue, B. Zhao, J. Yoon, S. Sun, A. Singh, E. Luo, C. Peng, T. Osbey, T. Wang, D. Echeazu, H. Yang, T. Wu, S. Patel, V. Kulkarni, V. Sundarapandiyan, A. Zhang, A. Le, Z. Nasim, S. Yalam, R. Kasamsetty, S. Samal, H. Yang, D. Sun, N. Shah, A. Saha, A. Zhang, L. Nguyen, L. Nagumalli, K. Wang, A. Zhou, A. Wu, J. Luo, A. Telluri, S. Yue, A. Wang, and D. Hendrycks. Humanitys last exam, 2025. URL https://arxiv.org/abs/2501.14249. J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. CoRR, abs/1506.02438, 2015. URL https: //api.semanticscholar.org/CorpusID:3075448. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Z. Shao, P. Wang, Q. Zhu, R. Xu, J.-M. Song, M. Zhang, Y. K. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024. URL https://api.semanticscholar.org/CorpusID:267412607. W. Shi, H. Tan, C. Kuang, X. Li, X. Ren, C. Zhang, H. Chen, Y. Wang, L. Shang, F. Yu, and Y. Wang. Pangu deepdiver: Adaptive search intensity scaling via open-web reinforcement learning. ArXiv, abs/2505.24332, 2025a. URL https://api.semanticscholar.org/CorpusID:279057242. W. Shi, H. Tan, C. Kuang, X. Li, X. Ren, C. Zhang, H. Chen, Y. Wang, L. Shang, F. Yu, and Y. Wang. Pangu deepdiver: Adaptive search intensity scaling via open-web reinforcement learning. arXiv preprint arXiv:2505.24332, 2025b. Z. Tao, J. Wu, W. Yin, J. Zhang, B. Li, H. Shen, K. Li, L. Zhang, X. Wang, Y. Jiang, P. Xie, F. Huang, and J. Zhou. Webshaper: Agentically data synthesizing via information-seeking formalization. 07 2025. doi: 10.48550/arXiv.2507.15061. H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10: 539554, 2022. doi: 10.1162/tacl_a_00475. URL https://aclanthology.org/2022.tacl-1.31/. H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal. Interleaving retrieval with chain-ofthought reasoning for knowledge-intensive multi-step questions. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001410037, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.557. URL https://aclanthology. org/2023.acl-long.557/. L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by weakly-supervised contrastive pre-training. ArXiv, abs/2212.03533, 2022. URL https://api. semanticscholar.org/CorpusID:254366618. J. Wei, Z. Sun, S. Papay, S. McKinney, J. Han, I. Fulford, H. W. Chung, A. Passos, W. Fedus, and A. Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. ArXiv, abs/2504.12516, 2025. URL https://api.semanticscholar.org/CorpusID:277857238. 18 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback J. Wu, B. Li, R. Fang, W. Yin, L. Zhang, Z. Tao, D. Zhang, Z. Xi, Y. Jiang, P. Xie, F. Huang, and J. Zhou. Webdancer: Towards autonomous information seeking agency. 05 2025. doi: 10.48550/arXiv.2505. 22648. Q. A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y.-C. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, Z. Qiu, S. Quan, and Z. Wang. Qwen2.5 technical report. ArXiv, abs/2412.15115, 2024. URL https://api.semanticscholar.org/CorpusID:274859421. Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259/. S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X. S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried, U. Alon, and G. Neubig. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=oKn9c6ytLx. 19 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Figure 4 Prompt used for initial data generator. Prompt used for initial data generator. Your task is to generate complicated question that will require search agent target_search_step search steps to answer by gathering information using search engine. You will first reason about the initial document and plan for gathering comprehensive information inside <think> and </think>. You will then call search engine by <search> query </search> and it will return the top searched results between <information> and </information> to collect information. You must conduct reasoning inside <think> and </think> first every time you get new information. You will call the search engine for n_search_step steps. After n_search_step searches, you must provide the question inside <question> and </question>, the answer inside <answer> and </answer>, and the answering step inside <answering steps> and </answering steps>. You can use your own knowledge to construct the search query, but the final answer and each of the answering step must be supported by the information you gathered from the search engine. The question should be understandable standalone as the agent will use the question to search for information without access to the initial document. An example question: How much did the film in which Jake Gyllenhaal played his second lead role gross in its initial run at the box office? Avoid How and Why question. The answer should be answer_type and short. Make sure the answer is correct and **unique** for the question generated. Initial document: context A. Appendix A.1. Use of large language models (LLMs) We acknowledge the use of LLMs (e.g. ChatGPT) for editing the text to correct grammatical errors and improve clarity and flow. All core scientific content and research ideas were authored solely by the authors. A.2. Implementation details for SAGE For data generator and search agent, we set temperature to 1 and disable thinking for gemini-2.5-flash. If the data generator does not generate (ğ‘, ğ‘) pair after issuing the maximum number of search calls, we force it to generate (ğ‘, ğ‘) pair by appending <think>I have used up all the search budget and will use the existing information to formulate new plan and generate the question, answer, and answering plans. to the prompt. Data generator setting We include the prompt used for the initial data generator agent in Table 4; and the prompt for feedback in Figure 7 and Figure 8. Search agent setting We include the prompt used for search agent in Table 5. LLM-as-a-judge setting We include the prompt used for reference-based LLM-as-a-judge in Prompt 6. We use gemini-2.0-flash with temperature 0. 20 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Figure 5 Prompt used for search agent. Prompt used for search agent. Answer the given question by using search engine. You will first reason about the question inside <think> and </think>, for instance, break down the question into multiple sub-questions that you will search for. You must call search engine by <search> query </search> and it will return the top searched results between <information> and </information>. Try to formulate the search query in the form of question. After receiving the information, you must reason about it inside <think> and </think> before issuing new query or providing the final answer. Each of your reasoning step should be grounded in the retrieved information. Do not use your own knowledge, but you can use commonsense knowledge or arithmetic knowledge. Do not use your own knowledge to write the query, the query should be based on the question and the retrieved documents. Do not infer the entities in the question, but you can use the entities in the retrieved documents to write the query. You can search as many times as your want. Try to break down the question for each search query and gather comprehensive information. If you have gathered enough information to answer the question, you can provide the answer to the query inside <answer> and </answer>, without detailed illustrations. Generate an answer based on the retrieved information, instead of your own knowledge. This is an example answer: <answer>Beijing</answer>. Question: question A.3. Implementation details for training the search agent PPO objective. We conduct downstream evaluation by training search agents with the Proximal Policy Optimization (PPO) (Schulman et al., 2017). It optimizes the language model by maximizing the below objective in Figure 9, with ğœ‹ğ‘¡â„ğ‘’ğ‘¡ğ‘ and ğœ‹ğ‘œğ‘™ğ‘‘ representing the current the previous policy model; ğ¼( ğ‘¦ğ‘¡) representing whether the loss masking apply to the token, we apply loss masking to retrieved document (ğ¼( ğ‘¦ğ‘¡) = 0). ğœ€ is clipping hyperparameter, and the advantage estimate ğ´ğ‘¡ is computed using Generalized Advantage Estimation (Schulman et al., 2015). Training details. For training the search agent, we set the learning rate of the policy LLM to 1e-6 and that of the value LLM to 1e-5. Training is conducted for 500 steps, with warm-up ratios of 0.285 and 0.015 for the policy and value models, respectively. We use Generalized Advantage Estimation (GAE) with parameters ğœ† = 1 and ğ›¾ = 1. Training is conducted on single node with 8 H100 GPUs. We use total batch size of 512, with mini-batch size of 256 and micro-batch size of 64. The maximum sequence length is set to 8,192 tokens, with maximum response length of 1024 and maximum length of 1000 tokens for retrieved content. We set the maximum number of retrieval calls to 8. A.4. Analysis details We include the prompt for reasoning strategy analysis in Prompt 10, which also presents the definition for each of the category. 21 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Figure 6 Prompt used for reference-based LLM-as-a-judge. Prompt used for reference-based LLM-as-a-judge. Judge whether the following [response] to [question] is correct or not based on the precise and unambiguous [correct_answer_list] below. Each answer in the [correct_answer_list] is separated by comma. [question]: question [response]: model_answer Your judgment must be in the format and criteria specified below: extracted_final_answer: The final exact answer extracted from the [response]. Put the extracted answer as None if there is no exact, final answer to extract from the response. [correct_answer_list]: gold_answer reasoning: Explain why the extracted_final_answer is correct or incorrect based on [correct_answer_list], focusing only on if there are meaningful differences between answer in the [correct_answer_list] and the extracted_final_answer. Focus on recall, i.e. if the extracted_final_answer covers all the points in the answer in the [correct_answer_list]. It is ok if it provides more details. It is also ok if the extracted_final_answer misses minor point from the correct_answer, as long as it is evident that they are referring to the same thing. Do not comment on any background to the problem, do not attempt to solve the problem, do not argue for any answer different than [correct_answer_list], focus only on whether the answers match. Ignore capitalization. correct: Answer yes if extracted_final_answer matches any of the answers in [correct_answer_list] given above, or is within small margin of error for numerical problems. Answer no otherwise, i.e. if there is any inconsistency, ambiguity, non-equivalency, or if the extracted answer is incorrect. confidence: The extracted confidence score between 0% and 100% from [response]. Put 100 if there is no confidence score available. Example questions Previous question: On what date did Tottenham Hotspur win their first major trophy, feat that made them the only non-league club to achieve it, while under the chairmanship of the individual who took office the year before the clubs establishment at its original ground, colloquially known as \"The Lane\"? Previous answer: April 27, 1901 Updated question: What was the date of the replay match in which the player, who notably scored goals in every round of Tottenham Hotspurs first major trophy victory, helped his team secure that trophy after the club had become limited company under chairman who served until 1943? Updated answer: April 27, 1901 Previous question: What is the death date of the actor who sang songs in the 1944 film that was Naushads first musical success? Previous answer: 2 August 1979 Updated question: What is the death date of the actor, born in Gujranwala, who sang songs in Naushads first musical success film of 1944? Updated answer: 2 August 1979 Previous question: What was the precise date Sir Frank Whittle, key figure honored at the Midlands Air Museum, received his knighthood as Knight Commander of the Order of the British Empire (KBE) for his pioneering work on the jet engine? Previous answer: July 1, 1948 Updated question: What was the precise date of the military retirement of the figure whose heritage center is located within the museum adjacent to the former Electric Railway Museum site? Updated answer: 26 August 1948 Table 7 Example question generated and updated by our pipeline. 22 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Figure 7 Prompt used for incorporating execution feedback to update the incorrect QA pair. Prompt used for incorporating execution feedback to update the incorrect QA pair. You will be given an output from question generator agent, which generates complicated question, answer pair; as well as the output from search agent, which attempts to solve the question generated in fixed number of turns. The answer from the search agent is not the same as the data generator agent. You task is to examine their traces and output the correct question, answer pair based on their retrieved documents. You can update either the question, the answer or both. You will first reason about why is there discrepancy between the search agents answer and the data generators answer. Output your reasoning trace inside <reason> and </reason>. You will then reason about how to update the question answer pair to make sure it is correct and requires the agent target_step search step to answer. search step is defined as call to the search tool. Output your reasoning trace inside <think> and </think>. For factual information, you should ONLY rely on the context provided for the data generator agent and the documents retrieved by both the data generator and search agent (inside <information> and </information>). If you find it non-trivial to update just the question and answer, you can generate new question answer pair ONLY based on the retrieved documents. The updated question should require the search agent at least target_step search steps to answer. However, the answer should be short, such as an entity, date or number. The question should be understandable standalone, as the search agent will solve the question without access to the documents (they will need to search for them). When you are ready to provide the new question, answer pair, you can provide the question inside <question> and </question>, the answer inside <answer> and </answer>, and the search step inside <search steps> and </search steps>. For each search step, output the exact search question; the sub-answer to the search question; and the retrieved document from the search agent and data generator agents output that supports the sub-answer. Make sure each step is absolutely needed to answer the question and there is no short cut. Tip: use retrieved document from different steps so avoid two sub-queries being solved by one search query. # Data generator agent Prompt: data_generator_agent_prompt Agents output: data_generator_agent_response # Search agent Prompt: search_agent_prompt Agents output: search_agent_response # Your output Figure 8 Prompt used for incorporating execution feedback to update the easy QA pair. Prompt used for incorporating execution feedback to update the easy QA pair. You will be given an output from question generator agent, which generates complicated question, answer pair to be solved by search agent for at least target_step **search** steps; as well as the output from search agent, which attempts to solve the question generated. The search agent is able to solve the question in less thantarget_step search steps. Your task is to update the question so that it requires the search agent more steps to solve. You will first reason about why the search agent is able to solve the question in fewer steps. Output your reasoning trace inside <reason> and </reason>. You will then reason about how to update the question so that it will require more search steps. For factual information, you should ONLY rely on the context provided for the data generator agent and the documents retrieved by both the data generator and search agent (inside <information> and </information>), without relying on other information not in the retrieved context. Output your reasoning trace inside <think> and </think>. If you find it non-trivial to update the plan, you can generate new question answer pair ONLY based on the retrieved documents. The updated question should require the search agent at least target_step search steps to answer. Note that some of the answering steps do not involve search and thus do not count. However, the answer should be short, such as an entity, date or number. The question should be understandable standalone, as the agent will solve the question without access to the documents (they will need to search for them). When you are ready to provide the new question, answer pair, you can provide the question inside <question> and </question>, the answer inside <answer> and </answer>, and the search step inside <search steps> and </search steps>. For each search step, output the exact search question; the sub-answer to the search question; and the retrieved document from the search agent and data generator agents output that supports the sub-answer. Make sure each step is absolutely needed to answer the question and there is no short cut. Tip: use retrieved document from different steps so avoid two sub-queries being solved by one search query. # Data generator agent Prompt: data_generator_agent_prompt Agents output: data_generator_agent_response # Search agent Prompt: search_agent_prompt Agents output: search_agent_response # Your output 23 SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback JPPO(ğœƒ) = ğ”¼ğ‘¥D, ğ‘¦ğœ‹old ( ğ‘¥;R ) 1 ğ‘¦ (cid:205) ğ‘¦ ğ‘¡=1 ğ¼( ğ‘¦ğ‘¡) ğ‘¡=1:ğ¼ ( ğ‘¦ğ‘¡ )= min(cid:16) ğœ‹ğœƒ( ğ‘¦ğ‘¡ ğ‘¥, ğ‘¦<ğ‘¡; R) ğœ‹old( ğ‘¦ğ‘¡ ğ‘¥, ğ‘¦<ğ‘¡; R) ğ´ğ‘¡, clip(cid:0)ğ‘Ÿğ‘¡ (ğœƒ), 1 ğœ€, 1 + ğœ€(cid:1) ğ´ğ‘¡ (cid:17) Figure 9 PPO objective for training search agents. Figure 10 Prompt used for reasoning strategy analysis. Prompt used for reasoning strategy analysis. You will be given question and trace of search agent solving this question. You will analyze and categorize the behavior for each of the thinking step. Below are some example categories, please feel free to propose new ones as you see appropriate: - Information inference: the agent makes an inference based on the piece of information in the retrieved document in its reasoning. - Conflict resolution: the agent reasons about conflicting information in the documents and makes decision. - Calculation: The agent performs numerical calculation. - Temporal reasoning: The agent performs temporal reasoning, such as deriving duration between two dates. - Self-correction: The agent recognizes that the previous search failed to yield the required information (a list of stations). It re-evaluates its state, confirms its goal, and decides to try more specific search query. - Hypothesis Generation: The agent makes guess / hypothesis that is not grounded in the retrieved documents. Output format: only return the list of strategies for each step: - Step i: [list of strategies] - Step i+1: [list of strategies] Question: question Agents reasoning trace: agent_trace Name WebDancer (Wu et al., 2025) WebShaper (Tao et al., 2025) WebSailor (Li et al., 2025a) WebPuzzle (Shi et al., 2025b) WebExplorer (Liu et al., 2025) Retriever Google Search Google Search Google Search Web Search Google Search SAGE (Ours) Wikipedia-based Explicit structure Input Knowledge graph and inter-page hyperlinks Knowledge graph Inter-page hyperlinks Web page or seed entity Webpage Seed entity Pre-selected rare entities Web page or seed entity Seed entity No No Passage Table 8 Comparison with concurrent work on synthetic data generation for search agents. Explicit structure refers to reliance on predefined or constructed graphs, entity links, or formal schemas during data generation. None of the concurrent work open-sources large-scale training data."
        }
    ],
    "affiliations": [
        "Google Cloud AI Research",
        "New York University"
    ]
}