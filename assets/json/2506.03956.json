{
    "paper_title": "Adapt before Continual Learning",
    "authors": [
        "Aojun Lu",
        "Tao Feng",
        "Hangjie Yuan",
        "Chunhui Ding",
        "Yanan Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Continual Learning (CL) seeks to enable neural networks to incrementally acquire new knowledge (plasticity) while retaining existing knowledge (stability). While pre-trained models (PTMs) have become pivotal in CL, prevailing approaches freeze the PTM backbone to preserve stability, limiting their plasticity, particularly when encountering significant domain gaps in incremental tasks. Conversely, sequentially finetuning the entire PTM risks catastrophic forgetting of generalizable knowledge, exposing a critical stability-plasticity trade-off. To address this challenge, we propose Adapting PTMs before the core CL process (ACL), a novel framework that refines the PTM backbone through a plug-and-play adaptation phase before learning each new task with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by aligning embeddings with their original class prototypes while distancing them from others, theoretically and empirically shown to balance stability and plasticity. Extensive experiments demonstrate that ACL significantly improves CL performance across benchmarks and integrated methods, offering a versatile solution for PTM-based CL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 6 5 9 3 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Adapt before Continual Learning",
            "content": "Aojun Lu College of Computer Science Sichuan University Chengdu, China aojunlu@stu.scu.edu.cn Tao Feng Department of Computer Science and Technology Tsinghua University Beijing, China fengtao.hi@gmail.com Hangjie Yuan College of Computer Science and Technology Zhejiang University Hangzhou, China hj.yuan@zju.edu.cn Chunhui Ding College of Computer Science Sichuan University Chengdu, China chunhuiding9@gmail.com Yanan Sun College of Computer Science Sichuan University Chengdu, China ysun@scu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Continual Learning (CL) seeks to enable neural networks to incrementally acquire new knowledge (plasticity) while retaining existing knowledge (stability). While pre-trained models (PTMs) have become pivotal in CL, prevailing approaches freeze the PTM backbone to preserve stability, limiting their plasticity, particularly when encountering significant domain gaps in incremental tasks. Conversely, sequentially finetuning the entire PTM risks catastrophic forgetting of generalizable knowledge, exposing critical stability-plasticity trade-off. To address this challenge, we propose Adapting PTMs before the core CL process (ACL), novel framework that refines the PTM backbone through plug-and-play adaptation phase before learning each new task with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by aligning embeddings with their original class prototypes while distancing them from others, theoretically and empirically shown to balance stability and plasticity. Extensive experiments demonstrate that ACL significantly improves CL performance across benchmarks and integrated methods, offering versatile solution for PTM-based CL."
        },
        {
            "title": "Introduction",
            "content": "In open-world scenarios, data often arrives in streaming fashion, necessitating machine learning paradigm capable of incrementally acquiring new knowledge without forgetting, known as Continual Learning (CL) [1, 2]. Effective CL hinges on neural networks ability to achieve the trade-off between plasticity, which enables the learning of new concepts, and stability, which ensures the retention of previously acquired knowledge. However, overemphasizing stability can limit the neural networks ability to acquire new information, while excessive plasticity can lead to catastrophic forgetting of existing knowledge [3, 4], which is known as the stability-plasticity dilemma [5]. Corresponding author Preprint. Under review. (a) Plasticity (b) Stability (c) Overall CL performance Figure 1: Performance comparison on ImageNet-A-Inc20 between the frozen PTM and the PTM adapted using our ACL. Plasticity: the average of the optimal accuracy of each task during CL; Stability: the average forgetting across previous tasks after learning the final task; Overall CL performance: the average accuracy across all tasks after learning the final task, i.e., the last accuracy. The advent of powerful Pre-Trained Models (PTMs) has significantly reshaped the machine learning domain, spurring considerable interest in their application to CL [2]. PTMs, typically trained on large-scale datasets like ImageNet [6, 7], exhibit strong generalization capabilities, making their transferable embeddings highly valuable for downstream incremental tasks. Consequently, prevalent strategy in PTM-based CL involves preserving this pre-existing knowledge by freezing the PTM backbone and finetuning only lightweight, task-specific modules (e.g., prompts or adapters). For instance, L2P [8] and DualPrompt [9] employ visual prompt tuning [10] to learn new tasks without modifying pre-trained weights. Similarly, SSIAT [11] and MOS [12] utilize adapter-based tuning, updating the PTMs while keeping the core backbone frozen. While these approaches effectively mitigate catastrophic forgetting by preserving the generalizable knowledge of PTMs, their plasticity critically depends on the alignment between pre-trained features and incremental task data. critical challenge emerges when significant domain gap exists between the PTMs pre-training distribution and the downstream incremental tasks. In such scenarios, the frozen PTM backbone may struggle to extract discriminative features for new tasks [13], resulting in diminished plasticity and, consequently, suboptimal CL performance. Conversely, sequentially finetuning the entire PTM backbone, while potentially boosting plasticity for the current task, risks degrading its generalizable knowledge and leads to irreversible forgetting [14]. Although finetuning the backbone only for the initial task before freezing it can partially mitigate this issue [15], this strategy remains insufficient when confronted with subsequent task distribution shifts. This underscores fundamental challenge in PTM-based CL: how to continually adapt the PTM to each incremental task to enhance plasticity while simultaneously preserving stability. To address this pivotal challenge, we introduce Adapting PTMs before the core CL process (ACL), novel framework that refines the PTM backbone through plug-and-play adaptation phase before learning each new task with existing CL approaches (e.g., prompt tuning). ACL augments existing PTM-based CL methods by furnishing them with more task-relevant feature foundation. Crucially, during the adaptation phase, the PTM backbone is tuned by encouraging its output embeddings to move closer to their respective original class prototypes while simultaneously distancing them from other class prototypes. This approach is theoretically and empirically shown to enhance plasticity while preserving stability. As illustrated in Fig. 1, integrating ACL not only significantly improves plasticity but also maintains stability, leading to superior overall CL performance. Extensive experiments across diverse benchmarks and established CL methods validate the effectiveness and broad applicability of ACL. Code available at https://github.com/byyx666/ACL_code. The contributions of this study are outlined as follows: (i) We point out that prevailing PTM-based CL methods yield suboptimal overall performance due to inherent limitations in plasticity, highlighting the critical need for an effective PTM adaptation mechanism. (ii) We theoretically demonstrate that enhancing plasticity while preserving stability, the core objectives of adaptation, is effectively realized by encouraging embeddings to converge towards their original class prototypes and diverge from others. (iii) We propose ACL, novel and versatile CL framework designed for integration with existing PTM-based methods, significantly boosting their plasticity and overall CL performance."
        },
        {
            "title": "2.1 Continual Learning (CL)",
            "content": "CL aims to enable neural networks to sequentially acquire knowledge from series of tasks without forgetting previously learned concepts [16, 17]. Traditional CL methods can be broadly categorized into three types. Replay-based methods retain subset of previous data information in memory buffer, which is subsequently utilized to recover old data distributions [18, 19, 20, 21]. Regularizationbased methods incorporate penalty terms that constrain model updates during the learning of new tasks [22, 23, 24, 25]. Architecture-based methods allocate task-specific parameter spaces within the network for each new task, thereby mitigating catastrophic forgetting [26, 27, 28, 29]. CL with PTMs. With the growing prevalence of PTMs [30, 31], PTM-based CL has recently garnered significant attention. Given that PTMs have been equipped with generalizable knowledge, these methods often freeze the pre-trained backbones and utilize additional trainable modules to learn task-specific knowledge [2]. Early research primarily focuses on applying visual prompt tuning [10] to CL, enabling models to learn new tasks without modifying the pre-trained weights [32, 33], e.g., L2P [8] and DualPrompt [9]. Recently, some studies have demonstrated that adapter-based tuning outperforms prompt-based methods in PTM-based CL [11, 34], e.g., SSIAT [11] and MOS [12]. In addition to developing additional trainable modules, several studies have focused on optimizing the classification head to enhance CL performance, e.g., FeCAM [35] and RanPAC [36]."
        },
        {
            "title": "2.2 Prototypical Networks",
            "content": "Prototypical networks [37] involve learning an embedding space where samples are classified by minimizing their distance to the mean embedding (prototype) of their respective class [38, 39]. In our research, we demonstrate, both theoretically and empirically, that the integration of this core principle into the adaptation phase of PTMs achieves desirable balance between plasticity and stability."
        },
        {
            "title": "2.3 Contrastive Learning",
            "content": "Contrastive learning [40] has emerged as powerful framework in self-supervised learning and supervised learning [41], which brings similar examples closer together in the feature space while pushing dissimilar examples apart. In the context of CL, several studies have leveraged contrastive learning to enhance stability [42, 43], e.g., Co2L [44] and PCL [45]. These approaches typically contrast embeddings from current task data against replayed samples from previous tasks (exemplar replay) to preserve learned representations. Unlike these methods that primarily utilize contrastive learning with replayed data to bolster stability, our work focuses on applying contrastive principles exclusively to the current tasks data. Furthermore, our primary objective through this application is to enhance plasticity, with stability being an emergent benefit of our formulation."
        },
        {
            "title": "3 ACL: Adapt before Continual Learning",
            "content": "To enhance the plasticity in CL with PTMs, we propose novel CL framework that introduces an adaptation phase before learning each incremental task. This phase is designed to adapt the pre-trained weights to the new data while preserving the previous knowledge without memory replay. In the following sections, we first outline the overall procedure of our proposed framework, termed ACL, and then provide detailed explanation of the specific techniques employed for adaptation."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "For clarity, we decouple the CL models into two components: (x) = C(ϕ(x)), where ϕ() : RD Rd is the backbone network and C() : Rd RYk is the classification head. The backbone ϕ() transforms input samples into embeddings, while the classification head maps these embeddings to classification results. With the introduction of PTM to CL [8], continual learners inherently acquire generalizability [15]. To leverage this capability, most existing CL methods based on PTMs keep the backbone ϕ() frozen throughout the learning process. To learn knowledge from new tasks, these methods typically introduce additional lightweight modules Θ (e.g., prompts and adapters), resulting in (x) = C(ϕ(x), Θ), and finetune only these modules and the classification head."
        },
        {
            "title": "3.2 Overall Procedure of ACL.",
            "content": "Figure 2: Illustration of ACL, comprising two phases: (1) adapting the weights of the PTM to enhance the discriminative capability of the generated features, and (2) learning the classification of features using frozen PTM weights. The procedure of ACL is shown in Fig. 2, which consists of two phase. At the beginning of learning the k-th task, phase is initiated to adapt the weights of the pre-trained backbone ϕk1 and lightweight modules Θk1 to the current data Dk, making the generated features more discriminative. After that, the adapted backbone ϕ k1 is frozen, while the classification head Ck1 and the adapted modules Θ k1 are further finetuned to learn the classification of features. Specifically, the adaptation process is formally defined as: k1, Θ ϕ k1 = A(ϕk1, Θk1, Dk), (1) where denotes the adaptation algorithm, which outputs the adapted backbone ϕ k1 and lightweight modules Θ k1. Crucially, is designed to enhance the separability of extracted features for the current dataset Dk while preserving the generalizable knowledge. Note that the detailed design of is elaborated in the following subsection. Then, the existing CL methods are utilized for leveraging the PTMs knowledge to perform CL. Specifically, the subsequent learning process is formulated as: ϕk = ϕ k1, Ck, Θk = F(Ck1, Θ (3) where represents the learning algorithm employed in this phase. The specific implementation of is determined by the CL methods integrated into our framework. k1, ϕk, Dk), The pseudo-code for ACL is presented in Alg. 1. Specifically, ACL iteratively applies the adaptation and core learning phases to each incremental task, continuing this until all tasks have been learned. Algorithm 1 Procedure of ACL 1: Input: PTM backbone ϕ0, Additional lightweight module Θ0, Classification Head C0, Incremental datasets {D1, D2, . . . , DK}; (2) 2: Output: Updated PTM; 3: for task = 1, 2, . . . , do 4: 5: 6: 7: 8: end for Get the incremental training set Dtrain Optimize ϕk1 and Θk1 to obtain ϕ Save ϕ Optimize Θ k1, i.e., ϕk = ϕ k1; ; k1 and Θ k1 via Eq. 1; k1 and Ck1 to obtain Θk and Ck via Eq. 3 with frozen ϕk;"
        },
        {
            "title": "3.3 Adaptation without Forgetting",
            "content": "This subsection explores strategies for adapting PTMs (plasticity) while mitigating catastrophic forgetting (stability). While the ultimate aim is harmonious balance between plasticity and stability, our analytical approach first prioritizes the objectives of plasticity, and then considers stability to guide design choices that do not compromise this primary goal. 4 Notation. We consider set of samples within the same class c, {(x1, c), (x2, c), . . . , (xM , c)}. Let = {z1, z2, . . . , zM } denote the d-dimensional embeddings extracted by the original PTM. Our adaptation goal is to transform these into new ideal set of embeddings = {z 1, 2, . . . , }. Optimizing for Plasticity. core objective of adaptation is to increase the discriminative power of the learned features. This is achieved by ensuring that embeddings of samples from the same class become more concentrated or tightly clustered. We quantify this by seeking to minimize the expected distance between pairs of embeddings from the same class: Lplastic(Z ) i,z Z [z jα 2 ], α > 0 (4) To achieve this concentration, these updated embeddings for class should ideally converge towards class-specific target vector, vc. We model these embeddings as normally distributed around vc, with covariance σ2I, i.e., (vc, σ2I), where is identity matrix. Thus, key aim for plasticity is the minimization of σ2. Optimizing for Stability under the Plasticity Objective. Since the vc does not influence the plasticity loss, we can therefore select vc to optimize for stability. Following prior research [24, 46], we employ feature distillation [47] to promote stability. This is quantified by minimizing the mean squared error between the original embeddings and the adapted embeddings : Lstable(Z ; Z) 1 (cid:88) i=1 zi i2 2. Given our modeling assumption (vc, σ2I), we can analyze the expected stability loss: E[Lstable(Z ; Z)] ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) i=1 E[zi i2 2]. Expanding the expectation term, we have: E[zi i2 = zi vc 2] = E[zi vc (z 2 + E[z vc2 vc)2 2] 2] 2E[(zi vc)(z vc)]. (5) (6) (7) The expectation E[] is taken over the distribution of zi and vc are considered fixed values with respect to this specific expectation over we can write: E[(zi vc)(z vc)] = (zi vc)E[z vc] = E[z E[vc (vc, σ2I). For the cross-term, i. Therefore, i] vc = vc vc = 0. Consequently, the entire cross-term becomes 0. Furthermore, i2 2] = Tr(σ2I) = dσ2. Therefore, the expected stability loss simplifies to: (vc, σ2I), E[z vc]. Since i, which E[Lstable(Z ; Z)] 1 (cid:88) i=1 (cid:0)zi vc 2 + dσ2(cid:1) . (8) To minimize Equation 8 with respect to vc, we only need to consider the term (cid:80)M 2, as dσ2 is independent of vc. Thus, vc for optimal stability is the mean of the original embeddings for class c, i.e., the original class prototype pc = 1 i=1 zi. Moreover, under our modeling assumptions, the stability can also be promoted by minimizing σ2, which aligns with the objectives of plasticity. i=1 zi vc2 (cid:80)M Final Objective of Adaptation. While aligning adapted features with their original class prototypes is beneficial for plasticity and stability, this approach alone may still be insufficient for achieving optimal feature discriminability in multi-class scenario. Specifically, it may compromise representation uniformity across the feature space [48], as excessive alignment could lead to collapsed yet nondiscriminative embeddings. Contrastive learning provides principled framework for addressing this by managing both alignment and uniformity [49]. In this paradigm, the positive component of the loss still encourages embeddings to move closer to their original prototype of the true class, aligning with the previous objective. Crucially, the negative component simultaneously pushes these embeddings away from the prototypes of other, incorrect classes."
        },
        {
            "title": "3.4 Detailed Adapting Algorithm",
            "content": "This subsection details the adaptation algorithm, which leverages contrastive loss to achieve the refined objectives discussed above. Consider an incremental learning task with dataset {(x1, y1), (x2, y2), . . . , (xN , yN )}, where each label yi {a + 1, . . . , + C} represents one of new classes introduced in the current task. The existing PTM, (), is used to extract an initial embedding zi = (xi) for each input sample xi. For each class among the new classes, its prototype pc is computed as the mean of these initial embeddings zi belonging to that class. During adaptation, we aim to adjust the PTM backbone = (xi) are refined. The objective is to maximize such that the newly produced embeddings the similarity between an adapted embedding and its corresponding class prototype pyi, while simultaneously minimizing its similarity to the prototypes pk of other classes = yi within the current task. We employ an InfoNCE loss [40] for this purpose. Specifically, for each adapted embedding and each class prototype pj (where {a + 1, . . . , + C}), we compute similarity score si,j. The definition of this similarity metric depends on the employed classification head. For linear classifiers, similarity is defined as the negative Euclidean distance: si,j = pj2; for cosine classifiers, similarity is defined as the cosine similarity: si,j = ipj . Let = {si,a+1, si,a+2, . . . , si,a+C} be the set of similarity scores for the adapted i2pj 2 embedding with all class prototypes within the current task. This set of scores is then transformed into probability distribution Oi = {oi,a+1, oi,a+2, . . . , oi,a+C} using the SoftMax function: oi,j = exp(si,j/t) k=a+1 exp(si,k/t) (cid:80)a+C , (9) where is temperature hyperparameter. In this study, is set to 0.1 for cosine similarity and 2 for distance-based similarity. The contrastive training objective for each sample xi with true label yi is to maximize the probability oi,yi corresponding to the true class yi. This is achieved by minimizing the cross-entropy loss: = log oi,yi."
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "Dataset. Given that PTMs are typically trained on ImageNet series datasets [7], evaluation on the standard ImageNet benchmark is meaningless due to the overlapping data distribution [15]. Hence, we evaluate our approach on two datasets that exhibit significant domain gap [15] with ImageNet, i.e., ImageNet-R [50] and ImageNet-A [51]. To simulate CL scenario, both datasets are equally divided into multiple tasks without overlapping data. Specifically, we create two task configurations: (1) 20 tasks with 10 classes each (Inc-10) and (2) 10 tasks with 20 classes each (Inc-20). Baselines. We compare our proposed method against six state-of-the-art PTM-based CL methods: L2P [8], DualPrompt [9], RanPAC [36], FeCAM [35], SSIAT [11], and MOS [12]. Since our framework is designed as plug-and-play component, we integrate it into these baseline methods to systematically assess its effectiveness. Furthermore, we include comparison with Aper [15], method that enhances CL performance by adapting the PTM using standard classification loss to the first task and subsequently freezing and merging it with the original model. Aper is specialized through various adaptation algorithms, including full Finetune, Visual Prompt Tuning (VPT), Scale and Shift (SSF), and Adapter-based tuning. In particular, Aper with VPT has two variants: VPTDeep, which prepends the prompts at every attention layer, and VPT-Shallow, which only prepends the prompts at the first layer [15]. Following these baselines, our validation focuses on general and realistic CL scenario i.e., class incremental learning, where the task identity of each sample is unknown during inference [1]. Implementation Details. We select representative PTM, denoted as ViT-B/16-IN1K, for our experiments. This PTM is initially pre-trained on ImageNet21K [7] and subsequently finetuned on ImageNet1K [6]. To ensure consistency and reproducibility, we adhere to the hyperparameter configurations provided by the open-source library PILOT [52] for all baseline CL methods. For each incremental task, we limit the adaptation phase to 1 training epoch to minimize computational overhead. For all results, we report mean std of 5 runs with different task orders. 6 Table 1: The AOA and LA (%) using six state-of-the-art CL methods. Improvement represents the boost of ACL towards original methods. Method L2P w/ Ours ImageNet-R-Inc20 ImageNet-R-Inc10 ImageNet-A-Inc20 ImageNet-A-Inc10 AOA LA AOA LA AOA LA AOA LA 75.650.86 79.900.91 71.910.27 74.200.57 74.660.96 79.040.99 69.240.78 71.030.45 50.501.31 54.380. 42.580.39 48.071.17 46.902.19 51.702.24 34.930.96 41.161.39 Improvement +4.25 +2. +4.38 +1.79 +3.88 +5.49 +4.80 +6. DualPrompt w/ Ours 74.500.68 78.800.36 69.430.51 74.230.49 71.390.52 76.760.40 65.710.24 70.720.23 55.070.59 60.490. 45.351.04 51.450.51 49.651.55 55.421.10 39.041.83 44.521.45 Improvement +4.30 +4. +5.37 +5.01 +5.42 +6.10 +5.77 +5. FeCAM w/ Ours 65.531.30 74.261.22 60.391.30 65.820.80 61.800.39 72.212.41 55.600.26 63.051.48 41.211.04 48.491. 33.430.18 41.280.61 42.031.14 46.191.19 33.790.10 38.620.44 Improvement +8.73 +5. +10.41 +7.45 +7.28 +7.85 +4.16 +4. RanPAC w/ Ours 81.301.02 85.450.63 76.070.85 79.140.21 78.610.26 84.410.54 72.840.23 78.200.25 66.830.69 72.250. 58.160.46 64.450.37 67.041.68 71.132.47 57.331.26 61.571.75 Improvement +4.15 +3. +5.80 +5.36 +5.42 +6.29 +4.09 +4. SSIAT w/ Ours 82.270.62 83.410.38 78.760.24 79.130.22 81.240.39 82.460.37 77.180.15 77.930.28 67.130.93 71.260. 59.570.32 63.910.39 68.930.94 71.670.80 56.340.70 59.970.57 Improvement +1.14 +0. +1.22 +0.75 +4.13 +4.34 +2.74 +3. MOS w/ Ours 79.160.60 85.080.79 74.070.36 77.030.43 77.250.20 83.520.68 71.500.20 76.540.37 66.171.06 70.911. 57.710.55 62.870.82 66.170.57 69.660.62 56.060.08 61.540.31 Improvement +5.92 +2. +6.27 +5.04 +4.74 +5.16 +3.49 +5. Evaluation Metrics. Since our work focuses on enhancing plasticity to improve CL performance, we employ two key metrics: one to measure plasticity and another to assess overall CL performance. Plasticity is quantified using the Average Optimal Accuracy (AOA), which is computed as the mean of the highest accuracy achieved for each task during the CL process. Overall CL Performance is evaluated using the Last Accuracy (LA), defined as the models average accuracy across all tasks after completing the final task. Formally, let denote the total number of tasks, Ab represent the classification accuracy for the b-th task after learning the final task, and denote the optimal classification accuracy achieved for the b-th task during the CL process. These metrics are defined as: AOA = 1 and LA = 1 b=1 b=1 Ab. (cid:80)K (cid:80)K"
        },
        {
            "title": "4.2 Main Results",
            "content": "Integration with Existing Methods. We begin by assessing ACLs effectiveness when incorporated into six state-of-the-art PTM-based CL methods. As shown in Tab. 1, ACL consistently improves the performance of these methods across diverse datasets and incremental steps. Notably, ACL achieves gains of up to 10.41% in AOA and 7.85% in LA compared to the original methods. These results demonstrate that ACL effectively enhances model plasticity, thereby improving the overall performance of existing PTM-based CL methods. Comparison with Aper. We further compare ACL with Aper, method that adapts the PTM using standard classification loss on the first task only. To ensure fair comparison, we integrate ACL with SimpleCIL [15], which uses the same prototypical classifier as Aper and involves no training after adaptation. As shown in Tab. 2, ACL consistently surpasses Aper across all datasets and incremental steps. Notably, ACL achieves performance gains of up to 4.64% in AOA and 5.14% in LA compared to the best-performing variants of Aper. These results demonstrate ACLs superiority in enhancing model plasticity, leading to better stability-plasticity balance than Aper."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "Key Components for Adaptation. We first conduct an ablation study to systematically investigate the impact of various strategies employed during the adaptation phase, focusing on three key aspects: loss design, adaptation steps, and adapted network components. As illustrated in Fig. 3, all ablation variants exhibit inferior performance compared to the original ACL framework across all integrated CL methods. These results underscore the critical importance of continually adapting the entire 7 Table 2: Performance comparison (%) between Aper and ACL. Bolded indicates the best, underline denotes the second best. Improvement represents the boost of ACL towards the best Aper variant. Method ImageNet-R-Inc20 ImageNet-R-Inc ImageNet-A-Inc20 ImageNet-A-Inc10 AOA LA AOA LA AOA LA AOA LA SimpleCIL 67.150. 61.350.00 67.460.13 61.350.00 58.510.45 49.240.00 58.910. 49.240.00 67.761.20 Aper w/ Finetune Aper w/ VPT-Deep 73.716.09 Aper w/ VPT-Shallow 70.320.77 74.860.58 Aper w/ SSF 72.951.32 Aper w/ Adapter 63.601.16 68.705.76 64.500.72 70.070.37 67.251.21 69.270.90 73.121.16 70.970.68 73.440.21 68.580.15 64.191.11 68.001.05 64.830.38 67.840.06 62.470.17 60.871.60 55.053.24 56.580.93 59.421.70 58.490. 51.741.91 46.113.25 46.901.46 50.241.47 49.220.03 59.992.40 52.033.79 56.061.80 57.751.64 58.930.24 50.442.34 42.154.09 45.611.84 47.931.57 49.230.07 ACL (Ours) Improvement 79.500. 73.930.38 77.490.78 72.260.43 64.760.52 56.880.31 63.490. 55.180.34 +4.64 +3.86 +4.05 +4.26 +3. +5.14 +3.50 +4.74 Figure 3: LA of original ACL and its ablation variants, including (1) replacing contrastive loss with standard classification loss for adaptation, (2) adapting the model for the first task only, and (3) adapting the lightweight modules only with frozen backbone. PTM backbone using the proposed contrastive loss for all incremental tasks, which facilitates more effective adaptation and knowledge retention. Full PTM Adaptation vs. Multi-Epoch Adaptation. To further delineate the advantages of adapting the entire PTM versus solely tuning lightweight modules, we extended our comparison by considering the impact of multiple adaptation epochs. The results, presented in Fig. 4, demonstrate two key findings: (1) adapting the entire PTM consistently outperforms tuning only lightweight modules, irrespective of the number of adaptation epochs used for the latter; (2) increasing adaptation epochs beyond two yields marginal or negligible performance gains for either strategy. These observations indicate that the performance benefits derived from full PTM adaptation cannot be replicated merely by increasing the adaptation epochs. This reinforces our claim that freezing pre-trained weights results in suboptimal balance between stability and plasticity. Contrastive Learning Paradigm for Adaptation. To validate the importance of negative pairs within the contrastive learning paradigm of ACL, we compared the original model against variant excluding these pairs during adaptation. The results, presented in Tab. 3, demonstrate that omitting negative pairs significantly lowers the CL performance. This finding highlights the critical contribution of contrastive learning paradigm proposed in Sec. 3.3. Table 3: The LA (%) of using original ACL and using its variant without negative pairs."
        },
        {
            "title": "Method",
            "content": "L2P"
        },
        {
            "title": "MOS",
            "content": "Original ACL (Ours) w/o negatives 48.071.17 33.042.13 51.450.51 34.944.25 41.280.61 33.720.14 64.450.37 59.150.67 63.910.39 58.530. 62.870.82 60.700."
        },
        {
            "title": "4.4 Visualization",
            "content": "We employ t-SNE [53] to visualize the feature representations extracted by the final PTMs, comparing those obtained without and with the proposed ACL framework. For simplicity, we adopt SimpleCIL [15] as the baseline method and evaluate on ImageNet-R-Inc20, selecting two classes per incremental task for clearer visualization. The results, shown in Fig. 5(a) and (b), demonstrate that the PTM adapted with ACL generates more discriminative feature representations than the frozen 8 (a) L2P (b) DualPrompt (c) Fecam (d) RanPAC (e) SSIAT (f) MOS Figure 4: LA with different adapting epochs on ImageNet-A-Inc20, the default setting is 1. model, even for classes in previously learned tasks. This indicates that ACL effectively enhances feature discriminability across all incremental tasks, achieving better stability-plasticity trade-off. To further validate our approach, we visualize Grad-CAM [54] results on samples with large domain gap [50] relative to the pre-training data, which highlight critical image regions for concept prediction. As depicted in Fig. 5(c), the frozen PTM often attends to irrelevant background regions. In contrast, the PTM adapted via ACL focuses more accurately on class-specific features. These findings underscore the necessity of adapting PTMs to incremental data, especially when the target distribution significantly diverges from the pre-training domain. (a) Frozen (b) w/ ACL (Ours) (c) Grad-CAM Figure 5: (a-b) Visualization of 2D feature representations using t-SNE. (c) Grad-CAM visualization, where important regions are highlighted with warm colors."
        },
        {
            "title": "4.5 Validation on Other Backbones",
            "content": "Validation on ViT-B/16-IN21K. To further validate the effectiveness of our proposed framework, we conduct additional experiments on ViT-B/16-IN21K, model pre-trained on ImageNet21K only. To simplify, we select the ImageNet-A-Inc20 as the benchmark. As shown in Tab. 4, ACL consistently enhances the CL performance across various CL methods. These findings underscore the versatility and generalizability of our framework. Validation on CLIP. While our study primarily focuses on visual models, the insights presented in our paper are potentially applicable to visual-language models, such as CLIP [31]. To demonstrate this, we employ Continual CLIP [55] as the baseline and evaluate ACL on the ImageNet-R-inc20 benchmark. Since the text labels for the same class are consistent, we only adapt the visual encoder 9 Table 4: The LA (%) using ViT-B/16-IN21K."
        },
        {
            "title": "Method",
            "content": "L2P"
        },
        {
            "title": "MOS",
            "content": "Original w/ ACL (Ours) 39.831.15 45.490.56 43.021.29 45.571.56 45.370.30 47.811.22 54.590.84 58.970.21 56.840.49 59.800. 54.170.45 58.550."
        },
        {
            "title": "Improvements",
            "content": "+5.66 +2.55 +2.44 +4.38 +2.96 +4. Table 5: The LA (%) using CLIP."
        },
        {
            "title": "Method",
            "content": "LA Continual CLIP w/ Ours 71.700.01 74.980.25 (+3.28) using ACL. The experimental results, summarized in Tab. 5, indicate that ACL significantly enhances the CL performance of CLIP. These findings demonstrate the potential of ACL to improve CL in the context of visual-language models."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we revisit CL with PTMs and argue that existing PTM-based CL methods overly prioritize stability at the expense of plasticity. To address this limitation, we propose ACL, framework that can be orthogonally integrated with existing PTM-based CL methods to enhance plasticity while simultaneously maintaining stability. Extensive experiments demonstrate the effectiveness of ACL in enhancing plasticity and achieving more balanced stability-plasticity trade-off. Future work will focus on exploring more effective or efficient adaptation algorithms within the ACL framework to further improve its performance and applicability. Limitations. Adapting the entire PTM introduces additional GPU memory consumption, specifically, approximately 7GB with the experiment settings in Sec. 4."
        },
        {
            "title": "References",
            "content": "[1] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. comprehensive survey of continual learning: Theory, method and application. arXiv preprint arXiv:2302.00487, 2023. [2] Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, and De-Chuan Zhan. Continual learning with pre-trained models: survey. arXiv preprint arXiv:2401.16386, 2024. [3] Michael McCloskey and Neal Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109165. Elsevier, 1989. [4] Ian Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013. [5] Stephen Grossberg. Adaptive resonance theory: How brain learns to consciously attend, learn, and recognize changing world. Neural networks, 37:147, 2013. [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [7] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021. [8] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 139149, 2022. 10 [9] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. In European Conference on Computer Vision, pages 631648. Springer, 2022. [10] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709727. Springer, 2022. [11] Yuwen Tan, Qinhao Zhou, Xiang Xiang, Ke Wang, Yuchuan Wu, and Yongbin Li. Semanticallyshifted incremental adapter-tuning is continual vitransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2325223262, 2024. [12] Hai-Long Sun, Da-Wei Zhou, Hanbin Zhao, Le Gan, De-Chuan Zhan, and Han-Jia Ye. Mos: Model surgery for pre-trained model-based class-incremental learning. arXiv preprint arXiv:2412.09441, 2024. [13] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: survey. IEEE transactions on pattern analysis and machine intelligence, 45(4):43964415, 2022. [14] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Finetuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2022. [15] Da-Wei Zhou, Zi-Wen Cai, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Revisiting classincremental learning with pre-trained models: Generalizability and adaptivity are all you need. International Journal of Computer Vision, pages 121, 2024. [16] Marc Masana, Xialei Liu, Bartłomiej Twardowski, Mikel Menta, Andrew Bagdanov, and Joost Van De Weijer. Class-incremental learning: survey and performance evaluation on image classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5):5513 5533, 2022. [17] Gido Van de Ven, Tinne Tuytelaars, and Andreas Tolias. Three types of incremental learning. Nature Machine Intelligence, 4(12):11851197, 2022. [18] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. Advances in neural information processing systems, 32, 2019. [19] Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and Qianru Sun. Mnemonics training: Multiclass incremental learning without forgetting. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 1224512254, 2020. [20] Ahmet Iscen, Jeffrey Zhang, Svetlana Lazebnik, and Cordelia Schmid. Memory-efficient In Computer VisionECCV 2020: 16th incremental learning through feature adaptation. European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVI 16, pages 699715. Springer, 2020. [21] Hanbin Zhao, Hui Wang, Yongjian Fu, Fei Wu, and Xi Li. Memory-efficient class-incremental learning for image classification. IEEE Transactions on Neural Networks and Learning Systems, 33(10):59665977, 2021. [22] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):35213526, 2017. [23] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International conference on machine learning, pages 39873995. PMLR, 2017. [24] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):29352947, 2017. 11 [25] Tao Feng, Mang Wang, and Hangjie Yuan. Overcoming catastrophic forgetting in incremental object detection via elastic response distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94279436, 2022. [26] Haeyong Kang, Rusty John Lloyd Mina, Sultan Rizky Hikmawan Madjid, Jaehong Yoon, Mark Hasegawa-Johnson, Sung Ju Hwang, and Chang Yoo. Forget-free continual learning with winning subnetworks. In International Conference on Machine Learning, pages 1073410750. PMLR, 2022. [27] Tatsuya Konishi, Mori Kurokawa, Chihiro Ono, Zixuan Ke, Gyuhak Kim, and Bing Liu. Parameter-level soft-masking for continual learning. In International Conference on Machine Learning, pages 1749217505. PMLR, 2023. [28] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 30143023, 2021. [29] Da-Wei Zhou, Qi-Wei Wang, Han-Jia Ye, and De-Chuan Zhan. model or 603 exemplars: Towards memory-efficient class-incremental learning. In ICLR, 2023. [30] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [32] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1190911919, 2023. [33] Dahuin Jung, Dongyoon Han, Jihwan Bang, and Hwanjun Song. Generating instance-level prompts for rehearsal-free continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1184711857, 2023. [34] Xinyuan Gao, Songlin Dong, Yuhang He, Qiang Wang, and Yihong Gong. Beyond prompt learning: Continual adapter for efficient rehearsal-free continual learning. In European Conference on Computer Vision, pages 89106. Springer, 2024. [35] Dipam Goswami, Yuyang Liu, Bartłomiej Twardowski, and Joost van de Weijer. Fecam: Exploiting the heterogeneity of class distributions in exemplar-free continual learning. Advances in Neural Information Processing Systems, 36, 2024. [36] Mark McDonnell, Dong Gong, Amin Parvaneh, Ehsan Abbasnejad, and Anton van den Hengel. Ranpac: Random projections and pre-trained models for continual learning. Advances in Neural Information Processing Systems, 36, 2024. [37] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017. [38] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. arXiv preprint arXiv:2005.04966, 2020. [39] Xikun Zhang, Dongjin Song, and Dacheng Tao. Hierarchical prototype networks for continual graph representation learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):46224636, 2022. [40] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 12 [41] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:1866118673, 2020. [42] Kotaro Nagata and Kazuhiro Hotta. Margin contrastive learning with learnable-vector for continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 35703576, 2023. [43] Yichen Wen, Zhiquan Tan, Kaipeng Zheng, Chuanlong Xie, and Weiran Huang. Provable contrastive continual learning. arXiv preprint arXiv:2405.18756, 2024. [44] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In Proceedings of the IEEE/CVF International conference on computer vision, pages 95169525, 2021. [45] Huiwei Lin, Baoquan Zhang, Shanshan Feng, Xutao Li, and Yunming Ye. Pcr: Proxy-based In Proceedings of the contrastive replay for online class-incremental continual learning. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2424624255, 2023. [46] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. Learning without memorizing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 51385146, 2019. [47] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. [48] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. [49] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International conference on machine learning, pages 99299939. PMLR, 2020. [50] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: In Proceedings of the IEEE/CVF critical analysis of out-of-distribution generalization. international conference on computer vision, pages 83408349, 2021. [51] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1526215271, 2021. [52] Hai-Long Sun, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Pilot: pre-trained modelbased continual learning toolbox. arXiv preprint arXiv:2309.07117, 2023. [53] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. [54] Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618626, 2017. [55] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an efficient continual learner. arXiv:2210.03114, 2022. [56] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [57] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 13 [58] Yuanhan Zhang, Zhenfei Yin, Jing Shao, and Ziwei Liu. Benchmarking omni-vision representation through the lens of visual realms. In European Conference on Computer Vision, pages 594611. Springer, 2022. [59] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019. [60] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019. [61] Aojun Lu, Tao Feng, Hangjie Yuan, Xiaotian Song, and Yanan Sun. Revisiting neural networks for continual learning: An architectural perspective. In IJCAI, pages 46514659, 2024."
        },
        {
            "title": "A Training Details",
            "content": "Training Settings. In the adaptation phase, we use learning rate of 1e-6 for prompt-based methods (i.e., L2P and DualPrompt) and 1e-4 for all other methods. Following conventions [8], all models are trained with batch size of 128. For all datasets, the data split and preprocessing follow open-source library PILOT [52]. Hardware Configuration. All experiments are conducted on RTX 3090 GPUs, with each experiment accommodated within single 24GB GPU."
        },
        {
            "title": "B Benchmark Selection Principle",
            "content": "A primary focus of this research is on CL scenarios characterized by significant domain gap between the pre-trained dataset (e.g., ImageNet-1K/21K) and the downstream tasks. Such scenarios are prevalent in real-world applications and pose substantial challenges to model plasticity. Datasets like ImageNet-R and ImageNet-A exemplify these large domain gaps. Conversely, some datasets utilized in prior CL research exhibit limited data distribution gap with the pre-training data, which does not align with our targeted challenging scenarios. To illustrate this distinction, Tab. 6 presents the performance of ViT-B/16-IN21K on several common benchmarks using SimpleCIL (without any training), with data sourced from [15]. Notably, datasets such as CIFAR100 [56] (81.26%), CUB [57] (86.73%), OmniBench [58] (73.15%), and VTAB [59] (84.38%) show high accuracy, indicating relatively small distribution shift from ImageNet. In contrast, ObjectNet [60] (53.59%), ImageNet-R (54.55%), and ImageNet-A (49.44%) demonstrate lower performance, confirming their larger domain divergence and suitability for evaluating model robustness and plasticity. Table 6: Performance of ViT-B/16-IN21K on multiple datasets with SimpleCIL. All results are sourced from [15]. Dataset CIFAR100 CUB OmniBench VTAB ObjectNet ImageNet-R ImageNet-A LA (%) 81.26 86.73 73.15 84.38 53.59 54. 49."
        },
        {
            "title": "C Validation on ObjectNet",
            "content": "Given its substantial domain gap with ImageNet, as evidenced in Tab. 6, ObjectNet is also relevant dataset for validating our approach. We conducted experiments on ObjectNet-inc20, adhering to the data preprocessing protocols from [15]. Tab. 7 details the LA (%) on ObjectNet, comparing the original performance of several CL methods against their performance when augmented with our ACL framework (denoted as \"w/ Ours\"). The results indicate that ACL generally enhances CL performance across most methods in this challenging, large-domain-gap setting. Table 7: The LA (%) on ObjectNet."
        },
        {
            "title": "Method",
            "content": "Original w/ Ours L2P DualP."
        },
        {
            "title": "MOS",
            "content": "55.910.33 58.390.51 53.990.30 57.070.42 54.380.57 56.570.58 63.790.12 64.920.29 64.630.28 65.220.26 62.750.30 60.061. Impact of Temperature Settings. We investigated the impact of temperature settings on the performance of our proposed ACL framework when integrated with existing CL methods. The results, presented in Tab. 8 and Tab. 9, demonstrate that while temperature influences CL performance, the application of ACL consistently benefits these baseline methods across wide spectrum of temperature values. 15 Table 8: The LA (%) across temperature (T) settings for CL methods using cosine classifier."
        },
        {
            "title": "Method without ACL",
            "content": "T=0.02 T=0.05 T=0.1 (current) T=0.2 T=0."
        },
        {
            "title": "Fecam\nRanpac\nSSIAT\nMOS",
            "content": "33.430.18 58.160.46 59.570.32 57.710.55 45.571.65 62.871.06 63.000.28 60.981.15 41.410.65 63.740.46 63.860.49 62.710.52 41.280.61 64.450.37 63.910.39 62.870.82 39.500.36 64.260.57 63.500.41 62.980.80 36.390.32 62.040.39 62.580.17 61.250. Table 9: The LA (%) across temperature (T) settings for CL methods using linear classifier."
        },
        {
            "title": "Method",
            "content": "without ACL T=0.4 T=1 T=2 (current) T=4 T= L2P DualPrompt 42.580.39 45.351.04 47.511.34 50.590.66 47.691.42 50.920.58 48.071.17 51.450.51 48.441.09 52.980. 46.701.10 52.590."
        },
        {
            "title": "E Additional Results",
            "content": "E.1 Main Results on AIA We supplement the results in Sec. 4.2 with the Average Incremental Accuracy (AIA), presented in Tab. 10 and Tab 11. AIA measures the models average performance after each incremental task [61]. Formally, let denote the total number of tasks, and let ab represent the classification accuracy evaluated on the test set of all learned tasks after training on the b-th task. AIA is computed as AIA = 1 b=1 ab. (cid:80)K Table 10: The AIA (%) using six state-of-the-art CL methods. Improvement represents the boost of ACL towards original methods. ImageNet-R-Inc ImageNet-R-Inc10 ImageNet-A-Inc20 ImageNet-A-Inc"
        },
        {
            "title": "Method",
            "content": "L2P w/ Ours 76.760.45 78.990."
        },
        {
            "title": "Improvement",
            "content": "+2.23 DualPrompt w/ Ours 74.850.18 78.990."
        },
        {
            "title": "Improvement",
            "content": "+4.14 FeCAM w/ Ours 66.151.24 70.330."
        },
        {
            "title": "Improvement",
            "content": "+4.18 RanPAC w/ Ours 81.180.94 83.290."
        },
        {
            "title": "Improvement",
            "content": "+2.11 SSIAT w/ Ours 81.640.34 82.800."
        },
        {
            "title": "Improvement",
            "content": "+1.16 MOS w/ Ours 78.840.43 81.680."
        },
        {
            "title": "Improvement",
            "content": "+2.84 50.421.12 54.711.38 +4.29 54.721.64 58.361.78 +3.64 41.890.95 46.671. +4.78 66.731.47 70.591.93 +3.86 66.541.36 69.841.42 +3.30 65.841.00 68.911. +3.07 44.241.25 48.621.63 +4.38 49.462.26 53.272.43 +3.81 42.960.65 45.560. +2.60 65.791.55 66.223.48 +0.43 65.621.43 68.141.28 +2.52 65.711.02 68.501. +2.79 74.610.61 76.540.42 +1.93 71.890.34 76.210.43 +4.32 61.980.42 67.961. +5.98 78.470.55 82.370.34 +3.90 80.040.34 81.680.27 +1.64 76.920.22 80.970. +4.05 16 Table 11: Performance comparison (%AIA) between Aper and ACL. Bolded indicates the best, underline denotes the second best. Improvement represents the boost of ACL towards the best Aper variant."
        },
        {
            "title": "SimpleCIL",
            "content": "Aper w/ Finetune Aper w/ VPT-Deep Aper w/ VPT-Shallow Aper w/ SSF Aper w/ Adapter ACL (Ours)"
        },
        {
            "title": "Improvement",
            "content": "ImageNet-R-Inc20 ImageNet-R-Inc10 ImageNet-A-Inc20 ImageNet-A-Inc10 66.970.46 71.770.91 75.086.13 70.210.91 76.290.80 73.131. 77.900.57 +1.61 67.580.47 71.541.02 74.711.34 71.200.71 74.310.44 68.700.66 76.330.46 +1. 58.351.16 60.651.94 56.033.22 56.420.83 59.650.94 58.371.17 63.501.85 +2.85 59.331.01 60.712.09 53.014.91 56.552.36 58.591.16 59.341. 62.641.53 +1."
        }
    ],
    "affiliations": [
        "College of Computer Science Sichuan University Chengdu, China",
        "College of Computer Science and Technology Zhejiang University Hangzhou, China",
        "Department of Computer Science and Technology Tsinghua University Beijing, China"
    ]
}