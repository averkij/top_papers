{
    "paper_title": "BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback",
    "authors": [
        "Hyunseo Kim",
        "Sangam Lee",
        "Kwangwook Seo",
        "Dongha Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Search-augmented large language models (LLMs) have advanced information-seeking tasks by integrating retrieval into generation, reducing users' cognitive burden compared to traditional search systems. Yet they remain insufficient for fully addressing diverse user needs, which requires recognizing how the same query can reflect different intents across users and delivering information in preferred forms. While recent systems such as ChatGPT and Gemini attempt personalization by leveraging user histories, systematic evaluation of such personalization is under-explored. To address this gap, we propose BESPOKE, the realistic benchmark for evaluating personalization in search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting authentic chat and search histories directly from humans, and diagnostic, by pairing responses with fine-grained preference scores and feedback. The benchmark is constructed through long-term, deeply engaged human annotation, where human annotators contributed their own histories, authored queries with detailed information needs, and evaluated responses with scores and diagnostic feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key requirements for effective personalization in information-seeking tasks, providing a foundation for fine-grained evaluation of personalized search-augmented LLMs. Our code and data are available at https://augustinlib.github.io/BESPOKE/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 6 0 1 1 2 . 9 0 5 2 : r Preprint. BESPOKE: BENCHMARK FOR SEARCH-AUGMENTED LARGE LANGUAGE MODEL PERSONALIZATION VIA DIAGNOSTIC FEEDBACK Hyunseo Kim, Sangam Lee, Kwangwook Seo, Dongha Lee Yonsei University {hyunseo00, salee, tommy2130, donalee}@yonsei.ac.kr"
        },
        {
            "title": "ABSTRACT",
            "content": "Search-augmented large language models (LLMs) have advanced informationseeking tasks by integrating retrieval into generation, reducing users cognitive burden compared to traditional search systems. Yet they remain insufficient for fully addressing diverse user needs, which requires recognizing how the same query can reflect different intents across users and delivering information in preferred forms. While recent systems such as ChatGPT and Gemini attempt personalization by leveraging user histories, systematic evaluation of such personalization is under-explored. To address this gap, we propose BESPOKE, the realistic benchmark for evaluating personalization in search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting authentic chat and search histories directly from humans, and diagnostic, by pairing responses with fine-grained preference scores and feedback. The benchmark is constructed through longterm, deeply engaged human annotation, where human annotators contributed their own histories, authored queries with detailed information needs, and evaluated responses with scores and diagnostic feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key requirements for effective personalization in information-seeking tasks, providing foundation for fine-grained evaluation of personalized search-augmented LLMs. Our code and data are available at https://augustinlib.github.io/BESPOKE/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Information-seeking tasks aim to address users information need by providing the desired information in an appropriate form. Recently, search-augmented LLMs have driven fundamental shift in information-seeking tasks, leveraging the retrieval-augmented generation (RAG). Unlike traditional search systems that simply list relevant content and require users to read, compare, and synthesize information themselves (Ji et al., 2024; Juneja et al., 2024), RAG reduces cognitive burden by integrating information into responses, thereby more effectively addressing users information needs. Nevertheless, these advances remain insufficient for fully addressing users information needs. To genuinely achieve this, system should accurately identify how the query can reflect different needs across users by considering their backgrounds (Salemi & Zamani, 2025a), and ensure that the information is delivered in form that matches their preferences (Kumar et al., 2024; Salemi et al., 2025). As shown in figure 1, although both users issue the same query, their different backgrounds lead one to focus on environmental implication while the other emphasizes performance. Moreover, they prefer different forms of information delivery, with one favoring narrative explanation and the other concise bullet summary. Since RAG cannot fully satisfy these requirements alone, there is growing need for systems that can adapt to diverse user intents and presentation preferences. In response to these needs, recent search-augmented LLMs such as ChatGPT (OpenAI, 2023) and Gemini (Gemini-Team, 2025) have moved beyond generic outputs by leveraging users prior chat Both authors contributed equally to this research. Corresponding author. 1 Preprint. Figure 1: Personalization in search-augmented LLM. It infers user-specific needs from history, conducts tailored search, and adapts the delivery of information to align with user preferences. and search histories as user contexts to personalize their responses. This shift enables them to better reflect each users distinct preferences, such as topic interests, informational needs, and delivery style. However, despite this advancement, systematic evaluation of these systems to diagnose limitations and guide further development remains largely underexplored. In this paper, we introduce BESPOKE, realistic benchmark specifically designed for evaluating personalization in search-augmented LLMs. BESPOKE is built from 2,870 real user history sessions collected over three weeks, where human annotators with diverse backgrounds engaged in natural conversations and web searches as part of their daily routines. Grounded in these fully human-annotated histories, BESPOKE additionally provides 150 user-annotated queries and their corresponding gold information needs, which explicitly outline the personalized requirements for each query. Unlike existing personalization benchmarks for information-seeking, such as LaMPQA (Salemi & Zamani, 2025a), which remain limited to QA-style interactions in constrained domains, BESPOKE encompasses broad spectrum of user activities on the web, including free-form chats that extend beyond information seeking and actual web-search histories. In addition to providing gold information for each query, BESPOKE offers response-judgement pairs with human-annotated scores and explanatory feedback, explicitly clarifying why response is deemed satisfactory or unsatisfactory. Based on these, we propose an evaluation framework that effectively assesses personalization in information-seeking tasks. By leveraging human-annotated feedback as additional context, our evaluation framework achieves better alignment with human judgment. Moreover, it not only assesses whether responses factually and accurately incorporate relevant information but also evaluates whether personalization is well achieved, delivering both scores and diagnostic feedback. This provides detailed diagnosis, which serves as supervision for developing personalized systems (Balepur et al., 2025; Salemi & Zamani, 2025b). With BESPOKE, we provide an extensive analysis of personalization in search-augmented LLMs, offering insights into how effective personalization can be achieved in information-seeking. Our results show that personalization is strongly influenced by how user contexts are constructed from user histories, while many models still fall short in delivering personalized responses. These findings highlight BESPOKE as challenging benchmark and emphasize the need for more effective methods. The main contributions of our work are summarized as follows: We propose BESPOKE, the benchmark that genuinely reflects real-world personalization scenarios in search-augmented LLMs through long-term, deeply engaged human annotation. BESPOKE provides an evaluation framework that offers diagnostic feedback under fine-grained criteria, enabling specific evaluation of personalization and guiding system improvements. Leveraging BESPOKE, we provide systematic analyses that highlight essential requirements for achieving effective personalization in information-seeking tasks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Search-augmented LLM evaluation. Search-augmented LLMs, which leverage RAG, have recently driven fundamental shift in information-seeking, enabling more factual and robust outputs (Gao et al., 2023b; Huang & Huang, 2024). To evaluate such systems, several methods (Ru 2 Preprint. Table 1: Comparison with existing information-seeking benchmarks. Dataset Personalization RAG-QA Arena (Han et al., 2024) Search-Arena (Miroyan et al., 2025) LaMP-QA (Salemi & Zamani, 2025a) BESPOKE (ours) Unconstrained Domain History Type QA-pair Chat + Search history Preference Annotation Chosen/Reject Chosen/Reject Score+Feedback Gold Info. Annotation et al., 2024; Park et al., 2025a) have been proposed, providing controlled settings that diagnose generation errors and test robustness to noisy contexts. However, these frameworks primarily rely on simple fact-based QA queries and consequently assess RAG systems mainly from the perspective of factuality. Another recent effort, RAG-QA Arena (Han et al., 2024), evaluates long-form QA by pairwise preference judgments, but the evaluation scope remains limited to specialized domains. Search Arena (Miroyan et al., 2025) broadens the scope to open-web sources and diverse query types, reflecting the growing adoption of open-setting evaluation as the standard paradigm for assessing information-seeking tasks (Gou et al., 2025; Kasai et al., 2024). Similar to RAG-QA Arena, it introduces pairwise evaluation framework that collects head-to-head human preference judgments between system outputs, allowing for comparative analysis of search-augmented LLMs under realistic search tasks. While Search Arena provides strong foundation, it remains limited to general preference judgments without incorporating personalized aspects of evaluation. Personalized LLM evaluation. Recent work on personalized LLM evaluation has investigated whether models can adapt outputs to user attributes or preferences. Early benchmarks (Salemi et al., 2023; Kumar et al., 2024) use synthetic personas, and LaMP-QA (Salemi & Zamani, 2025a) extends this line to information-seeking tasks by constructing dataset from StackExchange, pairing queries with user profiles from past questions. However, relying on platforms such as StackExchange constrains the dataset domain and restricts user histories to QA-style interactions. Taken together, these studies highlight two essential components for personalized evaluation: leveraging realistic user histories to represent preferences and reasoning over histories to infer information needs."
        },
        {
            "title": "3 CONSTRUCTING BESPOKE",
            "content": "We introduce BESPOKE, realistic and diagnostic benchmark that evaluates personalization in search-augmented LLMs with diagnostic feedback. To collect sufficient user histories and detailed feedback, we employ long-term, deeply engaged human annotation. Over 3 weeks, annotators freely engage in diverse activities like informationseeking and chatting, accumulating their own chat and websearch histories. Then they issue queries grounded in the information needs arising from these histories and provide preference scores and feedback on the sampled responses generated for the queries by search-augmented LLMs. The overall pipeline is illustrated in Figure 2. Table 2: Statistics of BESPOKE. The diversity index uses Shannons equitability (Appendix B.1), with values near 1 indicating higher user diversity. Statistics #Users (Div. index) #Sessions Search session Chat session #Avg. sesssion / user Value 30 (0.91) 2,870 2,153 717 95.67 3.1 TASK FORMULATION For user issuing query q, we define user history as Hu = {Su, Cu}, consisting of their search history Su and chat history Cu. search-augmented LLM begins by inferring the users information need nq implied by either explicitly or implicitly through their history Hu. Guided by the inferred nq, the model then searches for relevant information. By using both searched information and Hu, it subsequently generates response to address nq in manner that aligns closely with the users judgments. Under this task formulation, BESPOKE evaluates how accurately conveys the information the user seeks through and how well it is personalized to the user. 3.2 HUMAN ANNOTATOR & HISTORY COLLECTION Human annotator recruitment. The first step of constructing BESPOKE is to recruit human annotators. Since personalized responses depend on the users unique characteristics, it is essential that Preprint. Figure 2: Overview of our BESPOKE construction process. the benchmark reflects broad range of real-world user characteristics to meaningfully evaluate models ability to adapt across diverse user contexts. To achieve this, we recruit 30 human annotators with diverse backgrounds across professions and interests, ensuring these backgrounds cover wide range of interaction patterns and contexts. More details are provided in Appendix B.1. History collection. Recent systems such as Gemini (Gemini-Team, 2025) have demonstrated the value of leveraging both chat and search histories to personalize search-augmented LLM. Motivated by this development, BESPOKE incorporates both search histories Su and chat histories Cu as complementary sources of user context. To obtain these user histories, each annotator is assigned dedicated Google account created solely for this study. During the history collection period, they freely use this account in their daily lives for information seeking, conducting Google searches, and conversing with Gemini (Gemini-Team, 2025) according to their own interests and routines. We set the collection period to three weeks to ensure that enough interaction data is accumulated to reflect implicit user preferences. At the end of the history collection period, we collect the resulting histories after carefully removing all personally identifiable or sensitive information. For details on data handling and privacy protection, please refer to the Appendix B.2. 3.3 HUMAN ANNOTATION PIPELINE After collecting users search and chat histories, we employ multi-stage annotation process aimed at constructing user queries and obtaining fine-grained feedback on the model responses, encompassing both preference scores and feedback. This process consists of three stages: 1) authoring simple query together with its gold information need, 2) annotating multiple responses with both numerical scores and verbalized feedback, and 3) manually generating gold response. For more details about the human annotation pipeline, please refer to the Appendix B.3. Task types. Among the intent categories proposed in Search-Arena (Miroyan et al., 2025), we select four tasks: analysis, guidance, recommendation, and explanation, as the task types in BESPOKE. These tasks represent information-seeking scenarios where personalization is essential since the same information may need to be delivered in different tone or style depending on the user. Query & Gold information need. Each annotator is asked to issue five test queries, covering all four task types. For each of the five test queries, annotators are asked to provide two annotations: simple query and corresponding gold information need n+ . The simple query reflects how the user would ask in daily life, assuming that the system will infer the underlying information need from prior interaction history. In contrast, the gold information need is written in more detailed form than the simple query, explicitly stating their background, intent, and the specific aspects to be addressed, ensuring that the desired information is clearly specified. Response-Judgment pair (R-J pair). For each query q, we sample sets of response r, which the same annotator then assigns five-point Likert score with diagnostic feedback over 4 criteria: 4 Preprint. Need alignment: Assesses how well the response captures the users information need, focusing on whether it directly addresses the desired content while avoiding irrelevant information. Content depth: Evaluates the level of detail and complexity in the response. It assesses whether the explanation is thorough, concise, or advanced, given the users expectations and the query. Tone: Measures how well the responses tone (e.g., formal, casual) matches the users judgments. Explanation style: Examines whether the structure, flow, and illustrative methods, such as stepby-step guidance or examples, align with the users preferred way of understanding information. The judgment (s, ) captures user judgments and provides detailed assessment of personalization quality, highlighting which aspects of response align with or deviate from the users expectations. For each query q, this results in set of responsejudgment tuples (r, s, ). To obtain such diverse assessments, we adopt two-stage process for collecting judgments from each annotator. In the first stage, the goal is to capture spectrum of user judgments by having annotators evaluate multiple candidate responses for each query q. Specifically, for each query q, we construct set of information needs, consisting of the gold need n+ and 1 additional hypothetical needs plausibly inferred from using Gemini-2.5-Pro (Gemini-Team, 2025). For each of these information needs, Gemini-2.5-Pro-Grounding is used to produce response grounded on Google Search. Annotators then evaluate all responses along the four criteria, resulting in R-J pairs. In the second stage, the goal is to construct high-quality gold response r+. Annotators first search for passages relevant to n+ using Google Search, then prompt Gemini-2.5-Pro grounded on those passages to draft response. They iteratively provide scores and feedback on the four criteria, and Gemini refines the draft accordingly. This process continues until the output fully satisfies all four criteria with the maximum score of 5, which defines the gold response r+. During this refinement process, additional R-J pairs are also obtained as byproducts of the iterative updates. 3.4 EVALUATION Building upon the human-annotated data described in Section 3.3, our evaluation framework employs LLMs to evaluate generated responses following Zheng et al. (2023); Liu et al. (2023). In particular, we adopt GPT-5 as the backbone to mitigate self-preference bias using model distinct from Gemini, which is used in the human annotation process (Panickssery et al., 2024). The generated responses are evaluated along two axes: factual coverage and personalization quality. For factual coverage, we measure how well response reflects the gold information + at the level of atomic claims. Specifically, from each r+, we first use GPT-5 to extract atomic claims as candidate gold information set. We then manually inspect these candidates and retain only those verifiable q,1, . . . , i+ via web search, which collectively constitute the gold information set + q,n}. Then, given model response ˆr, GPT-5-based evaluator Er checks each i+ via binary classification to determine whether it is correctly expressed in ˆr without contradiction. Let Iˆr denote the set of gold claims judged as present in ˆr. Recall is then computed as Recall(ˆr) = Iˆr/I + , which directly measures whether gold information is conveyed in the response. This formulation assesses whether the gold information is accurately presented in the response, making it suitable for open-web settings where information may appear redundant or include irrelevant content. = {i+ q,j + For personalization quality, we evaluate each response along four criteria described in Section 3.3: need alignment, content depth, tone, and explanation style. Specifically, GPT-5-based evaluator Ep operates in few-shot setting to evaluate new response ˆr for given query q. For each query q, we first construct query-specific demonstration examples Dq = {(ri, si, fi)}m i=1 based on the R-J pairs collected in Section 3.3, where each tuple consists of response ri, its scalar score si, and diagnostic feedback fi for specific criterion. From these examples in Dq, we then leverage GPT-5 to generate query-specific gold rubric R+ , which provides personalized evaluation guidelines scaled from 1 to 5 points per criterion. Finally, the evaluator Ep incorporates Dq, R+ , and the gold information need n+ to produce scalar score and diagnostic feedback for each criterion on the new response ˆr: (s, ) = E(Dq, R+ , q, ˆr). For more details, please refer to the Appendix C.1. , n+ 5 Preprint."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Experimental settings. We evaluate 6 representative search-augmented LLMs: GPT-4o-search, o3search, Gemini-2.5-Flash, Gemini-2.5-Pro, Perplexity-sonar and Perplexity-sonar-reasoning. They are evaluated along two axes, factuality and personalization, using the metrics introduced in Section 3.4. For factuality, recall is assessed by Er as the proportion of gold information correctly expressed in response. For personalization, the Ep assesses responses across four criteria mentioned in Section 3.3. Personalization scores are reported on 0100 scale by multiplying raw 15 Likert ratings by 20, following Zhu et al. (2025). For more details, please refer to Appendix C.1. 4.1 META EVALUATION Before the main experiments, we validate our LLM-based evaluator Ep through meta-evaluation, confirming that its judgments align with those of human annotators. Dataset and Baselines. We construct metaeval dataset composed of 300 RJ pairs that are not used in the evaluators demonstration shots, serving as reference against human judgments. Specifically, for each of the 30 users, we sample two RJ pairs per query. We compare our evaluator Ep (w/ Feedback) against commonly used LLM-based evaluator baselines: generic one without personalization (w/o Personal.) and one that provides scores only without feedback as context (w/o Feedback). Table 3: Meta evaluation result. Pearson / Spearman correlation shows the agreement with human preference judgments, and Feedback Acc. represents the alignment with human feedback. More details are provided in Appendix C.2. Ep Type Criterion Pearson Corr. Spearman Corr. Feedback Acc. w/o Personal. Need Align. Content depth Tone Style Avg. 0.374 0.356 0.639 0.474 0.461 0.373 0.385 0.625 0.485 0.467 0.160 0.270 0.677 0.300 0.352 w/o Feedback 0.844 0.814 0.874 0.689 0.805 0.803 0.767 0.827 0.727 0. 0.849 0.825 0.852 0.712 0.810 Need Align. Content depth Tone Style Avg. Evaluation process. With the meta-evaluation dataset, we validate whether our Ep reliably reproduces human judgments for our four personalization criteria on the generated responses. Concretely, we assess the evaluator along two dimensions: whether it assigns scores consistent with human judgments (score alignment) and whether it produces feedback semantically similar to human feedback (feedback alignment). For score alignment, we compute the Pearson correlation and Spearman correlation between the evaluators scores and the human annotators scores on the same responses. Feedback alignment is then assessed by comparing annotator-written feedback with evaluator-generated feedback for the same response using an LLM-as-a-judge metaevaluator. Specifically, using GPT-5 as meta-evaluator, we determine if the two feedbacks convey the same content given the context (q, n+ ). The meta-evaluator then outputs binary label indicating feedback equivalence, and accuracy is defined as the proportion of cases judged equivalent. Need Align. Content depth Tone Style Avg. 0.882 0.855 0.893 0.797 0.857 0.850 0.870 0.903 0.857 0. 0.881 0.845 0.900 0.784 0.853 w/ Feedback (ours) , R+ Evaluation results. As presented in Table 3, evaluator baselines without personalization yield low correlations with human judgments. In contrast, our evaluator Ep demonstrates strong agreement with human judgments, clearly confirming its ability to assess responses in manner highly similar to humans. This suggests that personalized evaluation requires more than generic evaluator and instead calls for framework designed specifically for personalization. Notably, incorporating diagnostic feedback into the evaluators context yields superior alignment with human judgments compared to baselines that rely solely on scores, underscoring the critical role of human-annotated feedback in enhancing personalization evaluators. Similarly, for feedback alignment, our approach shows strong alignment with human feedback, consistently outperforming the baselines across all criteria. These results suggest that the human-annotated feedback provided by BESPOKE makes significant contribution to building an effective personalization evaluator. 4.2 EFFECT OF USER CONTEXT We investigate whether leveraging user history helps personalization in search-augmented LLMs and how user context can be most effectively constructed from it. To this end, we vary four aspects of user context construction. 1) History usage determines whether the search-augmented LLM is 6 Preprint. Table 4: Evaluation of search-augmented LLMs under different configurations. The best and secondbest results are shown in bold and underline. Experimental details are provided in Appendix C.3. Model pplx-sonar pplx-sonar -reasoning Gemini-2.5 flash-grounding Gemini-2.5 pro-grounding gpt-4o-search o3-search History Usage Query Awareness History Selection User Context Profile Profile Raw Profile Profile Profile Raw Profile Profile Profile Raw Profile Profile Profile Raw Profile Profile Profile Raw Profile Profile Profile Raw Profile Need Align. Content Depth Tone Style Recall Avg. 45.20 46.00 52.53 47.33 55. 46.53 47.87 49.87 46.40 54.27 47.87 49.73 52.93 49.63 55.73 47.60 49.07 54.83 51.60 56.40 44.23 44.17 50.27 45.97 53.80 51.60 54.53 55.73 55.40 59.07 47.47 48.00 56.93 50.20 59. 50.27 51.07 54.80 50.67 57.47 50.80 52.60 57.60 52.13 61.03 50.40 54.67 60.33 51.87 60.27 46.43 46.77 53.07 48.07 57.20 57.47 60.53 61.87 60.60 63.73 80.53 80.13 84.67 79.73 85. 75.87 81.20 80.00 76.00 83.33 79.47 78.30 82.67 77.20 82.83 74.67 80.40 83.33 79.73 84.40 79.67 81.33 84.80 81.00 84.83 78.53 75.87 84.80 78.27 85.20 56.53 62.80 71.47 60.53 72. 62.53 65.47 70.00 61.60 70.67 62.27 66.33 70.27 62.13 71.73 62.13 68.40 73.00 65.60 72.40 59.33 62.37 68.93 61.37 69.93 70.00 71.60 74.80 71.40 73.87 9.87 11.89 23.25 11.23 25. 11.11 14.34 21.92 11.58 23.93 15.42 17.25 27.14 16.08 28.09 12.83 16.67 25.41 15.00 25.32 8.13 9.23 16.24 9.17 19.23 22.05 25.73 28.61 23.88 30.53 47.92 49.76 57.77 49.79 59. 49.26 51.99 55.32 49.25 57.93 51.16 52.84 58.12 51.43 59.88 49.53 53.84 59.38 52.76 59.76 47.56 48.77 54.66 49.11 57.00 55.93 57.65 61.16 57.91 62.48 provided with user context constructed from user history or without it. 2) Query awareness specifies whether the context is fixed as static profile shared across all queries or dynamically constructed for each query. 3) History selection determines whether to use the entire history or only query-relevant histories, with relevance assessed by an LLM through pairwise evaluation of the query against each history session. (Weller et al., 2025). We adapt this approach for our experiments, given the inherent limitations of bi-encoders discussed in Section 4.4. 4) User context form refers to how user contexts are presented to the model, either as raw histories or as profiles structuring user preferences. Table 4 shows our experimental results. Overall, leveraging user history consistently improves personalization performance across all models, though the extent of improvement varies by criterion. Inferring user needs and delivering appropriate information remain challenging. For need alignment, content depth, and recall, the baseline performance is considerably lower. This highlights the difficulty of inferring users information need from query and searching information that adequately addresses it. Notably, recall remains substantially lower than the other two dimensions, suggesting that providing users with precise and detailed information remains highly challenging. However, these three dimensions show clear improvements once user contexts are incorporated, suggesting that leveraging user history enables search-augmented LLMs to better infer users information need and deliver detailed information at level appropriate to their knowledge. Personalization enables finer alignment of tone and style with user preferences. In particular, models already achieve relatively high scores for tone and style even without personalization. This is supported by our analysis based on the feedback presented in Section 4.3, indicating that users generally prefer the neutral and concise tone and style of default LLM outputs in information-seeking. Nevertheless, personalization for tone and style remains necessary, as leveraging user history yields substantial gains and enables models to more precisely match user preferences. Query-aware user contexts improve personalization. Performance also varies substantially depending on how user histories are leveraged to construct the user context. In particular, it is more effective to build contexts in query-aware manner ( in query awareness) rather than relying 7 Preprint. Figure 3: Comparison of GPT-4o-search responses in the lowest/highest performance setting, illustrating how feedback captures difference in personalization quality. More examples in Appendix F. on static user context ( in query awareness). This improvement occurs since single user may prioritize different personalization directions depending on the query topic, such as emphasizing professional background in work-related queries but personal interests in leisure-related ones. Selective history selection boosts context relevance. Furthermore, within query-aware approaches, selectively choosing relevant portions of user history ( in history selection) proves more effective than using the entire history indiscriminately ( in history selection). Indiscriminate inclusion can introduce irrelevant or outdated information that weakens the focus, whereas selective inclusion filters for the most pertinent interactions, ensuring the user context is concise and directly applicable to the current query. This targeted approach allows models to better extract and utilize key signals from the history, leading to more precise personalization. Structured profiles outperform raw histories. While query-aware selection is beneficial, simply inserting the selected histories as raw text offers limited gains (Raw in user context). Instead, constructing profiles from the selected histories (Profile in user context) provides clearer improvements. By organizing the selected histories into structured profile, implicit preferences become explicit, and the signals are presented in more usable form. Despite these advances, none of the models in any setting surpass an average score of 60. This ceiling highlights that personalization in realistic environments remains significant challenge. In realistic environments, user histories are not clean signals, as preferences are implicit and spread across long-term interactions, which makes them difficult to capture through explicit cues. To advance beyond this limitation, new approaches are needed that can disentangle implicit preferences from noisy interaction data and align them more effectively with the users query. 4.3 FEEDBACK AS DIAGNOSTIC SIGNAL Our evaluation framework provides diagnostic feedback, offering detailed diagnosis of where personalized systems perform well and where they fall short in specific aspects. To demonstrate the diagnostic power of our evaluators feedback, we analyze two settings discussed in Section 4.2: the configuration exhibiting the lowest personalization performance in GPT-4o-search (w/ full history profile) and the one achieving the most effective personalization (w/ selected history profile). Figure 3 illustrates the diagnostic feedback generated by our framework for responses of GPT-4osearch to user query about balanced diet. In the lowest performance setting, the feedback identifies key shortcomings, such as an overly narrow focus on macros without practical food specifics, basic knowledge depth, and an undesired explanation flow (red highlights). In contrast, the highest performance setting effectively addresses these deficiencies, as evident in the response itself (green highlights). For these improvements, our evaluation framework explicitly diagnoses how addressed elements are well-integrated through its feedback, while assigning higher scores accordingly to reflect these improvements. By explicitly identifying both strengths and shortcomings, our feedback for generated response can serve as diagnostic signal that exposes specific direction for improvement, offering insights for developing more effective personalization methods. 8 Preprint. Figure 4: Comparison of personalization performance across models under two settings. Personalized(red) generates responses conditioned on user context and the models own web search results, whereas Personalized + Gold Information(green) replaces web search results with gold information. 4.4 EFFECTIVE METHOD FOR HISTORY RETRIEVAL to scale. they are prohibitively costly and impractical limitations in real-world settings like BESPOKE. As discussed above, selectively incorporating histories into user context is essential for effective personalization. While leveraging LLMs can directly assess query-history relevance Bias discussed in Section 4.2, encoder retrievers offer an efficient alternative method for history selection, yet they face fundamental Since queries are primarily information-seeking, they often lack sufficient semantic signals to capture the personalization aspects, such as tone or style. Moreover, user preferences are typically implicit and dispersed across long-term interactions, which makes them difficult for bi-encoders to capture using only surface-level semantic similarity (Su et al., 2024; Lee et al., 2025). To overcome these limitations, we explore query expansion strategies inspired by prior work (Gao et al., including CoT expansion, which leverages step-by-step reasoning, 2023a; Su et al., 2024), and Pseudo-history expansion, which synthesizes pseudo-histories to simulate relevant user preferences. More details are provided in Appendix C.4. As shown in Table 5, using the raw query alone yields poor retrieval performance. In contrast, both expansion methods consistently improve performance, demonstrating that query expansion enriches the query with latent preference cues, making it easier for an efficient biencoder retriever to recover relevant histories. Table 5: Comparison of history retrieval using original and expanded queries. We use Stella-V5-1.5B model as retriever. Original Query +CoT +Pseudo-history 0.1221 0.3929 0.3911 0.0820 0.3809 0.3807 Query Type nDCG@20 nDCG@10 4. IMPACT OF WEB-SEARCH QUALITY ON PERSONALIZED RESPONSES As discussed in Section 4.2, responses often remain insufficiently personalized even when user context is provided. To pinpoint where this limitation originates, we design an experiment that removes web-search noise by directly providing the model with the gold information while disabling its builtin search tool, thereby ensuring both alignment with the users information need and the appropriate level of detail. As shown in Figure 4, incorporating the gold information yields consistent improvements in need alignment and content depth across all models. This confirms that the quality of retrieved information during the web-search stage is critical factor influencing the personalization quality of the final response. However, the improvements vary considerably depending on the model type. In particular, reasoning models show substantial improvements, while non-reasoning models achieve only modest ones. This indicates that reasoning models are more capable of analyzing and integrating the provided information thoroughly into their outputs. Taken together, our findings suggest that achieving optimal personalization requires both accurate web-search and advanced reasoning abilities to effectively integrate searched information. 9 Preprint."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce BESPOKE, the novel benchmark specifically designed for evaluating personalization in search-augmented LLMs. By constructing fully human-grounded dataset through long-term annotation, BESPOKE models real-world environments and provides feedback that enables detailed diagnosis of model strengths and weaknesses. We hope that BESPOKE will contribute to future research on developing personalized systems for more effective information seeking. ETHICS STATEMENT During recruitment, annotators are given clear explanation of the research purpose, procedures, and compensation, and only those who provide informed consent are enrolled. Over the three-week collection period, annotators review their own histories and remove any personally identifiable or sensitive information. We then apply secondary review and filtering step to ensure robust deidentification and removal of sensitive content. Annotators receive fair compensation for their participation, the adequacy and legality of which is reviewed and confirmed by legal expert prior to the study. Further details on data collection, anonymization, and compensation procedures are provided in Appendix B."
        },
        {
            "title": "REFERENCES",
            "content": "Nishant Balepur, Vishakh Padmakumar, Fumeng Yang, Shi Feng, Rachel Rudinger, and Jordan L. Boyd-Graber. Whose boat does it float? improving personalization in preference tuning via inferred user personas. ArXiv, abs/2501.11549, 2025. URL https://api. semanticscholar.org/CorpusID:275757229. Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedrelevance labels. ings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17621777, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.99. URL https://aclanthology.org/2023. acl-long.99/. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1), 2023b. Gemini-Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jimenez Gutierrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, and Yu Su. Mind2web 2: Evaluating agentic search with agent-as-a-judge, 2025. URL https://arxiv.org/abs/ 2506.21506. Rujun Han, Yuhao Zhang, Peng Qi, Yumo Xu, Jenyuan Wang, Lan Liu, William Yang Wang, Bonan Min, and Vittorio Castelli. Rag-qa arena: Evaluating domain robustness for longform retrieval augmented question answering. ArXiv, abs/2407.13998, 2024. URL https: //api.semanticscholar.org/CorpusID:271310035. Yizheng Huang and Jimmy Huang. survey on retrieval-augmented text generation for large language models. arXiv preprint arXiv:2404.10981, 2024. Kaixin Ji, Danula Hettiachchi, Flora D. Salim, Falk Scholer, and Damiano Spina. Characterizing information seeking processes with multiple physiological signals. Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024. URL https://api.semanticscholar.org/CorpusID:269484738. 10 Preprint. Prerna Juneja, Wenjuan Zhang, Alison Marie Smith-Renner, Hemank Lamba, Joel Tetreault, and Alex Jaimes. Dissecting users needs for search result explanations. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI 24, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703300. doi: 10.1145/3613904.3642059. URL https://doi.org/10.1145/3613904.3642059. Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. Realtime qa: Whats the answer right now?, 2024. URL https://arxiv.org/abs/2207.13332. Jieyong Kim, Hyunseo Kim, Hyunjin Cho, SeongKu Kang, Buru Chang, Jinyoung Yeo, and Dongha Lee. Review-driven personalized preference reasoning with large language models for recommendation. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 25, pp. 16971706, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400715921. doi: 10.1145/3726302.3730055. URL https://doi.org/10.1145/3726302.3730055. Seoyeon Kim, Kwangwook Seo, Hyungjoo Chae, Jinyoung Yeo, and Dongha Lee. VerifiNER: Verification-augmented NER via knowledge-grounded reasoning with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 24412461, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/ v1/2024.acl-long.134. URL https://aclanthology.org/2024.acl-long.134/. Sunghwan Kim, Tongyoung Kim, Kwangwook Seo, Jinyoung Yeo, and Dongha Lee. Stop playing the guessing game! target-free user simulation for evaluating conversational recommender systems. ArXiv, abs/2411.16160, 2024b. URL https://api.semanticscholar.org/ CorpusID:274234660. Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A. Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, Nedim Lipka, Chien Van Nguyen, Thien Huu Nguyen, and Hamed Zamani. LongLaMP: Benchmark for Personalized Long-form Text Generation, October 2024. Daehyun Kwak, Soobin Park, Inha Cha, Hankyung Kim, and Youn-Kyung Lim. Investigating the potential of group recommendation systems as medium of social interactions: case of spotify blend experiences between two users. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI 24, New York, NY, USA, 2024. Association for Computing ISBN 9798400703300. doi: 10.1145/3613904.3642544. URL https://doi. Machinery. org/10.1145/3613904.3642544. Sangam Lee, Ryang Heo, SeongKu Kang, and Dongha Lee. Imagine all the relevance: Scenarioprofiled indexing with knowledge expansion for dense retrieval. ArXiv, abs/2503.23033, 2025. URL https://api.semanticscholar.org/CorpusID:277452715. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. ArXiv, abs/2305.14251, 2023. URL https://api. semanticscholar.org/CorpusID:258841470. Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, and Joseph Gonzalez. Search arena: Analyzing search-augmented llms. ArXiv, abs/2506.05334, 2025. URL https://api. semanticscholar.org/CorpusID:279243096. Tong Niu, Shafiq Joty, Ye Liu, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Judgerank: Leveraging large language models for reasoning-intensive reranking. ArXiv, abs/2411.00142, 2024. URL https://api.semanticscholar.org/CorpusID:273798418. 11 Preprint. OpenAI. Gpt-4 technical report. 2023. URL https://api.semanticscholar.org/ CorpusID:257532815. Arjun Panickssery, Samuel R. Bowman, their own generations. J. Tomczak, recognize In A. Globerson, L. Mackey, D. Belgrave, InInc., URL https://proceedings.neurips.cc/paper_files/paper/2024/ and favor A. Fan, U. Paquet, formation Processing Systems, volume 37, pp. 6877268802. Curran Associates, 2024. file/7f1f0218e45f5414c79c0679633e47bc-Paper-Conference.pdf. and C. Zhang (eds.), Advances and Shi Feng. Llm evaluators in Neural Chanhee Park, Hyeonseok Moon, Chanjun Park, and Heu-Jeoung Lim. Mirage: metric-intensive benchmark for retrieval-augmented generation evaluation. ArXiv, abs/2504.17137, 2025a. URL https://api.semanticscholar.org/CorpusID:278033562. Soobin Park, Hankyung Kim, and Youn-kyung Lim. Reimagining personal data: Unlocking the potential of ai-generated images in personal data meaning-making. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, CHI 25, New York, NY, USA, 2025b. Association for Computing Machinery. ISBN 9798400713941. doi: 10.1145/3706598.3713722. URL https://doi.org/10.1145/3706598.3713722. Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng, Cunxiang Wang, Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jiarong Jiang, Tong He, Zhiguo Wang, Pengfei Liu, Yue Zhang, and Zheng Zhang. Ragchecker: fine-grained framework for diagnosing retrieval-augmented generation. ArXiv, abs/2408.08067, 2024. URL https: //api.semanticscholar.org/CorpusID:271874517. Alireza Salemi and Hamed Zamani. Lamp-qa: benchmark for personalized long-form question answering. ArXiv, abs/2506.00137, 2025a. URL https://api.semanticscholar.org/ CorpusID:279075447. Alireza Salemi and Hamed Zamani. Learning from natural language feedback for personalized question answering. 2025b. URL https://api.semanticscholar.org/CorpusID: 280649600. Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. Lamp: When large language models meet personalization. arXiv preprint arXiv:2304.11406, 2023. Alireza Salemi, Julian Killingback, and Hamed Zamani. ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation, May 2025. Kwangwook Seo, Donguk Kwon, and Dongha Lee. MT-RAIG: Novel benchmark and evaluation framework for retrieval-augmented insight generation over multiple tables. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2314223172, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1128. URL https://aclanthology. org/2025.acl-long.1128/. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, and Tao Yu. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. ArXiv, abs/2407.12883, 2024. URL https://api.semanticscholar.org/ CorpusID:271270735. Orion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates, Dawn Lawrie, and Benjamin Van Durme. Rank1: Test-time compute for reranking in information retrieval. In Second Conference on Language Modeling, 2025. URL https://openreview.net/forum?id=Pg0PAvbhGv. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. 12 Preprint. Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, Xiangru Tang, Heng Ji, and Jiaxuan You. Multiagentbench: Evaluating the collaboration and competition of llm agents. ArXiv, abs/2503.01935, 2025. URL https: //api.semanticscholar.org/CorpusID:276766372. 13 Preprint. Contents of Appendix Discussion A.1 Dataset scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Feedback as reward signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data collection details B.1 Details of recruiting human annotator . . . . . . . . . . . . . . . . . . . . . . . . B.2 Data handling & Privacy protection . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Validation for human annotated data . . . . . . . . . . . . . . . . . . . . . . . . . Experiment details C.1 Overall . . . . . C.2 Meta evaluation . . . . . . . C.3 User context analysis . C.4 History retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Case study Dataset statistics Data examples F.1 Gold information need and R-J pairs . . . . . . . . . . . . . . . . . . . . . . . . . F.2 User history and gold rubric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Comparison of R-J pairs in different history utilization setting . . . . . . . . . . . Prompts 15 15 15 15 16 16 17 17 17 17 18 18 19 19 19 20 14 Preprint."
        },
        {
            "title": "A DISCUSSION",
            "content": "A.1 DATASET SCALABILITY While our dataset is relatively small in scale due to its reliance on deeply engaged human annotation, it provides rich user histories that reflect authentic and realistic preferences. These high-quality annotations provide reliable foundation for personalization research (Kim et al., 2025), supplying signals that are difficult to obtain from synthetic or automatically collected data. Building on this foundation, the dataset can serve as seed material for LLM-based augmentation. Grounding augmentation in our dataset enables the synthesis of additional data that is both realistic and useraligned. This provides clear path toward scalability, enabling BESPOKE to expand beyond its initial scale while preserving authenticity in richer and more diverse user histories. A.2 FEEDBACK AS REWARD SIGNAL The feedback in BESPOKE not only enables evaluation but also points to new opportunities for training personalized systems. As discussed in Section A.1, authentic human annotations can be expanded through LLM-based augmentation to construct larger-scale training datasets. Building on such datasets, the feedback has the potential to serve as rich supervisory signal for developing user-tailored reward models. Unlike scalar ratings, it articulates explicit reasons for user satisfaction or dissatisfaction, which could be directly leveraged in reinforcement learning as natural language reward signal. This direction suggests scalable pathway toward reward modeling that remains closely aligned with genuine user preferences, which we leave as future work."
        },
        {
            "title": "B DATA COLLECTION DETAILS",
            "content": "B.1 DETAILS OF RECRUITING HUMAN ANNOTATOR To collect sufficient user histories, we conduct long-term, deeply engaged human annotation, as detailed in Section 3. Following prior work that leverages long-term human engagement (Kwak et al., 2024; Park et al., 2025b), we collect histories over three-week period from 30 annotators. To ensure diverse and representative benchmark dataset, we recruit human annotators from wide range of professions and interests, while conducting thorough screening process to collect valid user histories. We select only those deemed suitable through this screening as annotators. Specifically, we target individuals who are familiar with Google Search and generative AI services such as ChatGPT or Gemini, where active usage is defined as engaging with these tools at least 1-2 times per day. Additionally, candidates provide detailed descriptions of their professions and personal interests, allowing us to avoid recruiting individuals similar to those already selected. Instead, we prioritize new annotators with distinct professions and interests to maximize diversity. To quantify this diversity, we measure the distribution of annotators interests using the Shannon index, widely adopted diversity metric from ecology and information theory. First, we collect detailed interests from each annotator and normalize them into high-level categories by integrating synonyms and variations (e.g., yoga, gym workouts health/fitness), leveraging GPT-5. Next, we aggregate how many annotators interests include each normalized category. Finally, we compute the relative frequency pi for each category and calculate the Shannon index as: = (cid:88) i=1 pi log pi (1) where is the total number of categories. higher Shannon index indicates that annotators interests are spread across diverse categories rather than concentrated in few. To assess evenness, we compare this value to the theoretical maximum Hmax = log (achieved when all categories are equally represented) and derive Shannons equitability EH as: EH = Hmax 15 (2) Preprint. where is the observed Shannon index. Our dataset records Shannon equitability of EH = 0.91, confirming that the annotator group is not biased toward specific topics and instead reflects broad spectrum of interests, thereby enhancing dataset representativeness. Each recruited annotator receives dedicated Google account created for research purposes, with paid Google AI Pro plan. We also maintain ongoing communication channels between annotators and researchers to promptly address any questions arising during data collection or annotation processes. Annotators benefit from complimentary access to the AI Pro plan during the history collection period, along with adequate compensation for their annotation efforts. All compensation adheres to ethical and legal standards, as verified through review by legal experts. Furthermore, annotators receive comprehensive information on compensation and data collection procedures in advance, and we recruit only those who provide informed consent. B.2 DATA HANDLING & PRIVACY PROTECTION Our benchmark prioritizes user privacy and ethical data practices through rigorous de-identification process, ensuring no personal information is disclosed without consent. All direct identifiers (e.g., names, emails, phone numbers) are immediately deleted upon collection, while quasi-identifiers (e.g., IP addresses, account IDs) are removed entirely. Location data is generalized to city-level or higher, eliminating finer details. Sensitive expressions in text are filtered using regular expressions and manual review, with problematic content deleted or replaced with neutral alternatives. Human annotators are fully informed about the de-identification procedures and provided opportunities to review the final anonymized data. To further safeguard privacy, we adopt multi-stage review processes. The first stage is conducted by annotators themselves via Google Takeout exports, where they can delete sensitive items. The second stage is conducted by our team, who verify and de-identify any remaining identifiable elements through generalization, masking, or removal. Only records deemed fully anonymized, free of any risk to annotators or third parties, are incorporated into the dataset. This comprehensive approach guarantees that the user histories provided in our benchmark are ethically sourced, privacy-protected, and pose no risk of unintended disclosure, aligning with the highest standards of data ethics. B.3 VALIDATION FOR HUMAN ANNOTATED DATA Response-Judgment Pairs. For the annotation of responsejudgment (RJ) pairs, human annotators are asked to evaluate model responses along the four personalization criteria: need alignment, content depth, tone, and explanation style. To ensure high-quality and consistent annotations, we provide detailed guidelines with illustrative examples for each criterion, allowing annotators to clearly understand the evaluation standards. During the process, annotators review their own feedback three times to check whether their judgments are consistent and well aligned with their preference. This review process ensures that the collected feedback is both reliable and consistent for each annotator themselves, resulting in trustworthy RJ pairs for evaluating personalization. Gold Information Set. For each query, we construct the candidate gold information set by leveraging GPT-5 (reasoning effort = high) with the prompt outlined in Table 12, to extract atomic claims from the gold response r+, following methods similar to existing works (Ru et al., 2024; Min et al., 2023; Seo et al., 2025). For improving accuracy and consistency, we manually annotate gold information for two sampled instances and incorporate these annotations as exemplars in 2-shot prompting strategy. Subsequently, we perform manual inspection of this candidate set to ensure its appropriateness and quality. Since the information is derived directly from the response, it may contain personally identifiable elements, which are promptly removed to ensure privacy and uphold ethical standards, as mentioned in Appendix B.2. Next, we check whether each claim can be verified through real-world web sources, retaining only those that can be verified through Google Search. Finally, to eliminate redundancy, we merge claims that convey identical information but differ only in minor phrasing, resulting in gold information set + = {i+ q,1, . . . , i+ q,n}. 16 Preprint."
        },
        {
            "title": "C EXPERIMENT DETAILS",
            "content": "C.1 OVERALL We evaluate six representative search-augmented LLMs: GPT-4o-search, o3-search, Gemini-2.5Flash, Gemini-2.5-Pro, Perplexity-sonar, and Perplexity-sonar-reasoning. The prompts used for inference with these models are provided in Tables 13 and 14. To measure recall for factuality, for each query, we iterate through the gold information set and use GPT-5 (reasoning effort = high) to perform binary classification on whether each individual gold information item is included in the response without contradiction (Kim et al., 2024a). The recall is then computed as the proportion of gold information items that are correctly included. The prompt used for this classification is shown in Table 15. For personalization evaluation, we provide the query q, the gold information need n+ , the R-J pair for the query, and the corresponding gold rubric G+ as evaluation context. Based on this, we generate scores and feedback for the response across the four criteria with GPT-5 (reasoning effort = high). The prompt for generating the gold rubric is provided in Table 16, and the prompt used for evaluation is shown in Table 17. C.2 META EVALUATION In the meta-evaluation, we compare our personalized LLM-based evaluator Ep against baselines across different settings. The non-personalized version (w/o Personal.) is implemented by providing only the query, response, and general definitions of the individual criteria as the evaluation context. The score-only baseline (w/o Feedback) incorporates the query, the gold information need n+ , the response, R-J pairs containing only scores, and gold rubric is generated from these score-only R-J pairs and included in the evaluation context. Our full evaluator (w/ Feedback) follows the approach described in Section 3.4, utilizing the complete evaluation context including feedback information from the R-J pairs to assess the responses. For the feedback alignment assessment, the prompt used with GPT-5 as the meta-evaluator is provided in Table 18. This prompt instructs the meta-evaluator to determine if the evaluator-generated feedback and human-annotated feedback convey equivalent content, given the context (q, n+ ), and outputs binary label for equivalence. , R+ C.3 USER CONTEXT ANALYSIS In our experiments, all user profiles were generated using GPT-5 (reasoning effort = high). For the static profile, we aggregated the complete history of each user into single profile (Kim et al., 2024b), with the prompt used for this generation shown in Table 19. For history selection, we leverage GPT-5 in cross-encoder manner, following methods similar to Niu et al. (2024) and Weller et al. (2025), to evaluate the relevance of each history session to given query. The relevance was defined as whether the history would contribute to generating personalized response, and the judgment was made in binary classification. Only histories deemed relevant were retained. The prompt for this relevance assessment is provided in Table 20. For the dynamic profile, we constructed an adaptive profile for each query. We considered two variants: one based on the entire history, and another restricted to the subset of histories identified as relevant (indicated by under History selection). The prompt for dynamic profile generation is presented in Table 21. C.4 HISTORY RETRIEVAL To evaluate the effectiveness of history retrieval methods, we compare the performance of using the raw query alone against query expansion techniques. Specifically, we consider two variants of query expansion to enhance history retrieval: CoT expansion (Su et al., 2024), which prompts the model to infer the key aspects of raw query most important for retrieving relevant past sessions through step-by-step reasoning, and pseudo-history expansion, which adapts the HyDE (Gao et al., 2023a) idea of pseudo documents by generating pseudo history that serves as synthetic representation of the personalization signals likely to be relevant for retrieval. The prompts employed for these expansion methods are detailed in Table 23. To measure history retrieval performance, we designate oracle histories using an LLM as ground truth and evaluate the retrievers performance. Specifically, for each query, we first establish an 17 Preprint. Table 6: An example where the same query results in different information needs depending on the user, showing how User19 (AI researcher) emphasizes technical mechanisms and the role of artificial intelligence, while User20 (media producer) highlights sociocultural aspects. Query: How has the spread of social media affected peoples lives? User19: Works in AI research and development, with strong interest in the latest AI technologies. Gold information need: Im curious about the influence of social media in the age of AI. Could you explain, with specific examples, how it has affected various aspects of our daily lives? Gold Response: Recommender systems rank posts by signals like watch time, likes, comments, replays, and pauses (dwell time) . . . filter bubble is when this personalization narrows what you see . . . Generative AI creates text, images, audio, and video from prompts . . . Machine translation . . . User20: media-related PD with strong interest in social phenomena and cultural trends. Gold information need: Analyze how the spread of social media has impacted the lives of modern people. In particular, analyze changes in family relationships, the emergence of new professions . . . Gold Response: Social media lowered the cost of connection and raised the visibility of social proof, reshaping family relationships, job structures, and daily routines . . . Families: group chats and perpetual photo/video sharing strengthen bonds across distance . . . Creators/influencers/streamers produce . . . oracle set of histories by leveraging the gold rubric R+ , which captures the ideal personalization aspects. Using GPT-5, we iterate over all sessions in the users interaction history, assessing for each whether it contains, query-specific personalization signals that align with the gold rubricsuch as stable preferences, explicit constraints, background context, or adjacent decision-changing factorsand designate those that do as the ground-truth oracle histories. The prompt employed for oracle history selection is presented in Table 22. Subsequently, we assess the retrieval quality by inputting the actual query (or its expanded version) into the bi-encoder retriever and measuring how well it recovers these oracle histories, measured via nDCG@K."
        },
        {
            "title": "D CASE STUDY",
            "content": "Personalization is essential in information-seeking tasks because the same query can correspond to different information needs across users. For qualitative evaluation, we conducted case study in the BESPOKE dataset focusing on instances where users with different backgrounds issued the same test query. As shown in Table 6, User 19, an AI researcher, consistently focuses on technical mechanisms and AI-mediated effects, whereas User 20, media producer, emphasizes sociocultural phenomena and cultural trends. For the query How has the spread of social media affected peoples lives?, User 19s gold response emphasizes keywords such as recommender systems, generative AI, and machine translation, while User 20s gold response highlights keywords such as family relationships and job structures. This divergence underscores that an identical query does not uniquely determine the information need; it depends on the users background and preferences. Consequently, truly effective personalized search-augmented LLM must accurately infer the user-specific needs behind the query. To achieve this, the model could leverage user history as key signal for preference inference, ensuring that the generated responses are appropriately tailored to the individual."
        },
        {
            "title": "E DATASET STATISTICS",
            "content": "Table 7 summarizes the overall statistics of BESPOKE. On average, each user contributes 73.5 search sessions and 24.3 chat sessions, indicating that the dataset balances both query-driven and conversational interactions. Search sessions reflect realistic information-seeking behavior, while chat sessions capture interactive exchanges that vary in length and style. Chat sessions contain 4.3 turns on average, with some sessions extending up to 53 turns, demonstrating substantial variation in conversational depth. This distribution highlights that BESPOKE captures both short, focused dialogues and long, exploratory conversations, providing balanced benchmark for evaluating personalized search-augmented LLM across heterogeneous user behaviors. 18 Preprint. Table 7: Full statistics of BESPOKE User ID #Search Sessions #Chat Sessions Avg. turns per Chat Session Max turns per Chat Session 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 108 11 61 63 175 24 192 40 83 47 51 6 61 28 238 58 67 55 68 56 113 73 15 73 32 16 62 202 53 22 49 18 26 19 36 19 18 32 12 25 24 27 52 12 38 13 51 36 38 11 9 28 8 12 21 17 21 22 12 11 Avg. 73.48 24."
        },
        {
            "title": "F DATA EXAMPLES",
            "content": "F.1 GOLD INFORMATION NEED AND R-J PAIRS 4.65 4.50 4.85 8.58 2.67 5.16 4.67 4.56 3.42 1.88 2.71 5.19 4.58 4.25 3.00 3.31 4.53 2.31 3.92 7.27 4.22 2.43 4.25 6.00 4.05 3.24 4.57 6.45 3.42 4.36 4.30 10 12 18 53 15 13 21 18 10 4 7 12 20 19 11 7 25 10 10 31 8 10 9 21 20 8 17 16 7 8 15 In Table 8, we present an example of user query, the corresponding gold information need, and multiple R-J pairs. The query and gold information need are written by human annotators to explicitly capture the users true intent, while the paired responses show how the user assigns different scores and feedback to different model outputs. F.2 USER HISTORY AND GOLD RUBRIC Table 9 illustrates samples of the users chat history session and search history session, demonstrating how past interactions and browsing activities help shape the interpretation of the current query. The chat history consists of multi-turn user-assistant conversations, while the search history contains user queries along with visited site titles and page summaries. The table also introduces the gold rubric we used for evaluation, which defines four dimensions: need alignment, content depth, tone, and explanation style, with standardized 15 scoring guidelines. This rubric ensures consistent and interpretable personalized judgments while providing diagnostic signals of model behavior. F.3 COMPARISON OF R-J PAIRS IN DIFFERENT HISTORY UTILIZATION SETTING To further illustrate the effect of different history utilization settings, we provide two extended examples from Table 10 (Perplexity-sonar) and Table 11 (Gemini-2.5-flash). In both tables, the upper example corresponds to the responses generated with full history profile, while the lower example corresponds to the responses generated with query-aware selected history, holding all other experimental settings constant. These comparisons show how diagnostic feedback exposes concrete weaknesses (red) and improvements (blue) that arise from different user-context constructions. 19 Preprint. In Table 10, the upper example reveals several shortcomings. Although the destinations satisfied the Romance-language and regional preferences, the model did not explicitly map each recommendation to the five specified constraints: budget, safety, English usability, car-free mobility, and trip length. The response read as high-level advice, omitting field-ready details such as transit pass names, concrete price ranges, and concise day-by-day itineraries. Moreover, the tone drifted into marketing-like style, making assumptions about the users professional identity, which the annotator flagged as inappropriate. By contrast, the lower example addressed some of these gaps by providing clearer per-destination budgets and structured itineraries. The diagnostic feedback recognized this improvement in both need alignment and explanation style, while still urging the inclusion of explicit safety and mobility mapping and stricter adherence to standardized template. In Table 11, similar pattern emerges. In the upper example, the model produced broad overview centered on brands and communication, but failed to analyze the three facets explicitly requested: family relationships, the emergence and decline of professions, and lifestyle shifts. The feedback noted that the response relied on generic claims with no selective data and no concrete examples, leaving content depth and need alignment weak despite neutral tone. With the lower example, the output moved closer to the intended report format by explicitly covering family, jobs, and lifestyle and grounding them in platform-specific mechanisms. Nevertheless, the feedback emphasized that the analysis still lacked quantitative evidence, explicit mention of declining professions, and the structured reporting style that was requested. Taken together, these cases show that while utilizing query-aware selective histories consistently improves response quality, effective personalization also requires systematic coverage of all 4 criteria."
        },
        {
            "title": "G PROMPTS",
            "content": "20 Preprint. Table 8: Example of gold information need and two ResponseJudgment (R-J) pairs, which are discussed in Section 3.3 Query What is digital literacy and why is it important? Gold information need The user wants clear definition of digital literacy, breakdown of its core skills/components, examples of how it is applied in practice, and recent extensions such as AI literacy. They also want synonyms/antonyms (e.g., media literacy, information literacy vs. digital illiteracy) and diachronic perspective (how the concept has changed over time). Sampled response 1 Digital Literacy, Deeper Dive! Digital literacy goes beyond simply using digital devices. Its the integrated ability to find the information you need amid the flood of digital data (Access), critically analyze its authenticity (Analyze), create meaningful content of your own (Create), and communicate and act responsibly with others (Action). Think of it as having sturdy ship and precise compass to navigate the digital world! ... What Makes Up Digital Literacy? (Detailed Explanation!) - Technical Proficiency: more than just using tools, but understanding how they work and combining them to suit your needs. - Critical Thinking & Curation: identify who created information, purpose, and bias. - Creative Production: use digital tools to turn ideas into content... Practical Examples by Field When Is It Needed? Practical Example History class with VR ruins; Science class analyzing air pollution data with Excel Field Education Workplace Marketing campaigns with data-driven targeting; workflow automation with RPA/Python Daily Life Open banking dashboards; fact-checking fake news with SNU FactCheck Center ... Human judgment 1 Need Alignment: 4/5, It covers most of what wanted: clear definition, skills/components, where its used, and timely AI-era updates with practical examples. But it skips the synonyms/antonyms asked for, and quick then-vs-now line would make the diachronic angle explicit. Content Depth: 5/5, Totally solid right now: concrete, real-world examples and current tools make it genuinely useful without fluff. Tone: 4/5, Generally pleasant and friendly, but the emoji and exclamation points feel bit heavy for me, lighter would read more naturally. Explanation Style: 4/5, Clear sections and examples, but Practical Examples by Field table is hard to read; missing Synonyms/Antonyms subheading. Sampled response 2 Digital Literacy: An Essential Competency in Modern Society Importance and the Digital Divide Digital literacy is core competency for individuals and society. Beyond technical skills, it encompasses critical understanding of information, communication, and value creation ... The Multidimensional Importance and Positive Impact of Digital Literacy - Expanding Employment and Economic Opportunities demand in AI, big data, digital marketing; platforms enabling entrepreneurship. - Innovation in Education online platforms, VR/AR for personalized learning; foundation for lifelong learning. - Strengthening Democratic Participation digital media as forum, discern misinformation, promote digital citizenship. ... The Digital Divide: Deepening Inequality - From first-level (access) second-level (skills) third-level (outcomes). - Worsening income inequality due to automation/AI. - Political exclusion and crisis of representation for vulnerable groups. ... In conclusion, digital literacy is no longer optional but vital survival skill. ... Human judgment Need Alignment: 2/5, wasnt asking about the digital literacy gap. You didnt directly cover what asked for: clear definition, the key skills/components, where its used, synonyms/antonyms, and how its changed over time. Most of the space fixates on the digital divide instead of those four facets. Content Depth: 2/5, The quality of the content itself isnt bad, but its mostly high-level prose. wanted concise, concrete skills and examples. Instead got big-picture claims with few actionable details, so it reads like mini-paper more than useful, specific answer. Tone: 2/5, Its too stiff and boring. It feels full of intellectual vanity. dont like seeing you use such pretentious languageeven the title reads like conference paper. Explanation Style: 3/5, It was good that you divided it into two sections, each with its own subheading. But it doesnt map to the facets asked for, and the paragraphs feel heavy. Please switch to modular layout with short bullets under those headings. 21 Preprint. Table 9: Example of user history and gold rubric for scoring ResponseJudgment pairs Query What is digital literacy and why is it important? Chat history session User: Please recommend some trending web novels lately, but leave out romance and romance fantasy. Assistant: Youre looking for some trending web novels, huh? Leaving out romance and romance fantasy, Ive picked out only the most exciting stories for you. Lately, genres where the main character grows, overcomes - The Scoundrel Has Returned adversity, gets satisfying revenge, or shines as professional are popular. (Fantasy): synopsis, why its popular, where to read... User: told you to bring me links. Assistant: Apologies for missing the links. Here are versions with links... Search history session Search query: AI webtoon and web novel market Visits: <Title> Matured Webtoons and Web Novels: Breaking Genre Boundaries and Offering Personalized AI Recommendations <Page summary> The page discusses the strategy in responding to the maturing webtoon and web novel market. As the market enters new phase, the company plans to diversify genres and expand user engagement, incorporating intellectual property from games, films, and YouTube. They are also leveraging advanced AI technologies to analyze user preferences and offer personalized content recommendations while breaking traditional genre boundaries... Gold rubric Need Alignment - 5: Directly answers the exact facets the user requested, definition; skills/components; fields/situations of use; synonyms/antonyms; diachronic change (then vs now). Incorporates up-to-date elements (AI, deepfakes, algorithmic feeds/bias, IoT). No detours or filler. - 4: Covers nearly all facets with only minor omission or light coverage (e.g., brief or missing synonyms/antonyms), and minimal drift. - 3: Generic treatment; misses one or two key facets or treats them superficially; modern angles underemphasized. - 2: On-topic but focuses on secondary angles (e.g., long digital-divide discussion) or analogies instead of requested facets; multiple facets missing. - 1: Largely off-target or ignores the requested facets. in this order or equivalent: Content Depth - 5: Concise yet substantive; mixes clear concepts with actionable, field-specific examples and named tools/workflows; basic vs advanced snapshots when helpful; reflects current realities (AI verification, deepfakes, algorithmic bias, passkeys/MFA); no fluff. - 4: Solid detail with few gaps; mostly actionable; minor depth or recency improvements needed. - 3: Understandable but high-level or generic; lacks concrete tools/cases or field specificity; user must infer steps. - 2: Too abstract/academic or too shallow; few/no named tools or workflows; limited practical value. - 1: Severely mismatched depth (mini-paper or basic listicle) that the user cannot use. Tone - 5: Calm, professional, friendly; natural phrasing; at most 12 emojis/exclamations; no hype, cutesiness, pretension. - 4: Generally aligned; slightly dry or few extra emojis/exclamations, but not distracting. - 3: Neutral/robotic or somewhat academic; acceptable but not preferred. - 2: Off-putting: forced cheerfulness, heavy emojis/exclamations, cutesy metaphors, or stiff/pretentious vibe. - 1: Condescending, arrogant, or offensive. Explanation Style - 5: Clean, modular structure mapping to facets: Definition; Skills/Components; Where Used; Synonyms/Antonyms; Diachronic Change. Short bullets/brief paragraphs; separates concepts from examples (e.g., basic vs advanced); optional key takeaway; avoids dense tables. - 4: Clear sections and mostly bullets; minor misalignment (order or one missing subheading), slightly wordy, or small but readable table. - 3: Some structure but heavier paragraphs or headings that dont map cleanly; requires extra effort to scan. - 2: Loose/essay-like; long blocks; structure not matched to requested angles; dense, hard-to-read tables. - 1: Disorganized; no clear sections; incompatible format for quick scanning. 22 Preprint. Table 10: Comparison of Perplexity-sonar response judgments in different history retrieval setting Query Please recommend some overseas travel destinations. Gold information need want to travel somewhere for 5 days with budget of under USD 3,000. Here are my criteria: 1) Im interested in Romance language cultures, 2) want to travel to Europe or the Americas, 3) The destination should be reasonably safe, 4) should be able to communicate at least partially in English, and 5) need to be able to get around without car. Please recommend countries and regions that fit these requirements. Perplexity-sonar response with full history profile Based on your interest in practical AI workflows, Gen communications, and travel in Europe/South America, here are some personalized overseas travel destinations that fit well with efficient planning, ethos-aware choices, and engaging storytelling for your audience or career-related interests: 1. Spain (Barcelona and Madrid) - Easily accessible by low-cost airlines and extensive train networks (Renfe) ideal for efficient, budget-smart travel. - Vibrant Gen culture hubs for marketing storytelling and CSR-related event activations. - Plenty of English usability in major urban centers, plus rich options for sustainable travel choices (walking tours, urban bike rentals). - Use Rome2Rio and Skyscanner for streamlined booking and cost comparison between flights and trains. - Safety-wise, standard European precautions apply, ... Evaluator judgment Need Alignment: 2/5, The picks mostly fit Romance-language Europe/Americas, but you didnt map each destination to my five conditions or show that 5-day trip fits under USD 3,000. Please add, per destination: safety, English usability, and car-free mobility (explicitly), plus budget breakdown (flight, lodging/night, daily spend, 5-day total). Also, avoid assumptions about me (e.g., Gen Z/CSR focus). Content Depth: 2/5, This reads as high-level advice. need field-ready details: name the transit cards/passes and how to get around without car, concrete price ranges, few local neighborhoods/hidden gems, concise one-day walking plan, practical tips (e.g., pickpocket hotspots, booking/free hours), and sensible day-trip ideas. Tone: 2/5, The framing assumes my professional interests (Gen Z, CSR) and feels like marketing pitch. Please keep it calm and straightforward, lead with the bottom line, and be transparent with ranges and tradeoffswithout assumptions about who am. Explanation Style: 3/5, You use bullets, but its not in consistent, scannable template and lacks costs. Please structure each destination as: Highlights; Fit to my 5 conditions; Getting around (no car, pass names); Estimated costs (flight, lodging/night, daily, 5-day total); Local favorites; Sample one-day itinerary; Practical tips. Perplexity-sonar response with query-aware selected history profile Here is personalized shortlist of overseas travel destinations tailored to your preferencesRomance-language countries, good English usability in tourist areas, typical trip length of 57 days, and total budget target around USD 3,000 for the entire trip: Western Europe 1. Spain (Barcelona + Day Trips to Girona or Montserrat) Why fit: Spanish is Romance language; Barcelona is vibrant with good English in tourist zones. Youve also researched Spains one-week budget already, so youre familiar. English usability: High in hotels, tourist restaurants, museums, public transport signage mostly bilingual. Some local shops less so. 5-day idea: Base in Barcelona; day trip to Girona or Montserrat Monastery. 7-day idea: Add second base day or another day trip to Sitges or Tarragona... Evaluator judgment Need Alignment: 4/5, Good picks (Romance-language, Europe/Americas) and solid cost breakdowns with 5-day totals. However, you didnt explicitly map safety and car-free mobility for each destination, ... please avoid assumptions like Youve already researched Spains one-week budget. Content Depth: 3/5, Useful budgets and quick day-trip ideas, but its still generic. Please add field-ready details: specific transit passes with prices (e.g., Barcelona Hola BCN/T-Casual; Lisbon Viva Viagem; Porto Andante; Paris Navigo Easy/Paris Visite; Rome Roma Pass), few local neighborhoods/hidden gems (e.g., Gr`acia/El Born; Alfama/Mouraria; Canal St-Martin; Trastevere), concise one-day walking plan per city, and practical tips (pickpocket hotspots, free museum hours, booking windows). Tone: 4/5, Clear and calm overall. One assumption about my prior Spain research and no upfront bottom line. Lead with the conclusion (e.g., Bottom line: Portugal and Spain comfortably under $3000; France/Italy are tight; South America not advised for 5 days) and keep avoiding assumptions. Explanation Style: 4/5, Nicely bulleted and mostly consistent, but not in the preferred per-destination template. Please standardize: Highlights; Fit to your 5 conditions; Getting around (no car) with pass names; Estimated costs (flight, lodging/night, daily, 5-day total); Local favorites; Sample one-day itinerary; Practical tips. Put South America under clear Why not for 5-day trip section. 23 Preprint. Table 11: Comparison of Gemini-2.5 response judgments in different history retrieval setting Query How has the spread of social media affected peoples lives? Gold information need Analyze how the spread of social media has impacted the lives of modern people. In particular, write report analyzing changes in family relationships, the emergence of new professions and the decline of others, and shifts in lifestyle both online and offline, using examples. Gemini-2.5-flash response with full history profile Social media has profoundly reshaped the landscape of human interaction, communication, and commerce, creating both significant opportunities and considerable challenges in peoples daily lives. From revolutionizing how content is created and distributed to transforming brand engagement and fostering new avenues for community, its influence is pervasive. However, this digital ubiquity also brings concerns regarding mental well-being and the spread of misinformation. Heres breakdown of how social media has affected various aspects of life: Decision Matrix: ... Evaluator judgment Need Alignment: 2/5, This centers on creators/brands/AI and general communication; it doesnt analyze family relationships, the emergence/decline of professions, or online/offline lifestyle shifts with concrete examples. Please reframe into those sections and tie each point to social-media mechanisms with platform-specific examples. Content Depth: 2/5, Mostly broad, well-known points with minimal mechanisms, no selective data, and few concrete cases. Add 25 data points, clear causal reasoning, and specific examples per section (e.g., family vlogging on YouTube/TikTok; new roles like UGC creators/community managers vs. declines in print ad sales roles), plus short near-term outlook. Tone: 4/5, Neutral and readable overall. It feels bit like marketing deck and somewhat wordytighter, more concise phrasing would improve it. Explanation Style: 3/5, The matrix and checklist help, but the structure doesnt match the requested report. Organize into Family / Professions (new & declining) / Lifestyle (online & offline) with bullet points, selective data and platform examples, plus brief executive summary and outlook. Gemini-2.5-flash response with query-aware selected history profile The pervasive spread of social media has fundamentally reshaped how people connect, work, and live, presenting both remarkable opportunities for connection and significant challenges to well-being and productivity. Its net effect is complex tapestry of enhanced access, fragmented attention, and shifting social norms, demanding conscious navigation of its platforms and pervasive influence. Heres an at-a-glance map of key impact domains: - Jobs & Work: New career paths, personal branding, algorithmic work. - Lifestyle & Time Use: Attention fragmentation, habit formation, self-presentation. - Family Relationships: Intergenerational dynamics, communication shifts, privacy. - Information Ecosystem: News consumption, misinformation, polarization. - Mental Health: Social comparison, anxiety, loneliness... Evaluator judgment Need Alignment: 4/5, You addressed family, jobs, and lifestyle with platform-specific examples, but you didnt clearly cover the decline of professions and you drifted into secondary domains (news, civic life, education). Please focus the report on the three facets, call out specific declining roles due to social media, and make offline lifestyle shifts (e.g., shopping, dining, travel behaviors) explicit. Content Depth: 3/5, The mechanisms help, but there are no selective data points, named cases, or near-term predictions. Add 24 credible stats per section (e.g., ad spend shifting to social, creator economy size, % of families using group chats, live-commerce penetration), concrete cases, clearer causal links (why social media causes job declines), and short forecasts tied to each facet. Tone: 4/5, Clear and neutral overall; bit wordy and slightly conversational in places. Tighter phrasing and sticking to concise, objective language would sharpen it. Explanation Style: 4/5, Headings/bullets make it skimmable, but its not in the exact report format. Mirror the requested structureFamily / Professions (new vs declining) / Lifestyle (online vs offline)add brief executive summary and near-term outlook, and include bullet-point examples plus 12 data points per section; keep secondary impacts brief or label them as an add-on. 24 Preprint. Table 12: The prompt for extracting candidate gold information set Prompt for extracting candidate gold information set You are an information extraction assistant. Given user query and model response, extract atomic information claim that represent gold information aspects strictly following these rules. General rules (apply in all cases): Output strictly JSON with single top-level key: gold information (array of strings). No other keys or text. Each item should be single atomic information claim describing an aspect. Exclude any time-related content. Exclude user-specific private attributes or unverifiable personal details. Do not include proper nouns (titles/brand names/person names); use common-noun categories instead. If price information appears, collapse multiple prices into single range or ceiling/floor statement. Recommendation-style queries (when the response recommends items): Extract aspects of the recommended items without their specific names. Use the common-noun form for the item (e.g., Game, Headphones, Laptop). Information-seeking queries (all other cases): Extract factual information presented in the response, following all general exclusions above. <Query>: {query} <Response>: {gold response} Return JSON only in the form: { gold information: [ ... ]} Table 13: The prompt for non-personalized response generation Prompt for non-personalized response generation You are personalized information-seeking assistant. <Query>: {query} Provide personalized answer based on user context and accurate web-searched information. Do not ask follow-up questions or provide generic responses - deliver direct, tailored answer to the users query. Table 14: The prompt for response generation with user context Prompt for personalized response generation with user context You are personalized information-seeking assistant. <Query>: {query} <User Context>(follow these preferences to craft the answer): {user context} <GUIDELINES> - Match the organization, tone, depth, and style implied by the user context - Provide personalized answer tailored to the users preferences through web search. - Use accurate, up-to-date information obtained through web browsing. - Do not ask follow-up questions or provide evaluations; output only the final personalized answer. Provide personalized answer based on user context and accurate web-searched information. Do not ask follow-up questions or provide generic responses - deliver direct, tailored answer to the users query. Table 15: The prompt for Er Prompt for Er You are precise fact checker. Decide if the response contains the claims core meaning. Answer strictly with true or false. Consider it contained if semantically equivalent even with different words. Do not count if contradicted or absent. Claim: {claim} Response: {response text} Return strictly one token: true or false. 25 Preprint. Table 16: The prompt for generating gold rubric based on R-J pairs Prompt for generating gold rubric based on R-J pairs You are an expert annotator who derives single users personalized evaluation rubric from their past evaluations. Analyze responses in the JSON and infer the users implicit standards. Output only the analysis text between <BEGIN ANALYSIS> and <END ANALYSIS> {R-J pairs for each query} These evaluations of the query and gold information responses were all done by one person. For each response, they evaluated 4 criteria on 1-5 scale with feedback: Base scale guidance (1-5 for each criterion): Need Interest Alignment: 1: Irrelevant to the users actual interests within the broader topic. 2: On-topic but mainly secondary details the user likely finds uninteresting. 3: Generic one-size-fits-all; fails to reflect the users specific interests. 4: Captures core interests with minor drift toward less important details. 5: Pinpoints the exact core concepts and perspectives the user is most curious about. Content Level: 1: Completely mismatched depth (e.g., research paper for simple query). 2: Too hard to comprehend or so basic it has very little value. 3: Understandable but noticeably mismatched; requires extra effort or context. 4: Overall good match; could be slightly deeper or simpler to be perfect. 5: Perfectly matches the users knowledge level; effortless to comprehend. Tone: 1: Very uncomfortable or offensive to the user. 2: Disliked tone is prominent, causing discomfort/resistance. 3: Functionally okay but robotic or distant from preference. 4: Generally close to preferred tone with few awkward expressions. 5: Perfectly emulates the users preferred tone and manner; exceptionally natural. Explanation Style: 1: Completely incompatible with the users way of learning. 2: Structure/style does not fit, making it hard to absorb. 3: Understandable but requires more effort than preferred. 4: Mostly preferred and easy to follow; minor clarity improvements possible. 5: Exactly the methods/expressions easiest for the user (e.g., analogies, summaries, bullets). Based on the rubric above, review how the user evaluated responses for the query and derive the users personalized evaluation rubric. Provide your result strictly between <BEGIN ANALYSIS> and <END ANALYSIS> in the format below: - Provide concise, actionable personalized Rubric for the user for each criterion. You SHOULD provide all 5 scores for each criterion. - > (1) Personalized rubric with 15 scores for each criterion <BEGIN ANALYSIS> (1) Personalized rubric with 15 scores for each criterion 26 Preprint. Table 17: The prompt for Ep Prompt for Ep You are evaluating responses exactly like the specific human who wrote the examples. Replicate their preferences, strictness/leniency, tone, and feedback style precisely, including average score levels and feedback length from the examples. First, recall the gold information need as the users underlying intent, and use it as the primary reference for all judgments. Use the rubric below for scoring each criterion from 1 to 5 (Only integers are allowed). For each criterion, think step-by-step: (1) Identify key elements from the response, (2) Compare to examples and gold information need, (3) Assign score based on rubric, (4) Provide concise feedback mirroring example style. Personalized rubric for this user and query: {gold rubric} Instructions: - Personalize judgments to match the examples exactly; if patterns show leniency or strictness on any criterion (e.g., tone or interest alignment), apply similarly across all evaluations while referencing average scores from examples. - Be concise and actionable in feedback. Mirror the example evaluators language, politeness level, and any emojis precisely. - First, think step-by-step for each criterion between <think> and </think> tags, write your thoughts. Then, provide the score and feedback. <USER INPUT> Query: {query} Gold Information Need: {gold information need} <END USER INPUT> <EXAMPLES> Response: {response in R-J pair} Needs Alignment Score: {needs alignment score in R-J pair} Needs Alignment Feedback: {needs alignment feedback in R-J pair} Content Depth Score: {content depth score in R-J pair} Content Depth Feedback: {content depth feedback in R-J pair} Tone Score: {tone score in R-J pair} Tone Feedback: {tone feedback in R-J pair} Explanation Style Score: {explanation style score in R-J pair} Explanation Style Feedback: {explanation style feedback in R-J pair} ... <END EXAMPLES> <EVALUATE USER INPUT> Response: {new response} <THINK> 27 Preprint. Table 18: The prompt for feedback meta-evaluation Prompt for feedback meta-evaluation You are evaluating whether the Generated Feedback evaluates the responses {criterion} the way the Human Feedback author would. Judge human-likeness by checking if the Generated Feedback: - Generally reflects the Human Feedbacks perspective: main priorities, focus areas, and overall severity/leniency - Preserves the key issues and praise emphasized by the human; minor differences or additional nuance are acceptable if they do not distort the humans intent - Is mostly accurate and grounded in the Query, Gold Information Need, and Response - If the Generated Feedback reasonably emulates the human evaluator overall, even with minor differences, respond with O. Context Query: {query} Gold Information Need: {gold information need} Response: {response text} Rubric: {gold rubric} Human Feedback (reference): {human feedback} Generated Feedback (to judge): {generated feedback} Decision - If the Generated Feedback reasonably emulates the human evaluator overall, preserving the main intent and tone even with minor differences, respond with O. X. Respond with only one word: or X. Table 19: The prompt for generating static profile Prompt for generating static profile You are to produce user profile that helps with information-seeking tasks. Information-seeking tasks aim to address users information need by providing the desired information in an appropriate form. Analyze the users full search and chat history and infer preferences which help satisfy future personalized information seeking task for the user. Based on the users preferences, describe the users profile in as much detail as possible that would help satisfy future information seeking tasks. Express needs and preferences without relying on demographic labels (e.g., nationality, ethnicity, region); ground all claims in observable behaviors and content/format preferences. Structure the profile to maximize usefulness for information seeking: <OUTPUT GUIDELINES> - Specify overall interests (broad and specific) - For each interest: 1) frequently explored subtopics and focal aspects (from questions and follow-ups); 2) preferred presentation signals (organization, content depth, medium, tone, explanaition style) inferred from behavior; 3) personalization takeaways and actionable guidance (Treat language usage as behavior (not identity); avoid implying nationality or region) 28 Preprint. Table 20: The prompt for selecting relevant history Prompt for selecting relevant history You are strict reviewer selecting history sessions that help personalize the response to THIS query. Given the users query and ONE history session (a chat file or single search line), decide if this session contains concrete, query-specific personalization signals. Explicit rule: If the session is related to the query and will help personalize the response to that query, you MUST accept it (set keep=true). If not, reject. Keep ONLY if the session includes user-specific signals that would concretely change what the assistant says for this exact query: stable preferences (likes/dislikes), explicit constraints (budget/time/access), background (role/skill/industry), environment/config, or prior choices relevant to this topic. If the history session is past web search, it contains ONLY the search query string; judge relevance based on that query text alone (ignore any page content). Important: Accept indirectly related but decision changing constraints as adjacent constraints. For example, for game recommendation query, searches like GeForce Now, cloud gaming, Mac gaming, Steam Deck, low-end laptop, controller support, or RTX 3050 vs 3060 indicate platform/performance/input constraints that filter or change recommended titles. Classify such items as adjacent constraint and keep them. Reject off-topic content or history unrelated to this query topic. <Query>: {query} <History Session>: {session text} Table 21: The prompt for generating dynamic profile Prompt for generating dynamic profile You are personalization profiler. Goal: Create per-query personalized guidance on each criterion. <QUERY> <SEARCH HISTORY> <CHAT HISTORY> <INSTRUCTIONS> Do NOT default to general preferences. Extract query-topic-specific signals from the users history: Repeated subtopics and facets specific to THIS querys topic (subcategories, approaches, features, constraints). Format/style cues: preferred structure (checklists/templates/stepwise), concision vs detail, examples, references. Negative/avoid cues: content types they ignore/dislike, off-topic tangents, categories or formats they avoid. Determine domain fit: professional/work vs hobby/personal, using cues in the query and history. If professional/work: prioritize history mentioning projects, workplace tools/stacks, teams/clients, deadlines, compliance; de-prioritize hobby-only signals. If hobby/personal: prioritize history about personal projects, de-prioritize enterprise/production requirements. If mixed or ambiguous: assess the querys complexity/difficulty level to determine domain - complex/technical queries suggest professional context, simpler and easier queries suggest hobby/personal context. Selectively cite only history aligned with the inferred domain; ignore high-frequency but off-domain signals. <OUTPUT GUIDELINES> For EACH criterion (Need Alignment, Content Depth, Tone, Explanation Style), generate comprehensive, highly detailed profile tailored to THIS specific query using only the most relevant history signals. Provide extensive detail on what to emphasize: include specific topics, subtopics, approaches, methodologies, tools, frameworks, and examples that align with the users demonstrated interests. Elaborate thoroughly on what to avoid: specify particular content types, tangents, approaches, terminology levels, or presentation styles that the user has shown disinterest in or actively filtered out. Detail exactly how to deliver content: specify preferred formatting (bullet points, numbered lists, code blocks, tables), structural organization (step-by-step vs. overview-first vs. problem-solution), level of technical depth, use of examples, and pacing of information. Specify the reasoning behind each profile choice: explain why certain approaches work better for this user based on their demonstrated patterns and preferences. Preprint. Table 22: The prompt for selecting oracle history Prompt for selecting oracle history You are strict reviewer selecting history sessions that help personalize the response to THIS query. Given the users query and ONE history session (a chat file or single search line), decide if this session contains concrete, query-specific personalization signals. If GOLD RUBRIC is provided, treat it as authoritative ground truth for this querys personalization targets. Favor sessions that directly instantiate, confirm, or constrain the rubric. Explicit rule: If the session is related to the query and will help personalize the response to that query, you MUST accept it (set keep=true). If not, reject. Keep ONLY if the session includes user-specific signals that would concretely change what the assistant says for this exact query: stable preferences (likes/dislikes), explicit constraints (budget/time/access), background (role/skill/industry), environment/config, or prior choices relevant to this topic. If the history session is past web search, it contains ONLY the search query string; judge relevance based on that query text alone (ignore any page content). Important: Accept indirectly related but decision-changing constraints as adjacent constraints. For example, for game recommendation query, searches like GeForce Now, cloud gaming, Mac gaming, Steam Deck, low-end laptop, controller support, or RTX 3050 vs 3060 indicate platform/performance/input constraints that filter or change recommended titles. Classify such items as adjacent constraint and keep them. Reject off-topic content or history unrelated to this query topic. Query: {query} Gold Rubric: {gold rubric} History Session:{history session} ... Table 23: The prompt for CoT and Pseudo-history expansion in history retrieval Prompt for CoT expansion You are retrieval assistant for user history (chat + search). Given the original query, think step by step to infer the key aspects that seem important for retrieving relevant past sessions. Query: {query} No bullets, no extra text. Prompt for Pseudo-history expansion You are retrieval assistant for user history (chat + search). You are generating piece of the users past history entry (it can be either chat message to an assistant or search query with notes). The generated text should sound like something the user actually wrote before, and it must be highly useful for retrieving relevant history sessions. Guidelines: Write in natural style as if it were truly authored by the user. It may look like chat utterance (full sentence, conversational) or search query (short and keyword-based). Output ONLY the history text. Query: {query} History:"
        }
    ],
    "affiliations": [
        "Yonsei University"
    ]
}