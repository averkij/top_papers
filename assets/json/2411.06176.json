{
    "paper_title": "M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework",
    "authors": [
        "Yew Ken Chia",
        "Liying Cheng",
        "Hou Pong Chan",
        "Chaoqun Liu",
        "Maojia Song",
        "Sharifah Mahani Aljunied",
        "Soujanya Poria",
        "Lidong Bing"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at https://multimodal-documents.github.io."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 1 6 7 1 6 0 . 1 1 4 2 : r M-LONGDOC: BENCHMARK FOR MULTIMODAL SUPER-LONG DOCUMENT UNDERSTANDING AND RETRIEVAL-AWARE TUNING FRAMEWORK Yew Ken Chia 1, Maojia Song Liying Cheng 1 Hou Pong Chan 1 Chaoqun Liu 1, 2 Sharifah Mahani Aljunied 1 Soujanya Poria Lidong Bing"
        },
        {
            "title": "Singapore University of Technology and Design",
            "content": "1 DAMO Academy, Alibaba Group, Singapore 2 Nanyang Technological University, Singapore {yewken chia, maojia song, sporia}@mymail.sutd.edu.sg {houpong.chan, chaoqun.liu, mahani.aljunied}@alibaba-inc.com {chengliying9581, binglidong}@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct training corpus in fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at https://multimodal-documents.github.io"
        },
        {
            "title": "INTRODUCTION",
            "content": "The ability to comprehend long and complex multi-modal documents and respond to user queries about them is crucial in various practical applications such as business intelligence analysis, academic literature review, and legal research (Mathew et al., 2020). Recently, large multimodal models such as GPT-4V (OpenAI, 2023) have shown great potential in processing and analyzing diverse types of information, including text, images, and even structured data (Huang et al., 2024b). These models offer the promise of automating tasks that traditionally required extensive human effort, such as document analysis, information retrieval, and question-answering (Fujitake, 2024). However, real-world documents often present significant challenges due to their length, complexity, and multimodal nature, containing mix of text, figures, tables, and charts (Faysse et al., 2024). Thus, it is not clear whether current models are capable of an in-depth understanding of lengthy multimodal documents. On the other hand, while existing benchmarks have fostered great progress in document understanding, they often fall short in representing these challenges, typically focusing Yew Ken and Chaoqun are students under the Joint PhD Program between Alibaba and their corresponding university. Work done while Liying was at Alibaba. Corresponding authors. 1 Pages Tokens In-Depth DocVQA ChartQA InfoVQA TAT-DQA VisualWebBench PWC MP-DocVQA DUDE SlideVQA MMLongBench Ours 1.0 1.0 1.2 1.1 1.0 12 8.3 5.7 20.0 47.5 210.8 151.5 236.9 288.0 577.0 452.4 7000 2026.6 1831.5 151.5 2030.5 120988.0 (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) Figure 2: Comparison of benchmarks along three dimensions: the number of pages per document, the number of tokens per document, and the nature of the responses required. Specifically, we assess whether each benchmark emphasizes in-depth, comprehensive answers or focuses on short or extractive responses. Figure 1: Data distribution of document topics in our M-LongDoc benchmark. on documents with less than 50 pages, and limited to simpler extraction-based questions (Ma et al., 2024). To address these limitations, we introduce M-LongDoc, comprehensive benchmark consisting of 851 samples specifically designed to evaluate the performance of large multimodal models on lengthy and diverse documents. Unlike previous datasets (Mathew et al., 2020; Liu et al., 2024; Ma et al., 2024) that mainly contain short documents, M-LongDoc features recent documents spanning hundreds of pages, encompassing wide range of topics and document structures as shown in Figures 1 and 2. In addition, as shown in Figure 3, our benchmark goes beyond simpler extractive questions, requiring models to provide open-ended solutions that demonstrate in-depth understanding of the document content (Fan et al., 2019). M-LongDoc poses question answering task where models have to analyze and reason over texts, figures, or tables in each multimodal long document. We aim for this benchmark to serve as valuable resource for researchers and practitioners, enabling more rigorous testing and development of multimodal document understanding systems. Another challenge we have to overcome is that due to the lengthy content of multimodal documents and the in-depth solutions, the evaluation of the open-ended question-answering task becomes tricky. To assess such open-ended solutions in scalable and standardized manner, we design an automated evaluation framework that does not require reference answers or human annotation. Inspired by previous works in model-based evaluation (Zheng et al., 2023; Zhao et al., 2024; Liu et al., 2023c), our evaluation framework leverages detailed evaluation guide and multiple judge models to score the correctness of each generated solution. With our proposed M-LongDoc and evaluation framework, we conducted preliminary study on existing models and the results show that they struggle with figure and table-based questions compared to text-based questions, revealing their multimodal bias and weaknesses (Chen et al., 2024b). Furthermore, we observed that the models can be easily distracted by irrelevant content in the document pages (Shi et al., 2023), even with the aid of retrieval-augmented generation. To enhance the robustness of multimodal models against potentially irrelevant retrieved content, we propose retrievalaware tuning approach for multimodal document reading. This framework unifies supervised finetuning and retrieval augmented generation by including distracting content from other modalities and pages in each document. Thus, we adapt models to effectively incorporate the domain knowledge in multimodal documents while ignoring the content irrelevant to the given query. Compared to existing training methods, ours is the first to address retrieval-augmented multimodal training for rich document layouts. To support this training framework and the enhancement of open-source models, we further construct training corpus of 10,070 samples for question-answering over multi2 Figure 3: Example questions in different multimodal document question answering benchmarks. For illustration, we include content from the relevant page in the original document. The example question from M-LongDoc is more complex than those from other benchmarks, as it requires an explanatory answer rather than an extraction of short text span. Furthermore, it requires the model to understand the semantics of both image and text. Please note that in our benchmark setting, the model is provided with all page contents from the document, and not only the relevant page. modal long documents. Experiments show that our approach achieves 4.6% relative improvement in the correctness of model responses, compared to the baseline model. The key contributions of this work are threefold: 1) We introduce M-LongDoc, multimodal benchmark that more accurately represents the challenges of real-world document understanding tasks. Our automated evaluation framework enables scalable and standardized assessment of open-ended solutions. 2) Our evaluation of leading models indicates that most models struggle with figure and table-based questions compared to text-based questions, revealing their multimodal bias. 3) We propose retrieval-aware tuning framework that together with our large-scale training corpus, significantly improves the efficiency and effectiveness of multimodal document reading. Thus, we believe that this work contributes to the field of document understanding and paves the way for more capable and practical applications of large multimodal models in real-world scenarios. To accelerate the studies in our community, we will make the M-LongDoc benchmark, the training corpus for multimodal document reading, and our source code publicly available."
        },
        {
            "title": "2 M-LONGDOC BENCHMARK",
            "content": "To evaluate the multimodal long document understanding ability of existing models, we present MLongDoc, challenging and diverse benchmark. Notably, the benchmark focuses on open-ended questions that require in-depth solutions and analysis over very long documents with more than 200 pages on average. For diversity, the questions cover the academic, financial, and product domains, with multiple topics in each domain. 2.1 DATA COLLECTION To support our evaluation benchmark, we manually source high-quality multimodal documents from publicly accessible sources. Concretely, we source research papers1, company reports2 and product instruction manuals3 for the academic, financial, and product domains respectively. Thus, the 1https://arxiv.org 2https://www.annualreports.com 3https://www.manualslib.com 3 Figure 4: Overview of our data construction process with question verification stages. For brevity, we shorten the checklist prompts and include the full details in Appendix A.1. dataset covers range of document formats and domains. As research papers often require domain expertise, we constrain the academic domain to computer science topics. To reduce the risk of data contamination or memorization when evaluating existing models (Dong et al., 2024), we constrain the documents to be published in January 2024 or later. As most existing models are unable to process raw PDF files, we conduct simple data processing to extract the texts and relevant images from each document. Specifically, we use the PyMuPDF4 tool to automatically extract the text from each page. To extract the figures and tables from each page, we leverage an existing object detection model (Pfitzmann et al., 2022). Thus, the processed documents consist of interleaved textual and visual content, where the visual contents are extracted images of figures and tables. 2.2 QUESTION GENERATION To construct diverse and challenging open-ended questions, we leverage semi-automated pipeline. Concretely, as shown in Figure 4, given specified content category, we first randomly select page from the document that contains the specific content category, such as texts, tables, or figures. Consequently, we randomly select question generator from pool of leading multimodal models and instruct it to generate challenging question based on the document page. To ensure that the question generator has sufficient context, we also provide the previous page and subsequent page as additional inputs during the question generation process. To improve the quality of the generated questions, we conduct an automated verification process as preliminary filter for unsuitable questions. Concretely, the question generator is also instructed to reflect on the generated question and follow multi-step checklist to validate the question. For example, the checklist includes checking if the question is relevant to the document page, if the specified content category is required to answer the question, and whether the question is answerable. The question is rejected if it does not satisfy any condition in the checklist. Lastly, we employ team of annotators to conduct final validation for each question. We employ expert annotators who are Ph.D. students and above in computer science for the academic domain, and professional annotators for the finance and product domains. To be consistent, we provide similar checklist and instruction as our automated verification stage, and the annotation details are included in Appendix A.1. We found that 80.1% of the generated questions satisfied the automated verification. Of these questions that passed automated verification, 80.9% also satisfied the human verification. Thus, we only retain 851 questions that satisfied both the automated and human verification. 4https://pymupdf.readthedocs.io Figure 5: Our automated evaluation framework to assess the correctness of open-ended solutions for multimodal question answering. The full evaluation guide is included in Appendix A.3. The statistics of our benchmark dataset are shown in Table 1, where we ensure balanced distribution of questions and documents from each domain and question category. In this work, we focus on questions that require single page of content to answer, and only retain answerable questions. Please also note that while each question focuses on specific category in document page, the page may contain content from other categories as context. For instance, table-based question may also require comparisons to other tables or texts from the same page. Compared to the existing benchmarks in Figure 2, M-LongDoc poses greater challenge in two main aspects. Firstly, the significantly greater number of pages and tokens in each multimodal document poses extreme computational costs and opportunities to be distracted by irrelevant content (Shi et al., 2023). While this challenge may be mitigated by retrieval-augmented generation (Chen et al., 2022), our preliminary study in Section 2.4 shows that existing models are still hindered by their multimodal bias (Chen et al., 2024b). In addition, our benchmark poses challenging open-ended questions as shown in Figure 3, requiring models to produce in-depth analyses in their solutions. Thus, we believe M-LongDoc is more realistic and challenging benchmark compared with existing datasets focusing on short answers that can often be extracted directly from the source document. 2.3 AUTOMATED EVALUATION Given the challenging nature of our multimodal long document benchmark, it is crucial to have scalable and standardized evaluation method. However, it is less feasible to conduct comprehensive human evaluation due to high labour costs and lack of reproducibility (Clark et al., 2021). Thus, inspired by previous works in automatic evaluation (Zheng et al., 2023; Zhao et al., 2024; Liu et al., 2023c), we propose an evaluation framework based on committee of multimodal judges. Concretely, we leverage multiple leading multimodal models to score each answer to question based on the criteria of correctness. To provide clear guideline for evaluation, we define the task introduction and criteria as shown in Figure 5. To provide more detailed evaluation instruction beyond the basic definitions above, we further construct detailed evaluation steps, based on the task and criteria. Thus, the finalized evaluation guide for each judge model consists of the task introduction, criteria, and fixed evaluation steps."
        },
        {
            "title": "Documents\nQuestions",
            "content": "Text-based questions Figure-based questions Table-based questions Average pages per document Average text tokens per document Average figure images per document Average table images per document"
        },
        {
            "title": "Academic\nPaper",
            "content": "60 311 95 114 102 201.2 114,129.8 90.8 34."
        },
        {
            "title": "Product\nManuals",
            "content": "60 279 95 93 91 277.8 109,745.0 368.3 96."
        },
        {
            "title": "Financial\nReport",
            "content": "60 261 81 76 104 153.4 139,089.3 24.1 83."
        },
        {
            "title": "All",
            "content": "180 851 271 283 297 210.8 120,988.0 161.13 71.8 Table 1: Benchmark dataset statistics with respect to each domain. To provide more reliable evaluation and reduce intra-model bias (Verga et al., 2024), we leverage multiple judges to evaluate each candidate answer. Specifically, each judge model Mj is provided with the evaluation guide g, ground-truth evidence page as context c, question q, and candidate answer ˆa, and instructed to assign correctness score from 1 to 5. However, we observe some variance in the output scores, even with the same judge model and inputs. Thus, we sample multiple scores from each judge model Mj and aggregate the scores to obtain fine-grained, continuous score that better reflects the quality of the candidate answer: Score = 1 (cid:88) (cid:88) j=1 k=1 sj,k Mj(g, c, q, ˆa) (1) where = 3 is the number of judge models and = 5 is the number of sampled scores per judge model. While there may be some degree of subjectiveness in our framework, our analysis in later sections shows that it largely agrees with human preferences with minimal bias (Zheng et al., 2023). Thus, we believe this automated evaluation framework is reliable and more scalable. 2.4 PRELIMINARY STUDY Gemini-1.5-pro-002 w/ top = 1 pages w/ top = 5 pages w/ top = 10 pages w/ top = 20 pages Qwen2-VL-7B-Instruct w/ top = 1 pages w/ top = 5 pages w/ top = 10 pages w/ top = 20 pages Text Figure Table All 4.38 4.60 4.61 4.63 3.73 4.31 4.29 4.33 4.16 4.54 4.62 4.38 4.11 4.49 4.51 4. 3.57 4.05 3.78 4.17 3.65 4.08 OOM OOM OOM OOM 3.25 3.67 3.62 3.36 3.46 3.19 Table 2: Preliminary study on M-LongDoc for open-source and close-source models. We report the correctness score out of 5 for text-based, figure-based, table-based, and all questions respectively. To investigate the limitations of existing models, we conduct preliminary study on subset of 100 random samples from our M-LongDoc benchmark. Concretely, we select Gemini (Google, 2024) and Qwen2-VL (Wang et al., 2024) to represent highly capable models for the close-source and open-source settings respectively. While large multimodal models have shown impressive capabilities and support longer input contexts, they often struggle with understanding very long documents and may incur great computational costs (Dingjie et al., 2024). Thus, we focus our study on the retrieval-augmented generation paradigm (Lewis et al., 2020), which leverages retriever to select only the most relevant content, and the retrieved content is used to augment the generator model Figure 6: Our retrieval-aware multimodal tuning framework to enhance the ability of models to identify and utilize pertinent content in multimodal documents. At training time, the model is provided with more relevant pages retrieved from the document, which may contain both the gold evidence page and multiple distractor pages. inputs for question answering. Concretely, we use ColPali (Faysse et al., 2024) as state-of-the-art multimodal retriever and leverage the top pages of multimodal content as context. We include more details on the implementation and evaluation of retriever performance in Appendix A.2. Notably, as shown in Table 2, we observe significantly lower performance for figure-based and tablebased questions, as compared to text-based questions. We believe that this discrepancy suggests that current models are weaker in processing image-based contents in multimodal documents, or may be biased towards the textual content, even when they are trained on interleaved multimodal data (Chen et al., 2024b). Furthermore, we find that increasing the amount of retrieved content may not improve overall performance, and may even lead to worse performance or out-of-memory (OOM) issues. This indicates that the multimodal generator models may be easily distracted by irrelevant content in longer contexts (Shi et al., 2023). Thus, we believe it is crucial to address these challenges of existing models for processing multimodal long documents. Additionally, to verify the reliability of our automated evaluation, we conduct manual human scoring based on the same evaluation guide. For the samples in this preliminary study, we observed Pearson correlation of 88.9% with < 0.001 between the final aggregated score from the judge models, and the human annotator. Thus, we believe that our evaluation framework can achieve very high agreement with human preferences despite the open-ended and in-depth nature of the answers."
        },
        {
            "title": "3 RETRIEVAL-AWARE MULTIMODAL TUNING",
            "content": "While current multimodal models are often trained on interleaved inputs with multiple pieces of texts and images (Liu et al., 2023b), they may not be well-optimized for multimodal documents. Specifically, multimodal documents are more challenging to understand as they contain diverse multimodal content including unstructured texts, and images representing structured tables and figures. Thus, models require more fine-grained understanding and perception of the multimodal document content. Furthermore, the retrieval setting requires models to ground their outputs in the additional context by identifying and utilizing only the pertinent content. However, previous works (Shi et al., 2023) and our preliminary study have shown that they may still be easily misled by some irrelevant information in the retrieved content. To our knowledge, there is no open-source model that can address these challenges in multimodal long documents. To this end, we propose simple and effective retrieval-aware multimodal document tuning approach. Inspired by previous works in retrieval augmentation (Chen et al., 2022; Zhang et al., 2024), we include both retrieval context from the ground-truth evidence page as well as potentially irrelevant pages during training. Our approach as shown in Figure 6 presents training paradigm that is more realistic and similar to the challenges faced during test-time retrieval of multimodal content. Thus, the model learns to handle potentially noisy retrieval contexts, while improving its text understanding and visual perception capabilities to utilize the most relevant document content. 7 To construct the training data, we leverage the same process as shown in Figure 4 to construct training corpus of 10,070 samples across 300 documents, and leverage the respective question generator models to also produce high-quality answer based on each ground-truth evidence page. We omit the human verification stage for scalability and cost-efficiency, as majority of the automatically verified samples also satisfied human verification. To assess the quality of the generated training solutions, we evaluated using our automated scoring framework on random subset of 100 samples. We observed an average correctness score of 4.82, indicating very high quality of answers in the training data. To avoid data leakage, we ensure that the documents used to construct the training corpus do not overlap with the evaluation set. For example, we ensure that the training documents are from different companies and products, and are published in an earlier time period."
        },
        {
            "title": "4.1 TASK SETTING",
            "content": "To ensure practical task setting, we focus on the retrieval-based paradigm, which avoids the exorbitant cost to process the full document. Based on our preliminary study in Section 2.4, we use the top = 5 pages ranked by the retriever as reasonable amount of context for each question. Thus, each model is provided with the retrieved context and question as input, and required to provide an open-ended solution as output. As discussed in Section 2.3, we leverage an automated framework with multiple judge models to score the correctness of each output solution, on scale of 1 to 5. 4.2 MODELS To provide more comprehensive investigation of current models, we use both open-source and close-source models in this work. Concretely, we select GPT-4o (gpt-4o-2024-05-13)5, Claude 3.5 Sonnet (claude-3-5-sonnet-20240620)6 and Gemini 1.5 Pro (gemini-1.5-pro-002) (Google, 2024) due to their leading performance on multimodal benchmarks (Yue et al., 2023). Regarding opensource models, we specifically select models which support interleaved multimodal inputs with multiple images, and fine-grained visual perception of document content. Thus, we mainly focus on LLaVA-OneVision-7B (Li et al., 2024) and Qwen2-VL-7B-Instruct (Wang et al., 2024). We plan to expand our investigation to other capable open-source models as they are released. 4.3 HYPERPARAMETERS For all models, we use greedy decoding with temperature = 0 to reduce variance. In our training framework, we set the number of training epochs to be 1, batch size as 16, and learning rate as 1e-4. To reduce the training cost due to limited computational resources, we leverage LoRA (Hu et al., 2022) training with rank as 64 and alpha as 32. Due to training instabilities with other open-source models, we mainly focus the training experiments on the Qwen2-VL-7B-Instruct model, which demonstrates leading performance compared to similar-sized models."
        },
        {
            "title": "5 RESULTS",
            "content": "5.1 MAIN RESULTS To assess the effectiveness of our approach and the holistic performance of existing models, we report the main evaluation results in Table 3. First, we find that our retrieval-aware multimodal tuning significantly and consistently enhances the performance of Qwen2-VL, representing relative improvement of 4.6% in answer correctness. Thus, we view the proposed training approach as promising strategy to enhance multimodal long document understanding ability, and reduce the gap between open-source and proprietary models. Second, we observe that open-source models have worse performance in answering table-related questions compared to other question categories. This discrepancy highlights the need for more efforts to enhance the table understanding capability 5https://openai.com/index/gpt-4o-system-card/ 6https://www.anthropic.com/news/claude-3-5-sonnet"
        },
        {
            "title": "Question Category",
            "content": "GPT-4o Claude 3.5 Sonnet Gemini 1.5 Pro - - -"
        },
        {
            "title": "Proprietary Models",
            "content": "4.56 4.59 4.66 4.38 4.43 4.43 4.51 4.51 4.43 Open-Source Models LLaVA OneVision Qwen2-VL w/ Retrieval Tuning 7B 7B 7B 3.71 4.03 4.17 3.74 3.88 4.01 3.39 3.56 3.86 4.55 4.57 4.59 4.03 4.08 4. 4.38 4.42 4.43 3.57 3.83 4.00 4.53 4.54 4.52 3.30 3.62 3.77 4.49 4.51 4.51 3.62 3.84 4. Table 3: Evaluation of model performance for proprietary and open-source multimodal models. We report the correctness on our benchmark across different document domains and question categories. We bold the highest scores obtained by open-source models. Model Qwen2-VL w/o Image Inputs w/ Render Page as Inputs Question Category Text 4.08 4.22 3.99 Figure Table 3.83 3.37 3.70 3.62 3.38 3.39 Table 4: Analysis on alternative settings for our benchmark, including removing images from model inputs, and using only the render image of each page as document context, without text extraction. of open-source multimodal models. We include further qualitative analysis of the model predictions in the Appendix A.4. 5.2 EFFECT OF ALTERNATIVE SETTINGS While we mainly focus on the multimodal setting with extracted texts and images, we believe it is also important to explore other settings in practice. Our main data setting as discussed in Section 2.1 first extracts the texts, figures, and tables separately, with the figures and tables represented as individual images. As shown in Table 4, we find significant decrease in performance for figure-based and table-based questions when the image inputs are removed, although the model may still be able to answer the questions to an extent as the extracted text may contain partial information about the tables and figures. We believe that this underscores the importance of leveraging multimodal content in documents, even though many documents may contain majority of the content as texts. The performance increases slightly for text-based questions when image inputs are removed, suggesting that the images may mislead the model in rare cases. However, we believe this is acceptable as questions may cover wide variety of multimodal content in practice. On the other hand, we observe that it may be less optimal to use only rendered images of document pages as inputs, instead of separately extracting the texts, tables, and figures as in our main setting. While the rendered page image does contain the original information and layout of the document, including texts, tables, and figures, the model may be less capable of distinguishing the content between texts and tables."
        },
        {
            "title": "6 RELATED WORK",
            "content": "6.1 LARGE MULTIMODAL MODELS In recent years, large multimodal models have demonstrated their capability to process and comprehend data across various formats. Close-source models such as GPT-4o (AI, 2024) can reason across audio, vision and text. Claude 3.5 Sonnet (Anthropic, 2024) shows marked improvement on tasks that require visual reasoning like interpreting charts and graphs. On the other hand, Gemini 1.5 Pro 9 (Google, 2024) is capable of reasoning over multiple long documents and hours of video and audio. Open-source models such as Llava (Liu et al., 2023a), Idefics (Laurencon et al., 2023), Otter (Li et al., 2023), InternVL (Chen et al., 2024c), CogVLM (Wang et al., 2023), have also shown the potential over various types of multi-modal content including document images (Mathew et al., 2020), slides (Tanaka et al., 2023), and charts (Huang et al., 2024a). However, the benchmark performance of open-source models tends to lag behind that of close-source models (Yue et al., 2023), prompting an urgent need to bridge the gap. In this work, we introduce retrieval-aware multimodal tuning framework which can significantly improve the multimodal long document understanding ability of models."
        },
        {
            "title": "6.2 DOCUMENT UNDERSTANDING DATASETS",
            "content": "Given the practical and business applications of document understanding, researchers have devoted significant effort to this area by introducing new datasets and methods. SearchQA (Dunn et al., 2017), NarrativeQA (Koˇcisk`y et al., 2018), QuALITY (Zhu et al., 2020) are reading comprehension datasets over purely textual data with an average length ranging from 1850 to 60k tokens. FinQA (Chen et al., 2021), DocFinQA (Reddy et al., 2024) are introduced in the financial domain. MarkQA (Huang et al., 2023) tackes QA over knowledge bases with numerical reasoning. DocVQA(Mathew et al., 2020) presents visual question answering dataset on document images. VisualWebBench (Liu et al., 2024) is multimodal benchmark over single-page documents focusing on various QAstyle tasks. MMLongBench (Ma et al., 2024) is multimodal document understanding dataset with an average of 47.5 pages and 21k textual tokens. Methods such as PDFTriage (Saad-Falcon et al., 2023) enables models to retrieve the context from long and structured documents. TAT-LLM (Zhu et al., 2024) addresses QA over hybrid of tabular and textual data. ChartQA (Masry et al., 2022) is benchmark of extractive questions-answering task over chart image, while the Chocolate dataset (Huang et al., 2024b) annotates the types of factual errors in machine-generated chart captions. Compared to the datasets above, our M-LongDoc benchmark contains more lengthy documents with hundreds of pages and focusing on open-ended questions which require in-depth solutions. We further propose an automated and reliable evaluation framework to assess the correctness of model answers, which demonstrates very high agreement with human preferences. 6.3 RETRIEVAL-AUGMENTED GENERATION While recent multimodal models have shown impressive capability in many tasks, applying them directly to long document understanding tasks may face several challenges to the diverse multimodal content. Additionally, processing entire documents with large models is often impractical, as the text alone may contain millions of tokens, leading to substantial computational costs. Therefore, researchers have designed various retrieval augmented generation (Lewis et al., 2020; Chen et al., 2022) methods to address the issues. In this work, we have investigated multiple retrieval methods optimized for document page retrieval, including JINA-CLIP (Xiao et al., 2024) BM25 (Robertson & Zaragoza, 2009), BGE-M3 (Chen et al., 2024a) and ColPali (Faysse et al., 2024). However, despite using retrieval, existing multimodal models are constrained by their multimodal biases (Chen et al., 2024b) and susceptibility to irrelevant content that is inherent in the retrieved context (Shi et al., 2023). Thus, we further proposed retrieval-aware tuning framework to enhance the performance of models when leveraging retrieval for multimodal long documents."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we introduce M-LongDoc, benchmark dataset consisting of 851 samples and an automated framework to evaluate the performance of large multimodal models on document question answering tasks. This benchmark is specifically designed for long and diverse document formats containing text, figures, and tables, aligning with the demands of real-world applications. Unlike existing benchmarks, M-LongDoc features more recent and lengthy documents, often hundreds of pages long, and requires open-ended solutions rather than just extractive answers. We also propose retrieval-aware tuning approach designed for the efficient and effective processing of multimodal long documents. To our knowledge, this is the first training framework and model to directly address the retrieval setting for such documents. Experimental results demonstrate that our 10 method achieves relative improvement of 4.6% in the correctness of model responses compared to baseline open-source models. This improvement showcases the effectiveness of our approach in handling lengthy and complex multimodal documents, potentially aiding humans in various business and practical applications that require understanding and answering questions over such documents."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This work was substantially supported by DAMO Academy through DAMO Academy Research Intern Program."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "We will release the benchmark and training dataset publicly to facilitate further research in this area. To observe copyright rules, we do not release the documents directly, but instead the links to download each document. All annotators in this work were volunteers. While we focus on how models may answer questions based on multimodal documents, it is still possible for them to hallucinate information that is false or not verifiable."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "In this work, we have included the details of our training framework and hyperparameters in Section 3 and 4. As discussed above, our benchmark dataset and questions will be released under public licence. For reproducibility, our code will be found at https://anonymous.4open. science/r/private-multimodal-documents-B2CF/."
        },
        {
            "title": "REFERENCES",
            "content": "Open AI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024a. URL https://arxiv.org/abs/2402.03216. Meiqi Chen, Yixin Cao, Yan Zhang, and Chaochao Lu. Quantifying and mitigating unimodal biases in multimodal large language models: causal perspective, 2024b. URL https://arxiv. org/abs/2403.18346. Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. MuRAG: Multimodal retrievalaugmented generator for open question answering over images and text. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 55585570, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.375. URL https://aclanthology.org/2022.emnlp-main.375. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024c. URL https://arxiv.org/abs/2404.16821. Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matthew I. Beane, Ting-Hao Kenneth Huang, Bryan R. Routledge, and William Yang Wang. Finqa: dataset of numerical reasoning over financial data. In Proceedings of EMNLP, 2021. URL https://api.semanticscholar.org/CorpusID:235399966. 11 Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. All thats human is not gold: Evaluating human evaluation of generated text. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 72827296, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.565. URL https://aclanthology.org/2021.acl-long.565. Song Dingjie, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking MLLMs in long context. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=Uhwze2LEwq. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. Generalization or memorization: Data contamination and trustworthy evaluation for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 1203912050, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.716. URL https://aclanthology.org/2024.findings-acl.716. Matthew Dunn, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, and Kyunghyun Cho. Searchqa: new q&a dataset augmented with context from search engine. arXiv preprint arXiv:1704.05179, 2017. URL https://arxiv.org/pdf/1704.05179. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: In Anna Korhonen, David Traum, and Lluıs M`arquez (eds.), Long form question answering. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 35583567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/ v1/P19-1346. URL https://aclanthology.org/P19-1346. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Celine Hudelot, Colpali: Efficient document retrieval with vision language modArXiv, abs/2407.01449, 2024. URL https://api.semanticscholar.org/ and Pierre Colombo. els. CorpusID:270870444. Masato Fujitake. LayoutLLM: Large language model instruction tuning for visually rich document understanding. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 1021910224, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology. org/2024.lrec-main.892. Gemini Team Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Kung-Hsiang Huang, Hou Pong Chan, Yi Ren Fung, Haoyi Qiu, Mingyang Zhou, Shafiq Joty, ShihFu Chang, and Heng Ji. From pixels to insights: survey on automatic chart understanding in the era of large foundation models. CoRR, abs/2403.12027, 2024a. doi: 10.48550/ARXIV.2403. 12027. URL https://doi.org/10.48550/arXiv.2403.12027. Kung-Hsiang Huang, Mingyang Zhou, Hou Pong Chan, Yi Fung, Zhenhailong Wang, Lingyu Zhang, Shih-Fu Chang, and Heng Ji. Do LVLMs understand charts? analyzing and correcting factual errors in chart captioning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 730749, Bangkok, Thailand and virtual meeting, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.findings-acl.41. URL https://aclanthology.org/2024.findings-acl.41. 12 Xiang Huang, Sitao Cheng, Yuheng Bao, Shanshan Huang, and Yuzhong Qu. Markqa: large scale kbqa dataset with numerical reasoning. In Proceedings of EMNLP, 2023. URL https: //api.semanticscholar.org/CorpusID:264439563. Tomaˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. TACL, 6:317328, 2018. Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 94599474. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2020/ 2020. file/6b493230205f780e1bc26945df7481e5-Paper.pdf. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning, 2023. URL https://arxiv.org/ abs/2305.03726. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. URL https: //arxiv.org/abs/2408.03326. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023a. URL https://arxiv.org/abs/2304.08485. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023b. URL https: //openreview.net/forum?id=w0H2xGHlkw. Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal LLMs evolved in web page understanding and grounding? In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=egVSgtJJAx. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 25112522, Singapore, December 2023c. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.153. URL https://aclanthology.org/ 2023.emnlp-main.153. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiao wen Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, and Aixin Sun. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. ArXiv, abs/2407.01523, 2024. URL https://api.semanticscholar. org/CorpusID:270870089. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. URL https://aclanthology. org/2022.findings-acl.177. 13 Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. V. Jawahar. Docvqa: dataset for vqa on document images. 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 21992208, 2020. URL https://api.semanticscholar.org/ CorpusID:220280200. OpenAI. Gpt-4v(ision) system card, 2023. URL https://api.semanticscholar.org/ CorpusID:263218031. Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. Doclaynet: large human-annotated dataset for document-layout segmentation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 22, pp. 37433751, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393850. doi: 10.1145/3534678.3539043. URL https://doi.org/10.1145/3534678.3539043. Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, and Chris Tanner. Docfinqa: longIn Proceedings of ACL, 2024. URL https://api. context financial reasoning dataset. semanticscholar.org/CorpusID:266999305. Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3:333389, 2009. URL https://api.semanticscholar. org/CorpusID:207178704. Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, Ryan Rossi, and Franck Dernoncourt. Pdftriage: question answering over long, structured documents. arXiv preprint arXiv:2309.08872, 2023. URL https://arxiv.org/pdf/2309.08872. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Scharli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on multiple images. Proceedings of the AAAI Conference on Artificial Intelligence, 37(11):1363613645, Jun. 2023. doi: 10. 1609/aaai.v37i11.26598. URL https://ojs.aaai.org/index.php/AAAI/article/ view/26598. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating llm generations with panel of diverse models, 2024. URL https://arxiv. org/abs/2404.18796. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. URL https://arxiv.org/abs/2409. 12191. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. Han Xiao, Georgios Mastrapas, and Bo Wang. Jina CLIP: Your CLIP model is also your text retriever. In Multi-modal Foundation Model meets Embodied AI Workshop @ ICML2024, 2024. URL https://openreview.net/forum?id=lSDkG98goM. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. Tianjun Zhang, Shishir Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gonzalez. RAFT: Adapting language model to domain specific RAG. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=rzQGHXNReU. 14 Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Deli Zhao, and Lidong Bing. Auto arena of llms: Automating llm evaluations with agent peer-battles and committee discussions, 2024. URL https://arxiv.org/abs/2405.20267. Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 4659546623. Curran Associates, Inc., 2023. https://proceedings.neurips.cc/paper_files/paper/2023/file/ URL 91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks. pdf. Fengbin Zhu, Ziyang Liu, Fuli Feng, Chao Wang, Moxin Li, and Tat-Seng Chua. Tat-llm: specialized language model for discrete reasoning over tabular and textual data. arXiv preprint arXiv:2401.13223, 2024. Ming Zhu, Aman Ahuja, Da-Cheng Juan, Wei Wei, and Chandan Reddy. Question answering In Findings of the Association for Computational Linguiswith long multiple-span answers. tics: EMNLP 2020, pp. 38403849, 2020. URL https://aclanthology.org/2022. naacl-main.391.pdf."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DATA VERIFICATION To verify each question in our data construction process, we use the following guide to prompt the question generator models for automated verification. Similarly, we use the same guide for human annotation in the human verification stage. Based on the document content and question, answer yes or no only to the following questions: 1. Does the content contain any {category}? 2. Does the question require information from the {category}? 3. Is the question clear and answerable based on the {category}? 4. Is the question of reasonable difficulty and answer cannot be simply copied? Where {category} refers to table or figure or text, which is denoted with the question. Note: If questions require general knowledge or commonsense in addition to the content, it is still acceptable. In the document PDF file, each question is shown with the ID corresponding to excel sheet, and the document page as image In the excel sheet, indicate yes or no for each check. A.2 RETRIEVAL METHODS To support our retrieval-based document question answering setting, we currently include four stateof-the art methods to retrieve relevant pages based on each question. They include text-based sparse methods such as BM25 (Robertson & Zaragoza, 2009) embedding-based methods such as BGE-M3 (Chen et al., 2024a), multimodal piece-wise embedding methods such as JINA-CLIP (Xiao et al., 2024), and multimodal page-wise embedding methods such as ColPali (Faysse et al., 2024). Note that piece-wise embedding methods separate encode each piece of text, table image, or figure image, whereas page-wise methods can encode the entire page content as single image. Thus, we rank each page in the document based on the similarity score or relevance score of that page with respect to the given question. As each page may have multiple pieces of content, we consider the highest score of all pieces in page to be the page-wise relevance score. To compare the effectiveness of each method, we implement standardized MRR score which refers to the mean reciprocal rank of the gold evidence page for each question. Based on the results in Table 5, we find that ColPali which encodes each page as single image shows the best performance. Thus, we select ColPali as the preferred retrieval method in our main experiments."
        },
        {
            "title": "Retriever",
            "content": "BM25 CLIP BGE-M3 ColPali"
        },
        {
            "title": "Text",
            "content": "56.2 57.1 66.4 68."
        },
        {
            "title": "Figure Table All",
            "content": "31.2 37.9 36.4 67.5 42.0 50.4 53.6 65.9 43.1 48.5 52.1 67.4 Table 5: Retriever performance comparison. A.3 EVALUATION GUIDE To evaluate each model answer, we use the following scoring guide. Similarly, we use the same guide for human annotation in our analysis. You will be given one response to question based on multimodal document containing texts, figures, or tables. Your task is to rate the response on correctness using 1-5 scale. Please read and understand these instructions carefully, and keep them open for reference while reviewing. Correctness (1-5) refers to how accurately, comprehensively, and appropriately the response addresses the question based on the information in the document. 5 - Fully Correct: Completely accurate, comprehensive, fully integrates relevant information from all parts of the document, and provides coherent answer. 4 - Mostly Correct: Largely accurate with only minor errors or omissions, addresses most main points, and integrates information well. 3 - Partially Correct: Contains mix of accurate and inaccurate information, addresses some key points but misses others, and partially integrates information. 2 - Mostly Incorrect: Has multiple inaccuracies, addresses only small portion correctly, and shows minimal integration of information. 1 - Completely Incorrect: Contains significant errors, is irrelevant, or fails to address the question based on the document. Evaluation Steps: 1. Thoroughly review the multimodal document and question. 2. Carefully read the response, comparing it to the document information. 3. Assess the responses accuracy, comprehensiveness, and relevance. 4. Assign correctness score from 1 to 5 based on the criteria. Question: question Response: answer Evaluation Form (score only without explanation) Correctness: A.4 MORE EXAMPLES A.4.1 EXAMPLE OF M-LONGDOC Table 6 illustrates an example of challenging question in our M-LongDoc benchmark. This question tests the ability of the model to identify and analyze trends across different charts and draw meaningful comparisons. A.4.2 CASE STUDY OF RETRIEVAL-AWARE TUNING Table 7 displays sample question in M-LongDoc and the answers generated by Qwen2-VL and Qwen2-VL w/ Retrieval-aware Tuning. The answer generated by Qwen2-VL states that the Cosine method consistently shows the highest latent cosine similarity across all datasets, which is incorrect. In fact, the zero-shot stitching experiment does not involve the Cosine method. It appears that Qwen2-VL may have been misled by the keyword cosine appearing elsewhere in the retrieved context. In contrast, the answer generated by Qwen2-VL w/ Retrieval-aware Tuning correctly identifies that the affine method consistently obtains the highest latent cosine similarity (lcos) across all datasets. This demonstrates the effectiveness of our Retrieval-aware Tuning method in improving the models capability to comprehend retrieved context. 16 Question: How does the relationship between reference length percentile and the percentage of empty modes differ from the relationship between reference sentence length percentile and the probability of empty context? Explain the key differences in the trends shown by these two graphs. Relevant page (truncated): Table 6: An example of challenging question from M-LongDoc that requires the model to compare the trends of two charts in document. 17 Question: Which method consistently shows the highest latent cosine similarity (lcos) across all datasets (MNIST, F-MNIST, CIFAR-10, CIFAR-100) in the zero-shot stitching experiment, and what might this imply about the nature of transformations between latent spaces in autoencoders? Answer generated by Qwen2-VL: The method that consistently shows the highest latent cosine similarity (lcos) across all datasets (MNIST, F-MNIST, CIFAR-10, CIFAR-100) in the zero-shot stitching experiment is the Cosine method. This implies that the Cosine method is able to capture the most similar latent spaces across different architectures, modalities, and datasets. This suggests that the nature of transformations between latent spaces in autoencoders is not exclusively correlated by orthogonal transformations, but rather by class of transformations larger than orthogonal transformations. This finding highlights the importance of understanding and modeling the particular class of transformations that control the correlation between NNs during image autoencoding. Answer generated by Qwen2-VL w/ Retrieval-aware Tuning: The method that consistently shows the highest latent cosine similarity (lcos) across all datasets (MNIST, F-MNIST, CIFAR-10, CIFAR-100) in the zero-shot stitching experiment is the affine method. This method achieves the highest lcos values for all datasets, with scores of 0.94 for MNIST, 0.94 for F-MNIST, 0.96 for CIFAR-10, and 0.96 for CIFAR-100. This consistency in high lcos values suggests that the affine transformation is particularly effective in aligning the latent spaces of different autoencoders. The high lcos indicates strong alignment between the latent representations of the autoencoders, which implies that the affine transformation is capable of capturing the underlying structure and relationships between the latent spaces more accurately than other methods. The results imply that the nature of transformations between latent spaces in autoencoders is not exclusively controlled by orthogonal transformations. The affine transformation, which includes scaling, rotation, and translation, appears to be more versatile and effective method for aligning these spaces. This finding suggests that the latent spaces of image autoencoders are related by broader class of transformations, including those that are not strictly orthogonal, which is significant insight into the nature of these transformations and their implications for model stitching and zero-shot learning. Relevant page (truncated): Table 7: Sample answers generated by Qwen2-VL and Qwen2-VL w/ Retrieval-aware Tuning, respectively."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group, Singapore",
        "Nanyang Technological University, Singapore"
    ]
}