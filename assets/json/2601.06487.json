{
    "paper_title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
    "authors": [
        "Qiang Zhang",
        "Boli Chen",
        "Fanrui Zhang",
        "Ruixue Ding",
        "Shihang Wang",
        "Qiuchen Wang",
        "Yinfeng Huang",
        "Haonan Zhang",
        "Rongxiang Zhu",
        "Pengyong Wang",
        "Ailin Ren",
        "Xin Li",
        "Pengjun Xie",
        "Jiawei Liu",
        "Ning Guo",
        "Jingren Zhou",
        "Zheng-Jun Zha"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 1 7 8 4 6 0 . 1 0 6 2 : r Technical Report Tongyi DeepResearch ArenaRL: Scaling RL for Open-Ended Agents via Tournamentbased Relative Ranking Qiang Zhang1, Boli Chen1, Fanrui Zhang1, Ruixue Ding1, Shihang Wang1, Qiuchen Wang1 Yinfeng Huang2, Haonan Zhang2, Rongxiang Zhu2, Pengyong Wang2, Ailin Ren2, Xin Li2 Pengjun Xie1, Jiawei Liu, Ning Guo2, Jingren Zhou1, Zheng-Jun Zha 1 Tongyi Lab, Alibaba Group 2 Amap, Alibaba Group"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has substantially improved the performance of large language model (LLM) agents on tasks with verifiable outcomes, such as mathematics and code generation, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within group being compressed into narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To tackle this issue, we propose ArenaRL, reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring comprehensive pipeline covering supervised fine-tuning, RL training, and multi-dimensional automated evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate solutions that are more logically rigorous and robust on complex real-world tasks. The code is available at https://github.com/Alibaba-NLP/qqr."
        },
        {
            "title": "Introduction",
            "content": "The evolution of large language models (LLMs) into autonomous agents marks paradigm shift in artificial intelligence from passive question answering to active problem solving. By integrating long-horizon planning and tool use, such agents have demonstrated substantial potential in handling complex tasks Yao et al. (2022b); Li et al. (2025a); Team et al. (2025). In this progression, reinforcement learning (RL) has played pivotal role, particularly in deterministic tasks such as mathematical reasoning and code generation, where ground-truth labels provide explicit reward signals for optimization Dong et al. (2025); Li et al. (2025b). However, extending RL to open-ended agent tasks in real-world scenarios, such as personalized travel planning or in-depth industry analysis, poses fundamental challenges Li et al. (2025c). In these domains, the solution spaces are vast and Project leader, Corresponding author 1 Technical Report Tongyi DeepResearch Figure 1: (a) Illustration of discriminative collapse in pointwise evaluation: We analyse the rewards of trajectory groups generated for query example during RL training through two evaluation settings. First, the intra-group reward signal represents the pointwise rewards assigned to each trajectory within the group during single-round evaluation. And the intra-group variance, denoted as σgroup, quantifies the degree of variation among different trajectories. Secondly, the N-round noise statistic present the average reward and corresponding scoring noise band for each trajectory across independent evaluation repetitions, from which the noise variance σnoise is estimated. Observations reveal that the evaluation noise variance σnoise is substantial, comparable to the intragroup variance σgroup. This results in an extremely low signal-to-noise ratio (SNR), causing genuine advantages to be obscured by noise and hindering effective reinforcement learning optimization. (b) ArenaRL Performance: By shifting from pointwise scalar scoring to tournament-based relative ranking, ArenaRL significantly outperforms baselines (SFT, GRPO, GSPO) across diverse openended benchmarks. unstructured, and the notion of correctness is inherently subjective and multi-dimensional, rendering traditional ground-truth based RL inapplicable Jia et al. (2025). Mainstream RL approaches (e.g., GRPO Shao et al. (2024), GSPO Zheng et al. (2025)) typically assume the existence of verifiable reward function that can provide accurate reward feedback. However, this assumption often breaks down in open-ended settings where no objective ground truth is available. To address the issue of reward acquisition, recent work has adopted the LLM-as-Judge paradigm, assigning pointwise scalar scores to model outputs Viswanathan et al. (2025); Huang et al. (2025); Liu et al. (2025). We identify that this mechanism leads to severe phenomenon that we term discriminative collapse, as shown in Figure 1(a). As the policy is progressively refined, the generated trajectories become increasingly similar in distribution. Consequently, rewards for trajectories within the same group are compressed by the pointwise scoring scheme into narrow range (e.g., 0.80.9 on scale of 1), rendering rewards indistinguishable. Moreover, due to inherent noise in the LLM judge, such as decoding randomness Arias et al. (2025) and length preferences Hu et al. (2024), the reward outcomes exhibit certain degree of unreliability, with low signal-to-noise ratio between the reward signal and the interfering noise. In this situation, the pointwise evaluation mechanism struggles to distinguish truly superior samples. And the RL optimization process is driven more by spurious noise than by meaningful task-specific rewards, leading performance to stagnate or even deteriorate. To fundamentally address discriminative collapse, we draw inspiration from decision theory, where pairwise preference judgments are known to be more stable than pointwise quantitative assessments Fürnkranz & Hüllermeier (2010); Rafailov et al. (2023), and advocate paradigm shift from pointwise scalar scoring to intra-group relative ranking. To this end, we propose ArenaRL, an online policy optimization framework grounded in an adversarial arena. ArenaRL abandons unstable scalar 2 Technical Report Tongyi DeepResearch rewards in favor of constructing relative rankings over trajectories within each generated group. To ensure both depth and fairness in evaluation, we introduce process-aware pairwise evaluation mechanism that not only compares the reliability of the outcome, but also scrutinizes the logical coherence of the chain-of-thought and the effectiveness of tool invocations along the trajectory. central bottleneck in scaling pairwise preference optimization to open-ended agent tasks lies in the high computational cost. While exhaustive comparisons yield accurate rankings, the resulting O(N2) complexity is intractable for online training. To investigate the trade-off between ranking fidelity and sample efficiency, we designed and implemented five tournament topologies, ranging from exhaustive round-robin to single and double elimination formats. Our empirical analysis reveals critical challenge: standard single and double tournaments are highly sensitive to the initial pairing combinations. Random matching causes premature encounters and eliminations of high-quality trajectories, degrading overall ranking accuracy. To mitigate this issue, we innovatively propose seeded single-elimination mechanism. This approach utilizes the trajectory generated by greedy decoding as quality anchor for pre-ranking, providing low-biased initial estimate before seeding the tournament, after which binary-tree structure is used for efficient ranking. Experiments show that this design reduces the computational complexity to linear O(N) while robustly preserving the accuracy of intra-group relative ranking, thereby achieving an optimal balance between training efficiency and advantage estimation fidelity. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we introduce two comprehensive benchmark suites: (1) Open-Travel, which focuses on evaluating agents longhorizon planning capabilities under multiple hard constraints such as budget and spatiotemporal windows; and (2) Open-DeepResearch, which centers on assessing agents abilities in autonomous information retrieval, and report generation in realistic internet environments. Unlike traditional benchmarks that only provide test set He et al. (2025); Du et al. (2025); Coelho et al. (2025), the two proposed benchmarks offer complete pipeline from supervised fine-tuning (SFT) RL training multi-dimensional automated evaluation, establishing reproducible infrastructure for the community. Given the shared characteristics of open-ended tasks, we further extend our experiments to standard open-ended writing tasks Wang et al. (2025a) and conduct systematic evaluations on three public benchmarks. As demonstrated in Figure 1(c), ArenaRL yields substantial performance gains over strong baselines across travel planning, deep research, and open-ended writing tasks, validating the superiority of the tournament-based ranking paradigm. In summary, our major contributions are as follows: We identify and formalize the problem of discriminative collapse in open-ended tasks, and propose ArenaRL, which replaces unstable pointwise scalar rewards with tournament-based relative ranking mechanism to enable robust policy optimization. To address the high computational cost of pairwise comparisons, we design and validate seeded single-elimination tournament topology that achieves high-accuracy advantage estimation with only O(N) complexity. We construct the Open-Travel and Open-DeepResearch benchmarks with full training pipelines, filling critical gap in evaluating the full lifecycle of open-ended agents."
        },
        {
            "title": "2 Related Work",
            "content": "Open-Ended Agent Benchmark. The rapid development of LLMs has given rise to autonomous agents that interact with external environments and solve complex tasks Guo et al. (2025); Yang et al. (2025). Most existing benchmark studies in this area focus on deterministic settings, where tasks have clear goals and verifiable feedback Phan et al. (2025); Yang et al. (2018). Benchmarks such as WebShop Yao et al. (2022a), Mind2Web Deng et al. (2023), and SWE-bench Jimenez et al. (2023) have advanced research in web navigation and code generation by testing whether agents can reach target pages or produce automatically verifiable code. In contrast, many critical realworld applications, such as personalized travel planning Ning et al. (2025) and in-depth industry 3 Technical Report Tongyi DeepResearch analysis Li et al. (2025c), are inherently open-ended and unstructured. These tasks rarely have single gold solution; their quality depends on multi-dimensional trade-offs among the soundness of reasoning, satisfaction of personalized constraints, and practical usefulness of the final plan. More importantly, there is still lack of systematic trainingevaluation infrastructure tailored to open-ended agentic tasks. Existing open-ended benchmarks (e.g., VitaBench He et al. (2025), DeepResearchBench Du et al. (2025)) are predominantly static test suites that only support post-hoc evaluation. They lack complementary training pipelines to enable continuous improvement of open-ended agents. To address this gap, we introduce two high-quality benchmarks, Open-Travel and Open-DeepResearch. Unlike traditional benchmarks, they offer an integrated pipeline spanning SFT, RL-based exploration, and multi-dimensional evaluation, enabling systematic study of agent capabilities in creative, open-ended environments. Reinforcement Learning with LLMs. Reinforcement learning (RL) has emerged as powerful paradigm for aligning LLMs with complex objectives Zhang et al. (2025); Gao et al. (2025). In domains with clear ground truth (e.g., math and code), rule-based reward let algorithms like GRPO Shao et al. (2024) and DAPO Yu et al. (2025) to achieve remarkable success Dong et al. (2025). However, extending RL to open-ended tasks that lack objective outcomes remains highly challenging both theoretically and practically. Because the notion of correctness in such tasks is inherently subjective and multi-dimensional, mainstream methods follow the LLM-as-Judge paradigm. They specify rubrics and assign scalar scores to individual trajectories Viswanathan et al. (2025); Huang et al. (2025) or enforce multiple constraints to assess reliability Ning et al. (2025). Yet, this reward mechanism often struggles to distinguish fine-grained differences among high-quality trajectories in open-ended settings. To address this, recent work has begun to explore comparisonbased evaluation mechanisms. Writing-Zero Jia et al. (2025) assigns binary positive/negative advantages by comparing responses against random references, improving performance on openended writing tasks. Pref-GRPO Wang et al. (2025b) derives reward based on win rates via exhaustive pairwise comparison and shows promise on text to image tasks. Despite these advances, contrastive mechanisms remain unexplored for long-horizon agent tasks. Moreover, existing methods have inherent drawbacks: Writing-Zero provides only coarse-grained binary guidance, while the O(N2) computational cost of full pairwise comparison is prohibitive for training long-context agents. Motivated by this, ArenaRL proposes sparse tournament-based relative ranking paradigm. With an optimized tournament topology, ArenaRL maintains linear O(N) complexity while achieving high-accuracy advantage estimation."
        },
        {
            "title": "3 Preliminary",
            "content": "In this section, we formally define the open-ended agentic task and the associated reinforcement learning objective. We then critically examine the limitations of pointwise scalar evaluation in this setting. Finally, we introduce process-aware pairwise evaluation mechanism that enables fine-grained comparison between two trajectories and produces separate scores for each."
        },
        {
            "title": "3.1 Task Definition",
            "content": "We formulate the open-ended agentic task as conditional trajectory generation problem. Let denote the set of accessible tools. Given query sampled from task distribution D, the agent policy πθ synthesizes multi-step interaction trajectory τ. Formally, τ is defined as an interleaved sequence comprising chain-of-thought zk, tool invocations ak T, environmental feedback ok, and final answer y: τ = [z1, a1, o1, . . . , zk, ak, ok, . . . , zK, aK, oK, y] (1) The reinforcement learning objective is to align the agents behavior with reward signal while maintaining stability relative to reference model. We formulate the optimization objective as follows: 4 Technical Report Tongyi DeepResearch L(θ) = xD,τπθ (x;T) (cid:104) rϕ(x, τ) βD KL (cid:0)πθ(x) πref(x)(cid:1)(cid:105) (2) where rϕ(x, τ) represents the reward signal evaluating the quality of trajectory τ, πref is the reference policy, and β is coefficient controlling the regularization strength of the KL divergence KL to prevent policy degradation. 3.2 Pointwise Scalar evaluation In open-ended agentic tasks, the ground-truth reward function is intractable due to the absence of verifiable rules. Consequently, existing RL paradigms rely on an LLM as reward model to assign pointwise scalar score to the final answer of trajectory τ. We model the observed score ˆR(τ) as the true utility R(τ) corrupted by noise ϵ. The reliance on such pointwise scalar feedback presents critical vulnerability: discriminative collapse. As the agents capabilities improve, its generated responses {τi}G i=1 tend to converge within narrow band of high-quality solutions, causing the variance within the group to vanish (σgroup 0). In this regime, the reward model suffers from high epistemic uncertainty. Unable to discern subtle advantages between top-tier candidate trajectories, the judge hesitates, exhibiting high-variance scoring behavior driven by spurious correlations such as length preference or decoding stochasticity, rather than by genuine semantic merit. This scoring criterion drift, caused by the judges lack of discriminative ability, manifests macroscopically as high-amplitude noise ϵ. Standard algorithms like GRPO, which normalize scores based on group variance (Ai = ( ˆRi µ)/σ), catastrophically fail in this scenario. As σgroup vanishes, the normalization term inadvertently amplifies this drift-induced noise into dominant gradient signals. Consequently, the optimization process is hijacked by the reward models interfering noise, leading to performance stagnation or even degeneration."
        },
        {
            "title": "3.3 Process-Aware Pairwise Evaluation",
            "content": "To circumvent the pitfalls of pointwise scalar scoring, we pivot to pairwise comparisonbased evaluation paradigm. We construct an Arena Judge, denoted as . Given query and two candidate trajectories τa, τb, the judge evaluates them jointly and outputs separate quality score for each trajectory. Specifically, the input to consists of three components: (1) the user query x; (2) the core context of trajectories τi and τj (containing the chain-of-thought zk and tool invocations ak for each step, and the final answer y); and (3) comprehensive process-aware rubric u. This rubric enforces fine-grained scrutiny of logical consistency in the chain-of-thought, the precision of tool calls, and the reliability of the final answer. This ensures that the optimization signal reinforces the agents intrinsic reasoning capabilities rather than merely overfitting to surface-level features of the final answer. The detailed prompts are provided in the Appendix E. To mitigate the positional bias in LLM judges Wu et al. (2025a), we employ bidirectional scoring protocol. We conduct two independent evaluations by swapping the presentation order of the trajectories: (si, sj) = (x, τi, τj, u) + (x, τj, τi, u) (3) where si, sj denotes the quality score assigned to τi in the bidirectional pairwise evaluation, which eliminates the bias favoring the first or second position. 5 Technical Report Tongyi DeepResearch Figure 2: The overall of the proposed ArenaRL algorithm. ArenaRL replaces conventional pointwise scalar reward paradigm with intra-group relative ranking and designs five distinct tournament topologies to optimally balance training efficiency against the accuracy of advantage estimation."
        },
        {
            "title": "4 Methodology: ArenaRL",
            "content": "In this section, we details the proposed ArenaRL algorithm, as shown in Figure 2. Departing from the reliance on unstable pointwise scalar rewards, ArenaRL redefines reward modelling as relative quality ranking among trajectories within group. The core tenet is to efficiently construct dynamic arena for the trajectory group = {τ1, τ2, . . . , τN} sampled from the current policy πθ, thereby deriving robust advantage signals. We systematically investigate five tournament topologies to identify the optimal trade-off between computational cost and ranking fidelity."
        },
        {
            "title": "4.1 Round-Robin Tournament",
            "content": "In this scheme, every trajectory τi competes against all other 1 trajectories via our process-aware pairwise evaluation. The final score is defined as the normalized win rate: Score(τi) ="
        },
        {
            "title": "1\nN − 1",
            "content": "j=i I(si > sj) (4) where I() denotes the indicator function, which equals 1 if the score si (from the process-aware pairwise evaluation) is greater than sj, and 0 otherwise. The group ranking is determined by sorting the Score(τi) in descending order. While the Round-Robin can theoretically provide unbiased intra-group rankings, its quadratic complexity O(N2) renders it intractable for online training with large group size N. We primarily utilize it as the \"gold standard\" to benchmark the fidelity of other efficient topologies."
        },
        {
            "title": "4.2 Anchor-Based Ranking",
            "content": "To alleviate computational complexity, we introduce an Anchor-Based Ranking mechanism. For given input x, we first generate deterministic reference trajectory, denoted as the quality anchor τanc, using greedy decoding (Temperature=0). The remaining 1 trajectories τi, = {1, 2, ..., 1} 6 Technical Report Tongyi DeepResearch in group are produced via high-entropy sampling (e.g., Temperature=0.8) to ensure exploration diversity. Subsequently, each exploratory trajectory τi is individually compared with the anchor trajectory τanc, yielding pair of scores: si anc denotes the anchor score in the i-th comparison, and si denotes the corresponding score of τi. To establish the relative ranking within the group, we first compute the anchors average score sanc = 1 anc and then derive the final ranking based on the set of group scores formed by si, = {1, 2, ..., 1} and sanc. Although this topology attains linear computational complexity O(N), it suffers from loss of resolution. It effectively quantifies the extent to which sample outperforms the anchor, but fails to capture subtle differences between two exploratory samples, which may lead to ambiguity in ranking among suboptimal solutions. i=1 si N1 N1 4.3 Seeded Single-Elimination To bridge the trade-off between computational efficiency and ranking resolution, we propose hybrid topology: Seeded Single-Elimination. This approach operates in two distinct phases: (1) Seeding Phase. We first employ the anchor-based ranking mechanism (described in Section 4.2) to compute preliminary score for each trajectory, and then sequentially assign seed ranking si seed to obtain low-bias initial ordering. This initialization is critical for mitigating premature collisions, where high-quality trajectories might otherwise meet and eliminate each other in early rounds. (2) Elimination Phase. We construct binary tournament tree in which match-ups are arranged according to seed rankings (e.g., pairing the highest seed with the lowest: seed 1 vs. seed N). In each match, the winner advances while the loser is eliminated: τwin = argmaxττi,τj (si, sj) (5) The final ranking is primarily determined by the depth of survival within the tournament bracket. For trajectories eliminated in the same round (e.g., quarter-finals), intra-tier ties are further ranked using their accumulated average scores from previous matches. This topology preserves linear complexity O(N), specifically, requiring 1 comparisons for seeding and 1 for the tournament. Crucially, by leveraging high-quality priors from the seeding phase to guide the tournament structure, this method yields an accurate estimate of relative rankings, ensuring that strategy updates are driven by genuinely superior reasoning trajectories."
        },
        {
            "title": "4.4 Double-Elimination Tournament",
            "content": "We further investigate the Double-Elimination Tournament topology for group ranking estimation. Unlike the single-elimination format, this structure incorporates losers bracket, so that trajectory is eliminated only after sustaining two defeats. The ranking criteria mirror those of Seeded Single-Elimination, relying on advancement depth and accumulated average scores. To maintain computational budget comparable to Seeded Single-Elimination ( 2N comparisons), we initialize this format with random seeding rather than the anchor-based ranking mechanism. Although this topology is in principle more robust to isolated upsets, empirical results indicate that, without high-quality initial seeds, its ranking fidelity falls short of that achieved by Seeded Single-Elimination."
        },
        {
            "title": "4.5 Swiss-System Tournament",
            "content": "We also evaluate the Swiss-System Tournament, non-elimination format with dynamic pairing. In each round, trajectories with identical winloss records are matched against one another (e.g., \"10\" candidate competes against another \"10\" candidate). All trajectories participate in fixed number of rounds (K log2 N), with each round comprising N/2 matches. Final rankings are determined 7 Technical Report Tongyi DeepResearch by composite metric consisting of total wins and the Buchholz score (the sum of wins achieved by trajectorys past opponents). This topology incurs computational complexity of O(N log N). 4.6 Ranking-Based Policy Optimization Irrespective of the underlying tournament topology, ArenaRL produces relative ranking Rank(τi) {0, . . . , 1} for each trajectory in the group, where 0 denotes the highest rank. To enable stable optimization, we convert these discrete ranks into normalized advantage signals. We first map the ranks to quantile-based rewards: ri = 1 Rank(τi) 1 . We then compute the standardized advantage Ai within the group: Ai = ri µr σr + ϵ , (6) (7) where µr and σr denote the mean and standard deviation of the rank-based rewards {r1, . . . , rN}, respectively. Finally, we optimize the policy by maximizing the following objective function, which incorporates KL-divergence penalty to discourage excessive deviation from the reference policy πref: LArenaRL(θ) = xD, Gπθ (cid:34) (cid:18)"
        },
        {
            "title": "1\nN",
            "content": "N i=1 min (cid:18) πθ(τi x) πold(τi x) Ai, clip (cid:18) πθ(τi x) πold(τi x) (cid:19)(cid:35) , 1 ϵ, 1 + ϵ (cid:19) (cid:19) Ai (8) β KL (cid:0)πθ( x) (cid:13) (cid:13) πref( x)(cid:1) . By transforming relative quality ranking within trajectory groups into stable advantage signal, ArenaRL effectively drives the policy toward increasingly strong reasoning and planning behaviors on open-ended tasks."
        },
        {
            "title": "5 Benchmarking Open-Ended Agency",
            "content": "In this section, we introduce Open-Travel and Open-DeepResearch, two benchmarks constructed from realistic business scenarios and designed to comprehensively evaluate LLMs on open-ended agentic tasks. For each domain, we define domain-specific data, including shared tools, domain policy texts, and task instances. Each domain is instantiated through three-stage construction pipeline (as shown in Figure 3) that systematically decomposes real-world scenarios and incorporates expert annotation and rigorous validation. Stage I: Benchmark Data Collection This phase establishes rigorous evaluation foundation by curating high-quality, scenario-specific queries and generating reference trajectories to facilitate robust comparative analysis. Open-ended query construction. We begin by abstracting real-world application scenarios (such as travel planning and open-ended deep research) and collecting corpus of authentic user queries for each scenario. Domain experts then perform multiple rounds of filtering and refinement to obtain set of queries that are semantically precise and succinctly formulated. Finally, we select 50 queries for each of the five Open-Travel subtasks and 100 queries for Open-DeepResearch as the benchmark test sets. Baseline trajectory construction. To obtain reference baseline for subsequent pairwise comparison and win-rate evaluation (i.e., the basic agent capability level), we adopt high-performing closed-source models as the base models to generate complete tool-use trajectories and the corresponding open-ended answers. 8 Technical Report Tongyi DeepResearch Figure 3: The construction process of Open-Travel and Open-DeepResearch benchmarks. Stage II: Large-Scale Training Data Construction After fixing the benchmark, we first manually construct small set of representative seed queries tailored to the requirements of different sub-tasks in real business scenarios. We then use multiple LLMs with diverse styles (e.g., Qwen3 Yang et al. (2025), Qwen3-Max Bai et al. (2023)) as query generators to synthesize large-scale, multi-scenario open-ended queries. The resulting training dataset thus contains both complete queries collected from real business workflows and diversified queries produced via LLM-based expansion. Consistent with Stage I, we employ high-performing closed-source models as the base model to generate large-scale tool-use trajectories, which are used as cold-start data for SFT. The remaining queries, without explicit supervised trajectories, are reserved for subsequent RL. Stage III: Trajectory Data Quality Control We first feed the complete trajectory dataset into rule-augmented LLM-based quality inspection module, which evaluates each trajectory along three dimensions: effectiveness of open-ended tool usage, correctness of conversational content, and consistency of the final answers. This process filters out trajectories with formatting or logical errors. For trajectories that fail to meet the criteria, we iteratively rewrite and refine them until they pass the LLM-based quality inspection."
        },
        {
            "title": "5.1 Domains",
            "content": "Using the above pipeline, we modularly construct two domains: Open-Travel and OpenDeepResearch. In the following sections, we provide brief description of the policies for each domain. Open-Travel. In the Open-Travel domain, the agent is required to help users accomplish the following five types of itinerary planning subtasks, which jointly emphasize multi-constraint reasoning, multi-tool coordination, and personalized preferences: Route planning with multiple specified waypoints (defined as the Direction subtask); One-day trip planning in single city (denoted as (defined as the 1-Day subtask); Transportation-mode comparison (defined as the Compare subtask); Technical Report Tongyi DeepResearch Table 1: Statistics of the constructed Open-Travel and Open-DeepResearch benchmarks. Dataset Training Set SFT RL Test Samples Language Domain Open-Travel Open-DeepResearch 2,600 2, 1,626 2,216 Total 5,262 3,842 250 100 Chinese Chinese / English Travel Planning General Chinese / English Nearby point-of-interest (POI) search (defined as the Search subtask); Multi-day trip planning, evaluated as generalization task and excluded from the SFT training data (defined as the M-Day subtask). These rules are further intertwined with user-specific constraints (e.g., budget limits, time windows, traveling parties, and preference profiles), resulting in challenging reasoning tasks for the agent. Open-DeepResearch. In the Open-DeepResearch domain, the agent is required to assist users in conducting multi-turn search, reading, synthesis, and generation, and ultimately produce an open-ended answer. The open-ended summarization and generation tasks in this domain mainly fall into the following categories: Assisting users in writing open-ended technical documents (e.g., reports, design documents, or survey-style overviews); Helping users ideate, expand, or refine research topics, solution plans, or content outlines; Providing concise yet informative explanations, overviews, or summaries of complex concepts, systems, or domains."
        },
        {
            "title": "5.2 Dataset Statistics and Analysis",
            "content": "As shown in Table 1, the final Open-Travel and Open-DeepResearch datasets are constructed at reasonable scale with high diversity, providing the research community with practical and representative benchmark for evaluating open-ended agentic reasoning and tool-use capabilities."
        },
        {
            "title": "5.2.1 Dataset Scale",
            "content": "Training Set. Open-Travel contains 2,600 SFT samples and 1,626 RL samples, while OpenDeepResearch contains 2,662 SFT samples and 2,216 RL samples. The SFT data are mainly used to help the model acquire basic tool-calling formats, intent understanding, and multi-step reasoning patterns. The RL query samples are then used to further elicit and optimize the models open-ended agentic behaviors under realistic constraints. Test Set. We construct high-quality test set for leaderboard-style evaluation, consisting of 250 samples from Open-Travel and 100 samples from Open-DeepResearch. All test samples are manually checked to ensure representative clarity, diversity, and difficulty."
        },
        {
            "title": "5.2.2 Category Coverage",
            "content": "The dataset spans wide spectrum of functional categories. Beyond the travel-planning domain, it also covers areas such as sports, medicine, and variety of other everyday and professional scenarios. This broad topical distribution allows us to evaluate models not only on their specialized performance in specific domains (e.g., travel planning), but also on their overall competence as general-purpose open agents. 10 Technical Report Tongyi DeepResearch 5.3 Evaluation We adopt an LLM-as-a-judge evaluation paradigm, and use two strong proprietary models as dual judges to score both the reasoning trajectories and the final answers. Open-Travel. For Open-Travel, we evaluate models on the carefully curated Open-Travel test set. For each test sample, we independently invoke two powerful closed-source LLM judges from different model families to perform pairwise evaluation. Each judge compares the candidate agents output with the baseline output and assigns scores along multiple dimensions (e.g., answer correctness, consistency with the reasoning trajectory, etc.). Based on these judgments, we compute the win rate for each evaluation criterion, defined as the proportion of non-tied cases where the candidate output is preferred over the baseline. We then average the win rates obtained from the two judges and use this averaged value as the final performance metric for each Open-Travel subtask. Open-DeepResearch. For Open-DeepResearch, we adopt the same evaluation protocol as used for Open-Travel on the Open-DeepResearch test set. For each sample, two closed-source LLM judges from different model families are independently employed to evaluate both the reasoning trajectory and the final answer jointly. Notably, due to the long-context nature of DeepResearch tasks, models may occasionally experience context overflow, resulting in the inability to generate valid final answers. To account for this issue, we additionally report the valid generation rate (Val. %) for each model, defined as the proportion of test cases in which valid answer is successfully generated over the entire benchmark. Furthermore, for each evaluation criterion, we compute the candidate models win rate against the baseline conditioned on valid generations, i.e., the proportion of cases with valid outputs in which the candidate model is preferred over the baseline. We then aggregate the per-criterion win rates as well as the cross-criterion average win rate. As in Open-Travel, the final evaluation metric is obtained by averaging the scores produced by the two judge models."
        },
        {
            "title": "6 Experiments",
            "content": "To comprehensively evaluate the effectiveness of ArenaRL, we first report the performance of the five tournament topologies introduced in Section 4, thereby empirically justifying our selection of the Seeded Single-Elimination scheme. Building upon this optimal topology, we benchmark ArenaRL against strong baseline methods on our proposed Open-Travel and Open-DeepResearch datasets. Furthermore, recognizing the shared characteristics of open-ended problems, we extend our evaluation to standard open-ended writing tasks using three public benchmarks. Finally, to assess the robustness and practical applicability of ArenaRL in real-world settings, we conduct additional experiments on real business data derived from the Amap (Gaode Map) ecosystem."
        },
        {
            "title": "6.1 Experimental Settings",
            "content": "Baselines. We evaluate ArenaRL against two categories of baselines. First, we benchmark against four closed-source models, including GPT-4o Achiam et al. (2023), Grok-4 xAI (2025), Gemini-2.5-pro Team et al. (2023), and Claude-3.7-Sonnet Anthropic (2023). Second, we compare with representative reinforcement learning algorithms, specifically GRPO Shao et al. (2024) and GSPO Zheng et al. (2025). For these RL algorithm baselines, we employ the standard LLM-as-Judge setting, obtaining rewards through pointwise scoring. To maintain fairness, these baseline algorithms utilize the exact same judge models and evaluation rubrics as ArenaRL, and only evaluate the answer portion. Training Guideline. Our experiments strictly follow the common Cold-start RL paradigm to mitigate reward collapse during the initial RL exploration phase. 11 Technical Report Tongyi DeepResearch Table 2: Performance comparison of the five tournament topologies on the Open-Travel benchmark. Topology Comparison Cost Open-Travel Direction Search Compare 1-Day M-Day SFT Anchor-Based Ranking Swiss-System Double-Elimination Seeded Single-Elimination - 1 log 2N 2 2N 2 Round-Robin (Upper Bound) N(N 1)/ 10.6 18.0 20.9 12.6 16.9 23.3 29.7 41.3 43.0 52.4 69.9 66. 14.1 30.9 27.9 33.7 22.9 23.6 20.4 31.1 38.6 39.9 34.9 32. 7.1 17.6 11.1 12.3 18.1 19.0 Mean 16.4 27.8 28.3 30.2 32. 32.9 1. Cold-start phase. We utilize Qwen3-8B-Base Yang et al. (2025) as the backbone model. For the open-ended agent tasks (Open-Travel and Open-DeepResearch), the base model is fine-tuned on their respective SFT datasets to acquire fundamental tool-use and planning capabilities. For open-ended writing tasks, we randomly sample 10k examples from the DeepWriting-20K Wang et al. (2025a) dataset for supervised fine-tuning. 2. RL phase. For the open-ended agent tasks, we train on their corresponding RL splits. For open-ended writing, we utilize 10k examples from the DeepWriting-20K dataset (excluding those used for SFT) to conduct reinforcement learning. Evaluation Metrics. For open-ended agent tasks, we perform pairwise evaluations against the baseline trajectories in our benchmark using the multi-dimensional criteria defined in Section 5, and compute the corresponding win rates. For the Open-Travel tasks, we report the win rate on each of the five subtasks as well as the average win rate. For the Open-DeepResearch tasks, we report the valid generation rate (Val. %). And within the subset of valid generations, the win rates of the candidate model against the baseline under each of the seven evaluation rubrics, along with the final averaged win rate. Specifically, these rubrics capture complementary aspects of open-ended research capability: Framework (Frm.), assessing the structural completeness and logical coherence of the initial research plan; Tool Usage (Tool.), evaluating the appropriateness and efficiency of tool invocations and their alignment with the research workflow; Coverage (Cov.), measuring whether the retrieved information sufficiently covers the users requirements; Relevance (Rel.), assessing how well the response addresses all user queries and constraints; Accuracy (Acc.), evaluating the factual correctness and internal consistency of the content; Depth (Dep.), measuring the level of analytical depth and coherence of the reasoning process; and Clarity (Cla.), assessing the organization, readability, and practical usability of the final output. The complete task prompt for the judge models is shown in Figure 6. For open-ended writing, we adopt three complementary benchmarks for comprehensive assessment: WritingBench Wu et al. (2025b), HelloBench Que et al. (2024), and LongBench-write Bai et al. (2024). Considering the subjective nature of open-ended generation, and following established protocols Wang et al. (2025a), we employ the LLM-as-judge approach to score the generative quality of different models."
        },
        {
            "title": "6.2 Tournament Topology Analysis",
            "content": "Table 2 presents systematic comparison of different tournament topologies under unified RL configuration (group size = 8, number of groups = 8). The results indicate that the proposed Seeded Single-Elimination scheme achieves the best trade-off between efficiency and performance. Specifically, it attains an average win rate of 32.5%, which is comparable to the gold standard performance of 32.9% established by the computationally expensive Round-Robin tournament, while requiring only O(N) pairwise comparisons. In contrast, the Swiss Round and Double-Elimination formats fail to deliver comparable performance gains, due either to the lack of an effective initial prior or insufficient comparison depth. Notably, Seeded Single-Elimination even outperforms 12 Technical Report Tongyi DeepResearch Table 3: Performance comparison on Open-Travel and Open-DeepResearch benchmarks. Method GPT-4o Grok-4 Gemini-2.5-pro Claude-3.7-Sonnet SFT GRPO GSPO ArenaRL Open-Travel Open-DeepResearch Direction Search Compare 1-Day M-Day Mean Frm. Tool. Cov. Rel. Acc. Dep. Cla. Mean (Val. %) 2.4 17.0 8.6 18. 10.6 11.0 10.0 32.1 5.0 21.3 12.5 59.6 29.7 26.3 30.6 66.1 3.1 9.7 7.4 14.7 14.1 14.3 13.1 31.7 Closed-source Models 1.6 24.7 11.9 43.6 20.4 21.9 21.1 58.0 0.7 11.3 12.4 21.3 2.6 16.8 10.6 31.6 Fine-tuning & RL 7.1 8.6 11.4 21. 16.4 16.4 17.2 41.8 5.1 33.7 15.8 10.1 14.1 20.6 23.8 62.6 24.4 36.8 19.0 13.5 20.3 35.3 33.3 77.3 21.0 12.5 9.1 43.4 36.8 39.2 17.9 32.6 28.3 22.5 23.6 19. 2.3 10.8 36.1 17.5 45.7 38.6 27.0 17.4 12.2 (88.0) 34.8 (83.0) 28.3 (92.0) 19.1 (89.0) 23.4 14.1 15.6 35.3 23.5 23.5 40.5 16.7 21.4 78.8 57.1 55.6 15.6 14.1 26.5 11.8 31.0 9.5 57.1 61.6 16.7 (32.0) 25.2 (17.0) 25.2 (21.0) 64.3 (99.0) Table 4: Performance comparison on open-ended writing task across three public benchmarks: WritingBench, HelloBench, and LongBench-write. Method WritingBench HelloBench LongBench WB-A WB-B WB-C WB-D WB-E WB-F QA Summ. Heur. Quality Closed-source Models GPT-4o Grok-4 Gemini-2.5-pro Claude-3.7-Sonnet 67.90 80.32 80.89 68.36 66.34 78.65 80.39 66.53 68.56 79.75 82.49 68.70 69.95 81.46 84.33 70. 70.70 81.19 83.53 71.42 72.17 80.92 82.61 71.47 81.03 88.42 85.67 80.81 Fine-tuning & RL SFT GRPO GSPO ArenaRL 70.71 71.62 71.56 78. 69.36 71.18 70.70 77.70 67.88 68.67 68.87 77.58 63.72 66.84 66.08 75.02 69.69 72.56 72.29 79.35 70.64 70.33 69.83 77.16 78.44 79.09 80.06 79. 84.26 85.58 82.43 74.68 63.42 64.94 63.97 73.82 89.14 94.65 93.79 95.82 82.35 83.76 81.75 91.33 90.43 96.52 98.69 98.34 85.52 86.96 85.21 93. Mean 76.05 84.75 85.48 76.65 72.17 73.60 73.03 80.30 Round-Robin on the Search and 1-Day subtasks. This observation suggests that the anchor-based seeding mechanism effectively filters out noise and prevents high-quality candidates from being adversely affected by random matching fluctuations in the early stages. Based on these findings, we adopt Seeded Single-Elimination as the primary tournament topology in subsequent experiments and conduct further comparison with the remaining strong baselines."
        },
        {
            "title": "6.3 Main Results",
            "content": "As shown in Table 3, ArenaRL demonstrates strong performance across both open-ended agent benchmarks, outperforming four powerful closed-source models. On the Open-Travel benchmark, ArenaRL achieves an average win rate of 41.8%, substantially outperforming GRPO (16.4%) and GSPO (17.2%). In the Open-DeepResearch benchmark, ArenaRL not only achieves win rate of 64.3%, but also attains valid generation rate (Val. %) of 99%. In sharp contrast, the baseline methods perform poorly in terms of task completion, with the SFT model achieving valid generation rate of only 32%. We attribute this gap primarily to the inherently high token consumption required by deepresearch tasks, together with the prevalence of long-horizon samples in the SFT training data, which jointly lead to frequent context overflows. Notably, although GRPO and GSPO slightly improve the average win rate, their valid generation rates are inferior to that of the SFT baseline. These results highlight key limitation of standard pointwise reward schemes such as GRPO and GSPO: for long-horizon tasks involving complex tool use, assigning scalar score to single trajectory often fails to capture fine-grained policy improvements, and tends to be susceptible to spurious advantages such as length bias. In contrast, ArenaRLs comparison-based reward signal 13 Technical Report Tongyi DeepResearch Figure 4: (a) The impact of Group Size on performance of Open-Travel benchmark. (b) The (c) The performance trend of ArenaRL in consistency between LLM and human evaluations. training Qwen3-8b via direct RL without cold start. provides more discriminative gradient directions, effectively steering policy evolution toward more robust planning and reasoning capabilities within vast search space. Table 4 further validates the generality of ArenaRL on standard open-ended writing tasks. Across three benchmarks, ArenaRL maintains substantial lead in overall average score, outperforming GRPO by 6.70% and GSPO by 7.27%. In addition, ArenaRL surpasses two strong closed-source models, GPT-4o and Claude-3.7-Sonnet, further highlighting its superiority. Although ArenaRL is slightly inferior to GSPO on the HelloBench-QA subtask, this category of tasks is largely constrained by the models inherent knowledge, and ArenaRL remains highly competitive under this limitation. Across various open-ended writing scenarios, ArenaRLs comprehensive improvement indicate that ArenaRL is not only suitable for tool-augmented agents, but can also systematically enhance the models reasoning and expressive capabilities, thereby making it applicable to broader range of open-ended generation tasks."
        },
        {
            "title": "6.4 Further Analysis",
            "content": "Impact of Group Size N. We conduct an ablation study with set group size {2, 4, 8, 16} to further investigate the scalability properties of ArenaRL, as shown in Figure 4 (a). The results show clear monotonic improvement in model performance as the group size increases. Notably, even under the smallest configuration = 2, ArenaRL achieves an average win rate of 20.8%, outperforming the SFT baseline (16.4%). This confirms that even the most basic pairwise comparison setting can provide effective optimization gradients. The most pronounced performance gain occurs when is scaled up to 16, where the average win rate jumps to 41.8%. This effect is particularly striking on the challenging 1-Day planning task, where the score jumps from 34.9% at = 8 to 58.0%. These substantial improvements indicate that, for complex reasoning tasks, enlarging the candidate pool effectively broadens the exploration space, dramatically increasing the likelihood of discovering high-quality trajectories and thereby enabling the model to learn from stronger examples. Assessment of Consistency. To assess the reliability of the LLM-based evaluation mechanism, we analyzed the consistency between LLM and human evaluations on the Open-Travel and OpenDeepResearch benchmarks using confusion matrix (see Figure 4 (b)). The results show that most evaluation outcomes are concentrated along the diagonal, with an overall agreement rate of 73.9%. This relatively high level of consistency suggests that ArenaRLs performance gains do not simply stem from overfitting to the preferences of the specific judge model used during the RL phase, but instead reflect improvements that are broadly aligned with human assessments. Direct RL Training without Cold Start. To further test the robustness of ArenaRL, we bypassed the cold-start phase on the Open-Travel task and directly employed the Qwen3-8B model for ArenaRL 14 Technical Report Tongyi DeepResearch training, reporting its performance on the Search sub-task. As shown in Figure 4 (c), the models performance exhibited sustained and stable upward trend. At the initial step 0, the model scored 0, indicating that the generic model was initially incapable of handling such complex travel planning tasks. However, as RL training steps increase, the model rapidly acquired the corresponding tool invocation capabilities. Ultimately, it achieves peak score of 71% at step 160. This outcome indicates that ArenaRLs intra-group relative ranking mechanism sensitively captures effective optimization directions. Even when initial output quality is extremely low, it reliably provides gradients for policy refinement. This effectively mitigates RLs cold-start problem, demonstrating ArenaRLs capacity for self-evolution from scratch in scenarios lacking costly SFT annotated data. Case Study. Figures F.1 and F.2 present representative comparative case from the Open-Travel benchmark. This example involves complex travel request with multiple hard constraints, including time, destination, and budget. The baseline SFT model exhibits restatement tendency in its chain-of-thought, and its reasoning trajectory fails to align with the users intent, often overlooking specific constraints and providing only generic suggestions. In contrast, the model optimized with ArenaRL demonstrates strong strategic planning capabilities. It proactively retrieves information about multiple target attractions, performs logically coherent route planning, and ultimately produces persuasive, personalized itinerary. This substantial improvement in reasoning patterns powerfully demonstrates that our tournament-based ranking mechanism effectively incentivizes the model to explore and retain superior planning strategies."
        },
        {
            "title": "6.5 Application in Real-world Business Scenarios",
            "content": "To further verify the robustness and practicality of our proposed ArenaRL algorithm, we conducted experiments on real-world business data derived from the Amap (Gaode Map) ecosystem. The evaluation was divided into two distinct categories based on the nature of the user queries: quantifiable POI search tasks and complex open-ended tasks. Performance on Deterministic POI Search. In POI search scenarios characterized by explicit evaluation metrics, our ArenaRL-tuned model significantly outperformed the baseline, registering 75% to 83% gain in search accuracy. These results confirm the models ability to navigate rigid constraints and specific ranking criteria. Furthermore, this demonstrates that the tournament-based ranking mechanism of ArenaRL is highly effective at distinguishing subtle nuances across varied high-quality trajectories, ensuring robust optimization performance in deterministic task settings. Performance on Open-ended Planning Tasks. We extended our evaluation to complex, openended travel planning tasks that require multi-step reasoning and tool invocation. These scenarios include queries with vague intents and ambiance preferences (e.g., Find quiet bar near the Bund with river-view terrace for date, open after 10 PM) as well as complex cross-city logistics requiring multi-objective trade-offs (e.g., Depart Beijing West at 18:30, arrive in Tianjin by 22:00; minimize cost and transfers due to heavy luggage). On these tasks, the core business metric rose significantly from 69% to 80%. We observed substantial and consistent gains throughout the training process, reflecting the models enhanced capability to interpret ambiguous intents and satisfy multiple constraints. These results indicate that the efficient planning capabilities acquired via ArenaRL effectively transfer to practical applications, significantly improving user intent alignment and response quality in complex service scenarios."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we propose ArenaRL, novel reinforcement learning framework that shifts the training paradigm for open-ended agents from pointwise scalar scoring to intra-group relative ranking. Specifically, we introduce process-aware pairwise evaluation mechanism and systematically investigate five tournament topologies. Our findings reveal that the seeded single-elimination topology strikes favorable balance between advantage estimation accuracy and computational 15 Technical Report Tongyi DeepResearch efficiency. Extensive evaluations across travel planning, deep research, and open-ended writing tasks demonstrate that this competition-driven evolutionary paradigm not only provides robust advantage signals, but also fundamentally incentivizes agents to perform efficient reasoning and planning. As future work, we will explore how to efficiently extend ArenaRL to multimodal agent settings, further enhancing the generality of our framework. Technical Report Tongyi DeepResearch"
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anthropic. Introducing Claude, 2023. URL https://www.anthropic.com/index/ introducing-claude. Esteban Garces Arias, Meimingwei Li, Christian Heumann, and Matthias Aßenmacher. Decoding decoded: Understanding hyperparameter effects in open-ended text generation. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 999210020, 2025. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024. João Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie Callan, João Magalhães, Bruno Martins, et al. Deepresearchgym: free, transparent, and reproducible evaluation sandbox for deep research. arXiv preprint arXiv:2505.19253, 2025. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025. Johannes Fürnkranz and Eyke Hüllermeier. Preference learning and ranking by pairwise comparison. In Preference learning, pp. 6582. Springer, 2010. Chang Gao, Chujie Zheng, Xiong-Hui Chen, Kai Dang, Shixuan Liu, Bowen Yu, An Yang, Shuai Bai, Jingren Zhou, and Junyang Lin. Soft adaptive policy optimization. arXiv preprint arXiv:2511.20347, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Wei He, Yueqing Sun, Hongyan Hao, Xueyuan Hao, Zhikang Xia, Qi Gu, Chengcheng Han, Dengchang Zhao, Hui Su, Kefeng Zhang, et al. Vitabench: Benchmarking llm agents with versatile interactive tasks in real-world applications. arXiv preprint arXiv:2509.26490, 2025. Zhengyu Hu, Linxin Song, Jieyu Zhang, Zheyuan Xiao, Tianfu Wang, Zhengyu Chen, Nicholas Jing Yuan, Jianxun Lian, Kaize Ding, and Hui Xiong. Explaining length bias in llm-based preference evaluations. arXiv preprint arXiv:2407.01085, 2024. 17 Technical Report Tongyi DeepResearch Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, et al. Reinforcement learning with rubric anchors. arXiv preprint arXiv:2508.12790, 2025. Ruipeng Jia, Yunyi Yang, Yongbo Gai, Kai Luo, Shihao Huang, Jianhe Lin, Xiaoxi Jiang, and Guanjun Jiang. Writing-zero: Bridge the gap between non-verifiable tasks and verifiable rewards. arXiv e-prints, pp. arXiv2506, 2025. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, Ji-Rong Wen, Yuan Lu, et al. Deepagent: general reasoning agent with scalable toolsets. arXiv preprint arXiv:2510.21618, 2025a. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yongkang Wu, Ji-Rong Wen, Yutao Zhu, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025b. Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, et al. Webweaver: Structuring web-scale evidence with dynamic outlines for open-ended deep research. arXiv preprint arXiv:2509.13312, 2025c. Tianci Liu, Ran Xu, Tony Yu, Ilgee Hong, Carl Yang, Tuo Zhao, and Haoyu Wang. Openrubrics: Towards scalable synthetic rubric generation for reward modeling and llm alignment. arXiv preprint arXiv:2510.07743, 2025. Yansong Ning, Rui Liu, Jun Wang, Kai Chen, Wei Li, Jun Fang, Kan Zheng, Naiqiang Tan, and Hao Liu. Deeptravel: An end-to-end agentic reinforcement learning framework for autonomous travel planning agents. arXiv preprint arXiv:2509.21842, 2025. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, et al. Hellobench: Evaluating long text generation capabilities of large language models. arXiv preprint arXiv:2409.16191, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025. Vijay Viswanathan, Yanchao Sun, Shuang Ma, Xiang Kong, Meng Cao, Graham Neubig, and Tongshuang Wu. Checklists are better than reward models for aligning language models. arXiv preprint arXiv:2507.18624, 2025. 18 Technical Report Tongyi DeepResearch Haozhe Wang, Haoran Que, Qixin Xu, Minghao Liu, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Wei Ye, Tong Yang, Wenhao Huang, et al. Reverse-engineered reasoning for open-ended generation. arXiv preprint arXiv:2509.06160, 2025a. Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-grpo: Pairwise preference reward-based grpo for stable text-to-image reinforcement learning. arXiv preprint arXiv:2508.20751, 2025b. Yuanchen Wu, Ke Yan, Shouhong Ding, Ziyin Zhou, and Xiaoqiang Li. Towards rationale-answer alignment of lvlms via self-rationale calibration. arXiv preprint arXiv:2509.13919, 2025a. Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, et al. Writingbench: comprehensive benchmark for generative writing. arXiv preprint arXiv:2503.05244, 2025b. xAI. Grok 4, 2025. URL https://x.ai/news/grok-4. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 23692380, 2018. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable realworld web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022a. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022b. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Fanrui Zhang, Qiang Zhang, Sizhuo Zhou, Jianwen Sun, Chuanhao Li, Jiaxin Ai, Yukang Feng, Yujie Zhang, Wenjie Li, Zizhen Li, et al. Code-in-the-loop forensics: Agentic tool use for image forgery detection. arXiv preprint arXiv:2512.16300, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Zilin Zhu, Chengxing Xie, Xin Lv, and slime Contributors. slime: An llm post-training framework for rl scaling. https://github.com/THUDM/slime, 2025. GitHub repository. Corresponding author: Xin Lv. 19 Technical Report Tongyi DeepResearch Implementation Details. Cold-start phase. We employ the TRL framework optimized with DeepSpeed ZeRO-3. The model is trained for 3 epochs on 32 NVIDIA H20 GPUs, utilizing learning rate of 2 105 and batch size of 1 per GPU. RL phase. we implement ArenaRL based the Slime Zhu et al. (2025) framework. To focus optimization on reasoning quality, tokens corresponding to environmental feedback are masked out from the loss computation. Regarding hyperparameters, we set the group size = 16 and the number of groups = 8 for Open-Travel and open-ended writing tasks. For Open-DeepResearch, we adjust = 8 and = 4 to enhance training efficiency. Across all RL experiments, we employ the Adam optimizer with learning rate of 1 106. Furthermore, we utilize Qwen3-Max as the arena judge during training, leveraging its superior reasoning capabilities to provide high-fidelity pairwise comparison signals, with its prompt detailed in Appendix E. Besides, the RL training is conducted on 8 H20 GPUs. Evaluation For the Open-Travel and Open-DeepResearch benchmarks, we invoke Qwen3-Max and Claude-4-Sonnet Anthropic (2023) as evaluation model, comparing and scoring the candidate agents output against the baseline output for each test sample."
        },
        {
            "title": "B Writing Benchmarks",
            "content": "To ensure the comprehensiveness and multifaceted nature of our evaluation, we adopt three complementary benchmarks: WritingBench Wu et al. (2025b), HelloBench Que et al. (2024), and LongBenchWrite Bai et al. (2024). 1. WritingBench Wu et al. (2025b) is designed to assess models domain-specific writing proficiency and controllability across six professional and creative domains: (Academic & Engineering), (Finance & Business), (Politics & Law), (Literature & Arts), (Education), and (Advertising & Marketing). It focuses in particular on the ability to comply with complex, multidimensional constraints. 2. HelloBench Que et al. (2024) evaluates model performance on diverse set of in-the-wild tasks derived from real user queries. Our analysis concentrates on three key subsets: HelloBench-QA (open-ended QA), which tests the generation of detailed and nuanced responses; HelloBenchSumm. (text summarization), which evaluates the ability to summarize long-form texts; and HelloBench-Heur. (heuristic text generation), which measures creative reasoning and stylistic fidelity in long-form narrative continuation. 3. LongBench-write Bai et al. (2024) is designed to evaluate models ability to produce coherent ultra-long texts (e.g., on the order of 10,000 words), enabling us to assess the fundamental capacity to maintain topical consistency and global coherence as output length scales."
        },
        {
            "title": "C Benchmarking Tool Annotation",
            "content": "In this section, we will outline the tools employed by the Open-DeepResearch and Open-Travel benchmarks respectively. Open-DeepResearch. We annotate commonly used search tools for open-ended deep research as candidate tools. Specifically, we perform web search via the Google API and integrate summarization model into the search pipeline to prevent excessive context length. For each retrieved webpage, if the parsed content exceeds 2,500 characters, we invoke the Qwen3-Max Yang et al. (2025) model to automatically summarize the page content. 20 Technical Report Tongyi DeepResearch Open-Travel. We annotate six commonly used tool types for travel planning: Search poi: This tool is built on Amaps POI search service and retrieves location information via text queries. The input can be structured address (e.g., No. 10 Fuyong Street, Wangjing, Chaoyang District, Beijing) or POI name (e.g., Peoples Square). It returns multiple potentially relevant POIs, including: (1) detailed address; (2) geographic coordinates (the location field, formatted as \"longitude, latitude\"); and (3) business information (the Business field). Around search: This tool searches for POIs within circular area specified by center point and radius. Users can specify POI types or constrain the returned results using keywords (e.g., bank). It returns multiple potentially relevant POIs, each including: (1) detailed address; (2) geographic coordinates (the location field, where longitude and latitude are separated by comma, with longitude first and latitude second); and (3) business information. Get navigation: This tool provides integrated route planning based on Amaps navigation service, covering multiple transportation modes such as walking, driving, and public transit. The inputs are the latitudelongitude pairs of the origin and destination, with optional waypoints and route-planning mode (driving by default). The tool returns dictionary containing detailed route-planning information. Universal search: This tool performs general, city-level geographic knowledge search using the search engine. When users pose vague or open-ended questions (e.g., What are some fun things to do in Beijing? or What parks in Shanghai are suitable for family outings?), It is used to obtain recommendations and travel suggestions. The tool returns the corresponding search results. Search flights: This tool searches for intercity flight information based on date and city names. When users need to query flights between two cities, they provide the query date and departure city, and the tool returns list of flights, each including flight number, price, departure/arrival airports, and departure/arrival times. Search train tickets: This tool searches for intercity train ticket information based on date and city names. When users need to query train tickets between two cities, it returns list of train options, where each entry includes train ID, price, departure/arrival stations, and times, and explicitly distinguishes between direct and transfer routes. For these six tools, search poi, around search, and get navigation are implemented using Web service APIs from the Amap open platform1. The universal search tool is powered by the Bailian search engine2. The outputs of search flights and search train tickets are simulated with the detailed prompts provided in the Appendix E."
        },
        {
            "title": "D Tournament Algorithm Flow",
            "content": "In Algorithm 1, we present the detailed procedure of the proposed seeded single-elimination tournament, which achieves an excellent trade-off between training efficiency and the accuracy of advantage estimation."
        },
        {
            "title": "E Prompts",
            "content": "In this section, we will illustrate all the prompts used in our paper. E.1 Open-Travel Task Prompt See Figure 5. 1https://lbs.amap.com/api/webservice/guide/api-advanced/search 2https://bailian.console.aliyun.com/?spm=5176.12818093_47.resourceCenter.1.3dd916d04Ye5xn&tab=app#/mcpmarket/detail/WebSearch Technical Report Tongyi DeepResearch E.2 Open-DeepResearch Task Prompt See Figure 6. E.3 Open-ended Writing Task Prompt Prompts for open-ended writing tasks adapt according to the language type of the query, specifically the Chinese prompt (Figure 7) and the English prompt (Figure 8). E.4 Search Flights Tool Prompt See Figure 9. E.5 Search Train Tickets Tool Prompt See Figure 10."
        },
        {
            "title": "F Case Study",
            "content": "Figures F.1 and F.2 illustrate the interaction trajectories of the baseline SFT model and the model tuned via ArenaRL within representative case under the Open-Travel benchmark. These visualizations clearly illustrate that our ArenaRL algorithm substantially enhances the agents ability to perform complex reasoning and efficient action planning. 22 Technical Report Tongyi DeepResearch Algorithm 1: ArenaRL: Seeded Single-Elimination Advantage Estimation Input: Trajectory group = {τanc} {τ1, . . . , τN1}, Arena Judge (, ) Output: Standardized advantages = {A1, . . . , AN} 4.2) // Phase 1: Anchor-Based Seeding (Sec. 1 Initialize score set Sinit 2 for 1 to 1 do Obtain scores: si, si 3 Sinit Sinit {(τi, si)} anc (τi, τanc) 4 5 end 6 sanc 1 7 Sort descendingly based on Sinit to get ordered seeds: [τ1, . . . , τN] 8 Initialize accumulated scores V[τ] score in Sinit for all τ 9 Initialize ranking tiers Sinit Sinit {(τanc, sanc)} i=1 si N1 anc; N1 // Phase 2: Elimination Tournament (Sec. 4.3) 10 Initialize array of size 11 idxhead 1; 12 for 1 to N/2 do 13 idxtail 1 (τhigh, τlow) (P [k], [N + 1]) if is odd then // Tail pointer at start of last pair // Pair Seed vs + 1 B[idxhead] τhigh; B[idxhead + 1] τlow idxhead += 2 else B[idxtail] τhigh; B[idxtail + 1] τlow idxtail = 2 // Fill from front // Fill from back end 20 21 end 22 while > 1 do 23 Wround ; Lround for 1 to B/2 do 24 (τa, τb) (B[2k 1], B[2k]); V[τa] += sa; V[τb] += sb (τwin, τlose) (sa > sb)?(τa, τb) : (τb, τa) Wround Wround {τwin}; Lround Lround {τlose} (sa, sb) (τa, τb) // Determine result // Archive losers, promote winners // Champion enters final tier 14 15 17 18 19 25 26 28 29 end {Lround}; Wround 30 31 end 32 {B} // Phase 3: Rank Assignment (Hierarchical Sorting) 33 0 34 foreach Tier .reverse() do Sort Tier descendingly based on accumulated scores Assign Rank(τ) + + for all τ Tier 36 37 end // Phase 4: Advantage Calculation (Sec. 4.6) 38 Assign Reward: rτ 1 Rank(τ) N1 39 Compute statistics µr, σr from {rτ} µr 40 return Ai rτi σr+ϵ for all τi Technical Report Tongyi DeepResearch F.1 Case Study of SFT model Case Trajectory in Open-Travel Benchmark of SFT model (translated into English) Query: two-day weekend hiking trip from Chengdu to Qingcheng Back Mountain for two people, with focus on natural scenery and photography, and budget of 600 RMB per person. Step <think> The user would like to plan weekend hiking trip from Chengdu to Qingcheng Back Mountain for two people, with preference for natural scenery and photography, and budget of 600 RMB per person. need to first determine the location of Qingcheng Back Mountain, then plan transportation from Chengdu to that area, as well as hiking routes in Qingcheng Back Mountain. First, need to use the search_poi tool to look up the location information of Qingcheng Back Mountain. </think> <tool_call> {\"name\": \"search_poi\", \"arguments\": {\"query\": \"Qingcheng Back Mountain\"}} </tool_call> <tool_response> ... </tool_response> Step 2 <think> need to plan weekend hiking itinerary from Chengdu to Qingcheng Back Mountain for the user, suitable for two people, with preference for natural scenery and photography, and budget of 600 RMB per person. have already obtained the basic location information for Qingcheng Back Mountain, and now need to further understand the transportation options from Chengdu to Qingcheng Back Mountain, as well as the hiking routes in Qingcheng Back Mountain and the surrounding natural scenery and photography spots. First, need to find out the transportation options from Chengdu to Qingcheng Back Mountain. </think> <tool_call> {\"name\": \"universal_search\", \"arguments\": {\"query\": \"Transportation options from Chengdu to the Qingcheng Back Mountain\"}} </tool_call> <tool_response> ... </tool_response> Step 3 <think> need to plan weekend hiking itinerary from Chengdu to Qingcheng Back Mountain for the user, suitable for two people, focusing on natural scenery and photography, with budget of 600 RMB per person. have already obtained basic information and transportation options for Qingcheng Back Mountain, and now need to further understand the hiking routes and points of interest in Qingcheng Back Mountain in order to design detailed schedule. </think> <tool_call> {\"name\": \"universal_search\", \"arguments\": {\"query\": \"Hiking Routes and Prime Photography Technical Report Tongyi DeepResearch Spots on Qingcheng Back Mountain\"}} </tool_call> <tool_response> ... </tool_response> Step 4 <think> need to plan weekend hiking itinerary from Chengdu to Qingcheng Back Mountain for the user, with an emphasis on natural scenery and photography, and budget of 600 RMB per person. have already obtained the location information and some basic data for both Chengdu and Qingcheng Back Mountain, and now need to further understand the location of Chengdu in order to plan the transportation route. </think> <tool_call> {\"name\": \"search_poi\", \"arguments\": {\"query\": \"Chengdu\"}} </tool_call> <tool_response> ... </tool_response> Step <think> need to plan weekend hiking itinerary from Chengdu to Qingcheng Back Mountain for the user, with focus on natural scenery and photography, and budget of 600 RMB per person. have already obtained the location information of Chengdu and Qingcheng Back Mountain, as well as basic information about transportation and hiking routes. Now need to design specific travel plan and related details. First, need to determine the specific transportation route from Chengdu to Qingcheng Back Mountain. Based on previous search results, there are several main options for traveling from Chengdu to Qingcheng Back Mountain: 1. Take high-speed train from Chengdu to Qingchengshan Station, then transfer to scenic area shuttle bus or taxi to get to the Back Mountain area; 2. Drive directly to Qingcheng Back Mountain. will use the get_navigation tool to plan the transportation route from Chengdu to Qingcheng Back Mountain. </think> <tool_call> {\"name\": \"get_navigation\", \"arguments\": {\"start_lon\": 104.066301, \"start_lat\": 30.572961, \"end_lon\": 103.487136, \"end_lat\": 30.93071}} </tool_call> <tool_response> ... </tool_response> Step 6 <answer> Chengdu to Qingcheng Rear Mountain Hiking Guide (Nature Landscape Photography Trip for Two) Trip Overview Total budget: 600 RMB per person (1,200 RMB for two) Duration: 2 days over weekend (depart Saturday, return Sunday) Group size: 2 people Main activities: Hiking, nature landscape photography, leisure sightseeing Detailed Itinerary Day 1: Chengdu Qingcheng Rear Mountain Morning: Depart from Chengdu 07:0008:00 Depart from Chengdu for Qingcheng Mountain Recommended transport: High-speed train from Chengdu Xipu Station to Qingchengshan Station Duration: about 30 min Fare: 10 RMB Alternative: Self-drive Duration: about 1.5 25 Technical Report Tongyi DeepResearch hours Distance: 50 km 08:3009:00 After arriving at Qingchengshan Station, head to the Taian Ancient Town parking lot Take the scenic-area shuttle bus (16 RMB/person) or taxi ( 50 RMB/person) to the Rear Mountain entrance Buy Rear Mountain entrance tickets (20 RMB/person) Midday: Lunch in the Scenic Area 09:3011:00 Lunch inside the scenic area Recommended: Eat at Youyicun (Youyi Village) or Taian Ancient Town Try local specialties such as smoked pork ribs and potatoes Budget: About 3050 RMB per person Afternoon: Qingcheng Rear Mountain Hike 11:0018:00 Hiking route on Qingcheng Rear Mountain Route: Wulong Valley Youyi Village Baiyun Cave Cuiying Lake Feiquan Valley Taian Ancient Town Route difficulty: Recommended Route 2 (about 70% difficulty), includes boat ride, cable car, and hiking Main sights: Wulong Valley: Beautiful scenery, best starting point for the hike Youyi Village: Small mountain village with rest stops and snacks Baiyun Cave: Worth exploring Cuiying Lake: Boat ride available (3 RMB/person), very scenic Feiquan Valley: Flowing streams and natural landscapes Cable car option: Baiyun Cableway (45 RMB/person one way) to save energy Photography tips: The section from Wulong Valley to Youyi Village has the best views Cuiying Lake and Feiquan Valley are also great for photography 18:0019:00 Return to the scenic area entrance Take sightseeing bus (10 RMB/person) to the visitor center, then shuttle bus to Qingchengshan Station Or take taxi ( 20 RMB/person) directly to Qingchengshan Station Evening: Return to Chengdu 19:3021:00 Take the high-speed train back to Chengdu Day 2: In-Depth Exploration & Photography Morning: Depart from Chengdu 07:0008:00 Depart from Chengdu for Qingcheng Mountain Same transport options as Day 1 Midday: Lunch in the Scenic Area 08:3009:30 Lunch in the scenic area Same recommendations as Day 1 Afternoon: In-Depth Hiking on Qingcheng Rear Mountain 09:3017:00 Deeper exploration of Qingcheng Rear Mountain Route choice: You can try Route 3 (100% difficulty). Use caution; suitable for hikers with some experience. Key sights: Wanfo Cave: whole rock wall covered with Buddha statues, very impressive Qianfo Cave: Also worth visiting Santan Wuquan (Three Pools and Misty Springs): must-see classic spot Photography tips: Afternoon light is better for shooting caves and pools 17:0018:00 Return to the scenic area entrance Same transport options as Day 1 Evening: Return to Chengdu 18:3020:00 Take the high-speed train back to Chengdu Cost Breakdown (Total 1,200 RMB for Two) Transportation High-speed train: ChengduQingchengshan round trip About 40 RMB/person In-park transport: Scenic-area shuttle bus/taxi About 3050 RMB/person Return transport: From Qingchengshan back to Chengdu About 4060 RMB/person Tickets Qingcheng Rear Mountain entrance: 20 RMB/person (40 RMB for two) Cable car: Baiyun Cableway 45 RMB/person one way (round trip for two: 180 RMB, if needed) Cuiying Lake boat: 3 RMB/person (6 RMB for two) Food Meals in the scenic area: About 3050 RMB/person (60100 RMB for two) Snacks & supplies: Water, energy snacks, etc. About 50 RMB/person (100 RMB for two) Accommodation (If Extending the Trip) In-park lodging: Zhuolan Villa or local guesthouses About 200300 RMB/night Recommended: Lanxipan Guesthouse in Dujiangyan Offers pick-up/drop-off and has pleasant environment Photography Gear Camera: 8001,000 RMB for two if renting (about 200300 RMB/person) Lenses: About 500800 RMB for two (if renting) Other gear: Tripod, filters, memory cards, etc. About 200300 RMB/person Practical Tips Best visiting time JuneSeptember: 08:0018:00 (MonSun) JanuaryMay & OctoberDecember: 08:0017:30 Hiking gear Comfortable, non-slip hiking shoes (essential) Lightweight backpack Drinking water and energy snacks Sun protection (hat, sunscreen, etc.) Camera tripod (essential for photography) Photography suggestions Early morning and late afternoon light is best for landscapes After rain, Qingcheng Mountain is often shrouded in mistexcellent for photos Cuiying Lake and Feiquan Valley are popular photography spots Notes & safety 26 Technical Report Tongyi DeepResearch Mountain trails are steep; be careful and consider bringing trekking poles Toilets are scarce on the mountain; use water sparingly Mountain temperatures are lower than in the city; bring jacket Check the weather forecast in advance and avoid heavy rain Keep your phone charged and communication available Wish you wonderful trip and beautiful photos of Qingcheng Mountain! </answer> Technical Report Tongyi DeepResearch F.2 Case Study Following ArenaRL Case Trajectory in Open-Travel Benchmark Following ArenaRL (translated into English) Query: two-day weekend hiking trip from Chengdu to Qingcheng Back Mountain for two people, with focus on natural scenery and photography, and budget of 600 RMB per person. Step <think> The user wants to learn about hiking itinerary from Chengdu to Qingcheng Back Mountain, suitable for two people, with preference for natural scenery and photography, and budget of 600 RMB per person. need to first obtain basic information about Qingcheng Back Mountain, and then plan the travel route. </think> <tool_call> {\"name\": \"universal_search\", \"arguments\": {\"query\": \"Hiking Route from Chengdu to the Qingcheng Back Mountain, Natural Scenery, Photography\"}} </tool_call> <tool_response> ... </tool_response> Step 2 <think> have already obtained the basic information and transportation options for hiking from Chengdu to Qingcheng Back Mountain. Now need to further look up more detailed information, including the exact location of Qingcheng Back Mountain and the locations of its main scenic spots, in order to plan reasonable hiking route. First, need to use the search_poi tool to find the location information of Qingcheng Back Mountain. </think> <tool_call> {\"name\": \"search_poi\", \"arguments\": {\"query\": \"Qingcheng Back Mountain Scenic Area\"}} </tool_call> <tool_response> ... </tool_response> Step 3 <think> have already obtained the basic information and location of Qingcheng Back Mountain. Now need to continue searching for more information about the locations of the main scenic spots inside Qingcheng Back Mountain, in order to plan reasonable hiking route. According to previous search results, the main attractions of Qingcheng Back Mountain include Taian Ancient Town, Wulong Valley, Youyicun, Cuiying Lake, etc. need to look up the exact location of each of these spots one by one. </think> <tool_call> {\"name\": \"search_poi\", \"arguments\": {\"query\": \"Taian Ancient Town at the Qingcheng Back Mountain\"}} </tool_call> <tool_response> ... </tool_response> Step 28 Technical Report Tongyi DeepResearch <think> have already obtained the basic information of Qingcheng Back Mountain and the location of Taian Ancient Town. Now need to continue searching for the location of Wulong Valley, because it is one of the most important scenic spots in Qingcheng Back Mountain. </think> <tool_call> {\"name\": \"search_poi\", \"arguments\": Mountain\"}} </tool_call> <tool_response> ... </tool_response> {\"query\": \"Wulong Valley at the Qingcheng Back Step <think> have already obtained the basic information of Qingcheng Back Mountain, as well as the location information of Taian Ancient Town and Wulong Valley. Now need to continue searching for other main attractions, such as Youyicun, Cuiying Lake, etc., in order to plan complete hiking route. </think> <tool_call> {\"name\": \"search_poi\", \"arguments\": {\"query\": \"Youyicun at the Qingcheng Back Mountain\"}} </tool_call> <tool_response> ... </tool_response> Step 6 <think> have obtained the location information of the main attractions of Qingcheng Back Mountain, including Taian Ancient Town, Wulong Valley, and Youyicun. Now need to continue looking up the location of Cuiying Lake, and then plan complete hiking route. </think> <tool_call> {\"name\": Mountain\"}} </tool_call> <tool_response> ... </tool_response> \"Cuiying Lake at the Qingcheng Back \"search_poi\", \"arguments\": {\"query\": Step 7 <think> have collected the location information of the main attractions of Qingcheng Back Mountain, including Taian Ancient Town, Wulong Valley, Youyicun, and Cuiying Lake. Now need to continue searching for some other important attractions, such as Laifeng Pavilion and Water Curtain Cave, in order to plan complete hiking route. </think> <tool_call> {\"name\": \"search_poi\", \"arguments\": {\"query\": \"Laifeng Pavilion at the Qingcheng Back Mountain\"}} </tool_call> <tool_response> ... </tool_response> 29 Technical Report Tongyi DeepResearch Step <think> have collected the location information of the main scenic spots in Qingcheng Back Mountain, including Taian Ancient Town, Wulong Valley, Youyicun, Laifeng Pavilion, and Cuiying Lake. Now need to plan reasonable hiking route and provide detailed navigation information. Based on the information collected, will plan hiking route starting from Taian Ancient Town, passing through Wulong Valley, Youyicun, Laifeng Pavilion, and Cuiying Lake, and finally returning to Taian Ancient Town. In this way, the route can cover the main natural scenic spots of Qingcheng Back Mountain and is suitable for photography enthusiasts. Next, need to use the get_navigation tool to obtain the navigation information for this route. </think> <tool_call> {\"name\": 30.920645, \"103.473101,30.923365;103.473823,30.934175;103.476093,30.931739;103.483087,30.936879\"}} </tool_call> <tool_response> ... </tool_response> \"start_lat\": \"via_points\": \"get_navigation\", \"arguments\": {\"start_lon\": 103.490619, 103.490619, \"end_lon\": 30.920645, \"end_lat\": Step 9 <answer> Chengdu to Qingcheng Rear Mountain Hiking Guide (Weekend 2-Day Trip) Trip Overview Qingcheng Rear Mountain is one of the best hiking destinations around Chengdu, famous for its natural scenery and tranquil environment, and is especially suitable for photography enthusiasts. Below is detailed 2-day itinerary to help you fully experience the beauty of Qingcheng Rear Mountain. Transportation Getting to Qingcheng Mountain High-speed train: Take high-speed train from Chengdu Xipu Station to Qingchengshan Station (about 30 minutes, 10 RMB per person one way). Self-drive: About 2 hours from Chengdu. Navigate to Qingchengshan Rear Mountain Scenic Area Parking Lot (there are Parking Area and Parking Area B). Transport within the Scenic Area After getting off at Qingchengshan Station, youll find tourist shuttle buses to the Front and Rear Mountains on the left side outside the station. Rear Mountain costs 15 RMB per person one way (about 40 minutes). Inside the scenic area there are sightseeing shuttle buses (10 RMB per person), or you can walk. Tickets Adult entrance ticket: 20 RMB per person Student ticket: 10 RMB per person Sightseeing shuttle bus in the scenic area: 10 RMB per person (one way) Luggage storage: 5 RMB per use Accommodation Suggestions Inside the Scenic Area Zhuolan Shanzhuang (Zhuolan Mountain Villa): Located at the foot of the mountain, offers mountain views, good service, and pick-up/drop-off service. Qixin Shanyin Guesthouse: Located in Taian Ancient Town, with beautiful environment and attentive service. Youyicun Yuchengyuan: Located inside the Youyicun scenic area, quiet and secluded surroundings. Outside the Scenic Area There are many guesthouses and farmhouse inns in Taian Ancient Town, generally 200500 RMB per night. Booking in advance is recommended. Food and Dining Dining in the Scenic Area There are restaurants and snack stalls in the scenic area, but prices are relatively high. Its recommended to eat in the ancient town, where you can find 2-person set meals for around 30 RMB (one meat dish, one vegetable dish, one soup). Local specialties: farmhouse dishes, wild mushrooms from Qingcheng Mountain, free-range chicken, corn, etc. Bringing Your Own Food There are water sources and small shops along the hiking route, so you can bring some snacks and water. Its not recommended to carry too much foodkeep your pack as light as possible. Recommended Hiking Route Route Overview Taian Ancient Town Wulong Valley Youyicun (Another Village) 30 Technical Report Tongyi DeepResearch Laifeng Pavilion Cuiying Lake Taian Ancient Town Detailed Itinerary Day 1 Morning Depart from Chengdu for Qingcheng Mountain (by high-speed train or by car). After arriving at Qingchengshan Station, take the tourist shuttle bus to the Rear Mountain scenic area (about 40 minutes). Buy tickets at the scenic area entrance (20 RMB per person). Start hiking from Taian Ancient Town, following Qingcheng Mountain Road. Midday Arrive at Wulong Valley, which features waterfalls, mountain springs, and ancient plank roads with beautiful scenery. Have lunch and rest near Wulong Valley (you can bring your own food or eat at restaurant in the scenic area). Afternoon Continue hiking to Youyicun; the scenery along this stretch is very pleasant. Visit the Youyicun scenic spot and enjoy the mountain and water views. Head to Laifeng Pavilion, an excellent viewpoint for sightseeing and photography. On the way, enjoy the scenery of Cuiying Lake; the water is clear and the environment serene. Return to Taian Ancient Town in the evening and have dinner there. Day 2 Morning Depart from Taian Ancient Town and hike along Qingcheng Mountain Road. Focus on exploring Wulong Valley, the most scenic section of the Rear Mountain, with many waterfalls and mountain springs. You can choose to take bamboo raft in Wulong Valley for different way of sightseeing. Midday Have lunch and rest in Youyicun, and sample local farmhouse specialties. Afternoon Continue hiking to Cuiying Lake; the water is crystal clear and the surroundings are tranquil. Stay at Cuiying Lake for photos and to enjoy the reflections of the mountains in the water. Return to Taian Ancient Town in the evening, ending the trip. Recommended Photography Spots Wulong Valley: The most beautiful section of the Rear Mountain, with waterfalls, springs, and ancient plank roads. Bamboo rafting is available. The scenery is serene and highly photogenic. Laifeng Pavilion: Excellent vantage point for panoramic shots of the surrounding mountains and waters. Cuiying Lake: Clear water and tranquil surroundings, ideal for shooting reflections. Youyicun: rustic village integrated with natural scenery, combining cultural and natural elements. Suggested Hiking Gear Comfortable hiking shoesmountain trails are rugged, so footwear comfort is very important. Bring trekking polessome sections are steep and narrow; the left-hand ascent offers better photo spots and can be tricky. Pack light; avoid unnecessary items. Bring enough drinking water, but note that there are very few toilets in the scenic area, so dont drink excessively. Bring camera and spare batteriesQingcheng Rear Mountains natural scenery is excellent for photography. Practical Tips Best time to start: Set out around 8:309:00 a.m. to avoid peak crowds. Descent timing: Keep an eye on time; going down usually takes longer than going up. Weather: Check the forecast; trails can be slippery in the rain, so prepare for traction and safety. Sun protection: UV can be strong in the mountains; use sunscreen and wear hat. Warm clothing: Even in summer it can be cool on the mountain; bring light jacket. Pack light: Minimize what you carry so you can move easily. Environmental awareness: Take your trash with you and protect the natural environment. Budget Reference (Per Person) Transportation: Round-trip high-speed train 20 RMB + tourist shuttle bus 30 RMB = 50 RMB Tickets: Scenic area ticket 20 RMB + sightseeing shuttle bus 10 RMB (round trip) = 40 RMB Accommodation: 200500 RMB (depending on guesthouse or farmhouse inn) Food: 30 RMB/day (for two people) 2 days = 120 RMB Gear: Hiking shoes, trekking poles, photography gear, etc. about 100 RMB Other: Water, snacks, small souvenirs, etc. about 50 RMB Total: Approximately 460610 RMB per person This budget is for reference only; actual costs will vary based on accommodation choices and dining preferences. If you stay outside the scenic area or purchase equipment in advance, your total expenses may be lower. Wish you pleasant trip and wonderful photos of Qingcheng Rear Mountain! 31 Technical Report Tongyi DeepResearch </answer> 32 Technical Report Tongyi DeepResearch Figure 5: Prompt of Open-Travel task. Technical Report Tongyi DeepResearch Figure 6: Prompt of Open-DeepResearch task. 34 Technical Report Tongyi DeepResearch Figure 7: Chinese prompt of open-ended writing task. Technical Report Tongyi DeepResearch Figure 8: English prompt of open-ended writing task. 36 Technical Report Tongyi DeepResearch Figure 9: Prompt of search flights tool. 37 Technical Report Tongyi DeepResearch Figure 10: Prompt of search train tickets tool."
        }
    ],
    "affiliations": [
        "Amap, Alibaba Group",
        "Tongyi Lab, Alibaba Group"
    ]
}