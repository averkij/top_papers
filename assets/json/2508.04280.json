{
    "paper_title": "Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success",
    "authors": [
        "George Bredis",
        "Stanislav Dereka",
        "Viacheslav Sinii",
        "Ruslan Rakhimov",
        "Daniil Gavrilov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Interactive multimodal agents must convert raw visual observations into coherent sequences of language-conditioned actions -- a capability that current vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL) efforts could, in principle, endow VLMs with such skills, but they have seldom tested whether the learned behaviours generalize beyond their training simulators, and they depend either on brittle hyperparameter tuning or on dense-reward environments with low state variability. We introduce Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight, hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens while learning value only at the environment-step level: an arrangement, to our knowledge, not previously explored for large VLMs or LLMs. This simple decoupling removes unstable weighting terms and yields faster, more reliable convergence. Training a single VLM with VL-DAC in one inexpensive simulator at a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies that generalize widely: +50\\% relative on BALROG (game-centric agentic control), +5\\% relative on the hardest part of VSI-Bench (spatial planning), and +2\\% on VisualWebBench (web navigation), all without degrading general image understanding accuracy. These results provide the first evidence that a simple RL algorithm can train VLMs entirely in cheap synthetic worlds while delivering measurable gains on real-image agentic, spatial-reasoning, and web-navigation benchmarks."
        },
        {
            "title": "Start",
            "content": "Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success George Bredis, Stanislav Dereka, Viacheslav Sinii, Ruslan Rakhimov, Daniil Gavrilov T-Tech Correspondence: g.bredis@tbank.ru 5 2 0 2 6 ] . [ 1 0 8 2 4 0 . 8 0 5 2 : r Abstract Interactive multimodal agents must convert raw visual observations into coherent sequences of language-conditioned actions capability that current vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL) efforts could, in principle, endow VLMs with such skills, but they have seldom tested whether the learned behaviours generalize beyond their training simulators, and they depend either on brittle hyperparameter tuning or on dense-reward environments with low state variability. We introduce VisionLanguage Decoupled Actor-Critic (VL-DAC), lightweight, hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens while learning value only at the environment-step level: an arrangement, to our knowledge, not previously explored for large VLMs or LLMs. This simple decoupling removes unstable weighting terms and yields faster, more reliable convergence. Training single VLM with VL-DAC in one inexpensive simulator at time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies that generalize widely: +50% relative on BALROG (game-centric agentic control), +5% relative on the hardest part of VSI-Bench (spatial planning), and +2% on VisualWebBench (web navigation), all without degrading general image understanding accuracy. These results provide the first evidence that simple RL algorithm can train VLMs entirely in cheap synthetic worlds while delivering measurable gains on real-image agentic, spatial-reasoning, and webnavigation benchmarks. Code: https://github.com/corl-team/VL-DAC Introduction Large language models (LLMs) behave like capable singleturn agents in text-only domains, where reinforcement learning (RL) can be applied without manual annotation (OpenAI et al. 2024; DeepSeek-AI et al. 2025). Yet they still stumble when task unfolds over many turns, revealing open problems in long-horizon reasoning and credit assignment the main limitation to general-purpose agency. These challenges intensify for vision-language models (VLMs) ((Wang et al. 2024b), (Chen et al. 2024b)): in addition to planning across multiple steps, VLM must parse constantly changing visual stream. While state-of-the-art VLMs excel at describing static images and videos, they struggle to decide what to do next in interactive scenes (Chow et al. 2025; Paglieri et al. 2024). Figure 1: Real-world skill transfer after synthetic training. Our method, VL-DAC, improves agentic control, spatial planning, and embodied reasoning on BALROG, VSIBench, and ERQA. It demonstrates effective transfer from synthetic environments to real-world benchmarks. Collecting genuine, step-by-step vision-language interaction data is expensive and slow; as result, most training corpora contain only static image-text pairs, so VLMs become excellent describers but poor actors. If we want to teach model agentic skills or an understanding of dynamic environments, we need methods and data that provide such knowledge; arguably, multi-step training in dynamic environments might be the main path toward such learning. Simulators provide cheaper workaround, yet existing RL algorithms still stumble. RL4VLM (Zhai et al. 2024) depends on brittle coefficient that mixes thought and action probabilities, and slight mis-tuning makes learning diverge. LOOP (Putta et al. 2024) aggregates reward across an entire action sequence, so its credit assignment breaks down when successive states vary greatly. ArCHer (Zhou and Zanette 2024) counters variance with learned critic, but the critic trains well only under dense rewards and substantial off-policy buffer. Both of them are hard to maintain in long, sparse visual episodes. What we find. Experiments in several lightweight simulators (MiniWorld (Chevalier-Boisvert et al. 2023), GymCards, ALFWorld (Shridhar et al. 2021), and WebShop (Yao et al. 2023)) reveal that transferable visuomotor skills emerge when two ingredients are present: (i) simulator that is cheap enough to try many tasks, and (ii) an RL recipe that can be dropped in without delicate retuning. Training single VLM in one simulator at time still lifts performance on natural-image agentic, spatial-reasoning, and web-navigation benchmarks, showing that realism and scale are less limiting than the practicality of the learning rule. This recipe unlocks path toward environment scaling and scenarios in which one can easily train model in new environments and switch between them without brittle hyperparameter tuning, learning new skills off the shelf. Our solution: Vision-Language Decoupled Actor-Critic (VL-DAC). To meet that practicality requirement, we propose VL-DAC, an RL objective that cleanly separates the learning signals: Action loss-token-wise Proximal Policy Optimization (Schulman et al. 2017). Value loss-computed once per environment step, with gradients stopped at the VLM backbone. This token/step split, to our knowledge unused at VLM scale, eliminates RL4VLMs brittle weighting term, avoids LOOPs sequence-level credit-assignment pitfalls, and dispenses with ArCHers bulky replay buffer and reward requirement. The outcome is concise, environment-agnostic algorithm that converges faster and ports across simulators with minimal fuss-exactly what is needed to push RLtrained VLMs into new domains at low cost. Contributions Vision-Language Decoupled Actor-Critic (VL-DAC). We propose an RL objective that pairs token-wise PPO updates with step-level value head whose gradients are stopped at the VLM backbone; minimal stabilization kit (KL regularization, value warm-up, and stop-gradient) lets VL-DAC train without the fragile weighting terms or replay buffers required by earlier methods. Cheap-simulator real-task transfer. Training the same VLM in one lightweight simulator at time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already yields sizable relative gains on different benchmarks. This shows that simulator affordability and algorithm simplicity are the key ingredients for transfer. Skill-transfer study. We provide the first systematic analysis of how simulator-acquired skills map onto agentic, spatial, and web-interaction benchmarks, and ablate each VL-DAC component to pinpoint the elements that drive stability and generalization. Taken together, our results demonstrate that modest algorithmic tweak, combined with low-cost simulators, suffices to unlock practical RL training for VLMs, endows them with transferable real-world competence, and opens path toward environment scaling and large-scale learning from experience."
        },
        {
            "title": "Background",
            "content": "Vision-Language Agents in Interactive Environments We model each episode as finite-horizon Markov Decision Process (MDP) = S, A, P, R, γ, where γ [0, 1) is the discount factor. Unlike classical RL, the state st is tuple (xt, ct) consisting of an RGB image (or stack of images) xt RHW 3 and an optional text context ct (system prompt, dialogue history, etc.). The action at is sequence of natural-language tokens that fully specifies the next low-level step in the environment (e.g., \"turn left 15\" or \"click button id=OK\"). An agent executes trajectory τ = (s1, a1, . . . , sT , aT ) and seeks to maximize the discounted return J(θ) = Eτ πθ (cid:104) (cid:88) (cid:105) γ t1 R(st, at) , t=1 where the policy πθ(at st) is parameterized by large vision-language model (VLM) and factorizes autoregressively, (cid:1). (cid:0)a(i) st, a(<i) πθ(at st) = (cid:81)at i=1 πθ During training, we may additionally learn state-value function Vϕ(st) = Eτ πθ [(cid:80) k0 γ kR(st+k, at+k)], but the way action and value updates interact differs across methods, as reviewed next. In VL-DAC, we retain this shared backbone but prevent value-head gradients from flowing back, thereby eliminating cross-signal interference. Existing RL Algorithms for Multi-Step VLMs & LLMs Below, we summarize the three baselines that dominate recent work and pinpoint the specific pain points that motivate our Vision-Language Decoupled Actor-Critic (VLDAC) objective introduced in Section . RL4VLM (Zhai et al. 2024). The policy is decomposed into thought segment (athought) and an action segment (aaction). RL4VLM multiplies token-logits of the thought span by λ [0, 1], effectively rescaling gradient magnitudes: log πθ(at st) = = λ log πθ(athought st) + log πθ (cid:0)aaction st, athought (cid:1), (1) after which, PPO updates are applied at the step level. But λ needs to be tuned for each model-environment setup. This makes it hard to scale the method beyond single environment and limits environment scaling. LOOP (Chen et al. 2025b). LOOP employs leave-oneout advantage estimation and trains an LLM in multi-step scenario using PPO. Because it uses PPO, different policyupdate levels (token, step, and trajectory) can be explored; the authors show that the best quality is achieved at the token level. LOO advantage estimation: = (cid:32) R(cid:0)s0:T , a0:T (cid:1) 1 (cid:88) j= (cid:33) R(cid:0)s0:T , a0:T (cid:1) (2) that yields greater stability without extra hyperparameters (besides the PPO (Schulman et al. 2017) parameters). Let at = (a1 ) denote the tokens emitted at time t. We apply the PPO objective independently to each token: , . . . , aat LVL-DAC policy (θ) = Eτ (cid:104) 1 at (cid:16) min at (cid:88) i=1 rt,iAt, clip(rt,i, 1 ϵ, 1 + ϵ)At (3) (cid:17)(cid:105) , where rt,i = πθ(ai ) and the advantage At is still computed at the step level using GAE (Schulman et al. 2018). )/πθold(ai st, a<i st, a<i Step-level value loss. Vϕ shares the backbone with πθ but has its own MLP head. The value head predicts Vϕ(st) once per environment step: Vϕ(st) = MLPϕ (4) (cid:0)FVLM(st)(cid:1). (cid:0)Vϕ(st) ˆRt (cid:1)2 The value loss is LValue(ϕ) = 1 . For step2 level advantage estimation, we use GAE (Schulman et al. 2018). Stabilization. For stabilization, we employ well-known techniques from the classical RL setup (Lehmann 2024), but these are currently underexplored in large language-model scenarios. We warm up ϕ for epochs before updating θ, use StopGrad for the value head, and apply per-token forward KL penalty: LKL(θ) = DKL (cid:0)πθ( st) πold( st)(cid:1). (5) Full objective. The final training loss combines the three terms: L(θ, ϕ) = LVL-DAC policy (θ) + β LKL(θ) + α LValue(ϕ). (6) We show empirically that this simple decoupling yields more stable learning curves and higher final returns than both RL4VLM (Zhai et al. 2024) and LOOP (Chen et al. 2025b). We further demonstrate that simple RL training transfers the learned skills to downstream benchmarks. For the concrete prompting setup, refer to Appendix A."
        },
        {
            "title": "Experiments",
            "content": "Our study asks four questions: Q1 Does VL-DAC train more simply (in terms of hyperparameter search) than RL4VLM in diverse simulators? We also explore how each stabilization tweak (KL, value warm-up, stop-gradient) contributes to performance, how brittle RL4VLMs λ can be (beyond the original exploration), and how our method performs in models of different sizes and architectures. Q2 How does VL-DAC compare with LOOP when long multi-step credit assignment is required? Q3 Do policies learned in one cheap simulator transfer skills to benchmarks, especially on agentic tasks? Q4 Is the method scalable to tasks that require long-term planning, such as WebShop, and how does such training contribute to the web benchmark? Figure 2: Vision-Language Decoupled Actor-Critic (VLDAC) pipeline. vision-language model receives RGB frames and text context, predicts token-wise actions via PPO, and learns step-level value head whose gradients are stopped at the backbone. The approach sidesteps any need for tuning token mixtures but suffers from extreme credit-assignment noise: single bad token can wipe out the reward signal for the entire chain, making long-horizon tasks hard to learn. ArCHer (Zhou and Zanette 2024). ArCHer trains critic with bootstrapped one-step TD (Sutton 1988) at the step level and trains the actor LM from critic feedback. Since the method is primarily designed to be off-policy, it requires large replay buffer. The method works under dense rewards, but two practical issues emerge when we want to train onpolicy (e.g., when it is hard to maintain large buffer) or have sparse rewards (due to the critic design): Replay bottleneck. Memory demands grow with episode length, which is acute for vision tasks where each step embeds high-dimensional image, multiple images, or video. Reward sparsity. When rewards arrive only at episode termination, the critics bootstrap targets become nearly constant, offering little learning signal. Vision-Language Decoupled Actor-Critic (VL-DAC) Training VL-DAC retains the intuitive separation between reasoning (thought) and behaviour (action) tokens advocated by RL4VLM, but removes the additional coefficient by moving the policy loss to the token level while keeping the value loss at the step level. Figure 2 presents the overall pipeline of our approach. Token-level policy loss. Although token-wise reinforcement losses have been explored in earlier language or visionand-language work ((Misra, Langford, and Artzi 2017), (Chen et al. 2025b)), they have not been combined with step-level value objective nor systematically evaluated on modern high-capacity VLMs. Our contribution is therefore to integrate this granularity in multi-step VLM scenarios with Vision-Language Decoupled Actor-Critic design Figure 3: Episode success rates across environments. Success rates (%) of our method vs. RL4VLM (averaged over multiple λ values) on six environments: MiniWorld-Hallway, OneRoom, FourRooms (top row), WallGap, EZPoints, ALFWorld (bottom row). While RL4VLM requires tuning λ per environment, our method performs robustly without tuning. We first summarize the experimental setup, then tackle the four questions in turn. We do not include ArCHer in the main-text experiments, since it works poorly under the onpolicy scenario (where the training buffer equals the replay buffer) and when rewards are sparse (due to the one-step TD bootstrap). For experiments with ArCHer, see Appendix E."
        },
        {
            "title": "Setup",
            "content": "Simulators. We use several lightweight environments: MiniWorld (four navigation tasks) for navigation and routeplanning, Gym-Cards/EZPoints (card-selection logic) an easy-to-check environment, ALFWorld (textas conditioned household tasks) for navigation, spatial reasoning, and agentic capabilities, and WebShop (e-commerce browsing) as domain that requires long-term understanding and web-based planning. All produce RGB frames plus textual instruction; the agent answers with free-form text that consists of thoughts and action tokens. The total response is parsed into environment actions. Model and training. Unless noted otherwise, we finetune Qwen2-VL-7B (Wang et al. 2024b) with LoRA (Hu et al. 2022) adapters for 25k-50k environment steps. If table refers to the model as base, it corresponds to Qwen2-VL-7B, unless stated otherwise. For the hyperparameter grid, check Appendix B. Compute budget. Training VL-DAC for 50k environment steps on Qwen2-VL-7B takes 20 GPU-hours on single NVIDIA H100-80GB. Q1. Stability: VL-DAC vs. RL4VLM Comparison with RL4VLM. Figure 3 plots SR over 50k steps for Hallway, FourRooms, OneRoom, WallGap, ALFWorld, and Gym-Cards. Curves for RL4VLM are shown as an average of the thought-coefficient λ values recommended by the authors; VL-DAC uses the same optimizer and other hyperparameters, with no extra tuning. VL-DAC reaches high SR in five of six tasks, whereas RL4VLM diverges or plateaus whenever λ is not properly tuned. All RL4VLM experiments here use the same stabilization techniques as VLDAC. For results without average and additional details on runs, see Appendix D. Stabilization ablation. Figure 4 shows SR on OneRoom when we add KL regularization, value warm-up, and stopgradient one at time on top of RL4VLM (λ=0.3, the best setting for OneRoom in our experiments). Each component improves convergence speed and reduces variance; all three together boost convergence, and adding VL-DAC on top further increases training stability and final quality. The illustrated standard deviation intervals were obtained with four different seeds. Evaluation metrics. Simulator success rate (SR) is the percentage of episodes that reach the goal. Skill transfer is assessed using skill-based benchmarks (and their subsets), along with suite of captioning tasks to check for regressions. For the full evaluation setup, see Appendix C. Model and λ comparison. Table 1 reports RL4VLM peak SR across different λ values and models, alongside VLDACs off-the-shelf run. To produce standard deviations, we ran each model under the same setup with four different seeds. RL4VLM training with different models and λ setups"
        },
        {
            "title": "Setup",
            "content": "SR Qwen2-VL-7B RL4VLM (λ = 0.35) RL4VLM (λ = 0.5) Qwen2-VL-7B Qwen2-VL-7B Ours RL4VLM (λ = 0.35) Gemma3-4B RL4VLM (λ = 0.5) Gemma3-4B Ours Gemma3-4B 0.98 0.00 0.93 0.07 0.98 0.02 0.55 0.38 0.82 0.14 0.93 0.05 Table 1: RL4VLM vs. ours. Evaluated on Qwen2-VL and Gemma over four seeds with varying λ. Qwen2-VL peaks at λ = 0.35 in OneRoom, while Gemma prefers λ = 0.5. Our method is robust and low-variance across both, even on the harder Gemma task."
        },
        {
            "title": "Base",
            "content": "ALFWorld-tuned"
        },
        {
            "title": "Balrognaive\nBalrogCoT",
            "content": "3.21% 0.75% 4.19% 0.92% 3.94% 0.98% 6.02% 1.19% Table 2: Balrog performance across prompting strategies. RL training (notably VL-DAC) raises scores even with naive prompts, and Chain-of-Thought prompting adds further >50% boost. shows huge changes in both the standard deviation and the best SR, whereas our method works consistently, independently of the setup. Interestingly, for RL4VLM, the optimal λ changes with the model, and on Gemma3-4B (Team et al. 2024), RL4VLM exhibits very large standard deviation regardless of λ, which casts doubt on its practical usability. Bottom line. VL-DAC inherits the best of RL4VLM after the stabilization tweaks and removes the hyperparameter that still limits RL4VLM in practice due to the need for tuning. Q2. Long-horizon credit: VL-DAC vs. LOOP On four sparse-reward MiniWorld environments (Hallway, FourRooms, OneRoom, and WallGap), we trained LOOP (Chen et al. 2025b) and VL-DAC. As Figure 5 shows, LOOPs success rate plateaus after 15-30k steps, whereas VL-DAC keeps climbing. The difference stems from credit assignment: LOOP feeds the same noisy, sequence-level return to every token, while VL-DACs step-level critic delivers stable advantages throughout training. In long-horizon, sparse-reward settings, sequence-level methods like LOOP stall, whereas VL-DACs decoupled token/step objective continues improving, yielding up to +34 pp higher success without extra tuning. Q3. From MiniWorld/ALFWorld to skill-based benchmark tests Tables 2 and 3 list downstream scores after training in one simulator at time. BALROG(Paglieri et al. 2024) probes long-horizon agentic skills required to solve videogames, SI-Bench (Yang et al. 2025) subsets test spatial reasoning and planning, ERQA (Team et al. 2025) checks spatial reasoning, Figure 4: Ablation study of stabilization tricks. Adding KL regularization, value warm-up, and stop-gradient cuts variance sequentially; replacing the step-level policy loss with VL-DACs token-level objective yields the smooth ascent reported in Figure 3. uirBench (Wang et al. 2024a) covers multi-image understanding, and VideoMMEspatial (Fu et al. 2024) checks spatial understanding. BALROG: +50% relative gain in agentic success after ALFWorld training (mean std across four seeds), demonstrating that multi-step environments chiefly improve agentic skills. Skill-specific benchmarks: +5 % relative on the VSIBench Route-Planning task following ALFWorld training. Training in OneRoom also yields substantial gains on VSI-Bench subsets. We extended ERQA evaluation to naive and CoT (Wei et al. 2022) prompting and show improvements in both cases. Gains are also observed on MuirBench and the spatial subset of VideoMME. All results are single-seed due to the dataset scale. Image and video understanding: Table 4 shows that the model does not lose skills on general-purpose benchmarks ((Zhang et al. 2024a), (Fu et al. 2024), (Zhang et al. 2024b), (Yue et al. 2024), (Liu et al. 2024b), (Ying et al. 2024), (Hudson and Manning 2019), (Zhao et al. 2024), (Fu et al. 2024), (Chen et al. 2024a), (Yan et al. 2025)), on after training and sometimes even improves. Also, it is important to note that earlier research indicates that supervised learning needs accurate, large-scale data curation to yield small improvements in similar set of benchmarks without degrading performance on others. Figure 5: Long-horizon credit assignment: VL-DAC vs. LOOP. On four sparse-reward MiniWorld tasks, LOOP plateaus once early successes exhaust its high-variance sequence-level gradient, whereas VL-DAC continues improving. Token-wise advantages coupled with step-wise critic unlock sustained learning. VSI-Benchroute plan VSI-Benchrelative direction Base ALFWorld-tuned OneRoom-tuned 30.93 32.47 31. 32.01 31.61 33.05 ERQAnaive 38.25 39.00 39.25 ERQACoT MuirBench VideoMMEspatial 39.00 39.25 38.50 41.23 42.58 41.12 64.8 66.7 66. Table 3: Skill-specific benchmarks. Models trained in two different environments outperform the base model in their corresponding skill categories. Benchmark Base ALFWorld OneRoom Hallway GQA Mirb MMBenchdev MMEperception MMERealWorld MMStaravg MMT-mival MMTval VideoMME 62.02 37.38 78.86 1681 41.81 56.53 59.90 62.10 57.70 62.35 36.64 78.52 1688 41.46 57.03 60.40 62.36 58. 62.06 37.25 79.04 1670 41.76 57.51 60.66 62.65 57.40 62.12 37.25 78.52 1678 42.01 57.26 60.47 62.71 57.70 Table 4: Benchmark gains for Qwen2-VL finetuned on ALFWorld, MiniWorld-Hallway, and MiniWorldOneRoom. The finetuned model surpasses its instruct baseline in temporal and spatial reasoning, multi-image/video comprehension, and embodied-AI tasks. Q4. different domain: WebShop VisualWebBench We next train in WebShop for only 2k steps (due to compute budget). VL-DAC lifts VisualWebBench accuracy by up to +2 pp on different subsets over the base model, showing that even short interaction budgets can improve certain skills. We also explore how models trained in other environments affect the same benchmark. Mean and std computed across 4 seeds."
        },
        {
            "title": "Discussion",
            "content": "From Simple Recipe to Two-Stage Roadmap Our results suggest concise two-stage recipe for turning vision-language model into competent interactive agent. Stage 1 is algorithmic: adopt token-wise PPO objective coupled with step-wise value head. This decoupling, realized in VL-DAC, removes thought-action mixture coefficients, replay buffers, and other brittle knobs, giving hyperparameter-free learner that scales from 4 to 7 models without retuning. Stage 2 is environmental: feed the learner one of several lightweight simulators that span different action semantics-navigation, manipulation, card logic, and browser interaction. Stage 1 guarantees simple RL recipe; Stage 2 supplies the behavioural coverage necessary for realworld transfer. Why Simulator Diversity Matters Performance improvements grow with new skills. ALFWorld alone imparts agentic priors that lift BALROG success by over 50 % relative; ALFWorld and MiniWorld inject spatial planning and reasoning that raise VSI-Bench by 5 % relative; and WebShop injects UI-sequencing patterns that boost VisualWebBench by 2 %. Diverse simulators enhance wider range of skills. Limitations and Open Challenges Sparse-reward variance. Although the critic converges even with terminal rewards, the method still struggles in hard, sparse-reward settings. Beyond screen-based tasks. All environments studied here involve discrete interface actions on rendered images; continuous-control robotics remains untested. Single-agent assumption. VL-DAC does not address cooperative or adversarial multi-agent settings where credit must be distributed across agents. Memory and planning. Current models struggle to process and train in environments that require long-term abstract memory and planning (e.g., MiniWorld-WallGap). Model scale and task demands. Our evaluation covers 47B-parameter models; we have not yet assessed web caption webqa heading ocr element ocr element ground action prediction action ground basenaive basecot Snaive Scot ORnaive ORcot 27.81 0.11 28.38 0.20 29.31 0.02 29.04 0.12 28.19 0.00 29.21 0.00 71.44 0.00 61.11 0.11 70.32 0.00 62.58 0.05 70.91 0.00 59.89 0.00 75.62 1.26 74.83 0.00 76.34 0.00 72.66 0.00 74.03 0.12 74.44 0. 82.36 0.00 78.75 0.01 83.49 0.22 79.95 0.00 83.31 0.19 76.20 0.24 87.49 0.14 83.29 0.00 87.33 0.14 84.02 0.00 86.68 0.00 83.78 0.00 4.98 0.00 6.17 0.21 5.34 0.00 6.41 0.00 3.91 0.00 6.05 0.00 83.50 0.00 78.32 0.56 82.52 0.00 78.64 0.00 84.47 0.00 78.64 0.00 Table 5: VisualWebBench breakdown. 2k-step WebShop run lifts overall accuracy; web-caption and UI-action metrics benefit most. WS refers to WebShop, OR to OneRoom. smaller (below 1B) or much larger (tens to hundreds of billions) models. Additionally, successful training requires models to produce strictly structured, machineparsable outputs and to maintain coherent chain-ofthought reasoning across steps. Future Directions: Scaling the Environment Spectrum promising next step is to procedurally generate curricula that expand both task horizon and required skill set as model capacity grows, akin to the role of MineDojo (Fan et al. 2022) or Crafter (Hafner 2022) in open-world RL. We envision an open RL4VLM Gym where each contribution adds small, cheap environment rather than single monolithic photorealistic world. Such repository would enable systematic study of environment-set scaling laws: how many distinct interaction types are required for an additional n% transfer gain? Algorithmically, VL-DAC could pair with hierarchical RL, using the step-level value head to supervise sub-goal policies while token-wise PPO refines low-level text actions, or integrate memory-augmented transformers to curb variance as horizons exceed 100 steps."
        },
        {
            "title": "Connection to Prior Work",
            "content": "VLM and LLM training in multi-step scenarios. RL4VLM (Zhai et al. 2024), LOOP (Chen et al. 2025b), ArCHer (Zhou and Zanette 2024), and some other domainspecific methods ((Putta et al. 2024), (Bai et al. 2025), (Bai et al. 2024)) pursue long-horizon training, yet they rely on delicate mixture coefficients, sequence-level gradients with high variance, or replay buffers that collapse under sparse rewards. VL-DAC inherits the stability of PPO-based RLHF while, for the first time, demonstrating consistent transfer across agentic, spatial, and web-interaction tasks using the same hyperparameters. These findings underscore that minimal algorithmic tweak, coupled with diversified simulator set, is sufficient to unlock practical RL training for VLMs and to endow them with real-world competence. Benchmarking. Classical perception-centric suites such as MMBench, MME, and Video-MME are indispensable for gauging static understanding, but they lack the agentic dimension, capacity to decide and act under long-horizon feedback. Recent game-based evaluations like BALROG (Paglieri et al. 2024) and VideoGameBench (Zhang et al. 2025) close this gap by measuring whether models can plan, execute, and adapt inside fully interactive worlds that resemble classic reinforcement-learning settings. Our study leverages both families: the perception benchmarks verify that VL-DAC training leaves core recognition intact, whereas BALROG (Paglieri et al. 2024) exposes the gains in goaldirected control. The contrast underscores key takeaway: agentic evaluation is where progress now moves fastest, and RL with brittle hyperparameters can translate simulator experience into measurable improvements on these harder benchmarks. Real-task transfer. Generalization from synthetic practice to real-world queries has been actively explored in single-step reasoning research ((Chen et al. 2025a), (Stojanovski et al. 2025)). Our findings extend that evidence to the multi-step regime: VL-DAC-trained VLMs master spatial-navigation, manipulation, and web-interaction skills in cheap simulators and then transfer them to BALROG (Paglieri et al. 2024), VSI-Bench (Yang et al. 2025), and VisualWebBench (Liu et al. 2024a) with only modest domain gaps. By showing that interactive rehearsal scales beyond toy boards and text puzzles to full visual control loops, we strengthen the emerging view that procedural curricula plus lightweight RL offer practical path toward robust real-task competence."
        },
        {
            "title": "Conclusion",
            "content": "This work demonstrates that reinforcement learning in synthetic, interactive environments is powerful and scalable strategy for enhancing vision-language models. By moving from coupled action-and-critic optimization to decoupled (two-level) optimization and introducing stabilization techniques, we significantly improve the stability and generalization of RL-based training for VLMs. Our approach avoids brittle hyperparameter tuning while achieving competitive success rates across diverse environments. More importantly, we show that models trained in these synthetic settings generalize effectively to skill-specific and generalpurpose benchmarks-outperforming strong baselines without additional supervision. These findings position RL as viable, data-efficient alternative to traditional supervised fine-tuning, opening new directions for training embodied, multimodal agents that reason and act in complex visual domains. Future work will explore scaling to more realistic 3D worlds and integrating longer-horizon planning into visionlanguage training. References Bai, H.; Zhou, Y.; Cemri, M.; Pan, J.; Suhr, A.; Levine, S.; and Kumar, A. 2024. DigiRL: Training In-The-Wild DeviceControl Agents with Autonomous Reinforcement Learning. arXiv:2406.11896. Bai, H.; Zhou, Y.; Li, L. E.; Levine, S.; and Kumar, A. 2025. Digi-Q: Learning Q-Value Functions for Training DeviceControl Agents. arXiv:2502.15760. Chen, J.; He, Q.; Yuan, S.; Chen, A.; Cai, Z.; Dai, W.; Yu, H.; Yu, Q.; Li, X.; Chen, J.; Zhou, H.; and Wang, Enigmata: Scaling Logical Reasoning in M. 2025a. Large Language Models with Synthetic Verifiable Puzzles. arXiv:2505.19914. Chen, K.; Cusumano-Towner, M.; Huval, B.; Petrenko, A.; Hamburger, J.; Koltun, V.; and Krahenbuhl, P. 2025b. Reinforcement Learning for Long-Horizon Interactive LLM Agents. arXiv:2502.01600. Chen, L.; Li, J.; Dong, X.; Zhang, P.; Zang, Y.; Chen, Z.; Duan, H.; Wang, J.; Qiao, Y.; Lin, D.; and Zhao, F. 2024a. Are We on the Right Way for Evaluating Large VisionLanguage Models? arXiv:2403.20330. Chen, Z.; Wu, J.; Wang, W.; Su, W.; Chen, G.; Xing, S.; Zhong, M.; Zhang, Q.; Zhu, X.; Lu, L.; Li, B.; Luo, P.; Intern VL: Scaling Lu, T.; Qiao, Y.; and Dai, J. 2024b. up Vision Foundation Models and Aligning for Generic In 2024 IEEE/CVF Conference Visual-Linguistic Tasks. on Computer Vision and Pattern Recognition (CVPR), 2418524198. IEEE. Chevalier-Boisvert, M.; Dai, B.; Towers, M.; de Lazcano, R.; Willems, L.; Lahlou, S.; Pal, S.; Castro, P. S.; and Terry, J. 2023. Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks. CoRR, abs/2306.13831. Chow, W.; Mao, J.; Li, B.; Seita, D.; Guizilini, V.; and Wang, Y. 2025. PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding. arXiv:2501.16411. DeepSeek-AI; Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; Zhang, X.; Yu, X.; Wu, Y.; Wu, Z. F.; Gou, Z.; Shao, Z.; Li, Z.; Gao, Z.; Liu, A.; Xue, B.; Wang, B.; Wu, B.; Feng, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; Dai, D.; Chen, D.; Ji, D.; Li, E.; Lin, F.; Dai, F.; Luo, F.; Hao, G.; Chen, G.; Li, G.; Zhang, H.; Bao, H.; Xu, H.; Wang, H.; Ding, H.; Xin, H.; Gao, H.; Qu, H.; Li, H.; Guo, J.; Li, J.; Wang, J.; Chen, J.; Yuan, J.; Qiu, J.; Li, J.; Cai, J. L.; Ni, J.; Liang, J.; Chen, J.; Dong, K.; Hu, K.; Gao, K.; Guan, K.; Huang, K.; Yu, K.; Wang, L.; Zhang, L.; Zhao, L.; Wang, L.; Zhang, L.; Xu, L.; Xia, L.; Zhang, M.; Zhang, M.; Tang, M.; Li, M.; Wang, M.; Li, M.; Tian, N.; Huang, P.; Zhang, P.; Wang, Q.; Chen, Q.; Du, Q.; Ge, R.; Zhang, R.; Pan, R.; Wang, R.; Chen, R. J.; Jin, R. L.; Chen, R.; Lu, S.; Zhou, S.; Chen, S.; Ye, S.; Wang, S.; Yu, S.; Zhou, S.; Pan, S.; Li, S. S.; Zhou, S.; Wu, S.; Ye, S.; Yun, T.; Pei, T.; Sun, T.; Wang, T.; Zeng, W.; Zhao, W.; Liu, W.; Liang, W.; Gao, W.; Yu, W.; Zhang, W.; Xiao, W. L.; An, W.; Liu, X.; Wang, X.; Chen, X.; Nie, X.; Cheng, X.; Liu, X.; Xie, X.; Liu, X.; Yang, X.; Li, X.; Su, X.; Lin, X.; Li, X. Q.; Jin, X.; Shen, X.; Chen, X.; Sun, X.; Wang, X.; Song, X.; Zhou, X.; Wang, X.; Shan, X.; Li, Y. K.; Wang, Y. Q.; Wei, Y. X.; Zhang, Y.; Xu, Y.; Li, Y.; Zhao, Y.; Sun, Y.; Wang, Y.; Yu, Y.; Zhang, Y.; Shi, Y.; Xiong, Y.; He, Y.; Piao, Y.; Wang, Y.; Tan, Y.; Ma, Y.; Liu, Y.; Guo, Y.; Ou, Y.; Wang, Y.; Gong, Y.; Zou, Y.; He, Y.; Xiong, Y.; Luo, Y.; You, Y.; Liu, Y.; Zhou, Y.; Zhu, Y. X.; Xu, Y.; Huang, Y.; Li, Y.; Zheng, Y.; Zhu, Y.; Ma, Y.; Tang, Y.; Zha, Y.; Yan, Y.; Ren, Z. Z.; Ren, Z.; Sha, Z.; Fu, Z.; Xu, Z.; Xie, Z.; Zhang, Z.; Hao, Z.; Ma, Z.; Yan, Z.; Wu, Z.; Gu, Z.; Zhu, Z.; Liu, Z.; Li, Z.; Xie, Z.; Song, Z.; Pan, Z.; Huang, Z.; Xu, Z.; Zhang, Z.; and Zhang, Z. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948. Fan, L.; Wang, G.; Jiang, Y.; Mandlekar, A.; Yang, Y.; Zhu, H.; Tang, A.; Huang, D.-A.; Zhu, Y.; and Anandkumar, A. 2022. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35: 1834318362. Fu, C.; Dai, Y.; Luo, Y.; Li, L.; Ren, S.; Zhang, R.; Wang, Z.; Zhou, C.; Shen, Y.; Zhang, M.; Chen, P.; Li, Y.; Lin, S.; Zhao, S.; Li, K.; Xu, T.; Zheng, X.; Chen, E.; Ji, R.; and Sun, X. 2024. Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis. arXiv:2405.21075. Hafner, D. 2022. Benchmarking the Spectrum of Agent Capabilities. arXiv:2109.06780. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. Hudson, D. A.; and Manning, C. D. 2019. Gqa: new dataset for compositional question answering over realworld images. arXiv preprint arXiv:1902.09506, 3(8): 1. Lehmann, M. 2024. The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations. arXiv:2401.13662. Liu, J.; Song, Y.; Lin, B. Y.; Lam, W.; Neubig, G.; Li, Y.; and Yue, X. 2024a. VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? arXiv:2404.05955. Liu, Y.; Duan, H.; Zhang, Y.; Li, B.; Zhang, S.; Zhao, W.; Yuan, Y.; Wang, J.; He, C.; Liu, Z.; Chen, K.; and Lin, D. 2024b. MMBench: Is Your Multi-modal Model an AllAround Player?, 216233. Springer Nature Switzerland. ISBN 9783031726583. Misra, D.; Langford, J.; and Artzi, Y. 2017. Mapping Instructions and Visual Observations to Actions with Reinforcement Learning. arXiv:1704.08795. OpenAI; :; Jaech, A.; Kalai, A.; Lerer, A.; Richardson, A.; El-Kishky, A.; Low, A.; Helyar, A.; Madry, A.; Beutel, A.; Carney, A.; Iftimie, A.; Karpenko, A.; Passos, A. T.; Neitz, A.; Prokofiev, A.; Wei, A.; Tam, A.; Bennett, A.; Kumar, A.; Saraiva, A.; Vallone, A.; Duberstein, A.; Kondrich, A.; Mishchenko, A.; Applebaum, A.; Jiang, A.; Nair, A.; Zoph, B.; Ghorbani, B.; Rossen, B.; Sokolowsky, B.; Barak, B.; McGrew, B.; Minaiev, B.; Hao, B.; Baker, B.; Houghton, B.; McKinzie, B.; Eastman, B.; Lugaresi, C.; Bassin, C.; Hudson, C.; Li, C. M.; de Bourcy, C.; Voss, C.; Shen, C.; Zhang, C.; Koch, C.; Orsinger, C.; Hesse, C.; Fischer, C.; Chan, C.; Roberts, D.; Kappler, D.; Levy, D.; Selsam, D.; Dohan, D.; Farhi, D.; Mely, D.; Robinson, D.; Tsipras, D.; Li, D.; Oprica, D.; Freeman, E.; Zhang, E.; Wong, E.; Proehl, E.; Cheung, E.; Mitchell, E.; Wallace, E.; Ritter, E.; Mays, E.; Wang, F.; Such, F. P.; Raso, F.; Leoni, F.; Tsimpourlas, F.; Song, F.; von Lohmann, F.; Sulit, F.; Salmon, G.; Parascandolo, G.; Chabot, G.; Zhao, G.; Brockman, G.; Leclerc, G.; Salman, H.; Bao, H.; Sheng, H.; Andrin, H.; Bagherinezhad, H.; Ren, H.; Lightman, H.; Chung, H. W.; Kivlichan, I.; OConnell, I.; Osband, I.; Gilaberte, I. C.; Akkaya, I.; Kostrikov, I.; Sutskever, I.; Kofman, I.; Pachocki, J.; Lennon, J.; Wei, J.; Harb, J.; Twore, J.; Feng, J.; Yu, J.; Weng, J.; Tang, J.; Yu, J.; Candela, J. Q.; Palermo, J.; Parish, J.; Heidecke, J.; Hallman, J.; Rizzo, J.; Gordon, J.; Uesato, J.; Ward, J.; Huizinga, J.; Wang, J.; Chen, K.; Xiao, K.; Singhal, K.; Nguyen, K.; Cobbe, K.; Shi, K.; Wood, K.; Rimbach, K.; Gu-Lemberg, K.; Liu, K.; Lu, K.; Stone, K.; Yu, K.; Ahmad, L.; Yang, L.; Liu, L.; Maksin, L.; Ho, L.; Fedus, L.; Weng, L.; Li, L.; McCallum, L.; Held, L.; Kuhn, L.; Kondraciuk, L.; Kaiser, L.; Metz, L.; Boyd, M.; Trebacz, M.; Joglekar, M.; Chen, M.; Tintor, M.; Meyer, M.; Jones, M.; Kaufer, M.; Schwarzer, M.; Shah, M.; Yatbaz, M.; Guan, M. Y.; Xu, M.; Yan, M.; Glaese, M.; Chen, M.; Lampe, M.; Malek, M.; Wang, M.; Fradin, M.; McClay, M.; Pavlov, M.; Wang, M.; Wang, M.; Murati, M.; Bavarian, M.; Rohaninejad, M.; McAleese, N.; Chowdhury, N.; Chowdhury, N.; Ryder, N.; Tezak, N.; Brown, N.; Nachum, O.; Boiko, O.; Murk, O.; Watkins, O.; Chao, P.; Ashbourne, P.; Izmailov, P.; Zhokhov, P.; Dias, R.; Arora, R.; Lin, R.; Lopes, R. G.; Gaon, R.; Miyara, R.; Leike, R.; Hwang, R.; Garg, R.; Brown, R.; James, R.; Shu, R.; Cheu, R.; Greene, R.; Jain, S.; Altman, S.; Toizer, S.; Toyer, S.; Miserendino, S.; Agarwal, S.; Hernandez, S.; Baker, S.; McKinney, S.; Yan, S.; Zhao, S.; Hu, S.; Santurkar, S.; Chaudhuri, S. R.; Zhang, S.; Fu, S.; Papay, S.; Lin, S.; Balaji, S.; Sanjeev, S.; Sidor, S.; Broda, T.; Clark, A.; Wang, T.; Gordon, T.; Sanders, T.; Patwardhan, T.; Sottiaux, T.; Degry, T.; Dimson, T.; Zheng, T.; Garipov, T.; Stasi, T.; Bansal, T.; Creech, T.; Peterson, T.; Eloundou, T.; Qi, V.; Kosaraju, V.; Monaco, V.; Pong, V.; Fomenko, V.; Zheng, W.; Zhou, W.; McCabe, W.; Zaremba, W.; Dubois, Y.; Lu, Y.; Chen, Y.; Cha, Y.; Bai, Y.; He, Y.; Zhang, Y.; Wang, Y.; Shao, Z.; and Li, Z. 2024. OpenAI o1 System Card. arXiv:2412.16720. Paglieri, D.; Cupiał, B.; Coward, S.; Piterbarg, U.; Wolczyk, M.; Khan, A.; Pignatelli, E.; Łukasz Kucinski; Pinto, L.; Fergus, R.; Foerster, J. N.; Parker-Holder, J.; and Rocktaschel, T. 2024. BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games. arXiv:2411.13543. Putta, P.; Mills, E.; Garg, N.; Motwani, S.; Finn, C.; Garg, D.; and Rafailov, R. 2024. Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents. arXiv:2408.07199. Schulman, J.; Moritz, P.; Levine, S.; Jordan, M.; and Abbeel, P. 2018. High-Dimensional Continuous Control Using Generalized Advantage Estimation. arXiv:1506.02438. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347. Shridhar, M.; Yuan, X.; Cˆote, M.-A.; Bisk, Y.; Trischler, A.; and Hausknecht, M. 2021. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. arXiv:2010.03768. Stojanovski, Z.; Stanley, O.; Sharratt, J.; Jones, R.; Adefioye, A.; Kaddour, J.; and Kopf, A. 2025. REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards. arXiv:2505.24760. Sutton, R. S. 1988. Learning to predict by the methods of temporal differences. Machine learning, 3(1): 944. Team, G.; Mesnard, T.; Hardin, C.; Dadashi, R.; Bhupatiraju, S.; Pathak, S.; Sifre, L.; Rivi`ere, M.; Kale, M. S.; Love, J.; Tafti, P.; Hussenot, L.; Sessa, P. G.; Chowdhery, A.; Roberts, A.; Barua, A.; Botev, A.; Castro-Ros, A.; Slone, A.; Heliou, A.; Tacchetti, A.; Bulanova, A.; Paterson, A.; Tsai, B.; Shahriari, B.; Lan, C. L.; ChoquetteChoo, C. A.; Crepy, C.; Cer, D.; Ippolito, D.; Reid, D.; Buchatskaya, E.; Ni, E.; Noland, E.; Yan, G.; Tucker, G.; Muraru, G.-C.; Rozhdestvenskiy, G.; Michalewski, H.; Tenney, I.; Grishchenko, I.; Austin, J.; Keeling, J.; Labanowski, J.; Lespiau, J.-B.; Stanway, J.; Brennan, J.; Chen, J.; Ferret, J.; Chiu, J.; Mao-Jones, J.; Lee, K.; Yu, K.; Millican, K.; Sjoesund, L. L.; Lee, L.; Dixon, L.; Reid, M.; Mikuła, M.; Wirth, M.; Sharman, M.; Chinaev, N.; Thain, N.; Bachem, O.; Chang, O.; Wahltinez, O.; Bailey, P.; Michel, P.; Yotov, P.; Chaabouni, R.; Comanescu, R.; Jana, R.; Anil, R.; McIlroy, R.; Liu, R.; Mullins, R.; Smith, S. L.; Borgeaud, S.; Girgin, S.; Douglas, S.; Pandya, S.; Shakeri, S.; De, S.; Klimenko, T.; Hennigan, T.; Feinberg, V.; Stokowiec, W.; hui Chen, Y.; Ahmed, Z.; Gong, Z.; Warkentin, T.; Peran, L.; Giang, M.; Farabet, C.; Vinyals, O.; Dean, J.; Kavukcuoglu, K.; Hassabis, D.; Ghahramani, Z.; Eck, D.; Barral, J.; Pereira, F.; Collins, E.; Joulin, A.; Fiedel, N.; Senter, E.; Andreev, A.; and Kenealy, K. 2024. Gemma: Open Models Based on Gemini Research and Technology. arXiv:2403.08295. Team, G. R.; Abeyruwan, S.; Ainslie, J.; Alayrac, J.-B.; Arenas, M. G.; Armstrong, T.; Balakrishna, A.; Baruch, R.; Bauza, M.; Blokzijl, M.; et al. 2025. Gemini Robotics: arXiv preprint Bringing AI into the Physical World. arXiv:2503.20020. Wang, F.; Fu, X.; Huang, J. Y.; Li, Z.; Liu, Q.; Liu, X.; Ma, M. D.; Xu, N.; Zhou, W.; Zhang, K.; et al. 2024a. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411. Wang, P.; Bai, S.; Tan, S.; Wang, S.; Fan, Z.; Bai, J.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Fan, Y.; Dang, K.; Du, M.; Ren, X.; Men, R.; Liu, D.; Zhou, C.; Zhou, J.; and Lin, J. 2024b. Qwen2-VL: Enhancing VisionLanguage Models Perception of the World at Any Resolution. arXiv:2409.12191. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E. H.; Le, Q. V.; and Zhou, D. 2022. Chainof-thought prompting elicits reasoning in large language In Proceedings of the 36th International Conmodels. ference on Neural Information Processing Systems, NIPS"
        },
        {
            "title": "ISBN",
            "content": "22. Red Hook, NY, USA: Curran Associates Inc. 9781713871088. Yan, K.; Ling, Z.; Liu, K.; Yang, Y.; Fan, T.-H.; Shen, L.; Du, Z.; and Chen, J. 2025. MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning? arXiv:2502.09933. Yang, J.; Yang, S.; Gupta, A. W.; Han, R.; Fei-Fei, L.; and Xie, S. 2025. Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces. arXiv:2412.14171. Yao, S.; Chen, H.; Yang, J.; and Narasimhan, K. 2023. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. arXiv:2207.01206. Ying, K.; Meng, F.; Wang, J.; Li, Z.; Lin, H.; Yang, Y.; Zhang, H.; Zhang, W.; Lin, Y.; Liu, S.; Lei, J.; Lu, Q.; Chen, R.; Xu, P.; Zhang, R.; Zhang, H.; Gao, P.; Wang, Y.; Qiao, Y.; Luo, P.; Zhang, K.; and Shao, W. 2024. MMTBench: Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI. arXiv:2404.16006. Yue, X.; Ni, Y.; Zheng, T.; Zhang, K.; Liu, R.; Zhang, G.; Stevens, S.; Jiang, D.; Ren, W.; Sun, Y.; Wei, C.; Yu, B.; Yuan, R.; Sun, R.; Yin, M.; Zheng, B.; Yang, Z.; Liu, Y.; Huang, W.; Sun, H.; Su, Y.; and Chen, W. 2024. MMMU: Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 95569567. IEEE. Zhai, Y.; Bai, H.; Lin, Z.; Pan, J.; Tong, S.; Zhou, Y.; Suhr, A.; Xie, S.; LeCun, Y.; Ma, Y.; and Levine, S. 2024. FineTuning Large Vision-Language Models as Decision-Making In The Thirty-eighth Agents via Reinforcement Learning. Annual Conference on Neural Information Processing Systems. Zhang, A. L.; Griffiths, T. L.; Narasimhan, K. R.; and Press, O. 2025. VideoGameBench: Can Vision-Language Models complete popular video games? arXiv:2505.18134. Zhang, K.; Li, B.; Zhang, P.; Pu, F.; Cahyono, J. A.; Hu, K.; Liu, S.; Zhang, Y.; Yang, J.; Li, C.; and Liu, Z. 2024a. LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models. arXiv:2407.12772. Zhang, Y.-F.; Zhang, H.; Tian, H.; Fu, C.; Zhang, S.; Wu, J.; Li, F.; Wang, K.; Wen, Q.; Zhang, Z.; Wang, L.; Jin, R.; and Tan, T. 2024b. MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans? arXiv:2408.13257. Zhao, B.; Zong, Y.; Zhang, L.; and Hospedales, T. 2024. Benchmarking multi-image understanding in vision and language models: Perception, knowledge, reasoning, and multihop reasoning. arXiv preprint arXiv:2406.12742. Zhou, Y.; and Zanette, A. 2024. ArCHer: training language model agents via hierarchical multi-turn RL. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. Hyperparameter MiniWorld ALFWorld EZPoints Env. Steps Learning Rate (init final) Scheduler GAE λg γg Value Loss Coeff. KL β Policy Freeze (steps) Grad Accum. Steps Mini-batch Size PPO Epochs Eval Episodes Obs. Image Length 51200 5e-5 1e-7 cosine 0.95 0.99 0.15 0.05 2 128 1 2 50 4 51200 5e-5 1e-7 cosine {0.9, 1} {0.9, 1} 0.15 0.05 2 128 1 2 50 4 51200 5e-5 1e-7 cosine 1 1 0.15 {0.05, 0.15} 2 128 1 2 50 1 Table 7: Training hyperparameters per environment for RL4VLM. Values are shared across environments unless specified otherwise. Algorithm steps Learning Rate (init final) Scheduler γg KL β Grad Accum. Steps Mini-batch Size PPO Epochs Eval Episodes Obs. Image Length"
        },
        {
            "title": "MiniWorld",
            "content": "75 5e-5 1e-7 cosine {0.99, 1} 0.05 128 1 2 50 4 Table 8: Training hyperparameters on MiniWorld for LOOP. 200704, min pixels = 3136, max num rames = 32. For Balrog and skill-specific benchmarks, we use separate codebases. Appendix D: Detailed Training Dynamics In Figure 7, results without averaging over the thought coefficient λ can be found. Figure 7: Episode success rates without averaging over the thought-probability coefficient. Figure 6: Example of prompt template for MiniWorld environments."
        },
        {
            "title": "Hyperparameter",
            "content": "Hyperparameter MiniWorld ALFWorld EZPoints Env. Steps Learning Rate (init final) Scheduler GAE λg γg Value Loss Coeff. KL β Policy Freeze (steps) Grad Accum. Steps Mini-batch Size PPO Epochs Eval Episodes Obs. Image Length 51200 5e-5 1e-7 cosine {0.95, 0.99} {0.99, 1} 0.15 0.05 2 128 1 2 50 51200 5e-5 1e-7 cosine {0.9, 0.95, 1} {0.9, 0.99, 1} 0.15 0.05 2 128 1 2 50 4 51200 5e-5 1e-7 cosine 1 1 0.15 {0.05, 0.15} 2 128 1 2 50 1 Table 6: Training hyperparameters per environment for our approach. Values are shared across environments unless specified otherwise. Appendix A: Input example Figure 6 shows an illustrative example of template prompt for our environments. Appendix B: Hyperparameters We list parameters for training our approach in Table 6. RL4VLM hyperparameters can be found in Table 7. We aimed to search the same hyperparameter space; however, since RL4VLM requires wider search due to λ, the resulting range is somewhat narrower. If run with given hyperparameter was completed, then for the final comparison we took the best success rate. For the OneRoom and ALFWorld environments, curves are plotted as mean std over 4 seeds. Additionally, for the LOOP experiments we searched over the hyperparameters in Table 8. Appendix C: Qwen2-VL-7b Evaluation Setup For most benchmarks we use the lmm-eval framework. Since scores from the original Qwen2-VL paper are not easily reproducible, we reran most evaluations. The evaluation hyperparameters are max pixels = Appendix E: ArCHer On-policy runs In Figure 8 we show how ArCHer performs differently under off-policy (very large buffer, which is hard to maintain in the case of images and videos) and on-policy (replay buffer equals rollout size) scenarios. In this experiment we use the 20Q environment from LMRL. The rollout size equals 512, while the replay buffer in the off-policy scenario equals 100k. For the on-policy setup, we also experimented with τ (in Polyak averaging) and plotted the curve for the best value. All other parameters were set to the defaults in the ArCHer paper. Figure 8: 20Q LMRL performance curves for on-policy and off-policy ArCHer."
        }
    ],
    "affiliations": [
        "T-Tech"
    ]
}