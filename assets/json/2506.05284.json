{
    "paper_title": "Video World Models with Long-term Spatial Memory",
    "authors": [
        "Tong Wu",
        "Shuai Yang",
        "Ryan Po",
        "Yinghao Xu",
        "Ziwei Liu",
        "Dahua Lin",
        "Gordon Wetzstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, among other control signals. Due to limited temporal context window sizes, these models often struggle to maintain scene consistency during revisits, leading to severe forgetting of previously generated environments. Inspired by the mechanisms of human memory, we introduce a novel framework to enhancing long-term consistency of video world models through a geometry-grounded long-term spatial memory. Our framework includes mechanisms to store and retrieve information from the long-term spatial memory and we curate custom datasets to train and evaluate world models with explicitly stored 3D memory mechanisms. Our evaluations show improved quality, consistency, and context length compared to relevant baselines, paving the way towards long-term consistent world generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 4 8 2 5 0 . 6 0 5 2 : r Video World Models with Long-term Spatial Memory 1 Stanford University Tong Wu*1, Shuai Yang*2,4, Ryan Po1, Yinghao Xu1, Ziwei Liu5, Dahua Lin3,4, Gordon Wetzstein1 2 Shanghai Jiao Tong University 3 The Chinese University of Hong Kong 4 Shanghai Artificial Intelligence Laboratory 5 S-Lab, Nanyang Technological University https://spmem.github.io/ Figure 1: We augment video world models with memory. In this context, we consider the conventional approach of conditioning autoregressively generated frames with few recent context frames as short-term working memory. We explore two additional mechanisms modeling different types of long-term memory: spatial and episodic memory. The former is represented as point map that is autoregressively generated along with the video frames and fused into the spatial memory by extracting only its static scene parts. To remember visual detail and identities for long time horizons, we also store sparse set of historical reference frames as an episodic memory. Together, our memory mechanisms significantly improve the long-term consistency of emerging video world models."
        },
        {
            "title": "Abstract",
            "content": "Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, among other control signals. Due to limited temporal context window sizes, these models often struggle to maintain scene consistency during revisits, leading to severe forgetting of previously generated environments. Inspired by the mechanisms of human memory, we introduce novel framework to enhancing long-term consistency of video world models through geometry-grounded long-term spatial memory. Our framework includes mechanisms to store and retrieve information from the long-term spatial memory and we curate custom datasets to train and evaluate world models with explicitly stored 3D memory mechanisms. Our evaluations show improved quality, consistency, and context length compared to relevant baselines, paving the way towards long-term consistent world generation. * denotes equal contribution."
        },
        {
            "title": "Introduction",
            "content": "World models are generative systems that learn to predict an environment in response to actions, making them well suited for simulating complex, interactive settings [27, 2, 29, 74, 92]. Video diffusion models [11, 35, 41, 79, 52] have emerged as powerful approach to architecting world models, especially when used with autoregressive next-frame prediction [1, 12, 17, 21, 38, 50, 58, 63, 73, 81, 33]. Existing video generation models, however, often struggle with long-horizon consistency due to limited temporal context windows, frequently forgetting previously seen scenes during revisits. This is due to the relatively small number of previously generated context frames that the model can consider when generating new framesa problem primarily caused by the quadratic growth of computational complexity in the attention module of the underlying diffusion transformers. To address this challenge, current world models simply keep the number of context frames low to maintain computational feasibility. Several very recent approaches explore progressive downsampling of temporally more distant frames to increase the temporal context window size [24, 87]. Yet, all these approaches rely on image-based representations of the past and lack persistent 3D understanding of the world, limiting spatial consistency. Inspired by the mechanisms of human memory [3], we propose new framework to enhance the longterm consistency of video world models through long-term spatial memory grounded in geometry. Drawing from cognitive theories, our approach incorporates three distinct forms of memoryspatial, working, and episodiceach modeled through dedicated representation. Similar to existing models, our framework relies on set of recently generated context frames. We consider this short-term working memory mechanism, making both static and dynamic aspects of the most recent past accessible in the form of pixel data. To help remember long-term spatial relationships, we introduce an additional long-term spatial memory. This mechanism primarily relies on an explicit 3D representation of the generated world, augmented by sparse episodic memory in the form of set of keyframes from the past. We implement the spatial memory using geometry-grounded point cloud representation. Before storing newly generated information into this memory mechanism, we filter out dynamic parts of the world to primarily remember the static parts of the work in this memory. The primary contribution of our work is the design of our memory mechanism, combining short-term working memory with long-term spatial and sparse episodic memory, as illustrated in Figure 1. We develop approaches to store newly generated information into the spatial and episodic memory bank as well as retrieve information from it to effectively condition the generation of new video frames. Moreover, we curate custom dataset to train and evaluate proof-of-principle implementation of the proposed mechanisms. Our evaluations show that the quality and 3D consistency of our approach surpasses that of relevant baselines. With this work, we hope to contribute to the growing community effort of unlocking infinite-length, consistent world generation capabilities for computer graphics, robotics, and other interactive applications."
        },
        {
            "title": "2 Related work",
            "content": "We consider world model to be generative system that autoregressively generates image or video frames, conditioned on actions or camera pose. This work builds on several important concepts in the generative AI literature, which we briefly review in the following. Image and video generation. In recent years, diffusion models have emerged as the state-of-the-art paradigm for image generation, surpassing the performance of GAN-based methods in both fidelity and diversity by modeling complex distributions through iterative denoising [42, 48, 62, 65, 19]. These successes have naturally extended to the task of video generation, as diffusion-based architectures have been adapted to include the temporal domain, enabling the generation of high-quality video clips spanning the order of tens to hundreds of frames [91, 45, 78, 53, 20, 14, 8]. Additional advancements in efficiency and controllability have also followed, through techniques such as improved tokenization [61, 75] and flow-matching objectives [46, 37]. Autoregressive video generation. To support generation of longer videos with online inference capabilities, autoregressive (AR) approaches and architectures have been introduced in the diffusion framework [60, 40, 76, 71]. The autoregressive regime is also particularly intuitive, given the natural temporal ordering of video data. Taking inspiration from LLMs, state-of-the-art methods model 2 video data by processing frames into spatio-temporal tokens [45, 91, 78, 61, 20, 53, 14], learning to autoregressively generate new tokens until full video is formed. This is in stark contrast with conventional diffusion-based frameworks, which choose to iteratively denoise an entire image/video at once [9, 43, 35, 56, 79, 41, 28, 49]. Recent approaches include training conditional diffusion models that generate new frames given set of previous clean frames, allowing AR generation simply by passing newly generated frames as context for future frames [35, 32, 16, 38, 90, 23, 22]. Others modify the diffusion objective by assigning independent noise levels for each frame during training, which also supports AR inference [13, 80, 58]. In practice, these methods can be used for infinite-length generations by utilizing sliding window context, but suffers from limited memory and drift. Controlled video generation. Controlled or conditional video generation is core component of world simulators. series of works have explored explicit camera control, including [72, 30, 31, 31, 5, 4], which enable novel-view or multi-view generation by disentangling camera trajectories from dynamic content. Most of these approaches take either single image or text prompt as input, and often struggle to maintain long-term consistency, especially in dynamic scenes. More recent efforts [85, 7, 6] focus on re-filming or generating synchronized views in dynamic settings, further advancing controllability. Beyond direct camera injection, structural conditioning via point clouds, tracking, or 3D-aware priors has proven effective for improving spatial consistency and trajectory alignment [83, 54, 25, 82, 15]. In addition to spatial and motion-level control, some models support action-based or scene-level conditioning [51, 18], where either structured action vocabularies or segmented text prompts serve as high-level drivers of video progression. Long-context video generation. While advances in autoregressive methods have allowed for generation of longer, and even potentially inifinite-length videos [14], the reliance on sliding context windows limit their effective memory. The straightforward approach of increasing memory by training with longer context lengths is effective but computationally demanding. Recent works have explored efficient architectural alternatives to attention-based transformers, such as state-space models and linear attention [67, 47]. While efficient, such methods often perform worse than conventional diffusion transformer architectures. Other concurrent works have proposed compressing prior frames to lower context length [87, 24], subsequently lowering computational demand at the cost of information lost in context frames. separate branch of works ground previously generated frames in 3D representation (e.g., point clouds [83, 25, 82, 54]) and query this representation based on the camera position of future frames. These methods allow for precise camera control, but struggle to handle dynamic scenes with complex motions. In this work, we adopt the conventional approach of concatenating recent context frames as working memory in diffusion-based generation to autoregressively generate multiple future frames. Meanwhile, we iteratively predict and filter the static point maps of newly generated frames to update global spatial memory, which serves as geometric guidance for long-term generation. We further incorporate sparse set of historical reference frames as an episodic memory."
        },
        {
            "title": "3 Method",
            "content": "Our framework includes mechanisms for storing new observations into memory and retrieving information from it. Drawing inspiration from three distinct forms of human memoryspatial, working, and episodicwe introduce memory storage mechanism that models each type using dedicated representation. In the following section, we describe how each memory type is constructed and maintained (Sec. 3.2), how it is integrated into the model as conditional signal (Sec. 3.3), and how we curate the appropriate data to facilitate efficient learning of these mechanisms (Sec. 3.4). 3.1 Preliminaries Diffusion models [57, 34, 59] learn to model data distribution by reversing forward diffusion process denoted by xt = αtx0 + σtϵ, where the positive scalar parameters αt and σt determine the signal-to-noise ratio based on predefined noise schedule, and ϵ is drawn from standard Gaussian distribution. The diffusion model then learns to predict the noise added through the following denoising objective: L(θ) = Et,x0,ϵ ϵθ(xt, t) ϵ2 2 . (1) Figure 2: Overview of our system. latent video generation model, implemented by diffusion transformer (DiT), is conditioned on three different memory mechanisms when autoregressively generating new frames. First, recent context frames model short-term working memory. Second, point cloud representation (left) is autoregressively generated along with the video frames. This long-term spatial memory contains the static parts of the world. Third, set of historical reference frames (lower left) is stored as sparse, long-term episodic memory. Together, these memory mechanisms enable consistent long-term video generation. Video diffusion models commonly operate in latent space, following two-stage process: first, input videos are encoded using 3D variational autoencoder (VAE) [64, 26, 10], and then diffusion model is trained to model the resulting latent representations. Generation proceeds by sampling within the latent space and decoding the results back to pixel space. Our prototype implementation builds on CogVideoX [79], which adopts this two-stage framework. Specifically, CogVideoX employs diffusion transformer with 3D attention blocks to capture the distribution of the latent space. Our model improves upon this architecture with additional control signals for enabling long-term memory. 3.2 Spatial Memory Storage Coherent Static Structure for Spatial Guidance. Human spatial memory refers to our ability to encode, store, and retrieve information relating to the physical layout and structure of our environment. To this end, we construct persistent spatial memory in the form of long-term static point map that captures high-confidence, temporally consistent 3D structures. To further separate static elements like buildings from dynamic elements like characters and animals, we adopt truncated signed distance function (TSDF) fusion [84]. We denote the TSDF value and associated weight of voxel as D(v) and (v), respectively. Given new observation from frame i, the voxel update rule follows the standard weighted averaging: D(v) = (v) D(v) + wi di(v) (v) + wi , (v) = (v) + wi, (2) where di(v) is the truncated signed distance between voxel and the observed surface in frame i, and wi is frame-dependent confidence weight (typically set to 1). This fusion process inherently filters out dynamic elements in the scene: due to inconsistent depth observations across frames, such voxels accumulate low-confidence, noisy TSDF values and are naturally suppressed in the final fused volume. During the autoregressive generation process, spatial memory is incrementally updated with newly observed static maps, which are reconstructed in an online recurrent manner by CUT3R [69] and filtered by TSDF-Fusion to eliminate the dynamic parts. In humans, working memory is in charge of Recent Frames for Dynamic Context Guidance. temporarily holding information needed for performing reasoning and comprehension tasks. Similarly, our model requires knowledge of nearby previous frames to generate temporally coherent future frames. Drawing on this concept, we incorporate short-term memory stream based on recent frames, 4 which provides motion continuity atop the static scene structure. We adopt simple autoregressive generation strategy, where the model generates future frames by conditioning each step on the most recent + 1 latent frames. This procedure can be iteratively applied, enabling open-ended video generation with consistent temporal dynamics. Representative Historical Slots to Enhance Details. Human episodic memory is type of explicit memory that stores specific important events from the past, allowing us to recall the relevant experiences when needed. While the fused static point cloud captures stable scene geometry, the fused static point cloud is often too sparse to preserve detailed visual cues from the past. To compensate for this, we maintain set of representative historical frames as auxiliary references. Specifically, during generation, we monitor the size of newly revealed unknown regions via maskbased visibility checks. When the revealed area exceeds predefined threshold, the corresponding frame is selected and added to the memory set in an incremental fashion. 3.3 Memory-guided Video Generation Different from those video generation models that focus on camera control under the same temporal sequences [82, 7, 6, 83, 25], we mainly focus on the temporal progression. For example, for car driving on the road, when the camera moves to the left, the car should complete reasonable dynamic motion in accordance with the hints of the camera language and the input prompt, while maintaining the consistency of the static part to achieve the simulation of the interactive world model. Based on this requirement, we carefully design several key modules to achieve dynamic and static decoupling. As illustrated in Figure 2, first, we introduce static point cloud rendering as an additional conditioning input to our video diffusion model. The condition video is rendered from the current static spatial memory along the input trajectory, with background regions lacking point clouds set to black. We then utilize the pre-trained 3DVAE [79] to encode the static point cloud rendering into condition latents. We follow similar design as the ControlNET [88] to add the static point clouds rendering to guide the camera movement and keep the static areas consistency. We copy the first 18 pre-trained DiT blocks from CogVideoX as the condition DiT to process the condition latents. In the condition DiT, we process the output feature from each main DiT block through zero-initialized linear layer before adding it to the corresponding feature map in the main DiT. Second, to support the generation of new dynamic elements and the temporal extension of existing ones, we propose to concatenate the last five frames of source video tokens with the target video tokens along the frame dimension for dynamic context guidance. In addition, the target condition tokens are also combined with recent context tokens as mentioned above to ensure frame-level correspondence. Third, for modeling information exchange between memory frames and the frames currently being generated, we select the representative historical slots frames as auxiliary reference frames. This reference frames are also encoded by 3DVAE and patchify them as reference tokens. we add historical cross attention to guide information exchange between the frames currently being generated and memory frames. Specifically, the video tokens act as queries and the reference tokens serve as keys and values. 3.4 Geometry-grounded Video Dataset Creation To train and evaluate our geometry-grounded video generation model, we require custom dataset as shown in Figure 3 which is described in the following. Dataset Construction. We build our dataset from raw videos collected from MiraData [39], segmenting each video into multiple 97-frame clips. For each clip, the first 49 frames serve as the source sequence and the remaining 48 as the target, with shared transition frame to preserve temporal continuity. To recover scene geometry, we perform 4D reconstruction using Mega-SaM [44], extracting camera intrinsics, extrinsics, and per-frame depth maps. We apply TSDF-Fusion to the source frames, integrating RGB-D observations into volumetric grid. This process suppresses inconsistent depth caused by dynamic objects, yielding clean reconstruction of the static scene. Paired Training Data. Given the fused geometry, we project the target camera poses to render visibility masks and static-region reconstructions via point-based rendering. The full RGB frames of the target sequence are retained as future supervision, containing dynamic elements beyond the static 5 Figure 3: Dataset construction pipeline. We use Mega-SaM [44] to extract camera poses and dynamic point maps from the full video clip. For the source part, dynamic regions are erased via TSDF-Fusion, and the point cloud is rendered along the target trajectory to to serve as static geometry guidance for the target part. Qwen [77] generates annotations for actions in future target frames. Table 1: Quantitative evaluation. We evaluate our method and baselines using view recall (left) and user study (right). For view recall, we use standard metrics to compare the consistency of revisiting parts of scene. The user study provides relative average human ranking scores for three different criteria of all baselines. Our method outperforms these baselines in all cases. View Recall Consistency User Study Method PSNR SSIM LPIPS Cam-Acc Stat-Cons Dyn-Plaus TrajectoryCrafter DaS Wan2.1-Inpainting Ours 11.71 12.01 12.16 19. 0.4380 0.4512 0.4506 0.5996 0.5874 0.5875 0.6471 0.3069 1.6320 2.5660 2.1760 3. 1.7802 2.4396 2.3956 3.3846 1.6255 2.7033 2.2701 3.4011 scene memory. Our final dataset comprises 90K structured video samples, each paired with explicit 3D spatial memory and future observations. Additional details on memory storage and retrieval mechanisms as well as dataset creation are found in the supplement."
        },
        {
            "title": "4 Experiments",
            "content": "Implementation Details. We implement our conditional video diffusion model based on CogVideoX-5B-I2V [79] architecture, pretrained from DaS [25]. During training, we set the video length to 49 frames with resolution of 480 720. We trained for 6,000 iterations with learning rate of 2 105, using mini-batch size of 8 and are conducted on eight NVIDIA-A100 GPUs. At inference time, we adopt the latest 5 historical frames from the recent sequence to enable smooth motion prediction. At each auto-regressive iteration, the point map of the newly generated frames are predicted, aligned, and fused into the historical global point clouds, where we create new point rendering sequence given the aimed camera trajectory. Metrics and Baselines. Our evaluations primarily focus on baseline methods that use point-mapbased conditioning. This includes TrajctoryCrafter [82], DiffusionAsShader (DaS) [25], and also the state-of-the-art video generative model Wan2.1 [66]. Our test set includes 500 randomly selected video sequences from MiraData, which are not seen during training. We evaluate each method on view recall consistency and general video quality on multiple dimensions. Specifically, 1) For static 6 Table 2: Quantitative evaluation on VBench. Our model achieves top overall performance among relevant baselines, as measured by VBench metrics. Method Aesthetic Imaging Quality Quality Temporal Flickering Motion Background Subject Smoothness Consistency Consistency TrajectoryCrafter DaS Wan2.1-Inpainting Ours 0.5255 0.5635 0.5661 0.5835 0.6428 0.6617 0.6788 0. 0.6160 0.7520 0.6433 0.7580 0.9843 0.9856 0.9868 0.9886 0.8830 0.9325 0.9357 0.9359 0.9227 0.9494 0.9513 0.9506 Figure 4: Qualitative evaluation. We compare our approach to relevant baselines in several conditions. Baselines cannot accurately generate significant camera pose changes while maintaining consistent scene (top). When revisiting previously seen camera pose, baselines fail to complete sparse point clouds or forget details, resulting in inconsistency (center). The accuracy of prompted actions is often low, and sometimes the character disappears during the generation for the baselines (bottom). Our approach successfully handles these challenging scenarios. spatial information, the view recall consistency is evaluated using image reconstruction metrics (i.e., PSNR, SSIM, and LPIPS) for paired frames at the same camera location within video sequence generated with forward and reversed camera trajectory. 2) For general video quality evaluation, we use six metrics from Vbench [36] for the visual quality, motion smoothness, and consistency. Moreover, we conduct user study to further validate the criteria above. 4.1 Quantitative Evaluation Evaluating View Recall Consistency and Camera Accuracy. To verify that our method benefits from the spatial memory mechanism by maintaining high consistency and accurate camera control when revisiting previously generated parts of the world, we conduct experiments on reversed trajectories, where the same camera pose is visited twice, enabling the construction of paired data for reconstruction metrics. As shown in Table 1, our method achieves significantly improved scores in 7 Figure 5: Ablation of different memory mechanisms. We evaluate several variants of our model: w/o short-term working memory: the full model without recent context frames; w/o long-term episodic memory: the full model without sparse historical keyframes; full model including shortterm working and long-term spatial and episodic memory. Unsurprisingly, the working memory is required for smooth and plausible motions of dynamic objects. The episodic memory is crucial in helping remember visual details from the past, including previously seen characters or objects. terms of PSNR, SSIM, and LPIPS compared to all baselines, owing to our memory mechanisms. It is worth noting, however, that even the PSNR of our method is far from perfect, indicating that remembering each and every visual detail of complex scene is very challenging task. Evaluation on Video Quality. We further utilize VBench to evaluate general video quality across multiple dimensions, as shown in Table 2. Compared with baseline methods, our approach demonstrates better performance in aesthetic quality, reduced temporal flickering, smoother motion, and improved subject consistency. While Wan2.1 surpasses our method in imaging quality due to its advanced backbone, and also achieves higher score in background consistency, we note that the Wan2.1 inpainting model often fails to follow geometric guidance and tends to generate relatively static scenes, which makes it easier to maintain high background consistency scores. 4.2 Qualitative Evaluation We conduct qualitative comparisons with other geometry-grounded methods, focusing on three key criteria, as illustrated in Figure 4. First, our method demonstrates superior performance in accurately following camera trajectories, guided by point map rendering. In contrast, the Wan2.1 inpainting model and DaS model often fail, especially under significant camera motion. Second, for view recall consistency, we present pairwise comparisons between two frames generated at the same camera pose but at different points in the sequence. Compared with the baselines, our results exhibit significantly higher consistency in static regions during scene revisits, thanks to the static memory mechanism. Finally, we evaluate the ability to generate new actions based on instructions while incorporating static memory. We focus on the harmonious integration of static and dynamic elements, as well as how closely the dynamic components follow the instructions. The comparison shows that our method performs well in action prediction, whereas the others either fail to closely follow the instructions or suffer from action drifting, severe deformation, or even character disappearance. 4.3 User Study We selected 14 representative use cases, including novel-view synthesis of static scenes, novel-view synthesis of dynamic scenes with temporal progression of dynamic subjects in first-person and thirdperson perspectives, and scene styles covering realistic and game style. We conduct comprehensive user study, evaluating baselines and our methods from three perspectives: camera accuracy(Cam-Acc), static consistency(Stat-Cons), and dynamic plausibility (Dyn-Plaus). We invited 20 subjects, each with at least one year of experience in video/3D/4D generation, to rank the results generated by the four methods (TrajectoryCrafter, DaS, Wan2.1-Inpainting, and ours). Following ControlNet, we evaluate the results using the Average Human Ranking (AHR) metric, where participants rated each output on scale of 1 to 4 (with lower scores indicating poorer quality). The average rankings in Table 1 (right) show that our method outperforms the baselines by large margin in all metrics. 8 Table 3: Ablation of memory mechanisms. Using all three types, i.e., short-term working and long-term spatial and episodic memory, leads to the best results measured by VBench metrics. Method Aesthetic Imaging Quality Quality Temporal Flickering Motion Background Subject Smoothness Consistency Consistency w/o episodic mem w/o working mem Full model 0.5603 0.5551 0.5835 0.6485 0.6384 0.6701 0.7260 0.6740 0.7580 0.9870 0.9862 0.9886 0.9326 0.9331 0.9359 0.9489 0.9453 0. 4.4 Ablation Study To verify the effectiveness of each memory component in our video generation framework, we conduct comprehensive ablation studies, as shown in Table 3. Experimental results on VBench metrics indicate that each component consistently contributes to performance improvements. Unsurprisingly, the context frames play crucial role in enhancing short-term motion coherence. During the autoregressive generation process, the context frames enable smooth transitions in dynamic regions and help produce more plausible motions consistent with the preceding frames. The sparse set of historical reference frames enable the model to better retain and utilize temporally distant details. This episodic memory improves long-term consistency for static regions and subjects, and further enhances the plausibility and continuity of motions involving moving entities. The best results are achieved with both of these mechanisms as well as our long-term spatial memory. This is evidenced by both Table 3 and Figure 5, showing that each of our memory mechanisms significantly improves the models quality in terms of motion smoothness, static consistency, and overall visual quality."
        },
        {
            "title": "5 Discussion",
            "content": "Inspired by the mechanisms of human memory, we introduce geometrygrounded long-term spatial memory mechanism for video world models. This mechanism improves quality, spatial consistency, and context length compared to relevant baselines. Limitations and Future Work. The TSDF-Fusion algorithm we use for storing newly generated information into the spatial memory is far from perfect. Specifically, artifacts are introduced when looking at previously generated content from camera poses that are very different from those of the previous observations, as illustrated in Figure 6. Our memory mechanism is primarily designed to enable spatial consistency whereas frame packing strategies for extending the temporal context window size [87] primarily focus on character consistency. Future work may combine these mechanisms to achieve both types of consistency. The forgetting problem we tackle is just one of several challenges of video world models. Drift, or image quality degradation due to error accumulation over time, is another challenge that we do not address. Figure 6: Failure case. When the distance between consecutive camera poses is too large and the trajectory exhibits overly abrupt angles, the 4D reconstruction may fail, resulting in significant ghosting artifacts between frames. Consequently, TSDF-Fusion will filter out large portion of point clouds that should belong to static regions, ultimately leading to an extremely sparse spatial memory and loss of critical information. For example, Spiderman swiftly swinging between skyscrapers illustrates how such challenging camera trajectory can cause omissions in spatial memory storage, resulting in imprecise camera control and inconsistencies. Societal Impacts. Video generation models provide significant benefits for content creation but could be adapted for DeepFake generation. Such applications pose significant societal risks and 9 Figure S1: Autoregressive point cloud fusion. The system continuously updates spatial memory by integrating newly observed static maps. These maps are reconstructed online using CUT3R in recurrent manner, while TSDF-Fusion filters out dynamic elements to maintain map consistency. we strongly oppose the use of our work to create deceptive content intended to mislead or spread misinformation. Conclusion. Video world models play crucial role for content creation or creating training data for agents or robots. Enabling long-term consistency through memory mechanisms like ours makes these models more effective."
        },
        {
            "title": "6 Acknowledgment",
            "content": "We thank Google for their support. Ryan Po is supported by Stanford Graduate Fellowship."
        },
        {
            "title": "A Additional Implementation details",
            "content": "Autoregressive point cloud fusion. In autoregressive generation, the length of the generated video increases over time. At each step, new static points map is produced and updated into our spatial memory. Since Mega-SAM performs reconstruction in the NDC coordinate system, it is impossible to directly merge results from different stages without alignment, and long video inference will also fail due to CUDA memory limitations. Therefore, unlike the precise reconstruction using MegaSAM during the data construction stage, we employ CUT3R [69] for 4D reconstruction during the inference stage. CUT3R is unified online 3D perception framework featuring stateful recurrent model that incrementally updates persistent internal state representation with each new observation. Given an image stream (video or unordered photos), the model simultaneously updates its state and predicts metric-scale pointmaps (per-pixel 3D points in shared world coordinate system) and camera parameters for each input in an online manner. At each inference step, we save the state dict of the current CUT3R model and the parameters of the pose retriever to serve as initialization for the next inference step, ensuring that the reconstruction results of each step remain within the same coordinate system. Therefore, as shown in Figure S1, after tsdf-fusion, the filtered points cloud of the current step can be directly merged and aligned with the previous spatial static memory to achieve autoregressive point cloud fusion. Details in static point extraction for the dataset. In Mega-SAM, we first resize the input video resolution to 384672. Based on the initial results, we perform optical flow estimation to refine the estimated camera motion through pixel-level motion cues. Subsequently, Covariance-based Variable Decomposition strategy is employed to further enhance the robustness and accuracy of the predicted results. In TSDF-fusion, we compute the initial grid dimensions based on the current voxel 10 size and the scene bounds. If the maximum dimension exceeds predefined threshold (1200), we proportionally scale the voxel size to ensure that the grid dimensions remain within the limit. The final adjusted voxel size is returned. The core principle is to control the grid resolution to prevent memory overflow. Additional discussion on related works. Learning-Based 3D Reconstruction Traditional structure-from-motion (SfM) pipelines such as COLMAP [55] have long served as the gold standard for image-based 3D reconstruction due to their high accuracy, but they rely on incremental feature matching and bundle adjustment that do not scale well to large-scale or real-time applications. To overcome these limitations, learning-based approaches have emerged: the seminal DUT3R [70] method established new paradigm of dense matching and optimization in an end-to-end deep learning framework, eliminating the need for separate SfM or multi-view stereo modules. Building on this, recent approaches [89, 68] introduce feed-forward architectures that significantly accelerate inference, achieving near real-time reconstruction without iterative optimization. To further process dynamic scenes, more recent advances [86? , 69] further extend learning-based 3D reconstruction to dynamic scenes by incorporating recurrent models or persistent state, which improves efficiency and robustness in handling moving objects and changing environments. Some of these advances make it feasible to perform online estimation of camera poses and point clouds during video capture or generation. In this work, we use Mega-SAM during dataset construction to obtain more stable and accurate point cloud and camera pose estimations. However, due to concerns about time efficiency and global alignment, we adopt CUT3R in our iterative video generation process. CUT3R jointly estimates per-frame point clouds while incorporating dynamic-static disentanglement mechanism to preserve long-term static scene memory. Experimental results indicate that the difference in models does not lead to significant performance gap."
        },
        {
            "title": "References",
            "content": "[1] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and François Fleuret. Diffusion for world modeling: Visual details matter in atari. Advances in Neural Information Processing Systems, 37:5875758791, 2024. [2] Genesis Authors. Genesis: universal and generative physics engine for robotics and beyond. URL https://github. com/Genesis-Embodied-AI/Genesis, 2024. [3] Alan Baddeley. Essentials of human memory. Psychology Press, 2013. [4] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David B. Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. Proc. CVPR, 2025. [5] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, David B. Lindell, and Sergey Tulyakov. VD3d: Taming large video diffusion transformers for 3d camera control. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=0n4bS0R5MM. [6] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, and Di Zhang. Recammaster: Camera-controlled generative rendering from single video, 2025. URL https://arxiv.org/abs/2503.11647. [7] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di ZHANG. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=m8Rk3HLGFx. [8] A. Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, and Dominik Lorenz. Stable video diffusion: Scaling latent video diffusion models to large datasets. ArXiv, abs/2311.15127, 2023. URL https://api.semanticscholar.org/CorpusID: 265312551. 11 [9] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [10] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. [11] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1:8, 2024. [12] Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. arXiv preprint arXiv:2411.00769, 2024. [13] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In NeurIPS, 2024. [14] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhi-Xin Xu, Yuzhe Jin, Yupeng Liang, Yu-Ning Song, Peng Zhao, Bo Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, and Yahui Zhou. Skyreels-v2: Infinite-length film generative model. 2025. URL https://api.semanticscholar.org/CorpusID:277856899. [15] Luxi Chen, Zihan Zhou, Min Zhao, Yikai Wang, Ge Zhang, Wenhao Huang, Hao Sun, Ji-Rong Wen, and Chongxuan Li. Flexworld: Progressively expanding 3d scenes for flexiable-view synthesis. arXiv preprint arXiv:2503.13265, 2025. [16] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In ICLR, 2023. [17] Etched Decart, McIntyre, Campbell, Xinlei Chen, and Wachen. Oasis: universe in transformer. URL: https://oasis-model. github. io, 2024. [18] Julian Decart, Quinn Quevedo, Spruce McIntyre, Xinlei Campbell, Robert Chen, and Wachen. Oasis: universe in transformer. 2024. URL https://oasis-model.github.io/. [19] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. ArXiv, abs/2105.05233, 2021. URL https://api.semanticscholar.org/CorpusID: 234357997. [20] Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, and Yahui Zhou. Skyreels-a2: Compose anything in video diffusion transformers. 2025. URL https://api.semanticscholar.org/CorpusID:277509893. [21] Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, and Hongyang Zhang. The matrix: Infinite-horizon world generation with real-time moving control. arXiv preprint arXiv:2412.03568, 2024. [22] Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, and Jun Xiao. Vid-gpt: Introducing gpt-style autoregressive generation in video diffusion models. arXiv preprint arXiv:2406.10981, 2024. [23] Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, Jun Xiao, and Long Chen. Ca2vdm: Efficient autoregressive video diffusion model with causal generation and cache sharing. arXiv preprint arXiv:2411.16375, 2024. [24] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction. ArXiv, abs/2503.19325, 2025. URL https://api. semanticscholar.org/CorpusID:277313237. 12 [25] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, and Yuan Liu. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025. [26] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. In ECCV, 2024. [27] David Ha and Jürgen Schmidhuber. in Neural Inc., facilitate policy Information Processing Systems 31, pages evolution. 24512463. Curran Associates, URL https://papers.nips.cc/ 2018. paper/7512-recurrent-world-models-facilitate-policy-evolution. https:// worldmodels.github.io."
        },
        {
            "title": "In Advances",
            "content": "[28] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [29] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. [30] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for video diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=Z4evOUYrk7. [31] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and Hongsheng Li. Cameractrl ii: Dynamic scene exploration via cameracontrolled video diffusion models. arXiv preprint arXiv:2503.10592, 2025. [32] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. [33] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. [34] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [35] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In NeurIPS, 2022. [36] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [37] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Zhuang Nan, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. ArXiv, abs/2410.05954, 2024. URL https://api.semanticscholar. org/CorpusID:273228937. [38] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In ICLR, 2025. [39] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions, 2024. URL https://arxiv.org/abs/2407.06358. [40] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. In ICML, 2024. 13 [41] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [42] Alex X. Lee, Richard Zhang, Frederik Ebert, P. Abbeel, Chelsea Finn, and Sergey Levine. Stochastic adversarial video prediction. ArXiv, abs/1804.01523, 2018. URL https://api. semanticscholar.org/CorpusID:4591836. [43] Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, and Jun-Yan Zhu. Efficient spatially sparse inference for conditional gans and diffusion models. Advances in neural information processing systems, 35:2885828873, 2022. [44] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [45] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, Tanghui Jia, Junwu Zhang, Zhenyu Tang, Yatian Pang, Bin She, Cen Yan, Zhiheng Hu, Xiao wen Dong, Lin Chen, Zhang Pan, Xing Zhou, Shaoling Dong, Yonghong Tian, and Li Yuan. Open-sora plan: Open-source large video generation model. ArXiv, abs/2412.00131, 2024. URL https://api.semanticscholar. org/CorpusID:274436278. [46] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. ArXiv, abs/2210.02747, 2022. URL https://api. semanticscholar.org/CorpusID:252734897. [47] Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. ArXiv, abs/2409.02097, 2024. URL https://api.semanticscholar.org/ CorpusID:272366893. [48] Michaël Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. CoRR, abs/1511.05440, 2015. URL https://api.semanticscholar. org/CorpusID:205514. [49] Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, and Yutaka Matsuo. Ssm meets video diffusion models: Efficient long-term video generation with structured state spaces. arXiv preprint arXiv:2403.07711, 2024. [50] Parker-Holder, Ball, Bruce, Dasagi, Holsheimer, Kaplanis, Moufarek, Scully, Shar, Shi, et al. Genie 2: large-scale foundation world model. URL: https://deepmind. google/discover/blog/genie-2-a-large-scale-foundation-world-model, 2024. [51] Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktäschel. Genie 2: large-scale foundation world model. 2024. URL https://deepmind.google/discover/blog/ genie-2-a-large-scale-foundation-world-model/. [52] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan Barron, Amit Bermano, Eric Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, et al. State of the art on diffusion models for visual computing. Computer Graphics Forum, 43(2):e15063, 2024. [53] Di Qiu, Zhengcong Fei, Rui Wang, Jialin Bai, Changqian Yu, Mingyuan Fan, Guibin Chen, and Xiang Wen. Skyreels-a1: Expressive portrait animation in video diffusion transformers. abs/2502.10841, 2025. URL https://api.semanticscholar.org/CorpusID: 276409428. 14 [54] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed worldconsistent video generation with precise camera control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [55] Johannes Lutz Schönberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [56] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In ICLR, 2023. [57] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. 2015. [58] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. arXiv preprint arXiv:2502.06764, 2025. [59] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. [60] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [61] Anni Tang, Tianyu He, Junliang Guo, Xinle Cheng, Li Song, and Jiang Bian. Vidtok: versatile and open-source video tokenizer. ArXiv, abs/2412.13061, 2024. URL https://api. semanticscholar.org/CorpusID:274788955. [62] S. Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15261535, 2017. URL https://api.semanticscholar.org/ CorpusID:4475365. [63] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. [64] Villegas, Moraldo, Castro, Babaeizadeh, Zhang, Kunze, PJ Kindermans, MT Saffar, and Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In ICLR, 2023. [65] Carl Vondrick and Antonio Torralba. Generating the future with adversarial transformers. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 29923000, 2017. URL https://api.semanticscholar.org/CorpusID:8234308. [66] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [67] Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo, Peizhao Zhang, Tingbo Hou, Peter Vajda, Niraj Kumar Jha, and Xiaoliang Dai. Lingen: Towards high-resolution minute-length text-to-video generation with linear computational complexity. ArXiv, abs/2412.09856, 2024. URL https://api.semanticscholar. org/CorpusID:274763003. [68] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [69] Qianqian Wang*, Yifei Zhang*, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. In Proceedings of the IEEE/CVF Continuous 3d perception model with persistent state. Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [70] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2069720709, June 2024. [71] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. [72] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [73] Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, et al. Art-v: Auto-regressive text-to-video generation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73957405, 2024. [74] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World models for physical robot learning. In Conference on robot learning, pages 22262240. PMLR, 2023. [75] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. ArXiv, abs/2410.10629, 2024. URL https: //api.semanticscholar.org/CorpusID:273346094. [76] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. [77] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [78] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Textto-video diffusion models with an expert transformer. ArXiv, abs/2408.06072, 2024. URL https://api.semanticscholar.org/CorpusID:271855655. [79] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [80] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. [81] Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Gamefactory: Creating new games with generative interactive videos. arXiv preprint arXiv:2501.08325, 2025. [82] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models. arXiv preprint arXiv:2503.05638, 2025. [83] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. [84] Andy Zeng, Shuran Song, Matthias Nießner, Matthew Fisher, Jianxiong Xiao, and Thomas Funkhouser. 3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In CVPR, 2017. [85] David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, and Nataniel Ruiz. Recapture: Generative video camera controls for user-provided videos using masked video fine-tuning. arXiv preprint arXiv:2411.05003, 2024. [86] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. MonST3r: simple approach for estimating geometry in the presence of motion. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=lJpqxFgWCM. [87] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. [88] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [89] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [90] Zhicheng Zhang, Junyao Hu, Wentao Cheng, Danda Paudel, and Jufeng Yang. Extdm: Distribution extrapolation diffusion model for video prediction. In CVPR, 2024. [91] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. ArXiv, abs/2412.20404, 2024. URL https://api.semanticscholar.org/CorpusID: 275133398. [92] Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al. Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520, 2024."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "Stanford University",
        "The Chinese University of Hong Kong"
    ]
}