{
    "paper_title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
    "authors": [
        "Yunzhen Feng",
        "Ariel Kwiatkowski",
        "Kunhao Zheng",
        "Julia Kempe",
        "Yaqi Duan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical."
        },
        {
            "title": "Start",
            "content": "PILAF: Optimal Human Preference Sampling for Reward Modeling Yunzhen Feng Ariel Kwiatkowski Kunhao Zheng Meta FAIR Meta FAIR NYU Julia Kempe Meta FAIR & NYU Yaqi Duan NYU 5 2 0 2 6 ] . [ 1 0 7 2 4 0 . 2 0 5 2 : r February 7, Abstract As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from both an optimization and statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) has revolutionized large language models (LLMs) by incorporating human preferences, enabling significant progress in applications such as conversational AI (Achiam et al., 2023), personalized tutoring (Limo et al., 2023), and content curation (Yue et al., 2024). At the core of RLHF is reward modeling, critical process that translates human feedbacksuch as pairwise comparisons or rankingsinto measurable objective for model training. By formalizing human preferences, reward models then guide LLMs towards alignment through policy optimization. While numerous studies have focused on improving language models (LMs) by optimizing fixed reward functions (Dong et al., 2023; Liu et al., 2024c) or leveraging pre-existing preference datasets (Ethayarajh et al., 2024; Azar et al., 2024; Xu et al., 2024), comparatively less attention has been paid to the critical challenge of collecting effective data for human-labeling in RLHF, to maximize its utility. This is an important problem, as the quality of preference data directly impacts the effectiveness of reward modeling and, consequently, the overall Correspondence to: Yunzhen Feng (yf2231@nyu.edu). Joint second authors. Joint senior authors 1 Figure 1. Overview of our approach. (a) We consider full RLHF training setup, where language model (LM) policy is iteratively refined through active data collection. Our goal is to develop an optimal response sampling method for preference labeling. (b) We introduce PILAF, which generates responses by interpolating between the current policy and reference policy, balancing exploration and exploitation. (c) Our theoretical analysis shows that T-PILAF aligns the parameter gradient with the steepest direction for maximizing human values and achieves more favorable convergence in regions of high sensitivity. success of fine-tuning. This challenge is further compounded by the high cost of expert preference labeling (Lightman et al., 2023). , yb Preference data is usually generated by sampling response pairs (ya ) to prompt xi from policy, and presenting them to human labelers for preference annotation. It is commonly assumed that the annotation follows the Bradley-Terry (BT) model, under an oracle reward. Next, we use maximum likelihood estimation (MLE) on these preference data to train reward model, which then serves as measurable objective to optimize the policy (i.e. LLM) while staying close to reference policy. In Direct Preference Optimization (DPO) (Rafailov et al., 2023), this pipeline is simplified by optimizing the policy with implicit reward modeling. However, all these pipelines give rise to misalignment of objectives: RLHF (or DPO) should, in principle, train its policy to maximize the (inaccessible) oracle objective which combines the oracle reward from the BT model with reference regularization. In practice, RLHF relies on preference data through the MLE objective in reward modeling or through methods like DPO, which are not designed to guide policy optimization towards maximizing oracle rewards. Thus, reward optimization (either directly or implicitly via DPO) and (optimal) policy optimization are not inherently aligned, potentially leading to inefficiencies (Sec. 2). , yb In this work, we study this misalignment by examining the sampling scheme that generates response pairs (ya ) for preference labeling, which is especially important when additional preference data is collected mid-RLHF training to mitigate the off-policy distributional shift, as is empirically standard (Touvron et al., 2023; Bai et al., 2022). We show that uniform sampling from the current policy, as is common, leads to misaligned gradients of the two objectives (reward model loss and true oracle objective). To tackle this issue, we present Theoretically Grounded Policy-Interpolated Learning for Aligned Feedback (T-PILAF), novel sampling method that aligns reward modeling with value optimization. Specfically, T-PILAF generates responses by interpolating the policy model and the reference model for balanced exploration and exploitation. We provide rigorous theoretical analysis to show that for preference data generated with T-PILAF, the gradient of the MLE loss with respect to the policy networks parameters is aligned with the policy gradient of the oracle objective in first-order sense. This alignment enables the policy to optimize directly for the oracle value, achieving both alignment and efficiency. Furthermore, we separately show from statistical perspective that T-PILAF aligns optimization with the steepest directions of the oracle objective. It thus makes the sampled preference pairs more informative, reducing variance and improving training stability. We then present PILAF, simple modification of our theoretical sampling scheme T-PILAF, which naturally lends itself to practical implementation. For clarity of exposition, we present our method in the context of DPO; however, PILAF can be adapted to wide class of preference optimization methods.1 See Figure 1 for an illustration of our setup, method, and the optimization and statistical principles underlying PILAF. We conduct extensive experiments to validate PILAFs effectiveness and robustness. As stand-in for expensive human annotators, we use well-trained reward modelSkyworkLlama-3.1-8B (Liu et al., 2024a)as proxy for the oracle reward. Throughout training, we query this model exclusively for preference labels, simulating human feedback. We then align the Llama-3.1-8B base model (Dubey et al., 2024) using these proxy-labeled preference data in two settings: iterative DPO (Xiong et al., 2024) and online DPO (Guo et al., 2024). In both scenarios, preference data is collected on-the-fly, either after each full training epoch in the iterative setting or after every training step in the online setting. Across all configurations, PILAF outperforms all the baselines, producing policy with higher reward (as measured by the proxy) and lower KL divergence from the reference model, reducing annotation and computation costs by over 40% in iterative DPO. Our key contributions are as follows: (Practical sampling algorithm) We propose PILAF (Section 5), an efficient sampling algorithm for generating response pairs in the RLHF pipeline for improved sample efficiency and performance, derived from its theoretically grounded variant T-PILAF (Section 3). (Theoretical optimality) We provide theoretical guarantees for the efficiency of our approach from both optimization and statistical perspectives (Section 4). (Empirical validation) We validate PILAF in both iterative and online DPO settings (Section 6) and observe that it consistently outperforms baselines by achieving higher reward and lower KL divergence from the reference model. Moreover, PILAF achieves comparable performance at significantly reduced annotation and computational costs. 1See Appendix for the extension to PPO."
        },
        {
            "title": "1.1 Related Work",
            "content": "Existing Sampling Schemes. In academic papers, uniform vanilla sampling is the most commonly used approach, while methods such as best-of-N and worst-of-N have also been explored (Dong et al., 2024). Xie et al. (2024) propose sampling one response from the current policy model and another from reference model, modifying the loss function to encourage optimistic behavior. Similarly, Zhang et al. (2024) sample one response from the current model but rank it alongside two offline responses from the reference model. Shi et al. (2024) present formula similar to ours based on intuition, introducing several hyperparameters and analyzing convergence speed with DPO in tabular setting. Liu et al. (2024d) train an ensemble of reward models to approximate posterior distribution over possible rewards and use Thompson sampling to generate responses with exploration. In contrast to these works, we theoretically establish the principles of response generation for preference labeling, making minimal assumptions and simplifications while demonstrating the optimality of our approach. Our approach eliminates the need for hyperparameter tuning. Policy Gradient. Our theoretical principle is closely related to the family of policy gradient methods (Williams, 1992; Sutton et al., 1999) in reinforcement learning, which optimize policy πθ by estimating and ascending the gradient of the expected return θJ(θ). Significant advancements have been made to improve the efficiency of these methods, including variance reduction techniques (Greensmith et al., 2004), off-policy gradient estimation (Degris et al., 2012), interpolating on-policy and off-policy updates (Gu et al., 2017), deterministic policy gradients (Silver et al., 2014), and three-way robust estimation approaches (Kallus & Uehara, 2020). Our study extends these principles to preference learning for LMs, aligning the MLE gradient with the oracle objective gradient by controlling the response sampling distribution, thereby improving learning efficiency. review of other RLHF literature, particularly on data selection for the preference dataset, is deferred to Appendix A."
        },
        {
            "title": "2 Problem Setup and Motivation",
            "content": "In this section, we introduce the setup for the problem studied in this work. In Section 2.1, we present the basic framework for aligning language models with human preferences. In Section 2.2, we provide an overview of the widely-used Direct Preference Optimization (DPO) method. Finally, in Section 2.3, we introduce the core problem investigated in this work: designing an optimal sampling scheme for response generation."
        },
        {
            "title": "2.1 Aligning LMs with Human Preferences",
            "content": "Language Model (LM). At the core of RLHF is language model that processes prompts and generates responses Y. Each response is represented as sequence of tokens = (y1, y2, . . . , yT ). The primary goal of RLHF is to guide the model to generate responses that align with human preferences. This translates to designing policy π 4 (parameterized as LM) that maps prompts to responses, maximizing reward that reflects human preferences (with KL regularization). Preference Data. The oracle reward for human values is inherently inaccessible. Instead, the alignment process approximates the reward using dataset of human-labeled preferences, = (cid:8)(xi, yw , yℓ i)(cid:9)n i=1 , where each sample contains: (i) prompt xi, independently drawn from distribution ρ, and (ii) pair of responses (yw is preferred over yℓ in human labeling. The response pair (yw i) is first generated from joint distribution µ( x) and then presented to human labelers for preference annotation. Human preferences are commonly modeled using the BradleyTerry (BT) model, which assumes: i), where yw , yℓ , yℓ P(cid:0)ya yb (cid:12) (cid:12) x(cid:1) = σ(cid:0)r(x, ya) r(x, yb)(cid:1) , (1) where r(x, y) represents the (unknown) oracle reward of response given prompt, and σ(z) = {1+exp(z)}1 is the sigmoid function, mapping differences in rewards to probabilities. We adopt the BT model throughout this paper. Reward Modeling. The preference data, encoding human judgment, is then used to train reward model, rθ, which serves as measurable objective for training the policy model. rθ is trained by solving MLE objective: min θ (cid:98)L(θ) := 1 (cid:88) i=1 (cid:16) log σ (cid:0)xi, yw (cid:1) rθ (cid:0)xi, yℓ rθ (cid:1)(cid:17) . This empirical loss approximates the expected negative log-likelihood L(θ) := Exρ, (ya,yb)µ(x) (cid:104) log σ(cid:0)rθ(x, yw) rθ(x, yℓ)(cid:1)(cid:105) . (2) (3) Policy Optimization. To align language model ϕ with human preferences, we optimize it to maximize the learned rewards rθ while staying close to reference policy πref. The objective is (cid:2)rθ(x, y)(cid:3) βDKL(πϕ πref). (4) maxϕ Exρ,yπϕ(x) It consists of two parts: (i) The reward term Exρ, yπ(x)[rθ(x, y)] encourages the policy to generate high-quality responses. (ii) The regularization term DKL(π πref) penalizes deviations from the reference policy πref and is defined as Exρ (cid:2)DKL (cid:0)π( x) (cid:13) (cid:13) πref( x)(cid:1)(cid:3). Here, β is regularization parameter that balances the trade-off between reward maximization and adherence to the reference policy. We assume β is fixed and practitioner-specified."
        },
        {
            "title": "2.2 Direct Preference Optimization",
            "content": "The above-described RLHF pipeline typically leverages the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) to perform policy optimization. This approach requires loading the policy network, reward model, reference model, and value model onto the GPU during training, making it highly resource-intensive. To improve computational efficiency and practicality, Direct Preference Optimization (DPO) (Rafailov et al., 2023) has been proposed, enabling direct alignment without the need for reward model or value model. key insight of DPO is that any policy πθ can be viewed as the optimal solution to problem (4) if the reward rθ is rθ(x, y) : = β log (cid:18) πθ(y x) πref(y x) (cid:19) . (5) Thus, DPO can directly optimize the policy πθ using (cid:98)L(θ) in Equation (2), where rθ is replaced by πθ as defined in Equation (5). This reformulation makes the objective dependent solely on θ, with the reward being implicitly learned through the policy itself. As result, the optimization process becomes significantly more efficient."
        },
        {
            "title": "2.3 Motivation: Realigning Oracle Reward Maximization",
            "content": "To fully align with human values, RLHF should, in principle, train the policy to maximize the oracle reward, r, as defined in the BT model. The corresponding oracle objective is then: J(π) : = Exρ, yπ(x) (cid:2)r(x, y)(cid:3) β DKL(π πref) . Since direct access to is unavailable, RLHF instead relies on preference data, either through MLE-based reward modeling or methods like DPO. However, these processes are not inherently designed to train the policy to directly maximize the oracle objective, J(π). In this work, we aim to design an optimal sampling distribution µ to realign DPO with the maximization of J(π). Such sampling strategy will improve the quality of the preference dataset, maximize the utility of limited data, and enhance both performance and efficiency. This focus is particularly crucial in scenarios where additional data is collected during midtraininga key phase in the iterative fine-tuning of LMs (Touvron et al., 2023; Bai et al., 2022; Xiong et al., 2024; Guo et al., 2024). At this stage, preliminary policy πθ (distinct from πref) is already in place, but its performance may fall short of expectations. It is thus necessary to gather additional preference data, ideally on-policy data that target areas where the current policy shows room for improvement. An effective sampling design can significantly enhance the efficiency of leveraging human feedback in this process."
        },
        {
            "title": "3 T-PILAF: Theoretical Sampling Scheme",
            "content": "We now present T-PILAF - theoretically grounded policy interpolation for aligned feedback - our sampling scheme for generating responses in data collection2. The scheme is shown (in Section 4) to be optimal from both optimization and statistical perspectives. Consider we have an initial policy πθ and aim to collect preference data to further refine its performance. We propose two complementary variants of policy πθ: one that encourages exploration in regions more preferred by πθ, reflecting an optimistic perspective, and another that focuses on areas less favored by πθ, reflecting conservative adjustment. Specifically, we define policies π+ θ and π θ around πθ as π+ θ (y x) := π θ (y x) := 1 + θ (x) 1 θ (x) πθ(y x) exp (cid:8)rθ(x, y)(cid:9) , πθ(y x) exp (cid:8) rθ(x, y)(cid:9), (6a) (6b) where the reward function rθ is defined in equation (5). The partition function + θ (x)) is given by + πθ(y x) exp{rθ(x, y)} dy. θ (x) : = (cid:82) θ (x) (or For any prompt , our sampling procedure involves the following steps: (i) Draw random variable ξ from Bernoulli(p0(x)), where p0(x) : = + θ (x) θ (x)/{1 + + θ (x) θ (x)}. (ii) If ξ = 1, independently draw responses ya, yb according to ya π+ θ ( x) and yb π θ ( x). If ξ = 0, draw responses as ya, yb πθ( x). In the next section, we will theoretically analyze T-PILAF. To account for the changes in sampling, we adopt slightly modified loss function in the theoretical framework: (cid:98)L(θ) : = 1 n (cid:88) i=1 w(xi) log σ (cid:16) (cid:0)xi, yw (cid:1) rθ (cid:0)xi, yℓ rθ (cid:1)(cid:17) . The newly introduced weight function is defined as w(x) : = (cid:8)1 + + θ (x) θ (x)(cid:9)/ θ , (7) where the normalization constant θ > 0 is given by θ : = 1 + (cid:82) θ (x) also modify the population loss in Equation (3) with the weight function. + θ (x) ρ(x) dx. We 2The in T-PILAF serves to distinguish the theoretical scheme from the derived, simplified, efficiently implementable PILAF."
        },
        {
            "title": "4 Theoretical Analysis",
            "content": "This section provides the theoretical grounding and analysis of our proposed sampling In the optimization analysis (Section 4.1) we show that scheme from two perspectives. T-PILAF aligns two objectives (gradient alignment property): maximizing the likelihood function (Equation (3)) becomes equivalent to gradient ascent on the value function J(πθ) (Equation (6)). Consequently, policy updates on πθ move the parameters in the direction of steepest increase of J. T-PILAF thus provides the potential to accelerate training and improve generalization, compared to vanilla (uniform) sampling. In the statistical analysis (Section 4.2) we focus on statistical error and show that the asymptotic covariance of the estimated parameter (cid:98)θ (inversely) aligns with the Hessian of the objective function when sampling with T-PILAF. As result, T-PILAF makes the sampled comparisons more informative, as they align with directions where is most sensitive. The net outcome is reduced statistical variance of our method through tighter concentration of estimates in directions that matter most for performance."
        },
        {
            "title": "4.1 Optimization Considerations",
            "content": "We begin by analyzing the DPO algorithm from an optimization perspective. Theorem 4.1 below formally illustrates how T-PILAF ensures alignment between the MLE gradient, θ L(θ), and the oracle objective gradient, θ J(πθ). Theorem 4.1 (Gradient structure in DPO training). Using data collected from our proposed response sampling scheme T-PILAF, the gradient of L(θ) satisfies θ L(θ) = β θ θ J(πθ) + T2 , where the constant θ is defined in equation (7), and the term T2 represents second-order error. The detailed proof of Theorem 4.1 is deferred to Appendix B.1. It involves calculation of explicit forms of the gradients θ L(θ) and θ J(πθ); the most notable technical contribution is showing how to leverage our sampling scheme to approximate the derivative σ of the sigmoid function. By using T-PILAF sampling, we can transform difference term of the form σ(r) σ(rθ) in θ L(θ) into linear difference rθ in θ J(πθ). Theorem 4.1 establishes the gradient alignment property, demonstrating that minimizing the likelihood-based loss function closely aligns with maximizing the oracle objective function J, with only minor second-order error. It highlights how the proposed sampling scheme enables the DPO framework to effectively guide the policy toward optimizing the expected reward. Beyond DPO, in Appendix F, we show how the same principle can be applied to PPO-based RLHF algorithms to help improve the sampling."
        },
        {
            "title": "4.2 Statistical Considerations",
            "content": "From statistical standpoint, we first examine the asymptotic distribution of the estimated parameter (cid:98)θ when it (approximately) solves the optimization problem (2). In Theorem 4.2, we formally characterize the randomness or statistical error inherent in (cid:98)θ under this idealized scenario. The detailed proof of Theorem 4.2 is provided in Appendix B.2.2. Theorem 4.2. Assume the reward model in the BT model (1) satisfies = rθ for some parameter θ. Under mild regularity conditions, the estimate (cid:98)θ asymptotically follows Gaussian distribution ((cid:98)θ θ) (0, Ω) as . We have an estimate of the covariance matrix Ω: Ω C1 Σ1 , where C1 > 0 is universal constant. When using T-PILAF, the matrix Σ is given by Σ : = Exρ (cid:104) Covyπ(x) (cid:2)θ r(x, y) (cid:12) (cid:12) x(cid:3)(cid:105) . (8) Next we analyze the performance of the output policy (cid:98)π = π (cid:98)θ from Theorem 4.2 in terms of the expected value J(π). In Theorem 4.3, we show that our proposed sampling method guarantees that the covariance of the statistical error in (cid:98)θ aligns inversely with the Hessian of at the optimal policy π. This alignment prioritizes convergence efficiency along directions where the Hessian has large eigenvalues, adapting to the geometry of the optimization landscape. It highlights the efficiency of our sampling scheme in reducing statistical error. For the detailed proof, see Appendix B.2.3. Theorem 4.3. The value function J(π) we define in equation (6) satisfies θ J(π) = 0 and 2 θ J(π) = 1 β Σ (9) for matrix Σ defined in equation (8). As corollary, suppose Σ is nonsingular, then there exists constant C2 > 0 such that for any ε > 0, lim sup (cid:26) J((cid:98)π) < J(π) C2 (cid:27) (1 + ε) P(cid:8)χ2 > (1 + ε) d(cid:9) exp (cid:110) 2 (cid:0)ε log(1 + ε)(cid:1)(cid:111) . (10) Our proposed sampling distribution µ ensures that the output policy (cid:98)π performs predictably and reliably. The value gap J(π) J((cid:98)π) asymptotically follows chi-square distribution, irrespective of the problem instance details, such as the underlying reward model r. This 9 structure-invariant statistical efficiency allows the method to achieve asymptotically efficient estimates without requiring explicit knowledge of the model structure. In addition to our analysis of the proposed sampling scheme in Section 3, we present generalized version of Theorem 4.2 that applies to any response sampling distribution µ. While not directly tied to the main focus of this work, this broader result may be of independent interest to readers. The proof of Lemma 4.4 is provided in Appendix B.2.1. Lemma 4.4. For general sampling distribution µ, the statement in Theorem 4.2 remains valid with the matrix Σ redefined as Σ : = Exρ,(ya, yb)µ(x) (cid:104) w(x) Var(cid:0)1{ya = yw} (cid:12) (cid:12) x, ya, yb(cid:1) g(cid:105) , (11) where the expectation is taken over the distribution µ(ya, yb x) : = 1 2 (cid:8)µ(ya, yb x) + µ(yb, ya x)(cid:9) . (12a) The variance term is specified as Var(cid:0)1{ya = yw} x, ya, yb(cid:1) = σ(cid:0)r(x, ya) r(x, yb)(cid:1) σ(cid:0)r(x, yb) r(x, ya)(cid:1) (12b) and the gradient difference is defined as : = θ r(x, ya) θ r(x, yb) . (12c) The general form of the matrix Σ offers valuable insights for designing sampling scheme. To ensure Σ is well-conditioned (less singular), we must balance two key factors when selecting responses ya and yb: Large variance: The variance in definition (12b) should be maximized. This occurs when r(x, ya) r(x, yb). Intuitively, preference feedback is most informative when annotators compare responses of similar quality. Large gradient difference: The gradient difference from definition (12c) should also be large. This requires responses with significantly different gradients. Only then can the comparison provide clear and meaningful direction for model training."
        },
        {
            "title": "5 PILAF Algorithm",
            "content": "We now demonstrate that the T-PILAF sampling scheme defined in Equation (6a) and (6b) can be naturally extended into an efficient empirical algorithm (PILAF). The first challenge in implementing these definitions lies in calculating the normalizing factors θ (x) and + θ (x), which can be computationally expensive for LLMs. To address this, we 10 simplify the process by omitting these factors and replacing them with 1.3 Consequently, the sampling process becomes straightforward: with probability 1/2, we sample using πθ, and otherwise, we sample using π+ θ and π θ . The second challenge lies in sampling response from πθ(y x) exp (cid:8) rθ(x, y)(cid:9) in an autoregressive way for next-token generation. We argue that the policy π+ θ ) can be approximated in token-wise manner: θ (and π θ (y x) π+ π+ θ (y1 x) π+ θ (y2 x, y1) π+ θ (yt x, y1:t1) π+ θ (yT x, y1:T 1), where π+ θ (yt x, y1:t1) = 1 Z(x, y1:t1) πθ(yt x, y1:t1) (cid:18) πθ(yt x, y1:t1) πref(yt x, y1:t1) (cid:19)β with Z(x, y1:t1) being partition function. The substitution of rθ uses the correspondence between the reward model rθ and the policy πθ in Equation (5), under the assumption that this correspondence holds for all truncations y1:t1. It gives us direct per-token prediction rule: π+ θ ( x, y1:t1) = softmax (cid:16)(cid:8)(1 + β) hθ β href (cid:9)(x, y1:t1) (cid:17) . Here hθ and href are the logits of the policies πθ and πref, respectively. β is the regularization coefficient from the objective function J(π) in Equation (6). Responses are then generated using standard decoding techniques, such as greedy decoding or nucleus sampling. Similarly, the generation for π θ follows π θ ( x, y1:t1) = softmax (cid:16)(cid:8)(1 β) hθ + β href (cid:17) (cid:9)(x, y1:t1) . For detailed, step-by-step proof, see Proposition 1 in Liu et al. (2024b). We formalize our final algorithm in Algorithm 1. Vanilla DPO (Rafailov et al., 2023; Guo et al., 2024) employs basic generation approach, sampling ya πθ at Step 3. In contrast, instead of only sampling from πθ, our sampling scheme interpolates and extrapolates the logits hθ and href with coefficient β, enabling exploration of wider response space to align learning from human preference with value optimization. The β here is the same parameter that controls the KL regularization in Equation (4), as set by the problem. , yb Cost analysis We summarize sampling and annotation costs per preference pair for PILAF and related sampling schemes in Table 1. In Vanilla sampling (from πθ), two generations and two annotations are required for human preference labeling, same to PILAF when the pair is sampled from πθ, which happens half the time. With 50% probability, PILAF uses π+ θ and π θ to generate, requiring two forward passes with πθ and πref to generate one sample. Thus, 3When the regularization coefficient β is sufficiently small, the term exp{rθ(x, y)} in equation (6a) stays θ (x) is approximately 1. close to 1 and has only minor effect. Consequently, the partition function + similar reasoning applies to θ (x). 11 Table 1. cost summary of PILAF and sampling methods from related works. Best-of-N method in Xiong et al. (2024) uses the oracle reward to score all candidate responses, then selects the highestand lowest-scoring onesinstead of providing preference label for only two responses. We restrict the oracle to providing only preference labels. Thus, we create Best-of-N variant that uses the DPO internal reward for selection and then applies preference labeling, with an annotation cost of 2. We compare with this variant in the experiment. Method ya yb Sampling Cost Annotation Cost Vanilla (Rafailov et al., 2023) Best-of-N (Xiong et al., 2024), N=8 Best-of-N (with DPO reward), N=8 Hybrid (Xie et al., 2024) PILAF (OURS) πθ πθ best of πθ worst of πθ best of πθ worst of πθ πθ π+ θ /πθ πref π θ /πθ 2 8 8 2 3 2 8* 2 2 2 on average, preference pair sampled with PILAF requires sampling cost of 3 forward passes (1.5 time the cost of Vanilla) with the same annotation cost. To compare, Xiong et al. (2024); Dong et al. (2024) perform Best-of-N sampling with = 8, which generates and annotates all 8 responses, selecting the best and worst of them. Xie et al. (2024) use Hybrid method that generates with πθ and πref, thus matching the sampling and annotation costs of the Vanilla method. We empirically compare PILAF with these methods in the next section. Algorithm 1 DPO with PILAF (ours). input Prompt Dataset Dρ, preference oracle O, πθ, πref. 1: for step = 1, ..., do Sample nt prompts {xi}nt 2: 3: With probability 1/2, sample ya i=1 from Dρ. , yb π yb θ . ) into (xi, yw Update πθt with DPO loss using {(xi, yw 4: Query to label (xi, ya 5: 6: end for , yb , yℓ i). i)}nt , yℓ i=1. πθ; with probability 1/2, sample ya π+ θ and"
        },
        {
            "title": "6 Experiments",
            "content": "In this section, we empirically evaluate PILAF in both an iterative DPO setting (Section 6.1, following Xiong et al. (2024); Dong et al. (2024)) and an online DPO setting (Section 6.2, following Guo et al. (2024)) where the model undergoes multiple rounds of refinement through active data collection. Our findings indicate that, without requiring any hyper-parameter tuning, our sampling scheme stabilizes training, achieves higher reward scores, and maintains lower KL divergence from the reference model. General Setup We align the Llama-3.1-8B base model (Dubey et al., 2024) in terms of helpfulness and harmlessness using the HH-RLHF dataset (Bai et al., 2022), widely-used benchmark dataset for alignment. It consists of 161k prompts in the training set. For response preference labeling, we use well-trained reward model to simulate human preferences by assigning preference to pairs of responses under the BT assumption in Equation (1). 12 Specifically, we employ the Skywork-Reward-8B model (Liu et al., 2024a), top-performing 8B model on RewardBench (Lambert et al., 2024), as our oracle O. During training, interaction with this reward model is limited to providing two responses for comparison. We set β = 0.1 in all the experiments. Supervised Fine-Tuning (SFT) To initialize training, following Rafailov et al. (2023), we first fine-tune the base model to obtain the SFT model as πref, which we fix as the reference model in all the experiments. We use the originally preferred responses from the HH-RLHF dataset as the SFT dataset and perform full-parameter tuning. Evaluation We present our results using the reward-KL curve, following Gao et al. (2023), with the reward evaluated by the oracle reward model O. To monitor the impact of our sampling scheme on the optimization trajectory, we evaluate the model every 50 gradient steps during training. We use the entire testset of HH-RLHF (8.55K samples) to evaluate. Baselines We compare our sampling method against existing methods in Table 1. Since we treat the oracle as proxy for human labelers that can only provide pairwise preferences, all baselines are constrained to query the oracle with exactly two samples at time. We thus adapt Best-of-N variant that deploys the internal DPO reward to select the top and bottom candidates, which are then presented to the oracle for preference labeling, as listed in Table 1. We compare PILAF against the baselines: Vanilla Sampling, Best-of-N Sampling (with DPO reward), and Hybrid Sampling combined with modified DPO loss (Xie et al., 2024). Full experimental details can be found in Appendix E."
        },
        {
            "title": "6.1 Iterative DPO",
            "content": "Implementation We first consider the iterative DPO framework (Xiong et al., 2024; Dong et al., 2024), in which preference data is collected in successive iterations rather than as single fixed dataset. At the start of each iteration, large dataset of responses is sampled using the current model, annotated for preferences, and then used to train the current model. Concretely, we set nt = Dρ in Algorithm 1, meaning that all prompts are used to generate new responses at each iteration. During the first iteration, when πref and πθ are identical, PILAF reduces to Vanilla Sampling. Hence, we choose to focus our comparison on the second iteration. For consistency, we initialize all runs with the same policy model obtained at the end of the first iteration via Vanilla Sampling. Results Figure 2 presents the reward-KL curve for iterative DPO. PILAF significantly outperforms all the other methods: it achieves the end-point rewards of the baselines already around halfway through training, with around 40% less training time. This reduction directly translates to savings in both annotation and computational costs. We summarize the final performance in Table 2. PILAF produces final policy with high reward value and 13 Figure 2. Reward-KL curve for Iterative DPO. All training runs start from the same model obtained at the end of the first iteration via Vanilla Sampling. Each dot represents an evaluation performed every 50 training steps. Table 2. Results of Iterative DPO. We report the average reward, KL divergence from the reference model, and objective on the testset. Higher reward and are better, while lower KL divergence is better. We use boldface to indicate the best result and underline to denote the second-best result. Method Vanilla Best-of-N Hybrid PILAF (Ours) Reward () KL () () -10.16 -10.13 -10.51 -9. 35.20 32.38 22.86 25.01 -13.68 -13.37 -12.80 -12.30 modestly small KL divergence from the reference model, thereby achieving the highest overall objective J."
        },
        {
            "title": "6.2 Online DPO",
            "content": "Implementation We further evaluate our sampling method in the online DPO setting (Guo et al., 2024), where new responses are generated and labeled at every training step, and these preference data are immediately used to update πθ. This setting corresponds to the case where nt (in Algorithm 1) is set to the training batch size, resulting in the most annotation-intensive and most actively on-policy alignment. By collecting and utilizing preference data on the fly for each batch, the policy is continuously refined using on-policy feedback throughout the entire training process. Similar to Iterative DPO, we initialize all training runs with the same πθ and focus on comparing the subsequent optimization. Further 14 details are in Appendix E. Figure 3. Reward-KL curve for Online DPO. Each dot represents an evaluation performed every 50 training steps. Results Figure 3 demonstrates the effectiveness of PILAF in the pure online setting, and we summarize the final performance in Table 3. Compared with Vanilla and Hybrid Sampling, PILAF achieves significantly better Reward-KL trade-off curve, attaining higher reward with lower KL. Although Vanilla eventually achieves roughly the same reward value as PILAF, it comes at the cost of substantially higher KL. When compared with Best-of-N, PILAF traces similar RewardKL trajectory but ends with higher reward and better final objective after the same number of iterations, translating to lower sample complexity and reduced annotation and computational cost. Table 3. Results of Online DPO. We report the average reward, KL divergence from the reference model, and objective on the testset. Method Vanilla Best-of-N Hybrid PILAF (Ours) Reward () KL () () -4.96 -5.54 -6. -4.88 21.50 12.35 16.46 15.42 -7.11 -6.77 -8.96 -6.42 Robustness Analysis Having established the effectiveness of PILAF, we further evaluate its robustness by testing whether it improves optimization and statistical convergence under challenging conditions, as predicted from our statistical theory in Section 4.2. Specifically, we replace the initial model with one that has overfit on fixed off-policy dataset. This setup 15 allows us to examine how different methods handle optimization starting from poor initial point. In Figure 4, we compare the performance of PILAF and Vanilla Sampling when both are initialized from an overfitted policy. We observe that Vanilla Sampling rapidly increases its KL divergence from the reference model while its reward improvement diminishes over time. In contrast, PILAF undergoes an early training phase with fluctuating KL values but ultimately attains policy with higher reward and substantially lower KL divergence. We hypothesize that PILAFs interpolation-based exploration design enables it to escape the suboptimal region of the loss landscape in which Vanilla remains. These results underscore PILAFs effectiveness in more robustly optimizing overfitted (or even adversarially initialized) policies. Figure 4. Online DPO with an overfitted initial policy. Each dot represents an evaluation performed every 50 training steps. Color saturation indicates the training step, with darker colors representing later steps."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduced Policy-Interpolated Learning for Aligned Feedback (PILAF), novel sampling method designed to enhance response sampling for preference labeling. Theoretical analysis highlights PILAFs superiority from both optimization and statistical perspectives, demonstrating its ability to stabilize training, accelerate convergence, and reduce variance. The method is straightforward to implement and requires no additional hyperparameter tuning. We empirically validated its performance in both iterative DPO and online DPO settings, where it consistently outperformed existing approaches. To achieve the same level of performance, PILAF consistently requires lower annotation costs, which can be substantial when annotations require experts in knowledge-intensive domains. 16 In future work, we hope to extend PILAF to other paradigms, such as KTO (Ethayarajh et al., 2024) and IPO (Azar et al., 2024). Due to resource constraints, our evaluations were conducted using 8B models and reward model to simulate human feedback. Future studies involving larger-scale experiments and real human labeling would further generalize our findings. Overall, this work takes an important step toward improving preference data curation in RLHF pipelines, laying the groundwork for more effective methods in alignment."
        },
        {
            "title": "Acknowledgment",
            "content": "YF and JK acknowledge support through NSF NRT training grant award 1922658. YD acknowledges support from NSF grant DMS-2413812. The authors would like to thank Gabriel Synnaeve, Wei Xiong, He He, Pu Yang, Angelica Chen for helpful discussions. This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Cen, S., Mei, J., Goshvadi, K., Dai, H., Yang, T., Yang, S., Schuurmans, D., Chi, Y., and Dai, B. Value-incentivized preference optimization: unified approach to online and offline rlhf. arXiv preprint arXiv:2405.19320, 2024. Das, N., Chakraborty, S., Pacchiano, A., and Chowdhury, S. R. Active preference optimization for sample efficient rlhf. In ICML 2024 Workshop on Theoretical Foundations of Foundation Models, 2024. Degris, T., White, M., and Sutton, R. S. Off-policy actor-critic. arXiv preprint arXiv:1205.4839, 2012. 17 Dong, H., Xiong, W., Goyal, D., Zhang, Y., Chow, W., Pan, R., Diao, S., Zhang, J., KaShun, S., and Zhang, T. Raft: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research, 2023. Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang, T. Rlhf workflow: From reward modeling to online rlhf, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866, 2023. Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(9), 2004. Gu, S. S., Lillicrap, T., Turner, R. E., Ghahramani, Z., Scholkopf, B., and Levine, S. Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning. Advances in neural information processing systems, 30, 2017. Guo, S., Zhang, B., Liu, T., Liu, T., Khalman, M., Llinares, F., Rame, A., Mesnard, T., Zhao, Y., Piot, B., et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024. Hu, J., Wu, X., Zhu, Z., Xianyu, Wang, W., Zhang, D., and Cao, Y. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Ji, K., He, J., and Gu, Q. Reinforcement learning from human feedback with active queries. arXiv preprint arXiv:2402.09401, 2024. Kallus, N. and Uehara, M. Statistically efficient off-policy policy gradients. In International Conference on Machine Learning, pp. 50895100. PMLR, 2020. Kosorok, M. R. Introduction to empirical processes and semiparametric inference, volume 61. Springer, 2008. Lambert, N., Pyatkin, V., Morrison, J., Miranda, L., Lin, B. Y., Chandu, K., Dziri, N., Kumar, S., Zick, T., Choi, Y., Smith, N. A., and Hajishirzi, H. Rewardbench: Evaluating reward models for language modeling. https://huggingface.co/spaces/allenai/ reward-bench, 2024. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step, 2023. URL https://arxiv.org/ abs/2305.20050. Limo, F. A. F., Tiza, D. R. H., Roque, M. M., Herrera, E. E., Murillo, J. P. M., Huallpa, J. J., Flores, V. A. A., Castillo, A. G. R., Pena, P. F. P., Carranza, C. P. M., et al. Personalized tutoring: Chatgpt as virtual tutor for personalized learning experiences. Przestrzen Spo(cid:32)leczna (Social Space), 23(1):293312, 2023. Liu, C. Y., Zeng, L., Liu, J., Yan, R., He, J., Wang, C., Yan, S., Liu, Y., and Zhou, Y. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024a. Liu, T., Guo, S., Bianco, L., Calandriello, D., Berthet, Q., Llinares, F., Hoffmann, J., Dixon, L., Valko, M., and Blondel, M. Decoding-time realignment of language models. arXiv preprint arXiv:2402.02992, 2024b. Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P. J., and Liu, J. Statistical rejection sampling improves preference optimization. In The Twelfth International Conference on Learning Representations, 2024c. Liu, Z., Chen, C., Du, C., Lee, W. S., and Lin, M. Sample-efficient alignment for llms. arXiv preprint arXiv:2411.01493, 2024d. Mehta, V., Das, V., Neopane, O., Dai, Y., Bogunovic, I., Schneider, J., and Neiswanger, W. Sample efficient reinforcement learning from human feedback via active exploration. arXiv preprint arXiv:2312.00267, 2023. Muldrew, W., Hayes, P., Zhang, M., and Barber, D. Active preference learning for large language models. In Forty-first International Conference on Machine Learning, 2024. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2023. Scheid, A., Boursier, E., Durmus, A., Jordan, M. I., Menard, P., Moulines, E., and Valko, M. Optimal design for reward modeling in rlhf. arXiv preprint arXiv:2410.17055, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shi, R., Zhou, R., and Du, S. S. The crucial role of samplers in online direct preference optimization. arXiv preprint arXiv:2409.19605, 2024. Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. Deterministic policy gradient algorithms. In International conference on machine learning, pp. 387395. Pmlr, 2014. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. Xie, T., Foster, D. J., Krishnamurthy, A., Rosset, C., Awadallah, A., and Rakhlin, A. Exploratory preference optimization: Harnessing implicit q*-approximation for sampleefficient rlhf. arXiv preprint arXiv:2405.21046, 2024. Xiong, W., Dong, H., Ye, C., Wang, Z., Zhong, H., Ji, H., Jiang, N., and Zhang, T. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. In Forty-first International Conference on Machine Learning, 2024. Xu, H., Sharaf, A., Chen, Y., Tan, W., Shen, L., Van Durme, B., Murray, K., and Kim, Y. J. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. In Forty-first International Conference on Machine Learning, 2024. Xu, J., Lee, A., Sukhbaatar, S., and Weston, J. Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023. Yue, Z., Zhuang, H., Bai, A., Hui, K., Jagerman, R., Zeng, H., Qin, Z., Wang, D., Wang, X., and Bendersky, M. Inference scaling for long-context retrieval augmented generation. arXiv preprint arXiv:2410.04343, 2024. Zhang, S., Yu, D., Sharma, H., Zhong, H., Liu, Z., Yang, Z., Wang, S., Hassan, H., and Wang, Z. Self-exploring language models: Active preference elicitation for online alignment. arXiv preprint arXiv:2405.19332, 2024."
        },
        {
            "title": "Contents",
            "content": "A. Additional Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B. Proof of Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.1. Optimization Considerations: Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.1.1. Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.1.2. Derivation of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.1.3. Proof of Claim (15) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2. Statistical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 B.2.1. Proof of Lemma 4.4 (Theorem B.4) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.2.2. Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 B.2.3. Proof of Theorem 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 C. Proof of Auxiliary Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 C.1. Proof of Auxiliary Results for Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1.1. Gradients of Policy πθ and Reward rθ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 C.1.2. Proof of Lemma B.2, Explicit Form of Gradient θ J(πθ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .33 C.1.3. Proof of Lemma B.3, Explicit Form of Gradient θ L(θ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .35 C.2. Proof of Auxiliary Results for Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 C.2.1. Proof of Condition (24) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2.2. Proof of Lemma B.5, Explicit Form of Hessian 2 θ L(θ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 C.2.3. Proof of Lemma B.6, Asymptotic Distribution of Graident θ (cid:98)L(θ) . . . . . . . . . . . . . . . . . . . . . 40 C.3. Proof of Auxiliary Results for Theorem 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 C.3.1. Proof of Equation (9) from Theorem 4.3, Explicit Form of Hessian θ J(π) . . . . . . . . . . . . . . 41 C.3.2. Proof of the Asymptotic Distribution in Equation (31) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 C.3.3. Proof of the Tail Bound in Equation (10) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 D. Supporting Theorem: Master Theorem for Z-Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 E. Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 F. Extension to Proximal Policy Optimization (PPO) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Additional Literature Review",
            "content": "RLHF. RLHF has emerged as cornerstone methodology for aligning large language models with human values and preferences (Achiam et al., 2023). Early systems (Ouyang et al., 2022) turn human preference data into reward modeling to optimize model behavior accordingly. DPO has been proposed as more efficient approach that directly trains LLMs on preference data. As LLMs evolve during training, continuing training on pre-generated preference data becomes suboptimal due to the distribution shift. Empirically, RLHF is applied iterativelygenerating on-policy data at successive stages to enhance alignment and performance (Touvron et al., 2023; Bai et al., 2022). Similarly, researchers have introduced iterative DPO (Xiong et al., 2024; Xu et al., 2023) and online DPO (Guo et al., 2024) to fully leverage online preference labeling. Ultimately, the quality of preference data play critical role in determining the effectiveness of the alignment. Sampling in Frontier LLMs. Technical reports of Frontier LLMs briefly mention sampling techniques. For instance, Claude (Bai et al., 2022) utilizes models from different training steps to generate responses, while Llama-2 (Touvron et al., 2023) further use different temperatures for sampling. However, no further details are provided, leaving the development of principled method an open challenge. Data Selection. There is line of research aimed at improving sample efficiency for preference labeling by selecting question and response pairs. Scheid et al. (2024) conceptualize this as regret minimization problem, leveraging methods from linear dueling bandits. Das et al. (2024); Mehta et al. (2023); Muldrew et al. (2024); Ji et al. (2024) draw insights from active learning, using various uncertainty estimators to guide selection by prioritizing sample pairs with maximum uncertainty. These approaches focus directly on dataset of questions and responses and are orthogonal to our work. Other Changes in Response Sampling. Several works also modify the sampling design directly (Liu et al., 2024c; Dong et al., 2023), but with the goal of improving policy network optimization based on reward model, rather than enhancing the reward modeling itself. Liu et al. (2024c) employ rejection sampling to approximate the response distribution induced by the reward model, thereby improving optimization. However, this approach requires access to the reward model and incurs higher computational and labeling costs. Similarly, Dong et al. (2023) use best-of-N sampling with the reward model to generate high-quality data for supervised fine-tuning (SFT). We consider these approaches orthogonal to our work. Additionally, Cen et al. (2024) introduce bonus term in the policy learning phase of online RLHF to promote exploration in response sampling, which aligns with the optimism principle."
        },
        {
            "title": "B Proof of Main Results",
            "content": "This section provides the proofs of the main results from Section 4, covering both optimization In Appendix B.1, we prove Theorem 4.1, which establishes the and statistical aspects. 22 gradient alignment property. For the statistical results, Appendix B.2 begins with the proofs of Lemma 4.4 and Theorem 4.2, which derive the asymptotic distribution of the estimated parameter (cid:98)θ, and concludes with the proof of Theorem 4.3, analyzing the asymptotic behavior of the value gap J(π) J((cid:98)π). B.1 Optimization Considerations: Proof of Theorem 4.1 We begin by presenting rigorous restatement of Theorem 4.1, formally detailed in Theorem B.1 below. Theorem B.1 (Gradient structure in DPO training). Consider the expected loss function L(θ) during the DPO training phase. Using data collected from our poposed response sampling scheme µ, the gradient of L(θ) satisfies θ L(θ) = β θ θ J(πθ) + T2 , where the constant θ is defined in equation (7), and the term T2 represents second-order error. To control term T2, assume the following uniform bounds: (i) R. (ii) For any policy πθ Π, the induced reward rθ satisfies rθ and supx,y θ rθ(x, y)2 . Under these conditions, T2 is bounded as T22 Exρ, ya,ybπθ(x) (cid:20) (cid:110)(cid:0)r(x, ya) r(x, yb)(cid:1) (cid:0)rθ(x, ya) rθ(x, yb)(cid:1)(cid:111)2(cid:21) , where the constant is given by = 0.1 (1 + e2R) G(cid:14)Z θ. The proof of Theorem B.1 is structured into three sections. In Appendix B.1.1, we lay the foundation by presenting the key components, including the explicit expressions for the gradients θ J(πθ) and θ L(θ), as well as for the sampling density µ. Then Appendix B.1.2 establishes the connection between θ J(πθ) and θ L(θ) by leveraging these results, completing the proof of Theorem 4.1. Finally, in Appendix B.1.3, we provide detailed derivation of the form of density function µ. B.1.1 Building Blocks To establish Theorem 4.1, which uncovers the relationship between the gradients of the expected value J(πθ) and the negative log-likelihood function L(θ), the first step is to 23 derive explicit expressions for the gradients of both functions. The results are presented in Lemmas B.2 and B.3, with detailed proofs provided in Appendices C.1.2 and C.1.3, respectively. Lemma B.2 (Gradient of value J(πθ)). For any πθ in the parameterized policy class Π, the gradient of the expected value J(πθ) satisfies θ J(πθ) = 1 2β Exρ; ya,ybπθ(x) (cid:20)(cid:110)(cid:0)r(x, ya) r(x, yb)(cid:1) (cid:0)rθ(x, ya) rθ(x, yb)(cid:1)(cid:111) (cid:8)θ rθ(x, ya) θ rθ(x, yb)(cid:9) (cid:21) . (13) Lemma B.3 (Gradient of the loss function L(θ)). For any πθ in the parameterized policy class Π and any sampling distribution µ of the responses, the gradient of the negative log-likelihood function L(θ) is given by θ L(θ) = Exρ; (ya, yb)µ(x) (cid:20) w(x) σ(cid:0)r(x, ya)r(x, yb)(cid:1)σ(cid:0)rθ(x, ya)rθ(x, yb)(cid:1)(cid:111) (cid:110) (cid:8)θ rθ(x, ya) θ rθ(x, yb)(cid:9) (cid:21) , (14a) where the average density µ is defined as µ(ya, yb x) : = 1 2 (cid:8)µ(ya, yb x) + µ(yb, ya x)(cid:9) (14b) as previously introduced in Equation (12a). In Lemma B.3, we observe that the gradient θ L(θ) is expressed as an expectation over the probability distribution µ. By applying the sampling scheme outlined in Section 3, we can derive more detailed representation of θ L(θ). This refined form will reveal its close relationship to the gradient θ J(πθ) given in expression (13). Before moving forward, it is crucial for us to first derive the explicit form of µ. Specifically, we claim that the distribution µ satisfies the following property µ(ya, yb x) πθ(ya x) πθ(yb x) = 2 {1 + + 1 θ (x) θ (x)} 1 σ(cid:0)rθ(x, ya) rθ(x, yb)(cid:1) , (15) where σ denotes the derivative of the sigmoid function σ, given by σ(z) = 1 (1 + exp(z))(1 + exp(z)) = σ(z) σ(z) for any . (16) With these key components in place, we are now prepared to prove Theorem 4.1. B.1.2 Derivation of Theorem 4.1 With the tools provided by Lemmas B.2 and B.3 and the sampling density expression in (15), we are now ready to prove Theorem 4.1. We begin by applying Lemma B.3 and reformulating equation (14a) as θ L(θ) = Exρ; ya, ybπθ(x) (cid:20) w(x) µ(ya, yb x) πθ(ya x) πθ(yb x) (cid:110) σ(cid:0)r(x, ya) r(x, yb)(cid:1) σ(cid:0)rθ(x, ya) rθ(x, yb)(cid:1)(cid:111) (cid:8)θ rθ(x, ya) θ rθ(x, yb)(cid:9) (cid:21) . (17) Substituting the density ratio from equation (15) into expression (17) and incorporating the weight function w(x) defined in equation (7), we obtain θ L(θ) = 1 2 θ Exρ; ya, ybπθ(x) (cid:34) σ(cid:0)r(x, ya) r(x, yb)(cid:1) σ(cid:0)rθ(x, ya) rθ(x, yb)(cid:1) σ(cid:0)rθ(x, ya) rθ(x, yb)(cid:1) (cid:8)θ rθ(x, ya) θ rθ(x, yb)(cid:9) (cid:35) . (18) Using the intuition that the first-order Taylor expansion σ(z) σ(z) σ(z) = (z z) + O(cid:0)(z z)2(cid:1) is valid when z, with : = r(x, ya) r(x, yb) and : = rθ(x, ya) rθ(x, yb), we find that σ(cid:0)r(x, ya) r(x, yb)(cid:1) σ(cid:0)rθ(x, ya) rθ(x, yb)(cid:1) σ(cid:0)rθ(x, ya) rθ(x, yb)(cid:1) (cid:110)(cid:0)r(x, ya) r(x, yb)(cid:1) (cid:0)rθ(x, ya) rθ(x, yb)(cid:1)(cid:111) = + second-order term. Reformulating equation (18) in this context, we rewrite it as θ L(ϕ) = (cid:34) 1 2Z θ xρ; ya,ybπθ(x) (cid:110)(cid:0)r(x, ya) r(x, yb)(cid:1) (cid:0)rθ(x, ya) rθ(x, yb)(cid:1)(cid:111) (cid:8)θ rθ(x, ya) θ rθ(x, yb)(cid:9) (cid:35) + T2 , (19) where T2 represents the second-order residual term related to the estimation error rθ r. By applying Lemma B.2, we observe that the primary term in equation (19) aligns with the direction of θ J(πθ), resulting in θ L(ϕ) = β θ θ J(πθ) + T2 . (20) 25 Next, we proceed to control the second-order term T2. The conditions r, rθ and sup(x,y)X Yθ rθ(x, y)2 G, lead to the bound (cid:12) (cid:12) (cid:12) σ(z) σ(z) σ(z) (z z) (cid:12) (cid:12) 0.1 (1 + e2R) (z z)2 , (cid:12) which in turn implies T22 0.1 (1 + e2R) θ Exρ; ya,ybπθ(x) (cid:20) (cid:110)(cid:0)r(x, ya) r(x, yb)(cid:1) (cid:0)rθ(x, ya) rθ(x, yb)(cid:1)(cid:111)2(cid:21) . (21) Finally, combining equation (21) with equation (20), we conclude the proof of Theorem 4.1. B.1.3 Proof of Claim (15) The remaining step in the proof of Theorem 4.1 is to verify the expression for the density ratio in equation (15). Based on the sampling scheme described in Section 3, we find that the sampling distribution for the response satisfies µ(cid:0)ya, yb (cid:12) (cid:12) x(cid:1) = {1 p0(x)} πθ(ya x) πθ(yb x) + p0(x) π+ θ (ya x) π θ (yb x) , (22) where the probability p0(x) is defined as p0(x) = + θ (x) θ (x)/{1 + + θ (x) θ (x)} and the policies π+ us to simplify equation (22) to θ and π θ are specified in equations (6a) and (6b), respectively. This allows µ(cid:0)ya, yb (cid:12) (cid:12) x(cid:1) = πθ(ya x) πθ(yb x) θ (x) 1 + + θ (x) 1 + exp (cid:8)rθ(x, ya) rθ(x, yb)(cid:9)(cid:111) (cid:110) . Similarly, we derive an expression for µ(yb, ya x). By averaging the two expressions, for µ(ya, yb x) and µ(yb, ya x), we obtain µ(ya, yb x) πθ(ya x) πθ(yb x) = πθ(ya x) πθ(yb x) 2 {1 + + θ (x)} θ (x) (cid:110) 2 + exp (cid:8)rθ(x, ya) rθ(x, yb)(cid:9) + exp (cid:8)rθ(x, yb) rθ(x, ya)(cid:9)(cid:111) . 26 Rewriting this expression using the formula for σ in equation (16), we arrive at (cid:8)1 + + θ (x)(cid:9) µ(ya, yb x) θ (x) πθ(ya x) πθ(yb x) 1 + exp (cid:8)rθ(x, yb) rθ(x, ya)(cid:9)(cid:111)(cid:110) (cid:110) 1 2 1 2 σ(cid:0)rθ(x, ya) rθ(x, yb)(cid:1) . = = 1 + exp (cid:8)rθ(x, ya) rθ(x, yb)(cid:9)(cid:111) Finally, rearranging terms, we recover equation (15), completing this part of the proof. B.2 Statistical Considerations In this section, we present the proofs for Theorems 4.2 and 4.3 and Lemma 4.4 from Section 4.2. We start with the proof of Lemma 4.4 in Appendix B.2.1, with rigorous restatement provided in Theorem B.4 below. Theorem B.4. Assume the reward model in the BT model (1) satisfies = rθ for some parameter θ. Assume that (cid:98)θ minimizes the loss function (cid:98)L(θ) in the sense that θ as the sample size . Additionally, suppose the θ rθ(x, y) are uniformly reward function rθ(x, y), its gradient θ rθ(x, y) and its Hessian 2 bounded and Lipchitz continuous with respect to θ, for all (x, y) Y. 0 and that (cid:98)θ θ (cid:98)L((cid:98)θ) Under these conditions, the estimate (cid:98)θ asymptotically follows Gaussian distribution ((cid:98)θ θ) (0, Ω) as . We have an estimate of the covariance matrix Ω: Ω Σ1 . For general sampling scheme µ chosen, the matrix Σ is given by Σ : = Exρ, (ya, yb)µ(x) (cid:104) w(x) Var(cid:0)1{ya = yw} (cid:12) (cid:12) x, ya, yb(cid:1) g(cid:105) , where the expectation is taken over the distribution µ(ya, yb x) : = 1 2 (cid:8)µ(ya, yb x) + µ(yb, ya x)(cid:9) . The variance term is specified as Var(cid:0)1{ya = yw} x, ya, yb(cid:1) = σ(cid:0)r(x, ya) r(x, yb)(cid:1) σ(cid:0)r(x, yb) r(x, ya)(cid:1) and the gradient difference is defined as : = θ r(x, ya) θ r(x, yb) . 27 Theorem B.4 establishes the asymptotic distribution of the estimated parameter (cid:98)θ, which serves as the foundation for the subsequent results. Next, we show that Theorem 4.2 directly follows as corollary of Theorem B.4, with the detailed derivation provided in Appendix B.2.2. Finally, in Appendix B.2.3, we prove Theorem 4.3, which describes the asymptotic behavior of the value gap J(π) J((cid:98)π). B.2.1 Proof of Lemma 4.4 (Theorem B.4) In this section, we analyze the asymptotic distribution of the estimated parameter (cid:98)θ for general sampling distribution µ. The parameter (cid:98)θ is obtained by solving the optimization problem minimizeθ (cid:98)L(θ) : = 1 (cid:88) i=1 w(xi) log σ (cid:16) (cid:0)xi, yw (cid:1) rθ (cid:0)xi, yℓ rθ (cid:1)(cid:17) . 2 (cid:1). (cid:0)n 1 We assume the optimization is performed to sufficient accuracy such that θ (cid:98)L((cid:98)θ) = op Under this condition, (cid:98)θ qualifies as Z-estimator. To study its asymptotic behavior, we use the master theorem for Z-estimators (Kosorok, 2008), the formal statement of which is provided in Theorem D.1 in Appendix D. To apply the master theorem, we set Ψ : = θ and Ψn : = θ (cid:98)L and verify the conditions. In particular, the smoothness condition (61) in Theorem D.1 translates to the following equation in our context: (cid:8)θ (cid:98)L((cid:98)θ) θ L((cid:98)θ)(cid:9) (cid:8)θ (cid:98)L(θ) θ L(θ)(cid:9) = op (cid:0)1 + (cid:98)θ θ2 (cid:1) . (24) This condition follows from the second-order smoothness of the reward function rθ with respect to θ. rigorous proof is provided in Appendix C.2.1. We now provide the explicit form of the derivative Ψθ = 2 following lemma. The proof of this result can be found in Appendix C.2.2. θ L(θ), as captured in the Lemma B.5. The Hessian matrix of the population loss L(θ) at θ = θ is 2 θ L(θ) = Σ , (25) where the matrix Σ is defined in equation (11). Next, we analyze the asymptotic behavior of the gradient θ (cid:98)L(θ). The proof is deferred to Appendix C.2.3. Lemma B.6. The gradient of the empirical loss (cid:98)L(θ) at θ = θ satisfies (cid:0)θ (cid:98)L(θ) θ L(θ)(cid:1) where the covariance matrix (cid:101)Ω Rdd is bounded as follows: (0, (cid:101)Ω) as , (26a) (cid:101)Ω Σ , (26b) with Σ defined in equation (11). 28 Combining these results, and assuming Σ is nonsingular, the master theorem (Theorem D.1) yields the asymptotic distribution of (cid:98)θ: (cid:0) (cid:98)θ θ(cid:1) (cid:0)0, Σ1 (cid:101)ΩΣ1 (cid:1) . Furthermore, from the bound (26b), the covariance matrix Ω; : = Σ1 (cid:101)ΩΣ satisfies Ω = Σ1 (cid:101)ΩΣ1 Σ1 . Therefore, we have established the asymptotic distribution of (cid:98)θ, completing the proof of Lemma 4.4. B.2.2 Proof of Theorem 4.2 Theorem 4.2 is direct corollary of Lemma 4.4, using our specific choice of sampling distribution µ. To establish this, we demonstrate how the general covariance matrix Σ in equation (11) simplifies to the form in equation (8) under our proposed sampling scheme. To establish the result in this section, we impose the following regularity condition: There exists constant 1 satisfying Varrθ (cid:0)1{ya = yw} (cid:12) (cid:12) x, ya, yb(cid:1) Varr (cid:0)1{ya = yw} (cid:12) (cid:12) x, ya, yb(cid:1) (27) (cid:12) x, ya, yb(cid:1) denotes for any prompt and responses ya, yb Y. Here Varrθ the conditional variance under the BT model (1), when the implicit reward function (cid:12) x, ya, yb(cid:1) is replaced by rθ. The term Varr represents the conditional variance under the ground-truth BT model, where the reward function is given by r. (cid:12) x, ya, yb(cid:1) Var(cid:0)1{ya = yw} (cid:12) (cid:0)1{ya = yw} (cid:12) (cid:0)1{ya = yw} (cid:12) We begin by leveraging the property of the sampling distribution µ from equation (15) and the derivative σ of the sigmoid function σ, given in equation (16). Specifically, we find that µ(ya, yb x) πθ(ya x) πθ(yb x) = = 2 {1 + + 2 {1 + + 1 θ (x) 1 θ (x) θ (x)} θ (x)} 1 σ(cid:0)rθ(x, ya) rθ(x, yb)(cid:1) σ(cid:0)rθ(x, yb) rθ(x, ya)(cid:1) (cid:12) x, ya, yb(cid:1) . (cid:0)1{ya = yw} (cid:12) Varrθ We then apply condition (27) and derive µ(ya, yb x) πθ(ya x) πθ(yb x) 2 {1 + + 1 θ (x) θ (x)} Varr 29 1 (cid:0)1{ya = yw} (cid:12) (cid:12) x, ya, yb(cid:1) . (28) (29) (cid:105) Next, substituting this result (28) into equation (11), alongside the weight function w(x) from equation (7), we reform Σ as Σ = Exρ; ya, ybπθ(x) (cid:20) 1 2 θ Exρ; ya, ybπθ(x) µ(ya, yb x) πθ(ya x) πθ(yb x) (cid:2) g(cid:3) . w(x) Var(cid:0)1{ya = yw} (cid:12) (cid:12) x, ya, yb(cid:1) (cid:21) (cid:2) gg (cid:12) The conditional expectation of gg simplifies as (cid:12) x(cid:3) Eya, ybπθ(x) (cid:104)(cid:8)θ r(x, ya) θ r(x, yb)(cid:9)(cid:8)θ r(x, ya) θ r(x, yb)(cid:9) (cid:12) = Eya, ybπθ(x) (cid:12) (cid:12) θ r(x, y) θ r(x, y) (cid:12) (cid:104) (cid:12) (cid:12) (cid:2) θ r(x, y) (cid:12) (cid:12) x(cid:3) Eyπθ(x) (cid:12) x(cid:3) . (cid:2) θ r(x, y) (cid:12) = 2 Eyπθ(x) (cid:2)θ r(x, y) (cid:12) 2 Eyπθ(x) = 2 Covyπθ(x) (cid:12) x(cid:3) (cid:105) Substituting this result into equation (29), we arrive at the conclusion that Σ 1 ϕ Exρ (cid:104) Covyπ(x) (cid:2)θ r(x, y) (cid:12) (cid:12) x(cid:3)(cid:105) , which matches the simplified form in equation (8) as stated in Theorem 4.2. B.2.3 Proof of Theorem 4.3 Gradient θ J(π) and Hessian 2 from the gradient expression (38) for θ J(πθ), evaluated at θ = θ with rθ = r. θ J(π): The equality θ J(π) = 0 follows directly The proof of the Hessian result, 2 differentiation of equation (38). For brevity, we defer this proof to Appendix C.3.1. θ J(π) = (1/β)Σ, involves straightforward but technical Asymptotic Distribution of Value Gap J(π) J((cid:98)π): To understand the behavior of the value gap J(π) J((cid:98)π), we start by applying Taylor expansion of J(πθ) around θ. This gives J(π) J((cid:98)π) = θ J(π)(θ (cid:98)θ) 1 2 (θ (cid:98)θ)2 θ J(π)(θ (cid:98)θ) + o(cid:0)θ (cid:98)θ2 (cid:1) . By substituting θ J(π) = 0 (a direct result of the optimality of π), the linear term vanishes. Introducing the shorthand : = 2 θ J(π) = (1/β) Σ, the expression simplifies to J(π) J((cid:98)π) = 1 2 ((cid:98)θ θ)H ((cid:98)θ θ) + o(cid:0)(cid:98)θ θ 2 (cid:1) . (30) When the sample size is sufficiently large, (cid:98)θ approaches θ, making the higher-order term negligible. Therefore, the value gap is dominated by the quadratic form. 30 From Theorem 4.2, we know the parameter estimate (cid:98)θ satisfies ((cid:98)θ θ) (0, Ω). Substituting this result into the quadratic approximation of the value gap, we find that the scaled value gap has the asymptotic distribution {J(π) J((cid:98)π)} 1 2 zΩ 1 2 HΩ 1 2 = : where (0, I). (31) This approximation provides clear intuition: the value gap is asymptotically driven by weighted chi-squared-like term involving the covariance structure Ω and the Hessian-like matrix H. To rigorously establish this result, we will apply Slutskys theorem. The full proof is presented in Appendix C.3.2. Bounding the Chi-Square Distribution: To bound the random variable X, we first leverage the estimate of the covariance matrix Ω provided by Theorem 4.2: Ω θ Σ1 , where the constant comes from condition (27). appearing in equation (31) can be bounded as It follows that the matrix Ω 1 2 HΩ 1 2 Ω 1 2 HΩ 1 2 Σ 1 HΣ 2 1 = 2 θ β = 1 + + θ β θ . Here the last equality uses the definition of the weight function from equation (7). Substituting this bound into the quadratic form, we derive = 1 2 zΩ 1 2 HΩ 1 2 1 + + θ 2β θ zz , where (0, I). Since zz follows chi-square distribution with degrees of freedom, is stochastically dominated by rescaled chi-square random variable θ 1 + + 2β θ χ2 d. Equivalently, we can express this dominance as (cid:26) lim sup {J(π) J((cid:98)π)} > θ 1 + + 2β θ (cid:27) P(cid:8)χ2 > t(cid:9) for any > 0. (32) This inequality, given in equation (32), corresponds to the first bound in equation (10). The second inequality in equation (10) provides precise tail bound for χ2 involves more technical details, we defer it to Appendix C.3.3. d. As its proof"
        },
        {
            "title": "C Proof of Auxiliary Results",
            "content": "This section provides proofs of auxiliary results supporting the main theorems and lemmas. In Appendix C.1, we present the auxiliary results required for Theorem 4.1. Appendix C.2 details the proofs of supporting results for Theorem 4.2. Finally, in Appendix C.3, we establish the auxiliary results necessary for Theorem 4.3. C.1 Proof of Auxiliary Results for Theorem 4.1 In this section, we provide the proofs of several auxiliary results that support the proof of Theorem 4.1. Specifically, Appendix C.1.1 presents the forms of the gradients of the policy πθ and the reward rθ, which serve as fundamental building blocks for deriving the lemmas. Appendix C.1.2 analyzes the gradient of the return function J(πθ), as defined in equation (6). Appendix C.1.3 focuses on deriving expressions for the gradient of the negative log-likelihood function L(θ). C.1.1 Gradients of Policy πθ and Reward rθ In this part, we introduce results for the gradients of policy πθ and reward rθ with respsect to parameter θ, which lay the foundation of our calculations. Lemma C.1 (Gradients of policy πθ and reward function rθ). The gradients of the policy πθ and the reward function rθ can be expressed in terms of each other as follows θ πθ(dy x) = πθ(dy x) (cid:110) θ rθ(x, y) Eyπθ(x) (cid:2)θ rθ(x, y)(cid:3)(cid:111) , 1 β θ rθ(x, y) = β θ πθ(y x) πθ(y x) . (33a) (33b) We now proceed to prove Lemma C.1. To begin, recall our definition of the reward function rθ as given in equation (5). It directly follows that θ rθ(x, y) = β θ πθ(y x) πθ(y x) . This result confirms equation (33b) as stated in Lemma C.1. Next, we express the policy πθ(dy x) in terms of the reward function rθ(x, y). By reformulating equation (5), we obtain πθ(dy x) ="
        },
        {
            "title": "1\nZθ(x)",
            "content": "πref(dy x) exp (cid:111) rθ(x, y) , (cid:110) 1 β (34a) 32 where Zθ(x) is the partition function defined as Zθ(x) = (cid:90) πref(dy x) exp (cid:111) rθ(x, y) . (cid:110) 1 β (34b) We then compute the gradient of πθ(dy x) with respect to θ. Applying the chain rule, we get θ πθ(dy x) = πref(dy x) exp rθ(x, y) (cid:111) 1 β θ rθ(x, y) 1 Zθ(x) 1 2 θ (x) (cid:110) 1 β (cid:110) 1 β πref(dy x) exp (cid:111) rθ(x, y) θ Zθ(x) . (35) We need the gradient of the partition function Zθ(x): θ Zθ(x) = (cid:90) πref(dy x) exp (cid:110) 1 β (cid:111) rθ(x, y) 1 β θ rθ(x, y) = Zθ(x) = Zθ(x) (cid:90) πθ(dy x) 1 β Eyπθ(x) 1 β θ rθ(x, y) (cid:2)θ rθ(x, y)(cid:3) . (36) Substituting equation (36) back into equation (35), we simplify the expression for the gradient of πθ(dy x): θ πθ(dy x) 1 Zθ(x) = πref(dy x) exp (cid:110) 1 β (cid:111) rθ(x, y) 1 β (cid:110) θ rθ(x, y) Eyπθ(x) (cid:2)θ rθ(x, y)(cid:3)(cid:111) . This matches equation (33a) from Lemma C.1, thereby completing the proof. C.1.2 Proof of Lemma B.2 Equality (13) in Lemma B.2 can be derived as consequence of more detailed result. We state it in Lemma C.2. Lemma C.2. For policy πθ, the gradients with respect to the parameter θ of its expected (cid:2)r(x, y)(cid:3) and its KL divergence from reference policy DKL(πθ πref) return Exρ, yπθ(x) are given by (cid:2)r(x, y)(cid:3) (cid:20) = θ Exρ, yπθ(x) 1 β θ DKL(πθ πref) 1 β2 = Exρ, yπθ(x) Exρ, yπθ(x) r(x, y) (cid:110) θ rθ(x, y) Eyπθ(x) (cid:2)θ rθ(x, y)(cid:3)(cid:111)(cid:21) , (cid:20) rθ(x, y) (cid:110) θ rθ(x, y) Eyπθ(x) (cid:2)θ rθ(x, y)(cid:3)(cid:111)(cid:21) . 33 (37a) (37b) Recall that the scalar value J(πθ) of the policy is defined as J(πθ) = Exρ, yπθ(x) (cid:2)r(x, y)(cid:3) β DKL(πθ πref) . Using Lemma C.2, we derive the gradient of J(πθ) as θ J(πθ) = θ Exρ, yπθ(x) (cid:2)r(x, y)(cid:3) β θ DKL(πθ πref) = 1 β Exρ, yπθ(x) (cid:20) (cid:8)r(x, y) rθ(x, y)(cid:9)(cid:110) θ rθ(x, y) Eyπθ(x) (cid:2)θ rθ(x, y)(cid:3)(cid:111)(cid:21) . (38) We rewrite the expression in equation (38) in two equivalent forms by exchanging the roles of ya and yb: θ J(πθ) 1 β = Exρ, yaπθ(x) (cid:20) (cid:8)r(x, ya) rθ(x, ya)(cid:9)(cid:110) θ rθ(x, ya) Eybπθ(x) (cid:2)θ rθ(x, yb)(cid:3)(cid:111)(cid:21) , (39a) θ J(πθ) 1 β = Exρ, ybπθ(x) (cid:20) (cid:8)r(x, yb) rθ(x, yb)(cid:9)(cid:110) θ rθ(x, yb) Eyaπθ(x) (cid:2)θ rθ(x, ya)(cid:3)(cid:111)(cid:21) . (39b) By taking the average of the two equivalent formulations above, we obtain equality (13) and complete the proof of Lemma B.2. We now proceed to prove Lemma C.2, tackling equalities (37a) and (37b) one by one. Proof of Equality (37a) from Lemma C.2: We begin by expressing the expected return as Exρ, yπθ(x) (cid:2)r(x, y)(cid:3) = Exρ (cid:20) (cid:90) (cid:21) r(x, y) πθ(dy x) . Taking the gradient of both sides with respect to θ, we have θ Exρ, yπθ(x) (cid:2)r(x, y)(cid:3) = Exρ (cid:20) (cid:90) (cid:21) r(x, y) θ πθ(dy x) . (40) Using the expression for the policy gradient θ πθ provided in Lemma C.1, the right-hand side of (40) simplifies to RHS of (40) = Exρ (cid:20) (cid:90) r(x, y) πθ(dy x) θ rθ(x, y) Eyπθ(x) (cid:2)θ rθ(x, y)(cid:3)(cid:111)(cid:21) (cid:110) 1 β = 1 β Exρ, yπθ(x) (cid:20) r(x, y) (cid:110) θ rθ(x, y) Eyπθ(x) (cid:2)θ rθ(x, y)(cid:3)(cid:111)(cid:21) . This completes the verification of equation (37a) from Lemma B.2. Proof of Equality (37b) from Lemma C.2: Recall the definition of the KL divergence DKL(πθ πref) = Exρ (cid:20) (cid:90) πθ(dy x) log (cid:18) πθ(y x) πref(y x) (cid:19)(cid:21) . Applying the chain rule, we obtain θ DKL(πθ πref) = Exρ (cid:20) (cid:90) θ πθ(dy x) log (cid:19)(cid:21) (cid:18) πθ(y x) πref(y x) + Exρ (cid:20) (cid:90) (cid:21) θ πθ(dy x) . Since the policy integrates to 1, i.e., (cid:82) πθ(dy x) = 1, it always holds that (cid:90) θ πθ(dy x) = θ (cid:90) πθ(dy x) = 0 , (41) (42) i.e., the second term on the right-hand side of (41) is zero. Using the expression (34a), we take the logarithm log (cid:19) (cid:18) πθ(y x) πref(y x) = 1 β rθ(x, y) log Zθ(x) . (43) Combining equations (42) and (43), we get θ πθ(dy x) log (cid:19) (cid:18) πθ(y x) πref(y x) rθ(x, y) θ πθ(dy x) log Zθ(x) (cid:90) θ πθ(dy x) (cid:90) = = 1 β 1 β (cid:90) (cid:90) rθ(x, y) θ πθ(dy x) . (44) Now, similar to the proof of equation (37a), we derive RHS of (41) = = 1 β 1 β (cid:20) (cid:90) (cid:21) rθ(x, y) θ πθ(dy x) Exρ Exρ, yπθ(x) (cid:20) rθ(x, y) (cid:110) θ rθ(x, y) Eyπθ(x) (cid:2)θ rθ(x, y)(cid:3)(cid:111)(cid:21) , which verifies equality (37b) from Lemma C.2. C.1.3 Proof of Lemma B. In this section, we prove full version of Lemma B.3 as stated in Lemma C.3 below. Equation (14a) from Lemma B.3 follows directly as straightforward corollary. 35 In Lemma C.3, we consider general class of distributions parameterized by θ that models the binary preference Pθ(ya yb x). The negative log-likelihood function is defined as (cid:104) L(θ) = Exρ; (ya,yb)µ(x) w(x) log Pθ(yw yℓ (cid:12) (cid:105) (cid:12) x) . The Bradley-Terry (BT) model described in equation (1) and the corresponding loss function L(θ) in equation (46) represent special case of this general framework. Lemma C.3 (Gradient of the loss function L(θ), full version). For general distribution class {Pθ}, the gradient of L(θ) with respect to θ is given by θ L(θ) = Exρ; (ya,yb)µ(x) (cid:20) w(x) (cid:110) P(cid:0)ya yb (cid:12) (cid:12) x(cid:1) Pθ (cid:0)ya yb (cid:12) (cid:12) x(cid:1)(cid:111) θ Pθ(ya yb x) Pθ(ya yb x) Pθ(yb ya x) (cid:21) , (45a) where µ is the average distribution defined in equation (14b). Specifically, for the BradleyTerry (BT) model where Pθ (cid:0)ya yb (cid:12) (cid:12) x(cid:1) = σ(cid:0)rθ(x, ya) rθ(x, yb)(cid:1) = (cid:26) 1 + (cid:18) (πθ/πref)(yb x) (πθ/πref)(ya x) (cid:19)β(cid:27)1 , the gradient of L(θ) becomes θ L(θ) = Exρ; (ya, yb)µ(x) (cid:20) w(x) σ(cid:0)r(x, ya)r(x, yb)(cid:1)σ(cid:0)rθ(x, ya)rθ(x, yb)(cid:1)(cid:111) (cid:110) (cid:8)θ rθ(x, ya) θ rθ(x, yb)(cid:9) (cid:21) . (45b) For notational simplicity, we focus on the proof for the case where the weight function w(x) = 1. The results for general weight function w(x) > 0 can be derived in similar manner. Recall that the negative log-likelihood function L(θ) is defined as L(θ) = (cid:104) log Pθ (cid:0)yw yℓ (cid:12) (cid:12) x(cid:1)(cid:105) . Based on the data generation mechanism, we can expand the expectation in L(θ) as L(θ) = Exρ; (ya, yb)µ(x) (cid:104) (cid:12) x(cid:1) (cid:8) log Pθ P(cid:0)ya yb (cid:12) + P(cid:0)yb ya (cid:12) (cid:12) x(cid:1) (cid:8) log Pθ (cid:0)ya yb (cid:12) (cid:12) x(cid:1)(cid:9) (cid:0)yb ya (cid:12) (cid:12) x(cid:1)(cid:9)(cid:105) . (46) Notice that we can exchange the roles of ya and yb in the expectation above. This means that we can equivalently express the expectation using the pair (yb, ya) µ( x). This 36 symmetry allows us to replace µ in equation (46) with the average distribution µ as defined in equation (14b). Next, we take the gradient of the loss function L(θ) with respect to the parameter θ and obtain θ L(θ) = Exρ, (ya, yb)µ(x) (cid:20) P(ya yb x) Pθ(ya yb x) (cid:8) θ Pθ(ya yb x)(cid:9) Note that P(cid:0)yb ya (cid:12) Using this, we can rewrite the gradient as (cid:12) x(cid:1) = 1 P(cid:0)ya yb (cid:12) + P(yb ya x) Pθ(yb ya x) (cid:12) x(cid:1) and Pθ (cid:8) θ Pθ(yb ya x)(cid:9) (cid:21) . (cid:0)yb ya (cid:12) (cid:12) x(cid:1) = 1 Pθ (cid:0)ya yb (cid:12) (cid:12) x(cid:1). θ L(θ) = Exρ; (ya, yb)µ(x) (cid:20)(cid:26) 1 P(ya yb x) 1 Pθ(ya yb x) We simplify the expression further to obtain P(ya yb x) Pθ(ya yb x) (cid:27) θ Pθ (cid:0)ya yb (cid:12) (cid:21) (cid:12) x(cid:1) . θ L(θ) = Exρ; (ya, yb)µ(x) (cid:20)(cid:110) Pθ (cid:0)ya yb (cid:12) (cid:12) x(cid:1) P(cid:0)ya yb (cid:12) (cid:12) x(cid:1)(cid:111) θ Pθ(ya yb x) Pθ(ya yb x) Pθ(yb ya x) (cid:21) . This establishes equation (45a) from Lemma B.3. As for the Bradley-Terry (BT) model, we use the equality σ(z) = 1 (1 + exp(z))(1 + exp(z)) = σ(z) σ(z) for any to derive the following expression θ Pθ(ya yb x) Pθ(ya yb x) Pθ(yb ya x) = θ rθ(x, ya) θ rθ(x, yb) . (47) By substituting this gradient expression from equation (47) into equation (45a), we directly obtain equation (45b), thereby completing the proof of Lemma B.3. C.2 Proof of Auxiliary Results for Theorem 4.2 In this section, we present the detailed proofs of the supporting lemmas used in the proof of Theorem 4.2. We begin in Appendix C.2.1 by establishing condition (24), which is crucial for the valid application of the master theorem for Z-estimators. Following this, in Appendix C.2.2, we compute the Hessian matrix 2 θ L(θ) explicitly. Finally, in Appendix C.2.3, we derive the asymptotic distribution of the gradient θ (cid:98)L(θ). 37 C.2.1 Proof of Condition (24) We begin by rewriting the left-hand side of equation (24) as follows: : = = (cid:8)θ (cid:98)L((cid:98)θ) θ L((cid:98)θ)(cid:9) (cid:8)θ (cid:98)L((cid:98)θ) θ (cid:98)L(θ)(cid:9) (cid:8)θ (cid:98)L(θ) θ L(θ)(cid:9) (cid:8)θ L((cid:98)θ) θ L(θ)(cid:9) . (48) We then leverage the smoothness properties of the function rθ, which guarantee the following approximations: θ (cid:98)L((cid:98)θ) θ (cid:98)L(θ) = 2 θ L((cid:98)θ) θ L(θ) = θ (cid:98)L(θ) ((cid:98)θ θ) + op θ L(θ) ((cid:98)θ θ) + op (cid:0)(cid:98)θ θ2 (cid:0)(cid:98)θ θ2 (cid:1) , (cid:1) . (49a) (49b) Assuming these equalities (49a) and (49b) hold, we substitute them into equation (48), leading to = = (cid:8)2 (cid:8)2 θ (cid:98)L(θ) ((cid:98)θ θ) + op((cid:98)θ θ2)(cid:9) θ (cid:98)L(θ) θ L(θ)(cid:9)((cid:98)θ θ) + op (cid:0)1 + (cid:8)2 θ L(θ) ((cid:98)θ θ) + op((cid:98)θ θ2)(cid:9) (cid:98)θ θ (cid:1) . (50) Using the law of large numbers, we know that 2 θ (cid:98)L(θ) (cid:8) θ (cid:98)L(θ) 2 θ L(θ)(cid:9)((cid:98)θ θ) = op 2 (cid:0) θ L(θ), which implies (cid:1) . (cid:98)θ θ2 Therefore, we conclude that as claimed in equation (24). = op (cid:0)1 + (cid:98)θ θ2 (cid:1) The only remaining task is to establish the validity of equalities (49a) and (49b). Proof of Equalities (49a) and (49b): We express the loss function (cid:98)L(θ) in the form (cid:98)L(θ) : = 1 (cid:88) i= w(xi) ℓθ (cid:0)xi, yw , yℓ (cid:1) , where the function ℓθ is defined as ℓθ(x, y1, y2) = log σ(cid:0)rθ(x, y1) rθ(x, y2)(cid:1) . We then calculate the gradient θ ℓθ and 2 θ ℓθ as follows: θ ℓθ(x, y1, y2) = σ(cid:0)rθ(x, y2) rθ(x, y1)(cid:1) (cid:8)θ rθ(x, y2) θ rθ(x, y1)(cid:9) 2 θ ℓθ(x, y1, y2) = σ(cid:0)rθ(x, y2) rθ(x, y1)(cid:1) and (cid:8)θ rθ(x, y2) θ rθ(x, y1)(cid:9)(cid:8)θ rθ(x, y2) θ rθ(x, y1)(cid:9) + σ(cid:0)rθ(x, y2) rθ(x, y1)(cid:1) (cid:8)2 θ rθ(x, y2) 2 θ rθ(x, y1)(cid:9) . 38 When the reward function rθ(x, y), along with its gradient θ rθ(x, y) and Hessian 2 θ rθ(x, y), is uniformly bounded and Lipschitz continuous with respect to θ for all (x, y) Y, it guarantees that the Hessian of the loss function, 2 θ ℓθ, is also Lipschitz continuous. This holds with some constant > 0 across all (x, y) Y, as demonstrated below: (cid:13) θ ℓθ(x, y1, y2) 2 (cid:13)2 From this Lipschitz property, we deduce θ ℓθ(x, y1, y2)(cid:13) (cid:13)2 θ θ2 . (cid:13) (cid:13)θ ℓθ(x, y1, y2) θ ℓθ(x, y1, y2) 2 θ ℓθ(x, y1, y2) (θ θ)(cid:13) (cid:13)2 θ θ2 2 and further derive (cid:13) (cid:13)θ (cid:98)L(θ) θ (cid:98)L(θ) 2 θ (cid:98)L(θ) (θ θ)(cid:13) (cid:13)2 (cid:13) (cid:13)θ L(θ) θ L(θ) θ L(θ) (θ θ)(cid:13) (cid:13)2 2 2 θ θ2 2 , θ θ2 2 . Finally, under the condition that (cid:98)θ equations (49a) and (49b), as previously claimed. θ, these results simplify to the expressions given in C.2.2 Proof of Lemma B.5, Explicit Form of Hessian 2 θ L(θ) From equation (14a) in Lemma B.3, we recall the explicit formula for the gradient θ L(θ). Taking the derivative of both sides of equation (14a), we obtain 2 θ L(θ) = Exρ; (ya, yb)µ(x) (cid:104) w(x) σ(cid:0)rθ(x, ya) rθ(x, yb)(cid:1) (cid:8)θ rθ(x, ya) θ rθ(x, yb)(cid:9)(cid:8)θ rθ(x, ya) θ rθ(x, yb)(cid:9)(cid:105) (cid:20) (cid:110) σ(cid:0)r(x, ya) r(x, yb)(cid:1) σ(cid:0)rθ(x, ya) rθ(x, yb)(cid:1)(cid:111) w(x) Exρ; (ya, yb)µ(x) (cid:8)2 θ rθ(x, ya) θ rθ(x, yb)(cid:9) (cid:21) . (51) When we set θ = θ, it follows that rθ = r. This simplification eliminates the second term in expression (51), reducing the Hessian matrix to θ L(θ) = Exρ; (ya, yb)µ(x) (cid:104) w(x) σ(cid:0)r(x, ya) r(x, yb)(cid:1) (cid:8)θ r(x, ya) θ r(x, yb)(cid:9)(cid:8)θ r(x, ya) θ r(x, yb)(cid:9)(cid:105) . Substituting the derivative σ with its explicit form, σ(z) = σ(z) σ(z) for any R, we refine the expression to θ L(θ) = Σ , where the covariance matrix Σ is defined in equation (11). This completes the proof of expression (25) from Lemma B.5. 2 39 C.2.3 Proof of Lemma B.6, Asymptotic Distribution of Graident θ (cid:98)L(θ) In this section, we analyze the asymptotic distribution of the gradient θ (cid:98)L(θ) at θ = θ, where the loss function (cid:98)L(θ) is defined as (cid:98)L(θ) = 1 (cid:88) i=1 w(x) log σ (cid:16) (cid:0)xi, yw (cid:1) rθ (cid:0)xi, yℓ rθ (cid:1)(cid:17) . Using the definition of the sigmoid function σ, we calculate that (log σ(z)) = σ(z)/σ(z) = σ(z) σ(z)/σ(z) = σ(z) for any real number R. This allows us to reformulate θ (cid:98)L(θ) as the average of i.i.d. vectors {ui}n i=1: θ (cid:98)L(θ) = 1 (cid:88) i=1 ui . Here each vector ui Rd is defined as ui : = w(x) σ(cid:0)rθ(xi, yℓ i) rθ(xi, yw )(cid:1) (cid:8)θ rθ(xi, yℓ i) θ rθ(xi, yw )(cid:9) . At θ = θ, we denote ui as and gi as . Notably, vector ui can be rewritten as ui = w(x) (cid:8)σ(cid:0)rθ(xi, ya ) rθ(xi, yb )(cid:1) 1{ya = yw , yb = yℓ i}(cid:9) gi , (52) (53) where gi is given by gi : = θ rθ(xi, ya ) θ rθ(xi, yb ) . From the structure of the BT model, it holds that E(cid:2)1{ya = yw , yb = yℓ i} (cid:12) (cid:12) xi (cid:3) = σ(cid:0)r(xi, ya ) r(xi, yb )(cid:1) , which implies E[u ] = 0. To analyze the asymptotic distribution of θ (cid:98)L(θ), we apply the central limit theorem (CLT) to its empirical form given in equation (52). By the CLT, we have (cid:0)θ (cid:98)L(θ) θ L(θ)(cid:1) (cid:0)0, (cid:101)Ω(cid:1) , , (54) where the covariance matrix (cid:101)Ω Rdd is given by (cid:101)Ω : = Cov(u 1) = E(cid:2)u 1(u 1)(cid:3) . Here we have used the property E[u ] = 0 in the second equality. We now compute the explicit form of the covariance matrix (cid:101)Ω. Using the definition of ui from expression (53), we find that 1)(cid:3) (cid:101)Ω = E(cid:2)u (cid:104) = w2(x) (cid:8)σ(cid:0)r(x1, ya 1) r(x1, yb 1)(cid:1) 1{ya 1}(cid:9)2 1 = yw 1 = yℓ 1 , yb 1(u 1(g . xρ; (ya,yb)µ(x) 1)(cid:105) (55) Taking the conditional expectation over the outcomes of winners and losers, and using the relation (cid:104)(cid:8)σ(cid:0)r(x1, ya (cid:16) 1{ya = Var = σ(cid:0)r(xi, ya 1) r(x1, yb 1 = yℓ 1 , yb 1 = yw (cid:17) 1)(cid:1) 1{ya (cid:12) (cid:12) x1, ya 1, yb (cid:12) 1 )(cid:1) σ(cid:0)r(xi, yb ) r(xi, ya )(cid:1) , 1 = yℓ 1} 1 = yw 1 , yb ) r(xi, yb 1}(cid:9)2 (cid:12) (cid:12) x1, ya (cid:12) 1, yb 1 (cid:105) we reduce equation (55) to (cid:101)Ω = Exρ; (ya,yb)µ(x) (cid:104) w2(x) Var(cid:0)1{ya 1 = yw 1 , yb 1 = yℓ 1} (cid:12) (cid:12) x1, ya 1, yb (cid:1) 1(g 1)(cid:105) . Bounding the weight function w(x) by its uniform bound w, we simplify further: (cid:101)Ω E (cid:104) w(x) Var(cid:0)1{ya 1 = yw 1 , yb 1 = yℓ 1} (cid:12) (cid:12) x1, ya 1, yb 1 (cid:1) 1(g 1)(cid:105) . This ultimately reduces to where Σ is defined in equation (11). (cid:101)Ω Σ (56) Finally, by combining equations (54) and (56), we establish the asymptotic normality of θ (cid:98)L(θ) and complete the proof of Lemma B.6. C.3 Proof of Auxiliary Results for Theorem 4. This section contains the proofs of the auxiliary results supporting Theorem 4.3. In Apθ J(π). Appendix C.3.2 rigpendix C.3.1, we derive the explicit form of the Hessian 2 orously establishes the asymptotic distribution of the value gap (equation (31)). Finally, Appendix C.3.3 proves the tail bound (10) on the chi-square distribution χ2 d. C.3.1 Proof of Equation (9) from Theorem 4.3, Explicit Form of Hessian 2 θ J(π) We begin by differentiating expression (38) for the gradient θ J(πθ) to obtain the Hessian matrix 2 θ J(πθ). The resulting expression can be written as θ J(πθ) = Γ1 + Γ2 + Γ3 , 41 where the terms are defined as follows: Γ1 : = 1 β Exρ (cid:20) (cid:90) (cid:8)r(x, y) rθ(x, y)(cid:9) Γ2 : = 1 β Exρ, yπθ(x) (cid:110) θ rθ(x, y) Eyπθ(x) (cid:20)(cid:110) θ rθ(x, y) Eyπθ(x) (cid:2)θ rθ(x, y)(cid:3)(cid:111) θ πθ(dy x) (cid:21) , (cid:2)θ rθ(x, y)(cid:3)(cid:111) θ rθ(x, y) (cid:21) , Γ3 : = 1 β (cid:20) Exρ, yπθ(x) (cid:8)r(x, y) rθ(x, y)(cid:9)(cid:110) 2 θ rθ(x, y) θ Eyπθ(x) (cid:2)θ rθ(x, y)(cid:3)(cid:111)(cid:21) . At the point θ = θ, we know that rθ = r. This simplifies the expression significantly: Γ1 = 0 and Γ3 = 0. Therefore, only term Γ2 contributes to the Hessian, and it further reduces to (cid:104) Γ2 = Exρ, yπθ(x) θ rθ(x, y) θ rθ(x, y)(cid:105) 1 β 1 β 1 β + Exρ (cid:104) Eyπθ(x) (cid:104) = Exρ Covyπθ(x) (cid:2)θ rθ(x, y) (cid:12) (cid:12) x(cid:3)(cid:105) . (cid:2)θ rθ(x, y)(cid:3) Eyπθ(x) (cid:2)θ rθ(x, y)(cid:3)(cid:105) From this simplification, we deduce 2 θ J(π) = (cid:104) Exρ 1 β Covyπ(x) (cid:2)θ r(x, y) (cid:12) (cid:12) x(cid:3)(cid:105) , which establishes equation (9) as stated in Theorem 4.3. C.3.2 Proof of the Asymptotic Distribution in Equation (31) The goal of this part is to establish the asymptotic distribution of n{J(π) J((cid:98)π)}, as stated in equation (31) from Appendix B.2.3. To achieve this, we first recast the value gap into the product of two terms and then invoke Slutskys theorem. We start by writing {J(π) J((cid:98)π)} = ((cid:98)θ θ)H ((cid:98)θ θ) (cid:125) (cid:124) (cid:123)(cid:122) Un J(π) J((cid:98)π) ((cid:98)θ θ)H ((cid:98)θ θ) (cid:123)(cid:122) (cid:125) (cid:124) Vn . By isolating Un and Vn in this way, we can handle their limiting behaviors separately: Un Vn zΩ 1 2 . 1 2 HΩ 1 2 with (0, I), 42 (57) (58a) (58b) If these two results are established, the desired asymptotic distribution of the value gap, as given in equation (31), follows directly from Slutskys theorem. To complete the proof, we proceed to verify equations (58a) and (58b). It is worth noting that equation (58a) is straightforward corollary of Theorem 4.2, so the main task is to establish the convergence result in equation (58b). Proof of Equation (58b): Since Σ is nonsingular, the matrix = (Z θ/β) Σ is also nonsingular. From equation (30), we know that for any ε (0, 1), there exists threshold η(ε) > 0 such that whenever θ θ2 η(ε), the following inequality holds: (cid:17) ε (cid:16)1 2 (θ θ)H (θ θ) J(π) J(πθ) (cid:17) + ε (cid:16) 1 2 (θ θ)H (θ θ) . This can be reformulated as (cid:12) (cid:12) (cid:12)Vn (cid:12) (cid:12) (cid:12) ε . 1 2 Next, under the condition that (cid:98)θ such that for any (ε, δ), θ, for any δ > 0, there exists an integer (ε, δ) Z+ Therefore, for any (ε, δ), we can conclude P(cid:8)(cid:98)θ θ2 > η(ε)(cid:9) δ . (cid:26)(cid:12) (cid:12) (cid:12)Vn (cid:27) (cid:12) (cid:12) (cid:12) > ε 1 2 δ . In simpler terms, Vn 1 2, which establishes equation (58b). C.3.3 Proof of the Tail Bound in Equation (10) We now establish the tail bound P(cid:8)χ > (1 + ε) d(cid:9) exp (cid:110) 2 (cid:0)ε log(1 + ε)(cid:1)(cid:111) , (59) as stated in equation (10). We first note that the moment-generating function (MGF) of distribution χ2 (t) = (1 2t) 2 , for any < 1 2. Mχ2 is given by Using Markovs inequality, for any > 0, we have > (1 + ε) d(cid:9) exp{t(1 + ε)d} Mχ2 P(cid:8)χ2 (t) = exp{t(1 + ε)d} (1 2t) 2 (60) for any < 1 2 log(1 2t). Solving for the optimal t, we obtain 2. We optimize the bound by choosing to minimize the exponent t(1 + ε)d = ε 2(1 + ε) . Substituting back into inequality (60), the bound simplifies to the desired inequality (59). 43 Supporting Theorem: Master Theorem for Z-Estimators In this section, we provide brief introduction to the master theorem for Z-estimators for the convenience of the readers. Let the parameter space be Θ, and consider data-dependent function Ψn : Θ L, where is metric space with norm L. Assume that the parameter estimate (cid:98)θn Θ satisfies Ψn((cid:98)θn)L 0, making (cid:98)θn Z-estimator. The function Ψn is an estimator of fixed function Ψ : Θ L, where Ψ(θ0) = 0 for some parameter of interest θ0 Θ. Theorem D.1 (Theorem 2.11 in Kosorok (2008), master theorem for Z-estimators). Suppose the following conditions hold: 1. Ψ(θ0) = 0, where θ0 lies in the interior of Θ. 2. 3. Ψn((cid:98)θn) 0 and (cid:98)θn θ0 0 for the sequence of estimators {(cid:98)θn} Θ. n(Ψn Ψ)(θ0) Z, where is tight4 random variable. 4. The following smoothness condition is satisfied: (cid:13) (cid:13) n(cid:0)Ψn((cid:98)θn) Ψ((cid:98)θn)(cid:1) n(cid:0)Ψn(θ0) Ψ(θ0)(cid:1)(cid:13) (cid:13)L 1 + (cid:98)θn θ0 0 . (61) Additionally, assume that θ (cid:55) Ψ(θ) is Frechet differentiable5 at θ0 with derivative Ψθ0, and that Ψθ0 is continuously invertible6. Then (cid:13) (cid:13) Ψθ0((cid:98)θn θ0) + n(Ψn Ψ)(θ0)(cid:13) (cid:13)L 0 and therefore (cid:0) (cid:98)θn θ0 (cid:1) Ψ1 θ0 ."
        },
        {
            "title": "E Experimental Details",
            "content": "We implement our code based on the open-sourced OpenRLHF framework Hu et al. (2024). We will open-source our code in the camera-ready version. 4A random variable is tight if, for any ϵ > 0, there exists compact set such that P(Z / K) < ϵ. 5Frechet differentiability: map ϕ : is Frechet differentiable at θ if there exists continuous, θ(hn)L/hn 0 for all sequences {hn} with θ : such that ϕ(θ + hn) ϕ(θ) ϕ linear map ϕ hn 0 and θ + hn Θ for all 1. 6Continuous invertibility: map : Θ is continuously invertible if is invertible, and there exists constant > 0 such that A(θ1) A(θ2)L cθ1 θ2 for all θ1, θ2 Θ. 44 We use both the helpful and the harmless (HH) sets from HH-RLHF (Bai et al., 2022) without additional data selection. We adopt the chat template from the Skywork-Reward-8B model (Liu et al., 2024a) to align with the reward template. This reward model, fine-tuned from Llama-3.1-8B, is used to simulate human preference labeling and matches our network trained for alignment. For SFT, we apply full-parameter tuning with Adam for one epoch, using cosine learning rate schedule, 3% warmup phase, learning rate of 5 107, and batch size of 256. These hyperparameters are adopted from Hu et al. (2024). For all the DPO training in both iterative and online settings, we use full-parameter tuning with Adam but with two epochs. The learning rate, warmup schedules, and batch size are all the same. During generation, we limit the maximum number of new tokens to 896 and employ top decoding with = 0.95 for all experiments. For Online DPO, we use sampling temperature of 1.0, following Guo et al. (2024), while in Iterative DPO, we set the temperature to 0.7 to account for the off-policy nature of the data, following Dong et al. (2024); Shi et al. (2024). Prompts are truncated to maximum length of 512 tokens (truncated from the left if the length exceeds this limit) for SFT, DPO, and generation tasks. For SFT data, the maximum length is further restricted to 1024 tokens. When the combined length of the response and the (truncated) prompt exceeds 1024 tokens, the response is truncated from the right. These truncation practices align with the standard methodology described by Rafailov et al. (2023). In contrast, for DPO, responses are not further truncated, as we are already limiting the maximum tokens generated during the generation process. When reproducing the Hybrid Sampling baseline (Exploration Preference Optimization, XPO) from Xie et al. (2024), we use α = 5 106 as suggested in the paper. We do not include comparison with Shi et al. (2024) and Liu et al. (2024d) in our experiments. While Shi et al. (2024) employs sampling method similar to ours, their approach requires significantly more hyperparameters to tune, whereas our method involves no hyperparameter tuning. On the other hand, Liu et al. (2024d) relies on training an ensemble of 20 reward models to approximate the posterior. Their sampling method requires solving the argmax of these rewards, which is computationally intractable. As workaround, they generate 20 samples and select the best one using best-of-N with = 20. This approach demands at least six times the computational resources compared to our method. E.1 Additional Results We present the full results for Online DPO with the overfitted initial policy, including scatter plot in Figure 5 and summary of the objective values in Table 4."
        },
        {
            "title": "We observe that Vanilla Sampling rapidly increases its KL divergence from the reference",
            "content": "45 model while its reward improvement diminishes over time. In contrast, PILAF undergoes an early phase of training with fluctuating KL values but ultimately achieves policy with higher reward and substantially lower KL divergence. We hypothesize that PILAFs interpolationbased exploration enables it to escape the suboptimal region of the loss landscape where Vanilla Sampling remains trapped. Conversely, Hybrid Sampling, despite its explicit exploration design, remains biased by the policy model and continues to exhibit high KL values. While KL divergence decreases over training, the reward improvement remains limited. Meanwhile, Best-of-N Sampling introduces an implicit exploration mechanism through internal DPO, which selects the best and worst responses, leading to wider coverage than Vanilla Sampling. However, despite achieving KL divergence similar to PILAF, it results in lower reward. These findings highlight the superiority of PILAF sampling, demonstrating its effectiveness in robustly optimizing an overfitted policy. Table 4. Results of Online DPO with an overfitted initial policy. We report the average reward, KL divergence from the reference model, and objective on the testset. Method Reward () KL () () Vanilla Best-of-N Hybrid PILAF -3.95 -4.49 -6.00 -3.54 39.85 27.90 18.20 26. -7.93 -7.28 -7.82 -6.19 Figure 5. Online DPO with an overfitted initial policy. Full results of the Figure 4. Each dot represents an evaluation performed every 50 training steps. Color saturation indicates the training step, with darker colors representing later steps. Extension to Proximal Policy Optimization (PPO) In this section, we briefly explore how the core principles of our PILAF sampling approach can be extended to PPO-based RLHF methods. Integrating Response Sampling in InstructGPT: The PPO-based RLHF pipeline used in InstructGPT (Ouyang et al., 2022) consists of three key steps: (i) Supervised Fine-Tuning (SFT) that produces the reference model πref. 46 (ii) Reward Modeling (RM) by solving the optimization problem (2), yielding an estimated reward function rθ. (iii) Reinforcement Learning Fine-Tuning, where the policy πϕ is optimized against the reward model rθ using the Proximal Policy Optimization (PPO) algorithm, following the optimization scheme (4). The key distinction between the PPO and DPO approaches lies in how the reward model rθ is representedexplicitly in PPO and implicitly in DPO. In response sampling for data collection, it is crucial to consider the iterative nature of the InstructGPT pipeline. During each iteration, additional human-labeled data is collected for reward modeling (step (ii)), and steps (ii) and (iii) are repeatedly applied to refine the model. Our proposed PILAF algorithm naturally integrates into this pipeline by improving the data collection process in step (ii), thereby enhancing reward model training and, in turn, policy optimization. Extensions of T-PILAF and PILAF: Extending our response sampling methods, PILAF and T-PILAF, to the PPO setup with an explicit rθ is both natural and straightforward. Within the theoretical framework of T-PILAF, as introduced in Section 3, the only required modification is replacing πθ with the language model πϕ and redefining the interpolated and extrapolated policies, π+ ϕ and π ϕ , following the same formulation as in equations (6a) and (6b). Specifically, we define π+ ϕ (y x) := π ϕ (y x) := 1 +(x) 1 (x) πϕ(y x) exp (cid:8)rθ(x, y)(cid:9) , πϕ(y x) exp (cid:8) rθ(x, y)(cid:9), (62a) (62b) where rθ is now explicitly produced by reward network, rather than being implicitly derived from πϕ, as in equation (5). To extend our empirical PILAF algorithm, as described in Section 5, we propose applying the same interpolation and extrapolation techniques directly to the logits of the language models πϕ and πref. In particular, we take π+ ϕ ( x, y1:t1) = softmax π ϕ ( x, y1:t1) = softmax (cid:16)(cid:8)(1 + β) hϕ β href (cid:16)(cid:8)(1 β) hϕ + β href (cid:17) (cid:9)(x, y1:t1) (cid:17) (cid:9)(x, y1:t1) , , where hϕ and href represent the logits of the language models πϕ and πref, respectively. Adaption of Theoretical Analysis: Our theoretical analyses can be extended to the PPO framework, assuming that the optimization process (4) in step (iii) of InstructGPT is solved exactly. In this case, the policy satisfies πϕ = πrθ, where πrθ(y x) : ="
        },
        {
            "title": "1\nZθ(x)",
            "content": "πref(y x) exp (cid:111) rθ(x, y) . (cid:110) 1 β Under this assumption, the output language model πϕ is implicitly function of the parameter θ. Building on this, we can adapt our optimization and statistical analyses as follows: Optimization Consideration: Using the same argument as in Theorem 4.1, we can prove that θ L(θ) = θ J(πϕ) + T2 , where > 0 is universal constant, and T2 represents second-order approximation error. In other words, if the policy optimization step is sufficiently accurate for the reward model rθ, then performing gradient descent on the MLE loss with respect to θ is equivalent to applying gradient ascent on the oracle objective J, following the steepest direction in the parameter space of θ. Statistical Consideration: Even with the new parameterization, the asymptotic distribution of (cid:98)θ from Theorem 4.2 remains unchanged. Moreover, the gradient and Hessian of with respect to θ retain the same form as in Theorem 4.1. As result, the statistical analysis extends naturally to PPO, allowing us to conclude that PILAF also maintains structure-invariant statistical efficiency for PPO methods."
        }
    ],
    "affiliations": [
        "Meta FAIR",
        "NYU"
    ]
}