{
    "paper_title": "RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models",
    "authors": [
        "Maya Varma",
        "Jean-Benoit Delbrouck",
        "Zhihong Chen",
        "Akshay Chaudhari",
        "Curtis Langlotz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RaVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given a fine-tuned VLM, RaVL first discovers spurious correlations by leveraging a region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RaVL mitigates the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RaVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 ] . [ 1 7 9 0 4 0 . 1 1 4 2 : r RAVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models Maya Varma Stanford University mayavarma@cs.stanford.edu Jean-Benoit Delbrouck Stanford University; Hugging Face jbdel@stanford.edu Zhihong Chen Stanford University zhihongc@stanford.edu Akshay Chaudhari Stanford University akshaysc@stanford.edu Curtis Langlotz Stanford University langlotz@stanford.edu"
        },
        {
            "title": "Abstract",
            "content": "Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RAVL, which takes fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given fine-tuned VLM, RAVL first discovers spurious correlations by leveraging region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RAVL mitigates the identified spurious correlation with novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RAVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RAVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on generaldomain and medical-domain VLMs confirm our findings."
        },
        {
            "title": "Introduction",
            "content": "Contrastive vision-language models (VLMs) (e.g., CLIP [36] and ALIGN [24]) are powerful class of models that jointly learn relationships between images and text. VLMs are generally pretrained on web-scale datasets with millions of image-text pairs and have been shown to exhibit impressive capabilities on wide range of downstream tasks. In particular, VLMs have the ability to perform tasks in zero-shot manner without utilizing explicit task-specific training data; this is accomplished by modeling downstream tasks (e.g., image classification, text-to-image retrieval) as image-text matching tasks [36]. However, pretrained VLMs can exhibit poor zero-shot performance when compared to state-of-the-art task-specific models, particularly on challenging or out-of-domain downstream tasks [36, 7, 17, 19]. As result, pretrained VLMs are often fine-tuned on domain-specific vision-language datasets in order Equal senior authorship. 1Code: https://github.com/Stanford-AIMI/RaVL 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Figure 1: Region-aware Vision-Language learning (RAVL). RAVL takes fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features. to improve zero-shot performance on tasks of interest. For instance, recent works have fine-tuned the CLIP VLM [36] on vision-language datasets consisting of (i) chest X-rays and paired physician reports [45], (ii) pathology data and paired text [17, 19], and (iii) product images and paired captions from online fashion retailers [7]. Domain-specific vision-language datasets used to fine-tune VLMs may be small in size, preventing VLMs from gaining the robustness benefits that come with training on diverse, web-scale data [6, 14]. As result, fine-tuned VLMs may capture spurious correlations between image features and textual attributes [56]. For instance, consider VLM fine-tuned on an animal image-text dataset where the presence of butterflies is closely correlated with the presence of flowers (Figure 1). Consequently, the VLM may learn to incorrectly associate the image features corresponding to flower with the textual attribute butterfly. At test time, the VLM is likely to exhibit degraded zero-shot classification performance on (i) images of butterflies without flowers and (ii) images of other animals with flowers. Improving robustness of fine-tuned VLMs to spurious correlations is challenging for the following two reasons. First, existing automated approaches primarily discover and mitigate spurious correlations at the global image level rather than intervening directly on fine-grained image features. Such approaches discover spurious correlations by identifying coherent groups of misclassified images in an automated fashion [13, 43, 22, 42]; then, the identified spurious correlation can be mitigated during training using data augmentation or robust optimization [43, 39, 22, 56]. However, recent works have suggested that such global image-level strategies (i) discover spurious correlations that align poorly with human-interpretable attributes [25] and (ii) may not effectively enable models to ignore spurious correlations during training [15, 18]. Second, existing approaches for discovering and mitigating spurious correlations are predominantly designed to improve robustness of unimodal image classification models [39, 43] or pretrained VLMs [60, 49]. These settings differ substantially from the fine-tuned VLM setting, which presents several unique challenges such as the absence of class and subgroup labels in the training set and the inclusion of free-form text. In this work, we address these challenges by introducing Region-aware Vision-Language learning (RAVL), an approach for improving the robustness of fine-tuned VLMs to spurious correlations. RAVL takes fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features, rather than operating at the global image level. Our contributions are: First, given fine-tuned VLM, RAVL discovers learned spurious correlations between image features and textual attributes. Using labeled classification dataset, we decompose images into candidate regions, utilize the VLM embedding space to group visually-similar regions into feature clusters, and quantitatively evaluate the effects of each feature on zero-shot classification errors. Second, given ranked list of image features that the VLM has learned to spuriously correlate with one or more textual attributes, RAVL mitigates the identified spurious correlations. Our key insight is that region-level information can be leveraged during VLM fine-tuning in order to improve model robustness. To this end, we introduce novel region-aware loss function 2 that encourages the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. In order to evaluate RAVL, we introduce large-scale evaluation framework for controlled, finegrained evaluations of VLM robustness on synthetic and real-world data. Our framework consists of 654 fine-tuned VLMs paired with annotations for the ground-truth spurious correlations learned by each VLM. Across these evaluation settings, (i) RAVL accurately discovers spurious correlations, achieving 191% improvement over the closest baseline, and (ii) RAVL effectively mitigates spurious correlations, achieving up to an 8.2% improvement on worst-group image classification accuracy. Qualitative evaluations on general-domain and medical-domain VLMs confirm the utility of RAVL. This paper is organized as follows. In Section 2, we introduce our problem setting. Then, in Section 3, we present Stage 1 of RAVL, including our proposed methodology for discovering spurious correlations, our large-scale evaluation framework, and experimental results. In Section 4, we introduce Stage 2 of RAVL, including our proposed methodology for mitigating spurious correlations as well as experimental results. Finally, we conclude in Section 5. Related Work: Our work builds on several recent research directions for discovering and mitigating spurious correlations. We provide an analysis of related works in Appendix Section A."
        },
        {
            "title": "2 Preliminaries",
            "content": "In this section, we formally describe our problem setting. Datasets used for fine-tuning VLMs can be expressed as DF = {(Ii, Ti)}m i=1, where Ii represents image inputs and Ti represents paired free-form text. We do not assume access to any class or subgroup labels. The performance of fine-tuned VLMs can be characterized with zero-shot classification tasks. In line with prior work [13, 22, 56], we assume that the zero-shot classification dataset includes validation split DV = {(Ii, yi)}n i=1 with images Ii and known ground-truth class labels yi Y, where denotes the set of all possible class labels. At evaluation time, classification performance is computed by encoding class labels in as text and matching images to the closest class label using embedding similarity. We do not assume access to any subgroup labels. Fine-tuned VLMs may learn spurious correlations between image features and textual attributes. Let ea represent the image features corresponding to visual concept (e.g., flowers in Figure 1) and represent class label (e.g., butterfly\" in Figure 1) such that ea and share no causal relationship. Then, fine-tuned VLM that has learned spurious correlation will be unable to disentangle ea and at evaluation time. This will manifest in low zero-shot classification performance on the following two subgroups of data: (i) images from class label without the feature ea and (ii) images from other class labels {y} with the feature ea. However, since neither the fine-tuning dataset DF nor the evaluation dataset DV include subgroup labels corresponding to visual concepts a, discovering and mitigating such spurious correlations poses challenge. For instance, in Figure 1, there are no annotations for flowers in datasets DF and DV , making it challenging to identify and address the learned spurious correlation between image features corresponding to flowers and the textual attribute corresponding to butterfly\". In the following sections, we will discuss our automated approach RAVL, which aims to address this challenge by employing fine-grained region-level information to discover (Section 3) and mitigate (Section 4) spurious correlations in fine-tuned vision-language models."
        },
        {
            "title": "3 Discovering Spurious Correlations in Fine-Tuned Vision-Language Models",
            "content": "In this section, we present the first stage of RAVL, which aims to discover learned spurious correlations in VLMs. In Section 3.1, we discuss our region-aware approach for discovering fine-grained spurious correlations. Then, in order to quantitatively evaluate the efficacy of spurious feature discovery methods, we introduce large-scale evaluation framework in Section 3.2. Finally, in Section 3.3, we use our evaluation framework to demonstrate that RAVL outperforms prior approaches in discovering fine-grained spurious correlations between image features and textual attributes. 3 3.1 Our Approach: Discovering Spurious Correlations The first stage of RAVL aims to identify spurious correlations between image features and textual attributes learned by fine-tuned VLM M. In contrast to prior works that have incorporated humans in the loop in order to identify spurious correlations [56, 30], RAVL is fully automated approach. Additionally, whereas previous automated methods for discovering spurious correlations focus predominantly on identifying groups of images with high error rates [22, 13], our approach identifies specific image features that model has learned to spuriously correlate with textual attribute. Our goal is to discover precise spurious correlations that can be easily interpreted by humans. As discussed in Section 2, model that has learned spurious correlation between an image feature ea and textual attribute will demonstrate low zero-shot performance on (i) images in DV with label without the feature ea and (ii) images in DV with other labels {y} with the feature ea. The key challenge lies in identifying such relationships when no annotations are provided for visual concepts a. RAVL addresses this challenge by (1) obtaining candidate image features in DV , (2) identifying the candidate image features that, when present in an image, directly contribute to classification errors, and (3) ranking the identified image features by degree of learned spurious correlations. Obtaining candidate image features. RAVL first utilizes the zero-shot classification dataset DV to identify candidate image features. To this end, we use the fine-tuned VLM to extract an image embedding for each image Ii in DV and text embedding for each class Y. Zero-shot classification is performed using the computed embeddings; this results in softmax-normalized image score distribution vector sIi RY, where represents the number of classes. Then, we decompose each image Ii in DV into set of candidate regions Ri. There are variety of ways in which an image can be decomposed into regions, such as dividing images into equal-sized segments (e.g., quadrants) or using region proposal networks (RPNs) [38]. Ideally, regions should capture key features in the image; however, we emphasize that RAVL does not require groundtruth region-level annotations. We then apply RoIAlign [16, 63] to the image encoder of to extract embeddings for each region. Zero-shot classification is performed using the computed region embeddings, resulting in softmax-normalized region score distribution matrix SRi RRiY. Given region-level embeddings for all candidate regions in DV , we next aim to identify coherent groups of image features that occur consistently throughout the dataset (e.g., features corresponding to flower\" or butterfly\" in Figure 1). To this end, we cluster the computed region-level embeddings using the K-Medoids algorithm with cosine distance. The optimal number of clusters is selected in an automated fashion using Silhouette distance. The resulting clusters (denoted as C) capture key image features in DV . For feature cluster C, let ec denotes the set of features in cluster c. Identifying candidate image features that directly contribute to classification errors. We now seek to identify features that, when present in an image, are directly responsible for prediction errors. Let Rc represent the set of regions assigned to cluster and let Ic represent the set of images associated with the regions in cluster c. We identify labels for images in Ic; we designate this label set as Yc. For each class label Yc, we identify all images in Ic with label y, and we designate zero-shot classification accuracy on this subset of ny in. Then, we identify all images in Dv with label that do not have region included in cluster c, and we designate zero-shot out images as py classification accuracy on this subset of ny We now introduce the cluster influence score, which evaluates the extent to which features ec contribute to mispredicted image classification labels. We restrict our evaluation to only include mispredicted images in Ic with ground-truth labels such that py out; we will refer to this subset as err , we extract (i) the image score distribution vector sIi and (ii) the region score distribution matrix SRi. We use sIi to identify the predicted image class ˆy, and we then identify the region rmax in Ri with the highest score for class ˆy. Ic. For each image Ii err in images as py in < py out. Definition 1 (Cluster Influence Score). For cluster and label y, the cluster influence score is the proportion of images Ii err is part of cluster (i.e., rmax Rc): with label where the identified highest-scoring region rmax H = 1 {Ii err yi = y} (cid:88) 1[rmax Rc] IiIerr ;yi=y (1) 4 The final cluster influence score for cluster is computed as the maximum over all labels as Hc = maxyYcH . High values of Hc show that features ec are similar to the incorrect label in the vision-language embedding space; this suggests that for given image with an incorrect prediction, feature ec is more likely to contribute to the misprediction than other features in the image. On the other hand, low values of Hc are likely to indicate that feature ec represents core feature associated with the class label or neutral feature that does not affect predictions. Given Hc for each feature cluster, we prune all clusters with influence scores below threshold of τl, which we set to 0.25 in all experiments. Ranking image features by degree of learned spurious correlation. For each remaining feature cluster, we next aim to determine the extent to which the presence or absence of features ec affects classification performance; we introduce the cluster performance gap metric to this end. Definition 2 (Cluster Performance Gap). For cluster and label y, the cluster performance gap is the weighted difference between zero-shot classification accuracy on images with features ec and images without features ec: Gy = wy (py in py out), (2) where wy is simple weighting factor computed as wy = 2 [min(ny Since spurious correlations result in consistent errors as opposed to isolated misclassifications, the weighting factor is designed to prioritize stronger spurious correlations that result in larger number of errors. Gy ranges between 0 and 1. The final performance gap metric for cluster is computed across all labels as Gc = (cid:80) . high value of Gc suggests that the presence or absence of features ec contribute to large class-level variations in image classification performance. out)/(ny in + ny in, ny out)]. Gy yYc Given Gc for each feature cluster, we rank clusters in order from highest to lowest values. The output of this stage is ranked list of image features that model has learned to spuriously correlate with one or more class labels in Y. 3.2 Experimental Setup: Designing Large-Scale Evaluation Framework We now discuss our approach for evaluating RAVL. Evaluating the accuracy of predicted spurious correlations is challenging because the ground-truth spurious correlations learned by model are typically unknown. Previous works on VLM robustness evaluate discovered spurious correlations with qualitative experiments, human-in-the-loop evaluations, or small-scale datasets [56]. Our aim in this section is to introduce large-scale experimental setup where the ground-truth spurious correlations learned by VLMs are known and annotated in advance; this can then enable us to determine whether the features discovered by RAVL in Section 3.1 accurately align with the groundtruth. Our evaluation framework is motivated by prior work [26, 13]; however, in contrast to existing approaches, we introduce evaluation settings that are designed (i) for evaluating robustness approaches at the fine-grained region level rather than the global image-level, and (ii) for evaluating VLMs rather than unimodal models. Designing Controlled Evaluations: Our evaluation framework artificially induces spurious correlations in the VLM fine-tuning data; then, given the known pre-defined spurious correlation and VLM that learned the desired spurious correlation, we can quantitatively evaluate the extent to which RaVL discovers the correlation. We create set of evaluation settings using data from two domains: (1) synthetic data (MNIST [11] and FashionMNIST [51]) and (2) real-world data (COCO [27]). Each evaluation setting consists of the following components: 1. Predefined spurious correlation: We define spurious image feature and textual attribute pair (eeval, aeval). For MNIST and FashionMNIST, eeval represents red rectangle; aeval is generated from the set of class labels {zero, one, two, three, four five, six, seven, eight, nine} for MNIST and {t-shirt, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, ankle boot} for FashionMNIST. For COCO, we sample eeval and aeval from the list of annotated attributes. 2. Fine-tuning dataset: We construct vision-language fine-tuning dataset Deval = {(Ii, Ti)}m i=1 with images Ii and text Ti. Dataset Deval is sampled from the training sets of MNIST, FashionMNIST, or COCO such that the presence of image feature eeval is closely correlated with the presence of text attribute aeval as measured by Cramers [57]. 5 Figure 2: RAVL accurately identifies spurious correlations. Using our evaluation settings, we show that RAVL consistently outperforms prior methods in discovering learned spurious correlations between image features and textual attributes. Here, we provide Precision@10 metrics for CLIPRN50 model fine-tuned on synthetic data (129 settings) and real-world data (171 settings). Table 1: Mean Precision@10 metrics demonstrate the efficacy of RAVL in discovering spurious correlations. On average across 654 evaluation settings, RAVL consistently outperforms baselines. Method Correlation Strength (τeval) Num. Eval Settings Random Distilling Failures George Domino Spurious-Aware Detection RAVL (Ours) 10 20 30 21.2 20.1 19.3 17.1 20.0 61.8 369 18.2 16.2 15.9 15.0 25.3 76.2 234 15.5 8.5 10.9 11.7 32.1 84.2 168 12.5 1.5 7.7 9.0 42.0 91.1 Table 2: Ablations show the utility of the cluster performance gap and influence metrics. We report Precision@10 metrics for CLIP-RN50 model fine-tuned on real-world data (171 settings). Ablation Correlation Strength (τeval) 20 30 Unweighted Gc Only Gc Only Gc & Hc (RAVL) 21.2 40.9 46.0 30.0 51.7 54.8 36.2 63.8 72. 40 55.0 66.7 83.3 3. Fine-tuned VLM: VLM is fine-tuned on Deval 4. Evaluation dataset: Model is evaluated using zero-shot classification dataset Deval = {(Ii, yi, Ri, Li)}n i=1 with images Ii, class labels yi, region bounding boxes Ri, and region-level labels Li. In particular, aeval must be included in the class label set, and eeval must be annotated in the region-level label set. Since Deval is designed to reflect real-world setting, we assume that correlation between aeval and eeval does not exist. Dataset Deval is constructed from the test sets of MNIST, FashionMNIST, or COCO. V . Given the four components listed above, we classify an evaluation setting as valid if model learned the intended spurious correlation. In order to measure this, we first identify images with label aeval in Deval and compute the performance difference between images with feature eeval and images without feature eeval; we designate this value as ϵ1. Then, for labels = aeval, we compute the maximum performance difference between images without feature eeval and images with feature eeval; we designate this value as ϵ2. Large values of ϵ1 and ϵ2 suggest that model has learned the desired spurious correlation between image feature eeval and textual attribute aeval, as defined in Section 2. We remove settings where ϵ1 or ϵ2 are below some predefined performance threshold τeval. The performance threshold τeval serves as quantitative indicator of learned correlation strength. Implementation Details: In total, we generate 620 fine-tuning datasets Deval (100 synthetic; 520 real-world). We then fine-tune model on each dataset with three random seeds, resulting in 1860 candidate evaluation settings. Finally, we filter out settings where model does not consistently learn the spurious correlation; to this end, we only retain settings where both ϵ1 and ϵ2 exceed τeval = 10 across all three random seeds. We repeat this procedure across various pretrained VLMs M, resulting in 654 valid experimental settings. Additional implementation details are provided in Appendix B. 3.3 Results: RaVL Effectively Discovers Spurious Correlations Comparisons to Prior Approaches: Given an evaluation setting with predefined spurious correlation (eeval, aeval), fine-tuned VLM M, and an evaluation dataset Deval , our goal is to determine the extent to which RAVL can discover the correlation between eeval and aeval. To this end, we use the labeled zero-shot classification dataset Deval , which includes ground-truth region bounding boxes and associated region labels. We provide the ground-truth bounding boxes as input to RAVL, which returns single top-ranked cluster of regions likely to include spurious features. We rank regions within the cluster based on similarity to the cluster medoid, and we utilize the provided region-level labels in Deval to evaluate the proportion of top-K regions that contain the desired spurious feature eeval. In line with prior work [13], we report performance with Precision@K metrics. We note that given an identified spurious feature eeval, the correlated textual attribute aeval can be detected by identifying the class label in Deval where the absence of feature eeval leads to degraded performance. There are few existing approaches for performing automated detection of fine-grained spurious features learned by VLMs. Here, we compare RAVL with four previously-developed methods: Distilling Failures [22], George [43], Domino [13], and Spurious-Aware Detection [56]. Distilling Failures, George, and Domino are state-of-the-art approaches for automatic identification of model failures resulting from spurious correlations; although these methods operate at the global image level and are designed for unimodal settings, we adapt these approaches for our setting by utilizing regions and zero-shot classification scores as input. Spurious-Aware Detection operates at the fine-grained region level by computing class-based performance gaps resulting from the presence or absence of particular features. To enable fair comparison with RAVL, we provide the same set of regions and associated embeddings as input to all baselines. We also compare RAVL with random baseline, where the ranked list of regions is shuffled randomly. Table 1 summarizes mean Precision@10 metrics across all 654 evaluation settings. Results demonstrate that RAVL consistently outperforms prior approaches in discovering spurious correlations between image features and textual attributes, contributing to 191% improvement over the closest baseline. In Table 1, we evaluate the effects of learned spurious correlation strength by varying the error threshold τeval from 10 to 40 and reporting performance for the subset of valid evaluation settings. Results show that RAVL is particularly effective when VLM learns strong spurious correlation; as learned correlation strength increases, performance of RAVL increases by 47% whereas most baselines degrade in performance. We also observe that Domino, George, and Distilling Failures often achieve performance near or below the random baseline across our evaluation settings; this suggests that methods designed for detecting errors resulting from spurious correlations at the global image-level cannot be easily adapted for fine-grained region-level discovery. Figure 2 demonstrates that our findings hold for both synthetic and real-world data. Ablations: Our ablation study evaluates the role of the cluster influence score Hc and the cluster performance gap metric Gc (Section 3.1) in enabling accurate discovery of spurious correlations between image features and textual attributes. We compare the following three metrics for ranking clusters: (1) an unweighted cluster performance gap metric where wy is set to 1, (2) the cluster performance gap with wy computed as in Section 3.1, and (3) combination of the cluster performance gap and cluster influence metric as used in RAVL. As shown in Table 2, the metrics utilized by RAVL consistently demonstrate the best performance across various learned correlation strengths (τeval). Our results suggest the utility of both the performance gap metric and the influence score in identifying fine-grained spurious correlations. Evaluations in the Wild: In addition to our controlled evaluations, we evaluate the ability of RAVL to surface spurious correlations learned by 12 off-the-shelf VLMs [12, 36, 20]; this presents realistic and uncontrolled evaluation setting. We consider two zero-shot classification tasks DV : (1) 397class scene classification task on SUN397 [52] and (2) binary classification of cardiomegaly in chest X-rays from ObjectCXR [23]. We use the cluster performance gap metric Gc, introduced in Section 3.1, to quantify the degree of the learned spurious correlation. Our results demonstrate that all evaluated models, which span range of architecture, training data, and parameter counts, show evidence of having learned spurious correlations; this is demonstrated by nonzero values of the cluster performance gap metric Gc. On average across the evaluated models, the top-ranked spurious feature cluster discovered by RAVL on SUN397 achieves cluster performance 7 Figure 3: RAVL surfaces spurious correlations in off-the-shelf VLMs. RAVL identifies spurious correlation learned by CLIP ViT-B/16 between the presence of text-based retail signage and the class label fast food restaurant in scene classification task. RAVL also surfaces spurious correlation learned by PubMedCLIP ResNet-50 between metal clips (found in clothing) and the class label cardiomegaly (a heart condition) on chest X-ray classification task. Table 3: RAVL effectively mitigates spurious correlations. Here, we report mean Image Overall, Image Worst-Group (Img. WG), Region Overall, and Region Worst-Group (Reg. WG) metrics across our real-world evaluation settings. Since performance of mitigation methods is dependent on the results of Stage 1, we report metrics across settings where Stage 1 Precision@10> 0.6 and Stage 1 Precision@10> 0.8. Method Stage 1 Discovery Precision@10> 0.6 Stage 1 Discovery Precision@10> 0.8 Img. Overall Img. WG Reg. Overall Reg. WG Img. Overall Img. WG Reg. Overall Reg. WG Standard FT Upsampled FT VL-ERM VL-GDRO Spurious-Aware RAVL (Ours) 64.0 66.6 68.8 69.1 69.8 69.8 31.4 37.8 32.2 33.7 33.6 39.1 72.0 74.3 75.6 75.6 76.5 78.9 46.9 52.2 50.3 50.4 50.6 57. 64.6 66.7 68.7 68.8 69.2 70.2 31.0 37.7 30.9 31.1 30.7 40.8 72.9 74.7 75.9 76.0 76.8 79.5 47.4 52.8 50.6 51.0 50.5 58.5 gaps (Gc) of 9.93.2 (minimum = 5.1, maximum = 14.0). On ObjectCXR, the mean value of Gc is 0.080.04 (minimum = 0.04, maximum = 0.12).2 Our results support findings from previous work suggesting that all models may learn spurious correlations [30]. In Figure 3, we provide qualitative examples of discovered spurious features for the CLIP ViT-B/16 model evaluated on SUN397 and the PubmedCLIP ResNet-50 model evaluated on ObjectCXR. For the CLIP ViT-B/16 model, RAVL surfaces feature cluster consisting of text-based retail signage. We observe significant performance gaps between images containing the RAVL-identified feature and images that do not contain the feature. For instance, we note 48.2 point difference in zero-shot classification accuracy for the class label fast food restaurant, suggesting that CLIP ViT-B/16 model can better classify scene of fast food restaurant when text-based retail sign is present. For the PubmedCLIP ResNet-50 model, RAVL discovers that the presence of metal clips (found in the patients clothing) is spuriously correlated with cardiomegaly. We observe that the presence of clips improves zero-shot classification accuracy for the class label cardiomegaly by 15.3 points. Our evaluations show that RAVL can surface fine-grained spurious correlations in realistic settings. Additional implementation details and qualitative examples are provided in Appendix D."
        },
        {
            "title": "4 Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models",
            "content": "In this section, we present the second stage of RAVL, which aims to mitigate learned spurious correlations in VLMs. In Section 4.1, we discuss our methodology for mitigating fine-grained spurious correlations with novel region-aware loss function. In Section 4.2, we use the evaluation framework previously introduced in Section 3.2 to demonstrate that RAVL substantially outperforms prior approaches in mitigating spurious correlations between image features and textual attributes. 2We note that since the formula for Gc involves summation over class labels, raw values of Gc for our 2-class chest X-ray classification task are lower than those for our 397-class scene classification task. 8 4.1 Our Approach: Mitigating Spurious Correlations As described in Section 3, Stage 1 of RAVL discovers image features that VLM has learned to spuriously correlate with textual attributes. We next aim to mitigate the spurious correlation. Motivated by prior work on fine-grained VLMs [58, 46], our key insight is that utilizing region-level information during VLM training can enable models to focus on relevant image-text relationships and ignore spurious correlations. Since dataset DF exclusively consists of images and text, ground-truth subgroup and class labels are not available. As result, we first assign plausible (i) region-level subgroup labels and (ii) image-level class labels to the vision-language fine-tuning dataset DF . To assign subgroup labels, we decompose each image Ii in dataset DF into set of candidate regions Ri. We then fit the trained K-Medoids clustering model from Section 3.1 on Ri and identify all spurious regions associated with the top ranked cluster. We represent the identified spurious regions as Rs and remaining non-spurious regions as Rr = Ri. In order to assign plausible class labels, we parse the paired text Ti associated with each image to identify samples that reference the class labels included in the zero-shot classification label set Y; we refer to the assigned class label for image Ii as ˆyi. such that Rs Rr We now introduce novel region-aware contrastive loss function for training VLM Mnew. For batch B, we define Rs . For image Ii B, encourages high embedding similarity between non-spurious regions Rr the first loss component Li and assigned class label ˆyi when compared to other class labels. as the set of all spurious regions in the batch: Rs IiB Rs = (cid:83) Li = log σm(Rr ˆyj σm(Rr , ˆyi) , ˆyj) + (Rs B) (cid:80) (3) for region embedding function and text embedding function g, σm(A, b) = B) is penalty that enforces Here, exp(maxaA(f (a), g(b) /τ )) with temperature τ . The term (Rs embedding-level dissimilarity between spurious regions and correlated class labels. The second loss component Li gions Rr exp(f (a), g(b) /τ ) with temperature τ . encourages high embedding similarity between non-spurious rei and assigned class label ˆyi when compared to other regions. We define σ(a, b) = Li = log σm(Rr , ˆyi) + (cid:80)B j=1, ˆyj = ˆyi σm(Rr , ˆyi) σm(Rr , ˆyi) + (cid:80) . (4) rj Rs σ(rj, ˆyi) The final loss is expressed as = λLCL + (1 λ) (cid:80)B A). Here, λ is hyperparameter and LCL takes the form of the original loss function used for training M; in our experiments, LCL is the CLIP objective [36]. Extended formulations of our loss function are provided in Appendix C. + Li i=1(Li 4.2 Results: RaVL Effectively Mitigates Spurious Correlations Comparisons to Prior Approaches: We use the evaluation framework previously introduced in Section 3.2 to compare RAVL with prior approaches. There are few existing approaches for mitigating spurious correlations in the setting of fine-tuned VLMs. Here, we compare RAVL with standard VLM fine-tuning, upsampled VLM fine-tuning, ERM, GDRO [39], and Spurious-Aware Mitigation [56]. Since ERM and GDRO are traditionally used in unimodal classification settings, we adapt these approaches for our setting by adding contrastive vision-language objective and using zero-shot classification scores during fine-tuning; we refer to these approaches as VL-ERM and VL-GDRO respectively. Table 3 summarizes mean zero-shot classification results across our real-world evaluation settings. Since performance of mitigation methods is dependent on the accuracy of the discovered spurious correlations in Stage 1, Table 3 displays results for two evaluation categories: (i) the 192 settings where RAVL Stage 1 Precision@10 is greater than 0.6, and (ii) the 106 settings where RAVL Stage 1 Precision@10 is greater than 0.8. In line with prior works on robustness [39, 56], we report image overall performance and image worst-group performance. Additionally, in order to evaluate the extent to which the VLM understands fine-grained features, we introduce two new metrics: region overall performance and region worst-group performance. Region-level accuracies are computed by performing zero-shot classification with region embeddings and comparing predicted labels to the ground-truth region-level labels provided in the zero-shot classification dataset. Results show that RAVL consistently outperforms prior approaches in mitigating spurious correlations. Across the two evaluation categories in Table 3, RAVL contributes to an improvement of up to 8.2% on image worst-group performance and 10.8% on region worst-group performance over the nearest baseline. Improvements in region worst-group performance are particularly notable, suggesting that RAVL can better interpret fine-grained features when compared to prior approaches. Additionally, as the accuracy of the discovered spurious correlations in Stage 1 increases, the performance of the RAVL mitigation approach increases proportionally. Our results demonstrate the efficacy of our fine-tuning procedure in mitigating spurious correlations when compared to prior approaches."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced RAVL, fine-grained region-aware approach for addressing spurious correlations in VLMs. We demonstrate through large-scale, controlled experiments as well as in-thewild evaluations that RAVL can discover (191% improvement in identified correlations) and mitigate (8.2% improvement on worst-group performance) spurious correlations in VLMs. We hope that our work can help (i) diagnose and correct critical failure modes in VLMs prior to deployment and (ii) drive progress towards the development of fine-grained approaches for model robustness."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We are thankful to Sophie Ostmeier, Eduardo Reis, and Ashwin Kumar for helpful discussions and feedback. MV is supported by graduate fellowship awards from the Department of Defense (NDSEG), the Knight-Hennessy Scholars program at Stanford University, and the Quad program. AC is supported by NIH grants R01 HL167974, R01HL169345, R01 AR077604, R01 EB002524, R01 AR079431, P41 EB027060, AY2AX000045, and 1AYSAX0000024-01; and NIH contracts 75N92020C00008 and 75N92020C00021. CL is supported by NIH grants R01 HL155410, R01 HL157235, by AHRQ grant R18HS026886, and by the Gordon and Betty Moore Foundation. JBD and CL are supported by the Medical Imaging and Data Resource Center (MIDRC), which is funded by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) under contract 75N92020C00021 and through The Advanced Research Projects Agency for Health (ARPA-H)."
        },
        {
            "title": "References",
            "content": "[1] Dyah Adila, Changho Shin, Linrong Cai, and Frederic Sala. Zero-shot robustification of zero-shot models with foundation models, 2023. [2] Amit Alfassy, Assaf Arbelle, Oshri Halimi, Sivan Harary, Roei Herzig, Eli Schwartz, Rameswar Panda, Michele Dolfi, Christoph Auer, Kate Saenko, PeterW. J. Staar, Rogerio Feris, and Leonid Karlinsky. Feta: Towards specializing foundation models for expert task applications, 2022. [3] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. [4] Louis Blankemeier, Joseph Paul Cohen, Ashwin Kumar, Dave Van Veen, Syed Jamal Safdar Gardezi, Magdalini Paschali, Zhihong Chen, Jean-Benoit Delbrouck, Eduardo Reis, Cesar Truyts, Christian Bluethgen, Malte Engmann Kjeldskov Jensen, Sophie Ostmeier, Maya Varma, Jeya Maria Jose Valanarasu, Zhongnan Fang, Zepeng Huo, Zaid Nabulsi, Diego Ardila, Wei-Hung Weng, Edson Amaro Junior, Neera Ahuja, Jason Fries, Nigam H. Shah, Andrew Johnston, Robert D. Boutin, Andrew Wentland, Curtis P. Langlotz, Jason Hom, Sergios Gatidis, and Akshay S. Chaudhari. Merlin: vision language foundation model for 3d computed tomography, 2024. [5] Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, Emily B. Tsai, Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S. Chaudhari, and Curtis Langlotz. Chexagent: Towards foundation model for chest x-ray interpretation, 2024. [6] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning, 2022. [7] Patrick John Chia, Giuseppe Attanasio, Federico Bianchi, Silvia Terragni, Ana Rita Magalhaes, Diogo Goncalves, Ciro Greco, and Jacopo Tagliabue. Contrastive language and vision learning of general fashion concepts. Scientific Reports, 12(1), November 2022. [8] Joseph Paul Cohen, Joseph Viviano, Paul Bertin, Paul Morrison, Parsa Torabian, Matteo Guarrera, Matthew Lungren, Akshay Chaudhari, Rupert Brooks, Mohammad Hashir, et al. Torchxrayvision: library of chest x-ray datasets and models. In International Conference on Medical Imaging with Deep Learning, pages 231249. PMLR, 2022. [9] Elliot Creager, Jörn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In International Conference on Machine Learning, pages 21892200. PMLR, 2021. [10] Alex DeGrave, Joseph Janizek, and Su-In Lee. AI for radiographic COVID-19 detection selects shortcuts over signal. Nat. Mach. Intell., 3(7):610619, May 2021. [11] Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141142, 2012. [12] Sedigheh Eslami, Christoph Meinel, and Gerard De Melo. Pubmedclip: How much does clip benefit visual question answering in the medical domain? In Findings of the Association for Computational Linguistics: EACL 2023, pages 11511163, 2023. [13] Sabri Eyuboglu, Maya Varma, Khaled Kamal Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher Re. Domino: Discovering systematic errors with cross-modal embeddings. In International Conference on Learning Representations, 2022. [14] Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip), 2022. [15] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference on Learning Representations, 2021. [16] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn, 2017. [17] Zhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas Montine, and James Zou. visual-language foundation model for pathology image analysis using medical twitter. Nat. Med., 29(9):23072316, September 2023. [18] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. In Bernhard Schölkopf, Caroline Uhler, and Kun Zhang, editors, Proceedings of the First Conference on Causal Learning and Reasoning, volume 177 of Proceedings of Machine Learning Research, pages 336351. PMLR, 1113 Apr 2022. [19] Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. [20] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. If you use this software, please cite it as below. [21] Pavel Izmailov, Polina Kirichenko, Nate Gruver, and Andrew Wilson. On feature learning in the presence of spurious correlations. Advances in Neural Information Processing Systems, 35:3851638532, 2022. [22] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry. Distilling model failures as directions in latent space. In The Eleventh International Conference on Learning Representations, 2023. [23] JFHealthcare. Object-cxr - automatic detection of foreign objects on chest x-rays. 2020. [24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 49044916. PMLR, 1824 Jul 2021. [25] Nari Johnson, Ángel Alexander Cabrera, Gregory Plumb, and Ameet Talwalkar. Where does my model underperform? human evaluation of slice discovery algorithms, 2023. [26] Weixin Liang and James Zou. Metashift: dataset of datasets for evaluating contextual distribution shifts and training conflicts. In International Conference on Learning Representations, 2021. [27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision ECCV 2014, pages 740755, Cham, 2014. Springer International Publishing. [28] Evan Liu, Behzad Haghgoo, Annie Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In International Conference on Machine Learning, pages 67816792. PMLR, 2021. [29] Mazda Moayeri, Phillip Pope, Yogesh Balaji, and Soheil Feizi. comprehensive study of image classification model sensitivity to foregrounds, backgrounds, and visual attributes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1908719097, 2022. [30] Mazda Moayeri, Wenxiao Wang, Sahil Singla, and Soheil Feizi. Spuriosity rankings: Sorting data to measure and mitigate biases, 2023. [31] Nicolas M. Müller, Simon Roschmann, Shahbaz Khan, Philip Sperl, and Konstantin Böttinger. Shortcut detection with variational autoencoders, 2023. [32] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: De-biasing classifier from biased classifier. Advances in Neural Information Processing Systems, 33:2067320684, 2020. [33] Junhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin. Spread spurious attribute: Improving worst-group accuracy with spurious attribute estimation. In International Conference on Learning Representations, 2021. [34] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. September 2019. [35] Obioma Pelka, Sven Koitka, Johannes Rückert, Felix Nensa, and Christoph M. Friedrich. Radiology objects in context (roco): multimodal image dataset. In Danail Stoyanov, Zeike Taylor, Simone Balocco, Raphael Sznitman, Anne Martel, Lena Maier-Hein, Luc Duong, Guillaume Zahnd, Stefanie Demirci, Shadi Albarqouni, Su-Lin Lee, Stefano Moriconi, Veronika Cheplygina, Diana Mateus, Emanuele Trucco, Eric Granger, and Pierre Jannin, editors, Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis, pages 180189, Cham, 2018. Springer International Publishing. 12 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR, 1824 Jul 2021. [37] Pranav Rajpurkar, Jeremy Irvin, Aarti Bagul, Daisy Ding, Tony Duan, Hershel Mehta, Brandon Yang, Kaylie Zhu, Dillon Laird, Robyn L. Ball, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, and Andrew Y. Ng. Mura: Large dataset for abnormality detection in musculoskeletal radiographs, 2018. [38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28, pages 9199. Curran Associates, Inc., 2015. [39] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2020. [40] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. [41] Sahil Singla and Soheil Feizi. Salient imagenet: How to discover spurious features in deep learning? In International Conference on Learning Representations, 2022. [42] Sahil Singla, Besmira Nushi, Shital Shah, Ece Kamar, and Eric Horvitz. Understanding failures of deep networks via robust feature extraction. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021. Computer Vision Foundation / IEEE, 2021. [43] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Ré. No subclass left behind: Fine-grained robustness in coarse-grained classification problems. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1933919352. Curran Associates, Inc., 2020. [44] Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: the new data in multimedia research. Communications of the ACM, 59(2):6473, January 2016. [45] Ekin Tiu, Ellie Talius, Pujan Patel, Curtis Langlotz, Andrew Ng, and Pranav Rajpurkar. Expert-level detection of pathologies from unannotated chest x-ray images via self-supervised learning. Nat. Biomed. Eng., 6(12):13991406, December 2022. [46] Maya Varma, Jean-Benoit Delbrouck, Sarah Hooper, Akshay Chaudhari, and Curtis Langlotz. Villa: Finegrained vision-language representation learning from real-world data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [47] Maya Varma, Mandy Lu, Rachel Gardner, Jared Dunnmon, Nishith Khandwala, Pranav Rajpurkar, Jin Long, Christopher Beaulieu, Katie Shpanskaya, Li Fei-Fei, Matthew P. Lungren, and Bhavik N. Patel. Automated abnormality detection in lower extremity radiographs using deep learning. Nature Machine Intelligence, 1(12):578583, December 2019. [48] Julia Winkler, Christine Fink, Ferdinand Toberer, Alexander Enk, Teresa Deinlein, Rainer HofmannWellenhof, Luc Thomas, Aimilios Lallas, Andreas Blum, Wilhelm Stolz, et al. Association between surgical skin markings in dermoscopic images and diagnostic performance of deep learning convolutional neural network for melanoma recognition. JAMA dermatology, 155(10):11351141, 2019. [49] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. arXiv preprint arXiv:2109.01903, 2021. https://arxiv.org/ abs/2109.01903. [50] Shirley Wu, Mert Yuksekgonul, Linjun Zhang, and James Zou. Discover and cure: Concept-aware mitigation of spurious correlation. In ICML, 2023. [51] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: novel image dataset for benchmarking machine learning algorithms, 2017. 13 [52] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Largescale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 34853492, 2010. [53] Kai Yuanqing Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. In International Conference on Learning Representations, 2020. [54] Hanwen Xu, Naoto Usuyama, Jaspreet Bagga, Sheng Zhang, Rajesh Rao, Tristan Naumann, Cliff Wong, Zelalem Gero, Javier González, Yu Gu, Yanbo Xu, Mu Wei, Wenhui Wang, Shuming Ma, Furu Wei, Jianwei Yang, Chunyuan Li, Jianfeng Gao, Jaylen Rosemon, Tucker Bower, Soohee Lee, Roshanthi Weerasinghe, Bill Wright, Ari Robicsek, Brian Piening, Carlo Bifulco, Sheng Wang, and Hoifung Poon. whole-slide foundation model for digital pathology from real-world data. Nature, 630(8015):181188, June 2024. [55] Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang. Adversarial domain adaptation with domain mixup. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 65026509, 2020. [56] Yu Yang, Besmira Nushi, Hamid Palangi, and Baharan Mirzasoleiman. Mitigating spurious correlations in multi-modal models during fine-tuning. In International Conference on Machine Learning, 2023. [57] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-of-distribution robustness via selective augmentation. In Proceeding of the Thirty-ninth International Conference on Machine Learning, 2022. [58] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training, 2022. [59] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? In International Conference on Learning Representations, 2020. [60] Michael Zhang and Christopher Re. Contrastive adapters for foundation model group robustness. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [61] Michael Zhang, Nimit S. Sohoni, Hongyang R. Zhang, Chelsea Finn, and Christopher Ré. Correct-ncontrast: contrastive approach for improving robustness to spurious correlations, 2022. [62] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, Andrea Tupini, Yu Wang, Matt Mazzola, Swadheen Shukla, Lars Liden, Jianfeng Gao, Matthew P. Lungren, Tristan Naumann, Sheng Wang, and Hoifung Poon. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs, 2024. [63] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1679316803, June 2022."
        },
        {
            "title": "Contents",
            "content": "A Related Work Extended Details on Evaluation Settings Extended Details on RAVL Mitigation Extended Evaluations D.1 Extended Results for RAVL Stage 1 (Discovery) . . . . . . . . . . . . . . . . . . D.1.1 Extended Comparisons to Prior Approaches . . . . . . . . . . . . . . . . . D.1.2 Additional details for in-the-wild evaluations . . . . . . . . . . . . . . . . D.2 Extended Results for RAVL Stage 2 (Mitigation) . . . . . . . . . . . . . . . . . . D.3 Computational Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . Extended Discussion"
        },
        {
            "title": "A Related Work",
            "content": "15 16 17 18 18 18 21 22 23 Machine learning models often learn spurious correlations (also known as shortcuts) between image features and class labels. For instance, models have been shown to rely on the presence of chest tubes rather than disease features when identifying collapsed lungs in chest X-rays [34]; surgical skin markings when detecting melanoma from skin lesions [48]; and environmental features when performing object recognition tasks [3]. Models that learn spurious correlations will generalize poorly to real-world settings. Our work builds on several recent research directions for (i) discovering and (ii) mitigating spurious correlations. Discovering Spurious Correlations. In the unimodal setting, prior works have developed automated methods for identifying systematic errors resulting from learned spurious correlations in vision models. Using labeled validation set, these approaches utilize clustering algorithms [13, 43] or lightweight models [42, 22, 31] to identify subgroups of images with high error rates; for instance, in the example in Figure 1, images containing butterflies without flowers may be identified as one such subgroup. Given set of images in the identified subgroups, user can then identify the common features and rectify the data or model. However, recent work has suggested that it is often challenging for humans to interpret identified subgroups and accurately determine the shared features resulting in model failure [25]. Additionally, such methods often focus solely on identifying images with high error rates (e.g. butterflies without flowers) rather than identifying the specific class of features contributing to the error (e.g. flowers). related line of work has aimed to identify spurious features using human supervision [41] or external concept banks [50]. In the vision-language setting, Yang et al. use an external off-the-shelf object detector to annotate features [56]. Then, for each feature, the difference in zero-shot classification accuracy between images containing the feature and those without the feature is measured; high performance gaps are used to signal spurious features. However, the efficacy of this approach is reliant on the quality of the object detector and human-in-the-loop is used to verify results; also, as we show in this work, performance gaps alone are not always sufficient for discovering spurious features. Mitigating Spurious Correlations. There is line of work aiming to mitigate spurious correlations in the context of deep learning [61, 39, 32, 9, 28, 33, 21]. These works explore strategies like data augmentation [55, 59, 57, 22, 50] and instance upsampling [39, 43]. While these approaches have been explored widely in unimodal tasks [29, 53], mitigating spurious correlations in vision-language settings has not been extensively studied. Some previous works have studied this problem within the context of pretrained VLMs [60, 49, 1]; however, their setting differs markedly from the fine-tuned 15 VLM setting, where datasets are composed of image-text pairs with no class or subgroup labels. In the fine-tuned VLM setting, existing works predominantly operate at the global image-level [56], which is unlikely to be sufficient for mitigating fine-grained spurious correlations."
        },
        {
            "title": "B Extended Details on Evaluation Settings",
            "content": "We create 654 evaluation settings using data from two domains: (1) synthetic data (MNIST [11] and FashionMNIST [51]) and (2) real-world data (COCO [27]). Below, we provide implementation details for the four components included in each evaluation setting: 1. Predefined spurious correlation: We define spurious image feature and textual attribute pair (eeval, aeval). For MNIST and FashionMNIST, eeval represents red rectangle; aeval is generated from the set {zero, one, two, three, four five, six, seven, eight, nine} for MNIST and {t-shirt, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, ankle boot} for FashionMNIST. For COCO, we sample eeval and aeval from the list of annotated attributes. 2. Fine-tuning dataset: Vision-language fine-tuning datasets Deval are sampled from the training sets of MNIST, FashionMNIST, and COCO such that the presence of feature eeval is correlated with the presence of text attribute aeval as measured by Cramers V. For MNIST and FashionMNIST, we synthetically generate text captions by randomly sampling from the following pre-defined prompt templates: THE IMAGE SHOWS [CLASS LABEL], THE DIGIT APPEARS TO BE [CLASS LABEL], THERE IS AN IMAGE SHOWING [CLASS LABEL], and THE NUMBER IS [CLASS LABEL]. In order to reflect real-world settings where spurious features (e.g. skin markings in dermoscopic images [48]) may not be annotated in text, text captions in our synthetic settings solely refer to class labels and do not describe the spurious feature. For COCO, we use the provided text captions. 3. Fine-tuned VLM: We fine-tune each model on dataset Deval using single NVIDIA A100 GPU with an initial learning rate of 5e-5. We use batch size of 128 and train for 100 epochs with early stopping. We set the loss temperature as τ = 0.07. In line with prior works that explore the benefits of locked image-text training [2, 46], we freeze the text encoder and only learn weights for the image encoder. 4. Evaluation dataset: We construct zero-shot classification datasets Deval from the test sets of MNIST, FashionMNIST, and COCO. For MNIST and FashionMNIST, we generate region bounding boxes using equally-sized quadrants. For COCO, we use the ground-truth bounding boxes and associated labels. Evaluation datasets are sampled to ensure that correlation between aeval and eeval does not exist. For MNIST, we perform prompt ensembling for zero-shot classification using the following prompts: PHOTO OF THE NUMBER [CLASS LABEL]; THE DIGIT [CLASS LABEL]; AN IMAGE OF [CLASS LABEL]; [CLASS LABEL]. For FashionMNIST, we use the following prompts: PHOTO OF [CLASS LABEL]; THE [CLASS LABEL]; AN IMAGE OF [CLASS LABEL]; [CLASS LABEL]. For COCO, we use the following prompts: THERE IS [CLASS LABEL]; PHOTO OF THE [CLASS LABEL]; PHOTO OF [CLASS LABEL]; [CLASS LABEL]. Our 654 evaluation settings are summarized in Table 4. Datasets are licensed under CC BY, CC BY-SA, CC BY-NC, or MIT licenses. In Figure 4, we provide examples of both synthetic and real-world evaluation settings. Table 4: Evaluation settings. We evaluate our approach on 654 settings, divided across 2 data domains and 2 model initializations. Domain Model Initialization CLIP-RN50 CLIP-RN101 Synthetic Data Real-World Data 129 162 192 16 Figure 4: Example evaluation settings. Here, we provide examples of predefined spurious correlations, fine-tuning datasets, and evaluation datasets associated with synthetic evaluation setting (top row) and real-world evaluation setting (bottom row). The example synthetic evaluation setting consists of predefined spurious correlation between red rectangle (spurious image feature eeval) and nine (textual attribute aeval). This spurious correlation is visible in the vision-language fine-tuning dataset, where the presence of red rectangles and nines are strongly correlated, but not in the evaluation dataset. Similarly, the example real-world evaluation setting consists of predefined spurious correlation between person (spurious image feature eeval) and couch (textual attribute aeval). Again, this spurious correlation is visible in the vision-language fine-tuning dataset, where the presence of people and couches are strongly correlated, but not in the evaluation dataset."
        },
        {
            "title": "C Extended Details on RAVL Mitigation",
            "content": "In this section, we extend Section 4.1 by providing additional descriptions of our region-aware loss function. as the set of all spurious regions in the batch: Rs For batch B, we define Rs Ii in batch B, the first component of our region-aware loss function Li embedding similarity between non-spurious regions Rr Li the batch. We formulate Li = (cid:83) . For image is designed to maximize and assigned class label ˆyi; simultaneously, and other class labels in will minimize embedding similarity between non-spurious regions Rr IiB Rs as follows: Li = log σm(Rr ˆyj σm(Rr , ˆyi) , ˆyj) + (Rs B) (cid:80) , (5) where (Rs class labels as expressed below. Including this term in the denominator of Li embeddings of spurious regions away from correlated class labels. B) is penalty term that encourages dissimilarity between spurious features and correlated is meant to pull (Rs B) = (cid:88) rj Rs max ˆykB σ(rj, ˆyk) (6) The formula for Li includes two similarity functions: σ and σm. We define σ and σm as follows. Let represent region embedding function (associated with the image encoder of VLM M) and 17 let represent text embedding function (associated with the text encoder of VLM M). Then, for an arbitrary region a, the function (a) will generate region embedding (a) Rd with embedding dimension d. For an arbitrary class label b, g(b) will generate text embedding g(b) Rd. Given this notation, we establish the following definitions for σ and σm: σ(a, b) = exp(f (a), g(b) /τ ) σm(A, b) = exp(max aA (f (a), g(b) /τ )) (7) (8) R, the function σm(Rr In the loss term Li , ˆyi) will compute the maximum similarity between regions in Rr and class label ˆyi. We specifically use the maximum operation in this computation since there are likely to be regions included in Rr that do not reflect the class label; for instance, in the example provided in Figure 1, there may be non-spurious regions such as trees or leaves included in Rr , which do not align with the animal class labels. The maximum operation ensures that the similarity between at least one region in Rr and the class label should be high. The second component of our region-aware loss function Li is designed to maximize embedding similarity between non-spurious regions Rr will minimize embedding similarity between other regions in the batch and class label ˆyi. We formulate Li and assigned class label ˆyi; simultaneously, Li as follows: Li = log σm(Rr , ˆyi) + (cid:80)B j=1, ˆyj = ˆyi σm(Rr , ˆyi) σm(Rr , ˆyi) + (cid:80) . (9) rj Rs σ(rj, ˆyi)"
        },
        {
            "title": "D Extended Evaluations",
            "content": "D.1 Extended Results for RAVL Stage 1 (Discovery) In this section, we extend the results provided in Section 3.3 with additional evaluations of Stage 1 of RAVL. Our goal is to evaluate the ability of RAVL to discover fine-grained spurious correlations between image features and textual attributes. D.1.1 Extended Comparisons to Prior Approaches We implement RAVL according to the details provided in Section 3.1. RAVL includes clustering step that identifies groups of visually-similar regions. For all evaluation settings, we identify the optimal number of clusters by sweeping all cluster sizes ranging between 2 and 5; we then select the optimal number of clusters using Silhouette scores. We select these bounds to be larger than the class label set size by several multiples in order to ensure that clusters adequately separate distinct features. Prior works [13, 43] have also utilized overclustering approaches for this objective. We note that users can adjust the bounds based on the composition of their dataset; for instance, complex datasets with diverse features may require larger range. For MNIST and FashionMNIST, the size of the label set is 10; For COCO, the size of the label set ranges between 2 and 5. For all baselines, we utilize the official implementations provided by the authors. We adapt Domino, George, and Distilling Failures for our setting by providing region embeddings as input rather than image embeddings. In Figure 5, we provided an extended version of Figure 2. We demonstrate that RAVL consistently outperforms baselines across two domains (synthetic images and real images), two model initializations (CLIP-RN50 and CLIP-RN101), and four learned correlation strengths (measured by varying τeval {10, 20, 30, 40}). D.1.2 Additional details for in-the-wild evaluations Evaluations on scene classification: Here, we provided extended details on our in-the-wild evaluations performed on scene images (Section 3.3). We leverage ten off-the-shelf VLMs as our model M: CLIP-RN50, OpenCLIP-RN50, CLIP-RN101, OpenCLIP-RN101, CLIP-ViTB/32, OpenCLIP-ViTB/32, CLIP-ViTB/16, OpenCLIP-ViTB/16, CLIPViTL/14, and OpenCLIP-ViTL/14 [36, 20]. The four RN models utilize ResNet vision encoders 18 Figure 5: RAVL accurately identifies spurious correlations. Here, we provide an extended version of Figure 2, which demonstrates that RAVL consistently outperforms prior methods in discovering learned spurious correlations between image features and textual attributes. Here, we provide Precision@10 metrics for CLIP-RN50 model fine-tuned on synthetic data (129 settings) and realworld data (171 settings); CLIP-RN101 model fine-tuned on synthetic data (162 settings) and real-world data (192 settings); and an average across both model architectures. and the six ViT models utilize Vision Transformer backbones. The CLIP models were trained on proprietary dataset with 400M image-text pairs. OpenCLIP ResNet models were trained on YFCC15M [44], and OpenCLIP ViT models were trained on LAION2B [40]. We select SUN397 as our zero-shot classification dataset DV [52]. SUN397 consists of scene images from 397 classes. We use the test data from official partition number 1, which consists 19,850 images. We then use an off-the-shelf region proposal network [63] to identify candidate regions. For each VLM M, we perform 397-class zero-shot scene classification on SUN397. We use prompt ensemble consisting of two prompt templates as provided by CLIP [36]. Due to the large size of the zero-shot classification dataset DV , we perform clustering using the CLARA (Clustering for Large Applications) algorithm, which is an efficient implementation of K-Medoids, and fix the number of clusters as 2, which is 794 in this case. Evaluations on chest X-ray classification: Here, we provided extended details on our in-the-wild evaluations performed on medical images (Section 3.3). In recent years, range of vision [47, 54, 37] and vision-language [45, 5, 4, 62] models have been proposed for learning diagnostic patterns in medical images, and there is critical need for methods capable of identifying spurious correlations in this domain. Our goal is to determine if RAVL can effectively surface spurious correlations learned by real-world fine-tuned VLMs developed for medical image interpretation. We leverage two off-the-shelf variants of the PubMedCLIP model [12] as our VLM M: PubMedCLIPRN50 and PubMedCLIP-ViTB/32. The PubMedCLIP-RN50 model utilizes ResNet-50 vision encoder and was fine-tuned from the CLIP-RN50 model. The PubMedCLIP-ViTB/32 model utilizes Vision Transformer backbone for the vision encoder and was fine-tuned from the CLIP-ViTB/32 19 model. Both variants of PubMedCLIP are fine-tuned using ROCO, large radiology dataset consisting of images and captions collected from PubMed [35]. We select Object-CXR as our zero-shot classification dataset DV . Object-CXR is dataset of 10,000 frontal chest X-rays compiled from around 300 township hospitals in China [23]. Twelve radiologists with 1-3 years of experience annotated the images, identifying foreign objects within the lung field using bounding boxes, ellipses, or masks, excluding support devices. We retain only bounding boxes and exclude chest X-rays without annotations, resulting in 8,726 object annotations across 4,372 images. For our evaluations, we use the Object-CXR dev split, which includes 974 object annotations across 489 images. We assign image-level labels to the dataset using torchxrayvision [8], library that includes variety of pretrained chest X-ray models. Specifically, we use the XRVDENSENET121-DENSENET121-RES224-ALL pretrained model to produce multi-class labels for variety of diseases, including Enlarged Cardiomediastinum, Cardiomegaly, Lung Opacity, Lung Lesion, Edema, Consolidation, Pneumonia, Atelectasis, Pneumothorax, Pleural Effusion, and Fracture. disease is identified as present if it meets confidence threshold of 0.60. For each PubMedCLIP VLM M, we perform binary zero-shot classification of cardiomegaly in Object-CXR. Cardiomegaly is medical condition characterized by the presence of an enlarged heart. After performing manual search over the text prompt space, we identify CARDIOMEGALY and NORMAL as the prompts that lead to the highest zero-shot classification accuracy for both model variants. The PubMedCLIP-RN50 model achieves an overall zero-shot classification accuracy of 74.2, with an accuracy of 14.0 on the group of images with cardiomegaly and an accuracy of 91.9 on the group of images without cardiomegaly. The PubMedCLIP-ViTB/32 achieves an overall zero-shot classification accuracy of 39.4, with an accuracy of 80.4 on the group of images with cardiomegaly and an accuracy of 28.0 on the group of images without cardiomegaly. Interestingly, given the selected prompts, we note that the PubMedCLIP-RN50 and the PubMedCLIP-ViTB/32 models exhibit inverse trends, with PubMedCLIP-RN50 achieving higher performance on the class of images without cardiomegaly and PubMedCLIP-ViTB/32 achieving higher performance on the class of images with cardiomegaly. Given VLM and zero-shot classification dataset Deval , we apply RAVL in order to surface learned spurious correlations. Similar to our controlled evaluations on synthetic datasets, we perform K-Medoids clustering with the number of clusters ranging from 20 to 50. The optimal number of clusters is selected using Silhouette distance; we use 24 clusters for PubMedCLIP-RN50 and 20 clusters for PubMedCLIP-ViTB/32. The final cluster performance gap metric Gc associated with the top-ranked spurious feature cluster is 0.041 and 0.119 for the PubMedCLIP-RN50 and PubMedCLIP-ViTB/32 models respectively. Figure 6: RAVL surfaces spurious correlations in off-the-shelf VLMs. Here, we extend Figure 3 with additional examples of spurious correlations discovered by RAVL in off-the-shelf-VLMs. Extended Results: In Figure 6, we extend the qualitative results provided in Figure 3 with additional examples of spurious correlations surfaced by RAVL in off-the-shelf VLMs. We make the following observations: For the OpenCLIP ViT-L/14 model, RaVL surfaces feature cluster consisting of green plants and fences. We observe performance gap of 18.3 points between images with class 20 label outdoor chicken coop that contain the RaVL-identified feature and those that do not contain the feature. This suggests that the OpenCLIP ViT-L/14 model can better classify an outdoor chicken coop scene when green plants and fences are present. For the CLIP ViT-B/32 model, RaVL surfaces feature cluster consisting of people. We observe performance gap of 24.3 points between images with class label pub (indoor) that contain the RaVL-identified feature and those that do not contain the feature. This suggests that the CLIP ViT-B/32 model can better classify pub (indoor) scene when people are present. For the OpenCLIP ResNet-101 model, RaVL surfaces feature cluster consisting of chairs. We observe performance gap of 23.3 points between images with class label restaurant patio that contain the RaVL-identified feature and those that do not contain the feature. This suggests that the OpenCLIP ResNet-101 model can better classify restaurant patio scenes when chairs are present. D.2 Extended Results for RAVL Stage 2 (Mitigation) We train model Mnew using single NVIDIA A100 GPU with an initial learning rate of 5e-5. We use batch size of 128 and train for 100 epochs with early stopping. We set the loss temperature as τ = 0.07 and use λ = 0.8 in loss function L. In line with prior works that utilize locked image-text fine-tuning [2, 46], we freeze the text encoder and solely learn weights for the image encoder. We generate candidate regions for the fine-tuning dataset Deval using region proposal network with identical settings to prior work [63]. Below, we provide additional implementation details for the five mitigation baselines we explore in this study. Since there are limited number of existing approaches designed for mitigating spurious correlations in fine-tuned VLMs, we adapt several existing methods for our setting in order to train model Mnew: Standard VLM Fine-Tuning: We perform standard VLM fine-tuning with the original loss function LCL used to train model M. In our experiments, LCL is the CLIP objective [36]. Upsampled VLM Fine-Tuning: We perform VLM fine-tuning with the original loss function LCL used to train model M. In our experiments, LCL is the CLIP objective [36]. We utilize weighted sampler to upsample minority groups during training; class and subgroup labels are derived from Stage 1 of RAVL as detailed in Section 4.1. VL-ERM: Since empirical risk minimization (ERM) is traditionally used in unimodal classification settings, we adapt ERM for our multimodal setting by incorporating an extra contrastive vision-language objective function; this loss function is intended to ensure that VLM Mnew learns image-text relationships during training. Specifically, the final loss function during training is LV LERM = λLCL + (1 λ)LERM . Here, LCL takes the form of the original loss function used to train model M; in our experiments LCL is the CLIP objective [36]. We set λ = 0.8. During training, we apply ERM to zero-shot classification logits computed using image embeddings and text embeddings of class labels. We utilize weighted sampler to upsample minority subgroups; class and subgroup labels are derived from Stage 1 of RAVL as detailed in Section 4.1. VL-GDRO [39]: Since GDRO is traditionally used in unimodal classification settings, we adapt GDRO for our multimodal setting by incorporating an extra contrastive visionlanguage objective function; this loss function is intended to ensure that VLM Mnew learns image-text relationships during training. Specifically, the final loss function during training is LV LGDRO = λLCL + (1 λ)LGDRO. Here, LCL takes the form of the original loss function used to train model M; in our experiments LCL is the CLIP objective [36]. We set λ = 0.8. During training, we apply GDRO to zero-shot classification logits computed using image embeddings and text embeddings of class labels. In line with standard practice, we utilize weighted sampler to upsample minority subgroups; class and subgroup labels are derived from Stage 1 of RAVL as detailed in Section 4.1. Spurious-Aware Mitigation [56]: Spurious-aware mitigation aims to address spurious correlations in VLMs using combination of five loss functions: one CLIP objective function, two contrastive image objective functions meant to address spurious correlations in the image space, and two contrastive language objective functions meant to address spurious 21 Table 5: RAVL effectively mitigates spurious correlations across various model initializations. Here, we provide an extended version of Table 3 with breakdown of results by model initialization (CLIP-RN50 vs. CLIP-RN101). Our results demonstrate that RAVL consistently outperforms prior methods in mitigating spurious correlations. We report mean Image Overall (Img. Overall), Image Worst Group (Img. WG), Region Overall (Reg. Overall), and Region Worst Group (Reg. WG) metrics across our real-world evaluation settings. Method Discovery Precision@10> 0.6 Discovery Precision@10> 0.8 Img. Overall Img. WG Reg. Overall Reg. WG Img. Overall Img. WG Reg. Overall Reg. WG 0 Standard FT 5 - C Upsampled FT VL-ERM VL-GDRO Spurious-Aware RAVL (Ours) 1 Standard FT 0 1 - C Upsampled FT VL-ERM VL-GDRO Spurious-Aware RAVL (Ours) 64.2 65.2 66.0 66.8 67.4 67.9 63.9 67.4 70.5 70.5 71.3 71.0 35.8 36.7 30.0 31.6 31.6 36.9 28.9 38.4 33.5 34.9 34.8 40.4 73.2 73.7 73.4 74.1 74.0 77.8 71.3 74.6 77.0 76.5 78.1 79. 50.1 51.0 45.2 45.8 45.2 55.4 45.0 52.8 53.4 53.1 53.9 59.2 64.4 65.2 65.4 66.1 66.2 67.8 64.7 67.8 70.8 70.5 71.2 71.8 36.0 38.0 28.9 29.6 28.2 38.6 27.6 37.5 32.2 32.1 32.4 42. 74.3 74.3 73.7 74.7 74.5 79.0 72.0 75.0 77.4 76.8 78.4 79.8 51.8 52.3 45.4 46.3 45.2 56.5 44.4 53.2 54.0 54.1 53.9 59.8 correlations in the text space. We note that Spurious-Aware Mitigation was explicitly designed for the fine-tuned VLM setting. We follow the implementation of Spurious-Aware Mitigation provided by [56]. In our work, since we solely fine-tuned the vision encoders of VLMs M, we use version of Spurious-Aware Mitigation with the CLIP objective function and two contrastive image objective functions. Prior works on model robustness predominantly evaluate model performance using image worstgroup scores [39]. In addition to image worst-group accuracy, we also report region overall and region worst-group accuracies, which evaluate the extent to which the VLM understands fine-grained features. Region-level accuracies are computed by performing zero-shot classification with region embeddings and comparing predicted labels to the ground-truth region-level labels provided in the zero-shot classification dataset. In Table 5, we provide an extended version of Table 3 with breakdown of results by model initialization (CLIP-RN50 and CLIP-RN101). We demonstrate that RAVL consistently outperforms prior methods across both model initializations. In Table 6, we provide an extended version of Table 3 with breakdown of results by the learned correlation strength of the original VLM M. RAVL consistently outperforms prior methods across four correlation strengths τeval {10, 20, 30, 40}. D.3 Computational Complexity Analysis In this section, we provide an analysis of the computational complexity of RAVL. RAVL is computationally inexpensive; in particular, the RAVL discovery stage can be run efficiently on CPU and the mitigation stage adds only small computational overhead. Below, we provide an analysis of computational complexity for each stage of RAVL. Computational complexity analysis of RAVL Stage 1: The discovery stage of RAVL is specifically designed to be run on labeled validation dataset DV ; in real-world settings, validation datasets are often relatively small in size due to the human effort needed for securing labels, rendering this stage as computationally inexpensive for diverse applications. Even if the validation dataset is large in size, RAVL operates efficiently as follows: First, RAVL preprocesses images by decomposing each image into candidate regions; there are variety of ways in which user can decompose an image into regions, such as by using equal-sized segments (e.g. quadrants) or running inference with region proposal networks (RPNs). Both methods are inexpensive and only need to be run once in an offline manner. Similar approaches have been applied to large-scale datasets in prior work [63]. Then, embeddings need to be generated for each region, which can be done by utilizing VLM for inference (forward passes only). Across set of 10 FashionMNIST and COCO 22 Table 6: RAVL effectively mitigates spurious correlations across learned correlation strengths. Here, we provide an extended version of Table 3 with breakdown of results by the learned correlation strength (τeval {10, 20, 30, 40} of the original model M. We use the subset of 106 evaluation settings where RAVL Stage 1 Precision@10 is greater than 0.8. Our results demonstrate that RAVL consistently outperforms prior methods in mitigating spurious correlations across various correlation strengths and model initializations. We report mean Image Worst Group (Img. WG) and Region Worst Group (Reg. WG) metrics across our real-world evaluation settings. We note that there are no valid evaluation settings for CLIP-RN101 when the learned correlation strength τeval of the original model is set to 40. Method τeval = 10 τeval = 20 τeval = τeval = 40 Img. WG Reg. WG Img. WG Reg. WG Img. WG Reg. WG Img. WG Reg. WG 0 Standard FT 5 - C Upsampled FT VL-ERM VL-GDRO Spurious-Aware RAVL (Ours) 1 Standard FT 0 1 - C Upsampled FT VL-ERM VL-GDRO Spurious-Aware RAVL (Ours) 36.0 38.0 28.9 29.6 28.2 38.6 27.6 37.5 32.2 32.1 32.4 42.2 51.8 52.3 45.4 46.3 45.2 56.5 44.4 53.2 54.0 54.1 53.9 59.8 29.0 27.7 23.3 22.7 22.2 35.3 26.4 36.0 32.1 30.7 30.4 39. 43.8 43.5 38.8 37.1 37.9 56.8 43.1 49.4 51.5 52.8 51.2 56.4 31.4 33.4 25.7 28.6 24.6 41.0 17.3 25.6 18.1 16.6 19.3 28.8 46.6 47.3 38.3 37.6 38.4 60.2 35.1 40.0 42.5 45.3 48.3 54. 26.0 33.4 22.9 26.4 17.1 38.0 39.5 46.5 32.5 28.2 30.6 53.5 evaluation settings, we observe embedding generation to take mean of 24.5 seconds on single A100 GPU. Finally, given candidate regions and corresponding embeddings, the remainder of the RaVL discovery procedure (clustering and computation of metrics) can be run completely on CPU. Across set of 10 evaluation settings on COCO and FashionMNIST, we observe that clustering and computation of metrics require mean of 3.4 seconds to run. Computational complexity analysis of RAVL Stage 2: The mitigation stage of RAVL requires finetuning VLM Mnew. Across set of 10 evaluation settings on COCO and FashionMNIST, we observe that the inclusion of our fine-grained region-aware loss function at this stage adds an average of 0.15 seconds per training step (on single A100 GPU) in comparison to the original fine-tuning procedure for M."
        },
        {
            "title": "E Extended Discussion",
            "content": "Societal Impact: The goal of our work is to improve robustness of fine-tuned VLMs to spurious correlations. As VLMs become more commonplace in society, we hope that our approach can enable users to better detect and mitigate model failures prior to deployment. We also note that our work includes series of evaluations on medical images; rigorous clinical testing is necessary before robustness approaches are deployed in healthcare settings. Limitations: In line with prior works in vision-only and vision-language settings, our method is specifically designed to surface and mitigate local, fine-grained spurious features. There may be some sources of spurious signal that do not manifest in this way; for instance, features like image brightness or gender can be considered global features, where the spurious signal is not localized to particular image region. Our approach is not designed for these global spurious features. Rather, our problem setting is inspired by the many real-world, practical examples of region-level spurious features that have been demonstrated in literature to affect model performance, such as image-level markings in dermoscopic images [48], medical devices in radiographs [34], and text markers in medical images [10]."
        }
    ],
    "affiliations": [
        "Stanford University",
        "Hugging Face"
    ]
}