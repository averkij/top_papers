{
    "paper_title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
    "authors": [
        "Yucheng Li",
        "Huiqiang Jiang",
        "Qianhui Wu",
        "Xufang Luo",
        "Surin Ahn",
        "Chengruidong Zhang",
        "Amir H. Abdi",
        "Dongsheng Li",
        "Jianfeng Gao",
        "Yuqing Yang",
        "Lili Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench."
        },
        {
            "title": "Start",
            "content": "Preprint. Under review. SCBENCH: KV CACHE-CENTRIC ANALYSIS OF LONG-CONTEXT METHODS Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu Microsoft Corporation, University of Surrey yucheng.li@surrey.ac.uk,{hjiang,yuqyang}@microsoft.com https://aka.ms/SCBench"
        },
        {
            "title": "ABSTRACT",
            "content": "Long-context Large Language Models (LLMs) have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBENCH (SharedContextBENCH), comprehensive benchmark for evaluating long-context methods from KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, and 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With SCBench, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs (Codestal-Mamba), Mamba-Attention hybrids (Jamba-1.5-Mini), and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on six Transformer-based long-context LLMs: Llama-3.1-8B/70B, Qwen2.5-72B/32B, Llama-3-8B-262K, and GLM-4-9B. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. 4 2 0 2 3 1 ] . [ 1 9 1 3 0 1 . 2 1 4 2 : r Figure 1: KV Cache lifecycle. Prior benchmarks focus on single-request, while real-world applications reuse KV cache across requests. We propose SCBench and categorize long-context methods into KV Cache Generation, Compression, Retrieval, and Loading from KV-cache-centric perspective. Work during internship at Microsoft. Corresponding author. 1 Preprint. Under review."
        },
        {
            "title": "INTRODUCTION",
            "content": "Long-context capability is becoming standard for Large Language Models (LLMs), with many of them supporting context windows ranging from 128K to 10M tokens (Reid et al., 2024; Lieber et al., 2024; Dubey et al., 2024; Gradient, 2024). These extended context windows unlock wide range of real-world applications, such as repository-level code understanding and debugging (Bairi et al., 2024; Park et al., 2023; Liu et al., 2024c; Jimenez et al., 2024), long-document questionanswering (Caciularu et al., 2023; Li et al., 2024b), many-shot in-context learning (Agarwal et al., 2024), and long-generation Chain-of-Thought (CoT) reasoning (OpenAI, 2024a; Snell et al., 2024). Despite the benefit, Long-context inputs also present unique challenges for LLM inference due to high computational costs and memory demands. This has led to the development of efficient long-context solutions leveraging sparsity in various stage of KV cache. In this paper, we introduce an unified analysis framework for efficient long-context methods in KV cache centric perspective, consisting of the four essential stages of KV cache: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, and 4) KV cache loading. First, KV cache generation, aka prefilling, processes the input prompt and produce the KV cache to be used in decoding. In this stage, sparse attention methods are proposed to reduce the complexity of the attention operation (Child et al., 2019; Beltagy et al., 2020; Jiang et al., 2024). Second, KV cache compression techniques prune KV states to reduce the memory costs in decoding (Xiao et al., 2024b; Li et al., 2024c). Third, KV cache retrieval aims to skip KV cache generation of an incoming request, and instead retrieves and reuses KV cache from history KV cache pool for more efficient inference (Zheng et al., 2024; Yao et al., 2024a). At last, KV cache loading aims to load only partial of the KV cache for each decoding step to save the memory and computing cost (Tang et al., 2024; Liu et al., 2024b). However, these methods are only evaluated on single-request benchmarks (Hsieh et al., 2024; Zhang et al., 2024a; Kamradt, 2023; Li et al., 2024a), which fail to covers the full lifecycle of KV cache in real applications. Typically, real-world applications often require reusing prompt memory (i.e., KV cache) and involving multiple requests or multi-round interactions (Qin et al., 2024). The reuse of KV cache, known as prefix caching, is already crucial component in popular inference frameworks (Zheng et al., 2024; vLLM, 2024) and used by LLM providers (Gemini, 2024; Claude, 2024; OpenAI, 2024b; Azure, 2024). In addition, testing with multiple requests is especially crucial for the long-context methods mentioned earlier, as many achieve efficiency through query-conditioned compression. For instance, Arora et al. (2024) reports that Mambas compression of previous information based on the current query can prevent it from answering follow-up queries. To address this gap, we introduce SCBench, benchmark designed to evaluate efficient long-context methods that covers the full lifecycle of KV cache in real-world scenarios, particularly for shared context and multi-round interactions where KV Cache is reused for follow-up queries. As shown in Fig. 2b, SCBench assesses four key long-context abilities across 12 tasks with two shared context modes. Each test example includes shared context and multiple follow-up queries. The four long-context capabilities and their corresponding tasks are: 1. String Retrieval Capability: fundamental requirement for long-context LLMs is retrieving relevant context with exact matches from long inputs. We extend previous retrieval tasks like NIAH and Multi-NIAH (Kamradt, 2023; Hsieh et al., 2024) by introducing three comprehensive (a) Two Shared Context Modes (b) Overview of SCBench Figure 2: Long-context tasks often involve contexts sharing, e.g., multi-turn dialogues, multi-step reasoning, and repository-level tasks. (a) Illustration of two common shared-context patterns. (b) Overview of tasks and scenarios covered by our benchmark, encompassing four categories of longcontext abilities and two shared-context modes. 2 Preprint. Under review. (a) Performance Across Different Requests (b) Performance in Different Abilities Figure 3: Overview of performance results for SCBench. (a) Performance trends of various longcontext methods across multiple requests. Methods with O(n) memory cost in decoding show improving performance as requests increase. In contrast, methods with sub-O(n) KV cache in decoding, like KV cache dropping methods, perform well only in the first request. (b) Specific performance of different long-context methods across various long-context capability tasks. All evaluated long-context methods exhibit some loss in Retrieval capability while largely maintaining Global Information processing capability. string retrieval tasks: key-value retrieval, prefix-suffix retrieval, and multi-hop retrieval, measuring capability at different levels of granularity. 2. Semantic Retrieval Capability: Real-world applications often require long-context LLMs to understand semantic meaning before succeeding in retrieval. We considered various semantic retrieval scenarios across different domains, building four distinct tests: RepoQA (Liu et al., 2024c) and long-form QA (covering English, Chinese, and multiple-choice questions) (Zhang et al., 2024a). 3. Global Information Capability: We also assess the capability of long-context LLMs to process and aggregate global information through three tasks: many-shot in-context learning (Agarwal et al., 2024), summarization, and long array statistics (Zhang et al., 2024a). 4. Multi-tasking Capability: In real applications, LLMs often handle multiple tasks with shared long-context input. Our benchmark evaluates this capability through two tasks: RepoQA with NIAH and summarization with KV retrieval. In addition, as shown in Fig. 2a, our benchmark includes two typical shared context modes: Multi-turn Mode, where the context is cached within single session, and Multi-request Mode, where it is cached across multiple sessions. With SCBench, we conduct an extensive KV cache-centric analysis in the four stages as shown in Fig. 1 (details in Sec. 2). Specifically, we evaluate 13 long-context methods across four stages and eight categories on eight opensource long-context LLMs, including Llama3.1-8B/70B (Dubey et al., 2024), Qwen2.572B/32B (Team, 2024), Llama-3-8B-262K (Gradient, 2024), GLM-4-9B-1M (GLM et al., 2024), Codestal Mamba (team, 2024), and Jamba-1.5mini (Lieber et al., 2024). These methods span gated linear RNNs (e.g., Codestal Mamba), hybrid models (e.g., Jamba-1.5), sparse attention (e.g., A-shape, Tri-shape, MInference (Jiang et al., 2024)), prompt compression (e.g., LLMLingua-2 (Pan et al., 2024)), KV cache dropping (e.g., StreamingLLM (Xiao et al., 2024b), SnapKV (Li et al., 2024c)), KV cache quantitation (e.g., Figure 4: Performance of various long-context methods at different compression rates on SCBench using Llama-3.1-8B (Dubey et al., 2024). 3 Preprint. Under review. KIVI (Liu et al., 2024e)), semantic retrieval (e.g., CacheBlend (Yao et al., 2024a)), and KV cache loading (e.g., Quest (Tang et al., 2024), RetrievalAttention (Liu et al., 2024b)), as detailed in Table 1. Additionally, we introduce Tri-shape, novel, training-free sparse attention method that demonstrates improved first-turn performance in our evaluations. Our experimental results reveal the following insights: 1) Sub-O(n) memory is almost infeasible in multi-turn decoding, as shown in Fig. 3. Sparse decoding methods (sub-O(n) memory) perform well on the first query but lose accuracy in subsequent requests. In contrast, sparse encoding methods (O(n) memory with O(n2)computation during pre-filling) can approximate full attention accuracy across multiple queries. 2) Task performance shows varying decline trends, as illustrated in Fig. 3b. Sparse KV cache methods excel in tasks requiring global information, whereas O(n) memory is essential for tasks involving exact match retrieval. 3) All long-context methods experience performance degradation as the budget decreases, as shown in Fig. 4. However, sub-O(n) memory methods exhibit significant performance drop at 1/4 compression rate. Methods such as RetrievalAttention and KIVI, which maintain O(n) memory with sparse decoding, sustain higher performance even at higher compression rates. 4) Long-generation scenarios exhibit distribution shift issues, as generation length and the number of rounds increase, the importance distribution of the KV cache changes significantly. This out-of-distribution (OOD) issue leads to performance degradation, even for O(n) memory methods like RetrievalAttention, as observed in extended tasks, as shown in Fig. 3. Our contributions are as follows: We propose new benchmark, SCBench, to evaluate long-context methods on multi-round and multi-request scenarios in two typical KV cache reuse scenarios, providing more realistic assessment. We design an extensive set of downstream tasks, covering four long-context capabilities across 12 subtasks in various domains. We systematically categorize long-context methods from KV-cache-centric perspective and evaluate 13 different long-context methods (including our newly proposed sparse attention method, Tri-shape) on eight state-of-the-art open-source long-context LLMs using SCBench. Our comprehensive analysis reveals key insights into the effects of sparsity in encoding and decoding, task complexity, and more."
        },
        {
            "title": "2 A KV CACHE-CENTRIC PERSPECTIVE ON LONG-CONTEXT METHODS",
            "content": "Table 1: We evaluated long-context methods on SCBench, where represents the token size of the input prompt and represents the generation token size, with m. Methods Taxonomy Stage P-stage Efficient D-stage Efficient KV Cache Size Prefilling Complexity Decoding Complexity Codestral Mamba (team, 2024) Gated Linear RNN Jamba (Lieber et al., 2024) Gated Linear RNN + Full Attention ❶ ❶ LLMLingua-2 (Pan et al., 2024) Prompt Compression ❶ A-shape (Xiao et al., 2024b) Tri-shape MInference (Jiang et al., 2024) StreamingLLM (Xiao et al., 2024b) SnapKV (Li et al., 2024c) PyramidKV (Cai et al., 2024) KIVI (Liu et al., 2024e) Sparse Attention KV Cache Dropping KV Cache Quantitation CacheBlend (Yao et al., 2024a) KV Cache Retrieval Quest (Tang et al., 2024) RetrievalAttention (Liu et al., 2024b) KV Cache Loading ❶ ❷ ❷ ❸ ❹ (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) O(k) O(n) O(kn) O(km) O(n2) O(nm) O(αn) O(α2n2) O(αnm) O(n) O(kn) O(nm) O(k) O(n2) O(km) O(n) O(n) O(n) O(n2) O(n2) O(n2) O(nm) O(nm) O(km) Recently, series of works (Gu & Dao, 2024; Xiao et al., 2024b; Jiang et al., 2024) have explored various strategies to reduce the inference cost of long-context LLMs, enabling their application (Jimenez 4 Preprint. Under review. et al., 2024; OpenAI, 2024a) to downstream tasks at lower computational expense. In long-context LLM inference, the KV cache plays pivotal role by effectively reducing the computational overhead during the decoding phase. This importance has led to the development of numerous system-level optimizations (Sheng et al., 2023; Qin et al., 2024) focused on KV cache management and scheduling. In this work, we propose novel perspective: these long-context methods can be viewed as optimizations centered around the KV cache at different stages. Specifically, we introduce KV-cache-centric framework that systematically categorizes long-context methods into four stages: KV Cache Generation, Compression, Retrieval, and Loading, as illustrated in Fig. 1. Specifically, the four stages of the KV-cache-centric framework are defined as follows: 1. KV Cache Generation: This stage optimizes the efficient generation of KV cache during inference. Techniques include sparse attention (e.g., A-shape, Tri-shape, MInference (Jiang et al., 2024)), SSM or hybrid approaches (e.g., Mamba (Gu & Dao, 2024; Dao & Gu, 2024; Lieber et al., 2024)), and prompt compression (e.g., LLMLingua-2 (Pan et al., 2024)). 2. KV Cache Compression: After generation, the KV cache is compressed before being stored. Methods include KV cache dropping (e.g., StreamingLLM (Xiao et al., 2024b), SnapKV (Li et al., 2024c)) and KV cache quantization (e.g., KIVI (Liu et al., 2024e)). 3. KV Cache Retrieval: Relevant KV cache blocks are retrieved from storage pool based on the requests prefix, reducing time-to-first-token (TTFT). Approaches include semantic retrieval methods like CacheBlend (Yao et al., 2024a). 4. KV Cache Loading: This stage dynamically loads the KV cache and computes sparse attention, from KV cache storage (e.g., VRAM, DRAM, SSD, or RDMA) to GPU on-chip SRAM, including Quest (Tang et al., 2024), RetrievalAttention (Liu et al., 2024b), and MagicPIG (Chen et al., 2024). In our work, we evaluate all four stages of 13 long-context methods, as shown in Table 1. Additionally, we list the KV cache size, pre-filling stage complexity, decoding stage complexity, and whether efficient operations are performed during the pre-filling and decoding stages for each method. Tri-shape Sparse Attention: We also introduce novel training-free sparse attention method, Tri-shape, with improved first-turn accuracy, as shown in Fig. 5. Specifically, in addition to retaining the sink token and local window regions preserved by A-shape, Tri-shape also retains the last window query region, forming triangular pattern for sparse attention during the pre-filling stage. The motivation arises from the observation from our SCBench that A-shape with dense decoding exhibits significant performance improvement after multiple requests. Tri-shape can notably enhance performance in both turn-0 and multirequest scenarios, as detailed in Sec. 4. Furthermore, it preserves the ability of LLMs to follow instructions, as demonstrated in the case study in F. This is an example of how our SCBench can provide insights for novel algorithm innovation. Notably, some recent concurrent works (Acharya et al., 2024) have also proposed similar patterns to accelerate the long-context pre-filling stage. Figure 5: The sparse attention methods framework."
        },
        {
            "title": "3 BENCHMARK BUILDING",
            "content": "SCBench comprises 12 tasks covering four long-context abilities: string retrieval, semantic retrieval, global information processing, and multi-tasking, across two shared context modesmulti-turn and multi-request. These tasks span various domains, including code, retrieval, question answering, summarization, in-context learning, multi-hop tracing, and multi-tasking, as shown in Fig. 2b. In total, SCBench includes 931 multi-turn sessions with 4,853 queries, averaging 5 turns per session. Task statistics are provided in Table 2, with examples and configurations in Table 3. Below, we detail the construction of our benchmark. 5 Preprint. Under review. Table 2: Overview of SCBench tasks. Task Description Capability Avg. Input Length Avg. Output Length #Sessions / #Turns Key-value retrieval from many key-value pairs Find string with specific prefix and suffix in dict Tracking variables assignment in long input Functions retrieval from GitHub repo English Question Answering Chinese Question Answering English Multi-Choice Questions Retr.KV Retr.Prefix-Suffix Retr.MultiHop Code.RepoQA En.QA Zh.QA En.MultiChoice Math.Find ICL.ManyShot En.Sum Mix.Sum+NIAH Multi-tasking of En.Sum and Needle in Haystack Mix.RepoQA+KV String Retrieval String Retrieval String Retrieval Semantic Retrieval Semantic Retrieval Semantic Retrieval Semantic Retrieval Math computation tasks within long sequence arrays Global Information Global Information Global Information Multi-tasking Multi-tasking Hundreds-shot in-context learning Summarize doc given multiple docs as input Multi-tasking of RepoQA and KV retrieval Total - - 3.1 LONG-CONTEXT TASK DETAILS 125K 112K 124K 65K 198K 1.5M 188K 120K 22K 104K 105K 68K 227K 943 914 410 6,058 272 322 215 172 975 1,170 3,441 5,318 1,684 100/500 100/500 90/450 88/440 69/351 35/189 58/299 100/240 54/270 79/350 70/560 88/ 931/4,853 String Retrieval The most fundamental requirement for long-context LLMs is the ability to identify and retrieve information relevant to specific query from lengthy, potentially noisy input. To evaluate this, string retrieval tasks are widely used, where models must retrieve specific string based on given conditions (Hsieh et al., 2024; Zhang et al., 2024a). Our benchmark incorporates complexity analysis, similar to approaches used in algorithmic problem-solving, such as LeetCode, to design three distinct tasks with varying levels of difficulty. Additionally, by varying the position of the target string, our benchmark further evaluates how well models utilize the full extent of their claimed context window (Kamradt, 2023). (i) Retrieve.KV: Given large JSON object containing numerous key-value pairs, the models must accurately retrieve the value corresponding to specified key (Liu et al., 2024d). The random KVs in this task present significant challenges for long-context LLMs, as the input is often incompressible, requiring strict O(n) space to store. This makes it particularly useful for testing the fuzziness of memory in long-context methods, especially in KV Cache usage. In each session, five KV pairs are retrieved, with the target KVs evenly distributed across the full length of the input. (ii) Retrieve.Prefix-Suffix: Given large list of variable-length strings, the models must accurately retrieve string with specific prefix and suffix. This task is particularly challenging, as the models need to implement complex functions to match both the prefix and suffix (similar to prefix tree, with computational cost of O((cid:80) wi 2), where wi represents the length of the i-th string1). The presence of distractors that share either the prefix or suffix, but not both, prevents models from relying on simple lookup mechanisms or induction heads (Olsson et al., 2022) to solve the task effectively. (iii) Retrieve.MultiHop: This task, first proposed in RULER (Hsieh et al., 2024), is designed to evaluate the multi-hop tracing capabilities of LLMs within long input prompt. It requires models to capture and memorize changes in key information from the input context, making it ideal for testing long-context methods in KV cache reuse. Five multi-hop variable assignment chains are embedded throughout the context, and each turn in the test session requires the models to retrieve the exact multi-hop chain, i.e., all variables assigned to specific value. Semantic Retrieval In addition to string retrieval, many real-world long-context applications require semantic understanding beyond simple string matching, such as retrieving function based on textual descriptions or answering questions from long document. These tasks are crucial in SCBench, as lossy long-context methods may struggle to abstract or comprehend information in multi-request scenarios. (i) Code.RepoQA: This task requires the model to retrieve specific function (including the function name, input parameters, and full implementation) from long chunk of source code based on precise natural language description. Unlike the original RepoQA benchmark (Liu et al., 2024c), our inputs are extended to 64K tokens, with target functions evenly selected based on their position in the codebase. The function descriptions were generated using GPT-4 based on the functions themselves. Additionally, we expanded the range of repositories and programming languages in our test to include 1https://leetcode.com/problems/prefix-and-suffix-search/ 6 Preprint. Under review. Table 3: Task examples and configurations in SCBench. We use different colors to highlight the questions, answers, and distractors in our examples. Source Configuration Example Task Retr.KV Lost in the Middle (Liu et al., 2024d) Retr.Prefix-Suffix Ours Retr.MultiHop RULER (Hsieh et al., 2024) num kv pairs = 2500 len of key & value = 36 metric = Accuracy size of dict = 6000 len of string = [65, 123) metric = Accuracy num chains = 2 num hops = 2 metric = Accuracy Code.RepoQA RepoQA (Liu et al., 2024c) func description from GPT-4 metric = Pass@1 En.QA Zh.QA InfiniteBench (Zhang et al., 2024a) ground_truth from human metric = Accuracy En.MultiChoice InfiniteBench (Zhang et al., 2024a) ground_truth from human metric = Accuracy Input: {<key #1>: <value #1>, ..., <key #100>: <value #100>} Turn 1: The value of the <key #1> is? Answer 1: ...<value #1>... Turn 2: The value of the <key #20> is? Answer 2: ...<value #20>... Turn 3: The value of the <key #40> is? Answer 3: ...<value #40>... Input: Dictionary = [<str #1>, <str #2>, ..., <str #100>] Turn 1: Prefix: <px #1>; Suffix: <sx #1>. The word with both prefix and suffix from the dict is? Answer: <str> Turn 2: Prefix: <px #2>; Suffix: <sx #2>. Answer: <str> Input: VAR X1 = 12345 ...... VAR Y1 = 54321 .....<noise> VAR X2 = X1 ...... VAR Y2 = Y1 ......<noise> VAR X3 = X2 ...... VAR Y3 = Y2 ......<noise> Turn 1: Variables that are assigned to 12345? Answer 1: X1 X2 X3 Turn 2: Variables that are assigned to 54321? Answer 1: Y1 Y2 Y3 Input: <func 1> + <func 2> + ... + <func 100> Turn 1: <description of func 1>. Answer 1: <func 1> Turn 2: <description of func 20>. Answer 2: <func 20> Input: Read the book below and answer question. <context> Turn 1: <question> Be very concise. Answer 1: ...<ans>... Turn 2: <question> Be very concise. Answer 2: ...<ans>... Input: Read the book and answer the question. <context> Turn 1: <question> + <Option A,B,C,D>. Answer 1: ...<ans>... Turn 2: <question> + <Option A,B,C,D>. Answer 2: ...<ans>... Math.Find Ours len_array=30000 num_digits=3 metric = Accuracy Input: <a large array of number> Turn 1: The max number in the array is? Answer 1: ...<max number>... Turn 2: The max number in the array is? Answer 2: ...<max number>... ICL.ManyShot ManyShotICL (Srivastava et al., 2023) num_examples = 150 Tasks = date, salient, tracking7 metric = Accuracy Input: ICL Demo. 1 + Demo. 2 + ..... + Demo. 1000 Turn 1: <question>. Answer 1: ...<ans>... Turn 2: <question>. Answer 2: ...<ans>... En.Sum Ours Mix.Sum+NIAH Ours Mix.RepoQA+KV Ours Concatenated arXiv papers ground_truth from GPT-4 num document = 8 metric = ROUGE Input: Doc 1 + Doc 2 + Doc 3 + ... + Doc 10. Turn 1: Please summarize Doc 1. Answer 1: ... <summary of Doc 1>... Turn 2: Please summarize Doc 3. Answer 2: ... <summary of Doc 3>... Turn 3: Please summarize Doc 5. Answer 2: ... <summary of Doc 5>... num needle = 5 num document = 8 metric = ROUGE + Acc num KV pairs = 100 metric = Pass@1 + Acc Input: Doc 1 + <Passkeys> + Doc 2 + ... + <Passkeys> + Doc 10. Turn 1: Please summarize Doc 1. Answer 1: ...<summary of Doc 1>... Turn 2: What is the needle? Answer 2: ..<needle>... Input: <func 1> + KV pairs + <func 2> + ... + KV pairs + <func 100> Turn 1: <description of func 1>. Answer 1: <func 1> Turn 2: The value of the <key #1> is? Answer 2: ...<value #1>.. Python, C++, Java, PHP, Rust, Go, and TypeScript, compared to the original RepoQA. Each test session involves GitHub repository, and the model is required to retrieve one function per turn, with total of 5 turns per session. (ii) En.QA, Zh.QA, En.MultiChoice: These three tasks are extended from InfiniteBench (Zhang et al., 2024a), which provides high-quality, human-annotated QA tests based on fictional novels to eliminate the influence of external knowledge. These tasks require models to locate and process information within lengthy inputs, performing reasoning through aggregation or filtering to derive answers. There are two primary types of questions: 1) Aggregation involves compiling scattered information throughout the input. An example question is, How much money in total did spend on food? 2) Filtering equires identifying specific information from larger set. An example question is, What color dress did wear when met for the second time? In SCBench, we combine QA pairs that share the same input context to create shared context test sessions. Global Information Processing In addition to retrieval, some long-context tasks require leveraging and aggregating global context information, such as summarization, statistical tasks, and in-context learning (Yu et al., 2020; Srivastava et al., 2023; Hao et al., 2022). Our benchmark includes three relevant tasks to assess how well different long-context methods handle global information in multirequest settings. (i) Many-shot ICL: We use datasets from Big-Bench Hard (Srivastava et al., 2023) to evaluate manyshot in-context learning (ICL) capabilities. This includes three sub-tasks: date understanding, salient error translation detection, and tracking seven shuffled objects. We construct many-shot ICL contexts shared across different turns within test session. All three sub-tasks are presented as multiple-choice questions with four options provided. 7 Preprint. Under review. (ii) Math.Find: We extended the math find task from InfiniteBench (Zhang et al., 2024a), expanding from finding only maximum value to multiple statistical values. Given large array, LLMs are required to find the minimum or median values. LLMs must effectively comprehend global longcontext information, perform comparisons, and carry out statistical operations to answer the questions. (iii) En.Sum: This task uses concatenated academic papers from arXiv as input, with document lengths ranging from 8K to 20K tokens. Ground truth summaries were generated using GPT-4, which was prompted to produce concise one-sentence summaries for each document. The average length of the ground truth summaries is 654 tokens. The target documents for each turn are evenly distributed across the full context length. Multi-Tasking In real-world applications, LLMs often handle multiple tasks within single session using shared input context. For instance, users might request both summarization and content retrieval simultaneously. To reflect this, we include two multi-tasking tasks in SCBench: (i) Mix.Sum+NIAH: This task combines document summarization with the Needle in Haystack (Kamradt, 2023) task using shared input prompt. random \"needle\" is evenly inserted into the En.Sum tasks input (concatenated academic papers). The model alternates between summarization and NIAH retrieval in each test session. (ii) Mix.RepoQA+KV: This task combines the RepoQA task with KV retrieval using shared input prompt. Multiple KV pairs are evenly inserted into the RepoQA input (a long chunk of source code). total of 100 KV pairs are included, with four target KVs and the rest as distractors. The model alternates between RepoQA and KV retrieval in each test session. 3.2 LONG-CONTEXT SHARED CONTEXT MODES DETAILS In addition to the carefully designed long-context tasks, we include two shared context modes to more accurately reflect real-world long-context applications: multi-turn mode and multi-request mode, as shown in Fig. 2b. (i) Multi-turn Mode: typical scenario in long-context applications, including long-context chat, multi-step reasoning (e.g., Tree-of-Thought (Yao et al., 2024b)), and long-generation CoT. This mode is relevant to long-context methods with KV cache reuse, as the focus in each turn may shift significantly, potentially causing the models to lose information stored in KV cache. Following Zheng et al. (2023a); Wang et al. (2024), we use ground-truth answers instead of model-generated content as the context for follow-up turns. (ii) Multi-request Mode: Context sharing can occur across sessions or even users, such as multiple users working on the same code repository. In this case, models can encode the shared context and share the memory (KV cache) across multiple requests. Testing long-context methods in such scenarios is crucial, as some require the query for sparse encoding/decoding. For instance, MInference and SnapKV use the final part of the input (often the query) to estimate the overall sparse pattern. This mode tests how well these methods generalize without having the query."
        },
        {
            "title": "4 EXPERIMENTS & RESULTS",
            "content": "Models & Implementation Details We selected six open-source long-context LLMs for our study: Llama-3.1-8B/70B (Dubey et al., 2024), Qwen2.5-72B/32B (Team, 2024), Llama-3-8B-262K (Gradient, 2024), and GLM-4-9B-1M (GLM et al., 2024), along with two gated linear models: Codestal Mamba 7B (team, 2024), and Jamba-1.5-Mini (Lieber et al., 2024). This selection encompasses Transformer, SSMs, and SSM-Attention Hybrid models, representing some of the most effective context lengths among open-source Long-context LLMs. To ensure result stability, all experiments were conducted using greedy decoding in BFloat16 on four NVIDIA A100 GPUs. We evaluated all models using the HuggingFace or vLLM framework with FlashAttention-2 (Dao, 2024) implementation. Additionally, we employed MInferences implementation (Jiang et al., 2024) to reduce GPU memory overhead. More information of these models and our infrastructure can be found at C.1. Long-Context Method Details We evaluated eight categories of long-context solutions on our benchmark, including Gated Linear RNNs (e.g., Codestral-Mamba), SSM-Attention hybrid models 8 Preprint. Under review. Table 4: Average performance of various long-context methods across different base models in two shared context modes on SCBench. For additional results on base models such as Llama-3.1-70B, Qwen2.5-32B, and Llama-3-8B-262K, see Table 10 in D. Here, τ denotes the target compression rate. Methods τ LLaMA-3.1-8B 1 ❶ A-shape 1/32 ❶ Tri-shape 1/32 ❶ MInference 1/32 ❶ LLMLingua-2 1/3 ❷ StreamingLLM 1/32 ❷ SnapKV 1/32 ❷ PyramidKV 1/32 ❷ KIVI 1/8 ❸ CacheBlend 1 ❹ Quest 1/32 ❹ RetrievalAttention 1/32 GLM-4-9B-1M 1 ❶ A-shape 1/32 ❶ Tri-shape 1/32 ❶ MInference 1/32 ❶ LLMLingua-2 1/3 ❷ StreamingLLM 1/32 ❷ SnapKV 1/32 ❷ PyramidKV 1/32 ❷ KIVI 1/8 ❹ Quest 1/32 Qwen2.5-72B 1 ❶ A-shape 1/32 ❶ Tri-shape 1/32 ❶ MInference 1/32 ❶ LLMLingua-2 1/3 ❷ StreamingLLM 1/32 ❷ SnapKV 1/32 ❷ PyramidKV 1/32 ❷ KIVI 1/8 ❹ Quest 1/32 ❶ Jamba-1.5-Mini ❶ Mamba-Codestral - - Retr.String Retr.Semantic Global Multi-task AVG. Retr.String Retr.Semantic Global Multi-task AVG. Multi-turn Mode Multi-request Mode 57.1 14.0 18.1 39.1 5.7 0.4 6.1 6.3 12.0 56.7 8.2 25.0 48.9 27.2 31.5 38.2 5.8 0.7 18.1 18.1 26.5 20.1 51.5 24.0 25.7 45.6 4.2 0.7 3.8 4.9 12.9 5.2 67.4 0. 36.9 28.9 31.5 38.5 25.3 13.5 18.4 18.1 34.3 39.4 27.3 30.0 39.9 31.2 32.5 37.2 6.8 14.6 18.7 23.9 33.7 30.3 44.4 34.9 36.6 43.5 28.7 16.2 24.3 23.3 36.7 28.3 28.6 0.0 35.1 31.7 33.5 34.4 32.3 33.6 37.9 37.1 31.0 35.3 33.0 27.0 33.1 30.7 32.1 31.8 29.3 28.1 33.1 30.8 23.8 25. 38.9 36.7 37.7 38.4 46.2 40.2 43.6 42.0 49.3 41.9 37.5 11 65.7 33.7 37.9 57.8 49.6 14.3 21.1 22.6 50.7 65.6 20.1 35.5 72.8 58.5 64.0 70.8 24.5 12.7 34.0 34.9 51.2 36.4 77.0 58.0 63.8 72.8 27.3 18.7 34.1 34.3 57.4 31.5 47.5 0. 48.7 27.1 30.3 42.5 28.2 15.5 20.9 21.0 32.0 49.3 22.1 29.4 48.7 36.9 40.0 44.5 16.6 14.0 26.0 26.9 33.8 28.2 52.9 38.4 40.9 50.1 26.6 18.9 26.5 26.1 39.1 26.7 32.8 9.3 29.5 3.2 7.8 28.9 3.9 0.0 0.3 0.3 7.6 27.6 6.7 17.9 44.8 20.2 25.5 34.1 1.5 0.2 0.9 0.6 2.5 0. 31.1 15.2 18.6 28.6 2.7 0.0 0.0 0.0 3.0 1.2 21.7 3.9 36.4 33.2 25.7 35.6 24.4 11.3 14.2 15.1 30.1 35.8 25.6 26.7 31.1 24.1 25.2 29.0 14.8 10.2 10.1 4.9 20.3 15.6 46.8 35.5 38.3 44.7 31.2 4.2 6.2 17.5 40.7 24.8 61.8 25. 43.6 46.3 45.6 50.1 41.2 30.3 35.7 34.7 33.1 36.2 31.8 30.7 43.4 40.5 41.4 43.4 38.5 32.7 37.5 39.8 39.5 33.9 53.0 47.7 48.5 52.2 49.0 4.4 7.0 44.5 52.8 42.1 5.6 6.4 39.2 27.8 24.6 30.9 22.8 16.4 10.7 11.0 28.4 39.5 14.2 27.6 48.0 42.6 43.0 48.3 24.8 17.1 24.0 21.1 43.5 15. 52.4 43.1 44.9 52.0 25.8 0.0 0.0 11.2 46.6 15.2 38.9 54.8 37.2 27.6 25.9 36.4 23.1 14.5 15.2 15.3 24.8 34.8 19.6 25.8 41.8 31.8 33.8 38.7 19.9 15.1 18.1 16.6 26.5 16.2 45.8 35.4 37.6 44.4 27.2 2.2 3.3 18.3 35.8 20.8 48.0 7. (e.g., Jamba), sparse attention, KV cache dropping, prompt compression, KV cache quantization, KV cache retrieval, and KV cache loading, as detailed in Table 1. All methods were tested on Transformer-based long-context LLMs, except for Codestral-Mamba and Jamba. Additionally, we report the KV cache size, pre-filling stage complexity, decoding stage complexity, and whether efficient operations are applied during the pre-filling or decoding stages (details in Sec. 2). The exact implementation and configuration details are provided in C.2. Main Results Table 4, 10, and Fig. 6 illustrate the performance of various long-context methods across multiple tasks and shared context modes in different base LLMs. Key observations include: 1) In retrieval tasks, most long-context methods, except MInference, perform poorly, particularly in exact information retrieval such as string retrieval. 2) Sparse attention methods show significant improvements over sparse decoding methods as the number of request rounds increases, with A-shape demonstrating the greatest enhancement. Tri-shape, which incorporates dense bottom query tokens into A-shape, boosts first-round performance but has minimal impact on subsequent rounds. Tri-shape also generalizes well across tasks, ranking second only to MInference across models. Our analysis reveals that the Tri-shape bottom improves first-turn instruction-following, thus enhancing overall performance, while A-shape disrupts instruction information, leading to random outputs, as shown in Table 17. 3) KV cache compression methods generally underperform in shared scenarios, showing only slight advantages in the first round. 4) Prompt compression methods enhance global information tasks like many-shot ICL but degrade performance significantly in retrieval-related tasks. 5) SSMattention hybrid models perform well in single-turn interactions but exhibit degraded accuracy in multi-turn scenarios, especially in RepoQA and Math. Gated Linear RNN models perform poorly in shared context modes. 9 Preprint. Under review. (a) String Retrieval (b) Semantic Retrieval (c) Global Information Figure 6: Performance of different long-context methods across various tasks and turns. The results for multi-tasking tasks are shown in Fig. 10, and the results are averaged across all tested base LLMs. (a) Critical KVs Vary Across Queries (b) Attention Map of Retr.KV Across Turns Figure 7: Attention visualization of Retr.KV for the shared context across multiple turns."
        },
        {
            "title": "5 ANALYSIS",
            "content": "Sub-O(n) Memory is Almost Infeasible in Multi-Turn Decoding. We analyzed the attention distribution for the Retr.KV task across multiple turns with shared context. As shown in Fig. 7a, the critical key-value pairs (KVs) are highly query-dependent and vary significantly between turns. We found that, aside from initial and local tokens, attention focuses primarily on the first occurrence of the key in each query. Due to the unpredictability of future queries, the shared context memory (i.e., KV cache in Transformer models) must be fully preserved. This potential distribution shift of importance of history KV states explains why most sub-O(n) decoding methods, particularly KV cache compression methods and pure SSM models, fail in our SCBench benchmark. Previous studies have noted similar issues with SSMs, suggesting that the entire prompt needs to be repeated after each query to recover lost context memory (Arora et al., 2024). In Fig. 7b, we visualize the attention map for the Retr.KV task across turns. While important KVs remain consistent within turn, they differ significantly between queries. This explains why O(k) KV cache compression methods perform well in single-query tests but fail in follow-up queries. However, the SSM-attention hybrid model Jamba shows potential for reducing overall memory cost by utilizing SSM layers while maintaining O(n) memory in few attention layers for future lookups (Waleffe et al., 2024). Another promising approach is CPU-GPU collaboration for fast inference, where the full O(n) memory is stored in CPU RAM, and relevant KVs are dynamically loaded to the GPU, achieving sub-O(n) decoding on the GPU (Liu et al., 2024b). The Sparsity in Encoding and Decoding. We discussed how sub-O(n) sparse decoding often fails to maintain accuracy across multiple requests with shared context. Interestingly, these sparse approaches perform well in the encoding phase if decoding remains dense. As shown in Fig. 3a, with dense decoding (O(n) memory), Tri-Shape and A-Shape demonstrate strong performance in multi-request testing. While this success of sparse encoding with dense decoding has been observed in single-turn tests (Sun et al., 2024c; Jiang et al., 2024), we are the first to showcase its potential in shared context scenarios. In contrast, extending sparse patterns to the decoding stage leads to significant performance degradation (e.g., StreamingLLM). Even with dense encoding, 10 Preprint. Under review. sparse decoding methods generally underperform in shared context testing, particularly KV cache compression methods. This disparity may be due to redundancy in the encoding output, while decoding plays critical role in generation tasks (Deng et al., 2024). Redundant input prompts allow key information to be captured even with sparse encoding, but sparse decoding reduces per-layer connectivity, limiting the models ability to focus on critical tokens. Since sparse decoding relies on proxy tokens for global information access, it restricts the construction of complex attention functions (Yun et al., 2020). We emphasize the need for more sophisticated sparse patterns in sparse attention. Dynamic sparse attention methods can enhance connectivity and enable faster information propagation (Jiang et al., 2024), better approximating full attention performance compared to static sparse patterns, as shown in Fig. 6. Compressible and Incompressible Tasks. While O(n) memory is essential in multi-request scenarios with shared context, this requirement can be relaxed for highly compressible inputs in simpler tasks. For instance, the Needle-in-the-Haystack benchmark (Kamradt, 2023) embeds key information (the \"needle\") into repetitive noise (the \"haystack\"), allowing sub-O(n) methods to achieve reasonable accuracy since the noise is highly compressible. Similarly, tasks like summarization involve compressible contexts, where sub-O(n) methods can balance efficiency and performance. However, with dynamic and complex inputs, sub-O(n) methods often fail to store all necessary information, resulting in poor performance on challenging retrieval tasks. Tasks like Retr.KV and Retr.Prefix-Suffix, which involve random and incompressible key-value pairs and strings, require models to fully utilize their context window. In summary, while compressible tasks may overestimate models capabilities, sub-O(n) methods remain useful for simpler tasks due to their efficiency. Table 5: Results of query-awareness long-context methods. w/ (first) and w/o (later) query. LLaMA-3.1-8B Retr.String Retr.Semantic ❷ SnapKV ❶ Tri-shape ❶ MInference Sparse Methods without Query Awareness. One concern with longcontext methods in KV cache reuse scenarios is their reliance on the query for compression to achieve efficient encoding or decoding. However, in real-world applications, single context is often shared across multiple queries, requiring these methods to function without the query. This raises the question: can query-dependent long-context methods generalize effectively without it? In Table 5, we compare the performance of three query-awareness long-context methods w/ and w/o the query provided, highlighting degraded performance in the absence of the query using underlines. We observed that both the KV cache compression method SnapKV and the static sparse attention method Tri-shape struggled to maintain accuracy without the query. In contrast, the dynamic sparse attention method MInference demonstrated more robust generalization, likely due to its dynamic and sophisticated sparse patterns, particularly the presence of diagonal connections in its attention map. 19.0 / 9.7 31.4 / 25.7 40.4 / 35.6 17.9 / 14.6 31.1 / 45.6 35.4 / 50.1 5.1 / 0.0 28.0 / 24.6 28.3 / 30. 0.0 / 0.0 12.1 / 7.8 28.1 / 28.9 Multi-task Global"
        },
        {
            "title": "6 RELATED WORKS",
            "content": "Prefix Caching (also known as KV cache reuse) optimizes time-to-first-token in LLM inference frameworks, particularly for shared contexts like multi-turn conversations or chatbot sessions (Zheng et al., 2024; Kwon et al., 2023; Gim et al., 2024). This technique is widely adopted by LLM providers (Gemini, 2024; Claude, 2024; OpenAI, 2024b; Azure, 2024). Recent optimizations focus on enhancing KV cache efficiency. PagedAttention (Kwon et al., 2023) reduces memory costs by partitioning the KV cache into blocks with lookup table. HydraGen (Juravsky et al., 2024) and Cascade Inference (Ye et al., 2024) decouple attention computation for shared prefixes and unique suffixes, supporting batched multi-query kernels. RadixAttention (Zheng et al., 2024) accelerates KV lookups using radix tree with O(k) complexity and is integrated into the vLLM framework (vLLM, 2024). RAGCache (Jin et al., 2024) caches KV tensors for retrieved documents in retrieval-augmented generation, while CacheBlend (Yao et al., 2024a) improves cache utilization via partial recomputation. Despite these advancements, no existing long-context benchmarks evaluate KV cache reuse scenarios. Conversational and Multi-Turn Benchmarks While multi-turn benchmarks better reflect realworld applications, many evaluations still focus on single-turn (Li et al., 2023a; Finch et al., 2023). Benchmarks like MT-Bench (Zheng et al., 2023a), ShareGPT (Domeccleston, 2023), MINT (Wang et al., 2024), MT-Bench-101 (Bai et al., 2024a), and MT-Eval (Kwan et al., 2024) assess conversational 11 Preprint. Under review. abilities, instruction-following, and complex task-solving across turns. However, they primarily focus on model consistency and information extraction rather than evaluating long-context inputs. Long-Context Methods of LLMs Long-context inference faces two key bottlenecks: computational cost during pre-filling and memory cost during decoding (Fu, 2024). Pre-filling optimizations include state space models (Gu & Dao, 2024; Gu et al., 2022), linear attention methods (Peng et al., 2023; Sun et al., 2023), memory-based approaches (Munkhdalai et al., 2024), sparse attention (Jiang et al., 2024), hybrid techniques (Lieber et al., 2024; Ho et al., 2024; Ren et al., 2024; Xiao et al., 2024a), and prompt compression (Li et al., 2023b; Jiang et al., 2023; Pan et al., 2024). Decoding optimizations focus on: 1) Attention KV reuse to reduce storage (Shazeer, 2019; Ainslie et al., 2023; Sun et al., 2024c; Liu et al., 2024a; Nawrot et al., 2024); 2) Static KV compression (Xiao et al., 2024b; Han et al., 2024); 3) Dynamic KV compression, including cache discarding (Zhang et al., 2024b; Ge et al., 2024; Liu et al., 2023; Li et al., 2024c) and offloading (Ribar et al., 2024; Tang et al., 2024; Dai et al., 2024; Liu et al., 2024b; Chen et al., 2024; Sun et al., 2024a); 4) Hierarchical speculative decoding (Sun et al., 2024b). Most methods are tested on single-turn benchmarks and employ query-conditioned lossy techniques, which may degrade performance in multi-turn scenarios with prefix caching. This limitation motivates the design of SCBench, benchmark that evaluates long-context solutions in shared context settings."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This paper addresses key gap in the evaluation of long-context methods, which used to fully rely on single-turn interactions and overlook the performance of shared long-context scenarios that is common in real-world LLM applications. To bridge this gap, we introduce SCBench, comprehensive benchmark designed to test long-context methods with KV cache reuse across multiple domains, featuring 12 tasks that span four long-context capabilities: string retrieval, semantic retrieval, global information processing, and multi-tasking, across two shared context modes. Using our benchmark, we category long-context methods into 4 stage in KV-cache centric perpective, including KV cache generation, compression, retrieval, loading. We evaluated eight categories of long-context methods, including gated linear RNNs, hybrid models, sparse attention, KV cache dropping, quantation, retrieval, loading, and prompt compression, on eight state-of-the-art LLMs, including Llama-3.18B/70B, Qwen2.5-72B/32B, Llama-3-8B-262K, GLM-4-9B, Codestal Mamba, and Jamba-1.5. Our results show clear disparity in KV cache management: methods maintaining KV cache at O(n) excel in multi-request scenarios, while sub-O(n) methods perform well in single-turn settings but struggle with complex interactions. These findings highlight the importance of multi-turn, sharedcontext scenarios in developing and evaluating long-context methods, offering more realistic benchmark and key insights for improving long-context models and future architecture design in practical applications."
        },
        {
            "title": "REFERENCES",
            "content": "Shantanu Acharya, Fei Jia, and Boris Ginsburg. Star attention: Efficient llm inference over long sequences. ArXiv preprint, abs/2411.17116, 2024. URL https://arxiv.org/abs/2411. 17116. Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Luis Rosias, Stephanie C.Y. Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, and Hugo Larochelle. Many-shot in-context learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=AB6XpMzvqH. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 48954901, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.298. URL https://aclanthology.org/2023.emnlp-main.298. 12 Preprint. Under review. Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, and Christopher Ré. Just read twice: closing the recall gap for recurrent language models. ArXiv preprint, abs/2407.05483, 2024. URL https://arxiv.org/abs/ 2407.05483. Microsoft Azure. Prompt caching. https://learn.microsoft.com/en-us/azure/ ai-services/openai/how-to/prompt-caching, 2024. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. MT-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 74217454, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.401. URL https://aclanthology.org/2024.acl-long.401. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: bilingual, multitask benchmark for long context understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 31193137, Bangkok, Thailand, 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.172. URL https://aclanthology.org/2024.acl-long.172. Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675698, 2024. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. ArXiv preprint, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150. Avi Caciularu, Matthew Peters, Jacob Goldberger, Ido Dagan, and Arman Cohan. Peek across: Improving multi-document modeling via cross-document question-answering. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 19701989, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long. 110. URL https://aclanthology.org/2023.acl-long.110. Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, et al. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. ArXiv preprint, abs/2406.02069, 2024. URL https: //arxiv.org/abs/2406.02069. Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, et al. Magicpig: Lsh sampling for efficient llm generation. ArXiv preprint, abs/2410.16179, 2024. URL https://arxiv.org/abs/2410. 16179. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. ArXiv preprint, abs/1904.10509, 2019. URL https://arxiv.org/abs/1904. 10509. Claude. Prompt caching with claude. https://www.anthropic.com/news/ prompt-caching, 2024. Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei Bi, and Shuming Shi. Sequence can secretly tell you what to discard. ArXiv preprint, abs/2404.15949, 2024. URL https://arxiv.org/abs/2404.15949. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=mZn2Xyh9Ec. Preprint. Under review. Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=ztn8FCR1td. Yichuan Deng, Zhao Song, and Chiwun Yang. Attention is naturally sparse with gaussian distributed input. ArXiv preprint, abs/2404.02690, 2024. URL https://arxiv.org/abs/ 2404.02690. Domeccleston. Domeccleston/sharegpt: Easily share permanent links to chatgpt conversations with your friends. https://github.com/domeccleston/sharegpt, 2023. Accessed: 2024-09-15. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. ArXiv preprint, abs/2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. Sarah E. Finch, James D. Finch, and Jinho D. Choi. Dont forget your ABCs: Evaluating the state-of-the-art in chat-oriented dialogue systems. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1504415071, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.839. URL https://aclanthology.org/2023.acl-long.839. Yao Fu. Challenges in deploying long-context transformers: theoretical peak performance analysis. ArXiv preprint, abs/2405.08944, 2024. URL https://arxiv.org/abs/2405.08944. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells In The Twelfth International you what to discard: Adaptive kv cache compression for llms. Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=uNrFpDPMyo. Gemini. Context caching. https://ai.google.dev/gemini-api/docs/caching, 2024. In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin Zhong. Prompt cache: Modular attention reuse for low-latency inference. In P. Gibbons, G. Pekhimenko, and C. De Sa (eds.), Proceedings of Machine Learning and Systems, volume 6, pp. 325 338, 2024. URL https://proceedings.mlsys.org/paper_files/paper/2024/ file/a66caa1703fe34705a4368c3014c1966-Paper-Conference.pdf. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. ArXiv preprint, abs/2406.12793, 2024. URL https://arxiv.org/abs/ 2406.12793. Gradient. Llama-3 8b instruct gradient 4194k (v0.1), 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id= tEYskw1VY2. Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/ forum?id=uYLFoz1vlAC. Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. LMinfinite: Zero-shot extreme length generalization for large language models. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 39914008, Mexico City, Mexico, 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.222. URL https://aclanthology.org/ 2024.naacl-long.222. 14 Preprint. Under review. Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting: Scaling in-context learning to 1,000 examples. ArXiv preprint, abs/2212.06713, 2022. URL https://arxiv.org/abs/2212.06713. Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=g4OTKRKfS7R. Namgyu Ho, Sangmin Bae, Taehyeon Kim, hyunjik.jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, and Se-Young Yun. Block transformer: Global-to-local language modeling for fast inference. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=6osgTNnAZQ. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. RULER: Whats the real context size of your long-context language models? In First Conference on Language Modeling, 2024. URL https://openreview.net/forum? id=kIoBbc76Sy. Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and eran malach. Repeat after me: Transformers are better than state space models at copying. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=duRRoGeoQT. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua: Compressing prompts for accelerated inference of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1335813376, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.825. URL https: //aclanthology.org/2023.emnlp-main.825. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=fPBACAbqSN. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=VTF8yNQM66. Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin Jin. Ragcache: Efficient knowledge caching for retrieval-augmented generation. ArXiv preprint, abs/2404.12457, 2024. URL https://arxiv.org/abs/2404.12457. Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel Fu, Christopher Ré, and Azalia Mirhoseini. Hydragen: High-throughput llm inference with shared prefixes. ArXiv preprint, abs/2402.05099, 2024. URL https://arxiv.org/abs/2402.05099. Greg Kamradt. Needle in haystack - pressure testing llms, 2023. Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li, Lifeng Shang, Xin Jiang, Qun Liu, and Kam-Fai Wong. MT-eval: multi-turn capabilities evaluation benchmark for large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 2015320177, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.1124. URL https://aclanthology.org/2024. emnlp-main.1124. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model In Proceedings of the 29th Symposium on Operating Systems serving with pagedattention. Principles, SOSP 23, pp. 611626, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702297. doi: 10.1145/3600006.3613165. URL https://doi.org/ 10.1145/3600006.3613165. 15 Preprint. Under review. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. LooGLE: Can long-context language models understand long contexts? In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1630416333, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.859. URL https://aclanthology.org/2024.acl-long.859. Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning. ArXiv preprint, abs/2404.02060, 2024b. URL https://arxiv.org/ abs/2404.02060. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, CG Ishaan Gulrajani, Liang, and TB Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. 2023. URL https://github. com/tatsu-lab/alpaca_eval, 2023a. Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 6342 6353, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/ 2023.emnlp-main.391. URL https://aclanthology.org/2023.emnlp-main.391. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. SnapKV: LLM knows what you are looking for before generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024c. URL https://openreview.net/forum?id=poE54GOq2l. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformermamba language model. ArXiv preprint, abs/2403.19887, 2024. URL https://arxiv.org/ abs/2403.19887. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-ofexperts language model. ArXiv preprint, abs/2405.04434, 2024a. URL https://arxiv.org/ abs/2405.04434. Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, et al. Retrievalattention: Accelerating longcontext llm inference via vector retrieval. ArXiv preprint, abs/2409.10516, 2024b. URL https://arxiv.org/abs/2409.10516. Jiawei Liu, Jia Le Tian, Vijay Daita, Yuxiang Wei, Yifeng Ding, Yuhan Katherine Wang, Jun Yang, and Lingming Zhang. Repoqa: Evaluating long context code understanding. ArXiv preprint, abs/2406.06025, 2024c. URL https://arxiv.org/abs/2406.06025. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024d. doi: 10.1162/tacl_a_00638. URL https://aclanthology.org/2024.tacl-1.9. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= JZfg6wGi6g. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. KIVI: tuning-free asymmetric 2bit quantization for KV cache. In Forty-first International Conference on Machine Learning, 2024e. URL https://openreview.net/ forum?id=L057s2Rq8O. 16 Preprint. Under review. Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. ArXiv preprint, abs/2404.07143, 2024. URL https://arxiv.org/abs/2404.07143. Piotr Nawrot, Adrian Łancucki, Marcin Chochowski, David Tarjan, and Edoardo Ponti. Dynamic memory compression: Retrofitting LLMs for accelerated inference. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id= tDRYrAkOB7. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. ArXiv preprint, abs/2209.11895, 2022. URL https://arxiv.org/abs/2209.11895. OpenAI. Learning to reason with llms, 2024a. URL https://openai.com/index/ learning-to-reason-with-llms/. OpenAI. Prompt api-prompt-caching/, 2024b. caching in the api. https://openai.com/index/ Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 963981, Bangkok, Thailand and virtual meeting, 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.57. URL https://aclanthology.org/ 2024.findings-acl.57. Joon Sung Park, Joseph C. OBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, 2023. URL https://api.semanticscholar.org/CorpusID:258040990. Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, Stanisław Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=7SaXczaBpG. Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. Mooncake: Kimis kvcache-centric architecture for llm serving. ArXiv preprint, abs/2407.00079, 2024. URL https://arxiv.org/abs/2407.00079. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint, abs/2403.05530, 2024. URL https://arxiv.org/abs/2403.05530. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. ArXiv preprint, abs/2406.07522, 2024. URL https://arxiv.org/abs/2406.07522. Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient LLM inference. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=OS5dqxmmtl. Noam Shazeer. Fast transformer decoding: One write-head is all you need. ArXiv preprint, abs/1911.02150, 2019. URL https://arxiv.org/abs/1911.02150. 17 Preprint. Under review. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Re, Ion Stoica, and Ce Zhang. FlexGen: High-throughput generative inference of large language models with single GPU. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3109431116. PMLR, 2329 Jul 2023. URL https://proceedings.mlr. press/v202/sheng23a.html. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. ArXiv preprint, abs/2408.03314, 2024. URL https://arxiv.org/abs/2408.03314. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, and Beidi Chen. Shadowkv: Kv cache in shadows for high-throughput long-context llm inference. ArXiv preprint, abs/2410.21465, 2024a. URL https://arxiv.org/abs/2410. 21465. Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. In First Conference on Language Modeling, 2024b. URL https://openreview.net/forum?id= HVK6nl3i97. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. ArXiv preprint, abs/2307.08621, 2023. URL https://arxiv.org/abs/2307.08621. Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder architectures for language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024c. URL https://openreview.net/forum?id=25Ioxw576r. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. QUEST: Queryaware sparsity for efficient long-context LLM inference. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=KzACYw0MTV. Mistral AI team. Codestral mamba, 2024. URL https://mistral.ai/news/ codestral-mamba/. Qwen Team. Qwen2.5: party of foundation models, 2024. URL https://qwenlm.github. io/blog/qwen2.5/. Philippe Tillet, H. T. Kung, and David Cox. Triton: an intermediate language and compiler for In Proceedings of the 3rd ACM SIGPLAN International tiled neural network computations. Workshop on Machine Learning and Programming Languages, MAPL 2019, pp. 1019, New ISBN 9781450367196. doi: York, NY, USA, 2019. Association for Computing Machinery. 10.1145/3315508.3329973. URL https://doi.org/10.1145/3315508.3329973. vLLM. Automatic prefix caching. https://docs.vllm.ai/en/latest/automatic_ prefix_caching/apc.html, 2024. Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, et al. Michelangelo: Long context evaluations beyond haystacks via latent structure queries. ArXiv preprint, abs/2409.12640, 2024. URL https://arxiv.org/abs/2409.12640. Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mambabased language models. ArXiv preprint, abs/2406.07887, 2024. URL https://arxiv.org/ abs/2406.07887. 18 Preprint. Under review. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=jp3gWrMuIZ. Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming In The Twelfth International Conference on Learning language models with attention sinks. Representations, 2024b. URL https://openreview.net/forum?id=NG7sS51zVF. Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, and Junchen Jiang. Cacheblend: Fast large language model serving with cached knowledge fusion. ArXiv preprint, abs/2405.16444, 2024a. URL https://arxiv.org/abs/2405.16444. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024b. Zihao Ye, Ruihang Lai, Bo-Ru Lu, Chien-Yu Lin, Size Zheng, Lequn Chen, Tianqi Chen, and Luis Ceze. Cascade inference: Memory bandwidth efficient shared prefix batch decoding, 2024. URL https://flashinfer.ai/2024/02/02/cascade-inference.html. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effectively and thoroughly. ArXiv preprint, abs/2410.02694, 2024. URL https://arxiv.org/abs/2410.02694. Dian Yu, Kai Sun, Claire Cardie, and Dong Yu. Dialogue-based relation extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 49274940, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.444. URL https://aclanthology.org/2020.acl-main.444. Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, and Xia Hu. KV cache compression, but what must we give in return? comprehensive benchmark of long context capable approaches. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 46234648, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.266. URL https://aclanthology.org/2024.findings-emnlp.266. Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. O(n) connections are expressive enough: Universal approximability of sparse transformers. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 9ed27554c893b5bad850a422c3538c15-Abstract.html. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. Infinitebench: Extending long context evaluation beyond 100K tokens. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1526215277, Bangkok, Thailand, 2024a. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.814. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024b. 19 Preprint. Under review. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023a. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. SGLang: Efficient execution of structured language model programs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=VqkAKQibpq. Ningxin Zheng, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han, Lingxiao Ma, Yuqing Yang, Fan Yang, Chengruidong Zhang, Lili Qiu, Mao Yang, and Lidong Zhou. Pit: Optimization of dynamic sparse deep learning models via permutation invariant transformation. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 23, pp. 331347, New York, NY, USA, 2023b. Association for Computing Machinery. ISBN 9798400702297. doi: 10.1145/3600006.3613139. URL https://doi.org/10.1145/3600006.3613139. COMPARED TO PRIOR LONG-CONTEXT BENCHMARK We have compared SCBench against existing long-context benchmarks across long-context capability assessed, request types considered, and implementation they adopted, as shown in Table 6. Table 6: Comparison of Long-Context Benchmarks. Long-Context Capability Request Type Implementation Precise Retrieval LongBench (Bai et al., 2024b) InfiniteBench (Zhang et al., 2024a) RULER (Hsieh et al., 2024) LongCTXBench (Yuan et al., 2024) HELMET (Yen et al., 2024) Michelangelo (Vodrahalli et al., 2024) SCBench Semantic Retrieval MultiTasking Global Information Single Question MultiTurn MultiRequest KV Cache Reuse We also directly compare the testing results of long-context methods on prior benchmarks and SCBench to show the unique insights our benchmark provides. We mainly compare two common long-context capability: summarization (as shown in Table 7), and retrieval (as shown in 8). The summarization sub-tasks we used is En.Sum for InfiniteBench (Zhang et al., 2024a), and gov-report for LongBench (Bai et al., 2024b). The retrieval sub-tasks we used is Retr.KV for InfiniteBench (Zhang et al., 2024a), and Passage-retrieval for LongBench (Bai et al., 2024b). In addition, LongCTXBench (Yuan et al., 2024) also analyzes the performance boundaries of long-context efficient methods from KV-cache-centric perspective. However, it does not consider multi-request scenarios and only focuses on the KV Cache Generation and Compression stages. Table 7: Comparing the summarization capability of efficient long-context methods on prior benchmarks and our SCBench. Prior Benchmarks SCBench Model InfiniteBench LongBench Llama-3.1-8B-Inst A-Shape Tri-Shape Minference StreamingLLM SnapKV LLMLingua 28.5 24.5 27.4 28.9 27.3 28.3 23.1 36.6 33.5 33.9 33.9 32.0 33.2 32.0 Multi Request 38.3 28.8 30.2 36.7 30.2 29.9 30.1 Turn-1 Turn-2 Turn-3 Turn-4 Turn-5 44.2 26.1 32.1 40.6 29.4 36.2 32. 42.1 30.8 30.0 36.1 26.1 29.4 22.5 35.8 33.8 34.0 39.7 27.7 28.6 26.6 37.6 40.8 41.0 43.5 27.3 28.1 25.7 42.3 40.4 40.3 43.7 26.9 31.0 26.6 We found SCBench can better identify the weakness of long-context methods under the KV cache reuse scenarios, such as the general incapability of KV cache compression methods on multi-request Preprint. Under review. Table 8: Comparing the retrieval capability of efficient long-context methods on prior benchmarks and our SCBench. Prior Benchmarks SCBench Model InfiniteBench LongBench Llama-3.1-8B-Inst A-Shape Tri-Shape Minference StreamingLLM SnapKV LLMLingua 57 0 21 33 0 4 0 100 42 100 100 84 100 90 Multi Request 56 3 5 14 0 0 0 Turn-1 Turn-2 Turn-3 Turn-4 Turn62 0 14 31 2 0 0 59 12 19 35 1 0 1 68 22 25 46 0 0 2 66 28 32 56 0 0 0 70 33 38 50 0 0 0 mode and follow-up queries in the multi-turn mode, as well as the increasing accuracy of sparse attention under multi-turn mode. HYPER-PARAMETERS OF EFFICIENT LONG-CONTEXT METHODS We conduct extensive experiments with various computing budgets for the efficient long-context methods we covered. The results are shown in Figure 8 and Figure 9 for the multi-turn mode and multi-request mode respectively. From the results, we can derive the following insights: 1) Most methods show minimal performance degradation at 1/2 budget (e.g., A-shape and Tri-shape drop by 5-6 points, SnapKV drops by 11 points). However, as sparsity increases, performance declines significantly. For example, StreamingLLM and SnapKV drop by 26 and 19 points, respectively, under 1/4 budget. 2) More accurate sparse methods can maintain performance even under higher sparsity. For instance, MInference achieves performance at 1/32 budget comparable to A-shape and Tri-shape at 1/4 budget. 3) While some methods exhibit similar performance in single-turn scenarios, they diverge significantly in multi-turn and multi-request scenarios. For example, SnapKV outperforms StreamingLLM in turn-1 but performs significantly worse in turn-2. In some tasks, changing the budget has little impact on turn-1 performance but substantially affects turn-2 and subsequent turns, such as in Long Document QA tasks and summarization."
        },
        {
            "title": "C EXPERIMENT DETAILS",
            "content": "C.1 LONG-CONTEXT METHODS DETAILS This section will introduce the long-context methods (as shown in Table 1) that involved in our paper. State Space Models (SSMs) are powerful models often used for modeling dynamic systems, particularly in time series analysis, control theory, and machine learning. As language are naturally time series data, recent advancements have integrated SSMs into language modeling architectures, showcasing their potential as alternatives to traditional models like RNNs and Transformers. Due to their linear complexity, they are especially suitable for long sequence tasks. For instance, models such as S4 (Hasani et al., 2023) and Mamba (Gu & Dao, 2024) have demonstrated superior efficiency in handling sequential data with reduced computational complexity compared to their predecessors and comparable accuracy in tasks such as language modeling. However, SSMs were also criticized for their reduced memorization capability and their limited capability in copy-pasting (Jelassi et al., 2024). Mamba-Attention Hybrid Architecture interleaves blocks of Transformers and Mamba layers, aiming to obtain the benefits of both architecture, i.e., the expressive power of Transformer and the linear complexity of Mamba layers. Jamba (Lieber et al., 2024) and Samba (Ren et al., 2024) are representative efforts on this direction. Waleffe et al. (2024) also highlights the potential of such hybrid architectures and found only few number of attention layers can lead to significant performance increase compared to pure SSMs models. 21 Preprint. Under review. Figure 8: Hyper-parameters analysis: averaged performance of efficient long-context methods with different computing budgets under the multi-turn mode of SCBench. The input length is 128K, meaning that 4K, 8K, 16K, 32K, and 64K correspond to sparsity budgets of 1/32, 1/16, 1/8, 1/4, and 1/2, respectively. Figure 9: Hyper-parameters analysis: averaged performance of efficient long-context methods with different computing budgets under the multi-request mode of SCBench. The input length is 128K, meaning that 4K, 8K, 16K, 32K, and 64K correspond to sparsity budgets of 1/32, 1/16, 1/8, 1/4, and 1/2, respectively. Sparse Attention is extensively studied for long sequence processing, including image synthesis and multi documents question answering. We test three sparse attention approach in our paper: A-shape, Tri-shape, and MInference. In A-shape attention, each token is only allowed in to attend to initial tokens and local tokens, resulting A-shape on its attention map (Xiao et al., 2024b). Tri-shape attention is variant of A-shape method, we introduced in our paper, where we add dense attention space at the bottom of the triangle A-shape attention matrix. This is based on the promising results of sparse encoding with dense decoding, where the dense space we added is natural extrapolate of the dense decoding idea. MInference (Jiang et al., 2024) is the state-of-the-art dynamic sparse attention approach where the exact sparse pattern are dynamically built on-the-fly to better approximate full attention operation. 22 Preprint. Under review. KV Cache Compression is series of studies that attempt to solve the linearly growing memory (often referred as KV Cache) cost in LLMs inference. For example, StreamingLLM (Xiao et al., 2024b) use constant size of KV Cache in their decoding steps, where only the state of initial and local tokens are preserved, and the rest part of KV Caches are evicted from the memory. SnapKV (Li et al., 2024c) introduces the concept of the observation window. It selects the top-K KVs that are extensively attended to in the observation window, and removes other KVs from the Cache. This method was reported to performance well in simple Neeld-in-A-Haysatck tasks and many other natural language tasks. KV Cache Quantization aims to reduce the memory footprint of KV cache via quantization. KIVI (Liu et al., 2024e) employed per-channel quantization for the Key tensor, and per-token quantization for Value tensor. In our evaluation, we use 2 bit algorithms with group size of 32 an residual length of 32. KV Cache Retrieval indicates the operation to retrieve pre-cached KV cache and reuse them for incoming requests. Most of the frameworks employ an exact match algorithm, i.e., only retrieve and reuse the KV cache is the the shared context match exactly. However, there also exists approach such as CacheBlend (Yao et al., 2024a) that retrieve KV cache once the input is similar enough semantically with one former request from the cache base. KV Cache Loading Due to the huge memory result from the long-context KV cache, researchers have proposed novel approaches that make use of the extensive CPU RAM and load only partial of the KV cache to GPU per-token for more efficient decoding. For example, Quest (Tang et al., 2024) estimates the importance of Keys in page granularity, and only the topK important Keys and corresponding Values are loaded to CUDA HBM for the attention computation. Retrieval Attention (Liu et al., 2024b) constructs vector database on CPU RAM to find the topK critical Keys efficiently. In additional, it tailored pipeline for decoding to hide the memory movement from CPUs to GPUs. Prompt Compression aims to compress the prompt to obtain more compact representation of the input before send it to the LLMs (Li et al., 2023b; Jiang et al., 2023). LLMLingua-2 (Pan et al., 2024) is supervised model that assess the importance of individual token as token classification task. It was shown in provide up to 20x compression on many tasks, with only minimal performance sacrifice. C.2 ADDITIONAL IMPLEMENTATION DETAILS In Table 9, we report the configuration we used for the long-context methods we involved in our experiments. For the Mamba-Codestral-7B-v0.1 model and AI21-Jamba-1.5-Large, we report the architecture details of other models. For SSMs models, the state size and number of layers are crucial properties, as all previous information are compressed and saved in this fixed size of states. Moreover, the number of groups and number heads are also important as they implement channel mixing which shown to be critical for the expressive power. For Mamba-Attention hybrid architecture, the present the ratio of attention layers and mamba layers. As the Jamba model is also MoE, we also represent the number of experts and the number of experts activated per token. In Sparse Attention, we report the the local size and initial size of tokens that Tri-shape and A-shape can attend to. For Tri-shape, we add dense space of size 64 at the bottom of the attention matrix. MInference is dynamic sparse attention, where the exact sparse patterns are built conditioned on the inputs. According to Jiang et al. (2024), we search the sparse patterns for attention heads with the task of KV retrieval, and we also report the search space (i.e., the distribution of sparse index) for the exact pattern. In KV Cache compression, we report the composition of KV used in StreamingLLM. The observation window and max capacity of KV Cache size, the kernel size used to identify top-k KVs are reported in the Table. For KV cache quantization, retrieval and loading, we use the default hyper-parameters in their original implementation and reported them in Table 9. We use tensor parallel when testing models larger than 7B parameters, with 8*A100 40GB machines or 4*H100 80GB machines. Specifically, we use our customized A-shape, Tri-shape, and MInference kernels in sparse attention testing, utilizing PIT (Zheng et al., 2023b) with FlashAttention (Dao, 2024) 23 Preprint. Under review. Table 9: Configurations of long-context methods in SCBench. Methods Configurations SSMs Mamba-Codestral-7B-v0.1 Hybrid Models AI21-Jamba-1.5-Large Sparse Attention Tri-Shape A-Shape MInference chunk size: 256, conv kernel: 4, expand: 2, head dim: 64, hidden size: 4096, intermediate size: 8192, groups: 8, norm before gate: true, num heads: 128, num hidden layers: 64, state size: 128 num hidden layers: 72, hidden size: 8192, intermediate size: 24576, num attention heads: 64, num key value heads: 8, mamba state: 16, mamba conv: 4, mamba expand: 2, mamba conv bias: true, num experts: 16, num experts per tok: 2, attention:mamba = 1:7, number layers per block: 8 num local: 4096, num initial: 128, num dense rows: num local: 4096, num initial: 128 Pattern search data: KV retrieval a-shape: 1024/4096 vertical-slash: 30/2048, 100/1800, 500/1500, 3000/200 block-sparse: 100 blocks KV Cache Compression KV Cache Quantization KV Cache Retrieval KV Cache Loading StreamingLLM num local: 4096, num initial: 128 PyramidKV SnapKV KIVI CacheBlend window size: 32, max capacity prompt: 4096, kernel size: 5, pooling: avgpool window size: 32, max capacity prompt: 4096, kernel size: 5, pooling: avgpool bit size: 2; group size: 32 residual length: 32 recompute ratio: 15%, chunk size: 512 Quest chunk size: 16; token budget: RetrievalAttention topk: 2000; index type: IVF index Prompt Compression LLMLingua-2 compression rate: 0.333 implemented on Triton (Tillet et al., 2019). vLLM-0.52 is used as the inference framework in our testing, and the flash_attn-2.5 kernels were overwritten with our own kernels. For KV Cache compression, our implementation is based on the huggingface implementation of SinkCache for StreamingLLM3, and official implementation of 4. For SSMs and Mamba-Attention Hybrid models, we use the triton version of mamba5 kernels together with causal-conv1d-1.46. For prompt compression, we use the official implementation of LLMLinugua-27 to compressed the prompt first then use vLLM for further inference."
        },
        {
            "title": "D ADDITIONAL EXPERIMENT RESULTS",
            "content": "The results for Llama-3.1-70B, Qwen2.5-32B, and Llama-3-8B-262K are shown in Table 10. We can found similar the following key insights from Table 10. MInference consistently outperforms other approaches across tasks, particularly in multi-turn mode, demonstrating strong results in both retrieval and multi-task scenarios. Sparse attention methods like A-shape and Tri-shape show promise, with Tri-shape excelling in multi-request mode due to its integration of bottom query tokens, which boosts first-turn performance and improves instruction-following. However, Tri-shapes advantage decreases slightly in multi-task settings, although it still ranks second overall. KV cache compression methods underperform in shared contexts, offering minimal gains, especially in retrieval and global information tasks, with SnapKV showing particularly poor results. Prompt compression methods perform well in tasks requiring global context, such as many-shot ICL, but struggle significantly 2https://github.com/vllm-project/vllm 3https://huggingface.co/docs/transformers/main/en/kv_cache#sink-cache 4https://github.com/FasterDecoding/SnapKV 5https://github.com/state-spaces/mamba 6https://github.com/Dao-AILab/causal-conv1d 7https://github.com/microsoft/LLMLingua 24 Preprint. Under review. Table 10: The average results of various long-context methods on Llama-3.1-70B, Qwen2.5-32B, and Llama-3-8B-262K with two shared context modes on SCBench. Methods Llama-3-8B-262K A-shape Tri-shape MInference StreamingLLM SnapKV LLMLingua-2 Llama-3.1-70B A-shape Tri-shape MInference StreamingLLM SnapKV LLMLingua-2 Qwen2.5-32B A-shape Tri-shape MInference StreamingLLM SnapKV LLMLingua-2 Retr.String Retr.Semantic Global Multi-task AVG. Retr.String Retr.Semantic Global Multi-task AVG. Multi-turn Mode Multi-request Mode 29.2 9.9 11.1 17.5 0.5 0.5 3.4 20.9 4.8 6.7 19.5 0.2 0.7 6.7 46.8 15.0 18.5 35.4 0.2 3.3 3.4 33.3 27.2 29.6 33.5 12.6 4.2 21.0 45.4 34.7 37.1 42.5 6.4 3.7 38. 42.6 33.8 34.6 39.9 4.3 3.9 28.2 26.7 25.6 26.3 26.7 22.6 21.9 24.5 45.7 40.5 42.0 43.1 22.8 25.0 38.7 40.6 38.7 40.4 40.8 8.4 27.1 38.9 63.5 55.6 60.6 66.0 10.1 0.5 23.0 70.3 26.9 31.1 65.6 3.7 1.5 31. 73.4 59.5 64.0 69.9 6.3 1.5 26.9 38.2 29.6 31.9 36.2 11.4 6.7 18.0 45.6 26.7 29.2 42.4 8.3 7.7 28.8 50.9 36.7 39.4 46.5 4.8 9.0 24.3 17.1 7.8 8.2 8.3 0 0.0 3.9 3.1 3.2 3.8 7.3 0.0 0.1 4. 25.0 9.6 11.7 17.7 0.0 0.0 2.7 30.0 27.3 22.4 32.1 1.0 1.1 24.4 47.9 35.7 40.5 43.7 10.9 14.0 32.0 44.5 34.1 37.4 42.7 1.8 4.9 26.6 25.5 22.0 22.5 25.6 22.6 24.5 42.4 48.1 46.3 46.5 48.2 31.2 36.9 38. 55.3 53.7 56.4 56.4 7.4 9.8 36.5 34.1 35.2 35.9 40.0 0.1 0.1 22.6 47.8 33.8 34.2 46.1 0.0 0.0 26.7 49.9 38.6 41.1 48.6 0.0 0.0 22.4 26.7 23.1 22.3 26.5 5.9 6.4 23.3 36.7 29.7 31.2 36.3 10.5 12.8 25. 43.7 34.0 36.7 41.4 2.3 3.7 22.1 Figure 10: Performance of different long-context methods across various turns in Multi-tasking tasks on SCBench. The results are averaged across all tested base LLMs. in retrieval tasks, leading to performance degradation. Meanwhile, StreamingLLM and SnapKV consistently deliver the weakest results, particularly in multi-turn mode, indicating they are not wellsuited for long-context tasks with repeated requests. Overall, methods like Tri-shape and MInference, which combine sparse attention and efficient token management, demonstrate the most consistent improvements, while compression-focused approaches show limited effectiveness in more dynamic or retrieval-heavy tasks. In Table 11 showcases the performance of various methods across range of tasks, including retrieval (Retr.KV, Retr.PS), QA (En.QA, Zh.QA), summarization (En.Sum), code understanding and function retrieval (RepoQA), math, and in-context learning (ICL). Each method demonstrates varying strengths and weaknesses across these domains. Retrieval tasks (Retr.KV, Retr.PS), which test exact information retrieval ability, are dominated by methods such as GLM-4-1M and MInference. GLM-4-1M consistently performs well in these tasks, with Retr.KV at 49.0 and Retr.PS at 39.2. MInference also demonstrates strong performance in retrieval, particularly with score of 51.2 in Retr.KV. However, methods like StreamingLLM and Preprint. Under review. Table 11: The results breakdown of SCBench for all sub-tasks in multi-turn mode. Methods Retr.KV Retr.PS Math.Find RepoQA En.QA Zh.QA En.MC ICL EN.Sum Math Mix.Sum +NIAH Mix.RepoQA +KV GLM-4-1M MInference A-shape Tri-shape StreamingLLM SnapKV LLMLingua-2 Llama-3.1-8B MInference A-shape Tri-shape StreamingLLM SnapKV LLMLingua-2 Llama-3-8B MInference A-shape Tri-shape StreamingLLM SnapKV LLMLingua-2 Llama-3.1-70B MInference A-shape Tri-shape StreamingLLM SnapKV LLMLingua-2 Qwen2.5-72B MInference A-shape Tri-shape StreamingLLM SnapKV LLMLingua-2 Qwen2.5-32B MInference A-shape Tri-shape StreamingLLM SnapKV LLMLinguaJamba-1.5-Mini Codestral-Mamba 49.0 51.2 25.2 32.2 0.0 0.2 0.0 80.8 70.8 17.8 24.2 0.2 0.0 0.0 24.0 16.0 2.2 3.4 0.8 0.0 0.0 27.2 28.0 1.2 2.8 0.0 0.0 0.0 40.8 43.4 17.4 21.0 0.0 0.0 0. 56.4 27.8 14.4 18.2 0.0 0.0 0.0 67.4 0.0 39.2 28.6 42.4 47.8 0.0 0.0 1.6 42.8 15.6 5.6 7.0 0.0 0.0 1.6 15.8 3.6 2.0 3.8 0.0 0.0 0.4 1.6 1.0 0.0 0.2 0.0 0.0 4. 62.2 46.4 32.0 31.4 0.0 0.0 3.2 39.4 27.8 13.6 16.6 0.0 0.0 4.0 28.6 0.0 58.6 34.8 14.0 14.4 0.0 26.0 15.8 47.6 30.8 18.5 23.2 0.1 0.0 15.4 47.8 32.8 25.4 26.2 0.7 1.4 9. 33.8 29.4 13.2 17.1 0.5 2.2 16.0 51.5 47.0 22.7 24.8 1.2 3.4 9.3 44.7 50.6 16.9 20.8 0.7 10.0 6.2 37.5 0.4 60.5 53.4 42.3 44.8 0.2 0.5 2.0 40.4 48.2 34.3 34.5 0.5 0.0 2. 41.8 42.5 32.3 33.4 0.0 0.0 1.1 67.0 60.2 50.0 50.5 0.4 0.0 31.6 65.5 59.3 45.9 48.0 0.2 0.0 4.3 64.5 57.5 46.4 47.3 0.4 0.0 6.7 47.5 0.0 33.6 33.3 28.1 28.1 5.2 13.9 5. 29.3 30.1 21.2 24.6 8.7 1.7 23.5 29.0 30.1 21.7 24.1 9.6 1.2 21.2 35.4 33.0 27.0 28.0 6.0 1.3 33.6 40.0 41.2 31.9 32.8 3.8 0.3 32.5 37.1 34.5 30.1 30.1 3.3 0.3 31.4 32.8 5. 15.2 15.0 14.1 15.6 2.0 2.4 3.5 21.1 22.5 17.1 20.6 10.5 1.9 23.0 13.8 12.3 12.7 12.8 1.3 1.3 13.4 20.7 23.3 18.0 18.7 0.8 1.5 22.5 10.9 11.4 12.8 12.6 0.5 1.0 14.7 6.0 8.0 4.6 6.8 0.0 0.8 15. 21.7 5.1 50.2 47.0 49.3 47.0 42.4 49.6 44.1 50.0 32.2 70.0 34.2 70.0 20.1 45.6 57.0 42.6 57.9 49.3 46.2 48.1 50.5 48.1 39.1 68.9 17.7 42.6 61.5 50.4 53.1 30.7 53.5 32.6 46.6 32.2 48.3 33.3 48.8 58.9 17.7 62.2 57.2 34.8 62.2 58.5 57.4 54.4 46.7 52.2 55.5 55.6 23.0 61.1 14.9 64.5 74.7 56.7 65.7 66.7 67.0 64.1 52.7 64.4 57.5 64.8 63.9 19.3 70.8 34.2 73.8 72. 68.3 75.9 65.3 76.3 54.1 76.7 59.6 76.3 17.0 21.1 18.1 37.4 66.7 66.7 61.8 38.9 21.8 33.3 37.8 14.4 37.7 10.6 32.9 9.6 34.1 12.2 3.0 5.8 7.2 6.5 9.4 32.8 40.7 22.0 39.8 14.2 33.4 13.4 35.2 17.2 9.6 27.2 3.4 4.2 35.4 11.2 37.5 11.8 37.0 10.4 32.8 11.8 32.7 12.8 4.6 4.2 1.4 2.1 7.2 31.6 41.2 37.4 39.8 35.2 36.5 32.8 37.0 33.4 3.8 3.6 1.9 8.8 37.1 22. 37.9 12.2 38.2 12.8 33.7 12.0 35.8 12.4 0.0 3.9 2.0 0.0 33.1 33.2 35.5 10.4 35.8 10.4 30.6 8.8 33.8 11.2 3.6 0.6 2.3 41.6 29.5 20.4 48.0 18.0 5.6 4.0 67.4 68.3 65.3 64.6 12.7 41.6 48.2 60.7 56.3 50.8 50.8 28.9 4.1 49. 67.0 68.6 64.8 65.3 20.1 0.9 45.9 62.1 52.1 35.3 38.3 7.0 2.2 1.4 71.9 72.0 69.4 70.0 14.5 2.7 53.8 69.8 70.7 67.2 68.2 12.5 2.7 52.7 71.0 12.4 78.2 73.4 51.8 63.4 0.0 0.9 0. 70.7 59.3 16.6 25.0 0.5 0.0 49.6 60.0 63.4 46.4 55.9 0.0 0.0 0.2 78.4 77.0 18.6 23.9 0.3 0.7 60.6 82.0 73.6 46.6 57.5 0.5 0.5 0.9 77.0 69.1 51.8 59.8 0.1 0.4 1.1 71.6 0. SnapKV show almost no retrieval capability, with near-zero scores, indicating poor handling of exact information recall. For natural language tasks like QA (En.QA, Zh.QA) and summarization (EN.Sum), we see different pattern. GLM-4-1M and Qwen2 models excel in these areas, particularly in English and Chinese QA tasks. For example, Qwen2-72B achieves scores of 40.0 in En.QA and 66.7 in EN.Sum, indicating strong natural language processing abilities. MInference also performs well but is slightly behind GLM-4-1M and Qwen2, with comparable scores. Interestingly, methods like Tri-shape and A-shape show moderate performance in QA but underperform in summarization tasks compared to the top performers. In code understanding tasks (RepoQA), GLM-4-1M leads with score of 60.5, followed by Qwen2-72B at 65.5, demonstrating strong capabilities in handling structured language and retrieving functional information. Methods like MInference (53.4) and Tri-shape (44.8) perform moderately well, while StreamingLLM and SnapKV are almost ineffective, scoring near zero. This suggests that StreamingLLM and SnapKV struggle with code-related tasks requiring structured reasoning. In math tasks, MInference and GLM-4-1M are the top performers, with scores of 34.8 and 58.6, respectively, showing proficiency in handling mathematical reasoning. However, methods like Trishape and A-shape struggle in math tasks, indicating that these sparse attention mechanisms may not 26 Preprint. Under review. Table 12: The results breakdown of SCBench for all sub-tasks in multi-requests mode. Methods Ret.KV Ret.PS Ret.MH RepoQA En.QA Zh.QA EN.MC ICL EN.Sum Math.Find Mix.Sum +NIAH Mix.RepoQA +KV GLM-4-1M MInference A-shape Tri-shape StreamingLLM SnapKV LLMLinguaLlama-3.1-8B MInference A-shape Tri-shape StreamingLLM SnapKV LLMLingua-2 Llama-3-8B MInference A-shape Tri-shape StreamingLLM SnapKV LLMLingua-2 Llama-3.1-70B MInference A-shape Tri-shape StreamingLLM SnapKV LLMLingua-2 Qwen2.5-72B MInference A-shape Tri-shape StreamingLLM SnapKV LLMLingua-2 Qwen2.5-32B MInference A-shape Tri-shape StreamingLLM SnapKV LLMLingua-2 Jamba-1.5-Mini Mamba-Codestral 50.6 46.8 26.2 34.0 0.0 0.0 0.0 56.2 48.6 0.2 4.0 0.2 0.2 0.0 11.8 6.0 0.6 1.2 0.0 0.0 0.0 2.4 3.4 0.2 0.2 0.0 0.2 0.0 37.8 40.4 13.2 17.2 0.0 0.0 0.0 27.2 27.8 11.0 14.2 0.0 0.0 0. 64.4 0.0 44.6 40.2 25.8 30.4 0.0 0.0 1.6 16.8 15.6 0.0 0.2 0.4 0.4 1.6 4.0 0.6 0.2 0.2 0.0 0.0 1.6 0.0 0.0 0.0 0.0 0.0 0.0 2.8 45.2 28.6 22.0 25.4 0.0 0.0 2. 23.0 12.8 7.0 9.2 0.0 0.0 2.8 15.2 0.0 39.2 15.4 8.6 12.0 0.0 0.0 3.0 15.5 22.5 9.3 19.2 0.4 0.4 10.1 35.6 18.3 22.5 23.2 0.0 0.0 10.1 7.0 18.5 9.3 11.2 0.0 0.0 10. 10.2 16.9 10.4 13.1 0.0 0.0 5.3 24.9 12.6 10.7 11.8 0.0 0.0 5.3 29.7 8.4 54.3 45.0 39.5 40.5 0.0 0.0 1.8 45.0 43.2 33.9 20.3 0.0 0.0 1.6 22.7 31.4 25.5 26.1 0.0 0.0 1. 62.5 57.3 43.9 44.5 0.0 0.0 6.7 64.3 56.4 42.7 44.1 0.5 2.7 6.7 60.2 55.0 43.6 45.7 0.0 0.0 2.2 51.4 0.2 32.8 30.5 24.5 25.1 7.7 8.9 24.2 25.1 23.6 25.6 17.9 7.6 14.3 19. 28.2 26.5 22.2 23.6 3.8 4.3 19.9 32.2 30.4 25.6 28.5 9.8 11.7 32.2 37.0 38.5 29.3 31.6 5.4 11.0 35.1 35.6 34.2 26.5 28.5 3.4 12.1 29.7 31.9 8.5 5.0 5.0 4.5 5.3 0.3 0.9 4. 9.8 12.5 13.7 10.1 5.9 6.1 14.5 8.1 8.6 8.5 9.2 0.1 0.1 14.5 18.3 16.5 13.7 20.1 8.4 7.0 17.1 3.8 4.1 3.7 3.8 1.6 1.1 3.8 3.0 3.0 2.8 3.0 0.8 1.7 3.7 19.6 2. 32.3 70.4 35.4 67.8 27.9 69.3 30.1 68.1 3.8 56.7 4.0 63.3 28.4 70.7 65.9 54.1 62.9 62.6 59.8 59.6 54.6 60.4 16.4 45.2 18.2 32.3 61.6 73.0 61.1 33.0 62.0 33.0 52.8 28.9 30.7 30.7 0.0 67.8 0.0 73.3 61.6 76.7 78.6 67.4 70.5 59.4 59.8 59.6 69.0 58.9 25.3 66.3 37.4 76.7 72.1 50.0 82.1 74.1 79.9 68.5 66.4 67.8 73.8 68.1 9.4 8.2 10.1 13.7 79.2 76.7 79.0 84.1 78.6 85.2 63.3 81.9 72.5 83.0 3.0 5.9 5.9 13.3 70.8 60. 75.1 35.6 24.5 42.6 38.5 38.9 31.5 34.7 0.1 0.1 33.1 38.3 36.6 30.1 29.2 6.9 7.3 33.7 36.8 36.4 31.1 31.7 0.1 0.2 33.7 38.4 34.3 30.1 33.3 18.7 19.9 35.0 41.6 42.2 38.1 39.5 5.1 7.2 36. 37.3 37.8 31.9 34.2 12.8 13.7 31.6 37.0 6.4 21.2 23.5 20.7 21.4 2.9 5.9 11.6 38.4 51.0 49.2 47.2 2.7 4.3 17.0 6.9 7.3 6.2 5.2 0.0 0.0 17.0 38.4 51.0 49.2 47.2 8.6 14.2 30. 43.2 45.8 37.3 37.9 0.0 0.0 34.2 44.4 46.2 47.2 52.0 3.6 2.5 18.0 25.2 2.6 66.2 66.9 63.1 63.0 0.0 0.0 48.6 55.4 45.9 43.6 38.2 0.0 0.0 42.9 53.5 60.4 55.4 56.8 0.1 0.0 42. 62.2 61.1 45.7 44.7 0.0 0.0 50.7 71.1 71.3 68.0 69.2 0.0 0.0 48.9 68.7 60.0 44.5 47.7 0.0 0.0 44.9 68.5 9.6 29.8 29.8 22.0 23.0 0.0 0.0 0.9 23.0 15.9 11.9 10.9 0.0 0.0 2. 14.8 19.5 15.0 15.0 0.0 0.2 2.3 33.4 31.2 21.8 23.6 0.0 0.0 2.8 33.6 32.7 18.2 20.7 0.0 0.0 2.8 31.1 37.2 32.7 34.5 0.0 0.0 0.0 27.7 0.5 generalize well to numerical reasoning. StreamingLLM and SnapKV again show little to no ability in math, with minimal scores across the board. Finally, in in-context learning tasks, where the models ability to generalize and adapt is tested, GLM-4-1M and Qwen2 models stand out. Qwen2-72B achieves high score of 66.7, while GLM-41M also scores well at 47.0, indicating strong adaptability. MInference, Tri-shape, and A-shape show moderate ICL performance, but methods like SnapKV and LLMLingua-2 lag significantly, reflecting their limited generalization capabilities in ICL. Overall, GLM-4-1M and MInference consistently perform well across most tasks, especially in retrieval, QA, and ICL, with the Qwen2 models also excelling in natural language processing and in-context learning. Sparse attention methods like A-shape and Tri-shape show moderate performance in specific areas, while methods like StreamingLLM and SnapKV consistently underperform across the board, particularly in tasks requiring retrieval and code understanding. In Table 12, we present the results breakdown for the multi-request mode. Comparing the performance across multi-turn and multi-request modes, we found the following key differences, particularly in retrieval tasks. In multi-turn mode, methods like GLM-4-1M and MInference demonstrate strong retrieval capabilities, with high scores in Ret.KV (49.0 and 51.2, respectively). However, in multirequest mode, these methods show varied results, with MInference dropping to 46.8 in Ret.KV and 27 Preprint. Under review. GLM-4-1M slightly improving to 50.6. Sparse attention methods like A-shape and Tri-shape perform relatively poorly in both modes but exhibit more stable results across multiple requests. Notably, the performance of MInference in math tasks significantly improves in multi-request mode (from 34.8 to 51.0), indicating its ability to adapt better over repeated queries. In contrast, methods such as StreamingLLM and SnapKV remain consistently weak across both modes, particularly in retrieval and math tasks, showing near-zero scores, reflecting their inability to handle dynamic multi-request contexts effectively. Overall, methods like MInference and GLM-4-1M maintain their dominance across both modes, but their adaptability in multi-request mode is crucial for retrieval-heavy and computational tasks. Note that we did not run ERROR PROPAGATION USING GENERATION AS CONTEXT. Table 13: Results when disabling golden answer as context. The later number indicate the gap compared to goldenanswer-as-context. Following Zheng et al. (2023a); Wang et al. (2024), in our multi-turn testing, we use the golden answer instead of the model generation as the context for the next query. This prevents potential interference from misleading generations in subsequent turns. However, this approach naturally provides an in-context learning environment where the model can learn from previous turns in answering later queries. Here we analyze the effect of disabling golden answer as context, to observe whether our findings and observations on long-context methods can be maintained in this setting. Llama-3.1-8B 32.4 /-2 16.5 /-1 A-shape Tri-shape 27.5 /+2 StreamingLLM 14.8 /-6 34.5 /+0 MInference 36.8 /-13 23.1 /-7 24.7 /-7 5.60 /-8 26.2 /-19 41.6 /-6 15.8 /-12 17.1 /-13 2.80 /-11 25.2 /- 29.8 /-21 22.0 /-9 19.3 /-13 5.60 /-7 25.4 /-19 47.7 /+1 29.8 /+2 34.7 /+2 7.00 /-12 31.7 /-8 Turn 3 Turn 1 Turn 2 Turn Turn 4 As shown in Table 13, we have found similar results on multi-turn setting when model generation is used as context compared to our main results at Table 4: dense decoding methods perform generally better than sparse decoding. And more robust and dynamic sparse patterns achieve better metrics to static sparse methods. But using model generation as context does demonstrate lower overall accuracy which indicates the error propagation where the follow-up turns will be impacted by misleading answer from previous queries."
        },
        {
            "title": "F CASE STUDY",
            "content": "In this section, we provide more detailed analysis for individual cases. We first present case study of the task En.Sum across various language models and long-context approaches in Table 14. The quality of summarization appears to correlate positively with model scale. For example, Llama-3.1-70B and Qwen2.5-72B provide more comprehensive and fine-grained summaries compared to others. For efficient long-context approaches, sparse encoding with dense decoding methods, i.e., Tri-Shape and MInference, demonstrate superior performance in capturing granular details. On the contrary, sparse decoding method such as StreamingLLM exhibited failure, producing simply random and incoherent output. We then present the results of Retr.Prefix-Suffix task in Table 15. Interestingly, Mmaba-Attention hybrid architecture Jamba achieve the most accuracy performance. This is non-trivial as Retr.PrefixSuffix task require an rather large space and time complex and Mamba layers are reported to perform poorly on such dimensions. On the contrary, full attention LLMs such as Llama and Qwen series models all failed in this task. Although many models can still remember variable length of prefix, but they often fail reproduce the entire string. For example, Llama-70B with MInference can almost retrieve the entire string, but misspell several characters in the middle. This can be attribute to the weakness of induction head (Olsson et al., 2022) in the Transformer attention heads, it can also result from the sparse input for these efficient long-context methods. In addition, we present result for some long-context methods in the multi-tasking test, i.e., Mix.RepoQA+KV in Table 16. The ground truth provides an answer from KV retrieval and one answer from reporqa. Both Llama-3.1-70B and its variant with MInference accurately retrieved the value, demonstrating good performance on the key-value retrieval. However, their reproduction of the Python function reveals interesting differences. While both models maintain the overall structure 28 Preprint. Under review. Table 14: Case Study of En.Sum. We use blue to indicate mising informaiton, and orange to mark potential hallucination. Model Ground Truth Jamba-1.5-Mini Llama-3.1-8B Llama-3.1-70B Qwen2.5-32B Qwen2.5-72B Llama-3.1-70B w/ A-Shape Llama-3.1-70B w/ Tri-Shape Llama-3.1-70B w/ MInference Response The key contribution of the paper \"Accelerating Clinical Evidence Synthesis with Large Language Models\" is that it introduces an LLM-driven pipeline called MetaSyns to streamline the entire clinical evidence synthesis process, including study search, screening, data extraction, and evidence synthesis, while incorporating human oversight to ensure accuracy and transparency. The paper introduces generative AI-based pipeline for conducting medical systematic reviews, utilizing large language models to improve the efficiency and accuracy of clinical evidence synthesis. The paper proposes large language model-based pipeline for accelerating clinical evidence synthesis, which can efficiently search, screen, and extract relevant studies, and synthesize high-quality clinical evidence, outperforming traditional methods and achieving high recall rates of 0.897-1.000. The paper introduces clinical evidence synthesis pipeline enhanced by large language models, named [blank], which decomposes the clinical evidence synthesis process into four main tasks and incorporates human expert oversight to minimize errors, enabling reliable and high-quality clinical evidence synthesis to improve clinical research efficiency. The paper introduces method that leverages large language models (LLMs) to enhance the efficiency and quality of clinical evidence synthesis across domains, demonstrating improved performance in study searching, screening, and data extraction phases compared to traditional approaches. The key contribution of the paper \"Accelerating Clinical Evidence Synthesis with Large Language Models\" is the development of large language model (LLM)-driven pipeline that significantly improves the efficiency and accuracy of clinical evidence synthesis by automating the literature search, study screening, data extraction, and evidence synthesis processes, while maintaining human oversight to ensure reliability. The paper proposes novel approach to accelerate clinical evidence synthesis by leveraging large language models (LLMs) to streamline the clinical evidence synthesis process, enabling the creation of more representative and accurate clinical evidence. The paper introduces clinical evidence synthesis pipeline enhanced by large language models, named [blank], which can accelerate the literature review process, achieving high recall rates and outperforming traditional language model embeddings-based methods in screening, and surpassing direct GPT-4 performance in result extraction, with accuracy ranging from 0.65 to 0.84. The paper introduces generative AI-based pipeline called [blank] that leverages large language models (LLMs) to improve the efficiency and accuracy of systematic medical literature reviews by optimizing study searching, screening, data extraction, and evidence synthesis, while maintaining high performance even when dealing with novel classes from different domains. Llama-3.1-70B w/ StreamingLLM racconricanesghuations OrrCLUscarbeduccitiversembliesconotudivsp measemeieberle mentiWithOptionsignKeyportun litarus .... [just random content] Table 15: Case Study of Retr.Prefix-Suffix. Orange is used to mark the difference of model response compared to the ground truth. Model Ground Truth Jamba-1.5-Mini Llama-3.1-8B Llama-3.1-70B Qwen2.5-32B Qwen2.5-72B Response RviB-YiaAUgc_nSFCgaamszHq RviB-YiaAUgc_nSFCgaamszHq RviBmszHq RviB-+KzjzmszHq RviB-YiaAUgcNfQdtayMOTzE RviB-wYxqnlxkxICLemscw_mszHq Llama-3.1-70B w/ A-Shape Llama-3.1-70B w/ Tri-Shape Llama-3.1-70B w/ MInference Llama-3.1-70B w/ StreamingLLM incerirketystick Wetzing950elperoberenkoin [just random content] RviB-mszHq RviB-mszHq RviB-YiaAUgc_nSFseaamszHq and indentation, they introduce several modifications to the function logic. Llama-3.1-70B reproduced the wrong function name and implements brand new algorithm, yet preserves only limited original elements. The MInference variant closely mirrors the base models output, with minor differences such as the addition of Python code block identifier. Notably, neither model exactly replicates the ground truth function, suggesting challenges in precise function reproduction. But we believe the 29 Preprint. Under review. Table 16: Case Study of Mix.RepoQA + KV. Orange indicate the potential model hallucination. Model Ground Truth Llama-3.1-70B Llama-3.1-70B w/ MInference Response Leaf, Leaf, int) -> bool: int) -> bool: Line, *, last: Line, *, last: 2b0ebd59-2c68-48b7-82a9-6bdfad08be0e def _can_omit_closing_paren(line: line_length: \"\"\"See can_omit_invisible_parens.\"\"\" length = 4 * line.depth seen_other_brackets = False for _index, leaf, leaf_length in line.enumerate_with_length(): length += leaf_length if leaf is last.opening_bracket: if seen_other_brackets or length <= line_length: return True elif leaf.type in OPENING_BRACKETS: # There are brackets we can further split on. seen_other_brackets = True return False 2b0ebd59-2c68-48b7-82a9-6bdfad08be0e def can_omit_closing_paren(line: line_length: \"\"\"See can_omit_invisible_parens.\"\"\" remainder = False length = 4 * line.depth _index = -1 for _index, leaf, leaf_length in line.enumerate_with_length(): if leaf.type in CLOSING_BRACKETS and leaf.opening_bracket is last: remainder = True if remainder: length += leaf_length if length > line_length: break if leaf.type in OPENING_BRACKETS: # There are brackets we can further split on. remainder = False else: # checked the entire string and line length wasnt exceeded if len(line.leaves) == _index + 1: return True 2b0ebd59-2c68-48b7-82a9-6bdfad08be0e python def can_omit_closing_paren(line: Leaf, line_length: \"\"\"See can_omit_invisible_parens.\"\"\" remainder = False length = 4 * line.depth _index = -1 for _index, leaf, leaf_length in line.enumerate_with_length(): if leaf.type in CLOSING_BRACKETS and leaf.opening_bracket is last: remainder = True if remainder: length += leaf_length if length > line_length: break if leaf.type in OPENING_BRACKETS: # There are brackets we can further split on. remainder = False else: if len(line.leaves) == _index + 1: return True Line, *, last: int) -> bool: results of MInference is more due to the limited long-context capability of the base Llama model instead of the sparse nature of the encoding approach. In Table 17, we also highlights the performance of A-shape and Tri-shape models in Retr.KV. Notably, Tri-shape demonstrates strong performance even in the first turn, effectively maintaining the instruction-following capabilities of the model. In contrast, A-shape significantly disrupts the models ability to follow instructions, leading to incomplete and erroneous outputs. This difference underscores Tri-shapes advantage in preserving task structure and comprehension from the outset, 30 Preprint. Under review. Table 17: Case Study of Retr.KV to compare A-shape and Tri-shape. Model Ground Truth Llama-3.1-70B Llama-3.1-70B w/ A-Shape Response Turn 1: cb59052b-9128-4979-9c0e-e1de4adcf73b Turn 2: a9ee6345-2331-4914-b32b-25a780931d Turn 1: 798c2306-5ad1-42a3-a8d7-3f5867f260c9: 5e369d13-ce48-40f9-824d-a55ef5748e9e, 5e257563-f131-4583-b72e-bd43b07a4e09: d768cb61-186a-4a56-bbbc-1c94f2d2597e, ff8fd84db1f8-4e94-b3a3-cf5f252541f9: 45e9039a-01ec-4fd3-a22e-098d1c3a2 Turn 2: e69af07e-d786-43ea-8cc4-74c452b3d66793c5c 4a0d0b3 d2e72f0c2e0b7b2a4e7f5e0d0 b7b2a4e7f5e0d0 b7b2a4e7f5e0d0b7b2a4 e7f5e0d0b7b2 a4e7f5e0d0b7b2a4e 7f5e0d0b7b 2a4e7f5e0d0b7b2a4e7f assistant<end_header_id>assistant<end_header_id> assistant<end_header_id> assisTurn 1: assistant<end_header_id>assistant tant<end_header_id>assistant<end_header_id>assistant Turn 2: 3ebf05c3-35af-483b-b46e-1bf3c67f3682 : a4a5d7a3-8a3c-4b3a-8c6d-7c0d1a4a4a <end_header_id>assistant<end_header_id> Llama-3.1-70B w/ Tri-Shape Turn 1: 6a6e0b3e-3d7c-4f33-ba46-7f42bb75b03f: 1f5eba0d-5ccf-4262-aa76-d7fbabdc0b9a Turn 2: 3ebf05c3-35af-483b-b46e-1bf3c67f3682: 1f5eba0d-5ccf-4262-aa76-d7fbabdc0b9a while A-shape tends to interfere with the models initial response, which can degrade the overall task performance."
        }
    ],
    "affiliations": [
        "Microsoft Corporation",
        "University of Surrey"
    ]
}