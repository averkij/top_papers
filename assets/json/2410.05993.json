{
    "paper_title": "Aria: An Open Multimodal Native Mixture-of-Experts Model",
    "authors": [
        "Dongxu Li",
        "Yudong Liu",
        "Haoning Wu",
        "Yue Wang",
        "Zhiqi Shen",
        "Bowen Qu",
        "Xinyao Niu",
        "Guoyin Wang",
        "Bei Chen",
        "Junnan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 2 3 9 9 5 0 . 0 1 4 2 : r ARIA : An Open Multimodal Native Mixture-of-Experts Model Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, Junnan Li(cid:0) Rhymes AI"
        },
        {
            "title": "Abstract",
            "content": "Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce ARIA, an open multimodal native model with best-in-class performance across wide range of multimodal, language, and coding tasks. ARIA is mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train ARIA from scratch following 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with codebase that facilitates easy adoptions and adaptations of ARIA in real-world applications. Code: https://github.com/rhymes-ai/Aria Website: https://rhymes.ai/"
        },
        {
            "title": "Introduction",
            "content": "In this report, we present ARIA, the first open mixture-of-experts (MoE) model that is multimodal native. The term multimodal native has been used in prior literature to refer to different model capabilities, without clear consensus. Here, we provide quantifiable definition: multimodal native model refers to single model with strong understanding capabilities across multiple input modalities (e.g. text, code, image, video), that matches or exceeds the modalityspecialized models of similar capacities. Our definition aligns with the user experience of proprietary multimodal models such as GPT-4o or Gemini-1.5, where user does not need to differentiate inputs from different modalities. Instead, the model is expected to seamlessly handle and integrate multiple modalities input with single model. While proprietary multimodal native models are not uncommon, their training recipes remain largely undisclosed. As result, most open-source models are modal-specialized or show subpar performance across modalities. In this research, we fill the gap and introduce training recipes for developing multimodal native models from scratch, which includes key aspects below: Model Architecture. The core of our model is fine-grained mixture-of-experts decoder, which enables faster training and inference speed over dense decoders, due to more efficient parameter utilization through expert specialization. ARIA MoE activates 3.5B parameters per text token and has total of 24.9B parameters. Visual input of variable length, size, and aspect is encoded as visual tokens using lightweight visual encoder of 438M parameters. ARIA has long multimodal context window of 64k tokens. (cid:0) corresponding author: junnanli@rhymes.ai Benchmark Category MMMU (val) Knowledge/Math (Multimodal) MathVista (testmini) DocVQA (test) Document/Chart/ Scene Text ChartQA (test) Understanding TextVQA (val) General Visual QA MMBench-1.1 Long Video Understanding Knowledge/Math/ Reasoning (Language) Coding EgoSchema (test) LongVideoBench (test) VideoMME (w subs) MMLU (5-shot) MATH (CoT) ARC Challenge HumanEval 1 1 - 2 . 3 l 50.7 51.5 88.4 83.4 - - - 45.7 50.2 69.4 51.9 83.4 72.6 2 1 - t 52.5 58.0 90.7 81.8 - - - 47.4 47.5 69.2 48.1 - 72. i 4 - 59.4 54.7 - - - 76.0 - 58.8 68.9 - 70.2 96.4 87.2 4 - 56.4 - 88.4 78.4 78.0 79.8 - 60.7 63.3 86.4 - - 67.0 4 - 69.1 63.8 92.8 85.7 - 82.2 72.2 66.7 77.2 89.1 76.6 96.7 90. R 54.9 66.1 92.6 86.4 81.1 80.3 66.8 65.3 72.1 73.3 50.8 91.0 73.2 l 5 . 1 - m 56.1 58.4 89.9 85.4 78.7 - 65.7 62.4 75.0 78.9 - - 74.3 5 . 1 - m 62.2 63.9 93.1 87.2 78.7 73.9 72.2 64.4 81.3 85.9 - - 84.1 Table 1: Performance comparison across various multimodal and language benchmarks. Results of competing models are collected from verified official sources or reruned with official settings. Data. ARIA is pre-trained on 6.4T language tokens and 400B multimodal tokens. We develop rigorous procedure to curate high-quality data from diverse set of sources. The multimodal pre-train data includes four major categories: interleaved image-text sequence from common crawl, synthetic image captions, documents transcriptions and question-answering pairs, synthetic video captions and questionanswering pairs. Training Pipeline. We design 4-stage training pipeline, including language pre-training, multimodal pre-training, multimodal long-context pre-training, and multimodal post-training. Each stage is designed to progressively enhance certain model capabilities while maintaining those acquired in early stages. Our pipeline efficiently and effectively exploits the data and compute resources to maximize model performance. Following this recipe, ARIA demonstrates state-of-the-art performance as an open multimodal native model. Compared to Pixtral-12B [Mixtral, 2024] and Llama3.2-11B [Dubey et al., 2024], Aria demonstrates superior performance across wide range of multimodal, language, and coding tasks, while enjoying lower inference cost due to the fewer number of activated parameters. In addition, ARIA also performs on par with proprietary models such as GPT-4o and Gemini-1.5 on various multimodal tasks. The detailed benchmark results are present in Table 1. We release ARIA under the Apache 2.0 license, free for both academic and commercial use. To facilitate easier adoption, we open-source training framework that enables finetuning ARIA on wide variety of data sources and formats, using as few as one GPU."
        },
        {
            "title": "2 Model",
            "content": "2.1 Fine-Grained Mixture-of-Experts MoE has emerged as preferred architecture over dense models for building computeefficient large language models [Fedus et al., 2022, Jiang et al., 2024, Dai et al., 2024, Ludziejewski et al., 2024]. The core idea of MoE is to replace each feed-forward layer (FFN) in Transformer with set of experts, where each expert is structurally identical to 2 an FFN. Each input token is routed to only subset of experts in each layer. The sparsity of expert activation ensures computational efficiency of an MoE layer. Due to the vast diversity of multimodal data, we hypothesize that expert specialization is important for an multimodal MoE to understand input from different data distributions. To this end, we use large number of fine-grained experts with smaller FFN hidden dimension than standard FFNs, similar to [Dai et al., 2024]. In particular, ARIA has 66 experts in each MoE layer, 2 of the 66 experts are shared among all inputs to capture common knowledge, whereas 6 more experts are activated for each token by router module. Table 2 shows the detailed architectural configuration. ARIA is significantly different from previous multimodal MoEs which either design modality-specific expert architectures or rely on upcycling from dense models [Lin et al., 2024b, Shen et al., 2023, Lin et al., 2024a]. Our multimodal native MoE is pre-trained from scratch with modality-generic experts. In Section 4.2, we show that multimodal expert specialization naturally arises after pre-training. #total parameters 24.9B #activated parameters 3.5B #experts 2+64 #activated experts 2+6 28 Table 2: Architectural configuration of our MoE decoder. denotes shared experts. 2560 expert FFN dim hidden dim #layers 2.2 Visual Encoder We design lightweight visual encoder to convert visual inputs (i.e. images or video frames) into continuous visual tokens with the same feature dimension as word embeddings, which enables the MoE to seamlessly integrate visual and language inputs. Drawing inspiration from previous work [Li et al., 2023, Bai et al., 2023, Laurençon et al., 2024], our visual encoder consists of Vision Transformer (ViT) and projection module. The ViT accepts images in their native aspect ratio as variable-length sequences of patches [Lee et al., 2023, Dehghani et al., 2023], which preserves the inherent information structure in images. We categorize image size into three ranges: (1) medium-resolution images, where the longer edge is resized to 490 pixels; (2) high-resolution images, where the longer edge is resized to 980 pixels; (3) ultra-high-resolution images, where an image is dynamically decomposed into multiple high-res images, following strategy similar to Liu et al. [2024]. We initialize the weights of our ViT using the SigLIP-SO400M model [Zhai et al., 2023] and continue pre-train the ViT on our multimodal data. Our projection module transforms the sequence of image embeddings from the ViT into sequence of visual tokens. It comprises single cross-attention layer and FFN layer. The cross-attention layer employs set of trainable vectors as queries and the image embeddings as keys. Medium-resolution images are processed by 128 queries, whereas high-resolution images are processed by an additional 128 queries (256 queries in total). The outputs from the cross-attention layer are then fed to an FFN, which then outputs visual tokens for the MoE decoder to further process. 2.3 Infrastructure ARIA is trained on an extensively modified Megatron framework [Shoeybi et al., 2019]. We eschew pipeline parallelism and instead implement combination of expert parallelism [Lepikhin et al., 2020] and ZeRO-1 data parallelism [Rajbhandari et al., 2020] to optimize performance. Due to the carefully designed parallelism method and the small model size, ARIA can be effectively trained without using tensor parallelism, which significantly reduces communication overhead and enhances training efficiency. We implement load balancing loss to prevent routing collapse and encourage balanced expert activation. We find that the expert-level load balancing loss in previous work [Fedus et al., 2022, Dai et al., 2024] is overly restrictive for our MoE due to the large number of experts. Therefore, we relax the load balancing to groups of experts, where each group contains 8 fine-grained experts. We also employ z-loss [Zoph et al., 2022] to stabilize training."
        },
        {
            "title": "3 Training",
            "content": "In this section, we delineate our 4-stage training pipeline. In each stage, the model aims to learn new capabilities while maintaining those acquired previously. We perform evaluation during each stage to ensure that such goal is achieved in data-efficient and computeefficient way. 3.1 Language Pre-training The first stage pre-trains the MoE decoder with large amount of curated language data converted into discrete text tokens, using next-token prediction loss, which enables the MoE to learn general knowledge about the world. The context window length is 8K tokens. Language Data. Our language pre-training data contains 6.4T tokens in total, curated from variety of data sources containing knowledge until May 2024. We de-duplicate the data at different granularities and perform rigorous quality filtering, using combination of rulebased approach and model-based quality classifiers. To enhance models in-context learning capability, we employ data clustering and pack similar data in the same sequence during training, akin to the approach in Shi et al. [2023]. However, their original method is less scalable and likely to generate numerous long-tail structures when processing trillions of tokens. Instead, we utilize minimum spanning tree algorithm for language data clustering, which resulted in noticeable performance gain. 3.2 Multimodal Pre-training The second stage pre-trains the MoE decoder and the visual encoder with mixture of language and multimodal data, using the same next-token prediction loss. This stage aims to enable the model with broad multimodal understanding abilities, while maintaining or even improving its language understanding. To this end, the language data contains high-quality subset of 1T tokens, covering topics including code, reasoning, and knowledge. The multimodal data contains 400B tokens from diverse set of sources, which can be categorized into four major categories below. Interleaved image-text web data. We extract and filter web pages from Common Crawl. The filtering process first removes web pages with low image or text quality. Then, it deduplicate images, and removes web pages where the images and the text have low overall CLIP score [Radford et al., 2021]. Additionally, we adjust the position of the images in the sequence, by moving an image to the front of sentence if the sentence has higher CLIP score and is in front of the image. In total, we curate 190B interleaved image-text tokens. Synthetic image captions. Alt texts directly extracted for web images are generally short, less descriptive, and noisy. It has been shown in previous work that synthetic data at scale can improve multimodal pre-training [Li et al., 2022]. We thus synthesize image captions using small model which has learned to generate longer and more descriptive image captions by re-writing the alt texts. We create synthetic captions for 300M images in the LAION-400M dataset Schuhmann et al. [2021], resulting in total of 70B multimodal tokens. Document transcriptions and QA. To improve the models capability of understanding text-heavy images, we transcribe document images into texts using public OCR methods. We also render images using plain text, chart json or table/equation latex code. In order to enhance the models ability to not only transcribe text but also understand its meaning, we use language model to create synthetic question-answering pairs. In total, our multimodal document data contains 102B tokens. Video captions and QA. We collect 4.4M videos of varying lengths from diverse range of sources. We train model to generate frame-level dense descriptions for the videos. Then, we use language model to generate question-answering pairs and video summarizations based on the dense video descriptions. In total, our video data contains 35B tokens. We select samples within 8K length for multimodal pre-training. 4 Model #Params activated (total) LongVideoBench test val VideoMME subs w/o subs MMLongBench-Doc acc f1 Open-source ARIA Qwen2-VL-7B Idefics2 MiniCPM-V-2.6 Llama3.2-11B Pixtral-12B InternVL-Chat-V1.5 InternVL2-40B LLaVA-OneVision-72B Qwen2-VL-72B Proprietary Gemini-1.5-Flash Gemini-1.5-Pro GPT-4o mini GPT-4o 3.9B (25.3B) 7B 8B 8B 11B 12B 26B 40B 72B 72B - - - - 65.3 56.8 49.4 55.7 45.7 47.4 51.7 60.6 63.2 61.7 62.6 64.4 58.8 66.7 63.0 55.6 49.7 54.9 45.5 44.9 51.2 59.3 61.3 60.4 61.4 64.0 56.5 66. 72.1 69.0 - 63.7 49.5 47.5 52.4 62.4 69.6 77.8 75.0 81.3 68.9 77.2 67.6 63.3 - 60.9 46.0 40.7 50.7 61.2 66.3 71.2 60.3 75.0 64.8 71.9 28.3 21.3 7.0 11.5 13.8 6.4 14.6 18.2 - 33.3 27.0 28.2 29.0 42. 24.6 22.7 6.8 11.6 11.3 6.0 13.0 17.9 - 35.7 21.3 20.6 28.6 44.9 Table 3: Evaluation of long-context multimodal understanding on videos and documents. Results of competing models are collected from verified official leaderboards or reruned with official settings. 3.3 Multimodal Long-Context Pre-training In this stage, we pre-train on long sequences to extend the models context window to 64K tokens. Language long-sequence data is selected from the pre-train data source. Multimodal long-sequence data contains long videos, long documents and synthetic long sequences constructed from short multimodal data. In particular, we concatenate sequence of independent images as input, and concatenate their image descriptions as target. This stage consumes 12B language tokens and 21B multimodal tokens, where 69% of the 33B tokens are long sequences. We increase the RoPE base frequency hyperparameter from 100K to 5M. After this stage, the model perfectly solves the needle-in-a-haystack task [Kamradt, 2023] for up to 64K context window. It also demonstrates substantial performance improvement on long video understanding and long multimodal document understanding tasks. 3.4 Multimodal Post-training The final post-training stage anneals the learning rate to converge the model. The learning focuses on improving the models question-answering and instruction-following capabilities, using mixture of high-quality open-source datasets and human-annotated datasets, covering domains including multimodal, code, math, and reasoning. This stage digests 20B tokens in total."
        },
        {
            "title": "4 Evaluation and Analysis",
            "content": "4.1 Benchmark Results In Table 1, we compare ARIA against leading open models of similar scale and proprietary models across variety of established benchmarks. In Table 3 and Table 4, we examine the long-context multimodal understanding and instruction following capability, respectively. Based on the evaluation result, we highlight the following key observations. ARIA is the best-in-class open multimodal native model, showing clear advantages over Pixtral-12B and Llama3.2-11B across wide range of multimodal, language, and coding tasks. ARIA is competitive against proprietary models on various multimodal tasks, including document understanding, chart reading, scene text recognition, and video understanding. ARIA excels in long-context multimodal understanding. Real-world multimodal data is complex by nature and often involves long sequences of interleaved vision-language input, 7 - - 2 Q s 3 - A B 2 1 - t 2 fi 5 . 1 - C - r I 4 3 - N L - - 5 . 2 - C M - 0 . 1 - m r - R n - 3 - a 4 - MIA-Bench (Multimodal) 8.76 7.60 8.07 5.14 8.43 7.54 7.56 7.63 7.06 7.70 7.94 8.86 MT-Bench (Language) - 8.53 6.27 6.41 7.68 - - - - - - - Table 4: Evaluation of instruction following capabilities. Results of competing models are copied from Qian et al. [2024] for MIA-Bench and Mixtral [2024] for MT-Bench. such as videos with subtitles or multi-page documents. ARIA excels in understanding such data, significantly outperforming open models such as Qwen2-VL-7B [Bai et al., 2023] and LLaVA-OneVision-72B [Li et al., 2024]. Compared to proprietary models, ARIA outperforms GPT-4o mini in long video understanding [Wu et al., 2024], and is superior to Gemini-1.5Flash in long document understanding, making ARIA preferable choice for processing long multimodal data in compute-efficient and time-efficient manner. ARIA has strong instruction following capabilities, outperforming other open models on both multimodal and language-only benchmarks. See Section 4.3 for qualitative examples. 4.2 Expert Modality Specialization We analyze the expert activation behavior across all layers in ARIA MoE after the multimodal pre-training stage. We use multimodal data from three domains for analysis: natural image, video, and PDF-format image. For each expert, we first compute its activation rate for both visual tokens and text tokens, denoted as Rv and Rt. Rv refers to the number of visual tokens that activates the expert divided by the total number of visual tokens processed by all experts of that layer, and Rt refers to the same meaning for text tokens. Then we compute the ratio Rv/Rt, which represents the experts level of visual specialization. higher visual specialization suggests that the expert is more frequently activated by visual tokens compared to text tokens. Figure 1 shows visualization of the expert visual specialization value (capped at 50). It is observed that number of visual-specialized experts exist in most layers. Furthermore, multiple layers (layer 4, 5, 14, 15, 16, 17, 20) have single visual expert specialized in all three visual domains. This analysis shows that despite its modality-generic architecture, our MoE learns effective expert utilization during pre-training. 6 Figure 1: Visualization of expert specialization in each MoE layer. 7 4.3 Qualitative Analysis 4.3.1 Multimodal Native Reasoning with Vision, Language, Coding Capabilities ARIA Example - Weather Forecast Information Extraction User Organize the highest temperature, lowest temperature, and humidity for all even dates into table. Write python code to draw line chart for the temperatures, and bar chart for the humidity. Use red and green colors for the temperatures, and use blue color for the humidity. ARIA Table of values for even dates: import matplotlib.pyplot as plt # Data for even dates days = [20 Sep, 22 Sep, 24 Sep, 26 Sep, 28 Sep, 30 Sep] high_temps = [33, 33, 32, 29, 27, 27] low_temps = [26, 26, 26, 24, 26, 26] humidity = [62, 63, 70, 76, 69, 66] # Line chart for temperatures plt.figure(figsize=(14, 6)) plt.subplot(1, 2, 1) plt.plot(days, high_temps, r-, label=Highest Temperature, marker=o) plt.plot(days, low_temps, g-, label=Lowest Temperature, marker=o) plt.xlabel(Days) plt.ylabel(Temperature (C)) plt.title(Temperature Variation) plt.legend() plt.grid(True) 8 ARIA Example - Weather Forecast Information Extraction ARIA (continued) # Bar chart for humidity plt.subplot(1, 2, 2) plt.bar(days, humidity, color=blue, alpha=0.7) plt.xlabel(Days) plt.ylabel(Humidity (%)) plt.title(Humidity) plt.ylim(0, 100) plt.grid(axis=y) plt.tight_layout() plt.show() Result Figures generated by the code provided by ARIA is shown as below. Weather forecast image source (14-day forecast, screenshot on Sep 20, 2024): https://www.timeanddate.com/weather/singapore/singapore/hourly 9 ARIA Example - Financial Report Analysis User How does the companys profit margin change from year 2020 to 2021? What factors cause such changes? Answer step by step. ARIA To determine how the companys profit margin changed from 2020 to 2021, we need to follow these steps: 1. Calculate the profit margins for both years: 2020: Net income: $57,411 million Total net sales: $274,515 million Profit margin: Profit Margin = (cid:18) Net Income Total Net Sales (cid:19) 100 = (cid:19) (cid:18) $57, 411 $274, 515 100 20.91% 2021: Net income: $94,680 million Total net sales: $365,817 million Profit margin: Profit Margin = (cid:18) Net Income Total Net Sales (cid:19) 100 = (cid:19) (cid:18) $94, 680 $365, 817 100 25.88% ARIA Example - Financial Report Analysis ARIA (continued) 2. Analyze the factors causing the change: Increase in Net Income: The net income increased from $57,411 million in 2020 to $94,680 million in 2021, showing significant rise of $37,269 million. Sales Growth: Total net sales also increased from $274,515 million in 2020 to $365,817 million in 2021, indicating strong growth in revenue. Cost Management: The cost of sales increased from $151,286 million in 2020 to $192,266 million in 2021, but the percentage increase in sales outpaced the increase in costs, leading to higher profit margin. Operating Expenses: Operating expenses grew from $38,668 million in 2020 to $43,887 million in 2021, but the rise in sales was more substantial, positively impacting the profit margin. Therefore, the companys profit margin improved from approximately 20.91% in 2020 to approximately 25.88% in 2021. This improvement is primarily due to substantial increase in net income and effective cost management relative to the growth in total net sales. Financial report image source: https://www.financestrategists.com/wealth-management/ financial-statements/ 11 ARIA Example - Handwritten Math Equation Explanation User Explain the equation according to the notes, then give an example with numbers. ARIA The equation in the image represents Bayes Theorem, which is used to update the probability estimate of hypothesis (in this case, having food allergy) given new evidence (claiming to have food allergy). Explanation: (CA): Probability that someone who definitely has an allergy would claim that they do. (A): General probability that someone has food allergy. (C): Probability that someone would claim to have food allergy. (AC): Probability that someone actually has food allergy given that they claim to have one. The equation: (AC) = (CA) (A) (C) Example with numbers: Assume: (CA) = 0.9, (90% of people with an allergy claim they do) (A) = 0.05, (5% of the population has an allergy) (C) = 0.1, (10% of the population claims to have an allergy) Using the formula: (AC) = 0.9 0.05 0.1 = 0.45 So, the probability that someone who claims to have food allergy actually does is 45%. Image source: https://ngathanblog.wordpress.com/2020/06/25/baysian-struggle/. 12 ARIA Example - Code Debugging from Screenshot User Debug this code. ARIA The code has logical error in the nested loop logic. The current implementation of the has_close_elements function does not correctly check if any two numbers in the list are closer to each other than the given threshold. Heres the issue: 1. The current condition if abs(numbers[i] - numbers[j]) < threshold: checks the absolute difference between each pair of numbers but should only check pairs where and are different indices. 2. The inner loop should start from + 1 instead of to avoid comparing number with itself. Heres the corrected version: def has_close_elements(numbers, threshold): \"\"\" Check if in given list of numbers, are any two numbers closer to each other than the given threshold. \"\"\" for in range(len(numbers)): for in range(i + 1, len(numbers)): # Start from + 1 to avoid comparing with itself if abs(numbers[i] - numbers[j]) < threshold: return False return True # Tests assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True This change ensures the function correctly checks all pairs of numbers and returns True if any pair is closer than the threshold, otherwise False. 13 ARIA Example - Multi-image Food Chain Identification User Based on the five species in the images, draw food chain. Explain the role of each species in the food chain. ARIA 1. Mushrooms (Image 1): Decomposers, breaking down dead organic matter and recycling nutrients. 2. Grass (Image 4): Primary producers, converting sunlight into energy via photosynthesis. 3. Grasshopper (Image 2): Primary consumers, herbivores feeding on plants. 4. Mouse (Image 3): Secondary consumers, herbivores that eat plants and possibly small insects. 5. Hawk (Image 5): Tertiary consumers, predators that eat mice and other small animals. Food Chain Grass Grasshopper Mouse Hawk Mushrooms Explanation 1. Grass serves as the base, providing energy to grasshoppers. 2. Grasshoppers eat the grass and are then consumed by mice. 3. Mice are preyed upon by hawks. 4. Hawks are apex predators, feeding on mice. 5. Mushrooms decompose dead matter, recycling nutrients back into the ecosystem. 14 4.3.2 Long Multimodal Context Window ARIA Example - Paper Reading User Summarize the key contribution of the paper, and explain how it uses various figures and tables to showcase such contribution. ARIA The paper introduces LONGVIDEOBENCH, benchmark for evaluating Large Multimodal Models (LMMs) in long-context video-language understanding. It addresses the challenge of processing longer video inputs by proposing question-answering benchmark that tests the models ability to understand and reason about video contexts interleaved with up to an hour-long video. This benchmark includes 3,763 videos with diverse themes and 6,678 human-annotated multiple-choice questions across 17 categories. The key contribution is the comprehensive evaluation of LMMs performance on long-form video understanding, highlighting the limitations of existing models in handling extended inputs. The paper demonstrates that while proprietary models like GPT-4 and Gemini-1.5-Pro show improvements with longer inputs, open-source models lag significantly behind. 15 ARIA (continued) Figures and tables are used to illustrate these points: ARIA Example - Paper Reading Figure 1: Demonstrates the performance of different models on the benchmark, showing how accuracy varies with the number of frames. Figure 2: Provides examples of the 17 categories of referring reasoning questions. Table 1: Lists the benchmark and previous benchmarks for comparison. Table 2: Details the 17 categories of referring reasoning questions. Table 3: Shows statistics of videos in LONGVIDEOBENCH by category groups and duration. Table 5: Validation set results categorized by duration groups. Table 7: Test set leaderboard of LMMs by duration groups and question categories. These visual aids help in understanding the performance metrics, the diversity of the dataset, and the specific challenges posed by longer videos and complex questions, thus emphasizing the need for improved models in long-form video understanding. User According to the paper, what are the two major difficulties in understanding long videos? ARIA The two major difficulties in understanding long videos, as outlined in the paper, are: 1. Retrieving details from long videos: Existing Large Multimodal Models (LMMs) often struggle to extract specific details from long sequences. To accurately assess tasks in LONGVIDEOBENCH, there is need for models to focus on granular details such as objects, events, or attributes, rather than providing summary or topic overview. 2. Reasoning contextual relations in long videos: Questions in LONGVIDEOBENCH require models to analyze the interconnections among diverse contents. This involves understanding the relationships among objects, events, or attributes within the video, which is significantly challenging for extensive inputs. The tasks demand models to derive the correct answer by examining the context and relations across multiple moments in the video. LongVideoBench paper source: https://arxiv.org/pdf/2407.15754v1.pdf. 16 ARIA Example - Long Video Understanding (Scene Splitting) User Please split this video into scenes, providing start time, end time, title and detailed descriptions for each scene. ARIA Certainly! Here are the scenes from the video, along with their start and end times, and detailed descriptions: 1. Scene 1: Introduction Start Time: 00:01 End Time: 00:05 Title: Introduction Description: The video opens with title screen featuring the smarthistory logo and the text \"Michelangelo, David, 1501-04,\" indicating conversation with Dr. Steven Zucker and Dr. Beth Harris. ARIA Example - Long Video Understanding (Scene Splitting) ARIA (continued) 2. Scene 2: Accademia Gallery, Florence Start Time: 00:05 End Time: 00:40 Title: Accademia Gallery, Florence Description: The scene transitions to the interior of the Accademia Gallery in Florence, showcasing Michelangelos David statue surrounded by visitors. The focus is on the statue and its historical context. 3. Scene 3: Historical Context Start Time: 00:40 End Time: 01:08 Title: Historical Context Description: The scene shifts to timeline highlighting significant artworks and events from 1498 to 1512, including Michelangelos David and the Sistine Chapel ceiling. 4. Scene 4: Comparison of Statues Start Time: 01:08 End Time: 01:50 Title: Comparison of Statues Description: The video presents comparison between Michelangelos David and Nanni di Bancos Isaiah for the Cathedral, emphasizing their historical significance. 5. Scene 5: Republic of Florence Start Time: 01:50 End Time: 02:08 Title: Republic of Florence Description: The scene highlights the Republic of Florence timeline, focusing on the Medici family and Savonarola, with focus on their impact on the city. 6. Scene 6: Savonarolas Execution Start Time: 02:08 End Time: 02:22 Title: Savonarolas Execution Description: The scene depicts the execution of Savonarola and two friars in Piazza della Signoria, emphasizing the historical context. 7. Scene 7: Republic Defeats Medici and Savonarola Start Time: 02:22 End Time: 02:36 Title: Republic Defeats Medici and Savonarola Description: The scene shows the Republic of Florence defeating the Medici and Savonarola, with focus on the historical figures involved. 18 ARIA Example, Long Video Understanding (Scene Splitting) ARIA (continued) 8. Scene 8: Davids Significance Start Time: 02:36 End Time: 03:29 Title: Davids Significance Description: The scene focuses on Michelangelos David, highlighting its special significance for the people of Florence. 9. Scene 9: David in Context Start Time: 03:29 End Time: 03:42 Title: David in Context Description: The scene provides broader context of Florence, showing the cityscape and comparing different David statues. 10. Scene 10: Donatellos David Start Time: 03:42 End Time: 03:57 Title: Donatellos David Description: The scene highlights Donatellos David, emphasizing its historical and artistic significance. 11. Scene 11: Michelangelos David Start Time: 03:57 End Time: 05:49 Title: Michelangelos David Description: The scene returns to Michelangelos David, focusing on its artistic details and historical context. 12. Scene 12: Davids Relocation Start Time: 05:49 End Time: 06:28 Title: Davids Relocation Description: The scene discusses the relocation of David to its current position outside the Palazzo della Signoria. 13. Scene 13: Historical Figures Start Time: 06:28 End Time: 06:42 Title: Historical Figures Description: The scene highlights historical figures like Piero de Medici and Savonarola, emphasizing their impact on Florence. 14. Scene 14: Michelangelos David Start Time: 06:42 End Time: 06:59 Title: Michelangelos David Description: The scene focuses on Michelangelos David as symbol of the newly reconstituted Republic of Florence. 19 ARIA Example - Long Video Understanding (Scene Splitting) ARIA (continued) 15. Scene 15: Accademia Gallery Start Time: 06:59 End Time: 07:03 Title: Accademia Gallery Description: The scene returns to the Accademia Gallery, showcasing the statue of David and its historical significance. 16. Scene 16: Cathedral of Florence Start Time: 07:03 End Time: 07:10 Title: Cathedral of Florence Description: The scene shifts to the Cathedral of Florence, highlighting its architectural significance. 17. Scene 17: Davids Significance Start Time: 07:10 End Time: 07:20 Title: Davids Significance Description: The scene emphasizes the significance of Michelangelos David in the context of Renaissance Florence. 18. Scene 18: Conclusion Start Time: 07:20 End Time: 07:24 Title: Conclusion Description: The video concludes with the smarthistory logo, summarizing the key points discussed. 19. Scene 19: Credits Start Time: 07:24 End Time: 07:27 Title: Credits Description: The scene provides credits for the video, including contributors and sources. The video has total of 19 scenes, each with specific start and end times, titles, and detailed descriptions. Video source: https://youtube.com/watch?v=QdlP8ai8trw. 20 ARIA Example - Code Understanding from Videos User On the page with text Predict 10 Labels on it, Python class is defined. Please note down the code that defines this class. After that, explain each part of the code. ARIA The Python class defined on the page with text Predict 10 Labels is as follows: class HiMom(nn.Module): def __init__(self): super().__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): = self.flatten(x) logits = self.linear_relu_stack(x) return logits ARIA Example - Code Understanding from Videos ARIA (continued) Explanation of the code: 1. Class Definition: The class HiMom is defined, nn.Module. inheriting from 2. Initialization (__init__): super().__init__(): Calls the initializer of the parent class. self.flatten = nn.Flatten(): Creates flatten layer to convert input tensors to 1D tensors. self.linear_relu_stack = nn.Sequential(...): Defines sequence of layers: nn.Linear(28*28, 512): fully connected layer with 28*28 input features and 512 output features. nn.ReLU(): ReLU activation function. nn.Linear(512, 512): Another fully connected layer with 512 input and output features. nn.ReLU(): ReLU activation function. nn.Linear(512, 10): Output layer with 10 output features. 3. Forward Method: = self.flatten(x): Flattens the input tensor. logits = self.linear_relu_stack(x): Passes the flattened tensor through the defined sequence of layers. return logits: Returns the output logits. Video source: https://www.youtube.com/watch?v=ORMx45xqWkA."
        },
        {
            "title": "5 Acknowledgement",
            "content": "The authors appreciate Fan Zhou, Chengen Huang, Yanpeng Li, Chongyan Zhu, Xiaoyi Ren, Peng Liu, Chao Li, Tianhang Zhu, Chenxu Lv, Xiang He, Shenzhi Wang, Chujie Zheng, Wenhao Huang, Ziyang Luo, Shiming Yang, Fengji Zhang, Yuhao Yang, Linli Yao, Xiao Li, Lihuan Zhang, Liying Li, Wen Xie, Jun Tian, Yifan Ye and Hanshu Yan for their valuable input and suggestions."
        },
        {
            "title": "References",
            "content": "J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-VL: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, et al. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. M. Dehghani, B. Mustafa, J. Djolonga, J. Heek, M. Minderer, M. Caron, A. Steiner, J. Puigcerver, R. Geirhos, I. M. Alabdulmohsin, A. Oliver, P. Padlewski, A. A. Gritsenko, M. Lucic, and N. Houlsby. Patch pack: Navit, vision transformer for any aspect ratio and resolution. In NeurIPS, 2023. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev, A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson, A. Spataru, B. Rozière, B. Biron, B. Tang, B. Chern, C. Caucheteux, C. Nayak, C. Bi, C. Marra, C. McConnell, C. Keller, C. Touret, C. Wu, C. Wong, C. C. Ferrer, C. Nikolaidis, D. Allonsius, D. Song, D. Pintz, D. Livshits, D. Esiobu, D. Choudhary, D. Mahajan, D. Garcia-Olano, D. Perino, D. Hupkes, E. Lakomkin, E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic, F. Zhang, G. Synnaeve, G. Lee, G. L. Anderson, G. Nail, G. Mialon, G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu, H. Touvron, I. Zarov, I. A. Ibarra, I. M. Kloumann, I. Misra, I. Evtimov, J. Copet, J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah, J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang, J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca, J. Johnstun, J. Saxe, J. Jia, K. V. Alwala, K. Upasani, K. Plawiak, K. Li, K. Heafield, K. Stone, and et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23, 2022. A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de Las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mixtral of Experts: sparse mixture of experts language model. arXiv preprint arXiv:2401.04088, 2024. G. Kamradt. Llmtest_needleinahaystack, 2023. URL https://github.com/gkamradt/ LLMTest_NeedleInAHaystack/blob/main/README.md. H. Laurençon, L. Tronchon, M. Cord, and V. Sanh. What matters when building visionlanguage models? arXiv preprint arXiv:2405.02246, 2024. K. Lee, M. Joshi, I. R. Turc, H. Hu, F. Liu, J. M. Eisenschlos, U. Khandelwal, P. Shaw, M. Chang, and K. Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In ICML, 2023. D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. 23 B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. J. Li, D. Li, C. Xiong, and S. C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. J. Li, D. Li, S. Savarese, and S. C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. B. Lin, Z. Tang, Y. Ye, J. Cui, B. Zhu, P. Jin, J. Zhang, M. Ning, and L. Yuan. MoE-LLaVA: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947, 2024a. X. V. Lin, A. Shrivastava, L. Luo, S. Iyer, M. Lewis, G. Ghosh, L. Zettlemoyer, and A. Aghajanyan. MoMa: Efficient early-fusion pre-training with mixture of modality-aware experts. arXiv preprint arXiv:2407.21770, 2024b. H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. J. Ludziejewski, J. Krajewski, K. Adamczewski, M. Pióro, M. Krutul, S. Antoniak, K. Ciebiera, K. Król, T. Odrzygózdz, P. Sankowski, M. Cygan, and S. Jaszczur. Scaling laws for finegrained mixture of experts. arXiv preprint arXiv:2402.07871, 2024. Mixtral. Pixtral 12b - the first-ever multimodal mistral model, 2024. URL https:// mistral.ai/news/pixtral-12b/. Y. Qian, H. Ye, J. Fauconnier, P. Grasch, Y. Yang, and Z. Gan. Mia-bench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In M. Meila and T. Zhang, editors, ICML, 2021. S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, 2020. C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki. LAION-400M: open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. S. Shen, Z. Yao, C. Li, T. Darrell, K. Keutzer, and Y. He. Scaling vision-language models with sparse mixture of experts. In EMNLP, 2023. W. Shi, S. Min, M. Lomeli, C. Zhou, M. Li, G. Szilvasy, R. James, X. V. Lin, N. A. Smith, L. Zettlemoyer, S. Yih, and M. Lewis. In-context pretraining: Language modeling beyond document boundaries, 2023. M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. H. Wu, D. Li, B. Chen, and J. Li. Longvideobench: benchmark for long-context interleaved video-language understanding. arXiv preprint arXiv:2407.15754, 2024. X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pretraining. In ICCV, 2023. B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022."
        }
    ],
    "affiliations": [
        "Rhymes AI"
    ]
}