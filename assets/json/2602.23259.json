{
    "paper_title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
    "authors": [
        "Jiangxin Sun",
        "Feng Xue",
        "Teng Long",
        "Chang Liu",
        "Jian-Fang Hu",
        "Wei-Shi Zheng",
        "Nicu Sebe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability."
        },
        {
            "title": "Start",
            "content": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving Jiangxin Sun1, Feng Xue1, Teng Long1, Chang Liu1, Jian-Fang Hu2, Wei-Shi Zheng2, Nicu Sebe1 1University of Trento, Trento, Italy, 2Sun Yat-sen University, Guangzhou, China jiangxin.sun@unitn.it, feng.xue@unitn.it, teng.long@unitn.it, chang.liu@unitn.it, hujianf@mail.sysu.edu.cn, wszheng@ieee.org, nicu.sebe@unitn.it"
        },
        {
            "title": "Abstract",
            "content": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of only driving like the expert suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability. Date: February 27, 2026 Correspondence: Feng Xue at feng.xue@unitn.it 6 2 0 2 6 2 ] . [ 1 9 5 2 3 2 . 2 0 6 2 : r"
        },
        {
            "title": "Introduction",
            "content": "End-to-end autonomous driving (E2E-AD) [26, 56, 62, 63, 81] aims to map sensor observations to control actions, e.g., steering, throttle, and brake, without relying on hand-crafted perception, prediction, or planning modules. Compared to traditional modular pipelines [48, 51, 53, 74, 76, 101], E2E-AD offers more unified representation of the driving task, enabling the policy to reason about complex interactions between the ego vehicle and dynamic environments. As such, E2E-AD has attracted growing attention due to its potential for simplified system design, joint optimization, and real-time decision-making. 1 Figure 1 Comparison between existing E2E-AD methods and RaWMPC. The first row shows the predicted trajectories, and the second row compares the core workflows. Black arrows denote test-time execution, while pink arrows indicate training-only steps. The comparison shows that prior methods often omit explicit hazard modeling and may trigger traffic violations, whereas RaWMPC uses risk-aware world model to evaluate action consequences and select safe, compliant actions in critical scenes. Early studies on E2E-AD [4, 6, 75] primarily focused on learning driving policies via reinforcement learning (RL) through online exploration. More recent research [5, 95] demonstrated that rule-based or RL-based agents leveraging privileged information (e.g., birds-eye-view segmentation and highdefinition maps) can produce superior driving decisions. Building upon these insights, state-of-the-art methods [28, 62, 63, 81, 90] generally follow an imitation learning (IL) framework as shown in Fig. 1, where agents using only sensor inputs (e.g., RGB images and LiDAR) are trained to replicate the privileged experts behavior through knowledge distillation on both the driving policy and latent features. Although some works have attempted to enhance the driving performance via future motion modeling [55, 62, 63, 73], action-aware future prediction [23, 28, 38, 39], and the integration of large language models [14, 21, 35, 57], these approaches still adhere to the learning objective of driving like an expert, as demonstrated in Fig. 1 (a). They cannot fundamentally resolve the inherent generalization dilemma of imitation learning: Since expert demonstrations cannot cover all scenarios and situations, imitationbased policies tend to produce unpredictable and often unsafe driving behavior when encountering unseen scenarios outside of expert demonstrations. More recently, model-based RL methods [37, 85] have emerged and attempted to improve generalization by learning environmental dynamics and planning over them. However, as illustrated in Fig. 1 (b), most of them still aim to maximize the expected reward and lack explicit modeling and sampling of rare but high-risk situations, and thus continue to struggle to guarantee safety in these scenarios. In this paper, we argue that enabling an E2E-AD system to learn and proactively avoid risky actions is more important than replicating expert driving behavior verbatim. Motivated by this perspective, we propose an E2E-AD framework that does not require any expert action supervision, called Risk-aware World Model Predictive Control (RaWMPC), as shown in Fig. 1(c). This framework discards expert demonstrations and instead leverages risk-aware world model to drive predictive control to overcome the generalization challenge. Different from model-based RL that trains an actor to maximize reward from imagined rollouts of world model, the world model in RaWMPC predicts near-future states for set of candidate driving behaviors and explicitly evaluates their risk, so as to select the lowest-risk candidate. To endow our world model with risk-awareness, we introduce risk-aware interaction strategy: Starting from scratch, the model selects self-identified highrisk actions to interact with the environment, from which our world model learns to predict the consequences of diverse risky behaviors. Without any expert demonstration, our world model can reach strong performance from scratch, and can be further accelerated if few video clips are provided by benchmarks 2 for light warm-up. Finally, to efficiently provide low-risk candidates at test time, we further propose self-evaluation distillation for driving policy learning. The well-trained world model is leveraged to identify safe and risky behaviors from the sampled action space, and to distill this knowledge, via safetyrisk contrastive learning, into generative action proposal network. Experiments on Bench2Drive and NAVSIM show that RaWMPC, even without expert demonstrations, surpasses previous state-of-the-art methods, while the optional light warm-up further helps to accelerate the convergence and improve performance. More importantly, since RaWMPC learns risk-awareness from interaction rather than expert labels, it achieves substantially higher driving performance in previously unseen scenarios. Our main contributions can be summarized as follows: We propose RaWMPC, an E2E-AD framework with zero expert requirement. Unlike IL and MBRL methods, RaWMPC uses world model to select low-risk behaviors from multiple candidate actions, which naturally improves the interpretability and reliability of its decisions. Within RaWMPC, we design risk-aware interaction learning strategy that enables the world model to acquire risk-awareness purely from environment interaction, without any expert demonstration. We introduce self-evaluation distillation scheme for driving policy learning, which provides high-quality candidate actions at test time, even outperforming policies learned directly from expert demonstrations."
        },
        {
            "title": "Driving",
            "content": "Learning-based autonomous driving approaches typically follow two main paradigms: imitation learning (IL) and reinforcement learning (RL) [7, 9]. Early studies explored RL extensively [2, 4, 6, 22, 32, 52, 58, 59, 75, 93] for its natural capability to refine driving strategies through interactive feedback. Subsequent works [37, 95] demonstrated that training sensor-based agents to imitate RL experts behavior could lead to superior performance, prompting shift toward leveraging privileged information (e.g., BEV segmentation and HD maps) to train stronger RL experts. More recently, model-based RL has revisited this line by learning explicit dynamics/world models and performing look-ahead rollouts [85], improving consequence-aware evaluation and sample efficiency. Nevertheless, these RL approaches are commonly driven by maximizing expected return, and they rarely provide an explicit mechanism to systematically discover and model rare-but-catastrophic outcomes, making reliable decisions in long-tail, highrisk scenarios still challenging. With large-scale driving datasets, imitation learning has attracted increased attention and achieved stateof-the-art performance in closed-loop autonomous driving [40, 57, 73, 82]. Most IL approaches rely on collecting trajectory data and features from RLbased [37, 95] or rule-based [10, 65] experts using privileged information, and train sensor-based IL agents to replicate expert behaviors through knowledge distillation. variety of research studies have explored ways to improve IL performance, such as multi-modal information fusion [10, 26, 54], object motion modeling [30, 55, 62, 63], action-aware future prediction [23, 28, 38, 39], feature alignment [27, 90], and the integration of large language models [14, 45, 56, 57, 64, 65, 83, 86]. Despite impressive results, the core objective of driving like the expert inherently limits generalization: expert demonstrations cannot cover all long-tail situations, and experts typically avoid dangerous behaviors, leaving IL agents with limited supervision on how to recognize and proactively avoid high-risk actions. Moreover, pure imitation often provides limited interpretability because it outputs single action without explicitly comparing alternative actions via consequence evaluation. Motivated by these insights, we propose RaWMPC, unified framework that replaces expert action supervision with risk-aware predictive control: it learns risk-aware world model via risk-aware interaction strategy that deliberately exposes the model to risky behaviors, and uses the learned model to predict and evaluate the consequences of multiple candidate actions, selecting low-risk behaviors with explicit risk evaluation."
        },
        {
            "title": "2.2 World Models",
            "content": "World models approximate environment transitions under the Markov Decision Process and have demonstrated success in RL [1720, 34, 46, 75, 77, 79] by predicting future states and rewards from current observations and actions. However, applying these models to complex tasks such as autonomous driving remains challenging. Previous research [1, 13, 15, 24, 36, 49, 50, 7072, 78, 92, 96 98, 102] has mainly leveraged world models to generate controllable future driving trajectories (e.g., RGB 3 images and 3D/4D representations) conditioned on specific actions and scene descriptions. Such predictive models can enlarge training data and increase diversity, potentially benefiting downstream imitation learning, especially for rare scenes such as traffic accidents. Beyond prediction, few recent attempts have utilized world models to improve closed-loop autonomous driving performance. In particular, some works have started to connect world modeling with planning and online evaluation in driving settings [8, 39, 91], while others provide high-fidelity generative platforms that enable closed-loop evaluation [3, 11, 84]. In particular, model-based IL methods [23, 38, 39, 61] employ world models to support better imitation: LAW [38] enhances end-to-end driving by predicting future information in latent world model to assist policy learning, and WoTE [39] performs online trajectory evaluation via BEV world model to score candidate trajectories. More recently, model-based RL methods have drawn increasing attention. Think2Drive [37] designed model-based RL expert to forecast action-conditioned future rewards for training more effective critic network, and Raw2Drive [85] leveraged world model pretrained from privileged experts to guide the learning of sensor agents. Despite these advances, most existing works still inherit supervision from experts or rewards, and they largely focus on imitation fidelity or expectedreturn maximization, lacking explicit mechanisms to systematically discover, model, and avoid rare-buthigh-risk outcomes. In contrast, RaWMPC uses the world model as risk evaluator within predictive control: we introduce risk-aware interaction strategy to intentionally explore risky behaviors so that catastrophic consequences become predictable and avoidable, and we select actions by explicitly minimizing risk over multiple candidates, enhancing the interpretability, reliability, and generalization of the decision-making process."
        },
        {
            "title": "3 Method",
            "content": "To demonstrate our solution, Sec. 3.1 introduces the overall network structure and pipeline of our RaWMPC. Sec. 3.2 presents our training scheme, i.e., risk-aware interactive training, for efficient optimization. To ensure the whole E2E-AD system runs efficiently during testing, Sec. 3.3 illustrates self-evaluation distillation method to train an action proposal network."
        },
        {
            "title": "3.1 Network Structure of RaWMPC",
            "content": ""
        },
        {
            "title": "3.1.1 Problem Setup",
            "content": "t:t+H1}N We consider closed-loop end-to-end autonomous driving (E2E-AD) setting. At each time step t, the agent receives three inputs: visual input It (multiview RGB images), an ego-centric measurement Mt (velocity and position), and set of candidate driving behaviors {An , where is the number of candidates and is the planning horizon. Each step of action consists of three values: = (steer [1, 1], throttle [0, 1], brake [0, 1]). Based on the driving history (I1:t, M1:t, A1:t1), RaWMPC aims to select the best one An from the candidates, so that the vehicle can move toward destination while ensuring safety and compliance with traffic rules. t:t+H1 n=1 3.1.2 Overview As illustrated in Fig. 2, RaWMPC begins from input encoding. visual encoder, an action encoder and an , ego-state encoder are used to map It, {An n=1 and Mt into embeddings it, {an , and mt, t:t+H1}N respectively. Then, based on the observed states s1:t = (i1:t, m1:t), we use world model to estimate its future state ˆsn conditioned on each action . Finally, we select the action embeddings an that enables the ego vehicle to advance safely while avoiding traffic infractions, by decoding ˆsn and computing cost value: t:t+H1}N n=1 t:t+H1 t+1:t+H t+1:t+H t:t+H1 = An where = arg min n{1,...,N } t:t+H1, C(cid:0)ˆsn t+1:t+H (cid:1). (1) where C() denotes the cost function in the decoding process (will be detailed in Eq. (6)), and is the index of optimal action. Compared to imitation learning schemes, RaWMPC offers improved interpretability and introduces an explicit mechanism for decision validation and risk mitigation. 3.1.3 World Model In our pipeline, the world model, denoted as M, is employed to predict future states given an action. Specifically, conditioned on the observed states s1:t = , the world (i1:t, m1:t) and the potential next action an model predicts near-future state ˆsn . Then, for the further-future time step {2, . . . , H}, the world model recursively rolls out, which can be formalized as an autoregressive factorization: t+1 pM(ˆsn (cid:89)H t+1:t+H s1:t, an (cid:16) t+k(s1:t, ˆsn ˆsn pM k= 1:t+H1) = t+1:t+k1) , an 1:t+k1 (cid:17) (2) Figure 2 Overview of RaWMPC. Multi-view images It, ego state Mt, and candidate action sequences {An n=1 are encoded and rolled out by world model over horizon H. Three decoders predict semantic segmentation, semanticguided traffic events, and future ego states, enabling action evaluation for predictive control. Training combines offline warm-up on logged trajectories with online simulator interaction using world-model-guided exploration. t:t+H1}N where pM denotes the conditional distribution defined by the world model M. an t:t+k1] denotes candidate action sequence containing action history. 1:t+k1 = [a1:t1, an 3.1.4 Semantic-Guided Decoding t+1:t+H Once the world model predicts sequence of future states, ˆsn , three transformer decoders separately map these states to task-specific outputs: semantic segmentation, potential traffic events (e.g., collision), and future ego-states (e.g., position). In what follows, we describe these decoders using the , where predicted state at time step + k, i.e., ˆsn {1, . . . , H}. t+k To enable higher-level understanding of driving scenes and provide visual explanations for predicted traffic events, we inject semantic attention from the segmentation decoder into the event decoder. The segmentation decoder adopts standard transformer attention: Attseg(Qc,Kc,Vc) = softmax(sim(Qc, Kc))Vc, (3) t+k ˆsn where Qc are learnable class queries, and Kc, Vc are derived from visual tokens ˆin . In the final layer, we follow SegViT [88] to predict the one-hot semantic segmentation map ˆYn for each input class query. We then augment the event decoder by fusing its attention logits with the corresponding semantic attention logits from the last segmentation layer: t+k t+k , Zc = pad(QcK Ze = QeK t+k = sigmoid(softmax(We [Ze, Zc])Ve) . ), ˆEn (4) t+k where Qe are learnable event queries, and Ke, Ve are computed from predicted future states ˆsn . pad() zero-pads Zc to match the size of Ze, and We is 1 1 convolution that fuses the concatenated logits. The output of the event decoder, ˆEn t+k [0, 1]α, represents the probabilities of α event types. Finally, for the future ego-state prediction, we decode the ego token ˆmn to obtain the speed and position ˆMn . Guided by the semantic attention map, the event decoder draws more attention to regions critical to specific events. For instance, when recognizing the vehicle collision event, the model focuses more on vehicle areas, improving the accuracy and reliability of event predictions. t+k ˆsn t+k t+k 3.1.5 Action Selection and Predictive Control Given the decoder outputs, we perform predictive control by evaluating each candidate action sequence over the planning horizon and selecting the one with the minimum predicted cost. Specifically, for the n-th candidate An , we consider (i) progress toward the target and (ii) the risk of traffic-violation events. Let be the target 3D position and ˆpn the predicted ego position at step + k. We define the progress as the reduction in target distance: t+k ˆMn t:t+H1 t+k ˆDn t+k = (cid:13) (cid:13)p ˆpn t+k1 (cid:13)2 (cid:13) (cid:13) (cid:13)p ˆpn t+k (cid:13) (cid:13)2, (5) We then define the predictive-control objective as: C(ˆsn t+1:t+H ) = (cid:88) ηk( ˆDn t+k + α (cid:88) λj ˆEn t+k,j), (6) k=1 j=1 where ηk = max(2k+1, 1/8) down-weights distant predictions to account for increasing uncertainty. We floor ηk at 1/8 to avoid vanishing contributions from distant steps, which stabilizes planning when is moderately large. λj > 0 reflects the severity of violation type (e.g., pedestrian/vehicle collisions receive larger weights). Finally, we select the action sequence that minimizes the horizon cost in Eq. (6), i.e., model-predictive control policy that favors faster progress while proactively reducing the probability of predicted violations. 3.1.6 Overall Loss of RaWMPC The RaWMPC framework is trained in an end-to-end manner with world model loss Lworld, segmentation loss Lseg from SegViT [88], ego-state loss Lego, and event loss Levent: = Lworld + Lseg + Lego + Levent. Following SegViT [88], the segmentation term includes classification loss Lcls (cross-entropy) and the binary mask loss. The mask loss consists of focal loss Lfocal [43] and dice loss Ldice [47] for optimizing the segmentation accuracy: (7) (8) Lseg = Lcls + Lfocal + Ldice. For the other three losses, we use Mean Squared Error (MSE) to supervise the ego-state and world model supervision, and take Binary Cross Entropy (BCE) loss for the event decoder: Lworld = Lego = Levent = 1 1 1 (cid:88)H k=1 (cid:88)H k=1 (cid:88)H k= ˆst+k st+k2 2 , (cid:13) (cid:13) 2 ˆMt+k Mt+k (cid:13) (cid:13) (cid:13) (cid:13) 2 BCE( ˆEt+k, Et+k), , (9) where BCE() denotes the BCE loss. All the annotations st+k, Mt+k, Et+k along the executed rollout under At:t+H1 are obtained from the simulator (e.g., CARLA)."
        },
        {
            "title": "3.2 Risk-aware Interactive Training",
            "content": "To enable RaWMPC to evaluate diverse actions and identify risky scenarios, we propose two-stage riskaware interactive training scheme, as shown in Fig. 2. We first warm-start the world model from logged driving trajectories. Then, we refine it via online simulator interaction, intentionally collecting both good Figure 3 Different action-selection ranges under three driving modes in online simulator interaction. Red denotes high cost and green denotes low cost. rand samples uniformly from all candidates, bad samples from the high-cost region, and good samples from the low-cost one. (safe, goal-directed) and bad (hazardous) rollouts to improve generalization under out-of-distribution controls and to learn rare but safety-critical events. Notably, RaWMPC does not rely on expert action labels for policy learning. The optional warm-up stage, when used, serves solely to initialize the predictive world model from observed state transitions, rather than to imitate expert actions. 3.2.1 Offline World Model Warm-up t=1 We bootstrap RaWMPC using small set of logged trajectories to achieve simple and basic stateforecasting capability. Given state-action sequences from NAVSIM or CARLA, the world {(st, at)}T model predicts the next state ˆst+1 and is trained with Lworld to match the ground-truth st+1. We supervise the segmentation and ego-state decoders using the simulator-provided annotations. Since the warm-up trajectories contain no traffic violations, we train the event decoder with an all-zero target. In this way, only small subset of training data (10%) is typically sufficient for warm-up, providing reliable initialization for long-horizon rollouts and stabilizing subsequent world model optimization. 3.2.2 Online Simulator Interactive Training Offline warm-up data are mostly concentrated around human-like safe behaviors and thus provide limited coverage of hazardous or unconventional actions. To learn the consequences of risky behaviors, we perform world-model-guided exploration: selected simulator rollouts are fed back to refine the same world model, progressively improving prediction fidelity and risk sensitivity. Specifically, to ensure temporal continuity and avoid unrealistic control jitter, we sample horizon-H action sequences (segment-wise) rather than single-step actions (step-wise). The segment-wise sampling allows 6 sustained safe or risky behaviors to unfold and reveals their long-term consequences. At each training step, we sample Ns horizon-H candidate action sequences with {An the current world model, evaluate their costs {C n} using Eq. (6), and rank candidates by costs. , roll out future states ˆsn t:t+H1}Ns t+1:t+H n=1 Modes for Interaction. We define three patterns for our risk-aware sampling strategy to select one to execute, as shown candidate from {An in Fig. 3: t:t+H1}Ns n= Rand samples uniformly from all candidates; Bad samples from high-cost candidates; Good samples from low-cost candidates. At the start of segment r, the practical control mode is sampled according to probability: mr = rand, w.p. ε1, w.p. (1 ε1)ε2, bad, good, w.p. (1 ε1)(1 ε2), (10) where w.p. means with probability. ε1 controls broad action-space exploration, and ε2 controls the fraction of risk-seeking interaction within modelguided sampling. Soft Candidates Selection in Three Modes. Given sorted candidate actions with costs {C n}, we construct two cost-quantile sets: Ngood as the bottom-K candidates and Nbad as the top-K candidates. To avoid low-information trajectories, we filter Nbad by removing degenerate rollouts (e.g., those caused by unrealistic excessive control jumps), yielding Nbad. In the rand mode, we randomly select the executing action sequence from all candidates. In the good mode, rather than deterministically selecting the minimum-cost candidate, we sample from Ngood using soft distribution to preserve diversity among low-cost plans and mitigate bias from imperfect model predictions: (n good) exp(C n/τg), Ngood. (11) where τg is temperature hyper-parameter that controls the softness of the sampling distributions in the good mode, trading off greediness for diversity. This stochastic selection avoids repeatedly executing single estimated optimum and encourages broader coverage of nominal behaviors. In the bad mode, instead of always executing the maximum-cost trajectory, we sample from high-cost 7 Figure 4 Self-Evaluation Distillation for Policy Learning. cVAE is trained with RaWMPC-scored actions in contrastive manner, pulling the condition prior toward positives and pushing it away from negatives. The welltrained decoder serves as the test-time action proposer. candidates to deliberately expose the model to spectrum of risky outcomes that are under-represented in safe logs: (n bad) exp(C n/τb), Nbad. (12) where τb is temperature hyper-parameter. Compared to argmax selection, this soft sampling strategy prevents over-concentration on extreme or degenerate failures while still biasing interaction toward high-risk regions. In this way, segment-wise interaction and soft costbased sampling bias exploration toward temporally coherent and informative safe and hazardous trajectories, enabling the world model to learn both reasonable dynamics and safety-critical consequences for risk-aware decision making."
        },
        {
            "title": "Learning",
            "content": "After risk-aware interactive training, RaWMPC can reliably score candidate action sequences by predicting their long-horizon consequences. To reduce the cost of online optimization at test time, we distill this evaluation capability into lightweight action proposal network, enabling efficient inference without expert demonstrations. The action proposal network corresponds to the Guidance module illustrated in Fig. 1(c), and is used to generate candidate action sequences for predictive control. Our key idea is to use RaWMPC as self-evaluator to pseudo-label sampled actions and train generative policy via contrastive learning. n=1 t:t+H1}Ns 3.3.1 Action Sampling and Pseudo-labeling Given state history s1:t (simplified as s), we randomly sample Ns horizon-H action sequences (simplified as {An}) and compute {An their costs {C n} with the pretrained RaWMPC. We then form pseudo labels by ranking costs: the lowestcost sequence is treated as positive example A+, and the top-K highest-cost sequences are treated . This construction transfers as negatives {A RaWMPCs knowledge (low-risk / high-quality actions) to the proposal network while avoiding any external supervision. }K j= 3.3.2 Action Proposal Network Following [60, 87], we adopt conditional VAE (cVAE) with an action encoder qθ(zA, s), conditional prior pγ(zs), and decoder pψ(Az, s). The decoder serves as the proposal policy at inference. For the positive action, we obtain Gaussian posterior q+ = qθ(zA+, s) = (µ+, diag((σ+)2)) and train the decoder to reconstruct A+. For each negative action , we compute , s) = )2)), but do not reconstruct negaN (µ , diag((σ tives to prevent the generator from imitating unsafe behaviors. The conditional prior is pc = pγ(zs) = (µc, diag((σc)2)). = qθ(zA 3.3.3 Contrastive Training Objective To address the lack of expert supervision in policy learning, we use an InfoNCE objective to make the conditional prior predictive of high-quality actions. Concretely, there are two potential contrastive formulations: 8 Using pc as the anchor, pulling pc toward q+ while pushing it away from {q}K j=1 . Using q+ as the anchor, pulling q+ toward pc while pushing it away from {q}K j=1 . We empirically found that the former often produces under-optimized trajectories. One possible reason is that negative samples are far more numerous and broadly cover the latent space, therefore pc is easily driven to region that is far from most negatives yet not sufficiently close to the positive. In contrast, the latter explicitly pulls pc toward the statistical center of the positive posterior and, via q+, indirectly separates it from the negatives, leading to more stable learning and higher-quality trajectories. Therefore, we adopt the latter design choice to define our InfoNCE objective as: Lc = log exp(ℓ+) exp(ℓ+) + (cid:80)K j=1 exp(ℓ ) ℓ+ = D(q+, pc)/τ, = D(q+, ℓ )/τ, , (13) where D(, ) is the Wasserstein-2 distance between Gaussians and τ is temperature. 3.3.4 Overall Loss of Action Proposal Network The total loss of our cVAE combines reconstruction, KL regularization, and contrastive loss: Ltotal = Ezq+ + β DKL (cid:2) log pψ(A+ z, s)(cid:3) (cid:0)q+ pc(cid:1) + λ Lc. (14) This self-evaluation distillation trains fast proposal policy that generates candidate action sequences consistent with RaWMPCs evaluations, eliminating the need for expert demonstrations during policy learning."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we present comprehensive performance comparison between the proposed framework and state-of-the-art methods. We also conduct extensive ablation studies to assess the effectiveness of our predictive control approach."
        },
        {
            "title": "4.1 Benchmarks",
            "content": "Following prior works [39, 40, 61], we evaluate RaWMPC on two widely used benchmarks: Bench2Drive [29] and NAVSIM [11]. They are complementary: Bench2Drive provides fully interactive Table 1 Comparison with SOTA approaches on the closed-loop Bench2Drive benchmark on CARLA simulator. means the higher the better. DS is taken as the primary metric in comparison and we rank all the methods accordingly, with bold indicating best performance. Method VAD [31] SparseDrive [69] GenAD [99] UniAD [25] MomAD [68] UAD [16] BridgeAD [89] TCP [81] WoTE [39] DriveDPO [61] ThinkTwice [28] DriveTransformer [30] DriveAdapter [27] Raw2Drive [85] Hydra-NeXt [40] HiP-AD [73] RaWMPC w/o Warm-up RaWMPC Pretrained VLM-based Approach ReAL-AD [44] Dual-AEB [94] ETA [21] VLR-Drive [35] ORION [14] SimLingo [57] Venue ICCV 2023 ICRA 2025 ECCV 2024 CVPR 2023 CVPR 2025 T-PAMI 2025 CVPR 2025 NeurIPS 2022 ICCV 2025 NeurIPS 2025 CVPR 2023 ICLR 2025 ICCV 2023 NeurIPS 2025 ICCV 2025 ICCV 2025 - - ICCV 2025 ICRA 2025 ICCV 2025 ICCV 2025 ICCV 2025 CVPR 2025 Scheme IL IL IL IL IL IL IL IL IL DS 42.35 44.54 44.81 45.81 47.91 49.22 50.06 59.90 61.71 IL & RL 62.02 62.44 63.46 64.22 71.36 73.86 86.77 87.34 IL IL IL RL IL IL PC PC SR(%) 15.00 16.71 15.90 16.36 18.11 20.45 22.73 30.00 31.36 30.62 31.23 35.01 33.08 50.24 50.00 69.09 69.62 88. 70.48 41.17 45.23 74.33 75.01 77.74 85.94 11.36 10.00 48.33 50.00 54.62 66.82 IL IL IL IL IL IL Efficiency Comfortness 157.94 170.21 - 129.21 174.91 189.53 - 76.54 - 166.80 69.33 100.64 70.22 214.17 197.76 203.12 203.25 206. - - 186.04 122.52 151.48 244.18 46.01 48.63 - 43.58 51.20 52.71 - 18.08 - 26.79 16.22 20.78 16.01 22.42 20.68 19.36 30.95 32.65 - - 25.77 0.59 17.38 30.76 closed-loop evaluation in CARLA [12] with dense annotations, while NAVSIM evaluates large-scale real-world planning via data-driven, non-reactive simulation-based short-horizon rollout with safetyand progress-aware metrics. Bench2Drive. Bench2Drive is CARLA Leadboard v2 closed-loop benchmark for multi-ability stress testing under complex interactions (e.g., cut-ins, overtakes, detours, emergency braking, and give-way). Its official dataset contains 2M fully annotated frames from short clips spanning 44 scenarios, 23 weather conditions, and 12 towns; the commonly used Base training set contains 1K clips. Closed-loop evaluation is performed on 220 short routes (each focused on single scenario), enabling stable and fine-grained comparison. We report four official metrics: Driving Score (DS), Success Rate (SR), Efficiency, and Comfortness. DS is the primary aggregate score with penalties for safety and rule violations; SR measures successful completion; Efficiency reflects progress; and Comfortness captures motion smoothness. NAVSIM. NAVSIM benchmarks sensor-based plan9 ning on large-scale real-world data built on OpenScene (a planning-oriented reprocessing of nuPlan logs). The task predicts 4-second future ego trajectory (typically 8 waypoints) given short history (e.g., 1.5 seconds) of observations. Following prior works [39, 61], we use the official splits: Navtrain (103K samples) and Navtest (12K samples). We report NAVSIM metrics including NC, DAC, EP, TTC, C, and the primary score PDMS, where PDMS = NC DAC (5EP + 5TTC + 2C)/12. These metrics jointly capture safety, compliance, progress, and motion quality. All results are computed with the official toolkits and recommended splits. Our experiments combine offline warm-up with online simulator interaction, leveraging RaWMPC to learn predictive dynamics and risk-aware decision making. We adopt Bench2Drive for interactive closed-loop evaluation and NAVSIM for large-scale real-world generalization. We further conduct ablations to analyze key training components and strategies. Table 2 Comparison with the SOTA approaches on NAVSIM test set. means the higher the better. PDMS is taken as the primary metric in comparison and we rank all the methods accordingly, with bold indicating best performance. Method Human Venue - Scheme - NC DAC 100 100 DrivingGPT [8] UniAD [25] Latent TransFuser [10] PARA-Drive [80] TransFuser [10] LAW [38] World4Drive [100] DiffusionDrive [42] WoTE [39] Hydra-NeXt [40] UAD [16] DriveDPO [61] GoalFlow [82] RaWMPC w/o Warm-up RaWMPC ICCV 2025 CVPR 2023 T-PAMI 2023 CVPR 2024 T-PAMI 2023 ICLR 2025 ICCV 2025 CVPR 2025 ICCV 2025 ICCV 2025 T-PAMI 2025 NeurIPS 2025 CVPR 2025 - -"
        },
        {
            "title": "IL\nIL\nIL\nIL\nIL\nIL\nIL\nIL\nIL\nIL\nIL",
            "content": "98.9 97.8 97.4 97.9 97.7 96.4 97.4 98.2 98.5 98.1 99.5 IL & RL 98.5 98.4 98.3 98."
        },
        {
            "title": "IL\nPC\nPC",
            "content": "90.7 91.9 92.8 92.4 92.7 95.4 94.3 96.2 96.8 97.7 96.9 98.1 98.3 98.2 98.3 EP 87.5 79.7 78.8 79.0 79.3 79.8 81.7 79.9 88.2 81.9 81.8 78.8 84.3 85.0 85.3 86. TTC 100 94.9 92.9 92.4 93.0 92.7 88.7 92.8 94.7 94.9 94.6 97.5 94.8 94.6 94.5 95.6 99.9 100.0 100.0 100.0 99.8 100.0 99.9 100.0 100.0 99.9 100.0 100.0 99.9 100.0 99.9 99.9 PDMS 94. 82.4 83.4 83.8 84.0 84.5 84.6 85.1 88.1 88.3 88.6 89.3 90.0 90.3 90.5 91."
        },
        {
            "title": "4.2 Implementation Details\nNetwork architecture. We employ a pretrained\nViT [66] as the vision encoder and use the SegViT\nsegmentation head [88] as the segmentation decoder.\nBEV features are extracted from multi-view images\nusing the query-based view transformer [41]. Fol-\nlowing previous works [10, 39], the input image res-\nolution is 1024 × 256, and the BEV feature map\nresolution is 256 × 256. We use down-sampling fac-\ntors of 32 (front-view) and 16 (BEV), resulting in\n512 visual tokens it (256 per branch). Measurement\ninputs are encoded through an MLP-based encoder\ninto 4 measurement tokens mt, and driving actions,\nrepresented as three scalar values (steer, throttle,\nbrake), are transformed into 3 action tokens at via\na linear layer. We implement the world model as\na transformer with 4 layers and 8 attention heads.\nRaWMPC predicts H=10 future steps conditioned\non the past 5 observed steps, and evaluates N =10\ncandidate action sequences proposed by the distilled\naction proposal network (described in Section 3.3)\nusing the cost in Eq. (6) during inference. During\naction selection, we use ηk = max(2−k+1, 1/8) to\ndownweight distant predictions, and set the event\nseverity weights to λj = 10, 15, 30 for off-lane driving,\ntraffic-sign violations, and collisions, respectively.",
            "content": "Training. RaWMPC is trained with the two-stage risk-aware interactive training strategy described in Section 3.2. We first perform an offline warm-up using 10% of the training data (100 clips for Bench2Drive and 10K samples for NAVSIM), and then refine the model via online interaction using the proposed riskaware training scheme. The random-sampling probability ε1 is linearly annealed from 1 to 0, while the risksampling probability ε2 is linearly increased from 0 to 0.3. We maintain replay buffer of size 10K frames to store recent interaction data (e.g., RGB images, semantic segmentation, ego measurements, and trafficevent annotations). We use segment-wise sampling of horizon-H=10 action sequences to ensure temporal continuity. In good/bad modes, we rank the Ns=50 candidates by cost and sample from the bottom/topK=5 sets using temperatures τg = 0.5 and τb = 1.0. An episode terminates when any of the following conditions is met: (1) the ego vehicle incurs 3 collisions, (2) the ego vehicle goes off-road or remains stuck for 100 consecutive steps, or (3) the ego vehicle successfully completes the route. Across offline warm-up and online interaction, we train RaWMPC using total of 1K clips on Bench2Drive and 100K samples on NAVSIM (comparable in scale to the official training sets for fair comparison), on four NVIDIA A100 GPUs. We use Adam [33] with an initial learning rate of 104 decayed to 105, and batch size of 16. Self-evaluation distillation. For self-evaluation distillation (Section 3.3), the action proposal network is implemented as cVAE [67] with 32-dimensional latent space. During contrastive training, we sample Ns=50 action sequences, treat the minimum-cost 10 sequence as the positive example A+, and use the top-K=5 highest-cost sequences as negatives. We optimize the proposal network with the objective in Section 3.3, using temperature τ =0.3, KL weight schedule β : 0 0.1, and contrastive weight λ=0.1. At inference, we sample =10 candidates from the cVAE decoder and select the final action by minimizing the predicted cost (Eq. (6)) under the worldmodel rollout."
        },
        {
            "title": "Art",
            "content": "In this section, we compare RaWMPC with state-ofthe-art end-to-end driving methods on the closed-loop Bench2Drive benchmark in CARLA and the NAVSIM test set. To reflect our main claim, we report two training settings: (i) w/o warm-up, where RaWMPC is trained without using any offline logged video, and (ii) the full setting, where small set of logged driving trajectories is used as an optional warm start. The warm start empirically accelerates convergence and further improves final performance, while RaWMPC already surpasses prior state-of-the-art even without it. 4.3.1 Evaluation on Bench2Drive Table 1 illustrates the closed-loop results on Bench2Drive. RaWMPC achieves the best overall performance among all compared methods, reaching 88.31 DS and 70.48% SR in the full setting. More importantly, even without warm-up (i.e., without logged trajectories), RaWMPC still attains 87.34 DS and 69.62% SR, surpassing strong recent baselines such as HiP-AD [73] (86.77 DS / 69.09% SR) and the pretrained-VLM method SimLingo [57] (85.94 DS / 66.82% SR). In addition, RaWMPC maintains competitive efficiency and achieves higher comfortness than most high-performing closed-loop agents, indicating that the gains are not obtained by aggressive maneuvers but by more reliable decision-making. 4.3.2 Evaluation on NAVSIM Table 2 summarizes the results on NAVSIM. RaWMPC achieves the highest PDMS of 91.3 among learning-based methods. Without warm-up, all RaWMPC still reaches 90.5 PDMS, already outperforming previous best methods (e.g., GoalFlow [82]: 90.3). The warm start further improves PDMS (90.591.3), consistent with the observation that small amount of logged trajectories can accelerate convergence and improve performance, while not being required to achieve state-of-the-art results."
        },
        {
            "title": "4.3.3 Generalization under weather-induced",
            "content": "domain shift To evaluate robustness beyond the training distribution, we conduct weather-shift study where all methods are evaluated exclusively on Rainy scenarios while being trained on either Sunny only or Sunny & Rainy data  (Table 3)  . key observation is that imitationbased methods are sensitive to training-domain coverage: when rainy conditions are absent from training (Sunny-only), their performance drops notably, reflecting limited transfer to previously unseen environments. In contrast, RaWMPC achieves the best DS and SR under both training regimes, and notably still significantly outperforms strong IL baselines (LAW, WoTE) and SimLingo when trained on Sunny only (i.e., facing an unseen rainy target domain). Moreover, compared with SimLingo, RaWMPC exhibits substantially smaller degradation when rainy data is removed from training, indicating stronger robustness to previously unseen conditions. Figure 5 shows qualitative example of this Sunnyonly Rainy shift. LAW fails to recognize the lead vehicle under the altered visual conditions and results in high-severity frontal collision. WoTE and SimLingo attempt evasive maneuvers that reduce the impact relative to direct frontal crash, yet the scene still ends in side-swipe/rear collision. plausible explanation is that the rainy shift degrades the reliability of the perceptiondecision stack, while the downstream policy does not explicitly optimize for minimum-risk clearance under uncertainty, yielding insufficient safety margins during close-proximity avoidance (e.g., inaccurate motion anticipation or mismatched ego response on wet roads). By contrast, RaWMPC explicitly evaluates the predicted consequences of candidate action sequences with risk-aware world model and selects the minimum-risk predictive-control behavior, maintaining safe clearance under uncertainty. We attribute this advantage to RaWMPCs riskaware predictive-control formulation: instead of reproducing expert actions, RaWMPC learns riskawareness from interaction and selects actions by minimizing predicted risk via the learned world model. Such an objective encourages transferable decision principles (e.g., maintaining safe margins and acting conservatively under uncertainty) that remain effective when appearance and dynamics change across domains. Thus, RaWMPC is less dependent on exhaustive expert action coverage for corner cases, which better matches the long-tail nature of real-world deployment where unseen scenarios are inevitable. 11 Figure 5 Qualitative comparison under weather-induced domain shift (Sunny-only Rainy). All methods are trained on Sunny-only data and evaluated in Rainy conditions. LAW [38] misses the lead vehicle, causing severe frontal collision. WoTE [39] and SimLingo [57] reduce severity by evasive maneuvers but still collide due to degraded perceptiondecision reliability and weak safety-margin enforcement. RaWMPC avoids collisions by selecting the minimum-risk predictive-control action under uncertainty. Table 3 Performance comparison under domain shift. All methods are trained on either Sunny only or Sunny & Rainy data, and evaluated exclusively on Rainy scenarios. indicates higher is better. Tested on Rainy Method LAW WoTE ICLR ICCV 2025 Venue Scheme Training Data DS IL IL IL Sunny & Rainy 34.54 23.58 Sunny only Sunny & Rainy 36.54 28.65 Sunny only Sunny & Rainy 51.69 33. Sunny only Sunny & Rainy 53.67 41.36 Sunny only SR(%) 7.09 3.56 7.85 5.21 13.68 8. 14.96 10.83 SimLingo (Pretrained-VLM) CVPR 2025 RaWMPC PC 4.3.4 Qualitative visualization of predictive control We provide some visualization results of the predictive control procedure of RaWMPC in Fig. 6. Given RGB observations and high-level navigation commands (e.g., keep going straight or merge left), our generative policy proposes small set of candidate action sequences (e.g., keep going straight, detour, brake, and lane change). For each candidate, the riskaware world model predicts the near-future semantic traffic state and the decoding module evaluates its consequence from both task progress and safety perspectives, including collision risk, off-lane/sidewalk intrusion risk, and progress-related penalties such as getting stuck in traffic. The final action is selected by comparing these predicted consequences and choosing the minimum-cost one under the navigation goal. In the first case, going straight collides with crossing pedestrian, while detours either collide with an oncoming vehicle or drive onto the sidewalk; RaWMPC chooses slow down, straight briefly, then stop to safely stop in front of the pedestrian (instead of an overly conservative early brake). In the second case, going straight or merging left immediately causes collision, steering right hits parked vehicle, and stopping leads to deadlock; thus RaWMPC selects pause briefly, then merge left for collision-free merge. These cases demonstrate that RaWMPC can proactively avoid risky behaviors by explicitly forecasting and comparing action consequences, rather than merely following single command or relying on fixed fallback maneuver."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "In this section, we provide comprehensive ablation studies of the proposed approach using the 12 Figure 6 Visualization of the predictive control procedure. At time t, we show the front-view and the BEV images (with segmentation). Dashed curves indicate candidate actions and highlighted agents denote key risks. Rollouts from + 1 to + 5 illustrate predicted consequences, and the bottom panel reports each actions outcomes and costs (e.g., collision, sidewalk intrusion, stopping distance). Scenario 1: RaWMPC slows down, proceeds briefly, then stops for the pedestrian. Scenario 2: RaWMPC waits briefly, then turns left to avoid both the front-left vehicle and the parked car. Bench2Drive dataset. Table 4 Ablation study on the proposed framework. 4.4.1 Analysis of Framework Method Table 4 analyzes core components aligned with our model design (Sec. 3.1.1). w/o Semantic Guidance removes semantic-guided event decoding, i.e., the fusion of semantic attention from the segmentation decoder into the event decoder. This leads to clear drop (DS 88.3182.36, SR 70.48%62.69%), showing that accurate safety-event prediction is crucial for risk-aware cost evaluation. w/o Segmentation Entire RaWMPC (Ours) w/o Semantic Guidance w/o Segmentation Decoder w/o Action Selection Metrics DS SR(%) 88.31 82.36 -5.95 70.85 -17.46 61.35 -26.96 70.48 62.69 -7.79 48.95 -21.53 30.98 -39.50 Decoder further removes the segmentation decoding branch and its supervision, resulting in large degra13 Table 5 Ablation study on the risk-aware training. Method Metrics DS SR(%) Risk-aware Sampling (Ours) ϵ-Greedy Sampling Random Sampling 88.31 83.86 -4.45 70.41 -17.90 70.48 61.74 -8.74 46.82 -23. Table 6 Results obtained using different action supervision in policy learning. Table 7 Results obtained using different amounts of prediction horizon. Prediction Horizon H=1 H=5 H=10 (Ours) H=15 Metrics DS 57.85 -30.46 74.98 -13.33 88.31 82.34 -5. SR(%) 28.64 -41.84 49.52 -20.96 70.48 62.38 -8.10 Policy Learning Data Metrics DS SR(%) Table 8 Results obtained using different amounts of offline learning data in warm-up. Pos. & Neg. Actions (Ours) Expert Actions Only Positive Actions 88.31 86.75 -1.56 83.65 -4.66 70.48 68.25 -2.23 66.52 -3.96 dation (DS 70.85 / SR 48.95%), which indicates that forecasting high-level semantics is essential for reliable long-horizon rollouts. w/o Action Selection disables predictive control in Eq. (1) (bypassing costbased ranking in Eq. (6)) and directly executes the proposal/guidance output, causing the most severe collapse (DS 61.35 / SR 30.98%). This confirms that selecting actions by explicitly evaluating predicted long-horizon consequences is the key to RaWMPC. 4.4.2 Analysis of Risk-Aware Training Table 5 evaluates the risk-aware interaction training strategy used to refine the world model. Our riskaware sampling follows the design in Sec. 3.2: besides random exploration, it uses the current RaWMPC to score candidate action sequences and deliberately collects both good (low-cost) and bad (high-cost) rollouts, improving coverage of rare safety-critical outcomes. Replacing it with ϵ-greedy sampling (random with probability ϵ1, otherwise only selecting low-cost rollouts) reduces performance (DS 83.86 / SR 61.74%), showing that excluding high-cost failures weakens learning of risky consequences. Pure random sampling further degrades results (DS 70.41 / SR 46.82%), indicating that unguided data collection is substantially less efficient for learning long-horizon consequences. Warm-up Data 0% 10% 20% 30% Metrics DS 87.34 -0.97 88.31 88.09 -0.22 86.95 -1.36 SR(%) 69.62 -0.86 70.48 70.32 -0.16 68.52 -1.96 performance (DS 88.31 / SR 70.48%). Training the proposal network only with expert actions slightly degrades performance (DS 86.75 / SR 68.25%), suggesting that self-evaluated targets align better with the predictive-control objective than direct imitation targets. Using only positive actions further drops performance (DS 83.65 / SR 66.52%), indicating that explicitly contrasting against high-risk negatives is important for preventing unsafe candidates and improving downstream selection. 4.4.4 Discussion on Prediction Horizon Table 7 ablates the planning horizon used in the world-model rollout and cost evaluation (Eq. (6)). Short horizons fail to capture delayed consequences, leading to poor performance (H=1: DS 57.85 / SR 28.64%; H=5: DS 74.98 / SR 49.52%). Increasing to H=10 yields the best results (DS 88.31 / SR 70.48%), as it provides sufficient look-ahead for risk assessment while keeping prediction uncertainty manageable. Further increasing to H=15 degrades performance (DS 82.34 / SR 62.38%), likely due to accumulated rollout errors that affect cost-based ranking. 4.4.3 Analysis of Self-Evaluation Distillation 4.4.5 Discussion on Warm-up Table 6 studies how to train the action proposal network in Sec. 3.3. Our default setting uses RaWMPC as self-evaluator to pseudo-label actions: the lowestcost sequence is treated as positive, while high-cost sequences serve as negatives, which yields the best Table 8 ablates the fraction of offline logged trajectories used for warm-up before interactive training. In this study, we keep the total number of training samples fixed and vary only the proportion allocated to offline warm-up data. Without warm-up (0%), 14 Table 9 Ablation study on the control pattern with learned world model. Table 10 Prediction accuracy of future traffic events with learned world model. Method Metrics DS SR(%) Predictive Control (Ours) Reinforcement Learning 88.31 73.58 -14.73 70.48 51.85 -18.63 performance drops (DS 87.34 / SR 69.62%), suggesting that training from scratch leads to less reliable rollouts and less stable early optimization stage. Using small amount of logged trajectories (10%) yields the best results (DS 88.31 / SR 70.48%), indicating that light warm-up provides useful predictive priors (e.g., basic dynamics modeling and perception decoding) that improve long-horizon rollout quality and downstream control. However, further increasing the warm-up ratio begins to degrade performance (20%: DS 88.09 / SR 70.32%; 30%: DS 86.95 / SR 68.52%). We attribute this trend to the fact that offline logged trajectories are strongly biased toward safe, human-like behaviors and contain few hazardous events. Consequently, allocating too much data to offline warm-up reduces the opportunities for subsequent online interaction to explore unconventional actions and collect safety-critical failuresespecially in dangerous scenarioswhich are essential for learning robust risk awareness. 4.4.6 Discussion on Control Pattern Table 9 compares predictive control to directly optimizing policy with model-based reinforcement learning (RL). Predictive control achieves substantially higher performance (DS 88.31 / SR 70.48%) than model-based RL (DS 73.58 / SR 51.85%). This validates the benefit of evaluating candidate action sequences via decoded future outcomes (segmentation, events, ego-states) and selecting the minimum-cost one, rather than relying on end-to-end policy optimization alone. 4.4.7 World-Model Prediction Accuracy Table 10 reports the event prediction quality used by the cost function in Eq. (6). The predictor achieves high accuracy across event types (0.910.96) and strong recall on collision-related events (e.g., pedestrian collision recall 0.99), providing reliable signals for risk-aware evaluation. We observe lower precision for some rare events (e.g., pedestrian collision precision 0.52), reflecting conservative tendency with more false positives; in safety-critical driving, priori15 Collision Accuracy Recall Precision Pedestrian Vehicle Static 0.93 0.96 0.91 0.99 0.52 0.84 0. 0.89 0.63 Running Traffic Sign 0.91 0.84 0.68 tizing recall can be preferable to missing hazards."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we proposed RaWMPC, risk-aware world-model predictive control framework for endto-end autonomous driving that does not require expert action supervision. RaWMPC learns an action-conditioned world model to roll out multiple candidate behaviors, predicts future semantics and safety-critical events, and selects actions by explicitly minimizing risk-aware cost. To make rare-butcatastrophic outcomes predictable and avoidable, we introduced risk-aware interaction strategy that intentionally collects both safe and hazardous rollouts, and we further proposed self-evaluation distillation to train an efficient action proposal policy using RaWMPC as self-evaluator. Extensive experiments on Bench2Drive and NAVSIM show that RaWMPC achieves state-of-the-art performance and stronger robustness under domain shift, even without offline warm-up, showing the potential to significantly reduce the reliance on costly real-world expert demonstrations. For future work, we will explore domain adaptation and more efficient planning to better support real-world deployment and sim-to-real transfer."
        },
        {
            "title": "Statements and Declarations",
            "content": "Competing interests. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Data availability. This work does not propose any new dataset. The datasets (Bench2Drive [29] and NAVSIM [11]) that support the findings of this study are openly available at the URLs: Bench2Drive and NAVSIM."
        },
        {
            "title": "References",
            "content": "[1] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. [2] Jinkun Cao, Xin Wang, Trevor Darrell, and Fisher Yu. Instance-aware predictive navigation in multiagent environments. In IEEE International Conference on Robotics and Automation, pages 50965102, 2021. [3] Wei Cao, Marcel Hallgarten, Tianyu Li, Daniel Dauner, Xunjiang Gu, Caojun Wang, Yakov Miron, Marco Aiello, Hongyang Li, Igor Gilitschenski, et al. Pseudo-simulation for autonomous driving. arXiv preprint arXiv:2506.04218, 2025. [4] Raphael Chekroun, Marin Toromanoff, Sascha Hornauer, and Fabien Moutarde. Gri: General reinforced imitation and its application to vision-based autonomous driving. In NeurIPS 2021, Machine Learning for Autonomous Driving Workshop, 2021. [5] Dian Chen and Philipp Krähenbühl. Learning from all vehicles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1722217231, 2022. [6] Dian Chen, Vladlen Koltun, and Philipp Krähenbühl. Learning to drive from world on rails. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1559015599, 2021. [7] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li. End-toend autonomous driving: Challenges and frontiers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [8] Yuntao Chen, Yuqi Wang, and Zhaoxiang Zhang. Drivinggpt: Unifying driving world modeling and planning with multi-modal autoregressive transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2689026900, October 2025. [9] Pranav Singh Chib and Pravendra Singh. Recent advancements in end-to-end autonomous driving using deep learning: survey. IEEE Transactions on Intelligent Vehicles, 9(1):103118, 2023. [10] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and Andreas Geiger. Transfuser: Imitation with transformer-based sensor fusion for autonomous driving. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. [11] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, et al. Navsim: Data-driven non-reactive autonomous vehicle simulation and benchmarking. Advances in Neural Information Processing Systems, 37:2870628719, 2025. [12] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on Robot Learning, pages 116, 2017. [13] Lan Feng, Quanyi Li, Zhenghao Peng, Shuhan Tan, and Bolei Zhou. Trafficgen: Learning to generate diverse and realistic traffic scenarios. In 2023 IEEE international conference on robotics and automation (ICRA), pages 35673575. IEEE, 2023. [14] Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, Dingkang Liang, Chong Zhang, Dingyuan Zhang, Hongwei Xie, Bing Wang, and Xiang Bai. Orion: holistic end-to-end autonomous driving framework by vision-language instructed action generation. arXiv preprint arXiv:2503.19755, 2025. [15] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [16] Mingzhe Guo, Zhipeng Zhang, Yuan He, Ke Wang, Liping Jing, and Haibin Ling. End-to-end autonomous driving without costly modularization and 3d manual annotation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [17] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31, 2018. [18] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2019. [19] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pages 25552565. PMLR, 2019. [20] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. [21] Shadi Hamdan, Chonghao Sima, Zetong Yang, Hongyang Li, and Fatma Güney. Eta: Efficiency through thinking ahead, dual approach to self-driving with large models. In Proceedings of 16 the IEEE/CVF International Conference on Computer Vision, 2025. [22] Mikael Henaff, Alfredo Canziani, and Yann LeCun. Model-predictive policy learning with uncertainty regularization for driving in dense traffic. In International Conference on Learning Representations, 2018. [23] Anthony Hu, Gianluca Corrado, Nicolas Griffiths, Zachary Murez, Corina Gurau, Hudson Yeo, Alex Kendall, Roberto Cipolla, and Jamie Shotton. Model-based imitation learning for urban driving. Advances in Neural Information Processing Systems, 35:2070320716, 2022. [24] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. [25] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [26] Bernhard Jaeger, Kashyap Chitta, and Andreas Geiger. Hidden biases of end-to-end driving models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 82408249, 2023. [27] Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, and Hongyang Li. Driveadapter: Breaking the coupling barrier of perception and planning in end-to-end autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 79537963, 2023. [28] Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, and Hongyang Li. Think twice before driving: Towards scalable decoders for end-to-end autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2198321994, 2023. [29] Xiaosong Jia, Zhenjie Yang, Qifeng Li, Zhiyuan Zhang, and Junchi Yan. Bench2drive: Towards multi-ability benchmarking of closed-loop end-to-end autonomous driving. arXiv preprint arXiv:2406.03877, 2024. [30] Xiaosong Jia, Junqi You, Zhiyuan Zhang, and Junchi Yan. Drivetransformer: Unified transformer for scalable end-to-end autonomous driving. In International Conference on Learning Representations (ICLR), 2025. [31] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 83408350, 2023. [32] Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen, VinhDieu Lam, Alex Bewley, and Amar Shah. Learning to drive in day. In 2019 international conference on robotics and automation (ICRA), pages 82488254. IEEE, 2019. [33] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [34] Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, and Peter Anderson. Pathdreamer: world model for indoor navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1473814748, 2021. [35] Fanjie Kong, Yitong Li, Weihuang Chen, Chen Min, Yizhe Li, Zhiqiang Gao, Haoyang Li, Zhongyu Guo, and Hongbin Sun. Vlr-driver: Large vision-language-reasoning models for embodied autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2696626976, 2025. [36] Hanyang Kong, Dongze Lian, Michael Bi Mi, and Xinchao Wang. Dreamdrone: Text-to-image diffusion models are zero-shot perpetual view generators. In European Conference on Computer Vision, pages 324341. Springer, 2024. [37] Qifeng Li, Xiaosong Jia, Shaobo Wang, and Junchi Yan. Think2drive: Efficient reinforcement learning by thinking with latent world model for autonomous driving (in carla-v2). In European Conference on Computer Vision, pages 142158. Springer, 2024. [38] Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang, and Tieniu Tan. Enhancing end-to-end autonomous driving with latent world model. In The Thirteenth International Conference on Learning Representations, 2025. [39] Yingyan Li, Yuqi Wang, Yang Liu, Jiawei He, Lue Fan, and Zhaoxiang Zhang. End-to-end driving with online trajectory evaluation via bev world In Proceedings of the IEEE/CVF Intermodel. national Conference on Computer Vision (ICCV), pages 2713727146, October 2025. [40] Zhenxin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Zuxuan Wu, and Jose M. Alvarez. Hydra-next: Robust closed-loop driving with open-loop training. In Proceedings of the IEEE/CVF International Con17 ference on Computer Vision (ICCV), pages 27305 27314, October 2025. [41] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer: learning birds-eye-view representation from lidar-camera via spatiotemporal transformers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [42] Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, and Xinggang Wang. Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1203712047, June 2025. [43] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988, 2017. [44] Yuhang Lu, Jiadong Tu, Yuexin Ma, and Xinge Zhu. Real-ad: Towards human-like reasoning in end-toend autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2778327793, 2025. [45] Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, et al. Lingoqa: Visual question answering for autonomous driving. In European Conference on Computer Vision, pages 252269. Springer, 2024. [46] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. In RSS, 2023. [47] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565571. Ieee, 2016. In Proceedings struction via online restoration. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15591569, June 2025. [50] Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, and Zehuan Wu. Maskgwm: generalizable driving world model with video mask reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2238122391, June 2025. [51] Brian Paden, Michal Čáp, Sze Zheng Yong, Dmitry Yershov, and Emilio Frazzoli. survey of motion planning and control techniques for self-driving urban vehicles. IEEE Transactions on intelligent vehicles, 1(1):3355, 2016. [52] Xinlei Pan, Xiangyu Chen, Qizhi Cai, John Canny, and Fisher Yu. Semantic predictive control for explainable and efficient policy learning. In International Conference on Robotics and Automation, pages 32033209, 2019. [53] Scott Drew Pendleton, Hans Andersen, Xinxin Du, Xiaotong Shen, Malika Meghjani, You Hong Eng, Daniela Rus, and Marcelo Ang. Perception, planning, control, and coordination for autonomous vehicles. Machines, 5(1):6, 2017. [54] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-modal fusion transformer for endto-end autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 70777087, 2021. [55] Katrin Renz, Kashyap Chitta, Otniel-Bogdan Mercea, Sophia Koepke, Zeynep Akata, and Andreas Geiger. Plant: Explainable planning transformers via object-level representations. In Conference on Robot Learning, pages 459470, 2023. [56] Katrin Renz, Long Chen, Ana-Maria Marcu, Jan Hünermann, Benoit Hanotte, Alice Karnsund, Jamie Shotton, Elahe Arani, and Oleg Sinavski. Carllava: Vision language models for camera-only closed-loop driving. arXiv preprint arXiv:2406.10165, 2024. [48] Michael Montemerlo, Jan Becker, Suhrid Bhat, Hendrik Dahlkamp, Dmitri Dolgov, Scott Ettinger, Dirk Haehnel, Tim Hilden, Gabe Hoffmann, Burkhard Huhnke, et al. Junior: The stanford entry in the urban challenge. Journal of field Robotics, 25(9): 569597, 2008. [57] Katrin Renz, Long Chen, Elahe Arani, and Oleg Sinavski. Simlingo: Vision-only closed-loop autonomous driving with language-action alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1199312003, June 2025. [49] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, Yifei Zhan, Kun Zhan, Peng Jia, Xianpeng Lang, Xingang Wang, and Wenjun Mei. Recondreamer: Crafting world models for driving scene recon- [58] Nicholas Rhinehart, Rowan McAllister, and Sergey Levine. Deep imitative models for flexible inference, planning, and control. In International Conference on Learning Representations, 2020. [59] Ahmad El Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. End-to-end deep re18 inforcement learning for lane keeping assist. arXiv preprint arXiv:1612.04340, 2016. [60] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data. In European Conference on Computer Vision, pages 683700. Springer, 2020. [61] ShuYao Shang, Yuntao Chen, Yuqi Wang, Yingyan Li, and Zhaoxiang Zhang. Drivedpo: Policy learning via safety dpo for end-to-end autonomous driving. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [62] Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, and Yu Liu. Safety-enhanced autonomous driving using interpretable sensor fusion transformer. In Conference on Robot Learning, pages 726737, 2023. [63] Hao Shao, Letian Wang, Ruobing Chen, Steven Waslander, Hongsheng Li, and Yu Liu. Reasonnet: End-to-end driving with temporal and global reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1372313733, 2023. [64] Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven Waslander, Yu Liu, and Hongsheng Li. Lmdrive: Closed-loop end-to-end driving with large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1512015130, 2024. [65] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In European Conference on Computer Vision, pages 256274. Springer, 2024. [66] Oriane Siméoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. [67] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems, 28, 2015. [68] Ziying Song, Caiyan Jia, Lin Liu, Hongyu Pan, Yongchang Zhang, Junming Wang, Xingyu Zhang, Shaoqing Xu, Lei Yang, and Yadan Luo. Dont shake the wheel: Momentum-aware planning in end-to-end autonomous driving. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2243222441, 2025. [69] Wenchao Sun, Xuewu Lin, Yining Shi, Chuang Zhang, Haoran Wu, and Sifa Zheng. Sparsedrive: 19 End-to-end autonomous driving via sparse scene representation. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 87958801. IEEE, 2025. [70] Shuhan Tan, Kelvin Wong, Shenlong Wang, Sivabalan Manivasagam, Mengye Ren, and Raquel Urtasun. Scenegen: Learning to generate realistic traffic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 892901, 2021. [71] Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, and Philipp Kraehenbuehl. Language conditioned traffic generation. In Conference on Robot Learning, pages 27142752. PMLR, 2023. [72] Shuhan Tan, Boris Ivanovic, Yuxiao Chen, Boyi Li, Xinshuo Weng, Yulong Cao, Philipp Kraehenbuehl, and Marco Pavone. Promptable closed-loop traffic simulation. In Annual Conference on Robot Learning, 2024. [73] Yingqi Tang, Zhuoran Xu, Zhaotie Meng, and Erkang Cheng. Hip-ad: Hierarchical and multigranularity planning with deformable attention for autonomous driving in single decoder. arXiv preprint arXiv:2503.08612, 2025. [74] Sebastian Thrun, Mike Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, James Diebel, Philip Fong, John Gale, Morgan Halpenny, Gabriel Hoffmann, et al. Stanley: The robot that won the darpa grand challenge. Journal of field Robotics, 23(9):661692, 2006. [75] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. End-to-end model-free reinforcement learning for urban driving using implicit affordances. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71537162, 2020. [76] Chris Urmson, Joshua Anhalt, Drew Bagnell, Christopher Baker, Robert Bittner, MN Clark, John Dolan, Dave Duggins, Tugrul Galatali, Chris Geyer, et al. Autonomous driving in urban environments: Boss and the urban challenge. Journal of field Robotics, 25(8):425466, 2008. [77] Hanqing Wang, Wei Liang, Luc Van Gool, and Wenguan Wang. Dreamwalker: Mental planning for continuous vision-language navigation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1087310883, 2023. [78] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1474914759, 2024. ings of the IEEE/CVF international conference on computer vision, pages 98139823, 2021. [79] Kehan Wen, Yutong Hu, Yao Mu, and Lei Ke. Mˆ 3pc: Test-time model predictive control using pretrained masked trajectory model. In The Thirteenth International Conference on Learning Representations, 2025. [88] Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xiaolin Wei, Chunhua Shen, et al. Segvit: Semantic segmentation with plain vision transformers. In Advances in Neural Information Processing Systems, 2022. [80] Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone. Para-drive: Parallelized architecture for real-time autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [81] Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, and Yu Qiao. Trajectory-guided control prediction for end-to-end autonomous driving: simple yet strong baseline. Advances in Neural Information Processing Systems, 35:61196132, 2022. [82] Zebin Xing, Xingyu Zhang, Yang Hu, Bo Jiang, Tong He, Qian Zhang, Xiaoxiao Long, and Wei Yin. Goalflow: Goal-driven flow matching for multimodal trajectories generation in end-to-end autonomous driving. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 16021611, 2025. [83] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee Wong, Zhenguo Li, and Hengshuang Zhao. Drivegpt4: Interpretable end-toend autonomous driving via large language model. IEEE Robotics and Automation Letters, 2024. [84] Xuemeng Yang, Licheng Wen, Tiantian Wei, Yukai Ma, Jianbiao Mei, Xin Li, Wenjie Lei, Daocheng Fu, Pinlong Cai, Min Dou, Liang He, Yong Liu, Botian Shi, and Yu Qiao. Drivearena: closedloop generative simulation platform for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2693326943, October 2025. [85] Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, and Junchi Yan. Raw2drive: Reinforcement learning with aligned world models for end-to-end autonomous driving (in carla v2), 2025. Accepted by NeurIPS 2025. [86] Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, and Matthew Gadd. Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model. arXiv preprint arXiv:2402.10828, 2024. [87] Ye Yuan, Xinshuo Weng, Yanglan Ou, and Kris Kitani. Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting. In Proceed- [89] Bozhou Zhang, Nan Song, Xin Jin, and Li Zhang. Bridging past and future: End-to-end autonomous driving with historical prediction and planning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 68546863, 2025. [90] Jimuyang Zhang, Zanming Huang, and Eshed OhnBar. Coaching teachable student. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78057815, 2023. [91] Kaiwen Zhang, Zhenyu Tang, Xiaotao Hu, Xingang Pan, Xiaoyang Guo, Yuan Liu, Jingwei Huang, Li Yuan, Qian Zhang, Xiao-Xiao Long, Xun Cao, and Wei Yin. Epona: Autoregressive diffusion world model for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2722027230, October 2025. [92] Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, and Raquel Urtasun. Copilot4d: Learning unsupervised world models for autonomous driving via discrete diffusion. In ICLR, 2024. [93] Peizhi Zhang, Lu Xiong, Zhuoping Yu, Peiyuan Fang, Senwei Yan, Jie Yao, and Yi Zhou. Reinforcement learning-based end-to-end parking for automatic parking system. Sensors, 19(18):3996, 2019. [94] Wei Zhang, Pengfei Li, Junli Wang, Bingchuan Sun, Qihao Jin, Guangjun Bao, Shibo Rui, Yang Yu, Wenchao Ding, Peng Li, et al. Dual-aeb: Synergizing rule-based and multimodal large language models for effective emergency braking. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 1488814895. IEEE, 2025. [95] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. End-to-end urban driving by imitating reinforcement learning coach. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1522215232, 2021. [96] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. Trafficbots: Towards world models for autonomous driving simulation and motion prediction. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 15221529. IEEE, 2023. 20 [97] Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, Wenjun Mei, and Xingang Wang. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1201512026, June 2025. [98] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu. Occworld: Learning 3d occupancy world model for autonomous driving. In European conference on computer vision, pages 5572. Springer, 2024. [99] Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming Zhang, and Long Chen. Genad: Generative end-to-end autonomous driving. In European Conference on Computer Vision, pages 87104. Springer, 2024. [100] Yupeng Zheng, Pengxuan Yang, Zebin Xing, Qichao Zhang, Yuhang Zheng, Yinfeng Gao, Pengfei Li, Teng Zhang, Zhongpu Xia, Peng Jia, XianPeng Lang, and Dongbin Zhao. World4drive: End-toend autonomous driving via intention-aware physical latent world model. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2863228642, October 2025. [101] Julius Ziegler, Philipp Bender, Markus Schreiber, Henning Lategahn, Tobias Strauss, Christoph Stiller, Thao Dang, Uwe Franke, Nils Appenrodt, Christoph Keller, et al. Making bertha drivean autonomous journey on historic route. IEEE Intelligent transportation systems magazine, 6(2): 820, 2014. [102] Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, and Jiwen Lu. Gaussianworld: Gaussian world model for streaming 3d occupancy prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 67726781, June 2025."
        }
    ],
    "affiliations": [
        "Sun Yat-sen University",
        "University of Trento"
    ]
}