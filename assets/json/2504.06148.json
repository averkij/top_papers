{
    "paper_title": "V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models",
    "authors": [
        "Xiangxi Zheng",
        "Linjie Li",
        "Zhengyuan Yang",
        "Ping Yu",
        "Alex Jinpeng Wang",
        "Rui Yan",
        "Yuan Yao",
        "Lijuan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https://github.com/CSU-JPG/V-MAGE."
        },
        {
            "title": "Start",
            "content": "V-MAGE: Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models Xiangxi Zheng 1 Linjie Li 2 Zhengyuan Yang 2 Ping Yu 1 Alex Jinpeng Wang 3 Rui Yan 1 Yuan Yao 1 Lijuan Wang 2 5 2 0 2 8 ] . [ 1 8 4 1 6 0 . 4 0 5 2 : r Abstract Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequatethey lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), game-based evaluation framework designed to assess MLLMs visual reasoning capabilities. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https: //github.com/CSU-JPG/V-MAGE. 1. Introduction Building on the success of Large Language Models (LLMs) in text-based tasks (Bai et al., 2023; Cai et al., 2024; Ope1Nanjing University, China 2Microsoft Corporation, USA 3Central South University, China. Correspondence to: Xiangxi Zheng <zhengxx@smail.nju.edu.cn>. Figure 1: V-MAGE selected five different games and designed several levels for each game to decompose the evaluation of model performance. The games used are FlappyBird, RaceGame, SuperMario, PongGame, and Tempest Run. nAI, 2023b), researchers have extended their capabilities to visual-text multimodal tasks through Multimodal Large Language Models (MLLMs)(OpenAI, 2023a; Liu et al., 2023a; Team, 2023; Yang et al., 2023; Li et al., 2024b; Wang et al., 2024; Chen et al., 2024c). Various multimodal evaluation benchmarks, such as MME(Fu et al., 2023), MMBench(Liu et al., 2023b), SEED-Bench(Li et al., 2023), MMMU(Yue et al., 2024), and MM-Vet(Yu et al., 2023; 2024), have driven improvements in MLLM performance. As models grow stronger, researchers are shifting toward openworld, dynamic, multi-round tasks beyond static benchmarks with fixed image-text inputs. Game-based evaluation has emerged as promising alternative, offering more natural and interactive assessment of models perception and reasoning abilities. Although progress has been made, current game-based MLLM benchmarks rely on text-based(Costarelli et al., 2024; Hu et al., 2024; Duan et al., 2024) or gridbased(Zhang et al., 2024; Anonymous, 2025; Paglieri et al., 2024) games. These settings require limited visual reasoning and can be fully represented in text, restricting their ability to assess spatial, temporal, and dynamic complexities crucial for real-world problem-solving. As result, they fail to yield compelling insights into MLLMs visual reasoning limitations or guide meaningful model improvements. Leveraging the rich visual data in video games offers valu1 Technical Report Figure 2: The overview of the V-MAGE benchmark, designed to evaluate visual-centric capabilities and higher-level reasoning of MLLMs across 5 free-form games with 30+ levels. V-MAGE assesses critical abilities in visual reasoning, providing comprehensive evaluation of model performance in complex, dynamic environments. able opportunity to assess true visual reasoning. There is pressing need for game-based evaluation environments that better align with how MLLMs process visual information, enabling improvements in end-to-end gameplay tasks and strengthening visual interactive capabilities. To address the lack of visually-centric game benchmarks, we present Visual-centric Multiple Abilities Game Evaluation (V-MAGE), game-based benchmark designed to evaluate visual-centric capabilities through flexible gameplay and carefully designed levels. This framework allows for thorough assessment of diverse model and agent abilities within dynamic, interactive game environments. V-MAGE addresses key limitations in current game-based evaluations of MLLM capabilities. Its defining features are as follows: Visual-Centric: Models receive only visual input, requiring pixel-level scene understanding, object tracking, and spatial-temporal reasoning. Flexible Gameplay: Unlike grid-based benchmarks, VMAGE features continuous-space environments, allowing models to explore almost infinite state space with no single correct solution. Granular Skill Assessment: Each game are deigned with different difficulty levels that targeting various skill dimensions (e.g., obstacles, acceleration, first-person vs. map-view challenges in Fig. 4). Extensible Evaluation Framework: V-MAGE extends beyond model evaluation to assess agentic skills that is current out-of-scope for SOTA MLLMs. Our gameagent-model three-module evaluation pipeline allows optimizations in both MLLMs and their agent strategies. Adaptive ELO-based Ranking: V-MAGE uses dynamic Elo system for performance comparison, avoiding manual score normalization and performance ceilings. With V-MAGE, we evaluate six state-of-the-art MLLMs across five interactive games across 30+ levels. Results highlight significant challenges posed by the dynamic visual interaction environment of V-MAGE for MLLMs. For example, while top models like InternVL2.5-78B (99.99/100) and Gemini-2.0-flash-exp (97.95/100) achieve near-human performance (100/100) on simple tasks like Race Level 1, they exhibit catastrophic performance degradation when facing compound challenges. GPT-4o scores only 1.93/10 vs. human score of 10/10 in FlappyBird Level 6, and Qwen2VL-72B achieves merely 0.61/10 in the same task. These results reveal that current MLLMs, despite excelling in static benchmarks, lack the perception, multi-step reasoning, and task orchestration required for human-level gameplay in dynamic settings. By introducing V-MAGE, we establish more realistic, interactive, and visually rich evaluation framework for MLLMs. This benchmark aims to drive future advancements in multimodal intelligence, guiding the development of models capable of robust visual reasoning and decisionmaking in complex environments. 2. Related work MLLMs and Multimodal Agents. As LLMs (OpenAI, 2023b; Gao et al., 2023; Touvron et al., 2023; Bai et al., 2023; Cai et al., 2024) advance, MLLMs have emerged to handle multimodal tasks by integrating text and visual inputs (Chen et al., 2024c; Wang et al., 2024; Liu et al., 2023a; Li et al., 2024a). Open-source models like InternVL and QwenVL are narrowing the gap (Chen et al., 2024b) with closed-source models such as GPT-4o (OpenAI, 2024), GPT-4V (OpenAI, 2023a), and Gemini (Team, 2023). MLLMs are now evolving into multimodal agents that acTechnical Report tively interact with the world. Robotic agents use visionlanguage-action models for real-time perception and manipulation (Driess et al., 2023), while virtual agents process multimodal inputs for tasks like customer support (Brohan et al., 2023; 2022). GUI agents automate interface navigation (Xu et al., 2024; Bonatti et al., 2024; Zhang et al., 2023), and game agents adapt to dynamic environments (Tan et al., 2024; Chen et al., 2024a) or generate NPC behavior (Rao et al., 2024; Park et al., 2023). These applications require sequential reasoning, memory, and adaptability capabilities that static benchmarks fail to capture. Traditional MLLM Benchmarks. Traditional MLLM benchmarks have focused on tasks like Visual Question Answering (VQA) (Antol et al., 2015; Goyal et al., 2017; Li et al., 2018; Marino et al., 2019) and image captioning (Chen et al., 2015; Agrawal et al., 2019; Sidorov et al., 2020). More comprehensive benchmarks, such as MME (Fu et al., 2023), MMBench (Liu et al., 2023b), SEED-Bench (Li et al., 2023), MMMU (Yue et al., 2024), and MM-Vet (Yu et al., 2023; 2024), introduce broader assessments across multiple domains. However, most of these evaluations rely on structured multiple-choice and VQA-style tasks, limiting their ability to measure real-world problem-solving and interactive reasoning. Evaluating MLLMs in Games. Recent work (Tan et al., 2024; Chen et al., 2024a) has explored MLLMs in interactive gaming environments. Meanwhile, game-based evaluation has evolved from text-only benchmarks (Costarelli et al., 2024; Hu et al., 2024; Duan et al., 2024) to vision-integrated tests (Zhang et al., 2024; Anonymous, 2025; Paglieri et al., 2024). However, most existing benchmarks rely on gridbased games (e.g., Tic-Tac-Toe, Chess) (Zhang et al., 2024; Anonymous, 2025; Paglieri et al., 2024), which can be fully represented in text. These evaluations primarily test gamestate recognition, and in some cases, additional visual input even confuses models, reducing performance (Paglieri et al., 2024). As result, these benchmarks provide limited insights into MLLMs visual perception and reasoning capabilities and offer little guidance for improving them. 3. V-MAGE Benchmark We present V-MAGE, benchmark built on video game environments designed to evaluate the comprehensive performance of MLLMs, with focus on visual-centric capabilities. V-MAGE consists of five distinct video game environments, each containing manually crafted levels with varying difficulties to holistically assess the visual perception and reasoning abilities of MLLM. The evaluation employs dynamic Elo-based framework with statistical stabilization, iteratively refining models relative capabilities through randomized pairwise comparisons across multi-round interactions. Below, we explain our benchmark construction and evaluation pipeline in detail. 3.1. Games and Levels Game Selection. Humans play games by perceiving visuals, making decisions, and interacting dynamically, yet most MLLM benchmarks fail to capture this process. Instead, they rely on grid-based environments (e.g., Tic-Tac-Toe, Minesweeper, Sokoban), where models bypass visual reasoning by converting game states into text. To address this limitation, V-MAGE shifts the evaluation paradigm by embedding models in dynamic visual environments that require real-time perception and action. V-MAGE features five human-playable video games  (Fig.1)  , each with 310 levels, totaling 30+ evaluation environments. Unlike grid-based setups, V-MAGE selects games with the following principles: The game state information can be conveyed through visual input, which aligns with real-world perception challenges. The game should feature free-form or continuous-space environments, moving beyond discrete grids or cells. This allows for more nuanced and flexible movement and interaction. The game can be customized with different levels, allowing evaluations of different model capabilities within the same game environment. Although the current benchmark centers on five selected games, it offers flexible framework for incorporating future games while preserving evaluation integrity. This positions V-MAGE as both rigorous diagnostic tool for current capabilities and an expandable testbed for emerging challenges in human-aligned game intelligence. Level Design. Conventional game-based benchmarks struggle to assess MLLMs effectively, as most models fail catastrophically on standard human-level difficulties (Zhang et al., 2024; Anonymous, 2025). This hinders systematic weakness analysis and meaningful inter-model comparisons. To address this, V-MAGE introduces multi-level assessment framework that evaluates models across skill dimensions and provides granular performance diagnostics through difficulty-stratified tasks. Specifically, we design the levels of each game by gradually increasing complexity through variations in control paradigms (e.g., map-view vs. first-person) and perceptual challenges (e.g., partial observability, dynamic obstacles). This mirrors human learning curves, systematically exposing model weaknesses. This level design is also extensible, allowing new levels to be introduced with model advancements while maintaining consistent evaluation metrics. For instance, in RaceGame  (Fig. 4)  , six levels progressively Technical Report Figure 3: V-MAGE games and evaluation pipeline. During the evaluation process, the Agent module receives visual game state information from the Game module, specifically in the form of game screenshots. Within the Agent module, these screenshots are structured into inputs for the model. In the baseline agent of V-MAGE, inputs to the MLLM models are constructed by combining screenshots from the most recent three frames with prompt containing the game rules. The output of the model is processed by the Agent module into response actions, which are subsequently sent back to the Game module. 3.2. Evaluated Capabilities Games, as simulations of the real world, require the integrated application of multiple skills during interactive gameplay. This makes game-based evaluation powerful tool for assessing models diverse capabilities and guiding its development. As shown in Fig. 2, we categorize the capabilities assessed by V-MAGE into two primary areas: 3.2.1. VISUAL-CENTRIC CAPABILITIES These capabilities reflect models ability to extract, interpret, and apply abstract information from visual data. Positioning: Understanding spatial relationships between objects. e.g., RaceGame involves identifying the finish line (trophy) relative to the cars position. Tracking: Predicting the trajectory and speed of moving objects. e.g., the model must analyze multiple frames to track the balls direction and speed, adjusting paddle movement accordingly in PongGame. Timing: Executing actions with precise timing. For example, in Mario, poorly timed jump can prevent the character from reaching the next platform. Visual Memory: Inferring abstract visual concepts beyond explicit cues. In FlappyBird, the model must estimate how the jumps height and horizontal movement affect future actions. Others: The model may also need to recognize objects or in-game text when required. Figure 4: RaceGame level design. increase in difficulty while sharing the core objective: navigating car to trophy. Levels 13 use map-view perspective, where models adjust absolute coordinatesanalogous to static visual grounding, yielding higher success rates. Levels 46 shift to first-person perspective, requiring real-time interpretation of velocity vectors and acceleration constraints. The introduction of maze-like obstacles further demands multi-step strategic planning under partial observability. For detailed information on the level design of all games, please refer to Appendix B.2. Empirical results in Appendix. will show model performance drop sharply in advanced levels, highlighting critical gaps in dynamic visual reasoning among current MLLMs. 4 Technical Report 3.2.2. HIGHER-LEVEL REASONING Due to the complexity of game tasks, advanced reasoning skills are often required for effective decision-making. Long-Term Reasoning: Games frequently involve extended decision-making processes. In Mario, if the model encounters dead end, it must recall previous choices and attempt an alternative path. Deliberation: Some tasks require breaking down complex objectives into sequential steps. In the 3D game Tempest Run, the model must analyze its current state, plan an optimal path, and execute actions in order. Adaptive Learning: As the game progresses, the model should evaluate success or failure based on visual feedback, refine its decision-making process, and use past experiences to improve performance on next attempts. 3.3. Evaluation Pipeline V-MAGE separates the game environment from the MLLM, ensuring that all information is conveyed solely through visual input. The MLLM plays like humanobserving the game on screen and responding in real time based on its understanding of the visual state. As illustrated in Fig. 3, V-MAGE operates in action cycles, with each cycle consisting of three modules: Game Module Runs the game, captures screenshot of the current state, and passes it to the Agent module. Agent Module Processes visual input, integrates past frames, reasoning history, and text prompts (e.g., game rules), and constructs the MLLMs input. Model Output & Execution The MLLM generates an action, which the Agent module processes and sends back to the Game module for validation and execution. We design V-MAGE to evaluate MLLM capabilities, thus we intentionally keep the evaluation framework minimalistic to reduce bias. However, the framework remains fully extensible researchers interested in evaluating agents can explore different agent strategies, as V-MAGE does not impose restrictions on agent design. This flexibility allows for broader research applications, while maintaining consistent and fair evaluation of MLLM performance. 3.4. Evaluation Metrics V-MAGE employs robust implementation of the Elo rating system to evaluate MLLMs, building on recent advancements in LLM benchmarking (Duan et al., 2024; Jiang et al., 2024). The system facilitates reliable model rankings by leveraging competitive evaluation mechanisms, ensuring robustness through dynamically balanced interactions. The core methodology for metrics evaluation in V-MAGE comprises two interconnected components: performance comparison and statistical stabilization. Performance Comparison Protocol. Each model begins with an initial Elo rating R(0) = 1500, where represents the set of candidate models. We conducted 100 evaluation rounds for each game level ℓ where models were randomly paired in each round through shuffle operation: Pt = {(At, Bt) At, Bt rand Mℓ, At = Bt} (1) where At and Bt denote the paired models in round t. For paired models (A, B), their game scores scoreA and scoreB are compared next. We first construct (m) = (scorem, valid ratem) (2) where valid ratem represents the proportion of actions output by model in this game round that conform to the formatting requirements. The outcome (SA, SB) {(1, 0), (0, 1), (0.5, 0.5)} is determined by: (1, 0) (0, 1) (0.5, 0.5) otherwise if (A) (B) if (A) (B) (SA, SB) = (3) The rating update mechanism follows the classical Elo formulation with enhanced stability controls. For models and with pre-match ratings RA and RB, their expected performance is calculated as: EA = 1 + 10(RB RA)/400 , EB = 1 1 1 + 10(RARB )/400 (4) where the denominator base 10 and scaling factor 400 establish logarithmic relationship between rating differences and expected outcomes. The post-match ratings become: R = RA + K(SA EA) = RB + K(SB EB) (5) where is constant determining the sensitivity of the rating system (typically set to 32), Stabilization through Randomized Iteration. To eliminate path dependence in rating updates, V-MAGE implements multi-pass stabilization protocol. All historical comparisons are aggregated into win/loss pool: (cid:91) (cid:91) (cid:91) = (Agℓt, Bgℓt, Sgℓt , Sgℓt ) (6) gG ℓLg t= which undergoes = 104 independent shuffles. For each permutation πi(W), complete rating recalculation yields R(i) . The final stabilized rating combines these trials: Rm = 1 (cid:88) i= R(i) (7) 5 Table 1: The comparison of V-MAGE with existing game-based evaluation benchmarks. *Text in V-MAGE only represents the instructions for game rules and output format. Technical Report GameBench(Costarelli et al., 2024) GameArena(Hu et al., 2024) GTBench(Duan et al., 2024) ING-VP(Zhang et al., 2024) LVLM-Playground(Anonymous, 2025) BALROG(Paglieri et al., 2024) V-MAGE Game word game word game word game grid based grid based word game / grid based free form Input text-only text-only text-only image-text image-text image-text image-text* Reasoning Type text reasoning text reasoning text reasoning text reasoning + visual aid text reasoning + visual aid text reasoning + visual aid visual-centric reasoning Level Design Table 2: Performance comparison across different games based on the elo ranking system. Game Closed-Source GPT-4o Gemini 2.0-flash-exp Race Flappybird Pong Tempestrun SuperMario (Avg) Average 1611 1589 1500 1514 1553 1526 1530 1515 1513 1551 1533 Qwen2VL-7B Qwen2VL-72B InternVL2.5-8B InternVL2.5-78B Random Open-Source Baseline 1447 1452 1481 1505 1398 1440 1517 1464 1501 1535 1523 1472 1497 1504 1483 1411 1455 1474 1503 1511 1513 1587 1538 1453 1464 1488 1438 1458 3.5. Comparison to Existing Game Benchmarks. Existing MLLM game benchmarks rely heavily on grid-based games (e.g., Tic-Tac-Toe, Minesweeper, Sokoban)(Anonymous, 2025; Zhang et al., 2024), where models primarily read game states rather than engage in visual reasoning(Paglieri et al., 2024). Since these environments can be fully represented in text, MLLMs often bypass perception by converting visual inputs into textual representations before reasoning. This fails to capture real-time decision-making, and sequential reasoning, making it an inadequate measure of true multimodal capabilities. V-MAGE addresses these shortcomings by evaluating models in continuous, fully visual environments where game states cannot be easily converted into text. Levels in VMAGE require temporal reasoning and real-time decisionmaking, ensuring that models engage directly with visual input. To enhance evaluation, V-MAGE integrates an agent module with customizable interaction strategies, allowing models to handle long visual contexts and incorporate guidance. This dual-layer approach assesses both core visual reasoning and how perception translates into action. Unlike traditional benchmarks that require manual score normalization (Anonymous, 2025; Zhang et al., 2024), V-MAGE employs dynamic Elo-based ranking system that adapts as models improve, preventing artificial performance ceilings. By shifting evaluation to more natural and complex game environments, V-MAGE provides more rigorous and representative test of MLLM capabilities. holistic comparison between V-MAGE and existing game benchmarks are presented in Table 1. 4. Experiments 4.1. Baseline As the baseline for the V-MAGE benchmark, we evaluate state-of-the-art MLLMs using minimal Agent strategy to ensure fair comparison. The baseline utilizes only the most recent frames (k=3) for reasoning, integrating them with reasoning history, action decisions, and game rules as input for the Model module  (Fig. 3)  . Detailed prompts can be found in Appendix Sec.B.2. As noted in Sec. 3.3, while V-MAGE allows customizable Agent strategies, the baseline remains standardized for consistency. Any modifications to Agent strategies will be explicitly stated on our leaderboard to ensure transparency in model comparisons. 4.2. Main Result Scores and Rankings. Overall, model performance improves as parameter size increases. As shown in Table 2, larger open-source MLLMs achieve results comparable to closed-source models, demonstrating the progress of open research in multimodal reasoning. GPT-4o exhibits clear advantage in RaceGame and FlappyBird, while Gemini2.0-flash-exp, Qwen2VL-72B, and InternVL2.5-78B each take the lead in the other three games. In terms of the final average score, GPT-4o currently ranks first, but the lead is not significant. In addition, we analyzed the performance trends across model scales. The result highlights that while parameter scaling improves performance in simpler tasks, it cannot bridge the inherent ability gap in more complex dynamic tasks. For more details, please refer to the Appendix A.2. Significant Performance Gap Between MLLMs and HuTechnical Report Figure 5: The MLLM trails humans by large margin in all six games. The levels with an asterisk (*) represent no history, more details about these levels can be found in Appendix Fig. 9. mans in Complex Scenes. We invited 5 human participants to play the game in the same environment as the MLLMs and calculated their average score, which served as the baseline for human performance. Fig. 5 compares the performance of Elo-winning MLLMs and human players across different game levels. While MLLMs approach human-level performance in low-complexity tasks, their performance declines significantly as task complexity increases. For example, in RaceGame Level 1 (without history frames), InternVL2.5-78B scores 99.99 points, nearly matching the human score of 100. However, in Level 4, which requires temporal reasoning, the best model (GPT-4o) drops to 23.47 points, while human performance remains at 100. This disparity is even more pronounced in Pong. In Level 2, humans average 10 points, while the top MLLM (Gemini-2.0-flashexp) scores just 0.35, with 28.6-point performance gap. Humans excel due to adaptive strategies and rapid decisionmaking, whereas models struggle to learn effective tactics, often behaving randomly. These results highlight critical limitation: MLLMs lack the reasoning and planning abilities needed to navigate dynamic game environments effectively. Multi-Frame Sequential Reasoning: An Ongoing Challenge for Existing Models. To assess the impact of multiframe input on reasoning, we designed three RaceGame levels that do not require historical frames (see Appendix B.2, Fig. 9). While multi-frame input is expected to enhance decision-making by providing more context, we find that GPT-4o is the only model that significantly benefits from multi-frame input(see Appendix A.2 ). Other models struggle to utilize this information effectivelyoften leading to performance degradation rather than improvement. 4.3. Analysis V-MAGE Poses Significant Challenges to MLLMs. The primary challenge lies in the limited visual abstraction capabilities of the model. V-MAGE emphasizes visual-centric reasoning, yet models frequently fail on tasks that are trivial for humans. Additionally, another major issue is the difficulty models face in understanding sequential game states from image sequences. On one hand, high frame count inputs are impractical due to context length constraints. On the other hand, even humans find interpreting discrete image sequences nontrivial. Current MLLMs likely lack sufficient training on sequential visual data, which contributes to their suboptimal performance. Furthermore, MLLMs struggle to maintain continuity in reasoning during interrupted dialogues. Hallucinations and inconsistencies often emerge across multiple reasoning steps, preventing the models from sustaining coherent logic. This limitation affects their ability to perform smooth, humanlike actions over consistent observation scales. Analysis of Model Errors in V-MAGE. V-MAGE presents unique challenge as there are no definitive ground truth 7 Technical Report Figure 6: Error analysis in RaceGame and FlappyBird. actions during gameplay. To better understand model performance, we manually analyzed sample of model inference records, focusing on reasoning patterns and error sources. Our findings indicate that visual perception deficiencies are the primary cause of errors, which resonates with V-MAGEs vision-centric focus. As shown in Fig. 6, perception-related errors fall into two key categories: Failure to extract key information: Some models miss critical visual cues needed for task completion. For example, in higher-level RaceGame stages, models sometimes struggle to discern the orientation of the car from the visual state, leading to actions that diverging from human judgment. Erroneous perception of target information: While some models can identify the key target from the visual input, they make perception errors. In FlappyBird, models frequently misjudge the position of the bird relative to the pipe gap, resulting in unreasonable actions. Directions for Improvement. The evaluation results reveal two critical pathways for advancing MLLM capabilities in dynamic visual environments: Enhancing Visual Perception and Reasoning Capabilities: Current MLLMs struggle with multi-image sequence reasoning and fast, accurate visual perception, particularly in real-world scenarios. While large-scale models have improved basic perception, human-level reasoning speed and stability remain challenge. Smaller models still underperform on fundamental tasks, limiting their practical deployment. Therefore, the primary focus for improvement should be on the foundational capabilities of the model for multi-image and video understanding and reasoning, and new evaluation metrics, such as reasoning speed and stability. Designing Advanced Agent Strategies. Multimodal agents are evolving rapidly, handling tasks like real-time tool use, software operation, and gaming. key challenge is optimizing agent strategies to better support MLLM reasoning and memory. As seen in V-MAGE, long-term reasoning may require agents to manage inference history and memory strategically, improving contextual input. Additionally, visual memory in gaming environments differs from text-based memory, necessitating research into intuitive visual memory storage and retrieval. While model improvements are crucial, advancements in agent strategies may yield even greater performance gains, shaping the next generation of multimodal intelligence. 5. Conclusion This paper introduces V-MAGE, game-based evaluation framework designed to assess the visual capabilities of 8 Technical Report MLLMs. As the first benchmark to focus specifically on visual-centric game evaluation, V-MAGE incorporates assessments across 30+ levels spanning five games, providing comprehensive and objective measure of MLLM performance. V-MAGE employs an Elo-based scoring system, benchmarking open-source models and commercial APIs against human performance in the same environment. Our evaluation reveals significant limitations in current MLLMs visual perception and reasoning. Based on these findings, we outline future research directions, including improvements in model architecture and agent strategies. V-MAGE enables detailed assessment of visual perception, reasoning, and agent effectiveness, serving as foundation for advancing multimodal intelligence. We plan to expand VMAGE with additional games and designs, further exploring the diverse capabilities of MLLMs and their agents."
        },
        {
            "title": "Impact Statement",
            "content": "This work introduces V-MAGE, an evaluation framework centered on assessing the visual capabilities of multimodal large models, filling the current gap in visual-centric benchmarks. Through V-MAGE, we evaluated the performance of various open-source models and commercial APIs. The results demonstrate the limitations of current multimodal large models in core visual capabilities and potential directions for future improvement. This work has certain social impacts. On the one hand, multimodal large models inspired and improved by V-MAGE may have stronger visual perception capabilities and achieve further performance in gaming tasks, potentially enabling them to complete more real-world tasks. However, we also recognize ethical considerations. Because V-MAGE may not comprehensively represent the rich video game environment, evaluations of model capabilities in this field could be biased, potentially leading to ranking errors. Our work evaluates the performance of current multimodal large models in video games. Through further game introduction and level design, our benchmark will provide more comprehensive and objective evaluation of model capabilities."
        },
        {
            "title": "References",
            "content": "Agrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D., Parikh, D., Lee, S., and Anderson, P. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 89488957, 2019. Anonymous. Are large vision language models good game In The Thirteenth International Conference players? on Learning Representations, 2025. URL https:// openreview.net/forum?id=c4OGMNyzPT. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. Vqa: Visual question answering. In ICCV, 2015. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: frontier large visionlanguage model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. Bonatti, R., Zhao, D., Bonacci, F., Dupont, D., Abdali, S., Li, Y., Lu, Y., Wagle, J., Koishida, K., Bucker, A., Jang, L., and Hui, Z. Windows agent arena: Evaluating multi-modal os agents at scale, 2024. URL https: //arxiv.org/abs/2409.08264. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jackson, T., Jesmonth, S., Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Leal, I., Lee, K.-H., Levine, S., Lu, Y., Malla, U., Manjunath, D., Mordatch, I., Nachum, O., Parada, C., Peralta, J., Perez, E., Pertsch, K., Quiambao, J., Rao, K., Ryoo, M., Salazar, G., Sanketi, P., Sayed, K., Singh, J., Sontakke, S., Stone, A., Tan, C., Tran, H., Vanhoucke, V., Vega, S., Vuong, Q., Xia, F., Xiao, T., Xu, P., Xu, S., Yu, T., and Zitkovich, B. Rt-1: Robotics transformer for real-world control at scale. In arXiv preprint arXiv:2212.06817, 2022. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., Florence, P., Fu, C., Arenas, M. G., Gopalakrishnan, K., Han, K., Hausman, K., Herzog, A., Hsu, J., Ichter, B., Irpan, A., Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Leal, I., Lee, L., Lee, T.-W. E., Levine, S., Lu, Y., Michalewski, H., Mordatch, I., Pertsch, K., Rao, K., Reymann, K., Ryoo, M., Salazar, G., Sanketi, P., Sermanet, P., Singh, J., Singh, A., Soricut, R., Tran, H., Vanhoucke, V., Vuong, Q., Wahid, A., Welker, S., Wohlhart, P., Wu, J., Xia, F., Xiao, T., Xu, P., Xu, S., Yu, T., and Zitkovich, B. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In arXiv preprint arXiv:2307.15818, 2023. Cai, Z., Cao, M., Chen, H., Chen, K., Chen, K., Chen, X., Chen, X., Chen, Z., Chen, Z., Chu, P., Dong, X., Duan, H., Fan, Q., Fei, Z., Gao, Y., Ge, J., Gu, C., Gu, Y., Gui, T., Guo, A., Guo, Q., He, C., Hu, Y., Huang, T., Jiang, T., Jiao, P., Jin, Z., Lei, Z., Li, J., Li, J., Li, L., Li, S., Li, W., Li, Y., Liu, H., Liu, J., Hong, J., Liu, K., Liu, K., Liu, X., Lv, C., Lv, H., Lv, K., Ma, L., Ma, R., Ma, Z., Ning, W., Ouyang, L., Qiu, J., Qu, Y., Shang, F., Shao, Y., Song, D., Song, Z., Sui, Z., Sun, P., Sun, Y., Tang, H., Wang, B., Wang, G., Wang, J., Wang, J., Wang, R., Wang, Y., Wang, 9 Technical Report Z., Wei, X., Weng, Q., Wu, F., Xiong, Y., Xu, C., Xu, R., Yan, H., Yan, Y., Yang, X., Ye, H., Ying, H., Yu, J., Yu, J., Zang, Y., Zhang, C., Zhang, L., Zhang, P., Zhang, P., Zhang, R., Zhang, S., Zhang, S., Zhang, W., Zhang, W., Zhang, X., Zhang, X., Zhao, H., Zhao, Q., Zhao, X., Zhou, F., Zhou, Z., Zhuo, J., Zou, Y., Qiu, X., Qiao, Y., and Lin, D. Internlm2 technical report, 2024. Chen, P., Bu, P., Song, J., Gao, Y., and Zheng, B. Can vlms play action role-playing games? take black myth wukong as study case, 2024a. URL https://arxiv.org/ abs/2409.12889. Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollar, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024b. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024c. Costarelli, A., Allen, M., Hauksson, R., Sodunke, G., Hariharan, S., Cheng, C., Li, W., Clymer, J., and Yadav, A. Gamebench: Evaluating strategic reasoning abilities of llm agents, 2024. URL https://arxiv.org/abs/ 2406.06613. Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: An embodied multimodal language model. In arXiv preprint arXiv:2303.03378, 2023. Duan, J., Zhang, R., Diffenderfer, J., Kailkhura, B., Sun, L., Stengel-Eskin, E., Bansal, M., Chen, T., and Xu, K. Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations, 2024. URL https://arxiv.org/abs/2402.12348. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu, Z., Lin, W., Yang, J., Zheng, X., et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. Hu, L., Li, Q., Xie, A., Jiang, N., Stoica, I., Jin, H., and Zhang, H. Gamearena: Evaluating llm reasoning through live computer games, 2024. URL https://arxiv. org/abs/2412.06394. Jiang, D., Ku, M., Li, T., Ni, Y., Sun, S., Fan, R., and Chen, W. Genai arena: An open evaluation platform for generative models, 2024. URL https://arxiv. org/abs/2406.04485. Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., and Li, C. Llavaonevision: Easy visual task transfer, 2024a. URL https: //arxiv.org/abs/2408.03326. Li, C., Gan, Z., Yang, Z., Yang, J., Li, L., Wang, L., Gao, J., et al. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1214, 2024b. Li, Q., Tao, Q., Joty, S., Cai, J., and Luo, J. Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 552567, 2018. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023a. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023b. Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Okvqa: visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. OpenAI. Gpt-4v(ision) system card. 2023a. URL https://cdn.openai.com/papers/GPTV_ System_Card.pdf. OpenAI. Gpt-4 technical report, 2023b. 10 Technical Report Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and Wang, L. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023. URL https: //arxiv.org/abs/2308.02490. Yu, W., Yang, Z., Ren, L., Li, L., Wang, J., Lin, K., Lin, C.-C., Liu, Z., Wang, L., and Wang, X. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. arXiv preprint arXiv:2408.00765, 2024. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Zhang, C., Yang, Z., Liu, J., Han, Y., Chen, X., Huang, Z., Fu, B., and Yu, G. Appagent: Multimodal agents as smartphone users, 2023. URL https://arxiv. org/abs/2312.13771. Zhang, H., Guo, H., Guo, S., Cao, M., Huang, W., Liu, J., and Zhang, G. Ing-vp: Mllms cannot play easy visionbased games yet, 2024. URL https://arxiv.org/ abs/2410.06555. OpenAI. Hello gpt-4o. OpenAI Blog, 2024. URL https: //openai.com/index/hello-gpt-4o/. Paglieri, D., Cupiał, B., Coward, S., Piterbarg, U., Wolczyk, M., Khan, A., Pignatelli, E., Łukasz Kucinski, Pinto, L., Fergus, R., Foerster, J. N., Parker-Holder, J., and Rocktaschel, T. Balrog: Benchmarking agentic llm and vlm reasoning on games, 2024. URL https://arxiv. org/abs/2411.13543. Park, J. S., OBrien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST 23), UIST 23, New York, NY, USA, 2023. Association for Computing Machinery. Rao, S., Xu, W., Xu, M., Leandro, J., Lobb, K., DesGarennes, G., Brockett, C., and Dolan, B. Collaborative quest completion with llm-driven non-player characters in minecraft. arXiv preprint arXiv:2407.03460, 2024. Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps: dataset for image captioning with reading comprehension. In ECCV, pp. 742758, 2020. Tan, W., Zhang, W., Xu, X., Xia, H., Ding, Z., Li, B., Zhou, B., Yue, J., Jiang, J., Li, Y., An, R., Qin, M., Zong, C., Zheng, L., Wu, Y., Chai, X., Bi, Y., Xie, T., Gu, P., Li, X., Zhang, C., Tian, L., Wang, C., Wang, X., Karlsson, B. F., An, B., Yan, S., and Lu, Z. Cradle: Empowering foundation agents towards general computer control. arXiv preprint arXiv:2403.03186, 2024. Team, G. G. Gemini: family of highly capable multimodal models, 2023. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and Lin, J. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. URL https://arxiv.org/abs/2409.12191. Xu, T., Chen, L., Wu, D.-J., Chen, Y., Zhang, Z., Yao, X., Xie, Z., Chen, Y., Liu, S., Qian, B., Torr, P., Ghanem, B., and Li, G. Crab: Cross-environment agent benchmark for multimodal language model agents, 2024. URL https: //arxiv.org/abs/2407.01511. 11 In the appendix, we provide the following contents: Sec A: Provides information on the models and their detailed performance. Technical Report Sec B: Presents specific details of the V-MAGE evaluation, including more information about the baseline agent and all the games and prompts used in V-MAGE. Sec C: Presents some case studies. A. Detailed Performance of Models A.1. Models The following models are involved in the V-MAGE evaluation: Table 3: Models involved in V-MAGE."
        },
        {
            "title": "Model",
            "content": "GPT-4o"
        },
        {
            "title": "Open Source",
            "content": "https://openai.com/index/hello-gpt-4o/ Gemini-2.0-flash-exp"
        },
        {
            "title": "Google DeepMind",
            "content": "https://deepmind.google/technologies/gemini/flash/ InternVL2.5-78B InternVL2.5-8B Qwen2VL-72B-Instruct Qwen2VL-7B-Instruct"
        },
        {
            "title": "Alibaba Cloud",
            "content": "https://huggingface.co/OpenGVLab/InternVL2_5-78B https://huggingface.co/OpenGVLab/InternVL2_5-8B https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct No"
        },
        {
            "title": "Yes",
            "content": "A.2. Detailed Statistics Performance Trends Across Model Scales. Cross-task result analysis reveals the limitations of parameter scaling: In RaceGame Level 1 (with historical frame input), Qwen2VL showed 391% improvement in score when scaling from 7B to 72B (from 11.22 to 55.1), reaching 55.1% of the human baseline score. However, in more complex tasks such as Tempestrun Level 4, InternVL2.5-78B (200.9 points) only improved by 14.5% compared to its 8B version (175.4 points), still achieving only 25.1% of the human score (800 points). This suggests that parameter scaling cannot compensate for the inherent ability gap in complex dynamic tasks. The detailed scores are presented below: Table 4: Performance analysis based on average scores in Race Level GPT-4o Level1 No History Level2 No History Level3 No History Level1 Level2 Level3 Level Level5 96.93 39.79 15.3 99.99 67. 48.97 23.47 17.35 Gemini Qwen2VL Qwen2VL InternVL2. InternVL2.5 2.0-flash-exp 7B 97.95 23.47 5. 35.71 8.16 6.12 7.14 9.18 86. 2.04 5.1 11.22 0.0 5.1 1. 14.28 72B 96.93 24.49 17.35 55. 2.04 8.16 3.06 5.1 8B 88. 6.12 12.24 28.57 4.08 7.14 0. 21.43 78B 99.99 26.53 23.47 64. 16.32 12.24 3.06 8.16 Random Human 2. 2.04 0.0 8.16 2.04 1.02 15. 6.12 100 100 100 100 100 100 100 12 Technical Report Table 5: Performance analysis based on average scores in Pong Gemini Qwen2VL Qwen2VL InternVL2.5 InternVL2.5 Level GPT-4o 2.0-flash-exp Level0 Level1 Level2 0.53 0. 0.21 0.56 0.45 0.35 7B 0. 0.33 0.2 72B 0.57 0.36 0. 8B 0.71 0.34 0.2 78B 0. 0.41 0.17 Level GPT-4o Level1 Level Level3 Level4 Level5 Level6 Level7 Level Level9 373.43 425.47 800.98 372.41 130. 235.7 443.85 96.93 691.77 Level10 20. Level GPT-4o Level1 Level2 Level3 Level Level5 Level6 Level7 3.33 0.74 0. 1.39 1.81 1.93 0.64 Level GPT-4o Level1 469.66 Level2 363.14 Level3 214. Level4 178.76 Table 6: Performance analysis based on average scores in Supermario Gemini Qwen2VL Qwen2VL InternVL2. InternVL2.5 2.0-flash-exp 7B 72B 8B 112. 132.64 430.57 255.08 261.2 325.48 525. 212.22 594.84 108.15 37.75 105.09 190. 115.29 234.67 326.5 164.27 56.12 170. 13.26 219.37 338.74 563.21 348.94 196. 295.89 488.73 182.63 509.13 29.59 72. 105.09 288.75 155.09 213.24 270.38 224. 79.58 207.12 16.32 78B 205.08 189. 609.12 448.93 432.61 345.88 489.75 170. 752.98 80.6 Table 7: Performance analysis based on average scores in Flappybird Gemini Qwen2VL Qwen2VL InternVL2. InternVL2.5 2.0-flash-exp 2.43 0.52 0.45 1. 1.46 1.09 0.09 7B 0.85 0. 0.22 1.59 1.52 1.69 0.04 72B 0.56 0.14 0.37 0.45 0.56 0. 0.16 8B 1.28 0.4 0.37 1. 1.28 1.2 0.0 78B 1.61 0. 0.46 0.7 0.52 0.71 0.15 Table 8: Performance analysis based on average scores in Tempestrun Gemini Qwen2VL Qwen2VL InternVL2.5 InternVL2.5 2.0-flash-exp 7B 72B 8B 482.92 359.36 199.37 203. 452.62 357.83 210.7 184.27 449.75 329. 199.47 175.4 525.98 374.46 222.23 196. 13 78B 479.66 337.73 217.23 200. Random Human 0.67 0.43 0.08 10 10 Random Human 89.79 97.95 57.14 56. 380.57 471.38 271.4 71.42 333.64 10. 800 800 1000 1400 800 800 900 1300 800 Random Human 1. 0.37 0.55 0.17 0.11 0.18 0. 10 10 10 10 10 10 Random Human 388.54 273.34 191.62 157. 2000 1500 1000 800 B. Specific Details of V-MAGE Evaluation B.1. V-MAGE Baseline Agent Technical Report In the baseline agent strategy, we utilize historical information comprising three preceding screenshots and the models reasoning outputs associated with these screenshots, combined with the current game screenshot, to form the <history> and <image current step> components (as shown in Sec. B.2) that are incorporated into the models input framework. The <history> component is structured as follows: This screenshot is <step> steps before the current step of the game. After this frame, your reasoning message was <history reasoning>. After the action was executed, the game info was <history action info>. Upon obtaining the models output, the system parses the structured response format specified in the prompt to systematically extract both the cognitive reasoning log and action parameters. These extracted elements are subsequently stored and fed back to the game module through standardized data pipeline. This process ensures consistent interpretation of the models decision-making trajectory while maintaining synchronization with the game state. Taking an input from the actual evaluation process in the Tempest Run as an example: Assume you are playing PC game called Tempest Run. You need to control character who moves through three-dimensional space inside futuristic tunnel filled with various obstacles and enemies. Your goal is to navigate through the tunnel, avoid or overcome obstacles, and run as far as possible. Avoid colliding with red spikes, purple walls, or failing to deal with green enemies. Use the optimal combination of movements to progress through the tunnel smoothly and efficiently. Monitor the characters position relative to obstacles and react appropriately to avoid losing progress. Now, will give you some history screenshots in the game for decision making. <image> This screenshot is three steps before the current step of the game. After this frame, your reasoning message was observation: The character is running forward in narrow tunnel with blue lines forming the walls and ceiling. Red spikes are visible on the ground ahead, and green enemies are further down the tunnel. The path appears clear with no immediate obstacles or enemies to avoid., reasoning: Maintaining the current trajectory with no action is optimal to continue moving straight towards the green enemy and prepare for potential slide to eliminate it., action: NONE. After the action was excuted, the game info was Game is running. ... <image> This screenshot represents the current step of the game. The last frame shows the current state of the game, while the previous frames show the characters previous movements. Important notes: 1. Use JUMP to jump over red spikes on the ground. 2. Use SLIDE to duck and kick green enemies to eliminate them. ... You should think step by step and respond with the following format, remember to respond with plain text without any special characters or symbols, DO NOT respond in markdown or Latex or any other format. Response: Observation: ... (Describe the characters current position and nearby obstacles or enemies.) Reasoning: ... (Think step by step and explain how you choose the action.) Action: ... (Choose ONE of the six actions to control the character. Do NOT add any other words.) B.2. Games and Prompts All the games have been modified based on publicly available code. The detailed design is provided below: 14 Technical Report Figure 7: RaceGame Level 1: Level Design and Prompt Overview. The images showcase the scene from Level 1, illustrating the level design and corresponding prompt. Elements in the same level will randomly change their initial positions while maintaining consistent relative difficulty. 15 Technical Report Figure 8: RaceGame Level 2-3: Level Design and Prompt Overview. The images showcase the scene from Level 2-3, illustrating the level design and corresponding prompt. Elements in the scene will randomly change their initial positions while maintaining consistent relative difficulty. 16 Technical Report Figure 9: RaceGame Level 1-3 No History: Level Design and Prompt Overview. The images showcase the scene from Level 1-3 No History, illustrating the level design and corresponding prompt. Elements in the scene will randomly change their initial positions while maintaining consistent relative difficulty. Same as the original levels except the input sequence has been changed to the single image. 17 Technical Report Figure 10: RaceGame Level 4: Level Design and Prompt Overview. The images showcase the scene from Level 4, illustrating the level design and corresponding prompt. Elements in the same level will randomly change their initial positions while maintaining consistent relative difficulty. 18 Technical Report Figure 11: RaceGame Level 5-6: Level Design and Prompt Overview. The images showcase the scene from Level 5-6, illustrating the level design and corresponding prompt. Elements in the same level will randomly change their initial positions while maintaining consistent relative difficulty. The prompt is the same as in Level 4. 19 Technical Report Figure 12: SuperMario Level 1: Level Design and Prompt Overview. The images showcase the scene from Level 1, illustrating the level design and corresponding prompt. 20 Technical Report Figure 13: SuperMario Level 2-4: Level Design and Prompt Overview. The images showcase the scene from Level 2-4, illustrating the level design and corresponding prompt.The prompt is the same as in Level 4. 21 Technical Report Figure 14: SuperMario Level 5-9: Level Design and Prompt Overview. The images showcase the scene from Level 5-9, illustrating the level design and corresponding prompt.The prompt is the same as in Level 4. 22 Technical Report Figure 15: SuperMario Level 10 (Standard Level): Level Design and Prompt Overview. The images showcase the scene from Level 10, illustrating the level design and corresponding prompt. This is The standard level that matches the difficulty of the human game. The prompt is the same as in Level 4. 23 Technical Report Figure 16: FlappyBird Level 1-3: Level Design and Prompt Overview. The images showcase the scene from Level 1, illustrating the level design and corresponding prompt. Levels are differentiated by the pipe gap width and the birds forward speed. Elements in the same level will randomly change their initial positions while maintaining consistent relative difficulty. 24 Technical Report Figure 17: FlappyBird Level 4-6: Level Design and Prompt Overview. The images showcase the scene from Level 4, illustrating the level design and corresponding prompt. Levels are differentiated by the pipe gap width and the birds forward speed. Elements in the same level will randomly change their initial positions while maintaining consistent relative difficulty. 25 Technical Report Figure 18: FlappyBird Level 7 (Standard Level): Level Design and Prompt Overview. The images showcase the scene from Level 7, illustrating the level design and corresponding prompt. Elements in the same level will randomly change their initial positions while maintaining consistent relative difficulty. This is The standard level that matches the difficulty of the human game. 26 Technical Report Figure 19: PongGame Level 1-3: Level Design and Prompt Overview. The images showcase the scene from Level 1, illustrating the level design and corresponding prompt. Levels are differentiated by the paddle width and the ping pong balls speed. The ping pong ball in the same level will randomly change its initial position while maintaining consistent relative difficulty. 27 Technical Report Figure 20: Tempest Run Level 1-4: Level Design and Prompt Overview. The images showcase the scene from Level 1, illustrating the level design and corresponding prompt. Levels are differentiated by the speed of barrier generation and the amount of visual information available. Elements in the same level will randomly change their initial positions while maintaining consistent relative difficulty. 28 C. Case Study Technical Report Figure 21: case studies group 1. 29 Technical Report Figure 22: case studies group 2."
        }
    ],
    "affiliations": [
        "Central South University, China",
        "Microsoft Corporation, USA",
        "Nanjing University, China"
    ]
}