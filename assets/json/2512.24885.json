{
    "paper_title": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts",
    "authors": [
        "Hengli Li",
        "Zhaoxin Yu",
        "Qi Shen",
        "Chenxi Li",
        "Mengmeng Wang",
        "Tinglang Wu",
        "Yipeng Kang",
        "Yuxuan Wang",
        "Song-Chun Zhu",
        "Zixia Jia",
        "Zilong Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue."
        },
        {
            "title": "Start",
            "content": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts Zhaoxin Yu Institute of Automation, CAS Qi Shen School of Artificial Intelligence, BUPT Hengli Li Institute for Artificial Intelligence, PKU NLCo, BIGAI 5 2 0 2 1 3 ] . [ 1 5 8 8 4 2 . 2 1 5 2 : r Chenxi Li Department of Automation, THU Yipeng Kang NLCo, BIGAI Mengmeng Wang NLCo, BIGAI Yuxuan Wang NLCo, BIGAI Tinglang Wu Yuanpei College, PKU Song-Chun Zhu NLCo, BIGAI Institute for Artificial Intelligence, PKU Department of Automation, THU Zixia Jia NLCo, BIGAI Zilong Zheng NLCo, BIGAI ABSTRACT Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignmentand by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional KeeperBurglar (adversarial), Mutual Friends (cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides simple, general mechanism for reliable strategic dialogue. KEYWORDS Strategic Reasoning, Dialogues, Dialogue Games, Belief Estimation, Theory of Mind"
        },
        {
            "title": "1 INTRODUCTION\nComplex dialogue settings such as negotiation [13, 24], debate [39],\ndeception for good [15], and common-ground alignment [33] are\nubiquitous for AI agents. In these scenarios, an agent must strate-\ngically perform dialogue acts to shape the interlocutorâ€™s beliefs to\nachieve its own objectives [9, 14, 25]. For example, in negotiation,\nthe agent should align and emphasize common preferences to\nincrease the chance its proposal is accepted, while simultaneously",
            "content": "Equal contribution. Any permutation of the three authors is acceptable to all authors. Corresponding author. Contact us at: lihengli@stu.pku.edu.cn, yuzhaoxin2024@ia.ac.cn, shenqi@bupt.edu.cn, s.c.zhu@pku.edu.cn, jiazixia@bigai.ai, zlzheng@bigai.ai adopting an adversarial stance concerning individual differences to protect its own interests. Executing such strategic acts depends on two essential components [9, 50]: (i) accurate belief estimation and (ii) effective use of those beliefs for utterance generation. While prior works [18, 33, 54] have demonstrated the benefits of modeling these subprocesses explicitly, they have only emphasized the former, leaving the latter underdeveloped. In particular, existing methods often relay all available belief information without principled criterion for what to reveal and how to reveal itan approach that can be suboptimal when belief states are complex. We therefore bridge belief estimation and utterance generation with mechanism that selectively and purposefully transforms beliefs into dialogue actions. We begin by giving precise definitions of Alignment and Adversarial dialogue acts within the game-theoretic framework of beliefs and common knowledge [2, 5, 6], thereby bridging belief estimation and dialogue behavior. Conceptually, an Adversarial Dialogue Act , introduces or emphasizes events that lie outside the opponents current belief support, strategically steering interaction toward some aggressive goals. By contrast, an Alignment Dialogue Act , restricts utterances to events entailed by shared common knowledge, promoting mutual understanding and coordination. In this view, belief estimation over events functions as constraints that define the permissible space of dialogue acts and operationalize their selection during generation. The proposed agent framework, BEDA (Belief Estimation for Dialogue Acts), shown in Figure 1, formalizes these conceptual definitions to support empirical experimentation and validate the underlying theoretical constructs. At high level, BEDA consists of three core components: World Set, Belief Estimator Module, and Conditional Generator. The World Set serves as structured representation of the environment, organizing events to provide foundation for belief formation. The Belief Estimator Module utilizes this structured information to infer the opponents perspective of the world, thereby modeling their beliefs. Finally, the Conditional Generator synthesizes the estimated belief states to generate utterances that align with the defined criteria for alignment and adversarial actions, Figure 1: An overview of the BEDA framework is provided using the Keeper-Burglar Game as an example. The world set, depicted at the top, represents structured understanding of the environment. At the beginning of each turn, the belief estimator processes the context and the world set to predict the beliefs. The beliefs are subsequently transmitted to the conditional generator, which incorporates selection phase to identify the most appropriate belief (i.e., the events that satisfy the Adversarial Dialogue Act ) for generating the keepers next utterance. ensuring communicative behaviors that adhere to the respective interactional strategies. We further evaluate the effectiveness of our approach on three distinct datasets: Conditional Keeper-Burglar Game (CKBG), corresponding to the Adversarial Dialogue Act ; Mutual Friends (MF), representing evaluating the Alignment Dialogue Act ; and CaSiNo [8], which presents more complex negotiation scenario. The experimental results demonstrate that our method delivers improvement across all evaluation settings, underscoring its robustness and generalizability. For example, in the case of CKBG, BEDA exhibits 20.6 improvement over the baseline method, when using GPT-4.1nano as backbone. In terms of MF, BEDA shows success rate improvement up to 30.4 compared to the vanilla baseline. Furthermore, we perform case analysis, demonstrating several benefits and characteristics of BEDA , such as mitigating useless information to enhance cooperation efficiency, and generating personalized misleading information in adversarial settings. Our contributions can be summarized as follows: We mathematically define the concepts of Adversarial Dialogue Act and Alignment Dialogue Act based on rigorous formulation of belief estimation and incorporate them into our framework, BEDA . This integration bridges the gap between belief estimation and dialogue act generation. For evaluation, we refine the KeeperBurglar dataset introduced by Hagendorff [15]. Our results underscore the importance of accurate belief estimation and show that the effectiveness of estimation functions as constraints on both Adversarial Dialogue Act and Alignment Dialogue Act behaviors. This integration of belief estimation in BEDA enhances the performance of our framework, yielding substantial improvements when compared to baseline models and other existing methods."
        },
        {
            "title": "2 BEDA\n2.1 Preliminaries: Logical Definition of Beliefs\nWe first introduce the concepts of information partition, event, and\nbelief [2, 5, 6], which serve as the foundational elements of our\nframework. These definitions follow Bonanno [5].",
            "content": "Definition 2.1 (Information Partition). Let ğ‘Š be finite set of states, an information partition is partition of ğ‘Š . Definition 2.2 (Event). Let ğ‘Š be finite set of states, subset of ğ‘Š is defined as an event. Definition 2.3 (Belief). Let ğ‘¥ ğ‘Š be state, at ğ‘¥ the agent knows event ğ¸ iff ğ¼ (ğ‘¥) is subset of ğ¸, i.e. ğ¼ (ğ‘¥) ğ¸, where ğ¼ (ğ‘¥) denotes the information subset ğ‘¥ belongs. Furthermore, the negation of an event is denoted as ğ¸ := ğ‘Š ğ¸. We also introduce the knowledge operator ğ¾ here. When given an event ğ¸, ğ¾ğ¸ outputs the set of state at which the agent knows ğ¸, i.e. ğ¾ğ¸ = {ğ‘¥ ğ¼ (ğ‘¥) ğ¸} Furthermore, randomness can also be introduced to the definition of belief: Definition 2.4 (Probabilistic Beliefs). Let ğ‘Š be set of states. Denote the probability of state ğ‘¥ ğ‘Š as ğ‘ƒ (ğ‘¥) and an event ğ¸, the probabilistic beliefs of an agent is ğ‘ƒ (ğ¸) := ğ‘ƒ (ğ‘¥). ğ‘¥ ğ¸"
        },
        {
            "title": "Constraints",
            "content": "We hypothesize that belief estimation functions as set of constraints in the process of dialogue act generation. In this subsection, we will introduce the belief constraints associated with two main dialogue acts: Adversarial Dialogue Act and Alignment Dialogue Act . The empirical verifications are provided in Section 4. Definition 2.5 (Adversarial Dialogue Act). Given two agents, namely, ğ´, ğµ, and their corresponding information partition set Fğ´, Fğµ, telling an event ğ¸ is ğœ–-Adversarial Dialogue Act from ğ´ to ğµ iff (cid:40)ğ‘ƒğ´ (ğ¸) 1 ğœ– ğ‘ƒğ´ (ğ¾ğµğ¸) 1 ğœ– where ğ‘ƒğ´ denotes the probabilistic belief of A, and ğ¾ğµ denotes the knowledge operator for B. , The concept of Adversarial Dialogue Act involves the intentional communication of information that is beyond the interlocutors understanding, aimed at achieving more assertive objectives. This definition encompasses two essential elements: first, the speaker must possess confidence in the information conveyed; second, the speaker must assume that the interlocutor lacks knowledge of the same information. This strategy is often employed to create deliberate misdirection. Definition 2.6 (Alignment Dialogue Act). Given two agents, namely, ğ´, ğµ, and their corresponding information partition set Fğ´, Fğµ, telling an event ğ¸ is ğœ–-Alignment Dialogue Act from ğ´ to ğµ iff (cid:40)ğ‘ƒğ´ (ğ¸) 1 ğœ– ğ‘ƒğ´ (ğ¾ğµğ¸) 1 ğœ– , where ğ‘ƒğ´ denotes the probabilistic belief of A, and ğ¾ğµ denotes ğµs knowledge operator. Unlike Adversarial Dialogue Act , the Alignment Dialogue Act tries to convey information that falls within shared common ground, with the goal of enhancing mutual trust and achieving alignment between interlocutors. Statistical Modeling of Dialogue Acts. Suppose ğ´ is the speaker and ğµ is the listener. Given the current context and task together as ğ¶, the world set as ğ‘Š , the power set of ğ‘Š as (ğ‘Š ), we denote the distribution of ğ´ conveying an event ğ¸ with utterance ğ‘¢ as ğœ‹ğ´ (ğ‘¢, ğ¸ ğ¶). By Bayes Rules, ğœ‹ğ´ (ğ‘¢, ğ¸ ğ¶) = ğœ‹ğ´ (ğ‘¢ ğ¸, ğ¶)ğœ‹ğ´ (ğ¸ ğ¶). ğœ‹ğ´ (ğ‘¢ ğ¸, ğ¶) is modeled by an LLM, given its strong language generation ability. Regarding ğœ‹ğ´ (ğ¸ ğ¶), to avoid injecting any additional human priors beyond the belief constraints, we adopt the maximum-entropy principle [19, 20, 55], assigning equal probability to all events that satisfy the constraints. Consequently, when the feasible set of ğ‘¢ coincides on both sides, we have arg max ğ‘¢ ğœ‹ğ´ (ğ‘¢, ğ¸ ğ¶) = arg max ğ‘¢ ğœ‹ğ´ (ğ‘¢ ğ¸, ğ¶). In summary, under either Adversarial Dialogue Act or Alignment Dialogue Act , generation reduces to the constrained optimization problem specified below. max ğ‘¢, ğ¸ (ğ‘Š ) s.t. ğœ‹ğ´ (ğ‘¢ ğ¸, ğ¶) ğ‘ƒğ´ (ğ¸ğ¶) 1 ğœ–, (cid:40)ğ‘ƒğ´ (ğ¾ğµğ¸ ğ¶) 1 ğœ– Adversarial Alignment ğ‘ƒğ´ (ğ¾ğµğ¸ ğ¶) 1 ğœ– , where, in practice, ğ‘ƒğ´ can be modeled with discriminator neural network."
        },
        {
            "title": "2.3 Components of the BEDA framework\nIn this subsection, weâ€™ll describe our framework, named BEDA ,\ndepicted mathematically in Section 2.2. As illustrated in Figure 1,\nBEDA is composed of three components: the world set, the belief\nestimator module, and the conditional generator.",
            "content": "World Set (ğ‘Š ). The world set represents finite collection of events. Each element of this set corresponds to an event that has been referenced in the dialogue or is relevant to the environment. Events within the world set can only be either true or false. Furthermore, not all events are shared as common knowledge between the two agents. Consequently, events specified in the set require additional discrimination and prediction. In this work, since we focus mainly on the whole paradigm, the world set is given, eliminating the need for dynamic construction, leaving only the attributes to be predicted. Belief Estimator (ğ‘ƒğ´). The belief estimator module comprises two belief estimators: one dedicated to determining the truthfulness of events i.e. [ğ‘ƒğ´ (ğ¸ğ¶), ğ¸ (ğ‘Š )] and the other functioning as predictor of the interlocutors beliefs i.e. [ğ‘ƒğ´ (ğ¾ğµğ¸ğ¶), ğ¸ (ğ‘Š )]. In this framework, each belief corresponds to an event from world set. The belief estimator module generates beliefs by analyzing the content of the dialogue and aligning it with the world set. However, it is not feasible for the module to store all possible settings. Consequently, robust generalization capability is essential for effective belief estimation. Therefore, we employ pre-trained encoder model, the BERT model [10], as the backbone. Given the input ğ¶, the model then outputs the vectors [ğ‘ƒğ´ (ğ¸ğ¶), ğ¸ (ğ‘Š )] and [ğ‘ƒğ´ (ğ¾ğµğ¸ğ¶), ğ¸ (ğ‘Š )]. Each entry is the models estimate of the truth probability of ğ¸ and, respectively, of ğ¾ğµğ¸. Conditional Generator (ğœ‹). The final component is the conditional generator, which synthesizes the context ğ¶, encompassing both the dialogue history and the task, alongside the event ğ¸. The event ğ¸ is selected according to the predictions of the belief estimator module and the chosen dialogue act. For instance, in the case of Adversarial Dialogue Act , only ğ¸ satisfying both ğ‘ƒğ´ (ğ¸) 1 ğœ– and ğ‘ƒğ´ (ğ¾ğµğ¸) 1 ğœ– are picked. The conditional generator then utilizes this integrated information to generate response ğ‘¢ to the interlocutor, represented as ğœ‹ (ğ‘¢ ğ¸, ğ¶). We hypothesize that incorporating enhanced beliefs into the generator significantly improves its performance and response quality."
        },
        {
            "title": "3 DATASETS\nWe conduct three experiments using three datasets: a competitive\nexperiment based on a refined dataset from the Keeper-Burglar\nGame [15], a cooperative experiment with mutual friends [16], and a\nmixed experiment utilizing the CaSiNo dataset [8]. The first experi-\nment tests the Adversarial Dialogue Act, while the second examines\nthe Alignment Dialogue Act. The third experiment provides a com-\nparative framework to analyze both fundamental dialogue acts. In\nall experiments, the configuration follows a dialogic interaction\nparadigm between two LLM agents. The proposed framework is im-\nplemented on one agent in the Keeper-Burglar Game [15], as shown\nin Figure 1, and applied to both agents in the other experiments.",
            "content": "Conditional Keeper-Burglar Game (CKBG). Building upon the Keeper-Burglar game [15], in which the agent, acting as the keeper, aims to deceive the burglar into selecting an incorrect container, we extend the original framework. This extension incorporates additional environmental conditions to increase the diversity of the game and to better simulate more realistic dialogue scenarios. We refer to this enhanced framework as the novel dataset Conditional Keeper-Burglar Game (CKBG). In this game, the world set comprises the environmental conditions, and the belief estimator predicts the specific conditions of which the burglar is aware. Moreover, during the selection process, the condition known to the Table 1: Statistical features of the CKBG dataset. (1) # Conditions: Number of types of different conditions allocated to the two agents. (2) Known Conditions: Number of conditions allocated to the two agents. (3) Avg. Words: Number of words in the training dialogue per setting. (4) Avg. Conditions: Number of types of different conditions allocated to the two agents per setting. (5) Vocabulary Size: Vocabulary size of training dialogues. # Settings # Conditions # Known Conditions Avg. Words Avg. Conditions Vocabulary Size Train 400 1017 1638 255.5 2.54 1490 Test 150 450 759 - 3.00 - keeper but unknown to the burglar will be forwarded to the conditional generation. This process aligns precisely with the framework established by the Adversarial Dialogue Act . An example of the designation is depicted in Figure 1 and Table 1 reports the statistical features of the CKBG dataset. Mutual Friends (MF). MF [16] is the task of identifying mutual friend between two agents. Its cooperative setting where the Alignment Dialogue Act is suitable. The world set is designed as various attributes possessed by the friend, and the belief estimator predicts the values of these attributes as believed by the other player. In the conditional generation phase, the estimation of the others belief will be used as condition to generate information within the common ground. CaSiNo. CaSiNo [8] is negotiation task in which two participants assume the roles of campsite neighbors negotiating the allocation of resources. In this context, the world set represents the preferences of both negotiators. To capture the alignment and adversarial nature of the negotiation, we introduce two distinct belief estimators. The first estimator predicts the preferences of the opponent, while the second estimates our own preferences from the opponents perspective. The selection mechanism operates as follows: in the Alignment Dialogue Act , an element that is known to both parties is selected, while in the Adversarial Dialogue Act , an element known exclusively to our side (and not to the opponent) is chosen. These two selected elements are subsequently combined for the conditional generation module."
        },
        {
            "title": "4.1 Settings",
            "content": "Metrics. We evaluate each task using the following metrics: CKBG: Success Rate, the proportion of episodes in which the keeper successfully misleads the burglar. MF: Success Rate (SR), the proportion of dialogues that correctly identify the mutual friend. To assess cooperation Table 2: Experimental results on Conditional Keeper-Burglar Game (CKBG) dataset. All experiments are performed three times and we report the mean. Backbones Methods w/o belief w/o belief + CoT w/o belief + Self Reflect rand belief BEDA (Ours) GPT-3.5 GPT-4.1-nano LLaMA2 (7B) LLaMA2 (13B) LLaMA3.1 (8B) Qwen2.5 (7B) Qwen2.5 (14B) 78.4 77.7 0.7 69.3 9.1 80.2 +1.8 86.9 +8.5 52.7 58.7 +6.0 59.3 +6.6 59.3 +6.6 73.3 +20.6 56.4 35.5 20.9 33.9 22.5 53.6 2.8 62.5 +6.1 46.6 34.9 11.7 44.3 2.3 43.7 2.9 51.6 +5.0 36.3 35.7 0.6 44.5 +8.2 41.3 +5.0 46.1 +9.8 47.6 63.1 +15.5 54.6 +7.0 52.3 +4.7 64.8 +17. 80.2 63.9 16.3 64.0 16.2 80.9 +0.7 92.7 +12.5 Table 3: Experimental results on Mutual Friends dataset. Bold: Optimal. Underline: Sub-Optimal. Methods Backbone Success Rate (%) #Avg. Turn SR/#Avg. Turn #Avg. Token SR/#Avg. Token w/o belief w/o belief + CoT w/o belief + Self Reflect rand belief MindDial BEDA (Ours) w/o belief w/o belief + CoT w/o belief + Self Reflect rand belief MindDial BEDA (Ours) w/o belief w/o belief + CoT w/o belief + Self Reflect rand belief BEDA (Ours) w/o belief w/o belief + CoT w/o belief + Self Reflect rand belief BEDA (Ours) GPT-3.5 GPT-3.5 GPT-3.5 GPT-3.5 GPT-3.5 GPT-3.5 GPT-4 GPT-4 GPT-4 GPT-4 GPT-4 GPT-4 GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini Qwen2.5 (7B) Qwen2.5 (7B) Qwen2.5 (7B) Qwen2.5 (7B) Qwen2.5 (7B) Qwen2.5 (14B) w/o belief Qwen2.5 (14B) w/o belief + CoT w/o belief + Self Reflect Qwen2.5 (14B) Qwen2.5 (14B) rand belief Qwen2.5 (14B) BEDA (Ours) 10.7 32.6 +21.9 32.6 +21.9 14.3 +3.6 24.3 +13.6 41.1 +30.4 75.0 77.9 +4.9 76.5 +1.5 75.1 +0.1 76.0 +1.0 82.5 +7.5 68.8 62.2 6.6 55.7 13.1 33.3 35.5 70.4 +1.6 33.3 29.8 3.5 19.1 14.2 15.9 17.4 31.9 1.4 55.7 62.3 +6.6 37.7 18.0 41.1 14.0 64.1 +8.4 5.7 7.9 9.4 11.0 6.0 8.8 9.7 8.8 8.7 10.0 8.9 7. 11.7 13.5 13.5 16.1 11.5 4.5 4.8 17.9 7.1 3.7 11.2 9.1 16.7 12.6 6.7 1.9 4.1 3.5 1.3 4.1 4.7 7.7 8.9 8.8 7.5 8.5 10.4 5.9 4.6 4.1 2.1 6. 7.4 6.2 1.1 2.2 8.6 5.0 6.8 2.3 3.3 9.6 - 205.0 243.3 130.0 - 295.7 - 537.2 538.7 729.1 - 500.0 724.2 368.0 592.5 436.2 869.1 545.9 573.1 636.7 993.8 506. 647.7 700.0 589.1 1325.8 628.4 - 0.159 0.134 0.110 - 0.139 - 0.145 0.142 0.103 - 0.165 0.095 0.169 0.094 0.027 0.081 0.061 0.052 0.030 0.016 0.063 0.086 0.089 0.064 0.031 0. efficiency, we also report #Avg. Turn (average number of dialogue turns per instance) and #Avg. Token (average number of whitespace-tokenized tokens per instance). For direct efficiency comparisons, we further report SR/#Avg. Turn and SR/#Avg. Token. CaSiNo: Agreement Rate, the proportion of negotiations that end in an agreement; and Agreement Reward, the mean utility conditioned on reaching an agreement. Backbones. In our experiments, we utilized large language models depicted as follows: GPT: Among the non-open-source models, we have chosen GPT-3.5 [31], GPT-4, GPT-4o, and GPT-4.1 [32] from OpenAI. LLaMA: As one of the most popular open-source LLM families, LLaMA is frequently used as backbone. We experimented with different sizes of LLaMA2 (7B, 13B) [40] and LLaMA3.1 (8B) [12] Qwen: Qwen2.5 [49] is series of models that are pre-trained on Alibabas latest large-scale dataset (up to 18 trillion tokens). We utilized different sizes of Qwen2.5 (7B, 14B) All experiments on the open-source models are done under the quantization of q4_k_m. The ğœ– mentioned for each dialogue act is constant across all backbone models. (a) CaSiNo: Average Agree Reward. (b) CaSiNo: Average Agree Rate. Figure 2: Experimental results on CaSiNo. Each experiment is performed for three times except for those with GPT-4 backbone."
        },
        {
            "title": "4.2 Conditional Keeper-Burglar Game (CKBG)\nThe results on CKBG, as presented in Table 2, provide a comparative\nanalysis of the performance of five distinct approaches. We compare\nBEDA with the performance of four approaches: w/o belief (without\nbelief estimation, relying solely on the language model itself.), w/o\nbelief + CoT (using Chain-of-Thought (CoT) technique [42]), w/o\nbelief + Self Reflect (using self reflect technique [38]), random belief\n(utilizing randomized belief estimation, each time the randomized\nselection process is conducted.). The belief estimator is trained\nfollowing the procedure described in Section 2.4. Specifically, we\nconstruct 400 data points from the first 50 settings of the original\ndataset and employ two w/o belief GPT-3.5 models to generate\ndialogue data with randomized belief annotations. To prevent data\nleakage, these 50 settings are distinct from those used for testing.\nA comparative analysis of the experimental results between the\nBEDA and the w/o belief model reveals a consistent performance\nimprovement when belief estimations are employed as probabilis-\ntic constraints. Specifically, the success rate exhibits a minimum\nincrease of 5.0 when LLaMA3.1-8b is used as the backbone model,\nwith the maximum improvement reaching 20.6 when Qwen2.5-14b\nis utilized as the backbone. Furthermore, in comparison to widely\nused methods such as CoT [42] and Self-Reflect [38], BEDA demon-\nstrates superior performance, surpassing these approaches by 15.5%\nand 15.4%, respectively, when averaged across all backbones. These\nfindings underscore the effectiveness of functioning belief es-\ntimations as probabilistic constraints in performing Adver-\nsarial Dialogue Act .",
            "content": "Furthermore, the precision of belief estimation is also crucial. BEDA consistently surpasses the random belief baseline across all backbone models by substantial margin. On average, BEDA achieves success rate exceeding that of random belief by more than 9.5 points across the evaluated models. The minimum observed improvement is 4.8, recorded with the LLaMA3.1-8b model, while the maximum enhancement reaches 14.0% when employing GPT4.1-nano as the backbone. This sustained performance superiority across diverse models suggests that enhanced accuracy in belief estimation significantly contributes to improved outcomes."
        },
        {
            "title": "4.3 Mutual Friends (MF)\nThe results for MF are presented in Table 3. We utilize the baselines\ndefined in Section 4.2 and include MindDial proposed by Qiu et al.\n[33] as an additional baseline. MindDial provides the LLM-agent\nwith all available belief information through prompts, without im-\nposing constraints. The training of our belief estimator follows the\nprocedure described in Section 4.2, and the complete test set from\nthe original dataset was used for evaluation.",
            "content": "Compared with MindDial, BEDA shows that belief estimation alone is insufficient; better usage mechanism is needed. As shown by the success rate (SR) in Table 3, BEDA attains state-ofthe-art results on GPT-3.5, GPT-4, GPT-4o-mini and Qwen2.5-7B, delivering an average gain of 9.3 points over the w/o belief variant the largest among all baselineswith maximum improvement of 30.4 when using GPT-3.5 as the backbone LLM. Although MindDial also estimates beliefs, it does not constrain which parts of the belief state are exploited for the task; accordingly, on GPT-4 its win rate (76.0%) is essentially indistinguishable from using random belief state (75.1%), leading to suboptimal performance. These results underscore that leveraging the estimated belief state to impose probabilistic constraints can improve cooperative success rate. In addition to SR, we also calculated the ratios of SR over average turns and SR over average tokens to reflect the efficiency of information exchange. The experimental results in Table 3 show that the BEDA method achieves the highest average SR while requiring the fewest average turns. Its SR/# Avg. Turn outperforms the baseline method across all experimental settings, with an average improvement of 2.3 compared to w/o belief. At the same time, BEDA average SR/# Avg. Token also reaches an optimal value of 0.11, which outperforms all comparison methods. The above results clearly demonstrate that probabilistic constraints based on belief estimation can also enhance the efficiency of information exchange in cooperative tasks. BEDA steers both agents content toward evidence diagnostic of the mutual friend, encourages more active elimination of incorrect candidates, and exchanges new information. Although this richer exchange makes individual utterances longer, the dyad completes the task in fewer turns, achieving higher success rate as well as lower #Avg. Turn and #Avg. Token, which means higher cooperative efficiency."
        },
        {
            "title": "4.4 CaSiNo\nWe compare BEDA â€™s results against the MindDial baseline [33],\nas well as other baselines defined in Section 4.2. Following the\nmethodology proposed by Qiu et al. [33], both negotiators are\ntreated symmetrically, and we utilize both closed-source and open-\nsource models as the backbone. The training procedure for the\nbelief estimator remains consistent with the approaches used in\nCKBG and MF. The experimental results are summarized in Fig-\nure 2. Specifically, Figure 2b reports the average agreement rate,\nwhich quantifies the probability of successfully reaching a deal,\nwhile Figure 2a presents the average agreement reward, reflecting\nthe quality of the negotiated outcome.",
            "content": "For the closed-source model, which is built upon GPT-3.5 and GPT-4, BEDA demonstrates competitive average agreement rate and achieves the highest average agreement reward among all evaluated methods. Moreover, when employing the open-source LLaMA2 (7B) backbone, BEDA generates higher-quality deals compared to MindDial and other baseline models. Notably, while BEDA utilizes fixed large language model (LLM), MindDial requires training in the LLM when using an open-sourced backbone. These findings suggest that the BEDA enhances negotiation outcomes and highlights the critical role as the probabilistic constraints of belief estimation in this process."
        },
        {
            "title": "5 FURTHER ANALYSIS\n5.1 Case Study\nWe perform case study analysis here. See Section H for more.",
            "content": "5.1.1 Conditional Keeper-Burglar Game (CKBG). As shown in case 5.1.1, which was generated by GPT-4.1 to illustrate belief prediction in the CKBG, the belief estimator progressively refines its inference as the dialogue unfolds, demonstrating increasing accuracy over time. By the second round of conversation, it successfully captures the event David opened the Tupperware three hours ago mentioned implicitly by the burglar and updates the shared belief state accordingly, notifying the keeper of this new evidence. Moreover, the estimator correctly identifies the burglars suspicious attitude toward the keeper, reflecting nuanced understanding of speaker intent. Leveraging these inferred beliefs, the keeper strategically responds with deliberately vague statement, as seen in the highlighted part, effectively maintaining ambiguity while aligning with the estimated belief context. 5.1.2 Mutual Friends (MF). As shown in case 5.1.2, we evaluate three dialogue settings, BEDA , w/o belief, and w/o belief + CoT, on identical samples; for comparability, only the first two turns are shown. The backbone LLM is GPT-4. We find that adding complex reasoning frameworks (e.g., CoT) provides little benefit for Alignment Dialogue Act : CoT frequently reproduces the backbones output verbatim (highlighted in brown and green, respectively), yielding inferior performance relative to BEDA . We also observe that, without belief-state constraints, LLM dialogues tend to exhibit two recurring hallucinations: (i) friend-list Case Study: Conditional Keeper-Burglar Game Ground Truth Belief State (Burglar John) World Events Known Unknown 1. There is resin container and an opaque Tupperware in the room. 2. The resin container contains an antique Rolex watch. 3. The opaque Tupperware contains pen cap. 4. The keeper Jacobs goal is to mislead burglar John. 5. The keeper Jacob probably lies to the burglar John. 6. The keeper Jacob opened the resin container 10 hours ago. 7. The burglars friend David opened the opaque Tupperware 3 hours ago. 8. noise is coming from the resin container. BEDA The Rolex watch is in the resin container. Burglar John: Hi, Jacob, where is the antique Rolex watch? opaque Tupperware or resin container? Predicted Belief (Burglar John) Known: 1, 4 Ground Truth: 1, 4, 5, 7 Belief Gap: 2 Homeowner Jacob: Oh, wouldnt trust the resin container if were you. Its been making strange noises lately, and heard someone opened it just an hour ago. The Tupperware, on the other hand, has been untouched for while, and can assure you, its the safer choice. Why not go for the Tupperware? Burglar John: Hmm, so youre saying the resin container might be suspicious because of the noises and recent opening, but you claim the Tupperware is untouched and safer. But since you might be lying, cant fully trust that. Given that, and knowing David opened the Tupperware three hours ago, do you think the watch could still be inside the Tupperware, or is it more likely in the resin container? Predicted Belief (Burglar John) Known: 1, 4, 5, 7 Ground Truth: 1, 4, 5, 7 Belief Gap: Homeowner Jacob: Well, considering that the resin container was opened quite while agoabout ten hoursand no one has touched it since, its probably just as safe as the Tupperware. In fact, the Tupperware has been untouched for hours, and David only opened it three hours ago, so its possible the watch is still in there. But honestly, Id say the resin container is more suspicious now, especially with those noises. If were you, Id go with the Tupperware, its less likely to have been disturbed recently. Burglar John: [STOP] Burglar chosed: opaque Tupperware. The correct answer is: resin container SYSTEM: The burglar has been cheated. comparison, where enumerating both agents lists still fails to isolate the mutual frienda problem exacerbated in weaker LLMs (See Table 11); and (ii) looping dialogue, where previously eliminated candidates are reintroduced, causing repeated turns without the model recognizing the loop (See Table 12). These hallucinations can be relieved when probabilistic constraints of belief estimation are introduced."
        },
        {
            "title": "5.2 Accuracy of Belief Estimation\nTo assess the reliability of our belief modeling, we show the accu-\nracy of the belief estimation across multiple datasets following the\nsame experimental setup as in the main experiments. As shown in\nTable 4, the belief estimator achieves high accuracy (â‰ˆ 0.9) on the\nsynthetic MF and CKBG datasets, which aligns well with the overall",
            "content": "Case Study: Mutual Friends BEDA (Ours) Bob: Hello Alex, great to meet you! Lets start by sharing some details about our friends. Ill begin with one: have friend who majored in Astrophysics, works at Assurant, and prefers to be outdoors. How about you? Alex: It seems that none of my friends match the description of your friend who majored in Astrophysics. However, do have friend who works at Assurant and prefers to be indoors. My friend majored in Greek. Could you tell me more about your friends who have preference for indoor environments? Bob: It looks like we dont have match there, as none of my friends who prefer indoor environments majored in Greek. However, do have friend who majored in English Language, works at Salem Communications, and prefers to be indoors. Could this be our mutual friend? Alex: Unfortunately, thats not our mutual friend either, as none of my friends majored in English Language. However, do have two friends who work at Salem Communications and prefer indoor environments, but both majored in Greek. Could one of them be our mutual friend? w/o belief Bob: Hello Alex, great to meet you! Lets start by narrowing down our search. Does our mutual friend have preference for working outdoors? Alex: No, our mutual friend does not have preference for working outdoors. Bob: Based on your clue, the only person on my list who fits that description is: \"Major: English Language, Company: Salem Communications, Location Preference: indoor.\" Is this our mutual friend? Alex: No, thats not our mutual friend. Our mutual friend has major in Greek, not English Language. Would you like to guess again based on the major? w/o belief+CoT Bob: Hello Alex, great to meet you! Lets start by narrowing down our search. Does our mutual friend have preference for working outdoors? Alex: No, our mutual friend does not have preference for working outdoors. Bob: Thank you for the information, Alex. Based on what youve told me, our mutual friend must have preference for working indoors. The only friend on my list who prefers working indoors is: \"Major: English Language, Company: Salem Communications, Location Preference: indoor.\" Is this our mutual friend? Alex: No, thats not our mutual friend. Our mutual friend has major in Greek, not English Language. Would you like to guess again based on the major? Table 4: Classification accuracy result of trained belief estimator while testing. Task #Train Data #Test Data Belief Pred. Acc (%) CKBG MF CaSiNo 400 1000 500 150 1117 100 89.02 90.13 74.44 effectiveness observed in our main experiments. In contrast, the performance on CaSiNo is notably lower. We attribute this to the dual cooperative-competitive nature of CaSiNo and the increased difficulty of its multi-class belief structure."
        },
        {
            "title": "6 RELATED WORKS",
            "content": "Dialogue Acts Generation. Dialogue acts, defined as functional units of communication in speech act theory [3], govern intentional interactions such as persuasion, negotiation, and debate. In the early days, people focused on dialogue act classification [17, 30, 35], whereas nowadays with the rise of LLMs, generation is important. Persuasion emphasizes attitude modification, where computational models optimize strategies [45] such as integrating emotional intelligence [36]. Negotiation centers on multi-party interest coordination, requiring balancing collaborative and competitive dynamics [7, 11, 24, 48]. Debate prioritizes logical argumentation structures [27, 41, 53]. Text-based social deduction games [28, 37, 44, 46, 47] serve as important application scenarios for dialogue act generation. Theory-of-Mind and Opponent Modeling. Theory of Mind (ToM) refers to the cognitive ability to understand mental states like beliefs and intentions of oneself and others, and to predict behavior according to the states [23]. Recent studies have shown that LLMs have ToM abilities to some extent, as they are able to distinguish between different belief states of multiple agents [22, 56]. In the context of opponent modeling [29], ToM serves as prevalent approach for modeling agents [34, 52]. Recent advancements in opponent modeling for Reinforcement Learning (RL) have enabled RL models to dynamically adapt their strategies during execution by responding to opponents employing unknown and changeable algorithms [1, 21, 51]. Furthermore, opponent modeling has practical applications in strategic game scenarios, such as poker [4], contract bridge [26] and deception game [15]."
        },
        {
            "title": "7 DISCUSSION AND FUTURE WORKS",
            "content": "Applying LLMs for belief estimation. While large language models (LLMs) can serve as belief estimators, our experiments indicate that lightweight encoders suffice to infer belief states from dialogue with substantial efficiency gains. In particular, compact encoder captures the salient conversational signals with accuracy comparable to an LLM at fraction of the computational cost. We therefore employ the smaller model in practice, reserving LLM-based estimators for settings that demand broader world knowledge or enhanced robustness to distribution shift. Dynamic construction of the world set. To isolate the contribution of belief estimation and its interaction with dialogue acts, this work holds the world set fixed. Future work may relax this assumption by learning dynamic world set that is expanded, pruned, or reweighted online based on interaction signals, potentially improving coverage and downstream performance. Finer modeling of dialogue acts. Although many specific acts (e.g., AGREEMENT, typically subsumed under Alignment Dialogue Act ) fall within the coarse categories Adversarial Dialogue Act and Alignment Dialogue Act , finer-grained act modeling may yield stronger agents. Hierarchical or compositional schemas that distinguish, for example, Agreement, Concession, Hedging, and Commitment, could provide more precise control and better credit assignment during learning."
        },
        {
            "title": "8 CONCLUSIONS\nWe presented BEDA , a simple yet general framework that casts be-\nlief estimation as probabilistic constraints for executing strategic di-\nalogue acts. By formalizing two core actsâ€”Adversarial (introduce\nor emphasize events outside the interlocutorâ€™s belief support) and\nAlignment (restrict utterances to common ground)â€”and instanti-\nating them with a world set, dual belief estimators (event truth\nand opponent knowledge), and a conditional generator, BEDA\nbridges the gap between estimating beliefs and using them dur-\ning generation. Across three settings, Conditional Keeperâ€“Burglar\n(adversarial), Mutual Friends (cooperative), and CaSiNo (negoti-\nation), BEDA consistently improves strategic reliability: it raises",
            "content": "success on CKBG (up to +20.6 points with GPT-4.1-nano), improves success in MF (gains of 4.311.2 points, and up to +30.4 in some configurations) while reducing turns, and negotiates higher-quality agreements in CaSiNo relative to strong prompting baselines. These results indicate that constraining generation by inferred belief structure is an effective organizing principle for dialogue agents, complementary to generic dialogue heuristics such as CoT or selfreflection. We see BEDAs constraint-based view as step toward strategic dialogue agents that reason aboutand act throughthe beliefs that structure interaction. (See Section for ethics statement and societal impacts.) [18] EunJeong Hwang, Yuwei Yin, Giuseppe Carenini, Peter West, and Vered Shwartz. 2025. Infusing Theory of Mind into Socially Intelligent LLM Agents. arXiv:2509.22887 [cs.CL] https://arxiv.org/abs/2509.22887 [19] E. T. Jaynes. 1957. Information Theory and Statistical Mechanics. Phys. Rev. 106 (May 1957), 620630. Issue 4. https://doi.org/10.1103/PhysRev.106.620 [20] E. T. Jaynes. 1957. Information Theory and Statistical Mechanics. II. Phys. Rev. 108 (Oct 1957), 171190. Issue 2. https://doi.org/10.1103/PhysRev.108.171 [21] Dong Ki Kim, Miao Liu, Matthew Riemer, Chuangchuang Sun, Marwa Abdulhai, Golnaz Habibi, Sebastian Lopez-Cot, Gerald Tesauro, and Jonathan How. 2021. policy gradient algorithm for learning to learn in multiagent reinforcement learning. In International Conference on Machine Learning (ICML). PMLR, 5541 5550. [22] Michal Kosinski. 2023. Theory of mind may have spontaneously emerged in large language models. Computing Research Repository (CoRR) abs/2302.02083 (2023), 169. [23] Alan Leslie. 1987. Pretense and representation: The origins of\" theory of mind.\". REFERENCES [1] Maruan Al-Shedivat, Trapit Bansal, Yura Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. 2018. Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments. In International Conference on Learning Representations (ICLR). https://openreview.net/forum?id=Sk2u1g-0- [2] Robert J. Aumann. 1976. Agreeing to Disagree. The Annals of Statistics 4, 6 (1976), 12361239. [3] J. L. Austin. 1962. How to Do Things with Words. Oxford University Press, Oxford. [4] Darse Billings, Denis Papp, Jonathan Schaeffer, and Duane Szafron. 1998. Opponent modeling in poker. AAAI Conference on Artificial Intelligence (AAAI)/Innovative Applications of Artificial Intelligence Conference (IAAI) 493, 499 (1998), 105. [5] Giacomo Bonanno. 2015. Game Theory (Open Access textbook with 165 solved exercises). Open Access. https://doi.org/10.13140/RG.2.1.3369.7360 [6] Adam Brandenburger and Eddie Dekel. 1987. Common knowledge with probhttps: ability 1. Journal of Mathematical Economics 16, 3 (1987), 237245. //doi.org/10.1016/0304-4068(87)90010-3 [7] Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale Lucas, Jonathan May, and Jonathan Gratch. 2021. CaSiNo: corpus of campsite negotiation dialogues for automatic negotiation systems. Computing Research Repository (CoRR) abs/2103.15721 (2021). [8] Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale M. Lucas, Jonathan May, and Jonathan Gratch. 2021. CaSiNo: Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). 3167 3185. https://doi.org/10.18653/V1/2021.NAACL-MAIN.254 [9] Judith Degen. 2023. The Rational Speech Act Framework. Annual Review of Linguistics 9, Volume 9, 2023 (2023), 519540. https://doi.org/10.1146/annurevlinguistics-031220- [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). 41714186. https://doi.org/10.18653/V1/ N19-1423 [11] Samuel Mooly Dinnar, Chris Dede, Emmanuel Johnson, Carrie Straub, and Kristjan Korjus. 2021. Artificial intelligence and technology in teaching negotiation. Negotiation Journal 37, 1 (2021), 6582. [12] Abhimanyu Dubey et al. 2024. The Llama 3 Herd of Models. Computing Research Repository (CoRR) abs/2407.21783 (2024). https://api.semanticscholar.org/ CorpusID:271571434 [13] FAIR. 2022. by combining language models with strategic reasoning. 378, 6624 (2022), 10671074. arXiv:https://www.science.org/doi/pdf/10.1126/science.ade9097 Human-level play in the game of <i>Diplomacy</i> Science https://doi.org/10.1126/science.ade [14] Michael C. Frank and Noah D. Goodman. 2012. Predicting Pragmatic Reasoning in Language Games. Science 336, 6084 (2012), 998998. https://doi.org/10.1126/ science.1218633 arXiv:https://www.science.org/doi/pdf/10.1126/science.1218633 [15] Thilo Hagendorff. 2024. Deception abilities emerged in large language models. Proceedings of the National Academy of Sciences (PNAS) 121, 24 (2024), e2317967121. [16] He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. 2017. Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings. In Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics, Vancouver, Canada, 17661776. https://doi.org/10.18653/v1/P17-1162 [17] Zihao He, Leili Tavabi, Kristina Lerman, and Mohammad Soleymani. 2021. Speaker Turn Modeling for Dialogue Act Classification. In Annual Conference on Empirical Methods in Natural Language Processing (EMNLP), Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Punta Cana, Dominican Republic, 21502157. https://doi.org/10.18653/v1/2021.findings-emnlp.185 Psychological review 94, 4 (1987), 412. [24] Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra. 2017. Deal or no deal? end-to-end learning for negotiation dialogues. Computing Research Repository (CoRR) abs/1706.05125 (2017). [25] Hengli Li, Song-Chun Zhu, and Zilong Zheng. 2023. DiPlomat: Dialogue Dataset for Situated Pragmatic Reasoning. In Advances in Neural Information Processing Systems (NeurIPS). [26] Junkang Li, Bruno Zanuttini, and VÃ©ronique Ventos. 2024. Opponent-model search in games with incomplete information. In AAAI Conference on Artificial Intelligence (AAAI), Vol. 38. 98409847. [27] Sha Li, Heng Ji, and Jiawei Han. 2021. Document-Level Event Argument Extraction by Conditional Generation. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). 894908. https://doi.org/10.18653/V1/2021.NAACL-MAIN.69 [28] Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. 2023. Avalonbench: Evaluating llms playing the game of avalon. In NeurIPS 2023 Foundation Models for Decision Making Workshop. [29] Samer Nashed and Shlomo Zilberstein. 2022. survey of opponent modeling in adversarial domains. Journal of Artificial Intelligence Research (JAIR) 73 (2022), 277327. [30] Bill Noble and Vladislav Maraev. 2021. Large-scale text pre-training helps with dialogue act recognition, but not without fine-tuning. In Proceedings of the 14th International Conference on Computational Semantics (IWCS), Sina ZarrieÃŸ, Johan Bos, Rik van Noord, and Lasha Abzianidze (Eds.). Association for Computational Linguistics, Groningen, The Netherlands (online), 166172. https://aclanthology. org/2021.iwcs-1.16/ [31] OpenAI. 2023. GPT-3.5 Turbo fine-tuning and API updates. https://openai.com/ index/gpt-3-5-turbo-fine-tuning-and-api-updates/ [32] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, SimÃ³n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Åukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Åukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David MÃ©ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub [44] Dekun Wu, Haochen Shi, Zhiyuan Sun, and Bang Liu. 2024. Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games. In Annual Meeting of the Association for Computational Linguistics (ACL). 82258291. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.490 [45] Jinghua Wu, Wenzhe Wang, and Yan Li. 2024. Emotional agents enabled bilateral negotiation: Persuasion strategies generated by agents affect infusion and preference. Expert Systems with Applications 252 (2024), 124202. https: //doi.org/10.1016/j.eswa.2024.124202 [46] Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, and Haobo Fu. 2024. Enhance reasoning for large language models in the game werewolf. Computing Research Repository (CoRR) abs/2402.02330 (2024). [47] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. 2023. Exploring large language models for communication games: An empirical study on werewolf. Computing Research Repository (CoRR) abs/2309.04658 (2023). [48] Atsuki Yamaguchi, Kosui Iwasa, and Katsuhide Fujita. 2021. Dialogue act-based breakdown detection in negotiation dialogues. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021. 745757. [49] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 Technical Report. Computing Research Repository (CoRR) abs/2412.15115 (2024). [50] Steve Young, Milica GaÅ¡iÄ‡, Blaise Thomson, and Jason D. Williams. 2013. POMDPBased Statistical Spoken Dialog Systems: Review. Proc. IEEE 101, 5 (2013), 11601179. https://doi.org/10.1109/JPROC.2012.2225812 [51] Xiaopeng Yu, Jiechuan Jiang, Wanpeng Zhang, Haobin Jiang, and Zongqing Lu. 2022. Model-based opponent modeling. Advances in Neural Information Processing Systems (NeurIPS) 35 (2022), 2820828221. [52] Luyao Yuan, Zipeng Fu, Jingyue Shen, Lu Xu, Junhong Shen, and Song-Chun Zhu. 2020. Emergence of pragmatics from referential game between theory of mind agents. Computing Research Repository (CoRR) abs/2001.07752 (2020). [53] Yiqun Zhang, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, and Kaisong Song. 2024. Can LLMs Beat Humans in Debating? Dynamic Multi-agent Framework for Competitive Debate. Computing Research Repository (CoRR) abs/2408.04472 (2024). [54] Hao Zhu, Graham Neubig, and Yonatan Bisk. 2021. Few-shot Language Coordination by Modeling Theory of Mind. In International Conference on Machine Learning (ICML) (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 1290112911. https://proceedings.mlr.press/ v139/zhu21d.html [55] Song Chun Zhu, Yingnian Wu, and David Mumford. 1998. Filters, Random Fields and Maximum Entropy (FRAME): Towards Unified Theory for Texture Modeling. International Journal of Computer Vision (IJCV) 27, 2 (01 Mar 1998), 107126. https://doi.org/10.1023/A:1007925832420 [56] Wentao Zhu, Zhining Zhang, and Yizhou Wang. 2024. Language Models Represent Beliefs of Self and Others. In International Conference on Machine Learning (ICML). https://openreview.net/forum?id=asJTE8EBjg Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe CerÃ³n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2023. GPT-4 Technical Report. Computing Research Repository (CoRR) abs/2303.08774 (2023). https://doi.org/10.48550/ARXIV.2303.08774 arXiv:2303.08774 [33] Shuwen Qiu, Mingdian Liu, Hengli Li, Song-Chun Zhu, and Zilong Zheng. 2024. MindDial: Enhancing Conversational Agents with Theory-of-Mind for Common Ground Alignment and Negotiation. In Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDial), Tatsuya Kawahara, Vera Demberg, Stefan Ultes, Koji Inoue, Shikib Mehri, David Howcroft, and Kazunori Komatani (Eds.). Association for Computational Linguistics, Kyoto, Japan, 746759. https: //doi.org/10.18653/v1/2024.sigdial-1.63 [34] Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick. 2018. Machine theory of mind. In International Conference on Machine Learning (ICML). PMLR, 42184227. [35] Vipul Raheja and Joel R. Tetreault. 2019. Dialogue Act Classification with ContextAware Self-Attention. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). 37273733. https://doi.org/10.18653/V1/N19-1373 [36] Azlaan Mustafa Samad, Kshitij Mishra, Mauajama Firdaus, and Asif Ekbal. 2022. Empathetic persuasion: reinforcing empathy and persuasiveness in dialogue systems. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). 844856. [37] Hisaichi Shibata, Soichiro Miki, and Yuta Nakamura. 2023. Playing the Werewolf game with artificial intelligence for language understanding. Computing Research Repository (CoRR) abs/2302.10646 (2023). [38] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, language agents with verbal reReflexion: Information Processing in Neural http://papers.nips.cc/paper_files/paper/2023/hash/ and Shunyu Yao. 2023. inforcement Systems 1b44b878bb782e6954cd888628510e90-Abstract-Conference.html In Advances (NeurIPS). learning. [39] Noam Slonim, Yonatan Bilu, Carlos Alzate, Roy Bar-Haim, Ben Bogin, Francesca Bonin, Leshem Choshen, Edo Cohen-Karlik, Lena Dankin, Lilach Edelstein, Liat Ein-Dor, Roni Friedman-Melamed, Assaf Gavron, Ariel Gera, Martin Gleize, Shai Gretz, Dan Gutfreund, Alon Halfon, Daniel Hershcovich, Ron Hoory, Yufang Hou, Shay Hummel, Michal Jacovi, Charles Jochim, Yoav Kantor, Yoav Katz, David Konopnicki, Zvi Kons, Lili Kotlerman, Dalia Krieger, Dan Lahav, Tamar Lavee, Ran Levy, Naftali Liberman, Yosi Mass, Amir Menczel, Shachar Mirkin, Guy Moshkowich, Shila Ofek-Koifman, Matan Orbach, Ella Rabinovich, Ruty Rinott, Slava Shechtman, Dafna Sheinwald, Eyal Shnarch, Ilya Shnayderman, Aya Soffer, Artem Spector, Benjamin Sznajder, Assaf Toledo, Orith Toledo-Ronen, Elad Venezian, and Ranit Aharonov. 2021. An autonomous debating system. Nat. 591, 7850 (2021), 379384. https://doi.org/10.1038/S41586-021-03215-W [40] Hugo Touvron et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. Computing Research Repository (CoRR) abs/2307.09288 (2023). https: //api.semanticscholar.org/CorpusID: [41] Henning Wachsmuth, Nona Naderi, Yufang Hou, Yonatan Bilu, Vinodkumar Prabhakaran, Tim Alberdingk Thijm, Graeme Hirst, and Benno Stein. 2017. Computational argumentation quality assessment in natural language. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 1: Long Papers. 176187. [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems (NeurIPS). http://papers.nips.cc/paper_files/paper/2022/hash/ 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html [43] Heinz Wimmer and Josef Perner. 1983. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young childrens understanding of deception. Cognition 13, 1 (1983), 103128."
        },
        {
            "title": "SETTINGS",
            "content": "This work advances agentic AI, aiming to build AI agents capable of operating in complex social settings. While such agents could, in principle, produce aggressive utterances, this is neither an objective nor desirable behavior in our research, and we try our best to promise that our technique is designed for the good of human beings. We mitigate this risk by employing safety-screened, pre-trained language models, curated training data, and decoding constraints designed to discourage harmful content. Our study follows community and institutional ethical guidelines and seeks to improve reliability, transparency, and responsible deployment of agentic systems."
        },
        {
            "title": "OPTIMIZATION",
            "content": "The original problem is: ğœ‹ (ğ‘¢, ğ¸ğ¶) arg max ğ‘¢,ğ¸ ğ‘Š s.t. ğ‘ƒğ´ (ğ¸ğ¶) 1 ğœ– ğ‘ƒğ´ (ğ¾ğµğ¸ğ¶) 1 ğœ– or ğ‘ƒğ´ (ğ¾ğµğ¸ğ¶) 1 ğœ– By Bayesian Rules, ğœ‹ (ğ‘¢ğ¸, ğ¶)ğœ‹ (ğ¸ğ¶) arg max ğ‘¢,ğ¸ ğ‘Š s.t. ğ‘ƒğ´ (ğ¸ğ¶) 1 ğœ– ğ‘ƒğ´ (ğ¾ğµğ¸ğ¶) 1 ğœ– or ğ‘ƒğ´ (ğ¾ğµğ¸ğ¶) 1 ğœ– As ğ¸ is chosen equally to avoid introducing new constraints, ğœ‹ (ğ¸ğ¶) = const, thus the problem is equivalent to: ğœ‹ (ğ‘¢ğ¸, ğ¶) arg max ğ‘¢,ğ¸ ğ‘Š s.t. ğ‘ƒğ´ (ğ¸ğ¶) 1 ğœ– ğ‘ƒğ´ (ğ¾ğµğ¸ğ¶) 1 ğœ– or ğ‘ƒğ´ (ğ¾ğµğ¸ğ¶) 1 ğœ– DETAILS ON LARGE LANGUAGE MODELS In our experiments, we utilized several large language models categorized into three families: GPT, LLaMA, and Qwen. The introduction and reasons for choosing these models are as follows: (1) GPT: Among the non-open-source models, we have chosen GPT-3.5 [31] and GPT-4 [32] from OpenAI. The selected version of GPT-3.5 is gpt-35-turbo-0125 and that of GPT-4 is gpt-4-1106-preview. (2) LLaMA: As one of the most popular open-source LLM families, LLaMA is frequently used as baseline for variety of NLG tasks. We experimented with different sizes of llama2 (13b-chat-q4_k_m, 7b-chat-q4_k_m) [40] and llama3.1 (8binstruct-q4_k_m) [12] (3) Qwen: Qwen2.5 [49] is series of models, that are pretrained on Alibabas latest large-scale dataset (up to 18 trillion tokens), and support over 29 languages. we utilized different sizes of Qwen2.5 (14b-instruct-q4_K_M, 7b-instructq4_K_M) Format Error. Despite manually tuning prompts, LLMs continue to exhibit format errors. Consequently, all reported values are based on those that adhere to the correct format, with violations excluded. In all experiments, the LLM temperature was set to 0. Experiments with open-source models were conducted via Ollama1, with random seeds and parameters adhering to default configurations. The random seed in PyTorch was fixed at 3407. For training all belief estimators, the cross-entropy loss function was employed with learning rate of 1e-5, using the Adam optimizer, while freezing the parameters of BERT [10]. CONDITIONAL KEEPER-BURGLAR GAME (CKBG) E.0.1 The CKBG Task. Derived from the classic \"Sally-Anne\" psychological experiment [43], Hagendorff [15] proposed the KeeperBurglar game to analyze the ability of LLMs to understand the false beliefs and deception of others. The game is situated in scenario containing two containers, one of which conceals valuable item. The LLM assumes the role of Homeowner whose objective is to mislead an intruding Burglar in order to protect the valuable possession. To enhance the games diversity and simulate more realistic dialogue scenarios, we extended the original \"first-order/secondorder\" framework by incorporating additional contextual elements. These include characters inspecting the containers and the presence of audible container noises, creating more dynamic and complex interaction environment. Therefore, its named the Conditional Keeper-Burglar Game (CKBG). E.0.2 Construction of CKBG Dataset. The conditions were randomly assigned to both the keeper and the burglar. The five condition classes are defined as follows: Informer: An individual has informed the burglar that the keeper may be deceitful. Burglar inspection: The burglar opened [Container x] [Time] hours ago and observed [Object x] inside. Keeper inspection: The keeper opened [Container x] [Time] hours ago. Outsider inspection: friend of the keeper opened [Container x] [Time] hours ago. Noise: noise was heard emanating from [Container x]. The dataset statistics are presented in Table 5. The conditions were randomly assigned to both the keeper and the burglar, yielding total of 450 condition instances for the test dataset. E.0.3 Details of World Set Designation and Selection Process. World Set Designation. The Conditional Keeper-Burglar Dataset was developed based on the work of Hagendorff [15], incorporating additional contextual elements to function as conditions. These modifications were intended to enhance the games variability and create an information asymmetry. The world set, which is comprehensive compilation of all potential conditions, is systematically organized into five distinct categories. Conditions from this set are randomly selected and incorporated into the prompt to provide the burglar with background information. Conversely, for the keeper, belief estimator is employed to determine subset of conditions 1https://ollama.com/ Table 5: Statistical feature of Dialogue Games Datasets. (1) # Conditions: Number of types of different conditions allocated to the two agents. (2) Known Conditions: Number of conditions allocated to the two agents. (3) Avg. Words: Number of words in the training dialogue per setting. (4) Avg. Conditions: Number of types of different conditions allocated to the two agents per setting. (5) Vocabulary Size: Vocabulary size of training dialogues. For more statistical information of CaSiNo and MF, please check Chawla et al. [8] and He et al. [16]. # Settings # Conditions # Known Conditions Avg. Words Avg. Conditions Vocabulary Size CaSiNo CKBG MF Train 500 - - 295.4 - 567 Test 100 - - - - - Train 400 1017 1638 255.5 2.54 1490 Test 150 450 759 - 3.00 - Train 1000 - - 436.5 - 3507 Test 1117 - - - - - from the world set that the burglar is assumed to know. This subset is then presented to the keeper as an informational prompt. The world set, which is synonymous with the condition set, consists of the following five conditions: Informer: The burglar has been informed that the keeper may lie to him. Burglar inspection: The burglar opened [Container x] [Time] hours ago and observed [Object x] inside. Keeper inspection: The keeper opened [Container x] [Time] hours ago. Outsider inspection: friend of the keeper opened [Container x] [Time] hours ago. Noise: noise was heard coming from [Container x]. Selection Process. Recall the definition of the framework presented in Section 2. The belief estimation predicts the probabilistic beliefs across five distinct conditions. The selfs belief reflects the keepers confidence regarding the occurrence of an event, while the opponents belief represents the burglars confidence in the same events. Since the keeper is provided with list of conditions, we denote the selfs beliefs on these conditions as 1 (known) and others as 0 (unknown). Furthermore, neural network, which models the opponents beliefs, will predict the burglars beliefs based on the ongoing conversation. Subsequently, events for which the burglars belief is 0 will be selected as inputs to the conditional generator for further generation. E.0.4 Experimental Details. The experiments are conducted within first-person, agent-centric framework, wherein two agents powered by large language models (LLMs) are assigned the roles of \"keeper\" and \"burglar\" to engage in interactive dialogues. During prompt construction, each agent is initially provided with rolespecific background information and task objectives via the system prompt. Additionally, the outputs of belief estimation module are incorporated into the system prompt as supplementary contextual information. To preserve conversational coherence and ensure consistent role alignment, the dialogue history between the two agents is interleaved as alternating user and assistant turns within the prompt sequence. Belief estimator Training. We implement the belief estimator using BERT-based architecture. The technical pipeline proceeds as follows: Sentence Encoding: Each utterance in the dialogue is independently encoded using BERT, with the final-layer [CLS] token embedding extracted as the utterance-level representation. Context-Event Fusion: The representations of all utterances are concatenated with event embeddings and passed through self-attention layer to facilitate information integration. Binary Classification: At each event-specific position within the fused representation, the corresponding embedding is fed into multilayer perceptron (MLP) with softmax activation, producing probabilistic output that indicates whether the burglar is aware of the particular event. The accuracy of the belief estimator is presented in Table 4. To better simulate the conditions of actual conversations in the training and test sets, we randomly clipped the final few rounds of each multi-turn conversation. This approach ensures that the belief estimator is trained and tested with less contextual information, thereby enhancing its adaptability to real-world scenarios. E.0.5 Prompt of Conditional Generation. Table 6 provides the prompt used for conditional generation in the CKBG task. MUTUAL FRIEND (MF) We depict the details of performing the MF task here. F.0.1 The MF task. The task of identifying mutual friends between two interlocutors, referred to as MF, was proposed by He et al. [16] and provides an ideal framework for examining the alignment of dialogue acts, where cooperation plays pivotal role. In this task, two players are each given list of friends, with each friend having distinct set of attributes, such as name, hobbies, and school. Importantly, there is exactly one friend common to both players. The Large Language Model (LLM) is tasked with role-playing both players, engaging in dialogue to exchange information over limited number of rounds. The game is considered successfully completed when both players simultaneously confirm the identity of the mutual friend and select the same individual. F.0.2 Details of World Set Designation and Selection Process. The world set is designed as follows: The [Interlocutor] suspects the [Attribute 1] of the mutual friend is [Value 1.1]. Table 6: Prompts used for conditional generation in the CKBG. The content in square brackets \"[]\" denotes the input slots for the belief information and related input variables. Notice: 1. Context: [context] 2. Your opponents belief state: [user_U] 3. Your belief state: [machine_U] Based on the context, the opponents belief state, and your belief state to provide your final choice to the following task: [task]. The [Interlocutor] suspects the [Attribute 1] of the mutual friend is [Value 1.2]. . . . The [Interlocutor] suspects the [Attribute n] of the mutual friend is [Value n.m]. The selection process for applying the Alignment Dialogue Act is as follows: know the [Interlocutor] suspects the [Attribute n] of the mutual friend is [Value n.m]. know The [Interlocutor] knows The [Interlocutor] suspects the [Attribute n] of the mutual friend is [Value n.m]. F.0.3 Experimental Details. We developed symmetric two-agent experimental framework in which two large language model (LLM) agents act as players collaborating toward common objective. Each agent possesses distinct friend list, but both share the same game goal. The initial game context, including the background and friend lists, is provided through system prompt. During the dialogue phase, the conversation history is structured by alternating between user and assistant roles within the prompt sequence, ensuring both conversational coherence and consistent role adherence throughout the interaction. At the end of the dialogue, conditional generation is employed to assess whether mutual friend has been identified, and this judgment is appended to the dialogue history, also presented as system prompt to the LLM. The evaluation phase commences once both players either confirm the identification of mutual friend or reach the maximum dialogue turn limit (20 turns). During this phase, the system prompts each player to select mutual friend from their respective friend lists. If both players select the same individual, the outcome is deemed success. Belief estimator Training. We implement the belief estimator using BERT-based architecture. The estimation of the belief state is essentially binary classification of the utterance-attribute-value triple. The technical pipeline proceeds as follows: Input wrapping: The utterance-attribute-value triple will be concatenated into long sentence, separated by [SEP], begin with [CLS]. For example: \"[CLS] [utterance] [SEP] [attribute] [SEP] [value] [SEP]\" Sentence Encoding: Each sentence is independently encoded through BERT, with the final-layer [CLS] token embedding extracted as the triple-level representation. Binary Classification: For each triple, the sentence representation vector is fed into an MLP with softmax activation, generating probabilistic output indicating whether the attribute of the mutual friend, as perceived by the other party, matches the given value. We have two GPT-3.5 agents play the game on the training set of the Mutual Friends dataset (which samples 1,000 data points) to generate training data for the belief estimator. After each player speaks, the system asks them to provide the attribute values of the mutual friend they currently believe in, which serves as the ground truth label for the training data. To better align with the testing scenario, this ground truth label is fed into the conditional generation module. For each utterance-attribute-value triple labeled as correct, we randomly alter either the attribute or the value to construct negative sample. The training data maintains 1:1 ratio between positive and negative samples. Ultimately, the 1,000 data points sampled from the original training set generated 67,690 training samples. We split the data into training, testing, and validation sets in 0.9:0.05:0.05 ratio. The belief estimator achieved an accuracy of 90.1% on the test set. F.0.4 Prompt of Conditional Generation. Table 7 provides the prompt used for conditional generation in the MF task. CASINO G.0.1 Details of World Set Designation and Selection Process. World Set Designation. In the CaSiNo experiment, we define the World Set as comprising all possible preference rankings of three resources (food, firewood, and water) for both agents. Consequently, the World Set contains ğ´3 3 = 6 distinct events (equivalent to 3! permutations). The belief estimator is operationalized as classifier, tasked with identifying the most probable preference ranking deemed accurate within the World Set, which is then integrated into the prompt. The World Set includes events as follows: The [Negotiator 1]s preference is: the most important thing is water, followed by firewood, and lastly food. The [Negotiator 1]s preference is: the most important thing is water, followed by food, and lastly firewood. The [Negotiator 1]s preference is: the most important thing is firewood, followed by water, and lastly food. The [Negotiator 1]s preference is: the most important thing is firewood, followed by food, and lastly water. The [Negotiator 1]s preference is: the most important thing is food, followed by water, and lastly firewood. The [Negotiator 1]s preference is: the most important thing is food, followed by firewood, and lastly water. The [Negotiator 2]s preference is: the most important thing is water, followed by firewood, and lastly food. The [Negotiator 2]s preference is: the most important thing is water, followed by food, and lastly firewood. Table 7: Prompts used for conditional generation in the MF. The content in square brackets \"[]\" denotes the input slots for the belief information and related input variables. Notice: 1. [name_opponent] currently considers the attributes of the mutual friend to be: **[belief_state_sentence].** 2. You must confirm whether there friend in your friend list that meet the above criteria; only then can they be identified as mutual friend. 3. When describing friend, give all his attribute values. Please provide your utterance directly. The [Negotiator 2]s preference is: the most important thing is firewood, followed by water, and lastly food. The [Negotiator 2]s preference is: the most important thing is firewood, followed by food, and lastly water. The [Negotiator 2]s preference is: the most important thing is food, followed by water, and lastly firewood. The [Negotiator 2]s preference is: the most important thing is food, followed by firewood, and lastly water. The [Negotiator 1]s preference isnt: the most important thing is water, followed by firewood, and lastly food. The [Negotiator 1]s preference isnt: the most important thing is water, followed by food, and lastly firewood. The [Negotiator 1]s preference isnt: the most important thing is firewood, followed by water, and lastly food. The [Negotiator 1]s preference isnt: the most important thing is firewood, followed by food, and lastly water. The [Negotiator 1]s preference isnt: the most important thing is food, followed by water, and lastly firewood. The [Negotiator 1]s preference isnt: the most important thing is food, followed by firewood, and lastly water. The [Negotiator 2]s preference isnt: the most important thing is water, followed by firewood, and lastly food. The [Negotiator 2]s preference isnt: the most important thing is water, followed by food, and lastly firewood. The [Negotiator 2]s preference isnt: the most important thing is firewood, followed by water, and lastly food. The [Negotiator 2]s preference isnt: the most important thing is firewood, followed by food, and lastly water. The [Negotiator 2]s preference isnt: the most important thing is food, followed by water, and lastly firewood. The [Negotiator 2]s preference isnt: the most important thing is food, followed by firewood, and lastly water. Selection Process. In the CaSiNo framework, mixed strategy is employed, where the conditional generation model is provided with two distinct events: one corresponding to the conditions of the Alignment Dialogue Act and the other to the conditions of the Adversarial Dialogue Act . For the Alignment Dialogue Act , only the first 12 conditions in the world set are considered relevant, with the remaining 12 conditions deemed inconsequential. Conversely, for the Adversarial Dialogue Act , only the last 12 conditions in the world set are taken into account. The underlying rationale for the Alignment Dialogue Act is as follows: \"I know you prefer xxx, and am aware that you are cognizant of your preference for xxx\" (assuming mutual awareness of preferences). In contrast, the rationale for the Adversarial Dialogue Act is: \"I know that do not prefer xxx, and am aware that you are unaware of my lack of preference for xxx\" (anticipating the opponents lack of knowledge). G.0.2 Experimental Details. We follow the same prompt strategy as in the experiments of the Conditional Keeper-Burglar Game, in which the two LLM-based agents are required to assume the roles of two negotiators; we undertake expanded experimental investigations on the CaSiNo dataset to systematically evaluate the influence of belief estimation mechanisms on the game of camping negotiations. The classification result of the belief estimator is shown in Table 4, we evaluate the accuracy of the belief estimator through fine-grained analytical framework: For each permutation of the three resources, which inherently contains three pairwise comparative relationships (ğ¶2 3 = 3 combinations). So among all the 6 permutations, we have: Only one permutation is entirely correct (all three pairwise relationships accurate); Only one permutation is entirely erroneous (no accurate pairwise relationships); Two permutations retain two accurate pairwise relationships; Two permutations preserve single accurate pairwise relationship. We define fine-grained accuracy as the ratio of accurate pairwise relationships for every single piece of data. The belief estimator does 6-class classification to select the best permutations for both 2 negotiators. G.0.3 Prompt of Conditional Generation. Table 8 provides the prompt used for conditional generation in the CaSiNo task. FAILURE CASE STUDY Through analysis of failed cases in the CKBG experiment, we identified that one contributing factor to the failures was the agents inability to effectively utilize the provided beliefs during dialogues. As demonstrated in Table 9, the keepers final dialogue failed to incorporate information from the belief, resulting in the burglar remaining unmisled by the keeper. Additionally, the 4th belief provided in Table 9 was incorrect, in which the burglar Peter must know he had opened the opaque plastic pouch 4 hours ago. Table 10 shows failure case in MF, where during the dialogue process, the Bob Agent exhibited hallucination by erroneously believing that one of its friends had hobby of animal fancy. This non-existent persona ultimately led to an incorrect final decision. Table 8: Prompts used for conditional generation in the CaSiNo. The content in square brackets \"[]\" denotes the input slots for the belief information and related input variables. Notice: 1. [opponent_name] thinks that you think [belief_state_self]. 2. [opponent_name] thinks think [belief_state_opponent]. 3. In fact, for you, [belief_state_gt] Please provide your utterance directly. Tables 11 and 12 respectively present two common hallucinations exhibited by GPT-4 on the Mutual Friends (MF) task in the absence of belief-state constraints."
        },
        {
            "title": "I DISCUSSION AND FUTURE WORKS",
            "content": "Apply an LLM for belief estimation. smaller model is more efficient than an LLM, and our experimental results have shown that simple encoder already has the capability of capturing beliefs from conversations. Its possible to use an LLM as belief estimator, but we consider smaller model is enough. Using weak instruct following model. As is noticed, LlaMA not been used to conduct experiments on the Mutual Friends (MF) dataset. Our experimental setup differs from [33] in that no supervised fine-tuning was applied to the LLMs. We demonstrate that the unmodified llama model cannot perform the Mutual Friends (MF) game. Based on Llama-3.1-8b, Llama-2-13b, and Llama-2-7b, we conducted preliminary experiments by sampling 50 data points from the test set. In five experimental settingsw/o belief, rand belief, cot, self-reflect, and BEDA the success rates did not exceed 5%. This outcome is attributed to the larger World Set in the Mutual Friend task, where Llama fails to extract meaningful information from the structured friend list, leading to significant hallucinations during generation. An example was shown in Table 13. Dynamic Construction of the World Set. This work primarily focuses on bridging belief estimation with dialogue acts. Future research will explore the development of dynamic world set. Table 9: Failure case of GPT3.5 on CKBG Background: antique miniature painting is in the plastic pouch ... Homeowner James: Peter, understand your thorough approach, but must insist that the antique miniature painting is actually in the enameled container. Trust me on this one, you wont regret choosing the enameled container. Burglar Peter: appreciate your persistence, James. However, think Ill go ahead and open the opaque plastic pouch to see if the painting is inside. believe its important to explore all options before making final decision. Beliefs provided by estimator (keeper knows but burglar doesnt know): The opaque plastic pouch contains antique miniature painting. The enameled container contains colorful paperclip. The keeper James probably lie to the burglar Peter. The burglar Peter opened the opaque plastic pouch 4 hours ago and saw antique miniature painting in it. The burglars friend Larry opened the enameled container 5 hours ago. Homeowner James: Peter, understand your need to explore all options, but must emphasize that the antique miniature painting is actually in the enameled container. Trust me on this one, youll find what youre looking for in the enameled container. Burglar Peter: STOP The burglar succeeds. Burglar choosed: opaque plastic pouch. The correct answer is: opaque plastic pouch Burglar Win Table 10: Failure case of GPT3.5 on MF ... Alexs belief: (Location Preference: outdoor, School: university of Redlands, Time Preference: morning, Hobby: unknow) Bob: Exactly, Alex! Weve identified our mutual friend based on the matching attributes: Hobby: Animal fancy(close enough to 3D printing) School: University of RedlandsTime Preference: morning Location Preference: outdoor Congratulations on finding this mutual friend with me! Its great that we found perfect match despite the slight difference in hobbies. Do you know their name or any other details about them? Bob choose to stop. Bobs answer: (Time Preference: morning, Location Preference: outdoor, School: university of Redlands, Hobby: unknow(not exist in friends list)) Bobs friends (Hobby: 3D printing, School: Colorado School of Mines, Time Preference: morning, Location Preference: outdoor) (Hobby: Foraging, School: University of Redlands, Time Preference: morning, Location Preference: outdoor) (Hobby: 3D printing, School: Western Kentucky University, Time Preference: morning, Location Preference: outdoor) (Hobby: Sand art, School: Western Kentucky University, Time Preference: morning, Location Preference: outdoor) (Hobby: 3D printing, School: Armstrong State College, Time Preference: morning, Location Preference: outdoor) Table 11: Friend-list comparison hallucination on MF without belief-state constraints apologize for the confusion, but none of my friends have hobby related to Bob: geocaching or work for Mercury Marine. Since we are looking for mutual friend with preference for outdoor activities and not in the afternoon, here is the list of friends have: 1. Hobby: Water sports, Company: Talend, Time Preference: morning, Location Preference: outdoor. 2. Hobby: Sand art, Company: Continental Can Company, Time Preference: evening, Location Preference: outdoor. 3. Hobby: Jogging, Company: Ann Taylor, Time Preference: afternoon, Location Preference: outdoor. 4. Hobby: Water sports, Company: DeVry, Inc., Time Preference: morning, Location Preference: outdoor. 5. Hobby: Water sports, Company: Baxter International, Time Preference: evening, Location Preference: outdoor. 6. Hobby: Sand art, Company: Illumina, Time Preference: evening, Location Preference: outdoor. 7. Hobby: Sand art, Company: Mercury Marine, Time Preference: morning, Location Preference: outdoor. 8. Hobby: Sand art, Company: Molex, Inc., Time Preference: evening, Location Preference: outdoor. 9. Hobby: Parkour, Company: Ann Taylor, Time Preference: morning, Location Preference: outdoor. 10. Hobby: Baton twirling, Company: Lucasfilm, Time Preference: morning, Location Preference: outdoor. 11. Hobby: Baton twirling, Company: Illumina, Time Preference: afternoon, Location Preference: outdoor. 12. Hobby: Water sports, Company: Torry Harris Business Solutions, Time Preference: morning, Location Preference: outdoor. Considering the information youve provided, none of these friends seem to match the criteria for our mutual friend. Could you provide more details about your friends company or any other preferences that might help us identify our mutual friend? apologize for the oversight. Given the new information youve provided and Alex: considering that our mutual friend prefers outdoor activities but not in the afternoon, can see that none of the friends on my initial list match this description. However, do have friend who enjoys jogging, works for Ann Taylor, prefers outdoor activities, and has an afternoon time preference Table 12: Looping dialogue hallucination on MF without belief-state constraints ... dont have friend with Major in Microbiology who prefers indoor locations Bob: and likes to meet in the morning. Lets try another friend from my list. One of my friends has Major in Senior High Education, prefers indoor locations, and likes to meet in the afternoon. dont have friend with Major in Senior High Education who prefers indoor Alex: locations and likes to meet in the afternoon. Lets try another friend from my list. One of my friends has Major in Microbiology, prefers indoor locations, and likes to meet in the morning. dont have friend with Major in Microbiology who prefers indoor locations Bob: and likes to meet in the morning. Lets try another friend from my list. One of my friends has Major in Senior High Education, prefers indoor locations, and likes to meet in the afternoon. dont have friend with Major in Senior High Education who prefers indoor Alex: locations and likes to meet in the afternoon. Lets try another friend from my list. One of my friends has Major in Microbiology, prefers indoor locations, and likes to meet in the morning. ... Table 13: Examples of Mutual Friends (MF) experiment generated by LLaMA 1. Unable to understand the task setup (Llama-2-7b (cot)) Bob (1st round): Great! Lets identify our mutual friend quickly. After checking my list, can confirm that the only friend we have is... Megan! 2. Generate non-existent friend information (Llama-3.1-8b (w/o belief)) Alex (3rd round): ... have another friend that matches the criteria: Friend #99: School: None of the schools mentioned by Bob, but one of them is indeed on my list (Albertson College of Idaho), Major: Computer Engineering, Hobby: Triathlon. No match. Friend #100: School: West Coast University, Major: Philosophy & Religion, Hobby: Flower collecting and pressing. No match. 3. Repeat the same conversation (Llama-2-13b (BEDA )) Alex (10th round): ... do have friend who attends Baylor University, majors in Metallurgical Engineering, and has hobby of Inline skating. ... Bob (11th round): ... do have friend who attends West Liberty State College, majors in Agricultural Business & Management, and has hobby of Dog sport. ... Alex (12th round): ... do have friend who attends Baylor University, majors in Metallurgical Engineering, and has hobby of Inline skating. ... Bob (13th round): ... do have friend who attends West Liberty State College, majors in Agricultural Business & Management, and has hobby of Dog sport. ... ..."
        }
    ],
    "affiliations": [
        "Department of Automation, THU",
        "Institute for Artificial Intelligence, PKU",
        "Institute of Automation, CAS",
        "NLCo, BIGAI",
        "School of Artificial Intelligence, BUPT",
        "Yuanpei College, PKU"
    ]
}