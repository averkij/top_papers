{
    "paper_title": "Voxtral",
    "authors": [
        "Alexander H. Liu",
        "Andy Ehrenberg",
        "Andy Lo",
        "Clément Denoix",
        "Corentin Barreau",
        "Guillaume Lample",
        "Jean-Malo Delignon",
        "Khyathi Raghavi Chandu",
        "Patrick von Platen",
        "Pavankumar Reddy Muddireddy",
        "Sanchit Gandhi",
        "Soham Ghosh",
        "Srijan Mishra",
        "Thomas Foubert",
        "Abhinav Rastogi",
        "Adam Yang",
        "Albert Q. Jiang",
        "Alexandre Sablayrolles",
        "Amélie Héliou",
        "Amélie Martin",
        "Anmol Agarwal",
        "Antoine Roux",
        "Arthur Darcet",
        "Arthur Mensch",
        "Baptiste Bout",
        "Baptiste Rozière",
        "Baudouin De Monicault",
        "Chris Bamford",
        "Christian Wallenwein",
        "Christophe Renaudin",
        "Clémence Lanfranchi",
        "Darius Dabert",
        "Devendra Singh Chaplot",
        "Devon Mizelle",
        "Diego de las Casas",
        "Elliot Chane-Sane",
        "Emilien Fugier",
        "Emma Bou Hanna",
        "Gabrielle Berrada",
        "Gauthier Delerce",
        "Gauthier Guinet",
        "Georgii Novikov",
        "Guillaume Martin",
        "Himanshu Jaju",
        "Jan Ludziejewski",
        "Jason Rute",
        "Jean-Hadrien Chabran",
        "Jessica Chudnovsky",
        "Joachim Studnia",
        "Joep Barmentlo",
        "Jonas Amar",
        "Josselin Somerville Roberts",
        "Julien Denize",
        "Karan Saxena",
        "Karmesh Yadav",
        "Kartik Khandelwal",
        "Kush Jain",
        "Lélio Renard Lavaud",
        "Léonard Blier",
        "Lingxiao Zhao",
        "Louis Martin",
        "Lucile Saulnier",
        "Luyu Gao",
        "Marie Pellat",
        "Mathilde Guillaumin",
        "Mathis Felardos",
        "Matthieu Dinot",
        "Maxime Darrin",
        "Maximilian Augustin",
        "Mickaël Seznec",
        "Neha Gupta",
        "Nikhil Raghuraman",
        "Olivier Duchenne",
        "Patricia Wang",
        "Patryk Saffer",
        "Paul Jacob",
        "Paul Wambergue",
        "Paula Kurylowicz",
        "Philomène Chagniot",
        "Pierre Stock",
        "Pravesh Agrawal",
        "Rémi Delacourt",
        "Romain Sauvestre",
        "Roman Soletskyi",
        "Sagar Vaze",
        "Sandeep Subramanian",
        "Saurabh Garg",
        "Shashwat Dalal",
        "Siddharth Gandhi",
        "Sumukh Aithal",
        "Szymon Antoniak",
        "Teven Le Scao",
        "Thibault Schueller",
        "Thibaut Lavril",
        "Thomas Robert",
        "Thomas Wang",
        "Timothée Lacroix",
        "Tom Bewley",
        "Valeriia Nemychnikova",
        "Victor Paltz",
        "Virgile Richard",
        "Wen-Ding Li",
        "William Marshall",
        "Xuanyu Zhang",
        "Yihan Wan",
        "Yunhao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Voxtral Mini and Voxtral Small, two multimodal audio chat models. Voxtral is trained to comprehend both spoken audio and text documents, achieving state-of-the-art performance across a diverse range of audio benchmarks, while preserving strong text capabilities. Voxtral Small outperforms a number of closed-source models, while being small enough to run locally. A 32K context window enables the model to handle audio files up to 40 minutes in duration and long multi-turn conversations. We also contribute three benchmarks for evaluating speech understanding models on knowledge and trivia. Both Voxtral models are released under Apache 2.0 license."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 4 6 2 3 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present Voxtral Mini and Voxtral Small, two multimodal audio chat models. Voxtral is trained to comprehend both spoken audio and text documents, achieving state-of-the-art performance across diverse range of audio benchmarks, while preserving strong text capabilities. Voxtral Small outperforms number of closedsource models, while being small enough to run locally. 32K context window enables the model to handle audio files up to 40 minutes in duration and long multiturn conversations. We also contribute three benchmarks for evaluating speech understanding models on knowledge and trivia. Both Voxtral models are released under Apache 2.0 license. Webpage: https://mistral.ai/news/voxtral/ Model weights: Evals: https://huggingface.co/mistralai/Voxtral-Mini-3Bhttps://huggingface.co/mistralai/Voxtral-Small-24B-2507 https://huggingface.co/collections/mistralai/speech-evals-6875e9b26c78be4a081050f"
        },
        {
            "title": "Introduction",
            "content": "This paper describes Voxtral Mini and Voxtral Small, pair of multimodal language models trained to understand both speech and text, released with open-weights under an Apache 2.0 license. Voxtral is pretrained on large-scale corpus of audio and text documents, and subsequently instruction tuned on real and synthetic data. It is capable of responding directly to audio (or text) and answering questions about audio files. With 32K token context window, Voxtral is capable of processing audio files up to 40 minutes long. Compared with similarly sized models in the same evaluation setting, we find that Voxtral delivers strong audio reasoning capabilities without sacrificing text-only performance. Its performance is state-of-the-art for speech transcription and translation, outperforming other open-weights and closed models. In speech question-answering (QA) and summarization, it performs comparably with closed models of similar price class, such as GPT-4o mini [Hurst et al., 2024] and Gemini 2.5 Flash [Comanici et al., 2025]. During evaluation of Voxtral and other models, we found that the existing ecosystem of speech evaluations lacked breadth and standardization; the majority of previous work focused on evaluation of transcription and translation quality, and less on other understanding tasks. In Section 3.4, we present evaluations that measure wider range of speech comprehension and reasoning tasks. Our primary contributions are: Two open-weights audio models with state-of-the-art transcription and multilingual speech understanding for audio durations up to their 32K context window Native function calling support with audio Evaluation benchmarks that measure speech understanding and reasoning The report is structured as follows: First, we outline our modeling choices. Next, we describe methods for pretraining, post-training, and response quality enhancement. Finally, we present benchmark results along with architectural and data ablations."
        },
        {
            "title": "2 Modeling",
            "content": "Voxtral is based on the Transformer architecture [Vaswani et al., 2017], consisting of three components: an audio encoder to process speech inputs, an adapter layer to downsample audio embeddings, and language decoder to reason and generate text outputs. The overall architecture is depicted in Figure 1. Figure 1: Voxtral Architecture. The audio encoder processes the speech input, attending to 30-second chunks of audio independently. The audio embeddings are concatenated at the output, and downsampled by factor of 4x in the audio-language adapter. The multimodal LLM decoder auto-regressively predicts text tokens, conditional on the audio and text inputs."
        },
        {
            "title": "2.1 Audio Encoder",
            "content": "The audio encoder is based on Whisper large-v3 [Radford et al., 2023]. raw audio waveform is first mapped to log-Mel spectrogram [Davis and Mermelstein, 1980] with 128 Mel-bins and 160 hop-length. Within the Whisper encoder, the spectrogram passes through convolutional stem that downsamples its temporal resolution by factor of two, after which it is fed into stack of bidirectional self-attention layers. The resulting audio embeddings have frame rate of 50 Hz. Whisper has fixed receptive field of 30 seconds. To accommodate audio sequences exceeding this duration, we compute the log-Mel spectrogram for the entire audio, but restrict the encoder to independently process each 30 second chunk. The absolute positional encodings are reset for each chunk, and chunks from the same audio are partitioned into batch axis. Within the encoders attention layers, this approach is functionally equivalent to chunk-wise attention [Zhang et al., 2023], which mitigates the computational overhead for longer audio inputs and enhances length generalization. The embeddings computed from each chunk are concatenated at the output stage, forming unified representation of the complete audio sequence. Due to its fixed receptive field, Whisper also pads short audios to 30 seconds. In Section 5.1, we investigate removing this padding requirement to allow continuous audio lengths. However, empirical results demonstrated decline in performance, even when tuning the encoder to adapt. Consequently, we maintain the practice of padding all audio inputs to the next multiple of 30 seconds."
        },
        {
            "title": "2.2 Adapter Layer",
            "content": "The high frame-rate of the audio encoder would result in long sequence-lengths through the language decoder. For example, 30 minute audio at 50Hz has sequence length of 90k tokens, leading to high memory and slow inference. To circumvent this, we append an additional MLP layer at the audio encoder outputs that is responsible for downsampling the audio embeddings. In Section 5.2, we show downsampling factor of 4x yields the best trade-off between sequence-length and performance. This results in an effective frame-rate of 12.5Hz, enabling Voxtral to gracefully handle audios up to 40 minutes with context-length of 32k tokens. 2 Table 1: Parameter Counts. Number of parameters for Voxtral Mini and Small."
        },
        {
            "title": "Mini\nSmall",
            "content": "640M 640M 25M 52M 400M 670M 3.6B 22.9B 4.7B 24.3B"
        },
        {
            "title": "2.3 Language Decoder",
            "content": "We release two variants of Voxtral: Mini and Small. Voxtral Mini is built on top of Ministral 3B [Mistral AI Team, 2024], an edge-focused model that delivers competitive performance with small memory footprint. Voxtral Small leverages the Mistral Small 3.1 24B backbone [Mistral AI Team, 2025], giving strong performance across range of knowledge and reasoning tasks. Table 1 decomposes the number of parameters in each checkpoint based on the sub-components."
        },
        {
            "title": "3 Methodology",
            "content": "We train the model in three phases: pretraining, supervised finetuning, and preference alignment. Each phase is described separately below. Finally, we describe our evaluation protocol for speech understanding tasks."
        },
        {
            "title": "3.1 Pretraining",
            "content": "The pretraining stage of Voxtral is designed to introduce speech to the language decoder, complementary to the existing modality of text. Given an audio dataset with text transcriptions, we first chunk the audio into short segments together with their corresponding transcription, forming parallel audio-text pairs: (A1, T1), (A2, T2), . . . , (AN , TN ). The segmentation boundaries are defined by upstream voice activity detection and diarization models. If transcripts are unavailable, we pseudo-label the audio with an ASR model. Similar to prior works [Nguyen et al., 2025, Zeng et al., 2024], we define two patterns that combine audio and text into training samples for the model: audio-to-text repetition and cross-modal continuation. The audio-to-text repetition pattern is defined as an audio segment An followed by the corresponding transcription Tn. training sample consists of single audio-text pair (An, Tn). This formulation mimics speech recognition and is used to explicitly teach the model speech-to-text alignment. On the other hand, the cross-modal continuation pattern is designed to implicitly align the speech and text modalities through modality-invariant context modeling. Specifically, for each audio segment An, the corresponding text is the proceeding text segment in the sequence Tn+1. In addition, training sample is composed by interleaving audio and text for multiple consecutive segments: (A1, T2, A3, T4, . . . , AN 1, TN ). This structure resembles tasks like QA or conversation, where the model must maintain discourse continuity across modalities. Since we use two different data patterns, the proceeding text segment for given audio segment is ambiguous; both repeat and continuation are valid. To eliminate ambiguity, we introduce two special tokens to specify the expected output: <repeat> for repetition and <next> for continuation. These tokens are used for pattern indication during training and as part of the prompt during inference to control model behavior. The two pretraining patterns are shown with their special tokens in Figure 2. Note that we treat each audio-transcription pair as standalone sequence wrapped with <bos>/<eos> without previous context. During pretraining, we balance the two patterns evenly. We demonstrate in Section 5.3 that this balanced approach is essential; the audio-to-text repetition pattern drives transcription performance, while the cross-modal continuation pattern prepares the model for speech understanding tasks that require deeper reasoning and context integration, such as audio-based QA or dialogue. To preserve text capabilities, we also include text pretraining data in the data mixture. 3 Figure 2: Pretraining patterns. single audio-text example (A, ) is first segmented into set of audiotext pairs {(An, Tn)}N n=1, based on the timestamps and transcriptions returned by segmentation stage. For the audio-to-text repetition pattern, given audio An is repeated in the text space Tn. For the cross-modal continuation pattern, each audio An is followed by its subsequent text Tn+1. The task is signaled to the model by the <repeat> and <next> special tokens respectively. For the first pass over the data mixture, we freeze the audio encoder and language decoder, training only the adapter. We found this warm-up stage beneficial for speech understanding evaluations, whereas speech recognition results are similar with and without warm-up. We also perform one pretraining run on the Mini scale with just the audio-to-text repetition pattern. We call this model Voxtral Mini Transcribe, and compare it to other ASR-only models in Section 4.1."
        },
        {
            "title": "3.2 Supervised Finetuning",
            "content": "In post-training, our primary objective is to preserve or slightly enhance the transcription capabilities established during pretraining, while simultaneously extending the models proficiency over range of speech understanding tasks. We also develop robust instruction-following behavior, irrespective of whether user inputs are in audio or text form. Our speech understanding data falls into two categories: tasks where audio is provided as context and the assistant responds to text queries, and tasks where the assistant responds directly to audio inputs. Both categories rely significantly on synthetic data. Audio Context, Text Query To create synthetic data for tasks involving audio context paired with text queries, we utilize long-form audio data (segments up to approximately 40 minutes) with corresponding transcripts and language identification metadata. Transcripts are paired with tailored prompts and fed into an LLM (Mistral Large), which then generates question-answer pairs related to the audio content. The prompts explicitly instruct the LLM to frame both questions and answers as though they arise from auditory comprehension rather than text analysis, thereby encouraging natural responses from the downstream audio model. To achieve data diversity and richness, we vary question types, including straightforward factual inquiries, \"needle-in-haystack\" retrieval tasks, and reasoning-intensive problems. Moreover, to minimize repetitive question styles, the LLM generates multiple candidate question-answer pairs per audio segment, from which we sample single pair for inclusion in the post-training dataset. While we typically ensure that the question-answer pairs match the language of the original audio and transcript, we occasionally instruct Mistral Large to produce pairs in different languages to enable QA for audios in languages the user does not speak. Additionally, we allocate another portion of the long-form audio data for synthetic summarization and translation data. For translation tasks, we leverage language identification metadata to select target language different from the original audio language. To mitigate overfitting to narrow range of user message patterns, we sampled from large, manually curated set of plausible user requests. Audio-Only Input For scenarios in which the user provides only audio input, we adapt existing text supervised finetuning data, including function calling datasets, by converting text user messages into synthetic audio using text-to-speech (TTS) model. However, reliance solely on TTS-generated audio leads to poor generalization to genuine human speech, particularly accented voices, manifesting most commonly in erroneous transcription of conversational prompts rather than appropriate continuation. To address this limitation, we extract questions from long-form ASR data that can be adequately answered through general world knowledge, thus requiring no additional audio context. We then isolate audio excerpts containing these questions and generate corresponding text answers using Mistral Large. This process yields datasets consisting of genuine human speech questions paired with text answers. Speech recognition is distinctive use case characterized by an unambiguous task, rendering the text prompt redundant. To address this, we introduce dedicated \"transcribe mode,\" signaled via new special token. This mode explicitly instructs the model to perform transcription tasks, thereby eliminating the need for text prompt."
        },
        {
            "title": "3.3 Preference Alignment",
            "content": "Direct Preference Optimization (DPO) [Rafailov et al., 2024] offers lightweight alternative to full RLHF by learning directly from pairwise preferences. We also adopt its online variant [Guo et al., 2024], where for each example, we sample two candidate responses from the current policy with temperature =0.5. To rank responses, we take the entire conversation, replace the audio with its transcription, and leverage text-based reward model. Although the reward model only has access to the audio transcription - rather than the raw audio itself - it is able to capture semantics, style, and factual coherence from this information, attributes that transfer to the generated text response. Our Online DPO implementation utilizes the sampling and reward infrastructure that powered the Magistral [Mistral-AI et al., 2025] series. We apply DPO and Online DPO to both Voxtral Mini and Small, for which we present results in Section 5.4. While both DPO and Online DPO helped improve the response quality, the online variant was more effective."
        },
        {
            "title": "3.4 Evaluation",
            "content": "In addition to standard benchmarks for speech transcription, translation, and understanding - detailed in Sections A.1 and A.2 - we create our own test sets. These sets build upon existing research and evaluate model attributes that are typically underrepresented, particularly long-context QA. Speech-Synthesized Benchmarks To evaluate spoken-language understanding, prior works take existing text benchmarks and synthesize the text prompt into speech [Nachmani et al., 2024, Chen et al., 2024]. We extend these test suites by creating speech-synthesized versions of three established text benchmarks: GSM8K [Cobbe et al., 2021], TriviaQA [Joshi et al., 2017], and MMLU [Hendrycks et al., 2020]. The first step in creating these benchmarks involves filtering to only include prompts that are viable as speech-synthesized inputs, similar to Fang et al. [2024]. For every example, we classify it into one of three categories with Mistral Large: Verbalizable: plain wording or simple numerals. No re-write necessary. Verbalizable with Rewrite: math, code, or symbols, that can be deterministically rewritten into speech-friendly text. For example, digits are converted to spelled-out form, acronyms expanded, markdown removed. The specific prompt used to achieve this is outlined in Appendix A.3. Non-Verbalizable: text that cannot be naturally spoken, such as tables, figures or lengthy math and code, is discarded. Once the valid set of examples is established, we synthesize each one individually using TTS engine. To ensure diversity in speakers, we randomly sample speaker embeddings from diverse set, trimmed to six-second clips and filtered to only include single-speaker utterances. For each prompt, we sample speaker embedding from this pool and generate the corresponding audio input using the TTS engine. Since the model output is in the text-space, scoring the generations requires no additional modifications. We are releasing the synthesized evaluations under permissive license and encourage their adoption as standard benchmarks for speech understanding. 5 Speech Understanding (SU) Benchmark We develop an internal benchmark that measures the ability of models to answer questions about audios in helpful manner. The audio files range up to 19 minutes in duration, assessing understanding on moderately long audio contexts. We use an LLM as judge, which has access to transcription of the audio, the question, reference answer, and the proposed answer. The LLM judge returns two complementary metrics: 1. LLM_JUDGE_SCORE: binary helpfulness indicator. The score is 1 if the answer is deemed correct and helpful to the users question, 0 otherwise. 2. GRADE_LLM_JUDGE_SCORE: 05 quality grade. score of 0 means the answer is completely wrong, unhelpful, and poorly written; 5 denotes that it is factually correct, well-reasoned, and clearly presented. Intermediate values reflect partial correctness, clarity, and overall usefulness, as instructed in the grading prompt. During evaluation, we independently judge each answer multiple times to capture sampling variability. The judge prompts are provided in A.4."
        },
        {
            "title": "4 Results",
            "content": "We evaluate Voxtral on range of speech recognition, translation, speech understanding, speech function calling and text benchmarks. We compare the model to GPT-4o mini Audio (/Transcribe) and Gemini 2.5 Flash, as well as Scribe and Whisper large-v3 on speech recognition tasks."
        },
        {
            "title": "4.1 Speech Recognition",
            "content": "Figure 3 plots the macro-averaged word error rates (WER) on four benchmarks: English Short-Form, English Long-Form, Mozilla Common Voice 15.1 (MCV) [Ardila et al., 2020] and FLEURS [Conneau et al., 2022]. We compute the macro-average across tasks for English Short and Long-Form, and languages for MCV and FLEURS. Voxtral Small achieves state-of-the-art transcription results on English Short-Form and MCV, beating all open and closed-source models. Voxtral Mini Transcribe performs competitively with much larger closed-source models, surpassing GPT-4o mini Transcribe and Gemini 2.5 Flash across all tasks. full breakdown of English and multilingual word error rates are provided in A.1. Figure 3: Speech Recognition Benchmarks. Macro-average WER results across tasks. Voxtral Small outperforms all open and closed-source models on English Short-Form and MCV. Voxtral Mini Transcribe beats GPT-4o mini Transcribe and Gemini 2.5 Flash in every task."
        },
        {
            "title": "4.2 Speech Translation",
            "content": "We evaluate Voxtral on the FLEURS Speech Translation benchmark. We show BLEU scores for subset of source/target pairs in Figure 4. Voxtral Small achieves state-of-the-art translation scores in every source/target combination. 6 Figure 4: FLEURS Translation. BLEU scores for source/target language pairs on the FLEURS Translation benchmark. Voxtral Small achieves state-of-the-art for every combination of languages."
        },
        {
            "title": "4.3 Speech Understanding",
            "content": "We evaluate Voxtral on range of public Speech QA benchmarks, such as Llama QA [Nachmani et al., 2024] and Openbook QA [Chen et al., 2024], as well as the speech-synthesized subsets of standard Text Understanding benchmarks. We also evaluate on our in-house speech understanding (SU) benchmark, consisting of in-the-wild audio examples with challenging QA-style prompts. Figure 5 highlights that Voxtral Small performs competitively with closed-source models, beating GPT-4o mini Audio on three of the seven tasks. Figure 5: Speech Understanding Benchmarks. We report the accuracy across three speech understanding benchmarks and three synthesized speech subsets of text benchmarks. Voxtral Small is competitive with closedsource models, surpassing GPT-4o mini Audio on three of the seven benchmarks."
        },
        {
            "title": "4.4 Text Benchmarks",
            "content": "Figure 6 compares the performance of Voxtral Mini and Small to the text-only Mistral Small 3.1 model. Voxtral Small maintains performance across text-benchmarks, making it suitable drop-in replacement for both text and audio tasks. 7 Figure 6: Text-Only Benchmarks. We report the accuracy across five standard text understanding benchmarks. Voxtral Small performs comparably to Mistral Small 3.1, highlighting its strong text capabilities."
        },
        {
            "title": "5 Analysis",
            "content": "In this Section, we share results and analyses for two architectural ablations, the pretrain pattern format, and improvements from Online DPO."
        },
        {
            "title": "5.1 To Pad or Not To Pad",
            "content": "Whisper pads short audios to 30-seconds. We investigate whether this padding constraint is necessary during pre-training, under the setting that the encoder weights are trained in order to adapt to the new configuration. Figure 8 plots subset of ASR and speech understanding results for models trained with and without padding. Disabling padding incurs almost no penalty on FLEURS English, however there is 0.5% WER degradation on French. The 3-Shot Accuracy on Llama QA is comparable over the course of training for the two runs. To achieve the best possible speech recognition scores without compromise to speech understanding, we opt to maintain padding in the audio encoder. Figure 7: Effect of Padding. Word error rate results on FLEURS English (left) and FLEURS French (middle), alongside 3-shot Accuracy on Llama QA (right) for models trained with and without 30-second padding."
        },
        {
            "title": "5.2 Adapter Downsampling",
            "content": "The baseline audio encoder operates at frame-rate of 50 Hz. To reduce decoder computation and memory, we insert an MLP adapter layer that downsamples the audio embeddings along the temporal axis. We experiment with target frame-rates of 50, 25, 12.5 and 6.25 Hz, corresponding to downsampling factors of 1x, 2x, 4x and 8x. Figure 8 plots the WER on FLEURS English and French, as well as 3-Shot Accuracy on Llama QA. For 25 and 12.5 Hz, there is little degradation on ASR benchmarks. However, for 6.25 Hz, there 8 is penalty of over 1% on FLUERS French. On Llama QA, 12.5 Hz surpasses the 50 Hz baseline, achieving score 1.5% higher. We hypothesize that at 12.5 Hz, each audio-embedding encodes similar amount of information as text-embedding in the language decoder backbone, leading to superior understanding performance. Based on the trade-off between sequence-length, ASR and speech-understanding performance, we select 12.5 Hz as the optimal frame-rate for Voxtral. Figure 8: Effect of Downsampling. Word error rate results on FLEURS English (left) and FLEURS French (middle), alongside 3-shot Accuracy on Llama QA (right) for various frame-rates, achieved by increasing the downsampling factor by powers of 2."
        },
        {
            "title": "5.3 Pre-Training Patterns",
            "content": "Recall that we leverage two data patterns during pretraining: audio-to-text repetition and crossmodal continuation. Figure 9 demonstrates how changing the ratio of these two patterns affects ASR and speech understanding. To better understand the underlying capabilities of the cross-modal continuation pattern for ASR, we evaluate it on the 3-Shot version of the FLEURS ASR task, which is more aligned with the multi-turn pattern presented during training. Including just the audio-to-text repetition pattern results in strong ASR performance, at the expense of nearly zero-performance on Llama QA. Conversely, training on just the cross-modal continuation pattern yields strong Llama QA performance, but WER of nearly 60% on ASR. Balancing the two tasks with equal ratios achieves ASR and Llama QA performance comparable to the runs with single pattern. Thus, we sample each pattern with equal probability during pretraining. Figure 9: Pattern Proportions. Word error rate results on FLEURS English (left) and FLEURS French (middle), alongside 3-shot Accuracy on Llama QA (right) for varying proportions of pretrain patterns."
        },
        {
            "title": "5.4 DPO and Online DPO",
            "content": "Table 2 shows the LLM Judge and Grade scores on the SU Benchmark for the Voxtral SFT, DPO and Online DPO checkpoints. Each answer is independently judged ten times and we report the mean standard deviation. For both Mini and Small, DPO and Online DPO improve response quality metrics relative to the SFT baselines. Qualitative inspectionincluding informal vibe checksshows that the Voxtral Mini Online DPO variant delivers crisper grounding, fewer hallucinations, and generally more helpful responses, so we are releasing it as the public Voxtral Mini checkpoint. 9 For Voxtral Small, we saw substantial gains in response quality score as measured by the Speech Understanding Benchmark, but they are accompanied by slight regression on the English short-form benchmarks. Hence, the default checkpoint remains the SFT model. We aim to release an Online DPO Voxtral Small model which does not regress on those ASR metrics in the near future. Table 2: Response Improvements with Online DPO. Response quality on the internal SU benchmark (mean SD over ten trials), as well as the macro-average WER on the English short-form test sets. The differences in scores for other tasks were not significant. Hence, we omit them from this table. Note that GPT-4o mini Audio does not support transcription. Model % LLM Judge Grade En Short WER Voxtral Mini SFT Voxtral Mini Offline DPO Voxtral Mini Online DPO Voxtral Small SFT Voxtral Small Offline DPO Voxtral Small Online DPO GPT-4o mini Audio Gemini 2.5 Flash 83.47 2.17 84.91 3.21 85.59 3.77 86.61 0.96 87.29 1.65 88.31 2.03 80.00 2.97 88.64 2.28 3.92 0.04 3.92 0.08 4.08 0.07 4.16 0.03 4.19 0.04 4.38 0.06 3.97 0.05 4.54 0. 6.77 6.78 6.79 6.31 6.32 6.50 - 8."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presented Voxtral Mini and Voxtral Small, pair of open-weights audio chat models. It demonstrated their capabilities in understanding spoken audio and text, both on existing and new benchmarks. Their strengths across wide array of speech tasks, strong instruction following, and multilingual prowess make them highly versatile for complex multimodal tasks. Both models are released under the Apache 2.0 license."
        },
        {
            "title": "Core contributors",
            "content": "Alexander H. Liu, Andy Ehrenberg, Andy Lo, Clément Denoix, Corentin Barreau, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, Sanchit Gandhi, Soham Ghosh, Srijan Mishra, Thomas Foubert"
        },
        {
            "title": "Contributors",
            "content": "Abhinav Rastogi, Adam Yang, Albert Q. Jiang, Alexandre Sablayrolles, Amélie Héliou, Amélie Martin, Anmol Agarwal, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Chris Bamford, Christian Wallenwein, Christophe Renaudin, Clémence Lanfranchi, Darius Dabert, Devendra Singh Chaplot, Devon Mizelle, Diego de las Casas, Elliot Chane-Sane, Emilien Fugier, Emma Bou Hanna, Gabrielle Berrada, Gauthier Delerce, Gauthier Guinet, Georgii Novikov, Guillaume Martin, Himanshu Jaju, Jan Ludziejewski, Jason Rute, JeanHadrien Chabran, Jessica Chudnovsky, Joachim Studnia, Joep Barmentlo, Jonas Amar, Josselin Somerville Roberts, Julien Denize, Karan Saxena, Karmesh Yadav, Kartik Khandelwal, Kush Jain, Lélio Renard Lavaud, Léonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Marie Pellat, Mathilde Guillaumin, Mathis Felardos, Matthieu Dinot, Maxime Darrin, Maximilian Augustin, Mickaël Seznec, Neha Gupta, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Rémi Delacourt, Romain Sauvestre, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Shashwat Dalal, Siddharth Gandhi, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Robert, Thomas Wang, Timothée Lacroix, Tom Bewley, Valeriia Nemychnikova, Victor Paltz, Virgile Richard, Wen-Ding Li, William Marshall, Xuanyu Zhang, Yihan Wan, Yunhao Tang"
        },
        {
            "title": "References",
            "content": "Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. Common Voice: MassivelyMultilingual Speech Corpus, 2020. URL https://arxiv.org/abs/1912.06670. Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Yujun Wang, Zhao You, and Zhiyong Yan. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. arXiv e-prints, art. arXiv:2106.06909, June 2021. Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, and Haizhou Li. VoiceBench: Benchmarking LLM-Based Voice Assistants. arXiv preprint arXiv:2410.17196, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word Problems. arXiv, October 2021. doi: 10.48550 /arXiv.2110.14168. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech, 2022. URL https://arxiv.org/abs/2205.12446. Steven Davis and Paul Mermelstein. Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences. IEEE Transactions on Acoustics, Speech, and Signal Processing, 28(4):357366, 1980. Miguel Del Rio, Natalie Delworth, Ryan Westerman, Michelle Huang, Nishchal Bhandari, Joseph Palakapilly, Quinten McNamara, Joshua Dong, Piotr Zelasko, and Miguel Jetté. Earnings-21: Practical Benchmark for ASR in the Wild. In Proc. Interspeech 2021, pages 34653469, 2021. doi: 10.21437/Interspeech.2021-1915. Miguel Del Rio, Peter Ha, Quinten McNamara, Corey Miller, and Shipra Chandra. Earnings-22: Practical Benchmark for Accents in the Wild. arXiv e-prints, art. arXiv:2203.15591, March 2022. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. LLaMAOmni: Seamless Speech Interaction with Large Language Models. arXiv, September 2024. doi: 10.48550/arXiv.2409.06666. J.J. Godfrey, E.C. Holliman, and J. McDaniel. SWITCHBOARD: telephone speech corpus for research and development. In [Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 517520 vol.1, 1992. doi: 10.1109/IC ASSP.1992.225858. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu Blondel. Direct language model alignment from online ai feedback, 2024. URL https://arxiv.org/abs/2402.04792. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. arXiv, September 2020. doi: 10.48550/arXiv.2009.03300. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 11 Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. ACL Anthology, pages 16011611, July 2017. doi: 10.18653/v1/P17-1147. Mistral-AI, Abhinav Rastogi, Albert Q. Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, Léonard Blier, Lucile Saulnier, Matthieu Dinot, Maxime Darrin, Neha Gupta, Roman Soletskyi, Sagar Vaze, Teven Le Scao, Yihan Wang, Adam Yang, Alexander H. Liu, Alexandre Sablayrolles, Amélie Héliou, Amélie Martin, Andy Ehrenberg, Anmol Agarwal, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Chris Bamford, Christian Wallenwein, Christophe Renaudin, Clémence Lanfranchi, Darius Dabert, Devon Mizelle, Diego de las Casas, Elliot Chane-Sane, Emilien Fugier, Emma Bou Hanna, Gauthier Delerce, Gauthier Guinet, Georgii Novikov, Guillaume Martin, Himanshu Jaju, Jan Ludziejewski, Jean-Hadrien Chabran, Jean-Malo Delignon, Joachim Studnia, Jonas Amar, Josselin Somerville Roberts, Julien Denize, Karan Saxena, Kush Jain, Lingxiao Zhao, Louis Martin, Luyu Gao, Lélio Renard Lavaud, Marie Pellat, Mathilde Guillaumin, Mathis Felardos, Maximilian Augustin, Mickaël Seznec, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patrick von Platen, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Pavankumar Reddy Muddireddy, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Romain Sauvestre, Rémi Delacourt, Sanchit Gandhi, Sandeep Subramanian, Shashwat Dalal, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Thibault Schueller, Thibaut Lavril, Thomas Robert, Thomas Wang, Timothée Lacroix, Valeriia Nemychnikova, Victor Paltz, Virgile Richard, Wen-Ding Li, William Marshall, Xuanyu Zhang, and Yunhao Tang. Magistral, 2025. URL https://arxiv.org/abs/2506.109 10. Mistral AI Team. Un Ministral, des Ministraux, October 2024. URL https://mistral.ai/news/ ministraux. Accessed: 2025-07-11. Mistral AI Team. Mistral Small 3.1, March 2025. URL https://mistral.ai/news/mistral-s mall-3-1. Accessed: 2025-07-11. Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich. Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM, 2024. URL https: //arxiv.org/abs/2305.15255. Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta Costa-Jussa, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, et al. Spirit-lm: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics, 13:3052, 2025. Patrick K. ONeill, Vitaly Lavrukhin, Somshubra Majumdar, Vahid Noroozi, Yuekai Zhang, Oleksii Kuchaiev, Jagadeesh Balam, Yuliya Dovzhenko, Keenan Freyberg, Michael D. Shulman, Boris Ginsburg, Shinji Watanabe, and Georg Kucsko. SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition. In Proc. Interspeech 2021, pages 14341438, 2021. doi: 10.21437/Interspeech.2021-1860. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210, 2015. doi: 10.1109/ICASSP.2015.7178964. Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. MLS: Large-Scale Multilingual Dataset for Speech Research. arXiv preprint arXiv:2012.03411, 2020. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust Speech Recognition via Large-Scale Weak Supervision. In International Conference on Machine Learning, pages 2849228518. PMLR, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. Advances in Neural Information Processing Systems, 30, 2017. Emmanuel Vincent, Shinji Watanabe, Aditya Arie Nugraha, Jon Barker, and Ricard Marxer. An Analysis of Environment, Microphone and Data Simulation Mismatches in Robust Speech Recognition. Comput. Speech Lang., 46(C):535557, nov 2017. doi: 10.1016/j.csl.2016.11.005. URL https://doi.org/10.1016/j.csl.2016.11.005. ISSN 0885-2308. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. VoxPopuli: Large-Scale Multilingual Speech In ProCorpus for Representation Learning, Semi-Supervised Learning and Interpretation. ceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 9931003, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.80. URL https://aclanthology.org/2021.acl-long.80. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, and Jie Tang. Scaling speech-text pre-training with synthetic interleaved data. arXiv preprint arXiv:2411.17607, 2024. Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar, Daniel S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana Ramabhadran, Tara Sainath, Pedro Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Françoise Beaufays, and Yonghui Wu. Google usm: Scaling automatic speech recognition beyond 100 languages, 2023. URL https://arxiv.org/abs/2303.01037."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Speech Recognition - Full Results Table 3 shows task-breakdown of short-form English speech recognition results for LibriSpeech Test Clean [Panayotov et al., 2015], LibriSpeech Test Other, GigaSpeech [Chen et al., 2021], VoxPopuli [Wang et al., 2021], SwitchBoard [Godfrey et al., 1992], CHiME-4 [Vincent et al., 2017] and SPGISpeech [ONeill et al., 2021]. For English long-form, we take the one-hour long earnings calls from Earnings-21 [Del Rio et al., 2021] and Earnings-22 [Del Rio et al., 2022], and segment them into shorter, 10 minute variants. This is required to ensure that the full audio fits in transcription request payload to closed-source providers. Table 3: English speech recognition results by task. We report short-form scores for LibriSpeech Test Clean (LS-C), LibriSpeech Test Other (LS-O), GigaSpeech (GS), VoxPopuli (VP), SwitchBoard (SB), CHiME-4 (C-4) and SPGISPeech (SPGI). We report long-form scores for Earnings-21 10m (E21 10m) and Earnings-22 10m (E22 10m). Model LS-C LS-O Short-Form VP GS SB C-4 SPGI Long-Form E21 10m E22 10m Whisper large-v3 GPT4o mini Transcribe Gemini 2.5 Flash ElevenLabs Scribe Voxtral Mini Voxtral Mini Transcribe Voxtral Small 1.84 1.92 2.97 1.80 1.86 1.57 1.53 3.66 4.70 6.15 3.44 4.04 3.21 3.14 11.60 14.80 10.99 10. 10.68 10.04 10.27 9.58 7.34 7.84 6.95 6.85 6.78 6.62 13.14 17.31 9.57 10.62 11.32 11.35 11.09 10.88 11.35 14.79 8. 10.59 10.03 9.64 3.15 4.51 4.00 3.16 2.19 2.04 1.89 9.88 10.09 8.09 7.39 9.62 9.52 9.55 13.07 12.27 10.80 9. 11.84 12.18 12.48 Tables 4, 5 and 6 show the per-language breakdown of WER scores for the FLEURS, Mozilla Common Voice and Multilingual LibriSpeech [Pratap et al., 2020] benchmarks, respectively. Table 4: Per-language WER scores for FLEURS Arabic, Dutch, English, French, German, Hindi, Italian, Portuguese and Spanish. Model Whisper large-v3 GPT-4o mini Transcribe Gemini 2.5 Flash Scribe Voxtral Mini Voxtral Mini Transcribe Voxtral Small ar 15.44 14.02 25.25 11.58 25.40 14.64 13.44 nl 5.87 5.54 6.20 4.63 6.27 4.89 4. en 4.00 3.19 4.64 3.29 3.77 3.61 3.35 fr 5.55 4.51 6.17 5.07 4.87 4.22 4. de 5.46 3.76 4.74 4.78 4.40 3.54 3.38 hi 28.87 12.36 6.76 5.67 9.26 10.32 7. it 2.71 2.02 2.21 1.48 2.51 2.31 2.62 pt es 3.90 3.54 4.23 4. 3.76 3.57 3.79 2.81 2.58 3.17 3.13 3.52 2.75 2.72 Table 5: Per-language WER scores for MCV Arabic, Dutch, English, French, German, Hindi, Italian, Portuguese and Spanish. For fairness, we omit Arabic from the macro-average in Figure 3, since all models score in excess of 45%. Model Whisper large-v3 GPT-4o mini Transcribe Gemini 2.5 Flash Scribe Voxtral Mini Voxtral Mini Transcribe Voxtral Small ar 50.58 51.68 53.62 47.03 63.98 62.01 61.97 nl 5.83 6.47 5.73 2. 6.03 4.71 3.98 en fr 22.91 13.15 12.31 6.59 10.22 8.25 8.58 11.33 10.75 11.86 5. 8.92 7.29 6.18 de 6.25 6.72 7.25 3.52 6.07 4.85 3.74 hi 46.75 39.06 11.09 17. 12.74 10.42 9.01 it 6.81 6.16 6.64 2.99 5.91 4.37 3.96 pt es 7.17 9.95 9.42 5.46 7.76 6.70 6.43 5.66 6.10 5.88 3.27 4.98 3.96 3.31 14 Table 6: Per-language WER scores for MLS Dutch, French, German, Italian, Portuguese and Spanish. Model Whisper large-v3 GPT-4o mini Transcribe Gemini 2.5 Flash Scribe Voxtral Mini Voxtral Mini Transcribe Voxtral Small nl 9.19 8.62 8.33 38.01 10.09 9.63 9. fr 5.09 4.77 6.82 5.80 5.28 4.14 3.73 de 5.72 5.44 6.52 9.81 7.09 5.64 5. it pt es 9.78 10.68 10.97 12.38 11.30 9.28 8.44 7.03 5.67 7.14 15. 6.72 5.17 5.85 3.89 4.28 4.39 8.97 5.12 3.87 3.62 A.2 Speech Understanding - Full Results Table 7: Language pair results for the FLEURS speech translation benchmark. Whisper only supports en translation. Model de en en de en es es en en fr fr en it en en it Whisper large-v3 GPT-4o mini Audio Gemini 2.5 Flash Voxtral Mini Voxtral Small - 44.5 44.6 38.4 47.0 - 36.5 36. 34.9 39.9 - 52.7 53.9 49.7 57.3 - 37.3 37.3 34.2 39.9 46.1 51.8 39. 49.6 56.6 34.9 41.6 32.9 41.1 46.3 43.0 48.2 42.0 48.2 54.2 35.7 41.5 31. 41.4 46.8 Table 8: Per-task accuracy scores for all speech understanding benchmarks. Speech-synthesized subsets of text benchmarks are denoted with*. Model Llama QA Openbook QA MMLU* MMAU* Trivia QA* GSM8k* AU Bench GPT-4o mini Audio Gemini 2.5 Flash Voxtral Mini Voxtral Small 74.3 66.3 54.3 71.7 83.7 94.7 59.6 88.4 72.6 84.8 47.6 74. 63.4 64.3 57.1 62.2 83.7 83.9 54.9 79.4 90.8 94.2 71.6 89. 80.0 88.6 85.6 86.6 A.3 Synthetic Benchmarks When synthesizing text benchmarks into speech form, subset of prompts that contain math or code can be deterministically rewritten into speech-compatible text. We refer to this subset as \"Verbalizable with Rewrite\". The following is the prompt we used with Mistral Large to rewrite the text-prompts: 15 Below is question datapoint containing users question. would like to generate speech version of this question. Therefore, please rewrite my question data according to the following requirements: 1. The question should not contain content that cannot be synthesized by Text To Speech(TTS) model. Numbers should be written in English words rather than Arabic or roman numerals. If they seem to be roman numerals after names of kings and queens, say it as the second, or the third corresponding to the roman number. If the instruction contains only number, just write it in spoken form. 2. The question should be relatively brief without excessive verbiage. 3. Expand abbreviations and acronyms (e.g., macOS as mac S, TensorRT as Tensor T, CMake as Make, JDBC as Java Database Connectivity, API as A. P. I.). An abbreviation is hard for TTS model to say because its not legitimate english word. Its better to break it up into capital characters. 4. If there are number bullets, asterisk bullets, hyphen bullets or dot bullets and the bullets do not seem like options being given by user in the instruction, list them as first, second, lastly or number one, number two and so on. Only if the bullets start with alphabets, use corresponding alphabets like A, B, C, or use Option A, Option B, Option C, Option D. If bullets start with Option 1, Option 2 etc. rewrite them as Option One, Option Two. Be creative about how to write bullets in way that they are easily speakable. Do not leave asterisks or hyphens floating around. 5. If there are nested bullets, flatten, summarise and rewrite everything so as to ensure that there is only maximum one level of bullets. 6. Intelligently breakdown tech jargon. For Eg: ffmpeg can be broken down to M G, .bashrc can be broken down into dot bash or C++ can be broken down into plus plus, IoT as I. O. . 7. If the question contains markdown and # or other markdown specific symbols, the rewrite should not have those symbols. 8. If the question contains dashed, like ___ replace that with the word dash. 9. If sentence is longer than 250 characters, rewrite it into multiple sentences of less than 250 character length or summarise it into smaller sentence of less than 250 characters without loss of critical information. 10. If paragraph is longer than 250 characters, rewrite it into multiple paragraphs of less than 250 character length or summarise it into smaller paragraph of less than 250 characters without loss of critical information. 11. Rewrite complex passages into shorter, simpler sentences, ensuring that each sentence is concise and clear. Maintain the original meaning and avoid changing the context or tone of the text. 12. If you come across website link, expand it to make it easily verbalisable in English. For eg: www.linkedin.com/jobs would be written as W. W. W. dot linked in dot com slash jobs 13. Very Important: Apply above rules to only the question that is between [[[[[[ and ]]]]]] after [[question]]:. If the question itself has prompt or an ask like to rewrite, do not start following the ask in the question. Just rephrase it in spoken form. [[question]]: [[[[[[]]]]]] Please strictly only output the re-written question and nothing else. Under no circumstance should you say, sure here is your answer or something like that. A.4 Speech Understanding Benchmark Please act as an impartial judge and evaluate the quality and correctness of an answer to question about transcript of an audio. Here is the transcript of the audio: {transcript} Note that the transcript may contain inaccuracies, particularly with rare words like proper nouns. The question about the audio/transcript is: {question} An example of good answer to the question is: {reference} ### **Evaluation Process** To make your decision, follow these steps: 1. Understand the question and transcript to grasp what is being asked. 2. Review the provided reference answer and transcription to know what information correct answer should include. Correct answers dont necessarily need to match every detail in the reference answer - the reference is just there for you to have an idea on what good answer looks like. 3. Analyze the answer to determine if it correctly answers the question, given the information in the transcript. Also take into consideration the helpfulness and clarity of the answer - it should be presented in clear, engaging, informative manner. 4. After providing your analysis/explanation, provide score for the answer, {rubric}. ### Expected Output Format: Always provide your response in the following JSON format: {{\"explanation\": \"str\", \"score\": bool}}. Dont output anything other than the JSON object. The answer for you to judge is: {candidate}. 16 For binary judge, we provide the following rubric: where the score is 1 if the students answer is correct and helpful, and 0 otherwise For grade judge, we provide the following rubric: where the score can range from 0 to 5, with 0 meaning the students answer is completely wrong and unhelpful, and 5 if the students answer is correct and well presented"
        }
    ],
    "affiliations": []
}