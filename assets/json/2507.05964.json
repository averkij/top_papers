{
    "paper_title": "T-LoRA: Single Image Diffusion Model Customization Without Overfitting",
    "authors": [
        "Vera Soboleva",
        "Aibek Alanov",
        "Andrey Kuznetsov",
        "Konstantin Sobolev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at https://github.com/ControlGenAI/T-LoRA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 4 6 9 5 0 . 7 0 5 2 : r T-LoRA: Single Image Diffusion Model Customization Without Overfitting Vera Soboleva AIRI, HSE University vvsoboleva@airi.net Aibek Alanov HSE University, AIRI alanov.aibek@gmail.com Andrey Kuznetsov AIRI, Sber, Innopolis kuznetsov@airi.net Konstantin Sobolev AIRI, MSU sobolev@airi.net Figure 1: Compared to standard LoRA single view fine-tuning, T-LoRA reduces overfitting related to position and background, enabling more versatile and enriched generation."
        },
        {
            "title": "Abstract",
            "content": "While diffusion model fine-tuning offers powerful approach for customizing pretrained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting diffusion model using just single concept image, as singleimage customization holds the greatest practical potential. We introduce T-LoRA, Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating timestepsensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available here. Corresponding author. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in personalization of pre-trained large-scale text-to-image diffusion models [36, 10, 22, 17] have transformed content creation, garnering significant attention from both academia and industry. This task enables pre-trained models with the ability to generate highly specific and customized outputs, such as particular objects, styles, or domains. The primary objectives of such customization are twofold: (1) ensuring high-quality preservation of the target concept, and (2) achieving precise alignment between the generated image and the input text. Fine-tuning based customization approaches have proven effectiveness at producing high fidelity concept samples [36, 22, 17]. However, they encounter fundamental limitations due to the restricted size of target datasets. This constraint often compromises generalization capabilities, resulting in undesirable artifacts in the generated outputs. Notably, background elements and positional biases from the training images frequently \"leak\" into new generations, substantially limiting the models flexibility and creative potential. Compared to standard full-weight fine-tuning, lightweight LowRank Adaptater (LoRA) [19] offers significant advantages [3, 37, 9, 39]: it significantly reduces the number of trainable parameters, making it suitable for resource-constrained settings, and it is less prone to overfitting, thereby better preserving the original models generative capabilities. Adapting diffusion model using only single concept image is the most extreme scenario. In this scenario the risk of overfitting is particularly pronounced and even lightweight methods [30, 19, 17] can exhibit susceptibility to overfitting. This is challenging yet most impactful task, on which we focus our research. single-image customization holds the greatest practical potential as in practice, users rarely have multiple images of their concepts with varying backgrounds. We hypothesize that the root cause of overfitting lies in the fine-tuning process applied during the noisiest steps of the diffusion process. At these steps, the model is trained to recover the training images from heavily corrupted inputs, which inadvertently restricts its capacity to generate diverse and flexible scene structures. Simultaneously, these noisy steps are crucial for preserving the structural coherence and fine-grained details of the target concept. Our analysis reveals that omitting these noisy steps during fine-tuning results in substantial loss of fidelity, underscoring the critical trade-off between maintaining concept precision and enabling generative diversity (Figure 3). Based on our analysis, we introduce T-LoRA, Timestep-Dependent Low-Rank Adaptation framework for diffusion model customization. The core idea behind T-LoRA is to prioritize training capacity for less noisy timesteps while reducing the training signal for the noisier ones. To implement this, we propose time-dependent masking strategy, called Vanilla T-LoRA, which restricts higher-rank LoRA components from participating in training and inference during noisier (higher) timesteps. Our further analysis reveals that standard LoRA adapters often exhibit an effective rank that is significantly smaller than the original rank hyperparameter. This property could limit the effectiveness of Vanilla T-LoRA, as masked and unmasked adapters may possess similar expressive power. To address this limitation, we introduce Ortho-LoRA, novel LoRA initialization technique that ensures orthogonality between adapter components, thereby explicitly separating information flows across different timesteps. Extensive experiments demonstrate the effectiveness of T-LoRA and its individual components (Vanilla T-LoRA and Ortho-LoRA). Our proposed framework significantly outperforms existing lightweight fine-tuning approaches in single-image diffusion model personalization tasks in both metrics and user study. These findings establish strong foundation for integrating temporal regularization and orthogonality into future diffusion model adaptation frameworks. To summarize, our key contributions are as follows: We perform detailed analysis of overfitting in diffusion model adaptation and reveal that it primarily occurs at higher (noisier) timesteps of the diffusion process. We propose T-LoRA novel Low-Rank Adaptation framework specifically designed to mitigate overfitting in diffusion model personalization. T-LoRA introduces an innovative rank masking strategy that balances training capacity across diffusion timesteps, applying stronger regularization to higher timesteps to mitigate overfitting. We explore the concept of effective rank in LoRA matrices and propose Ortho-LoRA, novel orthogonal weight initialization method. Ortho-LoRA improves the separation of information flows across timesteps and enhances effective rank utilization, further boosting the performance of T-LoRA."
        },
        {
            "title": "2 Background",
            "content": "Diffusion models Diffusion models [42, 18] are probabilistic generative models parametrized by neural networks and trained to approximate data distribution by progressively denoising Gaussiansampled variable. Specifically, we focus on text-to-image diffusion models [32, 33, 38, 34] εθ, which generate an image from text prompt . The model incorporates text encoder ET to obtain text embeddings = ET (P ) and conditional diffusion model εθ. Latent diffusion models [35] further encode the image into latent representation = E(x) via an encoder and reconstruct it with decoder D, such that D(z). Diffusion models εθ are trained to predict the noise contained in this noisy latent representation: (cid:104) Ep,t,z,ε min θ ε εθ(t, zt, p)2 2 (cid:105) (1) where ε (0, I), is diffusion timestep, and zt is noisy latent code generated through forward diffusion process zt = ForwardDiffusion(t, z, ε). During inference, random noise zT (0, I) is iteratively denoised to recover z0. LoRA Low-Rank Adaptation (LoRA) [19] is technique designed to efficiently fine-tune large pretrained models. LoRA parametrizes original model weight matrix update as as product of two low-rank matrices and B. The adapted parameters can be expressed as: = +BA, Rrm, Rnr, where is the rank of the matrices and and (n, m) is the dimensionality of . To ensure that the model behaves identically to the pre-trained model at the beginning of training, usually is initialized normally and with zero values. Diffusion Model Customization widely adopted approach for personalizing diffusion models is fine-tuning, which involves adjusting model weights to effectively generate specific concepts based on limited set of concept images. This method addresses the challenge of aligning the models output with user-defined concepts, facilitating tailored image generation. To bind new concept to special text token , εθ is fine-tuned on small set of concept images = {x}N i=1 with the following objective: Ep,t,z=E(x),xC,ε min θ (cid:104) ε εθ(t, zt, p)2 2 (cid:105) (2) where = ET (P ) is the text embeddings corresponding to the prompt = \"a photo of V*\". Several strategies have been introduced to enhance this fine-tuning process, including pioneering work on pseudo-token optimization [10] and diffusion model fine-tuning [36, 22]. Additionally, various lightweight parameterization techniques [17, 43, 19, 30, 14] have emerged, aiming to reduce computational costs while preserving the models flexibility and mitigating overfitting. Among these methodologies, Low-Rank Adaptation (LoRA) has gained particular attention. Despite its established nature, LoRA [19] is recognized as strong baseline in subject-driven generation for its lightweight design, high concept fidelity and strong alignment with input prompts. Furthermore, LoRA frequently serves as foundational component in other personalization techniques, both finetuning-based [6, 2, 16] and encoder-based [1, 24]. Overfitting problem Many existing approaches struggle with overfitting to position and background, challenge that worsens with fewer concept images. To address this, various techniques like concept masking [44, 20, 21], prompt augmentation [41, 12], regularization [36, 22, 45, 28], advanced attention mechanisms [20, 21, 27], and sampling strategies [47, 15, 40] have been explored. Unlike prior work, we investigate the root cause of overfitting, hypothesizing that fine-tuning during the noisiest timesteps of the diffusion process reinforces background elements from the training data, limiting generative flexibility. To counter this, we propose novel strategy that reduces the concept signal during noisy timesteps in both training and inference. Additionally, we adapt the LoRA [19] architecture to selectively control concept injection across timesteps, enhancing generalization and output diversity while mitigating overfitting."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce T-LoRA, timestep-dependent low-rank adaptation framework, which aims to address an overfitting problem in diffusion model personalization task. The overview of 3 (a) LoRA (b) Vanilla T-LoRA (c) T-LoRA Figure 2: Comparison of training methods: LoRA, the proposed Vanilla T-LoRA, and T-LoRA schemes. Modules marked with snowflake icon represent components with frozen parameters during training, while modules marked with fire icon indicate trainable components. method is shown in Figure 2c. In the following contents, we first discuss the motivation of the proposed method in Section 3.1. Then, in Section 3.2, we introduce time-dependent LoRA rank controlling strategy, namely Vanilla T-LoRA (Figure 2b). After that, in Section 3.3 we analyze effective rank of trained LoRA adapters in diffusion models and propose Ortho-LoRA weight initialization, which ensures orthogonality of LoRA components and provide more effective rank control on the noisiest timesteps. Finally, in Section 3.4 we present full T-LoRA method that incorporates Vanilla T-LoRA rank controlling strategy and Ortho-LoRA weight initialization. 3.1 Motivation Previous studies have shown that different timesteps in diffusion models play distinct roles throughout the generation process [7, 8, 23]. For example, the authors of [7] categorized the behavior of diffusion timesteps into three main stages: high timesteps (t [800; 1000]) concentrate on forming coarse features, middle timesteps (t [500; 800]) produce perceptually rich content, and the lower timesteps (t [0; 500]) focus on removing residual noise. Additionally, works [5, 11, 40] show that high timesteps contribute to image diversity, as inadequate representation of the prompts context during this stage makes it less likely to be restored in subsequent timesteps. To investigate the role of different timesteps in fine-tuning, we conducted an experiment where we uniformly sampled timesteps from various fixed intervals (see Figure 3). The results show that fine-tuning at higher timesteps [800; 1000] leads to rapid overfitting, causing memorization of poses and backgrounds, which limits image diversity and adherence to prompts. Importantly, these timesteps are essential for defining the overall shape and proportions of the concept. In the middle timesteps [500; 800], the generated context became richer, and the model better reproduce fine concept details. However, we lose information related to the overall shape. For example, as illustrated in Figure 3, the boot retains fittings that correspond to the original concept, but its height is distorted, resulting in shorter shoe. Finally, fine-tuning with lower timesteps [0; 500] demonstrated the best alignment with text prompts and yielded the richest contextual generation. However, this approach struggled to accurately reproduce the intended concept, often generating only some textural elements and losing both shape and fine details of the object. These findings highlight the necessity of managing the concept signal across timesteps. Concept information injection during noisier timesteps should be limited to encourage diversity. Middle timesteps should receive more concept information to produce correct fine details. The information at the lowest timesteps does not need to be restricted, as the risk of overfitting is minimal at this stage. 3.2 Vanilla T-LoRA To tackle the aforementioned challenge, we propose timestep-dependent fine-tuning strategy for diffusion models. Our approach involves parameterization that allocates fewer parameters for training at higher timesteps and more parameters at lower timesteps. Specifically, we introduce masking mechanism to dynamically adjust the ranks of the LoRA adapters based on the timestep within the diffusion process, referred to as Vanilla T-LoRA (see Figure 2b): 4 Figure 3: Motivational Experiment. The figure presents the results of fine-tuning the SD-XL model with LoRA across fixed interval of timesteps. We find that focusing on the noisiest timesteps leads to rapid overfitting, particularly affecting the positioning and background of generated images. In contrast, shifting to earlier timesteps results in outputs that are better aligned with text prompts, yielding richer and more flexible generations. However, completely removing the influence of the noisiest timesteps (t [800; 1000]) is not feasible, as they are crucial for preserving concept fidelity. (a) LoRA (b) Ortho-LoRA Figure 4: Singular values of the matrices for LoRA and Ortho-LoRA with = 64 after 800 training iterations. The red line indicates the rank that encompasses 95% of the singular values total sum. The actual rank of the LoRA matrices is lower than full rank, particularly in the cross-attention layers, whereas the Ortho-LoRA matrices maintain full rank. Wt = + BtAt = + BMtA, Rrm, Rnr Mt = Mr(t) = diag(1, 1, . . . , 1 (cid:125) ) Rrr , 0, 0, . . . , 0 (cid:125) (cid:123)(cid:122) (cid:124) rr(t) (cid:123)(cid:122) r(t) (cid:124) (3) We propose to define r(t) as linear function inversely proportional to the timesteps: r(t) = (r rmin) (T t)/T + rmin. At the maximum timestep , r(t) will equal rmin, hyperparameter of our method. Then, r(t) will linearly increase, reaching maximum value of at 0 timestep. This rank-masking strategy enables dynamic control over the amount of information injected at different timesteps during both adapter training and inference. Smaller ranks are used to generate coarse features and rich contextual information, thereby preserving the generative capabilities of the original model. Meanwhile, lower timesteps receive greater amount of information, facilitating the reproduction of concept fine details. 3.3 On the LoRA Orthogonality Masking matrix columns raises the issue of orthogonality, as non-orthogonal columns may have linear dependencies, limiting effective information exclusion. Our analysis of LoRA weights in diffusion model personalization shows that their effective rank is often much smaller than the specified rank (see Figure 4a), indicating redundancy among rank-1 components. This reduces the effectiveness of the Vanilla T-LoRA masking strategy and limits the models capacity utilization. Forcing orthogonality in and matrices might resolve this issue. One approach involves using SVD-like architecture and regularization as in AdaLoRA [46]: = + BSA, Lreg = λreg(AAT I2 + BT I2 ) (4) 5 where Rrm and Rnr are initialized normally and diag(Rr) initialized with zeros. AdaLoRA [46] showed that scheme requires around 10,000 iterations to achieve orthogonality, far exceeding the 2,000 iterations typical for diffusion model personalization. Our experiments confirm that conventional initialization fails to achieve orthogonality within this limited schedule, making orthogonal initialization essential. Additionally, initializing matrices with zeros significantly slows training (see Section 4.1 for details). To sum up we have the following restrictions in this task: (1) matrices and should be orthogonal from the beggining (2) matrix can not be initialized with zeros. To overcome these limitations, we employ the following LoRA trick, that allows to revisit LoRA weight initialization: = BSA (cid:125) (cid:123)(cid:122) new weights (cid:124) = ˆW + BSA + BSA (cid:124) (cid:123)(cid:122) (cid:125) LoRA (5) The equation above allows us to bypass LoRA weights initialization requirement (B = 0) and unlocks possibility to initialize LoRA with any type of weights. Specifically, as illustrated in Figure 2c, we initialize matrices and with factor matrices derived from the Singular Value Decomposition (SVD) [13] to enforce orthogonality, such that Ainit = , Binit = Ur, Sinit = Sr. We refer to this initialization method as Ortho-LoRA due to its inherent orthogonality properties. The choice of SVD for initialization is crucial. We explore six variants of Ortho-LoRA initialization: using top, middle, and bottom singular components from the decomposed original weights Rnm, and randomly initialized matrix Rnm. If we initialize the matrices with the top SVD components of , the architecture simplifies to PISSA [26]. However, we argue that this initialization is not optimal for diffusion model customization. Our findings indicate that using high singular values correlate with overfitting, while using low singular values can slows down training (Figure 5). We show that using the last SVD components of random matrix Rnm works best for diffusion model customization (for more details, please refer to 4.1). Finally, Figure 4b shows that in contrast to LoRA, Ortho-LoRA matrices retain full rank even after training, eliminating the need for additional orthogonal regularization. 3.4 T-LoRA Finally, we present the complete T-LoRA framework (Figure 2c), which integrates the timestepdependent rank control of Vanilla T-LoRA with the orthogonality-enforcing initialization of OrthoLoRA, yielding timestep adaptive solution for diffusion model personalization.: = BinitSinitMtAinit + BSMtA, ) , 0, 0, . . . , 0 Mt = Mr(t) = diag(1, 1, . . . , 1 (cid:125) (cid:123)(cid:122) (cid:125) (cid:124) rr(t) (cid:123)(cid:122) r(t) (cid:124) (6) where Ainit = [r :], Binit = [r :] and Sinit = S[r :] are initiaized with the last SVD components of random matrics = SV , (0, 1/r). And r(t) = (r rmin) (T t)/T + rmin as timesteps progress towards lower values."
        },
        {
            "title": "4 Experiments",
            "content": "Dataset We use 25 concepts from prior works [10, 36, 22], covering diverse range of categories including pets, toys, interior objects, accessories, and more. Only one image per concept is used for training. This images are manually selected to ensure clear concept visibility. Each concept is paired with 25 text prompts providing appearance, position, and background modifications, and 6 complex prompts involving multiple modifications (e.g., background and accessorization). full list of prompts is provided in Appendix A. For each concept, we generate 5 images per contextual prompt and 15 per base prompt (e.g., \"a photo of \"), totaling in 800 unique concept-prompt pairs. All experiments, except for the Initialization Investigation, involve full 25 concepts, for Initialization Investigation we randomly sampled 5 concepts to reduce computational demands. Evaluation Metric To assess concept fidelity, we calculate the average pairwise cosine similarity (IS) between CLIP ViTB/32 [31] embeddings of real and generated images, as described in [10]. This 6 Figure 5: Results for six Ortho-LoRA initialization variants based on principal, middle, and last SVD components of the original weights , and random matrix R. Higher singular values correlate with overfitting, while too small values can slow down the training. Initialization with last singular values from is optimal across most ranks. Table 1: Image Similarity (IS) and Text Similarity (TS) for LoRA, Vanilla T-LoRA, and T-LoRA across dfferent ranks. Methods Rank = 4 TS IS 0.250 LoRA [19] 0.890 Vanilla T-LoRA 0.894 0.259 T-LoRA 0.255 0.899 Rank = 8 TS IS 0.249 0.897 0.261 0.892 0.260 0.897 Rank = 16 IS TS 0.243 0.900 0.256 0.902 0.260 0.897 Rank = 32 TS IS 0.238 0.901 0.248 0.904 0.259 0.899 Rank = 64 TS IS 0.232 0.901 0.240 0.902 0.256 0.900 approach allows us to maintain the measurements independence from positional, appearance, and accessorization modifications by using the scene-independent prompt \"a photo of .\" Higher values of this metric typically indicate better subject fidelity. To further eliminate the influence of the training images background reproduction, we mask the background. Additionally, we provide DINO-IS, which is calculated the same way, but using DINO [4] embeddings. To evaluate the correspondence between generated images and contextual prompts (TS), we compute the average cosine similarity between the CLIP ViTB/32 embeddings of the prompt and the generated images. Experimental Setup In all our experiments, we train the Stable Diffusion-XL [29] model with batch size of 1, fine-tuning only the diffusion U-Net while keeping the text encoder fixed. Baseline models follow their original paper setups. For additional training details, please refer to Appendix B. 4.1 T-LoRA Design Decisions Analysis of Ortho-LoRA Initialization Strategies We investigated six variants of the Ortho-LoRA initialization, which are based on the principal, middle, and last components of the original weights , and random matrix R. The IS and TS metrics for these setups are presented in Figure 5. First, we observe that for most ranks, the points in the TS metric are ordered according to the initialization singular values: the principal components initialization yields the lowest TS, followed by the middle components initialization, and the last components initialization yields the highest TS. This suggests that higher singular values are strongly correlated with overfitting, particularly for the SVD initialization, which has the largest principal singular values (see Figure 6). Second, for ranks 4 and 8, we observe that initializing with the last SVD components of slows down the training process, whereas initializing with the last SVD components of does not have this effect. This discrepancy is likely due to the smaller values produced by the last SVD components of compared to those initialized with random matrix (as illustrated in the close-up image of the last singular values in Figure 6). Overall, initializing with the last SVD components from random matrix yields optimal results for most ranks, that is why we use it in all further experiments with Ortho-LoRA and T-LoRA. Selection of rmin In Figure 7, we present generation examples for Ortho-LoRA and T-LoRA with rmin set to 25% and 50% of the full rank. Both T-LoRA variants significantly improve alignment with the text and enable greater variety of positions and backgrounds for the concepts. As rmin decreases, the generation becomes more flexible. However, while the 50% performs well across most concepts, the 25% is often too small, leading to reduced concept fidelity. Consequently, we use T-LoRA at 50% in all subsequent experiments. 7 Figure 6: Comparison between singular values distribution from the models selfattention layers (size 1280) and from random matrix of the same size, with close-up on the smallest singular values. Figure 7: Generation outputs for Ortho-LoRA and T-LoRA with rmin set to 25% and 50% of the full rank = 64. OrthoLoRA generally exhibits poor alignment with the text and high degree of reproduction of training images. In contrast, TLoRA significantly enhances alignment with the text. Notably, T-LoRA at 50% maintains high image fidelity, whereas T-LoRA at 25% often struggles to accurately reproduce the concept. Table 2: Image Similarity (IS) and Text Similarity (TS) for T-LoRA compared to the baseline models. T-LoRA-64 LoRA-64 [19] OFT-32 [30] OFT-16 [30] GSOFT-64 [14] GSOFT-32 [14] Metric DINO-IS IS TS 0.802 0.900 0.256 0.808 0.901 0.232 0.804 0.901 0. 0.802 0.899 0.212 0.806 0.901 0.247 0.804 0.901 0.212 SVDiff [17] 0.414 0.753 0.295 4.2 Comparison with LoRA In Table 1, we present image and text similarity for LoRA, Vanilla T-LoRA, and T-LoRA across various ranks. For all ranks, both Vanilla T-LoRA and T-LoRA demonstrate superior text similarity compared to LoRA, while maintaining high image similarity that differs from LoRA by only third of decimal place. At lower ranks, Vanilla T-LoRA and T-LoRA show similar performance; however, the performance improvement of T-LoRA becomes more pronounced as the rank increases. At low ranks, LoRA approaches full rank, which is why Vanilla T-LoRA performs comparably to T-LoRA. In contrast, as the ranks increase, the effectiveness of masking in Vanilla T-LoRA diminishes, while T-LoRA continues to demonstrate its full potential. 4.3 Comparison with Baselines In addition to LoRA [19], we compare our T-LoRA with other lightweight customization methods, including OFT [30], GSOFT [14], and SVDiff [17]. The results are presented in Table 2. Our T-LoRA achieves the best text similarity across all methods, except for SVDiff; however, SVDiff exhibits very low image similarity and often fails to accurately represent the concept. While LoRA demonstrates the highest image similarity, it also exhibits the most significant overfitting. Notably, our methods image similarity differs from LoRAs by only third of decimal place. Figure 8 showcases examples of generation for each method. T-LoRA provides greater flexibility in generation concerning position and background changes while accurately representing the concept. 8 Figure 8: Generation examples for T-LoRA alongside other diffusion model customization baselines. 9 Table 3: Image Similarity (IS) and Text Similarity (TS) for multi-image customization setting."
        },
        {
            "title": "Methods",
            "content": "LoRA-64 [19] OFT-32 [30] T-LoRA-64 # Images = 1 TS IS 0.232 0.901 0.247 0.901 0.256 0.900 # Images = 2 IS TS 0.245 0.900 0.261 0.901 0.262 0.901 # Images = 3 TS IS 0.251 0.902 0.267 0.901 0.263 0.900 Table 4: User study results of the pairwise comparison of T-LoRA versus other baselines."
        },
        {
            "title": "Methods",
            "content": "Ortho-LoRA-64 Vanilla T-LoRA-64 LoRA-64 [19] OFT-32 [30] GSOFT-64 [14] SVDiff [17] Concept Preservation T-LoRA Alternative Text Alignment T-LoRA Alternative Overall Preference T-LoRA Alternative 50.3 51.7 39.3 52.5 49.0 90.1 49.7 48.3 60.7 47.5 51.0 9. 58.5 60.7 71.0 58.3 61.5 42.1 41.5 39.3 29.0 41.7 38.5 57.9 59.3 60.3 67.3 63.5 60.3 55.9 40.7 39.7 32.7 36.5 39.7 44.1 4.4 Multi-image Experiments In addition to the single-image experiments presented in the main paper, we evaluate T-LoRA against LoRA [19] and OFT [30] in the context of multi-image diffusion model customization. In this setting, each concept is represented by multiple images featuring diverse backgrounds. Table 3 summarizes the results for experiments conducted with 1, 2, and 3 images. For T-LoRA and LoRA we use = 64, and = 32 for OFT as it showed the best results in single-image setup. Our results demonstrate that T-LoRA consistently outperforms LoRA in terms of text similarity across all numbers of images, while achieving comparable image similarity. Notably, T-LoRA trained on single image surpasses LoRA trained on 2 or even 3 images of the same concept. When compared to OFT, T-LoRA achieves superior results in the 2-image scenario, while their performances are comparable in the 3-image scenario. 4.5 User Study Finally, we conduct Human Evaluation to fully investigate our models performance. Using an original image of the concept, text prompt, and two generated images (one from T-LoRA and the other from an alternative method), we asked users to respond to the following questions: (1) \"Which image more accurately represents the original concept?\" to evaluate image similarity (2) \"Which image aligns more closely with the text prompt?\" to assess text similarity; and (3) \"Which image overall demonstrates better alignment with the prompt and preserves the identity of the concept?\" to evaluate the overall preference. For each pair of methods, we randomly generated 60 unique concept-prompt pairs. In total, we collect 1,800 human assessments across six method pairs. Results in Table 4 show that T-LoRA significantly outperforms others in text similarity and overall preference, while achieving comparable or superior image similarity to most methods. The exception is LoRA, which surpasses T-LoRA in image similarity due to its tendency to overfit and fully reproduce the original object. Despite this, T-LoRA maintains strong overall impression, highlighting its balanced performance across criteria."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper addressed the challenge of personalizing diffusion models using single concept image, where overfitting and limited generative diversity are prevalent. We introduced T-LoRA, TimestepDependent Low-Rank Adaptation framework, featuring (1) rank masking strategy to regulate training across diffusion timesteps and (2) Ortho-LoRA, an orthogonal weight initialization technique to enhance effective rank utilization. Extensive experiments demonstrated that T-LoRA significantly outperforms existing approaches, achieving balance between concept fidelity and text alignment in data-limited settings. These findings lay strong foundation for integrating timestep-sensitive 10 adaptation strategies and orthogonality principles into future diffusion model frameworks, with promising implications for creative applications in text-to-image generation and beyond."
        },
        {
            "title": "References",
            "content": "[1] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano. Domain-agnostic tuning-encoder for fast personalization of text-to-image models. In SIGGRAPH Asia 2023 Conference Papers, pages 110, 2023. [2] Moab Arar, Andrey Voynov, Amir Hertz, Omri Avrahami, Shlomi Fruchter, Yael Pritch, Daniel Cohen-Or, and Ariel Shamir. Palp: prompt aligned personalization of text-to-image models. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [3] Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John Patrick Cunningham. LoRA learns less and forgets less. Transactions on Machine Learning Research, 2024. Featured Certification. [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [5] Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu Jiang, MingHsuan Yang, Kevin Patrick Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. In International Conference on Machine Learning, pages 40554075. PMLR, 2023. [6] Hong Chen, Yipeng Zhang, Simin Wu, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Identity-preserving disentangled tuning for subject-driven text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. [7] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon. Perception prioritized training of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1147211481, June 2022. [8] Kamil Deja, Anna Kuzina, Tomasz Trzcinski, and Jakub Tomczak. On analyzing generative and denoising capabilities of diffusion-based deep generative models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2621826229. Curran Associates, Inc., 2022. [9] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for finetuning text-to-image diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) 2023. Neural Information Processing Systems Foundation, 2023. [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [11] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. [12] Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, and Tali Dekel. Tokenverse: Versatile multi-concept personalization in token modulation space. arXiv preprint arXiv:2501.12224, 2025. [13] Gene H. Golub and Christian Reinsch. Singular value decomposition and least squares solutions. Linear Algebra, pages 134151, 1971. [14] Mikhail Gorbunov, Kolya Yudin, Vera Soboleva, Aibek Alanov, Alexey Naumov, and Maxim Rakhuba. Group and shuffle: Efficient structured orthogonal parametrization. In The Thirtyeighth Annual Conference on Neural Information Processing Systems. 11 [15] Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, et al. Photoswap: Personalized subject swapping in images. Advances in Neural Information Processing Systems, 36, 2024. [16] Zirun Guo and Tao Jin. Conceptguard: Continual personalized text-to-image generation with forgetting and confusion mitigation. arXiv preprint arXiv:2503.10358, 2025. [17] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73237334, 2023. [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [19] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [20] Miao Hua, Jiawei Liu, Fei Ding, Wei Liu, Jie Wu, and Qian He. Dreamtuner: Single image is enough for subject-driven generation. arXiv preprint arXiv:2312.13691, 2023. [21] Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom: narrowing real text word for real-time open-domain text-to-image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74767485, 2024. [22] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multiconcept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. [23] Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji. Autodiffusion: Training-free optimization of time steps and architectures for automated diffusion model acceleration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 71057114, 2023. [24] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86408650, 2024. [25] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https: //github.com/huggingface/peft, 2022. [26] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. Advances in Neural Information Processing Systems, 37:121038121072, 2024. [27] Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, and Seunggyu Chang. Dreammatcher: appearance matching self-attention for semantically-consistent text-to-image personalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81008110, 2024. [28] Lianyu Pang, Jian Yin, Baoquan Zhao, Feize Wu, Fu Lee Wang, Qing Li, and Xudong Mao. Attndreambooth: Towards text-aligned personalized text-to-image generation. Advances in Neural Information Processing Systems, 37:3986939900, 2024. [29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations. [30] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning. Advances in Neural Information Processing Systems, 36, 2024. 12 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [33] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [36] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510, 2023. [37] Simo Ryu, Hershel Don, Levi McCallum, Hamish Friedlander, Tim Hinderliter, laksjdjf, 2kpr, Ahsen Khaliq, Davide Paglieri, Jack Langerman, Jesse Andrews, Meng Zhang, Oscar Nevarez, Zeke Sikelianos, brian6091, Ethan Smith, hysts, and milyiyo. Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning. 3 2024. [38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [39] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual diffusion: Continual customization of text-to-image diffusion with c-lora. Transactions on Machine Learning Research, 2024. [40] Vera Soboleva, Maksim Nakhodnov, and Aibek Alanov. Beyond fine-tuning: systematic study of sampling techniques in personalized image generation. arXiv preprint arXiv:2502.05895, 2025. [41] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprint arXiv:2306.00983, 2023. [42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020. [43] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-image personalization. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. [44] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15943 15953, 2023. [45] Feize Wu, Yun Pang, Junyi Zhang, Lianyu Pang, Jian Yin, Baoquan Zhao, Qing Li, and Xudong Mao. Core: Context-regularized text embedding learning for text-to-image personalization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 83778385, 2025. [46] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In 11th International Conference on Learning Representations, ICLR 2023, 2023. [47] Yufan Zhou, Ruiyi Zhang, Tong Sun, and Jinhui Xu. Enhancing detail preservation for customized text-to-image generation: regularization-free approach. arXiv preprint arXiv:2305.13579, 2023."
        },
        {
            "title": "A Prompts",
            "content": "We use the following prompts in our experiments: e _ = [ o a o e V* , d V* , r V* , i l snowy V* , V* , V* p l z o i , V* d o t , V* s r o t , V* s a a r , V* r a e bow , h c o , V* h E e Tower V* h u i t a r d , V* h e and autumn v n b g n , V* on o e r w s l r o i , V* h n and palm e t a r d , V* h Sydney Opera House h c o , V* n t bed , V* t on V* swimming , , V* e g V* t a n , i , V* i s b d , V* p w a a t , V* n on e i , V* i b , e _ = [ o a o e V* , V* made o , V* , i l d V* , r V* , t a r d , V* on o e and t e r i c n , V* h E e Tower V* h u i t a r d , V* h e and autumn v n b g n , V* on o e r w s l r o i , V* h n and palm e t a r d , V* h Sydney Opera House h c o , V* h snow , V* on beach , V* on b s e e , V* on o wooden o , V* h t t a r d , V* h u u n b g n , V* on o u e i o t , V* h e i i e k u , V* h bed , V* on c r , V* on o snowy n n peak , V* h n s V* cave , t a r d , ] ] V* c f f V* s i u h cape , V* r a e e r g on a \" V* w r r V* r a c l , among r V* r a a a b a i i o l k h i d h t s i h g e r u and t e s t , e o b l c d n u t , h m i , t h , e d h p and r n l t s \" , c i n a u g a n t , p i s y c t u t h a o n t a r d , V* on n s i Tokyo dusk , V* on f a y i room , V* on wooden l V* a g b t i d h b and u a by warm glow v n n h V* a g among r V* on n e n i o c d p r a r i w a c a i m t by neon y h , n neon o l e , r d by w and t l , s i f i o e r n warm e , s y k d , h c o , t a a e a t a r d , t n r h a window ,"
        },
        {
            "title": "B Training Details",
            "content": "The Stable Diffusion-XL [29] model is used for all experiments. All the models, except SVDiff are trained using Adam optimizer with batch size = 1, learning rate = 1e 4, betas = (0.9, 0.999) and weight decay = 1e 4. All the experiments were conducted using single GPU H-100 per each model customization. LoRA We implement LoRA, Vanilla T-LoRA, Ortho-LoRA and T-LoRA based on the https: //github.com/huggingface/diffusers. For all ranks we train LoRA and Vanilla T-LoRA for 500 training steps and 800 trainings steps for Ortho-LoRA and T-LoRA OFT We implement OFT with PEFT library [25]. We train all OFT models for 800 training steps. GSOFT We use offitial repo https://github.com/Skonor/group_and_shuffle for GSOFT implementation. We train all GSOFT models for 800 training steps. SVDiff We implement the method based on https://github.com/mkshing/svdiff-pytorch. The model for all concepts were trained for 1600 training steps using Adam optimizer with batch size = 1, learning rate = 1e 3, learning rate 1d = 1e 6, betas = (0.9, 0.999), epsilon = 1e8, and weight decay = 0.01."
        },
        {
            "title": "C Limitations",
            "content": "Firstly, our method introduces an additional hyperparameter, rmin, to the standard LoRA framework. While we demonstrate that setting rmin to 50% of the full rank performs well in most cases, the optimal choice may vary across different concepts. Furthermore, lower rmin can contribute to more diverse generation, but it may also necessitate longer training times or more complex choice of r(t), such as non-linear approach, which we do not address in this work. Lastly, the SVD initialization of the weights introduces slight increase in time and computational overhead compared to the standard LoRA initialization."
        }
    ],
    "affiliations": [
        "AIRI, HSE University",
        "AIRI, MSU",
        "AIRI, Sber, Innopolis",
        "HSE University, AIRI"
    ]
}