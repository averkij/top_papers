{
    "paper_title": "SePPO: Semi-Policy Preference Optimization for Diffusion Alignment",
    "authors": [
        "Daoan Zhang",
        "Guangchen Lan",
        "Dong-Jun Han",
        "Wenlin Yao",
        "Xiaoman Pan",
        "Hongming Zhang",
        "Mingxiao Li",
        "Pengcheng Chen",
        "Yu Dong",
        "Christopher Brinton",
        "Jiebo Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning from human feedback (RLHF) methods are emerging as a way to fine-tune diffusion models (DMs) for visual generation. However, commonly used on-policy strategies are limited by the generalization capability of the reward model, while off-policy approaches require large amounts of difficult-to-obtain paired human-annotated data, particularly in visual generation tasks. To address the limitations of both on- and off-policy RLHF, we propose a preference optimization method that aligns DMs with preferences without relying on reward models or paired human-annotated data. Specifically, we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO leverages previous checkpoints as reference models while using them to generate on-policy reference samples, which replace \"losing images\" in preference pairs. This approach allows us to optimize using only off-policy \"winning images.\" Furthermore, we design a strategy for reference model selection that expands the exploration in the policy space. Notably, we do not simply treat reference samples as negative examples for learning. Instead, we design an anchor-based criterion to assess whether the reference samples are likely to be winning or losing images, allowing the model to selectively learn from the generated reference samples. This approach mitigates performance degradation caused by the uncertainty in reference sample quality. We validate SePPO across both text-to-image and text-to-video benchmarks. SePPO surpasses all previous approaches on the text-to-image benchmarks and also demonstrates outstanding performance on the text-to-video benchmarks. Code will be released in https://github.com/DwanZhang-AI/SePPO."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 5 5 2 5 0 . 0 1 4 2 : r Submitted as pre-print paper at arXiv SEPPO: SEMI-POLICY PREFERENCE OPTIMIZATION FOR DIFFUSION ALIGNMENT Daoan Zhang1 , Guangchen Lan2 , Dong-Jun Han3, Wenlin Yao4, Xiaoman Pan4, Hongming Zhang4, Mingxiao Li4, Pengcheng Chen5, Yu Dong4, Christopher Brinton2, Jiebo Luo1 1 University of Rochester, 2 Purdue University, 3 Yonsei University, 4 Tencent AI Lab, 5 University of Washington"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning from human feedback (RLHF) methods are emerging as way to fine-tune diffusion models (DMs) for visual generation. However, commonly used on-policy strategies are limited by the generalization capability of the reward model, while off-policy approaches require large amounts of difficult-toobtain paired human-annotated data, particularly in visual generation tasks. To address the limitations of both onand off-policy RLHF, we propose preference optimization method that aligns DMs with preferences without relying on reward models or paired human-annotated data. Specifically, we introduce Semi-Policy Preference Optimization (SePPO) method. SePPO leverages previous checkpoints as reference models while using them to generate on-policy reference samples, which replace losing images in preference pairs. This approach allows us to optimize using only off-policy winning images. Furthermore, we design strategy for reference model selection that expands the exploration in the policy space. Notably, we do not simply treat reference samples as negative examples for learning. Instead, we design an anchor-based criterion to assess whether the reference samples are likely to be winning or losing images, allowing the model to selectively learn from the generated reference samples. This approach mitigates performance degradation caused by the uncertainty in reference sample quality. We validate SePPO across both text-to-image and text-to-video benchmarks. SePPO surpasses all previous approaches on the text-to-image benchmarks and also demonstrates outstanding performance on the text-to-video benchmarks. Code will be released in https://github.com/DwanZhang-AI/SePPO."
        },
        {
            "title": "INTRODUCTION",
            "content": "Text-to-visual models have become crucial component of the AIGC (AI-generated content) industry, with the denoising diffusion probabilistic model (DDPM) (Ho et al., 2020; Kingma et al., 2021) being the most widely used technology. However, current pre-trained text-to-visual models often fail to adequately align with human requirements. As result, recent works (Liang et al., 2024a; Black et al., 2024; Wallace et al., 2024; Yang et al., 2024) adopt reinforcement learning (RL)-based approaches as the post-training process to better satisfy human needs, namely reinforcement learning from human feedback (RLHF). These methods can generally be divided into two categories: on-policy methods and off-policy methods. Similar to large language models (LLMs), in the post-training of diffusion models, onpolicy models use reward model (RM) to score output and then backpropagate the policy gradients based on the scoring results. typical approach is DDPO (Black et al., 2024), which utilizes vision language models as the reward model to improve the prompt-image alignment. Even though on-policy method is approved to be helpful in the Natural Language Processing (NLP) domain (Dong et al., 2024). However, aside from the existing issues with on-policy methods, such as reward hacking, which can lead to model collapse (Denison et al., 2024), in the text-to-visual task, despite the availability of Authors contributed equally. 1 Submitted as pre-print paper at arXiv numerous evaluation models, it is difficult to find solution that can provide comprehensive feedback on all aspects of the visual content (Kim et al., 2024). Additionally, constructing an effective and efficient reward model is extremely challenging and heavily dependent on the collection of costly paired feedback data. Another approach is to utilize fixed set of preference data (generated by human or other models) as training data, which is called off-policy method. This method allows the trained model to achieve distribution similar to that of the preference data. An example is Diffusion-DPO (Wallace et al., 2024), which uses the Pick-a-Pic (Kirstain et al., 2023) dataset that contains paired preference image data generated by various models with human ratings. Apparently, off-policy methods also depend on human feedback data with positive and negative sample pairs, which requires additional efforts, and their results are usually inferior to those of on-policy methods (Tang et al., 2024). Thus, in this paper, our aim is to build preference optimization method that can mitigate the issues of on-policy and off-policy, allowing the diffusion model to align with preferences without using reward model or paired human-annotated data. To enable models to learn preferences from human feedback, the construction of positive and negative examples is crucial. However, in most datasets, there is usually only one sample in each data point, which typically serves as positive example. Therefore, in the absence of reward model, our initial consideration is how to construct appropriate negative samples. Several previous works, such as SPIN-Diffusion (Yuan et al., 2024a) and DITTO (Shaikh et al., 2024), utilize the previous checkpoints to generate the so-called losing samples and then use preference optimization for model training. However, these methods cannot guarantee that the samples generated from the previous checkpoints are necessarily losing samples relative to the current model. To address this issue, we propose method called Semi-Policy Preference Optimization (SePPO). In our method, the positive samples are sampled from the supervised fine-tuning (SFT) dataset. In order to obtain sufficiently good negative samples that are not too far from the current model distribution, the losing samples are generated by the reference model. Unlike SPIN-Diffusion, where the reference model is set to use the latest checkpoint, and DITTO, where the reference model is the initial model. In our SePPO, the reference models are sampled from all the previous checkpoints. To be specific, we first study the selection strategies for the reference model, conducting experiments using three different approaches: (1) always selecting the initial checkpoint as the reference model, (2) selecting the checkpoint saved from the previous iteration, and (3) randomly selecting from all previous checkpoints. We found that as the number of training steps increases, compared to always using the initial checkpoint, selecting the checkpoint from the previous iteration makes the reference model less prone to overfitting and yields better results. Moreover, randomly selecting from all previous checkpoints produces similar results to selecting the checkpoint from the previous iteration and leads to more stable training process overall. In addition, to determine whether the samples generated from the reference model are genuinely losing images or winning images relative to the current model. We further design strategy to determine whether the sampled examples are positive or negative samples, namely Anchor-based Adaptive Flipper (AAF). If we have winning data point for the model to learn from and the reference model has higher probability than the current model to generate this winning data point, then the probability of sampling winning data point from the reference model distribution will be greater than that of generating losing data point. In other words, in this case, the losing images are not truly negative samples for the current model. This could negatively affect the model if we continue to use the direct preference optimization (DPO) (Rafailov et al., 2024) or SPIN (Yuan et al., 2024a) loss functions. Therefore, we design strategy where, if the reference model has higher probability than the current model of generating the winning data point, the model will learn from both the winning data point and the samples generated by the reference model. This not only helps avoid the negative effects of incorrectly judging sample quality but also increases the chances of the model outperforming the results of SFT to some extent. In summary, the main contributions of this work are as follows: 1. We design an iterative preference optimization method called Semi-Policy Preference Optimization (SePPO). Our model can achieve preference alignment without human annotation and reward model, which reduces labor cost and infrastructure burden; 2 Submitted as pre-print paper at arXiv 2. We develop strategy that first samples the reference model in each iteration, which enables to expand the space of policy exploration. We then design criterion to evaluate the quality of generated responses and adjust the preference optimization based on this quality information, process termed Anchor-based Adaptive Flipper (AAF); 3. SePPO exceeds all previous optimization methods on the text-to-image benchmark, and its effectiveness has also been validated on the text-to-video datasets."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Self-Improvement. Self-improvement methods use iterative sampling to improve pre-trained models, which has the potential to eliminate the need for an expert annotator such as human or more advanced models. recent work INPO (Zhang et al., 2024) directly approximates the Nash policy (the optimal solution) of two-player game on human-annotated preference dataset. Concurrently, the demonstrated self-improvement methods (Shaikh et al., 2024; Chen et al., 2024) align pre-trained models only using supervised learning dataset (one demonstrated response for each prompt) without using reward models and human-annotated data pairs. Specifically, it takes the demonstrated response as the winner and generates the loser by reference models. DITTO (Shaikh et al., 2024) fixes the reference model as the initial model and works well on small datasets. SPIN (Chen et al., 2024) takes the latest checkpoint as the reference model and uses it to generate responses in each iteration. However, these approaches have the following shortcomings. First, they only focus on transformerbased models for the text modality. Second, the selection of reference models is fixed, which has limited space for policy exploration. Third, either human-annotated preference dataset is necessarily required or they are built on strong assumption that the demonstrated responses are always preferred to the generated responses. In fact, the responses generated by the models are not necessarily bad and do not always need to be rejected. RM-Free PO for Diffusion Model Alignment. DPO-style methods (Rafailov et al., 2024) use closed-form expression between RMs and the optimal policy to align pre-trained models with human preference. Thus, no RM is needed during the training process. Diffusion-DPO is first proposed in Wallace et al. (2024). Based on offline datasets with labeled human preference, it shows promising results on text-to-image diffusion generation tasks. Diffusion-RPO (Gu et al., 2024) considers semantic relationships between responses for contrastive weighting. MaPO (Hong et al., 2024) uses margin-aware information, and removes the reference model. However, only off-policy data is considered and human-annotated data pairs are necessarily required with high labor costs. To involve on-policy data and minimize human annotation costs, self-improvement methods are being explored. SPIN-Diffusion (Yuan et al., 2024a) adapts the SPIN method to text-to-image generation tasks with diffusion models. It shows high efficiency in data utilization. However, first, it has the issues mentioned above as self-improvement method. Second, only the text-to-image generation task is considered in all previous works."
        },
        {
            "title": "3 BACKGROUND",
            "content": "3.1 DENOISING DIFFUSION PROBABILISTIC MODEL Given data sample x0 D, the forward process is Markov chain that gradually adds Gaussian noise to the sample as follows q(x1:T x0) = (cid:89) t=1 q(xtxt1), q(xtxt1) = (xt αxt1, (1 α)I), (1) where x1, , xT are latent variables, and α is noise scheduling factor. Equivalently, xt = αxt1 + 1 αϵ, where ϵ (0, I). With latent variable xt from the forward process, DDPM estimates the normalized additive noise by ϵθ(xt), where θ represents the parameters of the neural network. To maximize the evidence lower bound (ELBO) (Kingma & Welling, 2019), we usually minimize the loss function w.r.t. θ: LDM(θ; x0) = ϵ,t wtϵθ(xt) ϵ2(cid:105) (cid:104) , (2) 3 Submitted as pre-print paper at arXiv (a) ref = 0 (b) ref = 1 (c) ref = [0, 1] (ours) Figure 1: Illustration of the exploration area. In the k-th (k = 3) iteration, the green area satisfies the current KL constraint, while the blank area does not. where U(1, ) is uniformly distributed on the integer interval [1, ], wt = (1α)2αt1 (1αt)2 is weighting scalar, and σ2 is the variance of the additive noise. as prespecified constant in Kingma et al. (2021) is ignored in the loss function, because it has no contribution to training. In practice, wt is usually set to constant (Song & Ermon, 2019). = (1α)(1α 1αt 2σ2 2 ) t1 3.2 DIRECT PREFERENCE OPTIMIZATION Given prompt or condition c, the human annotates the preference between two results as xw xl. After preference data collection, in conventional RLHF, we train reward function based on the Bradley-Terry (BT) model. The goal is to maximize cumulative rewards with KullbackLeibler (KL) constraint between the current model πθ and reference model πref (usually the initial model) as follows cD, xπθ (c) where r() is the reward function induced from the BT model. max πθ (cid:104) r(x, c) βDKL (cid:0)πθ(xc)πref (xc)(cid:1)(cid:105) , (3) In DPO, the training process is simplified, and the target is converted to minimize loss function as follows LDPO(θ) = (xw,xl,c)D (cid:104) log σ (cid:16) β log πθ(xwc) πref (xwc) β log πθ(xlc) πref (xlc) (cid:17)(cid:105) , (4) where σ() (without subscript) is the logistic function."
        },
        {
            "title": "ITERATIVE ALIGNMENT FOR DIFFUSION MODELS",
            "content": "4.1 ITERATIVE ALIGNMENT FROM SEMI-POLICY PREFERENCE OPTIMIZATION When we have supervised fine-tuning data (c, xw), we consider using previous checkpoints to construct preference pairs. Specifically, in our method, we use the reference model for the sampling distribution to ensure high-quality generation of reference samples xref . However, how to select an appropriate reference model remains challenge. In RL, equation 3 is surrogate loss (Heess et al., 2017; Vieillard et al., 2020) of the hard KLconstrained target (cid:105) (cid:104) , r(x, c) max πθ s.t. cD, xπθ (c) cD, xπθ (c) (cid:104) DKL (cid:0)πθ(xc)πref (xc)(cid:1)(cid:105) δ, (5) where the training path in the policy space (a family of parametric functions) can be visualized in ball with radius δ as shown in Figure 1. δ as hard constraint radius is roughly proportional to 1/β. Assume that we save checkpoints in total during the training process. In the conventional DPO setting, the reference model is fixed to the initial checkpoint (ref = 0). However, this setting may have limited and fixed exploration area controlled by the hyperparameter β. To better study the 4 Submitted as pre-print paper at arXiv problem, in the k-th iteration, we design and test three sampling strategies for the reference model as follows: The reference model is sampled from the initial model, and is denoted as ref = 0. The reference model is sampled from the checkpoint saved in the last iteration, and is denoted as ref = 1. The reference model is randomly sampled from all previously saved checkpoints, and is denoted as ref = [0, 1]. The training algorithm is given in Algorithm 1. We omit for the sake of concision. In each iteration, the loss function adapted from diffusion DPO is as follows (cid:104) (cid:16) log σ βT wt(ϵθ(xw ) ϵw2 ϵref (xw ) ϵw2 L(θ; xw 0 , xref 0 ) = ϵw,ϵref ,t ϵθ(xref ) ϵref 2 + ϵref (xref (cid:17)(cid:105) , (6) 1 αtϵw,ref . Notably, in is generated from the reference model θref , which is on-policy ) ϵref 2) αtxw,ref = + iid (0, I), and xw,ref where U(1, ), ϵw, ϵref equation 6, the reference image xref 0 learning. To validate the effectiveness of ref = [0, 1], we use the Pick-a-Pic dataset and the stable diffusion 1.5 (SD-1.5) model to conduct all experiments for the ablation study. In experiments, we save checkpoint every 30 updates (K = 7 iterations). The experimental results are shown in Figure 2. In the beginning, the performances of all models are improved. However, as the number of steps increases, the performance of the model with ref = 0 starts to decrease and quickly exhibits overfitting. The model with ref = 1 shows unstable performances. Due to the fact that when all reference samples are generated from the last checkpoint in the current round of training, the model is prone to fall into non-global optima, leading to instability in the training process Florensa et al. (2018). Therefore, considering both model performance and stability, our method uses ref = [0, 1] to generate reference samples. It is noteworthy that our positive samples xw the reference samples xref approach Semi-Policy Preference Optimization (SePPO). 0 are collected off-policy from demonstrations, while 0 are collected on-policy from the saved policies. Therefore, we term our (a) PickScore (b) HPSv2 (c) ImageReward Figure 2: Ablation study of the reference model selection strategies. The x-axis is the number of diffusion steps . The y-axis is the testing score. (The higher is better.) 4.2 ANCHOR-BASED ADAPTIVE FLIPPER FOR PREFERENCE OPTIMIZATION On the other hand, this type of demonstrated method faces significant problem: When using generated reference images xref 0 , we cannot determine the relative relationship between the reference model and the current model (whether the reference image is better than the image generated by the current model or not). Existing methods, when using reference model to generate reference samples, always directly assume that the reference samples have relatively poor quality. For example, DITTO assumes that the earlier checkpoints used as the reference model produce worse-performing samples. However, as shown in the experiment in Figure 2, the reference model is not always inferior to the current model. 5 Submitted as pre-print paper at arXiv Algorithm 1 SePPO Require: Demonstrated data set (xw 0 , c) D; Number of diffusion steps ; Number of iterations K; Initial model θ0. 1: for = 1, , do 2: 3: 4: 5: end for Ensure: θK Sample reference model ref U(0, 1). Generate xref 0 θk θk1 ηk1L(θ; xw from θref , and compose data pairs (xw 0 , xref 0 , xref 0 , c). 0 ) # Or other optimizer, e.g., AdamW. Specifically, based on the DPO-style optimization method, we analyze the change in gradient updates when the reference image is better than the image generated by the current model. The gradient of the loss function in equation 6 is (cid:104) (cid:16) (cid:17) (cid:16) 0 ) = 0 , xref L(θ; xw 2βT wtσ ref ˆσ2 w) ϵw,ϵref ,t βT wt(ˆσ2 ) + ϵref (cid:17) (cid:125) (7) )ϵref 2. where ˆσ2 If xref , both the error term and the weight term σ() tend to be small. This leads to very small gradient update without making full use of the information in the data sample, even though we know that xw 0 is generated by experts with high quality. generated by the reference model is good and close to xw ) ϵw ϵθ(xref (cid:123)(cid:122) error term )ϵref 2ϵref (xref )ϵw2ϵref (xw )ϵw2 and ˆσ2 ref := ϵθ(xref := ϵθ(xw ϵθ(xw (cid:124) (cid:105) , Therefore, in order to avoid the impact of the uncertainty of the reference image, we design strategy that uses the winning image as an anchor, and evaluate the quality of the images generated by the reference model based on its performance relative to the current model on the anchor. Specifically, we design an Anchor-based Adaptive Flipper (AAF) to the loss function in equation 6 as follows: L(θ; xw 0 ) = ϵw,ϵref ,t (cid:104) (cid:16) log σ βT wt (cid:0)ϵθ(xw ) ϵw2 ϵref (xw sign (ϵθ(xref ) ϵref 2 ϵref (xref ) ϵw2 ) ϵref 2)(cid:1)(cid:17)(cid:105) , (8) where ϵw, ϵref iid (0, I) and xw,ref as = αtxw,ref 0 + 1 αtϵw,ref . sign is binary variable defined sign = sgn(ϵref (xw ) ϵw2 ϵθ(xw ) ϵw2), (9) where sgn(x) = 1 if > 0, and otherwise sgn(x) = 1. Intuitively, if the reference model has higher probability of generating noise ϵw compared to the current model, then in this situation, the reference image generated by the reference model is also more likely to be winning image than losing image. We formalize this claim in Theorem 4.1, which motivates the design of the criterion in equation 9. Theorem 4.1. Given two policy model parameters θ1 and θ2, it is almost certain that θ1 generates better prediction than θ2, if where ϵ (0, I), and xt = ϵθ1 (xt) ϵ2 ϵθ2 (xt) ϵ2(cid:105) (cid:104) αtx0 + 1 αtϵ. 0, (10) The proof of Theorem 4.1 is given in Appendix A.1. According to Theorem 4.1, if sign = 1, it means that xref 0 generated by ϵref has high probability to be better than the output of ϵθ. In this situation, xref is of good quality and should not be rejected. The effectiveness of the AAF is further 0 verified in the ablation study in Section 5. 6 Submitted as pre-print paper at arXiv"
        },
        {
            "title": "5.1.1 TEXT-TO-IMAGE",
            "content": "In the text-to-image task, we test our methods based on the stable diffusion 1.5 (SD-1.5) model. Following Diffusion-DPO (Wallace et al., 2024), we use the sampled training set of the Pick-a-Pic v2 dataset (Kirstain et al., 2023) as the training dataset. Pick-a-Pic dataset is human-annotated preference dataset for image generation. It consists of images generated by the SDXL-beta (Podell et al., 2024) and Dreamlike models. Specifically, as mentioned in Diffusion-DPO, we remove approximately 12% pairs with ties and use the remaining 851, 293 pairs, which include 58, 960 unique prompts for training. We use AdamW (Loshchilov & Hutter, 2019) as the optimizer. We train our model on 8 NVIDIA A100 GPUs with local batch size 1, and the number of gradient accumulation steps is set to 256. Thus, the equivalent batch size is 2048. We train models at fixedsquare resolutions. learning rate of 5 109 is used, as we find that smaller learning rate can help avoid overfitting. We set β to 2000, which stays the same in Diffusion-DPO. For evaluation datasets, we use the validation set of the Pick-a-Pic dataset, the Parti-prompt, and HPSv2, respectively. We utilize the default stable diffusion inference pipeline from Huggingface when testing. The metrics we use are PickScore, HPSv2 score, ImageReward score, and Aesthetic score. 5.1.2 TEXT-TO-VIDEO To further verify that SePPO works well in text-to-video generation tasks, we test our methods based on the AnimateDiff (Guo et al., 2024). We use the training set from MagicTime (Yuan et al., 2024b) as our training set and utilize the ChronoMagic-Bench-150 (Yuan et al., 2024c) dataset as our validation set. We use LoRA (Hu et al., 2022) to train all the models at the resolution 256 256 and 16 frames are taken by dynamic frames extraction from each video. The learning rate is set to 5 106 and the training steps are set to 1000. The metrics we use are FID (Heusel et al., 2017), LPIPS (Zhang et al., 2018), SSIM (Wang et al., 2004), PSNR (Hore & Ziou, 2010) and FVD (Unterthiner et al., 2019). 5.2 RESULTS 5.2.1 ANALYSIS OF TEXT-TO-IMAGE To verify the effectiveness of the proposed SePPO, we compare SePPO with the SOTA methods, including DDPO (Black et al., 2024), D3PO (Yang et al., 2024), Diffusion-DPO, SPO (Liang et al., 2024b) and SPIN-Diffusion. We first report all the comparison results on the validation unique split of Pick-a-Pic dataset in Table 1. Specifically, SFTw indicates that we use the winning images from the Pick-a-Pic dataset for supervised fine-tuning. SePPOr and SePPOw indicate that we use randomly chosen images or winning images in the Pick-a-Pic dataset as the training set for SePPO. SePPO outperforms previous methods across most metrics, even those that utilize reward models during the training process, such as DDPO and SPO. Moreover, SePPO does not require the three-stage training process like SPIN-Diffusion, nor does it require the complex selection of hyperparameters. Notably, we observe that SePPO significantly improves ImageReward, which may be attributed to the fact that ImageReward reflects not only the alignment between the image and human preference but also the degree of alignment between the image and the text. In contrast, the other metrics primarily reflect the alignment between the image and human preference. Previous methods like SPIN-Diffusion used the winning images from the Pick-a-Pic dataset as training data. In our experiments, aside from training using the winning images as done previously, we also conduct an experiment where we randomly select images from both the winner and the losing sets as training data for SePPO, which we refer to as SePPOr. Despite using lower-quality training data distribution, SePPOr still outperforms other methods on most metrics. Notably, when compared to SFTw, which is fine-tuned on the winning images, SePPOr still exceeds SFTw on three key metrics, further demonstrating the superiority of SePPO. To better evaluate SePPOs out-of-distribution performance, we also test the model using the HPSv2 and Parti-prompt datasets, which have different distributions from the Pick-a-pic dataset. As shown in Table 2, SePPO outperforms all other models on these datasets. It is worth noting that SPO 7 Submitted as pre-print paper at arXiv has not been tested on these two datasets and SPIN-Diffusion does not report their precise results. So, we reproduce the results by using the checkpoints of SPIN-Diffusion1 and SPO2 available on HuggingFace, referred to as SPIN-Diffusion and SPO. On the left side of Figure 3, we further explore the relationship between our AAF rate and model performance. AAF rate is defined as the ratio (#sign = 1)/(#total) in batch of data. The AAF rate reflects the comparative performance between the current model and all previous checkpoints; higher AAF rate indicates that the samples generated by the reference model are more likely to be negative samples for the current model, meaning that the current model is performing better relative to all previous models. We observe that, as training steps increase, both the AAF rate and PickScore gradually increase, showing similar trend. We then display the changes in PickScore during the training process of SePPO and SFT on the right side of Figure 3. SFT quickly converges and begins to fluctuate, while SePPO is able to steadily improve throughout the training process. We also visualize the results of SD-1.5, SPO, SPIN-Diffusion, and SePPO in Figure 4. SePPO is able to capture the verb nested and also generating better eye details. SePPO not only demonstrates superior visual quality compared to other methods but also excels in image-text alignment. This is because SePPOs training approach, which does not rely on reward model to guide the learning direction, allows the model to learn both human preferences and improve in areas that reward models may fail to address. Table 1: Model Feedback Results on the Pick-a-Pic Validation Set. Methods PickScore HPSv2 ImageReward Aesthetic SD-1.5 SFTw DDPO D3PO Diffusion-DPO SPO SPIN-Diffusion SePPOr SePPOw 20.53 21.32 21.06 20.76 20.98 21.43 21.55 21.33 21.57 23.79 27.14 24.91 23.91 25.05 26.45 27.10 27.07 27.20 -0.163 0.519 0.082 -0.124 0.112 0.171 0.484 0.524 0. 5.365 5.642 5.591 5.527 5.505 5.887 5.929 5.712 5.772 Table 2: Model Feedback Results on the HPSv2 and Parti-prompt Datasets. Aesthetic HPSv2 Parti HPS ImageReward HPS PickScore Parti HPS Methods Parti Parti HPS SD-1.5 SFTw Diffusion-DPO Diffusion-RPO SPO SPIN-Diffusion SePPOw 20.95 21.50 21.40 21.43 21.87 21.88 21.38 21.68 21.63 21.66 21.85 21. 27.17 27.88 27.23 27.37 27.60 27.71 26.70 27.40 26.93 27.05 27.41 27.58 21.90 21.93 27.92 27. 0.08 0.68 0.30 0.34 0.41 0.54 0.70 0.16 0.56 0.32 0.39 0.42 0.51 0.58 5.55 5.82 5.68 5.69 5.87 6.05 5. 5.33 5.53 5.41 5.43 5.63 5.78 5.64 5.2.2 ANALYSIS OF TEXT-TO-VIDEO We further validate SePPO on the text-to-video task in Table 3. Time-lapse video generation task is chosed and all the model are trained based on AnimateDiff. Our method achieves improvements across all metrics compared to both the vanilla and the fine-tuned AnimateDiff. We also visualize the video results in Figure 5, showing that SePPO achieves higher alignment with the text and better realism compared to the other methods. 5.2.3 ABLATION STUDY We perform an ablation study of our method in Table 4. When we remove AAF, meaning that all reference samples are treated as negative samples, the models performance drops significantly, demonstrating the effectiveness of AAF. Furthermore, when we replace the sign function sgn(x) in 1https://huggingface.co/UCLA-AGI/SPIN-Diffusion-iter3 2https://huggingface.co/SPO-Diffusion-Models/SPO-SD-v1-5 4k-p 10ep 8 Submitted as pre-print paper at arXiv (a) AAF Rate and PickScore (b) SFT vs. SePPO Figure 3: Relations of AAF rates and model performance. Figure 4: Text-to-image generation results of SD-1.5, SPO, SPIN-Diffusion and SePPO. The prompts from left to right are: (1) Photo of pigeon in well tailored suit getting cup of coffee in cafe in the morning; (2) Ginger Tabby cat watercolor with flowers; (3) An image of peaceful mountain landscape at sunset, with small cabin nestled in the trees and winding river in the foreground; (4) Space dog; (5) b&w photo of 42 y.o man in white clothes, bald, face, half body, body, high detailed skin, skin pores, coastline, overcast weather. 9 Submitted as pre-print paper at arXiv Figure 5: Text-to-video generation results of AnimateDiff (Raw), SFT, and SePPO. The prompt is: Time-lapse of lettuce seed germinating and growing into mature plant. Initially, seedling emerges from the soil, followed by leaves appearing and growing larger. The plant continues to develop... equation 9 with the indicator function 1(x > 0), i.e., choosing to only learn from the winning image when the reference image has higher probability of not being losing image, and applying DPO when the reference image has higher probability of being losing image, we observe almost no change in performances. This proves that our method is able to filter out insufficiently negative samplessamples that are better than the current models distribution and subsequently boost performance. Furthermore, we experiment with different sampling strategies for the reference model with AAF. When we set the reference models sampling method to using either the initial checkpoint or the most recent saved checkpoint, we observe performance drop in both cases. This indicates the effectiveness of our reference model sampling strategy. Table 3: Metric Scores on the ChronoMagic-Bench-150 Dataset. indicates the lower the better, and indicates the higher the better. FID LPIPS SSIM PSNR FVD AnimateDiff SFT SePPO 134.86 129.14 115.32 0.68 0.65 0.61 0.16 0.17 0.20 9.18 9.25 9.36 1608.41 1415.68 1300.97 Table 4: Ablations on the Pick-a-Pic Validation Dataset. PickScore HPSv2 Methods ImageReward Aesthetic w/o AAF w/ 1(x > 0) Ref as the initial (ref = 0) Ref as the latest (ref = 1) SePPOw 20.88 21.56 21.41 21. 21.57 26.78 27.18 27.04 27.05 27.20 0.366 0.606 0.562 0. 0.615 5.491 5.797 5.727 5.708 5.772 10 Submitted as pre-print paper at arXiv"
        },
        {
            "title": "6.1 LIMITATIONS",
            "content": "First, in diffusion models, the theoretical analysis of exploration in policy space constrained by reference models is an open problem. Second, the performance may be further improved if the pixel space (images before encoding) is also considered. We leave this to future work."
        },
        {
            "title": "6.2 CONCLUSION",
            "content": "Without using reward models or human-annotated paired data, we have developed Semi-Policy Preference Optimization (SePPO) method, which takes previous checkpoints as reference models and uses them to generate on-policy reference samples, which replace losing images in preference pairs. In addition, we design strategy for reference model selection that expands the exploration in the policy space. Furthermore, instead of directly taking reference samples as negative examples, we propose an Anchor-based Adaptive Flipper to determine whether the reference samples are likely to be winning or losing images, which allows the model to selectively learn from the generated reference samples. In text-to-image benchmarks, SePPO achieved 21.57 PickScore, exceeding all previous approaches on the SD-1.5 model. In addition, SePPO performs better than SFT on text-to-video benchmarks."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "For algorithms, we put the key parts (loss function) in Appendix B.2. We upload the main code for training to the supplementary material. The model checkpoints will be released shortly in https://github.com/DwanZhang-AI/SePPO. For datasets, we use open source datasets described in Section 5.1. For generated results, we upload generated videos to the supplementary material."
        },
        {
            "title": "REFERENCES",
            "content": "Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. Stanley Chan. Tutorial on diffusion models for imaging and vision. arXiv preprint arXiv:2403.18103, 2024. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, et al. Sycophancy to subterfuge: Investigating reward-tampering in large language models. arXiv preprint arXiv:2406.10162, 2024. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. RLHF workflow: From reward modeling to online RLHF. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. In International Conference on Machine Learning, pp. 15151528. PMLR, 2018. Yi Gu, Zhendong Wang, Yueqin Yin, Yujia Xie, and Mingyuan Zhou. Diffusion-RPO: Aligning diffusion models through relative preference optimization. arXiv preprint arXiv:2406.06382, 2024. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate your personalized text-to-image diffusion models without specific tuning. In The Twelfth International Conference on Learning Representations, 2024. Submitted as pre-print paper at arXiv Nicolas Heess, Dhruva Tb, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S.M. Eslami, Martin A. Riedmiller, and David Silver. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local Nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. Jiwoo Hong, Sayak Paul, Noah Lee, Kashif Rasul, James Thorne, and Jongheon Jeong. Marginaware preference optimization for aligning diffusion models without reference. arXiv preprint arXiv:2406.06424, 2024. Alain Hore and Djemel Ziou. Image quality metrics: PSNR vs. SSIM. In 20th International Conference on Pattern Recognition, pp. 23662369. IEEE, 2010. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, In International and Weizhu Chen. LoRA: Low-rank adaptation of large language models. Conference on Learning Representations, 2022. Kyuyoung Kim, Jongheon Jeong, Minyong An, Mohammad Ghavamzadeh, Krishnamurthy Dj Dvijotham, Jinwoo Shin, and Kimin Lee. Confidence-aware reward optimization for fine-tuning text-to-image models. In The Twelfth International Conference on Learning Representations, 2024. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in Neural Information Processing Systems, 34:2169621707, 2021. Diederik Kingma and Max Welling. An introduction to variational autoencoders. Foundations and Trends in Machine Learning, 12(4):307392, 2019. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-aPic: An open dataset of user preferences for text-to-image generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, et al. Rich human feedback for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1940119411, 2024a. Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Ji Li, and Liang Zheng. Step-aware preference optimization: Aligning preference with denoising performance at each step. arXiv preprint arXiv:2406.04314, 2024b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Omar Shaikh, Michelle Lam, Joey Hejna, Yijia Shao, Michael Bernstein, and Diyi Yang. Show, dont tell: Aligning language models with demonstrated feedback. arXiv preprint arXiv:2406.00888, 2024. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. 12 Submitted as pre-print paper at arXiv Yunhao Tang, Daniel Zhaohan Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, Remi Munos, Bernardo Avila Pires, Michal Valko, Yong Cheng, and Will Dabney. Understanding the performance gap between online and offline alignment algorithms. arXiv preprint arXiv:2405.08448, 2024. Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. FVD: new metric for video generation, 2019. Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, Remi Munos, and Matthieu Geist. Leverage the average: an analysis of KL regularization in reinforcement learning. Advances in Neural Information Processing Systems, 33:1216312174, 2020. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89418951, 2024. Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning of diffusion models for text-to-image generation. arXiv preprint arXiv:2402.10210, 2024a. Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, and Jiebo Luo. MagicTime: Time-lapse video generation models as metamorphic simulators. arXiv preprint arXiv:2404.05014, 2024b. Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. arXiv preprint arXiv:2406.18522, 2024c. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 586595, 2018. Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, and Dong Yu. Iterative Nash policy optimization: Aligning LLMs with general preferences via no-regret learning. arXiv preprint arXiv:2407.00617, 2024. Submitted as pre-print paper at arXiv"
        },
        {
            "title": "A THEORETICAL ANALYSIS",
            "content": "A.1 PROOF OF THEOREM 4.1 1αtϵ αt 1αtϵθ(xt) 0 := xt In step t, recall that the original image is x0 = xt θ is defined as (cid:98)xθ . We denote the means of these two Gaussian distributions as (cid:98)µθ 0 and µ0, respectfully. The performance of DDPM model θ can be measured by the KL distance 0x0) between the recovered image (cid:98)xθ (Chan, 2024) DKL((cid:98)xθ 0 and the original image x0. Thus, we give the standard of recovery performance from noisy image xt in Definition A.1. Definition A.1. Given two DDPMs θ1 and θ2, θ1 is better than θ2 (θ1 θ2) if its predicted image (cid:98)xθ1 0 has less KL divergence with the original image x0 as follows , and the image recovered by the DDPM αt DKL((cid:98)xθ1 0 x0) DKL((cid:98)xθ2 0 x0). (11) In the noise injection process, the variance of the images remains the same. (x0 and xt have the same variance.) With the fact that the KL divergence between two Gaussian distributions with the identical variance is proportional to the Euclidean distance of their means, we have DKL((cid:98)xθ 0x0) = = = = 1 2σ2 0 (cid:98)µθ (cid:13) 1 (cid:13) (cid:13) 2σ2 (cid:13) 0 1 αt 2σ2 0αt 1 αt 2σ2 0αt 0 µ02 (cid:104) xt 1 αtϵθ(xt) αt (cid:105) (cid:104) ϵθ(xt) (cid:13) 2 (cid:13) (cid:13) ϵθ(xt) ϵ2 (cid:105) E[ϵ] (cid:13) (cid:13) (cid:13) (cid:104) 1 αt 0αt . 2σ2 (cid:105) (cid:104) xt 1 αtϵ αt 2 (cid:105)(cid:13) (cid:13) (cid:13) (cid:13) (12) The last step is from Jensens inequality for the square-error function. Thus, given the condition ϵθ1(xt) ϵ2 ϵθ2(xt) ϵ2(cid:105) 0, (cid:104) (13) we have (14) The prediction from model θ1 has smaller KL distance compared to the prediction from model θ2. Thus, θ1 recovers better images and θ1 θ2 by the definition of performance measurement. As result, θ1 has higher probability of generating good result xθ1 0 . 0 x0) 0. 0 x0) DKL((cid:98)xθ DKL((cid:98)xθ"
        },
        {
            "title": "B SUPPLEMENTARY EXPERIMENTS",
            "content": "B.1 ABLATION STUDY ON ITERATION In this subsection, we perform an ablation study w.r.t. the number of iterations K. In Figure 6, we found that when changing the total number of iteration for saving checkpoints, relatively, the larger achieves better performance. However, the overall trend does not change significantly, which demonstrates the stability of SePPO on K. 14 Submitted as pre-print paper at arXiv Figure 6: Ablation study of = 10 and = 20. B.2 PSEUDOCODE In this subsection, we give the pseudocode of the loss function for training. def loss(target, model_pred, ref_pred, scale_term): #### START LOSS COMPUTATION #### if args.train_method == sft: # SFT, casting for F.mse_loss loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\") elif args.train_method == seppo: # model_pred and ref_pred will be # (2 * LBS) 4 latent_spatial_dim latent_spatial_dim # losses are both 2 * LBS # 1st half of tensors is preferred (y_w), # second half is unpreferred model_losses = (model_pred - target).pow(2).mean(dim=[1,2,3]) model_losses_w, model_losses_l = model_losses.chunk(2) with torch.no_grad(): # Get the reference policy (unet) prediction ref_pred = ref_unet(*model_batch_args, added_cond_kwargs=added_cond_kwargs ).sample.detach() ref_losses = (ref_pred - target).pow(2).mean(dim=[1,2,3]) ref_losses_w, ref_losses_l = ref_losses.chunk(2) pos_losses_w = model_losses_w - ref_losses_w sign = torch.where(pos_losses_w > 0, torch.tensor(1.0), torch.tensor(-1.0)) model_diff = model_losses_w + sign * model_losses_l ref_diff = ref_losses_w + sign * ref_losses_l scale_term = -0.5 * args.beta_dpo inside_term = scale_term * (model_diff - ref_diff) implicit_acc = (inside_term > 0).sum().float() / loss = -1 * (F.logsigmoid(inside_term)).mean() inside_term.size(0) return loss 15 Submitted as pre-print paper at arXiv Figure 7: Text-to-image generation results of SD-1.5, SPO, SPIN-Diffusion, and SePPO. Prompts from left to right: (1) Pink Chihuahua in police suit; (2) Detailed Portrait Of Disheveled Hippie Girl With Bright Gray Eyes By Anna Dittmann, Digital Painting, 120k, Ultra Hd, Hyper Detailed, Complimentary Colors, Wlop, Digital Painting; (3) Chic Fantasy Compositions, Ultra Detailed Artistic, Midnight Aura, Night Sky, Dreamy, Glowing, Glamour, Glimmer, Shadows, Oil On Canvas, Brush Strokes, Smooth, Ultra High Definition, 8k, Unreal Engine 5, Ultra Sharp Focus, Art By magali villeneuve, rossdraws, Intricate Artwork Masterpiece, Matte Painting Movie Poster; (4) winter owl black and white; (5) You are standing at the foot of lush green hill that stretches up towards the sky. As you look up, you notice beautiful house perched at the very top, surrounded by vibrant flowers and towering trees. The sun is shining brightly, casting warm glow over the entire landscape. You can hear the sound of nearby waterfall and the gentle rustling of leaves as gentle breeze passes through the trees. The sky is deep shade of blue, with few fluffy clouds drifting lazily overhead. As you take in the breathtaking scenery, you cant help but feel sense of peace and serenity wash over you. B.3 VISUAL GENERATION EXAMPLES We present more text-to-visual generation results of SePPO and other methods. In Figure 7, we show the text-to-image generation results of SD-1.5, SPO, SPIN-Diffusion, and SePPO. In Figure 8 and Figure 9, we show the text-to-video generation results of AnimateDiff (Raw), SFT, and SePPO. Submitted as pre-print paper at arXiv Figure 8: Text-to-video generation results of AnimateDiff (Raw), SFT, and SePPO. Prompt: Timelapse of night transitioning to dawn over serene landscape with reflective water body. It begins with starry night sky and minimal light on the horizon, progressing through increasing light and glowing horizon, culminating in serene early morning with bright sky, faint stars, and clear reflections in the water. Figure 9: Text-to-video generation results of AnimateDiff (Raw), SFT, and SePPO. Prompt: Timelapse of aurora borealis over night sky: starting with green arcs, intensifying with pronounced streaks, and evolving into swirling patterns. The aurora peaks in vivid hues before gradually fading into homogeneous glow on steadily brightened horizon."
        }
    ],
    "affiliations": [
        "Purdue University",
        "Tencent AI Lab",
        "University of Rochester",
        "University of Washington",
        "Yonsei University"
    ]
}