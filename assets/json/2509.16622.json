{
    "paper_title": "Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing",
    "authors": [
        "Mengqi Wang",
        "Zhan Liu",
        "Zengrui Jin",
        "Guangzhi Sun",
        "Chao Zhang",
        "Philip C. Woodland"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based large language models (DLLMs) have recently attracted growing interest as an alternative to autoregressive decoders. In this work, we present an empirical study on using the diffusion-based large language model LLaDA for automatic speech recognition (ASR). We first investigate its use as an external deliberation-based processing module for Whisper-LLaMA transcripts. By leveraging the bidirectional attention and denoising capabilities of LLaDA, we explore random masking, low-confidence masking, and semi-autoregressive strategies, showing that Whisper-LLaDA substantially reduces WER compared with the baseline. On LibriSpeech, the best cascade system achieves 2.25%/4.94% WER on test-clean/test-other, representing a 12.3% relative improvement over the Whisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA without acoustic features fails to improve accuracy, highlighting the importance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA as a standalone decoder for ASR with diffusion-based and semi-autoregressive decoding. Most experimental configurations achieve faster inference than the Whisper-LLaMA baseline, although recognition accuracy is slightly lower. These findings offer an empirical view of diffusion-based LLMs for ASR and point to promising directions for improvements."
        },
        {
            "title": "Start",
            "content": "AUDIO-CONDITIONED DIFFUSION LLMS FOR ASR AND DELIBERATION PROCESSING Mengqi Wang1, Zhan Liu2, Zengrui Jin2, Guangzhi Sun3, Chao Zhang2, Philip C. Woodland3 1 University of Illinois at Urbana-Champaign, 2 Tsinghua University, 3 University of Cambridge mengqiw3@illinois.edu, liuzhan22@mails.tsinghua.edu.cn 5 2 0 2 0 2 ] . e [ 1 2 2 6 6 1 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion-based large language models (DLLMs) have recently attracted growing interest as an alternative to autoregressive decoders. In this work, we present an empirical study on using the diffusion-based large language model LLaDA for automatic speech recognition (ASR). We first investigate its use as an external deliberation-based processing module for Whisper-LLaMA transcripts. By leveraging the bidirectional attention and denoising capabilities of LLaDA, we explore random masking, low-confidence masking, and semi-autoregressive strategies, showing that WhisperLLaDA substantially reduces WER compared with the baseline. On LibriSpeech, the best cascade system achieves 2.25%/4.94% WER on test-clean/test-other, representing 12.3% relative improvement over the Whisper-LLaMA baseline on the test-other split. In contrast, plain-text LLaDA without acoustic features fails to improve accuracy, highlighting the importance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA as standalone decoder for ASR with diffusion-based and semi-autoregressive decoding. Most experimental configurations achieve faster inference than the Whisper-LLaMA baseline, although recognition accuracy is slightly lower. These findings offer an empirical view of diffusion-based LLMs for ASR and point to promising directions for improvements. Index Terms Automatic Speech Recognition, Diffusion Large Language Model, Non-Autoregressive, Deliberation-based Processing 1. INTRODUCTION Automatic speech recognition (ASR) has seen remarkable progress in recent decades [14], driven largely by the growth in decoder model capacity [510] and the adoption of monotonic and left-toright autoregressive (AR) decoding, which sequentially generates tokens. However, despite their effectiveness, these approaches suffer from reduced efficiency and high computational cost due to the inherently sequential nature of AR decoding. Several decoding strategies have been proposed to accelerate inference in ASR systems based on the attention encoder-decoder (AED) architecture, among which non-autoregressive (NAR) Transformer decoders have emerged as widely adopted solution. By enabling greater parallelization compared with AR counterparts, NAR decoders significantly improve decoding efficiency and have attracted considerable attention in recent years [1114]. These NAR approaches can be broadly categorized as follows: (1) Mask-based NAR systems [1518] are trained to iteratively reconstruct multiple masked tokens simultaneously, conditioned on those tokens that remain unmasked. (2) The hybrid AR-NAR strategy [1923] integrates both AR and NAR decoding mechanisms, Equal contribution. Fig. 1: flowchart illustrating the proposed ASR system with audioconditioned LLaDA diffusion LLM used for deliberation processing. thereby achieving balance between the precision of AR models and the efficiency of NAR approaches. (3) Single-step connectionist temporal classification (CTC) [24]-based approaches [12, 25, 26] interpret entire token sequences in single pass by leveraging decoder side CTC alignments and contextual information, resulting in significant decoding speedup at the expense of marginal degradation in recognition accuracy. (4) Alignment-level refinement methods have been developed, which either iteratively refine latent CTC alignments through multiple parallel denoising passes [27], or utilize ground-truth alignments to facilitate accurate sequence reconstruction in single step [28]. However, despite substantially reducing decoding latency, existing NAR approaches generally suffer from slight degradation in recognition accuracy compared to AR methods, reflecting an inherent trade-off between efficiency and performance. This motivates the search for alternative paradigms that have the potential to mitigate this trade-off. Recent advances in diffusion-based large language models (DLLMs) [2931], exemplified by LLaDA [32], offer promising direction. By combining powerful self-attention mechanisms with strong semantic modeling capabilities, DLLMs provide new opportunities for enhancing ASR. In this work, we present an empirical study on integrating DLLMs into ASR in two ways: (1) as external deliberation-based processing modules, where DLLMs refine preliminary hypotheses generated by base recognizer; and (2) as internal decoders, directly replacing the conventional AR/NAR decoding backbone. These explorations highlight both the potential of DLLMs to reshape ASR decoding and the limitations that remain, pointing to important directions for future research. Prior studies have explored external deliberation-based processing mechanisms that refine original hypotheses or N-best lists [3336], showing notable error reductions. Building upon these observations and inspired by recent progress in deliberationbased refinement, our study makes the following contributions: (i) systematic investigation on utilizing LLaDA as an external language model within ASR systems, demonstrating significant improvements in recognition accuracy compared to conventional autoregressive (AR) baselines. (ii) novel exploration of employing LLaDA internally as diffusion-based decoder within endto-end ASR frameworks, highlighting substantial gains in parallel decoding capabilities and efficiency while maintaining competitive recognition performance relative to both AR and existing non-autoregressive (NAR) approaches. (iii) systematic investigation of decoding strategies is conducted on the LibriSpeech corpus through extensive hyperparameter studies, specifically investigating semi-autoregressive decoding strategy. 2. DIFFUSION-BASED LARGE LANGUAGE MODELS Diffusion-based large language models are defined by generative process over discrete token sequences through forward masking process and learned reverse process. The forward process gradually replaces tokens in clean sequence x0 with mask token [MASK] as function of continuous time [0, 1], where the sequence is fully revealed at given = 0; each token is independently masked with probability as increases, and all tokens are masked at = 1. The reverse process starts from fully masked sequence and iteratively predicts masked tokens conditioned on the unmasked ones, recovering coherent sample at 0. LLaDA [32] implements the forward and reverse process with bidirectional attention-based Transformer, which predicts all masked positions in parallel. During the pre-training stage, LLaDA trains mask predictor pθ( xt) using cross-entropy loss given by LPretrain(θ) = Et,x0,xt (cid:34) 1 (cid:88) iSt log pθ (cid:16) xi 0 xt (cid:35) (cid:17) , (1) 0, , xi 0, , xL where x0 = [x1 0 ] represents the clean sequence, xt is obtained by independently masking each position with probability (0, 1], and St is the set consists of the positions of all masked tokens. While the supervised fine-tuning (SFT) is implemented using the following loss function: LSFT(θ) = Et,p0,r0,rt log pθ (cid:16) ri 0 p0, rt (cid:17) , (2) 1 (cid:88) iS where (p0, r0) stands for paired prompt-response data, note that p0 remains unmasked and LSFT is applied to the masked tokens of response r0, whose positions are recorded in t. The formulation admits time-free parameterization, the model can be trained to predict masked tokens given xt without taking as input, while still matching the diffusion semantics. 3.1. Overall Architecture 3. METHODS We propose Whisper-LLaDA, model that integrates the WhisperLarge-v3 encoder [5] with the diffusion-based large language model LLaDA-8B-Instruct [32]. The Whisper encoder extracts speech representations, which are passed through window-level query Transformer (Q-Former) [6, 37], which utilizes 4 trainable queries with 0.33-second window, and projection layer to align modalities. The text decoder is LLaDA-8B-Instruct model, with LoRA [38] applied to the query, key, and value projection layers of its selfattention blocks (rank 8, scaling factor 4.0, dropout 0.1). Only LoRA, the Q-Former, and the projection layer are trainable with about 87M parameters. As shown in Figure 1, Whisper-LLaDA supports both direct ASR from audio and deliberation-based processing of transcripts from external ASR systems, conditioned either on acoustic features alone or jointly on acoustic features and the given transcripts. We investigate this deliberation functionality on transcripts produced by fine-tuned LLaMA-3.1-based [39] ASR system. 3.2. Training We trained both Whisper-LLaMA and Whisper-LLaDA on the LibriSpeech corpus [40], which contains 960 hours of English audiobooks. Speed perturbation was performed with factors of 0.9 and 1.1. During training, the Whisper encoder is frozen, while LLaMA/LLaDA are fine-tuned with LoRA. The resulting speech embeddings are concatenated with the textual prompt and groundtruth response embeddings. Following the diffusion formulation, the response block is randomly masked with probability (0, 1], and LLaDA predicts the masked tokens conditioned on the remaining context. The model is optimized using cross-entropy loss over masked positions: L(θ) = Et,p0,r0,rt,α log pθ (cid:16) ri 0 p0, rt, α 1 (cid:88) iS (cid:17) , (3) where α denotes the acoustic features of the input audio. 3.3. Inference We organize the inputs into three components: textual instruction, the acoustic features, and response block. The textual instruction specifies the ASR task. The acoustic features are obtained by passing audio through the Whisper encoder, the Q-Former, and projection layer, yielding 4096-dimensional embeddings aligned with LLaDA. For direct decoding, the response block is initialized with [MASK] tokens of length 128, which covers all LibriSpeech utterances as their lengths are shorter than 128 tokens. For deliberationbased processing, the response block is initialized with the WhisperLLaMA transcript, and its length exactly matches that of the transcript sequence. LLaDA then performs the denoising process on this response block. Figure 2 demonstrates the two decoding strategies considered in our work: diffusion-based and semi-autoregressive decoding, as well as two general deliberation-based processing strategies: diffusion-based and semi-autoregressive deliberation. 3.3.1. Deliberation-based Processing with LLaDA Due to the denoising nature and bidirectional attention mechanism of the diffusion large language model, we explored novel cascade framework for deliberation-based processing in ASR. We first obtain transcript from the Whisper-LLaMA system to initialize the response block of Whisper-LLaDA. proportion of response tokens are then selected and replaced with [MASK] based on the following strategies: (1) random selection; (2) select the lowestconfidence tokens, where confidence is yielded from forward step of Whisper-LLaDA based on prompt and audio embedding; and (3) semi-autoregressive deliberation-based processing: dividing the transcript into several sub-blocks and remasking sequentially. After remasking, Whisper-LLaDA recovers the masked positions conditioned on the input prompt, the speech embeddings, and the unmasked tokens of the transcript. The reconstruction under the first two strategies is completed in one pass and the last entails one pass per sub-block, closely aligning with our training procedures. For completeness, we also attempted plain-text version of LLaDA without Whisper embeddings. 3.3.2. Diffusion-based Decoding We define the total number of denoising steps as , which is also the maximum number of iterations executed by LLaDA. In each iteration, LLaDA predicts all masked tokens in parallel, conditioned on the recovered ones, and produces confidence scores from the softmax distribution over logits. The top-K tokens (K = 128/N ) with Fig. 2: Overview of decoding and deliberation-based processing strategies. (a) Diffusion-based decoding: generate the full response in parallel by iterative denoising. (b) Semi-autoregressive decoding: split the response into sub-blocks, apply diffusion within each, and proceed autoregressively across sub-blocks. (c) Diffusion-based deliberation: refine Whisper-LLaMA transcripts by randomly masking or masking low-confidence tokens, and then reconstructing them through diffusion. (d) Semi-autoregressive deliberation: refine transcripts in sub-blocks, combining diffusion within each sub-block and autoregression across sub-blocks. the highest confidence are retained, while the remaining tokens are re-masked. This schedule guarantees the completion within steps. To accelerate inference, we apply an early-stopping criterion: once an [EOS] token is denoised, all subsequent positions are forced to [EOS] to prevent redundant decoding. The decoding process ends once the entire response block is reconstructed. We evaluate with {1, 4, 8, 16, 32, 64, 128}. 3.3.3. Semi-autoregressive Decoding Beyond setting the maximum number of denoising steps , the response block is divided equally into sub-blocks. Unlike the fully parallel strategy, the semi-autoregressive method performs diffusion-based decoding within each sub-block for up to N/M steps, while proceeding autoregressively across subblocks. Specifically, we experiment with {1, 2, 4, 8, 16} and {1, 4, 8, 16, 32, 64, 128}. The number of denoising steps per sub-block is constrained between 1 and 128/M . The same early-stopping criterion is applied to improve efficiency. 4. EXPERIMENTS Our experiments were conducted on LibriSpeech test-clean and test-other sets [40]. All models were trained with the AdamW [41] optimizer using weight decay of 0.05. The learning rate followed linear warmupcosine decay schedule, starting from 1 106, increasing to 3 105 within 3000 steps, and decaying to minimum of 1 105. The best checkpoint was selected based on the WER on the dev-clean set in each run. 4.1. Baseline Table 1 summarizes the baseline results on LibriSpeech benchmark. We primarily consider two LLM-based ASR systems, WhisperLLaMA and Whisper-Vicuna [42], as our baselines. Both systems achieve strong recognition accuracy, with Whisper-LLaMA (Sys. 1) reaching 2.24%/5.63% WER on test-clean/test-other and Whisper-Vicuna (Sys. 2) showing slightly higher WER and larger real-time factor (RTF). For reference, we also report the performance of Whisper-Largev2/v3 (Sys. 34). These systems achieve very competitive results (e.g., 3.90% WER on test-other with Whisper-Large v3). We expect their larger proprietary training datasets (up to 5M hours) attribute to this. Therefore they are not directly comparable to our baselines. Table 1: Word error rate (WER%) and real-time factor (RTF) for all proposed frameworks on LibriSpeech test-clean and test-other datasets. Sys No. Model & Setting WER RTF clean other clean other 1 2 3 4 5 Whisper-LLaMA 3.1 Whisper-Vicuna Whisper-Large v2 Whisper-Large v3 Whisper-LLaDA Step 1 Step 4 Step 8 Step 16 Step 32 Step 64 Step 128 2.24 2.40 2.87 2.03 11.04 5.37 3.78 3.13 2.96 2.82 2. 5.63 5.82 5.16 3.90 17.56 10.72 7.39 6.32 5.80 5.79 5.75 0.253 0.472 0.196 0.186 0.033 0.046 0.055 0.073 0.112 0.185 0.333 0.253 0.459 0.216 0.195 0.039 0.052 0.063 0.080 0.122 0.194 0. 4.2. Deliberation-basd Processing We explored the use of LLaDA as an external deliberation processing module for transcripts generated by Whisper-LLaMA as follows. First, we applied plain-text LLaDA to correct errors from Whisper-LLaMA. We initialized LLaDA with the LLaDA-8BInstruct checkpoint and fine-tuned it on paired data consisting of Whisper-LLaMA transcripts and ground-truth text. The fine-tuned text-based LLaDA was then used to refine Whisper-LLaMA outputs. Notably, the response block length was set to match the length of the Whisper-LLaMA transcript. However, this approach introduced more errors, yielding 3.89% WER on test-clean and 6.91% on test-other, indicating that audio embeddings are essential for effective deliberation-based processing with LLaDA. Subsequently, we turned to Whisper-LLaDA for deliberationIn this setting, based processing on Whisper-LLaMA transcripts. we directly used the end-to-end Whisper-LLaDA checkpoint without any additional training on Whisper-LLaMA outputs. Specifically, we investigated three strategies: (1) random masking; (2) lowestTable 2: WER (%) of cascade deliberation-based processing with random/low-confidence masking on LibriSpeech dev/test sets. Whisper-LLaMA is the Whisper-LLaMA transcript baseline. Mask ratio Whisper-LLaMA 10% 30% 50% 70% 90% 100% Mask ratio Whisper-LLaMA 10% 30% 50% 70% 90% 100% Random masking Dev-clean Dev-other Test-clean Test-other 2.24 2.25 2.21 2.28 2.31 2.32 2.91 5.07 5.05 5.02 5.05 4.98 4.92 5.02 2.24 2.23 2.23 2.21 2.22 2.23 2.54 5.63 5.59 5.59 5.55 5.37 5.24 5.32 Low-confidence masking Dev-clean Dev-other Test-clean Test-other 2.24 2.27 2.28 2.32 2.34 2.31 2.91 5.07 5.06 5.03 5.02 4.97 4.96 5.02 2.24 2.24 2.21 2.18 2.18 2.26 2. 5.63 5.63 5.52 5.43 5.34 5.23 5.32 Table 3: WER (%) of cascade deliberation-based processing with different sub-block partitions on LibriSpeech dev/test sets. Whisper-LLaMA is the Whisper-LLaMA transcript baseline. Number of sub-blocks Whisper-LLaMA 2 4 6 8 10 WER (%) Dev-clean Dev-other Test-clean Test-other 2.24 2.21 2.21 2.26 2.30 2.27 5.07 4.83 4.84 4.86 4.86 4.81 2.24 2.25 2.22 2.23 2.20 2.21 5.63 4.94 5.21 5.26 5.25 5. confidence tokens masking; and (3) semi-autoregressive paradigm. The results for strategies (1) and (2) are presented in Table 2, while the results for strategy (3) are summarized in Table 3. The deliberation-based processing results demonstrate that Whisper-LLaDA yields consistent improvements over the WhisperLLaMA baseline, confirming the effectiveness of diffusion-based refinement in recovering tokens that are challenging for autoregressive counterparts. Random masking achieves the best results when the mask ratio is 90%, while low-confidence masking brings only marginal In addition, semi-autoregressive deliberation-based processing with sub-block partition further improves recognition accuracy, suggesting that the bidirectional attention of LLaDA provides useful supplement to the unidirectional attention in the Whisper-LLaMA model. Overall, these findings highlight the potential of Whisper-LLaDA as complementary deliberation-based processing module for ASR systems. improvements. 4.3. Diffusion-based Decoding We observe the following trends for diffusion decoding (Sys. 5) in Table 1. (a) Increasing the number of denoising steps steadily reduces WER, albeit with higher RTF. The best result on test-clean (2.82%) is achieved with 64 steps, while the lowest WER on test-other (5.75%) is obtained with 128 steps. These gains, however, diminish as the step count grows, indicating satu- (b) The RTFs for 164 steps remain consistently ration effect. lower than those of AR baselines. Notably, the 64-step setting achieves favorable trade-off, with 2.82%/5.79% WER on testclean/test-other, surpassing Sys. 2 on the latter. It also runs about 1.3 faster than Sys. 1 and 2.4 faster than Sys. 2, offering more efficient inference with moderate accuracy loss relative to most AR frameworks. (a) test-clean (b) test-other Fig. 3: Effect of the number of denoising steps and the number of sub-blocks on WER for (a) test-clean and (b) test-other. 4.4. Semi-Autoregressive Decoding We investigate an alternative decoding strategy, namely semiautoregressive decoding. The 128-token generation block is evenly divided into sub-blocks, within which LLaDA performs diffusionbased denoising, while proceeding in an autoregressive manner across sub-blocks. Up to 128 denoising steps are allocated and uniformly distributed among the sub-blocks. Figure 3 shows the effect of the number of denoising steps and the number of sub-blocks on overall performance: (a) Increasing the number of denoising steps per sub-block reduces WER, though the improvement saturates after roughly 16 steps per sub-block. (b) As the total number of steps grows, the performance gaps among settings become negligible. (c) The 4-block setting with 32 steps per sub-block achieves the best performance, with 2.40%/4.96% WER on test-clean and test-other. The performance on the latter is significantly better than the LLaMA/Vicuna-based ASR baselines. 5. CONCLUSION This work presented comprehensive empirical study of employing the diffusion-based language model LLaDA for automatic speech recognition. We examined three complementary perspectives: (i) applying plain-text LLaDA for transcript refinement, (ii) leveraging Whisper-LLaDA for cascade deliberation-based processing, and (iii) using LLaDA as standalone decoder. Our findings reveal clear contrasts across these settings. Plain-text LLaDA fails to improve Whisper-LLaMA outputs, underscoring the necessity of audio emIn contrast, beddings for effective deliberation-based processing. Whisper-LLaDA consistently enhances recognition quality, with random masking at high ratios and the semi-autoregressive framework delivering the largest gains. When used directly as decoder, LLaDA enables faster inference than autoregressive baselines but with slightly higher WER in most settings. Overall, these results highlight both the potential and the current limitations of diffusion-based models in ASR. LLaDA offers effective deliberation-based processing and efficient decoding when conditioned on acoustic features, yet its accuracy still lags behind extensively pretrained autoregressive systems. Future work should scale Whisper-LLaDA with larger and more diverse training data beyond LibriSpeech and systematically explore more advanced masking/remasking policies to narrow the gap while preserving efficiency. 6. REFERENCES [1] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, End-to-end speech recognition using lattice-free MMI, in Proc. Interspeech, Hyderabad, 2018. [2] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, Conformer: Convolutionaugmented transformer for speech recognition, in Proc. Interspeech, Shanghai, 2020. [3] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, Branchformer: Parallel mlp-attention architectures to capture local and global context for in Proc. ICML, Baltimore, speech recognition and understanding, 2022. [4] Z. Yao, L. Guo, X. Yang, W. Kang, F. Kuang, Y. Yang, Z. Jin, L. Lin, and D. Povey, Zipformer: faster and better encoder for automatic speech recognition, in Proc. ICLR, Vienna, 2024. [5] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, Robust speech recognition via large-scale weak supervision, in Proc. ICML, Honolulu, 2023. [6] C. Tang, W. Yu, G. Sun, X. Chen, T. Tan, W. Li, L. Lu, Z. Ma, and C. Zhang, SALMONN: Towards generic hearing abilities for large language models, in Proc. ICLR, Vienna, 2024. [7] Y. Chu, J. Xu, Q. Yang, H. Wei, X. Wei, Z. Guo, Y. Leng, Y. Lv, J. He, J. Lin, C. Zhou, and J. Zhou, Qwen2-audio technical report, arXiv:2407.10759, 2024. [8] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. J. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al., GPT-4o system card, arXiv:2410.21276, 2024. [21] T. Wang, X. Xie, Z. Li, S. Hu, Z. Jin, J. Deng, M. Cui, S. Hu, M. Geng, G. Li, H. Meng, and X. Liu, Towards effective and efficient nonautoregressive decoding using block-based attention mask, in Proc. Interspeech, Kos, 2024. [22] S. Arora, G. Saon, S. Watanabe, and B. Kingsbury, Semiautoregressive streaming ASR with label context, in Proc. ICASSP, Seoul, 2024. [23] H. Xu, T. M. Bartley, V. Bataev, and B. Ginsburg, Three-in-one: Fast and accurate transducer for hybrid-autoregressive ASR, in Proc. ICLR, Singapore, 2025. [24] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks, in Proc. ICML, Pittsburgh, 2006. [25] R. Fan, W. Chu, P. Chang, and J. Xiao, CASS-NAT: CTC alignmentbased single step non-autoregressive transformer for speech recognition, in Proc. ICASSP, Toronto, 2021. [26] K. Deng, Z. Yang, S. Watanabe, Y. Higuchi, G. Cheng, and P. Zhang, Improving non-autoregressive end-to-end speech recognition with pre-trained acoustic and language models, in Proc. ICASSP, Singapore, 2022. [27] E. A. Chi, J. Salazar, Align-refine: Non-autoregressive speech recognition via iterative realignment, arXiv:2010.14233, 2020. and K. Kirchhoff, [28] N. Chen, P. Zelasko, L. Moro-Velazquez, J. Villalba, and N. Dehak, Align-denoise: Single-pass non-autoregressive speech recognition, in Proc. Interspeech, Brno, 2021. [29] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg, Structured denoising diffusion models in discrete state-spaces, in Proc. NeurIPS, 2021. [9] J. Xu, Z. Guo, J. He, H. Hu, T. He, S. Bai, K. Chen, J. Wang, Y. Fan, K. Dang, B. Zhang, X. Wang, Y. Chu, and J. Lin, Qwen2.5-omni technical report, arXiv:2503.20215, 2025. [30] X. Li, J. Thickstun, I. Gulrajani, P. S. Liang, and T. B. Hashimoto, in Proc. Diffusion-LM improves controllable text generation, NeurIPS, New Orleans, 2022. [10] A. Abouelenin, A. Ashfaq, A. Atkinson, H. Awadalla, N. Bach, J. Bao, A. Benhaim, M. Cai, V. Chaudhary, C. Chen, et al., Phi-4-Mini technical report: Compact yet powerful multimodal language models via mixture-of-loras, arXiv:2503.01743, 2025. [11] N. Chen, S. Watanabe, J. Villalba, P. Zelasko, and N. Dehak, Nonautoregressive transformer for speech recognition, IEEE Signal Processing Letters, vol. 28, pp. 121125, 2020. [12] X. Song, Z. Wu, Y. Huang, C. Weng, D. Su, and H. Meng, Nonautoregressive transformer ASR with CTC-enhanced decoder input, in Proc. ICASSP, Toronto, 2021. [13] F.-H. Yu and K.-Y. Chen, Non-autoregressive transformer-based endto-end ASR using BERT, arXiv:2104.04805, 2021. [14] C.-F. Zhang, Y. Liu, T.-H. Zhang, S.-L. Chen, F. Chen, and X.-C. Yin, Non-autoregressive transformer with unified bidirectional decoder for automatic speech recognition, in Proc. ICASSP, Singapore, 2022. [15] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi, Mask CTC: Non-autoregressive end-to-end ASR with CTC and mask predict, in Proc. Interspeech, Shanghai, 2020. [16] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, Imputer: Sequence modelling via imputation and dynamic programming, in Proc. ICML, Vienna, 2020. [17] Y. Higuchi, H. Inaguma, S. Watanabe, T. Ogawa, and T. Kobayashi, Improved mask-CTC for non-autoregressive end-to-end ASR, in Proc. ICASSP, Toronto, 2021. [18] X. Zhang, H. Tang, J. Wang, N. Cheng, J. Luo, and J. Xiao, Dynamic alignment mask CTC: Improved mask CTC with aligned cross entropy, in Proc. ICASSP, Rhodes, 2023. [19] Z. Tian, J. Yi, J. Tao, S. Zhang, and Z. Wen, Hybrid autoregressive and non-autoregressive transformer models for speech recognition, IEEE Signal Processing Letters, vol. 29, pp. 762766, 2022. [20] Y. Li, L. Samarakoon, and I. Fung, Improving non-autoregressive speech recognition with autoregressive pretraining, in Proc. ICASSP, Rhodes, 2023. [31] S. Sahoo, M. Arriola, Y. Schiff, A. Gokaslan, E. Marroquin, J. Chiu, A. Rush, and V. Kuleshov, Simple and effective masked diffusion language models, in Proc. NeurIPS, Vancouver, 2024. [32] S. Nie, F. Zhu, Z. You, X. Zhang, J. Ou, J. Hu, J. Zhou, Y. Lin, J.-R. Wen, and C. Li, Large language diffusion models, arXiv:2502.09992, 2025. [33] W. Wang, K. Hu, and T. N. Sainath, Deliberation of streaming rnntransducer by non-autoregressive decoding, in Proc. ICASSP, Singapore, 2022. [34] P. Pandey, S. D. Torres, A. O. Bayer, A. Gandhe, and V. Leutnant, Lattention: Lattice-attention in ASR rescoring, in Proc. ICASSP, Singapore, 2022. [35] K. Li, J. Mahadeokar, J. Guo, Y. Shi, G. Keren, O. Kalinli, M. L. Seltzer, and D. Le, Improving fast-slow encoder based transducer with streaming deliberation, in Proc. ICASSP, Rhodes, 2023. [36] I. E. Kang, C. Van Gysel, and M. H. Siu, Transformer-based model for ASR N-best rescoring and rewriting, in Proc. Interspeech, Kos, 2024. [37] J. Li, D. Li, S. Savarese, and S. Hoi, BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models, in Proc. ICML, Honolulu, 2023. [38] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al., LoRA: Low-rank adaptation of large language models, in Proc. ICLR, 2022. [39] H. Touvron, T. Lavril, G. Izacard, et al., LLaMA: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [40] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, LibriSpeech: An ASR corpus based on public domain audio books, in Proc. ICASSP, Brisbane, 2015. [41] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, arXiv:1711.05101, 2017. [42] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, et al., Vicuna: An opensource chatbot impressing GPT-4 with 90%* chatgpt quality, https: //lmsys.org/blog/2023-03-30-vicuna/, 2023."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "University of Cambridge",
        "University of Illinois at Urbana-Champaign"
    ]
}