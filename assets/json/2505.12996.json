{
    "paper_title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning",
    "authors": [
        "Jiaan Wang",
        "Fandong Meng",
        "Jie Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance."
        },
        {
            "title": "Start",
            "content": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning Jiaan Wang, Fandong Meng*, Jie Zhou Pattern Recognition Center, WeChat AI, Tencent Inc {torchwang,fandongmeng,withtomzhou}@tencent.com ExTrans-7B mExTrans-7B"
        },
        {
            "title": "ExTrans",
            "content": "5 2 0 2 9 1 ] . [ 1 6 9 9 2 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design new reward modeling method that compares the translation results of the policy MT model with strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.57B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeKR1. Furthermore, we extend our method to the multilingual settings with 11 languages. With carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance."
        },
        {
            "title": "Introduction",
            "content": "Recently, Large Reasoning Models (LRMs), e.g., OpenAI-o1 (OpenAI, 2024b) and DeepSeekR1 (Guo et al., 2025), have shown promising performance in various complex tasks like mathematics, coding, question-answering and search engine (Chen et al., 2025b; Li et al., 2025b; Zhang * Corresponding author. et al., 2024; Guan et al., 2025; Jin et al., 2025; Li et al., 2025a). With the help of the long chain-ofthoughts (CoT), LRMs can make deep thoughts and analyses for given task, and thus provide thoughtfully verified and explainable results. In view of the strong ability of LRMs, some researchers attempt to study the long CoT in neural machine translation (MT). Zhao et al. (2024) propose Marco-o1, which studies the long CoT in open-ended text generation. They use several examples to briefly show the effectiveness of LRMs in MT with slang or colloquial expressions. Chen et al. (2025a) and Liu et al. (2025) conduct empirical studies on how LRMs perform on MT, and further demonstrate the potential of LRMs in MT. Wang et al. (2024) construct MetaphorTrans, MT dataset on the literary domain, which contains English literary sentences with synthesized long CoT for translation, along with their corresponding Chinese translations. Based on MetaphorTrans, they train DRT LRMs via supervised fine-tuning (SFT), and demonstrate their effectiveness in English-toChinese literary MT. More recently, after the emergence of DeepSeekR1 (Guo et al., 2025), the significance of reinforcement learning (RL) in LRMs has become increasingly evident. Several studies try to leverage RL to enhance the MT ability of LRMs. R1-T1 (He et al., 2025) and MT-R1-Zero (Feng et al., 2025) utilize Comet (Rei et al., 2020), CometKiwi (Rei et al., 2022) or BLEU (Papineni et al., 2002) scores as the rewards to optimize LRMs via RL. DeepTrans (Wang et al., 2025) leverages DeepSeekv3 (Liu et al., 2024) as the reward model to provide reference-free evaluation scores in RL training. However, these reward modeling methods (especially Comet and CometKiwi) are not suitable for all domains (Karpinska and Iyyer, 2023; Wang et al., 2024), and might result in sub-optimal solutions. Besides, existing work typically focuses on high-resource languages, e.g., English and Chinese, and neglects the transferability of LRMs across different languages. Therefore, we argue that the improvement of MT ability brought by RL is still under-explored, especially in multilingual MT. In this paper, our goals are twofold: (1) Unleashing the potential of RL in MT LRMs. The essence of the reward modeling in the MT task is to use different signals to quantify the translation quality. R1-T1 and MT-R1-Zero only use traditional metrics (Comet, CometKiwi or BLEU) as the reward signals, ignoring the potential ability of large language models (LLMs). DeepTrans leverages the strong ability of LLM-as-a-judge, and it uses DeepSeek-v3 to quantify the translation quality in reference-free manner. In addition to LLMas-a-judge, we want to further explore the strong ability of LLM-as-an-exemplar in the reward modeling. The ability of LLM-as-an-exemplar is well adopted in knowledge distillation (Xu et al., 2024), where an advanced LLM serves as an exemplar (or teacher) to generate training data for another small model. In MT, we use an advanced LLM (i.e., DeepSeek-R1) to generate exemplar translations for source sentences. During RL training, the translations of the policy LRM are compared with the exemplar translations to provide reward signals. For example, if the policy LRM provides better translations than exemplar translations, higher rewards should be assigned to encourage it. The comparisons can also be made by an advanced LLM (e.g., DeepSeek-v3). In this manner, both the ability of LLM-as-a-judge and LLM-as-an-exemplar can be adopted in the reward modeling. Intuitively, strong exemplar can establish lower bounds for the policy model, potentially guiding it towards enhanced performance. (2) Transferring the success of MT LRMs to the multilingual settings. Previous work (Wang et al., 2024; Feng et al., 2025; Wang et al., 2025) generally focuses on highresource languages, and we want to transfer the successes from single to multiple translation directions. However, the ability of LLM-as-a-judge and LLM-as-an-exemplar is not ensured in lowresource languages (Zhu et al., 2024; Son et al., 2024). Thus, we design lightweight reward modeling method that only uses LLMs strong ability in high-resource MT reward modeling (i.e., Englishto-Chinese). When translating in other directions during RL training, we focus solely on verifying the correctness of the generation format and the target language to provide rewards. In this way, we avoid the potential inaccuracies of LLMs ability as the reward model in low-resource languages. Besides, we can effectively manage the training costs since the reward modeling in other directions can be achieved only via existing lightweight toolkits, i.e., regular expressions and language detection. Based on the motivation in our goal (1), we first train our Exemplar-enhanced Translation (abbr., ExTrans-7B) model via RL (using Qwen2.5-7B as the backbone) in English-to-Chinese literary translation, following Wang et al. (2024, 2025). The experimental results demonstrate the superiority of our ExTrans-7B. With the help of the exemplar (DeepSeek-R1), ExTrans-7B achieves the new state-of-the-art performance, and it outperforms strong LRMs (including OpenAI-o1 and DeepSeekR1) in terms of both automatic metrics and GPT-4o evaluation. Ablation studies also verify the rationality of our designation. Furthermore, we extend ExTrans-7B from English-to-Chinese to the multilingual settings with 11 languages (90 translation directions). The extension uses the lightweight reward modeling method in our goal (2), and we name the extended LRM as mExTrans-7B. Experimental results on the multilingual scenes show the surprising improvement brought by the lightweight method. Without quantifying the translation quality in other translation directions, the mExTrans-7B can effectively transfer the success from Englishto-Chinese to other directions, and enhance the model performance by large margin. mExTrans7B achieves competitive multilingual MT performances compared with QwQ-32B. Our main contributions are concluded as follows: We propose new reward modeling method that employs strong LRM as an exemplar, and compares the policy model with the exemplar to provide reward signals. In this way, both the strong abilities of LLM-as-a-judge and LLMas-an-exemplar are adopted in reward modeling. Based on the method, we train our ExTrans-7B. We propose lightweight method to extend ExTrans-7B from single translation direction to the multilingual settings (mExTrans-7B). Without quantifying the translation quality in other directions, the model can also generalize the MT ability across different languages. Extensive experiments demonstrate the superiority of ExTrans-7B and mExTrans-7B. ExTrans7B achieves the state-of-the-art performance in English-to-Chinese, while mExTrans-7B shows the effectiveness in multilingual settings. Both models outperform baselines in terms of automatic metrics and GPT-4o evaluation."
        },
        {
            "title": "2 Methodology",
            "content": "In this section, we first introduce the proposed reward modeling method, which quantify the translation quality by comparing the translations of the policy model with an exemplar LRM ( 2.1). Then, we discuss how to extend the RL training into multilingual settings via lightweight adaptations ( 2.3). Finally, we provide the training details of ExTrans7B and mExTrans-7B ( 2.3)."
        },
        {
            "title": "2.1 Reward Modeling",
            "content": "Given source sentence s, ExTrans first thinks about how to translate with long CoT (defined as th) and then provides the corresponding translation (defined as tr). The generation format can be defined as <think> th </think> tr, where <think> and </think> are two special tokens to indicate the boundary of th. Format and Thought Reward. We first use regular expression to verify the correctness of the generation format, and define the format reward as: rformat = (cid:40) 1 if format is correct 0 if format is incorrect (1) The format reward is basic and commonly-used reward in RL, designed to ensure that the policy model produces outputs in the correct format. Then, we follow Wang et al. (2025), and employ DeepSeek-v3 (671B) (Liu et al., 2024) to assess the quality of the thought process (i.e., th). With carefully designed prompt1, DeepSeek-v3 could classify th into three categories: thought with no analysis, slight analysis or detailed analysis. Therefore, we can define the thought reward as: rthought = 3 if v3th(s, th) = detailed analysis 2 if v3th(s, th) = slight analysis 1 if v3th(s, th) = no analysis (2) where v3th denotes using DeepSeek-v3 as the thought reward judger. As pointed out by Wang et al. (2025), rthought can encourage the policy model to generate meaningful thought content, which is important to improve the quality of tr. Exemplar-Enhanced Translation Reward. To leverage the strong ability of LLM-as-an-exemplar, we propose new method to quantify the translation quality of tr. As shown in Figure 1, we use an advanced LRM, i.e., DeepSeek-R1 (671B) (Guo et al., 2025), as an exemplar. For source sentence s, we also use DeepSeek-R1 to generate its translation, named exemplar translation (denoted as te r). Then, we use DeepSeek-v3 to compare the quality between tr and te using the following prompt: You are required to evaluate the quality of two translations of given text from [src lang] to [trg lang]. The given text is: {s} For the text, there are two translations: - Translation 1: {te r} - Translation 2: {tr} Please carefully compare the two translations, and determine which of the following situations the two translations belong to: - Situation 1: Translation 1 is significantly better than Translation 2 - Situation 2: Translation 1 is slightly better than Translation 2. - Situation 3: The quality of both translations is similar. - Situation 4: Translation 2 is slightly better than Translation 1. - Situation 5: Translation 2 is significantly better than Translation 1. Your assessment should be based on factors such as accuracy, fluency, and overall readability. where [src lang] and [trg lang] denote the source and the target languages. respectively. Next, we can define the exemplar-enhanced translation reward value as: rtrans = if v3tr(s, tr, te r) = Situation (i {1, 2, 3, 4, 5}) (3) where v3tr denotes using DeepSeek-v3 as the translation reward judger. In this manner, the strong exemplar translation te can establish lower bounds for the policy model, potentially guiding it towards enhanced performance. Besides, both the abilities of LLM-as-a-judge and LLM-as-an-exemplar are adopted in our reward modeling. CometKiwi Reward. In addition to rtrans, we also employ CometKiwi (Rei et al., 2022) as an auxiliary reward to quantify the translation quality: rcometk(s, tr) [0, 1] (4) 1The assessment prompt is shown in Appendix A. Overall Reward. The overall reward is the combiFigure 1: The illustration of the exemplar-enhanced translation reward. nation of the above rewards: expressions and language detection toolkits: (cid:40) rall = 0 rtrans + α rthought + β rcometk if rformat = 0 if rformat = 0 rgeneralize = 0 if rformat = 0 0 elif Lang(tr) = target language 1 else (6) where α and β are trade-off coefficients. (5)"
        },
        {
            "title": "2.2 Multilingual Reward Modeling",
            "content": "We extend the reward modeling from single translation direction to multilingual settings. straightforward way is to directly use rall (c.f. Eq. 5) in the multilingual settings. However, it suffers from the following issues: (i) the ability of LLM-as-ajudge and LLM-as-an-exemplar is not ensured in low-resource languages (Zhu et al., 2024; Son et al., 2024). The effectiveness of utilizing DeeSeek-R1 for generating exemplar translations (i.e., te r) or DeepSeek-v3 for providing rewards (i.e., rthought and rtrans) in languages other than English and Chinese remains uncertain. As pointed out by Larionov et al. (2025), the correlation between DeepSeekv3/R1 and human judgments in MT evaluation is significantly stronger for Chinese-English translations compared to other directions. (ii) It still needs high computational costs to obtain exemplar translations and rewards in various languages, neglecting the exploration of the internal transferability of MT capabilities across different languages. Therefore, we design lightweight method to generalize the MT capabilities to multilingual translation. Specifically, we choose English-to-Chinese as representative high-resource direction. During RL training, only if the policy model performs English-to-Chinese translation, we employ rall as the rewards to update the model. For other directions, we solely verify the correctness of the generation format and the target language to provide rewards, which can be trivially achieved via regular where Lang() denotes the language detection toolkit, which identifies the language of the model translations (i.e., tr). In this manner, we can avoid quantifying translation quality in most directions, allowing the policy model to transfer MT capabilities across languages."
        },
        {
            "title": "2.3 Training Details",
            "content": "Following Wang et al. (2025); Feng et al. (2025), we use Qwen2.5-7B-Instruct (Yang et al., 2024) as the backbone to train our ExTrans-7B and mExTrans-7B. The training process includes two stages: cold start SFT and RL training. Cold Start SFT. We first apply SFT using coldstart dataset that encourages think-then-translate In detail, we randomly select generalpattern. domain source sentences from WMT242, and use DeepSeek-R1 to generate seed translation samples (with long CoT) in the general domain. There are 4K English-to-Chinese samples used in the cold start SFT stage of ExTrans-7B. For the multilingual settings, we focus on 11 languages: English (abbr. En), Chinese (Zh), Arabic (Ar), Czech (Cs), German (De), Spanish (Es), French (Fr), Italian (It), Japanese (Ja), Russian (Ru) and Korean (Ko), resulting in 110 translation directions. Therefore, in addition to 4K English-to-Chinese samples, we further collect 8K samples for the other 109 directions. Consequently, there are 12K samples used to cold start SFT mExTrans-7B. RL Training. Following Guo et al. (2025); Wang et al. (2025); Feng et al. (2025), we use GRPO 2https://www2.statmt.org/wmt24/index. html algorithm (Shao et al., 2024) in the RL training stage. For the policy model π, given source sentence s, it can roll out number of generations {g1, g2, ..., gn}. GRPO optimizes the policy model π by maximizing the following objective: 1 (cid:88) (min(πAi, clip(π, 1 ϵ, 1 + ϵ)Ai) βD (7) 1 π = π(gis) π(gis) (8) where ϵ and β are hyperparameters. indicates the KL divergence between the policy model π and the reference model. Ai indicates the advantage that is calculated as follows: Ai = ri mean({r1, r2, , rn}) std({r1, r2, , rn}) . (9) where ri denotes the reward of gi. When training ExTrans-7B, we use rall (c.f. Eq. 5) as the reward signals. For training mExTrans-7B, both rall and rgeneralize (c.f. Eq. 6) are leveraged."
        },
        {
            "title": "3.1 Experimental Setups",
            "content": "Training Data in RL. Following Wang et al. (2024, 2025), we focus on the literary MT, where the translation generally requires cultural background. For the RL training of ExTrans-7B, we use MetaphorTrans (Wang et al., 2024), which contains 19K training English-to-Chinese samples. We only use the source sentences during RL training, which are selected from English literary books, and generally contain metaphors or similes. Only English-toChinese translation performs in ExTrans-7B training. For RL training of mExTrans-7B, we focus on 11 languages. In addition to the source English sentences from MetaphorTrans, we also use Par3 data (Thai et al., 2022) which involves literary sentences in Zh, Cs, De, Es, Fr, It, Ja and Ru. Thus, the above 8 languages along with English can serve as source languages, each of which could be paired with 10 other languages (i.e., total of 11 languages minus the source language itself) for translation. As result, there are 90 (910) directions used in the RL training of mExTrans7B. To explore the generalizability across different languages, in addition to 19K English-to-Chinese samples, we only collect 50 samples for the other 89 directions, resulting in 23.5K samples in total. Evaluation Data. We also leverage the test set of MetaphorTrans as our evaluation data, which contains 1K English literary sentences. Besides, we follow Wang et al. (2025), and employ two literature books (in English) as the evaluation data: (1) The Essential O. Henry Collection (by O. Henry) and (2) Orbital (by Samantha Harvey). Both books are rich in literary nuance, making them challenging for even human translators. When evaluating ExTrans-7B, the above English sentences are used to perform English-to-Chinese translation. When evaluating mExTrans-7B, English-to-X (X=Zh, Ar, Cs, De, Es, Fr, It, Ja, Ru or Ko) translations are conducted. To further evaluate the multilingual MT performance, we also randomly select 500 Russian literary sentences from Par3, and conduct Russianto-X translations. Evaluation Metrics. Since (1) the references in MetaphorTrans are synthesized via LLMs without human annotations; and (2) the references of The Essential O. Henry Collection and Orbital are missing, we adopt reference-free metrics in our experiments. Following Wang et al. (2025); Feng et al. (2025), we use CometKiwi (Rei et al., 2022) to evaluate the model translations. Moreover, following Wang et al. (2025), we use three evaluators implemented using GPT-4o in reference-free manners, which we refer to as GRF, GEA5 and GEA100, respectively. Among them, GRF assesses model performance from general perspective, while GEA5 and GEA100 evaluate it from literary perspective. For the details of the evaluation prompts, please refer to Appendix B. Implementation Details. For the implementation details of cold-start SFT, RL training, evaluation toolkits, and evaluation hyperparameters, please refer to Appendix C."
        },
        {
            "title": "3.2 Baselines",
            "content": "Non-reasoning LLMs. We leverage Llama-3.18B-Instruct (Grattafiori et al., 2024), Qwen2.5-7BInstruct, Qwen2.5-14B-Instruct (Yang et al., 2024) and GPT-4o (OpenAI, 2024a) as baselines. We also fine-tune LLama-3.1-8B-Instruct, Qwen2.57B-Instruct and Qwen2.5-14B-Instruct with only paired sentences of the MetaphorTrans training data (without long CoT). The fine-tuned LLMs are denoted as Llama-3.1-8B-MT, Qwen2.5-7B-MT and Qwen2.5-14B-MT. LRMs. QwQ-32B(-preview) (Qwen, 2024), Marco-o1-7B (Zhao et al., 2024), DeepSeek-Qwen7B, DeepSeek-Llama-8B, DeepSeek-Qwen-14B, DeepSeek-Qwen-32B, DeepSeek-R1 (Guo et al., 2025) and o1-preview (OpenAI, 2024b) are used MetaphorTrans O. Henry Orbital GRF GEA5 GEA100 CometKiwi GRF GEA5 GEA100 CometKiwi GRF GEA5 GEA100 CometKiwi Model Marco-o1-7B Qwen2.5-7B-Instruct Llama-3.1-8B-Instruct DeepSeek-Qwen-7B DeepSeek-Llama-8B Qwen3-8B (w/o CoT) Qwen3-8B (w/ CoT) Qwen2.5-7B-MT Llama-3.1-8B-MT DRT-7B DRT-8B DeepTrans-7B Qwen2.5-14B-Instruct DeepSeek-Qwen-14B Qwen3-14B (w/o CoT) Qwen3-14B (w/ CoT) Qwen2.5-14B-MT DRT-14B DeepSeek-Qwen-32B QwQ-32B-preview QwQ-32B Qwen3-32B (w/o CoT) Qwen3-32B (w/ CoT) GPT-4o o1-preview DeepSeek-R1 82.41 81.53 79.25 65.16 76.31 85.96 87.02 85.06 84.10 85.57 84.49 88. 84.74 83.92 83.48 88.27 85.66 87.19 84.78 86.31 88.06 84.08 88.29 85.57 87.11 84.29 3.57 3.62 3.31 2.67 3.24 4.22 4.22 3.93 3.88 4.05 3.91 4.21 3.87 3.81 4.09 4.29 4.02 4.13 3.87 4.00 4.09 4.15 4.33 3.86 4.06 4.02 64.24 66.21 59.58 43.66 56.89 72.71 74.00 72.29 69.33 75.05 69.65 75. 70.86 70.64 70.74 74.44 74.53 77.41 71.88 75.50 74.38 72.44 76.29 71.88 78.01 73.78 71.62 70.36 70.14 63.49 67.13 72.96 73.13 71.03 70.25 71.78 70.85 71.82 72.01 71.01 71.07 73.84 72.08 72.11 71.93 71.48 72.88 71.18 73.37 73.01 73.70 68.33 83.12 85.26 79.73 68.97 78.17 89.15 88.96 86.84 85.04 86.36 83.61 87. 86.83 83.27 88.78 89.74 87.27 87.38 87.03 87.61 88.02 89.87 89.76 88.30 89.73 89.79 3.71 3.83 3.43 2.86 3.39 4.13 4.10 4.05 3.87 3.96 3.75 4.22 3.98 3.82 4.14 4.16 4.05 4.00 4.03 4.03 4.21 4.17 4.17 4.00 4.14 4.17 63.11 66.50 57.17 45.64 56.14 75.81 76.44 71.05 66.60 69.51 64.76 76. 70.53 64.79 77.15 77.38 73.06 72.59 70.81 70.79 76.36 78.51 77.88 71.06 76.17 77.03 76.00 76.18 74.35 70.67 73.39 77.95 77.90 77.29 76.14 76.12 73.89 77.04 77.04 75.22 77.85 77.32 77.54 76.70 76.75 76.86 77.71 77.80 77.23 76.74 78.41 77.01 ExTrans-7B (cold start) ExTrans-7B 85.06 90.55 3.94 4.60 66.72 82.29 71.49 74.23 88.90 90.86 4.12 4.25 75.01 79.24 76.91 78.02 81.84 83.38 79.92 71.28 78.91 85.76 84.73 85.46 80.37 81.69 79.14 87.95 84.39 82.30 84.50 85.91 85.55 82.19 85.36 84.79 87.83 86.33 86.54 85.91 86.85 87.37 85.22 86. 3.89 4.00 3.54 3.16 3.64 4.29 4.32 4.12 3.87 3.84 3.65 4.22 4.09 4.01 4.30 4.29 4.14 3.98 4.16 4.04 4.15 4.25 4.26 4.17 4.26 4.27 67.64 70.10 59.90 51.91 59.75 78.06 77.66 70.55 64.38 65.56 61.36 76.92 71.65 69.10 79.26 79.01 75.84 69.36 73.62 71.03 76.55 79.59 79.71 73.54 76.80 80. 75.38 76.46 75.09 72.43 74.47 78.26 78.01 76.32 75.11 69.95 66.36 76.65 76.55 76.28 78.55 77.98 77.40 70.99 77.80 76.17 77.55 78.39 78.05 77.67 78.86 76.17 4.32 4.35 77.61 80.34 76.54 77.67 Table 1: Experimental results in English-to-Chinese literary translation. The bold and the underline denote the best and second-best scores, respectively. and denote statistically significant better than the DeepTrans-7B (Wang et al., 2025) with t-test < 0.01 and 0.05, respectively. denotes models are trained on MetaphorTrans. as baselines. More recently, Qwen3 LLM family3 is proposed, and we use Qwen3-8B, Qwen3-14B and Qwen3-32B as baselines. The Qwen3 LLMs support both reasoning and non-reasoning modes, which we denote as Qwen3 (w/ CoT) and Qwen3 (w/o CoT), respectively. We also use previous MT LRMs as our baselines, including DRT-7B, DRT8B, DRT-14B (Wang et al., 2024) and DeepTrans7B (Wang et al., 2025)."
        },
        {
            "title": "3.3 Results & Analyses",
            "content": "Table 1 shows the experimental results in Englishto-Chinese literary translation. Compare with previous MT LRMs. ExTrans7B significantly outperforms previous MT LMRs, i.e., DRT-7B/8B/14B (Wang et al., 2024) and DeepTrans-7B (Wang et al., 2025). Specifically, on the MetaphorTrans test set, ExTrans-7B outperforms DRT-7B by 5.8%, 13.6%, 9.6% and 3.4% in terms of GRF, GEA5, GEA100 and CometKiwi, respectively. Compared with DeepTrans-7B, the counterpart improvements are 1.9%, 9.3%, 9.2% and 3.4%, showing the effectiveness of our reward modeling method. With the help of the strong ex3https://github.com/QwenLM/Qwen3 emplar, ExTrans-7B can effectively enhance its MT capabilities by comparing itself with the exemplar. Compare with baselines that are trained on MetaphorTrans. Similar to ExTrans-7B, some baselines are also trained on MetaphorTrans, which are marked by in Table 1. We find that ExTrans-7B outperforms them not only on the MetaphorTrans test set, but also on the other two literary books. This finding also verifies the effectiveness of our reward modeling. The improvement brought by RL can be generalized to other genres. Compare with strong LRM baselines. We find that ExTrans-7B outperforms OpenAI-o1 and DeepSeek-R1 on MetaphorTrans, and achieves the new state-of-the-art performance in terms of all metrics, showing its superiority. In O. Henry and Orbital, ExTrans-7B also achieves the best performance in terms of most metrics. Comparing ExTrans-7B with its cold start version, i.e., ExTrans-7B (cold start), the significant improvement brought by the RL training is demonstrated. Intermediate-Stage Analyses. To provide deeper understanding of ExTrans-7B, we analyze the model performance and reward change during the RL training stage. Figure 2 shows the details, (a) GRF and GEA100 (b) CometKiwi (c) rall (d) rcometk Figure 2: Performance and Rewards change of ExTrans-7B during RL training. Among them, (a) and (b) are conducted on the MetaphorTrans test set, while (c) and (d) are conducted on its training set. The horizontal axis denotes the number of training steps, and there are 600 steps (2 epochs) in total. The vertical axis denotes the value of the corresponding metrics."
        },
        {
            "title": "Model",
            "content": "GRF GEA5 GEA100 CometKiwi MetaphorTrans ExTrans-7B ExTrans-7B (w/o rtrans) ExTrans-7B (w/o rcometk) 90.55 88.10 90.04 ExTrans-7B ExTrans-7B (w/o rtrans) ExTrans-7B (w/o rcometk) 90.86 88.61 89.14 4.60 4.34 4. 4.25 4.08 4.54 82.29 76.90 82.39 O. Henry 79.24 74.65 78.86 74.23 76.79 60.32 78.02 79.79 69. Table 2: The experimental results of ablation studies. and we analyze it from the following aspects: In terms of GPT-based evaluation metrics, we compare the changes between ExTrans-7B and previous DeepTrans-7B (Wang et al., 2025). As shown in Figure 2 (a), the performance of ExTrans-7B is consistently better than that of DeepTrans-7B. After only 100 steps of RL training, ExTrans-7B is able to beat the final DeepTrans-7B. In terms of CometKiwi, ExTrans-7B also show its superiority compared with DeepTrans-7B (c.f. Figure 2 (b)). The CometKiwi score of ExTrans-7B generally increases along with the training process. In terms of reward values, we find that the exemplar-enhanced translation reward (i.e., rtrans) generally increases during RL training, while the CometKiwi reward (i.e., rcometk) continues to fluctuate. Both rewards are important for training our ExTrans-7B, which we will further discuss in 3.4. Human Evaluation. To verify the superiority of ExTrans-7B, we also employ human evaluation on the translations of ExTrans-7B, DeepTrans-7B, QwQ-32B and Qwen3-32B (w/ CoT). The results are provided in Appendix D."
        },
        {
            "title": "3.4 Ablation Study",
            "content": "There are four types of rewards in rall, i.e., rformat, rthought, rtrans and rcometk (c.f. Eq. 5). Among them, rformat and rthought are basic rewards and have already been verified in Wang et al. (2025). We further verify the effectiveness of the exemplarenhanced translation reward (rtrans) and cometkiwi reward (rcometk). To this end, we design the following two variants of ExTrans-7B: (1) ExTrans7B (w/o rtrans) removes rtrans in the reward modeling during RL training; while (2) ExTrans-7B (w/o rcometk) removes rcometk. As shown in Table 2, when removing the exemplar-enhanced translation reward, the model performance in terms of GPT-4o metrics decreases. When removing the CometKiwi reward, the CometKiwi score will significantly decrease, e.g., 74.2360.32. These findings validate the rationality behind our reward modeling design."
        },
        {
            "title": "3.5 Multilingual Generalization",
            "content": "As described in 2.2, we extend the reward modeling to the multilingual settings via lightweight method, and train mExTrans-7B LRM. Table 3 shows the experimental results in English-to-X and Russian-to-X literary translation. We compare mExTrans-7B with its backbone, i.e., Qwen2.5-7BInstruct, and its cold-start version, i.e., mExTrans7B (cold start). Besides, QwQ-32B and o1-preview serve as strong LRM baselines for comparison. We analyze the results from the following aspects: Qwen2.5-7B-Instruct mExTrans-7B (cold start): The cold start SFT (c.f. 2.3) significantly improves the model performance in most translation directions, showing the effectiveness of powerful data created by the advanced LRMs (Guo et al., 2025; Yang et al., 2025). For example, in EnAr translation, mExTrans-7B (cold start) outperforms Qwen2.5-7B-Instruct by 17.85 GRF, 0.90 GEA5, 14.10 GEA100, and 6.18 CometKiwi; while the counterpart improvements in RuAr are 8.38 GRF, 0.43 GEA5, 9.70 GEA100 and 2.98 CometKiwi. mExTrans-7B (cold start) mExTrans-7B: Model Src Trg English (MetaphorTrans) Src Trg English (MetaphorTrans) Src Trg Russian (Par3) Src Trg Russian (Par3) Qwen2.5-7B-Instruct QwQ-32B o1-preview mExTrans-7B (cold start) mExTrans-7B Qwen2.5-7B-Instruct QwQ-32B o1-preview mExTrans-7B (cold start) mExTrans-7B Qwen2.5-7B-Instruct QwQ-32B o1-preview mExTrans-7B (cold start) mExTrans-7B Qwen2.5-7B-Instruct QwQ-32B o1-preview mExTrans-7B (cold start) mExTrans-7B Qwen2.5-7B-Instruct QwQ-32B o1-preview mExTrans-7B (cold start) mExTrans-7B Ar Cs De Es Fr 54.20 / 2.53 / 34.00 / 57.49 78.29 / 3.82 / 60.60 / 63.55 90.18 / 4.20 / 76.43 / 67.75 72.05 / 3.43 / 48.10 / 63.67 81.10 / 4.01 / 61.26 / 67.22 54.92 / 2.30 / 29.55 / 56.92 75.69 / 3.47 / 55.18 / 64.67 89.02 / 4.16 / 77.71 / 73.26 59.20 / 2.38 / 32.76 / 59.21 66.52 / 2.78 / 37.84 / 63.12 76.95 / 3.25 / 52.70 / 68.02 83.21 / 3.79 / 70.62 / 70.04 89.12 / 4.04 / 79.92 / 74.02 78.41 / 3.46 / 56.00 / 68.91 82.98 / 3.84 / 65.22 / 71.60 83.03 / 3.69 / 64.58 / 68.79 89.14 / 4.00 / 76.22 / 72.29 92.22 / 4.12 / 82.66 / 73.90 85.65 / 3.98 / 70.38 / 71.08 87.05 / 4.17 / 75.08 / 73.00 80.54 / 3.58 / 59.23 / 69.43 89.39 / 4.02 / 75.92 / 73.01 91.92 / 4.09 / 78.90 / 75.14 84.17 / 3.83 / 64.90 / 71.45 87.31 / 4.03 / 72.95 / 73.68 It Ja Ko Ru Zh 79.11 / 3.50 / 59.90 / 69.14 86.64 / 3.95 / 73.97 / 72.20 91.02 / 4.10 / 80.60 / 75.53 80.25 / 3.62 / 62.40 / 70.67 83.94 / 3.93 / 67.85 / 73.21 69.73 / 3.16 / 48.23 / 69.22 84.72 / 4.17 / 71.30 / 75.36 87.64 / 4.46 / 79.45 / 78.52 80.70 / 3.91 / 61.66 / 74.32 84.89 / 4.29 / 71.17 / 76. 56.83 / 2.61 / 37.82 / 64.39 81.42 / 3.99 / 67.84 / 71.90 88.78 / 4.55 / 80.62 / 77.82 77.67 / 3.51 / 52.42 / 71.94 83.08 / 4.09 / 65.28 / 74.95 67.45 / 3.05 / 47.12 / 63.98 85.50 / 3.99 / 70.78 / 69.87 90.30 / 4.19 / 77.67 / 73.30 79.78 / 3.65 / 57.30 / 67.94 84.26 / 4.05 / 66.22 / 71.68 81.53 / 3.62 / 66.21 / 70.36 88.06 / 4.09 / 74.38 / 72.88 87.11 / 4.06 / 78.01 / 73.70 86.17 / 4.36 / 72.99 / 71.77 90.44 / 4.54 / 80.36 / 74.16 Ar Cs De En Es 62.62 / 2.96 / 44.20 / 59.09 81.44 / 3.96 / 67.60 / 62.47 75.52 / 3.53 / 55.52 / 62.86 71.00 / 3.39 / 53.90 / 62.07 80.06 / 3.76 / 61.58 / 65.13 64.11 / 2.83 / 41.08 / 66.11 81.33 / 3.71 / 63.17 / 71.18 92.77 / 4.51 / 81.47 / 77.58 65.17 / 2.90 / 43.75 / 67.80 71.92 / 3.27 / 49.75 / 70.47 74.41 / 3.43 / 57.02 / 66.91 84.20 / 3.94 / 73.17 / 69.29 91.53 / 4.21 / 81.97 / 72.63 81.58 / 3.77 / 64.22 / 68.70 84.98 / 4.01 / 69.67 / 69.88 83.05 / 4.07 / 73.02 / 75.95 86.17 / 4.14 / 78.03 / 75.94 90.92 / 4.32 / 84.00 / 77.24 84.30 / 4.00 / 72.60 / 75.78 86.50 / 4.26 / 77.40 / 75. 78.06 / 3.79 / 65.14 / 67.99 88.61 / 4.26 / 78.55 / 70.89 92.48 / 4.38 / 82.05 / 72.16 83.10 / 4.02 / 66.75 / 69.24 86.24 / 4.21 / 72.58 / 70.03 Fr It Ja Ko Zh 82.25 / 3.83 / 66.44 / 68.19 88.27 / 4.17 / 77.60 / 69.26 92.69 / 4.20 / 81.90 / 71.24 83.71 / 3.96 / 68.83 / 68.87 86.56 / 4.13 / 73.88 / 68.77 82.36 / 3.79 / 65.52 / 70.27 89.28 / 4.22 / 77.05 / 71.47 92.56 / 4.38 / 81.97 / 73.79 82.06 / 3.86 / 66.95 / 70.71 85.28 / 4.13 / 72.10 / 72.05 74.30 / 3.36 / 53.20 / 63.84 83.67 / 4.29 / 72.04 / 67.76 90.83 / 4.61 / 82.36 / 71.77 82.47 / 3.90 / 65.23 / 68.45 85.08 / 4.21 / 71.26 / 69.88 63.98 / 2.78 / 40.32 / 60.57 78.14 / 3.92 / 65.95 / 64.25 90.47 / 4.66 / 82.97 / 70.51 79.89 / 3.71 / 59.15 / 65.17 83.98 / 4.01 / 66.08 / 67.08 84.13 / 3.89 / 69.22 / 65.80 89.15 / 4.32 / 77.70 / 66.82 91.78 / 4.44 / 82.12 / 68.81 87.69 / 4.17 / 73.20 / 66.85 90.32 / 4.45 / 79.70 / 67.13 Table 3: Experimental results in multilingual literary translation (in terms of GRF / GEA5 / GEA100 / CometKiwi). Src and Trg denote the source and the target language, respectively. The English sources are selected from the MetaphorTrans test set (Wang et al., 2024), while the Russian sources are selected from Par3 (Thai et al., 2022). With the help of the designed reward modeling method, mExTrans-7B further improves the model performance in the multilingual literary translation. We find that mExTrans-7B outperforms mExTrans7B (cold start) in almost all directions in terms of all metrics, demonstrating the effectiveness and efficiency of the lightweight reward modeling method. By solely verifying the correctness of the generation format and target language in other directions, the method can transfer MT capabilities from single high-resource direction (i.e., English-toChinese in our work) to others. Compare with strong LRM baselines: In terms of CometKiwi, we find that mExTrans-7B outperforms QwQ-32B in most directions, while QwQ32B performs better in terms of GPT-4o evaluation. Compared with QwQ-32B, the performance of mExTrans-7B is competitive. However, when comparing mExTrans-7B with o1-preview, we observe that o1-preview consistently outperforms mExTrans-7B across most metrics, indicating that performance gap remains when transferring MT capabilities to multilingual settings. 2025b; Li et al., 2025b; Zhang et al., 2024; Guan et al., 2025; Jin et al., 2025; Li et al., 2025a). Some researchers investigate the MT capability of deep reasoning LLMs. Zhao et al. (2024) and Liu et al. (2025) discuss the potential of long CoT reasoning in MT with some heuristic examples. Wang et al. (2024) argue that literary sentences that involve metaphors or similes are hard to translate, and might be suitable for LRMs. Based on this motivation, they construct MetaphorTrans dataset, and further train DRT LRMs by supervised finetuning on MetaphorTrans. R1-T1 (He et al., 2025) and MT-R1 (Feng et al., 2025) leverage Comet, CometKiwi or BLEU as reward signals to train LRMs. In view of the strong ability of LLM-asa-judge, DeepTrans-7B (Wang et al., 2025) uses DeepSeek-v3 to provide reference-free evaluation scores during RL training. Different from the above studies, we explore new reward modeling strategy to leverage the strong abilities of both LLM-as-ajudge and LLM-as-an-exemplar. Besides, we also generalize the MT LRM to multilingual settings via lightweight method."
        },
        {
            "title": "5 Conclusion",
            "content": "In recent years, large reasoning models (LRMs), e.g., OpenAI o1 (OpenAI, 2024b) and DeepSeekR1 (Guo et al., 2025), have pioneered growing research in long chain-of-thought (CoT) reasoning. Many studies make their efforts, and bring the success of LRMs to different tasks (Chen et al., In this paper, we aim to improve the MT ability of LRMs via RL. In detail, we propose new reward modeling method that employs strong LRM (i.e., DeepSeek-R1) as an exemplar, and compares the policy model with the exemplar to provide reward signals. In this way, we train ExTrans-7B LRM via RL. In addition, we design lightweight method to extend ExTrans-7B to the multilingual settings with 11 languages, resulting in mExTrans-7B. Experimental results in literary translation demonstrate the effectiveness and superiority of the proposed methods. ExTrans-7B achieves the state-of-theart performance, and it outperforms previous MT LRMs and general LRMs by large margin."
        },
        {
            "title": "Limitations",
            "content": "While we show the effectiveness of ExTrans-7B and mExTrans-7B, there are some limitations worth noting: (1) ExTrans-7B needs exemplar translations during RL training, which might be costly. For each training sample, we need to infer an advanced LRM (e.g., DeepSeek-R1-671B) to obtain the translations. (2) Though mExTrans-7B generalizes its multilingual ability, it still has gap behind the advanced LRMs (e.g., o1-preview) in the low-resource or unrepresentative translation directions. Future work could explore more effective methods to generalize the multilingual MT abilities of LRMs."
        },
        {
            "title": "Ethical Considerations",
            "content": "We discuss the main ethical considerations of (m)ExTrans-7B as follows: (1) Licenses. We will release our model checkpoints under CC-BY-NC- (2) Toxicity. The backbone of SA 4.0 license. (m)ExTrans-7B is Qwen2.5-7B-Instruct. Besides, during RL training, we use DeepSeek-v3 as the reward model to score the translation quality. Therefore, (m)ExTrans-7B might involve the same biases and toxic behaviors exhibited by these LLMs."
        },
        {
            "title": "References",
            "content": "Andong Chen, Yuchen Song, Wenxin Zhu, Kehai Chen, Muyun Yang, Tiejun Zhao, et al. 2025a. Evaluating o1-like llms: Unlocking reasoning for translation through comprehensive analysis. arXiv preprint arXiv:2502.11544. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. 2025b. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567. Zhaopeng Feng, Shaosheng Cao, Jiahan Ren, Jiayuan Su, Ruizhe Chen, Yan Zhang, Zhe Xu, Yao Hu, Jian Wu, and Zuozhu Liu. 2025. Mt-r1-zero: Advancing llm-based machine translation via r1zero-like reinforcement learning. arXiv preprint arXiv:2504.10160. Joseph Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. 2025. Deeprag: Thinking to retrieval step by step for large language models. arXiv preprint arXiv:2502.01142. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Minggui He, Yilun Liu, Shimin Tao, Yuanchang Luo, Hongyong Zeng, Chang Su, Li Zhang, Hongxia Ma, Daimeng Wei, Weibin Meng, et al. 2025. R1-t1: Fully incentivizing translation capability arXiv preprint in llms via reasoning learning. arXiv:2502.19735. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Marzena Karpinska and Mohit Iyyer. 2023. Large language models effectively leverage document-level context for literary translation, but critical errors perIn Proceedings of the Eighth Conference on sist. Machine Translation, pages 419451, Singapore. Association for Computational Linguistics. Svetlana Kiritchenko and Saif Mohammad. 2017. Bestworst scaling more reliable than rating scales: case study on sentiment intensity annotation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 465470, Vancouver, Canada. Association for Computational Linguistics. Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 193203, Tampere, Finland. European Association for Machine Translation. Daniil Larionov, Sotaro Takeshita, Ran Zhang, Yanran Chen, Christoph Leiter, Zhipin Wang, Christian Greisinger, and Steffen Eger. 2025. Deepseek vs. o3mini: How well can reasoning llms evaluate mt and summarization? arXiv preprint arXiv:2504.08120. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025a. Search-o1: Agentic searchenhanced large reasoning models. arXiv preprint arXiv:2501.05366. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. 2025b. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Sinuo Liu, Chenyang Lyu, Minghao Wu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2025. New trends for modern machine translation with large reasoning models. arXiv preprint arXiv:2503.10351. OpenAI. 2024a. Gpt-4o system card. arXiv preprint arXiv:2410.21276. OpenAI. 2024b. Learning to reason with large language https://openai.com/index/ models. learning-to-reason-with-llms/. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Team Qwen. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702, Online. Association for Computational Linguistics. Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Guijin Son, Dongkeun Yoon, Juyoung Suk, Javier AulaBlasco, Mano Aslan, Vu Trong Kim, Shayekh Bin Islam, Jaume Prats-Cristià, Lucía Tormo-Bañuelos, and Seungone Kim. 2024. Mm-eval: multilingual meta-evaluation benchmark for llm-as-a-judge and reward models. arXiv preprint arXiv:2410.17578. Katherine Thai, Marzena Karpinska, Kalpesh Krishna, Bill Ray, Moira Inghilleri, John Wieting, and Mohit Iyyer. 2022. Exploring document-level literary machine translation with parallel paragraphs from world literature. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 98829902, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jiaan Wang, Fandong Meng, Yunlong Liang, and Jie Zhou. 2024. Drt-o1: Optimized deep reasoning translation via long chain-of-thought. arXiv e-prints, pages arXiv2412. Jiaan Wang, Fandong Meng, and Jie Zhou. 2025. Deep reasoning translation via reinforcement learning. arXiv preprint arXiv:2504.10187. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. 2024. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. 2024. arXiv o1-coder: an o1 replication for coding. preprint arXiv:2412.00154. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2024. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. 2024. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 400410, Bangkok, Thailand. Association for Computational Linguistics. Score the following translation from [src lang] to [trg lang] on continuous scale from 0 to 100, where score of zero means \"no meaning preserved\" and score of one hundred means \"perfect preservation of meaning, with faithfulness, expressiveness, and elegance\". Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2024. Multilingual machine translation with large language models: Empirical results and analysis. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 27652781, Mexico City, Mexico. Association for Computational Linguistics."
        },
        {
            "title": "A Thought Reward Prompt",
            "content": "We borrow the prompt from Wang et al. (2025): translation question requires translating given text from [src lang] into [trg lang]. The given text is as follows: <text> {src} </text> Someone did this translation question, and began to think how to translate: <think> {think} </think> Please judge whether there is detailed analysis of the given text in this thinking process: 1. No analysis: Only very shallow thinking was done, and no detailed analysis of the given text was carried out. 2. Slight analysis: The given text was analyzed in detail, and how to translate it was discussed in detail. 3. Detailed analysis: The given text was analyzed in detail, and various translation possibilities were discussed in detail, and trade-offs were made. Please directly output your judgment results, such as: no analysis, slight analysis or detailed analysis"
        },
        {
            "title": "B Evaluation Prompt",
            "content": "The evaluation prompt of GRF borrows from Kocmi and Federmann (2023), and is also employed in Wang et al. (2024, 2025): [src lang] source: {src} [trg lang] translation: {hyp} Score: The prompt evaluates translation from general perspective, and achieves high correlation with humans (Kocmi and Federmann, 2023). The GEA100 prompt borrows from Wang et al. (2024), which includes system prompt and user prompt: SYSTEM PROMPT: Please evaluate the following Chinese translation of an English text. Rate the translation on scale of 0 to 100, where: - 10 points: Poor translation; the text is somewhat understandable but contains significant errors and awkward phrasing that greatly hinder comprehension for Chinese reader. - 30 points: Fair translation; the text conveys the basic meaning but lacks fluency and contains several awkward phrases or inaccuracies, making it challenging for Chinese reader to fully grasp the intended message. - 50 points: Good translation; the text is mostly fluent and conveys the original meaning well, but may have minor awkwardness or slight inaccuracies that could confuse Chinese reader. - 70 points: Very good translation; the text is smooth and natural, effectively conveying the intended meaning, but may still have minor issues that could slightly affect understanding for Chinese reader. - 90 points: Excellent translation; the text is fluent and natural, conveying the original meaning clearly and effectively, with no significant issues that would hinder understanding for Chinese reader. Please provide the reason first, followed by score. Format your evaluation in the JSON structure below: {\"reason\": \"reason for the score\", \"score\": int}"
        },
        {
            "title": "Model",
            "content": "Flu. Sem. Lit. QwQ-32B Qwen3-32B (w/ CoT) DeepTrans-7B ExTrans-7B -0.135 -0.025 0.035 0.125 -0.160 -0.055 0.085 0. -0.155 -0.130 0.090 0.195 Table 4: Human evaluation results (Flu.: fluency; Sem.: semantic accuracy; Lit.: literary quality). codes6 and the official models7. When calculating the GPT-4o evaluation metrics (GRF, GEA100 and GEA5), we set the temperature to 0.1. Since GPT-4o needs API costs, we randomly select 400 samples from each test set (MetaphorTrans, O. Henry and Orbital) to evaluate ExTrans-7B. To evaluate mExTrans-7B in the multilingual settings, in EnZh, we randomly select 400 samples; while in other directions, we randomly select 200 samples to conduct GPT-4o evaluation."
        },
        {
            "title": "D Human Evaluation",
            "content": "We conduct human evaluation to further evaluate the performance of ExTrans-7B. Following Wang et al. (2024), we randomly select 200 samples from the MetaphorTrans test set, and employ three human evaluators with high levels of fluency in English and Chinese to assess the generated translations from three aspects: fluency (Flu.), semantic accuracy (Sem.) and literary quality (Lit.). Following the Best-Worst Scaling method (Kiritchenko and Mohammad, 2017), evaluators are asked to select the best and the worst generated translation on each aspect. The result scores are calculated based on the percentage of times each model is selected as best minus the times it is selected as worst. Thus, the final scores should range from -1 (worst) to 1 (best). Table 4 shows the results of human evaluation, and we can find that ExTrans-7B outperforms other LRMs, demonstrating its effectiveness. The Fleiss Kappa scores (Fleiss, 1971) of Flu., Sem. and Lit. are 0.81, 0.63 and 0.70, respectively, indicating good inter-agreement among evaluators. USER PROMPT: <text> {src} </text> <translation> {trans} </translation> The GEA100 prompt evaluates translation from literary perspective. The GEA5 prompt simply narrows the scoring scope of GEA100 from 100-point to 5-point scale. Implementation Details. Cold Start SFT. Llama-Factory framework (Zheng et al., 2024) is used during the SFT stage. We conduct experiments on 8NVIDIA H20 GPUs (96G) with 1e-5 learning rate and 8 (81 accumulation step) batch size. DeepSpeed ZeRO-3 optimization (Rasley et al., 2020) is also used during SFT. We set the number of SFT epochs to 2, and it costs about 1 GPU hour for ExTrans-7B while 3 GPU hours for mExTrans-7B. To create the cold-start SFT data, DeepSeekR1 (671B) is deployed on 2NVIDIA H20 GPUs (96G). During generation, we set the temperature to 0.1, and the top-p to 0.95. RL Training. We use GRPO RL algorithm implemented by verl4 (Sheng et al., 2024). 28 H20 GPUs are used, where 8 GPUs are used to deploy DeepSeek-v3 (awq quantization) as the reward model, and another 8 GPUs are used to optimize the policy model. We set the batch size to 64, the learning rate to 1e-6, the rollout number to 8 and the rollout temperature to 0.6, and the KL loss coefficient to 1e-3. The number of training epochs is set to 2. The hyperparameter α and β in Eq. 5 are both set to 1.0. In this way, the CometKiwi reward rcometk retains auxiliary status compared with the exemplar-enhanced translation reward rtrans. The RL training costs 1.0K GPU hours for ExTrans-7B, while 1.2K GPU hours for mExTrans-7B. Evaluation. When evaluating model performance on the test set, we use vLLM toolkit5 to accelerate the model generation. We use the sampling decoding strategy with 0.1 temperature. To calculate CometKiwi, we leverage the official 4https://github.com/volcengine/verl 5https://github.com/vllm-project/vllm 6https://github.com/Unbabel/COMET 7https://huggingface.co/Unbabel/ wmt22-cometkiwi-da"
        }
    ],
    "affiliations": [
        "Pattern Recognition Center, WeChat AI, Tencent Inc"
    ]
}