{
    "paper_title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning",
    "authors": [
        "Chengqi Duan",
        "Rongyao Fang",
        "Yuqing Wang",
        "Kun Wang",
        "Linjiang Huang",
        "Xingyu Zeng",
        "Hongsheng Li",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at https://github.com/gogoduan/GoT-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 2 2 0 7 1 . 5 0 5 2 : r GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning Chengqi Duan1, Rongyao Fang2, Yuqing Wang1, Kun Wang3, Linjiang Huang4, Xingyu Zeng3, Hongsheng Li2, Xihui Liu 1 1HKU MMLab, 2 CUHK MMLab, 3 Sensetime, 4 Beihang University"
        },
        {
            "title": "Abstract",
            "content": "Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at https://github.com/gogoduan/GoT-R1."
        },
        {
            "title": "Introduction",
            "content": "Visual generation [33, 34, 36, 9, 30, 23, 35] has witnessed great advances in recent years, enabling the creation of diverse and realistic visuals from natural language descriptions. Despite their impressive capabilities, these models often struggle with complex and compositional prompts that specify multiple objects with precise spatial relationships and attributes [19, 20]. This limitation stems from their direct mapping from text embeddings to visual features without explicit reasoning of the compositional structure of the desired scene. The Generation Chain-of-Thought (GoT) [10] framework tackles this challenge by introducing an intermediate semantic-spatial reasoning process that decomposes complex prompts into explicit object descriptions with location coordinates before image generation, significantly improving compositional fidelity. However, GoTs reasoning capability is gained from supervised fine-tuning with annotated data based on human-defined templates, which fundamentally limits the models ability to discover more effective reasoning strategies autonomously for diverse visual scenarios. We observe that the reasoning chains generated by GoT are good at template following but can be unfaithful to the text prompt, as shown in the left example of Fig. 1. In parallel with advancements in visual generation, recent work in language models has demonstrated that reinforcement learning (RL) can significantly enhance chain-of-thought reasoning capabilities. Models like OpenAI o1 [31] and DeepSeek-R1 [7] show that language models can discover sophisticated reasoning strategies through self-improvement. Inspired by these developments, we introduce GoT-R1, framework that applies reinforcement learning to improve semantic-spatial reasoning in visual generation. Preprint. Under review. Figure 1: GoT-R1 enhances visual generation through reinforcement learning. This figure demonstrates the improvement from GoT-finetuned model (left) to the RL-trained GoT-R1 model (right). The model before RL generates spatially misaligned reasoning process. The RL process enhances the models semantic-spatial reasoning capabilities, as demonstrated by its Generation Chain-of-Thought, leading to generated image that is more closely aligned with the prompt. Extending reinforcement learning to enhance the reasoning abilities of visual generation models presents unique challenges, unlike those encountered in code, mathematics, or traditional language tasks. First, designing appropriate reward mechanisms for visual generation is particularly challenging, as evaluating visual outputs requires assessing different dimensions: semantic fidelity to the prompt, accurate spatial arrangement of objects, proper binding of attributes to entities, coherence, and aesthetic quality. Second, optimizing solely on end-result rewards is suboptimal as it leaves the reasoning process unsupervised, potentially creating misalignments between the prompt, reasoning chain, and final image. Without explicit process supervision, the model may generate visually coherent but compositionally incorrect images, or fail to translate well-planned reasoning into accurate visual generation. Therefore, effective reinforcement learning for visual generation necessitates comprehensive reward framework that evaluates both the reasoning process and the final output. To address these challenges and inspired by the strong visual understanding and reasoning capabilities of multimodal large language models (MLLMs) [2, 26, 31, 44], we leverage an MLLM-based base model for visual generation and propose dual-stage Reinforcement Learning (RL) framework with unified MLLM-based multi-dimensional rewards. Our base generation model is an auto-regressive unified MLLM which takes text prompts as input and outputs the reasoning chain followed by sequence of image tokens. Our reward model evaluates both the reasoning process and the final image output through comprehensive set of reward signals: (1) prompt-to-reasoning semantic alignment, which assesses how well the reasoning chain captures the textual content; (2) prompt-to-reasoning spatial alignment, which evaluates the fidelity of planned spatial arrangements; (3) reasoning-toimage alignment, which measures how faithfully the generated image reflects the planned reasoning; and (4) prompt-to-image alignment, which evaluates the overall quality and compositional accuracy of the generated image. We leverage MLLMs as reward models due to their ability to make nuanced judgments about text-image correspondence that align well with human assessments. We also enhance MLLMs spatial evaluation capability by transforming bounding box coordinates into visualized bounding boxes drawn on blank canvas, improving the reliability of the prompt-to-reasoning spatial reward. Through careful reward design and the adoption of Group Relative Policy Optimization (GRPO) [7], GoT-R1 enables models to autonomously discover effective reasoning strategies for complex visual scenes. Experimental results demonstrate significant improvements over the baseline model on T2ICompBench benchmark, advancing the state of compositional image generation. Figure 1 illustrates how GoT-R1 substantially improves the handling of compositional prompts. In summary, our main contributions are: We propose GoT-R1, framework that enhances the semantic-spatial reasoning abilities for visual generation by reinforcement learning, enabling models to discover effective reasoning strategies autonomously beyond predefined patterns. We design comprehensive dual-stage multi-dimensional reward framework that evaluates both the intermediate reasoning process and final visual output from multiple perspectives, addressing the unique challenges of reinforcement learning for visual generation. 2 We demonstrate significant performance improvements on the T2I-CompBench [21], particularly in compositional tasks requiring precise spatial relationships and attribute binding."
        },
        {
            "title": "2 Related work",
            "content": "Text-Driven Visual Generation Recent advancements in text-driven visual generation have been dominated by two main paradigms: diffusion models and autoregressive approaches. Diffusion models [36, 35, 30, 34, 56, 33, 23, 51] have demonstrated remarkable success in generating highfidelity images from text prompts by iteratively denoising an initial noise map. Autoregressive approaches [40, 24, 43, 17, 48, 53, 47, 12, 46], on the other hand, typically treat image generation as sequence modeling problem. They often represent images as sequence of discrete visual tokens (e.g., from VQGAN) or patches and generate them element by element, commonly using large transformer architectures conditioned on textual input. Despite continuous improvements in generation quality, these methods still struggle with complex scenes involving complex text understanding, precise spatial relationships and attribute binding among multiple objects. Several studies have attempted to leverage large language models to enhance image generation capabilities. Models such as Chameleon [42], Emu3 [45], and Janus [49, 6] explore unified architectures for visual understanding and generation. However, these approaches have yet to demonstrate that reasoning capabilities effectively translate to improved generation quality. Recently, GoT [10] introduced explicit semantic-spatial reasoning into image generations. Multimodal Large Language Models Multimodal Large Language Models (MLLMs)[1, 2, 31] integrate vision encoders with LLMs, demonstrating strong visual understanding, sophisticated reasoning, and semantic analysis. Advanced MLLMs further enhance spatial understanding by grounding textual concepts to image regions[26, 32, 11]. However, despite unification attempts (e.g., Janus [49]) and models incorporating generation (e.g., Chameleon [42], Emu2 [41]), there remains significant disconnect between understanding and generation capabilities. The rich semantic and spatial reasoning abilities of MLLMs are not yet fully leveraged in the generation process, as seen in models that generate images but may not fully utilize explicit semantic-spatial reasoning for synthesis. Reinforcement Learning for Reasoning Reinforcement Learning (RL) has emerged as powerful approach for enhancing reasoning capabilities in large models. The success of OpenAI o1 [31] and DeepSeek-R1 [7]demonstrates how RL can significantly improve reasoning in language models. notable algorithm contributing to some of these advancements is Group Relative Policy Optimization (GRPO) [38]. GRPO is an efficient reinforcement learning technique that enhances policy learning by evaluating and normalizing rewards among group of sampled candidate outputs from the model, eliminating the need for separate critic model. Recent work has extended these techniques to multimodal domains. [5, 8, 28, 52, 55] Vision-R1 [54] applies rule-based RL to enhance object localization in vision-language models without specialized reward models, using criterion-driven reward functions that evaluate completions based on visual feedback. Concurrent to our work, T2IR1 [22] introduces BiCoT-GRPO to jointly optimize semantic-level and token-level Chain-of-Thought reasoning for image generation, incorporating diverse vision experts as reward models."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present the details of our GoT-R1 framework. We first review the prerequisite knowledge including the Generation Chain-of-Thought (GoT) paradigm and Group Relative Policy Optimization (GRPO) algorithm in Section 3.1. Then, we describe our GoT-R1 framework in Section 3.2, including the network architecture and training strategy. In Section 3.3, we elaborate on our MLLM-based dual-stage multi-dimensional reward design. The reward system thoroughly evaluates the alignment between prompt, reasoning, and generated image to provide comprehensive supervision signals for effective reinforcement learning."
        },
        {
            "title": "3.1 Preliminary",
            "content": "Generation Chain-of-Thought (GoT) Generation Chain-of-Thought (GoT) [10] is paradigm that transforms visual generation through an explicit visual-semantic chain-of-thought reasoning process before outputting images. Unlike conventional text-to-image generation methods that directly map text embeddings to visual features, GoT decomposes complex prompts into reasoning chain with 3 Figure 2: The GoT-R1 framework illustrating the reinforcement learning process with Group Relative Policy Optimization (GRPO). Left: Overview of the candidate sampling and initial evaluation stage, where diverse reasoning chains (GoT) and corresponding image tokens are generated from an input prompt, with an MLLM-based reward model providing preliminary scoring. Right: Detailed illustration of how MLLM-based rewards and advantages facilitate model updates via GRPO. both semantic descriptions and spatial coordinates. For example, given the prompt \"A dog and cat playing together,\" GoT reasoning chain might include descriptions like \"a playful brown dog\" with coordinates (100, 200), (350, 450) and \"an orange tabby cat\" with coordinates (400, 250), (650, 500), specifying both semantic attributes and spatial positioning of each object. This explicit chain-ofthought reasoning enables precise control over object attributes, spatial arrangements, and inter-object relationships, significantly improving compositional fidelity in the generated images. In order to enable reasoning abilities of the generation model, GoT constructs large-scale training data with annotated reasoning chains following hand-crafted templates. The GoT framework is trained with the annotated data in supervised manner to generate the reasoning chains and images. However, this approach is inherently limited by the hand-crafted and fixed reasoning templates in the training data, preventing the model from discovering more effective reasoning strategies for diverse scenarios. Moreover, the GoT framework trained with supervised fine-tuning tends to generate templated but sometimes infaithful reasoning chains, which can bottleneck subsequent visual generation. Group Relative Policy Optimization (GRPO) Group Relative Policy Optimization (GRPO) is proposed by DeepSeek-R1 [38] to incentivize reasoning capabilities of large language models. It is an efficient RL algorithm that eliminates the need for separate critic model. For each question q, GRPO samples group of outputs {oi}G i=1 from the current policy πθold. These outputs are evaluated using reward functions to obtain individual rewards {ri}G i=1. The advantage for each sample is computed by normalizing the rewards within the group: The policy is then updated by optimizing the following objective: Ai = ri mean({rj}G std({rj}G j=1) j=1) JGRPO(θ) = qD,{oi}G i=1πθold (q) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 min (ri(θ)Ai, clip(ri(θ), 1 ϵ, 1 + ϵ)Ai) βDKL(πθπref) (cid:35) (1) (2) where ri(θ) = πθ(oiq) πθold (oiq) is the probability ratio, ϵ is the clipping parameter, and β controls the strength of the KL divergence penalty from reference policy πref. This group-based approach provides computationally efficient method for policy optimization while effectively leveraging relative performance differences within each group of samples."
        },
        {
            "title": "3.2 GoT-R1 Framework",
            "content": "GoT-R1 builds upon the Generation Chain-of-Thought (GoT) [10] framework for text-to-image generation by introducing reinforcement learning to enhance semantic-spatial reasoning capabilities. As discussed earlier, while GoT provides strong foundation for compositional image generation, its effectiveness is limited by predefined reasoning templates in the training data. Our framework addresses this limitation by enabling the model to autonomously discover better reasoning strategies through reinforcement learning while maintaining the end-to-end optimization. 4 Figure 3: Overview of our MLLM-based dual-stage multi-dimensional reward framework. The diagram illustrates MLLM-based rewards assessing the intermediate GoTs semantic and spatial fidelity to the prompt, as well as the final images alignment with both the prompt and the GoT. Network Architecture We adopt unified MLLM that jointly models text and image tokens as our base architecture. For example, Janus-Pro [6] is capable of visual understanding and generation tasks within single model, processing images as discrete tokens alongside text tokens with joint autoregressive modeling. This architecture allows us to generate textual reasoning chains and visual outputs in an end-to-end manner, enabling comprehensive optimization of the entire generation process. Training Strategy Our base model has been trained on text-to-image generation task without chain-of-thought reasoning processes. To incentivize the reasoning abilities, our training process consists of two stages: In the first stage, we fine-tune the pre-trained model with reasoning chain and generated image annotations from GoT dataset [10]. This stage of SFT establishes the basic capability to generate templated reasoning chains before generating image tokens, providing strong initialization for reinforcement learning. In the second stage, we apply reinforcement learning to guide the model to explore free-style and more effective reasoning chains. For each prompt , we sample different reasoning chains and corresponding images. These samples are then evaluated using our multi-dimensional reward function, which assesses both reasoning quality and generation fidelity. The model parameters are updated using GRPO to encourage high-reward reasoning strategies and generated images, and discourage the low-reward ones. The specific design of our reward function, which addresses the unique challenges of evaluating visual reasoning quality, is detailed in the following subsection."
        },
        {
            "title": "3.3 MLLM-based Dual-stage Multi-dimensional Reward",
            "content": "The GoT-R1 generation framework is composed of two stages: prompt to reasoning chain generation, and reasoning chain to image generation. straightforward integration with reinforcement learning would be to apply an end-to-end reward based solely on prompt-image alignment. However, without explicit constraints on the intermediate reasoning process, the reasoning chains may become unfaithful to the prompt or inconsistent with the final image, undermining the interpretability and controllability of the generation pipeline. To guide the model toward faithful and consistent generation, we design dual-stage reward mechanism with both result and intermediate process supervision. Specifically, we define three categories of rewards: (1) RP measures the alignment between Prompt and generated Image, (2) RP measures the faithfulness of Reasoning process to input Prompt, and (3) RRI measures the fidelity of generated Image to Reasoning process. For the prompt-to-reasoning alignment reward RP R, we further decompose the reward into two distinct aspectssemantic reward Rsem and layout reward Rspato ensure both the semantics and spatial arrangement in the reasoning process faithfully reflect the input prompt. All rewards are scaled to range [0,1]. We define total reward Rtotal as the product of individual rewards: Rtotal = RP RP RRI = RP (Rsem + Rspa) RRI (3) 5 MLLMs are uniquely well-suited as reward models in this context due to their strong cross-modal understanding and reasoning capabilities. Trained on large-scale image-text pairs, MLLMs can provide unified, interpretable, and fine-grained evaluations for both reasoning chains and generated images across diverse aspects such as semantic consistency and spatial arrangement. This makes them ideal for reward functions in reinforcement learning settings, where conventional metrics often fall short in providing nuanced multi-dimensional feedback. The rewards are demonstrated in Fig. 3. Prompt-Image Reward (RP ) The most intuitive reward design is the overall alignment between the input prompt and generated image. Leveraging the outstanding image understanding capabilities of MLLM, we utilize it to perform multi-dimensional evaluations of the final generated image, assessing whether it aligns with the composition (objects, attributes, layout etc.) specified in the prompt. The MLLM takes the input prompt and the generated image as input and predicts discrete score ranging from 0 to 10 where 10 stands for the best. Prompt-Reasoning Semantic Reward (Rsem) To assess semantic consistency between the input prompt and generated GoT reasoning, we leverage MLLMs to evaluate each GoT in terms of missing elements (attributes), internal contradictions, logical consistency, and formatting quality. Specifically, the GoT reasoning along with the input prompt are input to MLLM to assess the reasoning chain from four dimensions with score from 0 to 10: 1) Completeness: Does the reasoning chain include all concepts mentioned in the prompt? 2) Faithfulness: Does it introduce any content that contradicts the prompt? 3) Consistency: Is the reasoning logically aligned with the described scene? 4) Clarity: Is the content coherent and properly formatted? Prompt-Reasoning Spatial Reward (Rspa) To evaluate the correctness of spatial planning by the reasoning chain, our MLLM reward model assesses whether the GoT object coordinates follow the spatial relationship (e.g., \"left\" or \"top\") from the prompt. However, lightweight LLMs or MLLMs exhibit limited sensitivity to bounding box coordinates and relationships between different spatial locations. To bridge this capability gap, we propose an innovative MLLM-based layout evaluation approach based on critical observation: MLLMs exhibit superior spatial comprehension when processing visual data compared to coordinates in text form. Therefore, we convert textual coordinates into images by rendering corresponding bounding boxes on blank canvas. With this visual format, the MLLM demonstrates significantly better spatial understanding and can provide clear and accurate scoring of the reasoning chains spatial correctness. Figure 4 presents an illustration of this process. Figure 4: Prompt-Reasoning Spatial Reward Rspa process. For robust spatial evaluation, the MLLM assesses bounding boxes rendered on an image from the GoTs textual coordinates, rather than processing the coordinates directly as text. Reasoning-Image Reward (RRI ) During reinforcement learning, the model can occasionally generate images that deviate from its planned reasoning path. To further ensure that the GoT reasoning is faithfully reflected in the generated image, our framework incorporates an alignment reward between the GoT reasoning process and the generated image. Specifically, we expect each object planned in the GoT to appear at the corresponding location in the image. An MLLM is used to identify the location of each object in the generated image, yielding grounded bounding boxes denoted as BImage. For every object specified in GoT, we define its alignment reward as the Intersection over Union (IoU) between the planned bounding box (BGoT) and its grounded counterpart in the image (BImage). The overall reward RRI is then calculated as the average IoU across all objects."
        },
        {
            "title": "4.1 Training Settings",
            "content": "We trained two models separately based on Janus-Pro-1B and Janus-Pro-7B [6]. Our training process contains two stages: Pretraining on GoT-T2I dataset [10] and online GRPO [38] reinforcement learning with constructed prompt set. Specifically, We pretrain our model with LAHR-GoT [37],"
        },
        {
            "title": "Shape Texture",
            "content": "2D-Spatial Non-Spatial Complex"
        },
        {
            "title": "Diffusion Models",
            "content": "SD-v1.5 [35] SD-XL-base-1.0 [33] DALLE3 [4] Stable v3 [9] FLUX.1 [23] 0.3758 0.5879 0.7785 0.8132 0.7407 0.3713 0.4687 0.6205 0.5885 0.5718 0.4186 0.5299 0.7036 0.7334 0.6922 0.1165 0.2131 0.2865 0.3200 0.2863 Layout Guided Two-stage Models Ranni [14] LayoutGPT-Llama7B [13] 0.6893 0.3296 0.4934 0.3654 0.6325 0.3982 0.3167 0.1443 Auto-regressive Models Emu3 [45] Janus-Pro-1B [6] Janus-Pro-1B-GoT GoT-R1-1B Janus-Pro-7B [6] Janus-Pro-7B-GoT GoT-R1-7B 0.7544 0.3411 0.6336 0.7632 0.6359 0.6551 0.8139 0.5706 0.2261 0.4456 0.5174 0.3528 0.5008 0.5549 0.7164 0.2696 0.5621 0.6589 0.4936 0.5836 0.7339 - 0.0968 0.2140 0.2674 0.2061 0.2457 0.3306 0.3112 0.3119 0.3003 0.3140 0. - 0.2990 - 0.2808 0.3070 0.3101 0.3085 0.3113 0.3169 0.3047 0.3237 0.3773 0.3771 0.3703 - 0.2768 - 0.2721 0.3490 0.3749 0.3559 0.3754 0.3944 Table 1: Quantitative evaluation of text-to-image generation on T2I-CompBench. GoT models refer to Janus-Pro finetuned using the GoT framework, while GoT-R1 models denote further training via GRPO on the GoT-finetuned checkpoints. GoT-R1 models are evaluated under guidance scale 5. JourneyDB-GoT [39] and FLUX-GoT [10] datasets for 70000 steps, followed by 1000 steps of GRPO. Our constructed dataset for GRPO consists of prompts from T2I-Compbench [21] training dataset and Laion-Aesthetics. When training with GRPO, the overall reward is calculated as the product of individual rewards described in Section 3.3. We also apply HPS v2.1 [50] to improve generation quality. We employ low-rank adaptation (LoRA) [18] to efficiently update the MLLM, with rank and lora alpha set to 32. Both phases operate end-to-end. In our GRPO training setup, we adopt batch size of 8, learning rate of 105, and employ cosine learning rate schedule. For each input, we sample group of = 16 candidates and set both the text and image temperatures to 1.0. As the reward model, we adopt Qwen2.5VL-7B [3]. The loss is computed over the entire generated output sequence. GRPO training was conducted on 8 NVIDIA L40S GPUs in approximately 48 hours."
        },
        {
            "title": "4.2 Quantitative Evaluation",
            "content": "Table 4.2 presents an evaluation of text-to-image (T2I) generation performance on the T2ICompBench [21]. We compare our model against three main categories: (1) Diffusion models that directly map textual input to images with frozen encoders, and (2) Two-stage models, which first plan structured layout and subsequently generate the image accordingly. (3) Auto-regressive models that incorporate LLMs or MLLMs to enhance generation. The GoT-R1 framework establishes new state-of-the-art in compositional text-to-image generation. After just 1000 GRPO fine-tuning steps on GoT-finetuned checkpoint, it delivers up to 15 % boost in evaluation metrics. GoT-R1-7B secures the top score in five of six evaluation categories and shows significant advantage on the Complex benchmark, which consists of mixed natural-language compositional prompts. In shape category, GoT-R1-7B delivers performance similar to FLUX. Our 7B model performs way better than other layout guided models in every category. GoT-R1-1B also demonstrates better performance than Janus-Pro-7B [6] and even surpasses FLUX in color attribute. These gains highlight the effectiveness of combining structured reasoning process with reinforcement-guided optimization for compositional image synthesis."
        },
        {
            "title": "4.3 Qualitative Evaluation",
            "content": "Figure 5 presents qualitative comparison among the base model Janus-Pro-7B, the GoT-finetuned model Janus-Pro-7B-GoT, and our GRPO-enhanced model GoT-R1-7B. We showcase examples generated from compositional prompts involving multiple attributes, relative spatial relationships, and object numeracy. While the GoT-finetuned model produces images of higher quality than the base model, it still struggles with complex compositional generation. In contrast, GoT-R1-7B demonstrates 7 Figure 5: Qualitative comparison among the base model Janus-Pro-7B, the GoT-finetuned checkpoint Janus-Pro-7B-GoT, and our GRPO-enhanced model GoT-R1-7B. Our model demonstrates superior performance on prompt alignment and image quality. Method Baseline wRP wRRI wRP wRP R&RP wRP R&RRI wRRI &RP wRsem wRspa GoT-R1-1B Rsem Rspa RRI RP Color Shape Texture 2D-Spatial Non-Spatial Complex 0. 0.4456 0.5621 0.7050 0.3340 0.7401 0.7289 0.7118 0.6507 0.7323 0.7067 0.4671 0.2563 0. 0.4893 0.4582 0.4299 0.4729 0.4685 0.6075 0.3940 0.6308 0.6485 0.6243 0.5913 0.6251 0.6115 0. 0.5174 0.6589 0.2140 0.2283 0.0076 0.2398 0.2557 0.2579 0.1797 0.2133 0. 0.2674 0.3070 0.3089 0.2537 0.3076 0.3094 0.3097 0.3010 0.3094 0.3089 0. 0.3490 0.3619 0.2488 0.3724 0.3653 0.3583 0.3452 0.3568 0.3648 0.3749 Table 2: Ablation study on reward design. All models are trained for 1000 steps using GRPO based on the Janus-Pro-1B-GoT (Baseline). Evaluations are conducted with guidance scale of 5. stronger prompt alignment, accurately reflecting even unnatural prompts in its generations. In addition, GoT-R1-7B generates detailed and aesthetically appealing visual contents. These gains are largely attributed to our MLLM-based reward design, which guides the model to optimize both semantic and spatial alignment across the GoT reasoning process and output image. By leveraging fine-grained evaluations from MLLM, our reward formulation enables GoT-R1-7B to excel not only in visual quality but also in faithfully capturing the intent of complex prompts."
        },
        {
            "title": "Method",
            "content": "To assess the quality of reasoning, we compared the self-explored Generation Chain-of-Thought from GoTR1-7B against the predefined GoT of Janus-Pro-7B-GoT. GPT-4o [1] evaluated the GoT content for 100 prompts randomly sampled from each of T2I-CompBenchs Color, Spatial, and Complex categories, plus 100 from LAION-5B [37]. Voting was based on four criteria: relevance to the input prompt, accuracy of object descriptions and bounding boxes, and the clarity and Table 3: GPT-4o vote results comparing Janus-Pro-7B-GoT and GoT-R1-7B on GoT quality. Janus-Pro-7B-GoT GoT-R1-7B Spatial Complex LAION-5B"
        },
        {
            "title": "Color",
            "content": "21 79 16 84 29 71 31 69 8 fluency of the text. As detailed in Table 3, GoT-R1-7Bs self-explored reasoning is overwhelmingly preferred by GPT-4o across all evaluated categories. This strong preference underscores GoT-R1s ability to autonomously discover and generate superior reasoning paths, key factor contributing to its enhanced compositional generation capabilities."
        },
        {
            "title": "4.5 Ablation Study on Reward Design",
            "content": "We conduct thorough ablation study on our MLLM-based dual-stage multi-dimensional reward and key training settings to validate their contributions. All ablation experiments are performed on T2I-CompBench, and trained for 1000 steps using GRPO based on the Janus-Pro-1B-GoT checkpoint, which serves as our baseline. Results, displayed in Table 2 and Table 4, are evaluated under guidance scale of 5. Ablation Study on Reward Design Table 2 results for models trained with only single reward component highlight their individual contributions and limitations. Training with only RP yields the best performance among these single-reward variants but still falls short of the full GoT-R1-1B, as the GoT reasoning process is largely bypassed. Relying solely on RP leads to poorer outcomes, underscoring the necessity of rewarding the final generated image. Furthermore, using only RRI can be detrimental, because the absence of prompt-reasoning reward RP results in misaligned reasoning process and thus provides harmful guidance to image generation. Further experiments in Table 2, where individual reward components are removed from our full reward set, reinforce this conclusion. Removing either RRI or RP leads to noticeable degradation in performance. Critically, removing RP while retaining RRI once again results in more significant performance decline, as the model attempts to align the image with potentially flawed reasoning. These findings collectively justify the importance of our comprehensive reward design that aligns all stages of the generation process. Method Ablation Study on RP Composition Regarding the composition of RP R, we ablate its two constituents, Rsem(prompt-reasoning semantic reward) and Rspa (prompt-reasoning spatial reward), by training models where only one is active. The results in Table 2 demonstrate their complementary roles: Rsem primarily enhances attribute binding, whereas Rspa improves spatial consistency, confirming the value of their combination within RP R. Ablation Study on Training Details We further ablate three key settings in our training. In our configuration, the total reward Rtotal is the product of its individual terms. We evaluate an alternative setting that sums the rewards instead. Moreover, we ablate our novel MLLM layout evaluation approach, where instead of converting GoT layout plans to image for MLLM to assess, Rspa is given by MLLM evaluating GoT layout directly from its textual coordinates. Last but not least, we replace all MLLM-based rewards with conventional metrics: CLIP similarity for the prompt-image reward and Grounding DINO [27] for the reasoning-image alignment. The results are presented in Table 4. The findings affirm the efficacy of our specific training configurations in optimizing GoT-R1. Table 4: Ablation study on training details. We present results on T2I-Compbench evaluated under guidance scale 5. Baseline Sum reward Text evaluated Rspa Conventional rewards GoT-R 0.2140 0.2254 0.2158 0.1388 0.2674 0.6336 0.7154 0.7166 0.5914 0.7632 0.4456 0.4385 0.4289 0.4284 0.5174 0.5621 0.5608 0.6311 0.5607 0.6589 0.3070 0.3080 0.3098 0.2936 0.3101 0.3490 0.3638 0.3554 0.3353 0. Spatial Non-Spatial Complex Shape Texture Color"
        },
        {
            "title": "5 Conclusion and Disscussion",
            "content": "In conclusion, this paper introduce GoT-R1, novel framework that significantly enhances visual generation by applying reinforcement learning to semantic-spatial reasoning. Building upon the Generation Chain-of-Thought methodology, GoT-R1 empowers models to autonomously discover effective reasoning strategies, moving beyond the limitations of predefined templates. key innovation is the dual-stage multi-dimensional reward system, which leverages MLLMs to comprehensively evaluate both the intermediate reasoning process and the final visual output, ensuring robust supervision across the generation pipeline. This reward mechanism assesses critical aspects such as semantic alignment and spatial accuracy. Evaluation results demonstrate GoT-R1s superior performance on the T2I-CompBench, particularly in complex compositional tasks requiring precise spatial relationships and attribute binding. By successfully transferring self-explored sophisticated reasoning capabilities to the visual generation domain, GoT-R1 advances the state-of-the-art and opens new avenues for creating more accurate and contextually aware visual content. However, as with all powerful genera9 tive AI, the responsible development and deployment of such technology are paramount to mitigate potential risks, such as misuse for disinformation, and to ensure ethical application."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [5] Liang Chen, Lei Li, Haozhe Zhao, and Yifan Song. Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning. [6] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [7] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [8] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025. [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [10] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, and Hongsheng Li. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. [11] Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, and Xihui Liu. Puma: Empowering unified mllm with multi-granular visual generation. arXiv preprint arXiv:2410.13861, 2024. [12] Rongyao Fang, Shilin Yan, Zhaoyang Huang, Jingqiu Zhou, Hao Tian, Jifeng Dai, and Hongsheng Li. Instructseq: Unifying vision tasks with instruction-conditioned multi-modal sequence generation. arXiv preprint arXiv:2311.18835, 2023. [13] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36:1822518250, 2023. [14] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-to-image diffusion for accurate instruction following. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47444753, 2024. [15] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [16] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [17] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis, 2024. [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 10 [19] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [20] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2I-CompBench++: IEEE An Enhanced and Comprehensive Benchmark for Compositional Text-to-Image Generation . Transactions on Pattern Analysis Machine Intelligence, (01):117, January 5555. [21] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [22] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. [23] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [24] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [25] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv e-prints, pages arXiv2402, 2024. [26] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [27] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [28] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoningchain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. [29] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. [30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [31] OpenAI. Introducing openai o1. https://openai.com/o1, 2025. [32] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [33] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [37] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [38] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 11 [39] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in neural information processing systems, 36:4965949678, 2023. [40] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [41] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. [42] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [43] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. 2024. [44] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37:121475121499, 2025. [45] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [46] Yuqing Wang, Zhijie Lin, Yao Teng, Yuanzhi Zhu, Shuhuai Ren, Jiashi Feng, and Xihui Liu. Bridging continuous and discrete tokens for autoregressive visual generation. arXiv preprint arXiv:2503.16430, 2025. [47] Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. arXiv preprint arXiv:2412.15119, 2024. [48] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. [49] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation, 2024a. URL https://arxiv. org/abs/2410.13848, 2024. [50] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. CoRR, 2023. [51] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. [52] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [53] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. [54] Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning, 2025. [55] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [56] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023."
        },
        {
            "title": "A Qualitative Evaluation",
            "content": "We present more qualitative analysis on our GoT-R1-7B model in Figure 6. This figure showcases comparison of text-to-image generation capabilities among the baseline Janus-Pro-7B, the GoT-finetuned Janus-Pro-GoT-7B, and our GoT-R1-7B model across various prompts, highlighting differences in image quality and prompt adherence. Figure 6: Samples of text-to-image generation by Janus-Pro-7B, Janus-Pro-GoT-7B and GoT-R1-7B. Method Architecture Overall Single Obj. Two Obj. Counting Colors Position Attr. Binding Frozen Text Encoder Mapping Methods SDv1.5 [35] SDv2.1 [35] SD-XL [33] DALLE-2 [34] SD3 (d=24) [9] Unet+CLIP Unet+CLIP Unet+CLIP Unet+CLIP MMDIT+CLIP+T5 LLMs/MLLMs Enhanced Methods LayoutGPT [13] LlamaGen [40] Chameleon [42] LWM [25] SEED-X [15] Emu3-Gen [45] Janus [49] JanusFlow [29] GoT [10] Unet+Llama Autoregressive Autoregressive Autoregressive Unet+Llama Autoregressive Autoregressive Autoregressive Unet+Qwen2.5-VL Janus-Pro-7B-GoT GoT-R1-7B Autoregressive Autoregressive 0.43 0.50 0.55 0.52 0.62 0.41 0.32 0.39 0.47 0.49 0.54 0.61 0.63 0.64 0.64 0.75 0.97 0.98 0.98 0.94 0. 0.97 0.71 - 0.93 0.97 0.98 0.97 0.97 0.99 0.99 0.99 0.38 0.51 0.74 0.66 0.74 0.51 0.34 - 0.41 0.58 0.71 0.68 0.59 0.69 0.69 0.94 0.35 0.44 0.39 0.49 0. 0.26 0.21 - 0.46 0.26 0.34 0.30 0.45 0.67 0.48 0.50 0.76 0.85 0.85 0.77 0.67 0.56 0.58 - 0.79 0.80 0.81 0.84 0.83 0.85 0.85 0.90 0.04 0.07 0.15 0.10 0. 0.11 0.07 - 0.09 0.19 0.17 0.46 0.53 0.34 0.43 0.46 0.06 0.17 0.23 0.19 0.36 0.07 0.04 - 0.15 0.14 0.21 0.42 0.42 0.27 0.43 0.68 Table 5: Evaluation of text-to-image generation on GenEval benchmark [16]. Obj.: Object. Attr.: Attribution."
        },
        {
            "title": "B Quantitative Analysis",
            "content": "As demonstrated in Table 5, on the GenEval benchmark, our GoT-R1-7B model establishes new state-of-the-art, achieving the highest overall score of 0.75 among all listed models. Moreover, the results represent substantial advancement over Janus-Pro-GoT-7B. The performance gains are particularly striking in critical compositional abilities. For instance, when compared to Janus-ProGoT-7B, GoT-R1-7B demonstrates an improvement from 0.69 to 0.94 in two-object generation, 1 and the attribute binding score improves markedly from 0.43 to 0.68. Beyond these key areas, GoT-R1-7B demonstrated broad enhancements across various other categories, further underscoring the comprehensive benefits of our approach. These quantitative results strongly validate the efficacy of our proposed GoT-R1 framework in augmenting reasoning capabilities through reinforcement learning, leading to superior outcomes in complex visual generation tasks. MLLM-based Reward Evaluation Prompts We present the prompt used in our paper in Figure[ 7, 8, 9, 10, 11]. Specifically, Figure 7 details the prompt used for evaluating the semantic consistency between prompt and reasoning chain. Figure 8 shows the prompt for evaluating the spatial layout predicted in reasoning chain. Figure 9 displays the assessment prompt for prompt-image alignment. Figure 10 illustrates the prompt used for grounding in the reasoning-image reward. Figure 11 provides the prompt utilized for comparing reasoning chains with GPT-4o. Human: You are professional image caption evaluator. You will evaluate how well detailed AI-generated caption aligns with brief image prompt. You will be given: 1. brief image prompt that describes what should be in the image 2. detailed caption that was generated based on the brief prompt Your task is to evaluate if the detailed caption is aligned with and faithful to the brief prompt. Consider: - Does the detailed caption include all elements from the brief prompt? - Does the detailed caption add elements that contradict the brief prompt? - Is the detailed caption reasonable and consistent with what the prompt describes? - Is the caption coherent and properly formatted? The score should be from 0 to 10: - 0: Completely nonsensical output, messy code, or gibberish that fails to function as caption - 1-2: Severe misalignment. The detailed caption fails to represent key elements or completely contradicts the brief prompt - 3-4: Poor alignment with significant omissions or contradictions to the brief prompt - 5-6: Moderate alignment with some missing elements or noticeable inconsistencies - 7-8: Strong alignment with minor inconsistencies or additions that dont contradict the prompt - 9-10: Perfect alignment. The detailed caption faithfully includes all elements from the brief prompt with appropriate elaboration Brief prompt: <Prompt> Detailed caption: <Reasoning Chain> Note to only ouput with dictionary with score in this format: {\"score\": ...} Assistant: Figure 7: Prompt for Rsem evaluation. 2 Human: Determine if objects are arranged as described in the prompt by analyzing the image. ORIGINAL IMAGE PROMPT: prompt COORDINATE SYSTEM EXPLANATION: - The image shows object bounding boxes with names labeled at the top-left corner of each box SCORING RULES: - Score 8-10 if the objects are shown in the image and their positions MATCH the relationship in the prompt * 10: Perfect match with clear relationship * 9: Strong match with minor ambiguity * 8: Good match with some ambiguity - Score 5-7 if the relationship is partially correct or ambiguous * 7: Mostly correct with some misalignment * 6: Relationship is ambiguous but leaning toward correct * 5: Borderline case where relationship could be interpreted either way - Score 1-4 if the objects are NOT shown in the image or positions CONTRADICT the relationship in the prompt * 4: Slight contradiction or missing one object * 3: Clear contradiction but objects are present * 2: Major contradiction or missing multiple objects * 1: Complete mismatch with the prompt Please answer in order to: Verify if the objects are shown in the ORIGINAL IMAGE PROMPT. Decide if the relationships between objects match what is described in the ORIGINAL IMAGE PROMPT. Your response MUST be formatted as: {{ \"reasoning\": ..., \"score\": ... }} Output only the dictionary with nothing else. <Image> Visualized reasoning chain </Image> Assistant: Figure 8: Prompt for Rspa evaluation. 3 Human: You are an expert in visual analysis specializing in compositional accuracy evaluation. Your task is to compare the caption with an image and assess ONLY how well the image matches the described elements, objects, and their relationships. Analyze: Compositional accuracy: Evaluate if all key elements mentioned in the caption appear in the image with correct relationships, positioning, and attributes as specified. EVALUATION CRITERIA: 1. Object Presence: Are the key objects mentioned in the image prompt correctly shown in the image? 2. Spatial Positioning: Are the objects positioned in the EXACT spatial relationships described in the caption? Pay special attention to positional terms like \"on top of,\" \"next to,\" \"inside,\" \"left of,\" \"right of,\" \"behind,\" \"in front of,\" etc. Examples of STRICT spatial interpretations: - \"Left of\" means the object must be positioned horizontally to the left, not above, below, or on top. - \"On top of\" means the object must be directly above and touching, not beside or below. Compositional accuracy score (0-10): - 8-10: Perfect match. Image contains all elements with EXACTLY the spatial relationships described. - 5-7: Minor mismatch. All objects present but with slightly incorrect spatial relationships. - 0-4: Major mismatch. Objects present but with completely incorrect spatial relationships, or missing key objects. Caption: <prompt> Your response MUST be formatted as: {{ \"description\": \"ONE sentence describing the image accurately, including the spatial relationship observed\", \"caption\": repeat of the image caption provided, \"reasoning\": \"ONE sentence explaining if the spatial positioning in the image EXACTLY matches or contradicts the caption\", \"score\": ... }} Output only the python dictionary with nothing else. <Image> Generated Image </Image> Assistant: Figure 9: Prompt for RP evaluation. Human: Locate the <object>, report the bbox coordinates in JSON format. Assistant: Figure 10: Prompt for RRI grounding. 4 Human: You are an assistant tasked with evaluating two detailed image captions based on given input prompt. Your goal is to determine which caption provides better and more accurate description of the image, considering the object descriptions and their corresponding positions. Task: Evaluate the two detailed image captions provided below, based on the given input prompt. Select the caption that is better and more accurate description of an image, considering the object descriptions and their corresponding bounding boxes. The detailed captions includes the bounding boxes of the objects in the image, which are represented as (x1, x2), (y1, y2). (Assume standard image coordinate system where (0,0) is the top-left corner). Input Prompt: <prompt> Detailed Caption A: <Reasoning Chain A> Detailed Caption B: <Reasoning Chain B> When deciding which caption is better, please consider the following: Relevance to the Input Prompt: How well does each caption address and align with the original input prompt? Accuracy of Object Descriptions: Are the objects described correctly and in sufficient detail? Accuracy of Bounding Boxes: Do the provided bounding boxes (x1, x2), (y1, y2) accurately represent the location and extent of the described objects? Completeness: Does the caption identify and describe the key objects relevant to the input prompt? Does it miss any important elements or include irrelevant ones? Clarity and Coherence: Is the caption easy to understand? Are the object descriptions and their spatial relationships (implied by bounding boxes) presented logically? Naturalness and Fluency: Does the caption read like natural and well-written description? Specificity vs. Generality: Does the caption provide an appropriate level of detail based on the input prompt, or is it too vague or overly specific? Output Format: Please provide your response in the following format: {{ Reasoning: \"<Your reasoning here>\", Selected Caption: \"<A or B>\", }} Assistant: Figure 11: Prompt for GPT-4o reasoning chain comparison."
        }
    ],
    "affiliations": [
        "Beihang University",
        "CUHK MMLab",
        "HKU MMLab",
        "Sensetime"
    ]
}