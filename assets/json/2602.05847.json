{
    "paper_title": "OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention",
    "authors": [
        "Zhangquan Chen",
        "Jiale Tao",
        "Ruihuang Li",
        "Yihao Hu",
        "Ruitao Chen",
        "Zhantao Yang",
        "Xinlei Yu",
        "Haodong Jing",
        "Manyuan Zhang",
        "Shuai Shao",
        "Biao Wang",
        "Qinglin Lu",
        "Ruqi Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to \"think with omnimodal cues\" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities."
        },
        {
            "title": "Start",
            "content": "OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention Zhangquan Chen 1 Jiale Tao 2 Ruihuang Li 2 Yihao Hu 3 Ruitao Chen 2 Zhantao Yang 2 Xinlei Yu 4 Haodong Jing 5 Manyuan Zhang 6 Shuai Shao 2 Biao Wang 2 Qinglin Lu 2 Ruqi Huang 1 6 2 0 2 5 ] . [ 1 7 4 8 5 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While humans perceive the world through diverse modalities that operate synergistically to support holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, novel reinforced framework that improves mixedOmniVideo-R1 empowmodality reasoning. ers models to think with omnimodal cues by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities. 1. Introduction Human cognition is inherently multimodal; we perceive the physical world by processing visual and auditory signals in parallel, integrating them to construct coherent understanding of complex environments (Zhou et al., 2025c; Benchekroun et al., 2023; Zhao et al., 2025c; Chen et al., 2024b; Lu et al., 2025; Yu et al., 2025a). As Large Language Models (LLMs) evolve into Multimodal LLMs (MLLMs), the ability to interpret such multisensory inputs has become cornerstone of artificial general intelligence (Yu et al., 2025b;c; Cai et al., 2025; Li et al., 2024a; 2025c). However, contrary to the expectation that more modalities yield better understanding, current omnimodal models often exhibit paradoxical behavior. The work was conducted during the internship of Zhangquan Chen (czq23@mails.tsinghua.edu.cn) at Tencent HY. 1THU 2Tencent HY 3HNU 4NUS 5XJTU 6CUHK. Correspondence to: Ruqi Huang <ruqihuang@sz.tsinghua.edu.cn>, Jiale Tao <jialetao.std@gmail.com>. Preprint. February 6, 2026. Figure 1. Pre-trained MLLMs (e.g., Qwen3-Omni) often exhibit suboptimal performance in audio-visual reasoning tasks due to inherent modality bias. To address this limitation, we reinforce the audio-visual reasoning ability by leveraging query intention and modality attention. This phenomenon is also evident in the state-of-the-art models. As shown in Fig. 1, pre-training inherently involves trade-offs across heterogeneous tasks, which can induce natural modality bias. Consequently, within the Qwen330B-A3B family, the Omni variant (Xu et al., 2025c) (audiovisual) substantially underperforms the VL variant (Bai et al., 2025a) (visual-only), dropping from 72.1 to 68.5 on MMStar (Chen et al., 2024a) and from 80.1 to 75.9 on MathVista_mini (Lu et al., 2023). These results expose key limitation of current paradigms: instead of enabling synergistic fusion, incorporating the audio modality can undermine the models established visual reasoning capability. natural response is to increase mixed audio-visual supervision during pre-training; however, scaling high-quality mixed-modality data and aligning it with downstream reasoning needs is non-trivial. On the other hand, existing post-training pipelines commonly rely on supervised finetuning (SFT) or vanilla reinforcement learning (RL) (e.g., GRPO) (Zhao et al., 2025b; Xing et al., 2025; Yang et al., 2025b; Zhang et al., 2023; Sun et al., 2024). Yet these post-training methods do not explicitly train audio-visual mixed-modality reasoning behaviors, such as locating and composing evidence across modalities. That is, they provide OmniVideo-R1 little supervision over intermediate evidence-tracking. As result, the model may ignore decisive audio or visual cues and still produce the correct answer by exploiting dataset biases or unimodal shortcuts. To address this challenge, we present OmniVideo-R1, the first post-training framework designed to improve mixedmodality reasoning. We posit that solving such problem requires more than just balancing datasets; it requires instilling robust reasoning behaviors that allow the model to actively select and fuse information. Specifically, OmniVideo-R1 optimizes two fundamental capabilities: (1) query-intensive grounding and (2) modality-attentive fusion built upon query-intensive reasoning. Inspired by the think with images paradigm (OpenAI, 2025), we first introduce query-intensive grounding, which enables the model to explicitly localize and reason over audio-visual segments relevant to the users query before generating response. Since the grounding annotations conditioned on query intent are costly to obtain, we design self-supervised training scheme that leverages multiple timecaption pairs. This design allows the model to generate grounding hypotheses and validate them against the corresponding textual descriptions. For learning robust query intention behavior, we then propose modality-attentive fusion, which maximize the utilization of audio-visual cues. To achieve this, we design contrastive learning-based strategy that explicitly encourages the model to derive higher confidence from mixed audio-visual inputs compared to single-modality counterparts. This forces the model to discover synergistic relationships between visual and audio events, ensuring that the fused representation is strictly superior to its constituent parts. By combining these strategies into unified RL framework, OmniVideo-R1 turns mixed-modality understanding into query-driven reasoning process with audio-visual cues. Our primary contributions are summarized as follows: We propose OmniVideo-R1, the first RL-based framework designed to improve mixed-modality reasoning. We construct high-quality corpus of 80K audiovisual training samples through dedicated datacleaning pipeline, specifically curated to support complex reasoning tasks. We introduce two-stage RL paradigm that incorporates self-supervised grounding and contrastive fusion, enabling the model to learn query intention and modality attention without relying on process-level annotations. Extensive experiments demonstrate that OmniVideo-R1 consistently outperforms strong open-source baselines on audio-visual benchmarks while effectively maintaining robust visual-only performance. 2. Related Work 2.1. Omnimodal Large Language Models The integration of audio and visual modalities is more closely to real-world recordings (Liu et al., 2023; Deng et al., 2024; Wang et al., 2025b; Zhao & Chen, 2023; Wang et al., 2025a; Hou et al., 2026), and requires models to form cohesive understanding of the surroundings, like humans (Zhao et al., 2025b). Early efforts always focused on silent video understanding (Bai et al., 2025b; Zhang et al., 2024; Feng et al., 2025a) or treated audio simple add-on to text (Li et al., 2025b), which fragments natural omnimodal representations and thereby limits performance. Consequently, subsequent works have aimed for deeper cross-modal fusion. MiniCPM-o-2.6 (Yao et al., 2024) and Baichuan-Omni-1.5 (Li et al., 2024c) extend visionlanguage foundations with audio processing capabilities, enabling operation across broader range of modalities. Ola (Liu et al., 2025) adopts progressive modalityalignment strategy that incrementally strengthens the language models ability to exploit additional modalities. The Video-LLaMA series (Zhang et al., 2023) concatenates audio and visual tokens to support joint audiovideo understanding, whereas the Video-SALMONN (Sun et al., 2024) series employs multi-resolution causal Q-Former (Li et al., 2023) to process audio and video simultaneously. Moreover, InternVideo series (Wang et al., 2022) aligns video with audio events, speech, and text via cross-modal contrastive learning, thereby facilitating integrated audiovideo representation learning. Qwen2.5-Omni (Xu et al., 2025b) introduces thinkertalker architecture, an end-to-end multimodal framework capable of perceiving diverse input types. More recently, Qwen3-Omni (Xu et al., 2025c) leverages an Audio Transformer (AuT) for audio encoding and incorporates TM-RoPE, further enhancing the audiovisual understanding capabilities. Despite these advances, current multimodal models still exhibit substantial limitations on complex tasks, particularly in scenarios that demand tightly integrated audiovisual understanding or more sophisticated logical reasoning. 2.2. Reinforced Multimodal Reasoning Reinforcement learning has become widely adopted paradigm for enhancing the performance of large language models (Shen et al., 2021; Lan et al., 2025). Recent work combines RL with vision and language to elicit stronger reasoning capabilities (Chen et al., 2025c; Ni et al., 2025). 2 OmniVideo-R1 Figure 2. The schematic illustration of our OmniVideo-R1. Based on the dataset collected from data preparation, our training consists of two stages: (1) QI stage establishes query-intensive grounding behavior by aligning multiple timecaption pairs without process-level annotations. (2) MA stage further performs modality-attentive fusion by optimizing contrastive modality reward. Some approaches, inspired by DeepSeek-R1 (Guo et al., 2025), introduce purely textual chain-of-thought (Thawakar et al., 2025; Chen et al., 2025a; Dong et al., 2024). Building on this, methods such as VisRL (Chen et al., 2025b), SIFThinker (Chen et al., 2025d), GRIT (Fan et al., 2025), and CogCoM (Qi et al., 2024) enable thinking with images by integrating visual evidence into the reasoning trajectory. Beyond these efforts, several studies have extended the notion of reasoning to omnimodal models. R1-Omni (Zhao et al., 2025a) is primarily designed for audiovisual referring segmentation, whereas EchoInk-R1 (Xing et al., 2025) investigates the direct application of vanilla GRPO (Guo et al., 2025) in the omnimodal setting. In addition, OmniR1 (Zhong et al., 2025) adopts dual-system architecture to tackle long-horizon videoaudio reasoning, and HumanOmnv2 (Yang et al., 2025b) enhances model capabilities through training on datasets curated for complex human intention understanding. However, compared with silent video reasoning (Feng et al., 2025b; Wang et al., 2025c), current explorations of omnimodal reasoning remain relatively limited. Existing approaches concentrate on directly transferring vanilla RL, designing intricate multi-branch architectures, or constructing specialized training datasets. Yet omnimodal models inherently require deeper multimodal fusion in order to unlock stronger reasoning capabilities, thereby achieve genuine aha moments. Consequently, there is still conspicuous absence of training methodologies that are tailored to the distinctive characteristics of such omnimodal models. 3. Methodology Preliminary. Reinforcement learning (Christiano et al., 2017) has emerged as particularly effective approach for substantially enhancing the robustness and factual accuracy of large language models (Ouyang et al., 2022). In practice, off-policy learning settings are typically used during policy model training to improve sample efficiency. However, for Mixture-of-Experts (MoE) models (e.g., Qwen3-Omni-30BA3B (Xu et al., 2025c)), the activation of different experts can induce substantial shifts in the token distribution. Under such conditions, token-level importance sampling often introduces high-variance noise into the training gradients, which accumulates over long sequences and is further exacerbated by clipping mechanisms. To this end, our method performs optimization directly at the sequence level, following the formulation introduced by Group Sequence Policy Optimization (GSPO) algorithm (Zheng et al., 2025). The optimization objective is formulated as: (θ) = xD,{yi}G i=1πθold (x)(Pθ), (1) 3 OmniVideo-R1 where the response yi is sampled from old policy model πθold based on the input x, and Pθ is: Pθ ="
        },
        {
            "title": "1\nG",
            "content": "G i=1 min (cid:16) si(θ) bAi, clip (si(θ), 1 ε, 1 + ε) bAi (cid:17) . (2) Here, we also adopt the group-based advantage estimation: bAi = R(x, yi) mean (cid:0){R(x, yi)}G std (cid:0){R(x, yi)}G (cid:1) i=1 i=1 (cid:1) , (3) R() denotes the reward function that will be introduced below, and we define the importance ratio si(θ) based on sequence likelihood (Zheng et al., 2023): si(θ) = exp 1 yi yi t= log πθ(yi,tx, yi,<t) πθold(yi,tx, yi,<t) . (4) training stages. That is, we first induce the model to develop query-intensive reasoning behavior, and then, further train it to integrate multiple modalities in logically consistent manner. In the first stage (QI), the model is trained with self-supervised objective defined over multiple pairs of grounding and caption generated within the reasoning trajectory. In the second stage (MA), we promote deeply fused understanding by first decoupling the modality-specific inputs and then performing contrastive learning across them. Notably, throughout the entire training pipeline, OmniVideo-R1 doesnt rely on any explicit process-level annotations for query-intensive grounding or modality fusion. Fig. 3 illustrates our reasoning process in comparison with Qwen3-Omni-30B-A3B. Our OmniVideo-R1 endows the model with the ability to \"think with omnimodal cues\", i.e., to perform query-intensive grounding that identifies key cues, thereby enabling more accurate and reliable reasoning to the final answer. 3.1. Data Preparation We first collect raw data from LLaVA-Video (Zhang et al., 2024) and Video-Vista (Li et al., 2024b), and perform structural validation to remove problematic samples with metadata issues (e.g., silent videos). To further filter out viowquality samples that are misaligned with our multimodal setting, we apply three-stage refinement pipeline as shown in Fig. 7, which consists of (i) quality assessment, (ii) heuristic filtering, and (iii) categorical balancing. Figure 4. Pipeline for our data preparation consisting of 3 stages. Figure 3. Visualization of the responses and underlying reasoning process generated by OmniVideo-R1 and Qwen3-Omni-30B-A3BInstruct, -Thinking to an audio-visual understanding question. Method overview. As shown in Fig. 2, OmniVideo-R1 adopts GSPO to optimize the entire reasoning process, enabling the model to extract intention-relevant cues and to effectively integrate audiovisual information throughout reasoning. This model behavior emerges through two For data quality assessment, we employ Gemini-2.5Pro (Google, 2024) to score each sample along four key dimensions: video dependency sv, audio dependency sa, question logic sq, and response accuracy sr. Each dimension is normalized to maximum of 1, and the weighted composite score sc is computed as weighted sum over the four dimensions. Subsequently, Qwen-3-32B (Yang et al., 2025a) is used to categorize the samples according to the 15-category taxonomy described in the Appendix. 4 OmniVideo-R1 After scoring and categorizing data, we apply the following filtering rules: (i) sr = 1, (ii) sq 0.8, (iii) sc 0.7. Any sample that fails to satisfy any of these rules is discarded. Finally, we mitigate long-tail bias by pruning sparse categories (i.e., those with fewer than 10 samples). Observing significant gap between the top two classes, we further require that the number of samples in the largest class does not exceed three times that of the second-largest class. Specifically, we first retain all samples with both sa = 1 and si = 1, then sort the remaining data in descending order of sc and remove samples that exceed the specified count, resulting in smoother data distribution. After applying all of the above steps, we obtain 88173 examples for the first training stage training. Considering the high-quality requirements for audio-visual fused data in the second training stage, we then derive subset of 12887 examples by keeping only instances with high audio-visual dependency, i.e., sv 0.7 and sa 0.7. 3.2. Query-intensive Grounding (QI) Query-intensive grounding operations aim to help the model identify key frames containing critical audio-visual cues within video sequence (Wang et al., 2025c). However, human annotations of prompt-related key frames are often complex and expensive. To address this issue, we propose novel grounding approach that establishes correspondence between grounding and captioning without relying on any dense annotations, thereby enabling self-supervised learning of the models procedural behavior. Specifically, given one question and the corresponding audiovisual content (Q, A, ), we encourage the model to produce outputs in the structured format <time>...</time><caption>...</caption> ... <thinking>...</thinking><answer>...</answer>. reward of rformat = 1.0 is assigned to responses that strictly comply with this output For each rollout, we denote the multiple generated timecaption pairs by {T1, C1, T2, C2, . . . , TN , CN }. We then perform self-supervised learning by evaluating the consistency reward between each Ti and Ci, i.e., template. rcons = 1 N i=1 E(L) cons (cid:0)G(A, ; Ti), Ci (cid:1), (5) where G(A, ; Ti) extracts the audio-visual segment from (A, ) corresponding to the time span Ti, and E(L) cons() denotes soft evaluation function implemented via judger model (i.e., Qwen3-VL-235B-A22B-Instruct) with predefined rules. The detailed rules and the associated prompts are provided in Appendix. On the one hand, we perform self-supervised learning by enforcing the correctness of each timecaption pair. On the other hand, we also require the grounding to be precise, i.e., it should (i) minimally and effectively cover all groundtruth intention-related cues Tgt, and (ii) avoid redundant predictions. Formally, for each i, , we except: ! # Ti Tgt = Tgt [Ti Tj = , = j] . (6) \" [ i=1 However, in this work, we tackle the challenging setting where no ground-truth Tgt is available, and instead propose soft approximation to solve Eq. 6. Specifically, we first crop all predicted segments and then concatenate them into single sequence, which is subsequently evaluated along two dimensions: content completeness and precision. In other words, we assess whether the audio-visual information contained within the grounded segments is adequate and accurate for supporting the reasoning process from the question to the final answer R. Accordingly, we define the completeness reward as: rcomp = E(M ) comp (cid:16) i=1 G(A, ; Ti), Q, (cid:17) , (7) where LN i=1 G(A, ; Ti) denotes the temporally ordered concatenation of all grounded audio-visual segments, yielding single composite video clip. Here, E(M ) comp() is the intent evaluation function instantiated with = 3 predefined rules. More details are listed in the Appendix. Meanwhile, we also leverage the outcome signal, following (Guo et al., 2025). Specifically, we softly evaluate the quality of the final answer and assign continuous score rans; the detailed evaluation protocol is provided in Appendix. Finally, the reward in our QI training stage is defined as RQI = rformat + rans + (cid:0)rcons + rcomp (cid:1). (8) 1 2 We establish unified training framework from three complementary perspectives: (i) global format regularization rformat, (ii) outcome-based constraints rans, and (iii) (cid:1). (cid:0)rcons + rcomp process-level self-supervision rintent = 1 2 Under this training design, the model is expected to infer the underlying intention, extract task-relevant cues, and perform reasoning over these audiovisual content. 3.3. Modality-attentive Fusion (MA) As QI stage is primarily evaluated in vision-centric manner, relying solely on query-intensive grounding still prevents the model from capturing the subtle but decisive sound cues (as shown in Fig. 5). This inability to leverage audio cues further leads to substantial redundant outputs (as shown 5 OmniVideo-R Method WorldSense IntentBench 67.8 67.2 71.5 56.2 64.6 66.4 VideoHolmes Gemini-2.0-Flash Gemini-2.5-Pro (Google, 2024) Gemini-3-Pro Daily-Omni Closed-source Models 67.8 81.4 81.1 Open-source Models 35.2 47.5 53.1 49.9 58.5 71.8 79.4 63.6 75.8 82.8 Table 1. Performance of different methods on range of audio-visual benchmarks, including Daily-Omni (Zhou et al., 2025b), WorldSense (Benchekroun et al., 2023), IntentBench (Yang et al., 2025b), and VideoHolmes (Cheng et al., 2025). Our training was conducted on QI and on both QI + MA. The best is highlighted, and the second-best is underlined. VideoLLaMA2-7B (Cheng et al., 2024) Qwen2.5-Omni-7B (Xu et al., 2025a) MiniCPM-o-7B (Yao et al., 2024) VITA-1.5-7B (Fu et al., 2025b) Ola-7B (Liu et al., 2025) HumanOmniV2-7B (Yang et al., 2025b) video-SALMONN 2+-7B (Tang et al., 2025) video-SALMONN 2+-72B (Tang et al., 2025) Qwen3-Omni-30B-A3B-Instruct (Xu et al., 2025c) Qwen3-Omni-30B-A3B-Thinking (Xu et al., 2025c) OmniVideo-R1 64.2 54.5 54.2 57.4 69.3 65.7 68.5 74.2 25.4 45.4 36.9 47.1 50.9 56.5 54.0 48.0 65.8 35.2 16.4 46.9 57.8 49.0 57.3 62. 30.6 64.9 67.0 in Fig. 6). To address this issue, we propose modalityattentive fusion scheme, whose central idea is to encourage the model to fully exploit and synergistically integrate both audio and visual information to improve accuracy. Concretely, for each input x, we compare the models performance under three rollout settings: (i) combined audio visual input; (ii) silent-video-only input; and (iii) audio-only input. For desirable multimodal understanding model, the performance with full multimodal input should not be inferior to that with any single-modality input, especially on datasets where both acoustic and visual cues are required to correctly answer the question. Denote the soft scores associated with these three rollouts by r1 ans, respectively. We then define the attention reward as ans, and r3 ans, r2 rattn = ( α, 0, if r1 ans r2 otherwise ans and r1 ans r3 ans 4. Experiments We evaluate OmniVideo-R1 with several state-of-the-art (SOTA) methods on an array of categories as follows. More details about benchmarks are listed in the Appendix. Training. OmniVideo-R1 is trained based on Qwen3Omni-30B-A3B following the pipeline described in Sec. 3. As detailed in Sec. A, we use 88173 samples for QI stage (Sec. 3.2) and 12887 samples for MA stage (Sec. 3.3). Hyper-parameters. For OmniVideo-R1, we conduct training under 128H20 setup with global batch size of 256. We set the balancing coefficient of all rewards to 1 and use learning rate of 1 106. The rollout number is 8, and the maximum sequence length is 32768. Additional details are provided in the Appendix. (9) Evaluation metric. For multiple-choice questions, we report Accuracy, which is calculated based on exact matches between the models predictions and the ground truth. where α is hyperparameter controlling the magnitude of the attention reward (set to α = 0.3 in our experiments). This contrastive formulation explicitly encourages the model to achieve superior performance when audio and visual information are effectively fused, rather than relying predominantly on single modality. Building upon the contrastive learning strategy, our MA training stage focuses on enhancing model capabilities in more targeted subset of data which specifically requires integrated audiovisual understanding. This stage aims to advance the reasoning paradigm from query-intensive grounding to deeper multimodal understanding. Formally, the reward for this stage is defined as: RMA = rformat + rans + rattn. (10) 6 4.1. Omnimodal Understanding We first assess OmniVideo-R1 on suite of audio-video understanding benchmarks. After the OmniVideo-R1 training phase, the model shows remarkable improvements across multiple benchmarks. Notably, OmniVideo-R1 outperforms the open-source SOTA model Video-SALMONN 2+-72B (which has larger parameter scale) by at least 4.3% (82.8 vs. 79.4). Additionally, on specific benchmarks, OmniVideo-R1 even exceeds the latest closed-source SOTA model Gemini3-Pro, achieving 2.1% advantage (82.8 vs. 81.1) on Daily-Omni and 3.8% improvement (74.2 vs. 71.5) on IntentBench. Interestingly, certain reasoning-oriented variants have been Method Gemini-2.0-Flash Gemini-2.5-Pro (Google, 2024) Gemini-3-Pro OmniVideo-R1 Audio Type Video Duration Music Sound Speech (0,1] min (1,5] min (5,10] min (10,30] min Closed-source Models 29.7 38.5 56.2 40.3 57.7 54.1 43.2 61.7 55.7 49.4 57.8 61.0 Open-source Models 43.2 64.4 56. 41.1 55.0 52.9 34.9 55.9 52.5 Avg. 41.5 58.9 55.5 26.4 VideoLLaMA2-7B (Cheng et al., 2024) 29.2 23.1 Qwen2.5-Omni-7B (Xu et al., 2025b) 29.3 27.5 MiniCPM-o-7B (Yao et al., 2024) 29.7 20.9 HumanOmniV2-7B (Yang et al., 2025b) 30.5 24.2 Baichuan-Omni-1.5-7B (Li et al., 2024c) 30.7 Qwen3-Omni-30B-A3B-Instruct (Xu et al., 2025c) 30.8 37.0 Qwen3-Omni-30B-A3B-Thinking (Xu et al., 2025c) 26.4 37.2 OmniVideo-R1 40.7 44.8 (+7.8pp) Table 2. Accuracy comparison of OmniVideo-R1 and other methods on OmniVideoBench (Li et al., 2025a). The best is highlighted and the second-best is underlined. The performance gains of our method over the base model are indicated in red parentheses. 28.3 26.7 26.2 29.3 32.4 29.7 35.2 41. 29.3 30.7 30.2 31.6 31.4 38.0 38.5 46.6 29.6 25.3 34.5 29.6 28.4 37.7 35.5 43.9 32.0 41.6 31.4 36.6 28.9 46.8 46.8 53.8 28.2 27.4 28.5 29.4 31.8 37.4 35.6 43.5 30.7 25.3 28.6 31.1 31.3 35.8 37.2 38.1 observed to underperform compared to their base counterparts on specific benchmarks (e.g., Qwen3-Omni-30B-A3BThinking vs. Qwen3-Omni-30B-A3B shows 48.0 vs. 54.0 on WorldSense). In contrast, OmniVideo-R1 consistently demonstrates superior performance over the base model across all evaluated benchmarks, underscoring both the effectiveness and robustness of our approach. Furthermore, we evaluate OmniVideo-R1 on more challenging benchmark focused on synergistic audio-visual understanding, with strong emphasis on modality complementarity and logical consistency. As shown in Tab. 2, OmniVideo-R1 surpasses Qwen3-Omni-30B-A3B by 21.1% (44.8 vs. Previous methods per37.0). formed close to random guessing on this benchmark, but OmniVideo-R1 breaks through this bottleneck and achieves significant gains, consistently surpassing the base model across all evaluation dimensions. These results highlight the substantial potential of audio-visual joint reasoning through accurately grounded key cues. 4.2. Visual-only Understanding On the other hand, to assess whether the model suffers performance degradation in single modality after mixedmodality post-training, we evaluate OmniVideo-R1 on suite of silent-video benchmarks. As shown in Tab. 3, OmniVideo-R1 exhibits no evident degradation and even demonstrates improvements compared to the base model; specifically, it achieves gains of 4.4% (73.6 vs. 70.5), -1.4% (74.1 vs. 75.2), and 3.4% (51.9 vs. 50.2) on Video-MME, MLVU, and LVBench, respectively. This robustness stems from the models ability to effectively ground behaviors during inference, allowing it to proficiently capture key cues regardless of whether the input is purely visual or audio-visual. These results confirm our core objective of fostering modality integration to enhance reasoning, rather than resulting in trade-offs between modalities. 4.3. Different Training Strategies Following the dataset curated for OmniVideo-R1, we first attempt to use these 88173 examples to directly learn the final response in the QA SFT setting, as reported the model is supervised only on in Tab. 4. That is, In contrast, CoT SFT augments the final answers. with chain-of-thought annotations generated by Gemini2.5Pro (Google, 2024) and then fine-tunes the model on these CoT-augmented examples. Vanilla RL instead applies standard GRPO (Guo et al., 2025) on under <think>...</think><answer>...</answer> protocol, using mixture of format and soft-response scores as the reward. As shown in Tab. 4, all these approaches yield noticeable improvements over the base model on audiovisual understanding benchmarks, confirming the effectiveness of after our data preparation pipeline. However, the performance gains of these baselines are consistently smaller than those achieved by OmniVideo-R1. On Daily-Omni, our method surpasses the second-best Vanilla RL by 12.0% (82.8 vs. 73.9), and on WorldSense it outperforms the second-best CoT SFT by 11.1% (65.8 vs. 59.2). These ablation results further validate the effectiveness and superiority of our training paradigm. 4.4. Case study We further present several qualitative cases for QI-only training, and our QI+MA (OmniVideo-R1) training. As shown in Fig. 5, QI training yields strong reasoning behavior; however, in some cases the model overlooks critical audio cues, resulting in inaccurate inferences. In contrast, our QI+MA, first establishing the desired reasoning behavior and then booming deeper audio-visual reasoning, enables the model to better exploit both audio and visual evidence. 7 OmniVideo-R1 Method GPT-4o Gemini-2.0-Flash Gemini-2.5-Pro (Google, 2024) VideoLLaMA3-7B (Zhang et al., 2025) InternVideo2.5-8B (Wang et al., 2025e) Qwen2.5-VL-7B (Bai et al., 2025b) Qwen2.5-VL-72B (Bai et al., 2025b) video-SALMONN 2+-7B (Tang et al., 2025) Qwen3-Omni-30B-A3B-Instruct (Xu et al., 2025c) Qwen3-Omni-30B-A3B-Thinking (Xu et al., 2025c) OmniVideo-R Video-MME 71.9 72.4 86.9 66.2 65.1 65.1 73.3 73.4 70.5 69.7 73.6 MLVU(Dev) 64.6 71.0 81.2 73.0 72.8 70.2 74.6 73.6 75.2 72.9 74.1 LVBench 30.8 57.9 69.2 45.3 46.4 45.3 47.3 49.7 50.2 49.0 51.9 Table 3. Performance of different methods on various visual-only benchmarks, including Video-MME (Fu et al., 2025a), MLVU (Zhou et al., 2025a) and LVBench (Wang et al., 2025d). The best is highlighted and the second-best is underlined. Method QA SFT CoT SFT Vanilla RL Full OmniVideoBench Daily-Omni WorldSense 39.1 42.2 41.5 44.8 69.9 73.1 73.9 82.8 57.4 59.2 58.0 65.8 Table 4. Performance on different training strategies in terms of Qwen3-Omni-30B-A3B-Instruct. The best is highlighted. Method w/o rintent w/o rattn w/o QI (10K) w/o QI (80K) w/o MA w/o audio input w. timestamps Base model Full OmniVideoBench Daily-Omni WorldSense 38.4 43.7 41.6 41.0 43.6 37.6 43.4 37.0 44.8 75.9 82.1 76.1 76.9 82.0 68.7 81.7 63.6 82.8 55.1 65.5 58.6 59.2 65.3 50.3 66.1 54.0 65.8 Table 5. Performance on different ablated settings in terms of Qwen3-Omni-30B-A3B-Instruct. The best is highlighted. Moreover, as illustrated in Fig. 6, QI training tends to introduce redundant grounding, as its primary objective is to shape reasoning behavior. Our QI+MA further use MA to maximize the utilization of audio-visual cues. 4.5. Ablation Study Component Removal. We first perform ablations on various designs (w/o rattn, rintent, or QI stage) in Tab. 5. The results suggest that the observed performance gains are mainly attributable to two factors: (1) rintent, which encourages accurate grounding of the primary cues, and (2) modalityattentive training, which further strengthens the models ability to perform comprehensive audio-visual reasoning. It can be observed that performing MA stage training alone can bring substantial improvements (as shown in w/o QI setting in the table). For instance, on OmniVideoBench, this strategy yields 12.4% gain over the base model (41.6 vs. 37.0). Furthermore, QI stage training (w/o MA setting in the table) also significantly improved the models capability, yielding 17.8% gain over the base model (43.6 vs. 37.0). Removing rintent or rattn both results in certain performance drop. 8 Input Configuration. We further ablate the impact of input configuration in Tab. 5. Specifically, we use OmniVideo-R1 trained with both audio and video, but perform inference under w/o audio input setting. We observe performance drop on WorldSense (50.3 vs. 54.0), but slight improvement on Daily-Omni (68.7 vs. 63.6). This mixed behavior can be attributed to two factors: (1) when the evaluation benchmark inherently relies on audio, the mismatch between training (w. audio) and inference (w/o audio) naturally leads to degraded performance; (2) owing to the enhanced reasoning capability and robust grounding of key visual cues, the model can actually perform better on tasks where audio is non-essential or visual information alone is sufficient. Moreover, many recent methods introduce explicit temporal cues by overlaying timestamps (Ge et al., 2025). While this can strengthen temporal perception, it simultaneously occludes part of the original visual content. In contrast, our rintent reward inherently promotes temporal correction during training (e.g., inaccurate temporal grounding directly degrades caption quality), endowing OmniVideo-R1 with an implicit sense of time. As result, our method is insensitive to such numeric overlays (w. timestamps), exhibiting only marginal performance differences. 5. Conclusion In this paper, we propose OmniVideo-R1, query-intensive deep fusion framework for audio-visual reasoning. Our training pipeline consists of two stages. First, without relying on any process-level annotations, we encourage the model to think with omnimodal cues by learning in selfsupervised manner grounded in intermediate time-caption pairs. Second, we explicitly enhance cross-modal fusion by contrasting the models learning under full audio-visual input versus single-modality input, thereby improving its ability to build coherent multimodal representations. Extensive experiments show that OmniVideo-R1 consistently outperforms prior methods on multiple benchmarks, laying solid foundation for future work in audio-visual reasoning. OmniVideo-R1 Figure 5. Visualization of the results obtained from the training of QI, and QI+MA. Red highlights the incorrect text, while green highlights the correct text. Yellow highlights the model overemphasizes one modality while neglecting cues from the other modality. 9 OmniVideo-R1 Figure 6. Visualization of the results obtained from the training of QI, and QI+MA. Red highlights the incorrect text, while green highlights the correct text. 10 OmniVideo-R"
        },
        {
            "title": "References",
            "content": "Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025a. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Benchekroun, Y., Dervishi, M., Ibrahim, M., Gaya, J.-B., Martinet, X., Mialon, G., Scialom, T., Dupoux, E., Hupkes, D., and Vincent, P. Worldsense: synthetic benchmark for grounded reasoning in large language models. 2023. Cai, H., Shen, B., Jin, L., Hu, L., and Fan, X. Does tone change the answer? evaluating prompt politeness effects on modern llms: Gpt, gemini, llama. arXiv preprint arXiv:2512.12812, 2025. Chen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao, Y., Lin, D., et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37: 2705627087, 2024a. for visual reasoning. arXiv preprint arXiv:2508.06259, 2025d. Cheng, J., Ge, Y., Wang, T., Ge, Y., Liao, J., and Shan, Y. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. Cheng, Z., Leng, S., Zhang, H., Xin, Y., Li, X., Chen, G., Zhu, Y., Zhang, W., Luo, Z., Zhao, D., et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Deng, A., Chen, D., Han, G., Yang, H., Liu, Z., and Liu, F. Separation fusion transformer and efficient reuse matching network for aerial tracking. IEEE Geoscience and Remote Sensing Letters, 2024. Dong, Y., Liu, Z., Sun, H.-L., Yang, J., Hu, W., Rao, Y., and Liu, Z. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024. Fan, Y., He, X., Yang, D., Zheng, K., Kuo, C.-C., Zheng, Y., Narayanaraju, S. J., Guan, X., and Wang, X. E. Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879, 2025. Feng, K., Gong, K., Li, B., Guo, Z., Wang, Y., Peng, T., Wu, J., Zhang, X., Wang, B., and Yue, X. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025a. Chen, L., Li, L., Zhao, H., Song, Y., and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/ Deep-Agent/R1-V, 2025a. Accessed: 2025-02-02. Feng, K., Gong, K., Li, B., Guo, Z., Wang, Y., Peng, T., Wu, J., Zhang, X., Wang, B., and Yue, X. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025b. Chen, Z., Liu, C., and Duan, H. three-phases-lora finetuned hybrid llm integrated with strong prior module in the education context. In International Conference on Artificial Neural Networks, pp. 235250. Springer, 2024b. Chen, Z., Luo, X., and Li, D. Visrl: Intention-driven visual perception via reinforced reasoning. arXiv preprint arXiv:2503.07523, 2025b. Chen, Z., Zhang, M., Yu, X., Luo, X., Sun, M., Pan, Z., Feng, Y., Pei, P., Cai, X., and Huang, R. Think with 3d: Geometric imagination grounded spatial reasoning from limited views. arXiv preprint arXiv:2510.18632, 2025c. Chen, Z., Zhao, R., Luo, C., Sun, M., Yu, X., Kang, Y., and Huang, R. Sifthinker: Spatially-aware image focus Fu, C., Dai, Y., Luo, Y., Li, L., Ren, S., Zhang, R., Wang, Z., Zhou, C., Shen, Y., Zhang, M., et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2410824118, 2025a. Fu, C., Lin, H., Wang, X., Zhang, Y.-F., Shen, Y., Liu, X., Cao, H., Long, Z., Gao, H., Li, K., et al. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction. arXiv preprint arXiv:2501.01957, 2025b. Ge, Y., Ge, Y., Li, C., Wang, T., Pu, J., Li, Y., Qiu, L., Ma, J., Duan, L., Zuo, X., et al. Arc-hunyuan-video7b: Structured video comprehension of real-world shorts. arXiv preprint arXiv:2507.20939, 2025. 11 OmniVideo-R Google. Gemini 2.5 pro. https://deepmind. google/technologies/gemini/, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hou, Y., Wang, Y., Xia, X., Tian, Y., Li, Z., and Quek, T. Q. S. Toward secure sar image generation via federated angle-aware generative diffusion framework. IEEE INTERNET OF THINGS JOURNAL, 13(2):27132730, JAN 15 2026. ISSN 2327-4662. doi: 10.1109/JIOT.2025. 3630329. Lan, G., Zhang, S., Wang, T., Zhang, Y., Zhang, D., Wei, X., Pan, X., Zhang, H., Han, D.-J., and Brinton, C. G. Mappo: Maximum posteriori preference optimization with prior knowledge. arXiv preprint arXiv:2507.21183, 2025. Li, C., Chen, Y., Ji, Y., Xu, J., Cui, Z., Li, S., Zhang, Y., Tang, J., Song, Z., Zhang, D., He, Y., Liu, H., Wang, Y., Wang, Q., Wu, Z., Luo, J., Pan, Z., Xie, W., Zhang, C., Wang, Z., Tian, J., Wang, Y., Cao, Z., Dai, M., Wang, K., Wen, R., Ma, Y., Pan, Y., Chang, S., Taheri, T., Xia, H., Plachouras, C., Benetos, E., Li, Y., Zhang, G., Yang, J., Peng, T., Wang, Z., Liu, M., Peng, J., Zhang, Z., and Liu, J. Omnivideobench: Towards audio-visual understanding evaluation for omni mllms, 2025a. URL https:// arxiv.org/abs/2510.10689. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., and Qiao, Y. Videochat: Chat-centric video understanding. Science China Information Sciences, 68 (10):200102, 2025b. Li, X., Ma, Y., Huang, Y., Wang, X., Lin, Y., and Zhang, C. Synergized data efficiency and compression (sec) optimization for large language models. In 2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS), pp. 586591, 2024a. doi: 10.1109/EIECS63941.2024.10800533. Li, X., Lu, Y., Cao, J., Ma, Y., Li, Z., and Zhou, Y. Catch: modular cross-domain adaptive template with hook. In International Symposium on Visual Computing, pp. 4152. Springer, 2025c. Li, Y., Sun, H., Lin, M., Li, T., Dong, G., Zhang, T., Ding, B., Song, W., Cheng, Z., Huo, Y., et al. Baichuanomni technical report. arXiv preprint arXiv:2410.08565, 2024c. Liu, F., Liu, J., Chen, Q., Wang, X., and Liu, C. Siamhas: Siamese tracker with hierarchical attention strategy for aerial tracking. Micromachines, 14(4):893, 2023. Liu, Z., Dong, Y., Wang, J., Liu, Z., Hu, W., Lu, J., and Rao, Y. Ola: Pushing the frontiers of omni-modal language model. arXiv preprint arXiv:2502.04328, 2025. Lu, B., Lu, Z., Qi, Y., Guo, H., Sun, T., and Zhao, Z. Predicting asphalt pavement friction by using texture-based image indicator. Lubricants, 13(8):341, 2025. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Ni, C., Zhao, G., Wang, X., Zhu, Z., Qin, W., Chen, X., Jia, G., Huang, G., and Mei, W. Recondreamer-rl: Enhancing reinforcement learning via diffusion-based scene reconstruction. arXiv preprint arXiv:2508.08170, 2025. OpenAI. Introducing openai o3 and o4-mini, URL https://openai.com/ April 2025. index/introducing-o3-and-o4-mini/. https://openai.com/index/introducing-o3-and-o4-mini/. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Qi, J., Ding, M., Wang, W., Bai, Y., Lv, Q., Hong, W., Xu, B., Hou, L., Li, J., Dong, Y., et al. Cogcom: Train large vision-language models diving into details through chain of manipulations. 2024. Shen, Y., Fang, Z., Xu, Y., Cao, Y., and Zhu, J. rank-based sampling framework for offline reinforcement learning. In 2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI), pp. 197202, 2021. doi: 10.1109/CEI52496.2021.9574597. Li, Y., Chen, X., Hu, B., Wang, L., Shi, H., and Zhang, M. Videovista: versatile benchmark for video understanding and reasoning. arXiv preprint arXiv:2406.11303, 2024b. Sun, G., Yu, W., Tang, C., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., Wang, Y., and Zhang, C. video-salmonn: Speech-enhanced audio-visual large language models. arXiv preprint arXiv:2406.15704, 2024. 12 OmniVideo-R1 Tang, C., Li, Y., Yang, Y., Zhuang, J., Sun, G., Li, W., Ma, Z., and Zhang, C. video-salmonn 2: Captioningenhanced audio-visual large language models. arXiv preprint arXiv:2506.15220, 2025. Thawakar, O., Dissanayake, D., More, K., Thawkar, R., Heakl, A., Ahsan, N., Li, Y., Zumri, M., Lahoud, J., Anwer, R. M., Cholakkal, H., Laptev, I., Shah, M., Khan, F. S., and Khan, S. Llamav-o1: Rethinking step-by-step visual reasoning in llms, 2025. URL https://arxiv. org/abs/2501.06186. Wang, J., Feng, X., Yu, Y., Wang, X., Werghi, N., Han, X., Zhou, H., Shi, K., Zhong, S., Cai, J., et al. Fuzzy actor critic learning-based interpretable control and stabilityinformed guarantee with error mapping for discrete-time nonlinear system. Chaos, Solitons & Fractals, 199: 116878, 2025a. Wang, Q., Liu, F., Zhang, B., Liu, J., Xu, F., and Wang, Y. Siamctca: Cross-temporal correlation aggregation siamese network for uav tracking. Drones, 9(4):294, 2025b. Wang, S., Jin, J., Wang, X., Song, L., Fu, R., Wang, H., Ge, Z., Lu, Y., and Cheng, X. Video-thinker: Sparking\" thinking with videos\" via reinforcement learning. arXiv preprint arXiv:2510.23473, 2025c. Wang, W., He, Z., Hong, W., Cheng, Y., Zhang, X., Qi, J., Ding, M., Gu, X., Huang, S., Xu, B., et al. Lvbench: In An extreme long video understanding benchmark. Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2295822967, 2025d. Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y., Wang, Z., et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. Wang, Y., Li, X., Yan, Z., He, Y., Yu, J., Zeng, X., Wang, C., Ma, C., Huang, H., Gao, J., et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025e. Xing, Z., Hu, X., Fu, C.-W., Wang, W., Dai, J., and Heng, P.- A. Echoink-r1: Exploring audio-visual reasoning in multimodal llms via reinforcement learning. arXiv preprint arXiv:2505.04623, 2025. Xu, J., Guo, Z., Hu, H., Chu, Y., Wang, X., He, J., Wang, Y., Shi, X., He, T., Zhu, X., Lv, Y., Wang, Y., Guo, D., Wang, H., Ma, L., Zhang, P., Zhang, X., Hao, H., Guo, Z., Yang, B., Zhang, B., Ma, Z., Wei, X., Bai, S., Chen, K., Liu, X., Wang, P., Yang, M., Liu, D., Ren, X., Zheng, B., Men, R., Zhou, F., Yu, B., Yang, J., Yu, L., Zhou, J., and Lin, J. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025c. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Yang, Q., Yao, S., Chen, W., Fu, S., Bai, D., Zhao, J., Sun, B., Yin, B., Wei, X., and Zhou, J. Humanomniv2: From understanding to omni-modal reasoning with context. arXiv preprint arXiv:2506.21277, 2025b. Yao, Y., Yu, T., Zhang, A., Wang, C., Cui, J., Zhu, H., Cai, T., Li, H., Zhao, W., He, Z., et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Yu, X., Chen, Z., Zhang, Y., Lu, S., Shen, R., Zhang, J., Hu, X., Fu, Y., and Yan, S. Visual document understanding and question answering: multi-agent collaboration framework with test-time scaling. arXiv e-prints, pp. arXiv2508, 2025a. Yu, X., Xu, C., Zhang, G., Chen, Z., Zhang, Y., He, Y., Jiang, P.-T., Zhang, J., Hu, X., and Yan, S. Vismem: Latent vision memory unlocks potential of vision-language models. arXiv preprint arXiv:2511.11007, 2025b. Yu, X., Xu, C., Zhang, G., He, Y., Chen, Z., Xue, Z., Zhang, J., Liao, Y., Hu, X., Jiang, Y.-G., et al. Visual multi-agent system: Mitigating hallucination snowballing via visual flow. arXiv preprint arXiv:2509.21789, 2025c. Zhang, B., Li, K., Cheng, Z., Hu, Z., Yuan, Y., Chen, G., Leng, S., Jiang, Y., Zhang, H., Li, X., et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. Zhang, H., Li, X., and Bing, L. Video-llama: An instructiontuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. Xu, J., Guo, Z., He, J., Hu, H., He, T., Bai, S., Chen, K., Wang, J., Fan, Y., Dang, K., et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025a. Zhang, Y., Wu, J., Li, W., Li, B., Ma, Z., Liu, Z., and Li, C. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. Xu, J., Guo, Z., He, J., Hu, H., He, T., Bai, S., Chen, K., Wang, J., Fan, Y., Dang, K., et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025b. Zhao, J., Wei, X., and Bo, L. R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning. arXiv preprint arXiv:2503.05379, 2025a. 13 OmniVideo-R1 Zhao, J., Yang, Q., Peng, Y., Bai, D., Yao, S., Sun, B., Chen, X., Fu, S., Wei, X., Bo, L., et al. Humanomni: large vision-speech language model for human-centric video understanding. arXiv preprint arXiv:2501.15111, 2025b. Zhao, S., Zhou, S., Blanchard, R., Qiu, Y., Wang, W., and Scherer, S. Tartan imu: light foundation model for inertial positioning in robotics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2252022529, 2025c. Zhao, Z. and Chen, B. M. Benchmark for evaluating initialization of visual-inertial odometry. In 2023 42nd Chinese Control Conference (CCC), pp. 39353940. IEEE, 2023. Zheng, C., Ke, P., Zhang, Z., and Huang, M. Click: Controllable text generation with sequence likelihood contrastive learning. arXiv preprint arXiv:2306.03350, 2023. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Zhong, H., Zhu, M., Du, Z., Huang, Z., Zhao, C., Liu, M., Wang, W., Chen, H., and Shen, C. Omni-r1: Reinforcement learning for omnimodal reasoning via two-system collaboration. arXiv preprint arXiv:2505.20256, 2025. Zhou, J., Shu, Y., Zhao, B., Wu, B., Liang, Z., Xiao, S., Qin, M., Yang, X., Xiong, Y., Zhang, B., et al. Mlvu: Benchmarking multi-task long video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1369113701, 2025a. Zhou, Z., Wang, R., and Wu, Z. Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities. arXiv preprint arXiv:2505.17862, 2025b. Zhou, Z., Wang, R., and Wu, Z. Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities, 2025c. URL https://arxiv.org/ abs/2505.17862. 14 OmniVideo-R1 In this Appendix, we provide more technical details, including 1) Detailed descriptions of training dataset and benchmarks in Sec. A; 2) the specific prompts used in our experiments in Sec. B; 3) the hyperparameter configurations of training settings in Sec. C; and 4) the limaitation and future work in Sec. A. Dataset A.1. Training Dataset As illustrated in Fig. 4, we primarily perform three-stage data filtering and selection pipeline to obtain high-quality audiovideo data. To more clearly present the distribution of the processed data, we report descriptive statistics of the resulting training dataset as shown in Fig. 7. That is, our dataset comprises 16 categories with varying numbers of samples, ranging from 35 to 34598. The questions with audiovideo are of high quality and exhibit substantial diversity in content. A.2. Benchmarks A.2.1. AUDIO-VISUAL BENCHMARKS OmniVideoBench (Li et al., 2025a): large-scale, carefully curated benchmark for evaluating synergistic audiovisual reasoning, with particular emphasis on modality complementarity and logical coherence. It contains 1000 high-quality questionanswer pairs, derived from 628 diverse videos spanning from few seconds to 30 minutes. Daily-Omni (Zhou et al., 2025b): an audiovisual question answering dataset containing 684 daily-life videos from diverse sources, rich in both auditory and visual cues, and providing 1197 multiple-choice QA pairs spanning 6 major tasks. WorldSense (Benchekroun et al., 2023): benchmark emphasizing omnimodal collaboration, with strongly coupled audiovideo tasks that require synergistic multimodal perception. It contains 1662 synchronized audiovisual videos across 8 domains and 67 subcategories, and 3172 multiple-choice QA pairs covering 26 tasks for comprehensive evaluation IntentBench (Yang et al., 2025b): benchmark designed to evaluate models understanding of complex human intentions and emotions, comprising 633 videos and 2689 questions grounded in both auditory and visual cues. VideoHolmes (Cheng et al., 2025): Sherlock Holmesinspired benchmark for evaluating complex video reasoning in MLLMs, featuring 1837 questions from 270 annotated suspense short films across seven tasks, each requiring models to connect dispersed visual clues and underlying causal events. 15 A.2.2. VISUAL-ONLY BENCHMARKS Video-MME (Fu et al., 2025a): the first full-spectrum, multimodal evaluation benchmark for MLLMs in video analysis, covering 6 major visual domains and 30 subdomains, with 900 videos ranging from 11 seconds to 1 hour (totaling 254 hours) and 2700 QA pairs. MLVU (Zhou et al., 2025a): the benchmark focuses on long videos and diversity in both video types and evaluation tasks, with durations ranging from 3 minutes to 2 hours and total of 9 different evaluation tasks. In this paper, we use its dev subset for evaluation. LVBench (Wang et al., 2025d): benchmark specifically designed for ultra-long video understanding spanning several hours, aimed at testing MLLMs long-term memory and extended comprehension abilities. It contains 103 videos and 1549 questionanswer pairs in total. B. Instruction Details B.1. OmniVideo-R1 As illustrated in Fig. 8, we employ specialized system prompt as well as the fixed suffix to the user prompt for OmniVideo-R1. In this way, the model can undergo training for specific reasoning behaviors starting from zero-RL. B.2. Data Preparation In terms of data preparation, in the last stage, we perform categorization based on Qwen-3-32B (Yang et al., 2025a) as the instruction shown in Fig. 9, dividing the data into 16 categories. The final results of this categorical analysis are shown in Fig. 7. B.3. Consistency Judger As shown in Fig. 2, the consistency judger is mainly used for rewarding the consistency of time-caption pairs. It is primarily based on Qwen3-VL-235B-A22B-Instruct (Bai et al., 2025a) for scoring, and the corresponding prompt is illustrated in Fig. 11. B.4. Completeness Evaluator Meanwhile, as shown in Fig. 2, the completeness evaluator is mainly used to assess the completeness of multiple audiovideo segments produced by query-intensive grounding. It is also based on Qwen3-VL-235B-A22B-Instruct (Bai et al., 2025a) for scoring, and the corresponding prompt is illustrated in Fig. 12. OmniVideo-R1 Figure 7. (a) Our training data covers 16 categories. (b) Number of questions in terms of each category. Figure 8. System prompt and user prompt suffix for OmniVideo-R1 reasoning. Figure 9. Instruction for data categorizing in data preparation. 16 OmniVideo-R1 Figure 10. Instruction for data quality assessment in data preparation. OmniVideo-R1 Figure 11. System prompt and user prompt for consistency judger. Figure 12. System prompt and user prompt for completeness evaluator. 18 OmniVideo-R1 C. Implementation details We set more of the key hyperparameters as follows: FPS_MAX_FRAMES 64 to cap the number of frames per sample, lr_warmup_fraction 0.05 to gradually ramp up the learning rate at the start of training, ϵ 3 104 and ϵhigh 4 104 as clipping thresholds, KL regularization coefficient β 0.03 to penalize large deviations from the reference policy, and moe_aux_loss_coeff 103 to weight the auxiliary load-balancing loss for the mixture-of-experts. D. Limitation & Future Work. (1) Current methods still rely on outcome-based groundtruth for training. Exploring how to effectively strengthen the model in the absence of ground-truth could be an important direction for future research. (2) The multimodal training paradigm is not restricted to audiovisual inputs. With query intention and modality attention, it can extend to more modalities (e.g., 3D)."
        }
    ],
    "affiliations": [
        "CUHK",
        "HNU",
        "NUS",
        "THU",
        "Tencent HY",
        "XJTU"
    ]
}