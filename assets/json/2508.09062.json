{
    "paper_title": "VertexRegen: Mesh Generation with Continuous Level of Detail",
    "authors": [
        "Xiang Zhang",
        "Yawar Siddiqui",
        "Armen Avetisyan",
        "Chris Xie",
        "Jakob Engel",
        "Henry Howard-Jenkins"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce VertexRegen, a novel mesh generation framework that enables generation at a continuous level of detail. Existing autoregressive methods generate meshes in a partial-to-complete manner and thus intermediate steps of generation represent incomplete structures. VertexRegen takes inspiration from progressive meshes and reformulates the process as the reversal of edge collapse, i.e. vertex split, learned through a generative model. Experimental results demonstrate that VertexRegen produces meshes of comparable quality to state-of-the-art methods while uniquely offering anytime generation with the flexibility to halt at any step to yield valid meshes with varying levels of detail."
        },
        {
            "title": "Start",
            "content": "VertexRegen: Mesh Generation with Continuous Level of Detail Xiang Zhang1* Yawar Siddiqui2 Armen Avetisyan2 Chris Xie2 Jakob Engel2 Henry Howard-Jenkins2 1UC San Diego 2Meta Reality Labs Research 5 2 0 2 2 ] . [ 1 2 6 0 9 0 . 8 0 5 2 : r Figure 1. Mesh generation process of VertexRegen (coarse-to-fine process) vs. previous work (partial-to-complete process). Prior work generates meshes face by face, with the step count corresponding to the face count in the figure. In contrast, VertexRegen produces meshes with continuously increasing level of detail, where each step yields valid mesh Mk."
        },
        {
            "title": "Abstract",
            "content": "We introduce VertexRegen, novel mesh generation framework that enables generation at continuous level of detail. Existing autoregressive methods generate meshes in partial-to-complete manner and thus intermediate steps of generation represent incomplete structures. VertexRegen takes inspiration from progressive meshes and reformulates the process as the reversal of edge collapse, i.e. vertex split, learned through generative model. Experimental results demonstrate that VertexRegen produces meshes of comparable quality to state-of-the-art methods while uniquely offering anytime generation with the flexibility to halt at any step to yield valid meshes with varying levels of detail. 1. Introduction Meshes are essential for 3D asset representation and are widely used in industries such as film, design, and gaming due to their compatibility with most 3D software and *Work conducted while the author was an intern at Meta. Project page: https://vertexregen.github.io hardware. However, manually creating high-quality meshes is costly and time-consuming, prompting extensive research into automated 3D content creation. Largely, these approaches have used alternative representations such as neural fields [11, 27, 31, 38], voxels [34, 51] or point clouds [54, 59] which are later post-processed to meshes instead of direct mesh modeling. Unfortunately, these post-processed meshes often exhibit poor topology, overtessellation, and artifacts, lacking the quality of artistcrafted meshes. Recently, there has been surge of approaches that directly generate meshes, using autoregressive models to represent meshes as sequences of triangles [24, 15, 37, 43]. These methods capture the high-fidelity and aesthetic qualities of artist-created works without the need for postgeneration conversion. Although significant progress has been made in improving tokenization schemes [2, 4, 37] and network architectures [15], these approaches do not fundamentally alter key characteristic of generation: namely, to produce valid mesh, the full sequence must be generated to completion. Consequently, these methods offer no control over the level of detail during generation; early stopping results in mesh with missing faces. Simple extensions, 1 such as face-count conditioning tokens in EdgeRunner [43], have been proposed to introduce some coarse control over the detail by pre-specifying target face count. However, each generation still must be completed in its entirety to yield complete mesh, and thus single generation still offers only single level of detail. To allow for continuous level of detail generation, we take inspiration from Hoppes progressive mesh formulation [17]. Progressive meshes use two reversible operations to transition between levels of detail. The edge collapse operation simplifies mesh into coarser one by reducing one edge at time. The vertex split operation reverses this simplification to add more detail by using information stored during edge collapse. By starting with coarse mesh and keeping edge collapse records, this approach creates an efficient, lossless representation of the original mesh, allowing for continuous resolution adjustments by applying any number of vertex split operations. In this paper, we build on this progressive mesh representation by learning the vertex split, i.e. reversing the edge collapse operation, as generative problem. This allows the resulting mesh generation to inherit the properties of progressive meshes and provides solution for anytime mesh generation, where the process can be stopped early to yield coarser mesh rather than an incomplete one. By properly serializing the vertex split sequence, the entire generation process can be modeled using Transformer trained with next-token prediction objective, widely adopted paradigm in mesh generation. We evaluate VertexRegen on the task of unconditional mesh generation, demonstrating superior results both qualitatively and quantitatively. We further provide examples of shape-conditioned generation. These experimental results illustrate the ability of our method to generate compelling meshes at continuous resolution. Our contributions can be summarized as follows: Inspired by Hoppes progressive meshes [17], we reframe mesh generation as the reversal of edge collapse operations, i.e. generating vertex splits. We formulate token-efficient parameterization of progressive mesh, through half-edge data structure. We propose VertexRegen for continuous level of detail mesh generation. VertexRegen creates meshes in coarse-to-fine fashion, rather than partial-to-complete, uniquely providing solution for anytime generation. 2. Related Work 3D Mesh Generation. Recent advancements in 3D shape generation have explored various representations, including point-clouds [30, 44, 52, 59], signed distance functions (SDFs) [7, 19, 22, 23, 36, 53, 57], neural radiance fields (NeRFs) [16, 18, 46, 55], and Gaussian splatting [14, 42]. These implicit representations require iso-surface extraction techniques [24, 35, 47] to output meshes, often resulting in over-tessellated and excessively smooth outputs, which pose challenges for downstream applications such as geometric processing and manipulation. In contrast, direct mesh generation inherently produces structured, well-defined geometry without the need for postprocessing or surface extraction, making it an increasingly prominent approach in recent years. Early methods tackle this task by generating meshes from surface patches [13], deforming ellipsoids [45], predicting mesh graphs [8], or employing binary space partitioning [6]. More recent techniques leverage generative models, particularly diffusion models and sequence-based approaches. PolyDiff [1] applies discrete diffusion, while PolyGen [29] autoregressively predicts vertices and faces using two separate networks. The sequence modeling paradigm has been further refined by representing the entire mesh as single sequence. MeshGPT [37] introduces tokenization scheme based on vector quantized variational auto-encoder (VQVAE), while MeshXL [2] directly models discretized triangle soup sequences without compression. MeshAnything [4] extends MeshGPT by incorporating point-cloud encoder for shape-conditioned generation. PivotMesh [49] introduces hierarchical approach, generating pivot vertices before producing the full mesh. Further research has focused on optimizing tokenization and sequence modeling. Adjacency-aware compression techniques [3, 43, 50] improve tokenization efficiency and enable higher face counts within fixed context windows, while Meshtron [15] leverages Hourglass Transformers and sliding window attention to scale MeshXL sequences more effectively. Despite these advancements, most existing approaches follow partial-to-complete paradigm, where mesh regions are constructed sequentially at uniform level of detail. Our work instead adopts coarse-to-fine approach, continuously increasing the level of detail as new tokens are generated, achieving better control over geometric complexity. Level of Detail Representations. Level of Detail [26] (LOD) is widely used technique in computer graphics to optimize rendering performance by reducing the complexity of 3D models based on their size, distance from the camera, or importance in scene. Different LOD strategies depend on the underlying representation of 3D shapes. For meshes, progressive meshes [17] generate mesh sequence starting from coarse base model, gradually refining it through series of transformations that incrementally add detail. Traditional mesh simplification methods [12, 20] can be adopted to construct such sequences by iteratively reducing polygon count while preserving geometric fidelity. Progressive simplicial complexes [32] extend progressive meshes to handle arbitrary meshes, including non-manifold and non-orientable surfaces. Recent studies have incorporated neural networks for 2 process can be repeated until there are no candidate edge collapses, e.g. without flipping face normals. vertex split, vsplit(vs, vl, vr, vt), defines the inverse of edge collapse. It restores vt and the two faces, {vt, vs, vl} and {vs, vt, vr}, which vanished during edge collapse, as well as vs to its original position. In case (vs, vt) is boundary edge, either vl or vr will not exist, and there is only one face restored during the vertex split. The combination of edge collapse and its inverse vertex splits enables the progressive mesh representation, PM(M) = (M0, {vsplit0, , vsplitn1}) to express an arbitrary triangle mesh, M, as combination of coarse mesh, M0, obtained through edge collapse, and the sequence of vertex split records required to reverse them: M0 vsplit0 M1 vsplit1 vsplitn1 3.2. VertexRegen Traditional progressive meshes require starting an initial detailed mesh to be simplified into coarse mesh, M0, and recording each of these collapse steps as vertex split records to form the representation. We observe that the formation of the PM representation serves effectively as forward (edge collapse) and reverse (vertex split) processes that enable transition between detailed and coarse meshes. VertexRegen frames the creation of detailed mesh as the generation of PM representation. Analogous to denoising for diffusion models, we train generative model to reverse edge collapse. Concretely, VertexRegen first generates coarse mesh, M0, from scratch, before increasing the level of detail through the generation of vertex split records. 3.2.1. Progressive Mesh Parameterization We frame generation as an autoregressive sequential modeling task, with the full mesh sequence taking the form: : [ <bos>, [M 0 sequence], <sep>, #M0 [vsplit 0], ..., [vsplit n-1], <eos> ] #vsplits In the following, we will define the formation of the subsequences [M 0 sequence] and [vsplit]. M0: Coarse Mesh Tokenization. For the initial coarse mesh, we follow the tokenization scheme defined in MeshXL [2]. In this formulation, embeddings are learned for discretized coordinates in an 3 grid. vertex is represented via the sequential look-up of x-value, y-value, and z-value. face is then constructed as the concatenation of its 3 vertices, totaling 9 tokens. The full mesh sequence is then defined as the concatenation of its constituent faces. : [ <x>, <y>, <z> ], : [ [v 1], [v 2], [v 3] ], : [ [F 1], [F 2], ..., [F N] ] #vertex #face #mesh Figure 2. Illustration of edge collapse and its inverse operation, i.e. vertex split. During edge collapse, vertex vt is collapsed into vs, resulting in two degenerate triangles (shaded in yellow), which forms two new edges in the result mesh. LOD representations. Neural progressive meshes [5] propose learned approach supporting LOD with subdivision-based encoder-decoder. Similar representations have been explored for reconstruction approaches with signed distance functions (SDFs) [3941] and neural fields [28]. However, these approaches are not generative; they typically encode and decode existing meshes [5] or optimize the LOD representation per shape [39]. In contrast, our approach is purely generative. VertexRegen learns to create meshes from scratch, progressively adding detail through autoregressive sequence generation. This allows us to generate new, high-fidelity meshes without relying on pre-existing structures, distinguishing our method from both traditional and neural LOD techniques. 3. Method 3.1. Progressive Meshes Overview progressive mesh [17] (PM) proposes an efficient and lossless continuous-resolution representation for arbitrary triangle meshes. The representation is built off two observations: (i) that single mesh transformation, edge collapse, is sufficient for effective simplification of meshes; (ii) that edge collapse transformations are invertible via vertex split operation. In the following, we provide details of the edge collapse operation and its inversion with vertex splits, each illustrated in Fig. 2. The edge collapse operation, ecol(vs, vt), unifies two adjacent vertices vs and vt into single vertex vs. The operation results in the vanishing of two faces, {vt, vs, vl} and {vs, vt, vr}, as well as the vertex vt. In the general case, new position for vs is also specified; however, for the case of half-edge collapse, the original position for vs is kept. An initial mesh, M, can be simplified into more coarse mesh, Mn, by successive edge collapse operations: ecol0 M1 ecol1 ecoln1 The order in which the edge collapse operations are performed is determined such that each successive edge collapse results in the minimum increase in Quadratic Error Metrics (QEM) [12] with respect to the original mesh. This 3 Figure 3. Face count distribution of the coarsest mesh M0 and original mesh M. The coarsest meshes (M0) contain significantly fewer faces than original (M), with an average of 18 and 457 faces, respectively. Following [37], we sort vertices in z-y-x order (lower to higher). Within each face, vertices are cylindrically permuted to have the lowest-indexed vertex be first. Figure 4. Illustration of half-edge data structure and the traversal process to determine the neighbors of vs and vt in Mk+1. Starting from half-edge H1 ls, we traverse the faces in clockwise direction until H2 sr, where we collect the vertices v1 and v2 into Nk+1(vs). this leverages Although MeshXL [2] tokenization scheme to produce full detailed meshes, in VertexRegen only the coarsest level of the mesh is parameterized in this way. These coarse meshes consist of substantially fewer faces than the full detailed mesh, as demonstrated in Fig. 3, and the resulting coarse mesh only accounts for 5.68% of the total sequence length on average. Vertex Split Generation. Consider the k-th vertex split operation vsplitk, which converts lower-detailed mesh Mk into higher-detailed mesh Mk+1. We denote the neighbor vertices of vertex in Mk as Nk(v). After determining the target vertex vs to split (in Mk) and the new vertex vt (in Mk+1), we need to obtain Nk+1(vs) and Nk+1(vt) from the vertex split record to connect vs and vt with correct neighbor vertices in Mk+1, which are prohibitive to generate as the number of neighbor vertices Nk+1(vs) and Nk+1(vt) is often large. However, as mesh Mk is generated from Mk+1 during edge collapse by merging vt into vs, we have Nk+1(vs) Nk+1(vt) = {vl, vr} Nk+1(vs) Nk+1(vt) {vs, vt} = Nk(vs) (1) (2) which indicates we will only need to record vertex vl and vr. They split the vertex ring surrounding vs in Mk, where two halves of the vertices on the ring (top and bottom vertices in Fig. 2) belong to vt and vs respectively in Mk+1. However, an ambiguity arises in determining which half of the ring corresponds to vts neighbors in Mk+1. In the following, we demonstrate how the half-edge data structure [48] can be utilized to resolve this ambiguity. Denote Hk ij as half-edge pointing from vi to vj, with vk being the third vertex in the associated face. As shown in 4 Figure 5. Illustration of VertexRegen tokenization. The sequence begins with base mesh M0, followed by vertex split subsequences. special <nil> token indicates either vl or vr does not exist. Fig. 4, we begin the traversal from H1 twin of the next half-edge, H2 until we reach Hr the faces above (vl, vs) and (vs, vr) clockwise, obtaining: ls and proceed to the 1s. This operation is repeated s. Throughout this process, we traverse {vk Hk s, = r} = Nk+1(vs) {vl, vr, vt} (3) where Hk is the half-edge encountered during traversal. In conjunction with Eqs. (1) and (2), Nk+1(vt) can also be determined. When (vs, vt) is boundary in Mk+1, we follow either sr counterclockwise until we reach half-edge of the boundary. ls clockwise or After identifying the neighbors of vs and vt in the mesh Mk+1, we add the new vertex vt to the mesh Mk and reconnect the vertices in Nk+1(vt) to vt. Finally, we restore two faces, {vs, vl, vt} and {vr, vs, vt}, which are associated with the half-edges Ht rs, respectively. Using the half-edge data structure, the orientation of the newly created faces remains consistent with the rest of the mesh. When (vs, vt) lies on the boundary, only one face is restored. sl and Ht Hence, with the above half-edge formulation, each vertex split operation can be uniquely defined by the selection of the target vertex vs and two neighbors vl and vr, as well as the position to place the new vertex vt. Vertex Split Tokenization. Although vs, vl and vr only require references to existing vertices in the mesh, in practice we implement this reference as raw prediction of each vertex to avoid vocabulary size proportional to the sequence length. Thus, in the majority of cases, each vertex Figure 6. Generation process for VertexRegen. M0 represents the predicted initial coarsest mesh, followed by subsequent meshes generated through the predicted vertex split sequence. split is represented by subsequence of 12 tokens (or 10 tokens when (vs, vt) is boundary edge): vsplit : [ <s x>, <s y>, <s z>, <l x>, <l y>, <l z> <nil>, <r x>, <r y>, <r z> <nil>, <t x>, <t y>, <t z> ] #vs #vl #vr #vt where the special token <nil> signifies vl or vr does not exist, and only one of vl and vr is allowed to be <nil> for valid vertex split subsequence. In Fig. 5 we illustrate the entire tokenization process. The above progressive mesh serialization can then be trained with the standard next-token prediction target. Vertex Split Decoding. The vertex split sequence must be consistent with the actual geometry to be valid. We maintain state machine and perform vertex split on the fly as new tokens are generated. After the coarsest mesh M0 is generated, we initialize the state. During each generation step, we enforce vs to be vertex in the current mesh Mk, (vs, vl), (vs, vr) are valid edges (if vl and vr are not <nil>), and only one of vl and vr can be <nil>. Lastly, we decode vt, and perform vertex split with generated vs, vl, vr, vt information. 3.3. Conditional Generation We consider generation conditioned on shapes. We adopt pre-trained point-cloud encoder [58]. LLaVa-style [21] projector is leveraged to project the condition features to the token embedding space. The projected feature tokens are prepended to the mesh sequence as the prefix. We supervise training with the next-token prediction objective while masking the loss for the prefix tokens."
        },
        {
            "title": "Tokenization",
            "content": "Flattened Coords. MeshXL [2] MeshAnything V2 [3] AMT EdgeRunner [43] VertexRegen (Ours) EdgeBreaker [33] Progressive COV (%, ) MMD (103, ) 1-NNA JSD () (%) 51.76 50.33 51.39 51.03 8.30 8.50 7.81 8.29 50.84 52.25 49.44 50.22 3.81 4.84 3.22 2.89 Table 1. Quantitative comparisons with state-of-the-art methods for unconditional mesh generation. Best results are bolded, second best are underlined. VertexRegen can generate meshes with comparable quality while enjoying the benefits of continuous level of detail. Figure 7. Unconditional generation under face count constraints. VertexRegen achieves significantly better COV, MMD, and 1-NNA in early stages of generation. 4. Experiments 4.1. Datasets We pre-train our model and the baselines presented using two primary sources of meshes: Objaverse-XL1 [9] and Inian additional set of licensed artist-created meshes. tially, we select meshes containing fewer than 8,000 faces without applying decimation. We use the CGAL [10] library to implement edge collapse and vertex split operations, where we modify the vertex placement to either vs or vt. We filter out non-manifold meshes and those that cannot be processed, which results in final dataset of approximately 1.5M meshes with an average of 457 faces. For unconditional generation evaluation, we construct highquality subset by further filtering with the alignment split of Objaverse-XL and with fewer than 800 faces. This yields approximately 18k meshes. We adopt similar filtering process for shape-conditioned experiments. We apply robust data augmentations to input meshes, including random shift within range [0.1, 0.1], random scaling between [0.9, 1.1], random rotation by 0, 90, 180, or 270. During pre-training, we handle sequences exceeding the context window by discarding vertex split subsequences, while for other baselines, we truncate the tokenized sequence directly. The set for quantitative evaluation is chosen to avoid truncation for any method evaluated. 1We neither used assets from Sketchfab nor obtained any from the Polycam website. 4.2. Implementation Details VertexRegen and all other baselines are built upon the pretrained OPT-350M [56], with newly initialized token embeddings and position embeddings. We train our model on 64 H100 GPUs for approximately four days, with an effective batch size of 256. We use the AdamW [25] optimizer with weight decay of 0.1, and betas (0.9, 0.95). cosine scheduler is employed, starting with an initial learning rate of 1 104 and gradually decreasing to 5 106. We clip the gradient norm to 1.0. During inference, we adopt the top-p sampling strategy with = 0.95 by default. 4.3. Results 4.3.1. Unconditional Generation We follow evaluation protocols from prior works [1, 2, 37], employing point-cloud-based metrics to assess unconditional generation. Specifically, we sample the same number of meshes as the evaluation dataset and randomly sample 2,048 points per mesh. Coverage (COV) measures the diversity of the generated samples, where higher values indicate greater diversity. Minimum Matching Distance (MMD) computes the average distance from each reference sample to its nearest neighbor in the generated set, serving as measure of generation quality, with lower values being preferable. 1-Nearest Neighbor Accuracy (1-NNA) evaluates both diversity and quality, where an optimal value is achieved at 50%. Additionally, we compute JensenShannon Divergence (JSD) to directly quantify the sim6 Figure 8. Qualitative comparison with state-of-the-art methods. VertexRegen is able to generate meshes with comparable quality to other baselines. Figure 9. Qualitative results on shape-conditioned generation on meshes held out from training. The first column shows the point cloud used as the condition, followed by the generation sequence progressing from the coarsest mesh M0 to the final output in the last column. 7 ilarity between the generated and reference distributions. We present results in Tab. 1. Our method can generate meshes of comparable quality to other state-of-the-art methods, with the added advantage of continuous level of detail. We also show qualitative examples and their generation process in Fig. 6, and more generation results comparing with state-of-the-art methods in Fig. 8. Generation with face count constraint. We compare our method with the baselines under varying face count constraints. Our approach allows for generation to be paused at any point, accommodating the varying constraints. In contrast, to adapt to these constraints, baseline methods require either: (i) directly truncating the generation process, yielding incomplete intermediate meshes, or (ii) fine-tuning with an additional face count condition token. For the direct truncation version, we evaluate unconditional generation metrics w.r.t. different face count constraints and present the results in Fig. 7. Additionally, we adopt similar approach as proposed in EdgeRunner, where we split [1, 800] range into 4 buckets and prepend the conditioning token to the mesh sequence. We compare VertexRegen and MeshXL fine-tuned with such conditioning scheme at face constraint 400, as shown in Tab. 2. Due to its ability to generate meshes with continuous level of detail from coarse to fine, VertexRegen effectively captures the overall structure even with very limited face counts, yielding significantly better COV, MMD, and 1NNA early in the process. As the face limit increases, all methods demonstrate improved results. Notably, for baselines such as EdgeRunner, while they enhance tokenization efficiency over MeshXL, they do not fundamentally address continuous level-of-detail generation and thus follow similar trend to other baselines in the plot. 4.3.2. Conditional Generation We show qualitative examples of shape-conditioned generation in Fig. 9. We condition on 4,096 sampled points with normals from dense meshes. VertexRegen can generate coarse M0, starting as simply as tetrahedron, and progressively refine it by generating sequence of vertex splits. 4.4. Ablation Studies Tokenization efficiency. We compute the tokenized sequence length of our proposed tokenization scheme (Sec. 3.2.1) on our dataset and report the average compression ratio relative to MeshXL (9 tokens per face) in Tab. 3. For VertexRegen, there are two primary sequence types: (i) MeshXL-style sequence for the initial coarsest mesh M0 and (ii) sequences encoding vertex splits. The latter requires 12 tokens for two non-boundary faces or 10 tokens per boundary face. As result, the highest compression is achieved when M0 is minimized and all vertex splits occur @400 Faces COV MMD (%, ) (103, ) 1-NNA JSD () (%) V 50.92 VertexRegen MeshXL (w/ FCC) 41.20 8.31 10.03 51.03 59.06 2.88 264 147 5.19 308 168 Table 2. Comparison between VertexRegen and MeshXL with face count condition (FCC) when face count constraint is 400. MeshXL MeshAnything v2 EdgeRunner VertexRegen w/ HE w/o HE Compression 1. 0.46 0.47 0.73 0.89 Table 3. Compression ratio of different tokenization schemes, VertexRegen with and without leveraging half-edge (HE) structure. Guided Decoding w/o w/ COV (%, ) 51.12 51.03 MMD (103, ) 1-NNA JSD () (%) V 8.31 8.29 50.75 50.22 3.37 2.89 211 120 176 Table 4. Effects on geometry-guided decoding. on non-boundary faces. On average, VertexRegen achieves compression ratio of 0.73, approaching the theoretical limit of 0.67. The discrepancy arises from the overhead introduced by M0 (5.68% of all total tokenized length) and boundary vertex splits (3.64% of all vertex splits). Without the half-edge data structure, identifying which half of the ring surrounding vs is associated with vs or vt in Mk+1 requires recording an additional vertex. This increases the token count to 15 per two non-boundary faces (or 13 per boundary face) and results in an average increase of 22% in tokenized sequence length. Guided decoding. In Tab. 4, we ablate the effects of geometry-constrained decoding. As one step of the vertex split operation is dependent on all the preceding operations, predicting an invalid vertex split may break the chain and end the generation prematurely. With guided decoding, the model is able to generate longer sequences with more faces. 5. Conclusion In this work, we introduced VertexRegen, novel mesh generation framework that enables continuous levels of detail through generative process based on vertex splits. Unlike conventional auto-regressive approaches that synthesize meshes in partial-to-complete manner, VertexRegen reinterprets mesh generation as the reversal of edge collapse, providing an effectively anytime solution to mesh generation. Our experimental results demonstrate that VertexRegen achieves competitive performance compared to state-of-the-art methods while offering the unique advantage of halting generation at any stage to obtain meshes at different levels of detail."
        },
        {
            "title": "References",
            "content": "[1] Antonio Alliegro, Yawar Siddiqui, Tatiana Tommasi, and Matthias Nießner. Polydiff: Generating 3d polygonal meshes with diffusion models. arXiv preprint arXiv:2312.11417, 2023. 2, 6 [2] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Billzb Wang, Jingyi Yu, Gang Yu, et al. Meshxl: Neural coordinate field for generative 3d foundation models. Advances in Neural Information Processing Systems, 37:9714197166, 2024. 1, 2, 3, 4, 6 [3] Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. Meshanything v2: Artist-created mesh generation with adjacent mesh tokenization. arXiv preprint arXiv:2408.02555, 2024. 2, 6 [4] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Zhongang Cai, Lei Yang, Gang Yu, Guosheng Lin, and Chi Zhang. Meshanything: Artist-created mesh In The Thirgeneration with autoregressive transformers. teenth International Conference on Learning Representations, 2025. 1, 2 [5] Yun-Chun Chen, Vladimir Kim, Noam Aigerman, and Alec Jacobson. Neural progressive meshes. In ACM SIGGRAPH 2023 Conference Proceedings, pages 19, 2023. 3 [6] Zhiqin Chen, Andrea Tagliasacchi, and Hao Zhang. Bsp-net: Generating compact meshes via binary space partitioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2 [7] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander G. Schwing, and Liang-Yan Gui. Sdfusion: Multimodal 3d shape completion, reconstruction, and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 44564465, 2023. 2 [8] Angela Dai and Matthias Niessner. Scan2mesh: From unIn Proceedings of structured range scans to 3d meshes. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [9] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. 6 [10] Andreas Fabri and Sylvain Pion. Cgal: the computational geometry algorithms library. In Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, page 538539, New York, NY, USA, 2009. Association for Computing Machinery. 6 [11] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: generative model of high quality 3d texIn Advances In Neural tured shapes learned from images. Information Processing Systems, 2022. 1 [12] Michael Garland and Paul S. Heckbert. Surface simpliIn Proceedings of fication using quadric error metrics. the 24th Annual Conference on Computer Graphics and Interactive Techniques, page 209216, USA, 1997. ACM Press/Addison-Wesley Publishing Co. 2, 3 [13] Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, and Mathieu Aubry. papier-mˆache apIn Proceedings proach to learning 3d surface generation. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2 [14] Antoine Guedon and Vincent Lepetit. Sugar: Surfacealigned gaussian splatting for efficient 3d mesh reconstrucIn Proceedings of tion and high-quality mesh rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 53545363, 2024. [15] Zekun Hao, David Romero, Tsung-Yi Lin, and Ming-Yu Liu. Meshtron: High-fidelity, artist-like 3d mesh generation at scale. arXiv preprint arXiv:2412.09548, 2024. 1, 2 [16] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to In The Twelfth International Conference on Learning 3d. Representations, 2024. 2 [17] Hugues Hoppe. Progressive meshes. In Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, page 99108, New York, NY, USA, 1996. Association for Computing Machinery. 2, 3 [18] Heewoo Jun and Alex Nichol. ing conditional 3d implicit functions. arXiv:2305.02463, 2023. 2 Shap-e: GeneratarXiv preprint [19] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusionsdf: Text-to-shape via voxelized diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1264212651, 2023. [20] Peter Lindstrom and Greg Turk. Fast and memory efficient In Proceedings Visualization98 polygonal simplification. (Cat. No. 98CB36276), pages 279286. IEEE, 1998. 2 [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, pages 3489234916. Curran Associates, Inc., 2023. 5 [22] Minghua Liu, Chong Zeng, Xinyue Wei, Ruoxi Shi, Linghao Chen, Chao Xu, Mengqi Zhang, Zhaoning Wang, Xiaoshuai Zhang, Isabella Liu, Hongzhi Wu, and Hao Su. Meshformer: High-quality mesh generation with 3d-guided reconstruction model. In Advances in Neural Information Processing Systems, pages 5931459341. Curran Associates, Inc., 2024. 2 [23] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, and Wenping Wang. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 99709980, 2024. 2 [24] William E. Lorensen and Harvey E. Cline. Marching cubes: In high resolution 3d surface construction algorithm. Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques, page 163169, New York, NY, USA, 1987. Association for Computing Machinery. 2 [25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 6 [26] David Luebke, Martin Reddy, Jonathan D. Cohen, Amitabh Varshney, Benjamin Watson, and Robert Huebner. Level of Detail for 3D Graphics. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2002. 2 [27] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1 [28] Felix Mujkanovic, Ntumba Elie Nsampi, Christian Theobalt, Hans-Peter Seidel, and Thomas Leimkuhler. Neural gaussian scale-space fields. ACM Transactions on Graphics (TOG), 43(4):115, 2024. 3 [29] Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive generative model of 3d meshes. In International conference on machine learning, pages 72207229. PMLR, 2020. 2 [30] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 2 [31] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [32] Jovan Popovic and Hugues Hoppe. Progressive simplicial complexes. In Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, page 217224, USA, 1997. ACM Press/Addison-Wesley Publishing Co. 2 [33] J. Rossignac. Edgebreaker: connectivity compression for triangle meshes. IEEE Transactions on Visualization and Computer Graphics, 5(1):4761, 1999. 6 [34] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao, and Andreas Geiger. Voxgraf: Fast 3d-aware image synthesis with sparse voxel grids. Advances in Neural Information Processing Systems, 35:3399934011, 2022. 1 [35] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. Advances in Neural Information Processing Systems, 34:60876101, 2021. 2 [36] Jaehyeok Shim, Changwoo Kang, and Kyungdon Joo. Diffusion-based signed distance fields for 3d shape generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20887 20897, 2023. 2 [37] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1961519625, 2024. 1, 2, 4, David Novotny. Meta 3d assetgen: Text-to-mesh generation with high-quality geometry, texture, and pbr materials. In Advances in Neural Information Processing Systems, pages 95329564. Curran Associates, Inc., 2024. 1 [39] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1135811367, 2021. 3 [40] Danhang Tang, Mingsong Dou, Peter Lincoln, Philip Davidson, Kaiwen Guo, Jonathan Taylor, Sean Fanello, Cem Keskin, Adarsh Kowdle, Sofien Bouaziz, et al. Real-time compression and streaming of 4d performances. ACM Transactions on Graphics (TOG), 37(6):111, 2018. [41] Danhang Tang, Saurabh Singh, Philip A. Chou, Christian Hane, Mingsong Dou, Sean Fanello, Jonathan Taylor, Philip Davidson, Onur G. Guleryuz, Yinda Zhang, Shahram Izadi, Andrea Tagliasacchi, Sofien Bouaziz, and Cem Keskin. Deep implicit volume compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3 [42] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In The Twelfth International Conference on Learning Representations, 2024. 2 [43] Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. Edgerunner: Auto-regressive auto-encoder for artistic mesh generation. In The Thirteenth International Conference on Learning Representations, 2025. 1, 2, [44] Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis, et al. Lion: Latent point diffusion models for 3d shape generation. Advances in Neural Information Processing Systems, 35:1002110039, 2022. 2 [45] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European conference on computer vision (ECCV), pages 5267, 2018. 2 [46] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for highquality mesh. arXiv preprint arXiv:2404.12385, 2024. 2 [47] Xinyue Wei, Fanbo Xiang, Sai Bi, Anpei Chen, Kalyan Sunkavalli, Zexiang Xu, and Hao Su. Neumanifold: Neural watertight manifold reconstruction with efficient and highquality rendering support. In Proceedings of the Winter Conference on Applications of Computer Vision (WACV), pages 731741, 2025. 2 [48] Kevin Weiler. Topological structures for geometric modeling (Boundary representation, manifold, radial edge structure). Rensselaer Polytechnic Institute, 1986. 4 [38] Yawar Siddiqui, Tom Monnier, Filippos Kokkinos, Mahendra Kariya, Yanir Kleiman, Emilien Garreau, Oran Gafni, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, and [49] Haohan Weng, Yikai Wang, Tong Zhang, CL Chen, and Jun Zhu. Pivotmesh: Generic 3d mesh generation via pivot vertices guidance. arXiv preprint arXiv:2405.16890, 2024. 2 [50] Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, Tong Zhang, Shenghua Gao, and C.L. Philip Chen. Scaling mesh generation via compressive tokenization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11093 11103, 2025. 2 [51] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning probabilistic latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2016. 1 [52] Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, and Zhuowen Tu. Bayesian diffusion models for In Proceedings of the IEEE/CVF 3d shape reconstruction. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1062810638, 2024. 2 [53] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Instantmesh: Efficient 3d Shenghua Gao, and Ying Shan. mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 2 [54] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. In Advances in Neural Information Processing Systems, 2022. 1 [55] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):116, 2023. [56] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 6 [57] Qingcheng Zhao, Xiang Zhang, Haiyang Xu, Zeyuan Chen, Jianwen Xie, Yuan Gao, and Zhuowen Tu. Depr: Depth guided single-view scene reconstruction with instance-level diffusion. arXiv preprint arXiv:2507.22825, 2025. 2 [58] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, BIN FU, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. In Advances in Neural Information Processing Systems, pages 73969 73982. Curran Associates, Inc., 2023. 5 [59] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 58265835, 2021. 1,"
        }
    ],
    "affiliations": [
        "Meta Reality Labs Research",
        "UC San Diego"
    ]
}