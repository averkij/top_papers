{
    "paper_title": "Learning to Inference Adaptively for Multimodal Large Language Models",
    "authors": [
        "Zhuoyan Xu",
        "Khoi Duc Nguyen",
        "Preeti Mukherjee",
        "Saurabh Bagchi",
        "Somali Chaterji",
        "Yingyu Liang",
        "Yin Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have shown impressive capabilities in reasoning, yet come with substantial computational cost, limiting their deployment in resource-constrained settings. Despite recent efforts on improving the efficiency of MLLMs, prior solutions fall short in responding to varying runtime conditions, in particular changing resource availability (e.g., contention due to the execution of other programs on the device). To bridge this gap, we introduce AdaLLaVA, an adaptive inference framework that learns to dynamically reconfigure operations in an MLLM during inference, accounting for the input data and a latency budget. We conduct extensive experiments across benchmarks involving question-answering, reasoning, and hallucination. Our results show that AdaLLaVA effectively adheres to input latency budget, achieving varying accuracy and latency tradeoffs at runtime. Further, we demonstrate that AdaLLaVA adapts to both input latency and content, can be integrated with token selection for enhanced efficiency, and generalizes across MLLMs. Our project webpage with code release is at https://zhuoyan-xu.github.io/ada-llava/."
        },
        {
            "title": "Start",
            "content": "Zhuoyan Xu1*, Khoi Duc Nguyen1*, Preeti Mukherjee2, Saurabh Bagchi2,Somali Chaterji2, Yingyu Liang1,3, Yin Li1 1University of Wisconsin-Madison 2Purdue University 3The University of Hong Kong 5 2 0 2 7 1 ] . [ 2 5 0 9 0 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have shown impressive capabilities in reasoning, yet come with substantial computational cost, limiting their deployment in resource-constrained settings. Despite recent efforts on improving the efficiency of MLLMs, prior solutions fall short in responding to varying runtime conditions, in particular changing resource availability (e.g., contention due to the execution of other programs on the device). To bridge this gap, we introduce AdaLLaVA, an adaptive inference framework that learns to dynamically reconfigure operations in an MLLM during inference, accounting for the input data and latency budget. We conduct extensive experiments across benchmarks involving question-answering, reasoning, and hallucination. Our results show that AdaLLaVA effectively adheres to input latency budget, achieving varying accuracy and latency tradeoffs at runtime. Further, we demonstrate that AdaLLaVA adapts to both input latency and content, can be integrated with token selection for enhanced efficiency, and generalizes across MLLMs. Our project webpage with code release is at https://zhuoyanxu.github.io/ada-llava/. 1. Introduction Large language models (LLMs) [2, 43] have recently been extended to connect visual and textual data, giving rise to multimodal large language models (MLLMs). Exemplified by LLaVA [35, 36] and other works [1, 30, 32, 37, 54, 66], MLLMs have shown impressive visual reasoning capabilities, but come with significant computational costs. Several efforts have sought to improve the efficiency of MLLMs by exploring lightweight architectures, mixture of experts, or token selection techniques [8, 34, 49, 61, 66]. common characteristic of these MLLM methods is that they generate models with fixed accuracy and latency footprint when performing inference on given input. We argue that MLLMs with fixed computational foot- *Equal contribution Figure 1. Top: AdaLLaVA empowers base LLaVA model with the ability to adapt to varying compute budgets at inference time while maintaining minimal performance loss. Bottom: Given an image, text query and latency budget, AdaLLaVA learns to reconfigure operations within base MLLM, generating appropriate responses while sticking to the budget. prints are insufficient for real-world deployment. Consider an example of deploying an MLLM on server farm. Different requests may have different latency requirements, e.g., requests from mobile application require instant feedback to users, while those from recommendation system can tolerate higher latency due to less frequent updates. Further, the available computing resources may vary over time as the overall loads of the system fluctuate. Similarly, when deployed on an edge device, the latency budget often re1 mains constant, yet the computing resources may vary due to contention produced by other concurrent programs. In spite of this need, developing adaptive inference strategies for MLLMs that are robust across varying computational budgets [25] remains key research challenge. To bridge this gap, we propose latency-aware adaptive inference for MLLMs, aiming to dynamically adjust models computational load based on input content and specified latency or compute budget1. This problem is of both conceptual interest and practical significance. Our key insight is that modern MLLM can be viewed as collection of shallower models, where choosing among these models enables dynamic reconfiguration during inference. For example, prior works have shown that Transformer blocks in an LLM and some attention heads within these blocks can be bypassed with minimal impact on accuracy, while reducing latency [5, 26, 52]. Therefore, strategically selecting these operations during inference results in set of models with shared parameters but distinct accuracylatency tradeoffs, allowing the MLLM to flexibly respond to varying latency budgets. To this end, we present AdaLLaVA, learning-based framework for adaptive inference in MLLMs. As shown in Fig. 1, given an input image, text query, and latency budget, AdaLLaVA empowers an MLLM to answer the query about the image while adhering to the specified budget capability unattainable with the base MLLM. The key to AdaLLaVA lies in learned scheduler that dynamically generates an execution plan, selecting subset of operations within the MLLM based on the input content and specified latency budget. This execution plan ensures that inference is performed within the given budget while maximizing expected accuracy. To enable effective learning of the scheduler, we introduce probabilistic formulation in tandem with dedicated sampling strategy, designed to account for latency constraints at training time. experiments evaluate AdaLLaVA. Our results demonstrate that AdaLLaVA can achieve range of accuracy-latency tradeoffs at runtime. AdaLLaVA exhibits strong adaptability to different latency budgets, effectively trading accuracy for compute during inference. Across several benchmarks, AdaLLaVA retains comparable performance to its base MLLM while operating with higher efficiency (see Fig. 1). For example, on several comprehensive benchmarks, AdaLLaVA can achieve 99.0% and 98.2% average performance of the baseline model when using only 80% and 65% of the latency budget, respectively. Importantly, it consistently adheres to specified latency constraints and generates content-aware execution plans tailored to input images. Furthermore, we extensive conduct We to 1In this paper, we measure models latency and its budget using the number of floating-point operations (FLOPs). Thus, the terms compute budget and latency budget are used interchangeably throughout. show that AdaLLaVA can be integrated with token selection techniques designed to enhance efficiency, making it versatile solution for adaptive inference in MLLMs. Our key contributions are summarized into three folds. We present AdaLLaVA, novel adaptive inference framework for MLLMs. Our method is among the first to enable dynamic execution of MLLMs based on latency budget and the input content at inference time. Our key technical innovation lies in (1) the design of learning-based, latency-aware scheduler, which reconfigures base MLLM model during inference; and (2) probabilistic modeling approach, which incorporates hard latency constraints during MLLM training. Through extensive experiments, we demonstrate that (1) AdaLLaVA can adapt to range of latency requirements while preserving the performance of the base model; and (2) AdaLLaVA can be integrated with token selection techniques to further enhance efficiency. 2. Related Work Multimodal large language models (MLLMs). There has been growing interest in extending text LLMs to multimodal signals, including images [35], video [30], and audio [27]. This leads to the emergence of MLLMs, often involving combining vision encoders with existing LLMs. Flamingo [1] inserts gated cross-attention dense blocks between vision encoder and LLMs to align vision and language modality. BLIP2 [32] introduces Qformer with two-stage pretraining, bridging frozen image encoders and LLMs to enable visual instruction capability. LLaVA [35, 36] and MiniGPT-4 [66] use simple MLP to connect vision embedding and text token, achieving impressive performance across various tasks. Our work builds on these developments and aims to enable adaptive inference in MLLMs under varying latency budgets. Adaptive inference. Adaptive inference refers to the capability in which the computational complexity of making predictions is dynamically adjusted based on the input data, latency budget, or desired accuracy levels [18]. Early works focused on the selection of hand-crafted features in multi-stage prediction pipelines [15, 25, 60]. More recent works have extended these ideas to deep models. For convolutional networks, methods have been developed to selectively downsample inputs, skip layers, or exist early during inference [4, 12, 19, 24, 31, 42, 55, 59]. For vision Transformers, various approaches have been proposed, such as selecting different image patches [44, 46, 56], and choosing different attention heads and blocks [26, 41]. Similar ideas have also been explored for LLMs, where models selectively process tokens [47] or execute subset of the operations [11, 48] during inference. Our approach is conceptually similar to exisiting methods by dynamically selecting subset of model components during inference. Yet unlike prior methods, our work specifically targets the latency-aware inference of MLLMs, predicting feasible execution plans tailored for input while adhering to varying latency budgets. Efficient inference for MLLMs. MLLMs face major challenge in deployment due to their high compuSeveral works have tational costs during inference. designed lightweight model architectures to reduce the costs. Examples include Phi-2 [23], TinyGPT-V [63] and LLaVA-ϕ [68]. Vary-toy [58] enhances performance through specialized vision vocabulary in smaller models. TinyLLaVA [65] and LLaVA-OneVision [30] learn smallscale models with curated training data and pipeline. MoELLaVA [34] and LLaVA-MoD [50] improve efficiency by incorporating mixture-of-experts architectures and parameter sparsity techniques. Recent works also investigate input token selection, as an input image or video can produce large number of vision tokens. MADTP [6] and LLaVAPruMerge [49] introduce token pruning and merging technique to reduce the tokens counts. Recently, Pham et al. [45] propose to selectively disabling attention mechanisms for visual tokens in MLLMs. While our approach also aims to improve the efficiency of MLLMs, it focuses on dynamically adjusting an MLLM to fit varying latency budgets during inference. This makes our approach orthogonal to prior efforts on developing inherently more efficient MLLMs. Through our experiments, we will demonstrate that our approach is compatible with lightweight models and integrates seamlessly with existing token-pruning techniques (e.g., [8, 49]). 3. Adaptive Inference of MLLMs We now present AdaLLaVA, our adaptive inference framework for MLLMs. Given latency budget and an input image-query pair at inference time, AdaLLaVA leverages scheduler learned from data to dynamically reconfigure the execution of MLLMs. Importantly, this scheduler strategically selects subset of operations to execute, catered to the input budget and content. In doing so, AdaLLaVA ensures that the inference adheres to the latency constraint while preserving model accuracy. Fig. 2 (a) provides an overview of our framework, where our designed scheduler takes an input of both multimodal sample and latency budget, and outputs an execution plan. In what follows, we introduce the background on MLLMs (Sec. 3.1), outline our key idea for scheduling MLLMs (Sec. 3.2), present our approach for training and inference with the scheduler (Sec. 3.3), and further describe the details of our solution (Sec. 3.5). 3.1. Preliminaries: MLLMs An MLLM takes an image (or video) Xv and text query Xq = {xq} as its input, and generates an answer Xa = {xa} in text format. Specifically, Xv is first encoded by visual encoder hv() (including vision backbone and its projector) into set of visual tokens {zv Rd}. Similarly, Xq is processed by text encoder ht(), which embeds the words xq into set of text tokens {zq Rd} with zq = ht(xq). These tokens are combined into {zvq} = [{zv}, {zq}], and processed by an LLM (), which decodes the answer Xa in an autoregressive manner: (cid:16)(cid:104) {zvq}, {za <i} (cid:105) (cid:17) ; θ xa , (1) where {za swer xa <i} are text tokens from previously generated an- <i, i.e. za = ht(xa), and θ denotes LLM parameters. For the rest of our paper, we will primarily consider the learning of LLM parameters θthe major portion of parameters within the MLLM. Yet we note that learning encoder parameters (in hv() and ht()) can be done similarly. 3.2. Reconfiguring and Scheduling MLLMs Dynamic reconfiguration. Our key insight is that an MLLM can be conceptualized as collection of shallower models with shared parameters, each offering distinct accuracy-latency tradeoff. This perspective enables dynamic reconfiguration of the MLLM during inference to meet varying latency budgets. To this end, we propose equipping the LLM () with tunable binary switches (0, 1)K, which control the execution of individual operations, such as Transformer blocks or attention heads, at runtime. Each switch determines whether specific operation will be executed (1) or skipped (0). We defer the choice of these operations and the design of these switches to our model instantiation. Here, we focus on the concept of reconfigurable LLM decoding, expressed as (cid:16)(cid:104) {zvq}, {za (cid:105) <i} (cid:17) , s; θ xa . (2) Specifically, () now takes the switches as an additional input, and selectively executes subset of operations when generating its output. Note that the switches do not depend on the decoding step i, i.e., given the input tokens, fixed set of operations is applied to generate all output tokens, although the operations may vary for different inputs. Scheduler. The core of our method is scheduler g() that controls the execution of () during inference. The scheduler g() is trained to predict configuration of switches based on the input tokens {zvq} and an inference latency budget l. This is written as (cid:16) {zvq}, l; ϕ (cid:17) s, (3) where ϕ denotes the parameters of the scheduler g(). 3 Figure 2. Overview of AdaLLaVA. (a) Model architecture: Our latency encoder embeds an input latency budget into latency token, which is further processed by the early part of the LLM. The resulting embedding is then fed into the scheduler, leading to the output of an execution plan that control individual operations in the remaining part of the LLM. Our latency encoder and scheduler are jointly trained with the MLLM. (b) AdaLLaVA-L: the scheduler controls the execution of entire Transformer blocks. (c) AdaLLaVA-H: the scheduler controls the execution of attention heads and MLP neurons, by masking out their activation values and the corresponding weights. The goal of g() is to determine an execution plan that meets the latency requirement while maximizing the accuracy. This requires solving the following combinatorial optimization problem for each input sample: min Σi log (cid:16) xa = (cid:16)(cid:104) {zvq}, {za s.t. Latency (cid:16)(cid:104) (cid:16) {zvq}, {za (cid:105) <i} , s; θ l. (cid:105) (cid:17)(cid:17) , s; θ (4) <i} (cid:17)(cid:17) The objective here is to minimize the negative log likelihood of the target tokenthe standard loss used for training MLLMs, while the constraint ensures that the latency of executing the model falls within the budget. 3.3. Learning to Schedule Execution Plans Learning the scheduler g() poses major challenge. While it is tempting to pursue fully supervised approach, in which g() is trained to predict the exact solution to Eq. (4), doing so requires solving the optimization for each sample at every iteration during training. Even with small number of switches, this is prohibitively expensive. Deterministic modeling. possible solution is to solve relaxed version of the constrained optimization at training time. We initially explored this solution, where we task g() to predict hard execution plan with binary switches and attribute latency violation as part of the objective. This leads to the following loss arg min θ,ϕ Σi log (xa = ())+λ max(0, Latency(f ())l), where λ can be treated as the Lagrange multiplier. The execution of the LLM () relies on the output from the scheduler g(), allowing the joint optimization of () and g(). We empirically found that this method fails to enforce strict latency constraint on the scheduler and often produces 4 suboptimal execution plans that under-utilize the available resources. We demonstrate this limitation through experimental results in Sec. 4.3. Probabilistic modeling. In contrast, we propose probabilistic model to further relax the constraints, avoiding directly solving Eq. (4) while stabilizing the joint training of the LLM and the scheduler. Specifically, we task g() to model distribution over the choice of the switches s, in lieu of making hard decision: (cid:16) {zvq}, l; ϕ (cid:17) (cid:16) s{zvq}, l, ϕ (cid:17) . (5) With slight abuse of notation, we denote p(s{zvq}, l, ϕ) as the probability over the states of binary switches given the input {zvq}, latency budget l, and the scheduler parameters ϕ. Ideally, p(s{zvq}, l, ϕ) = 0 if the execution latency of exceeds the budget l. We now re-formulate the inference of MLLM as sampling from the following hierarchical distribution. (cid:16) (cid:16) xa s{zvq}, l, ϕ (cid:17) , (cid:104) xa {zvq}, {za <i} (cid:105) (cid:17) . , s, θ (6) Conceptually, this formulation defines the following generative process: (1) the scheduler considers the input and latency budget and outputs the conditional probability of the execution plan p(s{zvq}, l, ϕ); (2) an execution plan is then sampled from the predicted distribution without violating the latency constraint; and (3) the plan is executed to sequentially decode xa Modeling (cid:0)s{zvq}, l, ϕ(cid:1). Our design requires that the sampled execution plan strictly adheres to the latency budget while maximizing resource utilization. To achieve this, and generate the answer. we restrict the support of (cid:0)s{zvq}, l, ϕ(cid:1) to the states that have exactly activated switches, where is the maximum number of switches allowed to be turned on without violating l. Specifically, to sample s, (cid:0){zvq}, l; ϕ(cid:1) first outputs categorical distribution over available switches. Then, switches are picked one by one without replacement, following the categorical distribution. Training loss. The probabilistic model allows us to directly train the LLM and the scheduler with the following loss (cid:17)(cid:105) (cid:16) (cid:104) (cid:104) (cid:105) ED log xa {zvq}, {za <i} , l, θ, ϕ , arg min θ,ϕ where is the data distribution approximated by the training set (Xv, Xq, Xa, l) D. By marginalizing s, we have p(xa [{zvq}, {za <i}], l, θ, ϕ)) = p(xa (cid:104) [{zvq}, {za p(s{zvq},l,ϕ) (cid:105) <i}], s, θ) (7) . Thus, the loss function is transformed into (cid:104) (cid:16) ED,sp(s) log (cid:104) {zvq}, {za (cid:105) <i} (cid:17)(cid:105) , , s, θ xa arg min θ,ϕ where (s) = (cid:0)s{zvq}, l, ϕ(cid:1). 3.4. Training and Inference Approximate training. We present an approximate training scheme in the context of stochastic gradient descent (SGD). Specifically, for each training sample within minibatch, latency budget is first sampled uniformly from range of possible budgets, then an execution plan is sampled from (cid:0)s{zvq}, l, ϕ(cid:1). With the sampled guaranteed to satisfy the budget l, the next token xa can be decoded and the log-likelihood log (cid:0)xa <i}(cid:3) , s, θ(cid:1) (i.e., the (cid:2){zvq}, {za loss) can be readily computed. Optimizing this loss requires backpropagation through the sampling process (cid:0)s{zvq}, l, ϕ(cid:1), which we approximate using the GumbelSoftmax trick [22, 40]. See the supplement for more details. Adaptive inference. During inference, the scheduler outputs the probability p(s{zvq}, l, ϕ) over possible switch configurations s, given the input {zvq} and the latency budget l. In theory, decoding the answer Xa requires marginalizing over this distribution, which is infeasible due to the In practice, we approxilarge number of configurations. mate the inference by selecting the most probable execution plan from the scheduler. This approximation bypasses the marginalization and thus remains highly efficient. We empirically verify its effectiveness. Formally, this approximation is given by xa = arg max xa arg max xa Esp(s) (cid:16) (cid:104) xa (cid:104) {zvq}, {za <i} (cid:105) , s, θ (cid:17)(cid:105) (cid:16) xa (cid:104) {zvq}, {za <i} (cid:105) , s, θ (cid:17) , where = arg maxs (cid:0)s{zvq}, l, ϕ(cid:1). 5 3.5. Model Instantiation Design of tunable switches. We consider attaching binary switches to the LLM part of an MLLM, which accounts for the majority of computational costs. We explore two different designs of switches to select operations. AdaLLaVA-L (layer-level): This design attaches binary switches to entire Transformer blocks. When switch is off, the corresponding block is bypassed through its residual connection, becoming an identity mapping. The execution plan thus determines whether each layer is computed or bypassed (see Fig. 2(b)). AdaLLaVA-H (head/neuron-level): This design introduces binary switches within Transformer blocks, targeting individual attention heads in attention modules and specific neurons in MLP layers. When switch is off, its computation is skipped, and its contribution is removed. In MLP, switches function similarly to dropout [53], selectively disabling neuron activations (see Fig. 2(c)). Model architecture. We aim to keep the scheduler as light as possible to minimize the FLOPs overhead while ensuring that it remains expressive enough to make optimal decisions. To this end, we reuse part of the LLM () to extract visual-language features and encode the latency constraint for the scheduler. Specifically, we first design latency encoder that converts latency budget into token embedding, which is then appended to the original input sequence before being processed by the LLM layers. Within the LLM, the latency token is processed by few Transformer blocks, attending to all visual-language tokens. The processed token is then passed to lightweight scheduler that generates the execution plan for the rest of the LLM. Notably, the first few Transformer blocks in the LLM serve two purposes: it simultaneously processes regular MLLM tasks and learns resource allocation based on both content and budget constraints. This design is depicted in Fig. 2 (a). Implementation details. Our latency encoder uses the sinusoidal positional encoding [57] to map the scalar latency to 256-D vector. two-layer MLP, with GELU and layer norm, then converts this vector to latency token zs, ready to be appended to the input sequence of the LLM (see Fig. 2(a)). Our scheduler is implemented as linear layer that maps the processed latency token (from the bottom part of the LLM Transformer blocks) to logits, defining categorical distribution over switch selection. We use FLOPs to quantify the theoretical latency budget, following the calculation in [64]. Specifically, we report the average prefill FLOPs on target dataset, isolating it from variations in decoding length to ensure more consistent evaluation. We split the LLM evenly into two parts unless otherwise specified. We use the first part to process the latency token, and apply tunable switches exclusively to the latter part. In AdaLLaVA-H, we attach switch to each attention head Method LLM Vicuna-13B BLIP-2 [32] Vicuna-7B InstructBLIP [10] Vicuna-13B InstructBLIP [10] Vicuna-13B Shikra [7] LLaMA-7B IDEFICS-9B [28] LLaMA-65B IDEFICS-80B [28] Qwen-7B Qwen-VL [3] Qwen-7B Qwen-VL-Chat [3] Vicuna-7B LLaVA-1.5 [36] w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B Vicuna-7B Prumerge+ [49] w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B FastV (K=2,R=0.5) [8] Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B Budget (%) FLOPs (T) - - - - - - - - 100 100 85 60 100 85 60 100 100 85 60 100 85 60 100 100 85 60 100 85 60 - - - - - - - - 8.6 8.6 7.2 5.1 8.6 7.2 5.1 3.0 3.0 2.6 1.8 3.0 2.6 1.8 4.9 4.9 4.2 3.0 4.9 4.2 3.0 Prefill time VQAv2 (ms) - - - - - - - - 81 81 69 49 81 69 49 29 29 24 17 29 24 17 47 47 40 29 47 40 29 [14] 41.0 - - 77.4 50.9 60.0 78.8 78.2 78.5 78.4 77.1 75.0 77.9 76.8 74.2 76.8 76.3 75.3 73.0 76.0 75.0 72.2 77.7 77.8 76.9 74.5 77.4 76.6 73.9 SQAI VQAT [51] [39] 61 60.5 63.1 - - - 67.1 68.2 66.8 67.8 67.4 66.9 68.5 68.2 68.1 68.3 68.3 68.5 67.7 67.9 68.1 67.6 68.7 67.7 67.8 67.0 68.4 67.7 68.3 42.5 50.1 50.7 - 25.9 30.9 63.8 61.5 58.2 57.0 54.5 47.7 57.1 55.2 48.7 57.1 55.8 52.9 47.4 56.0 54.2 47.2 58.1 57.0 54.4 47.4 57.0 54.8 48.7 POPE MME MMBench [33] [38] [13] 85.3 - 78.9 - - - - - 85.9 85.9 86.4 86.1 86.9 86.7 85.0 84.0 85.1 85.7 85.6 86.6 86.4 86.4 82.5 82.8 83.3 83.8 84.3 83.9 82.4 1293.8 - 1212.8 - - - - 1487.5 1510.7 1521.0 1487.2 1463.8 1471.1 1494.9 1489.6 1462.4 1455.5 1429.5 1450.9 1503.2 1511.8 1458.0 1516.2 1494.3 1478.1 1463.1 1484.2 1520.5 1452.8 - 36.0 - 58.8 48.2 54.5 38.2 60.6 64.3 63.7 63.7 63.8 64.1 64.3 64.8 64.9 61.9 62.5 61.3 63.2 63.6 63.6 64.3 63.5 63.7 63.2 63.8 63.9 65.3 Table 1. Results on MLLM benchmarks. Budget (%): input latency budget w.r.t. the base model latency. AdaLLaVA-L: switches on selecting different Transformer blocks. AdaLLaVA-H: switches on select different attention heads and MLP activations. VQAv2: VQAv2 set. SQAI: ScienceQA set. VQAT: TextVQA set. Prumerge+ and FastV both use LLaVA 1.5. AdaLLaVA enables base MLLM to adapt to varying latency budgets with competitive performance, and can be further integrated with token selection to enhance overall efficiency. in the self-attention. For the MLP, channels are grouped to match the number of attention heads, with each group controlled by single switch This implementation reduces the design space while preserving control granularity. See ablation study on group size in the supplement. 4. Experiments and Results We now present our experiments and results. We introduce our setup (Sec. 4.1), present our main results (Sec. 4.2), and provide further analyses (Sec. 4.3). Additional experiments, including further ablations, are included in our supplement. 4.1. Experimental Setup Experiment protocol. In most of our experiments, we build on LLaVA-1.5 [36]. Training LLaVA and many other MLLMs typically involves two stages: (1) vision-language alignment pre-training; and (2) visual instruction tuning. We focus on the second stage and seek to jointly finetune the LLM within the MLLM and train our scheduler using visual instruction data, while keeping the vision encoder frozen. Once trained, we perform zero-shot inference across multiple benchmarks following the common practice in the community [36], but under varying latency budgets. Training details. Our model is initialized with the pretrained LLaVA-1.5 checkpoints. During finetuning, each training sample is paired with randomly sampled latency budget ranging from 0.5 to 1.0, as by default we only operate on the top half of the layers in LLM. We set the learning rate to 105 for the original LLaVA model and the scheduler, while keeping other training hyperparameters consistent with the original LLaVA stage-2 finetuning protocol. Benchmarks and metrics. We conduct comprehensive evaluation across multiple visual understanding benchmarks, including VQAv2 [14], ScienceQA [39], TextVQA [51], MME [13], and MMBench [38]. We also evaluate on hallucination benchmarks such as POPE [33]. For TextVQA, we specifically focus on the image-based subset, where each question is paired with its corresponding image content. For each benchmark, we report the official metrics on the same dataset splits as in LLaVA-1.5. We report accuracy for VQAv2, ScienceQA, TextVQA and MMBench, perception score for MME, and F1 score for 6 Figure 3. LLaVA-1.5-7B and additional (PruMerge+ / FastV). Results reported on VQAv2. Accuracy-latency tradoffs of AdaLLaVA with token selection techniques POPE. Additionally, we consider varying latency budgets (from 0.5 to 1.0) when evaluating AdaLLaVA. We report the Prefill FLOPs and time on MME benchmark. Baselines and model variants. Baselines in our main results include BLIP2 [10, 32], Shikra [7], IDEFICS [28], Qwen-VL [3], and LLaVA-1.5 [36]. Our closest competitor is the base model LLaVA-1.5 [36]. We evaluate AdaLLaVA with 7B and 13B (see supplement) models, and across two different designs: (a) AdaLLaVA-L for selecting Transformer blocks; and (b) AdaLLaVA-H for selecting attention heads and MLP activations. In our additional analyses, we also use Mipha-3B [67] as the base model. 4.2. Main Results Comparison to baselines. Our main results across six benchmarks are summarized in Tab. 1. AdaLLaVA demonstrates competitive performance with notable efficiency improvements across all benchmarks. AdaLLaVA-L, when applied to LLaVA-1.5 7B, maintains comparable performance under full computational budgets. With reduced compute budgets, AdaLLaVA-L shows minimal performance degradation with an average accuracy drop of only 1.5% at 85% budget and 3.4% at 60% budget. Remarkably, at 60% compute budget, AdaLLaVA-L even has slightly better results than the base model on ScienceQA (66.9 vs. 66.8) and POPE (86.1 vs. 85.9). AdaLLaVA-H shows similar results, with only 1% average performance drop at 85% budget, and 1.9% at 60% budget. The superior performance of AdaLLaVA-H compared to AdaLLaVA-L can be attributed to its head/neuron-level switching mechanism, allowing for more fine-grained control over computational resources than layer-level switches used in AdaLLaVA-L. Importantly, for all results, AdaLLaVA adheres to the specified latency budgets, as we will discuss in Sec. 4.3. Integration with token selection. Token selection techniques have demonstrated recent success in improving the efficiency of MLLMs [8, 49]. AdaLLaVA presents an orthogonal direction in adaptive inference. We now demonstrate that AdaLLaVA can be integrated with token selection to further enhance the efficiency. We combine AdaLLaVA with PruMerge+ [49] and FastV [8], two latest Figure 4. Visualization of attention between the input latency token and visual tokens with 100% latency budget. token selection methods designed for MLLMs. For FastV, we set filtering layer K=2 and filtering ratio R=50% to ensure consistent comparison. The results are shown in Tab. 1. When integrated with PruMerge+ and FastV, AdaLLaVA shows significantly improved efficiency across board, when compared to AdaLLaVA with LLaVA-1.5. Again, AdaLLaVA adapts to varying latency budgets and achieves competitive performance relative to the base model (PruMerge+/FastV). For example, with PruMerge+, AdaLLaVA-H shows 2.45% average performance boost at 85% compute budget and only 1.01% performance drop at 60%. surprising observation is that AdaLLaVA-H achieves strong performance at 85% latency budget, sometimes beating the base model with token pruning. Overall, our results suggest that AdaLLaVA complements to existing token selection approaches. When integrated with Prumerge+ at an 85% latency budget, our approach reduces computational requirements by 70% while maintaining performance with only 1.7% drop in accuracy. 4.3. Additional Analyses Latency adaptivity. We now evaluate the key capability of AdaLLaVA: its adaptivity to input latency budget, i.e., the ability to complete inference under varying latency requirements using single model. We report the accuracy-latency tradeoff of AdaLLaVA variants (i.e., Pareto curves), both with and without token selection, on the VQAv2 benchmark. These results are shown in Fig. 3. Our results shows that AdaLLaVA can empower base MLLM with static compute footprint (i.e., LLaVA-1.5, PruMerge+, or FastV as individual dots in the Fig. 3) to adapt to varying accuracy-latency tradeoffs (i.e., the corre7 Model VQAv2 SQAI VQAT POPE MME MMBench Mipha-3B w/ AdaLLaVA-L-100% w/ AdaLLaVA-L-85% w/ AdaLLaVA-L-60% 81.3 81.1 80.4 77.2 70.9 70.9 71.0 68.4 56.6 55.3 53.0 44.8 86.7 87.7 87.8 88.0 1488.9 1450.4 1429.3 1397.3 69.7 69.2 69.0 64. Table 2. Generalization of AdaLLaVA to Mipha-3B. AdaLLaVA-L (probabilistic scheduler) AdaLLaVA-L (deterministic scheduler) Latency budget Accuracy Success (%) Utilization (%) Accuracy Success (%) Utilization (%) 0.95 0.85 0.75 0.65 75.6 74.9 74.3 72.7 100.0 100.0 100.0 100.0 98.7 99.2 100.0 96.5 75.6 74.6 73.5 72.2 96.1 100.0 100.0 100. 87.6 80.4 83.2 83.1 Table 3. Ablation on deterministic vs. probabilistic modeling for the scheduler. Results reported using 7B model on VQAv2. results, we adopt the probabilistic version with conditional sampling (detailed in Sec. 3.5). We now compare these two approaches across different latency budgets on the VQAv2 benchmark, using AdaLLaVA-L 7B model. The results are summarized in Tab. 3. Our probabilistic model demonstrates superior adaptability across different latency budgets compared to the deterministic approach. We notice deterministic approach has noticeable performance drop given low latency budget due to under-utilization, and sometimes violates the latency budget. These results confirm our choice of the probabilistic modeling. Additional ablations. Ablations on (1) the number and granularity of switches; and (2) different designs of switches (i.e., AdaLLaVA-H vs. AdaLLaVA-L) are included in our supplement due to the space limit. 5. Conclusion and Discussion In this paper, we introduced AdaLLaVA, novel adaptive inference framework designed for MLLMs. AdaLLaVA features lightweight, learning-based scheduler and probabilistic modeling technique. It empowers base MLLM with the ability to adapt to varying latency budgets at inference time. Extensive experiments across benchmarks demonstrated that AdaLLaVA is capable of producing latencyand content-aware execution plans, effectively achieving range of accuracy-latency tradeoffs. Adaptive inference of MLLMs. Unlike LLMs, MLLMs include vision encoder and process large number of redundant visual tokens. While our paper focus on the scheduling of the LLM component, this adaptivity can be further extended to token selection and vision encoder. We hope this work will provide step toward making MLLMs more viable for real-world applications where computational resources may fluctuate significantly. Relationship to other efficiency methods. This paper explores adaptive inference in MLLMs, emphasizing adaptability to varying latency budgets in single model. Our approach is orthogonal to prior methods aimed at improving inference efficiency, such as sparse attention [9] and token Figure 5. Visualization of execution plans for different input. The plan is color-coded with enable or disable for the 16th to 32th Transformer blocks (left to right). The latency budget is 75%. sponding curves in Fig. 3). With varying latency budgets from 50% to 100%, AdaLLaVA effectively trades compute Integrating with token selection methods with accuracy. (PruMerge+ / FastV) further improves the overall efficiency. Thanks to our sampling process in the probabilistic modeling, AdaLLaVA maintains 0% latency violation. Content adaptivity. It is worth noting that AdaLLaVA is also adaptive to the input content, i.e., with the same latency budget, its execution plan is dynamically adjusted based on input. While not our main focus, we present results to illustrate our models content adaptivity, with the aim of providing insights into its behavior and aiding in its diagnosis. We visualize attention maps from the latency token to all input visual tokens, computed right before the latency token is fed into the scheduler. This is shown in Fig. 4. These attention maps point to key regions in the input image that are necessary for answering the target question. For example, in the top, attention concentrates on Yes Man for the movie title question but shifts to the actor name for actor identification question. Further, we visualize the execution plans of different input content given by our scheduler in Fig. 5. Under the same latency budget, AdaLLaVA generates distinct execution plans conditioned on the input visual content. These results show AdaLLaVAs ability to dynamically adjust its computational focus and execution plan based on the input image and text query. See our supplement for additional visualizations. Generalization across MLLMs. We further demonstrate that AdaLLaVA can generalize to other MLLMs beyond LLaVA. We consider Mipha-3B [67], lightweight MLLM built on Phi-2.7B [23]. Specifically, we apply AdaLLaVA-L on Mipha-3B, following its training strategy [67], and report the results on MME benchmark, shown in Tab. 2. These results have similar trend to those with LLaVA-1.5 in Tab. 1. Ablation: probabilistic vs. deterministic modeling of the scheduler. We present two design choices of the scheduler: deterministic and probabilistic (see Sec. 3.3). For our main 8 selection [49]. Indeed, many of these techniques (e.g. token selection as shown in the paper) can be integrated with our framework to further enhance efficiency. Practical deployment. Our work focuses on algorithmlevel innovation, leaving system-level serving optimization as future work. Conceptually, serving AdaLLaVA is similar to serving MoE-based LLMs [20], which also dynamically route tokens to different execution paths based on the input. We invite joint effort from the vision, learning, and systems communities to further explore this direction."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 1, 2 [2] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://wwwcdn.anthropic.com/ de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf, 2024. 1 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 6, 7, 3 [4] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015. 2 [5] Ruisi Cai, Saurav Muralidharan, Greg Heinrich, Hongxu Yin, Zhangyang Wang, Jan Kautz, and Pavlo Molchanov. Flextron: Many-in-one flexible large language model. In Forty-first International Conference on Machine Learning, 2024. 2 [6] Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, and Tao Chen. Madtp: Multimodal alignment-guided dynamic token pruning for accelIn Proceedings of erating vision-language transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1571015719, 2024. 3 [7] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 6, 7, 3 [8] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. 1, 3, 6, 7 [9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. 8 [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, InstructBLIP: Towards general-purpose and Steven Hoi. vision-language models with instruction tuning. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. 6, 7, 3 [11] Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547 5569. PMLR, 2022. 2 [12] Michael Figurnov, Maxwell Collins, Yukun Zhu, Li Jonathan Huang, Dmitry Vetrov, and Ruslan Zhang, Salakhutdinov. Spatially adaptive computation time for residual networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 10391048, 2017. [13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 6, 3 [14] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. 6, 3 [15] Alex Grubb and Drew Bagnell. Speedboost: Anytime preIn Artificial Intellidiction with uniform near-optimality. gence and Statistics, pages 458466. PMLR, 2012. 2 [16] Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation for stochastic neural networks. arXiv preprint arXiv:1511.05176, 2015. 1 [17] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from In Proceedings of the IEEE conference on blind people. computer vision and pattern recognition, pages 36083617, 2018. 1 [18] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: surIEEE Transactions on Pattern Analysis and Machine vey. Intelligence, 44(11):74367456, 2021. 2 [19] Hanzhang Hu, Debadeepta Dey, Martial Hebert, and Andrew Bagnell. Learning anytime predictions in neural netIn Proceedings of the works via adaptive loss balancing. AAAI Conference on Artificial Intelligence, pages 3812 3821, 2019. 2 [20] Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Shruti Bhosale, Hsien-Hsin S. Lee, Carole-Jean Wu, and Benjamin Lee. Toward efficient inference for mixture of experts. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [21] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 1 9 [22] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2017. 5, 1 [23] Mojan Javaheripi, Sebastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio Cesar Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. Phi-2: The surprising power of small language models. Microsoft Research Blog, 2023. 3, 8, 2 [24] Zequn Jie, Peng Sun, Xin Li, Jiashi Feng, and Wei Liu. Anytime recognition with routing convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 43(6):18751886, 2019. 2 [25] Sergey Karayev, Mario Fritz, and Trevor Darrell. Anytime In Proceedings of the recognition of objects and scenes. IEEE Conference on Computer Vision and Pattern Recognition, pages 572579, 2014. [26] Samir Khaki and Konstantinos Plataniotis. The need In The for speed: Pruning transformers with one recipe. Twelfth International Conference on Learning Representations, 2024. 2 [27] Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Yi Ren, Heriberto Cuayahuitl, Wenwu Wang, Xulong Zhang, Roberto Togneri, Erik Cambria, et al. Sparks of large audio models: survey and outlook. arXiv preprint arXiv:2308.12792, 2023. 2 [28] Hugo Laurencon. Introducing idefics: An open reproduction of state-of-the-art visual language model. https: //huggingface.co/blog/idefics, 2023. 6, 7, 3 [29] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 1 [30] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2, [31] Hengduo Li, Zuxuan Wu, Abhinav Shrivastava, and Larry Davis. 2d or not 2d? adaptive 3d convolution selection for efficient video recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 61556164, 2021. 2 [32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1, 2, 6, 7, 3 [33] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 6, 3 [34] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning, et al. Moe-llava: Mixture of experts for large visionlanguage models. arXiv preprint arXiv:2401.15947, 2024. 1, 3 [35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, [36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 1, 2, 6, 7, 3 [37] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge. https: // llavavl.github. io/ blog /20240130llava-next/, 2024. 1 [38] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an In European Conference on Computer all-around player? Vision, pages 216233. Springer, 2025. 6, 3 [39] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 6, 3 [40] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: continuous relaxation of discrete random variables. In International Conference on Learning Representations, 2017. 5 [41] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. Adavit: Adaptive vision transformers for efficient image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1230912318, 2022. [42] Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna Sattigeri, Leonid Karlinsky, Aude Oliva, Kate Saenko, and Rogerio Feris. Ar-net: Adaptive frame resolution for efficient action recognition. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VII 16, pages 86104. Springer, 2020. 2 [43] OpenAI. GPT-4 technical arXiv preprint report. arxiv:2303.08774, 2023. 1 [44] Bowen Pan, Rameswar Panda, Yifan Jiang, Zhangyang Ia-red2: Wang, Rogerio Feris, Interpretability-aware redundancy reduction for vision transformers. Advances in Neural Information Processing Systems, 34:2489824911, 2021. and Aude Oliva. [45] Phu Pham, Wentian Zhao, Kun Wan, Yu-Jhe Li, Zeliang Zhang, Daniel Miranda, Ajinkya Kale, and Chenliang Xu. Quadratic is not what you need for multimodal large language models. arXiv preprint arXiv:2410.06169, 2024. 3 [46] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. In Advances in Neural Information Processing Systems, 2021. 2 [47] David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. 2 10 [60] Zhixiang Xu, Kilian Weinberger, and Olivier Chapelle. The greedy miser: In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 12991306, 2012. learning under test-time budgets. [61] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 1 [62] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 1 [63] Zhengqing Yuan, Zhaoxu Li, Weiran Huang, Yanfang Ye, and Lichao Sun. Tinygpt-v: Efficient multimodal large arXiv preprint language model via small backbones. arXiv:2312.16862, 2023. 3 [64] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, et al. Llm inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363, 2024. 5 [65] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024. 3 [66] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations, 2024. 1, [67] Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, and Jian Tang. comprehensive overhaul of multimodal assistant with small language models. arXiv e-prints, pages arXiv 2403, 2024. 7, 8, 2 [68] Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Llava-ϕ: Efficient multi-modal arXiv preprint language model. Mou, and Jian Tang. assistant with small arXiv:2401.02330, 2024. 3 [48] Daniel Rotem, Michael Hassid, Jonathan Mamou, and Roy Schwartz. Finding the sweet spot: Analysis and improvement of adaptive inference in low resource settings. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1483614851, 2023. 2 [49] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. 1, 3, 6, 7, [50] Fangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Guanghao Zhang, Haonan Shi, Long Chen, Tao Zhong, Wanggui He, Siming Fu, et al. Llava-mod: Making llava tiny via moe knowledge distillation. arXiv preprint arXiv:2408.15881, 2024. 3 [51] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 6, 3 [52] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer: Fast large language model serving with consumergrade gpu. In Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles, pages 590606, 2024. 2 [53] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):19291958, 2014. 5 [54] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1 [55] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph Gonzalez. Skipnet: Learning dynamic routing In Proceedings of the Euroin convolutional networks. pean conference on computer vision (ECCV), pages 409 424, 2018. 2 [56] Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang. Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition. In Advances in Neural Information Processing Systems, 2021. [57] Waswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin. Attention is all you need. In NIPS, 2017. 5 [58] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, En Yu, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Small language model meets with reinforced vision vocabulary. arXiv preprint arXiv:2401.12503, 2024. 3 [59] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry Davis, Kristen Grauman, and Rogerio Feris. Blockdrop: Dynamic inference paths in residual networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 88178826, 2018. 2,"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we provide (1) additional implementation details (see Supp. A); (2) detailed results, including broader benchmarks and other LLM backbones on AdaLLaVA accompanying our experiments in Sec. 4 (see Supp. B); (3) further ablations on design of switches (see Supp. C) and (4) additional qualitative results on latency and content adaptivity (see Supp. D) We hope that this document will complement our main paper. A. Further Implementation Details"
        },
        {
            "title": "Recall",
            "content": "Probabilistic execution plan sampling. that in our probabilistic model, we define the distribution (cid:0)s{zvq}, l, ϕ(cid:1) via sampling process. Given the input tokens and latency budget l, the output of the lightweight scheduler is logits vector corresponding to the available switches: π1, π2, . . . πK R, where πi represents the relative likelihood of selecting the ith switch. The latency budget allows us to define k, the maximum number of switches allowed to activate. Then, sampled execution plan from (cid:0)s{zvq}, l, ϕ(cid:1) can be uniquely defined by subset of distinct elements from {1, 2, . . . , K}, corresponding to its activated switches. We sample the execution plan by randomly picking switches one by one, without replacement, following the logits {πi}K i=1. The complete sampling procedure is summarized in Algorithm 1, where Cat(Ω, {πi : Ω}) denotes the categorical distribution of selecting an element from Ω with probabilities parameterized by {ηi : Ω} = Softmax({πi : Ω}). The process ensures that the sampled execution plan adheres to the input budget while maximizing the utilization. Algorithm 1 Sampling (cid:0)s{zvq}, l, ϕ(cid:1) Input: Latency budget l, sampling logits {πi}K Output: Sampled binary vector {0, 1}K i=1 Determine number of selections based on Initialize available set of switches Ω {1, 2, . . . , K} Initialize (0, 0, . . . , 0) {0, 1}K for = 1 to do Sample ω Cat(Ω, {πi : Ω}) s[ω] 1 (activating the chosen switch) Ω Ω {ω} end for return Differentiable sampling with Gumbel-Softmax. Our designed scheduler is difficult to train as it involves nondifferentiable discrete sampling process, which prevents gradients from backpropagate to the scheduler during training. common workaround involves using score function estimator [16, 59]; however, this method often suffers from high variance and slow convergence. Instead, we employ Gumbel-Softmax [22], reparameterization In our trick for sampling from categorical distribution. implementation, the Gumbel-Softmax approximates ω Cat(Ω, {πi : Ω}) with continuous random vector ω: ω = Softmax ([gi + log ηi])iΩ , (8) where each gi is i.i.d. sample drawn from Gumbel(0, 1); and ηi = Softmax({πj : Ω})[i] is the probability of activating the ith switch, computed by the scheduler. Note that ω is continuous and has well-defined gradient. To maintain hard execution plan, we take the one-hot encoding of ω and apply the straight-through estimator (see [22] for more details). B. Detailed Results Full results on LLaVA 1.5. We report the full set of results on LLaVA 1.5, LLaVA-PruMerge, LLaVA-PruMerge+ and FastV in Tab. 5, as complement to Tab. 1. All experiments follow the same setting as described in Sec. 4.1. These results confirm that our AdaLLaVA framework successfully adapts to LLaVA 1.5 across different backbone sizes, and can be further combined with recent token selection methods (PruMerge, PruMerge+ and FastV) to further enhance efficiency. We maintain comparable performance while improving efficiency across multiple benchmarks. Additionally, our analysis reveals how performance varies under different latency constraints, demonstrating our frameworks ability to trade between accuracy and latency. Broader benchmarks. We extend our AdaLLaVA-L framework on broader benchmarks reported in [36], namely GQA [21], SEED-Bench [29], MM-Vet [62], LLaVaWILD [35], and VizWiz [17] (see Tab. 4). The model shows comparable performance and adaptive ability under different latency budget. The results demonstrate the strong generalization of AdaLLaVA to wide range of benchmarks. Model GQA SEED-Bench MM-Vet LLaVa-WILD VizWiz LLaVA-1.5-7B 62.0 AdaLLaVA-L-7B-100% 61.5 61.3 AdaLLaVA-L-7B-85% 58.7 AdaLLaVA-L-7B-60% 58.6 60.5 60.2 59. 31.1 30.7 30.0 23.9 65.4 64.2 62.1 46.3 50.0 54.3 51.5 44.8 Table 4. Results on broader benchmarks. AdaLLaVA-L maintains comparable performance under full computational budgets. With reduced compute budfocusing on operations within the last 16 layers. While both 4-head and 8-head (used in Sec. 4) configurations show comparable performance-latency tradeoffs, the 4-head version enables more granular latency control. Figure 7. Ablation studies on switch design choices. Design of the switches vs H. We also explore the performance of design of tunable switches, particularly AdaLLaVA-L versus AdaLLaVA-H. Both methods allow adaptivity to latency requirements without significant modification to the pretrained LLM, while AdaLLaVA-H offers better flexibility to latency input. As shown in Fig. 7, from FLOPs ranging from 5T to 8T, AdaLLaVA-H-8-head shows slightly better performance overall, reaching approximately 76% on VQA v2 Accuracy compared to AdaLLaVA-L-16-switch which peaks around 75%. Moreover, AdaLLaVA-H demonstrates finer-grained control over the accuracy-latency trade-off. This is evident from the smoother curve of AdaLLaVa-H, which can be attributed to its head/neuron-level switches providing more granular control over computational resources compared to the layer-level switches. This flexibility allows AdaLLaVaH to accommodate wider range of latency budgets. D. Additional Results on Latency and Content"
        },
        {
            "title": "Adaptivity",
            "content": "We provide further results to demonstrate AdaLLaVAs latency and content adaptivity. Model Response under different latency. Here we show additional results on model response given same image-text input under different latency budget, similar to Fig. 1. As shown in Tab. 6, given an image-query pair and latency constraint, AdaLLaVA learns to generate appropriate responses while adapting to varying computational budgets. Visualization for latency token attention. We provide additional results on content awareness by showing the keyquery attention scores of the latency token and the input visual tokens with different text questions, similar to Fig. 4. Fig. 8 further demonstrate the models content-aware behavior. For in the father-child scene, attention spreads across the entire street view for scene description but concentrates on the middle when asking about their activity. For the Happy Plaza image, attention focuses on the storefront sign when asking about the location name, but shifts Figure 6. Relative performance of applying AdaLLaVA-L to Mipha-3B under various latency budget. The center of the radar corresponds to 60% performance of the base Mipha-3B. gets, AdaLLaVA-L shows minimal performance degradation: an average accuracy drop of only 0.7% at 85% budget. Notably, AdaLLaVA-L shows 1.5% average performance boost at full compute budget. Generalization across MLLMs. We demonstrate that AdaLLaVA can generalize to other MLLMs beyond LLaVA. We consider Mipha-3B [67], lightweight MLLM built on Phi-2.7B [23]. Specifically, we apply AdaLLaVA-L on Mipha-3B, following its training strategy [67], and report the results on comprehensive MLLM benchmark (MME), shown in Fig. 6. We see that AdaLLaVA-L maintains comparable performance under full computational budgets. With reduced compute budgets, AdaLLaVA-L shows minimal performance degradation: an average accuracy drop of only 3.4% at 85% budget and 6.1% at 60% budget. These results have similar trend to those with LLaVA-1.5 in Fig. 1. C. Additional Ablation Studies We now conduct ablation study, exploring different design choices. We explore the performance of different designs of tunable switches, namely AdaLLaVA-L and AdaLLaVA-H (detailed in Sec. 3.5). All results are reported with LLaVA 1.5-7B Model on VQAv2 dataset benchmark. Number & granularity of switches. We here conduct ablation studies to examine how the number and granularity of switches affect performance. Fig. 7 (Left) compares switches for the last 16 layers (used in Sec. 4) versus 24 layers in AdaLLaVA-L. While 24 switches enable finer FLOPs control, they significantly reduce model performance. The 16-switch configuration provides better accuracy while maintaining efficient adaptability. Fig. 7 (Right) evaluates attention sampling group sizes in AdaLLaVA-H, 2 Method LLM Vicuna-13B BLIP-2 [32] Vicuna-7B InstructBLIP [10] Vicuna-13B InstructBLIP [10] Vicuna-13B Shikra [7] LLaMA-7B IDEFICS-9B [28] LLaMA-65B IDEFICS-80B [28] Qwen-7B Qwen-VL [3] Qwen-7B Qwen-VL-Chat [3] Vicuna-7B LLaVA-1.5 [36] w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B Vicuna-13B LLaVA-1.5 w/ AdaLLaVA-L Vicuna-13B w/ AdaLLaVA-L Vicuna-13B w/ AdaLLaVA-L Vicuna-13B w/ AdaLLaVA-H Vicuna-13B w/ AdaLLaVA-H Vicuna-13B w/ AdaLLaVA-H Vicuna-13B Vicuna-7B Prumerge [49] w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B Vicuna-7B Prumerge+ [49] w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B FastV (K=2,R=0.5) [8] Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-L Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B w/ AdaLLaVA-H Vicuna-7B Budget (%) FLOPs (T) 100 100 100 100 100 100 100 100 100 100 85 60 100 85 60 100 100 85 60 100 85 60 100 100 85 60 100 85 60 100 100 85 60 100 85 60 100 100 85 60 100 85 60 - - - - - - - - 8.6 8.6 7.2 5.1 8.6 7.2 5.1 16.7 16.7 14.2 10.0 16.7 14.2 10.0 1.4 1.4 1.2 0.8 1.4 1.2 0.8 3.0 3.0 2.6 1.8 3.0 2.6 1.8 4.9 4.9 4.2 3.0 4.9 4.2 3.0 Prefill time VQAv (ms) - - - - - - - - 81 81 69 49 81 69 49 157 157 133 94 157 133 94 16 16 14 10 16 14 10 29 29 24 17 29 24 17 47 47 40 29 47 40 29 [14] 41.0 - - 77.4 50.9 60.0 78.8 78.2 78.5 78.4 77.1 75.0 77.9 76.8 74.2 80.0 79.7 79.1 77.4 80.0 78.9 76.4 72.0 71.0 69.7 67.8 70.4 69.2 66.8 76.8 76.3 75.3 73.0 76.0 75.0 72.2 77.7 77.8 76.9 74.5 77.4 76.6 73.9 SQAI VQAT [51] [39] 61 60.5 63.1 - - - 67.1 68.2 66.8 67.8 67.4 66.9 68.5 68.2 68.1 71.6 72.4 72.4 71.8 72.6 72.3 71.3 68.5 69.1 68.6 68.7 67.9 67.2 68.1 68.3 68.3 68.5 67.7 67.9 68.1 67.6 68.7 67.7 67.8 67.0 68.4 67.7 68. 42.5 50.1 50.7 - 25.9 30.9 63.8 61.5 58.2 57.0 54.5 47.7 57.1 55.2 48.7 61.3 59.9 58.0 54.3 59.9 59.0 53.3 56.0 54.1 52.5 44.7 54.4 52.3 45.9 57.1 55.8 52.9 47.4 56.0 54.2 47.2 58.1 57.0 54.4 47.2 57.0 54.8 48.7 POPE MME MMBench [33] [13] [38] 85.3 - 78.9 - - - - - 85.9 85.9 86.4 86.1 86.9 86.7 85.0 85.9 86.9 86.2 87.3 87.3 86.1 85.0 76.3 74.2 75.6 75.8 77.2 75.5 76.4 84.0 85.1 85.7 85.6 86.6 86.4 86.4 82.5 82.8 83.3 83.8 84.3 83.9 82.4 1293.8 - 1212.8 - - - - 1487.5 1510.7 1521.0 1487.2 1463.8 1471.1 1494.9 1489.6 1531.3 1559.3 1563.9 1552.6 1531.9 1554.5 1529.5 1350.3 1312.6 1313.3 1332.5 1311.4 1309.7 1289.3 1462.4 1455.5 1429.5 1450.9 1503.2 1511.8 1458.0 1516.2 1494.3 1478.1 1463.1 1484.2 1520.5 1452. - 36 - 58.8 48.2 54.5 38.2 60.6 64.3 63.7 63.7 63.8 64.1 64.3 64.8 67.7 69.2 68.9 68.6 67.4 67.0 66.9 60.9 58.4 59.1 57.0 60.1 60.7 58.7 64.9 61.9 62.5 61.3 63.2 63.6 63.6 64.3 63.5 63.7 63.2 63.8 63.9 65.3 Table 5. Results of MLLMs on six benchmarks. Our AdaLLaVA can be applied to LLaVA 1.5 with different size of LLM with different design of switches. Percentage (%): The input latency requirement. AdaLLaVA-L: switches on selecting different transformer blocks. AdaLLaVA-H: switches on select different attention heads and MLP activations. VQAv2: VQAv2 set. SQAI: ScienceQA set. VQAT: TextVQA set. Prumerge: LLaVA 1.5 with PruMerge. evolution of the latency token from layers 12 to 16 using the same example in Fig. 4 of the main paper (see figure below). As seen in Fig. 9, the latency token progressively gathers key information from the input visual tokens for scheduling. to the promotional signage area when querying about special offers. Similarly, in the restaurant scene, attention distributes across the interior elements (tables, counter, chairs) when identifying the location type, but concentrates specifically on the womans clothing when asked about her attire. This consistently shows how the model adjusts its attention based on query requirements. Visualization for latency token across layers. We plot the Visual input example: User Describe this image. Describe this image. Describe this image. The image features wooden block with set of wooden letters arranged in way that spells out the word press stop. ... The image features woman walking down street, holding bouquet of flowers in her hand... LLaVA 1.5-7B AdaLLaVA-L-50% AdaLLaVA-L-75% The image features an older man sitting at table, surrounded by various items. He is using cell phone, possibly checking messages or browsing the internet... The image depicts man sitting at table with large number of papers and books on it. He is using cell phone, possibly checking or communicating with someone. The man is wearing tie, which adds to the professional setting. The table is surrounded by several chairs, with one close to the man and others further away. clock can be seen on the left side of the table, and handbag is placed on the right side. The scene appears to be workspace or study area, with the man working on his tasks and using the cell phone. The image features man sitting at table, surrounded by various items. He is wearing suit and tie, and appears to be using cell phone. The table is set with clock, cup, and book. There are also two chairs in the scene, one near the man and another further away. In the background, there are two more books placed on the ground, and handbag is located near the edge of the scene. The man seems to be engaged in task or conversation, as evidenced by his use of the cell phone. The image features wooden board with series of wooden chicks on it. The wooden chicks are placed in row, with each one occupying different position on the board. The arrangement of the chicks is close-up of the word press and the word stap. The image features wooden word puzzle made up of four wooden letters, each with number on them. The letters are arranged in way that reads Press Stop. The letters are placed on white background, creating visually appealing and playful display. The arrangement of the letters and numbers adds unique and creative touch to the word puzzle. AdaLLaVA-L-100% The image features an older man sitting at table, working on laptop. He is surrounded by various items, including cup, cell phone, and book. The table is adorned with red and black tablecloth, and there are several signs and posters on the wall behind him. In the room, there are two chairs, one near the man and another further away. clock is also visible on the wall, indicating the time. The scene appears to be workspace or study area, with the man focused on his tasks. The image features wooden tray with set of wooden letters arranged on it. The letters spell out the word Press Stop, with each letter positioned next to the other. The arrangement of the letters creates visually appealing and organized display. The wooden tray itself is placed on white surface, which further emphasizes the contrast between the wooden letters and the background. The image captures young woman standing in busy street, holding bouquet of flowers. She appears to be smiling, possibly indicating that she is enjoying the moment. The street is filled with various vehicles, including several motorcycles and motorcycle parked close to the woman. There are numerous people in the scene, some standing closer to the woman, while others are further away. Some of the people are closer to the left side of the image, while others are further back, creating sense of busy city street. The image depicts woman walking down street, holding bouquet of She is wearflowers in her hand. ing dress and appears to be enjoying her time outdoors. The street is lined with various vehicles, including motorcycles and truck, parked along the side. There are several people in the scene, some of whom are walking or standing near the vehicles. handbag can be seen resting on the ground, possibly belonging to one of the individuals. The overall atmosphere of the scene is lively and bustling, with people going about their daily activities. The image depicts woman walking down street, holding bouquet of flowers in her hand. She is wearing flowered dress and appears to be enjoying her time. The street is lined with various vehicles, including several motorcycles parked on both sides of the road. There are also few people walking around, some of whom are carrying handbags. The scene captures lively atmosphere with people going about their daily activities. Table 6. AdaLLaVA-L on LLaVA 1.5-7b model, generating appropriate responses while adapting to varying computational budgets. 4 Figure 9. Evolution of latency token across layers in AdaLLaVAL on 7b model. Figure 8. The key-query attention scores between latency token and visual tokens. The latency input is 1.0 in these examples."
        }
    ],
    "affiliations": [
        "Purdue University",
        "The University of Hong Kong",
        "University of Wisconsin-Madison"
    ]
}