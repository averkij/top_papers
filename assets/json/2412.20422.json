{
    "paper_title": "Bringing Objects to Life: 4D generation from 3D objects",
    "authors": [
        "Ohad Rahamim",
        "Ori Malca",
        "Dvir Samuel",
        "Gal Chechik"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in generative modeling now enable the creation of 4D content (moving 3D objects) controlled with text prompts. 4D generation has large potential in applications like virtual worlds, media, and gaming, but existing methods provide limited control over the appearance and geometry of generated content. In this work, we introduce a method for animating user-provided 3D objects by conditioning on textual prompts to guide 4D generation, enabling custom animations while maintaining the identity of the original object. We first convert a 3D mesh into a ``static\" 4D Neural Radiance Field (NeRF) that preserves the visual attributes of the input object. Then, we animate the object using an Image-to-Video diffusion model driven by text. To improve motion realism, we introduce an incremental viewpoint selection protocol for sampling perspectives to promote lifelike movement and a masked Score Distillation Sampling (SDS) loss, which leverages attention maps to focus optimization on relevant regions. We evaluate our model in terms of temporal coherence, prompt adherence, and visual fidelity and find that our method outperforms baselines that are based on other approaches, achieving up to threefold improvements in identity preservation measured using LPIPS scores, and effectively balancing visual quality with dynamic content."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 2 2 4 0 2 . 2 1 4 2 : r Bringing Objects to Life: 4D generation from 3D objects Ohad Rahamim, Ori Malca,1 Dvir Samuel1 Gal Chechik1,2 1Bar-Ilan University 2NVIDIA Figure 1. (Click to view Video online) Our method, 3to4D, takes static 3D object and textual prompt describing desired action. It then adds dynamics to the object based on the prompt to create 4D animation, essentially video viewable from any perspective. On the right, we display four 3D frames from the generated 4D animation. Each 3D frame is split into an RGB image and corresponding depth map on its top right."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in generative modeling now enable the creation of 4D content (moving 3D objects) controlled with text prompts. 4D generation has large potential in applications like virtual worlds, media, and gaming, but existing methods provide limited control over the appearance and geometry of generated content. In this work, we introduce method for animating user-provided 3D objects by conditioning on textual prompts to guide 4D generation, enabling custom animations while maintaining the identity of the original object. We first convert 3D mesh into static 4D Neural Radiance Field (NeRF) that preserves the visual attributes of the input object. Then, we animate *Equal Contribution 1 the object using an Image-to-Video diffusion model driven by text. To improve motion realism, we introduce an incremental viewpoint selection protocol for sampling perspectives to promote lifelike movement and masked Score Distillation Sampling (SDS) loss, which leverages attention maps to focus optimization on relevant regions. We evaluate our model in terms of temporal coherence, prompt adherence, and visual fidelity and find that our method outperforms baselines that are based on other approaches, achieving up to threefold improvements in identity preservation measured using LPIPS scores, and effectively balancing visual quality with dynamic content. Project page 1. Introduction Generative models are progressing rapidly, making it possible to generate images, videos, 3D objects, and scenes from text instructions only. It is now becoming possible to generate 4D content: dynamic 3D content conditioned on text prompts using text-to-4D methods [1, 13, 20]. The 4D generation has the potential to change content creation, from movies and games to simulating virtual worlds. Despite this promise, text-to-4D methods provide very limited control over the appearance of generated 4D content. Instead of generating 4D dynamic object using text control only, one may want to animate an existing 3D object, like your favorite 3D toy or character. Conditioning 4D generation on 3D assets offers several advantages: it enhances control, leverages existing 3D resources efficiently, and accelerates 4D generation by using 3D as strong initialization. Despite the availability of extensive, high-quality 3D models [3], current methods have not yet used 3D assets to guide 4D generation. Current approaches for text-to-4D generation [1, 13, 20] typically follow two-stage process. First, they generate static 3D object using text-to-3D techniques. Then, they add dynamics guided by textual prompts. At first, it may seem straightforward to follow the same two-stage process to animate an input 3D object. In practice, however, applying the second stage of these methods to introduce dynamics to the static 3D, tends to significantly modify the object itself, destroying its identity. These methods are not well fit for animating given 3D object while preserving its identity. In this paper, we introduce novel method for generating 4D scenes from user-provided 3D representations, taking simple approach that incorporates textual descriptions to govern the animation of the 3D objects. First, we train static 4D Neural Radiance Field (NeRF) based on the 3D mesh input, effectively capturing the object structure and appearance from multiple views, replicated across time. Then, our method modifies the 4D object using an imageto-video diffusion model [7, 23, 27], conditioning the first frame on renderings of the input object. This maintains the identity of the original object and adds motion based on provided text prompt. Unfortunately, we find that applying this approach naively is insufficient, because it reduces dramatically the level of dynamic motion in the video. To encourage the model to generate more dynamic movements, we propose two key improvements. First, new camera viewpoint selector that incrementally samples different viewpoints around the object during optimization. This gradualwidening sampling approach enhances the generation process, resulting in more pronounced movement in the animated 4D outcome. Second, we introduce masked variant of the SDS loss, using attention maps obtained from the Image-to-Video model. This masked SDS focuses on optimizing the object-relevant foreground areas of the latent space pixels, enhancing the optimization of elements related to the object. We name our approach simply 3to4D. We evaluate 3to4D with comprehensive set of metrics designed to assess various aspects of the generated 4D scenes across multiple viewpoints. We focus on three main criteria: temporal coherence of the generated video, adherence to the prompt description, and visual consistency with the initial 3D object. Since there are no existing approaches to generate 4D from given 3D object, we built three different baseline approaches based on existing approaches. Our results show that our method significantly outperforms an approach based on traditional text-to-video methods, achieving 3 times better LPIPS score compared to existing methods, indicating superior visual fidelity to the provided 3D object. We also find that our proposed incremental viewpoint selection protocol and the attentionmasked SDS enhance the dynamic content of the generated videos while still maintaining high degree of consistency with the original objects appearance. These improvements demonstrate that our method not only generates more realistic 4D scenes but also effectively balances visual quality and dynamic richness. This paper makes the following contributions: 1. We introduce the 3D-to-4D generation problem, adding temporal dimension and dynamic motion to static 3D model. 2. We propose novel architecture for generating 4D scenes conditioned on given 3D model and text prompt. 3. We present the first integration of Image-to-Video diffusion models with the Score Distillation Sampling (SDS) process. 4. We introduce two enhancements to improve motion gen- (a) new viewpoint sameration and optimization: pling strategy that incrementally captures perspectives around the object, creating more dynamic movement in the 4D scene. (b) masked SDS loss that uses the crossattention mechanism of the diffusion model to enhance the optimization of 4D content. 5. Improved 4D quality than baselines on an extensive set of metrics. 2. Related Work Personalization in Image and 3D Generation. In recent years, there have been significant advances in subject-driven image generation, enabling users to personalize generated images for specific subjects and concepts. The first works focused on personalized image generation and fine-tuned the diffusion model [19] or new token [5]. However, personalized 3D generation has remained challenging, as these generation models struggle to recognize that an object can 2 Figure 2. Workflow of our 3to4D approach, designed to optimize 4D radiance field using neural representation that captures both static and dynamic elements. First, 4D NeRF is trained to represent the static object (plant, left), having the same input structure at each time step. Then, we introduce dynamics to the 4D NeRF by distilling the prior from pre-trained image-to-video model. At each SDS step, we select viewpoint and render both the input object and the 4D NeRF from the same selected viewpoint. These renders, along with the textual prompts, are then fed into the image-to-video model, and the SDS loss is calculated to guide the generation of motion while preserving the objects identity. The attention-masked SDS, focuses learning on the relevant parts of the object, improving identity preservation. be the same, even when it appears different from various viewpoints. Recent work [17] addressed this challenge by initially partially fine-tuning DreamBooth and then using it to generate additional views, which were subsequently used for full fine-tuning. 4D Generation. With the rise of 3D generation, 4D generation has also emerged. Similar to how 3D generation approaches elevated image diffusion models for 3D generation, recent methods in 4D generation involve uplifting video diffusion models to optimize NeRF in an SDS (Score Distillation Sampling) supervision fashion for creating dynamic 3D scenes. Particularly by using spatio-temporal priors from text-to-video diffusion models. MAV3D [20] achieves text-to-4D generation by distilling text-to-video diffusion models. 4D-fy [1] combines text-toimage and text-to-video diffusion models to produce coherent 4D scenes. Animate124 [28] transforms single in-thewild image into 3D videos based on textual motion descriptions, Consistent4D [9] generates 4D scenes from statically captured videos. Image to video generation. Image-to-video models [2] condition on an image alone, unlike text-to-video models, which condition on textual prompt. The image-to-video approach allows users to create motion based on their specific description, while text-to-video models generate motion inferred from the conditioned image, limiting the users flexibility to control dynamics explicitly through prompt. Two works that combine both image and prompt conditioning are I2VGen [27], which is capable of generating high-resolution videos, and DynamiCrafter [23], family of models designed to handle various input image resolutions. Both these models project the given image into text-aligned representation space using pre-trained CLIP image encoder, the same way text is conditioned. The image features are then integrated into the model through crossattention layers in the U-Net architecture, similar to how text-conditioned features are processed. This approach effectively combines information from both the visual input and the textual prompt. SDS loss and masked SDS. Score Distillation Sampling (SDS) [14] is widely adopted method that uses image diffusion models to guide optimization problems via text prompts. It is primarily applied in 3D generation. Recent work shows that SDS can be optimized using maps that emphasize different features. For instance, [24] incorporates semantic map to improve the generation of multiple objects with diverse attributes. SV3D [22] applies mask to unseen areas, focusing learning on those regions. Additionally, [11] manipulates images by multiplying the loss in areas of interest using mask. Unlike our approach, which attends to the objects pixels in every frame, none of these works effectively handle 4D content. 3 3. Method Our method receives an input 3D model - like model of your favorite plant, and textual prompt - like plant blooming. Our goal is to animate the object, generating 4D scene that reflects the described action in the prompt, yielding 4D object of your favorite flower blooming. To bridge the gap between static 3D content and animated 4D scenes, our work enables objects to exhibit dynamic behaviors and simulate how they might move. This approach transforms static assets into animated objects, adding life to 3D objects by introducing motion that aligns with the users descriptions. Our approach is illustrated in Figure 2. 3.1. Initialize static 4D from 3D object We first optimize static 4D representation, where at every time t, the 4D NeRF captures the same static form of the input object. More specifically, beginning with the input 3D mesh, we randomly select camera position. ray is cast from the camera center through both the mesh and the neural representation. Along this ray, 3D points are sampled, and three properties are computed: color (RGB), depth, and surface normals from both representations. We then optimize the neural representation to align with the properties of the input object. This is achieved using the loss function: Lstatic = LM AE(RGBmesh, RGBN eRF ) + LM AE(Depthmesh, DepthN eRF ) + LM AE(N ormalmesh, ormalN eRF ). (1) Here, LM AE denotes the mean absolute error between the RGB, depth, and normals of the mesh renders and the neural representation renders. During this process, we randomly sample the time dimension, creating static 4D scene where the object stays unchanged over time. 3.2. Adding dynamics Next, we aim to bring the object to life by introducing motion to the static 3D input object. To do this, we need to condition the SDS process on the input 3D object to achieve the desired 4D output. Here, we propose to distill information from image-to-video diffusion models, to achieve the condition of the SDS. By aligning the 3D models renders from the same viewpoint as the NeRF and using them as input to the generation model, we effectively anchor the generated motion to the objects identity. Therefore, by rendering the object from all viewpoints, we can maintain the input objects identity in the 3D space while introducing motion. This approach ensures that the object remains consistent throughout the animation, preserving its characteristics across different perspectives. Optimizing the dynamic of the 4D scene using an imageto-video model can then be done using SDS [14] loss: (cid:21) (cid:20) ω(td) (cid:0)ϵϕ θLI2V = Etd,ϵ (cid:0)ztd ; td, y, Xobj(cid:1) ϵ(cid:1) Xθ θ (2) Here, ϵϕ and ϵ denote the predicted and actual noise for each video frame, respectively. We denote Xθ as collection of video frames, where Xθ = (cid:2)x0 (cid:3), which are rendered from the representation. Additionally, the rendered object is denoted as obj. θ, . . . , xV 1 θ This SDS loss (Eq. 2) is then added to the static loss (Eq. 1), which is applied to the frame. This combined approach generates dynamic motion while ensuring that the 3D object remains consistent at = 0. The overall loss is: = LI2V + λLstatic(x0 θ) (3) Here, λ is weighting hyperparameter used to balance the magnitude of Lstatic with that of LI2V , and x0 θ is the render at time = 0. Viewpoint sampling. When computing distillation scores from text-to-image and text-to-video models, it is common to sample camera positions uniformly at random [14]. However, randomly sampling viewpoints usually leads to inconsistent generations with varying appearances and motions, disrupting optimization and thus hurting motion generation (see ablation Sec. 5, Table 2 and Figure 7 for more details). To address this, we propose new viewpoint sampling protocol, inspired by [10]. This approach begins with limited set of closely spaced viewpoints, gradually expanding the azimuth range as optimization progresses. The rationale is that optimizing the 4D scene across all viewpoints simultaneously is challenging; by starting within smaller subspace, later optimization is provided with better initialization. We gradually increase the viewpoint coverage as the model gains stability from prior optimizations. Specifically, we begin the SDS optimization by sampling viewpoints within small range [A0, A0]. After iterations, we expand the range to [A1, A1], continuing to widen the range as optimization proceeds incrementally. By the end of the process, we extend the range to the full span of [180, 180]. Here, A0, A1, and are hyperparameters; refer to the implementation details for further information and to the ablation (Sec 5) for qualitative and quantitative results. We find that this protocol ensures greater consistency between viewpoints, resulting in improved optimization and more dynamic motion generation. 3.3. Attention-masked SDS The standard implementation of SDS loss computes the loss over the full latent representation. In 4D generation, this 4 includes areas that correspond to the (non-existing) background. This may cause severe interference when the generated image or video contains content in the background, because it may cause the 4D scene to include irrelevant content in the background. This problem does not appear to be severe when generating 3D objects by distilling from text-to-image models, perhaps because their training set also contains sufficient samples with white backgrounds. For video data, text-tovideo models often generate irrelevant backgrounds. To address this issue, we propose to find the spatial extent where the object is represented in the latent space, and then mask the loss to only cover the object and not the background. Where the mask can be extracted from the spatial cross-attention with the object token in the provided textual prompt. For example, in Fig.1 we masked the SDS loss with the spatial attention corresponding to the words plant, hulk, and elephant, respectively. The masked SDS loss is the pointwise product: LmaskedSDS = LI2V (4) Where is the attention mask. 3.4. Modeling time The current video generative models typically work with sequences of 16 frames. Here, we aim for our 4D representation to be flexible enough to generate videos at any time resolution, ensuring smooth and continuous dynamics without being constrained by fixed frame lengths. To ensure time smoothness, [1, 20] suggests sampling video from the NeRF at different times. At each optimization step, randomly select start time t0 = U[0, 1] and an end time, tV = U[t0, 1] then uniformly sample frames within this range. This method enables continuous sampling across the entire time range. However, since image-to-video extrapolates the condition image, randomly changing the starting time forces the static object to appear across range of times, resulting in smaller movements and limiting the dynamic range. However, this approach is not ideal for our method, as our optimization requires fixed time to represent the static input object. While image-to-video models extend condition image into video, 3to4D extends the input object into 4D representation. For this purpose, we fix the input object at time = 0. In 3to4D, we propose new time sampling strategy to fit the use of Image-to-Video models better. Given the time range [0, 1], we evenly select frame times within this range ti = i/V . The first frame is always fixed at t0 = 0, while the later frame times are adjusted with small noise, resulting in frame times of ti = i/V + ϵi. Our time sampling strategy ensures uniform sampling across the entire time range while maintaining the input object condition requirements. 4. Experiments 4.1. Dataset For collection of 3D objects, we used the Google Scanned Objects (GSO) [4] dataset. This is collection of highquality 3D scans of everyday items. We selected 20 objects from the GSO dataset, focusing on those that could support interesting dynamics and motion. We then queried ChatGPT for dynamic prompts and selected several prompts for each object, resulting in total of 62 prompts. 4.2. Generation quality metrics To evaluate our approach, we assess three main qualities: (1) the natural appearance of the generated 4D content, (2) alignment with the text prompt, and (3) preservation of the input objects identity. We use evaluation metrics for video generation described in Vbench [8] and [28]. (1) Video quality: The following metrics were measured: Motion Smoothness. We evaluate the smoothness of motion in the generated video to check whether the video is accurate to the physical laws of the real world. To evaluate this, we assess three metrics. First, we leverage the motion priors from the video frame interpolation model [12] (smoothness) as suggested in Vbench [8], as well as assessing the cosine similarity and LPIPS scores between every two consecutive frames (CLIP-F and LPIPS-F) [15, 26]. Dynamic Degree - Since static objects inherently exhibit smoothness, we introduce an additional metric to assess the dynamic content in the generated video. To quantify the amount of movement, we compute the optical flow between frames using [21] as suggested in Vbench [8]. (2) Agreement with prompt. To measure consistency with the prompt, we followed Vbench [8] and measured the similarity between the video frame features and the textual description features with ViCLIP [16] (style). Also, following [28] we measure the image-text cosine similarity between the textual CLIP [15] features of the prompt and the visual CLIP features of every generated frame (CLIP-T). (3) Agreement with input object. Ensuring visual consistency between the input 3D object and the generated 4D data. To evaluate this, we used LPIPS [26] to measure the perceptual similarity between the input object renders and the generated frames, assessing the consistency of visual appearance over time. Moreover, following [28] we use CLIP [15] to assess the similarity between the input 3D object and the generated frames, by calculating the cosine similarity 5 between the reference image visual features and the visual features of each frame in the rendered video (CLIP-I). We compute all metrics across four distinct viewpoints of the generated 4D data. Specifically, azimuth angles are set to {0, 90, 180, 270} degrees, and elevation is set to 0. We then averaged the scores across the viewpoints to obtain single score for each metric per object. We then averaged the scores, across all objects and also calculated the standard error of the mean to measure the variability among them. 4.3. Compared methods Since there are no existing methods that animate given 3D object, we adapted existing methods for this task. (1) 4D-fy, text-to-4D method [1]. We initialized the 4D NeRF with the input static object, added the static loss to the SDS loss (so the input object will be at time = 0), and replaced their random time sampling with our time sampling protocol. (2) Animate124, an image-to-4D method [28]. We provided frontal-view render of the input object as the input image, and then applied their pipeline. (3) STAG4D, video-to-4D method [25]. STAG4D generates 4D content based on video. We rendered the input object and generated video based on the render using DynamiCrafter. This video was then fed into STAG4D [25], 4.4. Implementation details We implement Image-to-Video SDS using the ThreeStudio framework [6]. Our implementation builds upon the textto-4D [1] capabilities within ThreeStudio. Networks and rendering: We used hash encodingbased neural representation, following the implementation in [1]. For the image-to-video model, we used DynamiCrafter [23], which generates videos at resolution of 256x256. The input 3D object was rendered using PyTorch3D [18], matching DynamiCrafter resolution with rendering size of 256x256. The number of frames is = 16. Running Time: Our NeRF representation conversion was performed over 2000 iterations with uniform viewpoint sampling, taking approximately 10 minutes on an NVIDIA A100 GPU. The second phase was run for 10,000 steps and took 240 minutes, and the third phase ran for 3,000 steps and required 80 minutes. Hyper parameters tuning: We found that incremental steps of 20, starting with A0 = 20 and increasing as Ai = Ai1 + 20, work best for all objects. In all experiments, we set λ = 104 (Eq.3). 5. Results We first provide qualitative examples of 3to4D, then quantitative and qualitative comparison of 3to4D with the baseline methods. Finally, quantitative and qualitative results of an ablation study, and the effect of different prompts. Qualitative results: Figure 3 shows four examples of 4D generations (right) from 3D object (left). See more video examples online) Quantitative comparison with baselines: Table (1) compares 3to4D with the three baselines. 3to4D achieves far better agreement with the input object (identity preservation) in both LPIPS and CLIP-I. It also generates slightly more smooth and natural-looking 4D than other baselines. Agreement with the prompt is lower, presumably because the content adheres to the input object, which may deviate from the canonical representation of the corresponding text term. In other words, the text prompt may push an object to have other appearance than the given input object. To illustrate the most important metrics, Fig.4 presents scatter plot comparing the dynamic degree and temporal style of each object against the LPIPS (agreement with the input object) score for all baselines. 3to4D achieves better object preservation across all objects, outperforming the other methods in maintaining the object identity while incorporating motion. Qualitative comparisons: Figure 5 shows two example objects, hand-bell, and Mario, and how they are transformed by various baselines. Importantly, the Mario figure on the bottom row is specific object that (we assume) the text-to-video model has observed during training. For both the hand-bell and Mario, STAG4D, 4D-fy, and animate124 significantly change the object appearance, leading to higher LPIPS score. Interestingly, 4Dfy and Animate124, succeed in preserving the identity of the Mario figure, but they generate more prototypical Mario, which is different from the input Mario. This yields better alignment score with the text prompt in Table.1. Ablation analysis: We conducted an ablation study to evaluate the contributions of each component of 3to4D. Table.2 provides the quantitative results. Without the Imageto-Video-based SDS, consistency with the input object declines significantly, leading to much higher LPIPS score. This suggests that the generated objects differ substantially from the input objects. Using random viewpoint instead of our incremental viewpoint selector results in reduced dynamics for the input object, yielding noticeably lower dynamic degree of 39 compared to 64.7. Additionally, the attention-masked SDS enhances the models ability to preserve consistency with the input object. Altogether, 3to4D achieves balanced trade-off between preserving the input objects identity, fulfilling the prompt, and maintaining high dynamic degree. Figure 3. (click-to-view-online) 3to4D brings various objects to life. On the left, we display the input object along with textual prompt describing the desired action. On the right, we present four frames from the generated object, viewed from the front. Each 3D frame is split into an RGB image and its corresponding depth map, shown in the top right corner. Agreement with input object Agreement with prompt Video quality LPIPS CLIP-I Style CLIP-T Smoothness LPIPS-F CLIP-F Dynamic Deg. 4D-fy* [1] Animate124* [28] STAG3D* [25] 44.3 0.2 34.8 0.2 101.2 0. 77.6 .04 80.2 0.2 69.5 0.3 21.5 .07 22.5 .09 21.7 .08 29.1 .04 29.6 .05 10.1 0.1 98.9 .001 98.9 .009 94.6 .005 2.7 .02 2.3 .03 4.4 .04 97.9 .01 98.1 .01 83.8 . 55.4 0.5 82.7 1.3 50.1 0.2 3to4D ( Ours) 15.0 0.1 90.2 .08 20.5 .01 27.4 . 99.2 .007 2.1 .02 98.3 .01 50.0 0.3 Table 1. Comparison between 3to4D and baseline approaches. The set of objects and prompts is described in Sec. 4.1. Metrics are explained in Sec. 4.2. Our 3to4D excels in preserving object identity and also improves video quality. Agreement with the prompt is lower because the given object slightly differs from the canonical meaning of the text word. * denotes that the methods are not identical to their corresponding papers because the methods are not designed for the 3D-to-4D task. Figure 7 illustrates the effect of ablation using two qualitative examples. Sensitivity to prompt. We explore the effect of different prompts, describing different dynamics, on the generated 4D scene. Figure. 6 shows the results when using the 3D ob7 ject Mario and supplying it with three different dynamic prompts: jumping (top row), running (middle row), and waving (bottom row). The Mario figure, moves differently, according to the specified actions in the description Agreement with input object Agreement with prompt Video quality LPIPS CLIP-I Style CLIP-T Smoothness LPIPS-F CLIP-F Dynamic Deg. 3to4D (Ours) w/o image-to-video w/o viewpoint selector w/o masked-SDS 15.0 0.1 40.3 0.2 15.0 0.2 15.8 0.1 90.2 .08 76.6 0.1 89.6 0.1 89.7 0.1 20.5 .01 21.1 .07 18.7 0.1 18.9 0.1 27.4 .05 28.5 .04 27.7 .09 27.5 . 99.2 .007 98.5 .007 99.2 .008 99.1 .007 2.1 .02 2.7 .02 2.1 .04 2.1 .02 98.3 .01 97.5 .01 98.3 .02 98.2 .01 50.0 2.2 64.7 2.0 43.8 0.6 50.8 0.6 Table 2. Ablation study. Evaluating the contribution of various components of our method on objects and prompts in Sec. 4.1. Figure 5. Qualitative comparison with competing methods. rendered image of the input object is shown on the left, alongside rendered images from our and other methods. While our method preserves the identity of the input object, all other baselines generate different objects. . 7. Conclusion We introduce 3to4D, novel approach for animating 3D objects into dynamic 4D scenes based on textual motion descriptions. Our method leverages Image-to-Video diffusion models for 3D-to-4D generation, preserving object consistency by conditioning on rendered images of the input object and applying tailored SDS loss. To enhance motion realism, we propose two key improvements: novel viewpoint sampling strategy for capturing dynamic perspectives and an attention-guided masked SDS loss for focused optimization. Our results demonstrate superior temporal coherence, prompt adherence, and visual fidelity compared to existing methods, establishing 3to4D as an effective solution for controlled 4D content creation. A. Appendix: Camera Viewpoint Sampling During the optimization of the 4D object, the standard protocol suggests sampling the azimuth of camera viewpoints uniformly at random. We hypothesized that this would make it challenging for the model to converge, and experiment with other protocols. We evaluate four viewpoint sampling protocols: In all four protocols, the camera remains consistently oriented toward the target object, ensuring steady focus. Uniform Figure 4. Comparison between our method and baselines, across all objects tested. We consistently achieve better LPIPS scores across all objects. 6. Limitations 3to4D has several limitations. First, it relies on distilling knowledge from video-generation models and inherits its challenges, such as limb confusion and missing object parts. For instance, we observed issues in generating dynamics when animating walking motions, where the method struggled with leg positioning, resulting in poor-quality motion generation. Also, current video generations are limited to small number of frames, and we cannot experiment with longer and more complex motion. 8 Figure 6. (Click to view video online). Different prompts generated different 4D, matching the movement description. The object in question is Mario figure (on the left), and we provide three distinct prompts that describe three different dynamics of the figure. On the right, the generated 4D illustrates the corresponding movements based on these prompts. Figure 7. Qualitative ablation result demonstrates the contribution of each part of 3to4D. Without our viewpoint selector (described in Sec. 3.2) the plant does not bloom. Without our attention-masked SDS, the plant is less rich in detail. intrinsic camera parameters are employed throughout the optimization process, ensuring consistency in rendering and evaluation. (1) Incremental: sampling viewpoints within small range [A0, A0]. After iterations, we expand the range to [A1, A1], widening the range as optimization proceeds incrementally. By the end of the process, we extend the range to the full span of [180, 180], where starting with A0 = 20 and increasing as Ai = Ai1 + 20. (2) Uniform: The standard sampling protocol from [1]. Uniformly samples camera viewpoint from entire azimuth range [180, 180]. (3) Four views: Viewpoints are randomly selected from predefined set of azimuths [0, 90, 180, 270(]. The rationale is that reducing the set of views could help the model converge faster. (4) Sweep: Performing discrete sweep on gradually expanding azimuthal angle interval. For instance, start sweeping from the lower bound of [20, 20] up to the upper bound. Then, smoothly transition to the expanded interval [40, 40], repeating the sweeping process, incrementing the azimuth range by 20 each time until reaching [180, 180]. After completing the entire sweeping process, switch to uniform protocol. Table 3 provides quantitative evaluation of four proIncremental achieves the highest motion score in tocols. the generated 4D, with only slightly decreased style fidelity. There was also slight increase in the LPIPS score between sweep and incremental, which reflects the connection to the given object. This increase is related to added motion, making the generated object appear somewhat different from the original, though the core object remains recognizable. 9 LPIPS CLIP-I Style CLIP-T Smoothness LPIPS-F CLIP-F Dynamic Deg. Uniform Four-views Sweep 14.0 0.6 14.0 0.5 14.3 0.6 90.1 0.5 90.0 0.4 90.0 0.5 19.0 0.6 19.3 0.5 19.6 0. 27.0 0.3 27.0 0.3 27.0 0.3 99.2 0.02 99.2 0.02 99.2 0.02 2.1 0.1 2.1 0.1 2.2 0.1 98.1 0.6 98.2 0.6 98.1 0.6 52.1 1.9 51.3 1.5 53.6 2.3 Incremental (Ours) 15.0 0.7 90.0 0.4 20.0 0.5 27.3 0.2 99.2 0.03 2.6 0. 98.1 0.6 59.4 2.2 Table 3. Comparison of 4 viewpoint sampling methods: Incremental, uniform, deterministic, and sweep. In sweep sampling, consecutive steps are visually similar to each other, as described in Sec.A. Our proposed incremental approach combines visual similarity with randomness, leading to better preservation of 3D perspectives while maintaining dynamic variability."
        },
        {
            "title": "References",
            "content": "[1] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score disIn Proceedings of the IEEE/CVF Contillation sampling. ference on Computer Vision and Pattern Recognition, pages 79968006, 2024. 2, 3, 5, 6, 7, 9 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [3] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 2 [4] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 25532560. IEEE, 2022. 5 [5] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2 [6] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, ZiXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: unified framework for 3d content generation. https://github.com/threestudioproject/ threestudio, 2023. [7] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2 [8] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 5 [9] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao. Consistent4d: Consistent 360 {deg} dynamic obarXiv preprint ject generation from monocular video. arXiv:2311.02848, 2023. 3 [10] Yoni Kasten, Ohad Rahamim, and Gal Chechik. Point cloud completion with pretrained text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 4 [11] Jeongsol Kim, Geon Yeong Park, and Jong Chul Ye. Dreamsampler: Unifying diffusion sampling and score distillation for image manipulation. In European Conference on Computer Vision, pages 398414. Springer, 2025. [12] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, ChunLe Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98019810, 2023. 5 [13] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85768588, 2024. 2 [14] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3, 4 [15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5 [16] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5 [17] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 23492359, 2023. [18] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Accelerating 3d deep learning with pytorch3d. arXiv:2007.08501, 2020. 6 [19] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 2 [20] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea 11 Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation. arXiv preprint arXiv:2301.11280, 2023. 2, 3, 5 [21] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. [22] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2025. 3 [23] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. 2, 3, 6 [24] Ling Yang, Zixiang Zhang, Junlin Han, Bohan Zeng, Runjia Li, Philip Torr, and Wentao Zhang. Semantic score distillation sampling for compositional text-to-3d generation. arXiv preprint arXiv:2410.09009, 2024. 3 [25] Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao. Stag4d: Spatial-temporal anchored generative 4d gaussians. In European Conference on Computer Vision, pages 163179. Springer, 2025. 6, 7 [26] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [27] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and I2vgen-xl: High-quality image-to-video Jingren Zhou. arXiv preprint synthesis via cascaded diffusion models. arXiv:2311.04145, 2023. 2, [28] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124: Animating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603, 2023. 3, 5, 6,"
        }
    ],
    "affiliations": [
        "Bar-Ilan University",
        "NVIDIA"
    ]
}