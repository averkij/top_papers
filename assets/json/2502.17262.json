{
    "paper_title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
    "authors": [
        "Chengyin Xu",
        "Kaiyuan Chen",
        "Xiao Li",
        "Ke Shen",
        "Chenggang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the \"emergence phenomenon\", wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose a Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 2 6 2 7 1 . 2 0 5 2 : r Unveiling Downstream Performance Scaling of LLMs: Clustering-Based Perspective Chengyin Xu, Kaiyuan Chen, Xiao Li, Ke Shen, Chenggang Li Seed-LLM, ByteDance {xuchengyin.98, chenkaiyuan.99, lixiao.20, shenke, lichenggang}@bytedance.com"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the emergence phenomenon, wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have emerged as transformative technologies in natural language understanding, generation, and reasoning [1, 14, 5]. Their impressive success heavily relies on scaling model parameters and pre-training data, with training loss empirically following power-law relationship with compute [18, 21]. However, this reduction in training loss primarily reflects an indomain compression effect and does not necessarily indicate improved out-of-domain generalization or downstream performancethe factor of primary concern in practice. Specifically, performance scaling of downstream tasks aims to predict the accuracy of the target LLM on downstream tasks using metrics from smaller models. Our objective is to develop prediction method that works reliably across diverse range of downstream tasks, minimizing the worst-case prediction error. Despite extensive efforts, reliable scaling law for downstream tasks remains elusive. One line of work attempts to extrapolate large-model performance by modeling the performance-loss relationship [6, 13, 8, 38, 26], but this often fails to capture the emergent behaviors of LLMs and the Equal contribution. Preprint. mismatch between in-domain loss and downstream metrics [42]. Another line of research focuses on direct extrapolation of performance-compute relationship [1, 19], yet the uneven difficulty distribution across different evaluation samples undermines its accuracy. We observe that different evaluation samples actually follow distinct performance scaling patterns, and thus applying single extrapolation formula to the entire evaluation set is suboptimal. We give the detailed analysis in Section 3. To address these challenges, we propose new performance scaling law, derived from the existing loss scaling law [21], specifically applicable to evaluation subsets that exhibit consistent performance scaling patterns. Building on the performance scaling law, we develop Clustering-On-Difficulty (COD) multi-stage framework for predicting downstream performance. Specifically, we first cluster tasks by their difficulty features, and then filter out clusters that lack valid extrapolation patterns. Next, we fit the performance-compute relationships in the remaining clusters under our performance scaling law, extrapolate the performance of large models within these clusters, and finally map the aggregated predictions to the complete task set. We validate our COD approach on eight evaluation sets, including popular MATH [15], BBH [32], and MMLU pro [36]. COD achieves an average prediction error of 1.36% on 70B-parameter LLM. Our results demonstrate that this difficulty-aware framework substantially outperforms existing methods, establishing promising paradigm for accurate downstream performance scaling of LLMs. Our contributions can be summarized as follows: We propose the COD framework to address high variance and emergent phenomena in the LLM performance by effectively modeling the difficulty distribution within the evaluation sets. We introduce performance scaling law for cluster-wise performance prediction, with theoretical support and experimental validation. Extensive experiments conducted across eight diverse evaluation sets demonstrate that COD achieves state-of-the-art average prediction error of 1.36% on 70B-parameter LLM."
        },
        {
            "title": "2.1 Loss Scaling Laws",
            "content": "Loss scaling laws provide systematic framework for understanding the relationship between computational resources, data, model size, and the final performance of LLMs. Early work by Kaplan et al. [21] demonstrates that the pre-training loss of LLMs follows power-law relationship with the compute (the number of floating-point operations) used in training. Subsequent studies extend these findings to other domains, such as computer vision [41], graph learning [24] and vision-language models [2, 16]. Recent research has also explored scaling laws in specific contexts, such as fine-tuning [17, 34], vocabulary size optimization [33], retrieval-augmented models [30], and hyperparameter tuning [23, 40]. These studies highlight the broad applicability of scaling laws and their potential to guide the efficient allocation of computational resources."
        },
        {
            "title": "2.2 Downstream Task Performance Scaling",
            "content": "Predicting downstream task performance remains critical challenge due to emergent abilities in LLMs that manifest only after exceeding task-specific thresholds [37, 28]. Recent works, such as using loss as proxy [6] or increasing metric resolution [19], have demonstrated potential but encounter challenges in aligning surrogate metrics with original task objectives. Here, we briefly review the two main types of methods for predicting downstream performance: 1. Loss-intermediate prediction. These methods predict the final training loss (or in-domain validation loss) of LLMs with loss scaling laws first, and then predict downstream performance through loss-performance relationships [6, 13, 8]. While these methods leverage established scaling laws for loss predictions, they encounter fundamental limitation: the inconsistent mapping between loss and performance metrics. In addition, Xiao et al. [38] employ the evaluation set answer loss as an intermediate variable for estimation. Although answer loss correlates with the final performance metrics, its predictability remains low as predicting answer loss shares the challenges with predicting performance, including emergence phenomenon and high variance in task difficulty. 2 Figure 1: Performance-loss relationship across difference model size (left) and learning rate schedule (middle). At equivalent loss values, smaller models or those with lower learning rates generally achieve higher accuracy than larger models or those with higher learning rates. Performance-compute relationship for different clusters of BBH samples(right). Different clusters demonstrate diverse scaling patterns. 2. End-to-end performance-compute prediction. These methods [19, 26, 1] directly model the relationship between performance and compute (or the number of model parameters). Additionally, Achiam et al. [1] estimate and fit this relationship using subset of the evaluation set. Hu et al. [19] address the challenge of non-emergent capabilities in smaller models by employing multiple non-greedy decoding evaluations, thereby enabling accurate extrapolation of performance predictions for models with up to 2.4B parameters."
        },
        {
            "title": "3 Pilot Study",
            "content": "In this section, we present the pilot experiments to illustrate the shortcomings of existing approaches. Training loss may mismatch downstream tasks performance. Predicting downstream performance based on training loss relies on the assumption that LLMs achieve identical downstream performance at the same loss valuean assumption that often does not hold. In practice, training loss primarily serves as an indicator of in-domain fitting, whereas downstream tasks typically represent out-ofdomain evaluations. Moreover, training configurations, such as model size and learning rate, can significantly affect not only the final loss but also the models generalization capabilities. Fig. 1(left) illustrates the performanceloss relationships for LLMs of different sizes on the CEval benchmark [29]. At the same training loss level, smaller models can outperform larger ones in terms of test accuracy. Because smaller models initially exhibit weaker in-domain fitting capacity, they typically require more training steps to reach the same loss value, which can lead to better in-domain generalization once they do. Fig. 1(middle) compares the performance of LLMs trained under different learning rate schedules on the GSM8k dataset [7]. At the same loss level, the performance under the cosine schedule is always worse than that under the constant schedule, indicating that lower learning rate may prioritize memorization over generalization, thereby diminishing downstream performance. Diverse scaling patterns within the evaluation set. Scaling patterns capture the performancecompute relationship for single task sample. However, different task samples exhibit unique computational thresholds, learning slopes, and upper bounds, making it challenging to find single fitting function (or set of fitting functions) that generalizes well across diverse task samples. Fig. 1(right) illustrates the performance-compute relationships on three random clusters of the BBH benchmark [32], with each cluster containing samples with similar difficulty. Even within single evaluation set, these scaling curves can vary significantly, indicating that one-size-fits-all performance-compute curve is insufficient for capturing the full spectrum of downstream benchmark. 3 Figure 2: The pipeline of Cluster-On-Difficulty downstream task performance scaling, including 4 stages: a. Represent task difficulty feature with task-wise passrate vector. Cluster on difficulty feature and filter outliers. b. Fit cluster-wise performance-compute curve. Classify clusters into extrapolatable clusters, non-extrapolatable clusters, and non-emergent cluster. c. Predict accuracy on extrapolatable clusters. d. Map subset accuracy prediction to full evaluation set performance. Taken together, these observations highlight the importance of modeling the heterogeneous scaling properties within an evaluation set and identifying robust intermediate metric to serve as reliable indicator of the downstream performance of LLMs."
        },
        {
            "title": "4 Method",
            "content": "In this section, we introduce the COD method in four parts, illustrated in Fig. 2: 1) We show the advantages of COD and present an improved mean-shift clustering algorithm (Section 4.1); 2) We derive performance scaling law corresponding to task difficulty variance, which enhances the benefit of extrapolating the performance-compute relationship for task clusters with similar difficulty features (Section 4.2). We fit cluster-wise performance-compute curves on small models and filter extrapolatable clusters; 3) We extrapolate the performance on extrapolatable clusters and predict the accuracy of the target large model on the predictable subset(Section 4.3); 4) We show how to map accuracy on the predictable subset to full evaluations (Section 4.4)."
        },
        {
            "title": "4.1 Clustering on Difficulty",
            "content": "Despite sharing common themes, tasks within evaluation sets demonstrate substantial difficulty differences. These differences result in diverse performance scaling patterns across tasks, making it challenging to apply universal fitting function for predictions. Instead, we propose clustering tasks with comparable performance scaling behaviors to enable more accurate predictions. This approach minimizes the heterogeneity of difficulty features within clusters while ensuring that each cluster contains sufficient number of samples for robust evaluation. We adopt the passrate metric to quantify the capabilities of small-scale models [19]. For each model, we conduct multiple evaluation runs (e.g., 100 trials) and calculate the mean accuracy as the expected probability of correct responses. For each task, we characterize its difficulty through the passrates of models of increasing size. These passrates are arranged in ascending order of model scale, forming feature vectors that ideally exhibit monotonic growth within the [0, 1] range, as model capability typically increases with size. However, we observe that some tasks deviate from the expected scaling pattern, showing non-monotonic difficulty features. This phenomenon may be attributed to metric instability or fluctuations in model performance during training. Improved clustering methods. We hope to adopt clustering algorithms with the following features: 1. Minimizing intra-class variance to ensure similar extrapolation properties within each cluster, 2. Automatic determination of cluster numbers, as the optimal number varies across evaluation sets and is difficult to pre-specify. 4 Figure 3: t-SNE visualization of different clustering methods. Each point represents an evaluation sample. DBSCAN(left): Continuous diffusion of clustering leads to too many samples within class and large inner-group variance. MeanShift(Middle): Keep the high-density region as unique group. Improved-MeanShift(Right): Constrain inner-group variance with the radius parameter. Among classical clustering algorithms, the K-Means algorithm [25] needs to specify the number of clusters in advance. Although there are methods for automatically selecting the optimal number of clusters, e.g. Elbow method [35] and Silhouette [27], these methods need to introduce additional hyperparameters. DBSCAN [11] is non-parametric density-based clustering algorithm that marks points in low-density regions as outliers while cluster points in the connected high-density regions. In practice, DBSCAN may lead to larger final intra-class variance and does not meet the clustering requirements of the current task. MeanShift [12] algorithm adopts an extra clustering radius parameter to constrain the intra-class variance, which better fits our demands. To further reduce intro-class variance, we propose an improved MeanShift algorithm to constrain the cluster diameter. At the same time, we maintain minimum number of tasks in each cluster to reduce metric fluctuations. We provide the t-SNE visualization of evaluation tasks on BBH [32]. Each point represents an evaluation sample and its color denotes the cluster type. Fig. 3. The improved MeanShift prevails, as it effectively splits dense areas into reasonable clusters. We explain the details of clustering algorithms in Appendix A.1, and smoothing techniques in Appendix A.2."
        },
        {
            "title": "4.2 Fitting",
            "content": "Following cluster analysis, we compute evaluation metrics of small models within each cluster and conduct separate extrapolation curve fitting procedures. Small models are trained with the same ratio of training tokens to Compute Per Token (CPT). We propose scaling law for downstream task performance, supported by theoretical analysis, which allows us to derive prediction formulas for performance scaling within clusters of tasks that share similar difficulty features. The fitting process initially excludes outlier samples, focusing only on the clustered sample set. For each cluster identified in the previous step, we compute accuracy metrics across small models, yielding an expected accuracy array for each cluster. By fitting these accuracy values against the computational costs of small models, we derive the expected accuracy-to-compute curve for each cluster. We derive the fitting formula for the downstream task scaling law based on the following three assumptions: 1. The relationship between the answer loss and the compute follows power law, which generalizes the power law in loss prediction into (Question, Answer) format data. 2. For task samples with finite set of answers, the model gives random guess choice if it cannot accurately solve it. 3. The task passrate is defined as the product of the predicted probabilities for each token, implying that each task sample has unique answer, and the model outputs the answer only without any intermediate reasoning progress. Note that these assumptions may not perfectly hold in practice, we provide additional discussions on Assumption 3 in Section 6. Under the above assumptions we can derive the scaling law for downstream task performance. 5 Proposition 1 (Scaling Law for Downstream Task Performance). Given language model trained with computational budget C, and set of downstream tasks , under the following assumptions: The expected accuracy on tasks can be modeled as: Ep[Acc(C)] = + (1 g) (cid:18) eaCbc + (cid:19) σ2 µ + o(µ) where: represents the random guess performance floor; 1 represents the maximum achievable performance improvement; a, b, are positive constants; µ = 1 #P (cid:80) σ2 = 1 #P (q,ans)P lossans; (cid:80)(lossans_t µ)2. We outline the key proof intuition here, with detailed proofs provided in Appendix B. Like existing approaches [19], we aim to establish the relationship between lossans and model passrate, leveraging loss power-law scaling to derive scaling formula for downstream task passrate metrics."
        },
        {
            "title": "Proof intuition",
            "content": "The assumption 3 ensures unique task answers and neglecting the impact of model thinking before answers, the probability of correct task completion equals the product of token probabilities in the model output. This implies negative logarithmic relationship between lossans and passrate for individual tasks. Previous works overlook that computing the passrate metric for an evaluation set requires averaging exp(lossans) across tasks, whereas applying the loss scaling law necessitates averaging loss before exponentiation. Mathematically, the performance scaling law computes the arithmetic mean of exp(lossans), while the loss scaling law after exponentiation yields the geometric mean. We show that the difference between arithmetic and geometric means can be estimated by σ2/2µ, where σ2 and µ denote the variance and mean of task passrates, respectively. Consequently, the downstream tasks performance scaling law derived from the loss scaling law is valid only for evaluation sets with limited difficulty variance. Our proposed clustering-based COD method constrains the variance of difficulty features within clusters, enabling better alignment with the performance scaling law. Finally, we constrain the model output space to finite answer set, random guessing yields an expected score for unsuccessful attempts. Proposition 1 demonstrates that metric of an evaluation set with similar difficulty features can be effectively modeled using the following formula: y(C) = + (1 g) eaCbc, (1) where and jointly influence how accuracy varies with C, controls the upper bound of the fitting curve, and represents the expected random guess metric for the model on this cluster. Parameters a, b, c, and are to be fitted."
        },
        {
            "title": "4.3 Extrapolation",
            "content": "We aim to identify clusters exhibiting robust scaling patterns for reliable performance extrapolation since some clusters have saturated or non-emergent performance on small models and are not expected to give reasonable predictions. We will show that performance prediction on these scalable clusters contributes to the prediction on the full evaluation set. We give the following definition to check whether cluster is extrapolatable. 6 Definition: cluster demonstrates scaling patterns if: (1) its expected accuracy increases monotonically with model size, and (2) the probability of correct responses converges to at least as computational resources approach infinity, where 1 is predefined threshold accounting for practical limitations such as ambiguous questions and finite training coverage. We filter clusters lacking scaling patterns by the following two rules: 1. Negligible accuracy growth with increased computational resources, manifested as minimal or values in Eq. (1); 2. Poor extrapolation reliability, indicated by excessive values in Eq. (1). In practice, we set the parameter ranges priori as > 1, > 0.1, and 0 < < 1. The predictable subset comprises all samples from clusters that exhibit scaling patterns. The final performance prediction for target model in the predictable subset is computed as the weighted average of the individual cluster predictions, with weights proportional to the cluster sizes."
        },
        {
            "title": "4.4 Mapping from Predictable Subset to Target Evaluation Set",
            "content": "We extend our predictions from the predictable subset to the complete evaluation set through principled mapping approach. Our method rests on the observation that extrapolatable and nonextrapolatable samples share question types but differ primarily in difficulty features, suggesting preserved partial order of metrics across these subsets. We formalize this relationship through mapping function : (T ) from predictable subset metrics to total evaluation set metrics . This function exhibits key properties: 1. continuity and smoothness over [0, 1], 2. monotonic increase, and 3. passage through points (0, 0) and (1, 1). Empirical validation reveals that quartic function optimally captures this relationship: (x) = α1x4 + α2x3 + α3x2 + (1 α1 α2 α3)x (2) To ensure reliable extrapolation, we calibrate the mapping curve using evaluation results of existing models as anchors. Our results show that the subset-to-full mapping generally maintains robustness across model architectures and training data, enabling the use of external models (e.g., Qwen272B [39]) as anchors for most tasks. We conduct corresponding experiments in Section 5.5. For datasensitive tasks, models with similar training distributions provide more reliable anchors, indicating that data consistency takes precedence over architectural variation ensuring mapping accuracy. This calibration strategy enables accurate metric predictions for the complete evaluation set while maintaining computational efficiency. Finally, combining Eq. (1) and Eq. (2), we get our final metric prediction = y(C0), where C0 is the estimated computation of training the target LLM."
        },
        {
            "title": "5.1 Experimental Setups",
            "content": "In our experimental setup, we train several smaller versions of the target model architecture for prediction. These models vary in size but share similar training procedures, with the training data scaled proportionally to their sizes. Downstream evaluation sets. We adopt the following widely-used benchmarks as our target downstream tasks: For evaluation, we adopt the following widely-used benchmarks, shown in Table 1. Evaluation sets cover popular downstream tasks of the language model, including math, logic, coding, reading comprehension, professional knowledge, etc. All models are evaluated in few-shot in-context learning manner, where they need to generate final answer labels based on given demonstrations and test inputs. We aligned our evaluation setups with LLaMa3 [10]. Model training. To establish performance predictions for large language models, we conduct systematic experiments with suite of smaller-scale models across different parameter counts and training data volumes, while controlling for other training configurations such as learning rate, batch size, and additional hyperparameters. All models are trained on constant learning rate scheduler and data with the same distribution. We list model configurations in Table 2. 7 Table 1: Information of evaluation datasets used in the study. Dataset GSM8K MATH BBH [32] [15] [7] TriviaQA MBPP [20] [3] AGIEval [43] DROP MMLU-pro [9] [36] Domain #Questions #Shots in Prompt Math Math Reasoning Knowledge Coding Comprehensive Reading Comprehensive 1,319 12,032 5 17,944 5 9,536 3 8,063 5 5,000 4 6,511 500 3 Table 2: Model architecture specifications across different sizes. 12B 122M 238M 411M 652M 973M 1.9B 7B Param. (M) Compute Per Token (B) Tokens (B) Layers Model Dimension FFN Dimension Heads KV Heads 122 1.535 26 8 1,024 3,584 8 238 2.684 45 10 1,280 4,480 10 10 411 4.275 72 12 1,536 5,376 12 12 652 6.378 108 14 1,792 6,272"
        },
        {
            "title": "5.2 Prediction Experiments",
            "content": "6,980 1,901 973 12,022 9.060 16.436 54.761 91.609 1,544 153 923 43 16 32 4,096 2,048 4,608 7,168 14,336 16,128 16 16 277 20 2,560 8,960 20 20 36 12 32 70B (Target) 68,452 475.131 8,012 80 8,192 28,672 64 8 Baselines. We evaluate our proposed COD performance scaling for LLMs against existing approaches. The evaluation is conducted on multiple public benchmarks mentioned above, where we utilize series of smaller models with identical data distribution and architecture but different configurations to estimate the downstream tasks performance of the target 70B large language model. We compare against three existing prediction methods: 1. End-to-end performance-compute prediction: Extrapolate larger model metrics directly from smaller model evaluation set metrics using performance scaling laws. 2. Passrate-compute prediction: Estimate large model passrates from smaller model passrates [1, 19]. We repeat and evaluate 100 trials for each evaluation set to enhance the performance reliability on smaller models. For fair comparison, we report the absolution prediction error on the passrate metric instead of greedy decoding accuracy. 3. Loss-intermediate performance prediction: First predict the final training loss of large language model, then estimate downstream task metrics based on the relationship between smaller model evaluation metrics and their corresponding losses [6]. We design two experimental groups to validate the benefits of clustering and the complete pipeline, respectively: 1. COD w/o. mapping: Performing difficulty-based clustering using K-Means, extrapolating within each cluster independently, and then aggregating metrics across clusters without requiring subset-to-full mappings. 2. COD complete: Complete multi-stage proposed approach consisting of clustering, predictable cluster filtering, subset extrapolation, and subset-to-full mapping. The comparative results across different benchmarks and estimation approaches are presented in Table 3. We evaluate prediction accuracy with the absolute error between predicted and actual performance. We report the prediction error on each single evaluation set and list the mean and the max prediction error. Results. Predictions with an absolute error of less than 2 percentage points (pp) are considered accurate estimations, and when the predicted values fall within the training metric fluctuation range, they are marked in green; predictions with an absolute error greater than 5 indicate invalid estimations, which reduce the overall reliability of the prediction method and are marked in red. These results show our approach significantly outperforms existing methods in both mean and maximum prediction errors, maintaining mean prediction error within 2 pp, thus offering practical guidance for large model 8 Table 3: Absolute prediction error on several evaluation sets. prediction error less than 2 pp is considered an accurate estimate (marked in green), while an error greater than 5 pp is regarded as an invalid estimate (marked in red). Method Overall Metrics Individual Task Sets Mean Max GSM8k MATH BBH TriviaQA MBPP AGIEval DROP MMLU-pro End-to-end Passrate Loss-intermediate 3.10 5.02 5.29 COD (w/o mapping) 2.24 1.63 COD (Complete) 6.00 8.80 9.39 5.26 2.38 4.00 6.71 9.39 4.70 2.23 3.86 8.80 6.95 0.50 1. 0.64 3.51 2.33 2.91 1.77 0.68 4.00 5.81 1.98 1.64 1.75 7.34 5.52 0.89 2. 6.00 6.78 1.41 5.26 2.38 4.11 0.26 5.37 1.08 0.23 3.72 2.74 5.55 0.57 1. training. While existing methods demonstrate good performance on certain evaluation sets, they consistently exhibit substantial estimation errors on minority of sets, undermining the credibility of their predictions. Figure 4: Performance-compute relationship across difference prediction method Through visualization of the performance-compute relationship, we illustrate the distinctive characteristics of different prediction methods, shown in Fig. 4. On the BBH evaluation set, while all three methods yield comparable estimates, end-to-end and loss-intermediate methods demonstrate inadequate fitting for small model evaluation points. In contrast, the cluster method reveals more sophisticated and well-fitted multi-phase trajectory. For MATH and MMLU pro evaluation sets, the critical challenge involves determining whether large model metrics will experience accelerated growth with increased computing power or encounter performance plateaus. The loss-intermediate method exhibits an underestimation of the model capability ceiling, while the end-to-end method shows prediction errors exceeding 3 pp. The clustering methods effectiveness can be attributed to its comprehensive analysis of evaluation set difficulty distributions and scaling laws. It successfully predicts growth patterns in mathematical evaluation sets where most problems demonstrate expanded improvement potential, while accurately capturing the diminishing scaling properties in evaluation sets with score saturation as computational resources increase."
        },
        {
            "title": "5.3 Comparison of Clustering Methods",
            "content": "We evaluate the impact of clustering methods on the estimation approach. Our goal is to control the average distance between samples and cluster centers within clusters, making difficulty features more similar within clusters. We also ensure that the minimum number of questions in any cluster is not less than 10, considering that too small clusters may lead to instability in metric values. We compared our proposed Improved-MeanShift algorithm with clustering methods including DBScan, MeanShift, and K-Means. Since standard K-Means lacks the ability to filter outliers and directly control intra-cluster distances, we made the following adjustments: (1) Search for the number of clusters such that the minimum cluster size is close to but not less than 10 samples; (2) Draw spheres around cluster centers with given threshold radius, and treat samples not covered by any sphere as outliers. If cluster drops to less than 10 samples, we treat its samples as outliers. 9 Table 4: Clustering performance on popular benchmarks. Method MMLU-pro GSM8k MATH BBH IAD OR(%) IAD OR(%) IAD OR(%) IAD OR(%) K-Means DBScan MeanShift Improved-KMeans Improved-MeanShift 0.3236 0.4242 0.2859 0.1609 0. - 0.56 0.39 2.85 4.40 0.2238 0.5131 0.2852 0.1321 0.1854 - 0.53 0.61 2.73 4.93 0.2238 0.4775 0.2110 0.0902 0.1463 - 0.68 1.44 2.22 2.66 0.6284 0.7113 0.2679 0.1953 0. - 18.92 20.72 37.23 33.58 Table 5: Prediction errors across clustering algorithms. Method Mean Max MMLU-pro GSM8k MATH BBH EE FE EE FE EE FE EE FE EE FE EE FE K-Means DBScan MeanShift Improved-KMeans Improved-MeanShift 3.62 3.76 8.16 3.93 4.08 4.38 2.12 1.68 3.15 1.33 1.84 3.92 1.23 1.66 2.20 8.99 4.36 3.08 4.08 2.23 3.69 3.72 3.15 0.56 1.27 3.69 3.69 3.08 0.61 1.35 0.01 0.00 2.62 2.34 8.16 4.08 4.12 4.38 4.16 3.53 0.67 0.74 2.55 2.26 2.12 3.92 4.08 0.81 0.51 0.02 2.20 2.23 1.14 1.28 0.29 8.99 4.36 0.65 2.17 1. We use Intra-cluster Average Distance (IAD) and Outlier Rate (OR) as direct evaluation metrics. With similar OR, smaller IAD indicates better clustering performance, as shown in Table 4. Additionally, we measure the benefits of different clustering methods on the prediction process by comparing the Extrapolation Errors(EE) of the predictable subset and Final prediction Errors (FE) after clustering, as shown in Table 5. Table 4 shows that Improved-KMeans and Improved-MeanShift achieve better clustering performance, which is attributed to their incorporation of intra-cluster distance constraints during the clustering process. From Table 5, we can observe that the two methods with better clustering performance correspond to smaller extrapolation errors of estimable subsets and final metric prediction errors. Although Improved-KMeans achieves optimal clustering performance, its downstream estimation performance on GSM8k is notably inferior compared to other evaluation sets. We believe this is because the K-Means algorithm requires pre-specifying more explicit number of clusters, but the number of clusters in evaluation sets is difficult to know in advance. While the number of clusters obtained through our search method is effective for some evaluation sets, it lacks stability and ultimately leads to excessive prediction errors in few evaluation sets. In contrast, our adopted Improved-MeanShift algorithm inherently does not require pre-specifying the number of clusters; instead, it automatically determines this based on our intra-cluster distance constraints. This results in more stable clustering performance and yields the smallest maximum estimation error across evaluation sets. We present additional clustering experimental results on more evaluation sets in Appendix C.1, where the conclusions are consistent with existing evaluation sets."
        },
        {
            "title": "5.4 Extrapolation Formula",
            "content": "To evaluate the effectiveness of different fitting formulas, we conducted an ablation study comparing various formulations of the accuracy-compute relationship. Our baseline formula incorporates random guess probability, exponential decay, and constant offset term: (C) = + (1 g) eaCbc (3) To understand the contribution of each component, we perform ablation experiments by removing or modifying different terms. 1) Without random guess component: f1(C) = eaCbc; 2) Without constant term c: f2(C) = + (1 g) eaCb ; 3) Direct power law relationship [19]: f3(C) = eaCb . The comparative results of these formulations are shown in Table 6. Results from three evaluation sets, BBH, Math, and MMLU-pro, are presented here, showing the Extrapolation Error of extrapolatable clusters (EE), the Task Ratio of predictable subset (TR), and the Final prediction Error (FE). These 10 Table 6: Ablation study results across different benchmarks. Method BBH EE TR(%) FE Direct Power Law w/o Random Guess w/o Constant Ours 8.90 10.27 2.14 0.29 49.06 45.75 57.26 52.46 8.88 11.20 4.01 1.77 MATH TR(%) 81.46 81.46 81.46 81.24 EE 3.81 4.04 1.40 1.14 MMLU-pro FE 3.35 3.55 1.56 1.28 EE 4.30 4.40 3.85 1.27 TR(%) 95.15 95.05 95.60 94.38 FE 4.27 4.37 3.88 1.35 results show that the proposed formula consistently achieves the smallest extrapolation error and final prediction error, while the ratio of estimable subsets remains similar across different clustering methods. In the control group, f1 performs poorly on tasks with finite answer sets, where small models achieve non-zero scores that f1 cannot effectively fit. f2 removes c, which determines the maximum value of the prediction curve, assuming that the evaluation set performance would reach perfect scores given sufficient computation and parameters. However, this assumption is unreasonable due to the limited training data distribution and ambiguous answers in evaluation set questions, leading to inaccurate predictions. Direct power-law fitting f3 fails to model both the metric range constraint of 0 to 1 and the characteristic that metric improvement is typically more difficult near Random Guess capability and capability saturation compared to other regions. We also observe that TR has little influence on prediction error, which also indicate the robustness of the proposed method. Some evaluation set with low TR due to the non-emergent subset, and our results show that this subset can largely be predicted with metrics of the extrapolatable clusters. We visualize the difficulty distribution of predictable subset and the full evaluation set in Appendix D."
        },
        {
            "title": "5.5 Anchor Point in Interpolation Mapping",
            "content": "During the mapping phase from estimable subset metrics to full evaluation set metrics, we discovered that models with different training data and architectures exhibit similar mapping relationships. This allows us to leverage metrics from pre-trained models to refine the mapping relationship, thereby improving the accuracy of our final metric estimation. We use both Qwen2-72B [39] and an in-house model with Mixture-of-Experts(MoE) [22] structure trained on the same data distribution as anchor points in the mapping phase. We first obtain the interpolation curve using only small model metrics with points (0,0) and (1,1), then verify the compatibility of anchor points with the existing interpolation curve. When the score of the full set is 0 (1), the score of the subset must also be 0 (1). Results show that despite differences in computational requirements, model architectures, and training data, these models share similar mapping relationships. This finding also indicates that estimable subset metrics are highly correlated with full-set metrics. Compared to loss-intermediate estimation, estimable subset metrics maintain predictability while reducing interference from other model parameters. Based on these observations, we incorporated the mapping relationships from pre-trained models into the interpolation process, thereby improving both the accuracy and confidence of our large model metric estimations. We establish three experimental configurations of our method: COD w/o. anchor: The complete estimation process is employed except for not using anchor point interpolation in the Mapping phase. COD w. out-of-distribution (OOD) anchor: The complete proposed methodology incorporates both difficulty-based clustering and predictable subset identification. Using the 72B Qwen2 pretraining model as the anchor model [39]. COD w. in-domain(ID) anchor: Using an in-house MoE model with consistent training distribution but different model architecture as the anchor point. We present the results in Table 7, demonstrating that incorporating both out-of-distribution models and in-distribution models as anchors consistently enhance prediction accuracy. These findings suggest 11 Table 7: Influence of anchor point usage in the mapping stage. Overall Individual Task Sets Method Mean Max GSM8k MATH BBH TriviaQA MBPP AGIEval DROP MMLU-pro w/o. anchor w. ood anchor w. id anchor 3.96 1.59 1.63 8.80 2.56 2. 2.17 2.22 2.23 5.46 0.94 1.28 5.08 1.84 1.77 1.68 1.04 1.64 8.80 2.56 2.19 2.38 1.86 2. 4.44 1.18 0.23 1.68 1.04 1.35 relatively stable correlation between the metrics of predictable subsets and the full dataset, indicating that the relationship between subset and full-set metrics remains consistent across models trained on different data and with varying architectures. This property enables us to leverage evaluation results from existing models to improve the accuracy of metric predictions for large models with new data and structures. Besides, the division of the evaluation set obtained through clustering is an intrinsic property of the evaluation set itself, independent of the model architecture and training data. Therefore, the predictable subset derived from clustering can also be extended to estimate the metrics of new models. Additionally, we conduct the ablation study on the interpolation method in Appendix C.2, and results indicate that quartic functions are suitable in our setting."
        },
        {
            "title": "6 Conclusion and Discussion",
            "content": "In this work, we introduce novel downstream performance scaling framework including (1) difficulty-based clustering approach that effectively models the underlying distribution of each evaluation set; (2) theoretically grounded scaling law for downstream task performance that provides fitting formula for performance-compute extrapolation; and (3) systematic methodology for identifying and leveraging predictable subset that provides robust intermediate metric for accurate full-set performance predictions. Our framework, while effective for dense transformers, has not been fully explored for cost-efficient MoE models and does not account for the annealing phase in training, where high-quality data can rapidly enhance performance. The COD method requires sufficient test cases and is not suited for multiple-choice tasks, where performance metrics may diverge from true passrates. Additionally, the frameworks theoretical foundation is insufficient for chain-of-thought reasoning, necessitating future adaptations to address these challenges. We provide detailed discussion in Appendix E. Looking forward, our approach can be further expanded across model architectures, training methods, and evaluation set types, while extending this framework to address chain-of-thought reasoning patterns offer promising avenues for future research."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. Adv. Neural Inform. Process. Syst. (NeurIPS), 35:2230022312, 2022. [3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [4] Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Tran, and Mehran Kazemi. Smaller, weaker, yet better: Training llm reasoners via compute-optimal sampling. arXiv preprint arXiv:2408.16737, 2024. [5] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. [6] Yangyi Chen, Binxuan Huang, Yifan Gao, Zhengyang Wang, Jingfeng Yang, and Heng Ji. Scaling laws for predicting downstream performance in llms. arXiv preprint arXiv:2410.08527, 2024. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [8] Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities of language models from the loss perspective. arXiv preprint arXiv:2403.15796, 2024. [9] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL-HLT, pages 23682378, 2019. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [11] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. density-based algorithm for discovering clusters in large spatial databases with noise. In KDD, volume 96, pages 226231, 1996. [12] Keinosuke Fukunaga and Larry Hostetler. The estimation of the gradient of density function, with applications in pattern recognition. IEEE Transactions on information theory, 21(1):3240, 1975. [13] Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et al. Language models scale reliably with over-training and on downstream tasks. arXiv preprint arXiv:2403.08540, 2024. [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [15] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2021. [16] Tom Henighan, Jared Kaplan, Maxwell Katz, Anselm Levskaya, Sam McCandlish, Andreas Stuhlmuller, Scott Gray, and Dario Amodei. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. [17] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. arXiv preprint arXiv:2102.01293, 2021. [18] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [19] Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding, Zebin Ou, Guoyang Zeng, et al. Predicting emergent abilities with infinite resolution evaluation. In Int. Conf. Learn. Rep. (ICLR), 2024. [20] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Annual Meeting of the Association for Computational Linguistics, pages 16011611, 2017. [21] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 13 [22] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. [23] Lucas Lingle. large-scale exploration of µ-transfer. arXiv preprint arXiv:2404.05728, 2024. [24] Qian Ma, Haitao Mao, Jingzhe Liu, Zhehua Zhang, Chunlin Feng, Yu Song, Yihan Shao, and Yao Ma. Do neural scaling laws exist on graph self-supervised learning? arXiv preprint arXiv:2408.11243, 2024. [25] James MacQueen et al. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281297, 1967. [26] David Owen. How predictable is language model benchmark performance? arXiv preprint arXiv:2401.04757, 2024. [27] Peter Rousseeuw. Silhouettes: graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:5365, 1987. [28] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models mirage? In Adv. Neural Inform. Process. Syst. (NeurIPS), 2023. [29] Christin Seifert, Jörg Schlötterer, et al. Ceval: benchmark for evaluating counterfactual text generation. In International Natural Language Generation Conference, pages 5569, 2024. [30] Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. Scaling retrieval-based language models with trillion-token datastore. arXiv preprint arXiv:2407.12854, 2024. [31] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [32] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics, pages 1300313051, 2023. [33] Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, and Ngai Wong. Scaling laws with vocabulary: Larger models deserve larger vocabularies. arXiv preprint arXiv:2407.13623, 2024. [34] Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efficiently: Insights from pretraining and finetuning transformers. In Int. Conf. Learn. Rep. (ICLR), 2022. [35] Robert Thorndike. Who belongs in the family? Psychometrika, 18(4):267276, 1953. [36] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. [37] Jason Wei, Yi Tay, Rishi Bommasani, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [38] Chaojun Xiao, Jie Cai, Weilin Zhao, Guoyang Zeng, Xu Han, Zhiyuan Liu, and Maosong Sun. Densing law of llms. arXiv preprint arXiv:2412.04315, 2024. [39] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 14 [40] Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. [41] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pages 12041213, 2022. [42] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3): 107115, 2021. [43] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. In Findings of the Association for Computational Linguistics, pages 22992314, 2024."
        },
        {
            "title": "A Improvements of Clustering Algorithm",
            "content": "A."
        },
        {
            "title": "Improved MeanShift Algorithm",
            "content": "We iteratively apply the MeanShift algorithm with predefined cluster radius and minimum cluster size K. In each iteration, for the clustered samples, we examine whether the distance between each sample and its cluster center exceeds R, and relabel those samples that exceed this threshold as unclustered. For clusters containing fewer than samples, we mark all samples in these clusters as unclustered. At the end of each iteration, we incorporate both the outliers from MeanShift and our marked unclustered samples into the next round of clustering, continuing this process until no further changes occur in sample labels. We present the pseudocode in Algorithm 1. Algorithm 1 Iterative MeanShift Clustering Algorithm Perform MeanShift clustering with radius on all samples labeled 1 Assign new labels to clustered samples for each newly labeled sample do Calculate distance disti to its cluster center if disti > then Reset label to 1 1: Initialize all labels in the evaluation set to 1 2: repeat 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: until no label changes end for for each cluster do if number of samples in cluster < then Reset all samples in this cluster to 1 end if end if end for Renumber all non-{1} newly labeled samples to avoid overlap with old labels Filtering Zero-performance Samples In the evaluation set, there may exist few extremely difficult problems that require sufficient model parameters to emerge. All small models may fail to solve these problems even after 100 evaluation attempts, resulting in difficulty feature vectors of all zeros. We refer to these as zero-performance samples. Their presence leads to two issues: 1. Zero performance on small models does not necessarily indicate zero accuracy on large models. For these samples, we cannot estimate when emergence will occur or predict large model metrics. 2. During clustering, they may be confused with other low-performing but non-zero samples. Including them in the same cluster would lower the expected accuracy of that cluster, leading to inaccurate fitting and extrapolation later. Therefore, we pre-filter these zero-performance samples before clustering, treating them as outliers that do not participate in the clustering process. This approach eliminates the need to consider their metrics under large models during subsequent extrapolation and prevents disruption to the clustering of normal difficult samples. A.2 Smoothing Techniques Horizontal smoothing: adjacent checkpoint smoothing. Metric fluctuations of individual samples in downstream tasks are not solely due to limited sampling. Another potential factor is noise from uneven data distribution in recent training batches. Therefore, in addition to performing 100 evaluations to mitigate sampling variance, we evaluated 100 times on each of the adjacent checkpoints before and after the selected model. We then averaged these accuracy expectation values across three checkpoints, further reducing sampling variance while offsetting noise from uneven training data distribution. This approach also reduces the number of zero-performance samples, further improving clustering and prediction effectiveness. Vertical smoothing. Each samples features represent the expected correct response rate across models of increasing size, forming partially ordered sequence. However, the Euclidean distance used for measurement does not consider this sequential information. For example, if cluster center has feature sequence of [0, 0, 0, 0.5], sample with [0, 0, 0.2, 0.5] and sample with [0.2, 0, 0, 0.5], sample clearly fits the cluster better than sample B, yet their Euclidean distances are identical. Note that this smoothing method may not be effective for all downstream tasks. Our current observations suggest that for freeform tasks with limited solution spaces (such as multiple choice, ordering, or judgment questions in freeform format), once models learn to answer within the solution space, their random guess metrics on the evaluation set will be non-zero, more significantly affected by recent training batch data and few-shot cases in prompts. In such cases, vertical smoothing is more likely to bring positive benefits. In our experiment, we only adopt horizontal smoothing, and leave vertical smoothing as an optional selection."
        },
        {
            "title": "B Proof of Proposition",
            "content": "We use Proposition B.1 to derive scaling law for downstream task performance (Proposition B.2). Proposition B.1 (Arithmetic-geometric mean difference). For any sequence of positive real numbers {xi}n i=1, let: µa = 1 (cid:80)n i=1 xi be the arithmetic mean; µg = (cid:81)n i=1 x1/n (cid:80)n σ2 = 1 i=1(xi µ)2 be the variance. be the geometric mean; Then the difference between the arithmetic mean and geometric mean can be estimated as: = µa µg = 1 (cid:88) i=1 xi (cid:33) xi = (cid:32) (cid:89) i=1 σ2 2µa + o(µa) Proof. Taking the logarithm of the geometric mean µg: log(µg) = 1 (cid:88) i= log xi Using Taylor expansion of log around µ: log = log µ + µ µ (x µ)2 2µ2 + (cid:0)(x µ)2(cid:1) (4) (5) (6) We can simplify: log(GM ) = 1 n (cid:88) i=1 log xi = log µ + = log µ + 1 1 µ (cid:88) i=1 (cid:32) 1 (cid:124) (cid:19) + o(µa) (xi µa)2 2µ2 (cid:32) (cid:33) 1 2µ2 (cid:88) 1 xi µa + (xi µa)2 (cid:18) (xi µa) µa (cid:88) i=1 (cid:123)(cid:122) equal to 0 i=1 (cid:123)(cid:122) σ2 +o(µa) (cid:33) (cid:125) = log µ σ2 2µ2 + o(µa) (cid:125) (cid:124) Therefore: (cid:18) (cid:18) µa µg = µa 1 exp (cid:19)(cid:19) σ2 2µ2 + o(µa) When σ2 2µ2 is small, this can be approximated as: σ2 2µa (7) (8) Proposition B.2 (Scaling law for downstream task performance). Given language model trained with computational budget C, and set of downstream tasks , under the following assumptions: 1. The relationship between the answer loss and compute follows power law, (cid:80) 1 (q,ans) lossans(C) aC + c; 2. For tasks with finite answer set, the model gives random guess choice if it cannot truly solve it; 3. Task passrate equals the product of the predicted probability of each token pans = tans p(t), which means that each task has unique answer and model output answer (cid:81) only without thinking progress. The expected accuracy on tasks can be modeled as: Ep[Acc(C)] = + (1 g) (cid:18) eaCbc + (cid:19) σ2 2µ + o(µ) (9) where: represents the random guess performance floor; a, b, are positive constants; µ = 1 #P (cid:80) σ2 = 1 #P (q,ans)P lossans; (cid:80)(lossans_t µ)2. Proof. We first use assumption 3 to establish the relationship between model passrate and loss on task. log(pans) = log (cid:33) p(t) = (cid:32) (cid:89) tans (cid:88) tans log(pt) = lossans (10) Then take the exponential of both sides, and then take the expectation with respect to different tasks in the evaluation set = (q, ans) . We note that both pans and lossans are functions of C. Ep[pans(C)] = Ep[exp(lossans(C))] (11) = 1 (cid:88) (q,ans)P exp(lossans(C)). (12) We can adopt Proposition B.1 to switch from arithmetic mean to geometric mean of loss, and apply the power law assumption 1. exp(lossans(C)) = exp 1 (cid:88) (q,anst)P (cid:124) 1 (cid:88) lossans(C) + (q,anst)P (cid:123)(cid:122) use loss scaling law (cid:125) σ2 2µ + o(µ) (13) = exp (aC c) + σ2 2µ + o(µ) (14) 18 Table C1: Clustering performance on advanced task benchmarks (IAD: Intra-cluster Average Distance, OR: Outlier Rate) Method TriviaQA AGIEval DROP MBPP IAD OR(%) IAD OR(%) IAD OR(%) IAD OR(%) K-Means DBScan MeanShift Improved-KMeans Improved-MeanShift 0.4388 0.7039 0.2521 0.1239 0.1871 - 6.38 6.77 11.97 11.54 0.4572 0.5591 0.2886 0.1536 0.2100 - 3.67 2.99 7.60 11. 0.5554 0.6651 0.2507 0.1428 0.1974 - 11.08 11.81 21.42 19.88 0.3383 0.5060 0.2167 0.1667 0.1745 - 12.80 15.60 19.40 21.60 Table C2: Prediction errors on advanced task benchmarks. Method Mean Max TriviaQA AGIEval DROP MBPP EE FE EE FE EE FE EE FE EE FE EE FE K-Means DBScan MeanShift Improved-KMeans Improved-MeanShift 2.44 3.36 2.97 8.99 3.04 2.79 6.43 4.36 3.21 2.34 4.18 4.90 3.38 3.80 5.96 5.56 1.13 1.61 1.58 2.38 2.97 2.46 2.61 2.68 1.66 1.64 2.53 1.11 0.81 6.43 6.27 3.03 2.66 1.57 3.64 4.90 2.63 3.23 4.18 4.00 2.40 1.18 1.12 5.96 5.56 3.99 5.24 2.39 1.58 1.64 1.11 2.38 0.26 0.23 1.56 2.67 1.41 1.22 3.25 2.19 where = #P , and µ, σ2 follow definitions in the proposition. Finally, we use assumption 2 to align the answer passrate and the accuracy metric. We can adopt the law of total expectation: Ep[Acc(C)] = Pp(correct)E[Acccorrect] + Pp(incorrect)E[Accincorrect] (15) Note that Pp(correct) = Ep[pans(C)], E[Acccorrect] = 1 and Pp(incorrect) = 1 Pp(correct). We also define as the random guess accuracy performance, thus we have E[Accincorrect] = g. Take these results into Eq. (15), and we have: Ep[Acc(C)] = Ep[pans(C)] + (1 Ep[pans(C)]) = + (1 g)Ep[pans(C)] = + (1 g) (cid:18) eaCbc + (cid:19) σ2 2µ + o(µ) (16) (17) (18) Proposition B.2 demonstrates that metric of an evaluation set with similar difficulty features can be effectively modeled using the following formula: (C) = + (1 g) exp (aC c) (19)"
        },
        {
            "title": "C Additional Ablation Studies",
            "content": "C.1 Comparison of Clustering Methods on Extra Evaluation Sets. We provide additional clustering evaluation results across more evaluation sets in Table C1 and Table C2, which maintain consistency with the conclusions presented in the main text. C."
        },
        {
            "title": "Interpolation Method",
            "content": "To evaluate different interpolation methods for prediction accuracy, we compared various mathematical approaches. Our baseline method uses quartic polynomial interpolation, which we compare against several alternative approaches, including Cubic spline interpolation, Cubic polynomial interpolation, and Quintic polynomial interpolation 19 Table C3: Comparison of different interpolation methods across benchmarks. Prediction Error BBH Math MMLU-pro Cubic Spline Cubic Polynomial Quintic Polynomial Quartic Polynomial 0.68 3.38 0.18 1.77 1.31 1.12 1.42 1.28 1.37 1.35 1.36 1.35 Figure C1: Performance mapping with different interpolation methods on BBH evaluation set. The cubic spline is overfitted, and the cubic polynomial method is underfitted. Quartic polynomials and quintic polynomials are comparable while quartic polynomial has fewer parameters. The comparative results across different benchmarks are shown in Table C3. We report the prediction error between the real performance of large model and the mapping result. Furthermore, in Fig. C1, we plot the mapping process using different interpolation formulas, where the x-axis represents the predictable subset indices and the y-axis represents the full set indices. The red points are the numerical values to be fitted, green points represent predicted values, purple points represent anchor points and blue points show the actual performance. The prediction performance shows certain robustness across different interpolation methods. We aim to use the simplest possible interpolation function while maintaining low prediction errors. Based on the above results, we observe that the Cubic Polynomial shows larger prediction errors due to underfitting on BBH, while the Cubic Spline exhibits some overfitting. Both Quartic Polynomial and Quintic Polynomial perform well, therefore we chose the Quartic Polynomial method as it requires fewer fitting parameters."
        },
        {
            "title": "D Difficulty Distribution of Predictable Subset",
            "content": "We analyze the proportion of predictable subset tasks across different difficulty levels. The difficulty distributions of predictable subset versus complete sets for different evaluation benchmarks are illustrated in Fig. D2. We use the scores from the 12B model as the basis for difficulty classification. The results show that MMLU-pro and GSM8k evaluation sets have larger proportions of predictable subset, indicating that most questions in these datasets exhibit good performance scaling properties. In contrast, many difficult questions with near-zero scores in the Math evaluation set fall outside the predictable subset, requiring adjustment during the mapping phase. Meanwhile, BBH shows consistent proportions of predictable subset across difficulty levels, as some of its questions demonstrate oscillatory patterns with limited improvement despite increased computing. The proportion of predictable subset can serve as metric for assessing evaluation set quality. Evaluation sets with larger predictable subset yield more reliable experimental conclusions from smaller models. When constructing evaluation sets, we recommend screening or supplementing unpredictable clusters and ensuring minimum number of questions for each difficulty feature to reduce metric volatility."
        },
        {
            "title": "E Limitations",
            "content": "Influence of model structure and training configurations. Mixture-of-Experts (MoE) models excel in training and inference cost, and are widely used in production. In this work, we reveal the 20 Figure D2: Difficulty distribution comparison on 12B model between predictable subset and full evaluation set. performance scaling on dense transformers, while prediction on MoE models is still underexplored. However, we believe that the proposed method is not significantly affected by the model architecture. If we apply the complete pipeline to MoE models, we expect to achieve similar results. Besides, we only study the pre-training performance prediction with constant learning rate and do not cover the impact of the annealing training. In this phase, higher-quality data is usually adopted, which can rapidly improve the models capabilities. As result, performance prediction faces greater challenges. Category of evaluation sets. The proposed Clustering-on-Difficulty method requires sufficient number of test cases, as too few samples can lead to unstable cluster metrics and ineffective estimation. However, from an evaluation set design perspective, an evaluation set with good predictive properties enables more effective generalization from small-scale to large-scale models, thus providing better guidance for model iteration. Furthermore, for multiple-choice tasks, the model only needs to assign higher probability to the correct option compared to others, creating discrepancy between this metric and the models true passrate. Given that more evaluation sets are adopting the Chain-of-Thoughts prompts, we have not included multiple-choice tasks that only require option selection. Chain-of-thought performance prediction. Proposition B.2 assumes that evaluation sets directly assess models ability to provide answers. However, increasingly more evaluations allow models to think before providing answers. Recent works on inference time scaling [31, 4] further demonstrate that for tasks involving mathematics, reasoning, and coding, training models to complete tasks through longer inference computation can significantly improve downstream task performance. In cases where the reasoning process or answers are not unique, the relationship between models answer loss and passrate on task may not necessarily follow the exponential relationship between the answer loss and the sample passrate. Although our approach maintains its prediction effectiveness in such situations, the theoretical explanation for these cases is insufficient. Therefore, we consider improving prediction methods based on chain-of-thought characteristics and expanding theoretical foundations as future work."
        }
    ],
    "affiliations": [
        "Seed-LLM, ByteDance"
    ]
}