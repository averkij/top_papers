{
    "paper_title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models",
    "authors": [
        "Minghao Fu",
        "Guo-Hua Wang",
        "Tianyu Cui",
        "Qing-Guo Chen",
        "Zhao Xu",
        "Weihua Luo",
        "Kaifu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 2 7 1 3 3 0 . 1 1 5 2 : r Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models Minghao Fu1,2,3,* Guo-Hua Wang3, Tianyu Cui3 Qing-Guo Chen3 Zhao Xu3 Weihua Luo3 Kaifu Zhang3 1School of Artificial Intelligence, Nanjing University 2National Key Laboratory for Novel Software Technology, Nanjing University 3Alibaba International Digital Commerce Group fumh@lamda.nju.edu.cn wangguohua@alibaba-inc.com"
        },
        {
            "title": "Abstract",
            "content": "Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. first-order analysis yields closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at https://github.com/AIDCAI/Diffusion-SDPO. 1. Introduction Text-to-image diffusion models [6] have achieved remarkable success in generating diverse and high-quality im- *Work done during the internship at Alibaba International Digital Commerce Group. G. Wang is the corresponding author. ages [11, 20]. However, aligning these powerful generative models with nuanced human preferences remains critical challenge. Recent approaches have begun to incorporate human feedback [5] into diffusion model training, drawing inspiration from alignment techniques used in large language models. In particular, Direct Preference Optimization (DPO) [31] has emerged as promising alternative to reinforcement learning for finetuning on human preferences. DPO directly optimizes the model on pairwise human comparisons (winner vs. loser outputs), and has been successfully adapted to text-to-image diffusion models in methods [14, 22, 23, 39, 49] to improve visual appeal and prompt alignment. Despite these advances, we find that existing DPO-based alignment of diffusion models still faces fundamental limitation: simply maximizing the preference margin between winner and loser outputs does not necessarily translate to better absolute generation quality of the finetuned model. In our empirical analysis, we find that standard Diffusion-DPO [39] exhibits unstable training dynamics, and the models generative quality can deteriorate as training proceeds. As illustrated in the left part of Fig. 1, we find that both the winners and losers denoising losses tend to increase over time, even though the preference margin (Lw Ll) becomes more negative in the intended direction. This indicates that the model is widening the relative preference gap by making the less-preferred outputs worse, rather than truly improving the preferred outputs. In other words, relative alignment comes at the expense of absolute quality. The lack of safeguard on the winners loss in existing DPO objectives leads to unstable training and potential collapse, corroborating observations in prior work [27, 35, 43] that overly aggressive preference optimization can harm generative performance. These findings motivate the need for new approach to preference-based diffusion finetuning that 1 Figure 1. Training dynamics of preference losses during DPO finetuning without (left) and with (right) our safe-λ mechanism on SD 1.5 [32]. Images beneath the plots illustrate samples generated at training steps {0, 500, 1000, 1500, 2000}. can increase preference alignment while preserving or improving the quality of the preferred outputs. To address this challenge, we propose Diffusion-SDPO1 Safeguarded Direct Preference Optimization method for diffusion models. The key idea in Diffusion-SDPO is to introduce simple yet effective winner-preserving update rule that controls the influence of the loser samples gradient at each training step. In contrast to standard DPO [31, 39] which updates the model by contrasting winner and loser equally, we derive an adaptive scaling factor for the losers gradient based on the geometry of the winner and loser gradients. Intuitively, our method downweights the loser branchs contribution whenever its gradient is misaligned with the winners gradient. Grounded in first-order analysis, the safeguard computes closed-form λsafe from the inner product of the winner and loser gradients, guaranteeing that each step does not worsen the preferred outputs reconstruction loss. In practice, Diffusion-SDPO seamlessly modifies the DPO objective with this adaptive loser scaling (see Fig. 1, right), which expands the preference margin while strictly controlling the absolute error of preferred outputs. Notably, our approach is model-agnostic and can be applied on top of various diffusion alignment frameworks [14, 22, 39, 49], acting as plug-in optimizer that stabilizes training. Our contributions can be summarized as follows: We show that enlarging the winnerloser margin in diffusion preference optimization does not guarantee higher 1Throughout the text, Diffusion-SDPO is used as conceptual umbrella for our method and its guiding principles. When referring to concrete instantiations, we write X+SDPO to denote the integration of SDPO with specific base DPO variant (e.g., Diffusion-DPO, DSPO, DMPO), which clarifies the application setting and configuration. quality and can degrade preferred outputs, revealing gap between relative alignment and absolute error control. Based on these analysis, we propose Diffusion-SDPO, winner-preserving training scheme that adaptively scales the loser gradient by its geometric alignment with the winner gradient to first order. Our method is simple to implement and adds negligible overhead. Extensive experiments on SD 1.5 [32], SDXL [29] (both UNets [33]), and the industrial-scale Ovis-U1 [40] (DiT [28]) show that our method is architecture-agnostic. It delivers consistent improvements in preference metrics while preserving or enhancing aesthetic quality, stabilizing training, and avoiding collapse. These benefits persist across text-to-image, image editing and unified generation setups. 2. Related Work Diffusion Models for Text-to-Image and Unified Generation. Diffusion models have become leading paradigm for image synthesis, offering strong quality and diversity [6]. Denoising diffusion with variational objective [13] and continuous-time score-based formulations with SDEs [17, 37] underpin modern systems. Refinements such as EDM [18] and rectified flow or flow matching [24, 26] clarify objectives and improve robustness. Guidance-based conditioning [7, 12] enhances controllability. For text-to-image generation, latent diffusion [32] enables efficient high-resolution synthesis and supports large systems like SD3 [8] and FLUX [20]. In parallel, unified generators handle text-to-image and image editing within single model [40]. Our method applies to both families and is architecture-agnostic, working with UNet [33]-style and DiT [28]-style backbones. Preference Optimization for Diffusion Models. Direct Preference Optimization [9, 31, 39] has been adapted to diffusion models to align generation with human comparisons while avoiding full reinforcement learning. broad class of variants [14, 41] calibrates the preference margin or the relative branch influence to improve stability and protect the generation. Other approaches seek to guide the update directions and step magnitudes in LLMs [48] by employing subspace projections and modest objective clipping [3, 4, 16, 43, 45]. Related work such as DPOP [27] promotes positivity constraints to mitigate failure modes in preference optimization, and MaPPO [21] incorporates prior knowledge via maximum-a-posteriori objective. Diffusion-specific methods further account for the multistep nature of denoising by reweighting across timesteps or by adding entropy regularization, exemplified by BalancedDPO [38], DSPO [49], and SEE-DPO [35]. In contrast, our Diffusion-SDPO introduces per-step, geometry-aware safe scaling factor based on the inner product between winner and loser output-space gradients, which provides direct control over the winner loss at each step while continuing to expand the preference margin. 3. Preliminaries Diffusion Models. Diffusion models [13, 36] construct Markov chain that gradually corrupts clean data with additive noise and then learn parametric denoiser to invert this corruption. Let variance schedule {βt}T t=1 be given and define αt = 1βt and αt = (cid:81)t s=1 αs. The forward process can be defined as: q(xt xt1) = N(cid:0)xt; αt xt1, (1 αt) I(cid:1), (1) which implies the following closed-form perturbation of clean sample x0: xt = αt x0 + 1 αt ϵ, ϵ (0, I). (2) Equivalently, the marginal distribution conditioned on x0 is q(xt x0) = N(cid:0)xt; αt x0, (1 αt) I(cid:1). (3) Learning proceeds by training network ϵθ that receives the noised input xt and the time index to predict the injected noise. Using the reparameterization in Eq. 2, the standard objective minimizes mean squared error between the true noise and the prediction: Ldiffusion = Ex0,t,ϵ (cid:13) (cid:13) ϵθ (cid:0)xt, t(cid:1) ϵ(cid:13) 2 2. (cid:13) (4) where x0 pdata, Uniform{1, . . . , } and ϵ (0, I). Minimizing Ldiffusion yields time-aware denoiser that can be applied in reverse order to iteratively remove noise and synthesize new samples from an initial Gaussian latent. Diffusion Model Alignment via Preference. Given prompt and two images xw 0 (preferred, winner) and xl 0 (less preferred, loser), preference alignment for diffusion models seeks parameters θ such that the model assigns higher likelihood to xw 0 [9, 39]. diffusion sampler produces trajectory (xT , . . . , x0) and, at each time t, reverse conditional pθ(xt xt+1, c) [13, 26, 36]. To instantiate DPO in this setting, we adopt the standard formulation wherein the stepwise preference score is the log-likelihood ratio with respect to frozen reference model [39, 49]: 0 than to xl rt(xt, c) = β log pθ(xt xt+1, c) pref(xt xt+1, c) . (5) The DiffusionDPO [39] loss applies Bradley-Terrystyle [2] logistic regression to the winner-loser pair at the same t: LDiffusion-DPO = (cid:104) (cid:16) log σ rt(xw , c) rt(xl t, c) (cid:17)(cid:105) , (6) and averages Eq. 6 over {0, . . . , T1} (or samples single per pair for an unbiased stochastic estimator). Equivalently, substituting Eq. 5 into Eq. 6 gives the explicit form (cid:104) LDiffusion-DPO = log σ (cid:16) β log pθ(xw pref(xw t+1, c) t+1, c) xw xw t+1, c) t+1, c) (cid:17)(cid:105) . (7) β log pθ(xl pref(xl xl xl Under common parameterizations, Eq. 5 reduces to simple residual comparisons. For DDPM-style Gaussians [13, 36], writing ˆϵθ = ϵθ(xt+1, c, t) for the predicted noise, ˆϵref = ϵref(xt+1, c, t) for the reference noise and ϵ for the groundtruth noise that forms xt, the log-ratio can be expressed as: log pθ(xt xt+1, c) pref(xt xt+1, c) (cid:13)ˆϵrefϵ(cid:13) (cid:13) (cid:13)ˆϵθ ϵ(cid:13) (cid:13) 2 2 2+1 1 2+const, (8) (cid:13) (cid:13) 2 2 and an analogous expression holds for velocity or flowmatching parameterizations by replacing the noise residual with the corresponding target [26]. For notational brevity, we write the stepwise contrastive objective as L(xt+1, c, t) = 1 2 ϵref(xt+1, c, t) ϵ2 2. Hence, the winner and loser margin loss are defined as t+1, c, t) and Ll = L(xl Lw = L(xw t+1, c, t), respectively. Substituting Eq. 8 into Eq. 7 gives the training loss 2 ϵθ(xt+1, c, t)ϵ2 2 ˆLDiffusion-DPO = t,ϵ,c,xw 0 ,xl 0 (cid:104) (cid:16) β(Lw Ll) log σ (cid:17)(cid:105) , (9) where = {(c, xw 0 , xl 0)} denotes the DPO training dataset. Limitations of Standard DPO. Substituting Eq. 8 into Eq. 7 yields an implementable objective whose inner term is the per-step error difference between winner and loser branches. DiffusionDPO [39] thus encourages decreasing the winners prediction error while increasing the losers at the same timestep. However, this objective does not guarantee monotonic decrease of the winner loss. Empirically, over-penalizing the loser can also worsen the preferred sample. In the left part of Fig. 1, the margin Lw Ll becomes increasingly negative, yet both Lw and Ll increase, indicating degradation of absolute performance and potential instability or collapse. This exposes gap between relative alignment (widening the margin) and absolute error control (preserving the preferred sample). The difficulty is that the winner and loser gradients are misaligned and vary across timesteps. We therefore introduce simple stepwise update that, to first order, guarantees the preferred loss does not increase at each step while still promoting margin expansion. 4. Method: Diffusion-SDPO (Safe DPO) We propose Diffusion-SDPO, novel preference optimization scheme that adds safety guard to the DPO update. The method adaptively scales the influence of the loser branch by time-dependent factor λt so that the preferred samples loss Lw does not increase after each parameter update. In practice, we follow the standard DiffusionDPO pipeline: given prompt and pair (xw 0), we compute the per-sample losses Lw and Ll at the same diffusion time t, and then modify the backpropagated update by multiplying the loser-branch gradient by the safety factor to enforce safe update condition. This directly addresses the limitation discussed above, because preventing any increase in the preferred loss ensures that preference-driven updates do not degrade the preferred output while still improving the preference margin. 0 , xl 4.1. Safe Update via First-Order Approximation Our objective is to ensure that gradient update driven by the preference loss (cf. Eq. 7) does not increase the winners loss. For clarity of exposition, consider linearized preference objective combining the two branches: Lpref(θ) = Lw(θ) λ Ll(θ), (10) where λ > 0 is scalar that adjusts the relative weight on the losers loss. Setting λ = 1 recovers the intuitive gradient direction of standard DPO (decrease Lw, increase Ll), while λ > 1 would place even more emphasis on penalizing the loser. Our goal is to find an upper bound on λ that guarantees Lw will not increase for an infinitesimal gradient step on Lpref. 2In practice, the actual Diffusion-DPO gradient (Eq. 6) includes logistic scaling factor σ() that multiplies the winner and loser gradients equally, thus not altering the update direction. We therefore analyze the simpler weighted difference objective that captures the same first-order direction. Let θLw and θLl denote the gradients of the winner and loser losses, respectively. gradient descent step of size η on Eq. 10 gives the parameter update: θ = η θLpref = η (cid:16) θLw λθLl(cid:17) . (11) The first-order change in the winners loss can be approximated by Taylor expansion: LwθLwθ =η (cid:16) θLw2 2λθLwθLl(cid:17) . (12) To prevent increase in Lw, we require Lw 0, i.e., θLwθ 0. Ignoring the trivial positive factor η, the safety condition becomes: θLw2 2 λθLwθLl 0. (13) Solving for λ yields bound on the allowable loser weight: λ θLw2 2 θLwθLl . (14) Notably, if the dot product θLwθLl is negative or zero, then Eq. 13 is automatically satisfied for any λ 0. In those cases, the update is intrinsically safe: the loser branch either helps reduce Lw or affects orthogonal parameter directions. The problematic scenario is when θLwθLl > 0, i.e., the losers gradient has component that would raise the winners loss. Eq. 14 then yields finite positive λ threshold. Any choice of λ above this threshold would violate the safety inequality, leading to Lw > 0 to first order. Conversely, choosing λ at or below this threshold ensures Lw 0 or negative, guaranteeing that the winners loss does not increase. 4.2. Closed-Form Safeguard in Output Space Directly evaluating the parameter-space bound in Eq. 14 is infeasible for high-dimensional model, since it would require computing and storing the full gradients θLw and θLl just to take their dot product. However, we can derive convenient proxy by considering gradients in the models output space. Modern diffusion models predict noise or image tensor as output, and the training loss (e.g., denoising score-matching loss [13]) is defined on this output. Let ow and ol denote the models output activations for the winner and loser branches respectively (for example, could be the predicted noise residual at certain diffusion step). Using the chain rule, we have θLw = woLw and θLl = oLl, where is the Jacobian o/θ and oL is the gradient of the loss with respect to the model output. Let gw = oLw and gl = oLl denote the outputspace gradients for the winner and the loser. Eq. 14 can then 4 . Table 2. Reward score comparison on the HPS V2 with SDXL. : results from our implementation due to the lack of official code. The full table is provided in Table 8. Algorithm 1: Training of Diffusion-SDPO. Input: Dataset = {(c, xw 0 , xl 0)}; model ϵθ; reference ϵref; safety slack µ [0, 1]; schedule length ; learning rate η. while not converged do t+1, c, t), t+1, c, t). 0 , xl t+1) from Eq. 2 and compute 1. Sample Uniform{0, . . . , 1}, ϵ (0, I), (c, xw 0) D. 2. Get (xw t+1, xl θ = ϵθ(xw ˆϵw ref = ϵref(xw ˆϵw θ = ϵθ(xl ˆϵl t+1, c, t), ref = ϵref(xl ˆϵl t+1, c, t), 3. Get per-branch residual objectives: 2 ˆϵw 2 1 2 ˆϵl 2 1 4. Compute λsafe using Eq. 17: 2/gwgl. λsafe = (1 µ)gw2 5. Scale only loser gradients: Ll detach + λsafe 6. Build loss using Eq. 9: LDPO = log σ (cid:0)β(Lw Ll 7. Update θ θ η θ LDPO. ref ϵ2 2, ref ϵ2 2. θ ϵ2 θ ϵ Lw = 1 Ll = 1 2 ˆϵw 2 ˆϵl scaled = Ll (cid:0)Ll Ll scaled)(cid:1). detach (cid:1) Output: Finetuned model ϵθ. : Ll detach is copy of Ll without gradient flow. be written as: λ θLw2 2 θLwθLl = gw(cid:0)J wJ w(cid:1)gw gw(cid:0)J wJ l(cid:1)gl (15) = gw2 2 gwgl gw(cid:0)J wJ w(cid:1)gw gw2 2 (cid:30)gw(cid:0)J wJ l(cid:1)gl gwgl . (16) (cid:124) (cid:123)(cid:122) ρ (cid:125) The factor ρ in Eq. 16 encodes local Jacobian geometry. Estimating ρ during training requires parameterspace backpropagation and adds memory usage or wallclock time (cf. Table 4). To keep the update lightweight, we do not track ρ explicitly. Instead, we absorb it into scalar safety slack µ [0, 1] that contracts the proxy in controlled way: λsafe = (1 µ) gw2 2 gwgl . (17) This removes any dependence on parameterspace Jacobians and uses only the outputspace gradients gw and gl that are already computed for the loss, so the additional cost is negligible. The slack µ serves as robust guardrail under arbitrary local geometry. Larger µ yields more conservative scaling of the loser branch; smaller µ recovers more aggressive update. In practice, we find fixed µ works well (see Fig. 4 for ablation on µ), and for an appropriate choice 5 Table 1. Reward score comparison on the HPS V2 with SD 1.5. Rows labeled + SDPO report the performance obtained by applying our SDPO to the corresponding base method in the preceding row. : results from our implementation due to the lack of official code. Best results are in bold. Owing to space constraints, the full table is provided in Table 7. Method PickScore() HPS() Aes.() CLIP() 0.3480 0.2088 SD 1.5 0.3591 SFT 0.2168 0.3420 Diff.-KTO 0.2164 MaPO 0.3528 0.2124 DPOP 0.3563 0.2144 0.3552 Diff.-DPO 0.2131 + SDPO 0.2174 0.3600 0.3598 0.2168 + SDPO 0.2172 0.3586 0.3551 0.2131 0.3612 + SDPO 0.2182 5.4933 5.7851 5.6288 5.6890 5.7071 5.6639 5.8744 5.8346 5.8474 5.6538 5.8574 0.2697 0.2838 0.2766 0.2760 0.2780 0.2743 0.2827 0.2837 0.2847 0.2766 0. DMPO DSPO IR() -0.0469 0.6619 0.5593 0.3308 0.3735 0.1705 0.6211 0.6483 0.6578 0.3171 0.7061 Method PickScore() HPS() Aes.() CLIP() 0.2290 0.3847 SDXL 0.3806 0.2228 SFT 0.3840 0.2293 MaPO 0.3840 Diff.-DPO 0.2288 + SDPO 0.2308 0.3879 0.3894 0.2273 + SDPO 0.2293 0.3889 0.3875 0.2302 0.3897 + SDPO 0.2308 6.1271 5.9689 6.1882 6.1380 6.1284 6.0424 6.1040 6.1101 6.1113 0.2900 0.2883 0.2934 0.2927 0.2938 0.2916 0.2944 0.2921 0. DMPO DSPO IR() 0.9047 0.8528 0.9703 1.0159 1.0326 1.0054 1.0745 1.0154 1.0521 of µ, the output-space scheme yields λsafe trajectories that closely match those obtained from parameter-space gradients (cf. Fig. 2). During training, we clip λsafe to [0, 1] for stability (if gwgl 0, we set λsafe = 1). Whenever the losers error vector has positive correlation with the winners error vector (gwgl > 0), λsafe provides finite limit to how strongly we can apply the losers gradient without risking an increase in the winners loss. For the logistic DPO objective, we implement this by scaling the backpropagated loser gradient with λsafe. Algorithm 1 summarizes the procedure to integrate SDPO into Diffusion-DPO. For other methods, λsafe is similarly used to scale the loser branch. 5. Experiments 5.1. Experimental Setting Datasets and Models. Following [22, 49], we finetune Stable Diffusion 1.5 (SD 1.5) and SDXL on preference pairs from Pick-a-Pic V2 (Pick V2) [19] training set. For evaluTable 3. Average win rate comparison (%) over the HPS V2 using SD 1.5. Each row reports Model 1 vs. Model 2 on identical prompts. The upper block summarizes SDPO augmentation results (base + SDPO vs. base), and the lower block compares each model against SD 1.5. Values > 50% indicate that Model 1 generally outperforms Model 2. PickScore HPS V2 Aes. CLIP ImageReward Mean Model 2 Model 1 SDPO augmentation effect (base+SDPO vs base) Diff.-DPO+ SDPO Diff.-DPO DSPO + SDPO DSPO DMPO + SDPO DMPO 66.12 52.62 73.50 Versus SD 1.5 Diff.-DPO SD 1.5 Diff.-DPO+ SDPO SD 1.5 DSPO SD 1.5 DSPO + SDPO SD 1.5 SD 1.5 DMPO DMPO + SDPO SD 1.5 76.38 80.75 78.38 81.75 68.50 82.25 ation, we use the test prompts from Pick V2, HPS V2 [42], and PartiPrompts [47]. Beyond SD 1.5 and SDXL, we also conduct experiments on Ovis-U1 [40] (3.6B), DiT [28] model trained in unified manner to support both text-toimage synthesis and image editing. To enable DPO finetuning on Ovis-U1, we construct mixed preference corpus that integrates text-to-image and editing pairs, totaling about 33K pairs. Training Details and Baselines. We integrate SDPO into Diffusion-DPO [39], DSPO [49], and DMPO [22] implementations and keep their official hyperparameters. All models are finetuned for 2000 steps with global batch size of 2048. The learning rate is 1 108 for SD 1.5 and 1 109 for SDXL. For the safeguard coefficient µ, on SD 1.5 we set 0.9 for Diffusion-DPO+SDPO and DMPO+SDPO, 0.2 for DSPO+SDPO. On SDXL, µ is fixed as 0.6 for all variants. We compare against several baselines: the original pretrained SD 1.5 and SDXL, supervised finetuning (SFT), Diffusion-KTO [23], MaPO [14], DPOP [27], and original Diffusion-DPO, DSPO, DMPO. For baselines we follow strict hierarchy. If official checkpoints are publicly available, we evaluate those directly. If checkpoints are unavailable but official code exists, we run the released implementation with the authors recommended settings. If neither is available, we reimplement the method from the paper. Evaluation. We evaluate models on automatic preference metrics, including PickScore [19], HPS V2 [42], LAION Aesthetic Classifier [34], CLIP [30] and ImageReward [44] scores. Sampling uses guidance scale of 7.5 and 50 denoising steps. For Ovis-U1, we additionally evaluate structured text-to-image alignment on GenEval [10] and DPG-Bench [15], as well as image-editing performance on ImgEdit [46] and GEdit-EN [25]. 78.62 53.00 79.25 69.50 86.25 86.00 89.75 74.50 88.12 70.62 53.25 67.12 66.88 83.38 78.75 79.12 68.38 79.25 52.50 48.50 53.12 57.50 56.88 58.88 56.75 53.00 58. 71.62 52.38 71.50 63.50 79.25 79.75 80.12 69.88 81.75 67.90 51.95 68.90 66.75 77.30 76.35 77.50 66.85 77.95 Figure 2. Training dynamics of λsafe on SD 1.5 (left) and Ovis-U1 (right) with two computation schemes (using output-space gradients vs. parameter-space gradients). The trajectories closely match throughout training, and the output-space variant requires substantially less computation while maintaining comparable aesthetic rewards (see Table 4). 5.2. Main Results Table 1,2 show that adding SDPO to Diffusion-DPO, DSPO, and DMPO consistently improves automatic reward metrics under SD 1.5 and SDXL, with DMPO+SDPO typically giving the best overall scores. Win-rate results on SD 1.5  (Table 3)  further confirm that each base method benefits from SDPO and that SDPO variants also outperform the SD 1.5 baseline, indicating stronger preference alignment without loss of quality. On SDXL, the gains are moderate yet consistent (see Table 9), suggesting reliable scaling to larger UNet [33] backbones. On the unified Ovis-U1 model  (Table 5)  , SDPO yields clear improvements in preference metrics and editing scores, demonstrating effectiveness on DiT backbone as well. While naive Diffusion-DPO can enlarge preference margins at the expense of fidelity, our safeguarded integrations preserve details and improve prompt adherence across diverse prompts. The visual evidence (cf. Fig. 5, 7, 8) aligns with the quantitative trends, indicating that SDPO stabilizes optimization and enhances perceptual quality. 6 Table 4. Ablation results for winner-preserving rules on SD 1.5 (prompts: HPS V2). We report PickScore/HPS V2, one-step training Time (s) and peak GPU memory (GB) on single NVIDIA A100 with batch size 16 and 128 gradient accumulation steps in BF16. : fixed λsafe in SDPO. : λsafe computed with parameterspace gradients. PickS. () HPS () Time () GPU Mem. () Method MaPO 0.2124 DPOP 0.2144 0.2131 Diff.-DPO Diff.-DPO+SDPO 0.2158 Diff.-DPO+SDPO 0.2176 Diff.-DPO+SDPO 0. 0.2760 0.2780 0.2743 0.2803 0.2828 0.2827 162 187 183 183 314 184 53.8 58.5 57.0 57.0 63.3 57.1 5.3. Ablation Study Modular Ablation & Computational Cost. Table 4 compares winner preserving strategies when embedded into MaPO, DPOP, and Diffusion-DPO. MaPO applies fixed winner weight and removes the reference model, which weakens calibration of absolute error though it requires less computation and GPU memory. DPOP protects the winner through thresholded update filtering, but this rule was designed for autoregressive language models and does not fully match diffusion training dynamics. Our SDPO preserves the winner by rescaling the loser update with safeguard coefficient λsafe selected in the output space according to directional alignment. fixed λsafethat is, holding λsafe constant throughout training (see Table 4, row 4)already improves over MaPO and DPOP on PickScore and HPS, whereas allowing λsafe to adapt during training yields further gains. These improvements support the hypothesis that outputspace selection of λsafe stabilizes the winner while maintaining pressure to enlarge the preference margin. Fig. 2 compares two mechanisms for computing the dynamic λsafe: using output-space gradients and using parameter-space gradients. The output-space trajectory closely matches the parameter-space trajectory in both level and trend, indicating similar effectiveness. Because it reuses signals from the standard backward pass and only performs lightweight reductions, the output-space variant adds essentially no runtime or memory overhead relative In contrast, as shown in to the Diffusion-DPO baseline. Table 4, computing λsafe from parameter-space gradients requires extra vector-Jacobian products and temporary buffers for per-parameter inner products, increasing training time by about 72% and peak memory by about 11%. Note that the two curves in Fig. 2 use different values of µ, which indicates that by adjusting µ, the output-space scheme can approximate the behavior of parameter-space estimation. Although computing λsafe in the output space is an approximation, our empirical results show that proper tuning of µ effectively compensates for this mismatch, al7 lowing the scaled gradients to closely match the parameterspace formulation while retaining the same training efficiency. Why does SDPO generalize across DPO variants? Fig. 3 contrasts the training dynamics of Diff.-DPO, DSPO, and DMPO with or without SDPO. Without SDPO, Lw Ll decreases as expected, whereas Lw remains nondecreasing and drifts upward in Diff.-DPO and DMPO, indicating unstable optimization. With SDPO, Lw drops early and remains low, Ll declines smoothly without overshoot, and Lw Ll decreases steadily to plateau. For DSPO, which already regularizes branch imbalance via its score preference objective and progressively increases the weight on the winner branch, adding SDPO causes no degradation and typically yields slightly improved reward results (cf. Table 1). We observe shared qualitative profile across the three SDPO-augmented settings: after basic rescaling, trajectories from different objectives largely overlap. Lw follows monotone, fast-then-slow descent, Ll descends smoothly, and their gap grows in stable manner across timesteps. This empirical regularity suggests that SDPO successfully corrects harmful update directions and magnitudes by acting on gradient geometry rather than on particular objective form, thereby normalizing training dynamics across DPO variants, preserving the preferred branch, and stabilizing preference alignment. Sensitivity of Hyperparameters. Fig. 4 indicates that the safeguard slack µ admits broad, gently convex optima rather than sharp tuning. The resulting flat plateaus for both HPS V2 and PickScore suggest that SDPOs behavior is governed primarily by gradientalignment geometry in output space, rather than by the specific loss form. On SDXL, all SDPOaugmented objectives maintain nearoptimal performance over wide interval centered around µ 0.6, consistent with smoother, highercapacity landscape. On SD 1.5, the more aggressive baselines (DiffusionDPO and DMPO) benefit from larger slack µ, which further contracts the loser contribution whenever its direction conflicts with the winner. In contrast, DSPO already progressively assigns greater weight to the winner branch during training, so smaller slack is sufficient. Practically, µ can be chosen by an earlyphase heuristic: increase µ if the winner loss Lw drifts upward or oscillates; decrease µ in small steps if the preference margin stalls despite stable Lw. Following this rule of thumb, we adopt single default on SDXL (µ = 0.6 for all objectives) and two defaults on SD 1.5 (µ = 0.9 for DiffusionDPO and DMPO; µ = 0.2 for DSPO). Table 5. Comparison of Ovis-U1 [40] variants on preference, structured alignment, and image editing benchmarks. Higher is better (). SDPO is particularly effective for preference alignment in large-scale models. Model Preference Eval () Structured Alignment Eval () Image Editing () CLIP ImgEdit GEdit-EN 0.3188 Ovis-U1 Ovis-U1 + DPO 0.3192 Ovis-U1 + SDPO 0.3201 HPS V2 GenEval 0.2986 0.2997 0.3082 DPG-Bench 83.72 83.78 84. 4.00 4.01 4.11 6.42 6.43 6.60 0.89 0.88 0.89 Figure 3. Training dynamics across three objectives with and without SDPO on SD 1.5. SDPO preserves gains with longer training. We further examine whether the safeguarding mechanism remains effective under extended training  (Fig. 6)  . The baseline exhibits nonmonotonic trajectory: both HPS V2 and PickScore increase early, then plateau or decline as training proceeds. In contrast, augmenting Diffusion-DPO with SDPO yields continued gains that stabilize at higher level, widening the gap over the baseline as steps increase. Qualitative examples in Fig. 1 corroborate this trend: baseline generations develop saturation and texture artifacts under prolonged training, whereas SDPO maintains crisp structure and consistent style. Overall, SDPO sustains improvements in preference metrics without sacrificing perceptual quality during longer training. This follows from its winnerpreserving constraint, which limits the influence of loserbranch gradients and ensures that the winner loss does not increase. Without SDPO, the growing contribution of the loser branch can raise the winner loss and is often accompanied by degraded visual quality. 6. Conclusions and Limitations In this paper, we presented Diffusion-SDPO, safeguarded preference optimization scheme that stabilizes DPO-style diffusion finetuning by preserving the preferred branch while improving preference matching. The method scales the loser gradient by its alignment with the winner and guarantees, to first order, that the winners reconstruction loss does not increase. Across SD 1.5, SDXL (both UNets), and Ovis-U1 (DiT), our method yields consistent improvements on automated preference, aesthetic, and prompt-alignment metrics with negligible computational overhead, while remaining model-agnostic, straightforward to implement, and applicable to multiple DPO variants. However, the safeguard is derived from first-order approximation, and its validity weakens when the loss landscape exhibits strong curvature. Moreover, the coefficient ρ (cf. Eq. 16) used to estimate gradient alignment can be noisy or biased, which leads to imperfect scaling of loser gradients. Nevertheless, our empirical results indicate that with suitably chosen µ, this estimation is accurate enough in practice and the safeguard behaves as intended. Future work includes developing second-order or trust-region safeguards to maintain robust winner preservation under diverse training dynamics."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, et al. Qwen-VL: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv:2308.12966, 2023. 12 [2] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. 3 [3] Jay Hyeon Cho, JunHyeok Oh, Myunsoo Kim, and ByungJun Lee. Rethinking DPO: The role of rejected responses in preference misalignment. arXiv:2506.12725, 2025. 3 [4] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust DPO: aligning language models with In International Conference on Machine noisy feedback. Learning, pages 42258 42274, 2024. 3 [5] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement In Advances in Neural learning from human preferences. Information Processing Systems, pages 43024310, 2017. 1 [6] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9):1085010869, 2023. 1, 2 [7] Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, pages 87808794, 2021. 2 8 Figure 4. Sensitivity of SDPO to hyperparameter µ measured by HPS V2 and PickScore across SD 1.5 and SDXL on HPS V2 prompt set. Figure 5. Qualitative comparison of different methods using SD 1.5. Prompt: 1) The Little Prince and the fox in Tim Burton style artwork. 2) futuristic modern house on floating rock island surrounded by waterfalls, moons, and stars on an alien planet. See Fig. 7 for more results. Figure 6. Comparison of DiffusionDPO with and without SDPO under longer training on SD 1.5 (Test prompts: Pick V2). [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, et al. Scaling rectified flow Transformers for high-resolution image synthesis. In International Conference on Machine Learning, pages 1260612633, 2024. 2 [9] Alexander Gambashidze, Anton Kulikov, Yuriy Sosnin, and Ilya Makarov. Aligning diffusion models with noiseconditioned perception. arXiv:2406.17636, 2024. 3 [10] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. GENEVAL: An object-focused framework for evaluating text-to-image alignment. In Advances in Neural Information Processing Systems, 2023. 6, 12 [11] Google. Introducing gemini 2.5 flash image (aka nanobanana). https://developers.googleblog.com/ en/introducinggemini25- flashimage/, 2025. 1 [12] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv:2207.12598, 2022. 2 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising difIn Advances in Neural Inforfusion probabilistic models. mation Processing Systems, pages 6840 6851, 2020. 2, 3, 4 [14] Jiwoo Hong, Sayak Paul, Noah Lee, Kashif Rasul, James Thorne, and Jongheon Jeong. Margin-aware preference optimization for aligning diffusion models without reference. In International Conference on Learning Representations Workshop, 2025. 1, 2, 3, [15] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. EllA: Equip diffusion models with LLM for enhanced semantic alignment. arXiv:2403.05135, 2024. 6, 12 [16] Audrey Huang, Wenhao Zhan, Tengyang Xie, Jason Lee, Wen Sun, Akshay Krishnamurthy, and Dylan Foster. Correcting the mythos of kl-regularization: Direct alignment without overparameterization via chi-squared preference optimization. In International Conference on Learning Representations, pages 116, 2025. 3 [17] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning, pages 1036210383, 2022. 2 [18] Tero Karras, Miika Aittala, Samuli Laine, and Timo Aila. Elucidating the design space of diffusion-based generative models. In Advances in Neural Information Processing Systems, pages 26565 26577, 2022. 2 [19] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-Pic: An open dataset of user preferences for text-to-image generation. In Advances in Neural Information Processing Systems, pages 36652 36663, 2023. 5, 6, 12 [20] Black Forest Labs. https : / / huggingface . co / black - forest - labs / FLUX . 1-dev, 2024. 1, 2, FLUX.1-dev. [21] Guangchen Lan, Sipeng Zhang, Tianle Wang, et al. MaPPO: Maximum posteriori preference optimization with prior knowledge. arXiv:2507.21183, 2025. 3 [22] Binxu Li, Minkai Xu, Meihua Dang, and Stefano Ermon. Divergence minimization preference optimization for diffusion model alignment. arXiv:2507.07510, 2025. 1, 2, 5, 6 [23] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, and Kazuki Kozuka. Aligning diffusion models by optimizing human utility. In Advances in Neural Information Processing Systems, pages 24897 24925, 2025. 1, 6 [24] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In International Conference on Learning Representations, pages 113, 2023. 2 9 [25] Shiyu Liu, Yucheng Han, Peng Xing, et al. Step1XEdit: practical framework for general image editing. arXiv:2504.17761, 2025. 6 [26] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, pages 115, 2023. 2, 3 [27] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv:2402.13228, 2024. 1, 3, 6 [28] William Peebles and Saining Xie. Scalable diffusion models with Transformers. In IEEE/CVF International Conference on Computer Vision, pages 41724182, 2023. 2, 6, 11 [29] Dustin Podell, Zion English, Kyle Lacey, et al. SDXL: Improving latent diffusion models for high-resolution image synthesis. In International Conference on Learning Representations, pages 113, 2024. 2 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763, 2021. 6, [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: your language model is secretly reward model. In Advances in Neural Information Processing Systems, pages 5372853741, 2023. 1, 2, 3 [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10674 10685, 2022. 2 [33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234241, 2015. 2, 6 [34] Christoph Schuhmann, Romain Vencu, Romai Beaumont, et al. LAION-Aesthetics: Predicting the aesthetic quality of images. https://laion.ai/blog/laionaesthetics/, 2022. 6, 12 [35] Shivanshu Shekhar, Shreyas Singh, and Tong Zhang. SEEDPO: Self entropy enhanced direct preference optimization. Transactions on Machine Learning Research, 2025. 1, 3 [36] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, page 22562265, 2015. 3 [37] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, pages 112, 2022. [38] Dipesh Tamboli, Souradip Chakraborty, Aditya Malusare, Biplab Banerjee, Amrit Singh Bedi, and Vaneet Aggarwal. BalancedDPO: Adaptive multi-metric alignment. arXiv:2503.12575, 2025. 3 [39] Bram Wallace, Meihua Dang, Rafael Rafailov, et al. Diffusion model alignment using direct preference optimization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 1, 2, 3, 4, 6 [40] Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, et al. OvisU1 technical report. arXiv:2506.23044, 2025. 2, 6, 8, 17 [41] Rui Wang, Qianguo Sun, Chao Song, Junlong Wu, Tianrong Chen, Zhiyun Zeng, and Yu Li. Linear preference optimization: Decoupled gradient control via absolute regularization. arXiv:2508.14947, 2025. 3 [42] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human Preference Score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv:2306.09341, 2023. 6, 12 [43] Teng Xiao, Yige Yuan, Huaisheng Zhu, Mingxiao Li, and Vasant Honavar. Cal-DPO: Calibrated direct preference In Advances optimization for language model alignment. in Neural Information Processing Systems, pages 114289 114320, 2024. 1, 3 [44] Jiazheng Xu, Xiao Liu, Yuchen Wu, et al. ImageReward: Learning and evaluating human preferences for textIn Advances in Neural Information to-image generation. Processing Systems, pages 1590315935, 2024. 6, 12 [45] Chenxu Yang, Ruipeng Jia, Naibin Gu, et al. Orthogonal finetuning for direct preference optimization. arXiv:2409.14836, 2024. 3 [46] Yang Ye, Xianyi He, Zongjian Li, et al. Imgedit: unified image editing dataset and benchmark. arXiv:2505.20275, 2025. 6 [47] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv:2206.10789, 2022. 6 [48] Wayne Xin Zhao, Kun Zhou, Junyi Li, et al. survey of large language models. arXiv:2303.18223, 2023. 3 [49] Huaisheng Zhu, Teng Xiao, and Vasant Honavar. DSPO: Direct score preference optimization for diffusion model In International Conference on Learning Repalignment. resentations, pages 113, 2025. 1, 2, 3, 5, A. Second-Order Considerations of SDPO Our theoretical guarantee for Diffusion-SDPO is explicitly first order: it controls the sign of the linear term in the Taylor expansion of the winner loss. In practice, the true change in Lw after an update is Lw = θLwθ + 1 2 θH wθ + O(θ3), (18) where is the Hessian of Lw and θ is the parameter update induced by the SDPO objective. The analysis in main text ensures that the first-order term θLwθ is nonpositive under the safe choice of λ. However, the quadratic term 1 2 θH wθ can in principle be positive when local curvature is large, so Lw might still increase slightly when the step is not infinitesimal. In other words, SDPO controls the direction of the update with respect to the gradient of Lw, while the Hessian governs how quickly the loss can bend back upward along that direction. The role of the slack parameter µ to compute λsafe can then be understood as an implicit trust-region style safeguard. Recall that the parameter-space analysis yields an upper bound on λ, and our output-space scheme uses λsafe = (1 µ)gw2 2 gwgl , (19) which shrinks the allowable contribution of the loser branch whenever gwgl > 0. Multiplying by (1 µ) reduces the contribution of the loser gradient in the update. In the regime where θLwθLl > 0, the first-order change in Eq. 20, LwθLwθ =η (cid:16) θLw2 2λθLwθLl(cid:17) , (20) is monotonically increasing as function of λ (note that both λ and η are > 0), so replacing λ by (1 µ)λ makes Lw strictly smaller. At the same time, we can make the dependence on the loser direction explicit. The SDPO update can be written as θ(λ) = η(cid:0)θLwλθLl(cid:1) = η θLw +ηλ θLl (cid:124) (cid:123)(cid:122) (cid:125) (cid:125) loser-induced part (cid:123)(cid:122) θ(0) (cid:124) . (21) The term ηλ θLl is the λ-dependent increment of the update along the loser direction. Replacing λ by (1 µ)λ scales this increment to η(1 µ)λ θLl, so its norm is multiplied by (1 µ) while the baseline part θ(0) is unIn particular, the portion of the step norm that changed. is directly attributable to the loser branch contracts linearly with (1 µ). The quadratic term then decomposes as θ(λ)H wθ(λ) = θ(0)H wθ(0) + 2ηλ (θLl)H wθ(0)+η2λ2 (θLl)H wθLl, (22) where the last two terms capture the curvature contribution associated with the loser direction. Under mild local spectral bound w2 Λ, we can control the quadratic term via 2 Λ v2 2, (cid:12) w2 v2 (23) for any vector v. Applying this with = θ(cid:0)(1 µ)λ(cid:1) and using the decomposition in Eq. (21) yields wθ(cid:0)(1 µ)λ(cid:1)(cid:12) (cid:12) (cid:12) (24) (cid:12) (cid:12)vH wv(cid:12) (cid:12) 2 θ(cid:0)(1 µ)λ(cid:1) (cid:12) (cid:12) 1 2 Λ(cid:13) (cid:13)θ(cid:0)(1 µ)λ(cid:1)(cid:13) 2 (cid:13) 2 2 Λ(cid:13) (cid:13)θ(0) + η(1 µ)λ θLl(cid:13) 2 (cid:13) 2 (cid:16)(cid:13) (cid:13)θLl(cid:13) (cid:13)2 + η(1 µ)λ (cid:13) (cid:13)θ(0)(cid:13) (cid:13)2 (cid:17)2 (cid:16)(cid:13) (cid:13)θLl(cid:13) (cid:13)2 + ηλ (cid:13) (cid:13)θ(0)(cid:13) (cid:13)2 2 Λ 2 Λ . = 1 1 1 (cid:17)2 (25) (26) (27) (28) 11 The last line coincides with the corresponding spectralnorm bound obtained for θ(λ) before contraction. Since (1 µ)λ λ for µ [0, 1], the above inequalities show that the curvature contribution under the contracted update is controlled by an upper bound that is no larger, and typically strictly smaller, than the original one. We therefore claim that SDPO shrinks the worst-case curvature impact in the loser direction, making it less likely that higher-order effects overturn the negative first-order change. This perspective also clarifies the interaction between SDPO and the base optimizer. In our experiments we use small learning rates and large gradient accumulation, so the effective θ per update is modest even for highcapacity backbones. In this regime, the second-order contribution behaves as small perturbation around the controlled first-order term. When combined with the slack contraction, this explains why we observe almost monotone or gently decreasing Lw trajectories, rather than the oscillatory behavior that would indicate strong curvature dominating the dynamics. Empirically, we can also see second-order effects indirectly through the sensitivity of SDPO to µ. If curvature induced large and frequent violations of the first-order approximation, performance would vary sharply with small changes in µ because the balance between the linear and quadratic terms would be fragile. Instead, we observe broad plateaus where both HPS V2 and PickScore remain nearoptimal over wide intervals of µ. This suggests that, in the regions visited during training, the winner loss is reasonably well approximated by its first-order geometry along SDPO updates, and the quadratic term acts as small correction rather than dominant force. Finally, the approximate match between parameter-space and output-space trajectories offers further evidence that higher-order interactions are not pathological in practice. The parameter-space variant implicitly incorporates more of the true curvature structure, yet its behavior can be closely reproduced by the cheaper output-space scheme with suitably chosen slack. This indicates that most of the relevant geometry is already captured by the alignment between gw and gl, and that the remaining second-order discrepancy can be effectively absorbed into scalar safety margin. fully second-order SDPO variant that explicitly constrains θH wθ would be an interesting direction for future work, but our results suggest that the present firstorder safeguard, combined with the slack µ, already offers favorable trade-off between theoretical control, computational cost, and empirical stability. B. Extension: SDPO on FLUX.1-dev Model. To further test the generality of SDPO, we also apply it to FLUX.1-dev [20], 12B-parameter rectifiedflow DiT [28] model designed for text-to-image generation. Table 6. Full reward and structured alignment results for FLUX.1-dev. We report automatic rewards (PickScore, HPS V2, LAION Aesthetics, CLIP, and ImageReward) as well as structured alignment metrics (GenEval and DPG-Bench) for the base model and its preferencealigned variants. The row marked with corresponds to Diff.-DPO trained with 0.01 learning rate. Method FLUX.1-dev Diff.-DPO Diff.-DPO Diff.-DPO + SDPO PickScore() HPS V2() Aesthetics() CLIP() ImageReward() GenEval() DPG-Bench() 0.2252 0.1998 0.2253 0.2314 0.2889 0.2467 0.2881 0.2991 6.0735 5.2857 6.0701 6.1910 0.3498 0.2573 0.3501 0.3566 0.9916 -0.7788 0.9938 1.1108 0.66 0.23 0.66 0. 85.75 58.97 85.78 86.36 FLUX.1-dev operates in latent space with flow-matching objective rather than the noise-prediction objective used in SD 1.5 and SDXL, but it still exposes time-conditioned image generator whose outputs can be scored by the same preference metrics. In our experiments, we start from the publicly released FLUX.1-dev checkpoint and keep the official tokenizer, text encoder, VAE, and sampling configuration unchanged. We integrate Diffusion-DPO and SDPO at the loss level only, treating the models velocity (or denoising) prediction at each time step as the output on which we define the per-step winner and loser residuals. Datasets. For DPO training on FLUX.1-dev, we construct an in-house preference dataset of 186K high-quality DPO examples. The images and texts cover broad range of domains (portraits, everyday objects, indoor and outdoor scenes, concept art, products, etc.), making the corpus suitable for general-purpose alignment rather than narrow style or category. To improve the textual side of supervision, we apply visionlanguage model (Qwen-VL-Max [1]) to re-caption short, noisy, or underspecified prompts, yielding richer and more semantically faithful descriptions while preserving the original user intent. Training Details. We finetune FLUX.1-dev with Diffusion-DPO and with our SDPO-augmented variant using TorchTitan in hybrid sharded data-parallel (HSDP) configuration. Within each 8-GPU group we apply fully sharded data parallelism (FSDP) to partition model parameters, and we use 2-way data parallelism across groups, resulting in 16 NVIDIA A100 GPUs in total. Training is performed at resolution of 1024 1024 with global batch size of 512 and learning rate of 1 105. For the DPO temperature, we set βDPO = 1000. For SDPO, we choose slack of µ = 0.99, which empirically yields effective safe scaling coefficients λsafe 0.1 at typical training steps. Under this configuration, finetuning completes in approximately three days on 16 A100 GPUs. Evaluation. We evaluate FLUX.1-dev using the Pick-aPic V2 [19] prompt sets with automatic reward metrics (PickScore [19], HPS V2 [42], LAION Aesthetics [34], CLIP [30], and ImageReward [44]), and additionally report its structured alignment performance on GenEval [10] and DPG-Bench [15]. During inference, we adopt 50-step sampler and follow the FLUX.1-dev model card by using classifier-free guidance scale of 3.5 for all variants to ensure fair comparison. Results. Table 6 reveals clear three-regime behavior on FLUX.1-dev. At the default learning rate (1 105), naive Diffusion-DPO catastrophically degrades the model: all five reward metrics drop, and both GenEval and DPG-Bench scores collapse. This suggests that on high-capacity rectified-flow backbone, aggressively enlarging the preference margin without any safeguard on the winner branch can drive the parameters far outside the local basin of the pretrained solution. When we instead reduce the learning rate by factor of 0.01 (Diff.-DPO with learning rate 1 107), the collapse disappears and all metrics revert to nearly the base FLUX.1dev level, indicating that the optimization has become so conservative that it behaves almost like an identity map and fails to extract meaningful gains from the preference supervision. Taken together, these two extremes highlight practical tension when directly applying Diffusion-DPO to large rectified-flow models: in naive settings, one tends to end up either in an aggressive regime that destabilizes the pretrained generator, or in an overly conservative regime where the model remains effectively unchanged. In contrast, Diff.-DPO + SDPO improves over the base model on all reported metrics while keeping the original, high learning rate. PickScore, HPS V2, Aesthetics, CLIP, and ImageReward all increase, and both GenEval and DPGBench scores also rise, indicating that SDPO not only enhances perceptual quality but also strengthens structured textimage alignment. C. Other Experimental Results We report the full reward score comparisons on Pick-aPic V2, HPS V2, and PartiPrompts in Tables 7 and 8, and summarize the win-rate comparison of SDXL models in Table 9. Across all datasets, adding SDPO on top of Diffusion12 DPO, DSPO, or DMPO consistently shifts the metric profile in the same direction as in the main text: PickScore, HPS V2, and ImageReward improve while aesthetic scores are preserved or slightly enhanced, indicating that SDPO strengthens preference alignment without paying quality penalty. Additional qualitative examples in Fig. 7 and Fig. 8 further illustrate this effect on both UNet and DiT backbones, showing sharper details, more faithful layouts, and fewer artifacts compared to their non-SDPO counterparts. Finally, Fig. 9 visualizes the temporal dynamics: as training progresses, baseline Diffusion-DPO drifts toward oversaturated or distorted images, illustrating how the winner loss can keep increasing even while the winner-loser margin becomes more negative when optimization focuses purely on enlarging the margin. In contrast, SDPO maintains coherent structure and style across steps, thanks to its winnerpreserving update rule and the stabilizing effect of the safe scaling λsafe. Taken together, these quantitative and qualitative results support our central claim that SDPO acts as plug-in, geometry-aware safeguard that consistently improves preference metrics while maintaining, and in many cases enhancing, the visual quality of diffusion generations. 13 Table 7. Full results of reward score comparison on Pick-a-Pic V2, HPS V2, and PartiPrompts using SD 1.5. : results from our implementation due to the lack of official code. Dataset Pick V2 HPS V2 PartiPrompts Method SD 1.5 SFT Diff.-KTO MaPO DPOP Diff.-DPO + SDPO DSPO + SDPO DMPO + SDPO SD 1.5 SFT Diff.-KTO MaPO DPOP Diff.-DPO + SDPO DSPO + SDPO DMPO + SDPO SD 1.5 SFT Diff.-KTO MaPO DPOP Diff.-DPO + SDPO DSPO + SDPO DMPO + SDPO PickScore() 0.2073 0.2128 0.2126 0.2097 0.2119 0.2109 0.2143 0.2131 0.2135 0.2110 0.2144 0.2088 0.2168 0.2164 0.2124 0.2144 0.2131 0.2174 0.2168 0.2172 0.2131 0.2182 0.2144 0.2181 0.2178 0.2152 0.2169 0.2167 0.2187 0.2178 0.2185 0.2163 0.2190 HPS() 0.2651 0.2765 0.2766 0.2702 0.2726 0.2690 0.2772 0.2769 0.2777 0.2710 0.2784 0.2697 0.2838 0.2766 0.2760 0.2780 0.2743 0.2827 0.2837 0.2847 0.2766 0.2848 0.2724 0.2821 0.2820 0.2754 0.2782 0.2755 0.2815 0.2819 0.2832 0.2775 0.2831 Aesthetics() 5.3907 5.6888 5.6288 5.5572 5.5688 5.4958 5.7172 5.6825 5.6917 5.5434 5.7312 5.4933 5.7851 5.6288 5.6890 5.7071 5.6639 5.8744 5.8346 5.8474 5.6538 5.8574 5.3466 5.5981 5.5630 5.4754 5.4894 5.4045 5.5880 5.5997 5.5975 5.4724 5.5956 CLIP() 0.3299 0.3408 0.3420 0.3365 0.3389 0.3357 0.3458 0.3428 0.3441 0.3382 0.3469 0.3480 0.3591 0.3420 0.3528 0.3563 0.3552 0.3600 0.3598 0.3586 0.3551 0.3612 0.3343 0.3389 0.3416 0.3366 0.3383 0.3391 0.3423 0.3385 0.3405 0.3388 0.3430 Image Reward() -0.1376 0.5767 0.5593 0.2435 0.3259 0.1020 0.5546 0.5642 0.5916 0.2813 0.6369 -0.0469 0.6619 0.5593 0.3308 0.3735 0.1705 0.6211 0.6483 0.6578 0.3171 0.7061 0.0637 0.5830 0.5697 0.3358 0.3644 0.2560 0.5425 0.5640 0.5955 0.3653 0.6381 Table 8. Full results of reward score comparison on Pick-a-Pic V2, HPS V2, and PartiPrompts using SDXL. : results from our implementation due to the lack of official code. Dataset Pick V2 HPS V2 PartiPrompts Method SDXL SFT MaPO Diff.-DPO + SDPO DSPO + SDPO DMPO + SDPO SDXL SFT MaPO Diff.-DPO + SDPO DSPO + SDPO DMPO + SDPO SDXL SFT MaPO Diff.-DPO + SDPO DSPO + SDPO DMPO + SDPO PickScore() 0.2242 0.2183 0.2242 0.2251 0.2257 0.2228 0.2240 0.2253 0.2263 0.2290 0.2228 0.2293 0.2288 0.2308 0.2273 0.2293 0.2302 0.2308 0.2277 0.2221 0.2278 0.2279 0.2290 0.2261 0.2268 0.2286 0.2296 HPS() 0.2846 0.2809 0.2871 0.2868 0.2876 0.2834 0.2871 0.2869 0.2882 0.2900 0.2883 0.2934 0.2927 0.2938 0.2916 0.2944 0.2921 0.2933 0.2880 0.2834 0.2902 0.2900 0.2907 0.2871 0.2897 0.2904 0.2913 Aesthetics() 5.9970 5.7922 6.0979 6.0115 5.9812 5.8797 5.9529 6.0119 5.9990 6.1271 5.9689 6.1882 6.1380 6.1284 6.0424 6.1040 6.1101 6.1113 5.7901 5.6496 5.8921 5.8294 5.7882 5.6947 5.7931 5.8273 5.8103 CLIP() 0.3684 0.3658 0.3684 0.3732 0.3746 0.3756 0.3761 0.3716 0.3770 0.3847 0.3806 0.3840 0.3840 0.3879 0.3894 0.3889 0.3875 0.3897 0.3591 0.3559 0.3580 0.3629 0.3645 0.3664 0.3664 0.3610 0. Image Reward() 0.7382 0.5974 0.8359 0.8357 0.8840 0.8818 0.9238 0.8555 0.9548 0.9047 0.8528 0.9703 1.0159 1.0326 1.0054 1.0745 1.0154 1.0521 0.8573 0.7515 0.9324 1.0638 1.0654 1.0514 1.1012 0.9558 1.0623 Table 9. Average win rate comparison (%) over the HPS V2 using SDXL. Pick HPS V2 Aesth. CLIP ImageReward Mean Model 2 Model 1 SDPO augmentation effect (base+SDPO vs base) Diff.-DPO+ SDPO Diff.-DPO 64.62 53.37 62.62 64.25 DSPO + SDPO DSPO 53.75 55.88 DMPO + SDPO DMPO Versus SDXL Diff.-DPO SDXL Diff.-DPO+ SDPO SDXL SDXL DSPO + SDPO SDXL DSPO DMPO SDXL DMPO + SDPO SDXL 48.25 59.25 37.88 54.63 58.63 60.88 63.38 66.75 57.00 68.00 64.88 68.50 15 47.75 59.75 52.25 52.50 49.12 40.25 44.62 46.00 47.63 57.75 48.62 55. 48.38 55.00 55.63 55.00 53.62 58.38 51.88 58.13 56.00 59.13 58.00 56.87 65.12 62.00 68.63 55.08 58.67 54.68 54.33 57.63 49.53 57.47 57.03 60.80 Figure 7. Qualitative comparison of images generated by different methods using SD 1.5. Prompt: 1) hyper-realistic landscape from Neil Blomkamp film featuring crashed spaceship, detailed grass, and photorealistic sky. 2) landscape featuring mountains, valley, sunset light, wildlife and gorilla, reminiscent of Bob Rosss artwork. 3) stylized portrait featuring sliced coconut, electronics, and AI in cartoonish cute setting with dramatic atmosphere. 4) tonalist painting of bipedal pony creature soldier. 5) praying mantis nun in grassy field during sunset. 6) comic book cover featuring superhero named Eagle Man with an eagle mask and wing logo, resembling traditional comic book cover. 7) Two motorcycles sit on the side of secluded road. 8) Yoko Ono flying on broomstick with lightning in the skies. 16 Figure 8. Qualitative comparison of images generated by Ovis-U1 [40] and its finetuned variants. Results are reported for three variants: the base, finetuned with DPO, and finetuned with our SDPO. 17 Figure 9. Qualitative comparison of generations from preference-aligned models without SDPO (left) and with SDPO (right). Each row corresponds to fixed prompt, and within each panel the training step increases from left to right. Without SDPO, the baseline gradually drifts and produces saturated, distorted, or low-fidelity images as the loser gradients degrades the winner branch. In contrast, SDPO keeps the winner branch stable, preserving structure, style, and prompt alignment even at later training steps. Prompts: 1) career woman on suits. 2) portrait of beautiful female space warrior in dense forest, by fiona staples, bold colors, dynamic composition, bright saturated hues, strong constrasts, vibrant, energetic, elaborate, hyper detailed, visually stunning and captivating art style. 3) pink-haired woman looking straight ahead, full lips, white military clothing with small red details, blue sky with blurred clouds, Chromatic Aberration, Geometric Shape, Photorealistic, Cosmic, Detailed, Bloom, masterpiece, best quality, extremely detailed CG unity 8k wallpaper, landscape, 3D Digital Paintings, award winning photography, Photorealistic, trending on artstation, trending on CGsociety, Intricate, High Detail, dramatic, high quality lighting, vivid anime color. 4) set of emerald bracelets in green in display box at the auction, uplight, very realist, very detailed, highest resolution, hyper realistic. 5) big mansion in the daytime. 6) two story house, contemporary minimalistic architecture, photorealistic rendering. 7) grilled cheese in the shape of heart. 8) close up of Italian pizza margherita, glass of fresh beer, candle light, polished wood table, UHD, high quality, high detail, ultra definition, high octane render, Style anime #cibo #pizza."
        }
    ],
    "affiliations": [
        "Alibaba International Digital Commerce Group",
        "National Key Laboratory for Novel Software Technology, Nanjing University",
        "School of Artificial Intelligence, Nanjing University"
    ]
}