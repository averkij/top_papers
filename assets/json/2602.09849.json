{
    "paper_title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation",
    "authors": [
        "Yucheng Hu",
        "Jianke Zhang",
        "Yuanfei Luo",
        "Yanjiang Guo",
        "Xiaoyu Chen",
        "Xinshu Sun",
        "Kun Feng",
        "Qingzhou Lu",
        "Sheng Chen",
        "Yangang Zhang",
        "Wei Li",
        "Jianyu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 1 9 4 8 9 0 . 2 0 6 2 : r BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation Yucheng Hu1,, Jianke Zhang1,, Yuanfei Luo2, Yanjiang Guo1, Xiaoyu Chen1, Xinshu Sun1, Kun Feng1, Qingzhou Lu1, Sheng Chen2, Yangang Zhang2, Wei Li2,, Jianyu Chen1, 1Tsinghua University, 2ByteDance Seed Equal contribution, Joint project lead, Corresponding Author"
        },
        {
            "title": "Abstract",
            "content": "Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent VisionLanguage-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, unified model that integrates linguistic planning, visual forecasting, and action generation within single framework. Initialized from pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning. Date: February 11, 2026 Project Page: https://cladernyjorn.github.io/BagelVLA.github.io"
        },
        {
            "title": "Introduction",
            "content": "The pursuit of generalist robots capable of performing complex manipulation tasks in unstructured environments remains central goal in robotics. robust embodied agent must possess three fundamental capabilities: understanding what to do based on instructions, predicting what will happen next, and executing the necessary motions. While recent Vision-Language-Action (VLA) models [4, 28, 40, 44] have made progress by incorporating vision language models (VLMs) [2, 18, 23] or visual generation models [12, 17, 24, 37], they often treat these capabilities as separate modules. Some methods focus on high-level planning [13, 18] but lack visual forecasting, while others focus on visual prediction [5, 12, 17] but struggle with the logical reasoning required for complex tasks [27]. unified framework that seamlessly integrates reasoning, prediction, and control remains key challenge. Meanwhile, the field of multimodal learning has witnessed the emergence of unified understanding and 1 Figure 1 Overview of our framework. We present BagelVLA, unified model that integrates linguistic planning, visual forecasting, and action generation within single framework. We construct massive hybrid dataset combining general multimodal data with large-scale robotic datasets. Robotic datasets with sub-tasks and keyframes are annotated to transfer the foundation models general reasoning and visual generation abilities to embodied settings. generation models [10, 38, 39, 47, 48]. Architectures like Bagel [10] employ single transformer backbone to jointly process and generate text and images, exhibiting emergent abilities in multimodal reasoning. These models provide an appealing prior for embodied agents: the model can think about the next step in text and imagine the outcome in pixels. However, such general-purpose models are not designed for embodied domain reasoning and continuous real-time control. To make unified multimodal priors actionable for long-horizon manipulation, we propose BagelVLA, unified VLA framework that integrates linguistic planning, visual forecasting, and action generation. Rather than treating these as isolated modules, BagelVLA interleaves them within unified transformer architecture. The model first generates textual plan to decompose the instruction (e.g., identifying the next object to manipulate), then predicts the future visual state, and finally generates the action. This design combines the logical reasoning of language models with the predictive power of visual generation, providing rich visual dynamics aligned with instruction to guide low-level control for long-horizon tasks. Realizing this interleaved behavior requires suitable training architecture and data, for which we design two-stage training strategy to inject embodied multi-modal planning capabilities into the model. In the first stage, we construct massive hybrid dataset combining general multimodal data [26, 31, 45] with large-scale robotic datasets [1, 19, 21, 46]. Robotic datasets from diverse embodiments are annotated to transfer the models general reasoning and visual predictive abilities to embodied settings. In the second stage, we introduce the action expert and fine-tune the full model to couple language, predicted visual dynamics, and control. This progressive approach ensures the model retains its high-level reasoning capabilities while acquiring precise low-level control policies. To address the high latency in combining visual generation, we introduce Residual Flow Guidance (RFG). Instead of generating future frames from scratch, RFG conditions on the current observation as strong structural prior and performs single-step denoising to predict the residual change toward the next keyframe. This mechanism allows the model to extract predictive visual features efficiently, guiding action generation without the computational cost of full image synthesis [11, 24], 2 which substantially reduces the foresight cost. We validate BagelVLA through extensive experiments in both simulation and real-world environments. Results show that explicitly coupling linguistic planning with visual forecasting significantly improves performance over baselines, particularly in long-horizon tasks. In real-world scenarios, BagelVLA demonstrates strong robustness, successfully generalizing to unseen instructions and diverse object arrangements where baseline methods often fail. Our contributions are as follows: We propose BagelVLA, which integrates linguistic planning, visual forecasting, and action generation into single architecture. By explicitly modeling the transition from language to visual dynamics, our approach enhances reasoning and control in long-horizon tasks. By exploring various schemes for learning action representations from interleaved planning, we introduce Residual Flow Guidance (RFG), which uses the current observation as structural prior and applies single-step denoising to capture future visual dynamics with minimal latency. BagelVLA substantially outperforms existing baselines in simulation benchmarks and demonstrates strong generalization to diverse instructions and environments in real-world experiments."
        },
        {
            "title": "2 Related Works",
            "content": "2.0.1 Vision-Language-Action Models Vision-Language-Action (VLA) models aim to enhance policy generalization to linguistic instructions and visual scenes by integrating vision-language models (VLMs) with action prediction. For example, methods like RT-2 [3] and OpenVLA [22] employ discrete action tokens compatible with VLMs, allowing direct mapping from vision-language representations to executable actions, though this can limit expressiveness in continuous control. In contrast, approaches such as Octo [41], 3D Diffuser Actor [20], and π0 [2] utilize continuous action representations via diffusion models to capture multimodal distributions, better handling fine-grained manipulations. However, these methodswhether discrete or continuous, overlook the alignment gap between VLM pre-training and VLA fine-tuning, resulting in degraded vision-language capabilities during adaptation. To mitigate this gap, other approaches [15, 17, 24, 37, 51, 53] introduce visual prediction tasks as bridge to map vision-language signals to action signals. For instance, VPP [17] proposes video prediction policy that conditions robot actions on future visual representations derived from video diffusion models. Cosmos Policy [24] directly fine-tunes large pretrained video model to serve as robot policy. Although pre-training with pixel prediction can be easily aligned with the robot observations, the absence of dedicated VLM backbone often leads to poor instruction-following performance, particularly in tasks requiring complex reasoning. 2.0.2 Unified understanding and generation models In multimodal learning, recent efforts [10, 38, 39, 47] have developed unified architectures for joint understanding and generation across modalities. For example, Bagel [10] uses single transformer to process and generate text and images, trained on interleaved datasets for emergent reasoning. Chameleon [39] employs token-based framework for mixed-modal input/output, supporting tasks like question answering and image generation. LMFusion [38] integrates language and vision in fused transformer, focusing on efficient crossmodal alignment, while Show-o [47] emphasizes unified multimodal understanding and generation, including text-conditioned image generation and editing for enhanced scene comprehension. These models, trained on diverse datasets including generation, QA, and editing, demonstrate strong capabilities in multimodal reasoning that can extend to embodied agents. Inspired by these, several VLA works [8, 34, 35, 52] have introduced action experts to transfer their capabilities to embodied scenarios. However, the lack of explicit embodied vision-language interleaved reasoning means these approaches only retain subset of the original models capabilities, failing to implement step-by-step multimodal chain-of-thought reasoning. This deficiency is deemed critical for complex long-horizon tasks. In contrast, our proposed methods successfully incorporate the multi-modal reasoning capability into robotic manipulation via complete data processing pipeline and progressive training paradigm."
        },
        {
            "title": "3 Methodology",
            "content": "Figure 2 Illustration of the BagelVLA framework. BagelVLA utilizes Mixture-of-Transformers (MoT) architecture, comprising three independent transformers specialized for linguistic, visual, and action modalities. To tackle longhorizon tasks and semantic generalization, we formulate language-conditioned action learning as long-sequence interleaved planning problem. As shown, we structure these modalities into unified sequence, enabling the model to generate predictions across all three modalities based on the interleaved context. To support this architecture, we have designed specific mechanisms to facilitate interaction among multiple flow-matching experts and to enhance inference efficiency."
        },
        {
            "title": "3.1 Preliminaries: Interleaved planning for Robot Control",
            "content": "i=1 For classic language-conditioned manipulation settings, policy is typically learned from demonstration dataset = {Li, τi}N , where each trajectory τi = {(v1, l1, a1), . . . , (vT , lT , aT )} consists of observations vt (images and proprioception), stage-specific language descriptions lt, and action chunks at. Conventional VLA models simplify this by conditioning purely on the global instruction L, learning direct mapping policy pθ(atvt, L). However, this formulation is insufficient for long-horizon tasks where global instruction (e.g., stacking blocks in specified order (redyellowbluegreen)) implicitly entails sequence of distinct stages. We address this by modeling the problem as Interleaved Planning. Instead of black-box mapping, we require the model to explicitly reason through the causal chain of the task. Formally, given the global instruction and current observation vt, BagelVLA models the joint distribution of the current subtask lt, the future outcome (keyframe) vt+k, and the action at. This joint distribution pθ(at, vt+k, ltvt, L) is factorized based on the logical dependency of manipulation: 1. Linguistic Planning: The model first identifies the immediate textual objective lt from the global instruction. We consider task decomposition to be the primary semantic capability of VLM-based architectures. 2. Visual Forecasting: Conditioned on this subtask, the model acts as world model to predict the physical outcome vt+k. 3. Action Generation: Finally, the action at is generated, grounded in both the textual plan and the visual forecast. 4 Consequently, our objective is formulated as the maximization of the following factorized likelihood: = (Ll + Lv + La) = max θ ED log pθ(ltvt, L) pθ(vt+kvt, L, lt) pθ(atvt, L, lt, vt+k)"
        },
        {
            "title": "3.2 Model Architecture\nTo address the interleaved planning problem defined in Sec. 3.1, we propose BagelVLA, a unified framework\nfor understanding, prediction, and action generation. As illustrated in Fig. 2, BagelVLA is designed to process\ndata across three modalities simultaneously. To leverage pre-existing large-scale multimodal data, we employ a\nMixture of Transformers (MoT) architecture to orchestrate experts managing different modalities: specifically,\nan LLM expert, a generation expert, and an action expert, all connected via self-attention mechanisms.",
            "content": "We initialize the LLM and generation experts using Bagel [10], unified MoT model for understanding and generation. On top of this foundation, we incorporate smaller transformer to serve as the action expert. Distinct from prior MoT-based VLA architectures [8, 35, 52], BagelVLA benefits from robust pre-training initialization for both its language and vision components and employs novel dual flow-matching mechanism (detailed in Sec. 3.3). Detailed model settings are described in Appendix A. 3.2.1 Understanding Expert & Generation Expert The understanding and generation experts adopt the architecture of Qwen2.5-LLM-7B [49]. Following Bagels configuration, we utilize two distinct visual encoders responsible for visual-language understanding and goal image prediction, respectively. Each input observation view vt is encoded by the SigLIP2 [42] and concatenated with the text instructions (and lt) to serve as input for the LLM Expert. We also utilize the VAE from FLUX [25] to encode images. For linguistic planning, the understanding expert attends to ViT features when autoregressively generating the subtask lt. We optimize textual-planning task using an autoregressive Cross-Entropy (CE) loss: Ll = log pθ(ltvt, L). For visual forecasting, the generation expert, while denoising the keyframe image, attends to all input views VAE and ViT features, and relevant textual information. It generates keyframe by iteratively denoising input VAE noise using Flow Matching [29, 33], denoted as: Lv = log pθ(vt+kvt, L, lt). 3.2.2 Action Expert We employ an independent transformer connected via the MoT framework as the action expert, which is responsible for processing proprioceptive and action modalities. The action expert shares similar architecture with the Qwen2.5 LLM; however, we reduce the intermediate size of the MLP to 1/5th of the original, resulting in 2B parameters. This compact size facilitates higher execution frequency during inference through KV-cache and asynchronous action generation. For action planning, we employ Flow Matching to learn action chunks, denoted as La = log pθ(atvt, L, lt, vt+k). During the denoising process, the action sequence can attend to the VAE and ViT features of the input views, the global instruction L, the generated subtask lt, and also the proprioceptive state input to the action expert. Notably, the action expert attends to the intermediate latent states of the image currently being generated. This involves handling the asymmetric information interaction between the dual Flow Matching modules of the generation and action experts. We detail the various schemes we explored in Sec. 3.3 and ablate these methods in Sec. 4.3."
        },
        {
            "title": "3.3 Conditioning Schemes in Dual Flow-Matching\nIn this section, we detail the computation of Lv and La within a unified interleaved input sequence, ensuring\nconsistency with the inference context. As illustrated in Fig. 3, we propose three interaction mechanisms for\nthe Flow Matching (FM) of keyframe prediction and action generation.",
            "content": "5 Figure 3 Illustration of different types of dual denoising schemes. (a) Complete Denoise: Image prediction and action generation are performed separately, requiring total of N1 + N2 denoising steps. (b) Joint Denoise: Image prediction and action generation are performed simultaneously, denoising together for steps. (c) Single-Step Denoise: Action generation is conditioned directly on the context from the first denoising step of the image prediction. Further implementation details, including the construction of the concatenated sequence and the attention mask are provided in Appendix B. Scheme 1: Complete Denoise As shown in Fig. 3(a), Complete Denoise prioritizes the full denoising of the keyframe image by the generation expert. The generated image is then fed back as context for action generation. During training, to ensure the action expert observes fully denoised image, we append the ground truth keyframe subsequent to the denoising sequence. The loss functions are defined as follows: Lv = (cid:2)vv,θ(L, vt, lt, τ, vτ La1 = (cid:2)va,θ(L, vt, lt, vτ =1 t+k) (v1 t+k t+k , τ, aτ ) (a1 a0 (cid:3) , vτ t+k = (1 τ )v0 t+k)2 2 (cid:3) , = (1 τ )a0 aτ )2 2 t+k + τ v1 , a1 t+k, v1 = at + τ a1 t+k = vt+k (1) where denotes the velocity predicted by the model for the corresponding modality. and lt represent the global instruction and sub-task plan, vt is the current input observation, vt+k is the target keyframe and at is the action chunk. τ denotes the denoising timestep (where τ = 0 represents initial noise and τ = 1 represents the ground truth target). This approach effectively combines World Model (WM) with an Inverse Dynamics Model (IDM) [12]. While theoretically sound for leveraging the WM, it suffers from high inference latency (total denoising steps N1 +N2) and the potential accumulation of visual errors. To mitigate these issues, we propose alternative schemes. Scheme 2: Joint Denoise As shown in Fig. 3(b), we synchronize the denoising processes of the keyframe and the action. Here, the action generation attends to the noisy image currently undergoing denoising. The computation for the action FM loss in Eq. 1 is modified as: La2 = (cid:2)va,θ(L, vt, lt, vτ t+k, τ, aτ ) (a1 a0 )2 2 (cid:3) During training, we append the action denoising block directly after the keyframe denoising sequence, allowing the action component to attend to the intermediate noisy keyframes. During inference, the model generates both keyframes and actions within steps, significantly reducing latency. Scheme 3: Single-step Denoise To further minimize the computational cost of action inference imposed by image denoising, we propose single-step denoise. In this scheme, action generation attends only to the KV-cache from the initial denoising step of the keyframe. This implies the model generates actions while conditioning on the initial noise as the keyframe input: La3 = (cid:2)va,θ(L, vt, lt, vτ =0 t+k , τ, aτ ) (a1 t )2 2 (cid:3) 6 Furthermore, based on Scheme 3, we introduce variant of Single-step Denoise where we inject current frame information into the initial image noise to provide stronger priors for both keyframe and action generation: Naive Single-step Denoise : vτ =0 t+k (0, I) Residual Flow Guidance (RFG) : vτ =0 t+k (vt, I) (2) (3) More details about implementing the above methods can be found in Appendix B. We provide an ablation study of these methods in Sec. 4.3. Based on the results, we select the Single-step Denoise (RFG) as our default setting for BagelVLA. Notably, we observe that RFG, which incorporates the initial frame vt prior, significantly reduces the required denoising steps as shown in Fig. 5. We hypothesize that this allows the WM to focus on modeling robot manipulation changes rather than reconstructing static background details. Further quantitative comparisons are available in Sec. 4.3.1."
        },
        {
            "title": "3.4 Data Engine",
            "content": "To construct large-scale pretraining dataset for subtask planning and keyframe prediction in embodied scenarios, we leverage diverse sources of manipulation demonstrations and apply tailored processing pipelines to four major data categories in Fig. 1 according to their characteristics. Details of all data annotations and components are provided in Appendix C. Robotic Data The robot data comprises self-collected expert demonstrations and publicly available data from diverse embodiments. For proprietary data, we manually annotate and segment videos to obtain lt, ensuring high-quality planning and keyframe prediction. For public datasets lacking fine-grained labels, we utilize Seed-1.5-VL-thinking [14] to synthesize lt and identify temporal boundaries (start and end frames). These samples are then filtered to retain high-quality instances. These two components are used exclusively for pretraining to transfer the models fundamental planning and prediction capabilities to the embodied domain. General Data General Data includes egocentric human videos and large-scale imagetext VQA data. For the former, we similarly employ Seed-1.5-VL-thinking to generate language annotations; however, due to the complexity of human-centered scenes, we do not annotate subtasks and instead predict only the final frame of each operation. These two data sources are mainly used to preserve the base models original understanding and generation capabilities."
        },
        {
            "title": "3.5 Training and Inference Strategy",
            "content": "BagelVLA requires the simultaneous alignment of three distinct planning tasks: linguistic planning, visual forecasting, and action generation. To achieve this, we divide the training process into two stages, maximizing the utilization of general multimodal data and large-scale embodied data without action labels. Detailed data recipes and implementation details can be found in Appendix and D. In this stage, we Stage 1: Pretraining - Finetuning Linguistic Planning and Learning Visual Dynamics exclusively finetune the understanding and generation experts to acquire capabilities in textual planning and keyframe prediction. To preserve the models general linguistic proficiency, we co-train with general Question-Answering (QA) data. Specifically, the pretraining data comprises: General VQA (Language Co-training): 2.98M QA pairs. Human-hand Data (Visual Dynamics): 310k episodes. Open-source Robot Data (Language Planning & Visual Dynamics): 146k episodes. Open-source Robot Data (Visual Dynamics): 297k episodes. Self-collected Real Robot Data (Language Planning & Visual Dynamics): 75k episodes. 7 In this stage, we introduce downstream robot data containing Stage 2: Finetuning - Learning Action Planning action labels for finetuning. We finetune the entire model on all three planning tasks simultaneously to obtain an interleaved planning model that performs robustly in specific scenarios. For the four scenarios used in our experiments, we employ the following finetuning strategies: Calvin (Visual & Action Planning): ABC split dataset. Robotwin (Linguistic, Visual & Action Planning): 50 tasks with 50 episodes each, totaling 2.5k episodes. ALOHA Basic Tasks (Visual & Action Planning): 3k episodes. ALOHA Long-horizon Tasks (Linguistic, Visual & Action Planning): 1.5k episodes. Inference Strategy During inference, the model generates textual plans, keyframes, and actions in an interleaved manner. At each denoising step, only single expert is activated (7B model for text and keyframe or 2B model for action generation). The single-step denoise scheme further enhances execution frequency. Specifically, we concatenate the current frame, instruction context, and pure noise image to compute the KV pairs of the understanding and generation experts, which then condition the action generation. This mechanism enables BagelVLA to infer at speed of 1.2 seconds per chunk on single RTX 5090 GPU (yielding real-world action frequency of 40Hz with chunk size of 48). We also introduce Asynchronous Execution [9, 50] to further boost inference speed. During training, we randomly replace the current frame with preceding image. This allows us to reduce the updating frequency of the KV contexts of understanding and generation experts during inference, updating only the proprioceptive inputs to output new action chunks. Under this setting, our policy can achieve an execution frequency of 72Hz."
        },
        {
            "title": "4 Experiment",
            "content": "We conduct extensive experiments to evaluate the interleaved planning capabilities of BagelVLA across diverse range of manipulation tasks. These experiments encompass two simulation environments, Calvin [36] and Robotwin [7], as well as basic tasks suite containing 9 skills of 30 tasks, and long-horizon task suite performed on the AgileX dual-arm robot system."
        },
        {
            "title": "4.1 Evaluation in Simulation Environment\nWe benchmark BagelVLA against π0[2], RDT [32] and two VLA models that incorporate future prediction\ncapabilities, UP-VLA [51] and VPP [17], in the Calvin and Robotwin environments.",
            "content": "In the Calvin environment, models are trained on the ABC split and evaluated in the environment. For Robotwin, we utilize training dataset consisting of 50 clean demonstrations for each of the 50 tasks. All models are then tested in both Clean and Randomized settings using unseen instructions. To verify the efficacy of interleaved planning, we conduct experiments with BagelVLA trained and tested both with and without interleaved planning. Further details regarding simulation experiments can be found in Appendix E. Table 1 Results on Calvin and Robotwin2.0 Benchmarks. Since Calvin consists exclusively of single-step tasks, we did not evaluate BagelVLAs performance under the textual planning setting in this domain. Detailed results can be found in Table 8 and 7. Model Calvin Robotwin ABC-D Clean Randomized π0 RDT UP-VLA VPP w/o Textual-planning w/o Keyframe-forecasting BagelVLA 3.648 - 4.078 4.329 - 3.345 4.405 46.42 34.50 52.92 - 54.00 56.72 75.26 16.34 13.72 15.16 - 19.20 15.92 20. 8 Figure 4 Visualization of interleaved planning results on real-world robotic tasks. Given global instruction and the current observation, BagelVLA leverages the context to identify the immediate subtask, predicts goal image for that subtask, and subsequently generates actions. The figure illustrates the interleaved planning results for Stack Cubes in Requested Order, Calculate and Place Symbol Blocks task, and task from the Agibot dataset [1]. More cases can be found in Appendix G. As presented in Table 1, BagelVLA outperforms all baselines on both the Calvin ABC-D split and the Robotwin tasks. BagelVLA achieves an average completion length of 4.41 on the Calvin ABC-D benchmark. This indicates that models leveraging only visual prediction as an auxiliary task can effectively generalize from in-domain training to Out-of-Distribution (OOD) scenarios involving background and color variations, while maintaining high manipulation accuracy. On the Robotwin benchmark, BagelVLA without textual-planning surpasses π0 in both Clean and Randomized settings, achieving success rates comparable to UP-VLA, which similarly employs visual prediction as an auxiliary task. This suggests that the visual prediction component within our interleaved planning framework yields consistent gains across different VLM backbones. However, when incorporating textual-planning, BagelVLA achieves state-of-the-art performance in both in-domain and out-of-domain settings on Robotwin, demonstrating the substantial effectiveness of the proposed interleaved planning scheme."
        },
        {
            "title": "4.2 Real-world Experiments",
            "content": "We evaluated BagelVLA on the Aloha-AgileX bimanual robot platform across two categories of dual-arm manipulation tasks. Multiple demonstrations of real-world evaluation are presented in Appendix for reference. These tasks were designed to assess the models performance on both basic tasks and long-horizon tasks that require planning. Specifically, we collect 3,000 trajectories categorized as basic tasks, covering 9 distinct skills ranging from short-horizon tasks such as pick-and-place to medium-horizon tasks such as sweeping rubbish. Furthermore, we designed two types of Long-Horizon planning tasks that necessitate subtask planning, for which we gathered 1,500 demonstrations. All collected data are manually annotated with subtasks and corresponding keyframes. We then fine-tune the pretrained BagelVLA on all trajectories and evaluate its multi-task learning capabilities. We compare BagelVLA with π0 [2] and VPP [17]. visualization of interleaved plans generated by BagelVLA for the real-world tasks is illustrated in Fig. 4. 4.2.1 Basic Task Experiments Table 2 Results on Real-World Basic Tasks without using subtask planning. We run 20 times for each task. More details and evaluation demos can be found in Appendix F.1 Model π0 VPP BagelVLA Pick&Place Seen 95 85 95 Unseen 55 45 85 Pick&Place Water Stack Flower Cubes Put Flowers in Vase Stack Bowls Pour Fries Sweep Rubbish Press Button Drawer Close Success Average 50 60 60 65 50 80 40 50 35 70 55 90 35 30 45 55 45 90 75 90 95 100 95 65.0 59.5 75.5 Table 2 presents the performance of BagelVLA in multi-task setting without the use of subtask planning. BagelVLA achieved the highest average success rate across the 9 categories of tasks, which demonstrates 9 its outstanding multi-task learning capabilities. Additionally, we tested the model on pick-and-place tasks involving unseen objects. As shown, BagelVLA significantly outperforms VPP and π0 in the OOD setting. This advantage stems from the powerful semantic features preserved during VLA fine-tuning, which are inherited from the pre-training of our understanding and generation experts."
        },
        {
            "title": "4.2.2 Long-Horizon Planning Task Experiments",
            "content": "Table 3 Results on Real-World Long-Horizon Planning Tasks. We run 20 times for each task. Difficulty settings for the tasks are defined as follows. For Stack Cubes: Easy (2-3 cubes, 1-2 layers), Middle (3-4 cubes, 2 layers), and Hard (3-5 cubes, 3 layers). For Calculate and Place Symbols: Easy (1-2 blocks for single-digit addition), Middle (2 blocks for the answer of double-digit addition), and Hard (3-5 blocks within double-digit addition). The Success Rate column shows the average success rate, while Planning Accuracy indicates correct motion trends across 20 different intermediate states. More details and evaluation demos can be found in Appendix F.2. Tasks Difficulty Stack Cubes in Requested Order Calculate and Place Symbol Blocks Easy Middle Hard Success Rate Planning Accuracy Easy Middle Hard Success Rate Planning Accuracy π0 VPP w/o Keyframe-forecasting w/o Textual-planning BagelVLA 75 60 90 75 95 35 15 45 40 65 10 0 25 15 60 40.0 25.0 53.3 43.3 73. 55 45 80 70 95 70 60 70 65 80 25 10 50 25 65 0 0 30 10 45 31.7 23.3 50.0 33.3 63.3 40 30 75 50 We collected data for two categories of long-horizon tasks that require planning. In the colored block stacking task, shown in the first row of Fig. 4, the model is instructed to stack cubes in the order specified by the instruction. This task challenges both the models visual-language interleave planning ability and its capacity to follow instructions at the action level. In the arithmetic equation arrangement task, shown in the second row, we require the model first to compute an arithmetic expression and then place the corresponding symbolic blocks in single sequence. The objective of this task is to verify whether the model can retain reasoning capabilities (such as performing simple addition) during the planning process. Table 3 displays the performance of the three models on these two long-horizon planning tasks. It is evident that although all three models were trained on the exact same action data, BagelVLA, with its interleaved planning capabilities, exhibits significant advantage in planning-oriented tasks. In addition to the average task success rate, we also measured the correctness of the motion trend for each subtask to assess the models semantic understanding and action-following fidelity. Overall, BagelVLA achieved planning accuracy of nearly 90%, which implies that its multi-modal planning is correct and possesses strong generalization abilities. Concurrently, we observed gap between task success rate and the planning accuracy, suggesting deficiencies in action mapping due to limitations in both the model and the dataset, specifically concerning the precision of fine-motor control."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "We conduct comprehensive ablation studies on the various modules of BagelVLA in both simulated and real-world environments. Through these experiments, we aim to answer the following questions: 1. What is the optimal interaction mechanism between the generation experts and the action experts? 2. How does RFG outperform naive single-step denoising? 3. What is the effect of BagelVLAs pre-training on action execution performance? 4. Does each modality within the interleaved planning framework contribute positively to the action generation process? 4.3.1 Comparison of Different Conditioning Schemes in Dual Flow-Matching We evaluate the three dual flow-matching interaction schemes described in Sec. 3.3 within the Calvin ABC-D environment. For complete denoise and joint denoise, the image noise initialization follows the formulation in Eq. 2. We utilize single-view inputs and train each method for 10k steps for testing. We denoise 50 times for image generation and 10 times for action generation. Table 4 reports the average task completion length and 10 the inference latency per action chunk for each interaction method, measured 20 times on single NVIDIA A800 GPU. Table 4 Ablation on Different Conditioning Schemes and RFG mechanism on Calvin single-view setting. Dual Flow-Matching Schemes Latency ABC-D Complete Denoise Joint Denoise Single-step Denoise (Eq. 2) RFG (Eq. 3) 6.04s 2.90s 1.23s 1.23s 2.480 2.038 3.345 3.600 The results indicate that the single-step denoising strategy not only significantly outperforms the other two approaches in terms of task success rate but also achieves superior inference speed. This performance gap can be attributed to the domain shift introduced during testing, where the model encounters scenes with altered color schemes. Under these conditions, models employing complete denoising or joint denoising are prone to encountering out-of-distribution (OOD) intermediate states during the flow-matching phase of the generation expert. This consequently leads to substantial degradation in action execution performance. Based on these empirical findings, we adopt single-step denoising as the default interaction mechanism for the dual flow-matching framework in BagelVLA across all subsequent scenarios and tasks. 4.3.2 Advantages of RFG over Naive Single-Step Denoising Figure 5 Predicted images using different denoising steps. The figure displays the generation results for the naive single-step denoise (Eq. 2) and RFG (Eq. 3) across varying denoising steps in real-world basic tasks and Robotwin randomized (unseen) scenarios. RFG demonstrates the capability to preserve backgrounds and achieve high-quality generation with very few steps. This provides strong support for reducing the inference latency of interleaved generation. More cases can be found in Appendix H. In contrast to the conventional naive single-step denoising approach, which employs Eq. 2 for noise initialization, Figure 6 Ablation on Real-World Basic Tasks 11 RFG utilizes Eq. 3. We compare these two methods across both the Calvin simulation environment and real-world basic tasks. In the real-world basic tasks shown in Fig. 6, RFG demonstrates significantly superior performance compared to naive single-step denoising on several tasks. Concurrently, as shown in Table 4, RFG achieves faster action learning convergence while maintaining the low inference latency characteristic of naive single-step denoising. This improvement stems from the fact that in the naive approach, action generation relies on intermediate features derived from single denoising step on pure Gaussian noise. Conversely, RFG incorporates the initial frame into the noise initialization, thereby providing stronger prior information for action generation. Furthermore, we observe that RFG offers distinct advantage in keyframe prediction, even though fully denoising the keyframe is not strictly required for action generation. Fig. 5 visualizes the predicted keyframes for both RFG and naive single-step denoising across different denoising steps. It is evident that RFG is capable of generating high-quality future frames with very few denoising steps (e.g., 10 steps). We hypothesize that this phenomenon arises because the inclusion of the first frame in Eq. 3 allows the model to focus its capacity on the dynamic regions, rather than learning complex static background information."
        },
        {
            "title": "4.3.3 Effectiveness of Large-Scale Language Planning and Visual Dynamics Pre-training\nIn the real-world basic tasks, we evaluate the impact of pre-training. By comparing the w/o pretrain variant\nwith baseline in Fig. 6, it is evident that the pre-trained baseline achieves a significantly higher success\nrate on pick&place (OOD)tasks. This indicates that pre-training solely on linguistic planning and visual\nforecasting is sufficient to enhance the model’s semantic generalization capabilities. Furthermore, on three\nmedium-horizon tasks (including sweep rubbish, pour fries and stack cubes), the model utilizing joint\npre-training exhibits higher accuracy. We attribute this improvement to the planning capabilities acquired\nfrom the language planning tasks during pre-training. During the subsequent action fine-tuning phase, the\nmodel retains these state prediction and planning capabilities, thereby enabling it to perform implicit subtask\nplanning even without explicitly utilizing interleaved planning during inference.",
            "content": "4.3.4 Effectiveness of Visual and Language Modalities in Interleaved Planning To verify the effectiveness of the interleaved planning mechanism, we investigate the performance impact of omitting textual planning and keyframe forecasting, respectively. Linguistic Planning: The results in Table 1 demonstrate that employing textual planning with BagelVLA in RoboTwin environment improves the success rate by 21%. Similarly, in the two categories of real-world long-horizon tasks shown in Table 3, the use of textual planning also yields substantial performance gains. These two sets of experiments conclusively prove that incorporating language planning significantly benefits long-horizon tasks. Visual Forecasting: The keyframe variant in Table 1 and 3 illustrates the impact of using keyframe prediction as training objective. The results indicate that visual planning markedly improves the accuracy of action planning in both simulation and real-world environments. The aforementioned experiments confirm that both visual planning and language planning play crucial roles within the interleaved planning framework."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented BagelVLA, unified Vision-Language-Action framework for long-horizon manipulation by interleaving linguistic planning, visual forecasting, and action generation within single transformer system. Building on Bagels unified multimodal backbone, we introduce an action expert and adopt two-stage training recipe to progressively transfer multimodal reasoning and visual dynamics into embodied planning, then couple these representations with control. To address the latency of visual foresight, we further propose Residual Flow Guidance (RFG), which captures task-relevant future dynamics with substantially reduced computational costs. Overall, our results suggest that explicitly coupling linguistic planning with predictive visual representations can improve robustness and instruction-following in long-horizon manipulation."
        },
        {
            "title": "6 Acknowledgements",
            "content": "We sincerely thank Weiwei Fang, Ziyang Liu, Zhelun Shi, Haitong Wang and Tingshuai Yan for their strong support and fruitful discussions."
        },
        {
            "title": "References",
            "content": "[1] AgiBot-World-Contributors, Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, Shu Jiang, Yuxin Jiang, Cheng Jing, Hongyang Li, Jialu Li, Chiming Liu, Yi Liu, Yuxiang Lu, Jianlan Luo, Ping Luo, Yao Mu, Yuehan Niu, Yixuan Pan, Jiangmiao Pang, Yu Qiao, Guanghui Ren, Cheng Ruan, Jiaqi Shan, Yongjian Shen, Chengshi Shi, Mingkang Shi, Modi Shi, Chonghao Sima, Jianheng Song, Huijie Wang, Wenhao Wang, Dafeng Wei, Chengen Xie, Guo Xu, Junchi Yan, Cunbiao Yang, Lei Yang, Shukai Yang, Maoqing Yao, Jia Zeng, Chi Zhang, Qinglin Zhang, Bin Zhao, Chengyue Zhao, Jiaqi Zhao, and Jianchao Zhu. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems, 2025. URL https://arxiv.org/abs/2503.06669. [2] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. [3] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control, 2023. URL https://arxiv.org/abs/2307.15818. [4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [5] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, and Minzhao Zhu. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation, 2024. URL https://arxiv.org/abs/2410.06158. [6] Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, Jiafeng Xu, and Yichu Yang. Gr-3 technical report, 2025. URL https://arxiv.org/abs/2507.15493. [7] Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, et al. Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation. arXiv preprint arXiv:2506.18088, 2025. [8] Xiaoyu Chen, Hangxing Wei, Pushi Zhang, Chuheng Zhang, Kaixin Wang, Yanjiang Guo, Rushuai Yang, Yucen Wang, Xinquan Xiao, Li Zhao, et al. Villa-x: enhancing latent action modeling in vision-language-action models. arXiv preprint arXiv:2507.23682, 2025. [9] Can Cui, Pengxiang Ding, Wenxuan Song, Shuanghao Bai, Xinyang Tong, Zirui Ge, Runze Suo, Wanqi Zhou, Yang Liu, Bofang Jia, et al. Openhelix: short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation. arXiv preprint arXiv:2505.03912, 2025. [10] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [11] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in neural information processing systems, 36:91569172, 2023. [12] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in Neural Information Processing Systems, 36, 2024. [13] Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, and Hang Li. Robix: unified model for robot interaction, reasoning and planning, 2025. URL https: //arxiv.org/abs/2509.01106. [14] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen, Ke Wang, Keyu Pan, Kun Zhang, Kunchang Li, Lanxin Li, Lei Li, Lei Shi, Li Han, Liang Xiang, Liangqiang Chen, Lin Chen, Lin Li, Lin Yan, Liying Chi, Longxiang Liu, Mengfei Du, Mingxuan Wang, Ningxin Pan, Peibin Chen, Pengfei Chen, Pengfei Wu, Qingqing Yuan, Qingyao Shuai, Qiuyan Tao, Renjie Zheng, Renrui Zhang, Ru Zhang, Rui Wang, Rui Yang, Rui Zhao, Shaoqiang Xu, Shihao Liang, Shipeng Yan, Shu Zhong, Shuaishuai Cao, Shuangzhi Wu, Shufan Liu, Shuhan Chang, Songhua Cai, Tenglong Ao, Tianhao Yang, Tingting Zhang, Wanjun Zhong, Wei Jia, Wei Weng, Weihao Yu, Wenhao Huang, Wenjia Zhu, Wenli Yang, Wenzhi Wang, Xiang Long, XiangRui Yin, Xiao Li, Xiaolei Zhu, Xiaoying Jia, Xijin Zhang, Xin Liu, Xinchen Zhang, Xinyu Yang, Xiongcai Luo, Xiuli Chen, Xuantong Zhong, Xuefeng Xiao, Xujing Li, Yan Wu, Yawei Wen, Yifan Du, Yihao Zhang, Yining Ye, Yonghui Wu, Yu Liu, Yu Yue, Yufeng Zhou, Yufeng Yuan, Yuhang Xu, Yuhong Yang, Yun Zhang, Yunhao Fang, Yuntao Li, Yurui Ren, Yuwen Xiong, Zehua Hong, Zehua Wang, Zewei Sun, Zeyu Wang, Zhao Cai, Zhaoyue Zha, Zhecheng An, Zhehui Zhao, Zhengzhuo Xu, Zhipeng Chen, Zhiyong Wu, Zhuofan Zheng, Zihao Wang, Zilong Huang, Ziyu Zhu, and Zuquan Song. Seed1.5-vl technical report, 2025. URL https://arxiv.org/abs/2505.07062. [15] Yanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu, and Jianyu Chen. Prediction with action: Visual policy learning via joint denoising process. Advances in Neural Information Processing Systems, 37:112386112410, 2024. [16] Ryan Hoque, Peide Huang, David Yoon, Mouli Sivapurapu, and Jian Zhang. Egodex: Learning dexterous manipulation from large-scale egocentric video. arXiv preprint arXiv:2505.11709, 2025. [17] Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. arXiv preprint arXiv:2412.14803, 2024. [18] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. pi0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. [19] Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, and Hang Zhao. Galaxea open-world dataset and g0 dual-system vla model, 2025. URL https: //arxiv.org/abs/2509.00576. [20] Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene representations, 2024. URL https://arxiv.org/abs/2402.10885. [21] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman 14 Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, Vitor Guizilini, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Muhammad Zubair Irshad, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph Lim, Jitendra Malik, Roberto Martín-Martín, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. Droid: large-scale in-the-wild robot manipulation dataset. 2024. [22] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model, 2024. URL https://arxiv.org/abs/2406.09246. [23] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [24] Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, et al. Cosmos policy: Fine-tuning video models for visuomotor control and planning. arXiv preprint arXiv:2601.16163, 2026. [25] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [26] Ang Li, Charles Wang, Deqing Fu, Kaiyu Yue, Zikui Cai, Wang Bill Zhu, Ollie Liu, Peng Guo, Willie Neiswanger, Furong Huang, Tom Goldstein, and Micah Goldblum. Zebra-cot: dataset for interleaved vision language reasoning, 2025. URL https://arxiv.org/abs/2507.16746. [27] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, Hang Yin, Michael Lingelbach, Minjune Hwang, Ayano Hiranaka, Sujay Garlanka, Arman Aydin, Sharon Lee, Jiankai Sun, Mona Anvari, Manasi Sharma, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Yunzhu Li, Silvio Savarese, Hyowon Gweon, C. Karen Liu, Jiajun Wu, and Li Fei-Fei. Behavior-1k: human-centered, embodied ai benchmark with 1,000 everyday activities and realistic simulation, 2024. URL https://arxiv.org/abs/2403.09227. [28] Wei Li, Renshan Zhang, Rui Shao, Jie He, and Liqiang Nie. Cogvla: Cognition-aligned vision-language-action model via instruction-driven routing & sparsification, 2025. URL https://arxiv.org/abs/2508.21046. [29] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. URL https: //arxiv.org/abs/2304.08485. [31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2024. URL https://arxiv.org/abs/2310.03744. [32] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. [33] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. [34] Hao Lu, Ziyang Liu, Guangfeng Jiang, Yuanfei Luo, Sheng Chen, Yangang Zhang, and Ying-Cong Chen. Uniugp: Unifying understanding, generation, and planing for end-to-end autonomous driving. arXiv preprint arXiv:2512.09864, 2025. [35] Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, and Jiangmiao Pang. F1: vision-language-action model bridging understanding and generation to actions. arXiv preprint arXiv:2509.06951, 2025. 15 [36] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters (RA-L), 7(3):73277334, 2022. [37] Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, and Elvis Nava. mimic-video: Video-action models for generalizable robot control beyond vlas. arXiv preprint arXiv:2512.15692, 2025. [38] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. [39] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [40] Gemini Robotics Team, Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, et al. Gemini robotics 1.5: Pushing the frontier of generalist robots with advanced embodied reasoning, thinking, and motion transfer. arXiv preprint arXiv:2510.03342, 2025. [41] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy, 2024. URL https://arxiv.org/abs/2405.12213. [42] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [43] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe HansenEstruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning (CoRL), 2023. [44] Shaoan Wang, Yuanfei Luo, Xingyu Chen, Aocheng Luo, Dongyue Li, Chang Liu, Sheng Chen, Yangang Zhang, and Junzhi Yu. Vlingnav: Embodied navigation with adaptive reasoning and visual-assisted linguistic memory. arXiv preprint arXiv:2601.08665, 2026. [45] Luis Wiedmann, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Leandro von Werra, Aritra Roy Gosthipaty, and Andrés Marafioti. Finevision: Open data is all you need, 2025. URL https://arxiv.org/abs/ 2510.17269. [46] Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, Shichao Fan, Xinhua Wang, Fei Liao, Zhen Zhao, Guangyu Li, Zhao Jin, Lecheng Wang, Jilei Mao, Ning Liu, Pei Ren, Qiang Zhang, Yaoxu Lyu, Mengzhen Liu, Jingyang He, Yulin Luo, Zeyu Gao, Chenxuan Li, Chenyang Gu, Yankai Fu, Di Wu, Xingyu Wang, Sixiang Chen, Zhenyu Wang, Pengju An, Siyuan Qian, Shanghang Zhang, and Jian Tang. Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation, 2025. URL https://arxiv.org/abs/2412.13877. [47] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [48] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models, 2025. URL https://arxiv.org/abs/2506.15564. [49] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [50] Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, and Jianyu Chen. Hirt: Enhancing robotic control with hierarchical robot transformers. arXiv preprint arXiv:2410.05273, 2024. [51] Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, and Jianyu Chen. Up-vla: unified understanding and prediction model for embodied agent. arXiv preprint arXiv:2501.18867, 2025. [52] Jianke Zhang, Yucheng Hu, Yanjiang Guo, Xiaoyu Chen, Yichen Liu, Wenna Chen, Chaochao Lu, and Jianyu Chen. Unicod: Enhancing robot policy via unified continuous and discrete representation learning. arXiv preprint arXiv:2510.10642, 2025. [53] Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, Fan Lu, He Wang, et al. Dreamvla: vision-language-action model dreamed with comprehensive world knowledge. arXiv preprint arXiv:2507.04447, 2025."
        },
        {
            "title": "A Details of Model Architecture",
            "content": "The architecture of each expert in BagelVLA is detailed in the table below. Table 5 Parameters of Model Architecture"
        },
        {
            "title": "Understanding Expert Generation Expert Action Expert",
            "content": "Size Input Modality Output Modality Encoder Image Resolution Hidden size Intermediate size Layers Loss Type FM Timestep Distribution 7B Image/Text Text ViT+MLP 256x256 3584 18944 28 CE - 7B Image Image VAE+MLP 256x256 (VAE) 3584 18944 28 MSE(FM) LogitNormal(0, 1) 2B Proprio/Action Action MLP - 3584 3584 28 MSE(FM) Beta(1.5,1) Dual Denoise Flow-Matching Implementation Details Here, we demonstrate how to implement the three dual flow-matching methods mentioned in Sec. 3.3. Specifically, this requires designing unified multi-task attention mask for training, enabling single input sequence to be used for the simultaneous computation of multiple task losses. When designing the corresponding interleaved sequences, we must not only prevent information leakage between different modalities but also align the training setup with special conditions encountered during inference, such as the time-sampling discrepancies that arise from varying numbers of denoising steps. We visualize the masking strategy used in our experiments in Fig. 7."
        },
        {
            "title": "C Data Details",
            "content": "C.1 Stage 1: Pretraining - Finetuning Language Planning and Learning Visual Dynamics In this stage, we exclusively finetune the Understanding and Generation Experts to acquire capabilities in sub-task planning and keyframe prediction. To preserve the models general linguistic proficiency, we co-train with general Question-Answering (QA) data. Specifically, the pretraining dataset comprises: General VQA (Language Co-training): 2.56M QA pairs. Human-hand Data (Visual Dynamics): 310k episodes. Open-source Robot Data (Language Planning & Visual Dynamics): 382k episodes. Self-collected Real Robot Data (Language Planning & Visual Dynamics): 4.5k episodes. C.2 Stage 2: Finetuning - Learning Action Planning In this stage, we introduce downstream robot data containing action labels for finetuning. We finetune the entire model on all three planning tasks simultaneously to obtain an interleaved planning model that performs robustly in specific scenarios. For the four scenarios used in our experiments, we employ the following finetuning strategies: Calvin (Visual Dynamics & Action Planning): ABC dataset. Robotwin (Language Planning, Visual Dynamics & Action Planning): 50 tasks with 50 episodes each, totaling 2.5k episodes. 18 Table 6 Details of Data component. Task name General VQA Dataset name LLaVA-Pretrain[30] FineVision[45] Number of samples 558k 2M AgibotWorld[1] GR[6] Open-source Robot Data Galaxea Open-World[19] Bridge[43] Robotwin[43] Human-hand Data Self-collected Data Egodex[16] Aloha 120k 80k 99k 55k 27.5k 310k 4.5k Aloha Short-horizon Tasks (Visual Dynamics & Action Planning): 3k episodes. Aloha Long-horizon Tasks (Visual Dynamics & Action Planning): 1.5k episodes. C.3 Implementation Details about task Annotation For open-source robotic datasets without subtask annotations, such as Bridge, we use the prompt template in Fig. 12 and apply Seed-1.5-VL-thinking to process videos (or image sequences) solely for pretraining. For datasets that do not provide the overal task descriptions(e.g., EgoDex, AgiBot), we adopt the prompt template in Fig. 13 to extract global task description used for planning or keyframe prediction."
        },
        {
            "title": "D Training and Evaluation Details",
            "content": "For all our experiments, we used learning rate of 1e-5 and employed packed datasets within the FSDP framework to maximize resource utilization. Pre-training was conducted on 64 A800 GPUs with batch size of approximately 1600 for 20,000 steps. For action fine-tuning and evaluation, we adopted different settings for various downstream scenarios: Calvin ABC-D Simulation Environment: We trained on 8 A800 GPUs (effective batch size 192) for 30,000 steps. We used an action chunk size of 10, did not include proprioceptive input, and used two camera views as input to predict only the third view. For evaluation, we tested on 1,000 tasks of length 5 from the D-split and reported the mean task completion length. Robotwin Simulation Environment: We trained on 8 A800 GPUs using 2,500 clean demonstrations (effective batch size 128) for 60,000 steps. We used an action chunk size of 16, sampling one action every 3 steps (effective action horizon of 48). All three camera views were used as input, and we predicted the primary view image. For evaluation, we tested 100 times on 50 tasks in both Clean and Randomized settings using unseen instructions and reported the success rate. Real-Robot Tasks: We trained on 32 A800 GPUs (effective batch size 512) for 50,000 steps. We used an action chunk size of 24, inputting three views (primary, left wrist, right wrist) and predicting the primary view image. For evaluation, we tested each task type 20 times with randomized initial positions and added distractor objects. For OOD tasks, we included unseen target objects."
        },
        {
            "title": "E Detailed Results in Simulation Environments",
            "content": "Table 8 and 7 present more detailed experimental results from the simulation environments. 19 Evaluation Demos of Real-World Tasks In this section, we detail the setup for two categories of real-robot tasks: Basic Tasks and Long-Horizon Planning Tasks. We also present demo videos of BagelVLA performing on each task type. F.1 Basic Tasks During testing, we incorporate several kinds of randomness to evaluate robustness and generalization: Novel Objects: Adding unseen objects. Distractors: Operating in the presence of irrelevant distractor objects. Visual Variations: Adapting to changes in background color and object color. The task suite for the basic tasks on the 14-DOF dual arm includes: Pick & Place: Grasping and placing wide range of objects. The training set includes toy fruits, computer mouse, colorful blocks, toy phones, and so on. The placed targets include colorful plates, baskets, boxes, and so on. Pick & Place Unseen: Grasping and placing unseen objects to unseen targets. We tested picking up OOD objects such as pears, peaches, purple block, and placing to novel targets, like pink plates, transparent plates, pink blocks, and so on. We found that although the training set scenes did not involve numerous distractor objects or unseen items, the model still robustly generalizes to new objects and targets with the correct semantics. Water Flower: This task involves grasping the handle of toy watering can to simulate the pouring action of watering plant. It rigorously tests the models fine-grained manipulation capabilities, as any action error could easily result in failure to grasp the handle or align with the flowerpot. Stack Cubes: The training data includes blocks of four different colors. The instructions require stacking several of these blocks together (up to three high), but without specific order. Put Flowers in Vase: Grasp bouquet lying flat on the table and insert it into vase. This task requires the model to precisely grasp the thin stems of the bouquet and align them with the opening of the vase, testing the accuracy of the manipulation. Stack Bowls: Stack bowls of three different colors according to specified color sequence. This task evaluates the models robustness to object positions and its ability to follow language instructions. Pour Fries: Open the lid of carton and pour the toy fries from inside it onto plate. This is relatively long-horizon task that requires the model to autonomously determine the next action based on its current progress. It tests both manipulation accuracy and long-horizon task capabilities. Sweep Rubbish: Grasp toy broom, sweep the randomly placed tissue paper trash on the table into dustpan, and then put down the broom. This is task that combines both long-horizon planning and dynamic control. The model must not only assess its current progress but also increase the sweeping speed to ensure the tissue paper rolls into the dustpan. Press Button: Press different buttons in specified color sequence. This is simple long-horizon task that also tests the models semantic following capabilities. Drawer Operation: Opening and closing drawer. This task primarily evaluates the accuracy of the manipulation. Fig. 8 illustrates several test scenarios for the basic tasks and presents video recordings of the models performance. F.2 Long-Horizon Planning Tasks We designed two distinct types of long-horizon planning tasks: (1) Stack Cubes in Requested Order and (2) Calculate and Place Symbol Blocks. We will now detail the setup for each and showcase corresponding demonstration videos. 20 Stack Cubes in Requested Order This task requires the model to stack scattered, multi-colored cubes from the tabletop into structure that matches specified shape and sequence given by language instruction. The target structures can range from one to three layers, with each layer containing one to three cubes. An example instruction is: Place the cubes in order: the first layer is blue and green block, the second layer is an orange block. The model must perform interleaved planning at each step based on this high-level command. This task involves very long sequence of actions, posing significant semantic-following challenge for conventional methods that do not employ explicit planning. In our experiments in Sec. 4.2.2, we demonstrate that our method holds distinct advantage on such long-horizon tasks. Calculate and Place Symbol Blocks This task requires the model to assemble scattered number and symbol blocks to form an arithmetic equation specified by language instruction, such as: Assemble the building blocks to complete the equation 21+3=? The initial scene may already contain partially arranged blocks, forcing the model to autonomously decide which block to grasp and place next. It must also place the correct blocks representing the calculated result. Similar to the stacking task, this task also involves long-horizon operational planning. Beyond that, it introduces an additional layer of complexity by requiring Chain-of-Thought (CoT) process: the model must first leverage the mathematical reasoning capabilities of the general-purpose VLM to compute the result, and then map this result back to the planning and action space. We use this task to validate the effectiveness and generalization capabilities of our interleaved planning framework on long-horizon reasoning tasks. Fig. 9 illustrates several test scenarios for the long-horizon planning tasks and presents video recordings of the models performance."
        },
        {
            "title": "G More Interleaved Planning Visualizations on diverse robotic Tasks",
            "content": "Similar to Fig. 4, in Fig. 10 we provide additional results of interleaved planning in real-world scenarios for reference. More Comparison using RFG and Naive Single-Step Denoising In Fig. 11, we provide additional comparison using RFG and naive single-step denoising for reference."
        },
        {
            "title": "I Usage of LLMs",
            "content": "In the final stages of preparing this manuscript, the authors used Large Language Model (LLM) solely for grammar checking and language polishing. The model assisted in improving sentence structure and correcting grammatical errors to enhance readability. 21 Figure 7 Attention Mask used for Different Conditioning Schemes. (a) Complete Denoise: Image prediction and action generation are performed separately, requiring total of N1 + N2 denoising steps. (b) Joint Denoise: Image prediction and action generation are performed simultaneously, denoising together for steps. (c) Single-Step Denoise: Action generation is conditioned directly on the context from the first denoising step of the image prediction. Further implementation details, including the construction of the concatenated sequence and the attention mask are provided in Appendix B. 22 Table 7 Evaluation on RoboTwin 2.0 Simulation (Clean vs Randomized, 50 tasks). The table shows success rates in percent (%) for various models. Models are trained using 50 clean demos per task, and evaluated using unseen instructions. Robotwin Tasks Adjust Bottle Beat Block Hammer Blocks Ranking Rgb Blocks Ranking Size Click Alarmclock Click Bell Dump Bin Bigbin Grab Roller Handover Block Handover Mic Hanging Mug Lift Pot Move Can Pot Move Pillbottle Pad Move Playingcard Away Move Stapler Pad Open Laptop Open Microwave Pick Diverse Bottles Pick Dual Bottles Place A2b Left Place A2b Right Place Bread Basket Place Bread Skillet Place Burger Fries Place Can Basket Place Cans Plasticbox Place Container Plate Place Dual Shoes Place Empty Cup Place Fan Place Mouse Pad Place Object Basket Place Object Scale Place Object Stand Place Phone Stand Place Shoe Press Stapler Put Bottles Dustbin Put Object Cabinet Rotate Qrcode Scan Object Shake Bottle Horizontally Shake Bottle Stack Blocks Three Stack Blocks Two Stack Bowls Three Stack Bowls Two Stamp Seal Turn Switch π0 RDT UP-VLA w/o Textual w/o Keyframe BagelVLA Clean Random Clean Random Clean Random Clean Random Clean Random Clean Random 90 43 19 7 63 44 83 96 45 98 11 84 58 21 53 0 85 80 27 57 31 27 17 23 80 41 34 88 15 37 20 7 16 10 36 35 28 62 54 68 68 18 99 97 17 42 91 3 27 56 21 5 1 11 3 24 80 8 13 3 36 21 1 22 2 46 50 6 12 1 6 4 1 4 6 2 45 0 10 1 2 0 11 7 6 29 13 18 15 1 51 60 0 1 24 41 4 23 81 77 3 0 61 80 64 74 45 23 72 25 8 43 2 59 37 2 42 3 1 10 5 50 19 6 78 4 56 12 1 33 1 15 15 35 41 21 33 50 4 84 74 2 21 51 76 1 35 75 37 0 0 12 9 32 43 14 31 16 9 12 0 11 0 32 20 0 13 1 1 2 1 27 6 5 17 4 7 2 0 17 0 5 6 7 24 4 18 5 1 51 45 0 2 17 30 0 100 66 38 21 69 54 81 99 4 45 4 20 48 51 79 8 86 2 52 82 74 56 63 71 97 20 66 86 45 74 31 27 56 36 76 32 76 79 7 7 56 47 100 98 8 61 42 69 34 43 17 16 0 0 41 72 35 28 0 0 0 0 0 7 13 0 21 7 18 31 4 1 20 16 26 0 24 48 0 27 1 0 1 4 24 0 12 56 0 0 2 23 68 54 0 0 1 12 2 26 100 63 32 19 84 78 100 0 76 6 0 51 60 86 5 57 0 74 89 59 53 71 82 95 37 23 97 36 94 15 14 44 46 77 48 63 59 12 45 68 66 99 98 15 59 42 70 29 37 7 18 2 0 60 60 26 63 0 0 0 0 0 2 6 0 13 5 22 33 7 6 29 26 56 1 40 71 12 34 3 10 1 7 35 0 15 50 0 4 3 84 82 0 2 7 21 1 14 99 80 46 23 95 98 87 97 18 44 2 64 9 22 64 6 62 8 15 33 50 55 42 62 55 8 46 82 21 76 18 18 40 31 45 33 44 93 10 21 72 38 87 83 5 29 37 88 23 52 56.72 4 13 25 5 43 29 41 37 1 3 1 7 2 3 31 1 3 14 11 15 19 17 2 2 0 6 55 0 35 2 12 6 8 27 9 23 52 0 1 4 3 60 44 2 31 12 48 8 10 100 87 84 45 85 100 91 99 38 75 12 87 78 92 92 27 96 83 93 79 81 90 91 99 63 94 100 57 97 62 46 66 71 87 61 90 94 42 52 81 77 100 100 45 95 63 90 77 49 14 16 4 2 20 35 51 41 0 8 1 32 0 1 30 0 37 0 34 56 12 29 26 11 0 5 58 0 34 5 14 3 0 21 2 29 58 10 0 21 32 73 74 5 6 52 8 30 15.92 75.26 20.87 Average 46. 16.34 34.50 13.72 52.92 15.16 54. 19.20 Table 8 Detailed results of evaluation on the Calvin ABCD benchmark. Method Tasks completed in row 1 3 4 5 Avg. Len π0* UP-VLA VPP w/o Keyframe-forecasting BagelVLA (Ours) 0.937 0.928 0.965 0.909 0. 0.832 0.865 0.909 0.792 0.954 0.740 0.815 0.866 0.676 0.893 0.629 0.769 0.820 0.546 0.824 0.510 0.699 0.769 0.422 0.741 3.65 4.08 4.33 3.35 4.41 Figure 8 Demos videos of BagelVLA on Basic Tasks. Figure 9 Demos videos of BagelVLA on Long-Horizon Planning Tasks. 24 Figure 10 Visualizations of interleaved planning results on diverse robotic tasks. Given global instruction and the current observation, BagelVLA leverages the context to identify the immediate subtask, predicts goal image for that subtask. 25 Figure 11 Predicted images using different denoising steps. The figure displays the generation results for the naive single-step denoise (Eq. 2) and RFG (Eq. 3) across varying denoising steps in real-world tasks and simulation scenarios. RFG demonstrates the capability to preserve backgrounds and achieve high-quality generation with very few steps. This provides strong support for reducing the inference latency of interleaved generation. 26 Figure 12 Prompt for Seed-1.5-VL-thinking. 27 Figure 13 Prompt for Seed-1.5-VL-thinking."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Tsinghua University"
    ]
}