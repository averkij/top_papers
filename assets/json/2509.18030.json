{
    "paper_title": "RadEval: A framework for radiology text evaluation",
    "authors": [
        "Justin Xu",
        "Xi Zhang",
        "Javid Abderezaei",
        "Julie Bauml",
        "Roger Boodoo",
        "Fatemeh Haghighi",
        "Ali Ganjizadeh",
        "Eric Brattain",
        "Dave Van Veen",
        "Zaiqiao Meng",
        "David Eyre",
        "Jean-Benoit Delbrouck"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce RadEval, a unified, open-source framework for evaluating radiology texts. RadEval consolidates a diverse range of metrics, from classic n-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical concept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT, TemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and standardize implementations, extend GREEN to support multiple imaging modalities with a more lightweight model, and pretrain a domain-specific radiology encoder, demonstrating strong zero-shot retrieval performance. We also release a richly annotated expert dataset with over 450 clinically significant error labels and show how different metrics correlate with radiologist judgment. Finally, RadEval provides statistical testing tools and baseline model evaluations across multiple publicly available datasets, facilitating reproducibility and robust benchmarking in radiology report generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 0 3 0 8 1 . 9 0 5 2 : r RadEval: framework for radiology text evaluation Justin Xu,* Xi Zhang Javid Abderezaei Julie Bauml Roger Boodoo Fatemeh Haghighi Ali Ganjizadeh Eric Brattain Dave Van Veen Zaiqiao Meng David Eyre Jean-Benoit Delbrouck,* University of Oxford University of Glasgow HOPPR justin.xu@ndm.ox.ac.uk, jeanbenoit.delbrouck@hoppr.ai https://github.com/jbdel/RadEval https://huggingface.co/IAMJB/RadEvalModernBERT"
        },
        {
            "title": "Abstract",
            "content": "We introduce RadEval, unified, open-source framework for evaluating radiology texts. RadEval consolidates diverse range of metrics from classic n-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical concept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT, TemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and standardize implementations, extend GREEN to support multiple imaging modalities with more lightweight model, and pretrain domainspecific radiology encoder demonstrating strong zero-shot retrieval performance. We also release richly annotated expert dataset with over 450 clinically significant error labels and show how different metrics correlate with radiologist judgment. Finally, RadEval provides statistical testing tools and baseline model evaluations across multiple publicly available datasets, facilitating reproducibility and robust benchmarking in radiology report generation."
        },
        {
            "title": "Introduction",
            "content": "Evaluating automated radiology report generation (RRG) systems remains fundamental challenge in the development of safe, accurate, and clinically useful medical AI. Unlike general-purpose text generation tasks, RRG demands evaluation methods that can assess not only linguistic fluency but also clinical factuality, domain-specific terminology, uncertainty calibration, and diagnostic relevance. In recent years, the evaluation of radiology report generation has steadily progressed: initial studies relied on classic natural language generation (NLG) metrics such as BLEU and ROUGE (Zhang et al., 2020; Chen et al., 2020); subsequent work *Equal contributions emphasized clinical accuracy through diseaselanguage inference classification and natural (NLI)-based metrics (Miura et al., 2021); this was followed by expert-annotated semantic graphs capturing entities and their relations (Delbrouck et al., 2022a); and, most recently, by evaluation approaches that leverage large language models (LLM) (Ostmeier et al., 2024; Bannur et al., 2024a; Huang et al., 2024). Despite efforts to establish fair benchmarking, such as shared metric codebases released for challenge tracks (Abacha et al., 2021; Delbrouck et al., 2023; Xu et al., 2024b) and public leaderboard (Zhang et al., 2024b), there is still no open-source repository that reproduces the different factuality-focused metrics, whose scores can vary with implementation choices. For instance, earlier studies have computed BERTScore with different pretrained models and settings varying the number of layers or whether scores are rescaled with baseline (Zhang et al., 2019) or swapped in F1CheXbert embeddings for the calculation (Smit et al., 2020b). Variants of the F1RadGraph metric likewise diverge depending on how they judge the correctness of entities and relations (Delbrouck et al., 2022b). Composite scores such as RadCliQ (Yu et al., 2023b) are similarly challenging to replicate. RadEval brings the following solutions: Unified open-source codebase: every factuality-oriented metric proposed to date is re-implemented in single, reproducible repository. Metric refinements: corrected and improved versions of existing metrics offer more faithful estimates of clinical correctness (Section 5 and Appendix B). Expanded expert test set: an updated, radiologist-annotated corpus enabling finegrained studies of how automatic metrics align with human judgments (Section 6). Ready-made baselines: published predictions from several widely-cited models are included so new systems can be benchmarked out-of-the-box. Built-in statistical testing: permutation and bootstrap tests (mirroring best practices in image captioning) enable users to determine whether score differences are statistically significant."
        },
        {
            "title": "2.1 Lexical Overlap Metrics",
            "content": "Early evaluation methods focused on string-level overlap between generated and reference reports, typically using metrics from the natural language processing (NLP) literature. ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) remain among the most common, measuring n-gram precision and recall. METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015) have also been adapted from image captioning literature. These metrics are straightforward to compute and require no domain-specific annotation or models, making them popular baselines. In addition, BERTScore (Zhang et al., 2019) has been proposed to address limitations of n-gram overlap metrics. Instead of relying on exact token matches, it computes semantic similarity between candidate and reference reports using contextualized embeddings from pretrained language model (e.g., BERT (Devlin et al., 2019)). Token-level similarity is calculated based on the sum of cosine similarities, offering improved sensitivity to semantic alignment and lexical variation. However, such metrics perform poorly in RRG settings due to their insensitivity to paraphrasing, semantic equivalence, or clinical correctness. Radiology reports are often sparse, redundant, or variable in linguistic expression, which causes n-gram metrics to underestimate report quality even when the clinical meaning is preserved."
        },
        {
            "title": "2.2 Clinical Concept-Based Metrics",
            "content": "To improve domain specificity, several works introduced evaluation metrics based on clinical concept extraction and comparison. F1CheXbert (Smit et al., 2020b) utilizes rule-based labeler that extracts 14 predefined CheXpert disease categories (Irvin et al., 2019) from both reference and generated reports, then computes an F1 score over presence/absence of these labels. SRR-BERT (Delbrouck et al., 2025) expands the label space to 55 labels, and also supports evaluation on structured reports. F1RadGraph (Delbrouck et al., 2022b) also offers more expressive alternative by representing reports as structured graphs of anatomical and observational entities and relations. pretrained graph extraction model is applied to both candidate and reference reports, and graph-level overlap metrics (e.g., precision, recall, F1) are computed. Although F1RadGraph has improved generality and some alignment with radiologist evaluations, it remains limited by the accuracy of the underlying parser and the quality of the training data. While the initial version of F1RadGraph was proposed by Jain et al. (2021), which computed entity and relation overlap separately and reported their average, it did not consider whether entities were matched based on textual spans, semantic types, or shared relations. This simplification may lead to overestimated alignment in complex cases. RaTEScore (Zhao et al., 2024) identifies medical entities and their types (e.g., anatomy, disease), applies synonym-aware semantic matching, and weights entities by diagnostic importance to better handle terminology variation and negation. Similar to BERTScore, the CheXbert vector similarity metric (Yu et al., 2022) measures alignment between generated and reference reports by computing the cosine similarity of their embeddings obtained via the CheXbert model (Smit et al., 2020a). In contrast to these standard clinical metrics, Temporal Entity F1 (Zhang et al., 2025a) is designed to assess temporal information quality in reports. It focuses on capturing the progression or stability of observations (e.g., worsening, improved, or stable) (Bannur et al., 2023), and is particularly useful for detecting temporal hallucinations."
        },
        {
            "title": "2.3 Composite and Learned Metrics",
            "content": "To improve alignment with human evaluations, ensemble-based or regression-trained metrics such as RadCliQ (Yu et al., 2023b) have been proposed. RadCliQ uses linear model trained on radiologistlabeled error counts, combining submetrics like BLEU, BERTScore, CheXbert vector similarity, and F1RadGraph. This strategy improves correlation with expert assessments but introduces interpretability challenges."
        },
        {
            "title": "Metrics",
            "content": "More recently, LLM-based generative evaluation has emerged as promising paradigm for RRG metric design. These approaches leverage foundation models reasoning and cross-domain generalization to assess report correctness, style, and completeness in free-text or structured manner. These LLM-driven evaluators offer high flexibility and often exhibit better alignment with radiologist judgment, especially on nuanced and out-ofdistribution findings. For instance, GREEN (Ostmeier et al., 2024) proposed an interpretable and open-source LLM-based evaluation pipeline using 7B parameter models, and includes normalized GREEN score, structured error summaries for interpretability, and zero-shot generalization across imaging modalities. CheXprompt (Zambrano Chaves et al., 2025) is GPT-based evaluator that detects and categorizes six types of clinically relevant errors: false positive and false negative findings, incorrect location or severity, false positive comparisons, and false negative comparisons. Similarly, FineRadScore (Huang et al., 2024) evaluates reports line by line, combining clinical severity with the number of incorrect lines. This reflects both the potential clinical risk and the effort required for correction, providing practical measure of report quality. RadFact (Bannur et al., 2024a) is also GPTbased evaluation suite that assesses the factuality of each sentence in generated report based on the corresponding reference sentences. It supports grounded evaluation and provides interpretable, sentence-level error analysis. Despite their flexibility and strong alignment with expert judgment, GPT-based evaluation methods face key limitations: high computational cost, deployment barriers due to model size or proprietary APIs, and potential inconsistencies in output. Clinical applications also raise data privacy concerns. RadEval addresses these challenges by standardizing interfaces and supporting lightweight, open-source alternatives like GREEN, enabling local, privacy-preserving, and reproducible evaluation for radiology report generation."
        },
        {
            "title": "3 RadEval",
            "content": "One of the critical obstacles to building AI systems that can match the accuracy and nuance of expert radiologists is the lack of standardized evaluation metrics. This gap hinders reliable analysis and comparison across different studies, and limits the real-world applicability of research progress. Despite rapid innovation in metric development, practical barriers remain for adoption and comparison: Each metric typically requires separate installation, dependencies, and data pre-processing pipelines. Some tools lack public code or require proprietary APIs. Evaluation outputs vary in format and interpretability. To mitigate this fragmentation, we propose system that consolidates access to wide range of RRG metrics, spanning from n-gram baselines to modern LLM evaluators. This system is designed to be modular, supporting plug-and-play integration of new metrics. By democratizing access to high-quality RRG evaluation tools, we aim to accelerate research on radiology report generation and encourage more standardized and reproducible benchmarking."
        },
        {
            "title": "4 Benchmarking",
            "content": "We conducted extensive benchmarking and evaluation of various models on publicly available datasets  (Table 3)  ."
        },
        {
            "title": "4.1 Datasets",
            "content": "For evaluation, we utilized the official test splits of MIMIC-CXR (Johnson et al., 2019b) and ReXGradient-160K (Zhang et al., 2025b), as well as the public validation set of CheXpert Plus (Chambon et al., 2024), as no official test split is available for the latter. Each study in these datasets may include multiple associated images, all of which were retained for evaluation. Depending on model support, either single representative image or all available images were used as input. We focused on specifically evaluating the generation of the Findings and Impression sections. Reports missing either section were excluded to ensure consistent evaluation across metrics. MIMIC-CXR widely used public dataset containing 377,110 chest X-ray images across 227,835 studies, each paired with radiology report (Johnson et al., 2019a). We use JPEG images from MIMIC-CXR-JPG instead of the original DICOMs. The official test split includes 2,347 studies with Findings sections and 2,224 with Impression sections. 223,"
        },
        {
            "title": "Comprises",
            "content": "CheXpert-Plus imagereport pairs from 187,711 studies across 64,725 patients (Chambon et al., 2024). We use its validation set, which contains 74 studies with Findings and 234 with Impression sections."
        },
        {
            "title": "The",
            "content": "ReXGradient-160K largest publicly available chest X-ray dataset to date in terms of patient coverage, including 160,000 imagereport pairs from 109,487 patients across 79 U.S. medical sites (Zhang et al., 2025b). Its official test set includes 10,000 studies with both Findings and Impression sections."
        },
        {
            "title": "4.2 Baselines",
            "content": "To evaluate the performance of existing RRG systems, we include representative set of baselines from different institutions and architectures: CheXpert-Plus (Chambon et al., 2024) Uses Swinv2-based vision encoder and two-layer BERT decoder. Two separate models are trained on MIMIC-CXR: one for the Findings section and another for the Impression. CheXagent (Chen et al., 2024) An instructiontuned foundation model for chest X-ray interpretation. It integrates vision encoder with crossmodal adapter to align visual and textual information. Training is conducted on CheXinstruct (Chen et al., 2024), diverse instruction dataset aggregated from 28 open-source medical datasets. MAIRA-2 (Bannur et al., 2024b) model designed for grounded radiology report generation, which involves not only producing clinically accurate text but also identifying the spatial locations of findings. It builds on the LLaVA architecture (Liu et al., 2023), incorporating frozen Rad-DINOMAIRA-2 vision encoder (Pérez-García et al., 2025), Vicuna-7B (Zheng et al., 2023) language model, and four-layer fully connected multilayer perceptron for vision-language alignment. Libra (Zhang et al., 2025a) temporallyaware multimodal large language model (MLLM) tailored for generating the Findings section in radiology reports. Unlike prior single-image methods, Libra leverages paired chest X-rays to capture disease progression. It integrates frozen RadDINO (Pérez-García et al., 2024) image encoder with the Meditron-7B (Chen et al., 2023) language model via Temporal Alignment Connector, which combines Layerwise Feature Extractor and Temporal Fusion Module to embed multi-scale visual changes over time into the model architecture. Med-CXRGen (Zhang et al., 2024a) Built on LLaVA-Med (Li et al., 2023), this model uses multistage visual instruction tuning and stitches multiple images for unified encoding. Separate models are trained for the Findings and Impression sections using the RRG24 dataset (Xu et al., 2024a)."
        },
        {
            "title": "RadEvalBERTScore",
            "content": "In this work, we also introduce domain-specific radiology language encoder trained using SimCSE (Gao et al., 2022) contrastive learning method for sentence embeddings to capture high-quality representations of radiology report text. We begin with ModernBERT-base architecture and train it on the Findings and Impression sections from MIMIC-CXR, CheXpert, and ReXGradient-160K. This pretraining setup allows the model to learn clinically meaningful semantic relationships within and across radiology reports. We demonstrate the effectiveness of our embedding model on zero-shot report-to-report retrieval task. The set-up mirrors classic textretrieval scenario: 1. small set of query reports and larger pool of candidate reports, each annotated with one or more radiology labels, are encoded with frozen text encoder. 2. For every query, we compute the cosine similarity to all candidates and obtain ranked list in descending similarity order. candidate is considered relevant to query if and only if the two reports share at least one label. For each of our experiments, we choose 10 queries and up to 200 positive candidates."
        },
        {
            "title": "Models",
            "content": "CheXpert 5200 HOPPR 8200 P@5 P@10 P@50 P@ mAP P@5 P@10 P@50 P@100 mAP 46.6 5.5 44.8 4.8 37.0 2.8 33.4 2.1 30.1 1.6 28.1 2.1 24.6 2.0 19.1 1.0 17.3 0.8 15.6 0.4 Devlin et al. (2019) Warner et al. (2024) 39.4 4.1 35.7 4.1 30.5 2.2 28.3 1.7 26.5 1.2 23.0 2.1 20.3 1.6 16.8 0.5 15.8 0.5 14.6 0.2 Sounack et al. (2025) 38.6 4.6 36.6 3.9 30.6 2.3 28.2 1.6 25.6 0.9 23.4 2.3 21.4 1.6 17.4 1.0 15.9 0.5 14.7 0.3 RadEvalBERT 60.3 3.1 56.4 2.6 46.4 2.0 41.4 1.7 36.4 1.2 38.6 2.0 34.8 2.2 26.9 1.0 23.7 1.0 20.1 0."
        },
        {
            "title": "Models",
            "content": "P@5 P@10 nDCG@5 nDCG@10 mAP"
        },
        {
            "title": "CheXbert Test Set",
            "content": "64.3 2.0 59.2 1.8 54.5 1.3 48.7 1.0 50.1 1.1 Devlin et al. (2019) Warner et al. (2024) 62.0 2.1 57.2 1.3 51.8 1.5 46.2 0.9 48.7 1.0 Sounack et al. (2025) 61.8 1.0 56.8 1.0 51.2 1.0 45.8 0.8 48.3 0.9 RadEvalBERT 70.2 2.2 64.6 1.5 58.9 1.6 53.3 1.1 53.4 1.0 Table 1: Zero-shot report-to-report retrieval performance of RadEvalBERTScore Evaluation. We evaluate on three datasets: CheXpert 5200 (five single-label categories), HOPPR 8200 (eight out-of-domain single-label categories), and the CheXbert multi-label test set. For CheXpert and HOPPR, we report Precision@{5, 10, 50, 100} and mean Average Precision (mAP); for CheXbert we report Precision@{5, 10}, normalized Discounted Cumulative Gain (nDCG@{5, 10}), and mAP. Values are presented as mean standard deviation over 10 random seeds."
        },
        {
            "title": "5.1 Metrics",
            "content": "Precision@K Fraction of the first retrieved reports that are relevant. hit is counted as soon as one label overlaps with the query. mean Average Precision (mAP) For each query, we compute Average Precision (i.e., the mean of the precision values at every rank where relevant report occurs. The scan stops at the last relevant rank, so items appearing afterwards cannot influence the score. mAP is the mean AP over all queries and reflects how early, on average, relevant reports are surfaced. nDCG@K graded variant that rewards richer matches. The gain between query and candidate is the number of shared labels. DCG is accumulated over the top positions with logarithmic discount 1/ log2(rank + 1); nDCG normalises by the ideal DCG, yielding score in [0, 1] where 1 means the system ranks the strongest overlaps highest."
        },
        {
            "title": "5.2 Datasets",
            "content": "CheXpert 5200 Five single-label categories (Atelectasis, Cardiomegaly, Edema, Consolidation, Pleural Effusion). We report Precision@{5, 10, 50, 100} and mAP."
        },
        {
            "title": "CheXbert Test Set",
            "content": "Multi-label reports with 14 chexpert possible findings (e.g., Airspace Opacity, Pneumonia, Support Devices). Because at most 20 positives exist per query, we report Precision@{5, 10} and nDCG@{5, 10} together with mAP. HOPPR 8200 This is an out-of-domain, single-label dataset used to evaluate generalization performance. The label categories include: acute rib fracture, air space opacity, cardiomegaly, lung nodule or mass, non acute rib fracture, pleural fluid, pneumothorax, and pulmonary artery enlargement. We report Precision@{5, 10, 50, 100} and mAP. This protocol provides complementary views of retrieval quality: Precision@K for top-K exact-hit rate, mAP for overall early-ranking performance, and nDCG@K for sensitivity to how many labels overlap."
        },
        {
            "title": "RadEval Expert Dataset",
            "content": "Dataset. We release an updated RadEval-expert dataset with board-certified radiologists annotating clinically significant and insignificant errors across different error categories. Building on ReXVal (Yu et al., 2023a), we annotate false predictions of findings, omissions of findings, incorrect locations/positions, incorrect severities, spurious comparisons, omissions of changes from prior studies, as well Table 2: Top-3 metrics by Kendalls τb (more negative is better; higher metric fewer errors). aligned = 95% CI < 0; misaligned = 95% CI > 0; ns = CI overlaps 0. Scope: pooled rows show (pairs, n); blocked rows show (blocks, pairs). Each study has K=3 candidates (Findings: 148 studies; Impression: 60)."
        },
        {
            "title": "Metric",
            "content": "τb [95% CI]"
        },
        {
            "title": "Scope",
            "content": "Overall (pooled) vs. total significant errors 0.183 [0.246, 0.122] green 0.133 [0.193, 0.071] srr_bert 0.107 [0.160, 0.052] radcliq radevalbertscore 0.076 [0.131, 0.017] rouge bertscore radgraph bleu chexbert 0.038 [0.091, 0.017] 0.027 [0.085, 0.032] 0.011 [0.068, 0.048] 0.034 [0.020, 0.086] 0.074 [ 0.012, 0.137] aligned aligned aligned aligned ns ns ns ns misaligned (pairs 194,376, 624) (pairs 194,376, 624) (pairs 194,376, 624) (pairs 194,376, 624) (pairs 194,376, 624) (pairs 194,376, 624) (pairs 194,376, 624) (pairs 194,376, 624) (pairs 194,376, 624) ALL (blocked) vs. total significant errors green bertscore bleu 0.195 [0.295, 0.087] 0.160 [0.260, 0.059] 0.153 [0.273, 0.029] aligned aligned aligned (blocks 159, pairs 477) (blocks 188, pairs 564) (blocks 89, pairs 267) ALL (blocked) vs. total insignificant errors 0.039 [0.147, 0.076] radcliq radevalbertscore 0.009 [0.126, 0.103] 0.008 [0.107, 0.094] bertscore ns ns ns (blocks 143, pairs 429) (blocks 133, pairs 399) (blocks 143, pairs 429) Impression only (blocked) vs. total significant errors 0.225 [0.399, 0.049] bertscore 0.215 [0.383, 0.042] rouge radevalbertscore 0.206 [0.399, 0.010] Findings only (blocked) vs. total significant errors green bleu bertscore 0.196 [0.314, 0.075] 0.168 [0.313, 0.020] 0.132 [0.246, 0.017] aligned aligned aligned aligned aligned aligned (blocks 57, pairs 171) (blocks 57, pairs 171) (blocks 53, pairs 159) (blocks 113, pairs 339) (blocks 68, pairs 204) (blocks 131, pairs 393) Per-category (blocked, significant-error endpoints) significant: false prediction of finding green radcliq bertscore 0.082 [0.198, 0.030] 0.052 [0.157, 0.052] 0.049 [0.158, 0.061] ns ns ns (blocks 134, pairs 400) (blocks 162, pairs 482) (blocks 162, pairs 482) significant: omission of finding significant: incorrect location/ position of finding significant: incorrect severity of finding significant: spurious comparison (not in reference) significant: omission of change from previous study significant: inarticulate report (grammar/readability) srr_bert chexbert green 0.512 [0.625, 0.390] 0.503 [0.626, 0.366] 0.250 [0.374, 0.126] aligned aligned aligned (blocks 91, pairs 271) (blocks 89, pairs 267) (blocks 113, pairs 337) 0.068 [0.213, 0.079] bertscore radevalbertscore 0.040 [0.201, 0.123] 0.018 [0.174, 0.139] rouge radevalbertscore 0.033 [0.223, 0.160] 0.001 [0.235, 0.219] bleu 0.007 [0.170, 0.191] rouge 0.153 [0.300, 0.001] bertscore 0.125 [0.263, 0.014] radcliq radevalbertscore 0.103 [0.247, 0.063] bleu green radgraph 0.127 [0.335, 0.099] 0.066 [0.241, 0.113] 0.027 [0.199, 0.137] ns ns ns ns ns ns ns ns ns ns ns ns (blocks 80, pairs 238) (blocks 76, pairs 226) (blocks 80, pairs 238) (blocks 56, pairs 168) (blocks 35, pairs 105) (blocks 61, pairs 181) (blocks 81, pairs 241) (blocks 81, pairs 241) (blocks 77, pairs 229) (blocks 37, pairs 111) (blocks 65, pairs 195) (blocks 61, pairs 183) 0.350 [0.560, 0.140] radcliq radevalbertscore 0.266 [0.476, 0.046] 0.251 [0.480, 0.013] bertscore aligned aligned aligned (blocks 35, pairs 105) (blocks 34, pairs 102) (blocks 35, pairs 105) as new category: inarticulate report/grammar. All error categories are labeled as either significant or insignificant. We also extend beyond the Impression to also cover the Findings section. The corpus comprises 208 studies (148 findings and 60 impressions), and each study has exactly K=3 annotated candidate reports per ground truth. Ground-truth reports come from MIMICCXR, CheXpert-Plus, and ReXGradient-160K, and candidate reports are generated by CheXagent, the CheXpert-Plus model, and MAIRA-2. Methods. We measure agreement between automatic metrics and radiologists judgments using Kendalls τb (Kendall, 1945) against radiologist error counts. We report: (1) pooled (overall) τb versus the total number of significant errors (treating each candidate independently; ignores study grouping), and (2) blocked (within-study, tieaware) τb with 95% block-bootstrap confidence intervals obtained by resampling study blocks (preserving section type). Blocked analyses cover: totals of significant and insignificant errors across all sections (ALL), the significant-error total within each section (Impression, Findings), and each individual significant-error category. We label aligned when the 95% CI lies entirely below 0 (higher metric fewer errors), misaligned when the CI lies entirely above 0 (higher scores more errors), and ns otherwise. Results. Overall (pooled vs. total significant errors), green, srr_bert, radcliq, and radevalbertscore show meaningful negative correlations (aligned), while rouge, bertscore, radgraph, and bleu are not significant; chexbert is misaligned (higher scores with more errors). Within studies (blocked, ALL), green, bertscore, and bleu are top for total significant errors (all aligned), and no metric tracks total insignificant errors (all ns). Table 2 reports correlations by section and across different error category types. From these results, green emerges as the most reliable single metric for tracking clinically significant errors, followed by srr_bert."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced RadEval, unified framework for evaluating RRG. By consolidating and standardizing diverse suite of evaluation metrics, including lexical overlap, clinical concept extraction, structured graph comparison, and LLM-based scoring, RadEval addresses longstanding reproducibility and benchmarking challenges in the RRG domain. We refined existing metrics, released high-fidelity expert-annotated test set, and benchmarked stateof-the-art report generation systems across multiple publicly available datasets. In addition, we demonstrated the utility of new domain-specific sentence encoder through zero-shot retrieval task and introduced an updated lightweight version of the GREEN metric capable of cross-modality evaluation. RadEvals modular architecture will help facilitate robust, reproducible, and clinically grounded evaluation ultimately helping accelerate the safe deployment of radiology AI systems."
        },
        {
            "title": "Limitations",
            "content": "While RadEval already unifies broad set of automated radiology text evaluation metrics, recently proposed LLM-based metrics (e.g., CheXprompt, FineRadScore, and RadFact) are not currently implemented due to their reliance on LLM APIs or lack of standardization though they remain valuable future additions. Additionally, some metrics depend on upstream parsers that may introduce noise. Current evaluations also focus primarily on chest X-ray radiology reports written in English from institutions in the U.S., limiting generalizability across languages and geographical regions. Finally, we aim to continue to expand the expertlabeled dataset with additional reports and clinical annotators, as well as to compute detailed correlation analyses across all automated metrics and annotations to better assess metric alignment with clinical judgment."
        },
        {
            "title": "References",
            "content": "Asma Ben Abacha, Yassine Mrabet, Yuhao Zhang, Chaitanya Shivade, Curtis Langlotz, and Dina DemnerFushman. 2021. Overview of the mediqa 2021 shared task on summarization in the medical domain. NAACL-HLT 2021, page 74. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan. Association for Computational Linguistics. Shruthi Bannur, Kenza Bouzid, Daniel Castro, Anton Schwaighofer, Anja Thieme, Sam Bond-Taylor, Maximilian Ilse, Fernando Pérez-García, Valentina Salvatelli, Harshita Sharma, and 1 others. 2024a. Maira2: Grounded radiology report generation. arXiv preprint arXiv:2406.04449. Shruthi Bannur, Kenza Bouzid, Daniel C. Castro, Anton Schwaighofer, Anja Thieme, Sam Bond-Taylor, Maximilian Ilse, Fernando Pérez-García, Valentina Salvatelli, Harshita Sharma, Felix Meissen, Mercy Ranjit, Shaury Srivastav, Julia Gong, Noel C. F. Codella, Fabian Falck, Ozan Oktay, Matthew P. Lungren, Maria Teodora Wetscherek, and 2 others. 2024b. Maira-2: Grounded radiology report generation. Preprint, arXiv:2406.04449. Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Pérez-García, Maximilian Ilse, Daniel C. Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, Anton Schwaighofer, Maria Wetscherek, Matthew P. Lungren, Aditya Nori, Javier Alvarez-Valle, and Ozan Oktay. 2023. Learning to exploit temporal structure for biomedical visionlanguage processing. Preprint, arXiv:2301.04558. Pierre Chambon, Jean-Benoit Delbrouck, Thomas Sounack, Shih-Cheng Huang, Zhihong Chen, Maya Varma, Steven QH Truong, Chu The Chuong, and Curtis P. Langlotz. 2024. Chexpert plus: Augmenting large chest x-ray dataset with text radiology reports, patient demographics and additional image formats. Preprint, arXiv:2405.19538. Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut. 2023. Meditron-70b: Scaling medical pretraining for large language models. Preprint, arXiv:2311.16079. Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. 2020. Generating radiology reports via memory-driven transformer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 14391449. Zhihong Chen, Maya Varma, Justin Xu, Magdalini Paschali, Dave Van Veen, Andrew Johnston, Alaa Youssef, Louis Blankemeier, Christian Bluethgen, Stephan Altmayer, Jeya Maria Jose Valanarasu, Mohamed Siddig Eltayeb Muneer, Eduardo Pontes Reis, Joseph Paul Cohen, Cameron Olsen, Tanishq Mathew Abraham, Emily B. Tsai, Christopher F. Beaulieu, Jenia Jitsev, and 4 others. 2024. vision-language foundation model to enhance efficiency of chest xray interpretation. Preprint, arXiv:2401.12208. Jean-Benoit Delbrouck, Pierre Chambon, Christian Bluethgen, Emily Tsai, Omar Almusa, and Curtis Langlotz. 2022a. Improving the factual correctness of radiology report generation with semantic rewards. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 43484360. Jean-Benoit Delbrouck, Pierre Chambon, Christian Bluethgen, Emily Tsai, Omar Almusa, and Curtis Langlotz. 2022b. Improving the factual correctness of radiology report generation with semantic rewards. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 43484360, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jean-Benoit Delbrouck, Maya Varma, Pierre Chambon, and Curtis Langlotz. 2023. Overview of the radsum23 shared task on multi-modal and multianatomical radiology report summarization. In The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 478 482. Jean-Benoit Delbrouck, Justin Xu, Johannes Moll, Alois Thomas, Zhihong Chen, Sophie Ostmeier, Asfandyar Azhar, Kelvin Zhenghao Li, Andrew Johnston, Christian Bluethgen, Eduardo Reis, Mohamed Muneer, Maya Varma, and Curtis Langlotz. 2025. Automated structured radiology report generation. Preprint, arXiv:2505.24223. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2022. Simcse: Simple contrastive learning of sentence embeddings. Preprint, arXiv:2104.08821. Alyssa Huang, Oishi Banerjee, Kay Wu, Eduardo Pontes Reis, and Pranav Rajpurkar. 2024. Fineradscore: radiology report line-by-line evaluation technique generating corrections with severity scores. arXiv preprint arXiv:2405.20613. Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David A. Mong, Safwan S. Halabi, Jesse K. Sandberg, Ricky Jones, David B. Larson, Curtis P. Langlotz, Bhavik N. Patel, Matthew P. Lungren, and Andrew Y. Ng. 2019. Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison. Preprint, arXiv:1901.07031."
        },
        {
            "title": "Saahil",
            "content": "Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew P. Lungren, Andrew Y. Ng, Curtis P. Langlotz, and Pranav Rajpurkar. 2021. Radgraph: Extracting clinical entities and relations from radiology reports. Preprint, arXiv:2106.14463. Alistair E. W. Johnson, Tom J. Pollard, Nathaniel R. Greenbaum, Matthew P. Lungren, Chih ying Deng, Yifan Peng, Zhiyong Lu, Roger G. Mark, Seth J. Berkowitz, and Steven Horng. 2019a. Mimic-cxrjpg, large publicly available database of labeled chest radiographs. Preprint, arXiv:1901.07042. Alistair EW Johnson, Tom Pollard, Seth Berkowitz, Nathaniel Greenbaum, Matthew Lungren, Chihying Deng, Roger Mark, and Steven Horng. 2019b. Mimic-cxr, de-identified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317. M. G. Kendall. 1945. The treatment of ties in ranking problems. Biometrika, 33(3):239251. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2023. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Preprint, arXiv:2306.00890. Chin-Yew Lin. 2004. Rouge: package for automatic In Text summarization evaluation of summaries. branches out, pages 7481. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Preprint, arXiv:2304.08485. Yasuhide Miura, Yuhao Zhang, Emily Tsai, Curtis Langlotz, and Dan Jurafsky. 2021. Improving factual completeness and consistency of image-to-text radiology report generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 52885304. Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen, Arne Edward Michalson Md, Michael Moseley, Curtis Langlotz, Akshay Chaudhari, and Jean-Benoit Delbrouck. 2024. GREEN: Generative radiology report evaluation and error notation. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 374390, Miami, Florida, USA. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Fernando Pérez-García, Harshita Sharma, Sam BondTaylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Matthew P. Lungren, Maria Wetscherek, Noel Codella, Stephanie L. Hyland, Javier Alvarez-Valle, and Ozan Oktay. 2024. Raddino: Exploring scalable medical image encoders beyond text supervision. Preprint, arXiv:2401.10815. Fernando Pérez-García, Harshita Sharma, Sam BondTaylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Matthew P. Lungren, Maria Teodora Wetscherek, Noel Codella, Stephanie L. Hyland, Javier Alvarez-Valle, and Ozan Oktay. 2025. Exploring scalable medical image encoders beyond Nature Machine Intelligence, text supervision. 7(1):119130. Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Ng, and Matthew Lungren. 2020a. Combining automatic labelers and expert annotations for accurate radiology report labeling using BERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 15001519, Online. Association for Computational Linguistics. Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Ng, and Matthew Lungren. 2020b. Combining automatic labelers and expert annotations for accurate radiology report labeling using bert. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 15001519. Thomas Sounack, Joshua Davis, Brigitte Durieux, Antoine Chaffin, Tom Pollard, Eric Lehman, Alistair EW Johnson, Matthew McDermott, Tristan Naumann, and Charlotta Lindvall. 2025. Bioclinical modernbert: state-of-the-art long-context encoder for biomedical and clinical nlp. arXiv preprint arXiv:2506.10896. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image deIn Proceedings of the IEEE scription evaluation. conference on computer vision and pattern recognition, pages 45664575. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, and 1 others. 2024. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. arXiv preprint arXiv:2412.13663. Justin Xu, Zhihong Chen, Andrew Johnston, Louis Blankemeier, Maya Varma, Jason Hom, William J. Collins, Ankit Modi, Robert Lloyd, Benjamin Hopkins, Curtis Langlotz, and Jean-Benoit Delbrouck. 2024a. Overview of the first shared task on clinical text generation: RRG24 and discharge me!. In Proceedings of the 23rd Workshop on Biomedical Natural Language Processing, pages 8598, Bangkok, Thailand. Association for Computational Linguistics. Justin Xu, Zhihong Chen, Andrew Johnston, Louis Blankemeier, Maya Varma, Jason Hom, William Collins, Ankit Modi, Robert Lloyd, Benjamin Hopkins, and 1 others. 2024b. Overview of the first shared task on clinical text generation: Rrg24 and Yuhao Zhang, Derek Merck, Emily Tsai, Christopher Manning, and Curtis Langlotz. 2020. Optimizing the factual correctness of summary: study of summarizing radiology reports. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 51085120. Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. RaTEScore: metric for radiology report generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1500415019, Miami, Florida, USA. Association for Computational Linguistics. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685. discharge me!. In Proceedings of the 23rd Workshop on Biomedical Natural Language Processing, pages 8598. F. Yu, M. Endo, R. Krishnan, I. Pan, A. Tsai, E. P. Reis, E. Kaiser Ururahy Nunes Fonseca, H. Lee, Z. Shakeri, A. Ng, C. Langlotz, V. K. Venugopal, and P. Rajpurkar. 2023a. Radiology report expert evaluation (rexval) dataset (version 1.0.0). RRID:SCR_007345. Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, Henrique Min Ho Lee, Zahra Shakeri Hossein Abad, Andrew Y. Ng, Curtis P. Langlotz, Vasantha Kumar Venugopal, and Pranav Rajpurkar. 2022. Evaluating progress in automatic chest x-ray radiology report generation. medRxiv. Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, Henrique Min Ho Lee, Zahra Shakeri Hossein Abad, Andrew Ng, and 1 others. 2023b. Evaluating progress in automatic chest x-ray radiology report generation. Patterns, 4(9). Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang, and 1 others. 2025. clinically accessible small multimodal radiology model and evaluation metric for chest x-ray findings. Nature Communications, 16(1):3108. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675. Xi Zhang, Zaiqiao Meng, Jake Lever, and Edmond S. L. Ho. 2025a. Libra: Leveraging temporal images for biomedical radiology analysis. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1727517303, Vienna, Austria. Association for Computational Linguistics. Xi Zhang, Zaiqiao Meng, Jake Lever, and Edmond S.L. Ho. 2024a. Gla-AI4BioMed at RRG24: Visual instruction-tuned adaptation for radiology report genIn Proceedings of the 23rd Workshop on eration. Biomedical Natural Language Processing, pages 624 634, Bangkok, Thailand. Association for Computational Linguistics. Xiaoman Zhang, Julián N. Acosta, Josh Miller, Ouwen Huang, and Pranav Rajpurkar. 2025b. Rexgradient160k: large-scale publicly available dataset of chest radiographs with free-text reports. Preprint, arXiv:2505.00228. Xiaoman Zhang, Hong-Yu Zhou, Xiaoli Yang, Oishi Banerjee, Julián Acosta, Josh Miller, Ouwen Huang, and Pranav Rajpurkar. 2024b. Rexrank: public leaderboard for ai-powered radiology report generation. arXiv preprint arXiv:2411.15122."
        },
        {
            "title": "A Score table",
            "content": "Table 3: Benchmarking results across multiple models on standard datasets under default system prompts. l E"
        },
        {
            "title": "T\nR\nE\nB",
            "content": ". T t E"
        },
        {
            "title": "1\nF",
            "content": "d i"
        },
        {
            "title": "Q\nL\nC",
            "content": "1 v"
        },
        {
            "title": "E\nT\na\nR",
            "content": "e S 1 c 1 4"
        },
        {
            "title": "T\nR\nE\nB",
            "content": "e S"
        },
        {
            "title": "R\nE",
            "content": "2 8 . . 7 2 1 . 2 4 . 9 0 1 . 5 1 1 . 7 0 . 8 4 1 . 0 0 2 . 2 7 . 8 6 4 . 1 4 4 . 4 6 . 5 6 4 . 2 0 4 . 9 9 7 9 . . 4 6 3 . 5 6 . 1 3 3 . 6 0 4 . 5 5 . 7 8 1 . 5 0 1 0 . 0 4 4 . 4 3 6 . 2 3 1 . 9 1 0 . 6 2 1 . 9 2 6 . 5 3 3 . 5 3 5 . 2 2 5 . 2 2 8 . 6 1 5 . 4 1 0 . 8 1 4 . 6 1 7 . 7 3 7 . 6 1 . 1 7 8 . 5 4 . 1 2 6 . 5 0 4 . 8 0 6 . 2 1 2 . 6 5 5 . 9 7 6 . 3 7 7 . 3 5 7 . 1 5 6 . 6 5 6 . 6 3 6 . 8 8 4 . 6 4 6 . 6 1 7 . 6 4 8 . 0 5 7 . 0 6 7 . 1 5 7 . 1 1 7 . 6 4 8 . 4 1 6 . 7 9 6 . 7 6 7 . 8 5 6 . 7 8 7 . 9 7 7 . 2 1 5 . 7 1 5 . 1 1 5 . 0 1 5 . 1 0 5 . 0 7 4 . 3 7 4 . 0 4 4 . 7 7 4 . 5 5 4 . 0 6 5 . 2 4 5 . 4 2 5 . 5 3 5 . 7 2 5 . 0 1 5 . 4 6 4 . 4 3 4 2 . 9 5 . 0 5 . 6 3 4 . 2 6 5 . 9 2 . 1 0 3 . 4 1 3 . 1 8 . 8 2 5 . 6 7 4 . 7 0 . 1 9 4 . 1 7 4 . 0 9 . 3 8 4 . 1 5 5 . 4 7 . 2 5 4 . 5 2 5 . 7 3 . 7 0 5 . 3 5 4 . 5 1 5 5 . . 0 1 2 . 0 3 . 7 5 1 . 0 4 1 4 9 . . 4 4 2 8 . 5 5 7 . 8 4 2 . 1 4 9 . 0 8 . 7 4 0 . 4 4 7 . 5 4 2 . 8 4 1 . 0 6 1 . 4 3 . 3 5 0 . 0 6 5 . 1 6 1 . 9 5 4 . 0 5 1 . 9 5 . 4 5 . 6 1 7 . 0 4 . 7 1 . 4 3 . 6 6 . 2 i F t l u - p C 5 . 0 5 9 . 6 4 5 . 4 4 1 . 7 1 . 4 4 0 . 1 4 9 . 3 3 1 . 7 4 0 . 8 4 9 . 2 8 . 3 4 8 . 1 4 1 . 4 7 . 4 4 . 6 8 . 5 . 2 9 . 1 6 . 7 1 2 . 7 1 5 . 5 1 0 . 1 0 . 1 1 2 . 9 i r n a a l - p C 5 . 4 4 4 . 4 2 . 1 5 8 . 0 5 9 . 2 5 . 6 0 . 8 6 . 2 n i e C M - 7 . 9 4 2 . 8 4 6 . 4 4 . 6 4 8 . 5 4 8 . 2 4 0 . 9 3 2 . 4 5 4 . 0 4 . 9 4 5 . 9 4 6 . 7 4 9 . 4 7 . 5 9 . 1 . 5 6 . 4 0 . 3 4 . 7 1 0 . 7 1 8 . 3 9 . 2 1 3 . 2 1 9 . 0 1 i r t R I I - 8 . 6 3 4 . 5 3 2 . 6 4 3 . 9 9 . 2 3 . 2 8 . 6 1 5 . 8 i F T 0 6 1 - i G 0 . 1 4 . 5 4 7 . 9 3 7 . 5 4 0 . 3 4 2 . 9 3 9 . 0 0 . 2 4 0 . 1 5 9 . 8 4 3 . 3 5 . 3 6 . 8 . 3 3 . 2 0 . 1 2 8 . 6 1 4 . 4 1 . 3 3 . 4 1 i r t K 0 6 1 - i G 8 . 7 3 6 . 0 3 . 0 3 5 . 1 3 2 . 0 4 . 0 2 . 1 4 ."
        },
        {
            "title": "R\nE",
            "content": "8 . 4 2 9 . 2 2 2 . 1 2 9 . 5 1 2 . 6 1 3 . 3 1 . 3 7 . 6 1 2 . 4 2 1 . 2 2 6 . 8 1 0 . 8 5 . 7 1 3 . 5 1 1 . 0 2 7 . 1 1 5 . 4 2 7 . 0 2 . 9 5 . 8 1 3 . 8 1 9 . 1 6 . 3 1 F"
        },
        {
            "title": "N\nE\nE\nR\nG",
            "content": "L l 4 . 7 2 8 . 5 7 . 3 2 6 . 8 1 9 . 7 6 . 4 1 4 . 3 7 . 8 1 8 . 6 2 2 . 4 2 9 . 0 2 3 . 0 2 9 . 9 1 3 . 7 1 6 . 2 2 5 . 3 1 9 . 5 2 0 . 2 2 1 . 0 1 5 . 0 2 1 . 0 2 5 . 2 . 3 . 4 1 2 . 7 1 . 1 2 2 . 8 8 1 . 0 8 . 4 6 1 . 3 5 2 . 6 3 . 1 3 2 . 7 5 2 . 7 2 . 7 1 2 . 9 0 2 . 5 8 . 6 3 2 . 6 4 1 . 7 1 . 5 3 2 . 7 4 1 . 9 1 . 4 1 2 6 . 7 . 8 . 2 6 2 . 2 5 . 5 5 2 . 9 1 2 . 0 2 . 5 5 1 . 7 3 2 . 6 6 . 6 9 2 . 6 0 3 . 1 8 . 2 9 2 . 0 8 2 . 4 2 . 0 5 2 . 8 0 2 . 5 4 . 6 3 3 . 6 9 2 . 8 4 . 5 6 3 8 3 . . 3 2 - R - s - p C g h 3 - 0 . 1 - i 7 - 0 . 1 - i 2 - M - R - s - p C - R - M l - p C g h 3 - 0 . 1 - i 7 - 0 . 1 - i 2 - M - R - s - p C - R - s P - p C 7 - 0 . 1 - i e e 2 - M - R - M l - p C d M"
        },
        {
            "title": "B Refined GREEN Metric",
            "content": "To extend the capabilities of the GREEN metric, we finetuned compact Gemma-2B model on the original GREEN dataset along with an additional 50,000 annotated radiology report pairs spanning multiple imaging modalities, including CT, MRI, and ultrasound. This represents an evolution beyond the original implementation, which focused exclusively on chest X-rays. By leveraging smaller, more lightweight language model, we achieve substantial improvements in computational efficiency averaging inference times of 23 seconds per report while maintaining performance comparable to larger models such as Llama and Phi. This reduced resource footprint makes the updated GREEN model more practical for large-scale or real-time deployment settings. Our work underscores the potential for continued improvement by incorporating more diverse imaging contexts and exploring even lighter architectures without sacrificing clinical alignment or interpretability."
        }
    ],
    "affiliations": [
        "HOPPR",
        "University of Glasgow",
        "University of Oxford"
    ]
}