{
    "paper_title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
    "authors": [
        "Md Mofijul Islam",
        "Alexi Gladstone",
        "Sujan Sarker",
        "Ganesh Nanduru",
        "Md Fahim",
        "Keyan Du",
        "Aman Chadha",
        "Tariq Iqbal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments."
        },
        {
            "title": "Start",
            "content": "Embodied Referring Expression Comprehension in Human-Robot Interaction Md Mofijul Islam, Alexi Gladstone, Sujan Sarker, Ganesh Nanduru, Md Fahim, Keyan Du, Aman Chadha, Tariq Iqbal University of Virginia, Stanford University, University of Dhaka, Amazon GenAI 5 2 0 2 6 ] . [ 1 8 5 5 6 0 . 2 1 5 2 : r Abstract As robots enter human workspaces, there is crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments. ACM Reference Format: Md Mofijul Islam, Alexi Gladstone, Sujan Sarker, Ganesh Nanduru, Md Fahim, Keyan Du, Aman Chadha, Tariq Iqbal, University of Virginia, Stanford University, University of Dhaka, Amazon GenAI. 2025. Embodied Referring Expression Comprehension in Human-Robot Interaction. In Proceedings of March 1619, 2026 (HRI 26). ACM, New York, NY, USA, 14 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nRobots that interact with people, from service robots in public\nspaces to assistive systems at home and work, must understand\ninstructions given by humans naturally and intuitively [54]. In hu-\nman‚Äìhuman communication, people utilize multimodal signals to",
            "content": "Work does not relate to position at Amazon. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. HRI 26, 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/XXXXXXX.XXXXXXX Figure 1: Refer360 data collection setup in human-robot interaction context (left). Here, the person is pointing towards an object while verbally describing it. Interaction frames from three different views (exo, ego, and depth). Highlighting the canonical frames, i.e., frames where the subject precisely points to an object (right). express object reference and intent unambiguously [44, 66]. Similarly, in humanrobot interaction (HRI) scenarios, people often communicate their instructions not solely through language; rather, they are grounded in embodied exchanges that combine verbal utterances with nonverbal cues, such as gestures, gaze, and posture [4, 37, 38, 45, 61]. For instance, in task scenarios such as passing an object or assembling an item, people rely on both gaze and pointing gestures to specify what this one or that side refers to. Endowing robots with the capability to understand human instructions unambiguously would enable them to be integrated smoothly into human teams and support more natural and intuitive interactions with people [5, 20, 24, 29, 71, 72]. In the past several years, embodied reference expression (E-RFE) comprehension has received extensive attention in humanrobot interaction (HRI), particularly in domains such as assistive interaction and collaborative task execution [22, 23, 29, 59]. The primary goal of much of this work is to enable robots to accurately infer human intent by grounding verbal commands in complementary nonverbal cues such as gaze, gesture, and posture. Such capabilities would allow robots to seamlessly assist humans in assistive scenarios by accurately interpreting instructions and proactively coordinating with human partners in collaborative tasks, thereby improving efficiency and reducing errors. To address this challenge, many early HRI systems often relied on single-modality inputs or highly scripted interactions, which constrained their adaptability and robustness. In contrast, real-world HRI demands robots that can comprehend nuanced, multimodal instructions across dynamic environments and temporal contexts. While the ability to interpret human instructions is essential for robots, current approaches face significant challenges in real-world HRI 26, Edinburgh, Scotland, Islam et al. Table 1: Comparison of the QA datasets. Existing VQA and EQA datasets do not contain nonverbal gestures (NV), multiple verbal (V) perspectives (MP), contrastive (C), and ambiguous (A) data samples, and outdoor scene data. Embodied (E) interactions refer to humans interacting using multimodal expressions. Embodied interactions refer to an agent navigating in an environment. Sythetic Environment. Please check the supplementary for detailed comparison with other related datasets. Datasets NV MP VQA [3] GRiD-3D [32] EQA [6] MT-EQA [73] CAESAR[24] EQA-MX[20] YouRefIt [5] Refer360 Views Exo Ego Image Frames Interaction Samples Environment Type 204K 8K 5K 19K 841K 750K 497K 1.3M 614K 445K 5K 19K 1M 8K 4K 14K Internet Simulated Simulated Simulated Simulated Simulated Indoor Image Image Interactive Interactive Image Image Video Indoor+Outdoor Video HRI scenarios. primary bottleneck is the scarcity of comprehensive datasets that capture embodied interactions across diverse and natural settings. Existing datasets, such as YouRefIt [5] and MoGaze [29], provide real-world embodied interactions, but they have critical limitations that hinder the development of robust comprehension models. First, they include verbal utterances tied to the speakers or observers perspective (e.g., left ball vs. right ball), introducing perspective bias that restricts generalization. Second, their reliance on single-view (exo or ego) recordings leads to viewpoint bias, limiting performance in varied environments. Multi-view data (including ego, exo, and top views) is needed to overcome occlusions and capture interaction nuances. Third, they only partially represent nonverbal gesturestypically capturing either pointing or gazedespite both being complementary for robust comprehension. Finally, most data are collected indoors in constrained settings with stationary cameras, which reduces ecological validity. Together, these limitations prevent existing datasets from supporting models that can fully comprehend embodied interactions in diverse, unconstrained real-world HRI contexts. comparison of existing datasets is provided in Table 1. To address these limitations, we developed comprehensive and diverse dataset, called Refer360, to facilitate the study of humanrobot interactions in real-world settings. We have collected data across both indoor and outdoor environments with varied lighting conditions, object arrangements, and scene appearances  (Fig. 1)  . Using multimodal sensing system, we captured interactions from multiple perspectivesincluding egocentric and exocentric visual viewpointsas well as depth, skeleton, infrared, audio, gaze, and pupil tracking. Expert annotators annotated all scenes and verbal utterances. The dataset comprises contributions from 66 participants across 392 sessions, spanning both laboratory and outsidelaboratory environments. In total, the Refer360 dataset contains 13, 990 multimodal interactions, amounting to 3.2 million synchronized frames (28, 736 canonical frames) and 17.62 hours of recording time. Beyond dataset biases, another significant challenge in understanding E-RFEs is extracting complementary representations from multimodal data, e.g., verbal instructions and nonverbal gestures. While existing multimodal models fuse multimodal representations from the frozen pre-trained encoders, leading to performance enhancements across various tasks, the representation gap between these frozen representations can result in suboptimal multimodal representations. Several approaches have been proposed in the literature to reduce the representation gap of unimodal signals [1, 33, 34, 40]. However, fusing these frozen representations using self-attention or cross-attention approach can overlook modalityspecific cues, limiting the models ability to effectively leverage and integrate the distinct, complementary cues in multimodal interaction signals (verbal and non-verbal). Thus, extracting salient representations across modalities can help to extract complementary representations. To address this challenge, we introduce multimodal guided residual module, MuRes, for learning complementary multimodal representations. Unlike existing approaches, MuRes not only aligns cross-modal features but also preserves modality-specific cues through guided residual connections. Inspired by the information bottleneck principle [2, 25, 58, 60, 64, 67, 68], we have designed MuRes as representation bottleneck that extracts and reinforces the most relevant signals from each modality. This selective reinforcement yields fused representations that are both aligned and modalityaware, enabling more comprehensive understanding of multimodal embodied interactions. Moreover, MuRes is lightweight and can be seamlessly integrated as an adapter into existing multimodal models, enhancing their representation capacity while remaining practical for robotic implementation. To evaluate the effectiveness of our module, we conduct extensive experimental analysis on our Refer360 dataset for comprehending referring expressions, alongside three benchmark datasets from the literature. Furthermore, we have integrated MuRes into existing multimodal models to show the effectiveness of utilizing MuRes for extracting salient complementary multimodal representations. Our experimental analysis suggests that MuRes helps improve the performance of these multimodal models. For example, integrating MuRes improved the CLIP models performance (IOU-25) by 3.4% and 4.99% on the Refer360 and CAESAR-PRO [19] datasets, respectively. In addition to evaluating our method on embodied referring expression (E-RFE) datasets, we also examine its effectiveness on broader visual question answering (VQA) tasks using the Embodied Referring Expression Comprehension in Human-Robot Interaction HRI 26, Edinburgh, Scotland, ScienceQA [43] and A-OKVQA [53] datasets, which reinforced the effectiveness of the proposed method on broader set of embodied interaction tasks. The results indicated that MuRes boosted the VQA accuracy of the VisualBERT model on the ScienceQA [43] dataset by 4.58% and the ViLT [27] model on the A-OKVQA [53] dataset by 2.86%. These performance improvements depict the significance of our proposed guided residual model for extracting complementary multimodal representations for various downstream tasks. The outcomes of these experiments promise to advance multimodal instruction comprehension by providing comprehensive dataset and valuable insights for improving state-of-the-art representation learning techniques for HRI."
        },
        {
            "title": "2 Related Work\n2.1 Embodied Referring Expression Datasets\nIn the literature, embodied interactions are studied in two forms.\nThe first involves agents navigating an environment to gather visual\ndata following verbal instructions [6, 73]. The second focuses on\ncomprehending referring expressions involving verbal and nonver-\nbal cues, where agents interpret and respond to human instructions\n[5, 9, 19, 24, 31, 62, 76]. We explore the second aspect of embod-\nied interactions, focusing on understanding multimodal referring\nexpressions.",
            "content": "Several datasets have been developed in the literature to study embodied referring expressions (E-RFE). For example, Chen et al. [5] developed an embodied referring expressions dataset where human refers to an object using verbal and pointing gestures. In their proposed dataset, Kratzer et al. [29] focused on capturing the human body motion and eye gaze. To incorporate both verbal and nonverbal signals, Islam et al. [24] developed synthetic dataset by generating nonverbal cues (pointing gesture and gaze) in virtual environment and template-based verbal instructions. While these datasets demonstrated the importance of developing diverse datasets towards comprehensively understanding E-RFE, they predominantly focus on indoor settings [5], static camera views without agent or human motion [29], scripted human interactions [19], limited sensor modalities [5, 29], and synthetic environments [19, 20, 24]. Recent HRI work has also highlighted that datasets constrained to narrow scenarios cannot capture the richness of situated human instructions, particularly when gestures, perspective-taking, and environmental diversity are involved [7, 16, 28, 55, 57, 63]. Therefore, these datasets provide limited data samples for developing models for comprehensive understanding of E-RFE in realistic HRI contexts."
        },
        {
            "title": "2.2 Multimodal Representation Learning\nThere has been significant progress in the last several years on\ndeveloping multimodal models, particularly focusing on Visual\nQuestion Answering (VQA) tasks [1, 10, 12‚Äì14, 27, 30, 33, 34, 36,\n40, 42, 52, 70, 74, 75, 77]. VQA tasks also involve E-REF tasks, but\nencompasses a more wider range of task families. For example,\nLi et al. [35] used a Transformer with Self-Attention to extract\nsalient multimodal representations, which were trained using vi-\nsually grounded language objectives. ViLT [27] processed visual\ninputs holistically, learning vision‚Äìlanguage representations with-\nout relying on the regional supervision typically associated with",
            "content": "Figure 2: Sample canonical frames from Refer360 dataset in three different views: Exo-view (RGB), Ego-view (RGB), and Exo-View (Depth). The first, second, and third rows contain interaction samples from home, lab, and outdoor location. object detection. Li et al. [33] designed Querying Transformer to bootstrap visionlanguage representation from frozen image encoder. These models achieved performance improvements on VQA tasks by utilizing representation alignment-based training objectives. However, as these objectives primarily focus on alignment, they are less effective at fusing modality-specific representations. Selfand cross-attention mechanisms mainly align tokens across modalities but often fail to extract complementary signals that are crucial for embodied referring expression comprehension. In HRI, where subtle nonverbal cues such as gaze and gesture significantly alter instruction meaning, the ability to capture complementary features is critical. Recent HRI research has begun to adapt multimodal transformers to embodied settings, showing that fusion modules or adapters improve robots ability to follow human instructions and interpret multimodal input in collaborative tasks [8, 26, 39, 56]. These works highlight the gap between generic VQA-style representation learning and the requirements of embodied instruction comprehension in HRI, motivating the need for approaches that explicitly enhance modality-specific fusion."
        },
        {
            "title": "3 Refer360: E-RFE Dataset\nIn this section, we introduce Refer360: a multimodal embodied\nreferring expression dataset. Unlike prior datasets limited to con-\ntrolled settings or single viewpoints, Refer360 provides synchro-\nnized recordings across diverse real-world contexts, including both\nlaboratory and unconstrained outdoor environments. As illustrated\nin Fig. 2 and summarized in Table 2, the dataset integrates comple-\nmentary sensing modalities: egocentric video and gaze captured\nthrough the Pupil Invisible eye tracker, exocentric RGB, depth,",
            "content": "HRI 26, Edinburgh, Scotland, infrared, and audio streams from the Azure Kinect DK mounted on an Ohmni telepresence robot [48], as well as 3D skeletal joint data obtained through Kinects body tracking SDK. This configuration yields aligned first-person and third-person perspectives, enriched with gaze and gesture signals crucial for resolving referential ambiguity. By offering multimodal, multi-view record of natural human instructions, Refer360 can be leveraged for tasks such as perspective-invariant reference resolution, gesturegaze fusion modeling, and training anticipatory models for HRI."
        },
        {
            "title": "3.1 Data Collection System\nThe goal of the Refer360 dataset is to study real-world HRI in which\na human provides object-referencing instructions to robots across\ndiverse environments, spanning controlled laboratory setups to out-\ndoor locations. To achieve this, we have developed a data collection\nsystem that synchronously captures multimodal data of embod-\nied interactions in lab and outside-lab environments, utilizing an\nAzure Kinect DK [46] and a Pupil Glass eye tracker [50]. It is worth\nnoting that by ‚Äúoutside-lab environment,‚Äù we encompass settings,\nincluding home, outdoor locations, etc.",
            "content": "Figure 1 depicts sample data collection setup of Refer360. The Azure Kinect DK is mounted on an Ohmni telepresence robot [48] to incorporate camera motion and replicate real-world settings. The Kinect sensor offers multiple data streams that capture different interaction modalities. Its RGB camera continuously records visual data, providing an external or exocentric perspective of the participants actions. The Pupil eye tracker records an RGB data stream, capturing the participants first-person or egocentric perspective. Additionally, the Kinect sensor captures depth, infrared, and audio data streams, enabling analysis of the participants environment and audio cues. We utilize Kinects Body Tracking SDK to capture 3D skeletal data with 32 body joints, allowing us to track the participants movements and postures. By combining exocentric and egocentric viewpoints, along with multimodal data from the same interaction, our system offers comprehensive understanding of embodied human-robot interactions. In addition to enabling multimodal sensing, incorporating the Ohmni telepresence robot into our data collection system was critical design choice. During the data collection process, the robot was teleoperated by researcher. Mounting the Azure Kinect DK on mobile robotic platform provided two key advantages. First, it allowed us to capture data from the robots own perspective, offering viewpoint that an autonomous system would rely on during interaction. This perspective is particularly important for embodied referring expression (E-RFE) tasks, where the robot must interpret human verbal and nonverbal instructions in real time. Second, teleoperating the robot during experiments provided us with the flexibility to adjust its position and orientation, enabling naturalistic scenarios in which robots dynamically change their viewpoint to enhance understanding. By situating the human instructions in the visible presence of robot, our setup ensured that the collected data reflected authentic HRI scenarios rather than isolated human behavior, which ensures the ecological validity of the dataset. We have developed Python-based application to synchronize the data collection process. It utilizes the pyKinectAzure library for Islam et al. the Kinect sensors data streams and Pupil Labs Real-time Python API for the Eye Trackers data streams. We log the UNIX timestamps of data capture events for multiple sensor data streams from Kinect and Eye Tracker. We used these timestamps to synchronize the captured data during post-processing. This timestamp-based synchronization method can be extended to seamlessly integrate various additional sensors for enhanced functionality and versatility. We will opensource this data collection system for future research. Details of the data collection system can be found in Appendix A."
        },
        {
            "title": "4 Data Collection Procedure\nAll data collection tasks involved participants providing object-\nreferencing instructions across multiple sessions, where the en-\nvironment setup, objects, and viewpoints varied. Our goal was\nto capture embodied human instructions using both verbal and\nnonverbal modalities under diverse real-world conditions.\nPre-Task Setup: Before beginning the study, participants reviewed\nthe consent documents and task instructions, which the Univer-\nsity‚Äôs Institutional Review Board (IRB) had approved. They then\ncompleted a demographic survey, reporting background informa-\ntion such as age, gender, and prior experience with robots. Each\nparticipant then wore the Pupil Invisible eye tracker, which pro-\nvided egocentric video and gaze data, and was introduced to the\nAzure Kinect DK sensor mounted on an Ohmni telepresence robot,\nwhich captured exocentric RGB, depth, infrared, and skeletal joint\ndata. This combination of sensors ensured synchronized multimodal\nrecordings from both first-person and third-person perspectives.\nData Collection Sessions: Each session lasted approximately one\nhour and required participants to issue embodied instructions ref-\nerencing physical objects in their surroundings. The sessions were\nconducted under two conditions:",
            "content": "Constrained condition: Participants were encouraged to follow guidelines on instruction format and employ both verbal and nonverbal modalities (e.g., gaze, pointing) to reference objects. Unconstrained condition: No specific guidance on instruction format or modality was given, allowing participants to provide instructions naturally and flexibly. During each interaction, participants were instructed to describe target object so that robot could identify it. Instructions could be given from different perspectives (e.g., the box to my right vs. the box to your left), capturing the variability and ambiguity present in situated communication. To facilitate synchronization, canonical Embodied Referring Expression Comprehension in Human-Robot Interaction HRI 26, Edinburgh, Scotland, Table 2: Statistical breakdown of Refer360 dataset."
        },
        {
            "title": "Frames",
            "content": "Canonical Frames Avg. Interaction Duration Total Duration Lab Outside-lab"
        },
        {
            "title": "Total",
            "content": "198 194 392 10,814 3,176 13,990 2,472,939 759,018 3.2M 22,356 6,380 28,736 4.484 sec 4.691 sec 4.531 sec 13.48 hr 4.14 hr 17.62 hr eventssuch as when participant pointed at an object or fixated gazewere timestamped using keystroke-based system, allowing alignment across all data streams. Objects varied across sessions and included wide range of everyday household and office items (see Appendix). Post-Task Survey: Upon completing the sessions, participants filled out post-task survey indicating their preferred referencing strategy: verbal-only, gesture-only, or combination of modalities. They also provided feedback on the naturalness and effectiveness of their interaction style. Finally, participants signed release form permitting the collected data to be shared as part of the Refer360 dataset. Please refer to the Appendix for further details on the data collection protocol and procedure."
        },
        {
            "title": "4.1 Dataset Post-processing\nWe have recorded a single video file utilizing the Kinect sensor\nfor each session, which contains three data streams: RGB, Depth,\nand Infrared. Using the data collection application, we read the\nKinect sensor‚Äôs IMU and 3D skeleton joint data and stored them\nin separate JSON files. We utilize the FFmpeg library to split the\nKinect video stream into three separate streams for RGB, Depth,\nand Infrared. The IMU time series data is split into two files: ac-\ncelerometer readings and gyroscope readings. We extracted the\nrecorded audio from Kinect as an MP3 file. For each session, the\nPupil eye tracker generates a video file in MP4 format and saves it\nto the Pupil Cloud with event timestamps.",
            "content": "One of the major challenges in data post-processing was synchronizing the Azure Kinect and Pupil Eye Tracker data and segmenting each interaction. We used the start and end times of each interaction for segmentation from the Pupil Cloud event timestamps log. Additionally, we logged canonical frames (Figure 1 (right)), i.e., frames where participants precisely pointed to the object of interest during data collection. We leveraged the FFmpeg library to split the data into individual interactions and these specific canonical frames for Kinect and eye-tracking data. We used the Pupil Labs Real-time Python API for the eye tracker to access the corresponding recordings stored in the Pupil cloud, matching them to the Kinect data using timestamps. Finally, we employed the OpenAI Whisper library to transcribe the audio data captured by the Kinect. Under the approved IRB, five human experts validated all interaction segmentation, synchronization, and audio transcriptions to ensure high-quality data. This dataset was annotated by human annotators from an external company, which provides data annotation services. Figure 2 illustrates sample interactions from Refer360 dataset along with the audio transcription."
        },
        {
            "title": "5 Dataset Analysis\nTable 2 provides a comprehensive statistical overview of the Re-\nfer360 dataset. Data collection comprised 392 sessions conducted\nacross both laboratory and outside-lab environments, yielding a\ntotal of 13, 990 interactions over 17.62 hours of recorded activ-\nity. These sessions captured approximately 3.2 million multimodal\nframes, including 28, 736 canonical frames aligned to key referenc-\ning moments such as pointing or gaze fixation. On average, each\nsession lasted 2.69 minutes and contained 36.65 frames, while indi-\nvidual interactions spanned roughly 4.53 seconds. This balance of\nshort interaction cycles and sustained session diversity underscores\nthe dataset‚Äôs suitability for studying embodied communication at\nboth fine-grained and contextual levels.",
            "content": "To complement the recorded interactions, post-task survey responses were analyzed to uncover participants preferred strategies for object referencing. The results demonstrate striking consensus: 96.97% (ùëõ = 63) of participants favored multimodal approach, combining verbal instructions with non-verbal gestures such as pointing and gaze. Only 3.03% (ùëõ = 2) reported relying on verbal instructions alone, and notably, no participant selected non-verbal gestures as standalone strategy. Beyond this quantitative preference, participants also reflected on the naturalness and effectiveness of different modalities, emphasizing that gestures enriched verbal communication by reducing ambiguity and aiding perspective alignment. These findings highlight an important behavioral insighthumans overwhelmingly default to multimodal referencing when interacting in embodied settings, reinforcing the need for models that can integrate verbal and non-verbal cues seamlessly."
        },
        {
            "title": "6 MuRes: Multimodal Guided Residual Model\nThe task of grounding objects through E-REF requires a comprehen-\nsive understanding of verbal utterances and nonverbal gestures. Ex-\nisting visual-language (VL) models often utilize pre-trained frozen\nencoders to extract visual and language representations, fusing\nusing self-attention or cross-attention approaches for downstream\ntask learning. These fusion approaches can lose salient information\ndue to the modality gap between frozen language and visual rep-\nresentations, resulting in sub-optimal multimodal representations\nand decreased downstream task performance. To prevent this from\nhappening, one of the prevalent approaches is to utilize a resid-\nual connection, which can improve gradient flow [15, 17, 18] and\nreinforce a prior representation. However, residual connections\ncontain no information bottleneck, resulting in visual and language\nrepresentations that contain unrelated information for downstream\ntasks. From this motivation, we design a multimodal guided resid-\nual model, MuRes, to reinforce salient multimodal representations\nfor downstream tasks (Fig. 3).",
            "content": "HRI 26, Edinburgh, Scotland, Figure 3: Multimodal Model, MuRes, with the Guided Residual module. Visual and language representations are extracted and projected from pre-trained VL model. The projected representations are fed into the cross-attention module as the query. The key and value are the original extracted visual and language representations on the residual connection. The output from the cross-attention module and the projection are summed for downstream task learning. Visual-Language Representations: Similar to existing models [1, 27, 33, 34, 75], we first extract visual and language representations using frozen pre-trained encoder. We used state-of-the-art VL models to extract visual (ùëâ Rùê∑ùëâ ) and language ùêø Rùê∑ùêø representations, such as CLIP [52], DualEncoder [69], ViLT [27], and BLIP-2 [33]. Here, ùê∑ùëâ and ùê∑ùêø are the dimensions of visual and language representations from the pre-trained encoders. Multimodal Guided Residual Model: We introduce multimodal guided residual module to reinforce salient portions of modality-specific representations, serving as an information bottleneck over vanilla residual connection [15] reinforcing entire representations. This is done by focusing on the most relevant parts of the visual or language representations using cross-attention. Cross-attention is similar to self-attention but has crucial difference in its inputs. In cross-attention, the query is different from the keys and values, whereas in self-attention these are the same. This allows for the usage of projected visual (ùëâ ùëù ) and language (ùêøùëù ) representations as the query (ùëû), and usage of the originally extracted visual (ùëâ ) and language (ùêø) representations as the key (ùëò) and value (ùë£): {ùëâ ùëî, ùêøùëî} = Cross-Attention(ùëû = {ùëâ ùëù, ùêøùëù }, ùëò = {ùëâ , ùêø}, ùë£ = {ùëâ , ùêø}) This design allows for maintaining beneficial aspects of residual connections, such as improved gradient flow and reinforcement of prior representations, while establishing an information bottleneck on the residual connection. After extracting the guided residual representations, they are added to the projected representations as in vanilla residual connections: ùëâ ùëì , ùêøùëì = ùëâ ùëù +ùëâ ùëî, ùêøùëù +ùêøùëî. Finally, we fused these representations (ùëâ ùëì , ùêøùëì ) for downstream task learning. Training Model: To demonstrate the MuRes models effectiveness at improving representations, we train for two downstream Islam et al. tasks: comprehending embodied referring expressions designed as an object bounding box prediction and visual-question answering designed as multiple choice question-answering task. We used regression loss for the object bounding box prediction task and classification loss for the multiple-choice question-answering task. We developed all models using the PyTorch and PyTorch-Lightning deep learning frameworks. We also used the HuggingFace library for pre-trained models (ViLT, Dual Encoder, CLIP, and BLIP-2). We used an embedding size of 512 for the Dual-Encoder and CLIP models, 768 for the ViLT model, and 1408 for the BLIP-2 model. We trained models using the AdamW optimizer with weight decay regularization set to 0.01 [41] and cosine annealing warm restarts with cycle length (ùëá0): {2, 4, 6}, and cycle multiplier (ùëáùëöùë¢ùëôùë° ): 2. For the Dual Encoder, CLIP, ViLT, and BLIP-2 models doing detection we used learning rate of 3ùëí 5, 3ùëí 6, 3ùëí 5, and 3ùëí 6 respectively, and all models for VQA used learning rate of 1ùëí 5. We used batch size of 32 for all models except BLIP-2 where we used batch size of 2 due to the model being much larger. All models for detection were trained for 10 epochs on Refer360 and 25 epochs on CAESAR-PRO with random seed of 33; and all models for VQA were trained for 20 epochs with random seed of 42."
        },
        {
            "title": "7 Experimental Setup\nWe have incorporated our proposed guided residual module MuRes\ninto the existing state-of-the-art multimodal models, including CLIP\n[52], DualEncoder [69], ViLT [27], BLIP-2 [33], and VisualBERT\n[35]. We have evaluated these models and baselines multimodal\nmodels on Refer360 and CAESAR-PRO [19] datasets focusing on\nembodied referring expression comprehension (E-RFE) tasks. We\nhave also evaluated these models on two more widely used datasets,\nScienceQA [43], and A-OKVQA [53], to assess their performance\non Visual Question Answering (VQA) tasks. We trained multiple\nvariations of our proposed residual module MuRes, each differing in\nthe type of residual representation of visual and language modalities.\nWe examined four distinct variations:",
            "content": "Visual-Only Residual Representation MuRes(V): This variant leverages the projected visual representation as the query in the guided residual modules to extract the salient multimodal residual representations. Language-Only Residual Representation MuRes(L): This variant utilizes the projected language representation as the query in the guided residual modules to extract the salient multimodal residual representations. Visual and Language Residual Representation MuRes (V+L): This variant employs projected visual and language representations as the query to extract the salient multimodal residual representations. Vanilla Models: Following the original residual architecture [15], this baseline directly summed visual and language representations to the projected representations without using any attention approach. We also evaluated several multimodal models in the vanilla mode without any residual connections. Embodied Referring Expression Comprehension in Human-Robot Interaction HRI 26, Edinburgh, Scotland, Table 3: Comparison of VL models performance on the E-REF comprehension task, designed as bounding box detection. The results suggest that our multimodal guided residual module, MuRes, enhances the performance of most baseline multimodal models on the Refer360 and CAESAR-PRO datasets. Best performance numbers in bold face. (V: Visual, L: Language) Refer360 Dataset Models Without Residual IOU-50 IOU-25 Vanilla Residual IOU-50 IOU-25 MuRes(V) MuRes(L) MuRes(V+L) IOU-25 IOU-50 IOU-25 IOUIOU-25 IOU-50 CLIP ViLT BLIP-2 Dual-Encoder 25.80 36.53 29.42 31.08 7.67 14.03 7.54 9.83 27.22 35.34 27.66 30. 8.35 14.37 7.31 8.98 29.20 - 25.45 31.36 9.15 - 7.71 8.92 28.30 - 26.81 29.43 7.50 - 7.94 9.03 26.65 37.05 16.44 31. 7.27 14.66 3.80 10.68 CAESAR-PRO Dataset [19] Models Without Residual IOU-50 IOU-25 Vanilla Residual IOU-50 IOU-25 MuRes(V) MuRes(L) MuRes(V+L) IOU-25 IOU-50 IOU-25 IOUIOU-25 IOU-50 CLIP ViLT Dual-Encoder 37.92 27.96 42.52 9.82 8.73 12.14 39.43 25.67 42. 10.83 8.06 11.61 42.91 - 36.72 11.91 - 8.51 39.56 - 37.97 10.85 - 10.32 39.06 28.52 37. 10.46 8.04 11."
        },
        {
            "title": "8 Experimental Results\n8.1 Embodied Referring Expression\nWe evaluated models on the Refer360 and CAESAR-PRO datasets\nfor the embodied referring expression comprehension task. Fol-\nlowing prior work on the embodied referring expression task [5],\nwe designed this task as an object bounding box detection task.\nAll models were trained following a similar setup outlined in Sec-\ntion 6. We have reported Top-1 accuracy for the VQA tasks. The\nexperimental results are presented in Table 3.",
            "content": "Results and Discussion: The experimental results in Table 3 indicate that augmenting existing multimodal models with the proposed multimodal guided residual module MuRes enhances embodied referring expression comprehension task performance on both the Refer360 and CAESAR-PRO datasets. More specifically, the results indicate that including visual reinforced representations enhances task performance. For example, augmenting MuRes into the CLIP[52] model and reinforcing the visual representation improved the object bounding detection task performance on our Refer360 dataset from 25.80% to 29.20% for IOU-25. Similarly, MuRes helps the CLIP[52] model enhance object bounding detection task performance on the CAESAR-PRO [19] dataset from 37.92% to 42.91% for IOU-25. This performance improvement underscores the importance of visual cues in object grounding and suggests that reinforcing visual representation can lead to better performance. Although the vanilla residual connection offers some performance improvement over models without any residual connectionbased fusion, the gains are modest compared to those achieved with MuRes. The key distinction lies in MuRess selective reinforcement of the most salient aspects of the visual-language representation, acting as an information bottleneck to extract only the relevant information. This targeted approach contrasts with vanilla residual connections, which indiscriminately reinforce the entire representation. These insights align with the findings from prior works on the information bottleneck [2, 21, 25, 58, 60, 64, 67, 68]. In the literature, it has been shown that information bottleneck helps the model to extract the relevant information and thus improve downstream task performance. Thus, the design choice of residual representation incorporation is pivotal in refining multimodal representation and, consequently, downstream task performance. The experimental results further suggest that the specific modality being reinforced can influence performance improvements. For example, reinforcing the visual modality with MuRes boosts the CLIP models performance for the object bounding box detection task from 25.80% to 29.20% for IOU-25. Conversely, emphasizing the language modality results in slightly lower enhancement, with performance increasing to 28.30%. This variance suggests that the object grounding task is predominantly reliant on visual information. Thus, the choice of modality for reinforcement should be carefully considered based on the downstream task."
        },
        {
            "title": "8.2 Visual Question-Answering Tasks\nWe evaluate the models on the ScienceQA [43] and A-OKVQA [53]\ndatasets for the VQA task. Visual question answering is central to\nHRI, as robots must interpret multimodal queries, ground them in\nvisual scenes, and provide accurate responses. Using ScienceQA\nand A-OKVQA enables us to assess whether our model facilitates\nsuch interactive understanding, which is crucial for natural hu-\nman‚Äìrobot collaboration. Following the evaluation protocols of\nthese benchmarks, we conduct multiple-choice QA evaluations.\nSimilar to the previous tasks, we incorporate different variations\nof our multimodal guided residual module MuRes into CLIP, ViLT,\nand VisualBERT models: MuRes(V), MuRes(L), MuRes(V+L), and\nVanilla multimodal models without residual connections for multi-\nmodal fusion. Since ViLT is a monolithic model that provides joint\nvision‚Äìlanguage representations, we split its output into separate\ntext and image representations based on the text length derived\nfrom the attention mask. All models were trained following the\nsetup outlined in Section 6 (Training Model). We report Accuracy\nfor ScienceQA and the Multiple Choice (MC) evaluation metric [53]\nfor A-OKVQA. The experimental results are presented in Table 4.\nResults and Discussion: The experimental results in Table 4\nsuggest that incorporating our multimodal guided residual module,\nMuRes, into multimodal models demonstrates consistent perfor-\nmance improvement across all variations evaluated compared to\nthose without residual connections. Specifically, the inclusion of",
            "content": "HRI 26, Edinburgh, Scotland, both visual and linguistic modalities (MuRes(V+L)) consistently yields the highest improvements. For example, in the ScienceQA dataset, CLIP model with MuRes VQA task accuracy increases from 21.31% to 51.85%. This performance improvement attributed to the information bottleneck in MuRes effectively extracts the salient representation from visual and language modalities, leading to more accurate answers. The gains from visual-only (MuRes (V)) and language-only (MuRes (L)) reinforcements underscore the importance of modality-specific enhancements, with visual reinforcements being particularly impactful in the VisualBERT model on the ScienceQA dataset, improved its performance from 34.95% to 37.13% using visual reinforcement and 37.63% using language reinforcement. These insights suggest that strategically leveraging multimodal guided residuals can significantly refine model performance in VQA tasks."
        },
        {
            "title": "9 Overal Discussion\nThe experiments provide several key insights into the comprehen-\nsion of E-RFEs. The proposed Refer360 dataset addresses critical\ngaps in prior work by capturing a broad spectrum of real-world ob-\njects and settings for embodied instructions. Unlike earlier datasets\nconstrained to a single view or limited modalities, Refer360 pro-\nvides synchronized multi-view recordings (egocentric, exocentric,\nand top-down) of referential behaviors, paired with both verbal ut-\nterances and rich nonverbal gestures. Spanning indoor and outdoor\nenvironments with varied lighting and context introduces essential\nenvironmental diversity. Multi-perspective data enable models to\ndevelop perspective-invariant understanding, such as distinguish-\ning ‚Äúthe object on my left‚Äù versus ‚Äúyour left‚Äù‚Äîa nuance often missed\nin single-view settings. Similarly, the inclusion of pointing and gaze\ncues enables models to learn how these signals jointly disambiguate\nreferences, which is crucial for robust comprehension of situated\ninstruction. Beyond serving as a benchmark, Refer360 provides a\nfoundation for fine-tuning large vision‚Äìlanguage models (VLMs),\ngrounding abstract multimodal knowledge in embodied contexts.\nAlongside the dataset, the MuRes offers a lightweight yet effec-\ntive fusion mechanism for E-RFEs. Across all evaluation setups, in-\ntegrating MuRes consistently improved performance over baseline\nmultimodal models, underscoring the importance of reinforcing\nmodality-specific cues rather than relying solely on alignment-\nbased fusions. By applying an information bottleneck to residual\nconnections, MuRes extracts complementary representations that\nbridge the gap between generic pre-trained encoders and the fine-\ngrained signals required for E-RFEs. Functioning as an adapter layer,\nMuRes selectively amplifies salient information before fusion, align-\ning with emerging strategies that augment frozen vision‚Äìlanguage\nbackbones with small, trainable modules. In our experiments (Ta-\nbles 3, 4), this design translated into notable improvements in\nunderstanding referring expressions and answering questions in\nhuman-centric scenes. These findings resonate with other recent\nefforts to extend general-purpose VLMs with targeted HRI modules,\nhighlighting MuRes as a compact and general-purpose adapter for\nembodied interaction.",
            "content": "Looking ahead, the integration of structured HRI datasets such as Refer360 with modular approaches (e.g., MuRes) paves the way for more embodied and intelligent robots. For example, Refer360 Islam et al. can be extended beyond embodied referring expressions to broader visual question-answering (VQA) tasks, opening the possibility of evolving into an embodied question-answering (E-QA) benchmark that integrates situated reasoning with multimodal embodied cues to enable more intuitive HRI. We envision systems that leverage the broad knowledge of foundation models while being grounded through fine-tuning or adapter-based learning on real interaction data. Evidence from hybrid approachessuch as PaLM-E, which combines internet-scale text with robot sensor inputs to achieve state-of-the-art embodied performancedemonstrates the promise of this direction. lightweight alternative is to begin with capable VLMs and inject HRI expertise through adapter modules, targeted training, or prompt-based alignment. By doing so, future robots could interpret human instructions with the flexibility of large language models while retaining the situational grounding required for natural collaboration. The combined contributions of Refer360 and MuRes provide concrete tools toward this vision, advancing progress toward robots that can naturally comprehend and engage with humans in real-world environments."
        },
        {
            "title": "10 Conclusion\nIn this paper, we have introduced a comprehensive and diverse\nbenchmark dataset of multimodal interactions, Refer360, and ex-\nplored a baseline adapter model, MuRes, to extract modality-specific\nsalient representations. To comprehensively study embodied re-\nferring expressions in real-world settings, we have curated Re-\nfer360 from various environments, collecting multimodal sensor\ndata including exo visual view, ego visual view, depth, infrared,\n3D skeletal data, audio, and robot camera motion to capture un-\nconstrained human interactions from multiple verbal and visual\nviewpoints. Consequently, Refer360 is the first embodied referring\nexpression comprehension dataset curated with such diverse sensor\ndata. Our extensive experimental analyses demonstrate that exist-\ning multimodal models cannot effectively understand embodied\nreferring expressions in real-world settings, primarily due to their\nfailure to bridge the gap between general pre-trained frozen visual-\nlanguage representations and salient modality-specific cues. To\naddress this issue, we explored a baseline multimodal guided resid-\nual module, MuRes, which acts as a bottleneck to extract salient\nmodality-specific representations. Our experiments suggest that\nincorporating MuRes into existing multimodal models improves\ndownstream task performance on four datasets comprising embod-\nied referring expression understanding and visual question answer-\ning. Our comprehensive multimodal benchmark dataset (Refer360),\nalong with our exploration of MuRes, show promising directions\nfor research into embodied referring expression comprehension.",
            "content": "References [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems 35 (2022), 2371623736. [2] Alexander Alemi, Ian Fischer, Joshua Dillon, and Kevin Murphy. 2016. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410 (2016). [3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering. In International Conference on Computer Vision (ICCV). [4] Michael Arbib, Katja Liebal, and Simone Pika. 2008. Primate vocalization, gesture, and the evolution of human language. Current anthropology 49, 6 (2008), 10531076. Embodied Referring Expression Comprehension in Human-Robot Interaction HRI 26, Edinburgh, Scotland, Table 4: Comparison of VL models performance on the visual question-answering task. The results suggest that our multimodal guided residual module, MuRes, enhances the performance of the multimodal models on the ScienceQA and A-OKVQA datasets. Best performance numbers in bold face. (V: Visual, L: Language) ScienceQA Dataset [43] A-OKVQA Dataset [53] Models Without Residual With Residual MuRes(V) MuRes(L) MuRes(V+L) Without Residual With Residual MuRes(V) MuRes(L) MuRes(V+L) CLIP ViLT VisualBERT Dual-Encoder 21.31 44.52 34.95 24. 33.36 47.05 36.63 35.55 40.75 42.78 37.13 37.13 31.33 42.58 37.63 31.93 51.85 49.33 39.03 43.57 29.41 31.61 29.88 32.64 32.78 31.21 32.47 33. 32.78 32.19 30.72 32.89 30.42 31.48 31.15 31.72 32.47 32.53 32.62 35.02 [5] Yixin Chen, Qing Li, Deqian Kong, Yik Lun Kei, Song-Chun Zhu, Tao Gao, Yixin Zhu, and Siyuan Huang. 2021. Yourefit: Embodied reference understanding with language and gesture. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 13851395. [6] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. 2018. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 110. [7] Anna Deichler, Siyang Wang, Simon Alexanderson, and Jonas Beskow. 2023. Learning to generate pointing gestures in situated embodied conversational agents. Frontiers in Robotics and AI 10 (2023), 1110534. [8] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. 2023. Palm-e: An embodied multimodal language model. (2023). [9] Fevziye Irem Eyiokur, Dogucan Yaman, Hazƒ±m Kemal Ekenel, and Alexander Waibel. 2025. CAPE: CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for Embodied Reference Understanding. arXiv:2507.21888 [cs.CV] https://arxiv.org/abs/2507.21888 [10] Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015. Are you talking to machine? dataset and methods for multilingual image question. Advances in neural information processing systems 28 (2015). [11] Ibai Gorordo. 2025. pyKinectAzure: Python wrapper for Azure Kinect SDK. https://github.com/ibaiGorordo/pyKinectAzure. Accessed: July 28, 2025. [12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition. 69046913. [13] Yuncheng Guo and Xiaodong Gu. 2025. MMRL: Multi-Modal Representation Learning for Vision-Language Models. arXiv:2503.08497 [cs.LG] https://arxiv. org/abs/2503.08497 [14] Zhiwei Hao, Jianyuan Guo, Li Shen, Yong Luo, Han Hu, and Yonggang Wen. 2024. ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning. arXiv:2410. [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770778. [16] Mark Higger, Polina Rygina, Logan Daigler, Lara Ferreira Bezerra, Zhao Han, and Tom Williams. 2023. Toward Open-World Human-Robot Interaction: What Types of Gestures Are Used in Task-Based Open-World Referential Communication?. In The 27th Workshop on the Semantics and Pragmatics of Dialogue (SemDial). [17] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Weinberger. 2017. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 47004708. [18] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. 2016. Deep networks with stochastic depth. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14. Springer, 646661. [19] Md Mofijul Islam, Alexi Gladstone, and Tariq Iqbal. 2022. PATRON: Perspectiveaware Multitask Model for Referring Expression Grounding using Embodied Multimodal Cues. [20] Md Mofijul Islam, Alexi Gladstone, Riashat Islam, and Tariq Iqbal. 2024. EQA-MX: Embodied Question Answering using Multimodal Expression. [21] Md Mofijul Islam, Alexi Gladstone, Riashat Islam, and Tariq Iqbal. 2024. EQA-MX: Embodied Question Answering using Multimodal Expression. In The Twelfth International Conference on Learning Representations. https://openreview.net/ forum?id=7gUrYE50Rb [22] Md Mofijul Islam and Tariq Iqbal. 2020. HAMLET: Hierarchical Multimodal Attention-based Human Activity Recognition Algorithm. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 1028510292. doi:10.1109/IROS45743.2020.9340987 [23] Md Mofijul Islam and Tariq Iqbal. 2021. Multi-GAT: Graphical Attention-based Hierarchical Multimodal Representation Learning Approach for Human Activity Recognition. In IEEE Robotics and Automation Letters (RA-L). [24] Md Mofijul Islam, Reza Manuel Mirzaiee, Alexi Gladstone, Haley Green, and Tariq Iqbal. 2022. CAESAR: Multimodal Simulator for Generating Embodied Relationship Grounding Dataset. In NeurIPS. [25] Riashat Islam, Hongyu Zang, Manan Tomar, Aniket Didolkar, Md Mofijul Islam, Samin Yeasar Arnob, Tariq Iqbal, Xin Li, Anirudh Goyal, Nicolas Heess, et al. 2023. Representation Learning in Deep RL via Discrete Information Bottleneck. In International Conference on Artificial Intelligence and Statistics. PMLR, 86998722. [26] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. 2022. Vima: General robot manipulation with multimodal prompts. arXiv (2022). [27] Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning. PMLR, 55835594. [28] Avgi Kollakidou, Frederik Haarslev, Cagatay Odabasi, Leon Bodenhagen, and Norbert Kr√ºger. 2022. Hri-gestures: Gesture recognition for human-robot interaction. In Proceedings of the 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. SCITEPRESS Digital Library, 559566. [29] Philipp Kratzer, Simon Bihlmaier, Niteesh Balachandra Midlagajni, Rohit Prakash, Marc Toussaint, and Jim Mainprice. 2020. Mogaze: dataset of full-body motions that includes workspace geometry and eye-gaze. IEEE Robotics and Automation Letters 6, 2 (2020), 367373. [30] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision 123, 1 (2017), 3273. RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D. arXiv:2308.12035 [cs.CV] https://arxiv.org/abs/2308.12035 [31] Shuhei Kurita, Naoki Katsura, and Eri Onami. 2023. [32] Jae Hee Lee, Matthias Kerzel, Kyra Ahrens, Cornelius Weber, and Stefan Wermter. 2022. What is Right for Me is Not Yet Right for You: Dataset for Grounding Relative Directions via Multi-Task Learning. arXiv preprint arXiv:2205.02671 (2022). [33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023). [34] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning. PMLR, 1288812900. [35] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. VisualBERT: Simple and Performant Baseline for Vision and Language. In Advances in Neural Information Processing Systems. [36] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. 2024. VILA: On Pre-training for Visual Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2668926699. [37] Ulf Liszkowski, Malinda Carpenter, Anne Henning, Tricia Striano, and Michael Tomasello. 2004. Twelve-month-olds point to share attention and interest. Developmental science 7, 3 (2004), 297307. [38] Ulf Liszkowski, Malinda Carpenter, Tricia Striano, and Michael Tomasello. 2006. 12-and 18-month-olds point to provide information for others. Journal of cognition and development 7, 2 (2006), 173187. [39] Hao Liu, Lisa Lee, Kimin Lee, and Pieter Abbeel. 2022. Instruction-following agents with multimodal transformer. arXiv preprint arXiv:2210.13431 (2022). [40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruction Tuning. arXiv:2304.08485 [cs.CV] [41] Ilya Loshchilov and Frank Hutter. 2017. Decoupled Weight Decay Regularization. In ICLR. [42] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. In Advances in Neural Information Processing Systems. Islam et al. Automation Letters (RA-L). [72] Mohammad Samin Yasar*, Md Mofijul Islam*, and Tariq Iqbal. 2022. IMPRINT: Interactional Dynamics-aware Motion Prediction in Teams using Multimodal Context. In ACM Transactions on Human-Robot Interaction (under-review). [73] Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal, Tamara Berg, and Dhruv Batra. 2019. Multi-target embodied question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 63096318. [74] Licheng Yu, Eunbyung Park, Alexander Berg, and Tamara Berg. 2015. Visual madlibs: Fill in the blank image generation and question answering. arXiv preprint arXiv:1506.00278 (2015). [75] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. 2022. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1812318133. [76] Zhuoyang Zhang, Kun Qian, Bo Zhou, Fang Fang, and Xudong Ma. 2024. Gazeassisted visual grounding via knowledge distillation for referred object grasping with under-specified object referring. Engineering Applications of Artificial Intelligence 133 (2024), 108493. [77] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition. 49955004. HRI 26, Edinburgh, Scotland, [43] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems 35 (2022), 25072521. [44] David McNeill. 1992. Hand and mind: What gestures reveal about thought. University of Chicago press. [45] David McNeill. 2012. How language began: Gesture and speech in human evolution. Cambridge University Press. [46] Microsoft. 2025. Azure Kinect. https://azure.microsoft.com/en-us/products/ kinect-dk. Accessed: July 28, 2025. [47] Microsoft. 2025. Azure Kinect Body Tracking Documentation. https://microsoft. github.io/Azure-Kinect-Body-Tracking/release/1.1.x/index.html. Accessed: July 28, 2025. [48] OhmniLabs. 2025. Ohmni Telepresence Robot. https://ohmnilabs.com/products/ ohmni-telepresence-robot/. Accessed: July 28, 2025. [49] OpenAI. 2025. Whisper: library for scalable reinforcement learning. https: //github.com/openai/whisper. Accessed: July 28, 2025. [50] Pupil Labs. 2025. Pupil Labs. https://pupil-labs.com/. Accessed: July 28, 2025. [51] Pupil Labs. 2025. Pupil Labs Real-Time API Documentation. https://pupil-labsrealtime-api.readthedocs.io/en/stable/. Accessed: July, 2025. [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning. PMLR, 87488763. [53] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: benchmark for visual question answering using world knowledge. In European Conference on Computer Vision. Springer, 146162. [54] Sarah Sebo, Brett Stoll, Brian Scassellati, and Malte Jung. 2020. Robots in groups and teams: literature review. Proceedings of the ACM on Human-Computer Interaction 4, CSCW2 (2020), 136. [55] Snehesh Shrestha, Yantian Zha, Saketh Banagiri, Ge Gao, Yiannis Aloimonos, and Cornelia Fermuller. 2024. Natsgd: dataset with speech, gestures, and demonstrations for robot learning in natural human-robot interaction. arXiv preprint arXiv:2403.02274 (2024). [56] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1074010749. [57] Anas Shrinah, Masoud Bahraini, Fahad Khan, Seemal Asif, Niels Lohse, and Kerstin Eder. 2024. On the design of human-robot collaboration gestures. arXiv preprint arXiv:2402.19058 (2024). [58] Ravid Shwartz-Ziv and Naftali Tishby. 2017. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810 (2017). [59] Stephanie Stacy, Qingyi Zhao, Minglu Zhao, Max Kleiman-Weiner, and Tao Gao. 2020. Intuitive Signaling Through an\" Imagined We\".. In CogSci. [60] Qingyun Sun, Jianxin Li, Hao Peng, Jia Wu, Xingcheng Fu, Cheng Ji, and Yu Philip. 2022. Graph structure learning with variational information bottleneck. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 41654174. [61] Ning Tang, Stephanie Stacy, Minglu Zhao, Gabriel Marquez, and Tao Gao. 2020. Bootstrapping an Imagined We for Cooperation.. In CogSci. [62] Zineng Tang, Lingjun Mao, and Alane Suhr. 2024. Grounding Language in Multi-Perspective Referential Communication. arXiv:2410.03959 [cs.CL] https: //arxiv.org/abs/2410.03959 [63] Jesse Thomason, Aishwarya Padmakumar, Jivko Sinapov, Nick Walker, Yuqian Jiang, Harel Yedidsion, Justin Hart, Peter Stone, and Raymond Mooney. 2019. Improving grounded natural language understanding through human-robot dialog. In 2019 International Conference on Robotics and Automation (ICRA). IEEE, 69346941. [64] Naftali Tishby and Noga Zaslavsky. 2015. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw). IEEE, 15. [65] Suramya Tomar. 2006. Converting video formats with FFmpeg. Linux J. 2006, 146 (June 2006), 10. [66] Michael Tomasello. 2010. Origins of human communication. MIT press. [67] Frederik Tr√§uble, Anirudh Goyal, Nasim Rahaman, Michael Mozer, Kenji Kawaguchi, Yoshua Bengio, and Bernhard Sch√∂lkopf. 2022. Discrete Key-Value Bottleneck. [68] Haoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. 2022. Rethinking minimal sufficient representation in contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1604116050. [69] Yu Wu, Linchao Zhu, Yan Yan, and Yi Yang. 2019. Dual attention matching for audio-visual event localization. In Proceedings of the IEEE/CVF international conference on computer vision. 62926300. [70] Hao Yang, Can Gao, Hao L√≠u, Xinyan Xiao, Yanyan Zhao, and Bing Qin. 2023. UNIMO-3: Multi-granularity Interaction for Vision-Language Representation Learning. arXiv:2305.13697 [71] Mohammad Samin Yasar and Tariq Iqbal. 2021. Scalable Approach to Predict Multi-Agent Motion for Human-Robot Collaboration. In IEEE Robotics and Embodied Referring Expression Comprehension in Human-Robot Interaction Resources Processed Dataset (49.71 GB): https://bit.ly/refer360-dataset-processed Raw Dataset (2572.36 GB): The link will be provided upon acceptance. Source Code (Data Collection): The link will be provided upon acceptance. Source Code (MuRes + baselines): The link will be provided upon acceptance. Model Checkpoints (5.4 GB): The link will be provided upon acceptance. Docker for training models (8.59 GB): We built Docker to facilitate easy reproduction of our experimental settings and training environment. We cannot currently share the Docker Hub link to maintain anonymity. We plan to share that link upon publication of the paper. For now, we share the Singularity container built from the same Docker used in our experiments: https://bit.ly/multimodal-docker Additional Experimental Results:"
        },
        {
            "title": "Quantitative Analysis",
            "content": "We have performed quantitative evaluation of the models by applying the ScienceQA [43] and A-OKVQA [53] datasets for the visual-question answering tasks. We have analyzed the response of VisualBERT with different variations of our proposed model, (MuRes), on multiple-choice question-answering tasks. The responses from VisualBERT model variations are similar to the variation presented in Table 4 from the manuscript (i.e., without residual, MuRes (V), MuRes (L), and MuRes (V+L)). Discussion: The model responses are presented in Fig. 4. These results suggest that augmenting the VisualBERT model with MuRes improves responses for the visual question-answering task. For instance, in Fig. 4 (a) [Q-A1], the VisualBERT models response to the question Which continent is highlighted? alongside an image of map shows that enhancing visual representations through MuRes yields the correct answer (Europe). However, enhancing only the language representations through MuRes leads to an incorrect answer (Asia). This question necessitates thorough understanding of the spatial location of the highlighted region (Europe) on the map, explaining why reinforcing the visual representations aids in improving the response. Conversely, in Fig.4 (a) [Q-A3], enhancing either visual or language representations does not yield the most accurate answer (Transparent) for the question: Which property do these three objects have in common?. Although the responses with either Vision or Language in Fig.4 (a) [Q-A3] are not entirely inaccurate, as the objects are somewhat shiny, only yhe model with both visual and language representations reinforced correctly answers Transparent. Therefore, identifying which modalities to reinforce thorough MuRes is critical aspect of enhancing the models responses. HRI 26, Edinburgh, Scotland, (a) ScienceQA [43] (b) A-OKVQA [53] Figure 4: Qualitative analysis of VisualBERT with and without the proposed guided residual module (MuRes) on two datasets. Incorporating MuRes improves visual question answering by aligning multimodal context in both ScienceQA and A-OKVQA tasks. Subfigure (a) shows qualitative results on questions involving diagrams from the ScienceQA dataset [43]. Subfigure (b) shows model reasoning on ambiguous visual questions from the A-OKVQA dataset [53]. Data Collection C.1 Data Collection System Our data collection system integrates an Azure Kinect DK [46] and Pupil Smart Glass, also known as the Pupil Invisible Eye Tracker [50]. The Azure Kinect DK was mounted on an Ohmni Telepresence robot [48], and the participants wore the Pupil Smart Glass to facilitate data collection in real-world scenarios. An Alienware m15 R4 laptop powered by an i7-10870H RTX processor served as the high-performance computing backbone. Python-based application was developed to facilitate coordination and synchronization among all system components. This application ensured seamless operation and synchronized data collection from multiple sensors. Sensor Specifications. Azure Kinect provides multitude of C.1.1 sensory data, including visual, depth, infrared (IR), skeletal tracking, and inertial measurement unit (IMU) data. In addition, pupil glass offers visual (RGB), IR, gaze tracking, and gesture recognition capabilities. The Pupil Invisible Eye Tracker is state-of-the-art device with range of features designed to capture precise and accurate eye-tracking data. The participants in our study were equipped with the Pupil Smart Glass and an Android smartphone, which recorded their eye-tracking data. The data is subsequently transmitted to the Pupil Cloud via the Pupil Invisible Android application. This seamless hardware and software integration ensures efficient and reliable data collection and HRI 26, Edinburgh, Scotland, transmission. The specifications of the Azure Kinect DK and Pupil Eye Tracker sensors are listed in Table 5 and 6. C.1.2 Data Collection Application. We developed Python application to coordinate and synchronize the various components of our data collection system. This application played central role, ensuring seamless integration and synchronized data capture from multiple sensors. We collected camera video feeds, time-series data from the inertial measurement unit (IMU) and skeleton joint positions, and session metadata using this system. We utilized the pyKinectAzure [11] python library to interface with the Azure Kinect SDK sensor, while the Pupil Labs Real-time Python API [51] facilitated communication with the Pupil Eye camera. Participants stood before the Ohmin robot, issuing verbal commands and non-verbal gestures to reference physical objects. An RGB camera on the Azure Kinect device continuously captured visual data, providing third-person view of the participants referencing gestures. Additionally, the Kinects depth and infrared sensors recorded supplementary data streams, enriching the external perspective of the interactions. The system also leveraged the Kinects infrared sensor to collect infrared data and the Azure Kinect Body Tracking SDK [47] to capture the 3D coordinates and orientations of 32 skeletal joints. Simultaneously, the Kinects microphone recorded the participants verbal instructions. Complementing this external viewpoint, the Pupil Invisible Eye Tracker provided an egocentric visual stream from the participants perspectives. Combining these exocentric and egocentric data sources gave the system comprehensive understanding of human-robot interactions. We stored the Azure Kinect recordings and the corresponding keystroke event times locally as MP4 and JSON files, respectively. For the Pupil eye tracker, the recordings of the participants ego view and keystroke events were saved in the Pupil Cloud using the Pupil Lab Android app and Pupil API, respectively. C.1.3 Time-based Synchronization. One of the significant challenges we faced was synchronizing the various data streams captured by different devices. To address this, we implemented time-based synchronization method that recorded the UNIX timestamps of different data capture events and data streams, enabling synchronization during post-processing. This synchronization is crucial for aligning the data streams captured from different devices. Our approach involved recording the timestamp at the start and end of each interaction and the timestamp of the event when the participant pointed to an object (i.e., canonical events). This was achieved using our Python-based system, which is operated by individuals recording the data collection sessions. We utilized different keystrokes on standard keyboard to denote different events. The Space\" key was pressed at the start and end of an interaction, while the G\" key was pressed to identify the canonical event of an interaction. The canonical event indicates when the participant points to an object using gaze or pointing gestures. Specifically, the G\" keystroke event time was used to identify the canonical frame, i.e., the frame where the participant actually pointed to an object. When the participant used cues other than pointing, such as gaze, the G\" key was pressed when the gaze event occurred. The Space\" Islam et al. grey ceramic bowl, foam miniature football, wireless computer mouse, wooden box, blue cupholders, plastic water bottle, keyboard, green plastic cup, white plastic basket, basketball, white plastic cup, flower vase, clorox wipe container, paper towel roll, mountain dew bottle, picture frame, TV remote, grey plastic basket, black metal water bottle, coffee cup with lid, transformers robot, pepsi bottle, egg carton, TV screen, blue plastic box, pringles box, grey dustbin, light green open plastic box with handle, tripod, white three-level plastic box, cardboard box, sunglasses, yellow lego box, mouthwash, pink plastic cup, white tumbler, white desk fan, blue plastic container with lid and handle, salsa jar, nutella jar, pink dustbin, black kickball, table tennis ball container, blue plastic water bottle, black desk clock, screwdriver, blue magazine, shoe rack, bicycle, pupil labs glasses box, microwave, frying pan, blue couch, wooden chair, white rope, kitchen sink, white fridge, iron stand, allen wrench set, white trash can, black dresser, light stand, desk lamp, black office chair, silver rice cooker, black standing fan, wooden table, white pillow, white air conditioning unit, grey sweatshirt, banana, grey laundry drying rack, grey apartment mailboxes, white fence, surge protector. Figure 5: Objects in Refer360 Dataset Table 5: Azure Kinect DK Sensor Specifications Sensor RGB Camera Depth Camera Motion Sensor Microphone Array Specification Resolution: 3840 2160 @ 30 fps Time-of-Flight; Resolution: 640 576 @ 30 fps LSM6DSMUS IMU (accel. & gyro), Sampling Rate: 1.6 Hz USB Audio 2.0; 7 channels; Sensitivity: 22 dBFS (94 dB SPL, 1 kHz); SNR: > 65 dB; Overload Point: 116 dB Table 6: Pupil Invisible Eye Tracker Specifications Sensor Eye Cameras Scene Camera Specification 200 Hz @ 192 192 px, infrared (IR) illumination 30 Hz @ 1088 1080 px, 82 82 field of view (FOV) keystroke event time was used to identify the start and end of an interaction, thereby facilitating the segmentation of interactions. The Q\" key was used to terminate session. The corresponding UNIX timestamp for these keystroke events was recorded for both the Azure Kinect and Pupil Lab Eye Tracker. This enabled us to synchronize the data streams from these two devices during post-processing. Though the time-based synchronization method is utilized to synchronize between the Azure Kinect Sensor and Pupil Eye Tracker, it is designed to be extensible. For example, our system can be expanded to incorporate multiple Azure Kinect devices to capture multiple views of the participant during interaction rather than just the ego and exo views. C.1.4 Data Collection Environment. The Refer360 dataset aims to study real-world human-robot interactions in which human provides object-referencing instructions to robots across diverse environments, ranging from controlled laboratory setups to outdoor locations. Refer360 contains embodied interaction data from lab and outside-lab environments. The outside lab refers to settings outside controlled lab settings, such as homes, outdoor locations, etc. While choosing objects, we prioritize those usually available in these environments. Our dataset contains 75 objects Embodied Referring Expression Comprehension in Human-Robot Interaction HRI 26, Edinburgh, Scotland, object referencing instructions accurately. This involves uniquely identifying the object, which requires extracting the objects location and other attributes from the instruction. This task is challenging as humans often use diverse formats when providing verbal instructions, and these instructions may sometimes lack the necessary features for object identification. Incorporating nonverbal cues, such as pointing or referencing the object in relation to another object, can significantly improve the efficiency of interpreting object referencing instructions. Furthermore, object referencing instructions can be given from multiple perspectives, such as the subjects or the robots perspective, which must be resolved for accurate object comprehension. The participants were given the flexibility to choose any perspective (subject, robot, or neutral) when providing instructions. This approach allowed us to diversify our dataset by including object-referencing instructions with varied spatial referencing and perspectives. For instance, an object could be referenced in relation to another object, such as The black box on top of the brown table.\" The object reference in the verbal instruction could be from the subjects perspective, e.g., The couch to my right,\" or it could be from the robots perspective, e.g., The lamp to your left.\" We had two distinct data collection conditions: constrained and unconstrained. In the constrained condition, subjects were briefed on the format of instructions and how they could employ various modalities (verbal and nonverbal) to make the interaction as natural as possible. We also suggested that participants use both verbal and nonverbal gestures to describe an object. In the unconstrained condition, we did not suggest whether to use verbal or nonverbal gestures to describe an object. We instructed the participant to describe an object to the robot. This allowed us to capture natural human instincts when providing instructions. This approach also helped eliminate biases that might be introduced by pre-guidance on the format of the instructions, allowing subjects to be flexible in their instruction delivery. Each subject participated in multiple sessions, each lasting approximately one hour. During each session, the subject performed several interactions. Using our data collection system, we recorded the subjects ego view, exo view, IMU, skeleton, and audio data stream for each session. Upon completion of the sessions, subjects were asked to complete post-task survey and sign consent form to permit the release of the dataset. The Universitys IRB approved the study. The demographic and post-task surveys are presented in Figure 6. C.2 Dataset Processing The developed Python-based application generated an Azure Kinect video file in MP4 format for each session. The MP4 file contains three data streams from Azure Kinects camera sensor: RGB, Depth, and Infrared. Separate JSON files contain the IMU and skeleton joints time series data and relevant session metadata. We utilized the FFmpeg [65] library to extract the Kinect video streams into separate MP4 files and the recording audio as an MP3 file. The IMU time series was split into two different files for the accelerometer and gyroscope readings. For each session, the Pupil eye tracker also generated one video file in MP4 format and saved it to the pupil cloud. (a) Demographic Survey (b) Post-Task Survey Figure 6: User interfaces shown during data collection. (a) Participants first completed demographic survey. (b) After viewing the stories, participants completed post-task evaluation survey. from the aforementioned environments, and complete list of objects is given in Fig. 5. The data collection process began with comprehensive introduction to the system, the purpose of the dataset, and the protocol to be followed during collection. Before participating in the data collection sessions, subjects completed demographic survey. Each session involved subjects providing embodied instructions that referenced objects in their surroundings, using both language and nonverbal gestures (gaze and pointing gestures). The ultimate goal of this dataset is to enhance social robots ability to interpret HRI 26, Edinburgh, Scotland, Islam et al. an external company specializing in data annotation services, ensuring accuracy and reliability. Our dataset contains several data collection sessions and after data post-processing results in each sessions folder structure shown in Figure 7. Here, transcription.txt is the text transcription of audio.mp3. In the subfolders in Videos and Framess, exo.mp4 and ego.mp4 refer to the videos from the Azure Kinect SDK camera and Pupil Eye Camera, respectively. Figure 7: Folder structure of the Refer360 dataset. Each interaction contains audio, video, skeletal, and sensor data across RGB, depth, and infrared modalities. The major challenge of data post-processing was segmenting the interactions and synchronizing the Azure Kinect and Pupil lab data. For the segmentation of each interaction from Azure Kinect data streams, we look into that interactions start and end time. We also identify the canonical frames, i.e., frames where the subject points precisely to the object. We split each interaction and canonical frame using the FFmpeg library. Next, we searched the corresponding Pupil recording for the Azure Kinect recording from the pupil cloud using Python Pupil Cloud API. For this purpose, we used the recording-start timestamp saved in the metadata file to find the matching Pupil recording in the pupil cloud. After downloading the Pupil video, we employed the same procedure as Azure Kinect recording to split the interactions and canonical frames at the timestamps recorded during data collection. Finally, we utilized the OpenAI whisper [49] library to transcribe Kinect audio data to the corresponding text. Note that we manually verified the synchronization and segmentation with five human experts whom the IRB approved. Subsequently, the dataset underwent annotation by human annotators sourced from"
        }
    ],
    "affiliations": [
        "Amazon GenAI",
        "Stanford University",
        "University of Dhaka",
        "University of Virginia"
    ]
}