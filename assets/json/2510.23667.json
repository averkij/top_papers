{
    "paper_title": "Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization",
    "authors": [
        "Amin Heyrani Nobari",
        "Lyle Regenwetter",
        "Cyril Picard",
        "Ligong Han",
        "Faez Ahmed"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, a few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), a foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines a resolution- and shape-agnostic autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on OpenTO, a new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on a single GPU across resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These results establish OAT as a general, fast, and resolution-free framework for physics-aware topology optimization and provide a large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at https://github.com/ahnobari/OptimizeAnyTopology."
        },
        {
            "title": "Start",
            "content": "OPTIMIZE ANY TOPOLOGY: FOUNDATION MODEL FOR SHAPEAND RESOLUTION-FREE STRUCTURAL TOPOLOGY OPTIMIZATION 5 2 0 2 6 2 ] . [ 1 7 6 6 3 2 . 0 1 5 2 : r Amin Heyrani Nobari Massachusetts Institute of Technology Cambridge, MA, 02139 ahnobari@mit.edu Lyle Regenwetter Massachusetts Institute of Technology Cambridge, MA, 02139 regenwet@mit.edu Cyril Picard Massachusetts Institute of Technology Cambridge, MA, 02139 cyrilp@mit.edu Ligong Han Red Hat AI, MIT-IBM Watson AI Lab Cambridge, MA, 02139 ligong.han@ibm.com Faez Ahmed Massachusetts Institute of Technology Cambridge, MA, 02139 faez@mit.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines resolutionand shape-agnostic autoencoder with an implicit neural-field decoder and conditional latent-diffusion model trained on OpenTO, new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on single GPU across resolutions from 64 64 to 256 256 and aspect ratios as high as 10:1. These results establish OAT as general, fast, and resolution-free framework for physics-aware topology optimization and provide large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at https://github.com/ahnobari/OptimizeAnyTopology."
        },
        {
            "title": "Introduction",
            "content": "Foundation modelslarge models trained on broad data that can be adapted to many downstream taskshave transformed modern AI. They underpin advances in vision, language, and multimodal reasoning, powering systems such as large language models (LLMs) and image generators that now drive progress across domains, including scientific breakthroughs in protein folding [21] and drug discovery [16]. In contrast, engineering design remains dominated by inverse problems, where the goal is to infer design that meets stated constraints while maximizing performance objectives. Designing bridge, for example, may require allocating fixed mass of steel so the finished span withstands specified loads with required stiffness. Because such performance maps are highly nonlinear, data-intensive, and constraint-sensitive, the field continues to rely on direct optimization [6] or narrowly scoped deep-learning surrogates Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization and generative models [38]. Limited datasets and the need for high precision have so far hindered the emergence of foundation models for engineering design [38]. Structural topology optimization (TO) exemplifies these challenges [49] and presents good testbed for developing engineering foundation models. TO seeks the optimal material distribution within design domain to maximize physics-based performance metrics such as stiffness or strength. Solving TO conventionally involves repeated Finite Element Analysis (FEA) and gradient-based updates until convergence [3, 41, 50]. This process is computationally expensive and must be entirely restarted when boundary conditions, geometry, or load cases change, limiting its practicality in interactive or large-scale design settings. Solving the underlying PDEs on arbitrary shapes and boundary conditions also yields huge, non-convex search space, where tiny design changes can trigger failure; high precision is therefore essential. learning methods comprehensive have reviews emerged detail to this accelerate expansive or field replace [49, op62], [38, these costs, alleviate deep While To timization 49]. our focus is on the evolution of models that directly predict near-optimal topologies from problem specifications (boundary conditions, forces, volume fraction) [13, 14, 20, 30, 33 35]. By directly predicting optimal topologies, these aim to substitute the entire optimization process with rapid predictive framework. These typically employ Deep Generative Models (DGMs)such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs) [2, 26, 32, 37, 48, 61, 63], and more recently Denoising Diffusion Probabilistic Models (DDPMs) [13, 14, 30]to frame TO as conditional generation task. Despite numerous innovations, these direct prediction approaches have faced significant criticism by few TO researchers for their limitations [34, 35, 62]. Figure 1: In TO, the objective is to distribute material in domain (a density field œÅ(x)) to obtain optimal physics-based performance. Above, we show an example of TO for maximum stiffness, given boundary conditions of material supports and forces applied. The most significant issue, termed the generalizability challenge, is characterized by the inability of current models to manage arbitrary boundary conditions and forces [62]. Current direct prediction methods are trained on finite, often small, sets of predefined problem configurations (e.g., maximum of 210 unique setups [35]). Consequently, they fail to generalize to unseen or more complex settings, fundamentally limiting their practical utility. Key limitations include: 1) Restricted domain geometry, often confined to fixed square grids [13, 14, 30, 33], rendering many models limited proofs-of-concept. 2) Simplistic problem representations, frequently adapting FEA fields as images for standard vision models (e.g., CNNs) [34]. While recent works [34, 64] show promise in addressing some of these domain and representation issues, more critical limitation persists: 3) Poor generalizability. Existing models are trained on narrow, controlled problem sets (e.g., only 42 boundary condition configurations [14, 30, 34]) and fail on unseen conditions, considered major hurdle by critics [62]. We introduce the Optimize Any Topology (OAT) framework, pre-trained latent diffusion model for general structural TO focused on minimum compliance. OAT represents the first attempt to simultaneously address the representation and shape/resolution limitations noted in prior work, while also tackling the generalizability challenge. It is positioned as foundational step towards TO models capable of handling any arbitrary boundary conditions and domain shape/resolution. Our foundational scope refers specifically to the models ability to generalize across arbitrary boundary conditions, forces, and resolutionscapabilities that have not been demonstrated in prior topology optimization research. We emphasize that OAT is focused on the minimum-compliance problem under linear elasticity and does not encompass all forms of physics-driven topology optimization. Thus, rather than claiming universal foundation model for physics-based design, we present OAT as the first step toward such general frameworks: foundation model that captures the minimum compliance problem in its most general structural and geometric form. The primary contributions of this work are as follows: 1) OpenTO Dataset: We introduce OpenTO, the largest and first general-purpose topology optimization dataset, comprising 2.2 million optimized structures with fully randomized boundary conditions, loads, shapes, and resolutions. OpenTO establishes scalable foundation for data-driven research in physics-based structural design. 2) OAT Framework: We propose Optimize Any Topology (OAT), the first deep learning framework capable of addressing the general topology optimization problemhandling arbitrary boundary 2 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization conditions and resolutions within single model. 3) State-of-the-Art Performance: Through extensive experiments, we demonstrate that OAT achieves up to 10x lower compliance error than prior models on established benchmarks, outperforming all existing architectures even without post-optimization refinement. It also performs better than existing methods on the foundational dataset of OpenTO. 4) Scalable Inference: We show that OAT achieves sub-second inference on single GPU and scales efficiently to higher resolutions, maintaining nearly constant inference time from 64 64 to 256 256 gridswhere prior methods experience severe slowdownsthus enabling near real-time topology generation across diverse design problems."
        },
        {
            "title": "2 Preliminaries",
            "content": "This section introduces the preliminaries of topology optimization and then examines the challenges and criticisms leveled towards deep learning approaches for TO. 2.1 Topology Optimization Structural Topology Optimization (TO) is the process of determining the optimal distribution of limited amount of material within defined design space to maximize (or minimize) specific physics-based performance metrics. Let ‚Ñ¶ Rd represent bounded design domain with boundary Œì = ‚Ñ¶. The systems physical behavior is described by state field : ‚Ñ¶ Rm (e.g., displacement), which is the solution to governing partial differential equation (PDE). For simplicity, considering only Dirichlet boundary conditions, this PDE takes the form: L(cid:0)œÅ(x), u(x)(cid:1) = (x) in ‚Ñ¶, u(x) = g(x) on ‚Ñ¶ (1) Here, œÅ(x) : ‚Ñ¶ [0, 1] is the material density at each point x, acting as the design variable we seek to optimize. is differential operator representing the underlying physics (e.g., elasticity), (x) represents applied forces or sources, and g(x) prescribes values for u(x) on portion of the domain (which can be on the boundary Œì or within ‚Ñ¶). The general continuous TO problem is to find the material density distribution œÅ(x) that minimizes performance objective J(cid:0)u(œÅ), œÅ(cid:1), such as structural compliance (inverse of stiffness) or thermal resistance. This is subject to the governing PDE (Eq. 6) and constraint on the total material volume Vmax: J(cid:0)u(œÅ), œÅ(cid:1) s.t. L(cid:0)œÅ, u(cid:1) = (x), u(x) = g(x) on , min œÅ() (cid:90) ‚Ñ¶ œÅ dx Vmax. (2) This process and the overall problem are depicted in Figure 1. Analytical solutions to the PDE in Eq. 6 are generally unobtainable for complex geometries and material distributions. These practical difficulties necessitate numerical approach to TO. This is usually done by discretizing the domain into mesh and solving the problem using Finite Element Analysis (FEA). In this paper, we focus on common type of topology optimization problem, the minimum compliance TO. The goal in minimum compliance TO is to distribute material to minimize compliance (i.e., maximize stiffness) given some boundary conditions and forces (Figure 1). We detail the underlying physics and optimization scheme for minimum compliance in Appendix A. Despite the cost and complexities of conventional optimization, optimizers such as SIMP [3, 41] are generalizable, meaning that the domain ‚Ñ¶ can have any arbitrary shape, loads may be applied at any node of the FEA mesh and in any direction/magnitude, and the boundary conditions may also be applied in any location and for any direction. As such, general framework should, in theory, be able to handle these generalities. This, however, proves to be rather difficult in deep learning frameworks built for TO. 2.2 Diffusion Models Œ±tx0 + Denoising Diffusion Probabilistic Models (DDPMs) [18] are class of generative models that learn to synthesize data by reversing noise corruption process. Given data sample x0 from distribution q(x0), fixed forward diffusion process gradually adds Gaussian noise over discrete time steps. This process can be characterized by xt = 1 Œ±tœµ, where œµ (0, I) is random noise, and Œ±t is predefined noise schedule that decreases from Œ±0 1 to Œ±T 0, such that xT is approximately distributed as (0, I). Typically, neural network, œµŒ∏(xt, t), is trained to reverse this process by predicting the noise component œµ added at time step to the noisy sample xt. The standard training objective, derived from variational lower bound on the data log-likelihood, simplifies to minimizing the mean squared error between the true and predicted noise: LDDPM = Ex0,t,œµ (cid:104) œµ œµŒ∏(xt, t)2(cid:105) . (3) Once trained, samples are generated by iteratively applying the learned reverse transition pŒ∏(xt1xt), starting from xT (0, I). Alternative parameterizations for the reverse process exist. For instance, the model can be trained Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Œ±tœµ 1 Œ±tx0, which can offer benefits in training stability, to predict velocity term vŒ∏(xt, t), such as = (cid:2)vŒ∏(xt, t) v2(cid:3). uniform signal to noise ratio and sample quality [22, 44]. The objective then becomes L(v) = Ex0,t,œµ For more efficient sampling, Denoising Diffusion Implicit Models (DDIMs) [53] introduce non-Markovian reverse process that allows for deterministic generation and significantly fewer sampling steps. DDIMs achieve this by deriving an update rule that directly predicts an estimate of x0 (or uses œµŒ∏) to take larger, deterministic steps towards the clean data sample. In this way, DDIMs enable fewer sampling steps while retaining quality crucial in the TO context, where the primary benefit of deep learning is solution speed. In our work, we use the velocity term formulation and DDIM rather than the vanilla DDPM objective."
        },
        {
            "title": "3 Related Works",
            "content": "We now discuss prior works closely related to our framework, building upon the established TO background and our papers motivation. Diffusion Models and Resolution Free Variants. Originating with Sohl-Dickstein et al. [52], diffusion models [54 56] are now state-of-the-art for conditional and unconditional image synthesis [10, 19, 31, 42, 43], offering stable training over GANs [15] and utility for inverse problems [57], an important feature for TO. Their iterative nature means slow inference, drawback partially addressed by faster sampling [24, 29, 45]. Nevertheless, pixel-space diffusion remains intensive for high resolutions, limiting prior TO works [34]; latent space compression offers path to mitigate these issues. Our work aligns with Latent Diffusion Models (LDMs) [40], which perform diffusion in compressed latent space typically learned via autoencoders [51, 59]. Critical to this paradigm is the nature of the latent representation upon which the diffusion model operates. Most conventional autoencoders work on fixedresolution/shape images/domains, unsuitable for TOs need for arbitrary aspect ratios and resolutions, thus motivating resolution-free latent representations. Recent efforts extend diffusion to arbitrary resolutions [5, 8, 12, 23, 27]. One such approach performs diffusion in infinite-dimensional continuous spaces using neural operators [5, 12, 23, 25, 27], offering training scalability but slow high-resolution inference as all cells/pixels must be sampled in inference. Alternatively, resolution-free autoencoders with finite-dimensional latents [8] preserve LDM efficiency, often using Implicit Neural Representations (INRs) for decoding arbitrary resolutions. Given INRs promise in TO [34, 35], we are inspired by Image Neural Field Diffusion (INFD) [8]. INFD introduces Convolutional Local Image Functions (CLIFs) for high-quality, resolution-free autoencoder with finite latents, relaxing INRs pixel-independence. Our approach extends this idea beyond variable resolution to arbitrary aspect ratios and domain geometries, integrating conditional latent diffusion model tailored for physics-based topology optimization. Representations of TO Problems. Many prior DL TO works used image-based boundary conditions and force representations [14, 20, 30, 33]. These often require FEA simulations, limiting FEA-free inference, or offer lower performance if FEA-free [14], and invariably suffer from resolution/shape dependence. Nobari et al. [34] proposed the Boundary Point Order-invariant MLP (BPOM), point-cloud boundary condition representation that matches FEA-based input performance without requiring FEA simulation (See in Figure 2) and mitigating the resolution/shape dependence issues. We adopt BPOM for its generalizability and efficiency."
        },
        {
            "title": "4 Methodology",
            "content": "Our method, OAT, Latent Diffusion Model (LDM) [40], first employs an autoencoder to map topologies to fixeddimension latents (decoded by neural field for arbitrary resolution), then conditional diffusion model generates topologies from these latents based on TO problem specifications (Figure 2; Appendix for implementation details). 4.1 Resolutionand Shape-Agnostic Autoencoder with Neural Fields. To handle the variability of mixed resolutions and aspect ratios  (Fig. 2)  , we employ an autoencoder with neural field renderer, inspired by INFD [8]. The encoder maps topology to latent = E(T ); decoder converts to feature tensor œï = D(z); and renderer R, extending the CLIF concept [8], reconstructs = R(œï, c, s) using œï, coordinates c, and cell/pixel sizes s. Scalable Training and Inference. Inspired by VQ-VAE [60] in LDMs [40], our autoencoder pads and resizes variable topologies to fixed 256 256 inputs for encoder  (Fig. 2)  . Decoder then generates fixed resolution feature tensor œï. key challenge in TO is high-frequency changes with small changes in boundary conditions. To address this, we employ convolutional renderer (unlike point-wise models [7, 34]) for better high-frequency details and scale 4 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 2: OAT generative framework overview. resolution-free autoencoder encodes variable OpenTO dataset topologies into fixed-resolution latent space. Latent diffusion models (LDMs) then conditionally generate topologies. Problem specifications, including forces and boundary conditions represented as point clouds, are processed by BPOM models and MLPs to form fixed-size embedding to condition the LDM. consistency [8]. Rs local operator (inputs: nearest œï pixel, coordinates, cell sizes; Fig. 2) requires connected patch sampling given its convolutional nature. Thus, scalable training is possible with random patches from non-padding areas  (Fig. 2)  . High-resolution inference is also possible by stenciled, overlapping padded tiles (padding > Rs receptive field) when memory is limited. Training Objective. This neural field design yields resolution and shape-agnostic autoencoder that gives us fixedresolution latent representations. We train this autoencoder to minimize reconstruction error. For ground truth patch or topology TGT (with pixel positions and cell sizes s) and its reconstruction = R(œï, c, s), the objective is to minimize L1 norm, which has proven more effective for reconstruction in prior works [7, 8]: LAE = TGT1. 4.2 Problem Representation In topology optimization, key challenge is to represent the underlying topology optimization problem, which includes domain shape, boundary conditions, and loads. This is difficult, as the number and position of these problem settings change in every configuration. We represent the TO problem (Eq. 8) by encoding its primary components using different resolution and shape-independent encoders. The problem domain, ‚Ñ¶ (rectangular grid with regular cells in our dataset), is described by its aspect ratio R2 and its resolution via cell/pixel size  (Fig. 2)  . The target material volume, Vmax, is represented as scalar volume fraction = Vmax/V‚Ñ¶, which indicates the ratio of allowed material to total volume of the domain V‚Ñ¶. The more complex components, forces, and boundary conditions are handled as point clouds. Boundary conditions, which fix specific degrees of freedom, are represented as point set Sboundary with binary features indicating directional fixture  (Fig. 2)  . Similarly, applied forces are point set Sforce with 2D vector features denoting load magnitude and direction  (Fig. 2)  . As these sets are order invariant, we employ the Boundary Point Order-invariant MLP (BPOM) approach [34] to learn the embeddings of Sboundary and Sforce. The complete latent problem embedding, , is concatenation of five embeddings from distinct neural networks: = concat(BPOMb(Sboundary), BPOMf(Sforce), MLPvf(V ), MLPcell(s), MLPratio(a)), (4) where BPOMb and BPOMf embed boundary conditions and forces, respectively, while Multi-Layer Perceptrons (MLPs) embed the volume fraction (V ), cell size (s), and aspect ratio (a). This construction yields fixed-size 5 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization embedding capable of representing TO problems with diverse domain shapes, resolutions, and arbitrary boundary conditions and forces. For brevity, we denote this entire encoding process as = LP( ÀÜP ), where ÀÜP encompasses all raw problem inputs. 4.3 Latent Diffusion and Classifier-Free Guidance Having trained the autoencoder, we obtain latent representations = E(T ) for each topology . Furthermore, the TO problem is encoded into latent problem embedding = LP( ÀÜP ). We then train our diffusion model to predict the velocity conditioned on the problem definition, ÀÜP , and train it with the following modified diffusion objective: LLDM = EzE(T ),t,œµN (0,I) (cid:20)(cid:13) (cid:13) (cid:13)v vŒ∏ (cid:16) zt, ÀÜP 2(cid:21) (cid:17)(cid:13) (cid:13) (cid:13) . (5) Œ±tœµ 1 Œ±tz. To enable classifier-free guidance [17], the problem inputs ÀÜP are omitted for 50% of where = training samples, allowing the model to learn both conditional and unconditional denoising. This allows us to control the influence of conditioning during inference as described by Ho and Salimans [17]. Classifier-free guidance can be applied in our approach by adjusting the denoising velocity. Normally, in conditional denoising, one would use the predicted velocity at time step t, vŒ∏(zt, ), which predicts the velocity for the current noisy sample zt given problem embedding . When applying classifier-free guidance we can also compute the un-conditional denoising, namely vŒ∏(zt, ), and rather than just using vŒ∏(zt, ), we measure the shift in velocity direction as result of conditioning, vŒ∏(zt, ) vŒ∏(zt, ) and amplify this shift by an over-relaxation factor, œâ, leading to final denoising velocity ÀÜvŒ∏(zt, t, ) = vŒ∏(zt, ) + , which can be used in place of vŒ∏(zt, ), mimicking what Ho and Salimans [17] propose for classifier-free guidance. vŒ∏(zt, ) vŒ∏(zt, t, ) (cid:16) (cid:17) 4.4 Post Generation Optimizer Refinement. Despite the ever-increasing quality of DGM-generated solutions and other deep learning methods for TO, we observe that these models often lack precision in many cases and can lead to failed designs. To address this, we follow the few-step direct optimization approach commonly used in literature [14, 34]. few optimization steps (5-10) of SIMP (small compared to 200-500 iterations for full optimization) are equivalent to local search operation and are applied to topologies synthesized by OAT to improve the accuracy of the final samples. We also incorporate this into our framework; however, as we discuss in our experiments, even without any direct optimization, OAT outperforms other models that require optimization steps. 4.5 OpenTO Dataset Existing datasets in DL for TO face four challenges that we have overcome in creating the OpenTO dataset: 1) Most existing datasets are limited to one specific domain shape (square) and one resolution (64 64), with the most expansive dataset including five different shapes and resolutions [34]; 2) Most existing works are limited to small set of pre-defined boundary conditions (a maximum of 42 in 2D dataset [34]); 3) Existing datasets typically feature only single applied force; 4) All existing datasets only apply forces and boundary conditions at the borders of the design space (boundary of domain), and lack interior forcing and fixtures. These limitations restrict the development of foundation models and are key factor behind the generalizability challenges. Thus, we see this as major gap and introduce the largest open-source Topology Optimization (OpenTO) dataset for general purpose topology optimization with 2.194M samples. To enable foundational scale of data, OpenTO comprises procedurally-created random TO problems, running SIMP solver, and obtaining optimal topologies from the solver. The data generation is done such that OpenTO overcomes the aforementioned limitations of prior works. The main features of what makes OpenTO effective are: 1) Design domains in OpenTO are fully random in both resolution and shape with aspect ratios ranging from very narrow (10 to 1), to square (1 to 1), and pixel/cell size ranging from 1/64 to 1/1024, covering the exhaustive range of rectangular domain shapes and resolution in most practical real-world applications of TO in 2D; 2) OpenTO includes randomly sampled boundary conditions, including interior boundary condition point, with every sample having unique boundary condition configuration; 3) OpenTO includes fully random forces including interior forces, and configurations with as many as 4000 loads (see Appendix on distributed forces) in one configuration. The exhaustive nature of OpenTO makes it the first dataset to tackle the general TO problem and starting point for the development of foundation models for TO. OpenTO includes 5,000 test samples, with fully randomized configuration not in the main training data for testing the performance of models in general problem setting. The full details of the procedural data generation can be found in Appendix D. 6 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Table 1: Quantitative evaluation on 64 64 datasets. All models were only trained on 64 64 data, except OAT, which is trained on the OpenTO dataset. w/ G: using classifier and regression guidance. CE: Compliance Error. VFE: Volume Fraction Error. + 5 and + 10 indicate 5 and 10 steps of post-generation direct optimization. OAT, without any postgeneration optimization, outperforms NITO + 10, the SOTA model that also uses 10 steps of optimization. Model CE % CE % Med VFE % TopologyGAN [33] cDDPM [14] TopoDiff [34] TopoDiff w/ [34] DOM w/o TA [14] DOM w/ TA [14] NITO [34] OAT (Ours) NITO + 5 [34] TopoDiff + 5 [34] TopoDiff w/ + 5 [34] OAT (Ours) + 5 48.51 60.79 3.23 2.59 13.61 4.44 8.13 1.74 0.30 3.55 2.24 0. NITO + 10 [34] TopoDiff + 10 [34] TopoDiff w/ + 10 [34] OAT (Ours) + 10 0.17 1.38 1.05 0.0503 2.06 3.15 0.45 0.49 1.79 0.74 0.47 0.32 0.12 0.42 0.44 0.041 0.071 0.33 0.32 0.014 11.87 1.72 1.14 1.18 1.86 0.74 1.40 0. 0.40 0.67 0.69 0.39 0.25 0.45 0.45 0.27 Table 2: Quantitative evaluation on 256 256 datasets. Same settings as 64 64. Model CE % CE % Med VFE % TopoDiff [34] NITO [34] OAT (Ours) NITO + 5 [34] OAT (Ours) + 5 16.62 9.178 1.51 0.25 0.54 0.59 0.96 0.51 0.09 0.37 2.92 1.52 -0. 0.34 -0.12 0.033 NITO + 10 [34] OAT (Ours) + 10 0.081 0.012 0.16 0.130 -0.0502 Table 3: Quantitative evaluation on the combined 5 shape datasets. NITO results are only available with direct optimization in the original work [34]. Model CE CE % Med VFE % 0.56 NITO + 10 [34] OAT (Ours) 1.94 OAT (Ours) + 5 0.82 OAT (Ours) + 10 0.23 0.13 0.89 0.12 0.091 0.39 0.103 0.35 0.28 Finally, OpenTO additionally includes the prior datasets developed in limited settings (194,000 samples from Nobari et al. [34]). Overall, OpenTO includes 2.194M samples of topologies, with 894,000 samples being labeled (with defined ÀÜP ) and the rest only including the topology. We use all samples to train our autoencoder, and only use the 894,000 samples with labels to train our LDM."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we perform experiments to showcase OATs increased performance compared to prior works trained on the smaller, limited datasets. This constitutes the first truly general TO benchmark on fully randomized problems. Before detailing the experiments, we will first briefly describe the evaluation metrics we use to measure the performance of each model. Evaluation Metrics: As we described in Section 2.1, TO involves both compliance objective and volume fraction constraint. Thus, we measure the performance of models with respect to these two requirements of the TO problem. First, to measure how well the models perform in compliance minimization, we measure compliance error (CE), which is calculated by subtracting the compliance of generated sample from the compliance of the SIMP-optimized solution for the corresponding problem, then normalizing by the SIMP-optimized compliance value. Since the mean compliance error tends to be dominated by just few outlier samples, we also report the median compliance error. We also report mean volume fraction error (VFE), which quantifies the absolute error between the generated topologys actual volume fraction and the target volume fraction for the given problem. This benchmarking is standard practice for DGMs in TO [14, 30, 33, 34]. Experimental Setup. Our experiments benchmark OAT both on existing benchmarks with limited boundary conditions and on our new OpenTO test set with generalized problem configurations. In all experiments, we train OAT exclusively on the OpenTO dataset, and never on any benchmark-specific data. On existing benchmarks, we compare against the specialized existing models trained specifically for these benchmarks and report performance metrics as reported by the original authors. On the OpenTO benchmark, only one prior work, NITO [34], has the architectural generalizability to be trained on this data. Thus, we train OAT with combined 729.94M (672.00M for LDM and 57.94M for AE) parameters, training the diffusion model for 50 epochs (349,219 steps). For fair baseline, we train variant of NITO for the same number of training steps, scaled to 732M parameters to approximately match OAT. For 7 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization each method, we then sample solutions for each problem and test performance with and without direct optimization. In all experiments, OAT sampling is done with 20 DDIM deterministic denoising steps with classifier-free guidance scale of 2.0 (as informed by our ablation studies presented in Appendix B. We sample 10 sets of solutions for each problem and report the average across runs. 5.1 Comparison To State of The Art Prior works are typically limited to specific domain shapes and resolutions. We first compare the performance of OAT with prior state-of-the-art on their own test sets [30, 33, 34, 34]. The 64 64 Benchmark. The most common benchmark that previous state-of-the-art models [14, 33, 33, 34] have trained on and benchmarked against is TO problems with single force, and only 42 distinct boundary condition configurations defined on 64 64 domain. We evaluate OAT on this benchmark with no specific training or fine-tuning. We also test post-generation optimization, which some prior works [14, 34] explore. Table 1 shows the results on the 64 64 benchmark. We can see that OAT has the lowest compliance error amongst models without direct optimization, with only 1.74% error compared to 2.59% from the best model (TopoDiff with classifier guidance), showing 32% reduction in compliance error. Notably, OAT achieves this while maintaining stricter adherence to the target volume fraction, demonstrating superior efficiency in material usage. Table 1 shows that when post-generation optimization is applied, OAT continues to outperform all competing methods, achieving the lowest compliance and volume-fraction errors across all settings. This represents the first instance in topology optimization where foundation-model-based approach not only generalizes across problem configurations but also surpasses all specialized, task-specific models trained on restricted datasets. This establishes OAT as major milestone in advancing generalizable, data-driven topology optimization. The 256 256 Benchmark. Nobari et al. [34], in their neural field-based method, expand the prior mentioned data by solving the same problems at much higher resolution of 256 256 resolution (still the same boundary conditions and single load) and test their method and the highest performing prior work, TopoDiff [30], on this data by retraining larger TopoDiff on this resolution. We report the results of our model and the reported values for NITO and TopoDiff [34], in Table 2. The trends here are similar to the 64 64 data, with an even larger gap in performance, with specialized models unable to beat the general framework of OAT in this harder high resolution benchmark. However, we do see that NITO, which predicts blurry gray samples [34], allows more freedom for direct optimization, thus yielding marginally better compliance errors with few-step optimization. However, note that OAT can perform nearly on par with these costly direct optimization methods without using any direct optimization. Extended five shape benchmark. Aside from extending the dataset to higher resolution, Nobari et al. [34] also expands the data by introducing three additional domain shapes apart from 64 64 and 256 256. The authors add data for 64x48, 64x32, and 64x16 domains, still with the same boundary condition configuration adapted to these new shapes. The authors then train their model on all of this data and report the performance on this benchmark. Unlike prior works, which could not handle this kind of mixed shape/resolution data, our approach, like NITO [34], can be adapted to this data easily. Table 3 shows how NITO model only trained on this dataset lags behind OAT, as we saw in prior benchmarks. 5.2 The General Benchmark The only prior work that can handle the OpenTO dataset is the NITO [34] model, which is non-generative neural field model. As such, we train NITO on OpenTO and report the performance of the model on our fully random benchmark. Notably, even though we train variant of NITO with the same number of parameters, we observe very poor visual quality in samples generated by NITO (Figure 3). The authors of the original work had also observed non-sharp boundaries and suggested that this issue [34] may be due to the deterministic (non-generative) nature of their model. OAT learns this more complex benchmark significantly better. Figure 3 shows that the quality of samples generated by OAT is more realistic. Nonetheless, OpenTO presents challenging benchmark that both OAT and NITO struggle with. Failure Study. Given the precise nature of the topology optimization problem, small changes in material distribution can cause catastrophic failures in the outcome. This often occurs when the model fails to place material on boundary conditions or loads precisely. The example in Figure 3 shows how failed topology looks very similar to the ground truth topology but has poor physical performance. Since DGMs often focus on distribution matching but fail to capture the precise requirements of the problem, such failures are common in DL-based TO. In this benchmark, we define failure as having compliance error greater than 8 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 3: Left: Samples from ground truth (top), NITO (middle), and OAT (bottom) on random OpenTO benchmark; compliance errors shown below. NITO outputs are noisy with many gray pixels, unsuitable for real-world use. Right: Example highlighting sensitivity in TO; slight deviations by OAT led to material misplacement at boundary conditions, causing design failure. This highlights the precision requirements of TO. Table 4: Quantitative evaluation on the fully general benchmark of TO problems. It can be seen that OAT outperforms NITO-L, which is the only existing model that could be trained on OpenTO. * Values are reported for the non-failure samples. Table 5: Inference time scaling results of generating multiple samples and reporting the results for the best of samples generated by OAT on the fully random benchmark. Model CE* % CE* % Med VFE* % Failure Rate % NITO OAT (Ours) NITO + 5 OAT (Ours) + 5 NITO + 10 OAT (Ours) + 10 13.14 8.17 5.10 6.29 3.56 4.31 6.43 3. 1.78 2.86 1.33 2.24 5.3861 -0.12 1.75 0.68 0.83 0.57 60.01 39. 23.88 22.11 16.39 15.90 Best of CE* % CE* % Med VFE* % Failure Rate % 2 4 8 16 32 64 7.40 5.50 4.14 1.97 1.76 0.4439 3.02 2.65 2.41 1.58 1.55 1. -0.0021 0.041 0.096 0.14 0.15 0.19 32.95 27.50 21.50 16.15 12.92 11.51 100%, meaning that the generated topology has compliance more than double that of the conventional optimizer. Despite OAT frequently incurring such failures, its failure rate is still significantly lower than NITOs, as shown in Table 4. Given that the topologies in most failure cases only need small corrections, 5 to 10 steps of optimization reduce OATs failure rate from 39% to only 16%. This confirms that these imprecisions can be healed with only handful of optimization steps. Further improvements to failure rate may be addressed by larger models and datasets, as well as more advanced paradigms emerging in foundation model training, such as reinforcement learning techniques [36, 47] or utilizing invalid data [39]. Generalized Performance. In addition to the failure rate, we also report the compliance and volume fraction errors for each method with and without direct optimization, measured only on the successful samples. OAT significantly outperforms NITO in both of these physics-based performance metrics. Notably, OAT has compliance error of less than 10% on the fully random general data, which reduces to only 5% after few steps of optimization. Comparing this to the first models for deep learning in TO, such as TopologyGAN [33], which had over 50% error on the limited 64 64 dataset, 5% error on fully random data marks significant step forward for the community and heralds potential paradigm shift toward foundation models. Inference Time Scaling Study. The high failure rate observed in Table 4 may seem stark upon first glance; however, we believe that the generative nature of OAT provides clear path to alleviate this issue. With generative models like OAT, one can generate vast numbers of candidates in short amount of time, considering batch inference of diffusion models scales better than sampling one problem at time. In Table 5, we generate multiple candidates for each problem and measure the performance of the best amongst all samples. These results clearly show how the failure rate goes down rapidly as more samples are evaluated, with as few as eight samples yielding only 20% failure and CE that is half that of the single sample run in Table 4. We believe this shows how powerful foundational generative frameworks Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization like OAT can be for physical AI. Moreover, this favorable scaling with more samples clearly shows how emerging approaches around reinforcement learning [36, 46] in generative models can be employed here to further improve the performance of OAT. 5.3 Sampling Efficiency & Inference Speed Model Parameters (M) Inference Time (s) 64 64 Resolution Table 6: Average inference time at 64 64 and 256 256 resolutions on an RTX 4090 GPU (10-run average). Some timings include 10 refinement iterations (marked w/ Ref). NITO-S and NITO-L are original and larger variants [34]; OAT timings use 20 DDIM steps. Parentheses show an increase factor when elements scale by 16x. SIMP times are reported using an Intel 14-900K CPU (RTX4090 for SIMP-GPU). All SIMP inference speeds are measured for 150 steps of optimization (average needed to converge). Inference speed is one of the key advantages of deep learning over iterative optimization. Its important to mention that high-level analysis of computational cost in this case would result in an O(N ) complexity for OAT, where is the number of pixels/elements for given sample. This is because the nonconstant cost of OAT is in the neural field renderer, which would have complexity of O(N ). This is compared to the expensive FEA simulations at each step of the optimization, yielding an O(N 3) complexity (Solving linear system of equations). However, given that FEA matrices in this setting are highly sparse and the meshes in our work are structured, specialized multi-grid solvers can, in theory, reach an O(N ) efficiency [1, 28, 58]. This high-level analysis is however, fails to realize the inference efficiency of deep learning models compared to conventional optimization, given that it does not take into account the highly parallelized inference of deep learning models on the GPU compared to the iterative nature of linear system solvers. This makes inference time an overall better measure of performance in practical consumer hardware. Thus, it is important to analyze the inference time of different approaches. Table 6 shows the results of inference speed for different models. NITO, the only model with comparable generalizability, is faster at 64 64 resolution, but requires the whole network for each sample point over the entire field, meaning that it scales poorly with resolution. In contrast, the latent diffusion model, whose autoencoder only has 40M parameters, can be sampled at high resolution much more efficiently. NITO heavily relies on post-sampling optimization, which often contributes the majority of inference cost. Even ignoring this optimization case, OAT scales significantly faster than NITO and generates samples much faster at higher resolutions, despite having the same number of parameters. OAT even outperforms similar diffusion models in inference speed. For example, compared to TopoDiffs 100 denoising steps, OATs DDIM sampling requires only 20 denoising steps, significantly accelerating inference. Most importantly, unlike some prior diffusion models, OAT remains faster than optimization even at low resolution, where direct optimization is relatively fast. Overall, OAT is significantly faster than SIMP, making it an attractive option for fast design space exploration. TopoDiff TopoDiff w/ DOM SIMP SIMP GPU NITO-S NITO-S w/ Ref NITO-L NITO-L w/ Ref OAT (Ours) OAT (Ours) w/ Ref TopoDiff TopoDiff w/ DOM SIMP SIMP GPU NITO-S NITO-S w/ Ref NITO-L NITO-L w/ Ref OAT (Ours) OAT (Ours) w/ Ref 10.81 (5.812) 22.04 (4.601) 7.82 (9.537) 69.45 (20.13) 68.30 (19.80) 0.16 (32.00) 2.88 (20.57) 0.67 (13.40) 3.52 (19.56) 0.51 (1.003) 3.28 (5.129) 1.86 4.79 0.82 3.45 9.13 0.005 0.14 0.05 0.18 0.508 0.637 553 1092 553 - - 22 22 732 732 730 730 121 239 121 - - 22 22 732 732 730 730 256 256 Resolution It is important to note that some critics of generative models for TO point to the break-even analysis of computational cost [62]. We estimate this and discuss this concern in more detail in Appendix G."
        },
        {
            "title": "6 Conclusion & Future Work",
            "content": "We have introduced Optimize Any Topology (OAT), the first foundation-model approach to structural topology optimization that is agnostic to domain shape, aspect ratio, and resolution. Trained on the new OPENTO corpus comprising 2.2 million optimized designs, OAT couples resolution-free latent auto-encoder with conditional diffusion prior and achieves much lower mean compliance error than prior deep-learning baselines on the canonical 64 64 benchmark (1.7 % versus 2.59 %) while delivering sub-second inference. Moreover, OAT is easily extendable to different TO problems and physics, such as heatsink optimization, given OATs auto-encoder has great zero-shot capability for reconstructing such topologies without re-training (see Appendix C). This clearly solidifies the case for OAT as foundational framework for TO. On fully general benchmark with random boundary conditions and resolutions, OAT attains < 10 % mean compliance error. These results demonstrate that large-scale pre-training combined with resolution-free latent diffusion is viable path towards real-time, physics-aware design exploration. Despite the promising performance of OAT, significant challenges remain. First, we note that OAT faces notable failure rate 10 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization on fully random benchmarks, which future work will focus on addressing through directions such as reinforcement learning [36, 46] based on optimizer guidance in diffusion models [11]. Future work should also focus on addressing multi-physics objectives, and develop few-shot fine-tuning approaches to quickly adapt OAT or other foundational frameworks to new physics such as stress-constrained and buckling TO problems."
        },
        {
            "title": "7 Acknowledgements",
            "content": "We would like to acknowledge Akash Srivastava and the MIT-IBM Watson AI Labs for helpful insights and guidance in the early stages of developing our methods. We would also like to acknowledge the crucial guidance from Professor Josephine Carstensen in developing the solver used to generate the data and perform benchmarks in this work. PI Ahmed acknowledges support from the National Science Foundation under CAREER Award No. 2443429. 11 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization"
        },
        {
            "title": "References",
            "content": "[1] Niels Aage, Erik Andreassen, Boyan S. Lazarov, and Ole Sigmund. Giga-voxel computational morphogenesis for structural design. Nature, 550(7674):8486, 2017. [2] Mohammad Mahdi Behzadi and Horea T. Ilies. Gantl: Toward practical and real-time topology optimization with conditional generative adversarial networks and transfer learning. Journal of Mechanical Design, 144(2), 2021. 021711. [3] Martin Philip Bends√∏e and Noboru Kikuchi. Generating optimal topologies in structural design using homogenization method. Computer Methods in Applied Mechanics and Engineering, 71(2):197224, 1988. [4] Shai Bernard, Jun Wang, and Mark Fuge. Mean squared error may lead you astray when optimizing your inverse design methods. In International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, page V03AT03A004, 2022. [5] Sam Bond-Taylor and Chris G. Willcocks. -diff: Infinite resolution diffusion with subsampled mollified states, 2024. [6] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. [7] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86288638, 2021. [8] Yinbo Chen, Oliver Wang, Richard Zhang, Eli Shechtman, Xiaolong Wang, and Michael Gharbi. Image neural field diffusion models, 2024. [9] Leszek F. Demkowicz. Mathematical Theory of Finite Elements. Society for Industrial and Applied Mathematics, Philadelphia, PA, 2023. [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34, 2021. [11] Yuan Dong, Dawei Li, Chi Zhang, Chuhan Wu, Hong Wang, Ming Xin, Jianlin Cheng, and Jian Lin. Inverse design of two-dimensional graphene/h-bn hybrids by regressional and conditional gan. Carbon, 169:916, 2020. [12] Giulio Franzese, Simone Rossi, Dario Rossi, Markus Heinonen, Maurizio Filippone, and Pietro Michiardi. Continuous-Time Functional Diffusion Processes. arXiv preprint arXiv:2303.00800, 2023. [13] Giorgio Giannone and Faez Ahmed. Diffusing the optimal topology: generative optimization approach. arXiv preprint arXiv:2303.09760, 2023. [14] Giorgio Giannone, Akash Srivastava, Ole Winther, and Faez Ahmed. Aligning optimization trajectories with diffusion models for constrained design generation. arXiv preprint arXiv:2305.18470, 2023. [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [16] Fei Guo, Renchu Guan, Yaohang Li, Qi Liu, Xiaowo Wang, Can Yang, and Jianxin Wang. Foundation models in bioinformatics. National Science Review, 12(4):nwaf028, 2025. [17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [19] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. [20] Jiangbei Hu, Ying He, Baixin Xu, Shengfa Wang, Na Lei, and Zhongxuan Luo. If-tonir: Iteration-free topology optimization based on implicit neural representations. Computer-Aided Design, 167:103639, 2024. [21] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ≈Ω√≠dek, Anna Potapenko, and et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583589, 2021. [22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models, 2022. [23] Gavin Kerrigan, Justin Ley, and Padhraic Smyth. Diffusion Generative Models in Infinite Dimensions. In International Conference on Artificial Intelligence and Statistics, pages 95389563. PMLR, 2023. 12 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization [24] Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. arXiv preprint arXiv:2106.00132, 2021. [25] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to pdes. Journal of Machine Learning Research, 24(89):197, 2023. [26] Baotong Li, Congjia Huang, Xin Li, Shuai Zheng, and Jun Hong. Non-iterative structural topology optimization using deep learning. Computer-Aided Design, 115:172180, 2019. [27] Jae Hyun Lim, Nikola Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli, Jean Kossaifi, Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al. Score-Based Diffusion Models in Function Space. arXiv preprint arXiv:2302.07400, 2023. [28] Haixiang Liu, Yuanming Hu, Bo Zhu, Wojciech Matusik, and Eftychios Sifakis. Narrow-band topology optimization on sparsely populated grid. ACM Trans. Graph., 37(6), 2018. [29] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022. [30] F. Maz√© and F. Ahmed. Diffusion models beat gans on topology optimization. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), Washington, DC, 2023. [31] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [32] Zhenguo Nie, Tong Lin, Haoliang Jiang, and Levent Burak Kara. Topologygan: Topology optimization using generative adversarial networks based on physical fields over the initial domain. Journal of Mechanical Design, 143(3), 2021. 031715. [33] Zhenguo Nie, Tong Lin, Haoliang Jiang, and Levent Burak Kara. Topologygan: Topology optimization using generative adversarial networks based on physical fields over the initial domain. Journal of Mechanical Design, 143(3), 2021. [34] Amin Heyrani Nobari, Giorgio Giannone, Lyle Regenwetter, and Faez Ahmed. Nito: Neural implicit fields for resolution-free topology optimization, 2024. [35] Amin Heyrani Nobari, Lyle Regenwetter, and Faez Ahmed. Towards domain-adaptive, resolution-free 3d topology optimization with neural implicit fields. 2024. V03AT03A012. [36] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. [37] Sharad Rawat and M.-H. Herman Shen. novel topology optimization approach using conditional deep learning. CoRR, abs/1901.04859, 2019. [38] Lyle Regenwetter, Amin Heyrani Nobari, and Faez Ahmed. Deep generative models in engineering design: review. Journal of Mechanical Design, 144(7):071704, 2022. [39] Lyle Regenwetter, Giorgio Giannone, Akash Srivastava, Dan Gutfreund, and Faez Ahmed. Constraining generative models for engineering design with negative data, 2024. [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [41] G. I. N. Rozvany, M. Zhou, and Torben Birker. Generalized shape optimization without homogenization. Structural optimization, 4:250252, 1992. [42] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pages 110, 2022. [43] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. [44] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. [45] Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion models. arXiv preprint arXiv:2104.02600, 2021. Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization [46] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [47] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [48] Conner Sharpe and Carolyn Conner Seepersad. Topology design with conditional generative adversarial networks. Volume 2A: 45th Design Automation Conference, 2019. V02AT03A062. [49] Seungyeon Shin, Dongju Shin, and Namwoo Kang. Topology optimization via machine learning and deep learning: review. Journal of Computational Design and Engineering, 10(4):17361766, 2023. [50] Ole Sigmund and Kurt Maute. Topology optimization approaches: comparative review. Structural and Multidisciplinary Optimization, 48(6):10311055, 2013. [51] Abhishek Sinha*, Jiaming Song*, Chenlin Meng, and Stefano Ermon. D2c: Diffusion-denoising models for few-shot conditional generation. arXiv preprint arXiv:2106.06819, 2021. [52] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 22562265. PMLR, 2015. [53] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [54] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019. [55] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:1243812448, 2020. [56] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [57] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. arXiv preprint arXiv:2111.08005, 2021. [58] Erik A. Tr√§ff, Anton Rydahl, Sven Karlsson, Ole Sigmund, and Niels Aage. Simple and efficient gpu accelerated topology optimisation: Codes and applications. Computer Methods in Applied Mechanics and Engineering, 410: 116043, 2023. [59] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in Neural Information Processing Systems, 34:1128711302, 2021. [60] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018. [61] Dalei Wang, Cheng Xiang, Yue Pan, Airong Chen, Xiaoyi Zhou, and Yiquan Zhang. deep convolutional neural network for topology optimization with perceptible generalization ability. Engineering Optimization, 54(6): 973988, 2021. [62] Rebekka Woldseth, Niels Aage, Andreas B√¶rentzen, and Ole Sigmund. On the use of artificial neural networks in topology optimisation. Structural and Multidisciplinary Optimization, 65(10):294, 2022. [63] Yonggyun Yu, Taeil Hur, Jaeho Jung, and In Gwun Jang. Deep learning for determining near-optimal topological design without any iteration. Structural and Multidisciplinary Optimization, 59(3):787799, 2019. [64] Zeyu Zhang, Yu Li, Weien Zhou, Xiaoqian Chen, Wen Yao, and Yong Zhao. Tonr: An exploration for novel way combining neural network with topology optimization. Computer Methods in Applied Mechanics and Engineering, 386:114083, 2021. 14 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization"
        },
        {
            "title": "Table of Contents for Appendices",
            "content": "A Topology Optimization and The Minimum Compliance Problem A.1 Minimum Compliance Topology Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.1 The Physics Model: Static Linear Elasticity . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.2 Finite Element Discretization and Compliance Calculation . . . . . . . . . . . . . . . . . . . A.1.3 Material Interpolation: The SIMP Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.4 The Minimum Compliance Optimization Problem . . . . . . . . . . . . . . . . . . . . . . . Ablation Studies Out of Distribution Reconstruction OpenTO Dataset D.1 Domain Definition and Discretization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Load and Constraint Specification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Problem Validation and Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Algorithm for Data Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Further Visualization of Results Implementation Details F.1 Autoencoder Architecture . F.2 Autoencoder Training . . . . . . . F.3 Latent Diffusion Architecture . F.4 latent Diffusion Training . F.5 Computational Resources . F.6 Dataset Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.7 Combining Prior Data With OpenTO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Break-Even Analysis Code & Data 16 17 17 17 18 19 21 21 21 22 23 30 30 30 31 31 32 43 43 43 15 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization"
        },
        {
            "title": "A Topology Optimization and The Minimum Compliance Problem",
            "content": "Structural Topology Optimization (TO) is the process of determining the optimal distribution of limited amount of material within defined design space to maximize (or minimize) specific physics-based performance metrics. Let ‚Ñ¶ Rd represent bounded design domain with boundary Œì = ‚Ñ¶. The systems physical behavior is described by state field : ‚Ñ¶ Rm (e.g., displacement), which is the solution to governing partial differential equation (PDE). For simplicity, considering only Dirichlet boundary conditions, this PDE takes the form: L(cid:0)œÅ(x), u(x)(cid:1) = (x) in ‚Ñ¶, u(x) = g(x) on ‚Ñ¶ (6) Here, œÅ(x) : ‚Ñ¶ [0, 1] is the material density at each point x, acting as the design variable we seek to optimize. is differential operator representing the underlying physics (e.g., elasticity), (x) represents applied forces or sources, and g(x) prescribes values for u(x) on portion of the domain (which can be on the boundary Œì or within ‚Ñ¶). The general continuous TO problem is to find the material density distribution œÅ(x) that minimizes performance objective J(cid:0)u(œÅ), œÅ(cid:1), such as structural compliance (inverse of stiffness) or thermal resistance. This is subject to the governing PDE (Eq. 6) and constraint on the total material volume Vmax: J(cid:0)u(œÅ), œÅ(cid:1) s.t. L(cid:0)œÅ, u(cid:1) = (x), u(x) = g(x) on , min œÅ() (cid:90) ‚Ñ¶ œÅ dx Vmax. (7) This process and the overall problem are depicted in Figure 1. While the ideal outcome is distinct structure ‚Ñ¶ with clear boundaries (binary densities), optimizing for such structure directly is challenging. Furthermore, analytical (closed-form) solutions to the PDE in Eq. 6 are generally unobtainable for complex geometries and material distributions. These practical difficulties necessitate numerical approach to TO. This is usually done by discretizing the domain into mesh with finite elements/cells, and solving the problem in this discretized space using Finite Element Analysis (FEA) [9]. So far, we described the general topology optimization problem in continuous space; however, as we alluded to, this problem is often not approached in the continuous form. To make the problem tractable, the continuous domain ‚Ñ¶ is discretized into finite element mesh Th, composed of small elements Th = {Ke}N e=1. Within this framework, the continuous material density œÅ(x) is approximated by vector of element densities œÅh = [œÅ1, . . . , œÅN ], where each œÅe [0, 1]. Similarly, the state field u(x) becomes vector uh = [u1, . . . , uM ] representing values at discrete points or \"degrees of freedom\" (DoFs) of the mesh (e.g., 2D displacements at nodes). Applied forces (x) and prescribed boundary values g(x) are also discretized into vectors fh and gh, where the boundary conditions are applied at specific DoFs Dh. The local element operator (e.g., stiffness matrix) Ke is derived from for each element [9]. The numerical TO problem then becomes finding the optimal discrete densities œÅh: min œÅh[0,1]N (cid:0)uh(œÅh), œÅh (cid:1) Jh s.t. K(œÅh) uh = fh, (cid:88) ve œÅe Vmax, e=1 ui = gi, Dh, (8) where Jh is the discretized objective function, K(œÅh) is the global system matrix (e.g., global stiffness matrix) assembled from element contributions, which depends on the material densities œÅh, and ve is the volume of element e. The first constraint, K(œÅh)uh = fh, is the discretized form of the governing PDE (Eq. 6). Although the goal is often binary design (œÅe {0, 1}, indicating presence or absence of material), directly solving this as mixed-integer non-linear program is computationally prohibitive for realistic problem sizes. Therefore, the optimization typically allows continuous densities œÅe [0, 1] and employs penalization techniques to encourage solutions that are nearly binary. widely used method is Solid Isotropic Material with Penalization (SIMP) [3, 41]. In SIMP, effective element densities are related to design variables œïe [0, 1] by œÅe = œïp e, where > 1 is penalization factor that makes intermediate density values (between 0 and 1) structurally inefficient, thus favoring œïe values close to 0 or 1. In our work, we focus on the minimum compliance problem, which involves solving the linear elasticity problem and using an FEA solver and iteratively optimizing the topology in discrete space. We detail this in the section below. A.1 Minimum Compliance Topology Optimization This section details prominent application of Topology Optimization (TO): minimum compliance structural optimization, often referred to as stiffness optimization. The primary objective is to determine the material layout that results in the stiffest possible structure under applied mechanical loads, subject to constraint on the total amount of material used. We will first describe the underlying physics model for this class of problems. 16 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization A.1.1 The Physics Model: Static Linear Elasticity In most structural topology optimization problems, including minimum compliance, the behavior of structure under load is modeled using linear elasticity. This model assumes that the structure is composed of material that exhibits linear elastic behavior (i.e., stress is proportional to strain via Hookes Law) and that deformations are small. The governing partial differential equation (PDE) for linear elasticity in its general dynamic form is the Navier-Cauchy equation: 2(1 + ŒΩ) 2u + EŒΩ (1 + ŒΩ)(1 2ŒΩ) + 2(1 + ŒΩ) (cid:18) (cid:19) ( u) + fbody = œÅphys u, (9) where u(x, t) is the displacement vector field (corresponding to u(x) in Eq. 6, where would be the number of spatial dimensions, e.g., = 2 or = 3), is its second derivative with respect to time (acceleration), fbody(x) is the body force vector field (e.g., gravitational loads, corresponding to (x) in Eq. 6 if is defined as the internal force operator), is the Youngs modulus (a measure of material stiffness), ŒΩ is the Poissons ratio (characterizing transverse contraction/expansion), and œÅphys is the physical mass density of the material. For static or quasi-static analyses, which are typical in minimum compliance problems, we are interested in the steady-state deformation under load. This allows us to set the inertial term œÅphys to zero. The governing PDE for static linear elasticity then becomes: 2(1 + ŒΩ) (cid:18) 2u + EŒΩ (1 + ŒΩ)(1 2ŒΩ) + 2(1 + ŒΩ) (cid:19) ( u) + fbody = 0. (10) This equation, along with appropriate boundary conditions (such as prescribed displacements g(x) on as in Eq. 6), defines the structural response. In the context of TO, the Youngs modulus is not uniform throughout the domain ‚Ñ¶; instead, it varies spatially depending on the material density distribution œÅ(x), which is the design variable. A.1.2 Finite Element Discretization and Compliance Calculation As discussed in the general TO framework (leading to Eq. 8), analytical solutions to Eq. 10 are generally intractable for complex domains and material distributions. Therefore, the Finite Element Method (FEM) is employed. The continuous domain ‚Ñ¶ is discretized into mesh Th of elements, and the continuous displacement field u(x) is approximated by vector of discrete nodal displacements uh RM . The body forces fbody(x) and any applied surface tractions are discretized into global force vector fh RM . Dirichlet boundary conditions (supports) are enforced by prescribing values for certain components of uh. The FEM discretization of Eq. 10 results in system of linear algebraic equations: K(œÅh)uh = fh, (11) where K(œÅh) RM is the global stiffness matrix. This matrix depends on the vector of element design variables (densities) œÅh = [œÅ1, . . . , œÅN ], as these densities determine the material properties (specifically, Youngs modulus) within each element. The compliance of the structure is measure of its overall flexibility; minimizing compliance is equivalent to maximizing stiffness. For the discrete system, compliance is calculated as the work done by the external forces: Using the equilibrium condition fh = K(œÅh)uh, compliance can also be written as = uT K(œÅh)uh. C(uh, fh) = h uh. (12) A.1.3 Material Interpolation: The SIMP Model To enable optimization, the material properties of each element must be related to the design variables œÅe. The new papers œÅe [0, 1] are the design variables for each element e. common approach is the Solid Isotropic Material with Penalization (SIMP) method. In this scheme, the Youngs modulus Ee of an element is interpolated based on its normalized density œÅe: Ee(œÅe) = Emin + œÅp (13) where Esolid is the Youngs modulus of the solid material, Emin is small positive Youngs modulus assigned to void regions (e.g., œÅe 0) to prevent the stiffness matrix K(œÅh) from becoming singular, and is the penalization factor, typically 3. This penalization makes intermediate densities (e.g., œÅe = 0.5) structurally inefficient, meaning they contribute less to stiffness than their cost in material. This encourages the optimization process to yield designs with densities close to 0 (void) or 1 (solid). The element stiffness matrices Ke are computed using Ee(œÅe) and then e(Esolid Emin), 17 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization assembled into the global stiffness matrix K(œÅh). Note that the choice of Emin is purely to avoid singular values and is selected to be 109 in our data generation compared to the much larger value of Esolid = 1.0. This choice is informed by prior literature recommending such value as good balance between stable optimization and accuracy of simulations [50]. Since in our implementation we set Emin = 109, the choice of œÅmin = 0 is used, although numerically any design element in mesh which receives zero density will effectively have Youngs modulus of 109 which given our choice Esolid = 1 would be equivalent to setting Emin = 0 and œÅmin = 109. A.1.4 The Minimum Compliance Optimization Problem The goal of minimum compliance topology optimization is to find the distribution of material densities œÅh that minimizes the compliance (Eq. 12), subject to the static equilibrium constraint (Eq. 11) and constraint on the total volume of material used. This is specific instance of the general discretized TO problem (Eq. 8). The formulation is: minimize œÅh C(uh, fh) = subject to: K(œÅh)uh = fh (cid:88) uh veœÅe Vmax (Static Equilibrium) (Volume Constraint) (14) e=1 œÅmin œÅe 1 for = 1, . . . , (Density Bounds), where ve is the volume of element e, Vmax is the maximum permissible total volume of the material (as defined in Eq. 7 and Eq. 8), and œÅmin is small positive lower bound for the element densities (e.g., 109 or 106) to ensure Ee is always positive if Emin = 0 is chosen, or to represent minimum manufacturable material thickness. This formulation is widely used in structural optimization. The aim of approaches like Neural Network based Inverse Topology Optimization (NITO), as alluded to in your previous work, would be to predict the optimal densities œÅh directly, given the problem definition (loads fh, boundary conditions represented in and fh, and the target volume fraction Vmax/ (cid:80) ve). 18 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 4: Top: Results of ablation studies on denoising steps. Observe that despite CE being lower in DDPM sampling, this is largely due to DDPM sampling using significantly more material, as evidenced by the much higher VFE values. Bottom: Ablation on the guidance scale shows that CE largely stays the same for the guidance scale of 1 and 2, while VFE is optimal at guidance scale of 2.0."
        },
        {
            "title": "B Ablation Studies",
            "content": "To determine the appropriate denoising steps and guidance scale, and the denoising strategy, we perform ablation studies by sampling the model with different parameters and testing the performance on the OpenTO benchmark mentioned in the experiments. Figure 4, shows the results of our ablation study on sampling parameters. To study the effects of denoising steps, we run both DDPM and DDIM denoising with guidance scale of 2.0 with 5, 10, 20, and 40 denoising steps. We observe that DDIM sampling largely achieves better performance despite initially having larger absolute VFE. Thus, we determine DDIM sampling to be more effective, especially for more efficient lower denoising step counts, crucial for computational efficiency. From here, we can clearly see in Figure 4 that DDIM sampling strikes the perfect balance of CE and VFE at 20 denoising steps, and any more comes with no benefits and, in fact, slightly worse performance. Finally, to determine the optimal guidance scale, we run DDIM sampling for guidance scales of 1, 2, 4, 6, 8, and 10 using the 20 denoising steps we found to be most effective. As seen in Figure 4, guidance scale of 2 strikes good balance between VFE and CE, which informs our final choice of DDIM denoising with 20 steps and guidance scale of 2. 19 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization"
        },
        {
            "title": "C Out of Distribution Reconstruction",
            "content": "To further demonstrate potential downline adaptations of OAT as foundational framework, we run reconstruction using our auto-encoder on completely out-of-distribution data, involving optimized topologies for heat sinks introduced by Bernard et al. [4]. This involves entirely different physics and optimization in comparison to the data we train OAT on. Despite this, we see an Intersection over Union (IoU) of 0.94 on these samples. The reconstruction quality is visualized in Figure 5. These results show how OAT, as pre-trained foundation model, can easily be extended to different TO problems involving different physics and topologies, even without retraining the entire framework. Figure 5: Heatsink optimized topologies reconstructed using OATs auto-encoder without any additional training. This demonstrates OATs zero-shot capability to extend to different physics and TO problems with relative ease. 20 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization"
        },
        {
            "title": "D OpenTO Dataset",
            "content": "Here we describe the details of the OpenTO dataset and the data generation specifics, and at the end, provide visual examples of the topologies and boundary conditions in the problem. The generation of each topology optimization problem within the OpenTO dataset follows structured, randomized pipeline designed to produce diverse set of problems. This diversity manifests in the domain discretization (resolution), aspect ratio, and the configuration of applied loads and boundary conditions. The overarching aim is to construct comprehensive dataset suitable for foundation model training for TO. D.1 Domain Definition and Discretization The first step involves defining the physical domain of the topology optimization problem. This is characterized by its resolution (total number of elements) and its aspect ratio. 1. Target Element Count (EC): The total number of discrete elements in the design domain is sampled from uniform distribution. Let EC be the target element count, then EC U[212, 214] This range ensures variety of problem sizes, from 4096 to 16384 elements. 2. Aspect Ratio (AR): The aspect ratio of the design domain is sampled from standard log-normal distribution. Let AR be the aspect ratio, then AR LogNormal(0, 1) where the parameters (0, 1) represent the mean and standard deviation of the natural logarithm of the variable, respectively. This choice allows for wide range of domain shapes, biased towards aspect ratios closer to unity but permitting more elongated or flattened domains. The bias towards square domains is mainly because these problems are more common and physically more stable, yielding fewer failed simulations in the optimization process. 3. Problem Dimensions (Nx, Ny): Given the target element count EC and the sampled aspect ratio AR, the dimensions of the problem domain, Nx (number of elements in the x-direction) and Ny (number of elements in the y-direction), are determined. Assuming AR = Nx/Ny, we have Nx = AR Ny. Since the total number of elements is EC Nx Ny, we can substitute to get EC (AR Ny) Ny = AR 2 . Thus, Ny (cid:114) EC AR Nx AR Ny The values for Nx and Ny are then rounded to the nearest integers, ensuring that their product Nx Ny is close to the target EC. Furthermore, if this leads to an element count outside the intended distribution, one side is randomly adjusted to ensure this does not occur. D.2 Load and Constraint Specification Once the domain is defined, loads and boundary constraints are applied. This involves determining the number of loads and constraints, their types, and their specific locations and orientations. Unlike prior datasets, which hand-select finite number of boundary conditions and apply single load on the boundary, OpenTO samples boundary conditions randomly and applies multiple random loads of different kinds. Furthermore, unlike prior works, which limit forces and boundary conditions to the edges of the domain, we sample internal boundary conditions and loads for full generality. OpenTO also includes loads, which we refer to as distributed loads, which are typically characterized formally as Neumann boundary conditions when loads are result of stress/pressure applied at the boundaries, and body forces when an internal load is applied to part of the domain interior (often seen in electromagnetic forces or gravitational body forces). Below are details of how loads and boundary conditions are sampled in OpenTO: 1. Number of Loads (N L): The number of applied loads is sampled from geometric distribution with parameter = 0.3, shifted by +1 to ensure at least one load. Let be the number of loads, then Geom(0.3) + 21 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization 2. Number of Constraints (N C): The number of boundary constraints (fixed degrees of freedom) is sampled from geometric distribution with parameter = 0.2, shifted by +2 to ensure at least two constraints. Let be the number of constraints, then Geom(0.2) + 2 This ensures baseline level of constraint necessary for valid mechanical problem. If fewer constraints are present, the FEA problem (Appendix A) will become singular and thus not solvable. 3. Load and Constraint Types and Placement: For each of the loads and constraints, type is selected from predefined discrete probability distribution. The available types and their selection probabilities are: Internal point force (single force in the interior of the domain): 50% Edge point (a point on one of the four edges, not corner): 10% Corner point (one of the four corners): 10% Distributed load/constraint (equal magnitude applied across many nodes of line segment on one edge) on partial edge: 10% Distributed load/constraint on full edge: 10% Internally distributed load/constraint (distributed on line or area of random ellipse): 10% The specific placement of load or constraint is then determined randomly based on the selected type. For example: Internal point: random coordinate (x, y) within the domain interior. Edge point: An edge (top, bottom, left, or right) is chosen uniformly at random, and then random position along that edge is selected. Corner point: One of the four domain corners ((0, 0), (Nx, 0), (0, Ny), (Nx, Ny) in elemental coordinates, or corresponding nodal coordinates) is chosen uniformly at random. Distributed on partial edge: An edge is chosen, and then random sub-segment of that edge is selected. Distributed on full edge: An edge is chosen. Internally distributed: region (e.g., rectangular or circular patch or line) is randomly defined within the interior of the domain. The specific shapes and parameters for these internal distributions are further randomized. For comprehensive details on the implementation of these variations, readers are directed to the projects codebase. Loads are typically defined by vector (e.g., magnitude and direction), which can also be randomized. 4. Constraint Direction: Each of the constraints is assigned direction of application. The degrees of freedom (DOFs) to be constrained are chosen based on the following probabilities: Constraint in the lateral (e.g., x-direction) direction only: 30% Constraint in the vertical (e.g., y-direction) direction only: 30% Constraint in both lateral and vertical directions: 40% D.3 Problem Validation and Iteration critical final step in the generation pipeline is the validation of the constructed problem. 1. Validity Check: The problem, now fully defined by its domain, loads, and constraints, is assessed to ensure it is: Fully constrained: The applied boundary conditions must be sufficient to prevent rigid body motion (translation and rotation) of the structure under the applied loads. This is typically checked by ensuring the global stiffness matrix is nonsingular. Not trivially solved: The problem should not be over-constrained to the point of having an obvious or degenerate solution. For instance, applying load and fixed constraint at the exact same degree of freedom might lead to issues or trivial (zero vector) forcing terms. 2. Regeneration on Failure: If the generated problem fails the validity check (e.g., due to coincident load and constraint application points, insufficient constraints, or other problematic configurations), the entire problem instance is discarded. The generation process, starting from the sampling of EC, is then re-initiated to produce new, statistically independent problem candidate. This iterative loop continues until valid problem instance is successfully generated. This rigorous, multi-stage randomized procedure ensures the creation of diverse and challenging set of topology optimization problems, forming the basis of the OpenTO dataset. 22 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization D.4 Algorithm for Data Generation Below is summary of the data generation process in algorithmic form. Algorithm 1 OpenTO Problem Instance Generation // Step 1: Domain Definition and Discretization Sample target element count EC U[212, 214] Sample aspect ratio AR LogNormal(0, 1) Calculate Ny,f loat (cid:112)EC/AR Calculate Nx,f loat AR Ny,f loat Set Ny round(Ny,f loat) Set Nx round(Nx,f loat) if Nx < 1 or Ny < 1 then 1: Initialize: ProblemIsValid false 2: while ProblemIsValid is false do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end if // Step 2: Load and Constraint Specification Sample number of loads Geom(0.3) + 1 Sample number of constraints Geom(0.2) + 2 Initialize empty lists: LoadsList, ConstraintsList for = 1 to do continue // Restart generation if dimensions are invalid Sample load type Ltype from {InternalPt (0.5), EdgePt (0.1), CornerPt (0.1), PartialEdge (0.1), FullEdge (0.1), InternalDist (0.1)} Determine load placement Lplace based on Ltype and domain (Nx, Ny) (randomized selection) Determine load magnitude and direction Lvec (randomized) Add (Ltype, Lplace, Lvec) to LoadsList 19: 20: 21: 22: 23: 24: end for for = 1 to do Sample constraint type Ctype from {InternalPt (0.5), EdgePt (0.1), CornerPt (0.1), PartialEdge (0.1), FullEdge (0.1), InternalDist (0.1)} Determine constraint placement Cplace based on Ctype and domain (Nx, Ny) (randomized selection) Sample constraint direction Cdir from {Lateral (0.3), Vertical (0.3), Both (0.4)} Add (Ctype, Cplace, Cdir) to ConstraintsList end for // Step 3: Problem Validation Perform check for sufficient constraints (prevent rigid body motion). Perform check for non-trivial solution (e.g., avoid coincident loads/constraints at same DOF if problematic). if problem is fully constrained AND not trivially solved then 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: end while 38: Output: Generated problem instance (Domain: Nx, Ny; Loads: LoadsList; Constraints: ConstraintsList) ProblemIsValid false // Problem discarded, loop will reiterate ProblemIsValid true end if else 23 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization"
        },
        {
            "title": "E Further Visualization of Results",
            "content": "Here we visualize more samples from OAT and NITO on the fully general OpenTO benchmark. The figures below clearly highlight how NITO fails to generate high-quality samples and generates largely invalid, blurry topologies that are not mostly not binary as ideally would be in TO. NOTE: Samples with CE higher than 100% are considered failed and do not enter CE computation in the reported results. Figure 6: Samples generated on the OpenTO benchmark by NITO and OAT. Compliance error for each generated sample is shown underneath. 24 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 7: Samples generated on the OpenTO benchmark by NITO and OAT. Compliance error for each generated sample is shown underneath. Figure 8: Samples generated on the OpenTO benchmark by NITO and OAT. Compliance error for each generated sample is shown underneath. 25 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 9: Samples generated on the OpenTO benchmark by NITO and OAT. Compliance error for each generated sample is shown underneath. Figure 10: Samples generated on the OpenTO benchmark by NITO and OAT. Compliance error for each generated sample is shown underneath. 26 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 11: Samples generated on the OpenTO benchmark by NITO and OAT. Compliance error for each generated sample is shown underneath. Figure 12: Samples generated on the OpenTO benchmark by NITO and OAT. Compliance error for each generated sample is shown underneath. 27 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 13: Samples generated on the OpenTO benchmark by NITO and OAT. Compliance error for each generated sample is shown underneath. Figure 14: Samples generated on the OpenTO benchmark by NITO and OAT. Compliance error for each generated sample is shown underneath. 28 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 15: Samples generated on the OpenTO benchmark by NITO and OAT. Compliance error for each generated sample is shown underneath. 29 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization"
        },
        {
            "title": "F Implementation Details",
            "content": "In this section, we describe the full details of the training and architecture of OAT. First, we discuss the autoencoder training, then we will go over the LDM training. F.1 Autoencoder Architecture The autoencoder comprises an encoder, decoder, and neural field renderer. Encoder The encoder maps an input topology to latent representation = E(T ). Input Processing: Topologies are padded and resized to fixed 256 256 resolution with 1 input channel. Core Structure: Convolutional Neural Network (CNN) with an initial convolution is followed by three downsampling levels, each employing ResNet-style blocks. Middle Section: Features are further processed by middle section containing additional ResNet-style blocks and an attention mechanism. Output: Final convolutional layers produce 1-channel latent tensor of size 64 64. Decoder The decoder reconstructs feature tensor œï = D(z) from the latent z. Input: The latent tensor is processed by initial convolutional layers. Core Structure: Symmetrical to the encoder, it features middle section with ResNet-style blocks and an attention mechanism, followed by three upsampling levels composed of ResNet-style blocks. Output: fixed-resolution feature tensor (256 256) œï with 128 channels. Neural Field Renderer The renderer reconstructs the topology = R(œï, c, s) from œï, pixel coordinates c, and cell sizes s. Architecture: convolutional renderer, inspired by Convolutional Local Image Functions (CLIF), is employed. It processes the decoded features, coordinates, and cell sizes, with optional positional encoding for the latter two. Network: The renderer consists of sequence of convolutional and ResNet-style blocks, terminating in an output convolution with Tanh activation function. Scalability: Training utilizes patch sampling. High-resolution inference employs stenciled, overlapping tiles. Full details of the architecture can be found in our publicly available code. F.2 Autoencoder Training The autoencoder is trained on the OpenTO dataset (2.2 million pre-training samples). Dataset and Preprocessing: Input topologies are resized to 256 256 for the encoder. The renderer is trained on 64 64 patches sampled from topologies, along with their coordinates and cell sizes. Training can also be performed on full images (full grid of coordinates and cell sizes for any given topology). Batch Size and Epochs: Batch size: 128. Training epochs: 50. Distributed Training and Precision: Training utilizes Distributed Data Parallel (DDP) on 4 H100 GPUs. Automatic Pytorch mixed precision training is employed. Optimizer and Learning Rate Schedule: AdamW optimizer is used for training. Cosine schedule is used for learning rate with 200 steps of warmup, linearly increasing learning rate from 0 to 104, then gradually reducing it to 105 during training. 30 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization F.3 Latent Diffusion Architecture The latent diffusion model operates on the latent encodings of the autoencoder, which have 64 64 resolution with 1 channel. The architecture of the model follows UNet architecture, which can be seen in full in our publicly available code. For conditioning of the LDM model, we use problem encoder architecture similar to what we described in the main body of the paper: Conditioning Mechanism Time Embedding: Standard timestep embedding is used to encode the diffusion timestep t. Problem Embedding (P ): The TO problem definition ÀÜP (boundary conditions, forces, volume fraction, aspect ratio, cell size) is encoded into fixed-size problem embedding using dedicated ProblemEncoder. Boundary conditions (Sboundary) and forces (Sf orce), represented as point clouds, are processed by separate BPOM modules (MLPs with mean/max/min pooling of point features concatenated). Scalar and low-dimensional conditions (volume fraction , cell size s, aspect ratio a) are processed by individual MLPs. The concatenated embeddings are passed through final MLP to produce . Combined Embedding: The problem embedding is projected to the same dimension as the time embedding and added to it. This combined embedding conditions the ResNet and Attention blocks within the U-Net. For full details of the architecture, please refer to the code we provide. F.4 latent Diffusion Training The LDM is trained on latent codes obtained from the pre-trained autoencoder, paired with their corresponding TO problem specifications ÀÜP . Dataset: Consists of latent tensors and their associated problem definitions (forces, boundary conditions, volume fraction, etc.). The dataset loader handles stochastic dropping of conditions for classifier-free guidance. Diffusion Process: DDPM noise schedule is used with velocity target, and cosine noise schedule for training. For inference, DDIM noise scheduler is employed for faster sampling. Training Objective: The model is trained to predict the velocity of the diffusion process, conditioned on the problem embedding . The objective is to minimize the mean squared error between the true and predicted velocity, as described in Equation 6 of the main paper: where = Œ±tœµ 1 Œ±tz. LLDM = EzE(T ),t,œµN (0,I)[v vŒ∏(zt, tP )2], Classifier-Free Guidance: Enabled by randomly setting the problem conditioning to null embedding during training with specified probability, specifically, we hide all of 50% of the time and hide boundary conditions and forces each with probability of 25%, separate from the full 59% hiding. Optimizer and Learning Rate: AdamW optimizer. Initial learning rate: 104, with 200 warmup steps same as for the autoencoder. cosine learning rate scheduler anneals the learning rate to final value of 105. Batch Size and Epochs: Batch size: 64. Training epochs: 50. Distributed Training and Precision: Training utilizes Distributed Data Parallel (DDP) on 2 H100 GPUs. Mixed precision training is employed. 31 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization F.5 Computational Resources All experiments and training were conducted using 4 H100 GPUs. Autoencoder training takes 2 days to complete on our implementation, and the diffusion model takes 4 days to train. We had multiple training runs, but we do not have an exact estimate of the time needed. Inference time is indicated for the RTX 4090 GPU in our model, which was used for inference experiments. F.6 Dataset Visualization Here we provide few examples of the OpenTO dataset we generate. We visualize forces and boundary conditions, and it can be seen that OpenTO contains samples that include non-repeating complex problem definitions with internal, edge, distributed, and point boundary conditions and forces. 32 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 16: Samples from OpenTO Dataset. 33 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 17: Samples from OpenTO Dataset. 34 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 18: Samples from OpenTO Dataset. 35 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 19: Samples from OpenTO Dataset. 36 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 20: Samples from OpenTO Dataset. 37 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 21: Samples from OpenTO Dataset. 38 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 22: Samples from OpenTO Dataset. 39 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 23: Samples from OpenTO Dataset. 40 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 24: Samples from OpenTO Dataset. 41 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization Figure 25: Samples from OpenTO Dataset. 42 Optimize Any Topology: Foundation Model for Shapeand Resolution-Free Structural Topology Optimization F.7 Combining Prior Data With OpenTO We generate 2M samples as described above and provide labels (configuration information) for 700,000 samples, and provide 10,000 test samples based on the same procedural generation process. Finally, it is important to note that we also merge the 194,000 data points used in prior work [34] into our dataset, expanding it to 2.2M total samples. Break-Even Analysis Ctrain CSIMPCinference Critics cite the break-even point [62], œÑ = (where is computational cost), which for OAT-like models is dominated by data generation. Our data generation cost (168 H100-days) vastly exceeded our final training run (16 H100-days). Ignoring minor preliminary development runs, the data-generation-dominated calculation yields break-even of œÑ 2.32 million uses. This cost seems hard to justify; however, we argue that this metric does not capture the full picture of deep learnings computational efficacy. This is because it ignores the hardware efficiency of parallel data generation and, more importantly, fails to capture the value of democratizing fast TO for users without large compute, enabling greater design exploration and more experiments. This metric points to over 2 million uses to justify total computational run time of less than one month. Although highlighting high development costs, we believe this argument justifies the large break-even point. Code & Data The code and data for OAT can be found at: https://github.com/ahnobari/OptimizeAnyTopology."
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology, Cambridge, MA, 02139",
        "Red Hat AI, MIT-IBM Watson AI Lab, Cambridge, MA, 02139"
    ]
}