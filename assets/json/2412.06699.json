{
    "paper_title": "You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale",
    "authors": [
        "Baorui Ma",
        "Huachen Gao",
        "Haoge Deng",
        "Zhengxiong Luo",
        "Tiejun Huang",
        "Lulu Tang",
        "Xinlong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent 3D generation models typically rely on limited-scale 3D `gold-labels' or 2D diffusion priors for 3D content creation. However, their performance is upper-bounded by constrained 3D priors due to the lack of scalable learning paradigms. In this work, we present See3D, a visual-conditional multi-view diffusion model trained on large-scale Internet videos for open-world 3D creation. The model aims to Get 3D knowledge by solely Seeing the visual contents from the vast and rapidly growing video data -- You See it, You Got it. To achieve this, we first scale up the training data using a proposed data curation pipeline that automatically filters out multi-view inconsistencies and insufficient observations from source videos. This results in a high-quality, richly diverse, large-scale dataset of multi-view images, termed WebVi3D, containing 320M frames from 16M video clips. Nevertheless, learning generic 3D priors from videos without explicit 3D geometry or camera pose annotations is nontrivial, and annotating poses for web-scale videos is prohibitively expensive. To eliminate the need for pose conditions, we introduce an innovative visual-condition - a purely 2D-inductive visual signal generated by adding time-dependent noise to the masked video data. Finally, we introduce a novel visual-conditional 3D generation framework by integrating See3D into a warping-based pipeline for high-fidelity 3D generation. Our numerical and visual comparisons on single and sparse reconstruction benchmarks show that See3D, trained on cost-effective and scalable video data, achieves notable zero-shot and open-world generation capabilities, markedly outperforming models trained on costly and constrained 3D datasets. Please refer to our project page at: https://vision.baai.ac.cn/see3d"
        },
        {
            "title": "Start",
            "content": "You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale Baorui Ma*, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, Xinlong Wang Beijing Academy of Artificial Intelligence (BAAI) 4 2 0 2 9 ] . [ 1 9 9 6 6 0 . 2 1 4 2 : r Figure 1. Benefiting from the proposed web-scale dataset WebVi3D, See3D enables both objectand scene-level 3D creation, including sparse-view-to-3D, (text-) image-to-3D, and 3D editing. It can also be used for Gaussian Splatting to extract meshes or render images."
        },
        {
            "title": "Abstract",
            "content": "Recent 3D generation models typically rely on limitedscale 3D gold-labels or 2D diffusion priors for 3D content creation. However, their performance is upper-bounded by constrained 3D priors due to the lack of scalable learning paradigms. In this work, we present See3D, visualconditional multi-view diffusion model trained on largescale Internet videos for open-world 3D creation. The model aims to Get 3D knowledge by solely Seeing the visual contents from the vast and rapidly growing video data You See it, You Got it. To achieve this, we first scale up the training data using proposed data curation pipeline that automatically filters out multi-view inconsistencies and insufficient observations from source videos. This results in high-quality, richly diverse, large-scale dataset of multi- *Equal contribution. Correspondence to XW and LT. view images, termed WebVi3D, containing 320M frames from 16M video clips. Nevertheless, learning generic 3D priors from videos without explicit 3D geometry or camera pose annotations is nontrivial, and annotating poses for web-scale videos is prohibitively expensive. To eliminate the need for pose conditions, we introduce an innovative visual-condition - purely 2D-inductive visual signal generated by adding time-dependent noise to the masked video data. Finally, we introduce novel visual-conditional 3D generation framework by integrating See3D into warpingbased pipeline for high-fidelity 3D generation. Our numerical and visual comparisons on single and sparse reconstruction benchmarks show that See3D, trained on costeffective and scalable video data, achieves notable zeroshot and open-world generation capabilities, markedly outperforming models trained on costly and constrained 3D datasets. Additionally, our model naturally supports other 1 image-conditioned 3D creation tasks, such as 3D editing, without further fine-tuning. Please refer to our project page at: https://vision.baai.ac.cn/see3d. 1. Introduction Recent advances in 3D generation are essential for fields like virtual reality, entertainment, and simulation, offering the potential not only to recreate intricate real-world structures but also to expand human imagination. Nevertheless, developing these models is constrained by the scarcity and high costs of accessible 3D datasets. Despite recent industry efforts [86, 108, 115] create extensive proprietary 3D assets, these initiatives come with substantial financial and operational burdens. Currently, building such large-scale 3D dataset for academia remains prohibitively expensive. This motivates us to pursue scalable, accessible, and affordable data source that can compete with advanced closed-source solutions, thereby enabling the broader research community to train high-performance 3D generation models. Human perception of the 3D world does not rely on specific 3D representation (e.g., point clouds[17], voxel grids [35], meshes [90], or neural fields [59]) or precise camera conditions. Instead, our 3D awareness is shaped by multiview observations accumulated throughout our lives. This raises the question: Can models similarly learn universal 3D priors from large collections of multi-view images? Fortunately, Internet videos offer rich source of multi-view images, captured from various locations with diverse sensors and complex camera trajectories, providing scalable, accessible, and cost-effective data source. Thus, how can we effectively learn 3D knowledge from Internet videos? The core challenges in achieving this goal are twofold: 1) filtering relevant, 3D-aware video data from raw sources, specifically static scenes with varied camera viewpoints that provide sufficient multi-view observations; and 2) learning generic 3D priors from videos lacking explicit 3D geometry and camera pose annotations (i.e. pose-free videos). This work carefully addresses these challenges and introduces pose-free, visual-conditional multi-view diffusion (MVD) model, See3D, for open-world 3D creation. Specifically, we establish novel video data curation pipeline that automatically filters out data with dynamic content or restricted camera viewpoints from source videos. The resulting dataset, termed WebVi3D, comprises 15.99M video clips from 25.48M source videos, totaling 4.41 years in durationorders of magnitude larger than previous 3D datasets, such as DLV3D (0.01M) [45], RealEstate10K (0.08M) [117], MVImgNet (0.22M) [113] and Objaverse (0.8M) [14]. MVD models have recently gained widespread attention due to their advantages of integrating the generative capabilities of 2D diffusion models while maintaining consistency across multiple views [50, 74]. Typically, these models rely on precise camera poses [21, 30, 48, 60, 74] or warped images rendered according to camera position [87, 112] as conditional inputs to guide 3D-consistent view generation. We refer to these conditions, derived from pose or 3D annotations, as 3D-inductive conditions. However, annotating web-scale videos is prohibitively costly, or even intractable in some cases, posing significant challenges for scaling. To address this, we propose novel, pose-free visual-condition derived from pixel-space hints within videos. It is purely 2D-inductive visual signal, created by introducing timedependent noise to masked input videos, free from any 3Dinductive bias. This enables training MVD model at scale, without requiring pose annotations. Intuitively, the proposed visual-condition can generalize effectively to tasks that rely on pixel-space hints distinct from those in videos, such as warping-based 3D generation [11, 75] and mask-based 3D editing [9], without requiring additional training, see Fig. 1. For instance, in warpingbased 3D generation, pixels from reference image are rearranged through warping operations, creating specific visual-condition to indicate camera movement. However, these warped images often exhibit artifacts or distortions, causing significant domain gap compared to video frames. Whereas, our visual-condition functions as generic one, capable of accommodating such unnatural images. To further harness the potential of See3D, we introduce an innovative visual-conditional 3D generation framework utilizing warping-based pipeline. This framework first constructs the visual-condition using See3D, then iteratively refines the geometry of novel views to build comprehensive scene observations. Finally, the generated images are used for Gaussian Splatting reconstruction [31, 37], which can be rendered from arbitrary viewpoints or converted into meshes through post-processing [53]. In summary, our key contributions are as follows. We present See3D, scalable visual-conditional MVD model for open-world 3D creation, which can be trained on web-scale video collections without pose annotations. We curate WebVi3D, multi-view images dataset containing static scenes with sufficient multi-view observations, and establish an automated pipeline for video data curation to train the MVD model. We introduce novel warping-based 3D generation framework with See3D, which supports long-sequence generation with complex camera trajectories. We achieve state-of-the-art results in single and sparse views reconstruction, demonstrating remarkable zeroshot and open-world generation capability, offering novel perspective on scalable 3D generation. Figure 2. Overview of See3D. (a) We propose four-step data curation pipeline to select multi-view images from Internet videos, forming the WebVi3D dataset, which includes 16M video clips across diverse categories and concepts. (b) Given multiple views, we corrupt the original data into corrupted images ci at timestep by applying random masks and time-dependent noise. We then reweight the guidance of ci through time-dependent mixture. (c) MVD model is capable of training at scale to generate multi-view images conditioned on vi is task-agnostic visual signal formed through time-dependent noise and mixture, it enables the trained model to robustly adapt to various downstream tasks. for the diffusion model to form visual-condition vi t, without requiring pose annotations. Since vi and the noisy latent xi 2. Related work Lifting 2D Generation into 3D. Recent advances in 3D generation have been largely driven by the success of 2D diffusion models [28, 70, 77, 78], which have revolutionized image and video generation. These works typically optimize 3D representations by maximizing the likelihood evaluated by 2D diffusion priors [38, 43, 47, 57, 66, 80, 82, 95, 109]. An alternative approach uses warping-inpainting pipeline, integrating an offline depth estimator with 2D diffusion-based inpainting model to iteratively generate 3D content [11, 16, 29, 60, 89, 110, 112]. However, 2D priors do not readily translate into coherent 3D representations. As result, 2D lifting-based approaches often struggle to preserve high geometric fidelity, leading to issues like multiview inconsistency and poor global geometry [111]. Directly Learning 3D Priors. To better preserve geometric features, some works focus on directly learning 3D priors. For instance, feed-forward approaches [7, 10, 23, 30, 41, 42, 49, 54, 72, 81, 83, 86, 92, 98, 106, 107, 119, 120] take single/few views as input and directly output 3D representations using an encoder-decoder architecture, eliminating the need for additional optimization process per instance. Another line of research involves training diffusion models to predict 3D representations, such as point clouds [61, 114], mesh [1, 34, 55], and implicit neural representation [8, 56, 100, 115]. However, these methods generally focus on object-level generation [14, 83, 101, 115, 120], limiting their applicability to scene-level generation. Although recent research has made strides in building scene-level 3D datasets [2, 12, 39, 45], their scale remains relatively limited. The reliance on costly, limited-scale 3D datasets restricts generalization to open-world or highly imaginative scenarios. In contrast, our approach curates large-scale, richly diverse dataset of multi-view images from Internet videos. By training the model at scale, it effectively supports both object-level and scene-level 3D creation. Learning Multi-view Priors for 3D Generation. MVD model inherits the generative capabilities of 2D diffusion models while capturing multi-view correlations, achieving both generalizability and 3D consistency. These merits have made it focal point in recent 3D generation research [21, 24, 46, 50, 52, 67, 71, 73, 74, 91, 112]. However, as 2D diffusion models are typically trained on 2D datasets, they lack precise control over image pose. To address this, MVD-based approaches often train their models on images paired with camera poses [22, 48, 71, 97, 99], where poses serve as essential conditional inputs, represented by camera extrinsics [71, 74], relative poses [48, 50, 73], or Plucker rays [21, 102]. Yet, pose-conditional models rely heavily on costly pose-annotated data, restricting training to smaller 3D datasets, thereby constraining their adaptability to outof-distribution scenarios. In contrast, we introduce novel visual-conditional approach that supports scalable, posefree MVD model training for open-world 3D generation. 3 3. Method The primary objective of this work is to build robust 3D generative model from the perspective of dataset scalingup. Previous works [14, 69, 87] laboriously collect 3D data from designed artists, stereo matching, or Structure from Motion (SfM), which can be costly and sometimes infeasible. In contrast, multi-view images offer highly scalable alternative, as they can be automatically extracted from the vast and rapidly growing Internet videos. By using multi-view prediction as pretext task, we demonstrate that learned 3D priors enable various 3D creation applications, including single view generation, sparse views reconstruction, and 3D editing in open-world scenarios. The following sections outline our approach  (Fig.2)  . Sec. 3.1 details the data curation pipeline, which selects static scenes with sufficient multi-view observations from raw video footage. Sec. 3.2 introduces our visualconditional multi-view diffusion model, which effectively learns general 3D priors from pose-free videos. Finally, Sec. 3.3 demonstrates new visual-conditional 3D generation framework utilizing warping-based pipeline. 3.1. Video Data Curation High-quality, large-scale video data rich in 3D knowledge is essential for learning accurate and reliable 3D priors. well-defined 3D-aware video clip should exhibit two key properties: 1) temporally static scene content and 2) significant viewpoint variation caused by the cameras egomotion. The first property ensures consistent 3D geometry across different viewpoints, while dynamic content can distort scene geometry and introduce biases that may degrade generation performance (Fig. 3a-Row1). The second property guarantees sufficient 3D observations from diverse viewpoints. When the model is trained on videos with limited viewpoint variation (Fig. 3a-Row2), it risks focusing on views adjacent to the reference view, rather than developing comprehensive 3D understanding. To obtain massive volume of 3D data, we collect approximately 25.48M open-sourced raw videos, totaling 44.98 years from the Internet, covering wide range of categories, such as landscapes, drones, animals, plants, games, and actions. Specifically, our dataset is sourced from four websites: Pexels [63], Artgrid [32], Airvuz [64], and Skypixel [88]. We follow Emu3 [94] to split the videos with PySceneDetect [6] to identify content changes and fadein/out events. Additionally, we remove clips with excessive text using PaddleOCR [84]. The detailed composition of our WebVi3D dataset is presented in Tab. 1. However, identifying 3D-aware videos presents nontrivial challenge. As most videos are derived from realworld footage, such videos often contains dynamic scenes or small camera movement. To address this, we propose Website Domain # Src. Vids Total Hrs. #Fil. Vids #Fil. Clips Fil. Hrs. Open Open Pexels Artgrid Airvuz Drone Shot Skypixel Landscape 6.18M 101.77K 0.61M 0.54M 92.49K 3.94M 5.10M 0.54M 94.75K 10.27M 105.47K 0.61M 2.65M 9.96K 1.10M 8.77K 5.87M 8.72K 6.37M 8.82K Total Open 25.48M 394.48K 2.30M 15.99M 36.27K Table 1. WebVi3D Dataset. Sourced from four open websites, we curate 2.30M videos, which are divided into 15.99M clips featuring temporally static scenes with large-range viewpoint. pipeline that automatically selects relevant, high-quality 3D-aware data (i.e., multi-view images) by leveraging priors from instance segmentation [26], optical flow [85], and pixel tracking [36]. This pipeline comprises four core steps: a) Temporal-Spatial Downsampling. To improve data filtering efficiency, we first downsample each video clip both temporally and spatially. The final resolution is set to 480p, and the temporal downsampling rate is set to 2. Note that this downsampling operation is applied only during data curation, not during model training. b) Semantic-Based Dynamic Recognition. We employ the instance segmentation model, Mask R-CNN [26], to generate motion masks for potential dynamic objects, such as humans, animals, and sports equipment. threshold is applied to filter out videos based on the proportion of frames containing these objects, as they are more likely associated with dynamic scenes. c) Non-Rigid Dynamic Filtering. To precisely filter out videos with dynamic regions, we use offline optical flow estimation [85] to obtain dense matching, which enables us to identify non-rigid motion masks in video frames. These masks are then analyzed based on their locations to further determine whether the video contains dynamic content. d) Tracking-Based Small Viewpoint Filtering. The previous three steps yield videos with static scenes. To further ensure these videos contain multi-view images captured from larger camera viewpoint, we track the motion trajectory of key points across frames and calculate the radius of the minimum outer tangent circle of the trajectory. Videos with small trajectory radius are then filtered out. More details about the data curation pipeline are provided in the Appendix B. Finally, we curate approximately 320M multi-view images from 15.99M video clips with static content and sufficient multi-view observations (see Fig.3b). To validate the effectiveness of our data acquisition method, we randomly select 10,000 video clips for human annotation, of which 8,859 were labeled as 3D-aware, representing 88.6% of the total. This indicates that our pipeline effectively identifies 3D-aware videos from massive source videos. As the volume of Internet videos continues to grow, this pipeline can continuously acquire more 3D-aware data, allowing for ongoing expansion of our dataset. ditional distribution, achieved by conditional diffusion model that minimizes: EX0,Y0,ϵ,t (cid:104) ϵθ(Xt, Y0, V, t) ϵ2 2 (cid:105) , (1) 0 (cid:9)N where Xt denotes the noisy latent. X0 = (cid:8)xi i=1 represents multi-view observation of 3D content, formed by sampling one clip from WebVi3D as described in Section 3.1, with = + being the number of frames in each clip. From X0, frames are randomly selected as reference views, noted as Y0 = (cid:8)yi (cid:9)S i=1, while the remaining frames are treated as target images, denoted = (cid:8)gi(cid:9)L i=1. Our approach focuses on constructing the visual-condition , which guides the diffusion model to generate plausible 3D content estimates from target viewpoints, ensuring consistency with the appearance of Y0. 0 desirable visualPrinciple of Visual-condition. condition should meet the following criteria: a) it can be constructed without the need for additional 3D annotations, b) it is independent of specific downstream tasks, and c) it offers sufficient generalization to support various taskspecific visual conditions, enabling precise control of camera movements. Ideally, this visual-condition can be derived from pixelspace hints within the original videos, implicitly guiding the model to learn camera control. Moreover, it should be robust enough to handle domain gaps between task-specific visual cues and pixels extracted from video data. For example, in warping-based generation, warped images often suffer from issues like self-occlusions, artifacts, and distortions, creating significant gap compared to real video data as shown in Fig.5 and Fig.6. Based on this, we propose constructing the visual-condition by applying masks and noise to the input video data. Our design principles are: Masking: Corrupting video data through random masking reduces reliance on direct pixel-space visual signals, helping the model partially mitigate the domain gap between task-specific visual cues and video data. Noise Addition: Adding noise to video data to approximate Gaussian distribution. When applied to different downstream tasks, task-specific visual inputs are similarly noised, aligning their distribution with Gaussian profile, and further bridging the gap between video data and task-specific visual input. Time-dependent Visual Condition. We initially add noise and random irregular masks to video data, creating what we call corrupted video data Ct, as defined in Eq.2. This corrupted data is then used as the visual-condition for MVD model training. key challenge of this strategy lies in determining the appropriate level of perturbation. If too much noise is added, the conditional signals become ineffective, Figure 3. (a-Row1): Dynamic content modifies scene geometry across views; (a-Row2): Limited camera movement provides insufficient multi-view observations; (b) Our WebVi3D comprises static scenes with diverse camera trajectories. 3.2. Visual Conditional Multi-View Diffusion Model Preliminary. Diffusion models [28, 77, 78] operate by perturbing the training data X0 q(X0) through forward diffusion process and learning to reverse it. The forward diffusion process Xt qt0(XtX0) can be formally represented by Xt = ϵ (0, I), where αt is variance schedule used in noise scheduler. In theory, Xt approximates an isotropic Gaussian distribution for sufficiently large timesteps t. The training objective is to learn the reverse process. 1 αtϵ, αtX0 + Objective. We aim for multi-view prediction: generating novel views along specified camera trajectories from single or sparse input while ensuring consistency with the input appearance. The MVD model inherits the generalizability of the 2D diffusion model while capturing cross-view consistency, which naturally aligns with our goal. Following this line, we present See3D, pose-free, visual-conditional MVD model trained on Internet videos to enable robust 3D generation, as shown in Fig.2. Challenge. The main technical challenge lies in learning precise camera control from pose-free videos. Previous works commonly incorporate camera parameters for both input and target views into diffusion models to guide multiview generation from specified viewpoints. However, training these models generally requires expensive 3D data with precise camera pose annotations, which limits scalability. To address this, we explore an alternative approach that conditions on 2D-inductive visual hints to implicitly control camera movement during training, thereby avoiding the need for hard-to-obtain camera trajectories. Formulation. Formally, we propose training the MVD model conditioned on 2D-inductive visual signals, referred to as visual-condition, without incorporating camera parameters. This task can be formulated as designing con5 resulting in poor visual quality and inaccurate camera control. Conversely, if insufficient noise is added, the corrupted data retains substantial details from the target images, causing the model to overly rely on visual hints from video data. To overcome this, we further introduce time-dependent noise distribution applied to video data. The core idea is to apply higher noise levels at larger time steps, effectively disrupting video data to prevent over-reliance on it. As the time step decreases, the noise level is reduced, providing cleaner conditional signals to facilitate content generation. However, as the noise level decreases, the risk of information leakage increases, causing the distribution to deviate from Gaussian distribution. To mitigate this, we propose gradually replacing corrupted data Ct with noisy latent Xt as the visual-condition (see Eq.3) , encouraging the model to primarily rely on Xt for generation while minimizing dependence on pixel-space signals from video data as timestep decreases. Additionally, Xt is predicted by the model, thus exhibiting task-agnostic property, as formalized by: Ct = αt(1 )X0 + 1 αtϵ, ϵ (0, I) (2) Vt = [Wt Ct + (1 Wt) Xt; ], (3) where = (cid:8)m0:S mS+1:N (cid:9), with m0:S as the zero matrix, keeping the reference images Y0 unmasked, and mS+1:N as random irregular masks applied to the target = (t), function is strictly monotoniimages G. cally increasing and requires < t, which guarantees that Ct contains at least as much information as Xt at earlier timesteps. Here, αt are variances used in DDIM [78], and (cid:9)N Vt = (cid:8)vi i=1 represents our final visual-condition, which is mixture of Ct and Xt, concatenated with masks along the channel dimension. The term Wt denotes balancing weight that decreases monotonically with timestep t, ranging from 1 to 0. In practice, an additional processing step assigns v0:S to the reference images Y0 directly, in order to inject the clean information of Y0 into the model, facilitating alignment between the predicted images and the reference images. Consequently, Eq.1 can be reformulated as EX0,Y0,ϵ,t . More detailed definitions of (t) and Wt are provided in the Appendix C.3. Model Architecture. Our model architecture is based on video diffusion model [5]. However, we removed the time embedding, as we aim for the model to control the camera movement purely through visual conditions, rather than inferring movement trends based on temporal cues. To further minimize the effect of temporality, we shuffle the frames in each video clip, treating the data as unordered X0. Specifically, we randomly select subset of frames from video clip as reference images, with the remaining frames as target images. The number of reference images is randomly selected to accommodate different downstream tasks. The ϵθ(Xt, Y0, Vt, t) ϵ2 2 (cid:105) (cid:104) multi-view diffusion model is optimized by calculating the loss only on the target images, as described in Eq.1. Additional details on the model architecture, including the design of self-attention layers, Zero-Initialize, trainable parameters, noise schedule, and cross-attention, are provided in the Appendix C.1. 3.3. Visual Conditional 3D Generation Overview. This section demonstrates the application of See3D for domain-free 3D generation, supporting longsequence novel view synthesis with complex camera trajectories. Starting with one or few input views, we iteratively generate warped images as visual hints, guided by predefined camera poses and estimated global depth [4]. See3D is then utilized to generate novel views along the predefined camera trajectory, conditioned on the proposed visualcondition. This iterative pipeline is illustrated in Fig.4, where the brown cameras represent the already generated views, and the gray cameras indicate the target views we aim to generate. Challenge. Recent warping-based 3D generation approaches [11, 20, 40] rely on monocular depth or point clouds, and perform global point-cloud alignment to recover the actual geometry for subsequent generations. However, as the reference view often provides limited scene observation, using offline methods tends to suffer from scale ambiguity and geometric estimation errors. Moreover, previous methods often overlook correcting these geometric errors, leading to distortions and stretching artifacts. These errors accumulate during iterative generation, severely degrading the generation quality. To address this, we propose an iterative strategy with sparse pixel-wise depth alignment, comprising two core steps: pixel-wise depth scale alignment and global metric depth recovery. Pixel-wise Depth Scale Alignment. We introduce pixelwise depth scale alignment using sparse keypoints. This approach performs high-degree-of-freedom independent optimization for all keypoints by leveraging multi-view matching priors from anchor views. Each keypoint independently identifies its multi-view correspondences, allowing for the recovery of both depth scale and surrounding geometry. The corrected scale is then propagated across the entire depth map using 2D distances between keypoints and their neighbors. Specifically, denote {Ti}N i=0 the predefined camera trajectory. Assuming we have generated images {Ii}n i=0, we now proceed to generate the next views using the warped image from the last anchor view In, referred to as the source view. We first utilize the pre-trained MoGe [93] to estimate the affine-invariant depth ˆDn of In. Inspired by [103], we perform sparse alignment with 1024 pairs of matching keypoints {mn, mi}k, obtained by the Figure 4. See3D for Multi-View Generation: From iteratively generated views (brown camera), we randomly select few anchor views (yellow stars) to guide the generation of target views along the gray camera trajectory. Keypoint matching is first performed to establish correspondences between the anchor views. Next, monocular depth estimation is applied to the latest anchor view, followed by our Iterative Sparse Pixel-Wise Depth Alignment to refine the depth and recover dense map. This dense depth is then used to warp images along the gray camera viewpoints. Subsequently, the warped images and anchor images are combined and processed according to Eq.2 and Eq.3, without random masking, forming the visual-condition, which guides MVD model to produce 3D-consistent target views. Finally, the gray camera turns to brown, guiding multi-view generation in the next iteration. pre-trianed extractor SuperPoint [15] and feature matcher LightGlue [44]. For each matched point, we optimize the corresponding scale αk and shift βk parameters, where [0, 1024], Our core idea is to recover the depth scaling by minimizing the L2 distance of re-projection between matching points. For each iteration, the warping operation Πni transforms pixels from the source images coordinate frame to the target images coordinate frame, formulated as: Πni( ˆdn) = ˆdnKiTiT 1 , where Ki, Kn, Ti, Tn represent the intrinsic and extrinsic parameters of the source and target frames, respectively. The alignment for each pair is performed using normalized coordinates, ensuring that the warping aligns with the matching prior: 1 αk, βk = argmin αk,βk ˆdk KiTiT 1 1 mt mt i2 2, (4) = αk ˆdk where the recovered depth of kth pixel is ˆdk + βk, the is the pixel-wise Hadamard Product. We minimize the matching loss via gradient descent to obtain best scale αk and shift parameters βk for each pixel. By performing individual scale recovery and geometry correction, we decouple the depth correlation among different points, achieving accurate single-view reconstruction. Global Metric Depth Recovery. After that, we set these recovered positions as sparse guidance ˆd n, and introduce Locally Weighted Linear Regression [103] (marked as LWLR in Fig.4) to recover the whole depth map based on the locations between guided points and the other target points. Denote (u, v) represent the 2D positions of the remaining target points, their depth ˆDn can be fitted to the sparse guided points by minimizing the squared locally weighted distance, which is reweighed by the diagonal weight matrix as: exp( 1 2π Wu,v = diag(w1, w2, ..., wm), wi = dist2 2b2 ), (5) where is the bandwidth of Gaussian kernel, and dist is the Euclidean distance between the guided point and the underestimated target point. Denote the homogeneous representation of ˆDn, the scale map Sscale and shift map Sshif of target points can be calculated by iterating every location on the whole image, which can be formulated as: ( ˆd Xβu,v)TWu,v( ˆd Xβu,v) + λS2 shif t, min βu,v ˆβu,v = (X TWu,vX + λ)1X TWu,v βu,v = [Sscale, Sshif t]T u,v, ˆd n, Dn = ˆd Sscale ˆDn + Sshif t, (6) where Dn is the scaled whole depth map, is the concatenation operator, λ is l2 regularization hyperparameter used for restricting the solution to be simple. Besides, the explicit constraint of the source frame with the target frames allows each novel view to maintain contextual consistency from preceding generations. 7 Novel View Generation. After obtaining the aligned depth Dn, we generate target visual hints through warping as ˆIj = Πnj(Dn). The warped images { ˆIj}n+m j=n contain unfilled regions, as indicated by the binary warping mask {Mj}n+m j=n , providing strong visual hints for See3D to perform novel view generation. To ensure strong multi-view consistency between the newly generated sequence and the previous content, we randomly select anchor views {Ik}, [1, ] from the earlier generated frames to guide subsequent generation. The generation process is formulated as: Ij = See3D( ˆIj, Mj, {I0, Ik}). We iteratively perform depth estimation, alignment, warping, and generation until all predefined multi-view images are obtained. 3D Reconstruction. We reconstruct the 3D scene using 3D Gaussian Splatting (3DGS) [37]. The training objective is to minimize the sum of photometric loss and SSIM loss, consistent with the original 3DGS approach. Additionally, we introduce perceptual loss (LPIPS [116]) to mitigate subtle inter-frame discrepancies in multi-view generated images during 3DGS reconstruction. LPIPS emphasizes higher-level semantic consistency between Gaussianrendered and generated multi-view images, rather than focusing on minor high-frequency differences. Furthermore, the potential inner-frame diversity may lead to inconsistencies with the corresponding camera poses. Following [18], we implement joint pose-Gaussian optimization, treating camera parameters as learnable variables alongside Gaussian attributes, thereby reducing gaps between generated viewpoints and their corresponding camera poses. 4. Experiments In Sec. 4.1 and Sec. 4.2, we present the single view and sparse views reconstruction with See3D as prior. Next, we conduct ablation experiments in Sec. 4.3 to validate the effectiveness of the proposed modules. Additional implementation details, more results on open-world 3D creation, and further ablation experiments are provided in the Appendix. 4.1. Single View to 3D Experimental Setting. See3D supports multi-view generation from single input view. Following prior work [112], our evaluation is conducted on the test split of three real-world datasets with various camera trajectories, including Tanks-and-Temples [39], RealEstate10K [117], CO3D [69]. We follow the approach in ViewCrafter [112] for constructing easy/hard evaluation sets based on different sampling rates applied to the original videos. We reimplement ViewCrafter using the official code released by [112] to validate our easy/hard set splitting, with results shown as ViewCrafter* in Tab. 2. We conduct comparisons with warping-based baselines, including LucidDreamer [11], camera-conditional video generation model MotionCtrl [96], warp-image conditional ViewCrafter [112], and multi-view diffusion model ZeroNVS [71]. We use the same point cloud rasterization as proposed in ViewCrafter [112] instead of depth-based warping to generate visual conditions for fair comparisons. Following [112], we evaluate only the visual quality of images generated by multiview diffusion without rendering novel views through 3D reconstruction. We report PSNR, SSIM, and LPIPS [116] as evaluation metrics. Among these, PSNR is traditional pixel-level metric that measures image similarity, which is significantly affected by viewpoint shifts. As such, PSNR reflects the accuracy of viewpoint control provided by our proposed visual-condition in multi-view generation. Results. The quantitative comparison results are presented in the top rows of Tab. 2. Only average metrics for the easy and hard sets are reported here, detailed values are available in the Appendix D.1. The results for ViewCrafter* are comparable to those reported in its original paper, confirming successful alignment between our method and the baselines. Numerically, our approach outperforms all baseline methods across all metrics. Specifically, compared to the re-implemented ViewCrafter, our approach achieves 4.63 dB improvement, demonstrating its capability to generate high-quality novel views. PSNR further demonstrates significant gains, indicating our proposed visual-condition enables precise camera control. Qualitative results are shown in the top rows of Fig. 5. See3D generates high-quality, realistic content within minutes. Dispite limited visual cues provided by the warped images, our method produces more reliable and realistic results with fewer artifacts. 4.2. Sparse Views to 3D Experimental Setting. We extend our model to the sparseview reconstruction task, evaluating it on three datasets: LLFF [58], DTU [33], and Mip-NeRF 360 [2]. We compare our method against several few-shot 3D reconstruction baselines, including optimization-based method MuRF [104], FSGS [118], and BGGS [25]; diffusion-based methods CAT3D [21], ZeroNVS (modified to handle multi-view input) [71], and ReconFusion [99]; as well as the feedforward method DepthSplat [105]. Following the evaluation protocols from [62, 99, 118], we use 3, 6, and 9 views as input. For few-shot reconstruction, dense multi-view images are generated from sparse views, similar to CAT3D [21], and 3DGS reconstruction is performed with pose optimization to render test views for evaluation. We report PSNR, SSIM, and LPIPS [116] to evaluate novel view synthesis performance. Results. Qualitative and quantitative results are presented in Tab. 2 and Fig. 5, respectively, with additional comparisons for 3, 6, and 9 input views available in Appendix D.2. The 3DGS model, trained on dense multi-view images gen8 Methods Tanks-and-Temples [39] RealEstate10K [117] CO3D [69] Single View PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS LucidDreamer [11] ZeroNVS [71] MotionCtrl [96] ViewCrafter [112] ViewCrafter* [112] Ours Sparse Views (3 Views) Zip-NeRF [3] MuRF [104] FSGS [118] BGGS [25] ZeroNVS [71] DepthSplat [105] ReconFusion [99] CAT3D [21] Ours 13.11 13.38 14.31 19.66 19.13 23.76 17.23 21.34 20.31 21.44 15.91 17.64 21.34 21.58 23.23 0.314 0.344 0.405 0.609 0.616 0.735 LLFF [58] 0.574 0.722 0.652 0.751 0.359 0.521 0.724 0.731 0.768 0.485 0.525 0.436 0.238 0.255 0.191 0.373 0.245 0.288 0.168 0.512 0.321 0.203 0.181 0.135 15.24 15.37 16.30 21.93 20.49 25.36 9.18 21.31 17.34 20.71 16.71 15.59 20.74 22.02 28.04 0.545 0.556 0.596 0.797 0.802 0. DTU [33] 0.601 0.885 0.818 0.862 0.716 0.525 0.875 0.844 0.884 0.357 0.397 0.363 0.161 0.183 0.146 0.383 0.127 0.169 0.111 0.223 0.373 0.124 0.121 0.073 13.90 14.23 16.16 20.17 19.07 24.28 12.77 - - - 14.44 13.85 15.50 16.62 17. 0.412 0.444 0.515 0.664 0.678 0.765 MipNeRF-360 [2] 0.271 - - - 0.316 0.254 0.358 0.377 0.442 0.473 0.495 0.418 0.283 0.339 0.251 0.705 - - - 0.680 0.621 0.585 0.515 0.422 Table 2. Quantitative Comparison of Single/Sparse Views Generation. The top rows are results given single view as input, where ViewCrafter indicates our re-implemented result. The bottom rows are novel view rendering quality given 3 views as input, where ZipNeRF and ZeroNVS are modified versions with sparse views input as reported in CAT3D. erated by See3D, surpassed state-of-the-art reconstruction models in novel view rendering. This indicates its ability to provide high-quality, consistent multi-view support for 3D reconstruction without imposing additional constraints. Compared to ReconFusion [99] and CAT3D [21], which also leverage diffusion priors for sparse-view reconstruction, our model exhibits effective scalability. Qualitative comparisons in Figure 5 reveal that NVS results produced by See3D exhibit fewer floating artifacts, suggesting its capability to generate more consistent and high-fidelity multiview images. 4.3. Ablation Study Scaling up Data. We investigate the impact of training data by ablating different proportions of our training dataset. The model is trained with 10%, 20%, 40%, 80%, and 100% of the training set, and its single-view generation performance is evaluated on RealEstate10K, achieving PSNR values of 19.32, 21.04, 22.57, 24.08, and 25.01, respectively. Additionally, training with unfiltered data results in generated content that often exhibits movement or deformation, leading to substantial performance drop with PSNR of 19.55. We analyze that this degradation likely stems from the lack of stationary and geometrically invariant properties in much of the source video content, which undermines multi-view consistency. In summary, these findings highlight the critical importance of data quality and diversity for effectively training large-scale MVD models. Visual-condition. Excluding the benefits of data scaling, we investigate the effectiveness of our visual-condition on pose-free data. Previous work [112] has demonstrated that warped images can serve as pivot condition to guide the model to generate the target viewpoint. However, due to the reliance on the annotated camera to control the projection and unprojection, warp-based conditions are inherently unscalable. Therefore, we compare the models ability to control cameras conditioned on pose-free visual-condition and conditioned on warped images. Specifically, we extract subset of MVImageNet [113] for training and testing. For each multi-view sequence in the training set, we select the point cloud of the first frame and render it into the subsequent 5 camera planes along the camera trajectory, based on the 3D annotations in the dataset. We obtain warped images and form pairs with the ground-truth multiviews to train an MVD model, referred to as MV-Posed. With the same experimental settings (training set, network architecture, batch size and predicted sequence length), we train an additional model without any 3D annotations, except for the modification of warp condition to the timedependent visual-condition Vt described in Sec.3.2, called MV-UnposeT. Meanwhile, we employ randomly masked multiple views as condition to train the model as an additional baseline, called MV-UnposeM. Model LPIPS PSNR SSIM MV-Posed MV-UnPoseM MV-UnposeT 0.182 0.443 0. 26.21 16.14 25.56 0.822 0.521 0.811 Table 3. Ablation Study on Visual-condition. The results are reported in Tab.3 and Fig.6, where the performance of MV-Posed and MV-UnposeT is comparable. In contrast, MV-UnposeM struggles to handle the gap between the warped image and masked images, in the case of geometric distortion and self-obscuration. These findings 9 Figure 5. Qualitative Comparison of Single/Sparse View Generation. The top three rows are results with single view input. The bottom two rows are novel view renderings from 3DGS, where Ours is trained on dense multi-view generation given 3 views as input. Our method outperformed other baselines in capturing high-frequency details, such as text and stairs. tion that includes: 1) new dataset, WebVi3D, curated via an automated pipeline, with the potential to evolve with the growing volume of Internet data. 2) new model, See3D, capable of scalable training without pose annotations, aligning with the concept of Get 3D by solely Seeing. 3) novel See3D-based 3D generation framework that supports long-sequence view generation with complex camera trajectories. We show that the 3D priors learned by See3D enable range of 3D creation applications, including single-view generation, sparse view reconstruction, and 3D editing in open-world scenarios. We believe See3D provides new direction to advancing the upper bound of 3D generation through dataset scaling. We hope our efforts will encourage the 3D research community to pay more attention to largescale unposed data, bypassing the costly 3D data barrier and chasing parity with powerful closed-source 3D solutions. Acknowledgments. We thank Wenyuan Zhang and YuShen Liu from Tsinghua University, as well as Yance Jiao, Hua Zhou, Liao Zhang, Yaohui Chen, Jinxin Xie, Yiwen Shao, and other colleagues from BAAI, for their valuable support and contributions to the See3D project. Figure 6. Top: Qualitative ablation of visual-condition; Bottom: As timestep decreases, visualize the trend of visual-condition. indicate that the visual-condition offers viable alternative to 3D-reliant warped conditions. Despite significant domain gap between Vt and warp images as shown in Fig.6, our model robustly handles this discrepancy, thanks to the time-dependent nature of the proposed condition. 5. Conclusion We propose scalable 3D generation framework from the perspective of dataset scaling, offering systematic solu-"
        },
        {
            "title": "References",
            "content": "[1] Antonio Alliegro, Yawar Siddiqui, Tatiana Tommasi, and Polydiff: Generating 3d polygarXiv preprint Matthias Nießner. onal meshes with diffusion models. arXiv:2312.11417, 2023. 3 [2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. 3, 8, 9 [3] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1969719705, 2023. 9, 23 [4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. 6 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 6 [6] Brandon Castellano. Pyscenedetect. https://github. [Online; com/Breakthrough/PySceneDetect/. accessed 13-Oct-2024]. [7] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from imIn age pairs for scalable generalizable 3d reconstruction. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1945719467, 2024. 3 [8] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf: unified approach to 3d generation and reconIn Proceedings of the IEEE/CVF International struction. Conference on Computer Vision, pages 24162425, 2023. 3 [9] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2147621485, 2024. 2 [10] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting In European Conference from sparse multi-view images. on Computer Vision, pages 370386. Springer, 2025. 3 [11] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 2, 3, 6, 8, 9, 19, 22, 23 [12] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: In Richly-annotated 3d reconstructions of indoor scenes. Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. [13] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. 20 [14] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 2, 3, 4, 20 [15] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 224236, 2018. 7, 18 [16] Paul Engstler, Andrea Vedaldi, Iro Laina, and Christian Rupprecht. Invisible stitch: Generating smooth 3d scenes with depth inpainting. arXiv preprint arXiv:2404.19758, 2024. 3 [17] Haoqiang Fan, Hao Su, and Leonidas Guibas. point set generation network for 3d object reconstruction from single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 605613, 2017. 2 [18] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds. arXiv preprint arXiv:2403.20309, 2, 2024. 8 [19] Martin Fischler and Robert Bolles. Random sama paradigm for model fitting with apple consensus: plications to image analysis and automated cartography. Communications of the ACM, 24(6):381395, 1981. [20] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene generation. Advances in Neural Information Processing Systems, 36, 2024. 6 [21] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything arXiv preprint in 3d with multi-view diffusion models. arXiv:2405.10314, 2024. 2, 3, 8, 9, 23 [22] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In International Conference on Machine Learning, pages 1180811826. PMLR, 2023. 3 [23] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 3dgen: Triplane latent diffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023. 3 [24] Junlin Han, Filippos Kokkinos, and Philip Torr. Vfusion3d: Learning scalable 3d generative models from video diffusion models. In European Conference on Computer Vision, pages 333350. Springer, 2025. 3 [25] Liang Han, Junsheng Zhou, Yu-Shen Liu, and Zhizhong Han. Binocular-guided 3d gaussian splatting with view consistency for sparse view synthesis. arXiv preprint arXiv:2410.18822, 2024. 8, 9, 23 [26] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. 4, 17 [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 20 [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3, 5 [29] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 79097920, 2023. 3 [30] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 2, 3 [31] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2 [32] Specializes in royalty-free digital content. https:// [Online; accessed artlist.io/stockfootage/. 15-Aug-2024]. 4 [33] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 406413, 2014. 8, 9 [34] Heewoo Jun and Alex Nichol. ing conditional 3d implicit functions. arXiv:2305.02463, 2023. 3 Shap-e: GeneratarXiv preprint [35] Abhishek Kar, Christian Hane, and Jitendra Malik. Learning multi-view stereo machine. Advances in neural information processing systems, 30, 2017. [36] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In Proc. ECCV, 2024. 4, 18 [37] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 8 [38] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, and Jinwoo Shin. Collaborative score distillation for consistent visual editing. Advances in Neural Information Processing Systems, 36:7323273257, 2023. 3 [39] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):113, 2017. 3, 8, 9, 22, 23 12 [40] Jiabao Lei, Jiapeng Tang, and Kui Jia. Rgbd2: Generative scene synthesis via incremental view inpainting using rgbd diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84228434, 2023. [41] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. 3 [42] Mengfei Li, Xiaoxiao Long, Yixun Liang, Weiyu Li, Yuan Liu, Peng Li, Xiaowei Chi, Xingqun Qi, Wei Xue, Wenhan Luo, et al. M-lrm: Multi-view large reconstruction model. arXiv preprint arXiv:2406.07648, 2024. 3 [43] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: Highresolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. 3 [44] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. Lightglue: Local feature matching at light speed. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1762717638, 2023. 7 [45] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 2, 3, [46] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885, 2023. 3 [47] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36, 2024. 3 [48] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 2, 3 [49] Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, and Ziwei Liu. Mvsgaussian: Fast generalizable gaussian splatIn European ting reconstruction from multi-view stereo. Conference on Computer Vision, pages 3753. Springer, 2025. 3 [50] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 2, 3 [51] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323, 2023. 17 [52] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99709980, 2024. [53] William Lorensen and Harvey Cline. Marching cubes: high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347353. 1998. 2 [54] Longfei Lu, Huachen Gao, Tao Dai, Yaohua Zha, Zhi Hou, Junta Wu, and Shu-Tao Xia. Large point-togaussian model for image-to-3d generation. arXiv preprint arXiv:2408.10935, 2024. 3 [55] Zhaoyang Lyu, Jinyi Wang, Yuwei An, Ya Zhang, Dahua Lin, and Bo Dai. Controllable mesh generation through sparse latent point diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 271280, 2023. 3 [56] Qi Ma, Yue Li, Bin Ren, Nicu Sebe, Ender Konukoglu, Theo Gevers, Luc Van Gool, and Danda Pani Paudel. Shapesplat: large-scale dataset of gaussian splats arXiv preprint and their self-supervised pretraining. arXiv:2408.10906, 2024. 3 [57] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1266312673, 2023. 3 [58] Ben Mildenhall, Pratul Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (ToG), 38(4):114, 2019. 8, [59] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [60] Norman Muller, Katja Schwarz, Barbara Rossle, Lorenzo Porzi, Samuel Rota Bul`o, Matthias Nießner, and Peter Kontschieder. Multidiff: Consistent novel view synthesis from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1025810268, 2024. 2, 3 [61] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 3 [62] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54805490, 2022. 8 [63] Provider of stock photography and stock footage. https: //www.pexels.com/search/videos/videos/. [Online; accessed 13-Oct-2024]. [64] Premiere online destination for drone pilots. https: [Online; ac- //www.airvuz.com/collections/. cessed 29-Sept-2024]. 4 [65] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 19 [66] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [67] Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to-3d. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9914 9925, 2024. 3 [68] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion paIn Proceedings of the 26th ACM SIGKDD rameters. International Conference on Knowledge Discovery & Data Mining, pages 35053506, 2020. [69] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. 4, 8, 9, 20, 22, 23 [70] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [71] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-shot 360-degree view synthesis from single real image. arXiv preprint arXiv:2310.17994, 2023. 3, 8, 9, 23 [72] Qiuhong Shen, Zike Wu, Xuanyu Yi, Pan Zhou, Hanwang Zhang, Shuicheng Yan, and Xinchao Wang. Gamba: Marry gaussian splatting with mamba for single view 3d reconstruction. arXiv preprint arXiv:2403.18795, 2024. 3 [73] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, single image to consisand Hao Su. arXiv preprint tent multi-view diffusion base model. arXiv:2310.15110, 2023. 3, 19 Zero123++: [74] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 2, 3, 18, 19 [75] Jaidev Shriram, Alex Trevithick, Lingjie Liu, and Ravi Ramamoorthi. Realmdreamer: Text-driven 3d scene generation with inpainting and depth diffusion. arXiv preprint arXiv:2404.07199, 2024. 2 [76] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 18 [77] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Deep unsupervised learning usand Surya Ganguli. ing nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. 3, 5 [78] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3, 5, 6, 21 and Stefano Ermon. arXiv preprint [79] Kunpeng Song, Ligong Han, Bingchen Liu, Dimitris Metaxas, and Ahmed Elgammal. Diffusion guided domain adaptation of image generators. arXiv preprint arXiv:2212.04473, 2022. 19 [80] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior. arXiv preprint arXiv:2310.16818, 2023. 3 [81] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. arXiv preprint arXiv:2312.13150, 2023. 3 [82] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 3 [83] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multiview gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2025. 3 [84] Paddle Team. Paddle ocr. https://github.com/ PaddlePaddle/PaddleOCR/. [Online; accessed 13Oct-2024]. 4 [85] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 4, [86] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. 2, 3 [87] Joseph Tung, Gene Chou, Ruojin Cai, Guandao Yang, Kai Zhang, Gordon Wetzstein, Bharath Hariharan, and Noah Snavely. Megascenes: Scene-level view synthesis at scale. arXiv preprint arXiv:2406.11819, 2024. 2, 4 [88] Videos and photos shot by DJI devices. https://www. skypixel.com/. [Online; accessed 29-Aug-2024]. 4 14 [89] Haiping Wang, Yuan Liu, Ziwei Liu, Wenping Wang, Zhen Dong, and Bisheng Yang. Vistadream: Sampling multiview consistent images for single-view scene reconstruction. arXiv preprint arXiv:2410.16892, 2024. [90] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European conference on computer vision (ECCV), pages 5267, 2018. 2 [91] Peng Wang and Yichun Shi. Imagedream: Image-prompt arXiv preprint multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. 3 [92] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Pf-lrm: Pose-free large reconstruction model Zhang. for joint pose and shape prediction. arXiv preprint arXiv:2311.12024, 2023. 3 [93] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision, 2024. 6 [94] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 4 [95] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: Highfidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024. [96] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 8, 9, 23 [97] Daniel Watson, William Chan, Ricardo Martin-Brualla, and Mohammad Jonathan Ho, Andrea Tagliasacchi, Norouzi. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628, 2022. 3 [98] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for highquality mesh. arXiv preprint arXiv:2404.12385, 2024. 3 [99] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2155121561, 2024. 3, 8, 9, 23 [100] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024. 3 [101] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Liang Pan Jiawei Ren, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, and Ziwei Liu. Omniobject3d: Largevocabulary 3d object dataset for realistic perception, reZhangyang Xiong, Tianyou Liang, et al. Mvimgnet: large-scale dataset of multi-view images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91509161, 2023. 2, 9, 20, [114] Yaohua Zha, Naiqi Li, Yanzi Wang, Tao Dai, Hang Guo, Bin Chen, Zhi Wang, Zhihao Ouyang, and Shu-Tao Xia. Lcm: Locally constrained compact point cloud model for masked point modeling. arXiv preprint arXiv:2405.17149, 2024. 3 [115] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 2, 3 [116] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 8 [117] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. ACM Trans. Graph. (Proc. SIGGRAPH), 37, 2018. 2, 8, 9, 20, 22, 23 [118] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian splatting. In European Conference on Computer Vision, pages 145163. Springer, 2025. 8, 9, 23 [119] Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, and Zexiang Xu. Long-lrm: Longsequence large reconstruction model for wide-coverage gaussian splats. arXiv preprint arXiv:2410.12781, 2024. 3 [120] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane meets gaussian splatting: Fast and generalizable singleview 3d reconstruction with transformers. arXiv preprint arXiv:2312.09147, 2023. [102] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, construction and generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Camera-controllable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. 3 [103] Guangkai Xu, Wei Yin, Hao Chen, Chunhua Shen, Kai Frozenrecon: Pose-free 3d Cheng, and Feng Zhao. scene reconstruction with frozen depth models. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 92769286. IEEE, 2023. 6, 7 [104] Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, and Fisher Yu. Murf: Multi-baseline radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2004120050, 2024. 8, 9, 23 [105] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. arXiv preprint arXiv:2410.13862, 2024. 8, 9, 20, 23 [106] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. [107] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. Grm: Large gaussian reconstruction model for efarXiv preprint ficient 3d reconstruction and generation. arXiv:2403.14621, 2024. 3 [108] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d-1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. 2 [109] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. arXiv preprint arXiv:2310.08529, 2023. 3 [110] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Interactive 3d arXiv preprint Freeman, and Jiajun Wu. Wonderworld: scene generation from single image. arXiv:2406.09394, 2024. 3 [111] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: Going from anywhere to everywhere. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66586667, 2024. [112] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 2, 3, 8, 9, 19, 22 [113] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, 15 . . . . . . 2 4 4 5 6 8 8 8 9 10 17 17 18 . 18 . 19 . 22 . 22 . 22 . 22 24 . 24 . 24 24 . . . . . . . . . . . . . ."
        },
        {
            "title": "Contents",
            "content": "1. Introduction 2. Related work 3. Method 3.1. Video Data Curation . 3.2. Visual Conditional Multi-View Diffusion Model 3.3. Visual Conditional 3D Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4. Experiments 4.1. Single View to 3D . 4.2. Sparse Views to 3D . . 4.3. Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5. Conclusion A. Broader Impact and Limitations B. Video Data Curation C. Technical Implementations . C.1. Model Architecture . . C.2. Training Configurations . . C.3. Definition of (t) and Wt . . D. More Experimental Results . D.1. Single View to 3D . . . D.2. Sparse Views to 3D . . . D.3. 3D Editing . . . . . . . . . . . E. Additional Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1. Effectiveness of Pixel-level Depth Alignment E.2. Efficacy of Scaling up Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F. Additional Visualizations"
        },
        {
            "title": "Appendix",
            "content": "A. Broader Impact and Limitations Broader Impact: Our model facilitates open-world 3D content creation from large-scale video data, eliminating the need for costly 3D annotations. This can make 3D generation more accessible to industries like gaming, virtual reality, and digital media. By leveraging visual data from the rapidly growing Internet videos, it accelerates 3D creation in real-world applications. However, careful consideration of ethical issues, such as potential misuse in generating misleading or harmful content, is crucial. Ensuring that the data used is curated responsibly to avoid bias and privacy concerns is vital for safe deployment. Limitations: While our model excels at long-sequence generation, it comes with some limitations regarding: 1) Inference Speed: The model requires several minutes for inference, making it challenging for real-time applications. Future work should aim to improve inference speed for real-time generation. 2) Focus on 3D Generation: The current model focuses only on 3D generation, avoiding the modeling of object motion. Future research could extend the model to simultaneously generate 3D and 4D content for dynamic scenes. 3) Model Scalability: While the data scaling approach is effective, the scalability of the model itself has not been explored. Expanding the models architecture could enhance its capability to handle more complex and diverse 3D content. B. Video Data Curation Our WebVi3D dataset is sourced from Internet videos through an automated four-step data curation pipeline. In this section, we provide further details on this pipeline process. Step 1: Temporal-Spatial Downsampling. To enhance data curation efficiency, we downsample each video both temporally and spatially. Temporally, we retain one frame for every two by downsampling with factor of two. Spatially, we adjust the downsampling factor according to the original resolution to ensure consistent visual appearance across different video aspect ratios. The final resolution is standardized to 480p in our experiment. Step 2: Semantic-Based Dynamic Recognition We perform content recognition on each frame to identify potential dynamic regions. Following [51], we utilize the off-the-shelf instant segmentation model Mask R-CNN [26] to generate coarse motion masks Mm for potential dynamic objects, including humans, animals, and sports activities. If motion masks are present in more than half of the video frames, the sequence is deemed likely to contain dynamic regions and excluded from further processing. Step 3: Non-Rigid Dynamic Filtering After filtering out videos with common dynamic objects, we implement precise strategy to identify and exclude videos containing non-rigid dynamic regions, such as drifting water and swaying trees. Following [51], we use the pretrained RAFT [85] to compute the optical flow between consecutive frames. Based on the optical flow, we calculate the Sampson Distance, which measures the distance of each pixel to its corresponding epipolar line. Pixels exceeding predefined threshold are marked to create non-rigid motion mask Ms. The number of pixels in Ms serves as an indicator of the likelihood of motion in the current frame. However, relying solely on this metric is unreliable, as most data are captured in real shots, where dynamic objects of interest are often concentrated near the center of the imaging plane. These moving regions may not occupy significant portion of the frame. Therefore, we also consider the spatial location of the dynamic mask and propose dynamic score to evaluate the motion probability for each frame. Let H, denote the height and width of an image, respectively. We define the central region as starting at = 0.25 W, = 0.25 H. The proportions of the mask occupying the entire image, Θi, and the central area Θc are calculated as: ΣW,H ΣW ,HH Θi = u,v=0Ms(u, v) , Θc = u,v=W ,H Ms(u, v) H/2 W/2 . (7) The dynamic score can be formulated as: Si = 2, Θi 0.12 & Θc 0.35 1.5, Θi 0.12 & 0.2 Θc < 0.35 1, Θi < 0.12 & 0.2 Θc < 0.35 0.5, Θi < 0.12 & Θc < 0.2 . 17 (8) This strategy targets the dynamic regions near the image center, enhancing data filtering accuracy. The final dynamic score for the entire sequence is calculated as: = ΣN i=0Si, (9) where represents the total number of extracted frames. If >= 0.25 , the sequence is classified as dynamic and subsequently excluded. Step 4: Tracking-Based Small Viewpoint Filtering. The previous steps produced videos with static scenes. We require videos that contain multi-view images captured from wider camera viewpoint. To achieve this, we track the motion trajectory of key points across frames and calculate the radius of the minimum outer tangent circle for each trajectory. Videos with substantial number of radii below defined threshold are classified as having small camera trajectories and are excluded. This procedure includes keypoint extraction, trajectory tracking, and circle fitting using RANSAC (Random Sample Consensus) [19]. Keypoint Extraction. To reduce computational complexity, we downsample the extracted video frames by selecting every fourth frame. SuperPoint [15] is then used to extract keypoints RN 2 from the first frame, where = 100 represents the number of detected keypoints used to initialize tracking. Trajectory Tracking. Keypoints are tracked across all frames using the pretrained CoTracker [36], which generates trajectories and visibility over time as: Tpred, Vpred = CoTracker(I, queries = K). Here, denotes the input frames, Tpred R1T 2 represents the tracked positions of each keypoint over time, and Vpred R1T 1 indicates the visibility of each point. Circle Fitting. For each tracked keypoint, circle fitting method is applied to its trajectory, selecting only frames where the keypoint is visible (Vpred = 1). Let Tvisible RM 2 be the filtered points, where is the number of visible points. We then use the RANSAC-based circle fitting algorithm on Tvisible to determine the circles center = (cx, cy) and radius r: (10) c, = RANSAC(Tvisible). (11) The RANSAC algorithm selects random subsets of three points to define candidate circles, computes the inliers, and optimizes for the circle with the highest inlier count and smallest radius. Finally, we count the number of circles with radius smaller than specified threshold, 20: (cid:88) count = I(ri 20), (12) i=1 where is the indicator function. The mean radius is also computed to provide an overall measure of circular motion. If the number of small-radius circles exceeds 40 and the average circular motion is less than 5, we classify this video as having small camera trajectories. User Study. To verify the effectiveness of our data curation pipeline, we conducted user study with randomly selected set of 10,000 video clips before filtering. We require our users to evaluate videos based on two aspects: static content and large-baseline trajectories. Only videos meeting both criteria are classified as 3D-aware videos. Among these, 1,163 videos met our criteria for 3D-aware videos, accounting for 11.6% of the total validation set. After applying our data screening pipeline, we randomly selected 10,000 video clips for annotation. In this filtered set, 8,859 videos were identified as 3Daware, yielding ratio of 88.6%, represents 77% improvement compared to the previous set. These results demonstrate the efficacy of our pipeline in filtering 3D-aware videos from large-scale Internet videos. C. Technical Implementations C.1. Model Architecture The main backbone of See3D model is based on the structure of 2D diffusion models but integrates 3D self-attention to connect the latents of multiple images, as shown in prior work [74]. Specifically, we adapt the existing 2D self-attention layers of the original 2D diffusion model into 3D self-attention by inflating different views within the self-attention layers. To incorporate visual conditions, we introduce the necessary convolutional kernels and biases using Zero-Initialize [76]. 18 Figure 7. Single-view to 3D. Compared with LucidDreamer [11] and ViewCrafter [112], which are also conditioned on warped images, our model can consistently generate high-fidelity views with detailed texture and structural information. The model is initialized from pretrained 2D diffusion model [65] and fine-tuned with all parameters, leveraging FlashAttention for acceleration. In accordance with prior work [73], switching from scaled-linear noise schedule to linear schedule is essential for achieving improved global consistency across multiple views. Additionally, we implement cross-attention between the latents of multiple views and per-token CLIP embeddings of reference images using linear guidance mechanism [79]. For training, we randomly select subset of frames from video clip as reference images, with the remaining frames serving as target images. The number of reference images is randomly chosen to accommodate different downstream tasks. The multi-view diffusion model is optimized by calculating the loss only on the target images, as outlined in Eq. 1. C.2. Training Configurations We initialize the See3D model from MVDream [74] and employ progressive training strategy. First, the model is trained at resolution of 512 512 with sequence length of 5. This phase involves 120,000 iterations, using 1 reference view 19 Figure 8. Sparse-views to 3D. Given 3 input views, our model generates clear, high-fidelity novel views that closely match the ground truth (GT), without artifacts or blurring. Note that the results from DepthSplat [105] are cropped and resized following the same data processing as the official source code. and 4 target views. Due to the relatively small sequence length, larger batch size of 560 is used to enhance stability and accelerate convergence. Next, the sequence length is increased to 16, and the model is trained for 200,000 iterations with 1 or 3 reference views and 15 or 13 target views, maintaining the resolution of 512 512. In this phase, the batch size is reduced to 228. Finally, multi-view super-resolution model is trained using the same network structure. It takes the multi-view predictions from See3D as input and outputs target images with multi-view consistency at resolution of 1024 1024, using batch size of 114. In all stages, all parameters of the diffusion model are fine-tuned with learning rate of 1e-5. Additionally, we render some multi-views or extract clips from datasets such as Objaverse [14], CO3D [69], RealEstate10k [117] , MVImgNet [113], and DL3DV [45] datasets, forming supplemental 3D dataset with fewer than 0.5M samples, please refer to Section E.2 for details on analysis and ablation. During training, this supplemental data is randomly sampled and incorporated into our WebVi3D dataset (16M). To enhance training efficiency, we utilize FlashAttention [13] alongside DeepSpeed with ZeRO stage-2 optimizer [68] and bf16 precision. We also implement classifier-free guidance (CFG) [27] by randomly dropping visual conditions with probability of 0.1. The See3D model is trained on 114 NVIDIA-A100-SXM420 Figure 9. Examples of Open-world 3D Editing. (a) Occlusion-free Editing: An Asian-style attic is added, and novel views are generated realistically. (b) Full Replacement Editing: vase is replaced with toy fox, seamlessly integrated into the scene from various viewpoints. (c) Occluded Editing: Hidden regions in the masked areas are inferred and completed to produce novel views. 40GB GPUs over approximately 25 days using progressive training scheme. During inference, DDIM sampler [78] with classifier-free guidance is employed. C.3. Definition of (t) and Wt In Eq.2, Ct is formulated as Ct = 1 αtϵ , where αt is composite function Definition for (t). that depends on α and t, with = (t) and (t) = β t. In our experiments, we set the hyper-parameter β = 0.2, which controls the noise level added to Ct. larger β increases the noise in Ct. As β approaches 1, Ct converges toward Gaussian distribution, improving robustness but reducing the correlation between Ct and X0, thereby weakening camera control. Conversely, as β approaches 0, the distributions of Ct and X0 become more similar, improving controllability. However, for downstream tasks, very small β creates significant domain gap between task-specific visual cues and the video data, compromising robustness. Thus, β serves as trade-off parameter, balancing camera control and robustness. αt(1 )X0 + Formulation for Wt. Recapping Eq.3 from the main manuscript, Vt = [Wt Ct + (1 Wt) Xt; ], where Wt is defined as piecewise function of t. (cid:40) Wt = vdecay end eb(tdecay endt), 1 (1 vdecay end) tpeakt tpeaktdecay end if < tdecay end, if tdecay end, , where tpeak = 1000, tdecay end = 300, vdecay end = 0.8, and = 0.075. To ensure that Wt remains within the range [0, 1], it is clamped as: Wt = clamp(Wt, 0, 1). As shown in Figure 10, 1) For between 300 and 1000, Wt decreases linearly as 21 Figure 10. Piecewise Function Wt, showing linear decay for timesteps between 300 and 1000, and monotonically decreasing concave behavior for < 300. decreases; 2) For < 300, Wt transitions to monotonically decreasing concave function of t. The rationale behind this design is to ensure that when Ct has significant noise, it exerts stronger influence on Vt, thus affecting MVD generation. Conversely, as the noise in Ct diminishes, Xt rapidly replaces Ct, reducing the risk of information leakage from Ct and improving the robustness of task-specific visual cues. The formulation of Wt enables flexible parameter tuning, such as vdecay end and b, to control its monotonic behavior. Smaller parameter values emphasize the impact of Ct on MVD, while larger values prioritize robustness. D. More Experimental Results Leveraging the developed web-scale dataset WebVi3D, our model supports both objectand scene-level 3D creation tasks, including single-view-to-3D, sparse-view-to-3D, and 3D editing. Additional experimental results for these tasks are presented below. D.1. Single View to 3D Table 4 presents quantitative comparison of zero-shot novel view synthesis performance on the Tanks-and-Temples [39], RealEstate10K [117], and CO3D [69] datasets. Our method consistently outperforms all others on both easy and hard sets, achieving the best results in every evaluation metric. Qualitative results are shown in Figure 7. Compared to warpingbased competitors such as LucidDreamer [11] and ViewCrafter [112], our approach more effectively captures both geometric structure and texture details, producing more realistic 3D scenes. These results highlight the robustness and versatility of our method in synthesizing high-quality novel views across diverse and challenging scenarios. D.2. Sparse Views to 3D Quantitative comparisons using 3, 6, and 9 input views are presented in Table 5. The 3DGS model trained on multi-view images generated by See3D outperformed state-of-the-art models in novel view rendering, demonstrating its ability to provide consistent multi-view support for 3D reconstruction without additional constraints. Qualitative comparisons in Figure 8 reveal fewer floating artifacts in the NVS results, indicating See3D generates higher-quality and more consistent multi-view images. D.3. 3D Editing Our model, trained on large-scale videos, naturally supports open-world 3D editing without the need for additional finetuning. Figure 9 illustrates three distinct editing scenarios: a) Occlusion-free Editing. An Asian-style attic is placed next to toy bulldozer in the original image, which serves as the reference view. Our model generates highly realistic images containing the Asian-style attic from various new viewpoints. b) Full Replacement Editing. The vase in the original image is completely replaced with toy fox. Our model generates new scenes from different viewpoints, seamlessly incorporating the toy fox into the designated area with no residual traces of the vase. c) Occluded Editing. Given an occluded edited image as reference view, our model can generate multiple novel views within the specified masked regions, inferring and filling in the hidden details of the occluded parts. 22 Dataset Method Tanks-and-Temples LucidDreamer [11] ZeroNVS [71] MotionCtrl [96] ViewCrafter ViewCrafter* Ours RealEstate10K LucidDreamer [11] ZeroNVS [71] MotionCtrl [96] ViewCrafter ViewCrafter* Ours CO3D LucidDreamer [11] ZeroNVS [71] MotionCtrl [96] ViewCrafter ViewCrafter* Ours Easy set Hard set LPIPS PSNR SSIM LPIPS PSNR SSIM 0.413 0.482 0.400 0. 0.221 0.167 0.315 0.364 0.341 0.145 0.164 0.125 0.429 0.467 0.393 0.243 0.331 0.225 14.53 14.71 15.34 21. 20.39 25.01 16.35 16.50 16.31 21.81 20.59 26.54 15.11 15.15 16.87 21.38 20.12 25.23 0.362 0.380 0.427 0. 0.648 0.756 0.579 0.577 0.604 0.796 0.825 0.872 0.451 0.463 0.529 0.687 0.703 0.781 0.558 0.569 0.473 0. 0.289 0.214 0.400 0.431 0.386 0.178 0.201 0.167 0.517 0.524 0.443 0.324 0.348 0.276 11.69 12.05 13.29 18. 17.86 22.52 14.13 14.24 16.29 22.04 20.40 24.18 12.69 13.31 15.46 18.96 18.02 23.33 0.267 0.309 0.384 0. 0.584 0.714 0.511 0.535 0.587 0.798 0.778 0.837 0.374 0.426 0.502 0.641 0.653 0.748 Table 4. Zero-shot Novel View Synthesis (NVS) on Tanks-and-Temples[39], RealEstate10K[117] and CO3D[69] dataset. Dataset Method LLFF Zip-NeRF [3] MuRF [104] FSGS [118] BGGS [25] ZeroNVS [71] DepthSplat [105] ReconFusion [99] CAT3D [21] Ours DTU Zip-NeRF [3] MuRF [104] FSGS [118] BGGS [25] ZeroNVS [71] DepthSplat [105] ReconFusion [99] CAT3D [21] Ours Mip-NeRF 360 Zip-NeRF [3] DepthSplat [105] ZeroNVS [71] ReconFusion [99] CAT3D [21] Ours PSNR 3-view SSIM LPIPS PSNR 6-view SSIM LPIPS PSNR 9-view SSIM LPIPS 17.23 21.34 20.31 21.44 15.91 17.64 21.34 21.58 23.23 9.18 21.31 17.34 20.71 16.71 15.59 20.74 22.02 28.04 12.77 13.85 14.44 15.50 16.62 17.35 0.574 0.722 0.652 0.751 0.359 0.521 0.724 0.731 0.768 0.601 0.885 0.818 0.862 0.716 0.525 0.875 0.844 0. 0.271 0.254 0.316 0.358 0.377 0.442 0.373 0.245 0.288 0.168 0.512 0.321 0.203 0.181 0.135 0.383 0.127 0.169 0.111 0.223 0.373 0.124 0.121 0.073 0.705 0.621 0.680 0.585 0.515 0.422 20.71 23.54 24.20 24.84 18.39 17.40 24.25 24.71 25.32 8.84 23.74 21.55 24.31 17.70 15.061 23.62 24.28 29. 13.61 13.82 15.51 16.93 17.72 19.03 0.764 0.796 0.811 0.845 0.449 0.499 0.815 0.833 0.820 0.589 0.921 0.880 0.917 0.737 0.523 0.904 0.899 0.900 0.284 0.260 0.337 0.401 0.425 0.517 0.221 0.199 0.173 0.106 0.438 0.340 0.152 0.121 0.104 0.370 0.095 0.127 0.073 0.205 0.406 0.105 0.095 0. 0.663 0.636 0.663 0.544 0.482 0.365 23.63 24.66 25.32 26.17 18.79 17.26 25.21 25.63 26.19 9.23 25.28 24.33 26.70 17.92 14.87 24.62 25.92 29.99 14.30 14.48 15.99 18.19 18.67 19.89 0.830 0.836 0.856 0.877 0.470 0.486 0.848 0.860 0.844 0.592 0.936 0.911 0.947 0.745 0.478 0.921 0.928 0. 0.312 0.288 0.350 0.432 0.460 0.542 0.166 0.164 0.136 0.090 0.416 0.341 0.134 0.107 0.098 0.364 0.084 0.106 0.052 0.200 0.451 0.094 0.073 0.059 0.633 0.602 0.655 0.511 0.460 0.335 Table 5. Quantitative Comparison of Sparse-view 3D Reconstruction E. Additional Ablation Studies E.1. Effectiveness of Pixel-level Depth Alignment We conducted additional ablation experiments to validate the effectiveness of the proposed pixel-level depth alignment. Specifically, we enabled and disabled pixel-level depth alignment when generating novel views through warping and visualized the warped results at specific generation step. As shown in Figure 11, the left image shows the reference GT image, the middle image corresponds to warping with pixel-level aligned depth, and the right one depicts warping without pixel-level aligned depth. The results demonstrate that pixel-level depth alignment not only effectively restores the scale of the depth map but also significantly corrects errors in monocular depth estimation (e.g., the toys neck and the tabletop). Consequently, integrating our proposed 3D generation pipeline improves generation quality. E.2. Efficacy of Scaling up Data Model LPIPS PSNR SSIM MV-UnposeT MV-UnposeT-10% MV-UnposeT-20% MV-UnposeT-60% MV-Posed 0.194 0.187 0.183 0.181 0.182 25.56 25.95 26.19 26.14 26.21 0.811 0.817 0.820 0.819 0. Figure 11. Ablation on Pixel-level Depth Alignment. Table 6. Ablation on Supplementary 3D Data. In the main manuscript, we conducted an ablation study on the 3D dataset MVImageNet [113] to evaluate the effectiveness of the proposed visual-condition. Table 3 shows that: 1) When conditioned on purely masked images, the MV-UnPoseM model performed the worst, struggling with the domain gap issue. 2) When conditioned on pose-guided warped images, the MV-Posed model achieved the best results, benefiting from pose annotations. 3) Our MV-UnposeT model, conditioned on the time-dependent visual-condition, demonstrated performance very close to that of the MV-Posed model. Intuitively, models trained entirely on 3D data tend to achieve optimal performance at specific data scale, establishing an upper bound at that scale. When the volume of video data matches that of 3D data, models trained on 3D still set the performance ceiling. However, as video data is virtually unlimited, scaling up the dataset can intuitively raise this upper bound. Following the same settings in Table 3, we further investigate the impact of supplementing multi-view data with 3D annotations on model performance. We conduct an ablation study using the MV-UnposeT model, trained on unposed multiview data with visual-condition. In this study, we progressively introduce 3D pose annotations at levels of 10%, 20%, 60%, and 100% into the training set. When the training data is entirely composed of 3D annotations, the model configuration is equivalent to the MV-Posed model. The results in Table 6 indicate that our MV-UnposeT model, initially trained on unposed data, improves steadily as 3D annotations are introduced. For instance, with only 20% 3D data (MV-UnposeT-20%), the models performance closely approaches that of the fully 3D-annotated MV-Posed model. This suggests that even small amount of 3D data in largely unposed multi-view dataset can significantly boost model performance, approaching the models trained on fully annotated 3D datasets. This insight is essential because unposed multi-view data is cost-effective and can be easily collected in large quantities. By incorporating small volume of high-quality 3D data, we can achieve performance comparable to models trained on large, expensive 3D datasets. Therefore, in our proposed WebVi3D dataset (16M samples), we incorporated small portion (0.5M samples) of 3D data to optimize model performance. F. Additional Visualizations Open-world 3D Generation with Long Sequences. We manually configured complex camera trajectories, including rotation, translation, zooming in, zooming out, focus distance adjustments and various random combinations, as shown in Figure 12 and Figure 13. Our model consistently generates high-quality, continuous novel views along these trajectories. 24 Figure 12. Examples of Long-sequence Generation. High-quality novel views generated along complex camera trajectories, maintaining spatial consistency and visual realism across extended sequences. Experimental visualizations demonstrate that the model effectively preserves spatial consistency and visual realism across long sequences. This highlights its robustness in handling intricate camera paths, including rapid transitions and diverse perspectives, making it highly applicable to open-world scenarios. Figure 13. More Examples of Long-sequence Generation."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence (BAAI)"
    ]
}