{
    "paper_title": "Model-Preserving Adaptive Rounding",
    "authors": [
        "Albert Tseng",
        "Zhaofeng Sun",
        "Christopher De Sa"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The main goal of post-training quantization (PTQ) is to produced a compressed model whose output distribution is as close to the original model's as possible. To do this tractably, almost all LLM PTQ algorithms quantize linear layers by independently minimizing the immediate activation error. However, this localized objective ignores the effect of subsequent layers, so reducing it does not necessarily give a closer model. In this work, we introduce Yet Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses Kronecker-factored approximations of each linear layer's Hessian with respect to the \\textit{full model} KL divergence. YAQA consists of two components: Kronecker-factored sketches of the full layerwise Hessian that can be tractably computed for hundred-billion parameter LLMs, and a quantizer-independent rounding algorithm that uses these sketches and comes with theoretical guarantees. Across a wide range of models and quantizers, YAQA empirically reduces the KL divergence to the original model by $\\approx 30\\%$ while achieving state of the art performance on downstream tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 8 8 9 2 2 . 5 0 5 2 : r Model-Preserving Adaptive Rounding Albert Tseng Cornell University albert@cs.cornell.edu Zhaofeng Sun Cornell University zs453@cornell.edu Christopher De Sa Cornell University cdesa@cs.cornell.edu"
        },
        {
            "title": "Abstract",
            "content": "The main goal of post-training quantization (PTQ) is to produced compressed model whose output distribution is as close to the original models as possible. To do this tractably, almost all LLM PTQ algorithms quantize linear layers by independently minimizing the immediate activation error. However, this localized objective ignores the effect of subsequent layers, so reducing it does not necessarily give closer model. In this work, we introduce Yet Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses Kronecker-factored approximations of each linear layers Hessian with respect to the full model KL divergence. YAQA consists of two components: Kronecker-factored sketches of the full layerwise Hessian that can be tractably computed for hundred-billion parameter LLMs, and quantizer-independent rounding algorithm that uses these sketches and comes with theoretical guarantees. Across wide range of models and quantizers, YAQA empirically reduces the KL divergence to the original model by 30% while achieving state of the art performance on downstream tasks."
        },
        {
            "title": "Introduction",
            "content": "The ever-growing size of modern large language models (LLMs) continues to pose challenges for cost-effective and efficient deployment [22, 30]. One way to achieve better cost-benefit scaling is through quantization, which can greatly reduce the size and compute requirements of model without significantly degrading quality [7]. For example, the latest post-training quantization (PTQ) methods can produce high quality 2-bit models, representing an 8 size reduction over 16-bit baseline [32, 20]. In memory-bound settings, this reduces I/O volume, and in compute-bound settings, using hardware-supported low precision datatypes can increase throughput [35]. At their core, PTQ methods, which quantize model parameters after training, can be viewed as generalized rounding algorithms that round high-precision model parameters θ to set of representable low-precision points C. The goal of these methods is to minimize the distance between the original model outputs (θ, X) and quantized model (θ, X) over some inputs sampled from an input distribution and model architecture : ˆθ arg min θC EXDDKL(M (θ, X)M (θ, X)) (1) Here, we use the KL divergence between the model output distributions to measure how close two models are since the KL divergence is directly used in or closely related to core metrics in many downstream uses of quantized models [17, 25]. Unfortunately, exactly solving Equation 1 is intractable. While some works have proposed running constrained optimization on the full model KL, these methods do not consistently outperform quantization algorithms that use local second order information. Instead, virtually all state-of-the-art quantization algorithms such as LDLQ [3] and GPTQ [7] minimize the local layerwise activation error for each linear layer Rmn as proxy for Equation 1: arg min ExRnDx(W )T 2 = arg min ExD tr((W )xT x(W )T ). (2) Preprint. Under review. This de facto proxy objective admits layerwise Hessian for the activation minimization problem H1 = ExD[xT x], which is then used to quantize weights. However, since Equation 2 does not account for the effect of future layers on quantizing the current layer, quantizing with H1 does not necessarily reduce Equation 1. To remedy this, we introduce Yet Another Quantization Algorithm (YAQA), new rounding algorithm that uses Kronecker-factored approximations of each linear layers Hessian with respect to the end-toend KL divergence. Consider the second order expansion of the layerwise quantization problem at the original model for linear layer Rmn: arg min EXDDKL(M (Θ, , X)M (Θ, W, X)) 1 2 (W )T (2 L)(W ) (3) where = DKL(M (Θ, , X)M (Θ, W, X)), Θ = θ and first order terms involving are 0 since DKL is minimized at the original model. Equation 3 is equivalent to assuming that the off-block diagonal elements of θ are 0, which recent works have suggested is empirically close to true [36]. Since the Hessian of the KL divergence is given by the Fisher Information Rmnmn to E[vec(W ℓ)vec(W ℓ)T ], where the expectation Matrix, we can simplify 2 is taken over independent samples and ℓ is computed over the output distribution (see Section 3.1) [9]. Although 2 is too large to manifest, this structure lets us find Kronecker-factored approximations of 2 (HO Rmm) (HI Rnn) with Hessian-vector products. These approximations form the basis of YAQA, which consists of two main components. First, we present series of Hessian sketches that use Hessian-vector products to tractably compute close-tooptimal HO, HI for hundred billion parameter scale models. Then, we introduce new adaptive rounding algorithm that can take advantage of these Hessians and comes with theoretical guarantees. This algorithm is quantizer independent, so YAQA can be used with hardware-supported datatypes (e.g. INT4) as well quantizers designed for memory-bound decoding (e.g. QTIP). Empirically, YAQA reduces the KL divergence by 30% across wide variety of models and quantizers while simultaneously achieving state of the art downstream task performance. In summary, we: 1. Introduce YAQA, which quantizes linear layers with Kronecker-factored approximations of the layerwise Hessian of the end-to-end KL divergence to the original model. 2. Describe close-to-optimal Kronecker-factored Hessian sketches that can be tractably computed for modern hundred-billion parameter LLMs. 3. Show that YAQA significantly reduces the KL divergence to the original model over existing rounding algorithms and achieves state of the art downstream results. Our code to compute these Hessians and quantize models is available here."
        },
        {
            "title": "2.1 Hessian Approximations",
            "content": "Prior works in optimization have proposed wide variety Hessian sketches for modern neural networks. Most of these sketches are based off the Fisher Information Matrix = E[vec(W ℓ)vec(W ℓ)T ], which is the Hessian of the KL divergence with respect to the parameters of distribution [9]. In the real FIM, which corresponds to H, ℓ is computed with Monte-Carlo sample over the model output logits, and in the empirical FIM, which is fundamentally different from [16], ℓ is the task loss [21]. Since is usually too large to manifest, past Hessian sketches have focused on producing Kronecker-factored approximations of HO HI . In KFAC, the authors show that for linear layers in nonsequential models, = E[xT (yℓ)T (yℓ)][21]. This gives simple approximation of the real FIM: HI = E[xT x] and HO = E[(yℓ)T (yℓ)]. In EKFAC, the authors add an eigencorrection to KFAC by observing that arg min SRmn (QO QI )diag(S)(QO QI )T 2 is the eigendecomposition of HI and likewise for HO [8]. Here, corresponds to better estimate of the eigenvalues of vs. KFACs implicitly rank-1 eigenvalue approximation SO SI . In Shampoo and SOAP, the eigencorrected version of Shampoo, the authors propose approximating the empirical FIM with HI = E[(W ℓ)T (W ℓ)], HO = E[(W ℓ)(W ℓ)T ], where ℓ is computed O(W ℓ)QI ], where QI SI QT = E[QT 2 batchwise [10, 34]. Finally, Adam corresponds to the eigencorrected approximation of the empirical Fisher using HI = HO = [15]."
        },
        {
            "title": "2.2 Model Compression",
            "content": "While PTQ has achieved popularity due to its combination of strong quality and relatively low resource overhead, there are many other ways of compressing models. For example, quantization aware training (QAT) and low precision training methods produce natively quantized models through modified training recipes [25, 33]. These methods require significantly more compute than PTQ but can give higher quality quantized models. In these settings, PTQ algorithms can be used for initialization or for rounding during training. Finally, methods such as pruning can also produce compressed models by removing entire parameters [12, 2, 29]."
        },
        {
            "title": "2.3 Layerwise Rounding and Incoherence Processing",
            "content": "Certain methods such as OBS, AdaRound, and SqueezeLLM have explored using information from the empirical Fisher matrix on the task loss (i.e., not the KL divergence to the original model) [12, 24, 14]. These methods assume the model is trained close to convergence and only use minimal information from the Empirical Fisher such as the diagonal. Such assumptions are not always true [13], which can lead to suboptimal performance. Furthermore, although quantizing models based on the empirical Fisher may reduce the task loss, the empirical Fisher is fundamentally different from the Hessian of the KL, so the resulting model may not be close to the original model [16]. In fact, the empirical Fisher of the KL is 0 since the KL is minimized at the original model and ℓ = 0, so the empirical Fisher gives no information when with the KL. Instead, most modern methods independently minimize the layerwise activation error with information from H1. In GPTQ and LDLQ, which are equivalent to each other, columns in are iteratively rounded with linear feedback from the Cholesky decomposition of H1 [3]. In AWQ, scaling from the activation channel norms is used to adjust the weights, corresponding to weighting the quantization problem by the diagonal of H1 [18]. In QuIP, Chee et al. [3] showed that one could bound the proxy error tr(W H1W ) of LDLQ by an increasing function of H1s incoherence: Definition 1 (Chee et al. [3]). Hessian Rnn is µ-incoherent if its eigendecomposition = QΛQT has maxi,j Qij = maxi,j eT Qej µ/ n. To reduce µH1 and thus the error of LDLQ, QuIP introduced incoherence processing, which conjugates H1 and by fast-to-multiply random orthogonal matrices Rmm, Rnn to concentrate their entries: W V, H1 H1V . In QuIP#, Tseng et al. [31] improved QuIPs incoherence processing with the random Hadamard transform (RHT), which achieves, with probability 1 δ, µH = (cid:112)2 log(n2/δ) and can be performed in O(n log n) time [11]. The RHT also makes approximately i.i.d Gaussian, enabling the use of specially designed Gaussian quantizers [31, 32]. The RHT has since become widely adopted in PTQ and low precision training works [3133, 1]."
        },
        {
            "title": "2.4 Finetuning and Gradient-Based Quantization",
            "content": "In Section 2.3, quantized representations are obtained without learning and are not updated once they are obtained. In contrast, certain recent methods have proposed finetuning algorithms that are essentially constrained QAT with much smaller compute budget. In PV-Tuning, learnable codebooks and code point assignments are jointly optimized to minimize the end-to-end KL divergence [20]. In DiscQuant, model weights are updated with gradient descent on constrained optimization problem over the full model KL divergence [4]. To the best of our knowledge, these two methods are the only prior quantization algorithms that explicitly consider the full model KL divergence over some localized proxy objective such as Equation 2."
        },
        {
            "title": "3 Yet Another Quantization Algorithm",
            "content": "Here, we describe YAQA, layerwise adaptive rounding method that rounds layers to minimize the full model KL divergence. YAQA consists of two components: 1) Kronecker-factored approximations 3 of the full layer Hessian with respect to the end-to-end KL divergence and 2) theoretically principled rounding algorithm that uses these Kronecker-factored Hessians. With respect to the two classes of methods in Section 2, YAQA rounds layers in one-shot way like methods in Section 2.3 but considers non-localized information like methods in Section 2.4. In the following sections, we first introduce two different Hessian sketches for close-to-optimal Kronecker-factored approximations of that can be tractably computed for hundred billion parameter scale models. Then, we introduce our rounding algorithm that uses these Hessians and characterize its theoretical properties."
        },
        {
            "title": "3.1 Kronecker Factored Hessians",
            "content": "Our goal is to find HI and HO that can be tractably computed for modern hundred-billion parameter scale LLMs and 2 = HO HI . Since the Kronecker product is reshaped rank-1 product [19], natural way to obtain HI and HO is to perform power iteration: (HI )i H(HO)i1 (HO)i12 (HO)i (HI )i1H (HI )i12 . (4) However, obtaining good estimate of for power iteration is practically difficult. Recall that = E[vec(W ℓ)vec(W ℓ)T ], where the expectation is taken over independent samples and ℓ is computed with Monte-Carlo sampled target from the output logits z: ℓ = CE(t Cat(z), z). In LLM, tokens within sequence are not independent due to sequence mixing from self-attention. As such, must be computed over entire sequences and not individual tokens, which increases the variance of the estimate and makes increasing the sample size particularly expensive. Furthermore, to get non rank-deficient estimate of HI Rnn, we need to backprop through O(n) sequences (and likewise O(m) for HO). For large models where the intermediate dimension size is O(104), this can be very expensive for multiple rounds of power iteration. To remedy these issues, we propose two different Hessian sketches that both achieve strong empirical performance while being tractable to compute. Our first sketch (A) performs power iteration on Hessian estimate that assumes tokens are independent within linear layer. This lets us reduce the variance to O(1/T ), where is the number of tokens, at the cost of slightly biased estimate. Our second sketch (B) runs one round of power iteration on 2 starting from an identity matrix initialization, which we show can be computed in single pass over dataset. This lets us use enough sequences to achieve acceptable variance without making Hessian collection too expensive."
        },
        {
            "title": "3.1.1 Hessian Sketch A",
            "content": "Sketch performs power iteration on Hessian estimate that assumes that attention does not mix within sequence and tokens within sequence are thus independent. In this setting, (2 L)A = ExD (cid:2)xT (yℓ)T (yℓ)(cid:3) (5) where ℓ is computed with the same Monte-Carlo sample as before but each x, pair corresponds to an individual token. This sketch is obviously biased but allows us to significantly reduce the variance by increasing the sample size to the number of tokens. Furthermore, (2 L)A still has sequence information from (yℓ), so it is still empirically close to 2 L. To perform power iteration with ( L)A, we compute (HI )i ExD (HO)i ExD (cid:2)xT (cid:10)(HO)i1, (yℓ)T (yℓ)(cid:11)(cid:3) /(HO)i12 (cid:2)(yℓ)T (yℓ) (cid:10)(HI )i1, xT x(cid:11)(cid:3) /(HI )i12 . (6) (7) The only things required for Equation 6 are the input and error signal dℓ dy , so we can simply use modified Pytorch backward pass that composes with FSDP to perform fully-distributed power iteration [26, 37]. To speed up convergence, we initialize (HI )0 H1 = E[xT x] and (HO)0 I. Empirically, power iteration converges in 3 full iterations with this initialization. End-to-end, sketch takes around 30 GPU-hours for O(10B) parameter model and 20M token calibration set. 4 Figure 1: (L, C) Normalized cosine similarity of H1, A, and different sequence counts for B, calculated against 256K sequences. and are both significantly higher than H1. (R) Average actual cosine similarity between and over decoder layers by projection matrix. and are close to each other, with the pre-attention score layers showing higher similarity than other layers."
        },
        {
            "title": "3.1.2 Hessian Sketch B",
            "content": "Sketch directly computes the result of one round of power iteration on 2 HI , HO I. Observe that power iterating on 2 starting with EsD (cid:2)(W ℓ)T (HO)i1(W ℓ)(cid:3) (HI )i (HO)i12 W involves computing the following updates (cid:2)(W ℓ)(HI )i1(W ℓ)T (cid:3) EsD (HO)i . (HI )i12 (8) (cid:2)(W ℓ)T (W ℓ)(cid:3) /m and If (HI )0 and (HO)0 are both I, then we have that (HI )1 = EsD (cid:2)(W ℓ)(W ℓ)T (cid:3) /n where the expectation and ℓ are computed over indi- (HO)1 = EsD vidual sequences. Both of these are easily computable in the same backward pass, letting us do one round of power iteration on both HO and HI in single pass over dataset. This sketch is conceptually similar to the preconditioning basis in Shampoo [10, 23], except that we compute the gradient per-sequence instead of per-batch and use the real Hessian instead of the empirical Fisher. End-to-end, sketch takes around under 50 GPU-hours for O(10B) parameter model and 64K sequences of 2K tokens each."
        },
        {
            "title": "3.1.3 Comparing Sketch A and Sketch B",
            "content": "Figure 1 Left and Center contain plots of the normalized cosine similarity of A, computed with various dataset sizes, and H1, which corresponds to the layerwise activation error Hessian from Equation 2. computed against ground truth calculated with 256K sequences. Here, the normalized cosine similarity denotes H,HOHI HOHI , where is calculated with 256K sequences and each plot is scaled so the largest data point is 1.0. This avoids computing H, which cannot be computed exactly in tractable way and requires many expensive Hessian vector products to estimate with low variance. In general, the cosine similarities of and are both much higher than H1 and are close to each other, with being slightly lower than B. In certain cases, such as for layer 24 Down in Llama 3.1 8B Instruct [22], can actually have higher cosine similarity than estimated with small dataset. This illustrates the case where the bias of is less than the variance of and where one might want to use instead of B. Finally, Figure 1 Right shows that and have (HO)A(HI )A,(HO)B (HI )B high cosine similarities (HO)A(HO)B (HI )A(HI )B to each other, with the pre-attention score layers (Q, K) having higher similarity than the post-attention score layers. This correlates with our construction of A, which assumes that attention does not do sequence mixing."
        },
        {
            "title": "3.2 Adaptive Rounding with Kronecker Factored Hessians",
            "content": "Now, we describe adaptive rounding algorithm that admits bounds on how well we can minimize Equation 3. Consider the family of adaptive rounding algorithms that rounds each entry as follows: Wi,j = (cid:0)W i,j + aT (W :i,:j W:i,:j)bj + aT (W :i,j W:i,j) + (W i,:j Wi,:j)bj (cid:1) , (9) where ai is the i-th column of strictly upper triangular matrix LO, bj is the j-th column of strictly upper triangular matrix LI , and is quantizer that performs nearest or stochastic Figure 2: Empirical CDF of ratio of tr(DIP denotes DI after incoherence processing HI with the RHT and likewise for DIP and HO. ratio of < 1 (left of vertical red line) indicates reduction in error bound. IP improves the error bound for most layers for both Hessian sketches. ) to tr(DO) tr(DI ), where DIP O) tr(DIP rounding. Since LO and LI are both strictly triangular, the feedback in Equation 9 effectively comes from rectangle missing corner. For simplicity, we restrict our analysis to the scalar quantization case where each entry is rounded alone. The theory in this section translates to the vector quantization case, which we detail in the Appendix. The final output of performing Equation 9 is that satisfies = (cid:0)W + LT OW + LI OW LI + LT where = . If we choose LO to be the LDL decomposition of HO (HO = (LO + I)DO(LO + I)T for unit lower triangular LO and diagonal DO) and LI to be the LDL decomposition of HI (HI = (LI + I)DI (LI + I)T ) [27], then we can bound the error of the adaptive rounding procedure: Theorem 1. Let HO Rmm and HL Rnn be two positive definite matrices and let perform nearest or stochastic rounding with E[(Q(x) x)2] σ2. Furthermore, let be the output of Equation 10 with LO and LI from the LDL decompositions of HO and HI , respectively. Then, (10) (cid:1) (HO HI )W tr(DI ) tr(DO)σ2 µ2 µ2 mn tr(H 1/2 )2 tr(H 1/2 )2σ2 where = and µO, µI are the incoherences of HO, HI (Definition 2). Theorem 1 tells us that the error of YAQA is bounded by trace product tr(DO) tr(DI ), which can be correspondingly bounded by the incoherences of HO and HI . This suggests that like in LDLQ, we should reduce the incoherence of all Hessians before quantizing with incoherence processing. Figure 2 shows the empirical CDF across layers of the ratio tr(DIP denotes DI after incoherence processing HI with the RHT and likewise for DIP and HO. For the vast majority of layers in Llama 3.1 8B Instruct and 70B Instruct, incoherence processing reduces the trace bound. O) tr(DIP ) tr(DO) tr(DI ) , where DIP Although HO, HI can be any postive definite matrices in Theorem 1, we can further bound the true proxy error HW the thing we actually care about with the cosine similarity between HO HI and H: Theorem 2. Let Rmnmn = L be the Hessian of linear layer with respect to the KL divergence to the original model outputs and let everything else be defined as in Theorem 1. Then, HW (cid:18) 2 2 2c + µ2 µ2 mnHI HO tr(H 1/2 )2 tr(H 1/2 )2σ2 (cid:19) where = H,HOHI HHOHI is the cosine simliarity between and HO HI . Theorem 2 lets us reason about when YAQA results in tighter bound on the true layerwise proxy error than current state-of-the-art adaptive rounding methods LDLQ and equivalently GPTQ. First, observe that LDLQ is equivalent to running YAQA with HO and HI H1. From Section 3.1.3 and Figure 1, we can empirically validate that is much higher for both and than H1. If we 6 Figure 3: Real-world HOs from and are approximately low rank and have similar spectrums. assume that we have reasonable amounts of feedback and F is approximately the same for 2 2c will be lower for YAQA. both YAQA and LDLQ, which is empirically true, then F Now, consider the part of Theorem 2 corresponding to the adaptive rounding incoherence bound from Theorem 1. In LDLQ, HO = and tr(DO) = m, so the ratio between the bound for arbitrary HI and HO and the bound for LDLQ is tr(H 1/2 Oµ2 µ2 mµ2 1 tr(H 1/2 1 )2H1 tr(H 1/2 )2 )2HI HO . (11) )2 kO tr(HO), so if HO has rank kO mµ2 Oµ2 µ2 From Cauchy-Schwarz, tr(H 1/2 Equation 11 tr(HO) mHO 1. This essentially tells us that when HO is sufficiently low rank, the second term in Theorem 2 is smaller than the second term in Equation 11 and we should expect YAQA to better minimize the true proxy error HW than LDLQ. Indeed, Figure 3 shows that empirically, real-world HO matrices have strong spectral decay and are approximately low rank. )2HI )2H1 1 tr(H 1/2 tr(H 1/ then 1 I"
        },
        {
            "title": "4 Experiments",
            "content": "(cid:105) (cid:104) log p(x) q(x) To test YAQA, we quantize Llama 3.1 and 3.2 Instruct models and evaluate both downstream perplexity and zeroshot accuracy and the KL divergence to the original model outputs (implementation details in Appendix). Although the KL divergence DKL(pq) = Exp and perplexity ( log q(τ ) for single target τ ) are closely related, the KL divergence considers information outside of the target token τ whereas perplexity only considers the mass on τ . Thus, it is possible for two models to have the same perplexity but different DKLs and vice versa, or even for model to have higher perplexity but lower DKL. Our goal is to produce model as close to the original model as possible, so we consider lower DKL to indicate closer model. Baselines and Setup The focus of YAQA is on the Hessian estimate and rounding algorithm, so our main analysis is on one-shot quantization without any additional finetuning. However, we include experiments with recovery finetuning to show that YAQA composes with finetuning. For YAQA-A, we use context length of 8K, 20M tokens, and 3 full rounds of power iteration. For YAQA-B, we use context length of 2K and 64K sequences. Our main rounding algorithm baseline is LDLQ, which is equivalent to the widely-used GPTQ and is the core rounding algorithm in many current state-of-the-art quantization works. We also include smaller comparsion to DiscQuant, which uses gradient-based optmization on the full-model KL. We do not directly compare to PV-Tuning since there are no public PV-Tuning models with either the QTIP or INT4 quantizer. However, LDLQ with the QTIP quantizer already outperforms PV-Tuning with the learnable AQLM quantizer on Llama 3.1, so we expect YAQA with QTIP to outperform PV-Tuning as well [32]. 4.1 One-Shot Quantization Table 1 shows our main results with the QTIP quantizer, incoherence processing, and no finetuning. Here, both YAQA Hessian sketches (YAQA-A and YAQA-B) consistently outperform LDLQ in KL divergence and downstream performance. YAQA-B generally outperforms YAQA-A due to its higher 7 Table 1: Results with incoherence processing, the QTIP quantizer, and no finetuning for Llama 3.1 and 3.2 Instruct models. YAQA strongly outperforms the LDLQ baseline that corresponds to the state-of-the-art QTIP paper, with YAQA-B outperforing YAQA-A due to being better (but more expensive) Hessian estimate. Regardless, YAQA consistently reduces the KL divergence to the original model by factor of 1/3. Individual zeroshot results are in the Appendix. ALGO. BITS DKL PPL 0-SHOT DKL PPL 0-SHOT W2 W2 C4 AVG W2 W2 C4 AVG BF16 0 LDLQ YAQA-A YAQA-B 2 3 4 2 3 2 3 4 0.497 0.138 0.045 0.378 0.110 0.036 0.335 0.094 0.030 3.1 70B INST. 3. 6.02 4.26 3.74 5.56 4.10 3.73 5.30 4.01 3.69 6.46 7.82 6.74 6.54 7.51 6.68 6. 7.34 6.64 6.51 3.1 8B INST. BF16 16 0 6. 8.02 LDLQ YAQA-A YAQA-B 2 3 4 2 3 2 3 4 0.356 0.069 0.019 0.284 0.050 0.015 0.241 0.044 0.013 9.39 10.70 8.50 7.05 8.15 6.63 8.79 10.09 8.38 6.89 8.13 6. 8.39 6.85 6.61 9.83 8.34 8.12 67.67 65.45 67.58 67.58 65.92 67.69 67.50 66.19 67.24 67. 69.82 62.51 69.29 69.41 64.06 69.09 69.62 64.32 69.31 69.78 3.2 3B INST. 9.58 10.62 0.455 0.085 0.021 0.333 0.059 0.015 0.288 0.047 0.014 15.30 10.69 9. 13.75 10.37 9.78 13.18 10.15 9.80 14.69 11.44 10.79 13.56 11.14 10.76 13.10 11.09 10.75 3.2 1B INST. 0 11.57 13.20 0.527 0.109 0.032 0.371 0.072 0.021 0.334 0.065 0. 19.86 12.95 12.05 17.22 12.56 11.83 16.66 12.51 11.83 19.66 14.44 13.63 17.64 14.02 13.44 17.36 13.97 13. 63.79 55.68 62.46 63.70 58.29 62.51 63.28 59.11 62.74 63.31 54.79 49.60 52.89 53. 50.02 53.86 54.17 50.90 53.41 53.84 Table 2: Results with incoherence processing, the INT4 quantizer, and no finetuning for Llama 3.1 8B Instruct. YAQA is quantizer agnostic and works with standard datatypes such as INT4. ALGO. BF16 LDLQ DISCQUANT YAQA-A YAQA-B DKL PPL 0-SHOT ACC W2 0 0.038 0.061 0.028 0. W2 6.50 6.76 6.83 6.71 6.72 C4 8.02 8.26 8.37 8.21 8. AVG ARCC ARCE BOOLQ HSWAG PIQA 69.82 51. 67.99 67.68 69.11 68.92 50.00 50.34 50.68 49.49 78.03 76.94 77.44 78.11 77.31 82.05 77.01 74.12 79.65 81. 57.74 56.83 56.55 57.13 56.98 79.92 79.16 79.92 79.98 79.82 cosine similarity, at the cost of being more expensive to compute. Regardless, YAQA reduces the KL divergence to the original model by factor of 1/3 across all models and bitrates. Interestingly, all methods get similar perplexity at 4 bits but YAQA is still able to reduce the KL divergence, indicating that YAQA better preserves the output distribution outside of the target next token used to calculate perplexity. Table 2 shows results for quantizing Llama 3.1 8B Instruct with the INT4 quantizer without finetuning. Again, YAQA outperforms LDLQ in both KL divergence and downstream tasks. Table 2 also includes comparison to DiscQuant. Interestingly, LDLQ also outperforms DiscQuant, showing that unlike YAQA, existing methods that optimize the end-to-end KL do not consistently outperform proxy objective-based methods."
        },
        {
            "title": "4.2 How Much Does Finetuning Help?",
            "content": "Multiple recent PTQ works have included recovery finetuning step that adjusts unquantized parameters to compensate for the effect of already-quantized parameters before quantization [6, 20, 8 Table 3: Results with the QTIP quantizer, incoherence processing, and recovery finetuning. Although recovery finetuning reduces the gap between YAQA and LDLQ, YAQA still improves upon LDLQ to achieve state-of-the-art results. ALGO. BITS DKL PPL 0-SHOT DKL PPL 0-SHOT W2 C4 AVG W2 W2 C4 AVG LLAMA 3.1 70B INSTRUCT LLAMA 3.1 8B INSTRUCT BF16 16 0 LDLQ YAQA-A YAQA-B 2 3 4 2 3 4 2 3 4 0.302 0.101 0. 0.279 0.098 0.032 0.266 0.091 0.029 3.52 5.01 3.96 3.71 4.92 3.88 3.68 4.82 3.87 3. 6.46 7.16 6.64 6.54 7.10 6.63 6.52 7.07 6.61 6.52 67.67 66.11 67.46 67. 66.26 66.94 67.59 66.99 67.42 67.69 0 0.185 0.048 0.016 0.163 0.042 0.014 0.147 0.038 0. 6.50 7.82 6.80 6.61 7.63 6.78 6.58 7.60 6.74 6.56 8.02 9.20 8.31 8. 9.06 8.28 8.10 8.96 8.27 8.11 69.82 65.44 68.42 69.47 67.54 69.23 69.50 66.38 68.88 70. Table 4: Results for Gemma 3 12B Inst. with INT4 without finetuning. Despite being trained on the original models outputs, the QAT model has higher DKL to the original model than the YAQA models. The QAT model also somehow outperforms the original model in downstream evals, implying that the QAT process is producing considerably different model. ALGO. QUANT. TYPE BITS DKL PPL 0-SHOT ACC W2 W2 C4 AVG ARCC ARCE BOOLQ HSWAG PIQA BF16 NONE 16 0 7.85 8.61 70.22 54.01 78.79 87.25 54.27 76. GOOGLE QAT QAT 4.5 0.089 7.56 8.52 70.83 54.52 79.76 86. 54.77 78.29 YAQA-A YAQA-B PTQ PTQ 4 4 0.058 0. 7.96 8.69 70.12 53.90 78.83 7.94 8.67 69.90 54.10 78.66 87.09 86.91 53.68 54.13 77.09 75.68 31, 32]. While recovery finetuning methods are mostly ad-hoc, their main goal is still to preserve the original model. This raises the question: how much of YAQAs improvements over LDLQ can be recovered from finetuning? Table 3 shows an experiment with QuIP#s recovery finetuning algorithm (details in Appendix) on top of the setup in 1 [31]. This finetuning algorithm was developed and optimized for LDLQ, so it is entirely possible that better finetuning algorithm exists for YAQA, which we leave for future work. For both models, recovery finetuning reduces the gap between LDLQ and YAQA. However, YAQA still reduces the KL divergence to the original model by 10-20% and maintains gap in downstream tasks, suggesting that YAQA uses global information that is not available during recovery finetuning. Finally, we note that the LDLQ results in this table correspond to the results in the state-of-the-art QTIP paper, so YAQAs results Table 3 set new state of the art across all PTQ methods and quantizers."
        },
        {
            "title": "4.3 How Does YAQA Compare to Quantization Aware Training?",
            "content": "While YAQA is PTQ method, it is perhaps interesting to compare it to much more expensive QAT methods that also try to preserve the original model. Table 4 compares YAQA with INT4 and no finetuning against Googles official QAT version of Gemma 3 12B Instruct, which uses 4.5 bit datatype [30]. We estimate the QAT process took O(109) tokens, or 100 more data than YAQA. Although the QAT model somehow outperforms the original model in downstream tasks despite being trained on the original model outputs [30], YAQA models have lower KL divergence to the original model. YAQA also maintains smaller gap in downstream performance, suggesting that the QAT process is actually producing considerably different model from the original model."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present YAQA, new LLM PTQ algorithm that quantizes linear layers with Kronecker-factored approximations of the Hessian with respect to the full model KL divergence. By considering the end-to-end KL divergence, YAQA improves upon existing state-of-the-art PTQ methods the vast majority of which only consider local layerwise activation errors. YAQA consists of two parts: near-optimal Kronecker-factored Hessian sketches for the full-model KL that can be tractably computed for hundred-billion parameter scale models, and new adaptive rounding algorithm for these Hessians that comes with theoretical guarantees. Empirically, YAQA reduces the KL divergence to the original model by 30% over LDLQ and GPTQ, setting new benchmark in post training quantization performance."
        },
        {
            "title": "Acknowledgments",
            "content": "A.T. was supported by the NSF Graduate Research Fellowship. C.D. was supported by DARPA D24AP00259-00. We thank Together AI for compute resources."
        },
        {
            "title": "References",
            "content": "[1] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms, 2024. [2] Jerry Chee, Megan Renz, Anil Damle, and Christopher De Sa. Model preserving compression for neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https:// openreview.net/forum?id=gt-l9Hu2ndd. [3] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit quantization of large language models with guarantees. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=xrk9g5vcXR. [4] Jerry Chee, Arturs Backurs, Rainie Heck, Li Zhang, Janardhan Kulkarni, Thomas Rothvoss, and Sivakanth Gopi. Discquant: quantization method for neural networks inspired by discrepancy theory, 2025. URL https://arxiv.org/abs/2501.06417. [5] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data. [6] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language models via additive quantization, 2024. [7] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=tcbBPnfwxS. [8] Thomas George, César Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast approximate natural gradient descent in kronecker factored eigenbasis. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/ 48000647b315f6f00f913caa757a70b3-Paper.pdf. [9] Christian Gourieroux and Alain Monfort. Statistics and Econometric Models. Number 9780521471626 in Cambridge Books. Cambridge University Press, December 1995. URL https://ideas.repec.org/b/cup/cbooks/9780521471626.html. [10] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 18421850. PMLR, 1015 Jul 2018. URL https://proceedings.mlr.press/v80/ gupta18a.html. [11] Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53 (2):217288, 2011. [12] Babak Hassibi, David Stork, and Gregory Wolff. Optimal brain surgeon: Extensions and performance comparisons. In J. Cowan, G. Tesauro, and J. Alspector, editors, Advances in Neural Information Processing Systems, volume 6. Morgan-Kaufmann, URL https://proceedings.neurips.cc/paper_files/paper/1993/file/ 1993. b056eb1587586b71e2da9acfe4fbd19e-Paper.pdf. [13] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/ abs/2203.15556. 11 [14] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv, 2023. [15] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. [16] Frederik Kunstner, Lukas Balles, and Philipp Hennig. Limitations of the empirical fisher approximation for natural gradient descent, 2020. URL https://arxiv.org/abs/1905. 12558. [17] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1927419286. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/ leviathan23a.html. [18] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration, 2023. [19] Charles F.Van Loan. The ubiquitous kronecker product. Journal of Computational and Applied Mathematics, 123(1):85100, 2000. ISSN 0377-0427. doi: https://doi.org/10.1016/ S0377-0427(00)00393-9. URL https://www.sciencedirect.com/science/article/ pii/S0377042700003939. Numerical Analysis 2000. Vol. III: Linear Algebra. [20] Vladimir Malinovskii, Denis Mazur, Ivan Ilin, Denis Kuznedelev, Konstantin Pavlovich Burlachenko, Kai Yi, Dan Alistarh, and Peter Richtárik. PV-tuning: Beyond straight-through estimation for extreme LLM compression. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id= YvA8UF0I37. [21] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored apIn Francis Bach and David Blei, editors, Proceedings of the 32nd proximate curvature. International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 24082417, Lille, France, 0709 Jul 2015. PMLR. URL https: //proceedings.mlr.press/v37/martens15.html. [22] meta llama. llama3. https://github.com/meta-llama/llama3, 2024. [23] Depen Morwani, Itai Shapira, Nikhil Vyas, eran malach, Sham M. Kakade, and Lucas Janson. new perspective on shampoos preconditioner. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=c6zI3Cp8c6. [24] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 71977206. PMLR, 1318 Jul 2020. URL https://proceedings.mlr.press/v119/nagel20a.html. [25] Markus Nagel, Marios Fournarakis, Yelysei Bondarenko, and Tijmen Blankevoort. Overcoming oscillations in quantization-aware training. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 1631816330. PMLR, 1723 Jul 2022. URL https://proceedings.mlr.press/ v162/nagel22a.html. [26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. URL https://arxiv.org/abs/1912.01703. [27] K. B. Petersen and M. S. Pedersen. The matrix cookbook, nov 2012. URL http://www2. compute.dtu.dk/pubdb/pubs/3274-full.html. Version 20121115. 12 [28] Neil Sloane. Hadamard Matrices neilsloane.com. http://neilsloane.com/hadamard/. [Accessed 02-02-2024]. [29] Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. In Workshop on Efficient Systems for Foundation Models @ ICML2023, 2023. URL https://openreview.net/forum?id=tz9JV2PRSv. [30] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786. [31] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks, 2024. [32] Albert Tseng, Qingyao Sun, David Hou, and Christopher De Sa. QTIP: Quantization with trellises and incoherence processing. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=7sdkLVuYCU. [33] Albert Tseng, Tao Yu, and Youngsuk Park. Training LLMs with MXFP4. In The 28th International Conference on Artificial Intelligence and Statistics, 2025. URL https://openreview. net/forum?id=a8z5Q0WSPL. [34] Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, and Sham M. Kakade. SOAP: Improving and stabilizing shampoo using adam for language modeling. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=IDxZhXrpNf. [35] Charlene Yang, Yunsong Wang, Steven Farrell, Thorsten Kurth, and Samuel Williams. Hierarchical roofline performance analysis for deep learning applications, 2020. URL https: //arxiv.org/abs/2009.05257. [36] Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Diederik Kingma, Yinyu Ye, Zhi-Quan Luo, and Ruoyu Sun. Adam-mini: Use fewer learning rates to gain more. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=iBExhaU3Lc. [37] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. URL https://arxiv.org/abs/ 2304.11277."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Experimental Setup and Implementation Details All Hessians were collected using the RedPajama v1 dataset [5]. We use Hadamard matrices from Neil Sloanes website for the base Hadamard matrix in incoherence processing as described in Section A.3 [28]. We use the OPTQ Wikitext2 and C4 dataset splits for KL divergence and perplexity calculation [7]. We use sequence length of 8192 for all KL divergence and perplexity evaluations. We evaluate all zeroshot results with the chat template applied. For finetuning experiments, we use the same setup as QTIP except that we normalize the activation error due to numerical instability from the default Adam ϵ = 108. For all Llama 3.1 70B Instruct experiments, we do not quantize the 0_v layer. The Gemma QAT comparison was run with the google/gemma-3-12b-it-qat-q4_0-gguf model on Huggingface, which was dequantized with the 4.52.0.dev0 nightly version of Transformers. Our code will be available on Github at later date. A.2 Memory and Compute Requirements YAQA requires storing O(n2) memory for HI and O(m2) memory for HO. In practice, we only need to store the lower triangular parts of HI and HO since they are symmetric. LDLQ and other layerwise activation methods only need to store HI , so YAQA requires roughly double the storage for the Hessians. We found that computing and storing the Hessians in FP32 was sufficient to maintain numerical stability and positive-definiteness with small regularization factor on the diagonal ( 104 tr(H)/n), but using TF32 for computations was not. The computational cost of computing Hessians with Sketch is O(btmn + bn2m + bm2n) on top of the cost of forward pass and backprop, where is the number of sequences and is the average number of tokens per sequence. The computational cost of computing Hessians with Sketch is O(pbt(n2 + m2)), again on top of forward pass and backward pass, where is the number of steps of power iteration. Since we must use larger for Sketch than Sketch to get acceptable variance, Sketch is empirically slower than Sketch A. A."
        },
        {
            "title": "Incoherence Processing with the Random Hadamard Transform",
            "content": "Although we describe incoherence processing for Hessians in the main body, incoherence processing also modifies the weights. The full definition of incoherence, including weight incoherence, is as follows: Definition 2 (Chee et al. [3]). Hessian Rnn is µ-incoherent if its eigendecomposition n. weight matrix Rmn is = QΛQT has maxi,j Qij = maxi,j µ-incoherent if maxi,j Wij = maxi,j eT Qej µ/ ej µW / mn. eT Incoherence processing with the RHT applies Random Hadamard Transformation on and H. Let be Hadamard matrix, defined as follows: H1 = [1] Hn = (cid:20)Hn1 Hn1 Hn1 Hn1 (cid:21) 1 2 (12) Then, the RHT performs Hlog2 nSx, where is random sign vector {1}n, Rn, and is power of 2. For non power-of-2 n, we follow QuIP# and use fixed small Hadamard matrix as base instead of H1. Full proofs for bounds on the behavior of the RHT can be round in QuIP#. A.4 YAQA Rounding Algorithm The YAQA rounding algorithm can be written as fixed point iteration (Algorithm 1). It can also be implemented iteratively (Python code), which is faster for slower quantizers than the fixed point iteration implementation above. 15 Algorithm 1 YAQA Rounding Algorithm Fixed Point Iteration Require: Weight matrix Rmn, p.d. HO Rmm, p.d. HI Rnn, quantizer Q, quantizer block sizes gx Z+m, gy Z+n. 1: LO, DO BlockLDL(HO, gx) 2: LI , DI BlockLDL(HI , gy) 3: ˆW Q(W ) 4: converged false 5: ˆW ˆW 6: while not converged do 7: ˆW ˆW = Q(W + LT 8: converged ( ˆW == ˆW ) 9: ˆW ˆW 10: 11: end while 12: return Quantized ˆW . OW LI + LT OW + LI ) def YAQA_i terati ve (W , Lin , Lout , td_x , td_y , cb ) : , = . shape hatW = torch . zeros_like ( ) Qidxs = torch . zeros (m , , dtype = cb . idx_dtype , device = . device ) assert % td_x == 0 and % td_y == 0 starts = [*[( // td_x -i -1 , // td_y -1) for in range ( // td_x ) ] , *[(0 , // td_y -i -1) for in range ( // td_y ) ]] for in tqdm ( range ( // td_x + // td_y ) ) : target = [] target_idx = [] start = starts [ ] jmax = max ( start [0] , start [1]) jm , jn = start while 0 <= jm < // td_x and 0 <= jn < // td_y : thing = [ jm * td_x :( jm +1) * td_x , jn * td_y :( jn +1) * td_y ] + ( Lout [ jm * td_x : , jm * td_x :( jm +1) * td_x ]. @ ( [ jm * td_x : , jn * td_y :] - hatW [ jm * td_x : , jn * td_y :]) @ Lin [ jn * td_y : , jn * td_y :( jn +1) * td_y ] + Lout [ jm * td_x : , jm * td_x :( jm +1) * td_x ]. @ ( [ jm * td_x : , jn * td_y :( jn +1) * td_y ] - hatW [ jm * td_x : , jn * td_y :( jn +1) * td_y ]) + ( [ jm * td_x :( jm +1) * td_x , jn * td_y :] - hatW [ jm * td_x :( jm +1) * td_x , jn * td_y :]) @ Lin [ jn * td_y : , jn * td_y :( jn +1) * td_y ]) target . append ( thing ) target_idx . append (( jm , jn ) ) jm += 1 jn -= 1 target = torch . stack ( target , dim =0) . reshape ( -1 , td_x * td_y ) qtarget , target_idx = cb . quantize ( target ) qtarget = qtarget . reshape ( -1 , td_x , td_y ) target_idx = target_idx . reshape ( -1 , td_x , td_y ) for in range ( len ( target_idx ) ) : jm , jn = target_idx [ ] hatW [ jm * td_x :( jm +1) * td_x , jn * td_y :( jn +1) * td_y ] = qtarget [ ] Qidxs [ jm * td_x :( jm +1) * td_x , jn * td_y :( jn +1) * td_y ] = target_idx [ ] return hatW , Qidxs A.5 Proofs Theorem 3. Let HO Rmm and HL Rnn be two positive definite matrices and let perform nearest or stochastic rounding independently on blocks of gx gy with E[(Q(vec(x) vec(x)))(Q(vec(x)) vec(x))T ] σ2I. Furthermore, let be the output of Equation 10 with LO and LI from the gx and gy-block LDL decompositions of HO and HI , respectively. Then, (HO HI )W tr(DI ) tr(DO)gxgyσ2 µ2 gxgyµ2 mn tr(H 1/2 )2 tr(H 1/2 )2σ2 where = and µO, µI are the incoherences of HO, HI (Definition 2). Proof. Let η = + LT OW LI + LT = (LO + I)T (LI + I) OW + LI (13) (14) (15) (16) (17) (18) Then, (HO HI )W = tr(W HI HO) = tr(W (LI + I)DI (LI + I)T (LO + I)HO(LO + I)T ) = tr(ηDI ηT DO) = tr(ηT η(DO DI ))"
        },
        {
            "title": "Since",
            "content": "η = + LT = + LT = Q() OW LI + LT OW LI + LT OW + LI OW + LI Q(W + LT OW LI + LT OW + LI ) (19) (20) and operates independently on gx gy-sized blocks, we have that tr(ηT η(DO DI )) gxgyσ2 tr(DO DI ) = gxgyσ2 tr(DO) tr(DI ). From Chee et al. [3], we have that for arbitrary p.d. Rkk, so tr(D) µ tr(H 1/2)2 tr(DO) tr(DI )gxgyσ2 µ2 gxgyµ2 mn tr(H 1/2 )2 tr(H 1/2 )2σ2 (21) (22) Theorem 1. Let HO Rmm and HL Rnn be two positive definite matrices and let perform nearest or stochastic rounding with E[(Q(x) x)2] σ2. Furthermore, let be the output of Equation 10 with LO and LI from the LDL decompositions of HO and HI , respectively. Then, (HO HI )W tr(DI ) tr(DO)σ2 µ2 µ2 mn tr(H 1/2 )2 tr(H 1/2 )2σ where = and µO, µI are the incoherences of HO, HI (Definition 2). Proof. This follows from setting gx = gy = 1 in Theorem 3. Lemma 1. Let A, Rnn be positive definite matrices and Rn be vector. Then, (cid:12) (cid:12)xT (cid:12) 2 2c where A,B xT (cid:12) (cid:12) (cid:12) AB = c. Proof. (cid:12) (cid:12) (cid:12) (cid:12) xT xT (cid:12) (cid:12) (cid:12) (cid:12) = xT (cid:12) (cid:12) (cid:12) (cid:12) (cid:13) (cid:13) (cid:13) (cid:13) x2 (cid:18) B 2 2c (cid:19) (cid:12) (cid:12) (cid:12) (cid:12) x2 B (cid:13) (cid:13) (cid:13) (cid:13)F (23) (24) (25) Theorem 2. Let Rmnmn = L be the Hessian of linear layer with respect to the KL divergence to the original model outputs and let everything else be defined as in Theorem 1. Then, HW (cid:18) 2 2 2c + µ2 µ2 mnHI HO tr(H 1/2 )2 tr(H 1/ )2σ2 (cid:19) where = H,HOHI HHOHI is the cosine simliarity between and HO HI . 17 Proof. From Lemma 1, we have that (cid:12) (cid:12) (cid:12) (cid:12) HW W (HO HI )W HOHI (cid:12) (cid:12) (cid:12) (cid:12) 2 2 2c. Then, HW W HW (cid:18) 2 (cid:18) 2 2 2c + 2 2c + (cid:19) (HO HI )W HOHI µ2 µ2 mnHI HO tr(H 1/2 )2 tr(H 1/2 )2σ (cid:19) . (26) (27) (28) A.6 Modified Pytorch Backward Pass to Compute Sketch class LinearNoBias ( torch . autograd . Function ) : @staticmethod @torch . amp . custom_fwd ( device_type = cuda ) def forward ( ctx , input , weight , mode , parent_class ) : ctx . sa _f or _ bac wa rd ( input , weight ) ctx . mode = mode ctx . parent_class = parent_class return input @ weight . @staticmethod @torch . amp . custom_bwd ( device_type = cuda ) def backward ( ctx , grad_output ) : it , reset , div = ctx . mode is_buffer = local_rank == ctx . parent_class . buffer_dev input , weight = ctx . saved_tensors ws = weight . shape grad_input = grad_output @ weight del weight if ctx . parent_class . collect_hess : grad_output = grad_output . reshape ( -1 , grad_output . shape [ -1]) input = input . reshape ( -1 , input . shape [ -1]) op_dtype = ctx . parent_class . op_dtype with torch . amp . autocast ( cuda , enabled = False ) : grad_output = grad_output . float () input = input . float () bs = input . shape [0] if it == 0: del grad_output if reset and is_buffer : ctx . parent_class . hin . mul_ (0) in_hess = sym_to_flat ( input . @ input ) / ctx . parent_class . scale del input torch . distributed . reduce ( in_hess , ctx . parent_class . buffer_dev , op = ReduceOp . AVG ) if is_buffer : ctx . parent_class . hin . add_ ( in_hess . to ( ctx . parent_class . hin . device ) . to ( op_dtype ) ) ctx . parent_class . ct += bs / ctx . parent_class . scale if div : ctx . parent_class . hin . div_ ( ctx . parent_class . ct ) ctx . parent_class . ct = 0 del in_hess torch . cuda . empty_cache () else : if it % 2 == 0: if reset and is_buffer : ctx . parent_class . hin . mul_ (0) if not is_buffer : out_features + 1) //2 , dtype = op_dtype , device = local_rank ) else : out_hess = ctx . parent_class . hout . to ( local_rank ) out_hess = torch . zeros ( ctx . parent_class . out_features * ( ctx . parent_class . torch . distributed . broadcast ( out_hess , ctx . parent_class . buffer_dev ) out_hess = flat_to_sym ( out_hess , ws [0]) . float () in_hess = input . @ ( input * (( grad_output @ out_hess ) * grad_output ) . sum ( dim = -1 , keepdims = True ) ) / out_hess . norm () **2 del input , grad_output , out_hess in_hess = sym_to_flat ( in_hess ) / ctx . parent_class . scale torch . distributed . reduce ( in_hess , ctx . parent_class . buffer_dev , op = ReduceOp . AVG ) if is_buffer : ctx . parent_class . hin . add_ ( in_hess . to ( ctx . parent_class . hin . device ) . to ( op_dtype ) ) 18 ctx . parent_class . ct += bs / ctx . parent_class . scale if div : ctx . parent_class . hin . div_ ( ctx . parent_class . ct ) ctx . parent_class . ct = 0 del in_hess else : if reset and is_buffer : ctx . parent_class . hout . mul_ (0) if not is_buffer : in_features + 1) //2 , dtype = op_dtype , device = local_rank ) else : in_hess = torch . zeros ( ctx . parent_class . in_features * ( ctx . parent_class . in_hess = ctx . parent_class . hin . to ( local_rank ) torch . distributed . broadcast ( in_hess , ctx . parent_class . buffer_dev ) in_hess = flat_to_sym ( in_hess , ws [1]) . float () out_hess = grad_output . @ ( grad_output * (( input @ in_hess ) * input ) . sum ( dim = -1 , keepdims = True ) ) / in_hess . norm () **2 del input , grad_output , in_hess out_hess = sym_to_flat ( out_hess ) / ctx . parent_class . scale torch . distributed . reduce ( out_hess , ctx . parent_class . buffer_dev , op = ReduceOp . AVG if is_buffer : ctx . parent_class . hout . add_ ( out_hess . to ( ctx . parent_class . hout . device ) . to ( ) op_dtype ) ) ctx . parent_class . ct += bs / ctx . parent_class . scale if div : ctx . parent_class . hout . div_ ( ctx . parent_class . ct ) ctx . parent_class . ct = 0 del out_hess torch . cuda . empty_cache () return grad_input . to ( local_rank ) , None , None , None A.7 Modified Backward Pass to Compute Sketch class LinearNoBias ( torch . autograd . Function ) : @staticmethod @torch . amp . custom_fwd ( device_type = cuda ) def forward ( ctx , input , weight , mode , parent_class ) : ctx . sa _f or _ bac wa rd ( input , weight ) ctx . mode = mode ctx . parent_class = parent_class return input @ weight . @staticmethod @torch . amp . custom_bwd ( device_type = cuda ) def backward ( ctx , grad_output ) : it , reset , div = ctx . mode is_buffer = local_rank == ctx . parent_class . buffer_dev input , weight = ctx . saved_tensors ws = weight . shape grad_input = grad_output @ weight del weight if ctx . parent_class . collect_hess : op_dtype = ctx . parent_class . op_dtype bs = input . shape [0] with torch . amp . autocast ( cuda , enabled = False ) : if it == 0: if reset and is_buffer : ctx . parent_class . hin . mul_ (0) grad_output = grad_output . float () input = input . float () in_hess = sym_to_flat ( torch . einsum ( btm , btn , bsm , bsk - > nk , grad_output , input , grad_output , input ) ) handle_in = torch . distributed . reduce ( in_hess , ctx . parent_class . buffer_dev , op = ReduceOp . AVG , async_op = True ) out_hess = sym_to_flat ( torch . einsum ( btm , btn , bsk , bsn - > mk , grad_output , input , grad_output , input ) ) handle_out = torch . distributed . reduce ( out_hess , ctx . parent_class . buffer_dev , op = ReduceOp . AVG , async_op = True ) del grad_output , input handle_in . wait () handle_out . wait () if is_buffer : op_dtype ) ) ctx . parent_class . hin . add_ ( in_hess . to ( ctx . parent_class . hin . device ) . to ( op_dtype ) ) ctx . parent_class . hout . add_ ( out_hess . to ( ctx . parent_class . hout . device ) . to ( ctx . parent_class . ct += bs if div : ctx . parent_class . hin . div_ ( ctx . parent_class . ct ) ctx . parent_class . hout . div_ ( ctx . parent_class . ct ) ctx . parent_class . ct = 0 19 Table 5: Full zeroshot accuracy results for Table 1. Higher is better MODEL ALGO. QUANT. 0-SHOT ACC ARCC ARCE BOOLQ HSWAG PIQA 3.1 70B INST 3.1 8B INST 3.2 3B INST 3.2 1B INST BF16 56.40 75.34 62. 61.51 82.92 LDLQ YAQA-A YAQA-B QTIP 2 QTIP 3 QTIP QTIP 2 QTIP 3 QTIP 4 QTIP 2 QTIP 3 QTIP 4 52.05 56.06 55.72 52.82 56.07 56.06 54.44 55.72 56.31 73.91 75.88 75. 74.07 75.88 75.42 73.44 74.33 76.09 62.17 62.23 62.29 62.17 62.20 62.17 62.17 62.17 62.29 58.10 61.00 61. 58.27 61.84 61.11 59.24 60.91 61.10 81.01 82.70 83.08 82.26 82.46 82.75 81.66 83.08 82.86 BF 51.37 78.03 82.05 57.74 79.92 LDLQ YAQA-A YAQA-B QTIP 2 QTIP 3 QTIP 4 QTIP 2 QTIP 3 QTIP 4 QTIP 2 QTIP 3 QTIP 4 41.89 50.34 50. 45.39 49.83 50.34 44.20 51.02 50.94 74.28 77.61 78.07 73.91 77.23 78.32 75.08 77.86 78.49 67.98 82.05 80. 70.34 81.76 82.45 70.64 81.04 82.02 51.67 56.57 57.32 52.59 56.85 57.35 52.91 57.24 57.47 76.71 79.87 79. 78.07 79.76 79.65 78.78 79.38 79.98 BF16 44.45 71.42 76. 51.19 75.73 LDLQ YAQA-A YAQA-B QTIP 2 QTIP 3 QTIP QTIP 2 QTIP 3 QTIP 4 QTIP 2 QTIP 3 QTIP 4 32.00 41.72 43.52 38.65 42.15 42.66 38.05 42.75 43.60 58.50 70.66 71. 66.46 69.70 70.24 68.22 69.95 71.04 69.42 74.19 75.93 66.76 74.92 76.57 68.75 75.66 75.96 45.57 50.46 51. 45.98 50.04 50.85 47.11 50.38 50.74 72.91 75.24 76.22 73.61 75.73 76.06 73.39 74.97 75.19 BF 32.85 57.24 66.09 44.74 73.01 LDLQ YAQA-A YAQA-B QTIP 2 QTIP 3 QTIP 4 QTIP 2 QTIP 3 QTIP 4 QTIP 2 QTIP 3 QTIP 4 27.30 29.44 30. 27.59 32.17 32.51 27.47 31.31 31.48 51.60 54.42 54.76 51.55 56.02 56.86 54.46 56.44 55.18 61.71 65.47 65. 62.84 65.47 64.37 62.51 62.60 64.86 38.66 43.31 43.70 39.28 43.51 44.33 39.89 43.20 43.42 68.72 71.82 72. 68.86 72.14 72.80 70.18 73.50 74.27 del in_hess , out_hess torch . cuda . empty_cache () torch . cuda . empty_cache () return grad_input . to ( local_rank ) , None , None , None 20 Table 6: Results without incoherence processing. All results are without finetuning and use an INT4 quantizer with 16-bit groupwise scale shared across 32 contiguous elements (4.5 bits per weight amortized). ALGO. DKL PPL 0-SHOT ACC W2 C4 AVG ARCC ARCE BOOLQ HSWAG PIQA BF16 0 6.50 8.02 69.82 51. 78.03 82.05 57.74 79.92 LDLQ 0.033 YAQA-A 0.025 YAQA-B 0. 6.75 8.21 68.35 6.67 8.17 69.36 6.65 8.15 68.95 49.74 49.74 50.68 77.36 77.65 78.20 78.35 81.62 79.72 56.83 57.16 57.06 79.49 80.63 79. A.8 Additional Results Table 5 contains full zeroshot results from the no finetuning table in the main body. Table 6 shows an experiment without incoherence processing. All quantized models in this table use an INT4 quantizer with 16 bit groupwise absmax scale shared across 32 contiguous elements, giving an effective 4.5 bits per weight. We used gx = 1 and gy = 32 for all methods. Even with out incoherence processing, YAQA outperforms LDLQ."
        }
    ],
    "affiliations": [
        "Cornell University"
    ]
}