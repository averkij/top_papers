{
    "paper_title": "Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization",
    "authors": [
        "Vage Egiazarian",
        "Roberto L. Castro",
        "Denis Kuznedelev",
        "Andrei Panferov",
        "Eldar Kurtic",
        "Shubhra Pandit",
        "Alexandre Marques",
        "Mark Kurtz",
        "Saleh Ashkboos",
        "Torsten Hoefler",
        "Dan Alistarh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent hardware-accelerated microscaling 4-bit floating-point formats such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to revolutionize large language model (LLM) inference. Yet, their practical benefits remain unproven. We present the first comprehensive study of MXFP4 and NVFP4 for post-training quantization, revealing gaps between their promise and real-world performance. Our analysis shows that state-of-the-art methods struggle with FP4, due to two key issues: (1) NVFP4's small group size provably neutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two scale quantization severely degrades accuracy due to high induced error. To bridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the classic GPTQ quantization algorithm that tailors the quantization process to FP4's unique properties, by using block-wise Hadamard transforms and format-specific optimizations. We support our proposal with a set of high-performance GPU kernels that enable the MR-GPTQ format with negligible overhead, by rotation fusion into the weights, and fast online computation of the activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and 2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on RTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches or outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the point where it nears that of NVFP4. We conclude that, while FP4 is not an automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock a new frontier of accuracy-performance trade-offs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 2 0 2 3 2 . 9 0 5 2 : r BRIDGING THE GAP BETWEEN PROMISE AND PERFORMANCE FOR MICROSCALING FP4 QUANTIZATION Roberto L. Castro ISTA & Red Hat AI Vage Egiazarian ISTA Denis Kuznedelev Yandex Research Andrei Panferov ISTA Eldar Kurtic ISTA & Red Hat AI"
        },
        {
            "title": "Mark Kurtz\nRed Hat AI",
            "content": "Saleh Ashkboos ETH Zürich Torsten Hoefler ETH Zürich Dan Alistarh ISTA & Red Hat AI"
        },
        {
            "title": "ABSTRACT",
            "content": "The recent hardware-accelerated microscaling 4-bit floating-point formats such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to revolutionize large language model (LLM) inference. Yet, their practical benefits remain unproven. We present the first comprehensive study of MXFP4 and NVFP4 for post-training quantization, revealing gaps between their promise and real-world performance. Our analysis shows that state-of-the-art methods struggle with FP4, due to two key issues: (1) NVFP4s small group size provably neutralizes traditional outlier mitigation techniques; (2) MXFP4s power-of-two scale quantization severely degrades accuracy due to high induced error. To bridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), variant of the classic GPTQ quantization algorithm that tailors the quantization process to FP4s unique properties, by using block-wise Hadamard transforms and format-specific optimizations. We support our proposal with set of high-performance GPU kernels that enable the MRGPTQ format with negligible overhead, by rotation fusion into the weights, and fast online computation of the activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and 2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on RTX5090. An extensive empirical evaluation demonstrates that MR-GPTQ matches or outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the point where it can near the accuracy of NVFP4. We conclude that, while FP4 is not an automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock new frontier of accuracy-performance trade-offs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Post-training quantization (PTQ) [39; 20; 34] is one of the most well-researched areas in model compression, in which the objective is to take an existing pre-trained model and reduce its size or computation while preserving most of its accuracy. With the advent of large language models (LLMs), PTQ has become highly-active research area, e.g., [20; 57; 3; 14; 48; 18; 49] with significant industry adoption and practical impact [28]. In this paper, we focus on quantization using the recently-introduced microscaling floating-point precision formats, specifically MXFP4 [45] and NVFP4 [41]. In nutshell, these formats work by grouping elements into blocks of 32 or 16 elements, respectively, quantized together with shared scale; to reduce the storage overhead, the scales themselves are also compressed, to distinct 8-bit format: standard sharing between Exponent and Mantissa bits (E4M3) for NVFP4, and E8M0essentially, rounding scales to powers-of-twofor MXFP4. As such, the NVFP4 format trades off additional space (4.5 bits per element on average, relative to 4.25 bits for MXFP4), in favor Equal contributions. Alphabetical ordering. Correspondence to: dan.alistarh@ist.ac.at. 1 of additional precision. The promise of these formats is two-fold: first, they are claimed to be more accurate than the prior-generation integer precision formats such as INT4 [38]. Second, they are supported in hardware: NVIDIA Blackwell GPUs support matrix multiplications across both NVFP and MXFP formats, whereas AMD GPUs will support MXFP4 [1]. Despite these developments, little is known about the accuracy of these formats on real models or their practical performance. Contributions. In this paper, we provide first thorough study of the accuracy and performance limitations of the NVFP4 and MXFP4 formats through the prism of current state-of-the-art quantization methods, coupled with computational support. We focus primarily on weight-and-activation quantization to 4-bits per parameter, and investigate the interaction between these new formats, real parameter distributions, and state-of-the-art quantization algorithms. Our main findings are: We begin with an analysis of quantization error induced by the NVFP4 and MXFP4 formats over both Laplace-like heavy-tailed distributions, which arise in real-world weights and activations [2; 16], and over Normal parameter distributions, arising when processing weights and activations via rotations in popular methods such as QuIP/QuIP# [6; 48] or QuaRot [4]. Interestingly, we can prove analytically and show empirically that rotations improve MXFP4 accuracy, but hurt NVFP4 accuracy when coupled with standard Round-to-Nearest (RTN) quantization. Based on this analysis, we propose new variant of the GPTQ weight quantization algorithm [20], called Micro-Rotated-GPTQ (MR-GPTQ), explicitly designed to maximize accuracy across both MXFP4 and NVFP4. The algorithm employs Hadamard rotations at the group level to normalize weights and activations, but in novel block-wise fused form, which, as we show, can be supported without any runtime overheads on Blackwell GPUs. In addition, MR-GPTQ introduces new efficient variant of the activation re-ordering heuristic for GPTQ, along with format-specific scale search optimizations. We perform the first extensive study of NVFP4 and MXFP4 practical accuracy, across standard Llama-3 [17] and Qwen-3 [53] models of different sizes, evaluated on standard zero-shot tasks [22]. We investigate broad set of compression methods, including RTN, GPTQ [20], SmoothQuant [52], QuaRot [4], and SpinQuant [36], as well as our new MR-GPTQ approach. Results show that: (1) both NVFP4 and MXFP4 are lossy, with MXFP4 inducing major accuracy drops ( 10% relative), and (2) that existing techniques are not well-suited for these new formats, as they do not always outperform RTN. On the positive side, we show that GPTQ and the MR-GPTQ variant yield consistently good recovery for NVFP4. Moreover, MR-GPTQ works particularly well in conjunction with MXFP4, recovering accuracy within 1-2% of NVFP4. For large models, we show that both formats can recover up to 98-99% of the baseline FP16 accuracy. Our main technical contribution is new set of GPU kernels specific to the Blackwell architecture called QuTLASS, showing that the micro-rotation component of MR-GPTQ can be supported without loss of performance relative to standard multiplications. Specifically, this comes in the form of lightweight fused kernel for online rotation of the activations. Remarkably, our kernel for MXFP4 can obtain higher throughput than an ideal NVFP4 matrix multiplication. Our kernels obtain near-ideal layer-wise speedups for both B200 and RTX5090 GPUs, of 3.6x and 6x, respectively, leading to end-to-end inference speedups of 2x and 4x, respectively."
        },
        {
            "title": "2 BACKGROUND ON MICROSCALING FLOATING-POINT FORMATS",
            "content": "General Definition. The microscaling MXFP4 and NVFP4 formats employ hierarchical quantization, where elements within block share common scale factor, enabling efficient hardware implementation. Given tensor divided into one-dimensional groups, we define Microscaling Block Floating-Point (MFP) representation as follows. The Group Size (G) is the number of elements in each group before quantization. The Element Representation (E) is the format used to represent the individual elements within each block. The Scale Representation (S): The format used to represent the scale values for each group. For floating-point (FP) formats, we use the notation ExMy to say that bits are allocated to the exponent, and bits are allocated to the mantissa. For instance, in the standard FP4 E2M1 representation, each FP4 element consists of 1 sign bit, 2 exponent bits, and 1 mantissa bit, providing 7 distinct positive values {0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 6.0} plus zero and the negatives. 2 Figure 1: Distribution fits for aggregate weights and activations of Llama-3.1-8B-Instruct, with and without rotations. The Normal distribution is clearly good fit for rotated weights and activations, while the Laplace distribution provides good fit for the native distributions. Although native weights appear Normal, they have much heavier tails, as evidenced by the Kurtosis value. The MXFP4 (Microscaling FP4) Format. This format [45] follows the specification (G = 32, = FP4, = E8M0). Its distinguishing features are the group size of 32 and its quantization of group scales to powers-of-two, given the use of E8M0, which dedicates all bits to the exponent and none to the mantissa. This design choice simplifies hardware multiplication; yet, as our experiments reveal, it often introduces quantization artifacts that can significantly impact model accuracy. The NVFP4 (NVIDIA FP4) Format was introduced by NVIDIA for the Blackwell architecture [41], and employs more flexible approach with (G = 16, = FP4, = E4M3). While sharing the FP4 element format with MXFP4, NVFP4 differs in two key aspects. First, it uses 16-element group size, and, second, it uses full FP8 representation for scales in E4M3, preserving more precise scaling information relative to E8M0. NVFP4 trades off more accurate representation for weight and activation distributions, at the cost of increased bits-per-element (4.5 NVFP4 vs 4.25 for MXFP4). Related Work. Early work on LLM quantization focused primarily on integer formats, with INT8 being the first to be investigated [12; 54], in conjunction with round-to-nearest (RTN) assignment over groups of consecutive weights and activations. FP formats introduce new possibilities but also challenges: while FP8 quantization is known to be near-lossless [28], the distribution of representable values in NVFP4/MXFP4 changes quantization dynamics. The GPTQ method [20] reached nearlossless INT4 compression via second-order weight adjustments. Its effectiveness for FP4 formats remains unexplored. Methods like AWQ [33], SqueezeLLM [27], and SpQR [14] relied on outlieraware quantization strategies that assume uniform grids and large group sizes. The FP4 formats small group sizes (16 or 32) and non-uniform grid inherently perform outlier mitigation, as we discuss in our analysis. Recent extreme compression techniques like QuIP [6], QuIP# [48] and QTIP [49] use rotation matrices to normalize the weight distributions. As we will see, this is not necessarily helpful for FP4 microscaling formats. LLM activations are known to be extremely challenging to quantize, due to outlier features, defined roughly as elements up to 100 larger than average [12]. SmoothQuant [52] addresses this for INT8 by rescaling to redistribute outliers between weights and activations. Recent rotation-based methods like QuaRot [4] and SpinQuant [36] mitigate outliers through Hadamard transforms. In this paper, we discover novel trade-offs for these approaches. Prior work investigating accuracy trade-offs under quantization, e.g., Yao et al. [54]; Liu et al. [35]; Huang et al. [26]; Gong et al. [24]; Li et al. [32]; Gong et al. [23]; Lee et al. [31]; Kurtic et al. [28] focuses almost exclusively on INT quantization. Despite industry claims about FP4s accuracy superiority [38; 41], rigorous evaluation remains absent so far, likely due to the recent introduction of this format. Our work addresses this gap."
        },
        {
            "title": "3 A QUANTIZATION ERROR ANALYSIS OF NVFP4 AND MXFP4",
            "content": "Prior work on quantization [39; 12; 15] identified the average and top-element (outlier) mean-square error (MSE) as key quantities that can predict quantized model accuracy. In this section, we perform model-based analysis of the NVFP4 and MXFP4 formats from the prism of these metrics. 3 Modeling Distributions. Early work on modeling LLM parameters assumed Normal (Gaussian) distribution [13], consistent with common initialization schemes. Yet, more recent studies have identified that distributions with high kurtosis, such as the Laplace or Student-t distributions, better model the sharp peaks and outlier-prone tails of weights and activations [2; 16]. Here, we follow the latter line of work and model weights and activations as following Laplace distribution. At the same time, interestingly, it can be proven that, after the Hadamard rotation, these tensors tend to follow normal distribution [6; 48]. We empirically validate these findings via fits over common models, illustrated in Figure 1. Formally, our modeling is as follows: Definition 1 (Modeling). Laplace distribution Laplace(0, b) with density fW (w) = 1 Var(W ) = 2b2. We fix unit variance throughout, so = 1/ Exp(λ) with rate λ = 1/b = We assume that the native weights and activations follow the 2b ew/b, and variance 2. The magnitude = is 2, that is fZ(z) = λeλz and FZ(z) = 1 eλz for 0. We assume that weights and activations rotated via the Hadamard transform follow Normal π ez2/2 2), 0, where where erf(z) is the standard Gauss error function distribution (0, 1). The magnitude = is half-normal with fZ(z) = and FZ(z) = erf(z/ 2 π (cid:82) 0 et2 (cid:113) dt. Microscaling Block Floating-Point (MFP) Quantization. We model quantization as follows. Consider i.i.d. blocks containing 2 elements drawn from some distribution: = (X1, . . . , XG) with Var(Xi) = 1 and Zi = Xi. We assume grid [0, 1] that is finite, symmetric around 0, and includes both 0 and 1; we write Q+ = [0, 1] and qmin = min(Q+ {0}). We use round-to-nearest (RTN) quantization, assuming probability 0 for rounding ties. Next, we formally define the scaling process. For simplicity, we will not not quantize the scale itself, and assume that values are normalized to [1, 1]. We remove these assumptions in our numerical validation (Section 3.2). Definition 2 (Scales). For block of elements X, we define the unquantized scale := max1iG Xi, the normalized entries Ui := Xi/s [1, 1], the quantized normalized entries (cid:98)Ui := RTNQ(Ui), and the de-normalized quantized values (cid:98)Xi := (cid:98)Ui. Definition 3 (Quantization Metrics). For group size G, we define: (i) The per-element MSE: MSE(G) := E[(X1 (cid:98)X1)2] (by symmetry, the choice of index can be arbitrary). (ii) The top- := element MSE per block: Let = arg max1iG Xi, ignoring ties. Define MSEtop(G) (cid:104) (XI (cid:98)XI )2(cid:105) . We always use the same MFP map, i.e. same scale s, for both metrics. Remark 1 (Quantization Dead-zone). The first positive quantization level in the grid Q, which we denote by qmin, induces the dead-zone half-width δ := qmin/2 on [0, 1]. If Ui < δ, then (cid:98)Ui = 0."
        },
        {
            "title": "3.1 ANALYTICAL MSE BOUNDS",
            "content": "Next, we derive bounds on quantization error across top and average elements. First, notice that, in simplified setting, applying the Hadamard rotation spreads the MSE evenly among elements. Lemma 1 (Top-Element MSE). Assume vector RG with coordinates i.i.d. (0, 1), to which we apply Hadamard rotation, perform MFP quantization in the y-domain to produce (cid:98)y, and reconstruct (cid:98)x = 1 (cid:98)y. Define the quantization error vectors εy = (cid:98)y and εx = (cid:98)x = 1 εy. The expected squared error on the original top coordinate = arg maxi xi is the per-element MSE: MSEtop(G) = E[(εx)2 ] ="
        },
        {
            "title": "1\nG",
            "content": "Eεy2 2 = MSE(G). Remark 2 (Outlier preservation). By contrast, it is immediate that MSEtop(G) = 0 in the absence of the Hadamard rotation, since we are doing absmax scaling, which preserves the top element. Asymptotic MSE Analysis. Thus, MSE is the key quantity we want to analyze. First, notice that, for any fixed grid with dead zone δ > 0, for both Laplace and Normal models, limG MSE(G) = Var(X1) = 1. Intuitively, this is because, as grows, the block maximum diverges, so U1 = X1/M 0 in probability; the mass that survives the dead-zone vanishes. Consequently, the dominant part of the MSE E[(X1 (cid:98)X1)2] becomes E[X 2 1 1{U1 < δ}] E[X 2 1 ] = 1. 4 To get more granular variant, we assume the large domain and examine the preserved mass: R(G) := 1 MSE(G) = E(cid:2)X 2 1 1{U1 δ}(cid:3), which captures the mass that escapes underflow. precise calculation yields the following: Lemma 2 (Rates). Let δ = qmin/2 (0, 1 2 ) be the dead-zone halfwidth in the normalized domain. For Laplace, we have: RL(G) = Θ(cid:0)(log G)2Gδ(cid:1), and for Normal: RN(G) = Θ(cid:0)(cid:112)log Gδ2(cid:1). Discussion. Since 0 < δ2 < δ < 1, we have that, for small G, the Laplace MSE should be below the MSE for the Normal distribution. Yet, for sufficiently large G, the Normal rate dominates the Laplace rate, meaning that MSEN(G) < MSEL(G). As such, we predict crossover phenomenon, where the MSE gap in favor of the (native) Laplace distribution will be inverted for larger group size in favor of the transformed Normal distribution. In short, transforms should hurt the original weights at small group sizes, and become effective as we increase it."
        },
        {
            "title": "3.2 NUMERICAL VALIDATION",
            "content": "Relative Errors. In practice, the weight and activation distributions are not of unit variance. Shared scales give us control over the variance during the quantization process, but the aggregation of the proposed quadratic errors will be dominated by groups with higher variance. To address this, when analyzing real weights and activations, we use the relative version of the errors proposed above. Definition 4 (Relative Metrics). Let = arg max1iG Xi be the top group element. We define the relative per-element MSE as MSErel(G) := E[ (Xi (cid:98)Xi)2/ 2 ], and the top-element (cid:80) i=1 top(G) := E[(XI (cid:98)XI )2/X 2 ]. (cid:80) i=1 MSE per block: MSErel MSErel is key metric in compression theory [47]; in the context of LLM compression, Malinovskii et al. [37] to present linear dependence between MSErel and end-to-end accuracy decline. Additionally, recent lattice-based PTQ methods explicitly optimize for MSErel when designing their lattice [48; 49; 37]. For MSErel top, Lemma 3 shows how it accurately reflects the outliers relative error as long as outliers are large, rare, randomly positioned, and MSErel top is consistent for outliers and non-outliers (as shown by the shared scale quantization analysis below). Figure 2 validates the analysis from Section 3.1 on samples from Laplace distribution, as well as on real weight and activation matrices from the Llama-3.1-8B-Instruct model. For MSErel (top row) and NVFP4 (G = 16), the Hadamard Transform has negative effect for small and positive effect for larger G, exactly as predicted. To interpret the other effects, we have to better understand the effect of the shared scales quantization. Shared Scales Quantization. Under fixed bit-width, microscaling floating point formats with shared scale (stored, e.g., in E8M0 or E4M3) trade range for accuracy. We begin our analysis by examining the range required to fully cover weights and activations. Figure 3 shows the logarithmic dynamic ranges of several FP8 formats and compares them with the empirical distributions of shared scales for weights and activations across multiple models. One can see that the dynamic range of = E4M3 covers the full range of these distributions. Trivially, = E8M0, having more range, can easily cover it too. When shared scales range is less than the dynamic range of S, they can always be represented by normal floating-point values with their relative error (a) bounded by 2M for mantissa precision and (b) translation-invariant to power-of-two shifts. For absmax quantization without rotations, this leads to MSErel tops being insensitive to the shared scale magnitude in expectation over high dynamic range intervals, and, as the results, to G. We formalize this in Lemma 4. This allows us to explain the effects of shared scale quantization on MSErel precision of the shared scales data type and the base data type E. We observe the following: top by relating it to the 1. For MXFP4, top values inherit their precision from the base data type, and not the shared scale data type. This is because = E8M0 is coarser than = E2M1, leading to shared scales inheriting effectively constant relative error from E2M1 regardless of G, as visible in Figure 2. Figure 2: The effect of Hadamard Transform (HT) on MXFP4 (E8M0) and NVFP4 (E4M3) quantization on Laplace distribution samples and Llama-3.1-8B-Instruct weights and activations for various group sizes. Figure 3: Ranges of FP8 scale format and observed weight and activation magnitudes. 2. By contrast, for NVFP4, shared scales inherit effectively constant relative error, regardless of G. This is because = E4M3 is finer than = E2M1, as visible in Figure 2. 3. Once the Hadamard Transform is applied, the maximum element error is spread across the whole group. This follows Lemma 1. From Figure 2, one can see that this leads to better precision than pure E2M1, but worse than pure E4M3. Moreover, one can see that for heavy-tailed distribution, such as Laplace or the observed model tensors, 2 grows faster than MSE(G) with G, leading to the error being reduced as we increase the group size G. Yet, this effect alone is not enough for it to improve over the E4M3 precision for reasonable group size G. Discussion. Our analysis so far showed that the MXFP4 format induces higher MSE for RTN quantization relative to NVFP4, and is worse at outlier preservation. At the same time, the format has lower memory and computational costs relative to NVFP4, and is likely to benefit from normalization via the Hadamard transform. By contrast, the NVFP4 format has lower MSE due to the smaller group size, and top value preservation as it is promoted to E4M3. In addition, the NVFP4 MSE may not benefit from normalizing transforms. In the following, we incorporate our analysis into the classic GPTQ algorithm, obtaining variant that is designed for FP4 formats, called MR-GPTQ."
        },
        {
            "title": "4 MR-GPTQ: AN FP4-FOCUSED VARIANT OF THE GPTQ ALGORITHM",
            "content": "Standard GPTQ. Given layers weights and calibration inputs X, GPTQ [20] aims to find quantized weights (cid:99)W that minimize the output reconstruction error: (cid:99)W XW 2 2. Assuming fixed quantization grid, GPTQ builds upon the Optimal Brain Quantization (OBQ) framework [21] to iteratively quantize and update remaining weights to compensate for the error leveraging second-order information, while avoiding OBQs high computational complexity. Specifically, while OBQ employs dynamic, greedy weight selection strategy for selecting the next weight to quantize, GPTQ observes that this greedy approach offers low benefits over quantizing weights in an arbitrary, fixed order, for heavily-parameterized layers. Thus, GPTQ quantizes weights across all rows in the same fixed order. This enables it to share the Hessian information, used to compute error updates, among rows. GPTQ typically implements this fixed order by processing the dimensions sequentially, column-by-column (front-to-back). The inverse Hessian must be updated only once per column (dcol times) rather than once per weight (drow dcol times), which reduces the overall computational complexity from O(drow d3 col}), providing orders-of-magnitude speedup, for weight matrix of size drow dcol. col) for OBQ, to O(max {drow d2 col, d"
        },
        {
            "title": "4.1 ADAPTING GPTQ TO FP4 FORMATS",
            "content": "Our analysis showed that, with RTN quantization, NVFP4 provides lower MSE relative to MXFP4, due to better outlier preservation and smaller group size. GPTQ induces an orthogonal direction in the design space, relative to RTN, as it allows for MSE error to be corrected by shifting it to other weight blocks. This suggests three general solution strategies: (1) GPTQ applied to the standard NVFP4 grid, with absmax scaling, leveraging the natural properties of NVFP4. This simply extends 6 RTN to GPTQ; (2) MR-GPTQ-MXFP4: GPTQ applied to the MXFP4 grid, on rotated weights and activations, as this reduces MSE for RTN; (3) MR-GPTQ-NVFP4: GPTQ on an MSE-optimized NVFP4 grid, with rotated weights and activations, which would focus on error minimization. While the first two approaches follow naturally from our analysis, the third approach wagers that the higher per-group local MSE caused by applying Hadamard rotations to NVFP4 can be compensated by optimizing the scales, together with the GPTQ updates. As such, options 2 and 3 would offer unified rotated/normalized format, that would apply to both NVFP4 and MXFP4. Next, we describe three key technical additions to the GPTQ algorithm that help bridge the gap between variants. Ingredient 1: MSE-Optimized Grids. Our first step in MR-GPTQ is to identify good initial grid. Recall that NVFP has both tensor (global) and per-group scales, which we denote by sT and sG, respectively. The quantized variant of the element Xi will be represented as ˆXi = sT sG Q(Xi/(sT sG)), where is the quantization operation. To minimize error, we solve the following optimization problem for each tensor, across its groups: min sT ,sG1 ,...,sGk (cid:88) ˆXi Xi2 2, where (sGi)i=1,k are the quantization scales for the groups. We solve this by using alternating optimization over the block scales and the per-tensor scale, respectively. For NVFP4 without rotations, we have found this to yield consistent improvements. For MXFP4 with rotations, we have found that single static value works stably across all layers, and further optimization does not yield improvements. We therefore use this approach in our implementation. Ingredient 2: Static Activation Reordering. The original GPTQ algorithm heuristically re-orders the weight columns following the dynamic act-order, i.e., descending order of the corresponding Hessian diagonal entries. This matrix shuffle is applied before the quantization grid and scales are computed. While this consistently improves accuracy, it also requires re-shuffling the matrix columns dynamically at runtime, which results in 10-20% end-to-end inference slow-down. Instead, we observe that we can apply the activation re-ordering statically, i.e. after the scales and the quantization grid have been computed in the first step, based on the original (arbitrary) column order. In practice, we first fix the grid and scales for each group, shuffle the columns before GPTQ is applied, and then finally shuffle the columns back, maintaining the microscaling group structure of the original matrix. Importantly, this benefits from the improved behaviour during the quantization process itself, without any runtime penalties. This can be applied to GPTQ over any grid, and provides similar improvements to standard dynamic act-order, without the runtime overheads. Ingredient 3: Fused Online Rotations. Our MR-GPTQ variants rotate the weights and activations via block-wise Hadamard transform Hk, with diagonal blocks, where is power-of-two. Mathematically, for linear layer with weights and activations X, both quantized, the operation occurs as Q(W Hk)Q(XHK)T , where Hk is the block-wise rotation, and is the quantization function. In the next section, we describe how this format can be supported efficiently at runtime."
        },
        {
            "title": "4.2 GPU KERNEL SUPPORT FOR MR-GPTQ VIA QUTLASS",
            "content": "To support the micro-rotation format described above, we introduce set of kernels optimized for NVIDIA Blackwell GPUs. These kernels constitute QuTLASS v1.0, high-performance library for low-precision deep learning quantization, building on NVIDIA CUTLASS [40]. QuTLASS provides full support for quantizationand matmul-related operations in both NVFP4 and MXFP4 microscaling formats. In addition, we release architecture-optimized implementations for different NVIDIA Blackwell compute capabilities, namely SM100 [42] and SM120 [44]. The kernels in QuTLASS can be grouped into two categories, which will handle the computation of Q(W Hk)Q(XHk)T : 1. Quantization-related kernels. While the product Hk is pre-fused in the weights, XHk occurs online. To avoid diminishing the benefits of FP4 hardware acceleration, QuTLASS provides lightweight fused kernels for online rotation. These kernels support unimodal block diagonal matrices with {16, 32, 64, 128}. We found experimentally that, for < 256, dense transformations remain memory-bound, meaning that any rotation (not just Hadamards) can be applied at essentially the same cost, as the full matrix can be loaded at runtime. (However, we found the Hadamard transform to yield best accuracy results, e.g., see Tables 12 and 13). To further reduce overhead, quantization and scale calculation are fused into the transformation kernel as custom 7 epilogue function. QuTLASS currently supports MSE and Abs-Max quantization methods, while its template-based design allows new methods to be easily integrated. 2. Matmul-related narrow precision kernels. Between FP4 quantization and matrix multiplication, hardware-mandated rearrangement of scaling factors is required [43] for tcgen05.mma. QuTLASS implements this step using Triton kernel. For the matmul itself, QuTLASS supports multiple backends, including CUTLASS [40] and FlashInfer [55], enabling flexible plug-and-play backend selection depending on workload and hardware."
        },
        {
            "title": "5.1 EXPERIMENTS WITH EMULATED QUANTIZATION",
            "content": "We first evaluate the highly-popular Llama 3.1-8B-Instruct model [17], examining the impact of quantizing both weights and activations for all linear layers in this model to the INT4 and FP4 formats, using different algorithms. To ensure compatibility, experiments are performed using simulated quantization in PyTorch. We use subset of tasks from the Open LLM Leaderboard V1 [5] for evaluation: GSM8K for grade school math [9], MMLU for world knowledge and reasoning [25; 8], Winogrande and HellaSwag for language understanding [46; 56]. (Other tasks in this harness yield similar scores across top methods.) The INT4 experiments use group size 32 with FP16 scales, matching the average bit-width of NVFP4. Algorithms. We consider both weights-and-activations quantization (W4A4, our main focus) and weight-only quantization (W4A16, as control). For W4A4, we implement the following: Round-to-nearest (RTN) quantization to the corresponding format, with absmax scales. In addition, we add Hadamard rotations matching the quantization group size (16 for NVFP and 32 for MXFP), denoted as RTN + HT. SmoothQuant [52] diagonal rescaling, with tuned α smoothening factor. We identified α = 0.6 to be the best in our experiments. QuaRot [4], which adds Hadamard rotations strategically at each linear layer. These should reduce quantization error, and most of them can be folded into the model. We use RTN for quantization post-rotation. SpinQuant [36], which adds trainable rotations to the model, similarly to QuaRot. subset of 1024 calibration sequences from FineWeb is used for training the matrices. GPTQ [20] weight quantization and RTN on the activations, with absmax scales. subset of 1024 calibration sequences from FineWeb, absmax scales, standard Hessian dampening factors (λ = 102), and standard quantization order are used. MR-GPTQ weight quantization, i.e., GPTQ with block rotations, MSE scale optimization, and static activation re-ordering over the rotated weights, as described in Section 4.1, with RTN on the activations. As control, we also implement weight-only quantization, via RTN, GPTQ, AWQ [34], as well as Hadamard rotations followed by RTN, denoted as RTN + HT. These results closely follow our findings for W&A quantization, and are thus deferred to the Appendix. In Appendix G, we perform an exhaustive sweep over DCT, DST, Hadamard, and GSR transforms and block sizes showing that the Hadamard transform matching the quantization group size provides the best results on average. Discussion. The accuracy results for W4A4 experiments on Llama-3.1-8B-Instruct are presented in Table 1. The variance for the NVFP4 experiments (i.e., for entries in the 7th column over 5 distinct seeds) is of approximately 0.3 average points, whereas the variance for the INT4 experiments is of approximately 1 point. We mark all top aggregate entries (within 2 standard deviations) as bold in the corresponding columns. We observe the following: 1. No Lossless Format: Across all formats, the accuracy drop is noticeable. The lowest average drop is for the NVFP4 format with SmoothQuant, GPTQ, or MR-GPTQ (these results are within variance of each other). The weight quantization results (Appendix Table 3), show that the induced error is roughly evenly split between weight and activation quantization. These results suggest that micro-scaling is not direct solution for accuracy recovery."
        },
        {
            "title": "Method",
            "content": "MMLU-CoT GSM8k HellaSwag WinoGrande Avg. Recovery %"
        },
        {
            "title": "Baseline",
            "content": "FP16 INT4 NVFP4 MXFP4 RTN RTN+HT GPTQ RTN RTN + HT QuaRot SpinQuant SmoothQuant GPTQ MR-GPTQ RTN RTN + HT SmoothQuant QuaRot SpinQuant GPTQ MR-GPTQ 72.76 65.96 68.30 66.36 68.26 67.41 66.50 66.50 68.90 68.85 69.12 62.21 62.38 63.93 49.86 61.80 63.49 67.19 85. 74.68 79.61 76.65 78.39 78.01 77.40 76.10 79.50 82.60 80.80 67.85 72.48 68.54 56.94 68.16 68.46 75.70 80.01 77.62 77.60 77.38 78.15 77.31 77.25 76.96 79.50 78.26 78. 73.99 75.29 75.10 73.50 74.87 76.01 76.91 77.90 74.19 73.48 72.48 74.11 73.48 75.14 75.32 74.70 74.51 75.24 73.24 71.67 73.56 71.43 72.93 74.51 74.80 78. 73.11 74.75 73.21 74.73 74.05 74.10 73.70 75.70 75.72 75.84 69.32 70.45 70.30 62.90 69.40 70.62 73.65 100 92.63 94.71 92.75 94.67 93.82 93.80 93.40 95.90 95.92 96. 87.83 89.26 89.06 79.70 88.00 89.47 93.31 Table 1: Unified accuracy comparison of Llama-3.1-8B-Instruct W4A4 under different quantization formats and methods. For each format, top methods within variance are marked in bold. 2. NVFP4 provides the best accuracy, with INT4 second, and MXFP4 third: On average, NVFP4 and INT4 quantization provide similar quality, with INT4 quantization having higher variance. The MXFP4 format is distant third in terms of accuracy, regardless of the method used, but benefits significantly from MR-GPTQ. 3. Quantization Method Efficiency: First, we note the good performance of standard RTN for INT4 (with rotations) and NVFP4 (without). Second, the Hadamard transform appears effective for INT4 and MXFP4 (which use group size 32), but is less effective for NVFP4 (which uses group size 16), aligning with our analysis. In particular, for round-to-nearest quantization, adding the Hadamard transform to NVFP4 hurts accuracy. Finally, the GPTQ and SmoothQuant methods appear to be consistently effective across all three formats."
        },
        {
            "title": "5.2 REAL QUANTIZATION EXPERIMENTS",
            "content": "Format Method 1B Llama3 3B 8B NVFP MXFP4 83.9 RTN 80.9 RTN+HT 85.7 GPTQ MR-GPTQ 87.3 86.1 QAT 87.1 QAT+HT 67.7 RTN 74.4 RTN+HT 68.4 GPTQ MR-GPTQ 79.8 68.2 QAT 84.5 QAT+HT 94.4 91.0 95.5 93.7 96.6 95.3 87.2 86.1 87.0 92.7 90.1 94.1 94.8 93.8 95.7 95.8 95.4 96. 88.1 89.3 89.7 93.3 92.3 95.4 70B 98.6 98.5 99.1 98.3 96.8 97.8 97.9 98.4 Qwen3 14B 98.5 98.1 98.7 98.9 96.3 96.0 96.2 97.3 32B 99.8 98.1 99.5 98.3 91.8 98.7 96.7 95.6 8B 98.9 96.0 98.1 97.4 97.8 98. 93.7 93.6 94.1 95.2 97.0 98.4 Figure 4: Recoveries with real quantization. Table 2: Per-model recoveries with real quantization. We integrate our kernels in vLLM [29], and perform accuracy evaluations directly in this setup over additional models, such as Llama-3.3-70B-Instruct [17], and the Qwen3 [53] family of models. The results are presented in Table 2 and aggregated in Figure 4. For this experiment, we also provide results for Quantization-Aware Training (QAT) performed using the balanced Generalized JensenShannon Divergence loss [19] between the quantized and the unquantized (frozen) model token distributions on subset of 92,995 samples (10%) from the Tülu 3 [30] instructions dataset. The results show that accuracies measured over real kernels for the Llama-3.1-8B-Instruct model track closely with the results from simulation, with slightly lower recoveries (within 0.2-0.3%). Smaller models (< 8B) and Llama-family models tend to have lower recovery rates, whereas Qwen3 models can achieve more than 99% average recovery in NVFP4. For NVFP4, standard GPTQ provides the highest recoveries on average, although RTN and MR-GPTQ are also competitive, with QAT only providing very limited benefits. For MXFP4, MR-GPTQ provides the best recovery among PTQ methods, while QAT consistently reduces the gap to full precision."
        },
        {
            "title": "5.3 COMPARISON BETWEEN VARIANTS ON THE PLATINUM BENCHMARK",
            "content": "Next, we perform detailed analysis between different versions of the GPTQ algorithm, on the two formats. As visible in Table 1, the standard evaluation harness struggles to distinguish variants, likely because of high noise in some of the evaluations. To address this, we examine the differences between GPTQ variants on the less noisy PlatinumBench benchmark [51], which includes carefully curated tasks and questions. These experiments are performed with real kernels, via our vLLM integration. Figure 5 include full results across tasks, for the following variants: Transform matrices: Hadamard with different block sizes, denoted by e.g. Had128. Scale optimization: Our approach (MSE) or the default (MinMax). Quantization ordering: Static Activation Ordering (ActOrder) or arbitrary/initial (Default). (a) NVFP4 accuracy across GPTQ variants. (b) MXFP4 accuracy across GPTQ variants. (c) NVFP4 averages and standard deviations. (d) MXFP4 averages and standard deviations. Figure 5: Comparison of NVFP4 and MXFP4 quantization methods on Platinum benchmark tasks. Top row shows accuracy results across different GPTQ/MR-GPTQ component combinations. Bottom row shows average recovery scores and standard deviations for each method. Discussion. We observe the following: The results suggest that Hadamard rotations provide statistically-significant advantage to our MR-GPTQ variant, with group-aligned Hadmard rotations (Had16), MSE and ActOrder, in the NVFP4 case as well. All other variants appear to be within variance of eachother on this benchmark, for NVFP4. We observe large gap ( > 4 points on average) between the top NVFP4 recovery (96.6%) and the top MXFP4 recovery (92.3%). Finally, the MXFP4 results show very large recovery gap between the variants with rotations and the variants without. Moreover, for MXFP4, larger Hadamard rotations (128 vs 32) appear to clearly help, whereas, for NVFP4, matching the rotation size to the group size appears ideal."
        },
        {
            "title": "5.4 KERNEL AND INFERENCE PERFORMANCE",
            "content": "Finally, we turn to computational performance. In Figure 6, we examine the performance of our kernels. On the left, we show throughput for single layer extracted from Llama-3.3-70B model using FlashInfer as backend. The curve labeled with ideal represents the upper bound for real 4-bit weight and 4-bit activation matrix multiplication, i.e., the measured matmul throughput not including the overhead of quantization-related operations. In contrast, the curves labeled actual show real measurements including the costs of Hadamards, quantization, and scale computation. The comparison highlights the small gap between idealized efficiency and practical implementations with our kernels, with speedups of up to 3.6 (out of 4) on B200 and 6 (out of 8) on RTX5090. Interestingly, MXFP4 outperforms NVFP4 on B200, with up to 15% higher throughput, despite their closely related numerical formats. Possible contributing factors include MXFP4s use of potentially more efficient power-of-two scales as well as larger group sizes, which could reduce overhead. On the right, the end-to-end speedup of vLLM running Llama-3.3-70B with MXFP4 quantization compared to the baseline BF16 implementation on single B200 GPU. The results demonstrate consistent performance gains across batch sizes, with speedups reaching up to 2.2 over the BF16 baseline, and nearly 4 on an RTX 5090 GPU (see Appendix for more details). Figure 6: QuTLASS performance for weights and activations while increasing batch size, for single linear LLM layer (left), and end-to-end using our vLLM integration (right)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We present first comprehensive study of the recently introduced MXFP4 and NVFP4 formats for LLM quantization, revealing gaps between the promise of these formats and their performance using state-of-the-art methods. To bridge these gaps, we introduce Micro-Rotated-GPTQ (MR-GPTQ), novel GPTQ variant adapted to these formats. We support this approach with QuTLASS, suite of high-performance GPU kernels that implement MR-GPTQs micro-rotations with negligible overhead. We hope that our results will provide basis and motivation for future work on improving accuracy for these novel formats. Code. The code for reproducing the quantization and accuracy evaluation experiments is provided at https://github.com/IST-DASLab/FP-Quant. The QuTLASS code is at https:// github.com/IST-DASLab/qutlass."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank members of the Red Hat AI team, particularly Rob Shaw, Michael Goin, and Kyle Sayers, for their support and useful suggestions. We would like to thank Lambda Cloud for their generous computational grant. We thank the NVIDIA and Google corporation for their grants, which supported part of this research. Roberto L. Castro and Andrei Panferov were supported in part by the BILAI Cluster of Excellence program (GreenAI), whereas Vage Egiazarian and Eldar Kurtic were supported by the ERC Proof-of-Concept Grant FastAI."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Advanced Micro Devices, Inc. AMD CDNA 4 Architecture Whitepaper. White paper, Advanced Micro Devices, Inc., June 2025. URL https://www.amd.com/ content/dam/amd/en/documents/instinct-tech-docs/white-papers/ amd-cdna-4-architecture-whitepaper.pdf. Accessed: 2025-09-24. [2] Mohammad Sadegh Akhondzadeh, Aleksandar Bojchevski, Evangelos Eleftheriou, and Martino Dazzi. Kurtail : Kurtosis-based llm quantization, 2025. URL https://arxiv.org/abs/ 2503.01483. [3] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end 4-bit inference on generative large language models. arXiv preprint arXiv:2310.09259, 2023. [4] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. [5] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard (2023https://huggingface.co/spaces/open-llm-leaderboard-old/ 2024). open_llm_leaderboard, 2023. [6] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36, 2024. [7] Euntae Choi, Sumin Song, Woosang Lim, and Sungjoo Yoo. Grouped sequency-arranged rotation: Optimizing rotation transformation for quantization for free, 2025. URL https: //arxiv.org/abs/2505.03810. [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] Tri Dao. Fast hadamard transform in cuda, 2023. URL https://github.com/ Dao-AILab/fast-hadamard-transform. [11] Tim Dettmers. 8-bit approximations for parallelism in deep learning. International Conference on Learning Representations (ICLR), 2016. [12] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:3031830332, 2022. [13] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. [14] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. [15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. [16] Johannes Dotzel, Zirui Liu, T.S. Jayram, and G. Edward Suh. Learning from students: Applying t-distributions to explore accurate and efficient formats for llms. In Proceedings of the 41st International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2024. 12 [17] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [18] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language models via additive quantization. arXiv preprint arXiv:2401.06118, 2024. [19] Erik Englesson and Hossein Azizpour. Generalized jensen-shannon divergence loss for learning with noisy labels, 2021. URL https://arxiv.org/abs/2105.04522. [20] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [21] Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal Brain Compression: framework for accurate post-training quantization and pruning. arXiv preprint arXiv:2208.11580, 2022. In NeurIPS 2022. [22] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for fewshot language model evaluation, September 2021. URL https://doi.org/10.5281/ zenodo.5371628. [23] Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Chentao Lv, Yunchen Zhang, Xianglong Liu, and Dacheng Tao. Llmc: Benchmarking large language model quantization with versatile compression toolkit, 2024. URL https://arxiv.org/abs/2405.06001. [24] Zhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan Zhao, and Rui Yan. What makes quantization for large language model hard? an empirical study from the lens of perturbation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1808218089, 2024. [25] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [26] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good are low-bit quantized llama3 models? an empirical study, 2024. [27] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. [28] Eldar Kurtic, Alexandre Noll Marques, Shubhra Pandit, Mark Kurtz, and Dan Alistarh. give me BF16 or give me death? accuracy-performance trade-offs in llm quantization. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2687226886, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1304. URL https: //aclanthology.org/2025.acl-long.1304/. [29] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611626, 2023. [30] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tülu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. URL https://arxiv.org/abs/2411.15124. 13 [31] Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, and Yongin Kwon. comprehensive evaluation of quantized instruction-tuned large language models: An experimental analysis up to 405b. arXiv preprint arXiv:2409.11055, 2024. [32] Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language models. arXiv preprint arXiv:2402.18158, 2024. [33] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. In Proceedings of the Learning on Systems (MLSys) Conference, 2024. [34] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. [35] Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. Do emergent abilities exist in quantized large language models: An empirical study. arXiv preprint arXiv:2307.08072, 2023. [36] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquantllm quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024. [37] Vladimir Malinovskii, Andrei Panferov, Ivan Ilin, Han Guo, Peter Richtárik, and Dan Alistarh. HIGGS: Pushing the limits of large language model quantization via the linearity theorem. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 1085710886, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-1896. doi: 10.18653/v1/2025.naacl-long.543. URL https://aclanthology.org/2025. naacl-long.543/. [38] Gunjan Mehta, Justin Xin, Riyad Islam, Yiheng Zhang, Asfiya Baig, Akhil Goel, and Sandro Cavallari. NVIDIA TensorRT Unlocks FP4 Image Generation for NVIDIA Blackwell GeForce RTX 50 Series GPUs. NVIDIA Technical Blog, May 2025. [39] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020. [40] NVIDIA. Cutlass: Cuda templates for linear algebra subroutines, 2017. URL https:// github.com/NVIDIA/cutlass. [41] NVIDIA. Nvidia blackwell architecture technical brief. \"https://resources.nvidia. com/en-us-blackwell-architecture\", 2024. [42] NVIDIA. NVIDIA DGX B200. https://resources.nvidia.com/ en-us-dgx-systems/dgx-b200-datasheet?ncid=no-ncid, 2025. [43] NVIDIA. cuBLAS. https://docs.nvidia.com/cuda/cublas/index.html# d-block-scaling-factors-layout, 2025. [44] NVIDIA. NVIDIA RTX Blackwell GPU Architecture. https:// images.nvidia.com/aem-dam/Solutions/geforce/blackwell/ nvidia-rtx-blackwell-gpu-architecture.pdf, 2025. [45] Open Compute Project Foundation (MX Alliance). OCP Microscaling Formats (MX) Specification Version 1.0. Open Compute Project Foundation Technical Specification, September 2023. URL https://www.opencompute.org/documents/ ocp-microscaling-formats-mx-v1-0-spec-final-pdf. 14 [46] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [47] Claude E. Shannon. Coding Theorems for Discrete Source With Fidelity CriterionInstitute of Radio Engineers, International Convention Record, vol. 7, 1959., pp. 325350. 1993. doi: 10.1109/9780470544242.ch21. [48] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks, 2024. URL https://arxiv.org/abs/2402.04396. [49] Albert Tseng, Qingyao Sun, David Hou, and Christopher De Sa. QTIP: Quantization with Trellises and Incoherence Processing. In Advances in Neural Information Processing Systems (NeurIPS) Spotlight, December 2024. URL https://arxiv.org/abs/2406.11235. [50] Albert Tseng, Tao Yu, and Youngsuk Park. Training llms with mxfp4, 2025. URL https: //arxiv.org/abs/2502.20586. [51] Joshua Vendrow, Edward Vendrow, Sara Beery, and Aleksander Madry. Do large language model benchmarks test reliability?, 2025. URL https://arxiv.org/abs/2502.03461. [52] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 3808738099. PMLR, 2023. [53] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [54] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. In Advances in Neural Information Processing Systems (NeurIPS 2022), 2022. [55] Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, and Luis Ceze. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. URL https://arxiv.org/abs/2501.01005. [56] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [57] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023."
        },
        {
            "title": "TECHNICAL APPENDICES AND SUPPLEMENTARY MATERIAL",
            "content": "A WEIGHT-ONLY QUANTIZATION RESULTS The results for weight-only quantization are provided in Table 3. One can observe that similary to the weight and activation quantization case INT4 and NVFP4 perform similarly, while MXFP suffers much significant accuracy drop. Even for weight-only case there is 2% accuracy drop on average relative to the original model. Format Quantization MMLU GSM8k HellaSwag WinoGrande Avg. Recovery% FP16 - 72. 85.10 INT4 NVFP"
        },
        {
            "title": "MXFP",
            "content": "RTN RTN+Had GPTQ RTN RTN+Had GPTQ AWQ RTN RTN+Had GPTQ AWQ 69.38 70.27 70.25 70.64 69.26 70.52 70.57 68.23 66.24 68.79 68. 81.80 82.56 80.52 82.26 80.82 82.49 82.71 80.36 77.56 81.43 78.70 80.00 79.41 79.18 79.01 79.24 78.52 79.35 79. 77.26 77.34 78.40 78.56 78.90 77.90 76.64 76.64 77.35 77.03 76.95 77.03 75.93 74.11 76.88 75.30 79. 77.12 77.16 76.60 77.37 76.41 77.33 77.40 75.44 73.81 76.37 75.18 97.71 97.76 97.05 98.02 96.80 97.96 98. 95.58 93.51 96.76 95.25 Table 3: Performance of Llama-3.1-8B-Instruct under different weight-only quantization settings."
        },
        {
            "title": "B REAL QUANTIZATION RESULTS",
            "content": "In this section we provide complete set of evaluation results for Llama-3 (Llama-3.2-1B-Instruct, Llama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, Llama-3.3-70B-Instruct) and Qwen-3 (Qwen-3-8B, Qwen-3-14B, Qwen-3-32B) model families. We turn off thinking mode for Qwen as it turned out that long reasoning chains-of-thought turned out to be detrimental for performance on GSM8k and MMLU-CoT. The scores were produced using QuTLASS vLLM integration."
        },
        {
            "title": "Format Quantization",
            "content": "MMLU GSM8k HellaSwag WinoGrande Avg. Recovery% - FP16 46.20 46."
        },
        {
            "title": "MXFP",
            "content": "RTN RTN+Had16 RTN+Had128 GPTQ GPTQ+Had16 GPTQ+Had128 QAT QAT+Had16 RTN RTN+Had32 RTN+Had128 GPTQ GPTQ+Had32 GPTQ+Had128 QAT QAT+Had32 36.08 32.80 38.28 37.79 38.99 35.47 27.85 32.72 30.46 30.89 34.48 26.84 29.44 35.68 15.60 28.12 31.39 25.02 29.95 29.80 32.98 31.16 38.51 37.60 11.83 19.41 25.55 13.50 27.60 28.13 20.32 36. 59.78 54.77 56.24 54.27 55.48 56.66 57.02 57.52 57.53 48.28 51.64 53.98 49.29 54.89 54.60 53.34 57.04 61.56 57.22 59.04 58.41 60.22 58.17 59.19 60.30 58.41 54.22 57.22 58.01 56.75 58.72 58.72 56.51 58. 53.47 44.87 43.28 45.23 45.82 46.70 45.71 46.05 46.57 36.20 39.79 43.01 36.60 42.66 44.28 36.44 45.20 83.91 80.94 84.59 85.71 87.35 85.50 86.12 87.09 67.70 74.42 80.44 68.45 79.80 82.83 68.16 84. Table 4: Performance of Llama-3.2-1B-Instruct for different weight & activation quantization settings."
        },
        {
            "title": "Format Quantization",
            "content": "MMLU GSM8k HellaSwag WinoGrande Avg. Recovery% - FP16 64.43 78."
        },
        {
            "title": "MXFP",
            "content": "RTN RTN+Had16 RTN+Had128 GPTQ GPTQ+Had16 GPTQ+Had128 MicroQAT+Had16 QAT QAT+Had16 RTN RTN+Had32 RTN+Had128 GPTQ GPTQ+Had32 GPTQ+Had128 MicroQAT+Had32 QAT QAT+Had32 60.62 59.91 54.34 61.76 60.26 60.19 60.66 62.06 62.03 56.81 55.58 55.95 57.68 59.79 59.56 59.49 56.17 59.83 70.43 64.82 67.48 70.36 68.76 70.89 69.98 75.06 72.93 60.80 57.77 60.80 62.32 68.92 67.78 65.66 64.90 72. 73.42 70.99 69.77 69.69 71.07 71.05 70.97 70.55 71.27 70.95 67.30 68.56 67.57 63.87 69.50 70.08 69.05 69.51 70.27 70.09 68.03 65.59 66.93 69.93 67.80 68.19 67.01 67.96 66.46 64.56 64.33 64.88 64.88 66.85 68.03 67.32 67.17 66. 71.49 67.52 65.02 64.61 68.28 66.97 67.56 67.05 69.09 68.09 62.37 61.56 62.30 62.19 66.27 66.36 65.38 64.44 67.28 94.45 90.96 90.38 95.51 93.68 94.51 93.79 96.64 95.25 87.24 86.11 87.15 86.99 92.69 92.83 91.46 90.14 94. Table 5: Performance of Llama-3.2-3B-Instruct for different weight & activation quantization settings."
        },
        {
            "title": "Format Quantization",
            "content": "MMLU-CoT GSM8k HellaSwag WinoGrande Avg. Recovery% - FP16 RTN RTN+Had RTN+Had128 GPTQ GPTQ+Had GPTQ+Had128 QAT QAT+Had RTN RTN+Had RTN+Had128 GPTQ GPTQ+Had GPTQ+Had128 QAT QAT+Had"
        },
        {
            "title": "MXFP",
            "content": "72.80 68.70 67.00 66.60 68.60 69.40 68.90 68.20 68.90 62.20 62.60 64.50 63.74 67.20 66.80 65.00 67.60 85.10 78.70 77.40 77.00 79.60 79.60 79.50 79.80 81.60 69.50 71.80 72.70 70.20 77.50 78.30 76.00 80. 80.00 78.40 77.30 77.50 78.70 78.40 78.30 78.90 79.00 73.80 75.20 76.00 75.52 77.00 76.90 77.60 78.30 77.90 73.40 74.40 75.50 75.50 75.10 73.60 74.40 75.10 72.60 72.30 73.30 7364 73.10 74.90 72.90 74. 78.90 74.80 74.00 74.10 75.60 75.60 75.10 75.30 76.10 69.50 70.50 71.60 70.78 73.70 74.20 72.90 75.30 94.80 93.80 93.90 95.70 95.80 95.10 95.40 96.50 88.10 89.30 90.70 89.66 93.30 94.00 92.30 95. Table 6: Performance of Llama-3.1-8B-Instruct for different weight & activation quantization settings."
        },
        {
            "title": "Format Quantization",
            "content": "MMLU GSM8k HellaSwag WinoGrande Avg. Recovery% - FP16 86.55 95."
        },
        {
            "title": "MXFP",
            "content": "RTN RTN+Had16 RTN+Had128 GPTQ GPTQ+Had16 GPTQ+Had128 RTN RTN+Had32 RTN+Had128 GPTQ GPTQ+Had32 GPTQ+Had128 85.50 85.02 85.24 85.54 85.58 85.59 83.42 83.86 84.37 83.77 84.82 84.90 93.48 93.63 91.81 94.09 93.40 94.16 92.65 93.56 94.47 94.47 94.54 93. 86.22 85.63 84.97 84.91 85.49 85.45 85.56 83.93 84.13 84.22 84.41 84.66 84.80 84.93 83.27 83.82 83.35 84.37 82.40 84.77 81.45 83.58 82.40 82.64 83.11 83. 88.19 86.97 86.86 86.33 87.37 86.71 87.52 85.36 86.28 86.37 86.32 86.78 86.86 98.61 98.49 97.89 99.07 98.32 99.24 96.79 97.83 97.93 97.88 98.40 98. Table 7: Performance of Llama-3.3-70B-Instruct for different weight & activation quantization settings."
        },
        {
            "title": "Format Quantization",
            "content": "MMLU GSM8k HellaSwag WinoGrande Avg. Recovery% - FP16 72.98 90."
        },
        {
            "title": "MXFP",
            "content": "RTN RTN+Had16 RTN+Had128 GPTQ GPTQ+Had16 GPTQ+Had128 QAT QAT+Had16 RTN RTN+Had32 RTN+Had128 GPTQ GPTQ+Had32 GPTQ+Had128 QAT QAT+Had32 70.78 70.19 69.09 70.90 71.06 70.45 70.94 71.34 67.69 67.57 67.27 68.01 69.13 69.53 69.45 70.35 90.30 86.35 86.66 88.17 88.32 87.41 89.08 89.23 84.23 83.78 81.58 84.23 84.84 86.43 87.34 89. 75.52 74.63 73.02 73.47 75.01 74.58 74.25 74.67 75.24 71.24 71.32 71.41 71.65 73.17 73.55 74.03 74.61 70.56 70.72 68.11 67.96 70.09 68.03 68.90 68.51 70.40 67.40 67.32 66.38 67.80 68.03 65.75 69.85 70. 77.49 76.61 74.42 74.30 76.04 75.50 75.25 75.80 76.55 72.64 72.50 71.66 72.92 73.79 73.82 75.17 76.28 98.86 96.04 95.88 98.13 97.43 97.11 97.82 98.79 93.74 93.56 92.48 94.11 95.23 95.26 97.00 98. Table 8: Performance of Qwen-8B for different weight & activation quantization settings."
        },
        {
            "title": "Format Quantization",
            "content": "MMLU GSM8k HellaSwag WinoGrande Avg. Recovery% - FP16 77.18 91."
        },
        {
            "title": "MXFP",
            "content": "RTN RTN+Had16 RTN+Had128 GPTQ GPTQ+Had16 GPTQ+Had128 RTN RTN+Had32 RTN+Had128 GPTQ GPTQ+Had32 GPTQ+Had128 75.73 74.98 74.46 74.88 75.49 75.10 72.92 73.19 73.17 72.57 74.36 74.11 91.28 92.04 91.13 91.28 91.43 90.52 90.22 89.54 85.60 89.54 89.92 89. 79.84 78.36 77.76 77.60 78.40 78.38 78.30 76.68 75.95 76.80 76.50 77.64 77.77 74.27 73.16 72.38 71.98 74.51 74.51 72.77 71.51 71.67 72.14 72.45 72.53 71. 80.81 79.63 79.29 78.79 79.77 79.95 79.17 77.83 77.59 76.93 77.77 78.61 78.23 98.54 98.12 97.50 98.71 98.94 97.97 96.31 96.01 95.19 96.23 97.28 96. Table 9: Performance of Qwen-14B for different weight & activation quantization settings."
        },
        {
            "title": "Format Quantization",
            "content": "MMLU GSM8k HellaSwag WinoGrande Avg. Recovery% - FP16 80.81 92."
        },
        {
            "title": "MXFP",
            "content": "RTN RTN+Had16 RTN+Had128 GPTQ GPTQ+Had16 GPTQ+Had128 RTN RTN+Had32 RTN+Had128 GPTQ GPTQ+Had32 GPTQ+Had128 79.85 78.90 78.49 79.54 78.60 79.11 77.07 78.22 78.36 77.01 78.46 78.90 94.24 89.23 89.69 92.87 90.90 90.52 72.33 93.03 88.10 88.55 82.41 90. 83.97 83.27 82.60 82.47 83.24 82.93 83.15 81.52 81.76 81.66 81.79 82.72 82.29 76.56 75.22 76.48 75.37 75.93 75.14 76.09 75.22 75.93 75.30 74.90 75.06 75. 83.35 83.15 81.80 81.51 82.90 81.89 82.22 76.54 82.24 80.86 80.56 79.66 81.83 99.76 98.15 97.79 99.46 98.26 98.65 91.83 98.67 97.01 96.66 95.58 98. Table 10: Performance of Qwen-32B for different weight & activation quantization settings."
        },
        {
            "title": "C SCALE QUANTIZATION ANALYSIS",
            "content": "As discussed in the main text, microscaling formats adopt scale quantization to reduce memory storage overhead and accelerate dequantization operations. However, scale quantization may introduce additional error due to rounding of scales onto coarser grid. Below we provide an analysis and explore alternative choices for scale quantization. MXFP format adopts E8M0 grid with exponentially spaced levels. It allows to represent values with very small and large magnitude, yet the distance between adjacent levels can be pretty large resulting in large approximation errors. E4M3 grid used in NVFP, on the other hand, has much narrower dynamic range [448, 448] with levels spread more uniformly. We note, that the sign bit is in fact never utilized, given that the scale is non-negative value by definition. Below, we explore several choices for 8-bit scale quantization with fixed group size of 16. Specifically, we measure weight and activation MSErel for range of EeMm formats with + = 7, as well as for E8M0 and INT8. For E8M0 scale quantization, we multiply the scale by 4/3 following [50], which yields an unbiased estimate of the original scale and reduces quantization error. Results for weight and activation quantization are shown in Figure 7 and Figure 8, respectively. Figure 7: MSErel for the weights of 15th block in the Llama-3.1-8B-Instruct model. Figure 8: MSErel for the activations of 15th block in the Llama-3.1-8B-Instruct model. One can observe that the E4M3 and E8M0 scales are not optimal for weight scale quantization. E4M3 and E8M0 increase MSErel by 10%, 40% on average, respectively. At the same time, FP8 options with larger mantissa (E1M6-E3M4) as well as INT8 perform close to FP4 without scale quantization. The pattern for activation pattern is similar except for the case of down_proj in feedforward layer, which is known to have more heavy-tailed distribution with pronounced outliers. We note that the observed behavior generalizes to other models considered in our study."
        },
        {
            "title": "D OUTLIERS ANALYSIS",
            "content": "Proof of Lemma 1. Let = 1 IG). The error vectors are related by εx = (cid:98)x = is orthogonal, it preserves the Euclidean norm: εx2 Squared Error (MSE) is defined as: be the normalized Hadamard matrix. is orthogonal (U = (cid:98)y = ((cid:98)y y) = εy. Since 2. The per-element Mean 2 = εy2 2 = εy2 MSE(G) ="
        },
        {
            "title": "1\nG",
            "content": "E[εx2 2] ="
        },
        {
            "title": "1\nG",
            "content": "E[εy2 2]. This establishes the second equality. To prove the first, we rely on the standard assumption in quantization analysis that the quantization error εy is statistically independent of the signal y. Since and are related by the invertible transformation = y, εy is also independent of x. Consequently, the reconstruction error εx = εy is also going to be independent of x. The index = arg maxi xi is function of x. Therefore, the error vector εx (and its components) is independent of the random index I. Further, since the coordinates of are i.i.d., we can apply symmetry to obtain that the probability that any coordinate has the largest magnitude is uniform: (I = i) = 1/G. We calculate the Top-Element MSE using the Law of Total Expectation: MSEtop(G) = E[(εx)2 ] = = (cid:88) i=1 (cid:88) i= E[(εx)2 I = i]P (I = i) E[(εx)2 = i]"
        },
        {
            "title": "1\nG",
            "content": ". Because (εx)2 E[(εx)2 = i] = E[(εx)2 ]. Substituting yields: is independent of the event {I = i}, the conditional expectation simplifies to MSEtop(G) = = ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 E[(εx)2 ] (cid:35) (cid:34) (cid:88) (εx)2 E i=1 (by linearity of expectation) E[εx2 2] = MSE(G). This completes the proof. Lemma 3 (Outliers MAPE). Let distribution be mix of two distributions: Xbase and Xoutliers with portions 1 and such that: 1. min(Xoutliers) > max(Xbase), 2. MSErel top(X XI Xoutliers) = MSErel top(X XI Xbase), 3. MSErel top(X )."
        },
        {
            "title": "Then the expected outlier relative quadratic error equals MSErel",
            "content": "top(X ) up to O(pG): (cid:80) i=1 EXX λXiXoutliers (Xi (cid:98)Xi) 2 (cid:80) i=1 λXiXoutliers MSErel top(X ). Proof. We expand the expectation conditioned on XI Xoutliers: (cid:80)G EXX i=1 λXiXoutliers (Xi (cid:98)Xi)2 i=1 λXiXoutliers (cid:80)G 2 (XI (cid:98)XI )2 2 + (cid:80) 1 + (cid:80) i=I i=I λXiXoutliers (Xi (cid:98)Xi)2 λXiXoutliers 2 = EXX XI Xoutliers (cid:34) = EXX XI Xoutliers (cid:35) (XI (cid:98)XI )2 2 + O(pG). By Assumption 2 this conditional expectation equals MSErel Hence the claim follows. top(X ), up to O(pG) from Assumption 3. Discussion. Assumption 1 is satisfied for outliers chosen by absolute value thresholds. Assumption 2 holds for floating-point quantization due to constant relative accuracy (no overflow/underflow), verified in Section 3.2. Assumption 3 holds in practice for LLMs since outliers are typically about 0.1% of elements [11]. Lemma 4 (Consistency of MSErel quantize with power-of-two translation-invariant quantization function : R+, : Q(x 2k) = 2k Q(x). top for smooth distributions). Let be distribution of values to Assume: 1. supp [2a, 2b] for integers < b, 2. supp , [x/ 2, 2] : fX (x) fX (y) α, 3. (xQ(x))2 x2 MSErel max."
        },
        {
            "title": "Then",
            "content": "ExX (cid:20) (x Q(x))2 x2 (cid:21) = (cid:90) 2 (x Q(x))2 x2 (cid:16) dx + (2b 2a) MSErel max α (cid:17) . Proof. We decompose the expectation over dyadic intervals: ExX (cid:20) (x Q(x))2 x2 (cid:21) = b1 (cid:88) (cid:90) 2i+1 i=a 2i (x Q(x))2 x2 fX (x) dx. Within each interval, write fX (x) = fX (2i) + (fX (x) fX (2i)). The first term yields (cid:90) 2 1 (x Q(x))2 x2 dx b1 (cid:88) i=a 2ifX (2i). The second term is bounded using Assumption 2 and 3, giving b1 (cid:88) (cid:90) 2i+1 i=a 2i"
        },
        {
            "title": "MSErel",
            "content": "max O(α) dx = (2b 2a) MSErel max O(α). 23 Finally, the normalization error in the discrete approximation of (cid:82) fX contributes an additional O(α) factor. Combining terms gives the stated result. Discussion. Assumptions 1 and 3 hold for absmax XI quantization since floating-point values are bounded with bounded relative error. Assumption 2 is supported empirically (Figure 3), where scale distributions are observed to be smooth."
        },
        {
            "title": "E QUTLASS RESULTS ON GEFORCE GPUS",
            "content": "Figure 9: Illustration of QuTLASS performance for weights and activations on MXFP4 while increasing batch size, for single linear LLM layer, showing the low-overhead of the quantizationrelated ops, and end-to-end using the Transformers library. Figure 9 illustrates additional QuTLASS performance results on an NVIDIA RTX5090 GPU. The figure on the left shows throughput for single layer extracted from MXPF4 quantized Qwen3-32B model, while the figure on the right shows the end-to-end speedups on Transformers running Qwen38B with MXFP4 quantization compared to the BF16 baseline implementation on single RTX5090 GPU."
        },
        {
            "title": "F STANDARD DEVIATION",
            "content": "We estimate the variance of evaluation scores by performing multiple quantization runs on Llama3.1-8B-Instruct, varying the seeds for GPTQ calibration set sampling, as well as the strategies for scale selection and quantization ordering. These results were generated using our vLLM integration with QuTLASS kernels. Figure 10 displays the scores as bar plots, while Table 11 lists the average recovery scores and their standard deviations. Additionally, we report the average recovery scores and their standard deviations for the Platinum benchmark suite [51] in Figure 13 and Table 11. We also report per-task recovery ( 24 Figure 10: Accuracy results for NVFP4 and MXFP4 across different combinations of MR-GPTQ components, averaged over five random seeds using vLLM kernels on the benchmark suite. 25 Figure 11: Accuracy results for MXFP4 across different MR-GPTQ component combinations on the Platinum benchmark tasks. Figure 12: Accuracy results for NVFP4 across different MR-GPTQ component combinations on the Platinum benchmark tasks. 26 Figure 13: Average recovery scores and standard deviations for NVFP and MXFP methods on the Platinum benchmarks."
        },
        {
            "title": "Format Method",
            "content": "Standard Bench % STD Platinum Bench % STD"
        },
        {
            "title": "MXFP",
            "content": "Had16+MinMax+ActOrder Had16+MSE+ActOrder Had128+MinMax+ActOrder Had128+MSE+ActOrder Ident+MinMax+ActOrder Ident+MSE+ActOrder Ident+MinMax+DefOrder Ident+MSE+DefOrder Had32+MinMax+ActOrder Had32+MSE+ActOrder Had128+MinMax+ActOrder Had128+MSE+ActOrder Ident+MinMax+ActOrder Ident+MSE+ActOrder Ident+MinMax+DefOrder Ident+MSE+DefOrder 95.88 96.33 95.52 96.11 95.84 96.18 96.06 96.38 92.79 93.78 93.42 93.63 89.78 90.54 89.16 90.02 0.332 0.163 0.416 0.347 0.487 0.589 0.655 0.441 0.554 0.445 0.416 0.817 0.570 0.330 0.372 0. 93.77 96.57 93.42 93.95 93.27 93.51 94.20 94.01 91.73 90.51 92.32 91.86 85.93 85.26 - 85.21 1.680 0.746 1.618 1.143 1.263 1.304 1.358 1.053 1.183 2.294 1.128 0.743 1.459 1.112 - 0.798 Table 11: Average Recovery scores and standard deviations for NVFP and MXFP methods across the Standard benchmark and Platinum benchmark."
        },
        {
            "title": "G THE EFFECT OF DIFFERENT LINEAR TRANSFORMS",
            "content": "In this section we ablate various choices of transforms adopted for outlier mitigation of outliers. Specifically, we consider the following options: Identity transform. Discrete Cosine Transform (DCT). Hadamard rotation [48; 4; 10]. Grouped Sequency-arranged Rotation (GSR) [7]. We sweep over different options of transform sizes ({16, 32, 64, 128, 256}) both for NVFP and MXFP formats. The average score on 5 tasks from LM Evaluation Harness (piqa, winogrande, hellaswag, arc-easy, arc-challenge) is reported. From these results, one can observe that rotations yield small improvement relative to identity transform for MXFP format and minor degradation for NVFP with RTN quantization. Different transform sizes perform more or less the same."
        },
        {
            "title": "Transformation Transformation Size Weight Quant",
            "content": "PIQA winogrande hellaswag arc-easy arc-challenge"
        },
        {
            "title": "Avg",
            "content": "FP16 -"
        },
        {
            "title": "DCT",
            "content": "DST Hadamard GSR - - - 32 64 128 256 16 64 128 256 16 32 128 256 16 32 64 256 - 0.8074 0.7301 0.792 0."
        },
        {
            "title": "RTN\nGPTQ",
            "content": "RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ 0.802 0.7933 0.79 0. 0.7786 0.7813 0.7862 0.7878 0.7737 0.7873 0.7916 0.7911 0.7824 0.7878 0.7856 0. 0.7911 0.7911 0.7856 0.7824 0.7867 0.7856 0.7927 0.7873 0.784 0.7965 0.7818 0. 0.7884 0.7938 0.7878 0.79 0.7933 0.7998 0.7873 0.79 0.7911 0.796 0.7878 0. 0.7797 0.8014 0.7261 0.7214 0.6859 0.7111 0.7119 0.7135 0.7024 0.7198 0.7206 0. 0.7135 0.7017 0.7143 0.7198 0.7198 0.7096 0.7253 0.7088 0.7024 0.7064 0.6993 0. 0.7096 0.7096 0.719 0.7348 0.7032 0.7151 0.7206 0.7111 0.6969 0.7253 0.7056 0. 0.6985 0.7214 0.7151 0.7222 0.7174 0.7143 0.6906 0.7293 0.7731 0.7698 0.7583 0. 0.7572 0.7718 0.7695 0.7673 0.7676 0.7715 0.7698 0.7692 0.7575 0.7628 0.7399 0. 0.7536 0.7638 0.7625 0.7637 0.7579 0.7674 0.7674 0.7697 0.7639 0.7668 0.763 0. 0.766 0.7729 0.7643 0.7738 0.7694 0.7683 0.762 0.77 0.7627 0.7717 0.7656 0. 0.7626 0.7756 0.7466 0.7664 0.742 0.7559 0.7353 0.7117 0.7306 0.7765 0.7466 0. 0.7563 0.7694 0.7256 0.7395 0.7395 0.7731 0.7635 0.7614 0.7677 0.7778 0.737 0. 0.7471 0.758 0.7534 0.7538 0.7395 0.7614 0.7551 0.7681 0.7681 0.7673 0.7513 0. 0.7702 0.7593 0.7588 0.7622 0.7546 0.7668 0.7454 0.763 0.5307 0.4923 0. 0.4889 0.4991 0.4693 0.4829 0.4599 0.5068 0.4701 0.494 0.4983 0.506 0.4804 0. 0.4667 0.5026 0.4804 0.5 0.4881 0.5009 0.4804 0.4812 0.465 0.5034 0.4881 0. 0.4863 0.5017 0.506 0.5273 0.4983 0.4949 0.4744 0.4863 0.494 0.5 0.4821 0. 0.4898 0.506 0.4735 0.4991 0.7274 0.708 0.7124 0.693 0.7029 0.6905 0. 0.6897 0.7116 0.6957 0.7001 0.7059 0.7074 0.692 0.6991 0.6903 0.7075 0.7028 0. 0.7013 0.7062 0.6923 0.6971 0.6963 0.7056 0.7017 0.7116 0.6948 0.7059 0.7072 0. 0.7031 0.7102 0.6988 0.7033 0.7024 0.7081 0.702 0.7094 0.703 0.7097 0.6904 0. Table 12: Performance of Llama-3-8B with different transformations with NVFP4 format."
        },
        {
            "title": "Transformation Transformation Size Weight Quant",
            "content": "PIQA winogrande hellaswag arc-easy arc-challenge"
        },
        {
            "title": "Avg",
            "content": "FP16 -"
        },
        {
            "title": "DCT",
            "content": "DST Hadamard GSR - - - 32 64 128 256 16 64 128 256 16 32 128 256 16 32 64 256 - 0.8074 0.7301 0.792 0."
        },
        {
            "title": "RTN\nGPTQ",
            "content": "RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ RTN GPTQ 0.7704 0.7699 0.7628 0. 0.7699 0.7508 0.7693 0.7889 0.7541 0.7731 0.7791 0.7894 0.7731 0.7835 0.7639 0. 0.7704 0.7769 0.7612 0.7693 0.7731 0.778 0.7737 0.7867 0.7715 0.7807 0.7862 0. 0.7807 0.7807 0.778 0.7818 0.7813 0.7845 0.7748 0.7856 0.7889 0.7949 0.7753 0. 0.7753 0.778 0.6875 0.693 0.7072 0.7009 0.6914 0.6969 0.7127 0.7111 0.6851 0. 0.6953 0.6946 0.6906 0.6898 0.6906 0.6985 0.689 0.6875 0.6772 0.6914 0.6906 0. 0.6906 0.7206 0.6946 0.7032 0.7088 0.6993 0.7064 0.6946 0.7024 0.7032 0.6977 0. 0.693 0.7111 0.7072 0.7009 0.7001 0.7056 0.6819 0.7151 0.7481 0.753 0.7447 0. 0.7405 0.7465 0.7454 0.7524 0.7398 0.7455 0.7392 0.7541 0.7493 0.7593 0.7441 0. 0.7402 0.7599 0.7491 0.7567 0.7493 0.7544 0.7499 0.7623 0.7518 0.763 0.7511 0. 0.7529 0.7646 0.7491 0.7624 0.7522 0.7682 0.7514 0.7631 0.7464 0.7613 0.7538 0. 0.7494 0.7542 0.7121 0.7327 0.7205 0.7365 0.6987 0.7281 0.7079 0.7529 0.6616 0. 0.6987 0.7504 0.7391 0.7399 0.7332 0.7483 0.7054 0.7189 0.7003 0.7462 0.7391 0. 0.6995 0.7218 0.7466 0.7471 0.7315 0.7635 0.7306 0.7538 0.7104 0.7576 0.6982 0. 0.742 0.7517 0.7226 0.7412 0.7226 0.7395 0.7197 0.7445 0.5307 0.471 0. 0.4582 0.4846 0.4437 0.4531 0.4556 0.465 0.4036 0.4804 0.4411 0.4744 0.4522 0. 0.4582 0.4753 0.4599 0.4693 0.4497 0.d923 0.4522 0.4923 0.4616 0.4701 0.5034 0. 0.4667 0.4923 0.4548 0.4915 0.4625 0.4795 0.4684 0.4735 0.4991 0.5026 0.4514 0. 0.4659 0.4846 0.4642 0.4923 0.7274 0.6778 0.6841 0.6787 0.6902 0.6688 0. 0.6782 0.6941 0.6488 0.6908 0.6707 0.6926 0.6809 0.6902 0.678 0.6917 0.673 0. 0.6675 0.6912 0.6809 0.6945 0.675 0.6923 0.6936 0.6944 0.6889 0.7027 0.6851 0. 0.6805 0.6969 0.6796 0.6971 0.6921 0.7028 0.6833 0.6962 0.6835 0.6941 0.6781 0. Table 13: Performance of Llama-3-8B with different transformations with MXFP4 format."
        }
    ],
    "affiliations": [
        "ETH Zürich",
        "ISTA & Red Hat AI",
        "Yandex Research"
    ]
}