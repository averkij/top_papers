{
    "paper_title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "authors": [
        "Pei Yang",
        "Hai Ci",
        "Yiren Song",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 7 3 5 4 0 . 2 1 5 2 : r X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale Pei Yang* Hai Ci* Yiren Song Mike Zheng Shou Show Lab, National University of Singapore https://showlab.github.io/X-Humanoid/ Figure 1. Our proposed paradigm for robotizing human videos. (1) We adapt Video Diffusion Transformer (DiT) into video-to-video architecture. (2) This model is finetuned on novel synthetic dataset of paired human-humanoid videos we generated using Unreal Engine. (3) At inference, the finetuned model translates real-world human videos (e.g., Ego-Exo4D [9]) into large-scale dataset of humanoid videos, which can be used to train downstream robotics VLA or world models."
        },
        {
            "title": "Abstract",
            "content": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and *Equal contribution. Corresponding author. world models is severely hampered by the scarcity of largescale, diverse training data. promising solution is to robotize web-scale human videos, which has been proven effective for policy training. However, these solutions mainly overlay robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for 1 robotizing humans. To bridge this gap, we introduce XHumanoid, generative video editing approach that adapts the powerful Wan 2.2 model into video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing new large-scale dataset of over 3.6 million robotized humanoid video frames. Quantitative analysis and user studies confirm our methods superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness. 1. Introduction Recent advances in robotics, such as vision-language-action (VLA) models [4, 5, 26] and world models [32, 33], show emerging promise for general-purpose autonomy. However, current research is suffered from data scarcity [21, 28]. Access to such large-scale robot data would enable more straightforward training and potentially unlock higher performance upper bound. promising strategy to alleviate this data scarcity is to leverage the vast existing human activity videos [21, 22, 25]. Here, significant challenge arises from the visual embodiment gap: the distinct physical appearance of humans versus robots prevents the direct use of this data. Some approaches attempt to robotize human videos by editing the content of egocentric videos, such as replacing human arms with rendered robot arms [21, 23]. While beneficial, these rule-based overlay methods often produce artifacts (e.g., wrong occlusions) and have not been successfully applied to third-person videos. This is because the thirdperson scenario is substantially more complex, involving full-body motions, dynamic backgrounds, and severe occlusions that are beyond the capabilities of simple inpaint-andoverlay techniques. To effectively transform third-person human videos into humanoid videos, and in turn generate large-scale data to alleviate data scarcity, we take new approach: leveraging modern generative video models for flexible human-to-humanoid video editing. We introduce X-Humanoid (X denotes transfer), adapting the powerful Wan 2.2 [44] diffusion transformer (DiT) model into video-in video-out structure. The model is then finetuned to take human video as input, outputting video where the person is replaced by fixed humanoid embodiment. The humanoids motion should be consistent with the original humans action, while all other video content should be best preserved, such as the background scene. Training this model requires paired human-humanoid videos, which are not yet available. We therefore designed novel data synthesis method using Unreal Engine, turning rich community resources into paired human-humanoid videos performing identical animations. We generate over 17 hours of paired 1080p 30 fps video data  (Fig. 2)  . Using only 6.4% of this new synthetic dataset, we performed LoRA finetuning on our adapted Wan 2.2 model. As illustrated in Fig. 1, this finetuned model can then robotize real-world web-scale videos. We apply our trained model to create large-scale, 60-hour (3.6 million frames) robotized version of the Ego-Exo4D [9] dataset, featuring the Tesla Optimus humanoid. We validate our approach through comparisons against existing video editing baselines. Both quantitative metrics and user study with 29 participants confirm our methods superior performance. For instance, 69.0% of users rated our results as having the best motion consistency, and 75.0% preferred our method for overall video quality. Furthermore, by leveraging powerful base model, our approach successfully edits challenging in-the-wild videos, faithfully preserving complex effects such as motion blur and adapting to the original videos quality. Our main contributions are summarized as follows: We introduce generative video editing approach to address data scarcity in robotics research, by adapting and finetuning modern video generation model. We introduce scalable pipeline to synthesize paired human-humanoid videos in Unreal Engine, releasing new 17+ hour, 1080p 30 fps dataset to train our model. We create and release large-scale, 60+ hour robotized dataset edited from Ego-Exo4D, featuring the Tesla Optimus humanoid, to help alleviate robotics data scarcity. 2. Related Work Data Scarcity in Robotics Research Current robotics research is advancing in two directions: (1) building robotic policies based on vision-language-action (VLA) models for instruction-following [4, 5, 12, 17, 17, 26, 34, 43, 45, 46, 53], and (2) training world models for future-state prediction [10, 11, 32, 33]. Both areas are fundamentally hindred by scarcity of large-scale robot data, which limits policy generalization [13] and future prediction accuracy [39]. Leveraging abundant, existing videos of human activity from datasets [8, 9] or the Interent presents significant opportunity to alleviate this data scarcity [13, 21]. Leveraging Human Activity Videos significant challenge in utilizing human videos is the visual embodiment gap: humans and robots possess distinct physical appearances, preventing the direct use of human videos for training. To circumvent this, prior works typically extract intermediate representations like poses or trajectories to guide execution [3, 29, 40], or utilize specific reward functions to pretrain visual encoders [16, 27, 28, 49]. While these methods successfully circumvented the visual domain gap issue, 2 Figure 2. Our synthetic data creation pipeline. We leverage rich community assets (e.g., Fab marketplace) for characters, animations, and environments (blue) to create paired human-humanoid videos in three steps: First (red), we align the skeletons of different characters and animations to ensure compatibility across all assets. Second (green), with the help of compatible skeletons, we transfer the same animation to both human and humanoid characters. Finally (yellow), we place the animated human and humanoid in diverse scenes and record them using identical camera setups and movements to produce the paired data. Figure 3. Visualization of sample pairs from our synthesized Human-Humanoid video dataset. To ensure diversity, the dataset encompasses wide variety of scenes, character motions, and camera parameters (e.g., focal length, exposure). We intentionally include challenging conditions, such as occlusions by obstacles and partial-body or off-center framing, to improve model robustness. alternative strategy could be bridging the gap at the pixel level, by directly training robot policies or world models on robot videos [21]. With such videos, there is no visual embodiment gap between the training videos and the actual robot, thus training could be more straightforward and potentially have higher performance upper bound. Such data has even wider benefits, because aside from VLA-based policies, robot videos could also facilitate the training of world model-centered policies [30]. Unfortunately, manually collecting such data [22, 48, 50] is costly and limited in scale and scene diversity. more scalable alternative is to robotize existing human videos by editing the video content to replace the human with robot. [21, 23, 24, 41] have explored this for egocentric videos by inpainting the human arm and overlaying rendered robot arm matching the human arms pose or hand motion. Despite producing numerous artifacts (e.g., occlusion errors), this rule-based overlay data has been shown to improve VLA performance [21]. However, robotizing third-person human videos, task beneficial for both humanoid policies and world models, remains unexplored. This scenario is substantially more complex, involving intricate full-body motions, dynamic backgrounds, and severe occlusions that are beyond the capabilities of existing rulebased methods. To address this complex issue, in this work, we attempt to robotize human videos by leveraging the capabilities of powerful, modern video generative models. Video Generation and Editing Modern video generative models have evolved from simple text-to-video generation [47, 51] to complex systems handling mixed-modal inputs and outputs [2, 6, 15, 19, 31, 35, 36, 38, 52]. Several existing models, such as Kling [20], Aleph [39], VACE [14], and MoCha [42], support image-conditioned video editing. These methods allows taking an input video (e.g., of human) and reference input image (e.g., of robot) to produce an output video where the human is replaced by the robot. However, we found these models generally prone to poor editing results for our specific task. Therefore, we do not use them. Instead, we leverage the powerful Wan 2.2 [44] model, which we adapt into video-to-video generation framework to effectively robotize human videos. 3. Methodology 3.1. Problem Formulation We aim to train video-to-video generation model that performs third-person human-to-humanoid editing. The models input is video of human performing an activity, and its output is video of specific humanoid robot model doing the same activity within the identical scene. critical constraint is that the robots motion must be consistent with the humans action and strictly per-frame aligned. To achieve this, we need to create paired training data (Sec. 3.2), design video-in video-out model architecture (Sec. 3.3), and finetune the model (Sec. 3.4). We then use our model to robotize more than 60 hours of Ego-Exo4D [9] videos, and also demonstrate the models capabilities on editing diverse in-the-wild Internet videos. 3.2. Creating Human-Humanoid Paired Videos The video editing models core capability is transforming the protagonist human into humanoid with identical actions, keeping other scene details unchanged. We thus use Unreal Engine to synthesize paired videos of humans and humanoid performing the same actions in the same scene, using rich community-sourced assets. Creating these paired videos involves three steps  (Fig. 2)  . First, to apply the same motion to different characters, we must resolve the skeletal incompatibility between our various character and animation assets. This is achieved through aligning the skeletons by manual IK Rig Retargeting. Second, with the skeletons mapped, we transfer (a.k.a. bake) each single motion animation to all human and humanoid characters. Third, we place both characters in the same scene, play the same animations, and record them with virtual cameras to get paired movie clips. To diversify the videos, we captured along various camera paths, intentionally capturing occlusions (e.g., behind obstacles) and offcenter characters. These videos cover varied exposure and color grading settings, 14-80mm focal lengths, and /2.8f /5.6 apertures. In total, we synthesized 11172 pairs of 1080p 30fps videos across 14 scenes, containing total of over 2.8 million frames. This took 10 days to render on one NVIDIA RTX 3060 GPU using Unreal Engine 5.3.2. Fig. 3 visualizes sample paired data. 3.3. Model Architecture We adapt Wan 2.2 [44], Diffusion Transformer- (DiT-) based video generation model, into video-in video-out architecture. As shown in Fig. 4, we first encode the input video into sequence of condition tokens and concatenate them before the generation tokens that are subsequently denoised. To establish strict spatio-temporal correspondence for video editing, we use identical positional embeddings for both sets of tokens. To prevent the input condition from being corrupted by the generation process, we apply unidirectional mask in each DiT blockss self attention map, preventing conditional tokens from attending to generation tokens. Finally, the model is finetuned to denoise only the generation tokens to produce the output video, while the models output on the conditional tokens is ignored. 3.4. Finetuning We adopt flow-matching finetuning objective, consistent with Wan 2.2s training [44]. The goal is to train the model vθ to predict ground-truth velocity vt. This velocity is defined along simple, linear path xt that interpolates from pure noise x0 (0, I) to the clean latent tokens x1 [7]: xt = tx1 + (1 t)x0, where [0, 1] is the timestep. The ground-truth velocity vt is the derivative of this path: vt = dxt dt = x1 x0. (1) The model is then finetuned with mean-squared error loss to predict this velocity, with loss function given by = 1 ,xgen xcon ,tvθ(xcon 1 , xgen , ctext, t) vt2. (2) Here, xcon and xgen denote condition and generation tokens, and ctext is fixed conditional text embedding (Humanoid video). At inference, the models predicted velocity is integrated from = 0 to 1 to produce the final, edited video: xgen 1 = x0 + (cid:90) 1 0 vθ(xcon 1 , xgen , ctext, t)dt. (3) 4. Experiments 4.1. Experimental Setup Data Processing We partition the synthetic video pairs from 14 virtual scenes, using 12 scenes for training and reserving 2 for validation. Both synthetic and real (EgoExo4D [9] and Internet) videos are downsampled to 15 fps, resized to 864400, and segmented into 90-frame (6second) clips. Any clips shorter than 90 frames are discarded. For evaluation, we sample 50 clips each from this validation set and the Ego-Exo4D dataset. 4 Figure 4. The network architecture of our method. The original Wan 2.2 model does not accept video input. It only contains generation tokens (red), which the model denoises to generate video. Parallel to these generation tokens, we encode the input human video into condition tokens (blue) and concatenate all tokens. During self-attention, we apply mask to prevent condition tokens from attending to generation tokens. At the output, we only retain the generation tokens and decode them to produce the edited video. Table 1. Quantitative baseline comparison. This table presents two-part evaluation. The first set of columns (Similarity Metrics) measures performance on our synthetic videos, where ground-truth data is available; we report PSNR, SSIM, and MSE (computed on pixel values [0, 255]). The second set (User Preference Rate) evaluates performance on real-world Ego-Exo4D [9] videos, which lack ground truth. Here, we report the percentage of times users preferred method across four criteria, allowing multiple bests. The results show our method, which does not require manual per-video labeling, significantly outperforms all baselines across every metric. Method Manual Per-Video Label Require Require Kling [20] MoCha [42] Runway Aleph [39] Not require Ours Not require SSIM Similarity Metrics (Synthetic Videos) PSNR (dB) 16.952 17.163 17.683 21.836 MSE ([0, 255]) 1518.785 1534.145 1295.640 459.302 0.288 0.315 0.402 0.671 User Preference Rate (Ego-Exo4D Videos) Motion Consistency 17.2% 20.7% 0.0% 69.0% Background Consistency 6.9% 24.1% 10.3% 75.9% Embodiment Consistency 3.79% 20.7% 3.4% 62.1% Video Quality 3.1% 13.8% 3.4% 62.1% Model Finetuning We performed rank-96 LoRA finetuning on Wan2.2-TI2V-5B model for 500 steps. We used four NVIDIA H200 GPUs using Distributed Data Parallel (DDP) with batch size of 1 per GPU. We employ the AdamW optimizer with learning rate of 1e-4, 50-step linear warmup, weight decay of 1e-2, and betas of 0.9 and 0.95. This finetuning process completed in 2.5 hours and consumed approximately 6.4% of the synthetic data. Evaluation and Metrics We conduct quantitative evaluations using two-pronged approach. Primarily, to assess performance on real-world Ego-Exo4D [9] human videos (which lack ground truth), we conduct user studies. 29 participants with computer vision or robotics backgrounds compared our method against baselines, evaluating 10 clips each. They selected the best-performing method(s) across four criteria: motion consistency, background consistency, embodiment consistency (similarity to the Tesla Optimus humanoid), and overall video quality. We report the preference rate (the percentage of times method was chosen as best). Secondly, on our synthetic validation set where ground-truth videos are available, we quantitatively measure similarity using PSNR, SSIM, and MSE. 4.2. Baseline Comparison Baseline Methods We adopt Aleph [39], Kling [20], and MoCha [42] as our baseline methods. All are imageconditioned video editing models or systems (as introduced in Sec. 2). Kling and MoCha additionally require manually labeled mask image (created with the help of SAM [18]), specifying the character to edit. Our method best preserves the original characters motion. Quantitatively, as shown in Tab. 1, 69.0% of users considered our results to have the best or joint-best motion consistency with the original video. Qualitatively, our method demonstrates better motion consistency than 5 Figure 5. Qualitative comparison to baselines. The videos are sampled at frames 15, 30, 45, 60, and 75. MoCha [42] generally maintains the original characters motion, but it often produces incorrect details, such as the upper arm shape in (a) and the leg pose in (b). Kling [20] generates the correct embodiment but fails on motion consistency, resulting in unsynchronized actions and undesirable scene alternations (e.g., dropping the green bag to the ground instead of putting it back in (a)). Runway Aleph [39] fails to produce both the correct robot embodiment (a) and the correct motion (b). In contrasts, our method successfully generates the correct embodiment and motions synchronized with the original human. Table 2. Ablation on model architecture. Similarity metrics are measured between output and ground-truth videos on the validation set (synthetic). Our 5B model achieves the best performances. VACE 1.3B VACE 14B Ours 5B Ours 14B PSNR (dB) SSIM MSE ([0, 255]) 16.424 0.265 1906.218 18.018 0.349 1240.809 21.836 0.671 459. 21.311 0.598 527.561 Table 3. Computational costs for our 5B and 14B models, normalized per video frame (batch size is 1). The 14B models training and inference times are over 10 times greater than the 5B models, making it unsuitable for efficiently robotizing web-scale videos. Training Time (s/video frame) Inference Time (s/video frame) 0.10 5.00 1.05 69.16 Ours 5B Ours 14B MoCha, as evidenced by the leg poses in Fig. 5(a) and the hand movement when grasping the green package in Fig. 5(b). In both figures, Kling and Aleph produces noticeably inconsistent and motions from the original video: even the robots position and orientation could be different. Our method best preserves the robot embodiments appearance. As shown in Tab. 1, 62.1% of users rated our generated robot as most consistent (or tied for most consistent) with Tesla Optimus appearance, followed by Kling at 37.9%. Qualitatively, our method accurately reproduces key details of the robots shoulder and leg joints, which baseline methods more oftenly fail to capture. According to survey feedback, the main factor affecting user preference for our results is the (face) mask material, as our method renders the reflective mask as matte. This is because our synthetic videos exhibits such matte masks (as shown in Fig. 3), and the model has faithfully learned this feature. Our method best preserves other aspects of the video content, achieving overall superior quality. Around three quarters of users preferred our video for background consistency and overall quality. As shown in Fig. 5, both our method and MoCha largely maintain environmental consistency with the original video. Kling, however, introduces additional elements (such as faucet and window in Fig. 5(a)), indicating tendency toward video reinterpretation rather than faithful preservation of motion and details. While such reinterpretation can sometimes reduce artifacts and improve perceived quality, it deviates from the intended goal of robotizing videos, and the generated results may not be physically suitable for VLA or world model training. Supp. contains all test set videos robotized by our methods, which are randomly sampled. From them, one can see the usability rate in their own use cases. 7 Figure 6. Qualitative ablation of model architectures. Both VACE models fail to generate the correct robot embodiment, while the 14B version of our model suffers from lower overall video quality. Our 5B model performs the best on this video editing task. Figure 7. Qualitative ablation on the inference prompt. The finetuning prompt (left) performs best. Any other prompts could lead to sub-optimal performances. Table 4. Ablation on finetuning steps. Similarity metrics are measured between output and ground-truth videos on the validation set (synthetic). Although more steps leads to better similarity, this reflects overfitting to the synthetic data, as shown in Fig. 9. PSNR (dB) SSIM MSE ([0, 255]) 500 Steps 21.836 0.671 459.302 1500 Steps 21.989 0.664 439.360 2500 Steps 22.264 0.665 415.034 Figure 8. Visualization of robotizing in-the-wild videos. Our method successfully transforms the human protagonist into the correct humanoid embodiment while preserving motion, background, and overall video quality (e.g., middle-left vs. middle-right). It also robustly handles complex video effects such as pillarboxes (top-right), camera cuts (bottom-left), and motion blur (middle-right and bottom-right). 4.3. Ablation Study Ablation on model choice and architecture. We also tested our method on Wan2.2-T2V-A14B, as well as two VACE [14] models Wan2.1-VACE-1.3B and Wan2.1-VACE-14B (details in Supp. A), because these VACE models also support video-in video-out. As shown in Fig. 6, neither VACE model was able to generate correct robot embodiment, while our 14B model produced lower visual quality than the 5B version. This is also supported by quantitative evaluations on the validation set (Tab. 2). Moreover, the 14B model increased computation time by at least tenfold (Tab. 3). Therefore, our method based on Wan2.2-TI2V-5B represents the optimal choice. Ablation on finetuning steps. We finetune our model for 500 steps for an optimal performance. With too few steps (e.g., 200), the robot appears to be simply overlaid onto the video frame, lacking correct occlusion with the environment, and the embodiment itself looks less realistic (see Fig. 9 in Supp. B). Conversely, with longer finetuning (e.g., 700 steps), the model begins to overfit to the synthetic datas domain. Its performance on the validation set continues to improve (Tab. 4), but starts to become worse on real videos, resulting in clearly unreasonable content (such as fusing the bicycles frame with the humanoids leg, as shown in Fig. 9). Ablation on how text prompt affects editing performance. As compared in Fig. 7, using the original finetuning prompt (Humanoid video) achieves the best performance. If using other prompts, the model tends to ignore the text descriptions and prioritizes following the videos original motion, except for the keyword Humanoid. Removing this keyword (right column) leads to generating incorrect robot embodiments, indicating the model has associated this keyword with the robots appearance. 4.4. Robotizing In-the-wild Internet Videos Finally, we apply our method to robotize diverse Internet videos. As shown in Fig. 8, our method seamlessly replaces the protagonist while preserving the original motion (topleft), fine details (middle-left), camera cuts (bottom-left), aspect ratio (top-right), video quality (middle-right), and even complex effects like motion blur (middleand bottomright). We attribute these capabilities to the preservation of Wan 2.2s powerful generative capability. The models suc8 cess across these diverse scenarios demonstrates that our video editing approach can effectively robotize web-scale in-the-wild videos, potentially generating substantial data for training robot policies and world models. 5. Conclusion In this work, to address the data scarcity problem, we studied how to generatively edit human videos into humanoid videos. The editing was achieved by video-to-video model finetuned on self-created synthetic video pairs. By releasing both the 17-hour synthetic dataset and 60-hour robotized version of Ego-Exo4D, we provide valuable new resources for research in humanoid VLA and world models. Limitations and Future Analysis As pioneering work to robotize exocentric human videos, we mainly consider single-person videos, leading to undefined behavior in multi-person scenes (e.g., Fig. 8, bottom-left); future work could address this by adding explicit controls. Our method also requires training new LoRAs for new humanoid embodiments, while future work could extend to one-shot, image-conditioned methods."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. 1 [2] Philip J. Ball, J. Bauer, F. Belletti, et al. Genie 3: new frontier for world models, 2025. 3 [3] Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, and Shubham Tulsiani. Track2act: Predicting point tracks from internet videos enables generalizable robot manipulaIn European Conference on Computer Vision, pages tion. 306324. Springer, 2024. 2 [4] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: vision-language-action flow model for general robot control, 2024. 2 [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka 9 Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for realworld control at scale, 2023. 2 [6] Google Deepmind. Veo 3. https : / / deepmind . google/technologies/veo/veo3/, 2025. Accessed: 2025-11-08. 3 [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 4 [8] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video, 2022. 2 [9] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, FuJen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin Liang, JiaWei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas Bertasius, David Crandall, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, and Michael Wray. Egoexo4d: Understanding skilled human activity from firstand third-person perspectives, 2024. 1, 2, 4, 5 [10] David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. 2 [11] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination, 2020. 2 [12] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world, 2024. 2 [13] Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, Loic Magne, Ajay Mandlekar, Avnish Narayan, You Liang Tan, Guanzhi Wang, Jing Wang, Qi Wang, Yinzhen Xu, Xiaohui Zeng, Kaiyuan Zheng, Ruijie Zheng, Ming-Yu Liu, Luke Zettlemoyer, Dieter Fox, Jan Kautz, Scott Reed, Yuke Zhu, and Linxi Fan. Dreamgen: Unlocking generalization in robot learning through video world models, 2025. [14] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing, 2025. 3, 8, 1 [15] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling, 2025. 3 [16] Simar Kareer, Dhruv Patel, Ryan Punamiya, Pranay Mathur, Shuo Cheng, Chen Wang, Judy Hoffman, and Danfei Xu. Egomimic: Scaling imitation learning via egocentric video, 2024. 2 [17] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An opensource vision-language-action model, 2024. 2 [18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023. 5 [19] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. 3 [20] Kuaishou. Kling. https://klingai.com/, 2024. Accessed: 2025-11-08. 3, 5, 6 [21] Marion Lepert, Jiaying Fang, and Jeannette Bohg. Masquerade: Learning from in-the-wild human videos using dataediting, 2025. 2, 3 [22] Marion Lepert, Jiaying Fang, and Jeannette Bohg. Phantom: Training robots without robots using only human videos, 2025. 2, 3 [23] Guangrun Li, Yaoxu Lyu, Zhuoyang Liu, Chengkai Hou, Jieyu Zhang, and Shanghang Zhang. H2r: human-to-robot data augmentation for robot pre-training from videos, 2025. 2, [24] Haoyun Li, Ivan Zhang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Zhiqin Yang, Zhentao Zhang, Boyuan Wang, Chaojun Ni, Wenkang Qin, Xinze Chen, Yun Ye, Guan Huang, Zhenbo Song, and Xingang Wang. Mimicdreamer: Aligning human and robot demonstrations for scalable vla training, 2025. 3 [25] Qixiu Li, Yu Deng, Yaobo Liang, Lin Luo, Lei Zhou, Chengtang Yao, Lingqi Zeng, Zhiyuan Feng, Huizhi Liang, Sicheng Xu, Yizhong Zhang, Xi Chen, Hao Chen, Lily Sun, Dong Chen, Jiaolong Yang, and Baining Guo. Scalable vision-language-action model pretraining for robotic manipulation with real-life human activity videos, 2025. 2 [26] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, and Tao Kong. Vision-language foundation models as effective robot imitators, 2024. 2 [27] Vincent Liu, Ademi Adeniji, Haotian Zhan, Siddhant Haldar, Raunaq Bhirangi, Pieter Abbeel, and Lerrel Pinto. Egozero: Robot learning from smart glasses, 2025. 2 [28] Yangcen Liu, Woo Chul Shin, Yunhai Han, Zhenyang Chen, Harish Ravichandar, and Danfei Xu. Immimic: Crossdomain imitation from human videos via mapping and interpolation, 2025. 2 [29] Priyanka Mandikal and Kristen Grauman. Dexvip: Learning dexterous grasping with human hand pose priors from video. In Conference on Robot Learning, pages 651661. PMLR, 2022. [30] Jiageng Mao, Sicheng He, Hao-Ning Wu, Yang You, Shuyang Sun, Zhicheng Wang, Yanan Bao, Huizhong Chen, Leonidas Guibas, Vitor Guizilini, Howard Zhou, and Yue Wang. Robot learning from physical world model, 2025. 3 [31] Minimax. Hailuo. https://hailuoai.com/video, 2024. Accessed: 2025-11-08. 3 [32] NVIDIA, :, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth 10 Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klar, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, WeiCheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical ai, 2025. 2 [33] NVIDIA, :, Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, Prithvijit Chattopadhyay, Mike Chen, Yongxin Chen, Yu Chen, Shuai Cheng, Yin Cui, Jenna Diamond, Yifan Ding, Jiaojiao Fan, Linxi Fan, Liang Feng, Francesco Ferroni, Sanja Fidler, Xiao Fu, Ruiyuan Gao, Yunhao Ge, Jinwei Gu, Aryaman Gupta, Siddharth Gururani, Imad El Hanafi, Ali Hassani, Zekun Hao, Jacob Huffman, Joel Jang, Pooya Jannaty, Jan Kautz, Grace Lam, Xuan Li, Zhaoshuo Li, Maosheng Liao, Chen-Hsuan Lin, Tsung-Yi Lin, Yen-Chen Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Seungjun Nah, Yashraj Narang, Abhijeet Panaskar, Lindsey Pavao, Trung Pham, Morteza Ramezanali, Fitsum Reda, Scott Reed, Xuanchi Ren, Haonan Shao, Yue Shen, Stella Shi, Shuran Song, Bartosz Stefaniak, Shangkun Sun, Shitao Tang, Sameena Tasmeen, Lyne Tchapmi, WeiCheng Tseng, Jibin Varghese, Andrew Z. Wang, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Jiashu Xu, Dinghao Yang, Xiaodong Yang, Haotian Ye, Seonghyeon Ye, Xiaohui Zeng, Jing Zhang, Qinsheng Zhang, Kaiwen Zheng, Andrew Zhu, and Yuke Zhu. World simulation with video foundation models for physical ai, 2025. [34] NVIDIA, :, Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Jim Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. Gr00t n1: An open foundation model for generalist humanoid robots, 2025. 2 [35] OpenAI. Sora. https://openai.com/sora/, 2024. Accessed: 2025-11-08. 3 [36] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2025. 3 [37] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation, 2023. [38] Runway. Gen 3. https : / / runwayml . com / research/introducing-gen-3-alpha, 2024. Accessed: 2025-11-08. 3 [39] Runway. Runway aleph. https://runwayml.com/ research / introducing - runway - aleph, 2025. Accessed: 2025-11-08. 2, 3, 5, 6 [40] Kenneth Shaw, Shikhar Bahl, and Deepak Pathak. Videodex: Learning dexterity from internet videos. In Conference on Robot Learning, pages 654665. PMLR, 2023. 2 [41] GigaWorld Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jiagang Zhu, Kerui Li, Mengyuan Xu, Qiuping Deng, Siting Wang, Wenkang Qin, Xinze Chen, Xiaofeng Wang, Yankai Wang, Yu Cao, Yifan Chang, Yuan Xu, Yun Ye, Yang Wang, Yukun Zhou, Zhengyuan Zhang, Zhehao Dong, and Zheng Zhu. Gigaworld-0: World models as data engine to empower embodied ai, 2025. [42] Orange Team. Mocha: End-to-end video character replaceIn GitHub Repository, ment without structural guidance. 2025. 3, 5, 6 [43] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy, 2024. 2 [44] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, 11 Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models, 2025. 2, [45] Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, and Jian Tang. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation, 2025. 2 [46] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation, 2023. 2 [47] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation, 2023. 3 [48] Sicheng Xie, Haidong Cao, Zejia Weng, Zhen Xing, Haoran Chen, Shiwei Shen, Jiaqi Leng, Zuxuan Wu, and Yu-Gang Jiang. Human2robot: Learning robot actions from paired human-robot videos, 2025. 3 [49] Ruihan Yang, Qinxi Yu, Yecheng Wu, Rui Yan, Borui Li, An-Chieh Cheng, Xueyan Zou, Yunhao Fang, Xuxin Cheng, Ri-Zhao Qiu, Hongxu Yin, Sifei Liu, Song Han, Yao Lu, and Xiaolong Wang. Egovla: Learning vision-language-action models from egocentric human videos, 2025. 2 [50] Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, and C. Karen Liu. Twist2: Scalable, portable, and holistic humanoid data collection system, 2025. 3 [51] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation, 2025. [52] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models, 2023. 3 [53] Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, Ya-Qin Zhang, and Xianyuan Zhan. Universal actions for enhanced embodied foundation models, 2025. 2 12 X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details of VACE Models As part of our ablation study in Sec. 4.3, we included two VACE [14] models: Wan2.1-VACE-1.3B and Wan2.1-VACE-14B, both of which utilized vace video as an extra input to accept the source human videos. We adopted the DiffSynth implementation and trained these models on the identical dataset and partitions as our main method. We employed LoRA finetuning, applying it to the QKV and output projections in all attention layers, as well as the first and third layers in all feed-forward networks. Both models were trained using the AdamW optimizer with learning rate of 1e-4. The 1.3B model was trained for 2500 steps over 14 hours, and the 14B model was trained for 16 hours, both using DDP on 4 H200 GPUs. Notably, we initially attempted to use the same fixed Humanoid video prompt as our main method, but this led to numerous failure cases. We therefore switched to using Qwen2.5-VL (Qwen2.5-VL-7B-Instruct) [1] to summarize each video, using this summary as the text prompt, which significantly reduced failures. B. Qualitative Ablation on Finetuning Steps Fig. 9 shows our models editing performance when finetuned with different steps. With too few steps (200), the model fails to generate with correct occlusions, as it overlays the robot in front of the objects, such as the bicycle. With more steps (700), the model starts to overfit to the synthetic data, causing performance degradation when editing real videos. For example, in the top-right video frame, the robots leg is fused with the bicycle frame. Therefore, if there is need, future works could explore how to further stablize the finetuning, such as leveraging prior preservation loss [37] to alleviate overfitting to the datasets domain when training steps is large. C. More on Baseline Comparison Similar to Fig. 5, this section provides additional qualitative visualizations comparing our method with baselines. We first present another comparison sample in Fig. 10. Following this, we present and analyze failure case of our model in Fig. 11 to highlight current limitations and provide insights for future improvements. Additionally, we have included videos of all generation results from our method on the test set (introduced in Sec. 4.1) in the supplementary materials. This subset is randomly sampled, allowing for broader inspection of our methods editing performance and the usability of the output videos. The subset is the one shown on the project page, 1 Figure 9. Qualitative ablation on finetuning steps. Too few steps (200) leads to incorrect occlusions (robot overlayed in front of the bicycle), while more steps (700) leads to overfitting to the synthetic data, causing degraded generation when editing real videos. with screenshots in Fig. 12. D. Network Inference Cost Editing 480p 90-frame video takes approximately 7.5 minutes on an H200 GPU, consuming approximately 56.2GB GPU memory. (End of text, figures follow) Figure 10. Qualitative comparison to baselines. The videos are sampled at frames 15, 30, 45, 60, and 75. MoCha achieves good motion consistency, but fails to generate the correct embodiment. Klings generated embodiment is mostly correct, but the motions (such as arm and leg poses) are not aligned with the original video. For both motion consistency and embodiment correctness, Alephs performance is between Kling and MoCha. In comparison, our method generates the most consistent motions with correct Tesla Optimus embodiment. 2 Figure 11. failure case of our model, with comparison to baselines. The videos are sampled at frames 15, 30, 45, 60, and 75. During editing, our model may at times cause details or small objects near the robot to disappear; for example, in this video, the seat back, which is not clearly distinguished from the background color, disappears. Furthermore, for this scene, generating the leg poses under the table is also challenge. Our method produces slightly wrong occlusion near the knees of the humanoid, while among all methods, only Kling achieved reasonable generation by changing the pose and location of the legs and feet under the table (although this hallucination is also undesired). This highlights that future work can focus on ensuring the correctness of details during the robotizing process. 3 Figure 12. Visualization of all test set videos. This subset is randomly sampled, allowing for broader inspection of our methods editing performance and the usability of the output videos."
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore"
    ]
}