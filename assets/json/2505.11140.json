{
    "paper_title": "Scaling Reasoning can Improve Factuality in Large Language Models",
    "authors": [
        "Mike Zhang",
        "Johannes Bjerva",
        "Russa Biswas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging a lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune a variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across a benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within a single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 2-8%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 0 4 1 1 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Russa Biswas",
            "content": "Department of Computer Science Aalborg University Copenhagen, Denmark {jjz, jbjerva, rubi}@cs.aau.dk"
        },
        {
            "title": "Abstract",
            "content": "Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 28%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research. https://huggingface.co/jjzha/fs1 https://github.com/jjzha/fs"
        },
        {
            "title": "Introduction",
            "content": "Recent progress in large language models (LLMs) has highlighted the benefits of allocating additional computation at inference time, known as test-time scaling (Wu et al., 2024; Muennighoff et al., 2025; Zhang et al., 2025). straightforward method for leveraging this extra compute is to fine-tune models on datasets containing long chains of thought as process with reflection, backtracking, and self-validation (i.e., thinking), which has shown particular promise in mathematical reasoning tasks (Muennighoff et al., 2025). In these settings, specialized reward models can verify intermediate steps (Lightman et al., 2023; Wang et al., 2024a; Wu et al., 2025b). However, for complex factual tasks, it remains unclear whether longer reasoning processes lead to higher factual accuracy. Preprint. Under review. Figure 1: Snapshot of Experiments. We show snapshot of the experiments executed in this study. There are four settings on how question can be answered; (1) direct answer from an instruction-tuned model, (2) step-by-step reasoning via an instruction-tuned model, (3) thinking trajectory, and (4) knowledge-graph enhanced thinking. We show an example of KG paths from Wikidata. We take the paths with the natural language to enhance the reasoning traces.1 In this work, we address this gap by conducting large scale experiments for factual open-domain QA. We extract reasoning traces from state-of-the-art reasoning models, namely QwQ-32B (Qwen Team, 2025) and Deepseek-R1 (DeepSeek-AI, 2025). To improve the accuracy and reliability of these traces, we additionally condition the two models with retrieved knowledge graph (KG) paths. KGs encode facts as directed, labeled graphs over entities and relations, and offer structured foundation for verifying the validity of each reasoning step. We ground the questions from the COMPLEXWEBQUESTIONS dataset (CWQ; Talmor and Berant, 2018), QA benchmark constructed from complex SPARQL queries and human-paraphrased questions, with multi-hop KG paths from Wikidata (Vrandeˇcic and Krötzsch, 2014). We intend to introduce verifiable fact-based scaffold to strengthen by introducing KG paths in the reasoning traces. To study the effectiveness of long reasoning on the multi-hop QA task, we fine-tune six variants of the Qwen2.5 architecture on both the original reasoning traces (rt) and the KG-enhanced traces, which we name fs1 and evaluate them across six benchmarks spanning total of 22.6K questions. An overview of our pipeline is illustrated in Figure 1. Our evaluation includes 183 experimental runs and over 1.7 million generated reasoning traces, allowing us to analyze deeply the effect of reasoning for complex multi-hop QA. This setup enables us to answer the following research question: RQ: To what extent does long reasoning influence the factual generalization capabilities of large language models on complex multi-hop questions? To address this question, our contributions are as follows:"
        },
        {
            "title": "1 Empirical Insights: Through 168 experimental runs, we demonstrate that smaller\ninstruction-tuned models improve factual accuracy with KG-enhanced reasoning traces,\nwhereas larger models show minimal gains. Additionally, parallel scaling experiments on\nsix datasets confirm that increasing test-time compute consistently boosts factual accuracy.\nLastly, a focused budget forcing experiment reveals an optimal reasoning length of around\n2K tokens as an optimal trade-off balancing accuracy and efficiency on our dataset.",
            "content": "1wdt: https://www.wikidata.org/prop/direct/, wd: https://www.wikidata.org/entity/ 2 Table 1: Training Data Statistics. We show the statistics of reasoning traces of QwQ-32B and Deepseek-R1 on CWQ based on the Qwen2.5-32B tokenizer. The original data consists of simply querying the question to the reasoning models, whereas fs1 indicates the statistics when queried with the knowledge graphs. We calculate the performance of the models final answer via exact match, semantic similarity (> 0.5), and LLM-as-a-Judge. We show that fs1 has higher performance. TOTAL Deepseek-R1 QwQ-32B Total Samples Avg. Reasoning Length (subwords) Avg. Answer Length (subwords) Exact Match Sem. Similarity (all-MiniLM-L6-v2) LLM-as-a-judge (gpt-4o-mini) rt fs1 rt 3,509 1,345.4 60.3 0.46 0.50 0.44 3,226 1,135.3 97.1 0.63 0.58 0.61 3,518 1,452.1 73.7 0.56 0.55 0. fs1 2,747 722.0 122.7 0.72 0.63 0.70 rt 7,027 1,398.7 67.0 0.51 0.52 0.49 fs1 5,973 928.7 109.9 0.67 0.60 0. Figure 2: Distribution of Reasoning Traces. In the figure, we show the distribution of the reasoning trace length among the queried models. On the left column, we show the reasoning traces from rt and on the right column, we show fs1. We show that particularly for fs1 and Deepseek-R1, the reasoning length is shorter."
        },
        {
            "title": "3 Open-source Reproducibility: All code, datasets, and models are publicly available,",
            "content": "licensed under MIT, facilitating transparency and future research."
        },
        {
            "title": "2.1 Distilling Reasoning Traces",
            "content": "To obtain reasoning traces, we use ComplexWebQuestions (CWQ; Talmor and Berant, 2018), dataset designed for complex multi-hop questions. The CWQ dataset is created by automatically generating complex SPARQL queries based on Freebase (Bollacker et al., 2008). These queries are then automatically transformed into natural language questions, which are further refined by human paraphrasing. We take the dev. set, which consists of 3,519 questions, to curate the reasoning traces. We query both QwQ-32B (Qwen Team, 2025) and Deepseek-R1 (671B; DeepSeek-AI, 2025). By querying the model directly with question, e.g., What art movement was Pablo Picasso part of?, we retrieve the reasoning traces surrounded by think tokens (<think>...</think>) and force 3 the model to give the final answer to the question in boxed{} format. We show four full examples in Figure 8 and Figure 9 for rt and Figure 10 and Figure 11 for fs1 (Appendix B). 2.2 fs1: Enhancing Reasoning Traces with Knowledge Graphs We attempt to steer the reasoning with KG paths to remove the inaccuracies in the reasoning traces. Since the CWQ dataset consists of entities from Freebase, we align them to their corresponding Wikidata entities. For each question in the dev. set of the CWQ dataset, relevant multi-hop KG paths are extracted from Wikidata using random walks using SPARQL queries as shown in Appendix F. Each QA pair in the dataset may contain multiple valid KG paths, which are linearized graphs. We use the following prompt to obtain the improved reasoning traces: [Question]nnWhile answering the question, make use of the following linearised graph as an inspiration in your reasoning, not as the only answer:nn[KG Paths]. We do this with QwQ-32B and Deepseek-R1 and show also in Table 1 the improved accuracy of the enhanced reasoning traces. Statistics. In Table 1, we compare the reasoning trace statistics of the original traces (rt) and fs1. While rt contains longer reasoning chains (up to 1.4K subwords), fs1 traces are typically shorter. We evaluate reasoning traces using three methods: (1) Exact Match, checking if the boxed{} answer exactly matches or is sub-phrase of any gold answer; (2) Semantic Match, accepting answers with maximum similarity score >0.5; and (3) LLM-as-a-Judge, verifying entity alignment using gpt-4o-mini-2024-07-18. Results show that fs1 achieves higher accuracy, indicating that it contains more factual answers."
        },
        {
            "title": "3.1 Training",
            "content": "We fine-tune six different Qwen2.5 modelsnamely, Qwen2.5-{0.5, 1.5, 3, 7, 14, 32}B-Instructusing both rt and fs1. Our fine-tuning setup adopts hyperparameters following Muennighoff et al. (2025): five epochs of training, sequences capped at 8,192 tokens as most traces do not exceed this amount, batch size of 16, learning rate of 1 105, and 5% warmup rate that decays to zero according to cosine schedule. We also apply weight decay of 1 104. Depending on the model size, we use hardware configurations comprising multiple nodes, each with eight AMD MI250x GPU modules alongside single 64-core AMD EPYC Trento CPU. We refer to Appendix for more details and costs of the experiments. Training is done using standard supervised fine-tuning loss and is defined as the average negative log-likelihood of generating the correct tokens over the generated output only: L(θ) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 1 y(i) y(i) (cid:88) j=1 log pθ(y(i) x(i), y(i) <j), (1) where is the number of samples in the dataset, x(i) and y(i) denote the input and target token x(i), y(i) sequences for the i-th sample, y(i) is the length of the target sequence, and pθ(y(i) <j) is the probability of predicting the token y(i) <j, parameterized by model parameters θ. given input x(i) and previous target tokens y(i)"
        },
        {
            "title": "3.2 Benchmarks and Evaluation",
            "content": "We show the test datasets, licenses, size and short description in Table 2. We have four baselines, namely Qwen2.5-72B-Instruct (Qwen Team, 2024a), QwQ-32B, Deepseek-R1, and o3-mini (OpenAI, 2025). To evaluate our models, we select suite of six QuestionAnswer (QA) benchmarks with total of 22.6K questions. We have four setups for benchmarking the models: 1 All models including baselines are evaluated zero-shot (i.e., only querying the question); 2 the Qwen2.5 suite is queried using chain-of-thought prompting (Wei et al., 2022), where we simply use the prompt [Question]nnThink step-by-step.; 3 we benchmark the Qwen2.5 suite fine-tuned on the original reasoning traces (rt); 4 we benchmark the Qwen2.5 suite fine-tuned on 4 Table 2: Test Benchmark. Overview of the QA test sets used in our evaluation. Test Size"
        },
        {
            "title": "Dataset",
            "content": "CWQ (Talmor and Berant, 2018) apache-2.0 3.5K ExaQT (Jia et al., 2021) cc-by-4.0 GrailQA (Gu et al., 2021) apache-2.0 3.2K 6.8K LC-QuAD 2.0 (Dubey et al., 2019) cc-by-4.0 3.1K Mintaka (Sen et al., 2022) cc-by-4.0 WebQSP (Yih et al., 2016) apache-2."
        },
        {
            "title": "TOTAL",
            "content": "4.0K 2.0K 22.6K Multi-hop QA from WebQuestionsSP with compositional SPARQL queries for Freebase paraphrased by crowd workers. Temporal-QA benchmark combining eight KB-QA datasets, focusing on time-specific queries. Freebase QA dataset with annotated answers and logical forms (SPARQL/S-expressions) across 86 domains. Wikidata/DBpedia QA with paraphrases and SPARQL from 22 templates spanning 10 question types. Multilingual QA (9 languages), entity-linked pairs across diverse domains (English test split). Enhanced WebQuestions with Freebase QA annotated with SPARQL (82% coverage). Figure 3: Data Overlap Analysis. We show data overlap between the training set/benchmarks. On the left, one can observe the count of similar questions when the cosine similarity >0.90 (measured with paraphrase-MiniLM-L6-v2; Reimers and Gurevych, 2019). In the middle, we measure exact match counts. On the right, we show the average pairwise cosine similarity across the full test sets. fs1. For inference of the instruction and reasoning models, we use temperature of 0.7 and top_p of 0.8. In Figure 12 (Appendix C.2), we show examples of each dataset in the test benchmark. Evaluation Metric. Similar to previous studies, e.g., Ma et al. (2025), we report pass@k, which reflects the probability that at least one out of randomly selected completions (drawn from total of completions per problem) is correct. Formally, pass@k is given by: Eproblems is the number of generated completions per problem and is the count of correct completions (Chen et al., 2021). For our benchmarks, we evaluate = {1, 2, 4, 8, 16}. In practice, pass@32 is typically reported for formal theorem-proving tasks, while pass@1 (when only one completion is generated per problem, reduces to standard top-1 accuracy) is standard for math and coding tasks as mentioned by Ma et al. (2025). Since for factuality test-time scaling is not yet established, we report it until = 16 and show in Section 4.1 that = 2 gives the best performance. (nc ) (n k) , where 1 (cid:21) (cid:20) To decide whether an answer is correct or not, our main evaluation approach is using LLM-as-ajudge with gpt-4o-mini-2024-07-18 to determine whether predicted answer is referring to the same real-world entity as the gold answer. The specific prompt is in Figure 13 (Appendix D). One notable limitation of the LLM-as-a-judge paradigm lies in its inability to consistently recognize semantically equivalent entities that differ in surface form. For instance, when evaluating answers like HawaiiAleutian Time Zone versus UTC-10:00, the model often fails to identify them as referring to the same temporal concept. To address this, we also compute semantic similarity using all-MiniLM-L6-v2 (Reimers and Gurevych, 2019) as an additional evaluation metric. We compare the predicted answer in one-to-many approach with the possible gold labels, and the highest 5 Figure 4: Main Results for Benchmark. We show the accuracy for single run for six datasets with four baselines and 24 different Qwen2.5 results. For the Qwen2.5 models, from left-to-right, we show the performance of the original instruction-tuned model, chain-of-thought, fine-tuned on rt, and finetuned on fs1. We show that for the smaller models, rt and fs1 outperform their original counterparts, but become less pronounced the larger the model gets. We show exact numbers in Appendix E. similarity score is considered. Finally, we also calculate an exact match by comparing overlapping sub-phrases and common words between the predicted and gold answers. Possible Data Leakage. In Figure 3, we show the overlap of the lower-cased questions in the training set of ComplexWebQuestions (CWQ_train) versus all the other benchmarks used in our study. On the left, we count the times that the cosine similarity between questions exceeds 0.90. We can see that there is the most overlap between CWQ_train and CWQ_test (254 questions), and the second most is between LC-QuAD 2.0 and ExaQT. In the middle, we show that there is almost to none exact string match between the questions. On the right, we show the average pairwise cosine similarity across the benchmarks and that almost all datasets have an average cosine similarity of 0.2."
        },
        {
            "title": "4 Results and Discussion",
            "content": "We show the main results in Figure 4 in terms of accuracy via LLM-as-a-Judge. All the models are benchmarked on the six datasets mentioned in Table 2. With respect to the baselines, we observe that R1-Distill-Llama-70B and o3-mini perform the best on all six datasets, followed by Qwen2.5-72B-Instruct and QwQ-32B. If we look at the other Qwen2.5 results, we notice that reasoning models perform better until the 1.5B parameter size. From the 3B parameter size onward, the differences in performance become less pronounced compared to their original instruction-tuned version. The detailed results of Exact Match and Semantic Similarity are provided in Table 5 and Table 6 respectively in Appendix In the following subsections, we report and analyze experiments on parallel scaling and budget forcing (Muennighoff et al., 2025)."
        },
        {
            "title": "4.1 Parallel Scaling: Can multiple inferences lead to more factual answers?",
            "content": "Parallel scaling can achieve lower latency by enabling multiple (identical) models to run simultaneously locally (via e.g., multiple GPUs or batching techniques) or via API based methods to generate multiple answers. Formally, parallel sampling entails an aggregation technique that combines inde6 Figure 5: Test-Time Scaling for Factual Reasoning. We show with Qwen2.5-32B-fs1 that parallel scaling via majority voting (measured in any@k or consensus@k) is beneficial for complex multi-hop question answering. pendent solutions into single final prediction, commonly known as best-of-N approach (Chollet, 2019; Irvine et al., 2023a; Brown et al., 2024a; Li et al., 2022). Formally, given set of predictions = {p1, . . . , pN }, the best-of-N method selects prediction as the final output. In this work, we present results using simple majority voting, following previous work (Wang et al., 2022; DeepSeek-AI et al., 2025; Ma et al., 2025). Formally, given set of answers {ai}N i=1 derived from the predictions in , consensus is defined as: consensus@N = arg maxa i=1 1(ai = a). For cases where < , we estimate consensus@k by randomly sampling answers from the complete set of predictions and determining the majority vote accordingly, averaging accuracy over multiple trials using Monte Carlo simulation, following (Ma et al., 2025). In case of ties, we also count it as correct. We evaluate at range of = {1, 2, 4, 8, 16} and 100 trials. Additionally, we also report any@k, it indicates whether at least one of the sampled answers matches the ground truth: any@k = 1(max1ik 1(ai = a) = 1). (cid:80)N In Figure 5, we explore parallel scaling by performing 16 inference runs with Qwen2.5-32B-fs1 on each test dataset. As increases, any@k, indicating whether at least one generation is correct, rises steadily across all benchmarks, demonstrating that parallel sampling markedly boosts the chance of producing correct answer. Consensus@k, which evaluates majority voting among outputs, peaks around = 2 and declines thereafter, suggesting that correct answers appear within the sample set, though they rarely form majority at higher k. Together, these findings confirm that parallel scaling offers meaningful gains for factual reasoning tasks: any@k continues to improve with more samples, whereas consensus@k drops, but has higher final performance, indicating positives of parallel sampling for complex reasoning scenarios."
        },
        {
            "title": "4.2 Budget Forcing: Does longer thinking time induce better factuality in LLMs?",
            "content": "Following Muennighoff et al. (2025), we investigate sequential scaling via interventions at decoding time that control the length of the models reasoning by imposing upper and/or lower bounds on the number of thinking tokens generated. To enforce maximum length, we append the end-of-thinking delimiter (and optionally the phrase Answer:) to prompt the model to terminate its reasoning early and output its best available answer. Conversely, to impose minimum reasoning length, we temporarily suppress the generation of the end-of-thinking delimiter and insert the prompt Wait to encourage continued reflection before providing final answer. We select generation lengths = {256, 512, 1,024, 2,048, 4,096, 8,192}. For this set of experiments, we take the first 1,000 questions from CWQ as test set. 7 Figure 6: Budget Forcing. We show the performance of budget forcing at token budgets of {256, 512, 1,024, 2,048, 4,096, 8,192}. The performance increases the more thinking time we give the model and show that it stagnates after 2,048 token budget. Figure 7: Trade-off Results. We take the Qwen2.5-32B-rt and fs1 and compare both budget forcing (until 2K tokens) and parallel sampling (taking any@k). We evaluate via pass@k. Parallel sampling overtakes budget forcing in both cases for factual reasoning. Figure 6 depicts the behaviours of both Qwen2.5-32B-rt and Qwen2.5-32B-fs1 under this scaling approach. We show that, as anticipated, the performance of both models increase when the generation length increases, though we can observe that this plateaus at 2,048 tokens. As indicated in Table 1, both QwQ-32B and Deepseek-R1 needs between 1-2K tokens to answer each question. Therefore we can hypothesize that in the case of factual reasoning, more than 2K tokens is redundant. In Figure 14 (Appendix G), we show an example of what happens during generation with token budget. Additionally, in Figure 7, we directly compare budget forcing and parallel scaling to illustrate their relative performance trade-offs. Specifically, we observe that parallel scaling consistently outperforms budget forcing by substantial margin in an any@k evaluation setting. This is due to parallel scalings inherent advantage of generating multiple diverse outputs, significantly increasing the likelihood of producing at least one correct solution. Conversely, budget forcing is constrained by its sequential generation approach, limiting its potential for improvement with increasing budget. Thus, for scenarios prioritizing correctness from multiple attempts (any@k), parallel scaling emerges as the more effective strategy."
        },
        {
            "title": "5.1 Graph-enhanced In-context Learning",
            "content": "Recent efforts to enhance the factual consistency of LLMs using KGs have been explored in different directions. Semantic parsing methods (Lan and Jiang, 2020; Ye et al., 2021) convert natural language questions into formal KG queries; the retrieval-augmented method (KG-RAG) (Li et al., 2023; Sanmartin, 2024; Jiang et al., 2023) aims to reduce LLMs reliance on latent knowledge by incorporating explicit, structured information from KG; reasoning on graphs (RoG) model (Luo et al., 2023) generates relation paths grounded by KGs as faithful plans which are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning."
        },
        {
            "title": "5.2 Long Form Factuality",
            "content": "Factuality in NLP involves multiple challenges (Augenstein et al., 2024). Several prior efforts explored factual reasoning, such as notably SAFE (Wei et al., 2024b) and SimpleQA (Wei et al., 2024a). These datasets, however, despite accurate entities, lack explicit grounding in structured knowledge subgraphs. In contrast, Tian et al. (2024) directly address factual accuracy by fine-tuning models on automatically generated preference rankings prioritizing factual consistency."
        },
        {
            "title": "5.3 Test-time Scaling",
            "content": "Parallel and Sequential Test-Time Scaling. Parallel scaling (Best-of-N) generates multiple solutions simultaneously to select the best candidate, enhancing coverage (Chollet, 2019; Irvine et al., 2023a; Brown et al., 2024a; Li et al., 2022). However, common selection methods like majority 8 voting (Wang et al., 2022) or reward-model-based scoring (Christiano et al., 2017; Lightman et al., 2023; Wang et al., 2024b; Wu et al., 2024; Beeching et al., 2025; Pan et al., 2024) often struggle to identify correct solutions, especially at large scales (Brown et al., 2024a; Hassid et al., 2024; Stroebl et al., 2024). In coding, this issue manifests in pinpointing the most functional or error-free snippet among multiple candidates. Our method specifically tackles these selection pitfalls. Sequential scaling iteratively refines solutions, exemplified by chain-of-thought prompting and iterative revision techniques (Wei et al., 2022; Nye et al., 2021; Madaan et al., 2024; Lee et al., 2025; Hou et al., 2025; Huang et al., 2022; Min et al., 2024; Muennighoff et al., 2025; Wang et al., 2024c; Li et al., 2025; Jurayj et al., 2025). Prominent systems such as OpenAIs o1, DeepSeek-R1, Qwens QwQ, and Kimi extensively apply iterative reasoning and backtracking (OpenAI, 2024; DeepSeek-AI et al., 2025; Qwen Team, 2024b; Kimi Team et al., 2025). Hybrid Methods. Hybrid scaling combines parallel and sequential strategies, often utilizing tree search algorithms like Monte-Carlo Tree Search guided by reward models (Silver et al., 2016; Hendrycks et al., 2021; Gao et al., 2024; Li et al., 2024; Snell et al., 2024). For instance, s1 (Muennighoff et al., 2025) employs sequential refinement alongside parallel strategies such as majority voting and tree search to mitigate diminishing returns from purely sequential methods. Methods and Limits of Test-Time Scaling. Test-time scaling methods broadly fall into parallel (Best-of-N) and sequential (iterative refinement) paradigms, with hybrid approaches merging both (Brown et al., 2024b; Irvine et al., 2023b; Levi, 2024). Yet, these methods often face diminishing returns and context window limitations, restricting effective iterative refinements (Brown et al., 2024a; Snell et al., 2024; Wu et al., 2024). Thus, improving the quality of reasoning itself remains essential. When sequential scaling saturates, parallel scaling provides further improvements. Techniques like majority voting and advanced tree-based searches (e.g., REBASE (Wu et al., 2024)) effectively boost coverage and accuracy, though at increased computational cost."
        },
        {
            "title": "5.4 Domain-specific Test-Time Scaling",
            "content": "Test-time scaling also spans specific domains like coding and medicine. Z1-7B optimizes coding tasks through constrained reasoning windows, reducing overthinking while maintaining accuracy (Yu et al., 2025). In medicine, extended reasoning boosts smaller models clinical QA performance significantly (Huang et al., 2025), complemented by structured datasets like MedReason, which enhance factual reasoning via knowledge-graph-guided paths (Wu et al., 2025a), similar to us."
        },
        {
            "title": "6 Conclusion and Outlook",
            "content": "In this work, we have systematically investigated whether longer thinking, in the form of extended chain-of-thought reasoning, yields tangible gains in factual accuracy on complex open-domain QA tasks. By distilling more than 13K original and knowledge-graph-enhanced reasoning traces from state-of-the-art models (QwQ-32B and Deepseek-R1), constructing the fs1 corpus of 6K KGgrounded reasoning trajectories, and fine-tuning six instruction-tuned Qwen2.5 variants across 168 experiments (totaling 1.7M traces) on six diverse benchmarks (22.6K questions), we have shown that smaller models (0.51.5B parameters) can indeed leverage longer, structured reasoning to improve single-run factual accuracy by up to 10 points over their instruction-tuned baselines. In contrast, models beyond 3B parameters exhibit less benefit from induced reasoning. Crucially, test-time scaling, whether via parallel repeated sampling or by expanding token budgets, consistently delivers 2-8 percent accuracy improvements even for our largest fine-tuned models, affirming the value of extra compute at inference for open-domain QA. By releasing all code, model checkpoints, and 1.7M reasoning traces (via both rt and fs1), we provide rich resource for future work on process-level verification, hybrid scaling strategies, and the development of factuality-aware reward models capable of validating intermediate reasoning steps outside mathematical domains. Limitations. Our approach assumes that conditioning on knowledge graph significantly improves the accuracy of reasoning traces, though it does not guarantee perfect outcomes. Additionally, accurately evaluating entity alignment poses challenges; we attempted to mitigate this limitation using LLM-based judgments and semantic similarity metrics, but these methods have their own 9 inherent uncertainties. Additionally, the test datasets used might be on the older side, where we do not have control on whether the data has been included in any type of LLM preor post-training. Future Work. Several future research directions emerge from both the KG and test-time scaling perspectives. One promising avenue is leveraging these reasoning traces to develop process reward models, which are designed for complex reasoning and decision-making tasks where evaluating intermediate (factual reasoning) steps is critical to achieving the desired outcomes. This in fact is crucial step towards more factual LLMs. This can be done together with KGs, one possible example could be (Amayuelas et al., 2025), where they attempt to ground every generation with knowledge graph entry. This can possibly be done during the generation of long reasoning."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "MZ and JB are supported by the research grant (VIL57392) from VILLUM FONDEN. We would like to thank the AAU-NLP group for helpful discussions and feedback on an earlier version of this article. We acknowledge the Danish e-Infrastructure Cooperation for awarding this project access (No. 465001263; DeiC-AAU-N5-2024078 - H2-2024-18) to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through DeiC, Denmark."
        },
        {
            "title": "References",
            "content": "Alfonso Amayuelas, Joy Sain, Simerjot Kaur, and Charese Smiley. 2025. Grounding llm reasoning with knowledge graphs. Preprint, arXiv:2502.13247. Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, and 1 others. 2024. Factuality challenges in the era of large language models and opportunities for fact-checking. Nature Machine Intelligence, 6(8):852863. Edward Beeching, Lewis Tunstall, and Sasha Rush. 2025. Scaling test-time compute with open models. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD 08, page 12471250, New York, NY, USA. Association for Computing Machinery. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. 2024a. Large language monkeys: Scaling inference compute with repeated sampling. ArXiv preprint, abs/2407.21787. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. 2024b. Large language monkeys: Scaling inference compute with repeated sampling. ArXiv preprint, abs/2407.21787. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. François Chollet. 2019. On the measure of intelligence. Preprint, arXiv:1911.01547. Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 42994307. 10 DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv preprint, abs/2501.12948. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 171 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv preprint, abs/2501.12948. Mohnish Dubey, Debayan Banerjee, Abdelrahman Abdelkawi, and Jens Lehmann. 2019. Lc-quad 2.0: large dataset for complex question answering over wikidata and dbpedia. In The Semantic WebISWC 2019: 18th International Semantic Web Conference, Auckland, New Zealand, October 2630, 2019, Proceedings, Part II 18, pages 6978. Springer. Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Interpretable contrastive monte carlo tree search reasoning. ArXiv preprint, Lijie Wen. 2024. abs/2410.01707. Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and Yu Su. 2021. Beyond i.i.d.: Three levels of generalization for question answering on knowledge bases. In Proceedings of the Web Conference 2021, WWW 21, page 34773488, New York, NY, USA. Association for Computing Machinery. Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, and Yossi Adi. 2024. The larger the better? improved llm code-generation via budget reallocation. ArXiv preprint, abs/2404.00725. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Preprint, arXiv:2009.03300. Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. 2025. Advancing language model reasoning through reinforcement learning and inference scaling. ArXiv preprint, abs/2501.11651. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. ArXiv preprint, abs/2210.11610. Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, and Yuyin Zhou. 2025. m1: Unleash the potential of test-time scaling for medical reasoning with large language models. Preprint, arXiv:2504.00869. Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Ziyi Zhu, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, and William Beauchamp. 2023a. Rewarding chatbots for real-world engagement with millions of users. ArXiv preprint, abs/2303.06135. Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Ziyi Zhu, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, and William Beauchamp. 2023b. Rewarding chatbots for real-world engagement with millions of users. ArXiv preprint, abs/2303.06135. Zhen Jia, Soumajit Pramanik, Rishiraj Saha Roy, and Gerhard Weikum. 2021. Complex temporal question answering on knowledge graphs. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, CIKM 21, page 792802, New York, NY, USA. Association for Computing Machinery. Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yaliang Li, and Ji-Rong Wen. 2023. Reasoninglm: Enabling structural subgraph reasoning in pre-trained language models for question answering over knowledge graph. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 37213735. 11 William Jurayj, Jeffrey Cheng, and Benjamin Van Durme. 2025. Is that your final answer? test-time scaling improves selective question answering. ArXiv preprint, abs/2502.13962. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, and 75 others. 2025. Kimi k1.5: Scaling reinforcement learning with llms. ArXiv preprint, abs/2501.12599. Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700. Yunshi Lan and Jing Jiang. 2020. Query graph generation for answering multi-hop complex questions from knowledge bases. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 969974, Online. Association for Computational Linguistics. Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, and Xinyun Chen. 2025. Evolving deeper llm thinking. ArXiv preprint, abs/2501.09891. Noam Levi. 2024. simple model of inference scaling laws. ArXiv preprint, abs/2410.16377. Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. 2025. Llms can easily learn to reason from demonstrations: structure, not content, is what matters! ArXiv preprint, abs/2502.07374. Qingyao Li, Wei Xia, Kounianhua Du, Xinyi Dai, Ruiming Tang, Yasheng Wang, Yong Yu, and Weinan Zhang. 2024. Rethinkmcts: Refining erroneous thoughts in monte carlo tree search for code generation. ArXiv preprint, abs/2409.09584. Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li, Xifeng Yan, Chao Zhang, and Bing Yin. 2023. Graph reasoning for question answering with triplet retrieval. arXiv preprint arXiv:2305.18742. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Gabriel Synnaeve, David Imrie, Peter Ying, Peter Battaglia, and Oriol Vinyals. 2022. Competition-level code generation with alphacode. ArXiv preprint, abs/2203.07814. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. ArXiv preprint, abs/2305.20050. Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning on graphs: Faithful and interpretable large language model reasoning. ArXiv preprint, abs/2310.01061. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. 2025. Reasoning models can be effective without thinking. Preprint, arXiv:2504.09858. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Peter Clark, Soheil Feizi, Eduard Hovy, Dan Roth, Luke Zettlemoyer, and Chris Callison-Burch. 2024. Self-refine: Iterative refinement with self-feedback. Preprint, arXiv:2303.17651. Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Feng Ji, Qi Zhang, and Fuli Luo. 2024. Imitate, explore, and selfimprove: reproduction report on slow-thinking reasoning systems. ArXiv preprint, abs/2412.09413. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. ArXiv preprint, abs/2501.19393. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Wojciech Zaremba, Illya Sutskever, Charles Sutton, Martin Wattenberg, Michael Terry, Jeff Dean, Douglas Eck, Bastien Potier, and Ethan Dyer. 2021. Show your work: Scratchpads for intermediate computation with language models. ArXiv preprint, abs/2112.00114. OpenAI. 2024. learning-to-reason-with-llms/. Accessed: 2025-03-27. Learning to reason with llms. https://openai.com/index/ OpenAI. 2025. Openai o3-mini: Pushing the frontier of cost-effective reasoning. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. 2024. Training software engineering agents and verifiers with swe-gym. ArXiv preprint, abs/2412.21139. Qwen Team. 2024a. Qwen2.5: party of foundation models. Qwen Team. 2024b. Qwq: Reflect deeply on the boundaries of the unknown. Qwen Team. 2025. Qwq-32b: Embracing the power of reinforcement learning. Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Diego Sanmartin. 2024. Kg-rag: Bridging the gap between knowledge and creativity. arXiv preprint arXiv:2405.12035. Priyanka Sen, Alham Fikri Aji, and Amir Saffari. 2022. Mintaka: complex, natural, and multilingual dataset for end-to-end question answering. In Proceedings of the 29th International Conference on Computational Linguistics, pages 16041619, Gyeongju, Republic of Korea. International Committee on Computational Linguistics. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 2016. Mastering the game of go with deep neural networks and tree search. ArXiv preprint, abs/1602.01783. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. ArXiv preprint, abs/2408.03314. Benedikt Stroebl, Sayash Kapoor, and Arvind Narayanan. 2024. Inference scaling flaws: The limits of llm resampling with imperfect verifiers. ArXiv preprint, abs/2411.17501. Alon Talmor and Jonathan Berant. 2018. The web as knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 641651, New Orleans, Louisiana. Association for Computational Linguistics. Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher Manning, and Chelsea Finn. 2024. Fine-tuning language models for factuality. In The Twelfth International Conference on Learning Representations. Denny Vrandeˇcic and Markus Krötzsch. 2014. Wikidata: free collaborative knowledgebase. Communications of the ACM, 57(10):7885. 13 Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. 2024a. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Preprint, arXiv:2312.08935. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024b. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. ArXiv preprint, abs/2203.11171. Yifei Wang, Yuyang Wu, Zeming Wei, Stefanie Jegelka, and Yisen Wang. 2024c. theoretical understanding of self-correction through in-context alignment. ArXiv preprint, abs/2405.18634. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024a. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Zixia Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc Le. 2024b. Long-form factuality in large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, Yihan Cao, Hui Ren, Xiang Li, Xiaoxiao Li, and Yuyin Zhou. 2025a. Medreason: Eliciting factual medical reasoning steps in llms via knowledge graphs. Preprint, arXiv:2504.00993. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. ArXiv preprint, abs/2408.00724. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2025b. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. Preprint, arXiv:2408.00724. Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, and Caiming Xiong. 2021. Rng-kbqa: Generation augmented iterative ranking for knowledge base question answering. arXiv preprint arXiv:2109.08678. Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh. 2016. The value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 201206, Berlin, Germany. Association for Computational Linguistics. Zhaojian Yu, Yinghao Wu, Yilun Zhao, Arman Cohan, and Xiao-Ping Zhang. 2025. Z1: Efficient test-time scaling with code. Preprint, arXiv:2504.00810. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. 2025. What, how, where, and how well? survey on test-time scaling in large language models. Preprint, arXiv:2503.24235. 14 Table 3: Statistics of fs1 Test Benchmark. We show the overall statistics for the fs1 test benchmark used in the study in terms of types of answers and which source the paths come from."
        },
        {
            "title": "Metric",
            "content": "Total entries Answer IDs present Answer IDs from Freebase Main answers (numeric) Main answers (dates) Alias answers (numeric) Alias answers (dates)"
        },
        {
            "title": "Count",
            "content": "22,603 8,168 8,159 1,135 2,853"
        },
        {
            "title": "A Training and Inference",
            "content": "For running Deepseek-R1, o3-mini, and LLM-as-a-judge experiments with gpt-4o, we use APIbased solutions via OpenAI2 or TogetherAI3. The costs of running inference on all data took around 250 USD. For fine-tuning and inferencing the local models. We make use of large HPC cluster with hardware configurations comprising multiple nodes (depending on model size; e.g., 32B models require 4 nodes for training and 1 node for inference), each with eight AMD MI250x GPU modules alongside single 64-core AMD EPYC Trento CPU. For all the experiments it resulted in around 6,500 GPU hours spent. A.1 Environmental Impact We acknowledge that conducting large-scale analysis using LLMs comes with an environmental impact. Experiments were conducted using private infrastructure in [Redacted] running on green energy. cumulative of 6,500 GPU hours of computation was performed on AMD MI250x GPU modules, which has TDP of 500 Watts. The experiments were ran from February to April 2025. During this time, the average carbon efficiency in [Redacted] was 0.085 kg/kW h.4 This means we released about 276 kg of CO2 equivalent. Estimations were conducted using the Machine Learning Impact calculator5 presented in (Lacoste et al., 2019)."
        },
        {
            "title": "B Example Reasoning Traces",
            "content": "We show four full examples in Figure 8 and Figure 9 for rt and Figure 10 and Figure 11 for fs1. For visualization purposes, we split up the specific parts in special tags: <question>Q</question>, <think>T</think>, <gold>G</gold> to indicate what it refers to."
        },
        {
            "title": "C Test Benchmark",
            "content": "C.1 Statistics of Test Benchmark In Table 3, we show breakdown of the test benchmark, where we are investigating what entity IDs are present and what type of questions we can derive from the test benchmark (e.g., numeric answers or date answers). C.2 Examples Test Benchmark In Figure 12, we show for each dataset in the test benchmark an example of how the question and answers look like. 2https://platform.openai.com/ 3https://api.together.ai/ 4According to https://app.electricitymaps.com/map. 5Find the tool here: https://mlco2.github.io/impact. Deepseek-R1 Example (rt) <question> In Family Guy, Seth Macfarlane plays the parent of Chris Griffin. What is his characters name? </question> <think> Okay, so need to figure out the name of the character Seth MacFarlane voices who is the parent of Chris Griffin in Family Guy. Let me start by recalling what know about the show.nnFirst, Family Guy is an animated sitcom created by Seth MacFarlane. Hes known for voicing several characters in the show. The main family is the Griffins. The children in the family are Chris, Meg, and Stewie, and theres also the talking dog Brian. The parents would be the father and mother of these kids.nnI remember that the father is Peter Griffin, who is the bumbling, overweight patriarch of the family. Seth MacFarlane does voice Peter Griffin, along with other characters like Stewie and Brian. The mother is Lois Griffin, but think shes voiced by Alex Borstein, not Seth MacFarlane. So, since the question is about the parent of Chris Griffin that Seth MacFarlane plays, it must be Peter Griffin.nnWait, let me doublecheck. Chris is the son, so his parents are Peter and Lois. Since Seth MacFarlane voices Peter, thats the answer. But just to be thorough, are there any other parents in the show that Seth voices? For example, maybe grandparent? But Chriss grandparents would be Peters parents or Loiss parents. think Peters father is named Francis Griffin, but dont recall if Seth voices him. Loiss parents are Carter and Babs Pewterschmidt, and they are voiced by other actors. So no, the main parent characters voiced by Seth would be Peter. nnAlso, Stewie is the baby, and Brian is the dog, both voiced by Seth, but they arent parents. So yes, the answer should be Peter Griffin. </think> The character voiced by Seth MacFarlane who is the parent of Chris Griffin in *Family Guy* is boxed{Peter Griffin}. <gold> [\"Peter Griffin\", \"Peter Lowenbrou Griffin\"] </gold> Figure 8: Example Instance (rt). successful reasoning trace for simple multi-hop question by Deepseek-R1. LLM-as-a-judge As mentioned, we use gpt-4o-mini-2024-07-18 to evaluate whether the predicted entities refer to the same real-world entity as the gold answers. In Figure 13, we give an example of how we prompt the model."
        },
        {
            "title": "E Exact Numbers",
            "content": "Given the results in Figure 4, we show the exact numbers in Table 4. Also, Table 5 and Table 6 show that results for the Exact Match and the Semantic Similarity scores for all the models. In Table 6, the accuracy values has not been multipled with 100. 16 Table 4: Exact Numbers of Main Results. We show the exact numbers from Figure 4. MODEL CWQ ExaQT GrailQA LCQuAD2.0 Mintaka WebQSP Qwen2.5-72B-Instruct QwQ-32B R1-Distill-70B o3-mini Qwen2.5-0.5B-Instruct + cot + rt + fs1 Qwen2.5-1.5B-Instruct + cot + rt + fs1 Qwen2.5-3B-Instruct + cot + rt + fs1 Qwen2.5-7B-Instruct + cot + rt + fs1 Qwen2.5-14B-Instruct + cot + rt + fs1 Qwen2.5-32B-Instruct + cot + rt + fs1 0.514 0.496 0.552 0. 0.192 0.139 0.238 0.243 0.285 0.263 0.338 0.339 0.382 0.351 0.395 0.402 0.465 0.438 0.435 0.353 0.500 0.498 0.485 0.481 0.491 0.497 0.485 0.491 0.491 0.458 0.551 0.497 0.123 0.102 0.176 0.195 0.242 0.207 0.274 0.271 0.284 0.269 0.309 0.306 0.343 0.335 0.347 0.247 0.409 0.418 0.427 0.417 0.427 0.432 0.438 0.425 0.418 0.402 0.423 0.438 0.164 0.159 0.218 0.246 0.274 0.258 0.305 0.326 0.310 0.312 0.346 0.327 0.365 0.345 0.362 0.286 0.402 0.397 0.395 0.367 0.394 0.388 0.402 0.378 0.410 0.409 0.431 0. 0.157 0.163 0.239 0.251 0.270 0.249 0.307 0.317 0.311 0.293 0.321 0.302 0.393 0.359 0.354 0.287 0.403 0.396 0.388 0.369 0.381 0.379 0.390 0.374 0.748 0.741 0.782 0.774 0.174 0.170 0.245 0.246 0.344 0.316 0.413 0.406 0.440 0.472 0.530 0.492 0.551 0.573 0.587 0.393 0.643 0.700 0.700 0.687 0.688 0.704 0.703 0.690 0.632 0.612 0.658 0.680 0.284 0.221 0.332 0.345 0.422 0.392 0.451 0.466 0.482 0.477 0.511 0.504 0.571 0.522 0.540 0.452 0.592 0.592 0.593 0.607 0.607 0.613 0.588 0.597 QwQ-32B Example (rt) <question> What Eastern Europe main religion worshipers also attend St. Marys Cathedral, in Batticaloa, Sri Lanka? </question> <think> Okay, so the user is asking about the main religion worshippers in Eastern Europe who attend St. Marys Cathedral in Batticaloa, Sri Lanka. Hmm, first need to break this down. Let me start by recalling that Batticaloa is city in Sri Lanka, which is in South Asia. The cathedral mentioned is Christian church, specifically Catholic cathedral since its named St. Marys.nnNow, the question is about Eastern Europeans who worship there. Wait, Eastern Europes main religions are predominantly Christian, like Eastern Orthodox or Roman Catholic in some countries. But Sri Lankas main religions are Buddhism, Hinduism, Islam, and then theres Christian minority. So why would Eastern Europeans be attending cathedral there? Maybe theres community of Eastern Europeans living or working in Batticaloa? Or perhaps its question about the cathedrals affiliation?nnAlternatively, maybe theres confusion in the question. St. Marys Cathedral in Batticaloa is likely part of the Roman Catholic Church. Eastern Europeans who are Catholic, like from countries such as Poland, Lithuania, or parts of Croatia, might attend. But Eastern Orthodox Christians would typically go to their own churches. Since the cathedral is Catholic, the main worshippers there would be local Catholics, but the question specifies Eastern Europeans. nnWait, perhaps the user is thinking of the term \"Eastern Europe\" in different context. Maybe theyre referring to the Eastern Catholic Churches, which are in communion with Rome but have Eastern rites. For example, Ukrainian Catholics or Ruthenian Catholics. But would they be in Sri Lanka? Maybe theres small expat community or missionary group. Alternatively, maybe the question is mixing up terms, and the answer is Roman Catholic, as thats the cathedrals denomination, and Eastern Europeans of that faith would attend. nAlternatively, could there be confusion with Batticaloas demographics? The area has significant Tamil population, but Tamils are mostly Hindu or Christian. The cathedral serves the local Catholic community, which includes Sri Lankan Tamils and others. But the question specifically asks about Eastern Europeans. Maybe the answer is Roman Catholics from Eastern Europe, but need to confirm if theres specific group. nnAlternatively, perhaps the question is trick, and the main religion in Eastern Europe (like Orthodox) isnt the one at the cathedral, so the answer is Roman Catholics. Since the cathedral is Catholic, Eastern European Catholics would be the ones attending. So the main religion worshippers there from Eastern Europe would be Roman Catholics. nnI think thats the most logical answer here. The cathedral is Catholic, so Eastern Europeans who are Roman Catholic would attend. The main religions in Eastern Europe include Orthodox and Catholic, so the Catholic ones from there would be the answer. </think> boxed{Roman Catholics}. <gold> [\"Catholicism\", \"Roman Catholic convert\", \"Roman Catholic\"] </gold> Figure 9: Example Instance (fs1). successful reasoning trace for simple multi-hop question by QwQ-32B. 18 QwQ-32B Example (fs1) <question> What geographic region is the country that used the Ottoman Lira used? While answering the question, make use of the following linearised graph as an inspiration in your reasoning, not as the only answer: Konstantin Mikhailovich Bazili, place of birth, Istanbul Istanbul, work location, Near East. Put your final answer within boxed{}. </question> <think> Okay, lets tackle this question step by step. The user is asking about the geographic region of the country that used the Ottoman Lira. First, need to recall what the Ottoman Lira was. From what remember, the lira was the currency used in the Ottoman Empire. So the country in question here is the Ottoman Empire. But since the Ottoman Empire is historical entity and not current country, the region would refer to where the empire was located. Now, the Ottoman Empire was large and extensive empire that existed from the 14th century until after World War I. Its heartland was in Anatolia (modernday Turkey) and Southeast Europe. The capital was Constantinople, which is now known as Istanbul, Turkey. The empire also controlled much of the Balkans, parts of North Africa, and the Middle East. ... (cutoff) ... Wait, but the question is about the geographic region of the country that used the Ottoman Lira. The country is the Ottoman Empire. The geographic region that it is in is the Near East. So combining the clues provided (Istanbul being part of the Near East), the answer would be the Near East. Alternatively, if they want the specific country now, like Turkey, but since the Ottoman Empire was not Turkey, but Turkey is the successor state. The question is bit ambiguous. However, given the clues pointing towards Near East as the work location linked to Istanbul, think the intended answer is the Near East region. So the final answer would be Near East. </think> The Ottoman Lira was the currency of the Ottoman Empire, which was centered in the geographic region known historically as the Near East. This region included the empire heartland in Anatolia (modernday Turkey), Istanbul (then Constantinople), parts of Southeast Europe, and the Middle East. The provided clues about Istanbul, the former capital, and its association with the Near East further reinforce this connection. boxed{Near East}. <gold> [\"Near East\"] </gold> Figure 10: Example Instance (fs1). successful reasoning trace for question by QwQ-32B. 19 Deepseek-R1 Example (fs1) <question> What part of the world map is the Government of the Republic of Seychelles located in? While answering the question, make use of the following linearised graph as an inspiration in your reasoning, not as the only answer: Seychellois Creole, country, Seychelles Seychelles, continent, Africa Seychellois Creole, indigenous to, Seychelles Seychelles, continent, Africa. Put your final answer within boxed{}. </question> <think> Okay, so need to figure out where the Government of the Republic of Seychelles is located on the world map. Let me start by recalling what know about Seychelles. think Seychelles is country made up of islands. But where exactly? The user provided linearised graph that mentions Seychelles is in the continent of Africa. Hmm, thats interesting because when think of Africa, usually picture the mainland, but there are also island countries associated with Africa. Let me verify that. Seychelles is an archipelago in the Indian Ocean, right? So its group of islands northeast of Madagascar. Since Madagascar itself is off the southeastern coast of Africa, Seychelles being northeast of there would place it in the Indian Ocean, near the African continent. The graph also mentions Seychellois Creole being indigenous to Seychelles, which makes sense as the local language. So, putting this together, the Government of the Republic of Seychelles is located in the country of Seychelles, which is part of the African continent. Even though its an island nation, geographically and politically, its considered part of Africa. Therefore, the answer should be that the government is located in Seychelles, which is in Africa. </think> The Government of the Republic of Seychelles is located in the country of Seychelles. According to the provided graph, Seychelles is part of the continent of Africa. Although geographically situated in the Indian Ocean northeast of Madagascar, it is politically and regionally classified as part of Africa. boxed{Africa} <gold> [ \"Africa\", \"Afrique\" ] </gold> Figure 11: Example Instance (fs1). successful reasoning trace for question by Deepseek-R1."
        },
        {
            "title": "Benchmark Test Examples",
            "content": "<cwq> Who was the president in 2012 of the country where Unidad de Valor Constante is the chosen currency? Answers: [ \"Rafael Correa\", \"Rafael Correa Delgado\", \"Rafael Vicente Correa Delgado\" ] </cwq> <exaqt> what nba team won the back to back championship in 1989 and 1990? Answer: [ \"Detroit Pistons\" ] </exaqt> <grailqa> lonnie wheeler contributed to book edition published by what publisher? Answer: [ \"Simon & Schuster\" ] </grailqa> <lcquad2.0> Name the shortest street in Sweden. Answer: [ \"Lantm{\"a}teribacken\" ] </lcquad2.0> <mintaka> How many books are in Goosebumps? Answer: [ \"235\" ] </mintaka> <webqsp> where did diego velazquez die? Answer: [ \"Madrid\" ] </webqsp> Figure 12: Text Examples. For each dataset in the benchmark, we show an example. LLM-as-a-Judge (gpt-4o-mini) gold: [Amazon Basin] predicted: The Amazon Rainforest Does the prediction refer to the same realworld entity as the gold answer? Respond only with 0 (no) or 1 (yes). Figure 13: Prompt LLM-as-a-Judge. We show the prompt that we use to prompt GPT-4o-mini for deciding whether the predicted and gold answer refer to the same real-world entity. 21 Table 5: Exact Match Results. Exact Match Accuracy across QA benchmarks."
        },
        {
            "title": "MODEL",
            "content": "CWQ ExaQT GrailQA LCQuAD2.0 Mintaka WebQSP Qwen2.5-72B-Instruct QwQ-32B R1-Distill-70B o3-mini Qwen2.5-0.5B-Instruct + cot + rt + fs1 Qwen2.5-1.5B-Instruct + cot + rt + fs Qwen2.5-3B-Instruct + cot + rt + fs1 Qwen2.5-7B-Instruct + cot + rt + fs1 Qwen2.5-14B-Instruct + cot + rt + fs Qwen2.5-32B-Instruct + cot + rt + fs1 0.577 0.598 0.319 0.650 0.585 0.468 0.497 0.419 0.460 0.444 0.502 0.462 0.426 0.367 0.531 0. 0.481 0.463 0.521 0.539 0.470 0.493 0.550 0.566 0.525 0.538 0.546 0.564 0.392 0.370 0.374 0.429 0.264 0.266 0.178 0.170 0.264 0.222 0.259 0. 0.231 0.214 0.284 0.274 0.319 0.287 0.281 0.267 0.316 0.326 0.319 0.336 0.344 0.357 0.329 0.340 0.324 0.306 0.727 0.396 0.473 0.356 0.391 0. 0.275 0.303 0.372 0.287 0.302 0.214 0.319 0.349 0.322 0.274 0.289 0.358 0.265 0.259 0.303 0.330 0.278 0.301 0.275 0.336 0.371 0.374 0.636 0. 0.394 0.331 0.315 0.248 0.285 0.291 0.332 0.276 0.277 0.237 0.304 0.319 0.337 0.297 0.301 0.330 0.315 0.312 0.327 0.342 0.337 0.342 0.321 0. 0.750 0.738 0.592 0.777 0.332 0.293 0.348 0.307 0.371 0.334 0.447 0.399 0.434 0.419 0.576 0.542 0.562 0.555 0.606 0.466 0.627 0.675 0.692 0. 0.695 0.706 0.684 0.689 0.652 0.653 0.407 0.715 0.554 0.485 0.459 0.407 0.504 0.497 0.451 0.360 0.526 0.474 0.532 0.516 0.659 0.554 0.526 0. 0.612 0.612 0.584 0.564 0.641 0.630 0.579 0.580 Table 6: Semantic Similarity Evaluation Results. Average Semantic Similarity and Accuracy (threshold > 0.5) across QA benchmarks. ExaQT LCQuAD2.0 WebQSP GrailQA Mintaka MODEL CWQ Sim Acc Sim Acc Sim Acc Sim Acc Sim Acc Sim Acc Qwen2.5-72B-Instruct QwQ-32B R1-Distill-70B o3-mini Qwen2.5-0.5B-Instruct + cot + rt + fs Qwen2.5-1.5B-Instruct + cot + rt + fs1 Qwen2.5-3B-Instruct + cot + rt + fs1 Qwen2.5-7B-Instruct + cot + rt + fs Qwen2.5-14B-Instruct + cot + rt + fs1 Qwen2.5-32B-Instruct + cot + rt + fs1 0.642 0.653 0.393 0.659 0.439 0.416 0.559 0. 0.553 0.486 0.618 0.608 0.529 0.492 0.643 0.632 0.553 0.567 0.648 0.620 0.614 0.602 0.665 0.669 0.597 0.592 0.659 0.667 0.543 0.541 0.530 0. 0.353 0.312 0.527 0.497 0.515 0.415 0.623 0.593 0.379 0.327 0.522 0.503 0.504 0.518 0.623 0.604 0.589 0.567 0.639 0.645 0.561 0.556 0.629 0. 0.562 0.573 0.417 0.551 0.301 0.314 0.410 0.398 0.443 0.384 0.503 0.479 0.438 0.419 0.512 0.506 0.439 0.479 0.530 0.471 0.532 0.537 0.562 0. 0.530 0.522 0.575 0.568 0.411 0.415 0.445 0.401 0.140 0.151 0.260 0.253 0.327 0.251 0.450 0.381 0.227 0.205 0.312 0.302 0.362 0.393 0.450 0. 0.474 0.486 0.503 0.511 0.471 0.464 0.528 0.527 0.266 0.415 0.215 0.291 0.169 0.126 0.273 0.233 0.235 0.190 0.308 0.334 0.207 0.143 0.252 0. 0.307 0.271 0.308 0.303 0.296 0.278 0.329 0.344 0.284 0.293 0.306 0.342 0.450 0.446 0.594 0.457 0.286 0.286 0.388 0.370 0.381 0.339 0.444 0. 0.369 0.344 0.426 0.428 0.396 0.398 0.436 0.411 0.431 0.416 0.456 0.450 0.406 0.403 0.453 0.453 0.266 0.286 0.215 0.291 0.134 0.138 0.261 0. 0.257 0.213 0.334 0.311 0.196 0.161 0.246 0.247 0.300 0.293 0.334 0.304 0.332 0.317 0.361 0.356 0.319 0.311 0.355 0.351 0.835 0.835 0.626 0. 0.480 0.452 0.588 0.554 0.597 0.527 0.685 0.666 0.632 0.619 0.758 0.738 0.657 0.704 0.776 0.654 0.766 0.789 0.826 0.819 0.799 0.804 0.825 0. 0.817 0.815 0.776 0.709 0.134 0.399 0.585 0.541 0.598 0.500 0.794 0.659 0.544 0.539 0.718 0.693 0.661 0.709 0.794 0.665 0.776 0.803 0.846 0. 0.820 0.827 0.845 0.835 0.633 0.619 0.571 0.601 0.373 0.422 0.546 0.529 0.546 0.501 0.602 0.594 0.542 0.535 0.611 0.609 0.505 0.551 0.624 0. 0.607 0.616 0.641 0.637 0.613 0.606 0.638 0.640 0.532 0.815 0.490 0.472 0.134 0.343 0.513 0.475 0.517 0.446 0.619 0.578 0.393 0.385 0.496 0. 0.466 0.530 0.619 0.553 0.598 0.627 0.649 0.629 0.616 0.600 0.645 0.638 0.429 0.424 0.794 0.445 0.316 0.282 0.393 0.368 0.368 0.328 0.462 0. 0.385 0.336 0.436 0.446 0.404 0.387 0.437 0.428 0.410 0.396 0.449 0.460 0.389 0.393 0.437 0."
        },
        {
            "title": "The query format to retrieve the Wikidata entities for Freebase entities is given by",
            "content": "SELECT ?wikientity WHERE {?wikientity wdt:P646 $FREEBASE_ENTITY} The general structure of the SPARQL queries for 2-hop paths between the source and target entities are given by SELECT ?p1 ?p1Label ?o1 ?o1Label ?p2 ?p2Label WHERE {wd:$SOURCE_ENTITY ?p1 ?o1. ?o1 ?p2 wd:$TARGET_ENTITY} SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],mul,en\". } SELECT ?o1 ?o1Label ?p1 ?p1Label ?p2 ?p2Label WHERE { ?p1 ?o1. ?o1 ?p2 wd:$TARGET_ENTITY} SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],mul,en\". }"
        },
        {
            "title": "Budget Force Example",
            "content": "Question: \"Where is the University of the Rockies in country where youd also find El Paso County?\" ### <Budget: 256> ### Okay, lets try to figure out where the University of the Rockies is located, specifically in country that also has El Paso County. First, need to recall some information about the University of the Rockies. think its private university, maybe in the United States. But Im not entirely sure. Let me start breaking this down step by step. The question mentions El Paso County. know theres an El Paso County in Texas, USA. El Paso itself is city in Texas, right on the border with Mexico. So if the country in question is the United States, then El Paso County is there. Now, the University of the Rockies does that name sound familiar? think its in Colorado. Wait, Colorado has county named El Paso as well? Hmm, let me check that. No, wait, El Paso County in Colorado? might be mixing things up. Let me think again. Texas has El Paso County, and theres also an El Paso County in <cutoff due to budget> <im_start>answer Answer: The University of the Rockies is located in the United States, specifically in Colorado. In the same country, El Paso County can be found in Texas. The linearised graph indicates that the university is in the United States, and since El Paso County exists in Texas (a state within the U.S.), the answer aligns with the given context. boxed{United States} # Wrong ### <Budget: 512> ### Okay, lets try to figure out where the University of the Rockies is located, specifically in country that also has El Paso County. First, need to recall some information about the University of the Rockies. think its private university, maybe in the United States. But Im not entirely sure. Let me start breaking this down step by step. The question mentions El Paso County. know theres an El Paso County in Texas, USA. El Paso itself is city in Texas, right on the border with Mexico. So if the country in question is the United States, then El Paso County is there. Now, the University of the Rockies does that name sound familiar? think its in Colorado. Wait, Colorado has county named El Paso as well? Hmm, let me check that. No, wait, El Paso County in Colorado? might be mixing things up. Let me think again. Texas has El Paso County, and theres also an El Paso County in Colorado? Or maybe not. Let me verify. Wait, actually, El Paso County is indeed in Colorado. The capital of Colorado is Denver, and El Paso County is one of the counties there. So if the country is the United States, then both the University of the Rockies and El Paso County (in Colorado) would be there. But need to confirm where the University of the Rockies is located. nLooking at the linearised graph provided: University of the Rockies is in the country United States. Then the United States contains Colorado. The motto of Colorado is \"Nil Sine Numine,\" which is Latin for \"Nothing Without Providence.\" So according to this graph, the University of the Rockies is in the United States, and since El Paso County is in Colorado (which is in the US), that fits. Therefore, the answer should be that the University of the Rockies is in Colorado, United States, where El Paso County is also located. But wait, the user mentioned \"a country where youd also find El Paso County.\" Since El Paso County is in the US (specifically Colorado, according to the graph), the country is the United States. So the University of the Rockies is in the US, specifically in Colorado. Therefore, the answer is Colorado <cutoff due to budget> <im_start>answer Answer: The University of the Rockies is located in the United States, specifically in the state of Colorado. This aligns with the provided graph, which states that the university is in the country United States and that Colorado (with its motto \"Nil Sine Numine\") is part of the U.S. Since El Paso County is also situated in Colorado, the answer is: boxed{Colorado} # Correct Figure 14: Budget Forcing Output Example. We show an example of question where we apply budget forcing, when the budget becomes more, the answer changes."
        },
        {
            "title": "G Budget Forcing Output Examples",
            "content": "In Figure 14, we show an example of what happens when the generation budget is limited to 256 and 512 tokens. The thinking process of model gets cut off once 256 or 512 tokens is reached and we force an answer with the answer special token. We show that in case of 256-token budget the answer is incorrect compared to 512-token budget."
        }
    ],
    "affiliations": [
        "Department of Computer Science Aalborg University Copenhagen, Denmark"
    ]
}