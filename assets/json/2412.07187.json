{
    "paper_title": "A New Federated Learning Framework Against Gradient Inversion Attacks",
    "authors": [
        "Pengxin Guo",
        "Shuang Zeng",
        "Wenhao Chen",
        "Xiaodan Zhang",
        "Weihong Ren",
        "Yuyin Zhou",
        "Liangqiong Qu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Federated Learning (FL) aims to protect data privacy by enabling clients to collectively train machine learning models without sharing their raw data. However, recent studies demonstrate that information exchanged during FL is subject to Gradient Inversion Attacks (GIA) and, consequently, a variety of privacy-preserving methods have been integrated into FL to thwart such attacks, such as Secure Multi-party Computing (SMC), Homomorphic Encryption (HE), and Differential Privacy (DP). Despite their ability to protect data privacy, these approaches inherently involve substantial privacy-utility trade-offs. By revisiting the key to privacy exposure in FL under GIA, which lies in the frequent sharing of model gradients that contain private data, we take a new perspective by designing a novel privacy preserve FL framework that effectively ``breaks the direct connection'' between the shared parameters and the local private data to defend against GIA. Specifically, we propose a Hypernetwork Federated Learning (HyperFL) framework that utilizes hypernetworks to generate the parameters of the local model and only the hypernetwork parameters are uploaded to the server for aggregation. Theoretical analyses demonstrate the convergence rate of the proposed HyperFL, while extensive experimental results show the privacy-preserving capability and comparable performance of HyperFL. Code is available at https://github.com/Pengxin-Guo/HyperFL."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 7 8 1 7 0 . 2 1 4 2 : r a"
        },
        {
            "title": "A New Federated Learning Framework Against Gradient Inversion Attacks",
            "content": "Pengxin Guo1*, Shuang Zeng2*, Wenhao Chen1, Xiaodan Zhang3, Weihong Ren4, Yuyin Zhou5, Liangqiong Qu1 1School of Computing and Data Science, The University of Hong Kong 2 Department of Mathematics, The University of Hong Kong 3 College of Computer Science, Beijing University of Technology 4 School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen 5 Department of Computer Science and Engineering, UC Santa Cruz {guopx, zengsh9}@connect.hku.hk, wc365@cam.ac.uk, zhangxiaodan@bjut.edu.cn, renweihong@hit.edu.cn, zhouyuyiner@gmail.com, liangqqu@hku.hk Abstract Federated Learning (FL) aims to protect data privacy by enabling clients to collectively train machine learning models without sharing their raw data. However, recent studies demonstrate that information exchanged during FL is subject to Gradient Inversion Attacks (GIA) and, consequently, variety of privacy-preserving methods have been integrated into FL to thwart such attacks, such as Secure Multi-party Computing (SMC), Homomorphic Encryption (HE), and Differential Privacy (DP). Despite their ability to protect data privacy, these approaches inherently involve substantial privacyutility trade-offs. By revisiting the key to privacy exposure in FL under GIA, which lies in the frequent sharing of model gradients that contain private data, we take new perspective by designing novel privacy preserve FL framework that effectively breaks the direct connection between the shared parameters and the local private data to defend against GIA. Specifically, we propose Hypernetwork Federated Learning (HyperFL) framework that utilizes hypernetworks to generate the parameters of the local model and only the hypernetwork parameters are uploaded to the server for aggregation. Theoretical analyses demonstrate the convergence rate of the proposed HyperFL, while extensive experimental results show the privacy-preserving capability and comparable performance of HyperFL. Code is available at https://github.com/Pengxin-Guo/HyperFL."
        },
        {
            "title": "Introduction",
            "content": "Deep Neural Networks (DNN) have achieved remarkable success on variety of computer vision tasks, relying on the availability of large amount of training data (Krizhevsky, Sutskever, and Hinton 2012; He et al. 2016; Dosovitskiy et al. 2021; Liu et al. 2021; Wang et al. 2023). However, in many real-world applications, training data is distributed across different institutions, and data sharing between these entities is often restricted due to privacy and regulatory concerns. To alleviate these concerns, Federated Learning (FL) (McMahan et al. 2017; Li et al. 2020a; Qu et al. 2022; Zeng *These authors contributed equally. Corresponding author. Copyright 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Left. Existing methods mainly explore defenses mechanisms on the shared gradients. Such mechanisms, including SMC, HE, and DP, inherently involve substantial privacy-utility trade-offs. Right. novel FL framework that breaks the direct connection between the shared parameters and the local private data is proposed to achieve favorable privacy-utility trade-off. et al. 2024; Guo et al. 2024; Zhang et al. 2024) has emerged as promising approach that enables collaborative and decentralized training of AI models across multiple institutions without sharing of personal data externally. Despite the privacy-preserving capability introduced by FL, recent works (Geiping et al. 2020; Huang et al. 2021; Hatamizadeh et al. 2023) have revealed that FL models are vulnerable to Gradient Inversion Attacks (GIA) (Fredrikson, Jha, and Ristenpart 2015; Zhu, Liu, and Han 2019). GIA can reconstruct clients private data from the shared gradients, undermining FLs privacy guarantees. To remedy this issue, various defense mechanisms have been integrated into FL, including Secure Multi-party Computing (SMC) (Yao 1982; Bonawitz et al. 2017; Mugunthan et al. 2019; Mou et al. 2021; Xu et al. 2022), Homomorphic Encryption (HE) (Gentry 2009; Zhang et al. 2020a,b; Ma et al. 2022; Park and Lim 2022) and Differential Privacy (DP) (Dwork 2006; Geyer, Klein, and Nabi 2017; McMahan et al. 2018; Yu, Bagdasaryan, and Shmatikov 2020; Bietti et al. 2022; Shen et al. 2023). These approaches primarily rely on existing defense mechanisms to enhance privacy protection against GIA without altering the FL framework, as illustrated in the left part of Figure 1. However, such mechanisms, including SMC, HE, and DP, inherently involve substantial privacyutility trade-offs. For example, SMC and HE, while providing strong security guarantees through encrypted information exchange, entail high computation and communication costs, making them unsuitable for DNN models with numerous parameters (Bonawitz et al. 2017; Zhang et al. 2020b). Although DP approaches are easier to implement, they often fall short in providing sufficient model protection while preserving accuracy (Geyer, Klein, and Nabi 2017; McMahan et al. 2018). These trade-offs within current defense algorithms have inspired us to explore alternative privacypreserving methods that strike better balance between privacy and utility, leading to the central question of this paper: Can we design novel FL framework that offers favorable privacy-utility trade-off against GIA without relying on existing defense mechanisms? To this end, we revisit the key to privacy exposure in FL under GIA, which lies in the frequent sharing of model gradients that contain private data. Most efforts aim at exploring various advanced defenses mechanisms on the shared gradients to enhance privacy preservation in FL, as shown in the left part of Figure 1. In contrast, we take new perspective striving for designing novel privacy preserve FL framework that breaks the direct connection between the shared parameters and the local private data to defend against GIA. In order to achieve this, we explore the potential of hypernetworks, class of deep neural networks that generate the weights for another network (Ha, Dai, and Le 2017), as promising solution, as illustrated in the right part of Figure 1 and Figure 2. Specifically, we introduce novel Hypernetwork Federated Learning (HyperFL) framework that adopts dualpronged approachnetwork decomposition and hypernetwork sharing to break the direct connection between the shared parameters and the local private data, as shown in Figure 2. In light of recent findings regarding minimal discrepancies in feature representations and considerable diversity in classifier heads among FL clients (Collins et al. 2021; Xu, Tong, and Huang 2023; Shen et al. 2023), we decompose each local model into shared feature extractor and private classifier, enhancing performance in heterogeneous settings and mitigating privacy leakage risks. To further strengthen privacy preservation, we employ an auxiliary hypernetwork that generates feature extractor parameters based on private client embeddings. Instead of directly sharing the feature extractor, only the hypernetwork parameters are uploaded to the server for aggregation, while classifiers and embeddings are trained locally. This auxiliary hypernetwork sharing strategy breaks the direct connection between shared parameters and local private data, maintaining privacy while enabling inter-client interaction and information exchange. Remarkably, the design of HyperFL is flexible and scalable, catering to diverse range of FL demands through its various configurations. We present two major configurations of HyperFL: (1) Main Configuration HyperFL, suitable for simple tasks with small networks, learns the entire feature extractor parameters directly (see Figure 2); (2) HyperFL for Large Pre-trained Models (denoted as HyperFL-LPM) targets for complex tasks by using pre-trained models as fixed feature extractors and generating trainable adapter parameters via hypernetwork (Houlsby et al. 2019) (see Figure 3). Both theoretical analysis and extensive experimental results demonstrate that HyperFL effectively preserves privacy under GIA while achieving comparable results and maintaining similar convergence rate to FedAvg (McMahan et al. 2017). We hope that the proposed HyperFL framework can encourage the research community to consider the importance of developing new enhanced privacy preservation FL frameworks, as an alternative to current research efforts on defense mechanisms front. We summarize our contributions as follows: To defend against GIA, we take new perspective by designing novel privacy preserve FL framework that effectively breaks the direct connection between the shared parameters and the local private data and propose the HyperFL framework. We present two major configurations of HyperFL: (1) Main Configuration HyperFL, suitable for simple tasks with small networks, learns the entire feature extractor parameters directly; (2) HyperFL-LPM targets for complex tasks by using pre-trained models as fixed feature extractors and generating trainable adapter parameters via hypernetwork. Both theoretical analysis and extensive experimental results demonstrate that HyperFL effectively preserves privacy under GIA while achieving comparable results and maintaining similar convergence rate to FedAvg."
        },
        {
            "title": "2 Related Work\nGradient Inversion Attacks. Gradient Inversion Attacks\n(GIA) (Fredrikson, Jha, and Ristenpart 2015; Zhu, Liu, and\nHan 2019) are adversarial attacks that exploit a machine\nlearning model’s gradients to infer sensitive information\nabout the training data. It iteratively adjusts the input data\nbased on gradients to approximate private attributes or train-\ning samples (Phong et al. 2017; Zhu, Liu, and Han 2019;\nGeiping et al. 2020; Yin et al. 2021; Luo et al. 2022; Geng\net al. 2023; Kariyappa et al. 2023).",
            "content": "Privacy Protection in Federated Learning. To protect the data privacy in FL, additional defense methods have been integrated into FL, such as Secure Multi-party Computing (SMC) (Yao 1982) based methods (Bonawitz et al. 2017; Mugunthan et al. 2019; Mou et al. 2021; Xu et al. 2022), Homomorphic Encryption (HE) (Gentry 2009) based methods (Zhang et al. 2020a,b; Ma et al. 2022; Park and Lim 2022) and Differential Privacy (DP) (Dwork 2006) based methods (Geyer, Klein, and Nabi 2017; McMahan et al. 2018; Yu, Bagdasaryan, and Shmatikov 2020; Bietti et al. 2022; Shen et al. 2023). Apart from these defense methods with theoretical guarantees, there are other empirical yet effective defense strategies, such as gradient pruning/masking (Zhu, Liu, and Han 2019; Huang et al. 2021; Li et al. 2022b), noise addition (Zhu, Liu, and Han 2019; Wei et al. 2020; Huang et al. 2021; Li et al. 2022b), Soteria (Sun et al. 2020), PRECODE (Scheliga, Mader, and Seeland 2022), and FedKL (Ren et al. 2023). However, these methods always suffer from privacy-utility trade-off problems, as illustrated in their Figure 2: The proposed HyperFL framework. HyperFL decouples each clients network into the former feature extractor (; θi) and the latter classifier head g(; ϕi). An auxiliary hypernetwork h(; φi) is introduced to generate local clients feature extractor (; θi) using the clients private embedding vector vi, i.e., θi = h(vi; φi). These generated parameters are then used to extract features from the input xi, which are subsequently fed into the classifier to obtain the output ˆyi, expressed as ˆyi = g(f (xi; θi); ϕi). Throughout the FL training, only the hypernetwork φi is shared, while all other components are kept private, thus effectively mitigating potential privacy leakage concerns. papers. In contrast to these approaches, with the help of hypernetworks (Ha, Dai, and Le 2017), this work proposes novel FL framework that effectively breaks the direct connection between the shared parameters and the local private data to defend against GIA while achieving favorable privacy-utility trade-off. Hypernetworks in Federated Learning. Hypernetworks (Ha, Dai, and Le 2017) are deep neural networks that generate the weights for another network, known as the target network, based on varying inputs to the hypernetwork. Recently, there have been some works that incorporate hypernetworks into FL for learning personalized models (Shamsian et al. 2021; Carey, Du, and Wu 2022; Li et al. 2023b; Tashakori et al. 2023; Lin et al. 2023). All of these methods adopt the similar idea that central hypernetwork model are trained on the server to generate set of models, one model for each client, which aims to generate personalized model for each client. Since the hypernetwork and client embeddings are trained on the server, which makes the server possessing all the information about the local models, enabling the server to recover the original inputs by GIA (see Table 3 and Figure 5 in Appendix). In contrast to existing approaches, this work presents Hypernetwork Federated Learning (HyperFL) framework, which prioritizes data privacy preservation over personalized model generation through the utilization of hypernetworks. more detailed discussion on related work is provided in Appendix."
        },
        {
            "title": "3.1 Problem Formulation\nIn FL, suppose there are m clients and a central server,\nwhere all clients communicate to the server to collabora-\ntively train their models without sharing raw private data.\nEach client i is equipped with its own data distribution P (i)\nXY\non X × Y, where X is the input space and Y is the label",
            "content": "space with categories in total. Let ℓ : R+ denotes the loss function given local model Θi and data point sampled from (i) XY , then the underlying optimization goal of FL can be formalized as follows arg min Θ 1 (cid:88) i= (x,y)P (i) XY [ℓ (Θi; x, y)] , (1) where Θ = {Θ1, Θ2, . . . , Θm} denotes the collection of all local models. In vanilla FL, all clients share the same parameters, i.e., Θ1 = Θ2 = = Θm. In contrast, personalized FL allows for variation in the parameters across clients, enabling Θi to be different for each client. Since the true underlying data distribution of each client is inaccessible, the common approach to achieving the objective (1) is through Empirical Risk Minimization (ERM). That is, assume each client has access to ni i.i.d. data points XY denoted by Di = (cid:8)(cid:0)xl sampled from (i) l=1, whose corresponding empirical distribution is ˆP (i) XY , and we assume the empirical marginal distribution ˆP (i) XY is identical to the true (i) XY . Then the training objective is i, yl (cid:1)(cid:9)ni arg min Θ 1 m (cid:88) i=1 Li(Θi), (2) where Li(Θi) = 1 ni loss over personal training data, e.g., empirical risk. l=1 ℓ(Θi; xl i) is the local average i, yl (cid:80)ni"
        },
        {
            "title": "3.2 Main Configuration HyperFL\nIn the Main Configuration HyperFL framework, which is\nshown in Figure 2, each client i has a classification net-\nwork parameterized by Θi = {θi, ϕi} consists of a feature\nextractor f : X → Rd parameterized by θi , and a clas-\nsifier g : Rd → RK parameterized by ϕi, where d is the\nfeature dimension and K is the number of classes. Addi-\ntionally, each client i has a private client embedding vi and\na hypernetwork h parameterized by φi, which is responsi-\nble for generating the parameters of the feature extractor\nf , i.e., θi = h(vi; φi). In this way, the hypernetwork can\ngenerate personalized feature extractor parameters for each",
            "content": "client by taking the meaningful client embedding as input. The client embeddings can be trainable vectors or fixed vectors, depending on whether suitable client representations are known in advance. In this work, we adopt trainable vectors. Then, the objective (2) can be reformulated as arg min φ,ϕ,v 1 (cid:88) i=1 Li(h(vi; φi), ϕi), (3) where φ = {φ1, φ2, . . . , φm}, ϕ = {ϕ1, ϕ2, . . . , ϕm}, = {v1, v2, . . . , vm}. Note that the feature extractor parameters are generated by the hypernetwork and not trainable, whereas the client embedding, the hypernetwork, and the classifier parameters are trainable. Then, in order to breaks the direct connection between the shared parameters and the local private data to defend against GIA while maintaining competitive performance, each client only uploads the hypernetwork parameters to the server for aggregation while keeping the classifier and the private client embedding trained locally. As illustrated in Figure 2, in each FL communication round, each client uploads its hypernetwork parameters φi to the server once the local training is completed while keeps the classifier parameters ϕi and client embedding vi local to strengthen privacy protection. Then, the server aggregate these φi to obtain the global φ. Next, clients download φ to replace their corresponding local hypernetworks and start the next training iteration. This framework provides natural way for sharing information across clients while maintaining the privacy of each client, by sharing the hypernetwork parameters. We will elaborate on this workflow in the following. Local Training Procedure. For local model training at each round, we first replace the local hypernetwork parameters φi by the received aggregated hypernetwork parameter φ. Then, we perform stochastic gradient decent steps to iteratively train the model parameters as follows: Step 1: Fix φi and vi, update ϕi. Train the classifier parameters ϕi by gradient descent for one epoch: ϕi ϕi ηgϕi ℓ (h(vi; φi), ϕi; ξi) , (4) where ξi denotes the mini-batch of data, ηg is the learning rate for updating the classifier parameters. Step 2: Fix new ϕi, update φi and vi. After getting new classifier, we proceed to update the hypernetwork parameters φi and client embedding vi for multiple epochs: φi φi ηhφi ℓ (h(vi; φi), ϕi; ξi) vi vi ηvvi ℓ (h(vi; φi), ϕi; ξi) , (5) where ηh is the learning rate for updating the hypernetwork parameters and ηv is the learning rate for updating the client embedding. Global Aggregation. Similar to common FL algorithms, the server performs weighted averaging of the hypernetwork parameters as (cid:88) φ = wiφi, (6) i=1 where wi is the aggregation weight for client i, usually determined by the local data size, i.e., wi = ni (cid:80)m . i=1 ni Figure 3: The proposed HyperFL-LPM framework within each client. In this framework, the weights of the pre-trained model are fixed, while only the classifier, hypernetwork, and client embedding are trainable. Note that θ here represents the parameters of the adapters."
        },
        {
            "title": "4.1 Privacy Protection Analysis\nIn this section, we present a comprehensive privacy analysis\nof our HyperFL framework. We consider the most common\nand widely adopted setting, where the server is an honest-\nbut-curious adversary, which obeys the training protocol but\nattempts to obtain the private data of clients according to\nmodel weights and updates (Liu et al. 2022; Li et al. 2023a).",
            "content": "Background: Gradient Inversion Attacks. Given neural network with parameters Θ and the gradients ΘLΘ(x, y) computed with private data batch (x, y), GIA tries to recover x, an approximation of as: arg min Lgrad (x; Θ, ΘLΘ (x, y)) + αRaux (x), (7) where Lgrad (x; Θ, ΘLΘ (x, y)) is gradient loss term used to enforce matching the gradients of recovered batch with the provided gradients LΘ(x, y), Raux (x) is regularization term utilized to regularize the recovered image based on image priors, and α is regularization coefficient. The differences between previous works lie in the choice of gradient loss terms and regularization terms (Zhu, Liu, and Han 2019; Geiping et al. 2020; Yin et al. 2021; Luo et al. 2022; Geng et al. 2023; Kariyappa et al. 2023). For example, Zhu et al. (Zhu, Liu, and Han 2019) use ℓ2-distance as Lgrad but do not use regularization term Raux . Geiping et al. (Geiping et al. 2020) adopt cosine similarity as Lgrad and the total variance as Raux . Luo et al. (Luo et al. 2022) utilize cosine similarity as Lgrad and apply two types of regularization within Raux : one gradient regularization for fully connected layer and another total variation regularization for convolution features. Geng et al. (Geng et al. 2023) adopt ℓ2-distance as Lgrad and divide Raux into three terms, total variation on the input x, clip and scale operation on the input x. Analysis on Proposed Leakage Defense. Unlike previous FL models where the entire model parameters are uploaded to the server for aggregation, the HyperFL framework only requires each client to upload the hypernetwork parameters to the server. Therefore, the objective function of an attacker tries to recover from the HyperFL framework should be changed as: arg min Lgrad (x; φ, φLφ,ϕ (x, y, v)) + αRaux (x), (8) where φ denotes the hypernetwork parameters, ϕ is the parameters of the classifier, is the client embedding. Note that there is major difference between objective (8) and objective (7). In objective (7), the server can obtain gradients of the entire model, while in objective (8), it can only access the gradients of the hypernetwork. Then, given that is only exposed to the feature extractor, obtaining the information of the feature extractor is essential to recover x. However, in HyperFL, the parameters of the feature extractor are obtained by inputting the client embedding into the hypernetwork. Therefore, it is necessary to first recover the client embedding. To achieve this pipeline, the objective (8) can be reformulated as bi-level optimization problem: arg min Lgrad (x, ˆv; φ, θ) + αRaux (x) s.t. ˆv = arg min Lgrad (x, v; φ, φLφ,ϕ (x, y, v)) , (9) where is an approximation of v, θ is the parameters of the feature extractor that generated by the hypernertwork h, i.e., θ = h(v; φ), θ = θt θt1 serves as an approximation for the gradient of the feature extractor θ (Zhang et al. 2019). However, challenge arises when solving the lowerlevel subproblem in objective (9). According to the chain rule, φL(x, y, v) = L(x,y,v) ϕ φ . To compute the graϕ dients of the hypernetwork, it is necessary to calculate the gradients of the classifier first 1. However, since the classifier is trained locally and not shared with the server, it is not 1We assume the label information can be obtained (Geiping et al. 2020; Zhao, Mopuri, and Bilen 2020; Yin et al. 2021; Ma et al. 2023). feasible to compute these gradients. As result, the gradients of the hypernetwork cannot be determined, making it challenging to recover the client embedding. One may question whether we can eliminate the need for the classifier in the process of recovering the client embedding. Drawing inspiration from the GIA procedure (Zhu, Liu, and Han 2019; Li et al. 2023a), we can replace the label information that needs to be optimized with the output of the hypernetwork in this context. By employing this approach, its able to bypass the requirement of the classifier. Then, the lower-level subproblem in objective (9) will be reformulated as Lgrad (θ, v; φ, φLφ,ϕ (x, y, v)) , (10) arg min v,θ where θ is the outputs of the hypernetwork. However, previous works have shown that simulating the optimization of both the input and output is challenging (Zhu, Liu, and Han 2019; Zhao, Mopuri, and Bilen 2020; Ma et al. 2023). Therefore, researchers propose to first identify the output and then optimize the input (Zhao, Mopuri, and Bilen 2020; Geiping et al. 2020; Zhu and Blaschko 2021; Yin et al. 2021; Ma et al. 2023; Wang, Liang, and He 2024). Specifically, they can identify the label based on the relationship between the known gradient and label information, as is typically lowdimensional (i.e., simple one-hot vector) (Zhao, Mopuri, and Bilen 2020; Yin et al. 2021; Ma et al. 2023). However, the output of our hypernetwork (i.e., θ) is high-dimensional and complex. This complexity makes it challenging to identify the ground-truth output θ using the known gradient information, and consequently, recovering the embedding becomes difficult. Even if we attempt to optimize both the input and output simultaneously, solving Eq. (10) remains challenging due to the large search space (Zhu, Liu, and Han 2019; Dang et al. 2021; Huang et al. 2021; Kariyappa et al. 2023). Thus, its challenging to recover the client embedding. Furthermore, even if the client embedding can be recovered (albeit with significant error), the input is still difficult to recover due to the same problems (i.e., inability to infer the output first and large search space) encountered when solving the upper-level subproblem in objective (9). In summary, the HyperFL framework effectively safeguards data privacy, as the combination of the hypernetwork, locally trained classifier, and private client embedding renders the recovery of using GIA unattainable, which is also demonstrated in our experiments."
        },
        {
            "title": "4.2 Convergence Analysis\nTo facilitate the convergence analysis of HyperFL, we make\nthe assumptions commonly encountered in literature (Li\net al. 2020b) to characterize the smooth and non-convex op-\ntimization landscape.\nAssumption 1. L1, · · · , Lm are all L-smooth: for all (ϕj,\nφj, vj) and (ϕk, φk, vk), Li(h(vk, φk), ϕk) ≤ Li(h(vj,\nφj), ϕj) + ((ϕk, φk, vk) − (ϕj, φj, vj))∇Li(h(vj, φj),\nϕj) + L\nAssumption 2. Let\nth client’s\ntraining\nents",
            "content": "it-th variance gradifor each variable is bounded: ξt local data uniformly at 2 (ϕk, φk, vk) (ϕj, φj, vj)2 2. step. The in each client sampled from the random at stochastic be of ϕLi (h (vt , (cid:13) σ2 (cid:13)φLi (cid:1)(cid:13) 2 t+1 σ2 (cid:13) i) , ϕt+1 i, φt (vt i, φt i) , ϕt (cid:0)h (vt i, φt , (cid:13) (cid:13)vLi (cid:1)(cid:13) 2 σ2 (cid:13) i, φt i) , ϕt i, ξt ) ϕLi (h (vt (cid:1) φLi (h (vt i) , ϕt+1 (cid:0)h (vt i) , ϕt+1 , ξt for = 1, , m. , ξt i, φt i)2 i, φt i) , ϕ (cid:1) vLi (h Assumption 3. The expected squared norm of stochastic gradients is uniformly bounded, i.e., ϕLi (h (vt i, φt i) , (cid:1)(cid:13) (cid:0)h (vt 2 G2, i, ξt ϕt , ξt i, φt (cid:13) (cid:13) (cid:1)(cid:13) 2 G2 for all = 1, , , ξt (cid:13)vLi (cid:13) and = 0, , 1. Here denotes the total number of every clients training steps. )2 G2, (cid:13) (cid:0)h (vt i, φt (cid:13)φLi i) , ϕt+1 i) , ϕt+1 Then we present the convergence rate for HyperFL. Theorem 1. Let Assumptions 1, 2 and 3 hold and L, , σi, be defined therein. Denote ηmin = min (cid:8)ηg, 1 (cid:9) and as the number of local training iterations between two communication rounds. Then we have 2 ηh, ηv 1 mT (cid:88) (cid:88) i=1 t=1 (cid:104)(cid:13) (cid:13)Lt (cid:13) (cid:13) 2(cid:105) 2 (cid:114) LM G2D 2T , (11) +η2 h+ E1 ηh +(E 1)η D, i, and η2 where L0 η2 min. According to Theorem 1, we can obtain an O( 1 ) convergence rate towards the stationary solution under smooth and non-convex conditions. This convergence rate is comparable to that of FedAvg in the non-convex scenario (Yu, Yang, and Zhu 2019). Furthermore, we can expedite the convergence for Polyak-Lojasiewicz (PL) functions (Karimi, Nutini, and Schmidt 2016), which are commonly encountered in non-convex optimization scenarios. Assumption 4. function is µ-PL function if for some µ > 0, it satisfies (x)2 2µ (cid:18) (x) inf (cid:19) (x) , x. We assume all L1, , Lm are µ-PL functions, and simply denote inf (x) by . Corollary 1. With assumptions as well as ηmin, L, and defined in Theorem 1 and extra Assumption 4, we have (cid:34) 1 (cid:88) i=1 Li (cid:0)h (cid:0)vt i; φt (cid:1) , ϕt (cid:35) (cid:1) (1 2ηminµ)t+1 + ηmin (12) . LM G2 4µ (cid:1)(cid:1) steps, we have ϵ log (cid:0) 1 (cid:35) (cid:1) ϵ. (13) ϵ If we set ηmin µϵ that LM G2 , after (cid:0) 1 (cid:34) 1 m (cid:88) i=1 Li (cid:0)h (cid:0)vt i; φt (cid:1) , ϕt When employing PL functions, the convergence rate of HyperFL is faster than that achieved solely through smoothness assumptions."
        },
        {
            "title": "5.1 Experimental Setup\nDatasets. For the Main Configuration HyperFL, we eval-\nuate our method on four widely-used image classification\ndatasets: (1) EMNIST (Cohen et al. 2017); (2) Fashion-\nMNIST (Xiao, Rasul, and Vollgraf 2017); (3) CIFAR-10\n(Krizhevsky, Hinton et al. 2009); and (4) CINIC-10 (Dar-\nlow et al. 2018). For the HyperFL-LPM, we evaluate our\nmethod on the EMNIST (Cohen et al. 2017) and CIFAR-10\n(Krizhevsky, Hinton et al. 2009) datasets.",
            "content": "Model Architectures. For the Main Configuration HyperFL, simlar to (Xu, Tong, and Huang 2023), we adopt two different CNN target models for EMNIST/Fashion-MNIST and CIFAR-10/CINIC-10, respectively. For the HyperFLLPM, we adopt the ViT (Dosovitskiy et al. 2021) and ResNet (He et al. 2016) pre-trained on the ImageNet dataset (Deng et al. 2009) as the feature extractor. The hypernetworks of HyperFL and HyperFL-LPM both are fully-connected neural network with one hidden layer, multiple linear heads per target weight tensor. The client embeddings are learnable vectors with dimension equals 64. Compared Methods. For the Main Configuration HyperFL, we compare the proposed method with the following approaches: (1) Local-only; (2) FedAvg (McMahan et al. 2017); (3) pFedHN (Shamsian et al. 2021); and some DPbased FL methods, including (4) DP-FedAvg (McMahan et al. 2018); (5) PPSGD (Bietti et al. 2022); and (6) CENTAUR (Shen et al. 2023). For the HyperFL-LPM, we compare our method with (1) Local-only with fixed feature extractor; (2) Local-only with adapter fine-tuning; (3) FedAvg with fixed feature extractor; and (4) FedAvg with adapter fine-tuning. Training Settings. We employ the mini-batch SGD (Ruder 2016) as local optimizer for all approaches, and the number of local training epochs is set to 5. The number of global communication rounds is set to 200 for all datasets. Average test accuracy of all local models is reported for performance evaluation. For privacy evaluation, we adopt the widely used IG (Geiping et al. 2020), state-of-the-art ROG (Yue et al. 2023), and tailored attack method for our defense framework to recover the input images. More details about experimental setup are provided in Appendix."
        },
        {
            "title": "5.2 Experimental Results\nPerformance Evaluation. As demonstrated in Table 1,\nthe performance of all the compared DP-based FL meth-\nods is inferior to FedAvg and Local-only. This is due to\nthe incorporation of DP mechanisms, which adversely af-\nfect model usability and result in decreased performance.\nIn contrast, our proposed HyperFL consistently surpasses\nthese methods across various datasets, demonstrating its out-\nstanding utility. Notably, HyperFL excels in both situations\nwhere Local-only outperforms (i.e., Fashion-MNIST and\nCINIC-10) and where FedAvg prevails (i.e., EMNIST and\nCIFAR-10). This further highlights HyperFL’s adaptability,\nexcelling in both centralized FL scenarios and cases requir-\ning personalization. Furthermore, the learned client embed-",
            "content": "dings, which are meaningful, can be found in the Appendix. Although pFedHN outperforms our method in two scenarios, it exhibits poor defense capability against GIA, as illustrated in Table 3. Method EMNIST Fashion-MNIST CIFAR-10 CINIC20 clients 100 clients 20 clients 100 clients 20 clients 100 clients 20 clients 100 clients 65.47 70.02 70.18 29.12 50.85 52.17 73.03 87.01 88.11 89.80 68.29 79.77 82.94 90.41 75.68 78.87 77.37 45.73 67.24 71.18 80.22 85.93 85.67 87.64 59.88 83.07 84.47 88.28 73.41 72.77 80.86 35.12 68.82 71.16 76. Local-only FedAvg pFedHN DP-FedAvg CENTAUR PPSGD HyperFL Table 1: The comparison of final test accuracy (%) of different methods on various datasets. We apply full participation for FL system with 20 clients, and apply client sampling with rate 0.3 for FL system with 100 clients. 66.11 76.24 80.07 32.03 51.86 53.92 78.73 63.60 57.00 63.88 27.30 48.82 49.98 66.74 64.84 59.11 70.36 29.94 51.01 52.91 72.21 The performance of HyperFL-LPM compared with Local-only and FedAvg is shown in Table 2. From this table, we can see that HyperFL-LPM can achieve comparable performance to baseline adapter fine-tuning methods with different pre-trained models, regardless of whether Localonly or FedAvg performs better. Further results for FedAvg with full parameter tuning (FPT) using ViT on EMNIST and CIFAR-10 are 78.46 and 97.78, respectively. It shows HyperFL-LPM is also comparable to FPT. This highlights the effectiveness of HyperFL-LPM. Arch Local-only Local-only FedAvg FedAvg HyperFL-LPM EMNIST ResNet ViT 72.83 76.95 80.35 80.04 68.99 70.92 75.21 76.42 80.32 79.92 CIFAR75.03 95.40 62.35 92.32 68.57 91.82 75.57 95.56 ResNet ViT 73.57 89.70 Table 2: The comparison of final test accuracy (%) of different methods on various datasets with 20 clients. Fixed feature extractor. Adapter fine-tuning. Privacy Evaluation. The reconstructed results of IG (Geiping et al. 2020) are provided in Table 3, while more results of ROG (Yue et al. 2023) and tailored attack method are provided in Table 5 and Figures 6 and 7 in Appendix. From Table 3, we can observe that the native FedAvg, pFedHN, and pFedHN-PC methods have much higher risk of leaking data information (indicated by the higher PSNR and SSIM values and lower LPIPS value). This can also be seen in the reconstructed images, which closely resemble the original ones, as illustrated in Figure 5 in Appendix. Although introducing DP improves data privacy, there is significant drop in model performance, as shown in Table 1. In contrast, HyperFL achieves similar level of privacy protection while outperforming all DP-based methods and the native FedAvg in terms of model accuracy. EMNIST CIFAR-10 Method PSNR SSIM LPIPS PSNR SSIM LPIPS 32.64 FedAvg pFedHN 31.24 pFedHN-PC 28.38 DP-FedAvg CENTAUR PPSGD HyperFL 7.74 9.52 9.73 7. 0.8925 0.8701 0.8713 0.2978 0.2136 0.1889 0.0526 0.0807 0.0645 0.7051 0.6712 0.6466 0.3010 0. 16.16 16.02 15.80 7.90 9.80 9.70 8.35 0.6415 0.6351 0.6247 0.2716 0.2723 0.2788 0.0536 0.0504 0. 0.3204 0.2882 0.2643 0.2732 0.3132 Table 3: Reconstruction results of IG. Training Efficiency. To validate the training efficiency of the proposed HyperFL framework, we compare the training time of HyperFL with other DP-based FL methods in Table 4. This table clearly shows the efficiency of the proposed HyperFL framework. Specifically, from this table we can see that the proposed HyperFL framework runs faster than all the compared DP-based FL methods and only slightly slower than the FedAvg method. This is because DP-based FL methods often incur additional computation cost due to their privacy-preserving mechanisms, whereas HyperFL achieves faster training by leveraging the advantages of hypernetworks, all while ensuring data privacy. FedAvg DP-FedAvg # Time (s) 23 194 PPSGD CENTAUR HyperFL 210 223 Table 4: Training time of per training round on the EMNIST dataset with 20 clients of different methods. Convergence Evaluation. To validate to convergence of the proposed HyperFL framework, we draw the training loss of FedAvg and HyperFL in Figure 4(a) and the trend of feature extractor parameters variation in Figure 4(b). From Figure 4(a), we can observe that HyperFL almost has the same convergence rate as FedAvg, which demonstrates the convergence property of HyperFL. Moreover, after convergence, the training loss of HyperFL is lower than that of FedAvg, which reflects why HyperFL performs better than FedAvg. Furthermore, in the later stages of the training process, the variation of the feature extractor parameters approaches zero, as depicted in Figure 4(b). This further confirms the convergence property of HyperFL. (a) (b) Figure 4: (a) Average training loss of different methods on the EMNIST dataset with 20 clients. (b) Parameter difference of the generated feature extractor of one client between adjacent training round on the EMNIST dataset with 20 clients."
        },
        {
            "title": "6 Conclusion\nIn this paper, we propose HyperFL, a novel federated learn-\ning framework that “breaks the direct connection” between\nthe shared parameters and the local private data to de-\nfend against GIA. Specifically, this framework utilizes hy-\npernetworks to generate the parameters of the local model\nand only the hypernetwork parameters are uploaded to the\nserver for aggregation to defend against GIA while without\ncompromising performance or incurring heavy computation\noverhead. We hope that the proposed HyperFL framework\ncan encourage the research community to consider the im-\nportance of developing enhanced privacy preservation FL\nframeworks, as an alternative to current research efforts on\ndefense mechanisms front.",
            "content": "Acknowledgments This work was supported by National Natural Science Foundation of China (62306253, 62206075), Guangdong Natural Science Fund-General Programme (2024A1515010233), and UCSC hellman fellowship. References Abadi, M.; Chu, A.; Goodfellow, I.; McMahan, H. B.; Mironov, I.; Talwar, K.; and Zhang, L. 2016. Deep learnIn Proceedings of the 2016 ing with differential privacy. ACM SIGSAC conference on computer and communications security, 308318. Baxter, J. 2000. model of inductive bias learning. Journal of artificial intelligence research, 12: 149198. Bietti, A.; Wei, C.-Y.; Dudik, M.; Langford, J.; and Wu, S. 2022. Personalization improves privacy-accuracy tradeoffs in federated learning. In International Conference on Machine Learning, 19451962. PMLR. Bonawitz, K.; Ivanov, V.; Kreuter, B.; Marcedone, A.; McMahan, H. B.; Patel, S.; Ramage, D.; Segal, A.; and Seth, K. 2017. Practical secure aggregation for privacyIn proceedings of the 2017 preserving machine learning. ACM SIGSAC Conference on Computer and Communications Security, 11751191. Carey, A. N.; Du, W.; and Wu, X. 2022. Robust Personalized Federated Learning under Demographic Fairness Heterogeneity. In 2022 IEEE International Conference on Big Data (Big Data), 14251434. IEEE. Cohen, G.; Afshar, S.; Tapson, J.; and Van Schaik, A. 2017. EMNIST: Extending MNIST to handwritten letters. In 2017 international joint conference on neural networks (IJCNN), 29212926. IEEE. Collins, L.; Hassani, H.; Mokhtari, A.; and Shakkottai, S. 2021. Exploiting shared representations for personalized federated learning. In International conference on machine learning, 20892099. PMLR. Dang, T.; Thakkar, O.; Ramaswamy, S.; Mathews, R.; Chin, P.; and Beaufays, F. 2021. Revealing and protecting labels in distributed training. Advances in Neural Information Processing Systems, 34: 17271738. Darlow, L. N.; Crowley, E. J.; Antoniou, A.; and Storkey, A. J. 2018. Cinic-10 is not imagenet or cifar-10. arXiv preprint arXiv:1810.03505. Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and FeiImagenet: large-scale hierarchical image Fei, L. 2009. database. In 2009 IEEE conference on computer vision and pattern recognition, 248255. Ieee. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations. Dwork, C. 2006. Differential privacy. In International colloquium on automata, languages, and programming, 112. Springer. Erdogan, E.; Kupc u, A.; and icek, A. E. 2022. Unsplit: Data-oblivious model inversion, model stealing, and label In Proceedings of inference attacks against split learning. the 21st Workshop on Privacy in the Electronic Society, 115 124. Fredrikson, M.; Jha, S.; and Ristenpart, T. 2015. Model inversion attacks that exploit confidence information and In Proceedings of the 22nd ACM basic countermeasures. SIGSAC conference on computer and communications security, 13221333. Geiping, J.; Bauermeister, H.; Droge, H.; and Moeller, M. 2020. Inverting gradients-how easy is it to break privacy in federated learning? Advances in Neural Information Processing Systems, 33: 1693716947. Geng, J.; Mou, Y.; Li, Q.; Li, F.; Beyan, O.; Decker, S.; and Improved Gradient Inversion Attacks and Rong, C. 2023. Defenses in Federated Learning. IEEE Transactions on Big Data. Gentry, C. 2009. fully homomorphic encryption scheme. Stanford university. Geyer, R. C.; Klein, T.; and Nabi, M. 2017. Differentially private federated learning: client level perspective. arXiv preprint arXiv:1712.07557. Guo, P.; Zeng, S.; Wang, Y.; Fan, H.; Wang, F.; and Qu, L. 2024. Selective Aggregation for Low-Rank Adaptation in Federated Learning. arXiv preprint arXiv:2410.01463. Ha, D.; Dai, A. M.; and Le, Q. V. 2017. HyperNetworks. In The 5th International Conference on Learning Representations. Hatamizadeh, A.; Yin, H.; Molchanov, P.; Myronenko, A.; Li, W.; Dogra, P.; Feng, A.; Flores, M. G.; Kautz, J.; Xu, D.; et al. 2023. Do gradient inversion attacks make federated learning unsafe? IEEE Transactions on Medical Imaging. He, K.; Chen, X.; Xie, S.; Li, Y.; Dollar, P.; and Girshick, R. 2022. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1600016009. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residIn Proceedings of the ual learning for image recognition. IEEE conference on computer vision and pattern recognition, 770778. He, Z.; Zhang, T.; and Lee, R. B. 2019. Model inversion attacks against collaborative inference. In Proceedings of the 35th Annual Computer Security Applications Conference, 148162. He, Z.; Zhang, T.; and Lee, R. B. 2020. Attacking and protecting data privacy in edgecloud collaborative inference IEEE Internet of Things Journal, 8(12): 9706 systems. 9716. Hore, A.; and Ziou, D. 2010. Image quality metrics: PSNR vs. SSIM. In 2010 20th international conference on pattern recognition, 23662369. IEEE. Houlsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.; De Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly, S. 2019. Parameter-efficient transfer learning for In International Conference on Machine Learning, NLP. 27902799. PMLR. Hu, E. J.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; Chen, W.; et al. 2022. LoRA: Low-Rank Adaptation of In International Conference on Large Language Models. Learning Representations. Huang, Y.; Gupta, S.; Song, Z.; Li, K.; and Arora, S. 2021. Evaluating gradient inversion attacks and defenses in federated learning. Advances in Neural Information Processing Systems, 34: 72327241. Jia, M.; Tang, L.; Chen, B.-C.; Cardie, C.; Belongie, S.; Hariharan, B.; and Lim, S.-N. 2022. Visual prompt tuning. In European Conference on Computer Vision, 709727. Springer. Jiang, X.; Zhou, X.; and Grossklags, J. 2022. Comprehensive analysis of privacy leakage in vertical federated learning during prediction. Proceedings on Privacy Enhancing Technologies. Karimi, H.; Nutini, J.; and Schmidt, M. 2016. Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part 16, 795811. Springer. Karimireddy, S. P.; Kale, S.; Mohri, M.; Reddi, S.; Stich, S.; and Suresh, A. T. 2020. Scaffold: Stochastic controlled averaging for federated learning. In International conference on machine learning, 51325143. PMLR. Kariyappa, S.; Guo, C.; Maeng, K.; Xiong, W.; Suh, G. E.; Qureshi, M. K.; and Lee, H.-H. S. 2023. Cocktail party attack: Breaking aggregation-based privacy in federated learning using independent component analysis. In International Conference on Machine Learning, 1588415899. PMLR. Kingma, D. P.; and Ba, J. 2015. Adam: method for stochastic optimization. ICLR. Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple layers of features from tiny images. Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25. Li, B.; Gu, H.; Chen, R.; Li, J.; Wu, C.; Ruan, N.; Si, X.; and Fan, L. 2023a. Temporal Gradient Inversion Attacks with Robust Optimization. arXiv preprint arXiv:2306.07883. Li, H.; Cai, Z.; Wang, J.; Tang, J.; Ding, W.; Lin, C.-T.; and Shi, Y. 2023b. FedTP: Federated Learning by Transformer Personalization. IEEE Transactions on Neural Networks and Learning Systems. Li, T.; Sahu, A. K.; Talwalkar, A.; and Smith, V. 2020a. Federated learning: Challenges, methods, and future directions. IEEE signal processing magazine, 37(3): 5060. Li, X.; Huang, K.; Yang, W.; Wang, S.; and Zhang, Z. 2020b. On the Convergence of FedAvg on Non-IID Data. In International Conference on Learning Representations. Li, Z.; Wang, L.; Chen, G.; Zhang, Z.; Shafiq, M.; and Gu, Z. 2022a. E2EGI: End-to-End Gradient Inversion in FedIEEE Journal of Biomedical and Health erated Learning. Informatics, 27(2): 756767. Li, Z.; Zhang, J.; Liu, L.; and Liu, J. 2022b. Auditing privacy defenses in federated learning via generative gradient In Proceedings of the IEEE/CVF Conference on leakage. Computer Vision and Pattern Recognition, 1013210142. Lin, Y.; Wang, H.; Li, W.; and Shen, J. 2023. Federated learning with hyper-networkA case study on whole slide image analysis. Scientific Reports, 13(1): 1724. Liu, Z.; Guo, J.; Yang, W.; Fan, J.; Lam, K.-Y.; and Zhao, J. 2022. Privacy-preserving aggregation in federated learning: survey. IEEE Transactions on Big Data. Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, 1001210022. Lowy, A.; and Razaviyayn, M. 2023. Private Federated Learning Without Trusted Server: Optimal Algorithms for Convex Losses. In The Eleventh International Conference on Learning Representations. Luo, Z.; Zhu, C.; Fang, L.; Kou, G.; Hou, R.; and Wang, X. 2022. An effective and practical gradient inversion attack. International Journal of Intelligent Systems, 37(11): 9373 9389. Ma, J.; Naas, S.-A.; Sigg, S.; and Lyu, X. 2022. Privacypreserving federated learning based on multi-key homomorInternational Journal of Intelligent Sysphic encryption. tems, 37(9): 58805901. Ma, K.; Sun, Y.; Cui, J.; Li, D.; Guan, Z.; and Liu, J. 2023. Instance-wise Batch Label Restoration via Gradients in Federated Learning. In The Eleventh International Conference on Learning Representations. McMahan, B.; Moore, E.; Ramage, D.; Hampson, S.; and Arcas, B. A. 2017. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, 12731282. PMLR. McMahan, H. B.; Ramage, D.; Talwar, K.; and Zhang, L. 2018. Learning Differentially Private Recurrent Language In International Conference on Learning RepreModels. sentations. Mou, W.; Fu, C.; Lei, Y.; and Hu, C. 2021. verifiable federated learning scheme based on secure multi-party computation. In International Conference on Wireless Algorithms, Systems, and Applications, 198209. Springer. Mugunthan, V.; Polychroniadou, A.; Byrd, D.; and Balch, T. H. 2019. Smpai: Secure multi-party computation for federated learning. In Proceedings of the NeurIPS 2019 Workshop on Robust AI in Financial Services, 19. MIT Press Cambridge, MA, USA. Nair, V.; and Hinton, G. E. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML10), 807814. Park, J.; and Lim, H. 2022. Privacy-preserving federated learning using homomorphic encryption. Applied Sciences, 12(2): 734. Phong, L. T.; Aono, Y.; Hayashi, T.; Wang, L.; and Moriai, S. 2017. Privacy-preserving deep learning: Revisited and In Applications and Techniques in Information enhanced. Security: 8th International Conference, ATIS 2017, Auckland, New Zealand, July 67, 2017, Proceedings, 100110. Springer. Qu, L.; Zhou, Y.; Liang, P. P.; Xia, Y.; Wang, F.; Adeli, E.; Fei-Fei, L.; and Rubin, D. 2022. Rethinking architecture design for tackling data heterogeneity in federated learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1006110071. Ren, H.; Deng, J.; Xie, X.; Ma, X.; and Ma, J. 2023. Gradient leakage defense with key-lock module for federated learning. arXiv preprint arXiv:2305.04095. Ruder, S. 2016. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747. Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; et al. 2015. Imagenet large scale visual recognition challenge. International journal of computer vision, 115: 211 252. Scheliga, D.; Mader, P.; and Seeland, M. 2022. Precode-a generic model extension to prevent deep gradient leakage. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 18491858. Shamsian, A.; Navon, A.; Fetaya, E.; and Chechik, G. 2021. Personalized federated learning using hypernetworks. In International Conference on Machine Learning, 94899502. PMLR. Shen, Z.; Ye, J.; Kang, A.; Hassani, H.; and Shokri, R. 2023. Share your representation only: Guaranteed improvement of the privacy-utility tradeoff in federated learning. The Eleventh International Conference on Learning Representations. Sun, J.; Li, A.; Wang, B.; Yang, H.; Li, H.; and Chen, Y. 2020. Provable defense against privacy leakage in federated arXiv preprint learning from representation perspective. arXiv:2012.06043. Tashakori, A.; Zhang, W.; Wang, Z. J.; and Servati, P. 2023. SemiPFL: personalized semi-supervised federated learning IEEE Internet of Things framework for edge intelligence. Journal. Van der Maaten, L.; and Hinton, G. 2008. Visualizing data using t-SNE. Journal of machine learning research, 9(11). Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30. Wang, Y.; Liang, J.; and He, R. 2024. Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks. In The Twelfth International Conference on Learning Representations. Wang, Y.; Wang, X.; Dinh, A.-D.; Du, B.; and Xu, C. 2023. Learning to Schedule in Diffusion Probabilistic Models. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Wang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4): 600612. Wei, W.; Liu, L.; Loper, M.; Chow, K.-H.; Gursoy, M. E.; Truex, S.; and Wu, Y. 2020. framework for evaluating gradient leakage attacks in federated learning. arXiv preprint arXiv:2004.10397. Xiao, H.; Rasul, K.; and Vollgraf, R. 2017. Fashion-mnist: novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747. Xu, B.; Wang, N.; Chen, T.; and Li, M. 2015. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853. Xu, J.; Tong, X.; and Huang, S.-L. 2023. Personalized Federated Learning with Feature Alignment and Classifier ColIn The Eleventh International Conference on laboration. Learning Representations. Xu, Y.; Peng, C.; Tan, W.; Tian, Y.; Ma, M.; and Niu, K. 2022. Non-interactive verifiable privacy-preserving federated learning. Future Generation Computer Systems, 128: 365380. Yao, A. C. 1982. Protocols for secure computations. In 23rd annual symposium on foundations of computer science (sfcs 1982), 160164. IEEE. Yin, H.; Mallya, A.; Vahdat, A.; Alvarez, J. M.; Kautz, J.; and Molchanov, P. 2021. See through gradients: Image batch recovery via gradinversion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1633716346. Yu, H.; Yang, S.; and Zhu, S. 2019. Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, 56935700. Yu, T.; Bagdasaryan, E.; and Shmatikov, V. 2020. Salvaging federated learning by local adaptation. arXiv preprint arXiv:2002.04758. Yue, K.; Jin, R.; Wong, C.-W.; Baron, D.; and Dai, H. 2023. Gradient obfuscation gives false sense of security in federated learning. In 32nd USENIX Security Symposium (USENIX Security 23), 63816398. Zeng, S.; Guo, P.; Wang, S.; Wang, J.; Zhou, Y.; and Qu, L. 2024. Tackling data heterogeneity in federated learning via loss decomposition. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 707717. Springer. Zhang, C.; Li, S.; Xia, J.; Wang, W.; Yan, F.; and Liu, Y. 2020a. {BatchCrypt}: Efficient homomorphic encryption for {Cross-Silo} federated learning. In 2020 USENIX annual technical conference (USENIX ATC 20), 493506. Zhang, J.; Zeng, S.; Zhang, M.; Wang, R.; Wang, F.; Zhou, Y.; Liang, P. P.; and Qu, L. 2024. FLHetBench: Benchmarking Device and State Heterogeneity in Federated Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1209812108. Zhang, M.; Lucas, J.; Ba, J.; and Hinton, G. E. 2019. Lookahead optimizer: steps forward, 1 step back. Advances in neural information processing systems, 32. Zhang, M.; Sapra, K.; Fidler, S.; Yeung, S.; and Alvarez, J. M. 2021. Personalized Federated Learning with First OrIn International Conference on der Model Optimization. Learning Representations. Zhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang, O. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, 586595. Zhang, X.; Fu, A.; Wang, H.; Zhou, C.; and Chen, Z. 2020b. privacy-preserving and verifiable federated learning scheme. In ICC 2020-2020 IEEE International Conference on Communications (ICC), 16. IEEE. Zhao, B.; Mopuri, K. R.; and Bilen, H. 2020. Improved deep leakage from gradients. arXiv:2001.02610. Zhu, J.; and Blaschko, M. B. 2021. R-GAP: Recursive GraIn International Conference on dient Attack on Privacy. Learning Representations. Zhu, L.; Liu, Z.; and Han, S. 2019. Deep leakage from gradients. Advances in neural information processing systems, 32. idlg: arXiv preprint (cid:13) (cid:13)ηggt i,ϕ 2(cid:21) (cid:13) (cid:13) (cid:104)(cid:12) (cid:12)gt i,ϕ 2(cid:105) (cid:13) (cid:13) (cid:20) 2 2 LG2 2 , = η2 η2 g"
        },
        {
            "title": "A Proofs of Theoretical Results",
            "content": "i, vt A.1 Proof of Theorem 1 Proof. Let ϕt i, φt be the model parameters maintained in the i-th client at the t-th step. Let IE be the set of global synchronization steps, i.e., IE = {nE = 1, 2, }. If + 1 IE, which represents the time step for communication, then the one-step update of HyperFL can be described as follows: ϕt φt vt SGD of ϕt ϕt+1 φt vt SGD of φt i,vt ϕt+1 φt+1 vt+1 For the second term on the right side of inequality (14), according to the law of total expectation, we have (cid:2)(cid:10)yt xt i, gt i,ϕ (cid:11)(cid:3) = (cid:2)(cid:10)ηggt (cid:11)(cid:3) i,ϕ, gt i,ϕ = (cid:8)E (cid:2)(cid:10)ηggt i,ϕ, gt i,ϕ = (cid:8)E (cid:2)(cid:10)ηggt , gt i,ϕξt i,ϕ = (cid:2)(cid:10)ηg gt (cid:11)(cid:3) i,ϕ, gt i,ϕ i,ϕ)2(cid:3) . = ηgE (cid:2)(gt (cid:9) (cid:11)(cid:3) ξt (cid:11)(cid:3)(cid:9) For the third term on the right side of the inequality (14), we have if t+1IE i(cid:80)m ϕt+1 j=1 wjφt+1 vt+1 . (cid:20) 2 (cid:13) (cid:13)yt xt 2(cid:21) (cid:13) (cid:13) = For convenience, we denote the parameters in each sub-step above as follows: xt = ϕt φt vt , yt = , ϕt+1 φt vt xt+1,1 = ϕt+1 φt+1 vt+1 , xt+1,2 = i(cid:80)m ϕt+1 j=1 wjφt+1 vt+1 xt+1 = if + 1 / IE, if + 1 IE. (cid:26)xt+1,1 xt+1,2 Here, the variable xt+1,1 represents the immediate result of one sub-step SGD update from the parameter of the previous sub-step yt represents the parameter obtained afi ter communication steps (if possible). Furthermore, we denote the learning rate and stochastic gradient of step as follows: , and xt+1, η = (cid:33) , (cid:32) ηg ηh ηv gt = = gt i,ϕ gt i,φ gt i,v ϕLi (h (vt (cid:0)h (vt φLi (cid:0)h (vt vLi gt = = gt i,ϕ gt i,φ gt i,v ϕLi (h (vt (cid:0)h (vt φLi (cid:0)h (vt vLi i, φt i, φt i, φt i, φt i, φt i, φt i, ξt i) , ϕt ) i) , ϕt+1 , ξt i) , ϕt+1 , ξt i) , ϕt i) i) , ϕt+1 i) , ϕt+1 (cid:1) (cid:1) , (cid:1) (cid:1) , where in the last inequality, we use the bounded gradient Assumption 3. From the above inequalities, and taking the expectation of inequality (14), we can get , (cid:2)Li (cid:0)yt (cid:1) Li (cid:0)xt (cid:1)(cid:3) ηgE (cid:2)(gt i,ϕ)2(cid:3) + η LG2 2 . (15) Secondly, by the smoothness of Li, we have (cid:16) Li xt+1,1 (cid:17) Li + (cid:68) (cid:1) + (cid:0)yt (cid:13) (cid:13)xt+1,1 (cid:13) 2 xt+1,1 (cid:69) yt , gt i,ϕ yt (cid:13) 2 (cid:13) (cid:13) . (16) Similar to inequality (15), taking the expectation of inequality (16), we get (cid:2)Li (cid:0)xt+1,1 (cid:1) Li (cid:0)yt (cid:1)(cid:3) ηhE (cid:2)(gt i,φ)2(cid:3) ηvE (cid:2)(gt i,v)2(cid:3) + (η2 + η2 v) (17) LG2 . Thirdly, by the smoothness of Li, we have Li (cid:0)xt+1,2 (cid:1) Li + (cid:0)xt+1,1 (cid:13) (cid:13)xt+1,2 2 (cid:1) + (cid:10)xt+1,2 xt+1,1 , gt i,φ (cid:11) xt+1,1 (cid:13) 2 (cid:13) . (18) where ξt set of client at step t, then [gt is the data uniformly chosen from the local data ] = gt . Next, we apply the inequality of the smoothness Assumption 1 to each sub-step of the one-step update for client i. Firstly, by the smoothness of Li, we have From the iterative formula of SGD, it is clear that φt+1 = φtE+ ηh (cid:88) t0=tE+1 gt0 j,φ, j, + 1 IE. (19) Li (cid:0)yt (cid:1) Li (cid:0)xt (cid:1)+(cid:10)yt xt i, gt i,ϕ (cid:11)+ (cid:13) (cid:13)yt xt (cid:13) 2 (cid:13) . (14) Then, for the third term on the right side of inequality (18), we apply the equality (19) and take the expectation, which (cid:88) (cid:104)(cid:13) (cid:13)(gt0 j,φ gt0 i,φ)(cid:13) (cid:13) 2(cid:105) + + η2 η2 yields (cid:20) 2 (cid:13) (cid:13)xt+1,2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ηh = 2 xt+1,1 2(cid:21) (cid:13) (cid:13) (cid:13) (cid:88) (cid:88) wj j= t0=tE+1 (gt0 j,φ gt0 i,φ) wjE (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) (gt0 j,φ gt0 (cid:13) (cid:13) (cid:13) i,φ) (cid:13) (cid:13) t0=tE+1 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:88) j=1 (cid:88) j=1 (cid:88) η2 η2 η2 2 2 2 wj wj t0=tE+1 (cid:88) j=1 t0=tE+1 (cid:20) 1 (cid:13) (cid:13)gt0 j,φ (cid:13) 2 (cid:13) + (cid:13) (cid:13)gt0 i,φ 2(cid:21) (cid:13) (cid:13) 1 η2 (E 1)LG2 2 , where in the last inequality, we use the bounded gradient Assumption 3. For the second term on the right side of inequality (18), we take the expectation and get (cid:104)(cid:68) 1 2ηh 1 2ηh xt+1,2 (cid:16) xt+1,2 xt+1,1 , gt i,φ (cid:69)(cid:105) (cid:17)2 xt+1,1 h(E 1)G2 + η =ηh (E 1)G2 2 + 1 2 ηh (cid:1) , i,φ + 1 2 (cid:0)gt (cid:1)2 i,φ ηh 1 2 (cid:0)gt Then, let ηmin = min{ηg, 1 (cid:16) (cid:17) (cid:104) Li 2 ηh, ηv}, we have (cid:0)xt (cid:1)(cid:105) xt+1,2 (cid:104)(cid:13) (cid:13)gt (cid:13) (cid:13) Li 2(cid:105) ηminE (cid:20) + η2 η2 + + (E 1)η2 + (23) 1 ηh (cid:21) LG2 2 . By rewriting the above inequality (23), we get (cid:0)xt (cid:2)Li (cid:1)(cid:3) 2(cid:105) (cid:104)(cid:13) (cid:13)gt (cid:13) (cid:13) (cid:0)xt+1 (cid:1) Li ηmin + (E 1)η2 ηmin + E1 ηh (24) LG2 2 . + Let be constant that satisfies the inequality η2 (E 1)η2 ity (24) can be further simplified as (cid:0)xt+1 (cid:2)Li + min, the aforementioned inequalL ηh η2 + η2 (cid:0)xt (cid:1)(cid:3) 2(cid:105) (cid:104)(cid:13) (cid:13)gt (cid:13) (cid:13) + ηmin . (25) LM G2 2 (cid:1) Li ηmin Now, by repeatedly applying inequality (25) for different values of and summing up the results, we get (cid:1) Li (x ηmin (cid:104)(cid:13) (cid:13)gt (cid:2)Li (cid:88) )(cid:3) (cid:0)x1 2(cid:105) (cid:13) (cid:13) t= (26) Dividing both side of inequality (26) by , we get )(cid:3) (cid:1) Li (x (cid:2)Li (cid:88) 2(cid:105) (cid:104)(cid:13) (cid:13)gt (cid:13) (cid:13) (cid:0)x1 ηminT 1 t= (27) ηh (cid:0)gt i,φ (cid:1)2 + ηmin LM G2 2 T. where we use the Cauchy-Schwarz inequality and the AMGM inequality in the first inequality, and the bounded gradient Assumption 3 in the second inequality above. Then, based on the above inequalities and taking the expectation of inequality (18), we have + ηmin LM G2 2 . (cid:0)x1 (cid:1) Li (x ) D, i, and we set Let us assume that Li (cid:113) 2D ηmin = LM G2T . Then, we have (cid:1) Li (cid:2)Li (cid:0)xt+1,2 (E 1)LG2 2 η2 + ηh (cid:1)(cid:3) (cid:0)xt+1,1 (E 1)G2 2 + 1 2 ηh (cid:0)gt i,φ (cid:1)2 . Summing up inequalities (15) and (17), we get (cid:2)Li (cid:0)xt+1, (cid:1) Li (cid:0)xt (cid:1)(cid:3) ηE (cid:2)(gt )2(cid:3) + η2 LG2 Summing up inequalities (15), (17), and (20), we get (cid:2)Li (cid:1) Li (cid:0)xt+1,2 (cid:1)(cid:3) (cid:0)xt (cid:104)(cid:0)gt (cid:1)2(cid:105) ηg 1 2 ηh ηv (cid:20) + η2 η2 + (E 1)η + + 1 ηh (cid:21) LG2 2 . (20) . (21) (22) 1 (cid:88) t= (cid:104)(cid:13) (cid:13)gt 2(cid:105) (cid:13) (cid:13) (cid:114) LM G2D 2T . Thus, we can get 1 mT (cid:88) (cid:88) i=1 t=1 (cid:104)(cid:13) (cid:13)gt 2(cid:105) (cid:13) (cid:13) (cid:114) 2 LM G2D 2T . (28) (29) A.2 Proof of Corollary 1 Proof. By rewriting inequality (25), we have (cid:1) Li (cid:0)xt+1 2(cid:105) (cid:104)(cid:13) (cid:13) (cid:13)gt (cid:13) ηminE (cid:2)Li + η2 (cid:0)xt min (cid:1)(cid:3) LM G2 2 (30) . By the PL Assumption 4, we have (cid:2)Li (cid:0)xt+1 (cid:1) Li (cid:0)xt (cid:1)(cid:3) 2ηminµE (cid:2)Li (cid:0)xt (cid:1) Li (x )(cid:3) + η2 min (31) LM G2 2 . Then, (cid:2)Li =E (cid:2)Li (cid:2)Li (cid:1) Li (x (cid:0)xt+1 (cid:0)xt (cid:0)xt (cid:1) Li (x (cid:1) Li (x LM G2 + η2 min )(cid:3) )(cid:3) + (cid:2)Li (cid:0)xt+1 )(cid:3) 2ηminµE (cid:2)Li (cid:1) Li (cid:0)xt (cid:0)xt (cid:1)(cid:3) (cid:1) Li (x i )(cid:3) Theorem 2. Suppose we select clients at each communication round. Let the hypernetwork parameter space be of dimension H, the embedding space be of dimension and the classifier parameter space be of dimension K. Let the ϕ, v, φ be the parameters learned from the individual dataset of clients. When Assumption 5 holds, there exists =O (cid:18) + + ϵ log (cid:16) R(LhLφ + LhLv + Lϕ) ϵ (cid:17) + 1 mϵ2 log 1 δ (cid:19) , (34) such that if the number of samples per client is greater than S, then we have with probability at least 1 δ for all ϕ, v, φ, LM G2 2 (cid:88) ni (Li(h(φ ; ), ϕ ) Li(h(φi; vi), ϕi)) ϵ, (35) =(1 2ηminµ)E (cid:2)Li (1 2ηminµ)2 (cid:2)Li (cid:0)xt (cid:1) Li (x (cid:0)xt1 )(cid:3) + η2 )(cid:3) min + 1 (cid:88) τ =0 (1 2ηminµ)τ η min (cid:1) Li (x LM G2 2 (1 2ηminµ)t+1 (cid:2)Li (cid:0)x0 + (cid:88) τ =0 (1 2ηminµ)τ η2 min )(cid:3) (cid:1) Li (x LM G2 (1 ηminµ)t+1D + ηmin LM G2 4µ . Therefore, we have (cid:34) 1 (cid:88) i=1 Li (cid:0)xt+1 (cid:1) (cid:35) (1 ηminµ)t+1D + ηmin LM G2 4µ . (32) If we set ηmin µϵ ϵ log( 1 LM G2 , after O( 1 (cid:35) ϵ )) steps, we have Li (cid:0)xt+ (cid:1) ϵ. (33) (cid:34) 1 (cid:88) i=1 Generalization Bound We further provide the generalization bound for HyperFL by employing the methodology outlined in (Baxter 2000). First, we make the following assumption: Assumption 5. We assume the weights of hypernetworks φi, the client embeddings vi and the weights of classifiers ϕi are bounded in ball of radius R, in which the following Lipschitz conditions hold: Li(h(φi; vi), ϕi) Li(h(φi; vi), ϕ ) Lϕϕi ϕ , Li(h(φi; vi), ϕi) Li(h(φi; vi), ϕi) Lhhi , h(φ h(φi; ; vi) h(φi; vi)) Lφφi φ ) h(φi; vi)) Lvvi i , . where ϕ, v, φ are the optimal parameters corresponding to the distribution of each individual client, respectively. Proof. First, we define the distance between (ϕ, v, φ) and (ϕ, v, φ) as d(cid:0)(ϕ, v, φ), (ϕ, v, φ)(cid:1) (cid:12) (cid:12) = (cid:12) (Li(h(φ ; i ), ϕ (cid:88) ni (cid:12) (cid:12) (cid:12), ) Li(h(φi; vi), ϕi)) (36) can find ϵ-covering where = (cid:80)m i=1 ni. By the Theorem 4 from (Baxter in an 2000), we d((ϕ, v, φ), (ϕ, v, φ)). Then, according to the notations used in our paper, we have that = O( 1 )), δ where C(ϵ, Hn . In our case, each element of Hn is parameterized by ϕ, v, φ. Therefore, from the triangle inequality and the Lipschitz conditions in Assumption 5, we can get d(cid:0)(ϕ, v, φ), (ϕ, v, φ)(cid:1) ) is the covering number of Hn mϵ2 log( C(ϵ,Hn ) ni (Li(h(φ ; ), ϕ ) Li(h(φi; vi), ϕi))(cid:12) (cid:12) ), ϕ ) Li(h(φi; ), ϕ ) ) Li(h(φi; vi), ϕ ) + Li(h(φi; vi), ϕ ) (cid:12) (cid:12)(Li(h(φ ; i ), ϕ ) Li(h(φi; ), ϕ ) ) Li(h(φi; vi), ϕ ) ) Li(h(φi; vi), ϕi))(cid:12) (cid:12) ; ) h(φi; i ) + Lhh(φi; ) i=1 h(φi; vi) + Lϕϕ ϕi) φi + LhLvv LhLφφ vi + Lϕϕ ϕi. (37) Now if there is parameter space such that ϕi, vi and φi have corresponding optimal point ϕ , which and φ , (cid:88) =(cid:12) (cid:12) (cid:88) =(cid:12) (cid:12) ; (Li(h(φ ni + Li(h(φi; Li(h(φi; vi), ϕi))(cid:12) (cid:12) ), ϕ (cid:88) ni + Li(h(φi; ), ϕ + Li(h(φi; vi), ϕ (cid:88) (Lhh(φ ni ϵ LhLφ+LhLv+Lϕ are away, respectively, we can get an upper bound of the distance between our model and optimal model, which is an ϵ-covering in d((ϕ, v, φ), (ϕ, v, φ)) matrix. From here we see that log(C(ϵ, Hn )) = O(m(d + + K) log( RLh(Lφ+Lv)+RLϕ )). ϵ Theorem 2 suggests that is influenced by several factors: the dimension of the parameters space, the number of clients, and the values of the Lipschitz constants. Specifically, the first part of right hand side of Eq. (34) is determined by the dimensions of the embedding vectors, hypernetwork parameters, and classifier parameters. This component is independent of the number of clients m, as each client has its unique embedding vector, hypernetwork and classifier. Additionally, this theorem points out that generalization depends on the Lipschitz constants, which influence the effective space reachable by the personalized models of clients. This indicates trade-off between the generalization ability and the flexibility of the personalized model. Related Work Gradient Inversion Attacks. Gradient Inversion Attacks (GIA) (Fredrikson, Jha, and Ristenpart 2015; Zhu, Liu, and Han 2019) is class of adversarial attacks that exploit the gradients of machine learning model to infer sensitive information about the training data by leveraging the fact that gradients contain information about the relationship between the input and the models output. The basic idea behind GIA is to intentionally modify the input data in way that maximizes the magnitude of the gradients with respect to the sensitive information of interest. By iteratively adjusting the input data based on the gradients, an attacker can gradually approximate the sensitive information, such as private attributes or training data samples, that the model was trained on (Phong et al. 2017; Zhu, Liu, and Han 2019; Geiping et al. 2020; Yin et al. 2021; Luo et al. 2022; Geng et al. 2023; Kariyappa et al. 2023). GIA can pose significant threat to privacy in scenarios where the model is used in sensitive applications or when the models training data contains sensitive information. These attacks highlight the need for robust privacy protection mechanisms to mitigate the risk of information leakage through gradients. Privacy Protection in Federated Learning. Although the local data are not exposed in FL, the exchanged model gradients may still leak sensitive information about the data that can be leveraged by GIA to recover them (Geiping et al. 2020; Huang et al. 2021; Li et al. 2022a; Hatamizadeh et al. 2023; Kariyappa et al. 2023). To further protect the data privacy, additional defense methods have been integrated into FL, and can be categorized into three classes: Secure Multi-party Computing (SMC) (Yao 1982) based methods (Bonawitz et al. 2017; Mugunthan et al. 2019; Mou et al. 2021; Xu et al. 2022), Homomorphic Encryption (HE) (Gentry 2009) based methods (Zhang et al. 2020a,b; Ma et al. 2022; Park and Lim 2022) and Differential Privacy (DP) (Dwork 2006) based methods (Geyer, Klein, and Nabi 2017; McMahan et al. 2018; Yu, Bagdasaryan, and Shmatikov 2020; Bietti et al. 2022; Shen et al. 2023). SMC, originating from Yaos Millionaire problem (Yao 1982), is framework that aims to protect the input data of each participating party by employing encryption techniques during collaborative computations. With the development of FL, SMC techniques have evolved and been adapted to federated systems to enhance the protection of sensitive data through parameter encryption (Bonawitz et al. 2017; Mugunthan et al. 2019; Mou et al. 2021; Xu et al. 2022). HE, introduced by Gentry (Gentry 2009), is an encryption algorithm that preserves the homomorphic property of ciphertexts. In the context of FL, HE enables the central server to perform algebraic operations directly on encrypted parameters without the need for decryption (Zhang et al. 2020a,b; Ma et al. 2022; Park and Lim 2022). DP is widely adopted privacy-preserving technique in both industry and academia by clipping the gradients and adding noise to personal sensitive attribute (Dwork 2006; Abadi et al. 2016). In the context of FL, DP is employed to prevent inverse data retrieval by clipping gradients and adding noise to participants uploaded parameters (Geyer, Klein, and Nabi 2017; McMahan et al. 2018; Yu, Bagdasaryan, and Shmatikov 2020; Bietti et al. 2022; Shen et al. 2023). However, SMC and HE methods are unsuitable to DNN models due to their extremely high computation and communication cost, while DP methods usually introduce additional computation cost and result in decrease in model performance (Bonawitz et al. 2017; Zhang et al. 2020b; Geyer, Klein, and Nabi 2017; McMahan et al. 2018). Apart from these defense methods with theoretical guarantees, there are other empirical yet effective defense strategies, such as gradient pruning/masking (Zhu, Liu, and Han 2019; Huang et al. 2021; Li et al. 2022b), noise addition (Zhu, Liu, and Han 2019; Wei et al. 2020; Huang et al. 2021; Li et al. 2022b), Soteria (Sun et al. 2020), PRECODE (Scheliga, Mader, and Seeland 2022), and FedKL (Ren et al. 2023). However, these methods still suffer from privacyutility trade-off problems, as shown in Tables 2 and 5 in (Huang et al. 2021) for gradient pruning/masking, Tables 1, 2, and 3 in PRECODE (Scheliga, Mader, and Seeland 2022), Table 1 in FedKL (Ren et al. 2023), and Figure 5 in Soteria (Sun et al. 2020). In contrast to these approaches, with the help of hypernetworks (Ha, Dai, and Le 2017), this work proposes novel FL framework that effectively breaks the direct connection between the shared parameters and the local private data to defend against GIA while achieving favorable privacy-utility trade-off. Hypernetworks in Federated Learning. Hypernetworks (Ha, Dai, and Le 2017) are deep neural networks that generate the weights for another network, known as the target network, based on varying inputs to the hypernetwork. Recently, there have been some works that incorporate hypernetworks into FL for learning personalized models (Shamsian et al. 2021; Carey, Du, and Wu 2022; Li et al. 2023b; Tashakori et al. 2023; Lin et al. 2023). All of these methods adopt the similar idea that central hypernetwork model are trained on the server to generate set of models, one model for each client, which aims to generate personalized model for each client. Since the hypernetwork and client embeddings are trained on the server, which makes the server possessing all the information about the local models, enabling the server to recover the original inputs by GIA ((see Table 3 and Figure 5 in Appendix)). In contrast to existing approaches, this work presents Hypernetwork Federated Learning (HyperFL) framework, which prioritizes data privacy preservation over personalized model generation through the utilization of hypernetworks."
        },
        {
            "title": "D Additional Experimental Results and",
            "content": "Experimental Details. D.1 Details of Experimental Setup Datasets. For the Main Configuration HyperFL, we evaluate our method on four widely-used image classification datasets: (1) EMNIST (Extended MNIST) (Cohen et al. 2017), dataset with 62 categories of handwritten characters, including 10 digits, 26 uppercase letters, and 26 lowercase letters; (2) Fashion-MNIST (Xiao, Rasul, and Vollgraf 2017), dataset designed for fashion product images, containing 10 categories of clothing items; (3) CIFAR-10 (Krizhevsky, Hinton et al. 2009), widely used benchmark dataset for image classification tasks, consisting of 60,000 color images distributed across 10 different classes; and (4) CINIC-10 (Darlow et al. 2018), composite image dataset that combines samples from CIFAR-10 and ImageNet (Russakovsky et al. 2015), comprising 270,000 images spanning 10 different classes. Similar to (Karimireddy et al. 2020; Zhang et al. 2021; Xu, Tong, and Huang 2023), we create non-IID data distribution by ensuring all clients have the same data size, in which s% of data (20% by default) are uniformly sampled from all classes and the remaining (100 s)% from set of dominant classes for each client. Following (Xu, Tong, and Huang 2023), we evenly divide all clients into multiple groups, with each group having the same dominant classes. Specifically, for the 10-category Fashion-MNIST, CIFAR10 and CINIC-10 datasets, we divide clients into 5 groups. Each group is assigned three consecutive classes as the dominant class set, starting from class 0, 2, 4, 6, and 8 for the respective groups. For EMNIST dataset, we divide clients into 3 groups, with each group assigned the dominant set of digits, uppercase letters, and lowercase letters, respectively. For the HyperFL-LPM, we evaluate our method on the EMNIST (Cohen et al. 2017) and CIFAR-10 (Krizhevsky, Hinton et al. 2009) datasets. Model Architectures. For the Main Configuration HyperFL, simlar to (Xu, Tong, and Huang 2023), we adopt two different CNN target models for EMNIST/Fashion-MNIST and CIFAR-10/CINIC-10, respectively. The first CNN target model is built with two convolutional layers. The first CNN target model is built with two convolutional layers (16 and 32 channels) followed by max pooling layers, two fully-connected layers (128 and 10 units), and softmax output layer, using LeakyReLU activation functions (Xu et al. 2015). The second CNN model is similar to the first one but adds one more 64-channel convolution layer. The hypernetwork is fully-connected neural network with one hidden layer, multiple linear heads per target weight tensor. The client embeddings are learnable vectors with dimension equals 64. For the HyperFL-LPM, we adopt the ViT-S/16 (Dosovitskiy et al. 2021) and ResNet-18 (He et al. 2016) pre-trained on the ImageNet dataset (Deng et al. 2009) as the feature extractor. When the pre-trainde model is ResNet, the adapter is inserted behind each resnet block. The adapter within each transformer block consists of down-projection layer, ReLU activation functions (Nair and Hinton 2010), and up-projection layer. The hypernetwork is fully-connected neural network with one hidden layer, multiple linear heads per target weight tensor. The client embeddings are learnable vectors with dimension equals 64. Compared Methods. For the Main Configuration HyperFL, we compare the proposed method with the following approaches: (1) Local-only, where clients train models locally without collaboration; (2) FedAvg (McMahan et al. 2017), widely-used FL method; (3) pFedHN (Shamsian et al. 2021), that utilizes central hypernetwork model trained on the server to generate set of models, one model for each client; and some DP-based FL methods, including (4) DP-FedAvg (McMahan et al. 2018), which incorporates differential privacy into FedAvg; (5) PPSGD (Bietti et al. 2022), personalized private SGD algorithm with user-level differential privacy; and (6) CENTAUR (Shen et al. 2023), which trains single differentially private global representation extractor while allowing personalized classifier heads. However, all these compared DP-based FL methods (i.e., DP-FedAvg, PPSGD, and CENTAUR) focus on user-level DP setting (McMahan et al. 2018), which cannot guarantee protection against honest-but-curious server attacks as they upload original gradients to the server. Therefore, we adapt these methods to fit the ISRL-DP setting (Lowy and Razaviyayn 2023), where users trust their own client but not the server or other clients, thereby defending against honest-butcurious server attacks. For the HyperFL-LPM, we compare our method with (1) Local-only with fixed feature extractor; (2) Local-only with adapter fine-tuning; (3) FedAvg with fixed feature extractor; and (4) FedAvg with adapter fine-tuning. Training Settings. For the Main Configuration HyperFL, mini-batch SGD (Ruder 2016) is adopted as the local optimizer for all approaches. Similar to (Xu, Tong, and Huang 2023), we set the step sizes ηh and ηv for local training to 0.01 for EMNIST/Fashion-MNIST and 0.02 for CIFAR10/CINIC-10. The setp size ηg is set to 0.1 for all the datasets. The weight decay is set to 5e-4 and the momentum is set to 0.5. The batch size is fixed to = 50 for all datasets except EMNIST (B = 100). The client embedding dimension is set to 64. The number of local training epochs is set to 5 for all FL approaches and the number of global communication rounds is set to 200 for all datasets. Furthermore, following (Xu, Tong, and Huang 2023), we conduct experiments on two setups, where the number of clients is 20 and 100, respectively. For the latter, we apply random client selection with sampling rate 0.3 along with full participation in the last round. The training data size per client Figure 5: Reconstructed images of IG. is set to 600 for all datasets except EMNIST, where the size is 1000. For the DP-based FL methods, the DP budget ϵ is set to 4 and the Gaussian noise σ is 1e5 to satisfy the (ϵ, σ) privacy guarantee. Average test accuracy of all local models is reported for performance evaluation. For the HyperFL-LPM, we conducted experiments with 20 clients. Furthermore, differently from HyperFL, batch size 16 is adopted for all datasets. The step sizes ηh and ηv for local training are 0.02 for EMNIST and 0.1 for CIFAR10 when using ViT pre-trained models. When using ResNet pre-trained models, the step size is set to 0.01 for all datasets. Privacy Evaluation. EMNIST and CIFAR-10 are used to evaluate privacy preservation capability of the proposed HyperFL. We choose subset of 50 images from each dataset to evaluate the privacy leakage. batch size of one is used. For experimental comparison, we set all the unknown variable in HyperFL are learnable and optimized simultaneously for IG (Geiping et al. 2020) and ROG (Yue et al. 2023). For the optimization of IG (Geiping et al. 2020), we optimize the attack for 10,000 iterations using the Adam optimizer (Kingma and Ba 2015), with an initial learning rate of 0.1. The learning rate is decayed by factor of 0.1 at 3/8, 5/8, and 7/8 of the optimization process. The coefficient of the TV regularization term is set to 1e-6. For the optimization of ROG (Yue et al. 2023), the Adam optimizer (Kingma and Ba 2015) with learning rate of 0.05 is adopted, and the total number of iterations is set to 100. To further demonstrate the privacy preservation capability of the proposed HyperFL, we design tailored attack method. Specifically, we first recover the client embedding according to Eq. (10), and then recover the input data by solving the upper-level subproblem in objective Eq. (9). Since θ cannot be obtained 2, we utilize model inversion attack methods (He, Zhang, and Lee 2019, 2020; Jiang, Zhou, and Grossklags 2022; Erdogan, 2The client embedding changes at each iteration, making θ impossible to calculate. Kupc u, and icek 2022) to solve the upper-level subproblem in objective Eq. (9). Peak signal to noise ratio (PSNR) (Hore and Ziou 2010), structural similarity (SSIM) (Wang et al. 2004), and learned perceptual image patch similarity (LPIPS) (Zhang et al. 2018) are adopted as the metrics for reconstruction attacks on image data. Lower LPIPS, higher PSNR and SSIM of reconstructed images indicate better attack performance. All experiments are conducted on NVIDIA GeForce RTX 3090 GPUs. D.2 Additional Experimental Results Privacy Evaluation. The visualization results of IG (Geiping et al. 2020) of the first 10 images are provided in Figure 5. From this figure, we can observe that the native FedAvg and pFedHN methods have much higher risk of leaking data information, as indicated by the reconstructed images closely resembling the original ones. Although introducing DP improves data privacy, there is significant drop in model performance, as shown in Table 1. In contrast, HyperFL achieves similar level of privacy protection while outperforming all DP-based methods and the native FedAvg in terms of model accuracy. The reconstructed and visualization results of ROG (Yue et al. 2023) are provided in Table 5 and Figure 6. From these results, we can observe that the proposed HyperFL can also defend against SOTA attack method. EMNIST CIFAR-10 Method PSNR SSIM LPIPS PSNR SSIM LPIPS FedAvg 24.26 0. 0.3024 23.09 0.9228 0.4363 HyperFL 3. 0.0459 0.7883 7.78 0.0137 0.7802 Table 5: Reconstruction results of ROG. The reconstructed visualization results of the tailored attack method are presented in Figure 7. These results demonstrate that even the tailored attack method is unable to reFigure 6: Reconstructed images of ROG. cover any information from the proposed HyperFL framework, thereby showcasing the robust privacy preservation capability of HyperFL. Figure 7: Reconstructed images of the tailored attack method. The first row contains the original images, while the second row shows the reconstruction results. Learned Client Embeddings. In our experiments, we learn to represent each client using trainable embedding vector vi. These embedding vectors are randomly initialized to the same value. By setting these embedding vectors trainable, they can learn continuous semantic representation over the set of clients. The t-SNE visualization (Van der Maaten and Hinton 2008) of the learned client embeddings of the EMNIST dataset with 20 clients is shown in Figure 8(b). Form this figure we can see that there is distinct grouping of the learned client embeddings into three clusters, which aligns with the data partitioning we employed, as shown in Figure 8(a). This phenomenon demonstrates the meaningfulness of the learned client embeddings in capturing the underlying relationship of the clients. In this way, personalized feature extractor parameters for each client can be generated by taking the meaningful client embedding as input for the hypernetwork. Therefore, the model can achieve better performance by adopting personalized feature extractors. (a) (b) Figure 8: (a) Label distribution of the EMNIST dataset with 20 clients. (b) The t-SNE visualization of the learned client embeddings of the EMNIST dataset with 20 clients."
        }
    ],
    "affiliations": [
        "College of Computer Science, Beijing University of Technology",
        "Department of Computer Science and Engineering, UC Santa Cruz",
        "Department of Mathematics, The University of Hong Kong",
        "School of Computing and Data Science, The University of Hong Kong",
        "School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen"
    ]
}