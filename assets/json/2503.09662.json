{
    "paper_title": "CoRe^2: Collect, Reflect and Refine to Generate Better and Faster",
    "authors": [
        "Shitong Shao",
        "Zikai Zhou",
        "Dian Xie",
        "Yuetong Fang",
        "Tian Ye",
        "Lichen Bai",
        "Zeke Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Making text-to-image (T2I) generative model sample both fast and well represents a promising research direction. Previous studies have typically focused on either enhancing the visual quality of synthesized images at the expense of sampling efficiency or dramatically accelerating sampling without improving the base model's generative capacity. Moreover, nearly all inference methods have not been able to ensure stable performance simultaneously on both diffusion models (DMs) and visual autoregressive models (ARMs). In this paper, we introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises three subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects classifier-free guidance (CFG) trajectories, and then use collected data to train a weak model that reflects the easy-to-learn contents while reducing number of function evaluations during inference by half. Subsequently, CoRe^2 employs weak-to-strong guidance to refine the conditional output, thereby improving the model's capacity to generate high-frequency and realistic content, which is difficult for the base model to capture. To the best of our knowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness across a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs like LlamaGen. It has exhibited significant performance improvements on HPD v2, Pick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be seamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by 0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using SD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 2 6 6 9 0 . 3 0 5 2 : r CoRe2: Collect, Reflect and Refine to Generate Better and Faster Shitong Shao1, Zikai Zhou1, Dian Xie1, Yuetong Fang1, Tian Ye1, Lichen Bai1, Zeke Xie1* 1The Hong Kong University of Science and Technology (GuangZhou) {sshao213, zikaizhou, dianxie, yfang870, tye610, lichenbai, zekexie}@hkust-gz.edu.cn; *: Corresponding author Figure 1. Left: Our proposed CoRe2 achieves an excellent balance between performance and efficiency across SD3.5, SDXL, and LlamaGen. Specifically, for SD3.5 and SDXL, it produces more faithful, realistic, and detailed images with high semantic consistency, while significantly reducing computational overhead compared to standard sampling. Even on LlamaGen, CoRe2 enhances the models generative capabilities with only minimal increase in computational cost. Right: Compared to previous inference-enhanced algorithms, CoRe2 achieves optimal performance across the three dimensions of efficiency, generalization, and effectiveness."
        },
        {
            "title": "Abstract",
            "content": "Making text-to-image (T2I) generative model sample both fast and well represents promising research direction. Previous studies have typically focused on either enhancing the visual quality of synthesized images at the expense of sampling efficiency or dramatically accelerating sampling without improving the base models generative capacity. Moreover, nearly all inference methods have not been able to ensure stable performance simultaneously on both diffusion models (DMs) and visual autoregressive models (ARMs). In this paper, we introduce novel plug-and-play inference paradigm, CoRe2, which comprises three subprocesses: Collect, Reflect, and Refine. CoRe2 first collects classifier-free guidance (CFG) trajectories, and then use collected data to train weak model that reflects the easy-to-learn contents while reducing number of function evaluations during inference by half. Subsequently, CoRe2 employs weak-to-strong guidance to refine the conditional output, thereby improving the models capacity to generate high-frequency and realistic content, which is difficult for the base model to capture. To the best of our knowledge, CoRe2 is the first to demonstrate both efficiency and effectiveness across wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs like LlamaGen. It has exhibited significant performance improvements Figure 2. concise explanation of why CoRe2 is effective (diffusion model for an example): we train weak model to reflect the easy-to-learn components. Then, W2S guidance is employed to refine the more (fine-grained) difficult-to-learn components. on HPD v2, Pick-of-Pic, Drawbench, GenEval, and T2ICompbench. Furthermore, CoRe2 can be seamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by 0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using SD3.5. Code is released at CoRe2. 1. Introduction Over the past few years, text-to-image (T2I) diffusion models (DMs) [11, 19, 22, 33, 35, 36, 38, 46] and visual autoregressive models (ARMs) [4, 40] have garnered significant attention within the domain of generative visual intelligence due to their ability to produce high-quality images. Compared to strengthening the generating capabilities of genFigure 3. Overview of Collect, Reflect and Refine (CoRe2). We initially generate trajectories corresponding to CFG to collect data. Next, we train weak model (i.e., the noise model equipped with MoE-LoRA) to capture the mapping from the conditional output to the CFG output, reflecting the easy-to-learn content. Finally, we employ W2S guidance to refine the conditional output (i.e., the fast mode) and the CFG output (i.e., slow mode), thereby enhancing the critical fine-grained information that is challenging to learn. erative models during the pre-training phase, considering post-training or even training-free strategies directly during the inference stage requires less overhead to improve the fidelity of synthesized images [1, 2, 6, 16, 48]. Through the implementation of inference-enhanced algorithms, such as classifier-free guidance (CFG) [10], CFG++ [6], Guidance Interval [16], PAG [1], and Z-Sampling [19], the sampling process of DMs has achieved significant advancements. However, prior solutions predominantly excel either by enhancing visual quality at the cost of significant additional inference latency [19, 21, 24] or by ensuring efficiency but falling short in terms of performance [6, 16, 36]. These algorithms fail to allow generative models to achieve quick and high-quality sampling. In addition to this, these algorithms are primarily designed for DMs, with their usability even more narrowly confined to specific architectures like SD1.5 [29], SD2.1 [28] and SDXL [26]. Their performance falls short on large-scale, flow-matching-based foundation models, such as SD3.5 [39] and FLUX [17]. Meanwhile, several methods, like PAG [1] and Z-Sampling [19], which are architecture-specific and depend on inversion operations unique to DMs, making them incapable of generalizing to models like ARMs. To address the current limitations of inference-enhanced algorithms, which struggle to simultaneously ensure effectiveness, efficiency, and generalization (see Fig. 1, right), we propose novel and plug-and-play sampling framework named CoRe2, which consists of three stages: Collect, Reflect, and Refine. This paradigm neither relies on the forward and reverse processes of DM nor is it architecture-specific, and it achieves self-improvement (see Fig. 1, left) on both DM and ARM without the need for an external assistant, reward model and new dataset. Specifically, CoRe2 relies exclusively on CFG, technique that enhances the coherence between image and text for both DM and ARM. During the Collect phase, it collects the sampling trajectories of the generative model to construct mapping from the conditional output to the CFG output. Unlike conventional CFG distillation [31, 37, 47], CoRe2 can pre-store the data, thereby eliminating the need to load the redundant pre-trained generative model into GPU memory during the Reflect stage. In the Reflect phase, we exclusively rely on weak model with limited capacity to learn the mapping from the conditional output to the CFG output, thereby reflecting the easy-to-learn content within CFG. In practical experiments, we implement this weak model using noise model [2, 48]. This way exhibits the following merits: (1) Noise model is extremely lightweight, incurring only 2.18% additional GPU latency compared to CFG sampling on SDXL; (2) Its limited fitting capacity suits our goal of reflecting only the easy-to-learn content in CFG, poorly capturing the difficult-to-learn aspects; (3) Its optimization objective is similar to CFG distillation, allowing it to replace CFG at suitable sampling intervals with minimal performance impact, thereby accelerating the sampling process (see the fast mode in Fig. 3). In the final stage, we design two distinct modes for inference: the fast mode and the slow mode, tailored for the later and earlier sampling intervals. The fast mode operates by directly utilizing the well-trained noise model to refine the conditional output, efficiently emulating the CFG output. In contrast, the slow mode adopts more nuanced approach by treating the CFG as the strong model and the noise model as the weak model. Through weak-to-strong (W2S) guidance [14], CoRe2 works to enhance the generative models overall performance, improving both visual quality and semantic faithfulness. By carefully balancing 2 the utilization frequencies of the slow and fast modes, we can generate higher-quality images while reducing GPU latency compared to the standard sampling. Our contribution consists of the following three parts: We propose the novel, plug-and-play CoRe2 that decouples from specific generative model paradigms while delivering excellent performance in terms of three dimensions: effectiveness, efficiency, and generalization. We empirically demonstrate the mechanism that our approach excels at refining intricate, high-frequency details that are often challenging for models to learn. We also further provide rigorous theoretical proof to explain why CoRe2 achieves such remarkable effectiveness. Our experimental results demonstrate that CoRe2 surpasses the vast majority of state-of-the-art (SOTA) methods across various DMs, including SDXL, SD3.5, and FLUX (see Appendix C.1), as well as ARM such as LlamaGen within renowned benchmarks such as HPD v2, Pick-of-Pic, Drawbench, GenEval, and T2I-Compbench. 2. Preliminaries i=1 xi) = p(xT ) (cid:81)T Diffusion Model and Autoregressive Model. Both DM and ARM utilizes multi-step denoising paradigm to iteratively generate high-quality images during the inference process. Starting with latent variable xT , this variable may adhere to Gaussian distribution (in the context of DMs [11]) or consist of masked tokens (in the case of ARMs [40]). These methods fundamentally aim to approximate the (abstract) estimator p(xt1xt) (1 ) during the training phase, thereby enabling sequential sampling in the form of p(x0 (cid:81)T i=1 p(xt1xt) during inference, where denotes the number of sampling steps. It is important to note that p(xt1xt) can take on different forms depending on the models focus. In DMs, it corresponds to the prediction of the score function, while in ARMs, it refers to the prediction of token. Within DMs, the encoder-only Transformer architecture is often employed to predict either the entire score function or the full tokens. This allows the sampling process to be refined as p(xt1xt) = (cid:82) p(xt1xt, x0)p(x0xt)dx0, offering structured and comprehensive approach to image generation. Regardless of whether it is an ARM or DM, both can leverage CFG to enhance their generative capabilities. For the sake of simplicity, we employ the noise prediction notation commonly used in DMs to illustrate this process: x0 strong = ϵt ϵt uncond + ω(ϵt cond ϵt uncond), (1) where ω refer to the CFG scale, which controls the strength of the guidance, while t, ϵt uncond indicate the specific time step during the sampling process, the conditional output and the unconditional output, respectively. CFG Distillation. CFG enhances the fidelity of the generated results by performing an additional number of function cond and ϵt evaluations (NFEs) under dropout-conditioning. Thus, injecting CFGs capability directly into the generative model can halve NFEs required during inference, thereby facilitating the deployment. This distillation paradigm was first introduced by [24], and its core approach is to minimize the DM loss: Lbase = E(ϵ,xt,c)ϵθ(xt, t, c) ϵ2 2, (2) while simultaneously minimizing the distillation loss: Ldistill = E(ϵ,xt,c)ϵθ(xt, t, c) ϵt strong2 2, (3) where ϵθ, ϵ, and ϵt strong stand for the score function estimator, the Gaussian noise, the text prompt and the CFG output. However, this distillation paradigm imposes significant training burdenrequiring both models that generate ϵθ(xt, t, c) and ϵt strong to be loaded into GPU memoryand it fails to focus solely on the easy-to-learn aspects. Noise Model. Noise model was primarily introduced by Golden Noise [48] and is used to refine the initial Gaussian noise via ViT backbone [7], thereby enhancing both the fidelity and aesthetics of the synthesized images. Noise Refine [2] employs this strategy by performing CFG distillation in the first sampling phase. Unfortunately, its effect is limited and cannot be extended across the complete sampling trajectory. To solve this, we present novel noise model architecture equipped with MM-DiT-Block [17] and use MoE-LoRA [23] to achieve CFG distillation. 3. CoRe2 Framework As illustrated in Fig. 3, CoRe2 is implemented via tripartite process, which is highly generalized, enhanced inference procedure. As presented in Fig. 2, our algorithm essentially works by enhancing those difficult-to-learn yet crucial details of the model during the inference phase. ). This stage focuses on assembling Stage-I Collect ( dataset precisely crafted for Reflect. The collected data inherently comprise elements that are easy-to-learn, as well as components that are significantly difficult-to-learn. We have opted for CFG, an inference-enhanced technique widely used in DMs and ARMs, to collect the required data. For model that requires sampling steps, the collected data for single sample can be represented as {(ϵt t=1, where stands for other conditions such as text embedding Gtext and timestep t. Due to the large size of Gtext, storing them consumes significant disk space. To address this issue, we apply singular value decomposition (SVD) fSVD to decompose and reduce the dimensionality of the text embeddings prior to storage: uncond, Ct)}T cond, ϵt Σ = fSVD(Gtext), store(U [:, : rank], Σ[: rank, : rank], [:, : rank]). (4) 3 spired by [14], we adopt W2S guidance in the slow mode. However, [14] only replaces the unconditional output with the weak output, we further extend this by substituting the noise model output with the weak output and the CFG output with the strong output. Consequently, the slow mode and the fast mode are (ωw2s is the W2S guidance scale) ϵt slow-mode =ϵt fast-mode =ϵt ϵt weak + ωw2s(ϵt weak, strong ϵt weak), (6) θ where ϵt weak denotes the output of ϵweak at timestep t. In practical implementation, we can use ϵt slow-mode to enhance inference or ϵt fast-mode to accelerate it. Specifically, the slow mode is applied in the early stages of sampling, while the fast mode is employed during the later stages. As shown in Fig. 1, by balancing the number of invocations of these two modes, we achieve acceleration of 1.31s and 2.89s on SDXL and SD3.5, respectively, under the same performance. Meanwhile, under the same GPU latency, we observe improvements of 0.39 and 0.47 in HPS v2 [43] on SDXL and SD3.5, respectively. We will go on to describe W2S guidance from both theoretical and empirical standpoint in order to comprehend how it improves inference. Theoretical Analysis. Under Eq. 6 we give this definition: these components The satisfies: i=1 wiµi µi2 ηeasy. (the formal definition in Def. C.2) Definition 3.1. Let Gaussian mixture model (GMM) g(x) = (cid:80)M i=1 wiN (µi, Σi) be the ground truth score function in DM with (cid:80)M i=1 wi = 1 at the timestep t. We partition the components of the score function estimation g(x) = (cid:80)M i=1 wiN (µi, Σi) into easy-to-learn and difficult-to-learn subsets based on their approximation errors (M1 is hyperparameter): Easy-to-Learn Components (i {1, . . . , M1}). for approximation error (cid:80)M1 Difficult-to-Learn Components (i {M1 + 1, . . . , }). These components exhibit larger approximation errors: ηeasy < (cid:80)M i=M1+1 wiµi µi2 ηdifficult. Through Definition 3.1 (using DM as an example), ϵt and ϵt (cid:80)M weak strong can be modeled as the estimator g(x) = i=1 wiN (µi, Σi). Now we can establish this theorem: Theorem 3.2. (the proof in Appendix C) Assume there are two macro-level DMs, denoted as ϵweak . According to Definition 3.1 (the symbols in Eq. 7 have the same meaning as in Definition 3.1 when the right superscripts weak and strong are removed), these models handle easy-to-learn and difficult-to-learn content, where it is assumed that the contents overlap. The key distinction (i.e., constraints) is as follows: and ϵstrong θ θ easy ηstrong ηweak ηstrong easy < ηweak easy < ηweak easy , (µi µstrong difficult ηstrong )(µstrong difficult, ηstrong µweak difficult < ηweak ) > 0. difficult, (7) Figure 4. Framework of noise models backbone and MoE-LoRA. We employ MM-DiT-Block to construct noise model for SD3.5 and SDXL, and we utilize MoE-LoRA to reflect easy-to-learn content across different timesteps. For LlamaGen, we replace the backbone with its native Llama block [41] w/o MoE-LoRA. Here, Σ denotes the diagonal matrix and rank indicates the reduced rank after dimensionality reduction. In our implementation, rank is set to 64 in SD3.5, SDXL and LlamaGen. Through this pipeline, we obtain the synthesized dataset = {(ϵ(t,i) uncond, C(t,i))}(T,N ) cond , ϵ(t,i) (t=1,i=1). Stage-II Reflect ( ). This phase is the most critical step, as the weak model trained here ultimately determines the success of the subsequent Refine stage. We require the weak model to perform comparably to the strong model in reflecting the easy-to-learn aspects of the image, while allowing noticeable gap to remain between the two when it comes to capturing the difficult-to-learn components. To achieve this, we select noise model with lightweight architecture and relatively limited representational capacity. As illustrated in Fig. 4 (1), we utilize the MM-DiT-Block [17] to construct the noise model. We lower the amount of layers in the architecture to reduce GPU delay. Additionally, we use the weight selection technique [45] to initialize the weak model, which speeds up the training process. However, noise model struggles to generalize effectively across different sampling steps. Thus, we enhance the existing weak model component by integrating low-rank adaptation (LoRA) [12] for each sampling step, thereby constructing MoE-LoRA [23] (see Fig. 4 (2)). This technique significantly improves the weak models generalization ability. Finally, the reflect loss can be denoted as Lreflect = αE(t,i)cos (cid:18) (cid:19) (cid:13) (cid:13)ϵweak (cid:13) θ (ϵ(t,i) uncond, C(t,i)) ϵ(t,i) cond (cid:13) (cid:13) (cid:13) 2 , (5) (cid:16) α(T t) (cid:17) where cos represents dynamic weights, designed to encourage the noise model to focus more on the critical initial sampling stages [34, 35], and the hyperparameter α is set to default value of 4. ). After obtaining the well-trained Stage-III Refine ( weak model ϵweak , we can leverage it for both fast and enhanced inference (i.e., fast and slow mode in Fig. 3). Inθ 4 strong ϵt weak and CFG ϵt Figure 5. Visualization of the frequency histogram with the W2S guidance ϵt uncond in SDXL. Note that W2S guidance in CoRe2 incorporates more high-frequency information compared to standard CFG. This effectively mitigates the base models limitations in generating fine-grained yet challenging-to-learn content during the pre-training phase. cond ϵt Figure 6. Visualization of the model output obtained by CoRe2, compared with those from the strong model (i.e., standard CFG) and the weak model (i.e., the noise model). We begin by generating 500 initial Gaussian noise samples and their corresponding model outputs for the text prompt olympic swimming pool. These are then projected into lower-dimensional space using TSNE [42] to produce the clear visualization. θ θ θ (cid:2)ϵstrong (xt, t) θ 2 alone. There exists ωw2s > 1 (i.e., scale) such that guidance ϵweak and the ground truth distribution (cid:80)M smaller than either (cid:80)M (cid:80)M the W2S guidance the mean square error between W2S (xt, t)(cid:3) θ i=1 wiN (µi, Σi) is 2 or i=1 wiN (µi, Σi) ϵweak (xt, t) ϵweak (xt, t) + ωw2s (xt, t)2 i=1 wiN (µi, Σi) ϵstrong In Theorem 3.2, ϵt and ϵstrong θ θ weak and ϵt strong correspond to the out- . Thus, there is always an optimal puts of ϵweak ωw2s > 1 that can improve the inference performance. Empirical Understanding. We use Fig. 5 to illustrate the differences in guidance direction between W2S guidance and the classical CFG. It can be observed that, compared to the classical CFG, W2S guidance encapsulates more highfrequency information, which contributes to refining details and textures within the image, thereby significantly enhancing image quality. Coupled with Definition 3.1, these highfrequency components can be identified as the so-called difficult-to-learn elements. Furthermore, as shown in slow-mode (i.e., CoRe2 in Fig. 2) is Fig. 2, the distribution of ϵt noticeably different from those of ϵt weak (i.e., Weak in Fig. 2) and ϵt strong (i.e., Strong in Fig. 2). This observation further indicates that W2S guidance indeed introduces significant corrections to the synthesized image, which, in our experiComparison between Z-CoRe2 and Z-Sampling. Table 1. Z-CoRe2 demonstrates both faster and superior performance compared to Z-Sampling. Method SDXL Pick-of-Pic Z-Sampling Z-CoRe2 (Ours) DrawBench Z-Sampling Z-CoRe2 (Ours) SD3.5 Pick-of-Pic Z-Sampling Z-CoRe2 (Ours) DrawBench Z-Sampling Z-CoRe2 (Ours) PickScore () AES () GPU Latency () 21.94 21. 23.10 23.20 21.81 22.11 22.67 22.83 5.98 6.06 5.69 5.71 5.81 5. 5.48 5.57 25.7653 23.7565 25.7653 23.7565 63.1081 57.4656 63.1081 57.4656 ments, manifest as notable performance improvements. Z-CoRe2. Note that the form of W2S guidance is not limited to Eq. 6. By integrating it with the resampling algorithm Z-Sampling [19], we derive novel algorithm, Z-CoRe2, which achieves superior performance compared to Z-Sampling while incurring lower inference costs. The algorithmic logic is outlined as xt1 = DDIM(xt, slow-mode), xt = DDIM-Inversion(xt1, fast-mode), (8) xt1 = DDIM(xt, ), where DDIM(, ) and DDIM-Inversion(, ) stand for the DDIM [36] and DDIM Inversion [25] sampling technique, respectively, and fast-mode, slow-mode and represent (reverse) fast mode in Fig. 3, (forward) slow mode in Fig. 3 and (forward) DDIM with the standard CFG, respectively. 4. Experiments Benchmark, Evaluation Metric and Dataset. In this paper, we utilize several popular benchmarks, including Pickof-Pic [15], DrawBench [30], HPD v2 [43], GenEval [8], and T2I-Compbench [13]. For Pick-of-Pic, DrawBench, and HPD v2, we evaluate performance using four distinct metrics: PickScore [15], AES [18], HPS v2 [43], and ImageReward [44]. For GenEval and T2I-Compbench, we follow the official evaluation protocols, which assess various dimensions such as object color, color binding, shape binding, and texture binding. Regarding the synthesized datasets obtained during the Collect phase, we gather 200k, 100k, and 100k samples on SD3.5 [39], SDXL [26], and LlamaGen [41], respectively. Details about benchmarks, evaluation metrics and datasets can be found in Appendix A. Our Implementation. Three essential elements make up our implementation: Collect, Reflect, and Refine. For the Collect phase, we employ the default CFG scale, and all prompts are refined using InternVL2-26b [5] to ensure In the Reflect phase, we utilize the the datasets quality. 5 Table 2. Comparison between CoRe2 and other SOTA methods on Pick-of-Pic, DrawBench and HPD v2. The full-capacity CoRe2 demonstrates outstanding performance and it incurs only negligible additional delay compare with the standard sampling. Method SDXL Pick-a-Pic Standard CFG++ Guidance Interval PAG CoRe2 (Ours) Drawbench Standard CFG++ Guidance Interval PAG CoRe2 (Ours) HPD v2 Standard CoRe2 (Ours) SD3.5 Pick-a-Pic Standard CFG++ Guidance Interval PAG Z-Sampling CoRe2 (Ours) Drawbench Standard CFG++ Guidance Interval PAG Z-Sampling CoRe2 (Ours) HPD v2 Standard CoRe2 (Ours) LlamaGen Pick-a-Pic Standard CoRe2 (Ours) Drawbench Standard CoRe2 (Ours) HPD v2 Standard CoRe2 (Ours) PickScore () HPSv2 () AES () ImageReward () GPU Latency () 21.7403 21.0357 20.1426 21.0409 21.7967 22.2300 22.2400 21.4100 22.1200 22.3433 22.4600 22. 22.0661 21.9211 22.0497 19.5623 21.3563 22.0876 22.7461 22.6327 22.7739 20.5133 22.4124 22.7800 22.6012 22.6445 19.9200 19.8600 20.6200 20.7800 20.7300 20. 27.4275 27.5571 22.3632 27.3398 27.9911 28.5300 28.8100 26.7600 28.4800 28.9763 28.4400 28.8400 27.9503 27.6814 27.7957 23.9618 26.8228 28.1279 29.0764 28.7629 29.0078 25.0731 28.5431 29.2160 28.8000 29. 25.1900 25.2200 26.3100 26.5900 26.6700 26.8800 6.0342 5.9449 5.9763 6.0608 5.9057 5.5880 5.5720 5.4670 5.7030 5.5475 6.1000 5. 5.8669 5.8211 5.8738 4.8977 5.8397 5.8948 5.4901 5.4205 5.5050 4.8975 5.4734 5.4926 5.7618 5.7623 5.7600 5.8600 5.6900 5.8100 6.0600 6. 0.5255 0.4215 -0.4111 0.2603 0.7042 0.5441 0.6333 0.1501 0.4349 0.7233 0.8889 0.9600 0.8411 0.7850 0.7989 -1.0452 0.6510 0.8924 0.8802 0.8082 0.8768 -1.0572 0.7694 0.9168 0.9860 1. -0.5500 -0.4700 -0.5000 -0.3700 0.0400 0.1200 9.6066 9.6066 9.5814 12.5134 9.8169 9.6066 9.6066 9.5814 12.5134 9.8169 9.6066 9. 22.9196 22.9196 21.8679 31.7245 26.8410 24.2429 22.9196 22.9196 21.8679 31.7245 26.8410 24.2429 22.9196 24.2429 44.4923 48.6858 44.4923 48.6858 44.4923 48. Table 3. Comparison between CoRe2 and the standard sampling on GenEval. CoRe2 performs better than the standard sampling. Method SDXL Standard CoRe2 (Ours) SD3.5 Standard CoRe2 (Ours) Single () Two () Counting () Colors () Positions () Color Attribution () Overall () 97.50% 97.50% 100.00% 100.00% 69.70% 78.79% 85.86% 87.88% 35.00% 42.50% 72.50% 73.75% 86.17% 87.23% 81.91% 79.79% 13.00% 14.00% 22.00% 27.00% 25.00% 16.00% 56.00% 59.00% 54.39% 56.01% 69.71% 71.23% Table 4. Comparison between CoRe2 and the standard sampling on T2I-Compbench. CoRe2 performs better than the standard sampling. Method Attribute Binding () Object Relationship () Complex () Numeracy () Overall () Color Shape Texture 2D-Spatial 3D-Spatial Non-Spatial SDXL Standard CoRe2 (Ours) SD3.5 Standard CoRe2 (Ours) 0.5632 0.6000 0.8036 0.8090 0.4746 0.5194 0.5851 0.5965 0.5247 0. 0.7166 0.7245 0.2056 0.1941 0.2542 0.2733 0.3577 0.3625 0.3942 0.3953 0.3109 0. 0.3171 0.3177 0.5120 0.5224 0.6277 0.6169 0.3409 0.3453 0.3780 0.3862 0.4112 0. 0.5096 0.5150 AdamW optimizer. Unless otherwise specified, the learning rate is set to 2e-6, with batch size of 64 and 10k iterations. In the Refine phase, we utilize the default slow mode for the full sampling path in all experimental results presented in the tables, unless specified otherwise. Additionally, the ablation results illustrated in the figures are obtained by balancing the utilization frequencies of the fast mode and the slow mode. Please check Appendix for more comprehensive implementation. 4.1. Main Results Our results on SDXL, SD3.5 and LlamaGen are presented below, and FLUXs results can be found in Appendix C.1. SDXL Comparison. SDXL is seminal and widely recFigure 7. Visualization of the standard sampling, CoRe2 and Z-CoRe2 on SD3.5, SDXL and LlamaGen. Compared to standard sampling, CoRe2 and Z-CoRe2 excel in generating images with more intricate high-frequency details, while also improving semantic consistency. In Appendix C.3, we present additional visualizations of CoRe2 across four distinct styles: anime, concept art, painting, and photography. tably, this superior performance is achieved with only an additional 0.21s delay compared to the standard sampling. Furthermore, CoRe2 demonstrates impressive performance on both GenEval and T2I-Compbench, surpassing the standard sampling by margins of 1.62% and 1.52% on the overall metrics, respectively. SD3.5 Comparison. SD3.5 is large-scale I2V DM that use flow matching as its noise schedule. In our experiments, we observe that many algorithms that perform effectively on SDXL fail to deliver similar results on SD3.5. In detail, the comparative results are presented in Tables 2, 3 and 4. Beyond the comparison methods considered on SDXL, we additionally include Z-Sampling [19] for evaluation. It can be observed that for Pick-of-Pic, DrawBench, and HPD v2, CoRe2 significantly outperforms the compared methods across all metrics except AES. This superior performance comes with only 5.7% increase in computational overhead compared to the standard sampling. Similar to its performance on SDXL, CoRe2 demonstrates significant improvements on both GenEval and T2I-Compbench, achieving an average performance gain of 1.52% and 0.54%, respectively, compared to the standard sampling. LlamaGen Comparison. LlamaGen is the first prominent visual ARM capable of scaling to resolution of 512512. Demonstrating the effectiveness of CoRe2 on LlamaGen further highlights the adaptability of our approach. However, due to the decoder-only Transformer architecture employed in our noise model, resulting in slightly higher latency compared to SDXL and SD3.5. As shown in Fig. 1, Figure 8. Ablation study of iteration numbers during the Reflect phase using SD3.5. We find that the 10k iterations version contribute the best performance to the full-capacity CoRe2. ognized generative model in the I2V research domain. To validate that CoRe2 can enhance both the quality and realism of synthesized images, and demonstrate its superiority over other inference-enhanced algorithms, we present our experimental results in Tables 2, 3 and 4. Table 2 illustrates performance comparison between CoRe2 and other SOTA algorithms on the Pick-of-Pic, DrawBench, and HPD v2 benchmarks. The evaluated SOTA methods include CFG++ [6], Guidance Interval [16], and PAG [1]. As shown, CoRe2 consistently surpasses the competing methods across all metrics, with the sole exception of AES. No7 Figure 9. Ablation study of ωw2s on SDXL. We observe that CoRe2 achieves optimal performance when ωw2s = 2.5. Figure 10. Ablation study of ωw2s on SD3.5. We observe that CoRe2 achieves optimal performance when ωw2s = 1.5. under identical GPU latency conditions, CoRe2 does not outperform the standard sampling. Nevertheless, the performance scaling laws of CoRe2 remain valid on LlamaGen. As illustrated in Table 2, although PickScore on Pick-ofPic is marginally lower than that of the standard sampling, CoRe2 achieves substantial improvements across all other metrics and benchmarks. These findings provide strong evidence of the robust generalization capabilities of CoRe2. Z-CoRe2 vs. Z-Sampling. As mentioned in Sec. 3, CoRe2 can be extended with Z-Sampling to achieve W2S guidance, resulting in Z-CoRe2. We present the comparison results of Z-CoRe2 and Z-Sampling on SDXL (50 steps) and SD3.5 (28 steps) in Table 1. The results clearly demonstrate that Z-CoRe2 not only outperforms Z-Sampling in terms of effectiveness but also achieves higher efficiency. 4.2. Ablation Study Number of Iterations during the Reflect Phase. From Fig. 8, we observe that as the number of iterations in the Reflect stage increases from 1k to 10k and then to 50k, the performance of the full sampling path using fast mode (i.e., the leftmost three points) progressively improves, indicating that the weak models capability is gradually enhanced. It is important to note that 0k represents the direct use of the conditional output as the result for the weak output. This approach is evidently less effective compared to its counterparts, such as 1k, 10k, and 50k. Nevertheless, the best performance is achieved at the 10k iterations version (i.e., the blue line), suggesting that the weak model should ideally remain moderately weak, striking balance rather than continually pushing its performance ceiling. W2S Guidance Scale ωw2s. In Fig. 9 and 10, we provide the ablation study results for ωw2s on SDXL and SD3.5, respectively, and the corresponding analysis for LlamaGen is detailed in Appendix C.2. As ωw2s increases, the performance of Z-CoRe2 follows trend of initial improvement, peaking before subsequently declining. The empirical optimal values are found to be 1.5 for SD3.5 and 2.5 for SDXL, indicating the importance of carefully tuning ωw2s for different models. Furthermore, for LlamaGen, we determine that the optimal performance is achieved when ωw2s=1.5. Visualization. In Fig. 7 and Appendix C.3, we showcase images synthesized by CoRe2, Z-CoRe2, and the standard It is evident that both CoRe2 and Z-CoRe2 sampling. significantly enhance detail textures, realism, and semantic faithfulness compared to the standard sampling and ZSampling. Furthermore, on LlamaGen, CoRe2 demonstrates the capability to address out-of-domain challenges for prompts such as type of digital...is maintained, showcasing its robustness in diverse scenarios. 5. Conclusion This paper presents groundbreaking inference mechanism named CoRe2, which seamlessly integrates efficiency, effectiveness, and generalization. It stands out as the first plug-and-play inference-enhanced algorithm to demonstrate simultaneous effectiveness across both DM and ARM. The core principle of CoRe2 revolves around gathering data to train weak model that reflects easy-tolearn aspects, followed by the application of W2S guidance to refine difficult-to-learn components, thereby significantly enhancing the high-frequency details in images."
        },
        {
            "title": "References",
            "content": "[1] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. In European Conference on Computer Vision, pages 117. Springer, 2024. 2, 7 [2] Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Jaewon Min, Minjae Kim, Wooseok Jang, Hyoungwon Cho, Sayak Paul, SeonHwa Kim, Eunju Cha, et al. noise is worth diffusion guidance. arXiv preprint arXiv:2412.03895, 2024. 2, 3 [3] Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. arXiv preprint arXiv:2410.08261, 2024. 11 [4] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. 1 [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 5 [6] Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. Cfg++: Manifold-constrained classifier free guidance for diffusion models. arXiv preprint arXiv:2406.08070, 2024. 2, 7 [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, Event Virtual, 2020. OpenReview.net. 3 [8] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equiIn Neural Information Processing Systems, Long librium. Beach Convention Center, Long Beach, 2017. NeurIPS. 11 [10] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In Neural Information Processing Systems Workshop, Virtual Event, 2021. NeurIPS. 2 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Neural Information Processing Systems, pages 68406851, Virtual Event, 2020. NeurIPS. 1, 3 [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 4 [13] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. 5 [14] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2025. 2, [15] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36: 3665236663, 2023. 5, 11 [16] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. Advances in Neural Information Processing Systems, 37:122458122483, 2025. 2, 7 [17] Black Forest Labs. Flux. https : / / blackforestlabs.ai/, 2024. 2, 3, 4 [18] Laion.ai. Laion-aesthetics. https://laion.ai/blog/ laion-aesthetics/, 2022. 5, 11 [19] Bai LiChen, Shitong Shao, zikai zhou, Zipeng Qi, zhiqiang xu, Haoyi Xiong, and Zeke Xie. Zigzag diffusion sampling: Diffusion models can self-improve via self-reflection. In The Thirteenth International Conference on Learning Representations, 2025. 1, 2, 5, 7 [20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740755. Springer, 2014. 11 [21] Yujian Liu, Yang Zhang, Tommi Jaakkola, and Shiyu Chang. Correcting diffusion generation through resampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87138723, 2024. [22] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In Neural Information Processing Systems, New Orleans, LA, USA, 2022. NeurIPS. 1 [23] Tongxu Luo, Jiahe Lei, Fangyu Lei, Weihao Liu, Shizhu He, Jun Zhao, and Kang Liu. Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models. arXiv preprint arXiv:2402.12851, 2024. 3, 4 [24] Chenlin Meng, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. arXiv preprint arXiv:2210.03142, 2022. 2, 3 [25] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. 9 generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, kigali, rwanda, 2023. OpenReview.net. [39] Stability.ai. Introducing stable diffusion 3.5. https: //stability.ai/news/introducingstablediffusion-3-5, 2024. 2, 5 [40] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1, 3 [41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 4, 5 [42] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9 (11), 2008. 5 [43] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis, 2023. 4, 5, [44] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai ImagereLi, Ming Ding, Jie Tang, and Yuxiao Dong. ward: Learning and evaluating human preferences for textto-image generation, 2023. 5, 12 [45] Zhiqiu Xu, Yanjie Chen, Kirill Vishniakov, Yida Yin, Zhiqiang Shen, Trevor Darrell, Lingjie Liu, and Zhuang Initializing models with larger ones. arXiv preprint Liu. arXiv:2311.18823, 2023. 4 [46] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 1 [47] Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, and Hai Huang. Long and short guidance in score identity distillation for one-step text-to-image generation. ArXiv 2406.01561, 2024. 2 [48] Zikai Zhou, Shitong Shao, Lichen Bai, Zhiqiang Xu, Bo Han, and Zeke Xie. Golden noise for diffusion models: learning framework. arXiv preprint arXiv:2411.09502, 2024. 2, 3 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. [26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations. 2, 5 [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 11 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 2 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 2 [30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 5 [31] Tim Salimans and Jonathan Ho. Progressive distillation for In International Confast sampling of diffusion models. ference on Learning Representations, Virtual Event, 2022. OpenReview.net. 2 [32] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Improved techCheung, Alec Radford, and Xi Chen. In Neural Information Processniques for training gans. ing Systems, Centre Convencions Internacional Barcelona, Barcelona SPAIN, 2016. NeurIPS. 12 [33] Shitong Shao, Xu Dai, Shouyi Yin, Lujun Li, Huanran Chen, and Yang Hu. Catch-up distillation: You only need arXiv preprint to train once for accelerating sampling. arXiv:2305.10769, 2023. [34] Shitong Shao, Bai LiChen, zikai zhou, Tian Ye, Yunfeng Cai, Kaishun Wu, and Zeke Xie. The blessing of smooth initialization for video diffusion models, 2024. 4 [35] Shitong Shao, zikai zhou, Bai LiChen, Haoyi Xiong, and Zeke Xie. IV-mixed sampler: Leveraging image diffusion models for enhanced video synthesis. In The Thirteenth International Conference on Learning Representations, 2025. 1, 4 [36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, kigali, rwanda, 2023. OpenReview.net. 1, 2, 5 [37] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 2 [38] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based"
        },
        {
            "title": "Appendix",
            "content": "A. Benchmark and Dataset In this section, we provide an overview of the benchmarks, evaluation metrics, and the datasets obtained in the Collect phase used in our main paper. A.1. Benchmark Pick-a-Pic. Pick-a-Pic [15] is an open dataset curated to capture user preferences for T2I synthesized images. The dataset is collected via an intuitive web application, allowing users to synthesize images and express their preferences. The Pick-a-Pic dataset includes over 500,000 examples spanning 35,000 unique prompts. DrawBench. DrawBench1 is benchmark dataset introduced to enable comprehensive evaluation of text-to-image synthesis models. It consists of 200 meticulously designed prompts, organized into 11 categories to assess model capabilities across range of semantic dimensions. These dimensions include compositionality, numerical reasoning, spatial relationships, and the ability to interpret complex textual instructions. DrawBench is specifically designed to provide multidimensional analysis of model performance, facilitating the identification of both strengths and weaknesses in text-to-image synthesis. HPD v2. The human preference dataset v2 (HPD v2) [43] is an extensive dataset featuring clean and precise annotations, designed to capture user preferences for images generated from text prompts. It includes 798,090 binary preference labels across 433,760 image pairs, aiming to address the limitations of conventional evaluation metrics that fail to accurately reflect human preferences. Following the methodologies presented in [3, 43], we employed four distinct subsets for our analysis: Animation, Concept-art, Painting, and Photo, each containing 800 prompts. GenEval. GenEval is an evaluation framework specifically tailored to assess the compositional properties of generated images, such as object co-occurrence, spatial positioning, object count, and color. By leveraging SOTA detection models, GenEval provides robust evaluation of text-to-image generation tasks, ensuring strong alignment with human judgments. Additionally, the framework allows for the integration of other advanced vision models to validate specific attributes, such as object color. The benchmark comprises 550 prompts, all of which are straightforward and easy to interpret. T2I-Compbench. T2I-Compbench is comprehensive benchmark designed to evaluate open-world compositional textto-image synthesis. It includes 6,000 compositional text prompts, systematically categorized into three primary groups: attribute binding, object relationships, and complex compositions. These groups are further divided into six subcategories: color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and intricate compositions. A.2. Evaluation Metric PickScore. PickScore is CLIP-based scoring model, developed using the Pick-a-Pic dataset, which captures user preferences for generated images. This metric demonstrates performance surpassing that of typical human benchmarks in predicting user preferences. By aligning effectively with human evaluations and leveraging the diverse range of prompts in the Picka-Pic dataset, PickScore offers more relevant and insightful assessment of text-to-image models compared to traditional metrics like FID [9] on datasets such as MS-COCO [20]. HPS v2. The human preference score version 2 (HPS v2) is an improved model to predict user preferences, achieved by fine-tuning the CLIP model [27] on the HPD v2. This refined metric is designed to align text-to-image generation outputs with human tastes by estimating the likelihood that generated image will be preferred, thereby serving as reliable benchmark for evaluating the performance of text-to-image models across diverse image distributions. AES. The Aesthetic Score (AES) [18] is calculated using model built on CLIP embeddings, further enhanced with multilayer perceptron (MLP) layers to evaluate the visual appeal of images. This metric provides quantitative measure of the aesthetic quality of generated images, offering valuable insights into their alignment with human aesthetic standards. 1https://huggingface.co/datasets/shunk031/DrawBench 11 ImageReward. ImageReward [44] is specialized reward model designed to evaluate text-to-image synthesis through human preferences. Trained on an extensive dataset of human comparisons, the model effectively captures user inclinations by assessing multiple aspects of synthesized images, including their alignment with text prompts and their aesthetic quality. ImageReward has shown superior performance compared to traditional metrics such as the Inception Score (IS) [32] and Frechet Inception Distance (FID), making it highly promising tool for automated evaluation in text-to-image synthesis tasks. A.3. Dataset SD3.5. For SD3.5, we initially collected total of 200k prompts to build dataset based on SD3.5. Among these, 100k prompts were sourced from Pick-of-Pic, refined using InternVL2-26b to ensure high-quality prompts. The remaining 100k prompts were obtained from the diffusiondb-prompt-upscale2 dataset, which were also refined using InternVL2-26b. Using these refined prompts, we were able to generate 100k CFG trajectories. SDXL. The prompt used on SDXL was the same as SD3.5, but we simply randomly selected 100k of them to generate CFG trajectories. LlamaGen. In LlamaGen, we employ the same prompts as SDXL, utilizing 100k prompts to construct its dataset. Furthermore, to ensure enhanced generative performance, our implementation of LlamaGen differs from the official version. Specifically, we adjusted the CFG scale to 9 and incorporated comprehensive negative prompt instead of using null prompt. The negative prompt includes: worst quality, normal quality, low quality, low res, blurry, distortion, text, watermark, logo, banner, extra digits, cropped, jpeg artifacts, signature, username, error, sketch, duplicate, ugly, monochrome, horror, geometry, mutation, disgusting, bad anatomy, bad proportions, bad quality, deformed, disconnected limbs, out of frame, out of focus, dehydrated, disfigured, extra arms, extra limbs, extra hands, fused fingers, gross proportions, long neck, jpeg, malformed limbs, mutated, mutated hands, mutated limbs, missing arms, missing fingers, picture frame, poorly drawn hands, poorly drawn face, collage, pixel, pixelated, grainy, color aberration, amputee, autograph, bad illustration, beyond the borders, blank background, body out of frame, boring background, branding, cut off, dismembered, disproportioned, distorted, draft, duplicated features, extra fingers, extra legs, fault, flaw, grains, hazy, identifying mark, improper scale, incorrect physiology, incorrect ratio, indistinct, kitsch, low resolution. B. Detail Implementation B.1. Architecture of Noise Model Here, we provide detail description of the noise model architecture used in SD3.5, SDXL, and LlamaGen. Given that the open-source versions of FLUX are distilled through CFG distillation technique, we propose modified CoRe2 to enhance the models performance on FLUX. Further details can be found in Appendix C.1. SD3.5. For SD3.5s noise model, we utilize the MM-DiT-Block architecture with an input latent shape of 128128. The patch size is set to 22, and the input and output latent channel sizes are both 16. The architecture employs 8 MM-DiTBlocks, with 24 attention heads incorporated. Text embedding information is injected following the structure of DiT, ensuring seamless integration of textual data into the model. SDXL. For SDXLs noise model, we adopt the MM-DiT-Block architecture with an input latent shape of 128128. The patch size is configured as 22, while the input and output latent channel sizes are both set to 4. The architecture includes 4 MM-DiT-Blocks, with 24 attention heads utilized. Text embedding information is incorporated following the DiT structure, enabling smooth and effective integration of textual data into the model. LlamaGen. Since both the input and output of LlamaGen are logits, the MM-DiT-Block cannot be used to construct the noise model. To reduce the gap between LlamaGen and the noise model, we decided to use Llama blocks as the components for the noise model. Furthermore, while the number of layers was reduced from LlamaGens 36 layers to 12 layers, all other settings remain unchanged. For the input and output of the noise model, as they are entirely in the form of logits, the 2https://huggingface.co/datasets/adams-story/diffusiondb-prompt-upscale 12 tok embedding layer, which typically converts discrete vectors into continuous encodings, is unnecessary and therefore omitted. B.2. Experimental Setup of the Reflect Phase For the training of noise models across SDXL, SD3.5, and LlamaGen, we adopt per-GPU batch size of 4, combined with gradient accumulation over 2 steps. This setup produces global batch size of 64 when utilizing 8 GPUs. For SDXL and SD3.5, we rely on the mean squared error (MSE) loss as the default choice for CFG distillation. In contrast, for LlamaGen, we use MSE loss during the initial training phase and later transition to Kullback-Leibler (KL) divergence, aiming to emphasize the easy-to-learn components. The learning rate is set to 2e-6 for SDXL and SD3.5, while LlamaGen operates with higher learning rate of 1e-4. All models utilize the AdamW optimizer with weight decay of 1e-3, and training is conducted using cosine learning rate scheduler to ensure smooth convergence. For the number of training iterations, we conduct ablation experiments on SD3.5 and found that among 1k, 10k, and 50k iterations, the noise model achieve the best performance with 10k iterations. Therefore, we adopt this setting for both SDXL and LlamaGen as well. B.3. Experimental Setup of the Refine Phase During the Refine stage, we adhere to the default sampling steps for both SD3.5 and SDXL, which are set to 28 and 50, respectively. Additionally, our ablation studies revealed that ωw2s achieves optimal performance with values of 2.5 on SD3.5 and 1.5 on SDXL. Unless otherwise specified, the experiments presented in our tables were all conducted using the full path executed in slow mode. C. How Does W2S Guidance Work? Here, we provide detailed theoretical analysis and proof of why W2S guidance works effectively in DMs. It is worth noting that this proof assumes premise where the weak model and the strong model perform comparably in learning easily learnable content, but exhibit significant gap in their ability to capture the more difficult-to-learn content. Lemma C.1. Assuming p(x0) can be modeled as mixture Gaussian model (GMM) where each Gaussian component is Dirac distribution, i.e., p(x0) = (cid:80)M i=1 wiδ(x xi) (M is the number of Gaussian components), it follows that both p(xt) and p(xsxt) are Gaussian distributions. Furthermore, the score function estimation ϵθ(xt, t) can be expressed as summation of the logarithms of the Gaussian components. We know p(xsxt) = (cid:82) p(xsxt, x0)p(x0xt)dx0, where q(x0) = (cid:80)M i=1 wiδ(x xi) subject to (cid:80)M i=1 wi = 1. Then, we can get (cid:90) (cid:90) p(xsxt) = = p(xsxt, x0)p(x0xt)dx0 p(xsxt, x0)p(x0)p(xtx0)/p(xt)dx0 (9) = 1/q(xt) (cid:90) (cid:88) i=1 p(xsxi t, xi 0)wip(xi txi 0). Since the forward process and the reverse process can be described as p(xtx0) = (xtαtx0, σ2 Nθ(x0αstxt, α2 estimator. In the DDIM process, the reverse process can also be formulated as ) and p(xsxt, x0) = st[σt σs]2), where Nθ in the second term stands for the predicted noise obtained by the score function Since ϵθ(xt, t) itself is fitting the score function, we have xs = αs xt σtϵθ(xt, t) αt + σsϵθ(xt, t). ϵθ(xt, t) σt = xt log pt(xt) xt (cid:88) i=1 wi log (µi(x), Σi(x)). 13 (10) (11) The condition we know is p(x0) is GMM, and p(xt) can be derived as: (cid:90) p(xt) = p(xtx0)p(x0)dx0 (cid:90) (cid:88) = wi i= 1 2πσt (cid:18) exp xt αtx02 2 2σ2 (cid:19) dx0. This simplifies to: p(xt) = (cid:18) exp 1 2πσt = (xtαtx0, σ2 ). (cid:19) xt αtx02 2 2σ2 (12) (13) So p(xt) is Gaussian distribution, and we can also get p(xsxt) = (xs αtsσ2 α2 the predicted noise as ϵθ(xt,t) Gaussian distribution, where the Gaussian componentsif the logarithm is removedform GMM. Definition C.2. Let g(x) = (cid:80)M i=1 wiN (µi, Σi) be the ground truth GMM with (cid:80)M i=1 wi = 1. We partition the components into easy-to-learn and difficult-to-learn subsets based on their approximation errors under fitted GMM g(x) = (cid:80)M x0, i=1 wi log (µi(x), Σi(x)) is reasonable. This term remains = xt log pt(xt) xt ). Therefore, modeling xt + (cid:80)P σt ts ts αsσ2 α2 sσ2 α2 α2 i=1 wiN (µi, Σi): Easy-to-Learn Components (i {1, . . . , M1}). The approximation error for these components satisfies: Ei[1,M1] (cid:16) (cid:104) DKL (µi, Σi)N (µi, Σi) (cid:17)(cid:105) ηeasy, where DKL is the KL divergence. Under the assumption Σi = Σi, this simplifies to M1(cid:88) i=1 wiµi µi2 ηeasy. Difficult-to-Learn Components (i {M1 + 1, . . . , }). These components exhibit larger approximation errors: ηeasy < (cid:88) i=M1+1 wiµi µi2 ηdifficult. Based on the above lemma and definition, we can get the following theorem: (14) (15) (16) Theorem C.3. Let us assume there are two macro-level diffusion models, denoted as ϵweak . According to Definition C.2, these models handle easy-to-learn and difficult-to-learn content, where it is assumed that the contents overlap. The key distinction (constraint) is as follows: θ and ϵstrong θ easy ηstrong ηweak ηstrong difficult < ηweak (µi µstrong easy < ηweak difficult, )(µstrong difficult ηstrong difficult, ηstrong easy < ηweak easy , µweak ) > 0. i There exists ωw2s > 1 (i.e., the W2S guidance scale) such that W2S guidance ϵweak is closer to the ground truth distribution (cid:80)M (xt, t) + ωw2s θ (xt, t) or ϵstrong θ (cid:2)ϵstrong θ (xt, t) alone. (xt, t) ϵweak θ i=1 wiN (µi, Σi) than either ϵweak (xt, t) ϵweak θ θ (xt, t)(cid:3) can be rewritten as (use Lemma C.1) Proof. Note that ϵweak θ (xt, t) + ωw2s (cid:2)ϵstrong θ (17) (xt, t)(cid:3) 1 σt xt (cid:34) (cid:88) i=1 (cid:104) log (µstrong wi (x), Σstrong (x))ωw2s + log (µweak (x), Σweak (x))(1ωw2s)(cid:105) (cid:35) . (18) Table 5. Comparison of FLUX on Pick-of-Pic with different W2S guidance scale ωw2s. ωw2s 0.7 0.85 0.925 0.9625 1.0 HPSV2 () 28.71 28.74 28.73 28.67 28.64 PickScore () 22.13 22.22 22.14 22.11 22.10 AES () 6.05 6.07 6.10 6.09 6.09 ImageReward () 1.07 1.02 1.01 0.98 0. Table 6. Comparison of FLUX DrawBench between CoRe2 and the standard sampling. Method Standard CoRe2 (Ours) HPSV2 () 29.75 29.76 PickScore () 22.85 22.88 AES () 5.82 5.82 ImageReward () 0.98 0."
        },
        {
            "title": "Each component can be further derived as",
            "content": "(x))ωw2s + log (µweak log (µstrong log(exp( (x), Σstrong ωw2sx µstrong 2Σstrong (x) (x)ωw2s(xT 2xµstrong (x)2 Σweak + (x))(1ωw2s) (x)2 2 )) i (x), Σweak (ωw2s 1)x µweak 2Σweak (x) (x)) + Σstrong 4Σstrong (x)Σweak (x) Σstrong (x) + µstrong,2 (x)ωw2sµstrong κ 4Σstrong (x)Σweak (x) Σweak (cid:13) (cid:13) (cid:13) (cid:13) + = = (x)(ωw2s 1)(xT 2xµweak (x) (x)(ωw2s 1)µweak i κ (x) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 + C, (x) + µweak,2 (x)) (19) (x), µweak (x), Σweak (x), Σstrong the variance of the strong model, where µstrong the expectation of the variance of the weak model, respectively, and refer to the weak model, constant. We aim to prove the existence of weighting factor ωw2s such that the combined mean B(ωw2s) = Σweak (ωw2s 1) is closer to the target mean µi than either µweak or µstrong alone. This is equivalent to minimizing the squared distance: (x) stand for the expectation of the strong model, ωw2s + Σstrong κ = Σweak ωw2sµstrong (ωw2s1)µweak Σstrong κ , i E(ωw2s) = B(ωw2s) µi2 2 . (20) First, we should simplify B(ωw2s). Assume Σweak = Σstrong . Second, we need to give the error function E(ωw2s) = (cid:13) )(µstrong ωw2s)µweak conditions given by the theorem, we know: 1) ηstrong µstrong µweak respect to ωw2s as E(ωw2s) = 2(µstrong 1. At ωw2s = 0, E(0) = 2(µstrong 2. At ωw2s = 1, E(1) = 2(µstrong µweak )(µweak )(µstrong ) > 0, meaning µstrong µweak µweak i difficult < ηweak lies between µweak ) (cid:0)ωw2s(µstrong µi) < 0 (since µstrong µweak ) + (µweak µi)(cid:1). is closer to µi). µi) < 0 (by the same reasoning). = Σi for simplicity. Then B(ωw2s) = ωw2sµstrong (cid:13)ωw2s(µstrong difficult, implying µstrong µi)(cid:13) 2 ) + (µweak . Review the (cid:13) µi < µweak µi, and 2) (µi and µi. Third, we can give the derivation of E(ωw2s) with µweak + (1 Finally, we optimize ωw2s. Since E(ωw2s) is linear in ωw2s, and E(0) < 0, E(1) < 0, the minimum of E(ωw2s) occurs at ωw2s > 1. Specifically, solving E(ωw2s) = 0 gives: µstrong µweak )(µstrong Given the condition (µi µstrong w2s > 1, which ensures E(ω ) > 0, it follows that ω exists ωw2s > 1 such that the mean square error (MSE) B(ωw2s)µi2 2 is smaller than either µweak individually. This concludes the proof. µweak w2s = ω . µi µweak (21) w2s) < ηstrong µi2 difficult. Thus, there 2 or µstrong µi2 2 C.1. Experiment on FLUX Given that Black Forest Lab has only open-sourced FLUX.1-dev and FLUX.1-schnell, both of which are derived using CFG distillation, it becomes impractical to employ the Collect, Reflect, and Refine stages from the main paper to enhance the highfrequency details in the images. Instead, we opted for more flexible implementation that marginally lowers the performance Figure 11. Visualization of the (modified) CoRe2 and the standard sampling on FLUX. Compared to CoRe2, which is obtained through the complete three-stage pipeline, the (modified) CoRe2 faces limitation due to the lack of an open-sourced version of FLUX without CFG distillation. As result, we treat FLUX.1-schnell weak model, while FLUX.1-dev as strong model when executing W2S guidance. Although FLUX falls slightly short in delivering the significant gains over standard sampling as seen in CoRe2 in Fig. 7, it nonetheless demonstrates certain degree of improvement in both visual quality and semantic consistency of the generated images. Table 7. Comparison of FLUX between CoRe2 and the standard sampling on GenEval. Method Standard CoRe2 (Ours) Single () 96.25% 97.50% Two () 83.84% 87.88% Counting () 68.75% 72.50% Colors () 79.79% 76.60% Positions () 79.79% 24.00% Color Attribution () 48.00% 44.00% Overall () 66.93% 67.08% Table 8. Comparison of FLUX on HPD v2 between CoRe2 and the standard sampling. Method Standard CoRe2 (Ours) HPSV2 () 29.08 29.15 PickScore () 22.70 22.78 AES () 6.01 6.01 ImageReward () 1.05 1.07 Figure 12. Ablation study of ωw2s on LlamaGen. We observe that CoRe2 achieves optimal performance when ωw2s = 1.5. upper bound of CoRe2. Specifically, FLUX.1-dev is designated as the strong model, while FLUX.1-schnell acts as the weak model. Interestingly, experimental results reveal that FLUX.1-schnell is not necessarily weaker than FLUX.1-dev, which results in the optimal ωw2s value potentially falling between 0 and 1. Through relevant ablation experiments, as presented in Table 5, we determine that ωw2s = 0.85 yields the best performance. Furthermore, we conduct comparative evaluations between CoRe2 and the standard sampling on both GenEval and DrawBench benchmarks with FLUX.1-dev in Table 6 and Table 7, respectively. The outcomes demonstrate that CoRe2 significantly enhances the fidelity of the generated images. Lastly, the visualization provided in Fig. 11 highlights that even this adapted version of CoRe2 delivers noticeable improvement in image details that is appreciable to the naked eye. 16 C.2. W2S Guidance Scale ωw2s on LlamaGen In this section, we present the findings of the ablation experiments on ωw2s for LlamaGen, as depicted in Fig. 12. Compared to SDXL and SD3.5, we notice that the tuning process for ωw2s on LlamaGen is notably more delicate and requires heightened precision. To tackle this, grid search with finer interval of 0.25 is utilized to pinpoint the optimal value of ωw2s that enables CoRe2 to deliver its peak performance. An example of the grid search methodology is provided for demonstration. From Fig. 12, it can be inferred that selecting ωw2s as 1.5, particularly when the entire sampling path operates in slow mode, yields significant performance boost compared to the standard sampling. C.3. Additional Visualization Here, we provide additional visualizations of CoRe2 as supplement to Fig. 7 in the main paper. Specifically, for SD3.5, we showcase the synthesized images generated by CoRe2 across different styles: anime, concept art, painting, and photography, which are displayed in Figs. 13, 14, 15 and 16, respectively. For SDXL, we also showcase the synthesized images generated by CoRe2 across different styles: anime, concept art, painting, and photography, which are displayed in Figs. 17, 18, 19 and 20, respectively. Figure 13. Visualization comparison about the standard sampling and CoRe2 in anime style on SD3.5. 17 Figure 14. Visualization comparison about the standard sampling and CoRe2 in concept-art style on SD3.5. Figure 15. Visualization comparison about the standard sampling and CoRe2 in painting style on SD3.5. 18 Figure 16. Visualization comparison about the standard sampling and CoRe2 in photography style on SD3.5. Figure 17. Visualization comparison about the standard sampling and CoRe2 in anime style on SDXL. 19 Figure 18. Visualization comparison about the standard sampling and CoRe2 in concept-art style on SDXL. Figure 19. Visualization comparison about the standard sampling and CoRe2 in painting style on SDXL. 20 Figure 20. Visualization comparison about the standard sampling and CoRe2 in photography style on SDXL. Figure 21. Visualization comparison about the standard sampling and CoRe2 on LlamaGen."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology (GuangZhou)"
    ]
}