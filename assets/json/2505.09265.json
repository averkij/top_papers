{
    "paper_title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
    "authors": [
        "Bin-Bin Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation. We present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using a pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys training-free without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods. The code and pre-trained models are available at https://github.com/gaobb/MetaUAS."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 5 6 2 9 0 . 5 0 5 2 : r MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning Bin-Bin Gao Tencent YouTu Lab, Shenzhen, China csgaobb@gmail.com Code and Models: https://github.com/gaobb/MetaUAS"
        },
        {
            "title": "Abstract",
            "content": "Zeroand few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation. We present novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose oneprompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys trainingfree without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods."
        },
        {
            "title": "Introduction",
            "content": "Visual anomaly classification (AC) and segmentation (AS) aims to group images and pixels into two different semantics, normal and anomalous, facilitating many applications such as industrial defect inspection in manufacturing [4, 39, 3, 74], medical image diagnosis [28, 65], and video surveillance [55, 40, 18], etc. Generally, AS is performed first, and then AC is obtained based on postprocessing of the segmentation results. Therefore, AS is more essential than AC. AS can be viewed as binary semantic segmentation assuming that pixel-level annotated images are available. Unfortunately, it is usually difficult to collect anomaly images due to their scarcity in practical applications, and pixel-level annotations also involve labor costs. Based on available training images, e.g., only normal, normal with noise, few-shot normal or anomaly, and multi-class normal, have led to different types of AS tasks, such as unsupervised [47, 31, 72, 12, 37, 29], fully-unsupervised [9, 27, 38], fewshot [24, 14, 66, 69, 15], and unified AS [70, 17], etc. These methods achieve excellent performance on seen objects but often perform poorly on unseen objects. To address this fragmentation, recent works, such as WinCLIP [26] and AnomalyCLIP [76], have attempted to design universal models, that are capable of recognizing anomalies for unseen objects. They typically build on vision-language models (i.e., CLIP [42]), benefiting from strong generalization 38th Conference on Neural Information Processing Systems (NeurIPS 2024). ability. However, WinCLIP [26] still struggles with handcrafted text prompts about defects. Further, AnomalyCLIP [76] learns general prompt embedding through developing specialized modules but requiring fine-tuning on an auxiliary domain dataset with pixel-level annotations. In addition to being flexible, these vision-language-based methods are still weak in anomaly segmentation. Accurate anomaly segmentation is crucial in real-world applications, such as industrial inspection, as anomalies often relate to the area, shape, and location where they occur. On the other hand, we know that visual representations are not dependent on language in the animal world [16]. In particular, the principles of visual perception in non-human primates are very similar to humans [58]. Although there is room for universal AS based on visual-language models and worthwhile further to pursue, in this paper, we want to explore how far we can go with pure visual model without any guidance from language. To explore more general AS framework, lets first review how the visual system perceives anomalies. Generally, humans can perceive anomalies when an input significantly deviates from those normal patterns stored in our brains. There is evidence to support this point in neuroscience. For example, predictive coding theory [43] postulates that the brain constantly generates and updates mental model. The mental model compares its expectations (or predictions) with the actual inputs from the visual cortex. This process allows the brain to perceive anomalies. In fact, PatchCore [47] captures normal local patch features, stores them in memory bank, and recognizes anomalies by comparing input features with the memory bank. In addition, some distribution-based methods [12] learn multivariate Gaussian distribution from normal local features and then utilize distance metric to measure anomalies. However, these memoryand distribution-based methods usually require certain number of normal images and thus are limited in universal (i.e., open-world) scenarios. Actually, we can build similar concepts in AS. First, given one normal image prompt for each class, we take it as the expected output. Then, the actual input could be any query images from the same class of the normal prompt. Last but not least, how to construct mental model to compare between given normal image prompt and any query images. Despite these challenges, we can imagine that the mental model should satisfy several basic principles. First, it should have strong generalization ability to perceive anomalies facing unseen objects or textures. Second, it can perform pixel-level anomaly segmentation only given one normal image prompt. Third, its training does not depend on target domain distribution or any guidance from language. To obtain the metal model, we rethink AS tasks and find they can be transformed into change segmentation between one normal image prompt and query images. From this novel perspective, we are capable of leveraging larger number of synthetic image pairs that exhibit appearance changes based on available image datasets. We assume these synthesized image pairs carry mask annotations indicating change regions. Inspired by the mental model in predictive coding theory [43], we propose simple but effective framework that learns the metal model in one-prompt meta-learning manner. The meta-learning ensures strong generalization [8] when applying the model for segment unseen anomalies. Our contributions are summarized as follows: We present novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs with object-level and local region changes, thereby overcoming the long-standing challenge of lacking large-scale anomaly segmentation datasets. We propose one-prompt meta-learning framework training on synthesized images and generalizing well on real-world scenarios. To handle geometrical variations between prompt and query images, we proposed soft feature alignment module that builds bridge between paired-image change perception and singe-image semantic segmentation. We provide pure visual foundation model for universal anomaly segmentation that can serve as an alternative to widely used vision-language models. Our method, which requires only single normal image prompt and no additional training, effectively and efficiently segments any visual anomalies. On three industrial anomaly benchmarks, our approach achieves state-of-the-art performance, while also enjoying faster speed and requiring fewer parameters."
        },
        {
            "title": "2 Related Work",
            "content": "Unsupervised AS aims to segment anomaly pixels for both normal and anomaly testing images only given full normal training images. Unsupervised AS can be categorized into two learning paradigms, separated and unified models. Most AS methods focus on training separated models for different 2 objects or textures. However, this separated paradigm may be impractical, as it requires high memory consumption and storage burden, especially with the number of classes increasing. In contrast, the unified models attempt to detect anomalies for all categories using single model. Compared to the separated mode, the unified paradigm is more challenging as it requires handling more complex data distributions. From modeling perspective, AS methods can be roughly grouped into three groups, embedding, discriminator, and reconstruction. Embedding-based methods, such as PaDiM [12], MDND [44], PatchCore [47], CS-Flow [49] and PyramidFlow [29], assume that offline features extracted from pre-trained model preserve discriminative information and thus help to separate anomalies from normal samples. They usually model normal features to normal distribution or store them in memory bank. Then, anomaly scores are calculated by comparing testing features and the modeled distribution or the memory bank. Discriminator-based methods, such as CutPaste [31], DRAEM [72], [10], and SimpleNet [37], typically convert unsupervised AS to supervised ones by introducing pseudo (synthesized) anomaly samples. The pseudo-anomaly samples are generated by pasting random patches or adding Gaussian noise to normal images or features. Naturally, binary anomaly classifier or segmentation model can be trained on normal and pseudo-anomaly samples in supervised manner. Reconstruction-based AS, such as autoencoder [59, 2, 20, 23], generative adversarial networks [41, 67, 71] and reconstruction networks [73, 45, 36], assume that anomalous regions should not be able to be properly reconstructed and thus result in high reconstruction errors since they do not exist in normal training samples. These methods tend to be computationally expensive because they involve reconstruction in image space. The recent knowledge distillation [5, 62, 61, 52, 13] or feature reconstruction methods [70, 75, 68] train student or reconstruction network to match fixed pre-trained teacher network and achieve good balance between effectiveness and efficiency. However, all these methods are limited to recognizing anomalies in close set as the same as the training set but often perform poorly on unseen classes in open-world scenarios. Few-shot AS pays attention to learning with only limited number of normal samples. TDG [54] proposes multi-scale hierarchical generative model, which jointly learns self-supervised discriminator and generator in an adversarial training manner. DifferNet [48] detects defects utilizing normalizing-flow-based density estimation from few normal image features. RegAD [24] learns the category-agnostic feature registration, enabling the model to detect anomalies in new categories given few normal images without fine-tuning. GraphCore [66] utilizes graph representation and provides visual isometric invariant feature. FastRecon [15] utilizes few normal samples as reference to reconstruct normal version for query sample with distribution regularization, where the final anomaly detection can be achieved by sample alignment. Some works [14, 69] consider another few-shot setting where limited number of samples is given from the anomalous classes. Instead of learning few-shot models with few normal or anomaly images, we push it to new extreme only using one normal image as visual prompt at the inference stage, not involving model training. Zeroand Few-shot AS mainly utilizes large pre-trained vision-language models, e.g., CLIP [42], have shown unprecedented generality, and achieved impressive performance. WinCLIP [26] firstly utilizes multiple handcrafted textual prompts on powerful CLIP model that can yield excellent zeroand few-shot AS performance. AnomalyCLIP [76] learns object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. InCtrl [77] detects residual CLIP visual features between test images and in-context few-shot normal samples. However, optimizing AnomalyCLIP [76] and InCtrl [77] requires an auxiliary domain dataset including normal and anomaly images. PromptAD [32] introduces the concept of explicit anomaly margin, which mitigates the training challenge caused by the absence of anomaly images. Furthermore, AnomalyGPT [21] incorporates visual-language model and large language model applying multiturn dialogues. It not only indicates the presence and location of the anomaly but also provides detailed description of anomalies. Instead of visual-language models, ACR [30] and MuSc [33] perform zero-shot AS only requiring information from batchand full-level testing images, but they may be limited in privacy protection scenarios. Overall, existing most methods primarily use textual prompts based on visual-language models to identify anomalies. Different from these methods, we explore universal AS using one normal image as visual prompt without guidance from language or information from testing images. 3 Figure 1: The proposed MetaUAS consists of an encoder, feature alignment module (FAM), and decoder. It is trained on synthesized dataset in one-prompt meta-learning manner for change segmentation tasks. Once trained, it can segment any anomalies providing only one normal image prompt."
        },
        {
            "title": "3 Method",
            "content": "3.1 Rethinking Anomaly Segmentation Unlike traditional image segmentation, since anomaly appearance has various ways, it is hard to exhaustively pre-define and collect enough anomaly samples to train an AS model. Most unsupervised AS methods have to use normal samples for building models. However, these unsupervised models are limited to recognizing anomalies in close set but often perform poorly on unseen classes in open-world scenarios. In contrast, humans can quickly learn novel concepts from few training examples. To this end, few-shot AS aims to adapt novel classes by only providing few normal images. Unfortunately, existing few-shot AS models are still far behind unsupervised ones. Zeroand few-shot AS methods rely on powerful vision-language models that are capable of handling unseen anomalies, benefiting from their strong generalization. For further boosting zero-shot performance, some recent works attempt to optimize models by an auxiliary domain dataset including normal and anomaly images with pixel-level annotations. Although they can generalize to different domains, training models with normal and anomaly images conflict with the original intentions of anomaly segmentation to some extent. In addition, visual representations are not dependent on language prompts. Given this perspective, natural question emerges: can an image prompt visual model replace textual prompts vision-language approaches for universal anomaly segmentation? And can such one-prompt vision model be trained on non-anomaly segmentation dataset? This paper explores and attempts to answer the above questions. Generally, anomalies mainly include appearance, disappearance, and exchange, which are very similar to the types of changes in change segmentation [60]. Change segmentation aims to identify changes that occur on pair of images captured at different times. Therefore, anomaly segmentation will be absorbed into change segmentation if we regard normal prompt and query as pair of images captured at different times. Indeed, one can imagine that if model is capable of perceiving changes, it would naturally generalize to anomaly segmentation. This simple transformation allows us to achieve universal AS with visual modality alone. This reason is change segmentation does not require anomaly images, as they can be trained using pairs of images containing any changes, which are easily synthesized by available image datasets. 3.2 One-Prompt Meta-Learning for Universal Anomaly Segmentation Given change segmentation dataset Dbase = {X ) denotes the i-th image pair and Yi is its corresponding change mask. MetaUAS trains meta-model based on the base set Dbase and then segments query images {X }N i=1 with the corresponding normal image prompt and p belongs to the same class. Note that the base and novel set are non-overlapping (i.e., DbaseDnovel=). from novel set Dnovel, where , Yi}i, where (X , , Overview. As shown in Fig. 1, our MetaUAS framework is mainly composed of an encoder, feature alignment module, and decoder. The encoder extracts hierarchical features using pre-trained model, while the decoder integrates query and prompt features to predict change heatmap. The feature alignment module is bridge between the encoder and the decoder. It aligns the query and normal prompt features to address the geometric variation in spatial position. Concretely, for query image and its corresponding prompt image (X and RHW 3), we parallelly extract }5 l=1 and {F muti-scale offline features, {F l=1 Rhlwlcl from the encoder, where denote }5 the l-th stage of the encoder. Then, the feature alignment module independently processes and for aligning query and prompt in feature space at each scale. Next, these aligned features are contacted and fed into the decoder for predicting change heatmaps. The MetaUAS is trained in meta-learning manner. At inference, given query image and its normal image prompt, the anomaly mask can be directly predicted. Next, we elaborately introduce them in this section. Encoder. MetaUAS is compatible with any hierarchical architecture. Considering efficiency, we use the standard convolution-based EfficientNet-b4 [57] as our encoder following UniAD [70]. Given query image and its prompt p, we extract multi-scale features from the l-th (l = 1, 2, , 5) stage of pre-trained encoder F(; θe), that is and (1) where θe is frozen to sufficiently utilize its generalization because it is pre-trained on large-scale ImageNet. = F(X q; θe), q = F(X p; θe), Feature Alignment Module. We expect to learn comparison between query and prompt features (F ) for improving change segmentation. simple and naive manner is to directly contact them along the channel dimension, that is and Fl = Concat(F , ). (2) Then, the fused features Fl are fed into decoder to perform change segmentation. This simple fusion manner may only work when the query and its prompt are aligned in pixel space. However, this usually does not hold in practical applications because it is hard to refrain from geometric variations between query and prompt. Therefore, we have to align query and prompt features for better change segmentation. We propose two alignment strategies, hard and soft alignment, for enabling the interaction between query and prompt features. Hard Alignment aims to search the most similar prompt feature at spatial dimension for each query feature. Here, we take cosine similarity as distance measure. Formally, for any query feature (i, j) Rcl , the most similar prompt feature is (cid:0) argmin l (i, j), (k, l) (cid:1), (i, j) (3) k,l where (i, j) and (k, l) denote spatial locations. Considering computational efficiency, we use 11 convolution layer Conv(; θa) with shared parameters θa to reduce the dimension of the channel before computing the cosine similarity, that is Conv(F l ; θa), Conv(F ; θa). (4) Soft Alignment is different from the hard alignment, which aligns each query feature with weighted combination on the prompt feature. Similar to the hard alignment, we first apply Eq. 4 to reduce computation. The weighted probability is computed with the softmax function on cross-similarity between the query and prompt features, that is Wijkl = Softmax (cid:0)F where the Softmax operation is applied to the last two dimensions, and thus (cid:80) Wijkl = 1. Finally, the aligned prompt feature can be obtained by the weight and original prompt feature, that is (cid:88) (k, l))T (cid:1) , (i, j)(F (cid:88) (5) (cid:80) l (i, j) WijklF (k, l). (6) The hard and soft alignment can adaptively align prompt features with query features, and thus handing geometric variation between query and prompt images to some extent. Appling Eqs. 3 or 6, we obtain an aligned prompt feature and then replace the original prompt feature in Eq. 2 with it. k Decoder. Considering efficiency, we apply the feature alignment module to three high-level features of prompt and query, and thus three fusion features {Fl}5 l=3 are derived. Change segmentation needs to predict each pixel to determine whether it is changed. Specifically, we utilize UNet [46] G(; θg) as our decoder because it is better suited for tasks requiring high precision and the preservation of fine-grained details. The UNet integrates all three fused features and two low-level original features and produces final feature at the original image resolution. Finally, segmentation head transforms the final feature to generate pixel-level change prediction ˆY . The segmentation head is implemented by simple 11 convolution layer, Conv(; θh), following sigmoid activation. 5 3.3 Synthesizing Change Segmentation Images Remote sensing [7] and street scenes [1] are two main scenarios in change segmentation. They mainly focus on semantic change, and various noises are included in unchanged background regions. Furthermore, the dataset scale is small and the diversity is insufficient. Therefore, it is not suitable for universal change segmentation. Recent works [64, 63, 22] have exploited generative diffusion models to create synthetic datasets and presented promising performance on real datasets. However, it is hard to guarantee generative annotations are accurate. In contrast, some works have shown that simple synthesis, such as copy-paste, also brings strong performance for instance segmentation [19, 51] and anomaly segmentation [31, 72]. Similar to these works, we want to leverage synthetic change segmentation dataset with accurate change masks. As early discussed, there are three main change types, appearance, disappearance, and exchange, where appearance and disappearance are pair of opposite concepts, and they can be transformed into each other by swapping paired images. Therefore, we only need to synthesize two change types to simulate all three ones. Object-Level Change. In the famous MSCOCO [35], instances are annotated with polygons, and thus their foreground masks are available. Following CYWS [50], given random instance and its binary mask from an image, we could make it disappear from the image by inpainting the mask region [56]. In this simple manner, we can simulate the disappearance change. Meanwhile, the change mask is freely available. The appearance change can be easily obtained by swapping original and unpainted images. It is challenging to synthesize the exchange change because two different instances usually mean different masks. But we can randomly paste an or multiple instances to given image for rough simulation. Figure 2: Selected synthesizing image pairs and their change masks. (a) and (b) simulate appearance and disappearance synthesizing with mask inpainting [56], and [50], (c) simulate exchange synthesizing with random pasting, and (d) simulate local region changes synthesizing with DRAEM [72]. Local-Region Change. Object-level changes are generated only by inpainting mask regions or randomly pasting objects as shown in Fig 2. However, anomaly changes are usually diverse, and they may be whole objects or local regions. The local disappearance and appearance may be failed by inpainting because local regions are easily restored by context. Following DRAEM [72], we first generate binary change mask with Perlin noise and then synthesize new image by filling the mask region with another image pixels. This synthesis can simulate local changes since the binary mask is generated randomly. Given changed image pair, we apply various data augmentations, such as scale transformations, translation, rotation, and color jittering to enhance the diversity of changes during training phase. 3.4 Training and Inference Traning. We train MetaUAS in meta-learning manner, and each meta-task {X , Yi} is one prompt-query pair. The binary cross-entropy loss is adopted to optimize the learnable parameters (θa, θg, and θh) of MetaUAS, that is , (cid:88) (cid:16) = Yi log( ˆYi) + (1 Yi) log(1 ˆYi) (cid:17) . (7) Inference. For class-specific query image q, we first randomly select normal image prompt from the corresponding normal training set and then process prompt-query pairs online to perform anomaly segmentation. For class-agnostic query image, we need to first construct class-aware i=1 via extracting offline features of all normal prompts {X prompt pool {Pi} i=1 in total of classes, and then derive the best matching prompt by computing the cosine similarity between the query feature and the prompt pool. Here, the query and prompt features are obtained by using global average pooling on the last stage feature from the encoder. } 6 (a) (b) (c) (d) (e) (f) (g) (a) (b) (c) (d) (e) (f) (g) Figure 3: Qualitative comparisons with state-of-the-art methods on MVTec, VisA and Goods. In both two sub-figures (left and right), (b) and (g) represent query images and their anomaly masks, while (a) represent the corresponding normal image prompts. The predicted anomaly maps are shown using different methods, including (c) WinCLIP+ [26], (d) AnomalyCLIP [76], (e) UniAD [70] and (f) our MetaUAS. Best viewed in color and zoom-in. Given query image and the corresponding prompt p, we successively feed them into the encoder, the feature alignment module, the decoder, and the segmentation head, and finally obtain predicted anomaly map ˆY . Following previous works, we take the maximum of ˆY as the image-level anomaly score, without additional post-processing."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experimental Setup Following previous works, we comprehensively evaluate MetaUAS on three industrial anomaly segmentation benchmarks, MVTec [4], VisA [78] and Goods [74]. We train universal change segmentation model on synthetic dataset. To demonstrate cross-domain generalization ability, we directly test it on three industry anomaly detection benchmarks without fine-tuning Competing Methods. We compare MetaUAS and its two variants (MetaUAS and MetaUAS+) with diverse state-of-the-art anomaly segmentation methods including zero-shot CLIP [42], WinCLIP [26], AnomalyCLIP [76], and one-shot PatchCore [47], WinCLIP+ [26], and fullshot UniAD [70]. MetaUAS segments any anomalies with only one normal image prompt. Here, the one normal prompt is randomly sampled from normal training images for each class. The results are mean and standard deviation based on 5 independent repeated tests with different random seeds. MetaUAS takes the best-matched normal image from the normal training set as the prompt of query image. Here, the matching degree is computed using the cosine similarity between the query image and all normal training images in feature space. MetaUAS+ builds on MetaUAS. Following WinCLIP+ [26], we also add the visual prior knowledge from the image encoder of CLIP model to our MetaUAS for fair comparison. The visual prior knowledge used in MetaUAS+ is kept exactly the same as WinCLIP+. Evaluation Metrics. Following previous works, we use ROC, PR, and F1max metrics for image-level anomaly classification. Similar to the anomaly classification evaluation, we use the same metrics and additionally report Per-Region Overlap (PRO) for pixel-level anomaly segmentation. We argue that the PR and F1max metrics are better for anomaly segmentation, where the imbalance issue is very extreme between normal and anomaly pixels [11, 78]. 4.2 Comparison with Previous Works Tables 1 and 2 present the comparison results of MetaUAS with the above-mentioned competing methods in generalization and efficiency, respectively. 7 Table 1: Quantitative comparisons on MVTec, VisA and Goods. Red indicates the best performance, while blue denotes the second-best result. Gray indicates the model is trained by full-shot normal images. Datasets Methods Venue Shot Auxiliary Anomaly Classification Anomaly Segmentation I-ROC I-PR I-F1max P-ROC P-PR P-F1max P-PRO V i o CLIP [42] PatchCore [47] WinCLIP [26] WinCLIP+ [26] AnomalyCLIP [76] UniAD [70] 0 ICML 21 1 CVPR 22 0 CVPR 23 1 CVPR 23 0 ICLR 24 NeurIPS 22 full MetaUAS MetaUAS MetaUAS+ 1 1 1 CLIP [42] PatchCore [47] WinCLIP [26] WinCLIP+ [26] AnomalyCLIP [76] UniAD [70] 0 ICML 21 1 CVPR 22 0 CVPR 23 1 CVPR 23 0 ICLR 24 NeurIPS 22 full MetaUAS MetaUAS MetaUAS+ 1 1 CLIP [42] PatchCore [47] WinCLIP [26] WinCLIP+ [26] AnomalyCLIP [76] UniAD [70] 0 ICML 21 1 CVPR 22 0 CVPR 23 1 CVPR 23 0 ICLR 24 NeurIPS 22 full MetaUAS MetaUAS MetaUAS+ 1 1 1 89.3 88. 62.0 74.4 79.00.8 89.61.1 88.90.3 93.10.2 37.10.9 42.20.8 82.70.5 90.4 92.81.2 96.40.7 93.80.5 93.50.2 38.41.2 42.51.0 83.90.4 91.5 96.7 34.5 44.7 92.7 96.7 91.1 96.8 96.3 98. 39.1 50.4 81.4 90.0 18.2 82.3 92.7 21. 95.6 61.9 24.8 11.2 6.5 90.70.7 95.70.6 92.50.3 94.60.2 59.31.4 57.51.1 82.60.6 94.2 95. 63.7 67.0 95.3 97.6 97.6 97.9 93.9 94.6 61.6 62.9 83.1 92. 56.5 74.5 67.4 59.1 64.21.0 66.00.7 75.50.5 95.50.3 16.51.7 26.01.5 84.60.5 75.5 80.52.6 82.12.7 81.31.0 94.40.1 15.90.2 23.20.4 79.30.3 81.9 90.8 21.3 34.3 85.4 93. 95.5 98.5 80.7 87.8 86.8 84.8 28.3 39.1 78.2 73. 78.7 22.4 51.0 1.8 5.4 3. 9.0 81.21.7 84.51.4 80.20.7 92.20.7 42.70.8 44.70.6 60.41.5 83.4 85.1 43.9 48.1 81.3 82.3 92.0 98.0 85.7 87. 45.6 48.6 57.3 85.5 55.3 71.3 57.3 51.8 48.31.0 54.20.5 71.30.1 84.30.5 4.50.2 9.30.3 55.61.0 52.2 53.50.2 58.60.2 71.50.1 85.50.6 5.70.4 11.30.5 56.61.2 57.2 67. 16.9 15.0 63.3 72.1 71.4 74.6 83.5 90.4 63.3 66.1 24.0 20. 71.4 73.0 58.2 44.5 10.2 16. 5.0 4.3 2.0 54.51.0 58.50.4 71.50.1 88.50.6 8.60.7 14.00.7 59.01.3 90.1 89.9 53.7 49.0 91.7 89. 97.4 97.9 85.7 86.2 70.8 88.0 55.5 55.8 Generalization. First, MetaUAS with one normal image prompt achieves competitive performance among all zero-, fewand full-shot methods both on MVTec and VisA. This suggests that it is possible to boost anomaly classification and segmentation performance only with visual information alone. But on Goods dataset, MetaUAS seems to perform similarly to other methods. Different from MVTec and VisA, Goods consists of six groups and each group contains dozens or even hundreds of subcategories (484 in total). In fact, it is challenging to address multi-classes with single model, and the state-of-the-art UniAD is not good even using all normal training images. In contrast, MetaUAS only uses one normal image prompt for each group, which means that the prompt image does not match most query images from multiple subcategories. Furthermore, MetaUAS significantly outperforms almost all competing models when the best-matched normal image is used to take as the normal prompt of each query image. In addition, WinCLIP+ boosts few-shot AS with dense similarity between few-shot prompts and query images. For fair comparison, we also add the visual prior knowledge from the image encoder of CLIP model to our MetaUAS (denoting as MetaUAS+). We can see that the performance can be further improved when introducing the visual prior of CLIP models to MetaUAS. Efficiency. We measure complexity and efficiency with the number of parameters and forward inference times. The evaluation is performed on one V100 GPU with batch size 32. The number of parameters of WinClIP+ and AnomalyCLIP is 10 and 20 of MetaUAS due to the large visionlanguage backbone. Compared to state-of-the-art, our MetaUAS+ achieves the best performance using the single model with half of the parameters and faster inference time. What is more, our MetaUAS has 10 fewer parameters and 100 speed improvement compared to WinCLIP+, which still performs better. Qualitative Comparisons. Figs. 3 and 4 show some selected visualizations from MVTec, VisA, and Goods testing images using the state-of-arts and our MetaUAS. Generally, MetaUAS segments anomalies more accurately and produces fewer false positives. MetaUAS is robust to different image prompts from the same category, especially for those categories with large geometric variations, such as screw. We believe that better performance can be derived if the objects or textures of the prompt image and query images can be roughly aligned. 8 Table 2: The complexity and efficiency comparisons. The performance of anomaly classification and segmentation is reported on MVTec. Methods #All Params(#Learnable) Input Size Times (ms) I-ROC Backbone P-ROC P-PR CLIP [42] PatchCore [47] ViT-B-16+240 E-b WinCLIP [26] WinCLIP+ [26] AnomalyCLIP [76] ViT-L/14@336px UniAD [70] ViT-B-16+240 Eb4 MetaUAS MetaUAS MetaUAS+ MetaUAS MetaUAS MetaUAS+ Eb Eb4+ViT-B-16+240 Eb4 Eb4+ViT-B-16+240 208.4 (0.0) 17.5 (0.0) 208.4 (0.0) 433.5 (5.6) 27.1 (7.7) 22.1 (4.6) 139.3 (4.6) 22.1 (4.6) 139.3 (4.6) 240240 256256 512 240240 518518 224224 256256 512512 13.7 36.4 145.1 201.3 339.5 154.9 5.0 3. 204.8 12.0 213.0 6.5 62.0 74.4 79.00.8 93.10.2 37.10.9 79.10.7 93.10.2 37.51.2 90.4 92.81.2 93.50.2 38.41.2 91.5 96. 34.5 44.7 91.1 96.8 18.2 82.3 90.70.7 94.60.2 59.31.4 94.2 95.3 63.7 67. 95.3 97.6 90.41.8 92.90.4 57.21.9 93.2 94.8 59.8 65.8 93.3 97.1 Figure 4: Anomaly segmentation for query images with different normal image prompts including 5 random prompts and the optimal prompt (denoting as prompt). The anomaly segmentation maps are generated with MetaUAS, MetaUAS and MetaUAS+. 4.3 Ablation Study We perform component-wise analysis on MVTec with 256256 inputs. The influence of feature alignment module. As reported in Tab. 3a, we conduct experiments with combinations of different align strategies and feature fusions. The experimental results demonstrate that using soft alignment is better than the other two ones (no alignment and hard alignment) in the comprehensive results of AC and AS. Moreover, we compare the feature concatenation (Concat) with element-wised addition (Add) and absolute difference (AbsDiff) for aggregating prompt and query features. The Concat operation is the best one. The Add is not suitable to fuse two types of features as it fails to depict the image changes, and it may lead to confusion between these two features, as its results are the worst. The AbsDiff is widely used in the change segmentation field. However, this method may result in the loss of contextual information. In contrast, direct concatenation preserves all information and allows the network to adaptively learn the fusion, yielding the best results. The effects of change types. The diversity of synthetic data is critical for good generalization. We use object-level changes and local region changes to ensure this diversity. Experiments show that object-level changes contribute more performance than local changes in Tab. 3c. This is natural because the object-level change is implemented by inpainting object regions and randomly pasting 9 Table 3: Ablation studies on MVTec. Default settings are marked in blue. (a) Effect of feature alignment module. No. Align Fusion I-ROC I-PR P-ROC P-PR P-PRO (b) Learn or freeze encoder? No. Backbone Learn? I-ROC I-PR P-ROC P-PR P-PRO 1 No Concat 82.8 92.5 88.4 44.9 67.5 2 Hard Concat 87.1 94.7 90.7 48.2 77.0 3 Soft Concat 91.3 96.2 94.6 59.6 82.6 4 Soft Add 71.8 86.9 73.2 24.0 45.2 5 Soft AbsDiff 84.1 92.4 88.4 45.9 68.4 E-b4 E-b4 E-b6 Learn 86.5 93.6 93.1 50.3 74.6 1 Freeze 91.3 96.2 94.6 59.6 82.6 2 3 Freeze 90.1 95.5 95.1 56.9 80.8 4 EViT-b3 Freeze 89.5 95.7 95.3 58.5 80.9 Freeze 76.2 87.8 87.6 33.7 61.0 5 M-v2 (c) Effects of change types and decoder module. (d) Effects of the number of training samples. No. ChangeType Decoder I-ROC I-PR P-ROC P-PR P-PRO No. #Samples I-ROC I-PR P-ROC P-PR P-PRO 83.1 92.8 87.7 44.3 76.1 1 Only Loc. 90.5 96.0 94.5 58.3 75.4 2 Only Obj. 91.3 96.2 94.6 59.6 82.6 3 Obj.+Loc. 4 Obj.+Loc. FPN-Cat 86.9 86.9 91.6 49.9 76.7 5 Obj.+Loc. FPN-Add 88.4 94.7 94.1 51.4 73.1 UNet UNet UNet 1 2 3 4 10% 30% 50% 70% 95% 82.0 91.9 85.4 36.5 62.1 87.4 93.6 89.1 50.6 73.8 91.0 96.2 92.9 57.1 74.3 91.1 96.4 94.5 57.0 78.3 91.3 96.2 94.6 59.6 82.6 objects, which has larger space than local region synthesis. Undoubtedly, combining them further enhances the diversity of synthetic changes, thus further improving performance. Learn or freeze encoder. In Tab. 3b, if we train the encoder like other modules, the performance drops on both AC and AS. We speculate that it makes the network overfit change segmentation dataset, which will degenerate generalization. We also evaluate other backbones, such as EfficientNet-b6 [57], EfficientViT-b3 [6], and MobileNetV2 [53]. The EfficientNet-b4 performs better than others. The reason might be that shallow networks cannot extract discriminative features, while deep networks focus more on semantic features. AS requires more structural and texture features. The effects of decoder module. In the decoder, we compare UNet [46] with FPN [34]. U-Net has been widely used and validated in many segmentation tasks due to its effectiveness and efficiency, while FPN is type of network designed for object detection that requires recognizing objects at various scales. As reported in Tab. 3c, both different types of FPN are worse than UNet. The effects on training samples scale. To investigate the influence of training samples scale on model performance, we conduct experiments with different training subsets where each one is generated by randomly sampling the original training set at various rates, such as {10%, 30%, 50%, 70%, 95%}. The performance of each model on the MVTec testing set is reported in Tab. 3d. It can be seen that MetaUAS still works when the number of training images is small scale (e.g., 50%), and the performance can further improve when increasing the number of training samples."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper is the first study to focus on universal anomaly segmentation using pure visual information, enabling the segmenting of unseen anomalies without any training on target anomaly datasets or reliance on language guidance. First, we rethink anomaly segmentation tasks and find they can be unified into change segmentation. This paradigm shift allows us to break away from the persistent challenge of lacking large-scale anomaly segmentation datasets. Naturally, we are capable of leveraging large-scale synthetic image pairs with object-level and local region changes derived from available image datasets. Second, we propose simple but effective universal anomaly segmentation framework, i.e., MetaUAS. We train MetaUAS in one-prompt meta-learning manner on this synthesized dataset. To handle geometrical variations between prompt and query images, we propose soft feature alignment module that bridges paired-image change segmentation and singe-image semantic segmentation. This makes it possible to use sophisticated semantic segmentation modules for change segmentation. MetaUAS achieves superior generalization using only one normal image prompt on three industrial datasets. Meanwhile, MetaUAS enjoys faster inference speed and fewer parameters. We believe MetaUAS will serve as an alternative to widely used vision-language models for universal anomaly segmentation. Limitation. The performance of MetaUAS can be affected by using inappropriate normal image prompts. In this study, we leverage cosine similarity to identify the most suitable prompts when the category of the query image is unknown. In scenarios involving fine-grained objects, it may be essential to train classification model to accurately predict the categories of the query images."
        },
        {
            "title": "References",
            "content": "[1] Pablo Alcantarilla, Simon Stent, German Ros, Roberto Arroyo, and Riccardo Gherardi. Street-view change detection with deconvolutional networks. Autonomous Robots, 42, 2018. [2] Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising autoencoders as generative models. In NeurIPS, 2013. [3] Paul Bergmann, Kilian Batzner, Michael Fauser, David Sattlegger, and Carsten Steger. Beyond dents and scratches: Logical constraints in unsupervised anomaly detection and localization. IJCV, 130(4), 2022. [4] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. MVTec-AD: comprehensive real-world dataset for unsupervised anomaly detection. In CVPR, 2019. [5] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed Students: Student-teacher anomaly detection with discriminative latent embeddings. In CVPR, 2020. [6] Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han. Efficientvit: Lightweight multi-scale attention for high-resolution dense prediction. In ICCV, 2023. [7] Hao Chen and Zhenwei Shi. spatial-temporal attention-based method and new dataset for remote sensing image change detection. Remote Sensing, 12(10), 2020. [8] Jiaxin Chen, Xiao-Ming Wu, Yanke Li, Qimai Li, Li-Ming Zhan, and Fu-lai Chung. closer look at the training strategy for modern meta-learning. In NeurIPS, 2020. [9] Yuanhong Chen, Yu Tian, Guansong Pang, and Gustavo Carneiro. Deep one-class classification via interpolated gaussian descriptor. In AAAI, 2022. [10] Li-Ling Chiu and Shang-Hong Lai. Self-supervised normalizing flows for image anomaly detection and localization. In ICCV, 2023. [11] Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In ICML, 2006. [12] Thomas Defard, Aleksandr Setkov, Angelique Loesch, and Romaric Audigier. PaDiM: patch distribution modeling framework for anomaly detection and localization. In ICPR, 2021. [13] Hanqiu Deng and Xingyu Li. Anomaly detection via reverse distillation from one-class embedding. In CVPR, 2022. [14] Choubo Ding, Guansong Pang, and Chunhua Shen. Catching both gray and black swans: Open-set supervised anomaly detection. In CVPR, 2022. [15] Zheng Fang, Xiaoyang Wang, Haocheng Li, Jiejie Liu, Qiugui Hu, and Jimin Xiao. FastRecon: Few-shot industrial anomaly detection via fast feature reconstruction. In ICCV, 2023. [16] Daniel Felleman and David Van Essen. Distributed hierarchical processing in the primate cerebral cortex. Cerebral cortex, 1(1), 1991. [17] Bin-Bin Gao. Learning to detect multi-class anomalies with just one normal image prompt. In ECCV, 2024. [18] Mariana-Iuliana Georgescu, Antonio Barbalau, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, and Mubarak Shah. Anomaly detection in video via self-supervised and multi-task learning. In CVPR, 2021. [19] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin Cubuk, Quoc Le, and Barret Zoph. Simple copy-paste is strong data augmentation method for instance segmentation. In CVPR, 2021. [20] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection. In ICCV, 2019. 11 [21] Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, and Jinqiao Wang. AnomalyGPT: Detecting industrial anomalies using large vision-language models. In AAAI, 2024. [22] Guan Gui, Bin-Bin Gao, Jun Liu, Chengjie Wang, and Yunsheng Wu. Few-shot anomaly-driven generation for anomaly classification and segmentation. In ECCV, 2024. [23] Jinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, and Hong Zhou. Divideand-Assemble: Learning block-wise memory for unsupervised anomaly detection. In ICCV, 2021. [24] Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, and Yan-Feng Wang. Registration based few-shot anomaly detection. In ECCV, 2022. [25] Loshchilov Ilya and Hutter Frank. Decoupled weight decay regularization. In ICLR, 2019. [26] Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and Onkar Dabeer. WinCLIP: Zero-/few-shot anomaly classification and segmentation. In CVPR, 2023. [27] Xi Jiang, Jianlin Liu, Jinbao Wang, Qiang Nie, Kai WU, Yong Liu, Chengjie Wang, and Feng Zheng. SoftPatch: Unsupervised anomaly detection with noisy data. In NeurIPS, 2022. [28] Dong-Yun Kim, Soo Jin Lee, Eun-Kyu Kim, Eunyoung Kang, Chan Yeong Heo, Jae Hoon Jeong, Yujin Myung, In Ah Kim, and Bum-Sup Jang. Feasibility of anomaly score detected with deep learning in irradiated breast cancer patients with reconstruction. npj Digit. Med., 5(1), 2022. [29] Jiarui Lei, Xiaobo Hu, Yue Wang, and Dong Liu. PyramidFlow: High-resolution defect contrastive localization using pyramid normalizing flow. In CVPR, 2023. [30] Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, and Stephan Mandt. Zero-shot anomaly detection via batch normalization. In NeurIPS, 2023. [31] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. CutPaste: Self-supervised learning for anomaly detection and localization. In CVPR, 2021. [32] Xiaofan Li, Zhizhong Zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yuan Xie, and Lizhuang Ma. PromptAD: Learning prompts with only normal samples for few-shot anomaly detection. In CVPR, 2024. [33] Xurui Li, Ziming Huang, Feng Xue, and Yu Zhou. MuSc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images. In ICLR, 2024. [34] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017. [35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. [36] Wenrui Liu, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Diversity-measurable anomaly detection. In CVPR, 2023. [37] Zhikang Liu, Yiming Zhou, Yuansheng Xu, and Zilei Wang. SimpleNet: simple network for image anomaly detection and localization. In CVPR, 2023. [38] Declan McIntosh and Alexandra Branzan Albu. Inter-realization channels: Unsupervised anomaly detection beyond one-class classification. In ICCV, 2023. [39] Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, and Gian Luca Foresti. VT-ADL: vision transformer network for image anomaly detection and localization. In ISIE, 2021. [40] Hyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning memory-guided normality for anomaly detection. In CVPR, 2020. [41] Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. OCGAN: One-class novelty detection using GANs with constrained latent representations. In CVPR, 2019. [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [43] Rajesh PN Rao and Dana Ballard. Predictive coding in the visual cortex: functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2(1), 1999. [44] Oliver Rippel, Patrick Mertens, and Dorit Merhof. Modeling the distribution of normal data in pretrained deep features for anomaly detection. In ICPR, 2021. [45] Nicolae-Catalin Ristea, Neelu Madan, Radu Tudor Ionescu, Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas Moeslund, and Mubarak Shah. Self-supervised predictive convolutional attentive block for anomaly detection. In CVPR, 2022. [46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. [47] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. In CVPR, 2022. [48] Marco Rudolph, Bastian Wandt, and Bodo Rosenhahn. Same same but differnet: Semisupervised defect detection with normalizing flows. In WACV, 2021. [49] Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bastian Wandt. Fully convolutional cross-scale-flows for image-based defect detection. In WACV, 2022. [50] Ragav Sachdeva and Andrew Zisserman. The change you want to see. In WACV, 2023. [51] Kuniaki Saito, Ping Hu, Trevor Darrell, and Kate Saenko. Learning to detect everything in an open world. In ECCV, 2022. [52] Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad Rohban, and Hamid Rabiee. Multiresolution knowledge distillation for anomaly detection. In CVPR, 2021. [53] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted residuals and linear bottlenecks. In CVPR, 2018. [54] Shelly Sheynin, Sagie Benaim, and Lior Wolf. hierarchical transformation-discriminating generative model for few shot anomaly detection. In ICCV, 2021. [55] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In CVPR, 2018. [56] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In WACV, 2022. [57] Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural networks. In ICML, 2019. [58] Keiji Tanaka. Inferotemporal cortex and object vision. Annual review of neuroscience, 19(1), 1996. [59] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008. [60] Guo-Hua Wang, Bin-Bin Gao, and Chengjie Wang. How to reduce change detection to semantic segmentation. PR, 2023. [61] Guodong Wang, Shumin Han, Errui Ding, and Di Huang. Student-teacher feature pyramid matching for anomaly detection. BMVC, 2021. 13 [62] Shenzhi Wang, Liwei Wu, Lei Cui, and Yujun Shen. Glancing at the patch: Anomaly localization with global and local feature comparison. In CVPR, 2021. [63] Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, and Chunhua Shen. DatasetDM: Synthesizing data with perception annotations using diffusion models. In NeurIPS, 2023. [64] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen. DiffuMask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models. In ICCV, 2023. [65] Tiange Xiang, Yongyi Lu, Alan Yuille, Chaoyi Zhang, Weidong Cai, and Zongwei Zhou. In-painting radiography images for unsupervised anomaly detection. In CVPR, 2023. [66] Guoyang Xie, Jingbao Wang, Jiaqi Liu, Feng Zheng, and Yaochu Jin. Pushing the limits of fewshot anomaly detection in industry vision: Graphcore. In ICLR, 2023. [67] Xudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, and Pheng-Ann Heng. Learning semantic context from normal samples for unsupervised anomaly detection. In AAAI, 2021. [68] Xincheng Yao, Ruoqi Li, Zefeng Qian, Yan Luo, and Chongyang Zhang. Focus the Discrepancy: Intra-and inter-correlation learning for image anomaly detection. In ICCV, 2023. [69] Xincheng Yao, Ruoqi Li, Jing Zhang, Jun Sun, and Chongyang Zhang. Explicit boundary guided semi-push-pull contrastive learning for supervised anomaly detection. In CVPR, 2023. [70] Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, and Xinyi Le. unified model for multi-class anomaly detection. In NeurIPS, 2022. [71] Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, and Seung-Ik Lee. Old is Gold: Redefining the adversarially learned one-class classifier training paradigm. In CVPR, 2020. [72] Vitjan Zavrtanik, Matej Kristan, and Danijel Skoˇcaj. DRAEM: discriminatively trained reconstruction embedding for surface anomaly detection. In ICCV, 2021. [73] Vitjan Zavrtanik, Matej Kristan, and Danijel Skoˇcaj. Reconstruction by inpainting for visual anomaly detection. PR, 112, 2021. [74] Jian Zhang, Runwei Ding, Miaoju Ban, and Linhui Dai. PKU-GoodsAD: supermarket goods dataset for unsupervised anomaly detection and segmentation. RA-L, 9(3), 2024. [75] Ying Zhao. OmniAL: unified cnn framework for unsupervised anomaly localization. In CVPR, 2023. [76] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and Jiming Chen. AnomalyCLIP: Objectagnostic prompt learning for zero-shot anomaly detection. In ICLR, 2024. [77] Jiawen Zhu and Guansong Pang. Toward generalist anomaly detection via in-context residual learning with few-shot sample prompts. In CVPR, 2024. [78] Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer. Spot-thedifference self-supervised pre-training for anomaly detection and segmentation. In ECCV, 2022. 14 (a) (b) (c) (d) (e) (f) (g) (a) (b) (c) (d) (e) (f) (g) Figure A1: Qualitative comparisons with state-of-the-art methods on MVTec, VisA and Goods. In both two sub-figures (left and right), (b) and (g) represent query images and their anomaly masks, while (a) represent the corresponding normal image prompts. The predicted anomaly maps are shown using different methods, including (c) WinCLIP+ [26], (d) AnomalyCLIP [76], (e) UniAD [70] and (f) our MetaUAS. Best viewed in color and zoom-in. 15 Table A1: Quantitative results on MVTec with MetaUAS, MetaUAS and MetaUAS+. Methods Categories Anomaly Classification Anomaly Segmentation MetaUAS MetaUAS MetaUAS+ bottle cable capsule carpet grid hazelnut leather metal nut pill screw tile toothbrush transistor wood zipper I-ROC I-PR 98.30.8 90.81.5 67.15.2 99.80.3 94.61.2 97.92.2 99.90.2 94.43.9 92.31.4 63.55.0 95.60.6 92.21.8 79.76.6 98.50.3 95.92.5 99.50.2 95.10.9 89.83.0 99.90.1 98.10.5 98.91.2 100.00.0 98.61.1 98.50.2 84.43.4 98.50.1 97.20.8 79.36.8 99.50.1 98.51. I-F1max 97.90.8 86.51.8 91.40.5 99.30.7 92.71.3 95.03.2 99.70.3 95.01.2 94.21.1 85.50.3 94.41.1 91.52.9 71.94.2 96.70.8 96.11.2 P-ROC P-PR P-F1max P-PRO 97.61.6 95.20.4 94.20.7 97.40.4 89.01.2 98.10.7 99.70.0 95.00.7 96.40.6 92.13.1 95.30.5 98.90.2 82.43.2 94.10.4 94.51.3 85.92.6 64.11.3 23.66.5 73.71.5 25.12.7 66.29.8 71.20.9 76.94.0 70.12.9 8.12.9 84.61.0 70.21.6 37.25.1 70.01.7 62.12.2 77.91.5 63.01.6 33.74.3 68.11.7 33.81.6 60.38.4 65.40.8 70.92.6 63.82.3 14.43.9 79.50.7 69.20.4 37.75.0 65.92.1 59.51.7 94.41.5 85.81.8 54.37.0 94.40.4 70.02.9 87.93.5 95.80.8 87.11.2 88.62.1 72.45.7 92.00.8 81.03.6 67.13.6 89.01.1 78.72.2 mean 90.70. 95.70.6 92.50.3 94.60.2 59.31.4 57.51.1 82.60. bottle cable capsule carpet grid hazelnut leather metal nut pill screw tile toothbrush transistor wood zipper mean bottle cable capsule carpet grid hazelnut leather metal nut pill screw tile toothbrush transistor wood zipper mean 99.6 95.3 80.1 99.6 96.2 99.3 100 96.2 95.3 84.2 95.1 93.6 91.0 98.8 89.3 94. 99.6 95.5 83.4 99.8 99.6 100 100 97.8 95.8 88.2 96.1 94.4 91.1 99.0 89.4 95.3 99.9 97.6 94.9 99.9 98.7 99.6 100 99.1 99.2 94.5 98.3 97.6 88.3 99.6 96.3 97.6 99.9 97.7 95.7 100 99.9 100 100 99.5 99.3 95.6 98.6 97.9 88.5 99.7 96.4 97. 98.4 91.9 93.5 98.9 94.8 97.9 100 95.2 94.7 87.6 93.4 92.3 79.2 96.8 93.7 93.9 98.4 91.9 92.7 98.9 98.2 100 100 96.3 95.0 91.3 94.0 92.3 80.5 96.8 93.4 94.6 97.5 96.3 95.8 97.0 90.8 98.8 99.7 96.3 94.8 95.0 94.6 98.9 86.0 94.3 94.2 95. 98.8 97.1 97.8 99.5 98.2 99.1 99.7 96.5 96.8 98.4 98.1 99.4 91.6 96.7 96.0 97.6 85.6 67.5 40.5 73.9 28.7 74.7 70.9 81.4 64.8 29.4 83.3 70.3 47.9 73.0 63.7 63.7 87.5 67.4 43.3 80.6 36.5 79.1 71.6 82.1 68.5 34.4 88.4 72.6 51.0 77.4 64.7 67. 77.5 65.9 48.3 68.7 37.1 68.0 65.5 73.3 59.9 33.4 78.8 70.5 48.0 68.4 61.5 61.6 78.1 66.4 49.4 71.0 39.4 74.1 65.5 73.7 60.9 33.9 79.3 70.9 50.6 69.9 61.0 62.9 95.4 90.2 57.6 93.2 75.6 89.1 96.4 91.0 86.3 61.7 91.2 78.6 72.8 88.2 79.0 83. 96.8 91.6 90.0 98.0 94.7 92.7 98.9 92.1 94.1 90.5 95.4 91.7 78.6 95.0 87.9 92.5 Implementation Details. Following UniAD [70], we extract multi-scale features from all 5 stages of EfficientNet-b4 [57] encoder. In the feature alignment module, the three highest-level features are used to perform queryprompt alignment, and the channel number is reduced to half of one of the original channels before calculating the similarity between query and prompt. Therefore, we derive three aligned features of query and prompt using the feature alignment module. Finally, these three aligned features and two original low-level query features from the first and second stages are fed into the decoder and segmentation head for change segmentation. The model is trained with 30 epochs on 8 Tesla V100 GPUs with batch size 128. We freeze the encoder and optimize the feature alignment module, the 16 Methods MetaUAS MetaUAS MetaUAS+ Table A2: Quantitative results on VisA with MetaUAS, MetaUAS and MetaUAS+. Anomaly Segmentation Anomaly Classification Categories I-ROC I-PR 84.71.1 candle 77.73.9 capsules 78.95.1 cashew chewinggum 95.80.2 83.52.4 fryum 73.06.0 macaroni1 60.84.2 macaroni2 75.413.6 pcb1 76.02.9 pcb2 77.13.4 pcb3 95.22.4 pcb4 95.81.4 pipe fryum 85.21.4 86.42.3 90.12.4 98.20.1 91.91.4 77.05.4 59.83.5 76.09.8 76.63.0 79.82.6 95.02.1 97.51.2 I-F1max 79.71.1 79.71.7 82.21.7 93.51.1 84.01.4 71.22.0 68.00.7 75.19.6 72.93.5 72.82.7 89.34.1 93.91. P-ROC P-PR P-F1max P-PRO 99.30.1 96.50.7 91.11.9 98.50.4 65.25.4 82.42.1 89.55.7 98.20.6 94.50.2 97.00.4 97.10.8 96.91.0 60.02.3 40.54.1 49.43.7 85.21.5 14.95.5 13.16.3 2.31.1 66.15.8 30.82.7 42.73.4 41.33.2 66.03. 57.31.7 44.83.4 50.42.4 79.61.1 23.06.6 21.28.0 7.52.9 62.94.1 39.02.7 42.91.9 45.62.4 62.72.7 63.03.2 76.92.5 51.84.2 69.60.9 23.92.5 31.54.0 56.413.7 71.46.3 66.44.0 58.51.5 69.33.4 86.24.3 mean 81.21.7 84.51.4 80.20. 92.20.7 42.70.8 44.70.6 60.41.5 84.4 candle 83.4 capsules cashew 84.3 chewinggum 95.0 84.1 fryum 71.6 macaroni1 60.3 macaroni2 86.9 pcb1 79.9 pcb2 79.7 pcb3 96.1 pcb4 95.6 pipe fryum mean 83.4 85.8 candle 84.5 capsules cashew 87.7 chewinggum 95.8 89.6 fryum 73.1 macaroni1 62.6 macaroni2 87.9 pcb1 80.4 pcb2 80.7 pcb3 96.6 pcb4 96.5 pipe fryum mean 85.1 85.4 90.0 92.1 98.0 92.8 74.3 57.9 84.8 78.7 81.6 95.3 97.8 85. 86.3 91.0 93.5 98.3 94.9 76.3 64.4 86.0 79.1 82.3 95.8 98.3 87.2 78.8 82.3 85.6 93.3 83.4 71.1 67.6 80.8 75.0 73.9 91.1 92.5 81.3 79.8 82.3 88.9 93.3 88.2 70.8 67.6 81.7 75.4 75.1 91.6 93.0 82. 98.9 97.1 88.8 98.6 67.1 81.0 91.0 98.6 95.9 96.4 95.4 95.1 92.0 98.3 98.3 98.5 99.5 96.6 96.9 97.7 99.3 97.4 96.8 97.2 99.0 98.0 59.8 48.3 43.5 85.9 13.7 4.7 2.8 78.8 34.9 46.4 43.7 64.8 43. 58.5 51.5 55.9 86.0 38.6 7.8 4.6 81.8 35.1 46.7 43.6 66.9 48.1 57.5 50.6 45.6 80.1 20.6 10.4 9.6 74.5 41.0 46.4 46.9 63.5 45.6 57.5 51.8 50.6 80.2 44.5 12.5 10.5 75.7 41.8 47.2 47.1 63.5 48. 55.9 74.7 48.8 70.4 22.4 24.6 65.1 63.8 64.5 52.5 62.8 82.6 57.3 92.9 80.4 88.1 85.1 81.9 81.1 89.6 82.4 77.4 85.7 84.3 96.6 85.5 decoder, and the segmentation head with AdamW [25] using weight decay 0.0005 and learning rate 0.0001. We conduct experiments based on the open-source framework PyTorch. We follow CYWS [50] and use the same procedure for synthesizing the change segmentation dataset. Specifically, given labeled image from an existing instance segmentation dataset, i.e., MS-COCO, we randomly selected one or several instances and then could make it disappear from the image by inpainting the mask region [56]. It is worth noting that the binary change mask between the inpainted and original images can be freely available because these selected instances have been manually annotated at the pixel level. We keep the dataset setup as similar to CYWS [50] as possible. Specifically, the change segmentation dataset is synthesized using the randomly selected 60,000 images from the MS-COCO training set. For each image, synthesized image is generated by inpainting union mask of random set of labeled instances. Then, all these 60,000 samples are divided into training and validation sets with ratio of 0.95:0.05. During training, we randomly employ object-level change and local-region change with probability of 0.5. Competing Methods. To demonstrate the superiority of MetaUAS, we compare MetaUAS and its variants (MetaUAS and MetaUAS+) with diverse state-of-the-art methods. Implementation and reproduction details are summarized as follows: 17 Table A3: Quantitative results on Goods with MetaUAS, MetaUAS and MetaUAS+. Methods Categories Anomaly Classification Anomaly Segmentation MetaUAS MetaUAS MetaUAS+ cigarette box drink bottle drink can food bottle food box food package mean cigarette box drink bottle drink can food bottle food box food package mean cigarette box drink bottle drink can food bottle food box food package mean I-ROC I-PR 58.93.8 55.11.2 52.13.4 55.01.4 52.92.6 52.71.7 54.51.0 63.23.6 59.31.1 48.42.0 64.40.5 65.62.5 50.41.8 58.50.4 I-F1max 74.20.5 70.60.2 66.70.0 75.00.1 77.70.3 64.80.1 71.50.1 P-ROC P-PR P-F1max P-PRO 88.41.3 92.40.7 86.61.4 89.90.3 86.41.7 87.51.5 88.50.6 21.13.3 7.31.6 7.61.1 8.40.6 4.40.7 2.80.6 8.60.7 28.92.9 12.82.0 14.00.8 14.20.6 8.31.1 5.71.4 14.00.7 62.93.3 64.60.7 58.40.5 59.41.0 57.22.3 51.63.2 59.01. 98.9 85.2 96.7 90.1 86.9 82.7 90.1 97.5 85.4 97.2 90.4 85.2 83.6 89.9 99.2 86.7 97.1 93.1 92.4 81.8 91. 96.3 86.8 97.5 92.9 87.4 78.1 89.9 96.0 80.9 91.5 86.2 84.5 74.8 85.7 96.4 81.3 91.8 86.7 84.1 76.9 86. 98.7 98.9 93.8 97.1 98.3 97.5 97.4 98.6 98.8 96.7 97.5 97.8 97.9 97.9 78.0 62.2 44.9 50.5 54.6 32.1 53. 74.9 58.7 42.8 44.1 46.5 27.0 49.0 73.8 61.3 53.7 52.1 54.8 37.0 55.5 74.0 61.4 54.9 52.2 54.6 37.4 55. 88.0 68.1 57.9 70.1 67.5 73.6 70.8 95.3 87.6 86.5 88.6 85.5 84.4 88.0 CLIP [42] is powerful vision-language model, and it has strong zero-shot generalization ability. Following previous works, we use two classes of text prompt templates, photo of normal [cls] and photo of an anomalous [cls], where cls denotes the target class name. The anomaly score is computed by cosine similarity between textual features and the class token of query image. For anomaly segmentation, we extend the above computation from class tokens to local patch tokens. WinCLIP [26] is zero-shot anomaly segmentation method based on CLIP. large set of handcrafted textual prompts is designed for anomaly classification. window scaling strategy is used to obtain better anomaly segmentation. We keep all parameters the same as in their paper. Note that no official implementation of WinCLIP is available, our results are based on an unofficial implementation 1. WinCLIP+ [26] combines the complementary prediction from both language-guided and visualbased for better anomaly classification and segmentation. The language-guided prediction is the same as WinCLIP. For visual-based prediction, it first simply stores multi-scale features for given few-shot normal images and retrieves the memory features based on the cosine similarity. The final anomaly score is derived by averaging these two scores. AnomalyCLIP [76] learns object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. But AnomalyCLIP requires fine-tuning on an auxiliary domain dataset including normal and anomaly images. AnomalyCLIP is zero-shot anomaly classification and segmentation method, and it is capable of recognizing any anomalies. We use the official model to report performance for anomaly classification and segmentation. UniAD [70] is unified unsupervised anomaly segmentation method for addressing multi-classes anomalies with single model. Different from most zero-/few-shot anomaly segmentation models, UniAD learns feature reconstruction with transformer-based encoder-decoder architecture on all normal training images. We use the official code to train the specific model for each dataset. PatchCore [47] is popular unsupervised anomaly classification method that enjoys training-free. For fair comparison, we modify the official implementation in two folds. First, we replace the original WideResNet-50 backbone with EfficientNet-b4. Second, the memory-bank construction is limited to only one normal image for each class. 1https://github.com/zqhang/Accurate-WinCLIP-pytorch"
        }
    ],
    "affiliations": [
        "Tencent YouTu Lab, Shenzhen, China"
    ]
}