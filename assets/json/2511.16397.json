{
    "paper_title": "AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser",
    "authors": [
        "Ren Ma",
        "Jiantao Qiu",
        "Chao Xu",
        "Pei Chu",
        "Kaiwen Liu",
        "Pengli Ren",
        "Yuan Qu",
        "Jiahui Peng",
        "Linfeng Hou",
        "Mengjie Liu",
        "Lindong Lu",
        "Wenchang Ning",
        "Jia Yu",
        "Rui Min",
        "Jin Shi",
        "Haojiong Chen",
        "Peng Zhang",
        "Wenjian Zhang",
        "Qian Jiang",
        "Zengjie Hu",
        "Guoqiang Yang",
        "Zhenxiang Li",
        "Fukai Shang",
        "Runyuan Ma",
        "Chenlin Su",
        "Zhongying Tu",
        "Wentao Zhang",
        "Dahua Lin",
        "Conghui He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\\% ROUGE-N F1 compared to Trafilatura's 63.6\\%, with exceptional structured element preservation (90.9\\% for code blocks, 94.0\\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction."
        },
        {
            "title": "Start",
            "content": "AICC: Parse HTML Finer, Make Models Better 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Ren Ma, Jiantao Qiu , Chao Xu , Pei Chu, Kaiwen Liu, Pengli Ren, Yuan Qu, Jiahui Peng, Linfeng Hou, Mengjie Liu, Lindong Lu, Wenchang Ning, Jia Yu, Rui Min, Jin Shi, Haojiong Chen, Peng Zhang, Wenjian Zhang, Qian Jiang, Zengjie Hu, Guoqiang Yang, Zhenxiang Li, Fukai Shang, Runyuan Ma, Chenlin Su, Zhongying Tu, Wentao Zhang, Dahua Lin, Conghui He (cid:66) Shanghai Artificial Intelligence Laboratory While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication, treating HTML-to-text extraction as fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, code blocks, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, novel extraction pipeline that reformulates content extraction as sequence labeling problem solved by 0.6B-parameter language model. Unlike textdensity heuristics, MinerU-HTML leverages semantic understanding and employs twostage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On WebMainBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.82% ROUGE-N F1 compared to Trafilaturas 63.58%, with exceptional structured element preservation (90.93% for code blocks, 93.99% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.82% average accuracy across 13 benchmarks, outperforming TfCC by 1.08ppproviding direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release WebMainBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is critical, often underestimated component of web corpus construction. Website: https://opendatalab.com/ai-ready/AICC Correspondence: Conghui He, heconghui@pjlab.org.cn * Contributed equally Project leader 5 2 0 2 6 2 ] . [ 2 7 9 3 6 1 . 1 1 5 2 : r AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser"
        },
        {
            "title": "1 Introduction",
            "content": "The remarkable capabilities of modern large language models (LLMs) are built upon massive-scale pretraining on diverse text corpora [5, 28, 29, 10]. As models scale to hundreds of billions of parameters and training extends to trillions of tokens, the quality and quantity of pretraining data have become critical determinants of model performance [14]. Among the various data sources available, web text has emerged as the dominant component due to its unparalleled scale and diversity. Common Crawl [8], continuously updated public repository of web snapshots containing petabytes of HTML documents, has become the de facto foundation for constructing large-scale pretraining corpora. However, transforming raw Common Crawl data into effective training material is far from trivial. Recent efforts have demonstrated substantial improvements in downstream model performance through sophisticated data curation strategies. RefinedWeb [18] showed that extensively filtered and deduplicated web data alone can outperform curated corpora that mix web text with books and technical documents. FineWeb [19] introduced careful ablation studies of filtering and deduplication strategies, producing 15-trillion token corpus that yields better-performing models than other public web datasets. DCLM [15] demonstrated that model-based quality filtering can dramatically improve benchmark performance, while Nemotron-CC [25] explored the trade-off between aggressive filtering and maintaining sufficient data quantity for long-horizon training. Despite this progress in data curation, critical component of the pipeline has received comparatively little attention: HTML-to-text extraction. Before any filtering or deduplication can be applied, raw HTML documents must first be converted to structured text formats. This extraction step faces significant challenges: HTML is primarily designed for rendering visual layouts, not for conveying semantic content. Web pages are riddled with navigation menus, advertisements, sidebars, footers, and other boilerplate elements that must be separated from the main content. Furthermore, structured elements such as mathematical formulas, code blocks, and tableswhich are crucial for technical and educational contentare often corrupted or lost entirely during extraction. Existing web corpora have largely relied on heuristic-based extraction tools: RefinedWeb and FineWeb use Trafilatura [1], while DCLM and Dolma [24] use Resiliparse [2]. Though DCLM found that both extractors significantly outperform Common Crawls WET files (pre-extracted text), improving MMLU scores by 2.5+ points, these tools remain fundamentally limited by their reliance on text density heuristics and hand-crafted DOM traversal rules. Such approaches struggle with non-standard layouts, fail to preserve document structure and coherence, and frequently corrupt structured elements. For instance, mathematical formulas rendered via MathJax are often reduced to raw LaTeX commands or stripped entirely; code blocks lose indentation and syntax highlighting markers; complex tables are flattened into unstructured text sequences. More critically, most recent work has treated extraction as fixed pre-processing step, focusing optimization efforts exclusively on downstream filtering and deduplication. We hypothesize that improving HTML extraction quality can be as impactful as aggressive filtering strategies, yet requires fundamentally different methods. In this work, we introduce AICC (AI-ready Common Crawl), large-scale pretraining corpus constructed by applying semantic-aware HTML extraction to Common Crawl. At the core of AICC is MinerU-HTML, novel two-stage extraction pipeline that reformulates content extraction as sequence labeling problem solved by compact 0.6B-parameter language model. Unlike heuristic methods, MinerU-HTML understands semantic context and preserves document structure, enabling high-fidelity extraction of main content while maintaining formulas, code blocks, and tables. Our comprehensive evaluation demonstrates that extraction quality significantly impacts downstream model capabilities: 1. Extraction Quality Matters: On WebMainBench, our newly constructed benchmark of 7,887 annotated web pages, MinerU-HTML achieves ROUGE-N F1 score of 0.8182, substantially outperform2 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser ing Trafilatura (0.6358). For structured element preservation, MinerU-HTML achieves 0.9093 edit similarity for code blocks and 0.9399 for formulas, compared to 0.1305 and 0.6107 for Trafilatura. 2. Corpus-Level Quality Improvement: direct comparison on 10,000 document pairs from Common Crawl, evaluated via an LLM-as-judge, shows AICC is preferred over its Trafilatura-based counterpart (TfCC) with 72.0% win rate. This confirms that MinerU-HTMLs extraction improvements on the benchmark generalize to the full corpus. 3. Extraction Impacts Downstream Performance: Models pretrained on AICC (62B tokens) achieve 50.82% average accuracy across 13 diverse benchmarks, outperforming models trained on TfCC (49.74%), RefinedWeb (49.13%), and even FineWeb (49.61%). The 1.08pp improvement over TfCCwhere both datasets undergo identical filtering and deduplication, differing only in extraction methodprovides direct evidence that extraction quality significantly affects model capabilities. 4. Structure Preservation Drives Gains: The improvements are consistent across all task categories, validating our hypothesis that preserving document structure and narrative coherence during extraction enhances the learning of contextual understanding and long-range dependencies. Notably, AICC significantly outperforms FineWeb on reading comprehension tasks (42.37% vs. 36.68%, +5.69pp). Our work makes the following contributions: (1) We introduce MinerU-HTML, semantic-aware HTML extraction pipeline that significantly outperforms existing heuristic methods on both content fidelity and structured element preservation. Critically, MinerU-HTMLs model-based approach offers inherent scalability advantages over rule-based extractors, as its performance can improve with more data and stronger base models (Figure 2). (2) We construct and publicly release WebMainBench, comprehensive benchmark for evaluating main content extraction and structured element preservation with 7,887 annotated pages. (3) We release AICC, large-scale pretraining corpus demonstrating that high-quality extraction can rival or exceed the benefits of aggressive filtering strategies for downstream model performance. (4) We provide extensive pretraining experiments isolating the contribution of extraction quality, shifting attention to this critical but often overlooked component of web data curation."
        },
        {
            "title": "2 MinerU-HTML: HTML Parsing for AI-ready CC",
            "content": "We present MinerU-HTML, novel two-stage pipeline for extracting high-quality content from raw HTML documents and converting it into AI-ready formats. In the first stage, MinerU-HTML extracts the main content from raw HTML to produce what we term Main-HTMLa cleaned subset of the original document that retains only content-bearing elements. In the second stage, we transform this Main-HTML into structured, AI-ready formats such as Markdown for downstream language model training."
        },
        {
            "title": "2.1 Main-HTML Extraction",
            "content": "MinerU-HTMLs Main-HTML extraction methodology is designed with scalability as central consideration. We begin by developing robust extraction pipeline for individual HTML documents that leverages compact language model to perform content classification with high accuracy and reliability. This single-document pipeline forms the foundation of MinerU-HTML. To scale this method to Common Crawls hundreds of billions of documents, we then introduce template-aware optimization strategy that exploits the structural regularity of web pages. By clustering pages generated from similar templates and distilling the language models decisions into reusable extraction rules, we achieve web-scale processing efficiency while preserving the quality of the core extraction pipeline. 3 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Figure 1: Overview of the MinerU-HTML Core Extraction Pipeline. The pipeline consists of three stages: (1) Pre-processing: Raw HTML is transformed into two parallel representationsSimplified HTML (streamlined input for the model with reduced tokens) and Mapping HTML (preserving original structure for faithful reconstruction). (2) Content Classification: MinerU-HTML-Classifier (0.6B parameter LM) performs sequential block classification on the simplified input, with custom logits processor implementing constrained decoding to ensure structured JSON output without hallucination. (3) Post-processing: Predicted labels (\"main\" or \"other\") are used to select corresponding blocks from the Mapping HTML, yielding the final Main-HTML as valid DOM subtree of the original document. We first describe the single-document extraction framework (2.1.1), then present the scaling strategy (2.1.2). 2.1.1 Core Extraction Pipeline MinerU-HTML transforms raw HTML documents into clean Main-HTML through three-stage pipeline, as illustrated in Figure 1. The architecture is designed to address two key challenges: (1) reducing the computational burden of processing lengthy HTML markup, and (2) ensuring that the extracted content remains faithful to the original document without hallucination. The pipeline operates as follows. In the pre-processing stage, the input HTML is partitioned into semantic blocks, generating two synchronized representations: Simplified HTML, which strips away rendering-oriented markup to create compact input for the language model, and Mapping HTML, which preserves the original block structure to enable faithful reconstruction. In the Content Classification stage, MinerU-HTML-Classifier, compact 0.6B-parameter language model, processes the Simplified HTML and classifies each block as either main content or boilerplate, with output constrained by custom logits processor to ensure structured formatting. Finally, in the post-processing stage, the predicted labels are projected back onto the Mapping HTML, non-content blocks are pruned, and the resulting Main-HTML is assembled as valid subtree of the original DOM. This architecture reformulates content extraction as sequence labeling problem, offering significant computational advantages over generative approaches. We detail each component below. 4 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Pre-processing and Post-processing The pre-processing stage serves as the foundation of our approach, addressing the fundamental incompatibility between HTMLs rendering-oriented structure and the input requirements of language models. Raw HTML documents contain extensive markup designed for visual presentation rather than semantic interpretation, resulting in prohibitively long sequences when directly fed to model. To overcome this limitation, our pre-processing module implements systematic transformation strategy that combines simplification and chunking operations to reduce sequence length while preserving semantic information. The pre-processing pipeline executes four sequential transformations. (1) Non-content tag removal: We eliminate elements such as <style>, <script>, <header>, and <aside> that rarely contribute to main content. (2) Attribute simplification: We retain only the class and id attributes while pruning all others, as these two attributes provide the most salient semantic indicators for content block differentiation. (3) Block-level chunking: The document is segmented at elements that induce line breaks during rendering. This approach preserves the structural integrity of cohesive units such as tables (<table>) and lists (<ul>) by treating them as atomic, indivisible blocks. To accommodate the widespread practice of using tables for page layout, we apply adaptive heuristics that permit selective splitting when structural analysis indicates layout rather than tabular data. (4) Partial content truncation: To manage excessively long individual blockssuch as tables with numerous cells, lists with extensive items, or verbose paragraphswe retain only representative subsets. For instance, we preserve subset of table cells or truncate paragraphs to their initial 200 characters. Our empirical validation demonstrates that such partial representations maintain sufficient information for accurate classification while substantially reducing input length. Following pre-processing, the document is transformed into sequence of simplified blocks, denoted as = [x1, x2, . . . , xn]. In parallel, the Mapping HTML is generated by applying only the block-level chunking to the original, unmodified HTML, ensuring that the final output constitutes valid DOM subtree. The post-processing module then leverages the models classification predictions to select content-bearing blocks from the Mapping HTML and assembles them into the final Main-HTML. Content Classification The pre-processing transformations described above enable us to reformulate content extraction as well-defined sequence labeling problem, analogous to tasks such as named entity recognition or part-of-speech tagging. This formulation provides both theoretical clarity and practical advantages for model training and inference. Formally, let an HTML document be represented as sequence of simplified blocks after preprocessing, denoted as = [x1, x2, . . . , xn]. Each block xi is associated with ground-truth binary label yi {0, 1}, where yi = 1 indicates that block xi belongs to the main content, and yi = 0 denotes boilerplate or auxiliary content. The objective is to train language model (i.e., MinerU-HTMLClassifier) fθ parameterized by θ that maps the input sequence to predicted label sequence: Ypred = fθ(X) = [y 1, 2, . . . , n], {0, 1} (1) For details of the MinerU-HTML-Classifier, please refer to Appendix A.1. The post-processing module subsequently utilizes Ypred to select the corresponding blocks from the Mapping HTML and construct the final Main-HTML output. This sequence labeling formulation offers several advantages. First, the pre-processing transformations substantially reduce the token load imposed on the model, enabling efficient processing of web documents. Second, by constraining the task to discrete block classification rather than free-form text generation, we limit the output to compact sequence of binary labels. This design inherently eliminates hallucination risks, as the model can only select from existing content rather than generate novel text, thereby guaranteeing that the extracted content constitutes faithful subset of the original document. 5 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser While the sequence labeling framework provides strong foundation, standard language model decoding could still produce malformed outputs or introduce spurious tokens. To ensure perfect adherence to the required output format, we employ constrained decoding mechanism that we describe next. Constrained Decoding To guarantee valid output formatting and completely eliminate the possibility of hallucination or format errors, we implement custom logits processor that operates as deterministic finite state machine (FSM) during decoding. The FSM enforces strict control over the generation of structured output, which follows JSON-like format (e.g., {\"1\": \"main\", \"2\": \"other\", ...}). The processor deterministically manages all syntactic tokensincluding braces, quotes, colons, and numeric keysby masking the logits at each decoding step to permit only valid continuations according to the current state. The model is granted probabilistic choice exclusively at the critical decision points of block classification, where the vocabulary is restricted to only two tokens: \"main\" and \"other\". This constraint effectively transforms the task into sequence of binary classification decisions, each made with high confidence over minimal vocabulary. By eliminating all degrees of freedom except for the semantic classification choices, this mechanism guarantees syntactically valid output and fundamentally precludes both format errors and hallucinated content. Scalability Advantages of Model-Based Extraction. Crucially, MinerU-HTMLs approachgrounded in the semantic understanding ability of MinerU-HTML-Classifier is inherently scalable: performance can improve with more training data and stronger base models (Figure 2). In contrast, purely heuristic methods like Trafilatura and Resiliparse offer limited improvement pathways, as rule updates designed to address specific failure modes often fail to generalize and may introduce conflicts with existing heuristics. This fundamental advantage makes MinerU-HTML more future-proof solution as language models continue to advance. 2.1.2 Scaling to Common Crawl The three-stage pipeline described in Section 2.1.1 provides an effective solution for extracting content from individual HTML documents. However, directly applying this approach to Common Crawlwhich contains hundreds of billions of documentswould require running GPU-based language model inference on each page, resulting in prohibitive computational costs. To achieve web-scale processing, we introduce template-aware optimization strategy that significantly reduces the number of pages requiring language model inference while maintaining extraction quality. Template-based generalization. Our approach exploits the structural regularity inherent in web content: most websites generate pages from shared templates, and pages instantiated from the same template exhibit nearly identical DOM structures. Rather than treating each page independently, we cluster structurally similar pages and apply MinerU-HTML-Classifier (Section 2.1.1) to only small representative subset. We then distill MinerU-HTML-Classifiers extraction decisions into explicit XPath-based rules and propagate these rules to all remaining pages in the cluster using efficient CPU-based processing. This template-based generalization strategy preserves the high-quality extraction provided by MinerUHTML-Classifier while dramatically reducing computational requirements. Specifically, we cluster pages at the subdomain level to maximize structural homogeneity, then execute the following procedure for each cluster: 1. Representative selection: For each cluster, we select representative page that maximizes structural coverage and diversity based on tag and attribute variety, DOM depth, and DOM width. AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Figure 2: Iterative improvement pathways for MinerU-HTML. MinerU-HTML follows virtuous cycle: the model-based extractor can be systematically improved by collecting more training data (including failure cases), retraining on expanded datasets, and leveraging advances in base model capabilities. This makes MinerU-HTMLs approach inherently more scalable and future-proof as language models continue to advance. 2. Full pipeline execution: We apply the complete three-stage pipeline (Section 2.1.1) to the representative page. MinerU-HTML-Classifier processes the Simplified HTML and produces block-level classification labels via constrained decoding, which are used to construct the MainHTML. 3. Rule synthesis: We analyze the models classification decisions by mapping the labeled blocks back to their positions in the original DOM structure. From these decisions, we derive generalized extraction rules expressed as XPath or CSS selectors combined with retain/prune operations. 4. Rule propagation: The synthesized rules are applied to all other pages within the cluster. Since rule application requires only DOM traversal and filtering, this step can be executed efficiently on CPUs without requiring GPU resources or language model inference. The clustering-based optimization delivers significant computational efficiency gains. In practice, typical subdomain cluster encompasses thousands of pages with similar structural patterns, yet requires language model inference for only single representative page. All remaining pages in the cluster are processed through efficient rule-based extraction methods. Our empirical analysis of Common Crawls 300 billion HTML documents reveals that this strategy produces approximately 1.2 billion distinct clusters, thereby requiring GPU inference for only 0.4% of total pages while preserving extraction quality equivalent to individual page processing."
        },
        {
            "title": "2.2 Document Formatting",
            "content": "The MinerU-HTML Main-HTML extraction pipeline described in the previous section produces cleaned HTML documents that contain only content-bearing elements. However, HTML remains markup language optimized for rendering rather than for training language models. To maximize the utility of the extracted content for downstream AI applications, we require format that is both semantically rich and compact. This section presents MinerU-HTMLs two-stage formatting pipeline 7 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser that transforms Main-HTML into AI-ready representations. We adopt two-stage conversion strategy with an intermediate representation. First, we parse Main-HTML into structured content lista JSON-based representation that explicitly categorizes each semantic unit by type (e.g., title, paragraph, code block, table, formula). This intermediate representation enables flexible filtering and format-specific rendering. Second, we convert the content list into the target format, with Markdown being our primary output for language model training due to its balance of expressiveness and simplicity. 2.2.1 HTML to Content List Conversion The first stage transforms Main-HTML into structured content list that explicitly represents the semantic type of each content element. Semantic type classification. Despite the diverse ways HTML elements can be combined, the semantic content of web pages can be systematically categorized into finite set of types: titles, text paragraphs, inline formulas, display formulas, inline code, code blocks, tables, lists, images, videos, and audio. We implement specialized detectors for each semantic type, processing the Main-HTML through these detectors sequentially to construct JSON-based content list. Each semantic type is encoded using dedicated JSON schema. An example of the content list is represented in Figure 7. The conversion process incorporates specialized handling for several element types that require sophisticated recognition: 1. Code blocks: We aggregate spatially proximate code-related HTML tags to form contiguous code blocks, then apply heuristic pattern matching to infer the programming language based on syntax characteristics. 2. Tables: We distinguish structurally simple tables (expressible in Markdown syntax) from complex tables containing row/column spans or nested structures. Simple tables are converted to Markdown format, while complex tables are preserved as HTML to maintain structural fidelity. Nested tables are linearized into sequential paragraphs. 3. Mathematical formulas: We employ hybrid extraction strategy combining renderer-agnostic parser with specialized tag-based extractors. The parser accommodates common rendering engines including MathJax and KaTeX. Extracted formulas are classified as inline (delimited by single dollar signs) or display mode (delimited by double dollar signs). Advantages of intermediate representation. The content list serves as versatile intermediate representation that provides three key benefits. First, it enables selective filtering by semantic type, allowing corpus curation based on specific content characteristics (e.g., isolating code-rich or formularich pages). Second, it facilitates format-specific pruning, where downstream conversion can omit particular element types (e.g., excluding images, audio, or video for language model training). Third, it provides unified representation across heterogeneous document sources, including PDFs, DOC files, PowerPoint presentations, EPUB books, and web pages, thereby enabling consistent processing pipelines across diverse input formats. 2.2.2 Content List to Markdown Conversion The second stage converts the structured content list into Markdown format. The conversion process iterates over the content list elements and applies type-specific rendering rules that map each semantic type to its corresponding Markdown syntax. The core conversion logic is illustrated with pseudocode in Appendix A.2. 8 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser"
        },
        {
            "title": "3 Evaluation of MinerU-HTML",
            "content": "To comprehensively evaluate MinerU-HTML, we construct WebMainBench, new benchmark designed to assess both main content extraction and structured element preservation. We compare MinerU-HTML against established baseline methods on WebMainBench and validate its generalization capabilities on the external WCEB benchmark."
        },
        {
            "title": "3.1 WebMainBench",
            "content": "3.1.1 Dataset Overview WebMainBench comprises 7,887 carefully annotated web pages, each containing: (1) the raw HTML document, (2) ground-truth Main-HTML identified by human annotators as valid DOM subtree, (3) the corresponding Markdown representation, and (4) rich metadata including language, content type, difficulty level, and structural characteristics. This comprehensive annotation enables fine-grained performance analysis across diverse web content types. An example data entry is shown in Appendix Figure 9. 3.1.2 Data Collection and Sampling We employ hybrid sampling strategy to ensure broad coverage of the web ecosystem. We randomly sample 90% of pages from Common Crawl to capture the long-tail distribution of web content, while the remaining 10% are drawn from high-traffic websites (Chinaz Alexa1) to include popular, professionally designed pages. 3.1.3 WebMainBench-Structured: Subset for Structured Element Extraction To rigorously evaluate structured element preservation during main content extraction, we curate WebMainBench-Structured, focused subset of 545 pages selected from WebMainBench that are rich in mathematical formulas, code blocks, and tableselements critical for technical and educational content but often corrupted by traditional extraction methods. In addition to standard annotations, each page includes groundtruth_content field containing ground-truth Markdown text with all structured elements faithfully preserved by human annotators, enabling precise element-level evaluation. Table 5 presents the element distribution: 257 pages (47.2%) contain formulas (predominantly inline, 198 instances), 127 pages (23.3%) contain code blocks (predominantly interline, 73 instances), and 179 pages (32.8%) contain tables (predominantly data tables, 151 instances). This distribution reflects realistic usage patterns in technical web content and provides sufficient samples for robust evaluation. 3.1.4 Annotation Principles Defining \"main content\" is inherently ambiguous, as it varies across different page types and use cases. We establish clear annotation guidelines based on two core principles: Contextual Integrity. We include content that is integral to understanding the primary article or page purpose. For example, abstracts, author information, and reference sections are included for academic papers, while related article sidebars and advertisements are excluded. Human-Generated Content. We prioritize substantive, human-authored material such as article bodies, user comments, and discussion threads, while excluding auto-generated metadata like timestamps, view counts, and algorithmic recommendations. 1https://malexa.chinaz.com/ 9 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Human annotators apply these principles using custom-built annotation tool (see Appendix Figure 16), ensuring consistent and high-quality ground truth. Additional details on the annotation process are provided in Appendix B.1."
        },
        {
            "title": "3.2 Evaluation Metrics and Baselines",
            "content": "3.2.1 Evaluation Metrics Main Content Extraction. We adopt ROUGE-N F1 (N=5) as our primary metric for evaluating overall main content extraction quality. ROUGE-N measures n-gram overlap between predicted and ground-truth content, providing robust assessment of semantic preservation. We employ the jieba tokenizer for consistent text segmentation. We choose ROUGE-N over ROUGE-L because the Longest Common Subsequence algorithm exhibits prohibitive O(n m) computational complexity on the long documents prevalent in our benchmark (average length: 2,304 tokens). Structured Element Extraction. To evaluate the fidelity of structured element preservation, we employ specialized metrics for each element type. For code blocks and mathematical formulas, we use normalized edit distance (Levenshtein distance): EditSim(s1, s2) = 1 EditDist(s1, s2) max(s1, s2) where s1 and s2 are the predicted and ground-truth element strings, and s1 and s2 represent corresponding string length. This metric captures character-level accuracy while normalizing for length variations. For tables, we compute the TEDS (Tree-Edit Distance based Similarity) score [32], which measures structural similarity by computing tree edit distance on the HTML DOM representation: TEDS(Ta, Tb) = 1 EditDist(Ta, Tb) max(Ta, Tb) where Ta and Tb are DOM trees of the predicted and ground-truth tables. TEDS is particularly effective for evaluating table structure preservation, as it penalizes row/column misalignment and cell boundary errors. 3.2.2 Baseline Methods We compare MinerU-HTML against two widely adopted extraction systems: Trafilatura [1], heuristic-based tool combining DOM analysis and text density heuristics used in RefinedWeb [18] and FineWeb [19], and Resiliparse [2], rule-based system optimized for large-scale web archiving used in DCLM [15] and Dolma [24]."
        },
        {
            "title": "3.3 Results on WebMainBench",
            "content": "3.3.1 Main Content Extraction Table 1 presents the performance comparison across different content types and difficulty levels. MinerU-HTML achieves an overall ROUGE-N F1 score of 0.8182, establishing state-of-the-art performance and significantly outperforming the best baseline method, Trafilatura (0.6358, Html+MD mode). MinerU-HTML demonstrates particularly strong performance on challenging content types where traditional heuristic methods struggle. On pages containing tables, MinerU-HTML achieves 0.7693 compared to Trafilaturas 0.5505. For mathematical equations, MinerU-HTML scores 0.8889 versus 0.7327. Most notably, on conversational content (forums, Q&A pages), MinerU-HTML achieves 0.7671 10 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Table 1: Mean ROUGE-N F1 on WebMainBench with different tracks name all simple mid hard table code equation conversational 0.6233 Resiliparse Trafilatura 0.6358 MinerU-HTML 0.8182 0.7099 0.7277 0.8837 0.6283 0.6391 0.8178 0.5304 0.5396 0. 0.5473 0.5505 0.7693 0.6474 0.6006 0.8368 0.7829 0.7327 0.8889 0.5346 0.5750 0.7671 compared to just 0.5750 for the best baseline, highlighting the advantage of our semantic, model-based approach over rigid heuristics. 3.3.2 Efficiency Analysis Our pre-processing pipeline achieves substantial token reduction, which is critical for efficient model inference. Compared to naive generative baseline that would directly process raw HTML (average: 44,706 input tokens, 2,304 output tokens), MinerU-HTMLs pre-processing reduces input to 5,735 tokens (12.83%) and output to 383 tokens (16.64%). This dramatic reduction makes language modelbased extraction computationally feasible and enables deployment of compact models at web scale. 3.3.3 Structured Element Extraction Table 2 presents comprehensive evaluation of structured element preservation on WebMainBenchStructured. MinerU-HTML demonstrates exceptional performance across all element types, significantly outperforming both baseline methods. Code Block Preservation. MinerU-HTML achieves an edit similarity score of 0.9093 for code blocks, substantially exceeding Trafilatura (0.1305) and Resiliparse (0.0641). This dramatic improvement indicates that MinerU-HTML successfully preserves code syntax, indentation, and special characterselements that are often corrupted by text-density-based heuristics. Qualitative analysis reveals systematic failure patterns in baseline methods: Trafilatura frequently strips indentation, converting properly formatted code into unstructured text (Figure 10), while Resiliparse often loses Markdown code block delimiters (the markers), rendering code indistinguishable from surrounding text. Figure 11 demonstrates representative case where both baselines fail to preserve code block structure, whereas MinerU-HTML maintains complete formatting integrity. Mathematical Formula Preservation. For mathematical formulas, MinerU-HTML achieves 0.9399 edit similarity, compared to 0.6107 (Trafilatura) and 0.6778 (Resiliparse). This high score reflects MinerU-HTMLs ability to correctly identify and preserve LaTeX expressions, MathML markup, and inline mathematical notation across diverse rendering engines. The baseline methods exhibit two primary failure modes: Trafilatura often fails to recognize mathematical content entirely, discarding formulas during extraction (Figures 12 and 13), while Resiliparse extracts the textual components but loses critical formatting delimiters and structural markers, rendering the mathematical notation unparseable by downstream processors. Table Structure Preservation. For table extraction, we evaluate structural preservation using the TEDS metric. MinerU-HTML achieves 0.7388, significantly outperforming Trafilatura (0.3405) and Resiliparse (0.0227). While this score is lower compared to code and formula extraction, this reflects the inherent complexity of table structure preservation rather than fundamental limitation of our approach. Complex table layouts with merged cells, nested structures, and inconsistent markup remain challenging even for advanced extraction methods. Nevertheless, MinerU-HTMLs 2.2 improvement over Trafilatura and 32.5 improvement over Resiliparse demonstrates substantial progress. As 11 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Table 2: Structured Element Extraction Performance on WebMainBench-Structured. Edit similarity scores (higher is better) for code blocks and formulas. Table TEDS measures structural preservation for tables. Extractor Code Edit Formula Edit Table TEDS 0.1305 Trafilatura Resiliparse 0.0641 MinerU-HTML 0.9093 0.6107 0.6778 0.9399 0.3405 0.0227 0.7388 Table 3: Results on WCEB name all simple mid hard 0.7833 Trafilatura Resiliparse 0.7225 MinerU-HTML 0.8002 0.8122 0.7697 0.8293 0.7785 0.7052 0.8005 0.7609 0.6985 0. illustrated in Figures 14 and 15, baseline methods consistently fail to recognize table boundaries and cell relationships, often linearizing tabular data into unstructured paragraphs, while MinerU-HTML preserves row-column structure and cell alignment. Implications. These results validate that MinerU-HTMLs Main-HTML extraction pipeline, combining semantic understanding with structural awareness, effectively preserves complex content structures that are crucial for downstream applications such as language model pretraining, technical documentation archiving, and knowledge base construction. The preservation of structured elements is particularly important for training models on technical and scientific content, where code, formulas, and tables convey information that cannot be adequately represented in plain text."
        },
        {
            "title": "3.4 Generalization to WCEB",
            "content": "To assess MinerU-HTMLs generalization capabilities beyond our benchmark, we evaluate on the Web Content Extraction Benchmark (WCEB) [3], unified collection of nine established datasets. WCEB provides cleaned and standardized ground truths, addressing inconsistencies prevalent in legacy datasets such as encoding errors and script injection artifacts. detailed description of the benchmark can be found in Table 4. Since WCEB uses plain-text ground truths, we convert the extracted MAIN-HTML by MinerU-HTML to plain-text using the html-text library2. Table 3 shows that MinerU-HTML maintains strong performance on WCEB, achieving an overall score of 0.8002 and outperforming the strongest baseline, Trafilatura (0.7833). The strong performance across both WebMainBench and WCEB validates MinerU-HTMLs robustness and generalization capabilities across diverse web content."
        },
        {
            "title": "4 Dataset Construction and Pretraining Experiments",
            "content": "To validate the effectiveness of MinerU-HTML for downstream language model training, we construct AICC from Common Crawl and conduct comprehensive pretraining experiments comparing it against strong baselines. This section describes our dataset construction pipeline, experimental setup, and downstream evaluation results. 2https://pypi.org/project/html-text/ 12 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Figure 3: Length ratio distribution between AICC and TfCC documents. Positive values indicate AICC extracts more content."
        },
        {
            "title": "4.1 Dataset Construction",
            "content": "4.1.1 Extraction and Initial Processing We apply MinerU-HTML to extract content from two Common Crawl snapshots (CC-2023-06 and CC-2023-14)3, converting raw HTML WARC archives into Markdown format. This extraction process yields AICC, large-scale multilingual web corpus optimized for language model training. For rigorous baseline comparison, we also construct TfCC by applying Trafilaturaa widely-adopted extraction toolto the identical two snapshots under the same processing pipeline. To quantify extraction quality differences, we sample 800k document pairs and compute length ratios: Length Ratio = LenAICC LenTfCC max(LenAICC, LenTfCC) where LenAICC and LenTfCC represent the length of extracted content from AICC and TfCC documents, respectively. The distribution (Figure 3) reveals that AICC documents are 1.16 longer on average, indicating that MinerU-HTML preserves more content. 4.1.2 Extraction Quality Analysis We evaluate how extraction length relates to perceived quality using an LLM-as-a-judge protocol [9] with DeepSeek-Chat-V3, see the prompt in Figure 22. We draw stratified sample of 10,000 AICC 3To enable direct comparison with existing datasets such as RefinedWeb and FineWeb, we use CC-2023-06 and CC-2023-14 snapshots in this paper. The released version of AICC is based on more recent snapshots (CC-2025-08 and CC-2025-13) to provide the community with the most up-to-date data. 13 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Figure 4: Extraction quality vs. length ratio. Pairwise win rates (AICC vs. TfCC) judged by DeepSeekChat-V3 on 10,000 stratified samples. sharp crossover at ratio = 0 (red dashed line) reveals that when AICC extracts more content (positive ratios), it is preferred in 7598% of comparisons; when it extracts less (negative ratios), TfCC is preferred in 5192% of cases. vs. TfCC document pairs uniformly across 20 length-ratio bins (from 1.0 to 1.0; 500 per bin) to ensure balanced coverage across the full spectrum of length differences. Figure 4 shows sharp crossover at length ratio 0. When AICC extracts less than TfCC (negative ratios), TfCC is preferred with win rates of 5192%. When AICC extracts more (positive ratios), the preference reverses: AICC wins 7598%, with preference increasing monotonically with the ratio. Weighting the bin-wise win rates by the population distribution in Figure 3, AICC achieves 72.0% overall win rate. This indicates that the additional content preserved by MinerU-HTML is overwhelmingly judged as valuable main content rather than noise, reflecting both higher recall and maintained precision relative to heuristic extraction. To understand failure modes, we manually examine cases from the extreme and near-zero bins, with examples provided in Appendix F: 1. [0.9, 1.0]: AICC longer than TfCC: Most AICC documents are preferred. For example, Figure 17 shows case where Trafilatura drops much of the main body content. 2. [-0.1, 0.1]: Similar length: Even at similar lengths, AICC can be superior. In one case (Figure 20), Trafilatura mishandles list items; in another (Figure 21), it misses important elements such as the title and author on dissertation page. 3. [-1.0, -0.9]: TfCC longer than AICC: TfCC often wins because AICC misses portions of the main content (see Figure 19). However, there are also instances where Trafilatura retains substantial boilerplate (e.g., advertisements) that MinerU-HTML correctly excludes (see Figure 18). While AICC does exhibit failure cases, the overall quality advantage over TfCC remains robust, as evidenced by the 72.0% win rate and the consistent performance gains observed in downstream pretraining experiments (Section 4.3). 14 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser 4.1.3 Quality Filtering and Deduplication To prepare both datasets for pretraining, we apply identical post-processing pipelines following established practices from RefinedWeb [18]: (1) exact deduplication using sha256 (2) language identification and filtering using FastText, (3) quality filtering using Gopher heuristic rules [20], (4) safety filtering with URL and keyword blacklists, and (5) fuzzy deduplication using MinHash with Locality-Sensitive Hashing. This pipeline reduces the corpora to 372B tokens (AICC) and 317B tokens (TfCC). The identical filtering ensures that any downstream performance differences can be attributed solely to extraction quality rather than filtering strategies."
        },
        {
            "title": "4.2 Pretraining Setup",
            "content": "Training. The models utilized in this work are 1.5B parameter decoder-only transformers derived from the Qwen3 model family. For the standard pretraining configuration, each model is trained from scratch on selected subset of 62B tokens. comprehensive description of the architecture and training setup is available in Appendix C. Evaluation Benchmarks. We evaluate pretrained models through in-context learning using the lm-evaluation-harness framework [11], reporting accuracy as the primary metric. The evaluation suite spans three task categories with 13 benchmarks: (1) General KnowledgeARC [7], BoolQ [6], CommonsenseQA [26], MMLU [13], and SciQ [30]; (2) ReasoningCOPA [12], HellaSwag [31], PIQA [4], SIQA [23], and WinoGrande [22]; (3) Reading ComprehensionCoQA [21], LAMBADA [17], and OpenBookQA [16]. Further evaluation details are provided in Appendix D. Baseline Datasets. We compare AICC against three strong baselines: 1. TfCC: Applies Trafilatura extraction to the same CC snapshots with identical post-processing (Section 4.1). This controlled comparison isolates extraction qualitys impact by eliminating confounding factors from filtering strategies. 2. RefinedWeb [18]: widely-adopted web corpus from Technology Innovation Institute using aggressive filtering and deduplication on Common Crawl. RefinedWeb has demonstrated performance comparable to curated corpora like C4 and serves as strong heuristic-based baseline. 3. FineWeb [19]: state-of-the-art 15-trillion token corpus from HuggingFace with optimized filtering and deduplication pipelines. FineWeb represents the current best practice for heuristic-based web corpus construction and outperforms RefinedWeb, C4, and Dolma. We exclude datasets employing model-based quality filtering (e.g., DCLM-Baseline [15]), as this introduces confounding factors that obscure extraction qualitys specific contribution."
        },
        {
            "title": "4.3 Downstream Evaluation Results",
            "content": "We compare models pretrained on AICC (MinerU-HTML-extracted) against TfCC (Trafilatura-extracted), RefinedWeb, and FineWeb across 13 benchmarks. Figure 5 and Table 7 present training dynamics at 15 checkpoints (4B63B tokens), while Figure 6 and Table 8 provide detailed task category comparisons between all four corpora at the final checkpoint. Overall Performance and Training Dynamics. At the final checkpoint (63B tokens), models trained on AICC achieve 50.82% average accuracy, outperforming FineWeb (49.61%), RefinedWeb (49.13%), and TfCC (49.74%). The 1.08pp improvement over TfCC provides direct evidence that extraction quality significantly impacts downstream capabilities, since both datasets differ only in extraction 15 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Figure 5: Training dynamics across 13 benchmarks for models pretrained on AICC and TfCC. Average accuracy at 15 checkpoints (4B63B tokens). AICC consistently maintains superior or competitive performance throughout training. method while undergoing identical post-processing. Notably, AICC surpasses even FineWeba stateof-the-art corpus with sophisticated quality filteringdemonstrating that semantic-aware extraction can rival aggressive filtering strategies. Figure 5 and Table 7 show this advantage persists throughout training: from the earliest checkpoint (4B tokens) to the final checkpoint, AICC consistently maintains superior or competitive performance, indicating stable quality improvements across the full training trajectory. Task Category Analysis. Table 8 and Figure 6 provide comprehensive comparison across all four datasets. The AICC vs. TfCC comparison is particularly informative for isolating extraction qualitys impact, revealing consistent improvements across task categories: General Knowledge improves by 1.93pp (47.54% vs. 45.61%), Reasoning by 0.49pp (59.83% vs. 59.34%), and Reading Comprehension by 0.35pp (42.37% vs. 42.02%). Notably, AICC achieves the best performance among all methods on General Knowledge tasks, and competitive performance on Reading Comprehension. Compared to the external baselines, AICC outperforms FineWeb on General Knowledge (47.54% vs. 46.86%) and Reading Comprehension (42.37% vs. 36.68%), while FineWeb shows slightly better performance on Reasoning tasks (60.69% vs. 59.83%). These results validate that extraction quality significantly impacts model capabilities, with semantic-aware extraction complementing aggressive filtering strategies."
        },
        {
            "title": "5 Conclusion",
            "content": "This work demonstrates that HTML-to-text extraction is critical but under-studied component of web corpus construction. Our controlled experiments show that extraction quality alone yields 16 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Figure 6: Task category breakdown comparing AICC, TfCC, RefinedWeb, and FineWeb at 63B tokens. AICC achieves the best performance on General Knowledge and Reading Comprehension tasks, significantly outperforming FineWeb on Reading Comprehension (+5.69pp). General Knowl. = General Knowledge, Reading Compr. = Reading Comprehension. performance gains comparable to sophisticated filtering strategies: models trained on AICC achieve 1.08pp improvement over TfCC (both with identical post-processing) and outperform RefinedWeb and FineWeb across multiple benchmarks. Large-scale validation on our 4.52-trillion token corpus confirms that AICC is preferred over TfCC in 72.0% of 10,000 document comparisons, with MinerU-HTML preserving 1.16 more content judged as valuable main content rather than noise. key insight is that model-based extraction offers inherent scalability advantages: while heuristic extractors face fundamental limitations where rule updates often conflict, MinerU-HTMLs performance can systematically improve with better data and models. This positions extraction quality as continuously improvable component rather than fixed constraint. Future work should explore: (1) JavaScript rendering for single-page applications; (2) learned clustering methods for improved template detection; (3) larger-scale pretraining validation (10B+ parameters); (4) multi-modal extraction; and (5) integration with model-based quality filtering. We release MinerU-HTML, WebMainBench, and AICC to facilitate further research into semantic-aware extraction methods and their impact on pretraining data quality. 17 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser References [1] Adrien Barbaresi. Trafilatura: web scraping library and command-line tool for text discovery and extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 122131, 2021. [2] Janek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury, Gabriella Pasi, and Benjamin Piwowarski, editors, Advances in Information Retrieval. 40th European Conference on IR Research (ECIR 2018), Lecture Notes in Computer Science, Berlin Heidelberg New York, March 2018. Springer. [3] Janek Bevendorff, Sanket Gupta, Johannes Kiesel, and Benno Stein. An Empirical Comparison of Web Content Extraction Algorithms. In 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2023). ACM, 2023. doi: 10.1145/3539618.3591920. URL https://dl.acm.org/ doi/10.1145/3539618.3591920. [4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [6] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29242936, 2019. [7] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [8] Common Crawl Foundation. Common crawl: Open-source web crawl data & infrastructure. https: //commoncrawl.org/. [9] Dingo Contributors. Dingo: comprehensive ai data quality evaluation tool for large models. https: //github.com/MigoXLab/dingo, 2024. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [11] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836. [12] Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. Semeval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In * SEM 2012: The First Joint Conference on Lexical and Computational SemanticsVolume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 394398, 2012. [13] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ. [14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. 18 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser [15] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. DataComp-LM: In search of the next generation of training sets for language models, 2024. [16] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260. [17] Paperno, Kruszewski, Lazaridou, QN Pham, Raffaella Bernardi, Pezzelle, Baroni, Boleda, and Fernández. The lambada dataset: Word prediction requiring broad discourse context. In 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016-Long Papers, volume 3, pages 15251534. Association for Computational Linguistics (ACL), 2016. [18] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. [19] Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024. [20] Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. [21] Siva Reddy, Danqi Chen, and Christopher Manning. Coqa: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. [22] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 87328740, 2020. [23] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. [24] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. [25] Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint, 2024. [26] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421. [27] Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 19 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser [29] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [30] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 94106, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4413. URL https://aclanthology. org/W17-4413. [31] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/ P19-1472. [32] Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. Image-based table recognition: data, model, and evaluation. In European conference on computer vision, pages 564580. Springer, 2020. 20 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser"
        },
        {
            "title": "Appendix",
            "content": "A MinerU-HTML Details A.1 MinerU-HTML-Classifier A.1.1 Training Data Construction To train MinerU-HTML, we construct large-scale training dataset designed to capture the diversity of the modern web. The dataset is curated through three-stage pipeline that ensures variety in page layout, language, and document format. First, we sample structurally diverse pages from Common Crawl by clustering pages based on DOM tree features using DBSCAN, yielding approximately 40 million layout-distinct candidates. Second, we apply language identification and format classification to obtain balanced 1 million page subset covering multiple languages and web formats. Third, we generate block-level annotations by processing pages through MinerU-HTMLs simplification algorithm and using large language model to label each block. After filtering, we obtain our final training dataset of 870,945 annotated samples. A.1.2 Training MinerU-HTML-Classifier We employ the Qwen3-0.6B [27] model as our base model, which is the smallest model in the Qwen3 series, featuring 32K context window and support for over 100 languages. Supervised fine-tuning is performed on the full set of 870K samples for fixed total of 4 epochs. A.2 Document Formatting Please see the conversion from content list to Markdown in Figure 8. [ // content list start { // node 1 \" type \": \" title \", \" bbox \": null , \" raw_content \": null , \" content \": { \" title_content \": \" Future Trends of Large Models \" , \" level \": 1 } }, ] 1 3 4 5 6 7 9 10 11 12 Figure 7: One title element of the content list. AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser 1 markdown = [] 2 for page in content_list : for element in page : if element [ ' type '] == ' title ': title_md = '# ' * element [ ' content '][ ' level '] + element [ ' content '][ ' title_content '] markdown . append ( title_md ) elif element [ ' type '] == ' code ': # render code block ... 3 4 6 7 8 9 10 11 markdown_content = 'nn '. join ( markdown ) Figure 8: Pseudocode of the content list to Markdown conversion. 1 { 2 3 4 5 7 8 9 10 11 \" track_id \": \" XXXX \", \" html \": \" <html >< body >< h1 cc - select = True > Hello world ! </ h1 > < aside > advertisement </ aside > </ body > </ html >\", \" main_html \": \" <html >< body ><h1 > Hello world ! </ h1 > </ body > </ html >\" , \" convert_main_content \": \"# Hello world !\", \" meta \": { \" language \": \" en \", \" style \": \" Normal \", \" level \": \" easy \", \" table \": \" without \", \" code \": \" without \", \" equation \": \" without \" } 13 14 } Figure 9: An example data from WebMainBench. It includes the raw source, the ground-truth MainHTML, its Markdown conversion, and rich set of metadata for fine-grained analysis. 22 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Evaluation of MinerU-HTML B.1 Benchmark Construction Data Sampling. WebMainBench is constructed using hybrid sampling strategy to ensure both broad representation and relevance. 90% of the samples are randomly drawn from the Common Crawl dataset to cover the long-tail web, while the remaining 10% are sampled from list of top-ranking websites (Chinaz Alexa4) to include popular, professionally designed pages. The final benchmark is highly diverse, containing pages from 5,434 unique top-level and 5,904 unique second-level domains. Annotation Rules. To address the ambiguity in defining \"main content\" for unconventional layouts, we establish two core annotation principles. First, Contextual Integrity dictates that content integral to the main articlesuch as table of contents, abstract, or reference listis included. Conversely, contextually independent elements like \"related articles\" sidebars or copyright footers are excluded. Second, the main content is defined as Human-Generated Content, including article bodies, user comments, and Q&A posts, while associated auto-generated metadata like usernames and timestamps are excluded. Annotation Process. The annotation for each page followed rigorous three-stage process using custom-built tool (see Appendix Figure 16) that allowed for tag-level granularity. The process involved: (1) an initial pass by one annotator, (2) review and correction pass by second annotator, and (3) final quality assurance check by senior inspector, who made the final adjudication to resolve any discrepancies. Pages uninterpretable due to rendering issues were discarded. Metadata Annotation. To enable detailed, fine-grained analysis, we annotate each page with rich set of metadata. This includes Language, identified by GPT-5 and labeled as en (English) or non_en (other), and Style, classified by GPT-5 as Conversational for pages with user-generated content or Normal otherwise. We also develop quantitative Difficulty Level, determined by an overall_complexity_score calculated for each page. To compute this score, we first measure four distinct metrics: DOM structural complexity (based on tree depth and width), text distribution sparsity (transitions between text/non-text nodes), content-type diversity (a count of rich content types), and link density (the ratio of hyperlinked text). These four values are individually normalized, and their weighted sum produces the final score. Based on the distribution of this overall_complexity_score across the benchmark, we then categorize pages into simple, medium, and hard using the 30th and 70th percentiles as dynamic thresholds. Finally, we add Rich Content Tags to identify the presence of tables (<table>), code blocks (<code>), and mathematical formulas (<math> or LaTeX patterns) using BeautifulSoup. 4https://malexa.chinaz.com/ 23 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Table 4: Details of Web Content Extraction Datasets Dataset Pages Source & Characteristics CleanEval 738 De-facto standard dataset from 2007 shared task combining development and evaluation sets of English web pages with basic structural markup ground truth CleanPortalEval 71 Extension of CleanEval featuring multi-page samples from 4 major news domains CETD Dragnet 700 Created for density-based extractor evaluation across 6 domains 1,379 Combined sources from popular RSS feeds, 23 major news sites, 178 Technorati blogs, plus CETR and CleanEval conversions L3S-GN1 621 Created by BoilerPipe authors with unique HTML annotation using span-wrapped CSS classes for 5-level content relevance GoogleTrends-2017 180 Dataset created for BoilerNet neural network training featuring binary CSS class annotations on DOM leaf nodes to distinguish content from boilerplate Readability 115 Mozilla reader mode test suite with original and simplified HTML for evaluation Scrapinghub 181 Created by Zyte for benchmarking proprietary extraction services Table 5: Distribution of Structured Elements in WebMainBench-Structured (N=545 pages). Element counts and subtype breakdowns for formulas, code blocks, and tables. Element Type Count SubType SubType Count Formula 257 Code Table 127 179 Inline Interline Inline + Interline Inline Interline Inline + Interline Data Layout Data + Layout 198 9 30 73 24 151"
        },
        {
            "title": "C Pretraining Details",
            "content": "The architecture of pretrained models in this work is as follows: Number of Layers: 24, Hidden Size: 2048, Intermediate Size: 5504, Number of Attention Heads: 16, Number of KV Heads: 16, Head Dimension=128, RoPE Theta: 1000000.0, Number of Total Parameters: 1,525,516,288, The pretraining step is as follows: Global Batch Size: 64, Context Length: 4,096, Training Steps: 240,000, Consumed Tokens: 62B. We employ the Qwen3 tokenizer [27] with vocabulary size of 151,936. The learning rate is set to 1 104, and the AdamW optimizer is employed with hyperparameters (Weight_decay = 0.01, β1 = 0.9, β2 = 0.95, ϵ = 108). 24 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser"
        },
        {
            "title": "D Evaluation Details",
            "content": "The number of randomly selected demonstrations for few-shot in-context learning for each task is listed in Table 6. Task ARC BoolQ CSQA MMLU SciQ COPA HellaSwag PIQA SIQA WinoGrande CoQA LAMBADA OpenbookQA # Shots 10 10 5 5 0 10 5 0 0 0 Table 6: Number of demonstrations in in-context learning (few-shot) used for each downstream task. CSQA = CommonsenseQA. The full performance results are shown in Table 7 and Table 8. Table 7: Average accuracy (%) across all 13 benchmarks during training. Models trained on AICC and TfCC are evaluated at 15 checkpoints from 4B to 63B tokens. AICC consistently maintains superior performance throughout training. Method 4.19 8. 12.58 16.78 20.97 25.17 29.36 33. 37.75 41.94 46.14 50.33 54.53 58. 62.91 AICC TfCC 41.12 40.68 44.47 43.03 45.25 44.62 47.21 45. 47.79 46.75 47.85 47.72 48.18 47.85 48.55 48.49 49.21 48.51 49.36 49. 49.43 49.19 50.43 49.49 49.61 49.65 50.64 49.82 50.82 49.74 Table 8: Performance breakdown by task category at the final checkpoint (63B tokens). Accuracy scores (%) comparing AICC, TfCC, FineWeb, and RefinedWeb across three task categories. AICC achieves the best performance on General Knowledge and Reading Comprehension tasks, while FineWeb performs best on Reasoning tasks. General Knowl. = General Knowledge, Reading Compr. = Reading Comprehension. Task Category AICC TfCC FineWeb RefinedWeb General Knowl. Reasoning Reading Compr. 47.54 59.83 42.37 45.61 59.34 42.02 46.86 60.69 36. 44.57 59.43 41.10 25 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser"
        },
        {
            "title": "E Structured Element Extraction Case Study",
            "content": "Figure 10: Code block extraction comparison. MinerU-HTML preserves proper indentation and code structure, while Trafilatura strips leading whitespace (converting formatted code into unstructured text) and Resiliparse fails to maintain code block delimiters. Figure 11: Code block extraction comparison. Both Trafilatura and Resiliparse fail to preserve Markdown code block formatting ( delimiters), rendering code indistinguishable from surrounding text, while MinerU-HTML maintains complete structural integrity. 26 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Figure 12: Mathematical formula extraction comparison. Trafilatura fails to recognize and extract the mathematical formulas entirely, discarding them during content extraction. Resiliparse extracts partial content but loses critical LaTeX delimiters and formatting markers. MinerU-HTML correctly preserves the complete formula structure. Figure 13: Mathematical formula extraction comparison. Trafilatura misses mathematical notation during extraction, while Resiliparse loses formatting delimiters necessary for proper rendering. MinerU-HTML successfully preserves both inline and display-mode formulas with correct LaTeX syntax. 27 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Figure 14: Table structure extraction comparison. Trafilatura and Resiliparse fail to recognize table boundaries and cell relationships, linearizing tabular data into unstructured text. MinerU-HTML preserves the table structure, maintaining row-column organization and cell alignment. Figure 15: Table structure extraction comparison. Both baseline methods collapse the table structure, losing critical semantic relationships between data cells. MinerU-HTML maintains structural fidelity, preserving headers, rows, and cell boundaries necessary for downstream table understanding. 28 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Figure 16: Screenshot of the web page annotation tool. The main content selection is highlighted in blue on the left, with real-time preview on the right. 29 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser AICC-TfCC pairwise document comparison Figure 17: AICC longer than TfCC. The AICC document is preferred over the TfCC document. Figure 18: TfCC longer than AICC. The AICC document is preferred over the TfCC document. 30 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Figure 19: TfCC longer than AICC. The TfCC document is preferred over the AICC document. Figure 20: Similar length. The AICC document is preferred over the TfCC document. 31 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser Figure 21: Similar length. The AICC document is preferred over the TfCC document. 32 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser 1 You are professional HTML content extraction evaluator , skilled in analyzing 2 the conversion quality between HTML code and Markdown text . will provide three 3 pieces of content : 4 5 1. ** Original HTML Code **: The complete HTML structure of the webpage . 6 2. ** Tool 's Extracted Markdown **: Markdown text extracted from HTML , suitable 7 8 3. ** Tool 's Extracted Markdown **: Markdown text extracted from HTML , suitable 9 for LLM training . for LLM training . 10 11 Note : The order of Tool and Tool is not fixed . Do not favor either tool 12 based on order ; evaluate objectively based on actual conversion quality . 13 14 Your Task : 15 1. Compare both Markdown extractions against the HTML code . Strictly check 16 extraction effectiveness for the following 8 module types : 17 18 ** HTML Element Identification :** 19 - `code `: Code blocks (< pre >, <code > tags ) 20 - `math `: Mathematical formulas ( MathJax , MathML , LaTeX ) 21 - `table `: Tables (< table > tags ) 22 - `image `: Images (< img > tags ) 23 - `list `: Ordered / unordered lists (<ul >, <ol > tags ) 24 - `title `: Headings (<h1 >-<h6 > tags ) 25 - ` paragraph `: Paragraph text (<p >, <div > containers ) 26 - `other `: Other visible content not covered above 27 28 ** Markdown Element Statistics :** 29 - Code blocks : ```...``` or indented code 30 - Formulas : $ ... $ $$ ... $$ (...) [...] 31 - Tables : ... format 32 - Images : ![](...) format 33 - Lists : -, *, 1. markers 34 - Headings : #, ## markers 35 - Paragraphs : Plain text blocks 36 37 2. ** Scoring Rules **: Evaluate which tool has better extraction quality . 38 lists ) is fully extracted . - ** Extraction Completeness **: Check if key content ( code , tables , images , - ** Format Accuracy **: Verify correct Markdown formatting ( code indentation , table alignment , image links ). - ** Semantic Coherence **: Ensure logical flow and heading hierarchy are preserved . 44 45 3. ** Issue Feedback **: Strictly identify problems by the 8 module types above ; 46 return empty list if no issues . 47 48 4. ** Return Result **: JSON format with 3 fields : score , name , reason . - `score `: 1 if Tool is better , 2 if Tool is better . - `name `: Must be one of the 8 module types , selecting the module with greatest difference . - `reason `: Objective description of performance differences in that module . 53 54 Example Output : 55 { 56 \" score \": [12] , \" name \": \"[ module_type ]\" , \" reason \": \"[ objective description of differences ]\" 40 41 42 43 50 52 57 58 59 } Figure 22: Rating prompt used for automated pairwise document comparison. The prompt instructs the judge model to compare two HTML extraction tools by evaluating 8 content module types (code, math, table, image, list, title, paragraph, other) and return structured JSON score. (Part 1/2) 33 AICC: 7.3T AI-Ready Corpus Built by Model-Based HTML Parser 1 ** Important Notes **: 2 1. Only use predefined module categories . 3 2. Focus on structured content ( code , tables , formulas , images ) conversion . 4 3. For paragraphs , check text coherence and semantic completeness . 5 6 ### Original HTML Code : 7 ``` html 8 {} 9 ``` 10 11 ### Tool 's Extracted Markdown : 12 ```md 13 {} 14 ``` 15 16 ### Tool 's Extracted Markdown : 17 ```md 18 {} 19 ``` 20 21 Return ONLY one JSON object , no other explanations or analysis ! 22 Figure 22: Rating prompt used for automated pairwise document comparison. The prompt instructs the judge model to compare two HTML extraction tools by evaluating 8 content module types (code, math, table, image, list, title, paragraph, other) and return structured JSON score. (Part 2/2)"
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
    ]
}