{
    "paper_title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
    "authors": [
        "Cheng Luo",
        "Zefan Cai",
        "Hanshi Sun",
        "Jinqi Xiao",
        "Bo Yuan",
        "Wen Xiao",
        "Junjie Hu",
        "Jiawei Zhao",
        "Beidi Chen",
        "Anima Anandkumar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods."
        },
        {
            "title": "Start",
            "content": "HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Cheng Luo 1 Zefan Cai 2 Hanshi Sun 3 Jinqi Xiao 4 Bo Yuan 4 Wen Xiao 5 Junjie Hu 2 Jiawei Zhao 1 Beidi Chen 3 Anima Anandkumar 1 5 2 0 2 8 1 ] . [ 1 4 7 5 2 1 . 2 0 5 2 : r Abstract Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods. 1. Introduction Modern Large Language Models (LLMs) increasingly support extremely long inputs: Llama-3 (Dubey et al., 2024) handles up to 128K tokens, Claude (Anthropic, 2024) supports up to 1 million tokens, while Gradient AI (Pekelis et al., 2024) extends Llama-3 to 4 million tokens. These extended context lengths improve performance on tasks such as book summarization (Pal et al., 2023) and video generation (Liu et al., 2024b), requiring millions of tokens. 1California Institute of Technology 2University of Wisconsin-Madison 3Carnegie Mellon University 4Rutgers University 5Microsoft. Correspondence to: Cheng Luo Anandkumar <an- <chengluo@caltech.edu>, ima@caltech.edu>. Anima Preprint. Work in Progress 1 Figure 1. Estimated memory consumption of inference Llama-38B model with 1 million token on single GPU. As context lengths increase, memory usage and latency grow significantly due to self-attention in transformers. To improve inference efficiency (Zhou et al., 2024), most LLM inference consists of two phases: prefill for input processing and decoding for token generation, with key and value states from attention cached for reuse (KV cache). However, as the context length increases, the KV cache memory grows rapidly, posing significant challenges for storage and efficiency. For example, generating 1 million tokens using Llama-3-8B model requires 207 GB of memory (15 GB for pre-trained parameters, 64 GB for activation, and 128 GB for KV cache 1) as shown in Figure 1. This makes longcontext inference unfeasible on consumer-level GPUs such as the NVIDIA RTX-4090 with 24 GB of memory. To address this memory challenge, modern LLM serving systems have introduced offloading strategies that transfer data to CPU memory, enabling efficient LLM inference within hardware constraints (Sheng et al., 2023; Lee et al., 2024; Aminabadi et al., 2022). By moving portions of the KV cache to the CPU, these methods allow for much longer context generation than would otherwise fit in GPU memory. However, these methods can hardly be applied to consumerlevel GPUs such as the RTX-4090, where only 8GB of memory remain for the KV cache and activations. This is due to two factors: (1) offloading does not reduce activation memory usage, and (2) because offloading is applied at coarse granularity, the remaining KV cache on the GPU ends up larger than the rest of the memory usage. 1The calculation is based on standard inference generation using Hugging Face with BF16 precision and batch size of 1. Details of how it is calculated can be referred to in Appendix A.3. HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Figure 2. Demonstrations of KV cache policies in inference. Full KV cache contains two main dimensions: layer and head. Layer-wise offloads KV cache in the layers dimension, with cache budget of all heads per layer. HEADINFER further reduces GPU memory by adaptively reallocating cache budgets in the heads dimension, with cache budget of one head. Our approach: We propose head-wise offload (HEADINFER), an inference framework that drastically reduces the GPU memory needed for LLM inference by offloading the KV cache at the level of individual attention heads. The key idea is to leverage attention head independence to decompose the attention computation in head-wise manner, requiring only one head of the KV cache on the GPU at time and offloading the rest to the CPU. As shown in Figure 2, HEADINFER frees the GPU system from storing the KV cache of the entire model or layer and instead stores only single head at any given time. This fine granularity slashes the GPU memory footprint while maintaining exact mathematical equivalence. To enable efficient long-context inference on consumer GPUs, HEADINFER integrates several key optimization techniques: adaptive heads grouping, chunked prefill, and asynchronous data transfer. Adaptive head grouping dynamically adjusts the number of attention heads retained on the GPU, progressively reducing the memory footprint as the context length increases. Chunked prefill reduces peak memory usage by splitting long input sequences into smaller segments, processing them incrementally to avoid excessive activation memory consumption. Asynchronous data transfer overlaps KV cache movement between GPU and CPU and ensures computation proceeds without stalls from memory transfers. Through roofline analysis (Williams et al., 2009), we demonstrate that HEADINFER maintains high compute efficiency while significantly reducing GPU memory consumption. We implement HEADINFER on the Hugging Face framework and evaluate it on representative LLMs, including Llama, Qwen, Mistral, and Gemma, with varying model sizes and sequence lengths. More than the 1 million token inferences shown in Figure 1, our results demonstrate that HEADINFER extends the Llama-3-8B models context length from 25K (standard inference) and 45K (standard offload) to 4 million tokens, achieving around 100x improvement in context length extension using NVIDIA RTX 4090. In summary, this paper presents the following contributions. HEADINFER enables inference with context length with over million tokens on single consumer GPU like NVIDIA RTX 4090. In-depth analysis using the roofline model: high compute efficiency is achieved during prefill without shifting computation into the memory-bound regime. Fully general and implementation-agnostic attention: HEADINFER supports dense as well as sparse attention, and it works with head-wise sparse attention such as duoattention (Xiao et al., 2024a). Support for massive model inference: HEADINFER collaborates with pipeline parallelism to support larger models like Llama3-70B with 1 million tokens. Easy-to-use and portable, requiring minimal code changes to the existing inference frameworks. The code example can be referred to in Appendix E. 2. Related Work 2.1. Generative Inference and KV Caching Generative LLM inference typically involves two main the prefill stage and the decoding stage. stages: In the prefill stage, the model processes the initial input prompt by computing attention scores for all tokens in the input sequence. For long-context input, it is common to adopt chunked prefill (Agrawal et al., 2024), which divides the prompt into fixed-length chunks to incrementally build the KV cache. This strategy significantly reduces peak memory usage by lowering linear layers peak intermediate activation size from the entire sequence to just the smaller chunk. In the subsequent decoding stage, each newly generated token from the prefill stage is fed back into the model, creating an autoregressive generation process. The LLM produces one new token at time, and each token attends to all previous KV cache. 2 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading 2.2. Lossy KV Cache Management Evit KV cache can reduce memory usage and computational complexity. One direction is to identify and retain only the most valuable tokens within the KV cache. Representative methods include Sliding Window Attention (Beltagy et al., 2020), Heavy Hitter (Zhang et al., 2023), and StreamingLLM (Xiao et al., 2024b). Another direction is to identify and retain the attention heads. Wu et al. (Wu et al., 2024) find way to evaluate the importance of attention heads. Head-wise sparsity such as duo-attention (Xiao et al., 2024a), HeadKV (Fu et al., 2024), and Razorattention (Tang et al., 2024) start to divide up KV cache budgets based on the importance of each head, which is usually determined by the need for retrieval or reasoning. Minference (Jiang et al., 2024a) takes this idea further by applying distinct sparse patterns to different heads. 2.3. Offloading KV Cache Offloading the KV cache from the GPU memory to CPU DRAM is another memory-efficient strategy. For instance, LayerKV (Xiong et al., 2024) implements efficient layerwise KV offloading and overlapping data transfers to improve the context length. FastDecode (He & Zhai, 2024) and NEO (Jiang et al., 2024b) also offload parts of the KV cache to the CPU and perform attention computations on the CPU. ShadowKV (Sun et al., 2024) combines SVD decomposition with offloading to reduce communication overhead. FlexInfer (Xu et al., 2024) introduces the vTensor abstraction to better manage heterogeneous memory resources. Infinigen (Lee et al., 2024) introduces dynamic KV cache management with offloading systems. 3. HEADINFER: Head-wise Offload 3.1. Background: Regular Inference with KV cache Generation. At each time step t, the input token xt is transformed into an embedding vector E(xt) by embedding the token. Then, linear projection generates the key Kt and the value Vt, which can be written as follows: Kt = WKE(xt), Vt = WV E(xt) (1) Here WK and Vt are projection weights, and the current Kt and Vt are appended to the existing key cache Kcache and value cache Vcache. Kcache = [K1, K2, ..., Kt], Vcache = [V1, V2, ..., Vt] (2) The state of the cache can be memory-intensive. Together with sequence length and hidden length D, Kt, Vt RSH . This takes 2 memory. 3 Attention with KV Cache. During the computation of self-attention at time step t, the model utilizes the entire sequence of keys and values from time steps 1 to (stored in the KV cache) alongside the new query Qt: At = Sof tmax( QtK cache dk )Vcache (3) where dk is the dimensionality of the keys. Offload KV Cache. When the context length grows on million scale, or the GPUs on-device High-Bandwidth Memory (HBM, or MHBM ) is small on the consumer GPU, it may become insufficient to store the entire key-value cache. In such scenarios, KV cache offloading provides practical approach to handling memory limitations. Offloading can temporarily move parts of the KV cache to CPU RAM or other external resources (e.g., NVMe storage and CPU disk). However, each offloading strategy introduces potential communication overheads that must be carefully weighed against the gains in usable context length. For batch of size B, transformer with layers, and the KV cache with the bytes per element sizeof (datatype), the total KV cache size is: SizeKV cache = 2 sizeof (datatype) (4) If SizeKVcache > MHBM , the system can offload some portion of the KV cache to external memory to avoid out-ofmemory errors. Let α(0 α 1) be the fraction of the KV cache that remains on the GPU, and 1 α be the fraction offloaded to external memory. The memory footprint on the GPU SizeonGP can be expressed as follows: SizeonGP = α SizeKV cache Therefore, we require: SizeonGP MHBM , α MHBM SizeKVcache (5) (6) 3.2. Head-wise Offload (HEADINFER) Head-wise KV Cache Generation. In transformer architectures, each self-attention layer is split into multiple attention heads H. Each head has its own set of key-value (KV) tensors: (h) RSDh , (h) RSDh (7) where is divided by so that each head handles subdimension Dh = D/H. Therefore, instead of single large KV cache, head-wise approach organizes cache in separate memory spaces. Formally, at time step t, we have: (h) cache = [K (h) 1 , ..., (h) ], (h) cache = [V (h) 1 , ..., (h) ] (8) HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Figure 3. Granularity of different methods. Each cube represents the entire attention process along three dimensions: Sequence (S), Layers (L), and Heads (H). Standard inference puts everything on the GPU. Chunked-prefill fetches only part of the sequence dimension of all tokens on the GPU at time. Layer-wise offloading fetches subset of layers on the GPU, offloading the rest. HEADINFER introduces an even finer approach that maintains only selected heads within layer. As result, each head stores its keys and values in contiguous memory space, enabling selective offloading of certain heads cache when memory constraints emerge. During self-attention at the time step t, we calculate the attention output for each head independently, using: A(h) = Sof tmax( Q(h) cache(h) dk )Vcache(h) (9) Finally, the outputs A(h) final output of attention for that layer. h=1 are concatenated to form the Head-wise Offload. Since the attention computation has head-wise independence, if we can keep the KV cache of single head rather than the entire layer, then the memory consumption can be reduced substantially. This leads to our proposed head-wise offload (HEADINFER) strategy. HEADINFER is designed to minimize the fraction of onGPU data α (the fraction of the total KV cache stored on GPU). Using Llama-3-8b as an example, we define Hall as the total number of attention heads of given model, which equals the number of heads per layer times the number of layers L. Hon is the number of heads retained on the GPU, and Hof is the number of heads offloaded to external memory (CPU); obviously, we have Hon + Hof = Hall Define α as the fraction of the KV cache that remains on the GPU. We can store all KV cache on the GPU if α = 1 or layer-wise offload if α = 1/L. In our head-wise scheme: α = Hon Hall = 1 (10) 3.3. Granularity: Sequences, Layers, and Heads HEADINFER differs from traditional offload in terms of granularity. When deploying large language models, each dimension of the model can become potential bottleneck in GPU memory. Naively offloading the entire KV cache or entire layers can be too coarse, leading to suboptimal memory usage. As shown in Figure 3, HeadInfer addresses this challenge by introducing hierarchical set of techniques, including chunked-prefill, layer-wise offload, and head-wise offload that operate at increasingly fine-grained levels of sequence (S), layers (L), and heads (H). Chunked-Prefill (S): Rather than processing all sequences of input tokens at once, HEADINFER divides the sequence into smaller chunks, each processed separately during the prefill stage. This partition helps reduce the activation memory usage. Layer-Wise Offload (L): Instead of storing the entire KV cache on the GPU, HEADINFER offloads it to the GPU and fetches the KV cache of specific layers on demand. Consequently, only the relevant portion of the KV cache resides on the GPU at any given time. Head-Wise Offload (H): Within each layer, HEADINFER can selectively offload the KV cache for all attention heads, fetching certain attention heads on demand. This offers the finest granularity by focusing on subset of heads within each layer. Here we keep only single head on the GPU, and the fraction of the total KV cache that occupies GPU memory is reduced by α = 1/(L H), with total size of: SonGP = 2 DH sizeof (datatype) (11) By reducing α, we can preserve GPU memory capacity for extended context inference and make the million-level inferences on consumer GPUs possible. By combining these techniques, HEADINFER allows precise control over which parts of the activation and KV cache remain on the GPU. Because these dimensions nest naturally as chunks are subsets of the sequence, layers are subsets of the model, and heads are subsets of the attention layer; HEADINFER can flexibly adapt to wide range of hardware constraints, such as available GPU memory, while meeting performance requirements. HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading larger KV cache fetches. For instance, we fuse all heads when processing the LLAMA3 inference 1-500K context, process that is identical to the standard attention calculation. We divide the attention heads into two groups when processing 500K-1M, four groups when processing 1M-2M, and eight groups when processing 2M-4M. Specifically for eight groups, HEADINFER performs head-wise offload with finest granularity, where only one head is left on the GPU. Extension: Combining with Head-wise Sparsity. HEADINFER is compatible with existing sparse optimization techniques. Our work focuses on integrating HEADINFER with head-wise sparsity, such as duo-attention (Xiao et al., 2024a). This head-wise sparsity reduces memory by truncating less important heads to fixed length (typically under 1k). We designate these heads as on-GPU Hon heads without offloading since they consume minimal GPU memory, and offloading KV cache smaller than 1k lead to performance degradation, as analyzed in section 5. The detailed design for the extension can be found in Appendix Algorithm 1 HEADINFER: Head-Wise Inference Require: Transformer model with layers, attention heads per layer, input sequence length # Initialization: 1: Allocate GPU memory for Hon heads. 2: Allocate CPU memory for Hoff heads. # Prefill Phase: for each layer in Transformer Layers do Compute key (h), value (h) on GPU if Hon (GPU-resident heads) then GPU, (h) Update GPU KV cache: (h) GPU else for each head in do 3: for each chunk of input tokens do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for end for Concatenate A(h) end if Compute attention output A(h) end for on GPU Async Prefetch next KV cache (h+1), (h+1) Async Update CPU KV cache: (h) CPU, (h) CPU for all heads to form layer output # Decoding Phase: for each layer in Transformer Layers do Compute key (h), value (h) on GPU if Hon (GPU-resident heads) then GPU, (h) Update GPU KV cache: (h) GPU else for each head in do 18: for each token to be generated do 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: end for end for Concatenate A(h) end if Compute attention output A(h) end for Async Prefetch next KV cache (h) Async Updated CPU KV cache: (h) CPU, (h) CPU, (h) CPU CPU on GPU for all heads to form layer output Figure 4. HEADINFER snapshot. All parameters are stored on the GPU. Head-wise partitioned KV cache is moved across GPU and CPU with the ping-pong memory. 4. HEADINFER for Memory-Efficient"
        },
        {
            "title": "Inference",
            "content": "For the offloading strategy, the potential communication overheads must be carefully considered against the gains in context length extension. This is because the offloading communication cost grows along with the context length. Fortunately, for this, HEADINFER can overlap the communication cost with evening growing attention computation. This is achieved by ping-pong memory design (Figure 4) for asynchronous offloading and prefetch. We present the implementation on algorithm 1. Overlapping Head-wise KV Cache Movement and GPU Compute. The overlapping mechanism is critical for offload performance, especially for long-context inference when the computation time can completely overlap the offload communication. While the GPU computes attention for the current head A(h) , it concurrently prefetches the next heads KV cache from the CPU and offloads the current heads cache back to the CPU. Ping-pong memory enables non-blocking PCIe transfers to synchronize memory movement with computation. This mechanism is presented in Algorithm 1 lines 10-11 and 25-26, as async prefetch and async update. Efficient Management of KV Cache. Effective KV cache management guarantees the feasibility of long context headwise offload. Key strategies here include head-wise partitioning and pre-allocation. Head-wise partitioning makes sure that each head has its own KV cache (h), (h), and allows selective offloading and retention based on memory availability and head importance. HEADINFER pre-allocates the CPUs KV cache memory and the GPUs ping-pong memory to avoid runtime memory allocation overheads. Adaptive Head-wise Offloading. Adaptive head-wise offloading reduces kernel launch overheads caused by headwise partitioning, especially for small context sizes. This involves fusing multiple attention heads into single kernel, reducing the number of kernel launches at the cost of HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Figure 5. Flashattention in the roofline plot analysis using the RTX-4090 device setting. 5. Analysis Although KV cache offload is proposed to reduce memory usage, it remains an open question whether offloading harms overall performance, especially when the context length is large and works with chunked-prefill. This section analyzes the theoretical peak performance for given GPU under constrained high-bandwidth memory (HBM) and peripheral component interconnect express (PCIe). Performance Model. We consider GPU characterized by HBM capacity MHBM , memory bandwidth Bmem, and compute throughput FGP (measured in FLOPS). We also incorporate the slower PCIe bandwidth Bpcie into the performance model to account for the offload. Memory Bound vs. Compute Bound. GPU operators can be classified as compute bound or memory bound, which is determined by the time spent in arithmetic operations and the time spent accessing HBM. Two primary bottlenecks define the systems performance regime: Memory-Bound: When Bmem (memory bandwidth) is insufficient to transfer the KV cache quickly enough, inference operates below the GPUs peak FLOPS capacity. Compute-Bound: When Bmem is sufficient, high compute efficiency is achieved and the throughput is determined by the GPUs peak computation rate FGP . Peak Performance. Assume, for simplicity, that we can approximate the per-token inference time as follows: max(Tcomp, Tmem) (12) During the prefill stage, Tcomp DS2LH captures FGP the compute part, and Tmem DSLH captures the memory part. When scaling S, Tcomp grows faster than Tmem, associated with the quadratic growth in sequence length, which makes compute throughput the limiting factor. Bmem During the decoding stage, each new token must attend to all previously generated tokens. Generating single token incurs costs is roughly: Tcomp DSLH and Tmem DSLH . When scaling S, Tcomp grows linearly as Tmem, but Bmem, Bpcie are usually slower than FGP , making the memory bandwidth the main bottleneck. Bmem FGP Roofline Model. The roofline plot typically displays kernels computation throughput and arithmetic intensity, providing visual representation of its hardware performance. We present the roofline analysis for the FlashAttention kernel (Dao et al., 2022; Dao, 2023; Shah et al., 2024) executed on an RTX-4090 (Figure 5), with details in Appendix C. We believe that the roofline model can analyze both GPU performance and heterogeneous systems performance. Our key observations from roofline analysis are: Prefill (compute-bound behavior). Prefill HEADINFER/Offload has higher arithmetic intensity than Prefill as it only offloads KV cache, and the arithmetic intensity grows as context length increases. For context lengths 10k, prefill remains compute-bound, even when offloading via slower PCIe bandwidth. In contrast, for short context lengths (S 1k), HEADINFER/Offload shifts to the memory-bound regime, leading to offload-induced slowdowns. The turning point is achieved at 2K, and 10K can ensure high computational efficiency. Decoding (memory-bound behavior). Decoding performance is primarily memory-bound. Consequently, relying on PCIe bandwidth during offloading substantially degrades overall throughput. HEADINFER (unchanged behavior). HEADINFERs head-wise offloading during both chunked-prefill and decoding does not alter the position of the roofline plot due to the independence of the attention heads. Although headwise computation impacts GPU utilization, the adaptive head strategy can preserve performance. 6 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Table 1. Performance(benchmark score) of different methods on LongBench v2 on single RTX-4090 GPU, under different task difficulties (Easy/Hard) and context lengths (Short/Medium/Long). Overall performance is average scores on all questions. LongBench V2 Standard 25K Chunked Prefill 30K Layer-wise offload 45K HEADINFER 1024K Overall Easy Hard 27.3 30.2 28.9 27.1 28.9 29.2 29.6 31.2 28.4 28.2 29.0 30.2 Short Medium Long 25.9 25.1 33.9 25.9 25.6 32.8 36.1 26.9 24.2 30.6 27.0 33.9 Table 2. Performance(benchmark score) of different methods on SCBench on single RTX-4090 GPU. kv and prefix-suffix are string retrieval in key-value and prefix-suffix scenarios. vt is variable tracking. qa-chn, qa-eng, and choice-eng are English/Chinese question answering. mf is finding the math answer. many-shot is finding multiple shots in context. summary is document summarization. kv SCBench 15.8 Standard 25K 21.4 Chunked Prefill 30K Layer-wise offload 45K 22.6 28 HEADINFER 1024K prefix-suffix 9.6 10.4 12.8 17.2 vt 4.6 6.9 8.4 42 qa-chn 9.4 9.4 10.4 11.9 qa-eng 13.3 15.5 15.7 23.0 choice-eng mf many-shot 2.6 2.2 2.2 9.4 16.3 25.2 25.9 25. 36.5 38.6 37.8 59.8 summary 32.3 33.5 33.6 37.1 6. Performance Evaluation 6.1. Experimental Setup We conduct our experiments on RTX-4090 GPUs, which are configured with 24.5 GB of HBM3, 4 AMD EPYC 7B13 with 64 CPUs each (a total of 256 cores), and 2 Gen4 NVMe of 512 GB each and 1 TB of DDR5 RAM in total. The GPUs are independently connected to the host with 16 PCIe 4.0 interfaces, providing 25.0 GB/s unidirectional D2H and H2D throughput for pinned host memory. 6.2. Long-Context Benchmarks We evaluate HEADINFER using the LongBench v2 (Bai et al., 2024) and SCBench (Li et al., 2024) benchmarks, and other long-context benchmark results such as Needle-in-aHaystack (NIAH) (Kamradt, 2023) and Ruler (Hsieh et al., 2024) are shown in the Appendix A.2. We use the Llama3-8B-Instruct-Gradient-1024k model, which supports up to 1 million context lengths. All lossless methods, including standard inference, chunked prefill, layer-wise offload, and HEADINFER, are running on single RTX-4090 GPU, with maximal context length achieved within 24 GB. The longcontext benchmark results aim to reveal the performance gain with HEADINFERs context length extensions. LongBench v2 is comprehensive suite of long-context datasets designed to assess long-context problems that require deep understanding and reasoning. It comprises 503 difficult multiple-choice questions within two difficulties, including Easy/Hard and word lengths, including Short of 0-32k words, Medium of 32k-128k words, and Long of 128k-2M words. As shown in Table 1, we measure their overall scores for different task categories. HEADINFER achieves the highest benchmark score for Medium (27.70) and Long (30.6). Layer-wise offload performs best on Short (36.1) for truncation side effects. SCBench is comprehensive suite of datasets encompassing multiple tasks, designed to assess long-context understanding capabilities. It contains subsets with different prompt lengths up to 500K tokens and 227K on average. As shown in Table 2, we measure their overall scores for different task categories. HEADINFER outperforms all other methods on all 9 tasks, with superior 1M context length. 6.3. Efficiency Results of Memory and Throughput We evaluate the maximum context length supported under RTX-4090 memory constraints, as well as prefill/decoding throughput. Our experiments use the Llama-3-8B, Llama-370B, Mistral (Jiang et al., 2023), Qwen (Bai et al., 2023a), and Gemma-2 (Team et al., 2024) models. The default number format for weights and activations is BFloat16, and 4-bit KV-quant is deployed with KIVI (Liu et al., 2024c). The chunk size is set to 10K based on our roofline analysis. LLM Inference on Consumer GPUs with 24G memory. We measure the GPU memory consumption of Llama3-8B inference with HEADINFER and 1 million context length. HEADINFER uses 17GB during prefill and 16.4GB during decoding; in contrast, other methods are unable to run at this context scale using 24GB RTX-4090. Accordingly, we measure the maximum achievable context length to assess memory efficiency. As shown in Table 3, HEADINFER outperforms other system optimization approaches, scaling from thousand-level contexts (10K-70K) to million-level contexts (1,024K4,200K). Compared to layer-wise offload, HEADINFER can maintain stable activation memory with chunked prefill and minimize KV cache GPU memory with head-wise offload. Note that with HEADINFER, the maximum context lengths for Llama3, Llama2, and Mistral are constrained by CPU RAM (512GB for KV cache), while the other methods are limited by GPU memory. This means we can use larger CPU RAM or offload to disk for more extended context. We leave this exploration for future work. 7 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Table 3. Comparison of maximum context length with system optimization methods on various models inference. All experiments within this table run on single RTX-4090 GPU with 24GB and AMD EPYC 7V12 CPUs with 1TB memory."
        },
        {
            "title": "Context Length",
            "content": "Llama-3-8B Llama-2-7B Mistral-7B Qwen2-7B Gemma-2-9b"
        },
        {
            "title": "Standard",
            "content": "Chunked Prefill 4-bit KV-quant Layer-wise offload HEADINFER 25K 30K 45K 45K 4096K 10K 20K 30K 60K 1024K 30K 40K 40K 45K 4096K 35K 70K 50K 50K 4200K 10K 10K 20K 35K 1300K Table 4. Llama3-70B Inference with long context input. Table 5. Ablation study of HEADINFER on Llama-3-8B. Llama-3-70B"
        },
        {
            "title": "Context Length",
            "content": "Llama-3-8B"
        },
        {
            "title": "Standard",
            "content": "HEADINFER + 10k chunk size HEADINFER + 5k chunk size 10K 950K 1000K Scaling up to Llama3 70B Architecture. Scaling ability to 70B models is key factor for demonstrating if HEADINFER is effective for larger-scale model inference. We evaluate HEADINFER on the Llama-3-70B model. The model inference is conducted using pipeline parallelism (Narayanan et al., 2019) across 8 RTX-4090 GPU nodes. As shown in table 4, HEADINFER achieves efficient inference of 1 million sequences with the 70B model, outperforming standard methods. Additionally, due to Llama-3-70Bs higher arithmetic intensity compared to Llama-3-8B, we can employ smaller chunk size without performance degradation. This reduced chunk size allows for better memory utilization, enabling increased KV cache allocation and extending the maximum context length from 950k to 1M. Prefill and Decoding Throughput of HEADINFER We evaluate the prefill and decode throughput of the inference Llama-3-8B model using HEADINFER with adaptive head grouping. Our HEADINFER implementation achieves 516 tokens/second during prefill for 1 million-token input and 0.15 tokens/second during decoding with 1 million-token KV cache. context length of 1 million cannot be achieved by other methods. For comparison, with 20K-token input, HEADINFER achieves 7210 tokens/second prefill and 6 tokens/second decoding. Standard inference achieves 7235 tokens/second prefill and 33 tokens/second decoding, while layer-wise offloading matches HEADINFER at 7210 tokens/second prefill and 6 tokens/second decoding. Additional latency results for different context lengths and different methods are provided in Appendix A.1. 7. Ablation Study How do different levels of granularity affect memory? To evaluate the impact of different levels of granularity on memory, we conducted detailed ablation study using the Llama-3-8B model. From the memory perspective, three"
        },
        {
            "title": "Standard",
            "content": "Layer-wise Offload Chunked Prefill HEADINFER Head=8 Group = 1 HEADINFER Head=4 Group = 2 HEADINFER Head=2 Group = 4 HEADINFER 25K 45K 30K 550K 1100K 2100K 4096K dimensions of sequences S, layer L, and head are managed through chunked-prefill, layer-wise offloading, and head-wise offloading, respectively. As shown in Table 5, HEADINFER with supports 4096K context length, far exceeding the 25K of standard methods, 45K of layer-wise offload with L, and chunked prefill with S. Adjusting head-group granularity shows trade-off, with context lengths ranging from 2100K to 550K. Note that HEADINFER with (Head=8 Group=1) works the same as layer-wise offloading combined with chunked-prefill for L. HEADINFER achieves 8 context extension than (Head=8 Group=1), showing its memory efficiency. 8. Conclusion In this paper, we introduced HEADINFER, novel head-wise KV cache management framework designed to enable efficient long-context large language model (LLM) inference on consumer GPUs. HEADINFER dynamically offloads KV cache components to CPU memory, employing headwise and asynchronous offloading strategies. Our Roofline analysis highlights HEADINFERs ability to preserve GPU computational efficiency while reducing memory footprint, making long-context inference feasible on consumer-grade GPUs. Our evaluations demonstrated HEADINFERs ability to achieve over 1-4 million context tokens on single consumer GPU with mathematical equivalency. We hope that our work will inspire future research in memory-efficient inference from the perspective of headwise offload. We believe that HEADINFER will be valuable tool for the community, enabling the inference of long context on consumer-grade hardware with limited resources. HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Impact Statement: Democratizing Access to Advanced AI Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023a. Artificial Intelligence (AI) has the potential to transform industries, revolutionize education, and empower individuals. However, the deployment of cutting-edge models, particularly large language models (LLMs), is often hindered by significant resource requirements, creating barriers to entry for smaller organizations and underserved communities. In this work, we introduce HEADINFER, memory-efficient inference framework designed to bridge this gap. By leveraging head-wise offloading strategies, HEADINFER enables resource-constrained devices to process unprecedentedly long contexts, achieving capabilities typically reserved for high-performance systems. For instance, HEADINFER allows consumer-grade GPU to handle over 1 million context tokens, democratizing access to advanced LLM functionalities. Ultimately, HEADINFER aligns with the broader vision of democratizing AI, ensuring that technological advancements benefit humanity as whole rather than select few. Through innovations like HEADINFER, we hope to inspire future where AI is universally accessible, fostering creativity, knowledge sharing, and inclusive progress."
        },
        {
            "title": "References",
            "content": "Agrawal, A., Kedia, N., Panwar, A., Mohan, J., Kwatra, N., Gulavani, B., Tumanov, A., and Ramjee, R. Taming throughput-latency tradeoff in llm inference with sarathiserve. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pp. 117134, 2024. Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Aminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., et al. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 115. IEEE, 2022. Anthropic, A. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 1, 2024. Badri, H. and Shaji, A. Half-quadratic quantization of large machine learning models, 2023. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023b. Bai, Y., Tu, S., Zhang, J., Peng, H., Wang, X., Lv, X., Cao, S., Xu, J., Hou, L., Dong, Y., Tang, J., and Li, J. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024. Beltagy, LongI., Peters, M. E., and Cohan, A. former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. De, S., Smith, S. L., Fernando, A., Botev, A., CristianMuraru, G., Gu, A., Haroun, R., Berrada, L., Chen, Y., Srinivasan, S., et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., Zheng, N., and Wei, F. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fu, Y., Cai, Z., Asi, A., Xiong, W., Dong, Y., and Xiao, W. Not all heads matter: head-level kv cache compression method with integrated retrieval and reasoning. arXiv preprint arXiv:2410.19258, 2024. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. He, J. and Zhai, J. Fastdecode: High-throughput gpuefficient llm serving using heterogeneous pipelines. arXiv preprint arXiv:2403.11421, 2024. 9 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao, Y. S., Keutzer, K., and Gholami, A. Kvquant: Towards 10 million context length llm inarXiv preprint ference with kv cache quantization. arXiv:2401.18079, 2024. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., and Ginsburg, B. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jiang, H., LI, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A. H., Li, D., Lin, C.-Y., et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. Jiang, X., Zhou, Y., Cao, S., Stoica, I., and Yu, M. Neo: Saving gpu memory crisis with cpu offloading for online llm inference. arXiv preprint arXiv:2411.01142, 2024b. Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model on million-length video and language with blockwise ringattention. CoRR, 2024b. Liu, Z., Yuan, J., Jin, H., Zhong, S., Xu, Z., Braverman, V., Chen, B., and Hu, X. Kivi: tuning-free asymmetric 2bit quantization for kv cache. In Forty-first International Conference on Machine Learning, 2024c. Loeschcke, S. B., Toftrup, M., Kastoryano, M., Belongie, S., and Snæbjarnarson, V. Loqt: Low-rank adapters for quantized pretraining. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Luo, C., Zhao, J., Chen, Z., Chen, B., and Anandkumar, A. Mini-sequence transformers: Optimizing intermediate memory for long sequences training. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N. R., Ganger, G. R., Gibbons, P. B., and Zaharia, M. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pp. 115, 2019. Pal, A., Karkhanis, D., Roberts, M., Dooley, S., Sundararajan, A., and Naidu, S. Giraffe: Adventures in expanding context lengths in llms. arXiv preprint arXiv:2308.10882, 2023. Kamradt, G. Needle in haystack-pressure testing llms. Github Repository, pp. 28, 2023. Pekelis, L., Feil, M., Moret, F., Huang, M., and Peng, T. Llama 3 gradient: series of long context models, 2024. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Biderman, S., Cao, H., Cheng, X., Chung, M., Grella, M., et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Lee, W., Lee, J., Seo, J., and Sim, J. Infinigen: Efficient generative inference of large language models with dynamic kv cache management. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pp. 155172, 2024. Li, Y., Jiang, H., Wu, Q., Luo, X., Ahn, S., Zhang, C., Abdi, A. H., Li, D., Gao, J., Yang, Y., et al. Scbench: kv cache-centric analysis of long-context methods. arXiv preprint arXiv:2412.10319, 2024. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen, B., Liang, P., Re, C., Stoica, I., and Zhang, C. Flexgen: High-throughput generative inference of large language models with single gpu. In International Conference on Machine Learning, pp. 3109431116. PMLR, 2023. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Liu, G., Li, C., Zhao, J., Zhang, C., and Guo, M. Clusterkv: Manipulating llm kv cache in semantic space for recallable compression. arXiv preprint arXiv:2412.03213, 2024a. Sun, H., Chang, L.-W., Bao, W., Zheng, S., Zheng, N., Liu, X., Dong, H., Chi, Y., and Chen, B. Shadowkv: Kv cache in shadows for high-throughput long-context llm inference. arXiv preprint arXiv:2410.21465, 2024. 10 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., and Li, S. Pose: Efficient context window extension of llms via positional skip-wise training. In The Twelfth International Conference on Learning Representations, 2023. Tang, H., Lin, Y., Lin, J., Han, Q., Hong, S., Yao, Y., and Wang, G. Razorattention: Efficient kv cache compression through retrieval heads. arXiv preprint arXiv:2407.15891, 2024. Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi`ere, M., Kale, M. S., Love, J., et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Williams, S., Waterman, A., and Patterson, D. Roofline: an insightful visual performance model for multicore architectures. Communications of the ACM, 52(4):6576, 2009. Wu, W., Wang, Y., Xiao, G., Peng, H., and Fu, Y. Retrieval head mechanistically explains long-context factuality. arXiv preprint arXiv:2404.15574, 2024. Xiao, G., Tang, J., Zuo, J., Guo, J., Yang, S., Tang, H., Fu, Y., and Han, S. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024a. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024b. Xiong, Y., Wu, H., Shao, C., Wang, Z., Zhang, R., Guo, Y., Zhao, J., Zhang, K., and Pan, Z. Layerkv: Optimizing large language model serving with layer-wise kv cache management. arXiv preprint arXiv:2410.00428, 2024. Xu, J., Zhang, R., Guo, C., Hu, W., Liu, Z., Wu, F., Feng, Y., Sun, S., Shao, C., Guo, Y., et al. vtensor: Flexible virtual tensor management for efficient llm serving. arXiv preprint arXiv:2407.15309, 2024. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., and Tian, Y. Galore: Memory-efficient llm training by gradient low-rank projection. In Forty-first International Conference on Machine Learning, 2024. Zhou, Z., Ning, X., Hong, K., Fu, T., Xu, J., Li, S., Lou, Y., Wang, L., Yuan, Z., Li, X., et al. survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294, 2024. 11 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading A. Experiment Details A.1. Prefill and Decoding Overhead of HEADINFER We evaluate the computational overhead of HEADINFER compared to baseline approaches across different context lengths using the Llama-3-8B model. Our analysis focuses on two key phases: prefill and decoding. Table 6. Prefill overhead (in seconds) of Llama3-8B under different context lengths. Prefill Latency(s) 1K 10K 20K 40K 100K 200K 400K 1M 2M 4M Standard Chunked Prefill Layer-offload HEADINFER (head=8/group=1) HEADINFER (head=4/group=2) HEADINFER (head=2/group=4) HEADINFER (head=1/group=8) HEADINFER Adaptive 0.11 0.11 0.12 0.12 0.13 0.14 0.21 0.13 1.23 1.23 1.24 1.24 1.23 1.23 1.27 1.24 2.83 2.83 2.84 2.84 2.89 2.94 3.06 2.84 - - 6.93 7.11 7.26 7.54 7.77 7. - - - 30.2 30.2 30.5 31.2 30.2 - - - 100 99 100 101 99 - - - 357 351 353 356 351 - - - - 2033 2035 2054 2033 - - - - - 7952 7975 7952 - - - - - - 27114 Table 7. Decoding overhead (in seconds per generated token) of Llama3-8B under different KV cache (context) sizes. Decoding Latency(s) 1K 10K 20K 40K 100K 200K 400K 1M 2M 4M Standard Chunked Prefill Layer-offload HEADINFER (head=8/group=1) HEADINFER (head=4/group=2) HEADINFER (head=2/group=4) HEADINFER (head=1/group=8) HEADINFER Adaptive 0.03 0.03 0.03 0.03 0.04 0.06 0.10 0.03 0.03 0.03 0.09 0.09 0.10 0.11 0.14 0. 0.03 0.03 0.17 0.17 0.16 0.17 0.21 0.17 0.04 0.04 0.28 0.28 0.28 0.30 0.33 0.28 - - 0.66 0.66 0.67 0.68 0.71 0.66 - - 1.3 1.3 1.31 1.32 1.33 1.73 - - 2.58 2.58 2.58 2.59 2.61 3.03 - - - - 6.41 6.46 6.51 6. - - - - - 13.7 13.8 13.7 - - - - - - 27.2 27.2 For prefill operations, HEADINFER demonstrates similar performance to standard approaches for shorter context lengths (up to 20K tokens). Beyond this range, HEADINFER scales efficiently with longer contexts, outperforming Layer-offload due to its fine-grained head-wise KV cache management. Notably, HEADINFER enables inference for 4M tokens on single GPU, which is otherwise infeasible. In decoding, HEADINFER maintains low latency at short and medium context lengths. For extended contexts (e.g., 1M and beyond), HEADINFER introduces manageable latency while supporting unprecedented context lengths, with adaptive configurations optimizing performance further. The performance of HEADINFER relies on its ability to dynamically adapt to varying hardware constraints and workload requirements by controlling the granularity of offloading. Specifically, HeadInfer Adaptive achieves optimal performance by selectively choosing the most suitable head group size based on the context length and memory limitations. HeadInfer (Head = 8 / Group = 1). This configuration aggregates all attention heads within layer into single group for offloading, effectively making the KV cache management layer-wise. As such, HeadInfer (Head Group = 8) is functionally equivalent to Layer-Offload, where all KV cache for layer is stored on either the GPU or offloaded to the CPU in single operation. However, HeadInfer provides the flexibility to adjust the granularity of KV cache management beyond this layer-wise approach. HeadInfer (Head = 1 / Group = 8). At the other extreme, this configuration offloads each attention head individually, offering the finest level of granularity for KV cache control. While this achieves the highest memory savings, frequent PCIe transfers and kernel launches introduce overhead, especially for shorter context lengths, which can impact throughput. HeadInfer Adaptive dynamically selects the optimal head group size (h) to balance memory efficiency and computational 12 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Figure 6. HEADINFER provides equal accuracy as standard inference on the Needle-in-a-Haystack benchmark throughput. This adaptability allows it to achieve superior performance across diverse hardware and context length requirements: Shorter Contexts: For shorter context lengths, PCIe transfer and kernel launch overheads dominate. Larger head groups (e.g., = 4 or = 8) are preferred to minimize overhead while maintaining memory efficiency. Longer Contexts: For longer context lengths, memory usage becomes the primary bottleneck. Smaller head groups (e.g., = 2 or = 1) are chosen to maximize context length while reducing GPU memory usage. This dynamic strategy enables HeadInfer Adaptive to consistently deliver the best performance by adapting to both memory and compute-bound regimes, as observed in performance and overhead evaluations. In summary: HeadInfer Adaptive balances memory usage and performance by dynamically adjusting head group sizes, providing the best of both worlds: memory efficiency and scalability. HeadInfer (Head Group = 8) is equivalent to Layer-Offload, serving as robust baseline for layer-level KV cache management. The head-wise approach of HEADINFER enables extended context lengths during prefill, maintaining high throughput without requiring specialized hardware or approximation techniques. This adaptability positions HeadInfer Adaptive as an essential method for large-scale language model inference in memory-constrained environments. A.2. Long-Context Benchmarks Details Needle-in-a-Haystack (Kamradt, 2023) is challenging pressure test designed to assess the ability of models to accurately identify and retrieve relevant information from lengthy context. Figure 6 presents the results on Needle In Haystack. We use 1024K for Llama-3-8B-Instruct-Gradient-1024k. HEADINFER can accurately recall the information queried within the context in question. Ruler (Hsieh et al., 2024). Designed for comprehensive evaluation of long context, the Ruler benchmark is recent synthetic benchmark suite that includes 13 complex tasks in four main categories. Each context length variation includes 2,600 examples, with tests conducted at 4K, 8K, 16K, 32K, 64K, and 128K tokens. The benchmark comprises four key categories. The retrieval category includes various Needle-in-a-Haystack tasks: Single (S-NIAH) for finding individual key-value pairs in noisy text, Multi-keys (MK-NIAH) for retrieving specific values among hard distractors, Multi-values (MV-NIAH) for finding all values linked to single key, and Multi-queries (MQ-NIAH) for retrieving values across multiple keys. The Multi-hop Tracing category features Variable Tracking (VT), requiring models to trace and return all variable names pointing to the same value through variable bindings. For aggregation, the benchmark includes Common Words Extraction (CWE) for identifying top-K common words from mixed sets and Frequent Words Extraction (FWE) for finding the most frequent words from Zeta distribution. The Question Answering category extends traditional QA datasets by adding distracting paragraphs, testing models ability to locate and utilize relevant information amid distractors. 13 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Table 8. Maximum achievable sequence lengths for different inference methods"
        },
        {
            "title": "Supported Sequence Length within Ruler",
            "content": "Standard Inference Chunked-Prefill Layer-wise Offload HEADINFER 16K 32K 32K 128K Table 8 demonstrates the maximum achievable sequence lengths for different inference methods on the Ruler benchmark. Standard inference, while straightforward, is limited to 16K tokens due to memory constraints. Both Chunked-Prefill and Layer-wise Offload methods double this capacity to 32K tokens through their respective optimization strategies. HEADINFER shows significant advancement by enabling processing of sequences up to 128K tokens - 4x improvement over other offloading methods and 8x improvement over standard inference. This extension in sequence length is achieved through HEADINFERs novel head-wise offloading strategy, which more efficiently manages GPU memory utilization. Table 9. Performance on Ruler benchmark tasks across different context lengths Context NIAH MK-2 MK-3 (%) Length (%) (%) 100.0 4K 100.0 8K 100.0 16K 100.0 32K 100.0 64K 128K 100.0 HEADINFER only 99.6 99.8 100.0 99.6 97.4 75.2 100.0 99.6 99.4 99.8 97.8 56.6 VT (%) 99.20 99.08 98.72 97.32 92.48 54. CWE (%) 99.38 94.68 56.90 2.78 0.10 0.10 FWE QA-1 QA-2 (%) (%) (%) 94.53 84.93 90.60 93.20 84.27 74.8 84.6 79.2 79.6 77.2 76.0 71.8 59.8 56.2 53.2 50.4 49.4 41. Table 9 presents comprehensive analysis of performance across different tasks and context lengths. Since the ruler designs different tasks for different context lengths, the rulers Multi-hop Tracing, Aggregation, and Question Answering become more difficult to complete in long sequence scenarios. This is different from scbench, which directly truncates long sequence tasks. However, even in this case, HEADINFER can still show the same performance as standard inference, which confirms its mathematical consistency. A.3. Memory Analysis of Llama-3-8B Inference In this work, we compare several inference configurations, each targeting different trade-offs between memory efficiency and inference speed. While Figure 1 gives the theoretical benefit of HEADINFER compared to other methods in terms of memory usage, we also give how the theoretical memory is calculated here. Table 10 provides comparative breakdown of memory usage across different inference strategies. These methods balance GPU memory consumption for weights, KV-cache, and activations while addressing the challenges of scaling to long-context inputs. Each strategy is outlined below: Standard: The standard inference method keeps all weights, KV-cache, and activations entirely in GPU memory. With context length of S, the KV-cache scales as Dh H, where is the number of layers, Dh is the hidden dimension per head, and is the number of attention heads. Activations require additional memory of + 2 I, where is the intermediate MLP dimension. While this approach achieves baseline performance, it is constrained by GPU memory limits (e.g., 128 GB for the KV-cache and 207GB for toal). Chunked-Prefill: By dividing the input into smaller chunks of size chunk, this method reduces activation memory from to chunk. The memory footprint for activations is chunkD + 2chunkI, where only part of the sequence resides in GPU memory during processing. Although the KV-cache size remains Dh H, this technique significantly lowers activation memory requirements, reduce total memory from 207GB to 143GB. 4-bit KV-Quant: This method compresses the KV-cache from fp16/bf16 (16-bit floating point) to 4-bit representation, reducing its size by factor of 4. The memory usage becomes SLDhH/4, while activations remain SD+2SI. 14 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Table 10. Memory usage comparison for Llama3-8B with 1 million context length"
        },
        {
            "title": "Method",
            "content": "Weight KV-cache Activation Total Total KV cache"
        },
        {
            "title": "128\nStandard\n128\nChunked Prefill\n32\n4bit-KV-quant\n8\nlayer-wise-offload\nHEADINFER\n1\n∗Total KV cache includes both GPU and CPU memory for offloading methods",
            "content": "64 0.625 64 64 0.625 15.08 15.08 15.08 15.08 15.08 207 143 111 87 16.7 128 128 32 128 128 Layer-wise Offload: This strategy offloads the KV-cache for entire layers to CPU memory as soon as computation for that layer is complete. On GPU, the memory required for KV-cache is reduced to Dh 2, the final 2 due to the ping-pong memory mechanism. However, offloading incurs communication overhead, making this approach more suitable for scenarios with sufficient PCIe bandwidth. HEADINFER: Our proposed approach achieves fine-grained control over memory by offloading KV-cache at the attention head level. With ping-pong memory mechanism, the on-GPU KV-cache is reduced to Dh 2, significantly lowering memory requirements. Activations are further minimized by combining chunked prefill with selective offloading. HEADINFER enables unprecedented scaling to 1M tokens and beyond, allowing context lengths of up to 4 million tokens with minimal GPU memory usage. Also, we observe the total estimated memory of 16.7 GB is close to the real measured memory of 17.0 GB, demonstrating the effectiveness of our estimation. Table 11 summarizes the GPU memory usage for weights, KV cache, and activations, as well as the total usage for in-device experiment. detailed inference setting for each model and comprehensive memory estimation of each method (including key hyperparameters, chunk sizes, and offloading policies) are provided here. As shown in table 11, we focus on five representative strategies for Llama-3-8B inference under varying context lengths: Table 11. Memory consumption analysis for different inference methods (in GB) on Llama-3-8B Method Weights KV cache Activation Total Total KV cache 19.77 Standard-25K 19.46 Chunked-Prefill-30K 19.30 4bit-KV-Quant-45K 18.25 Layer-Wise-Offload-45K HEADINFER-4000K 19.61 Total KV cache includes both GPU and CPU memory for offloading methods 15.08 15.08 15.08 15.08 15.08 1.56 0.63 2.81 2.81 0.63 3.13 3.75 1.41 0.35 3.91 3.13 3.75 1.41 5.63 500 Standard-25K: Baseline approach with unmodified inference. All model weights, activations, and the entire KV cache stay on the GPU. Context length is limited by GPU memory. Chunked-Prefill-30K: Splits the prompt into sequential chunks to reduce activation overhead. KV cache remains on GPU. We use chunk size (e.g., 4K or 8K tokens) such that each partial forward pass does not exceed GPU capacity. 4bit-KV-Quant-45K: Applies 4-bit quantization technique to KV cache, shrinking cache size by approximately 4 compared to FP16/BF16. However, the method can introduce additional on-GPU overhead in activation or conversion operations. In our tests, we adopt standard quantization library (Hooper et al., 2024) for uniform 4-bit KV representation. Layer-Wise-Offload-45K: Offloads each layers KV cache to CPU memory as soon as possible. During inference, only the KV cache for the currently processing layer is kept on the GPU. Once attention computation for that layer completes, its KV cache is swapped out to the CPU. This approach significantly lowers on-GPU KV cache usage but may incur additional offload overhead at each layer boundary. HEADINFER-4000K: Our proposed head-wise offloading approach (HEADINFER) that partitions the KV cache by attention heads. While some heads remain fully on GPU, the cache for other heads (or tokens) are immediately offloaded HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading to CPU memory. Despite very large total KV cache (reported as 500 GB in Table 11), only small fraction resides on the GPU at any time. This enables context lengths in the order of millions of tokens if sufficient CPU RAM is available. For total memory we define: Mtotal; =; Mweights + MKV cache + Mactivation. Weights (e.g., 15.08GB in Llama-3-8B) remain constant for all methods. The KV cache size grows with the number of tokens processed (sequence length) but can be reduced or offloaded depending on the method. Activation memory arises from forward-pass intermediate tensors (attention blocks, MLP layers, etc.) and is partially minimized by chunking or parallelization strategies. In Table 11, KV cache and Activation columns refer to the approximate GPU memory usage during inference; any data offloaded to CPU memory (or disk) is not included in these columns but is counted in Total KV cache if applicable. For instance, HEADINFER has GPU-side KV cache usage of about 3.9GB at any instant, whereas the overall KV cache across CPU and GPU is up to 500GB, enabling very long context windows. B. Motivation and Additional Related Work In this section, we first explain that the KV cache size becomes critical issue for long-text generation in LLM inference, and it becomes more problematic when deploying modern offloading-based inference systems. We then discuss why the existing KV cache management methods cannot fundamentally address the problem in an offloading-based system. B.1. Memory Requirements for Inference This section characterizes the memory requirements for transformer inference. It can be categorized into two components: i) Model Parameters and ii) Activation memory primarily referring to KV cache. The memory requirement for model parameters primarily depends on the hidden dimension D, the number of attention heads, and the number of Transformer layers L. Nearly all the parameters in Transformer block come from linear layers within attention blocks, multilayer perceptron (MLP) blocks, and one language modeling head (LM-head) block. Take Llama-3-8B as an example; the total parameters in transformer-based model can be approximated as 14 HD2 with 15GB of memory. The memory required for activation memory primarily consists of the KV cache, which depends on the model architecture, batch size B, and sequence length S, and it can be pretty significant. The memory can be estimated as 2 D. For instance, in the Llama-3-8B (Dubey et al., 2024) model architecture, serving with FP16 KV cache for 1 million tokens would require at least 207 GB of memoryexceeding the capacity of single 80GB GPU. B.2. KV Cache in LLM Inference Systems As discussed in the previous section, todays LLM serving systems exploit KV caching to avoid redundant computation of key and value projections during the chunked-prefill decoding stage. While this is an effective solution for short sequence generation with single client request, the KV cache quickly becomes key memory consumer when we generate long sequences or employ modern request batching techniques (Sheng et al., 2023). In the Llama-3-8B (Dubey et al., 2024) model architecture, serving with FP16 KV cache for 1 million tokens would require at least 256 GB of memoryexceeding the capacity of single 80GB GPU. Additionally, the latencies of pre-filling and decoding with such large contexts are significant, posing substantial challenges to the effective use of LLMs in long-context scenarios. The rapidly expanding KV cache leads to an urgent need and numerous efforts for KV cache compression, particularly in scenarios with limited GPU memory. Architectural modifications, such as Grouped-Query Attention (Ainslie et al., 2023), Pose (Zhu et al., 2023), Rope (Su et al., 2024), PI (Chen et al., 2023), LongNet (Ding et al., 2023), MST (Luo et al., 2024), LoQT (Loeschcke et al., 2024), Lora (Hu et al., 2021) and Galore (Zhao et al., 2024) require expensive model pre-training. One direction is non-Transformer architecture design, such as Mamba (Gu & Dao, 2023), Linear Attention (Katharopoulos et al., 2020), RWKV (Peng et al., 2023), Griffin (De et al., 2024). However, the transformer is still the most widely used model structure, and in this paper, we focus on KV cache reduction for typical transformers. KV cache token-drop methods, such as H2O (Zhang et al., 2023), StreamingLLM (Xiao et al., 2024b), InfiniGen (Lee et al., 2024), ClusterKV (Liu et al., 2024a) often compromise accuracy in long-context applications and are incompatible with essential KV cache optimization 16 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Figure 7. Token eviction methods cannot work when querying the less relevant information to the main theme. Here, we use 10K document from LongBench (Bai et al., 2023b) and add one sentence that is not relevant to the main theme. In this case, H2O discards tokens less relevant to the main theme, leading to error generation. StreamingLLM discards tokens based on the query but remaining question tokens, making it Hallucinations. HEADINFER can successfully output the exact information from the lengthy input, even when we compress 75% of the KV cache techniques like GQA. KV cache quantization (Hooper et al., 2024; Liu et al., 2024c; Badri & Shaji, 2023), although practical, does have the upper limit of memory saving by 4 8. LLM Inference Systems with Offloading. In modern GPU systems, there is significant disparity between CPU and GPU memory capacities and costs. CPU RAM typically offers much larger capacity at lower cost than GPU memory. For example, modern server-grade systems can easily accommodate 1-2TB of CPU RAM, while even high-end GPU cards like the NVIDIA A100 are limited to 80GB of memory. The cost difference is also substantialserver-grade DDR4/DDR5 RAM typically costs fraction per gigabyte compared to specialized GPU memory. This observation is supported by several prominent works offloading the KV cache to the CPU memory: FlexGen (Sheng et al., 2023) leverages this hardware characteristic by utilizing CPU memory as an extension of GPU memory, allowing for efficient LLM inference even with limited GPU resources. DeepSpeed (Aminabadi et al., 2022) implements sophisticated offloading strategies that take advantage of the larger CPU memory capacity to handle model weights and KV cache that would not fit in GPU memory alone. Infinitigen (Lee et al., 2024) builds on these foundations by introducing dynamic KV cache management that works synergistically with offloading systems, but its efficiency is highly related to token eviction. However, this memory hierarchy presents trade-off: while CPU memory provides larger capacity at lower cost, data transfer between CPU and GPU over PCIe becomes potential bottleneck due to limited bandwidth compared to GPU memory access speeds. This necessitates careful data movement management and strategic offloading decisions to maintain efficient inference performance. B.3. Challenges in KV Cache Management In this context, several recent works propose reducing the KV cache size through retrieval head evictions. However, all the prior works assume the persistence of attention patterns across layers, that is, if head is deemed retrieval head. KV token eviction affects long-context performance. Figure 7 shows significant performance degradation since the actual information required by the query might be discarded if considered unimportant, which uses the KV cache of all prior tokens for computing attention results, and the KV cache management method of H2O with KV cache budget of 2000 tokens. H2O (Zhang et al., 2023) is state-of-the-art technique that retains only small percentage of important tokens in the KV cache to reduce its size. It assesses the importance of each token in every iteration and removes unimportant ones before the next iteration to keep the KV cache size in check. The figure indicates that this is not the practice case, despite H2O-like approaches assuming that the attention pattern does 17 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading not change across heads. The tokens deemed unimportant in the one-head iteration could become important in other heads. Consequently, H2O exhibits high similarity until around 200 iterations (i.e., within the KV cache budget). However, as the sequence length extends beyond the KV cache budget, it struggles with the attention patterns dynamic nature, resulting in more error generation than the optimal case. Note that while we only show the scenario of KV cache budget of 2000 out of total sequence length of 10000 tokens for brevity, this issue would become more pronounced as the sequence length surpasses it. Prior works aiming to reduce the KV cache size through token eviction inherently have some challenges. Given the dynamic attention pattern across iterations, permanently excluding evicted tokens from retinal head token generation can result in non-negligible drop in accuracy. Instead, we must keep the full attention tokens from the retrieval head while selectively evicting less important heads. Furthermore, prior works iterative allocation of KV cache memory leads to inefficient KV cache management. The number of key/value tokens required increases during chunked-prefill, and each extended context inference demands effective memory management. Failing to account for this allocation may result in ineffective KV cache management. Thus, we need to adjust the memory of key/value token pre-allocation while considering the variances between retrieval and full head. C. Roofline Model for head-wise flash attention The Roofline model (Williams et al., 2009) serves as an effective theoretical framework to assess the potential performance of deploying model on particular hardware. Here we evaluate hardware performance of memory access and processing unit capabilities. Table 12. Performance comparison of different attention mechanisms under RTX-4090 setting Operator Prefill flashattention (1k) flashattention (10k) flashattention (100k) head-wise (1k) head-wise (10k) head-wise (100k) Decode flashattention (1k) flashattention (10k) flashattention (100k) head-wise (1k) head-wise (10k) head-wise (100k) Ops Memory Regular Arithmetic Intensity FLOPS Bound Memory (KV cache) Arithmetic Intensity FLOPS Bound Offload 17G 1.7T 172T 2.1G 215G 21T 17M 168M 1.7G 2.1M 21M 210M 21M 209M 2.1G 2.6M 26M 262M 17M 168M 1.7G 2.1M 21M 210M 820 8200 82000 820 8200 1 1 1 1 1 1 165T 165T 165T 165T 165T 165T 1T 1T 1T 1T 1T 1T compute compute compute compute compute compute memory memory memory memory memory memory 4.2M 42M 419M 0.5M 5.2M 52M 17M 168M 1.7G 2.1M 21M 210M 4100 41000 410000 4100 41000 410000 1 1 1 1 1 1 102T 165T 165T 102T 312T 312T 13G 13G 13G 13G 13G 13G memory compute compute memory compute compute memory memory memory memory memory memory Table 12 presents the analysis of layers in Llama-3-8b. From the table, we observe that during the prefill stage, the majority of computations are compute-bound, leading to high performance. Conversely, in the decode stage, all computations are memory-bound, resulting in performance significantly below the computational capacity of the GPUs computation units. Moreover, offload would make small context prefill memory-bound. Head-wise, the roofline model performs the same arithmetic intensity and peak performance as standard FlashAttention. We also show the roofline analysis on other GPUs, such as the A100, to demonstrate the generality of this analysis. Figure 8 shows that the prefill 1K offload is also positioned on memory-bound while the prefill 10K offload is on the compute-bound. The details data is listed on table 13. D. Extension: HEADINFER Implementation with Head-wise Sparsity Figure 9 shows our memory management framework. HEADINFER, which enables offloading the head-wise KV cache with head-wise sparsity. The key design principle behind HEADINFER is to exploit the redundancy of CPU memory capacity to increase the context size after identifying the important heads in the KV cache. As such, most of the heads for the KV cache are kept in the CPU memory as we generate new tokens, not discarding them like previous work. However, we do not HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Table 13. Performance comparison of different attention mechanisms under A100 setting Offload Regular Arithmetic Intensity FLOPS Bound Memory (KV cache) Arithmetic Intensity FLOPS Bound Operator Prefill flashattention (1k) flashattention (10k) flashattention (100k) head-wise (1k) head-wise (10k) head-wise (100k) Decode flashattention (1k) flashattention (10k) flashattention (100k) head-wise (1k) head-wise (10k) head-wise (100k) Ops Memory 17G 1.7T 172T 2.1G 215G 21T 17M 168M 1.7G 2.1M 21M 210M 21M 209M 2.1G 2.6M 26M 262M 17M 168M 1.7G 2.1M 21M 210M 820 8200 82000 820 8200 82000 1 1 1 1 1 312T 312T 312T 312T 312T 312T 1.4T 1.4T 1.4T 1.4T 1.4T 1.4T compute compute compute compute compute compute memory memory memory memory memory memory 4.2M 42M 419M 0.5M 5.2M 52M 17M 168M 1.7G 2.1M 21M 210M 4100 41000 410000 4100 41000 410000 1 1 1 1 1 1 102T 312T 312T 102T 312T 312T 23G 23G 23G 23G 23G 23G memory compute compute memory compute compute memory memory memory memory memory memory Figure 8. Flashattention in the roofline plot analysis using A100 device setting. bring the entire KV cache to the GPU for attention but load and compute only the retrieval head of keys and values, leaving other non-retrieval ones staying on the GPU without offloading. To do so, we maintain the head-wise cache pool in the CPU memory and iteratively load the necessary data. In detail, we use the pre-trained attention input head to speculate the important retrieval head. The speculation is done by processing customized dataset and analyzing the output. This reduces the waste of PCIe bandwidth by only transferring retrieval heads critical for attention computation. In addition, although the data is offloaded to CPU memory, which is much cheaper and larger than GPU memory, we manage the KV cache pool size so as not to fully utilize the CPU memory. D.1. Design Principles Interleaved KV cache Updates Across GPU and CPU. The uneven memory consumption and low PCIe link utilization (studied in section 3) during different attention head generation provide an opportunity to exploit the idle GPU memory and PCIe link during the KV cache update phase. To exploit this opportunity, during the attention processing, head of the attention KV cache can be dynamically fetched on the GPU to compute the attention weight output in parallel while the CPU prefetches the next head. key requirement to generate the attention output for given attention head is to stage its parameters (p), query (q), key (k), and value (v), and the attention weight generation is scheduled for each head. In case the key (k) and value (v) of the head are not present on the GPU, the generation operation will trigger prefetch read from the CPU memory where the head is offloaded, causing I/O operations in the critical execution path of updates. By leveraging the fact that multiple head attention, such as MHA (Vaswani et al., 2017) and GQA (Ainslie et al., 2023), are HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Figure 9. Demonstrations of KV cache policies in inference from the head-wise view. Upper plots illustrate symbolic plots of an attention map deploying different policies in LLM generation. Lower: the overview of HEADINFER. embarrassingly parallel, and HEADINFER partitions the attention into smaller subgroups, we can perform fine-grained attention generation scheduling across both GPU and CPU without impacting the consistency of generation or introducing computational dependencies between different subgroups. Furthermore, interleaving does not incur memory allocation and deallocation overheads because on the GPU, memory allocation is handled by PyTorch through lightweight memory pools, and on the host, the memory for all subgroups (except static GPU subgroups) is already pre-allocated and pre-pinned (if enabled) during initialization. As illustrated in Figure 10, the attention is partitioned into 4 sub-heads, out of which the first head is statically placed on the GPU for the entire inference lifetime; the KV cache corresponding to the head resides in the GPU memory. Therefore, the interleaved offloading adopted by HEADINFER scheduled all the heads to be updated on the GPU, i.e., for every head updated on the GPU, the host-to-device transform for the next head and the device-to-host transform for the previous head would occur in non-blocking fashion. This interleaved offloading makes sure that only two heads are maintained on the GPU while most KV cache are offloaded on the CPU. This interleave-centric design allows for efficient overlap between GPU computations and asynchronous head-wise KV cache movement across the PCIe link, which we will detail next. Overlapping Head-wise KV cache Movement and GPU Compute The data movement observed when the state-of-the-art framework enabling layer-wise KV cache offload (e.g., Flexgen(Sheng et al., 2023) and Deepspeed(Aminabadi et al., 2022)) runs an inference process. We observe that after the updates to the KV cache generation to given layer are computed on the GPU, the updated KV cache is H2D transferred to the CPU to continue inference in the subsequent chunked-prefill and decoding. Only when all the KV cache of next later are transferred to the GPU can the subsequent iteration begin. Given the parallel nature of attention, the sub-heads can be updated and transferred out of order and do not impact the accuracy of the inference. On contract, using the existing offloading solutions can be slow with head-wise sparsity attributed to (a) the KV cache of all heads within one layer staying on the GPU, which would occupy large memory for long context, and (b) not all attention layers are discretized uniformly, which causes blocking H2D transfer of KV cache. To mitigate the aforementioned challenges, we propose head-centric design illustrated in Figure 10 for efficient offloading interleaving of H2D transfor and GPU compute. It works as follows: while the GPU computes the generation of the initial head (H 1), the KV cache corresponding to the next head (H 2), including key (k) and value (v), are being prefetched using asynchronous H2D transfers, thereby overlapping GPU computations with next-head prefetching. Meanwhile, the CPU update for previous 0 is being uploaded using asynchronous D2H transfers. After this, three operations happen in parallel: (1) H2D transfer of the next head and prefetching of the next KV cache to be updated on the GPU (2) Updating of the previous head with KV cache from GPU; and (3) GPU generation of current head outputs, thereby exploiting full-duplex D2H and H2D transfers and parallel GPU computations. 20 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Figure 10. Workflow of HEADINFER generating model with (n+1) layers and (j+1) attention heads. Figure 11. Demonstrations of adaptive head-wise offloading. The attention phase is executed in parallel fashion by multiple heads of the mechisan. Consequently, our proposed overlapping of GPU computations and PCIe transfers does not incur any blocking overheads, as the computation speed on the GPU is slower than the PCIe throughput to transfer subheads back and forth between the GPU and CPU using high-throughput PCIe. Efficient Management of KV cache State-of-the-art hybrid KV cache offloading solutions (e.g., FlexFLow) by default retain the layer-wise KV cache corresponding to the statically GPU-resident head (h1 to h4) on the GPU during the attention computation, and for the remainder of the layer, KV cache are offloaded to the host memory. We extend this design with head-wise KV cache management to incorporate head-wise sparsity. Head-wise sparsity divides attention heads into two categories: important retrieval heads retain all tokens, while unimportant The streaming head only retains the most recent token. Since the memory usage of the retrieval head will be much larger than that of the streaming head, especially under long context inference, HEADINFER will selectively offload the retrieval head to the CPU and keep the streaming head in the GPU. Adaptive selecting head-wise granularity. In transformer-based architectures, attention heads often operate in parallel. However, processing these heads individuallyor in small groupscan incur repeated kernel launches and excessive PCIe transfers, particularly if the context size for each head is small. This overhead can quickly dominate total inference time, undermining the benefits of parallelism. Adaptive head-wise offloading addresses these inefficiencies by merging multiple heads into single HeadGroup. As shown on Figure 11, with reduced number of HeadGroups (e.g., 2-HeadGroup or 4-HeadGroup), fewer offloading operations are required, lowering both the latency and the bandwidth usage on the CPUGPU boundary. Conceptually, this batching of heads takes advantage of the fact that many computations within attention heads are structurally similar, thus allowing shared memory transfers and kernel calls. Layer-offload can be viewed as the extreme case of adaptive head-wise offloading. Instead of grouping only fraction of heads at time, layer-offload groups every head in given layer into one offload operation. While it can drastically reduce overhead further, the trade-off 21 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading is less granularity and potentially higher intermediate memory requirements. In practice, the decision to use 2-head, 4-head, or full-layer offloading depends on available hardware resources, batch size, and the typical context length being processed. By carefully tuning the grouping strategyranging from small, flexible head groups to large, layer-wide groupsadaptive head-wise offloading makes it possible to significantly optimize inference time in transformer-based models, particularly in latency-sensitive scenarios involving small context. D.2. Extension Experiment with Head-wise Sparsity We evaluate our head-wise extension of HEADINFER using 50% sparsity, which means half of the attention heads are sparse, on Llama3-8B models and compare against HEADINFER. For the prefill and decoding latency, the extension achieves close to 2 speedup. The results of this evaluation are shown in Table 14. Table 14. Prefill 1M, Decoding with 1M KV cache performance comparison Latency HeadInfer HeadInfer+duoattention 50% sparsity Prefill 1M (s) Decoding with 1M KV cache(s) 6.51 3.28 2054 1152 E. Easy-to-use and Portable Implementation HEADINFER is designed for straightforward integration with existing inference or training frameworks, such as Hugging Face Transformers, requiring only minimal modifications. Below, we illustrate how one can adapt standard Llama attention module to enable head-wise KV offloading with minimal impact on the rest of the code. E.1. Overview of Required Modifications Our changes largely center around intercepting or replacing the attentions forward pass so that: Heads are processed individually or in groups, rather than as single large multi-head block. Key and value states can be transparently stored in, or fetched from, CPU memory (or other off-device storage) before each heads attention is computed. Asynchronous transfers are used where possible to overlap CPUGPU data movement with on-GPU computation. E.2. Annotated Code Snippet In the listing below, we demonstrate key functions that illustrate how to integrate HEADINFER into transformers-style codebase. These snippets show how the standard forward method is patched with head-wise logic. We also provide HeadwiseOffloadedCache, class that manages CPUGPU memory movement, and small helper function for simulating or preparing decoding with large context windows. The implementation primarily requires modifying the attention mechanism class in transformer models. The key modifications are: 1. Attention Class Modification: Update the forward pass of the attention mechanism to support head-wise offloading: f r d ( f , d t s , t v e =None ) : e o s =None , Listing 1. Modified LlamaAttention forward pass # g l e o m a n r a = f . o ( d t s ) s e = f . o ( d t s ) u a = f . o ( d t s ) 22 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading # Headw p e n t i = r a . p [ 0 ] num heads = f . num heads d = f . d # h f head s o s q y t = r a . view ( c z , 1 , num heads , d ) s e = s e . view ( c z , 1 , num heads , d ) u a = u a . view ( t i , 1 , num heads , d ) # c e head e d l t s = [ ] h i n range ( num heads ) : d = r a [ . . . , d , d = s e [ . . . , d , d = u a [ . . . , d , : ] : ] : ] # Update e , d = t v e . a ( d , d , c o e l d Cache ( t v e ) e x , d ) d p = p h a n n ( d , d , d , e o s , t v e ) p . e ( d p ) u o . ( p , dim = 1) 2. KV Cache Management: Implement the OffloadedCache class for head-wise cache management: Listing 2. HeadwiseOffloadedCache implementation s a s f d a : f i ( f , d u = 1) : f . c e = [ ] f . u c = [ ] f . d u = d u e . f h e = c . a . e ( ) f . c r = c . a . e ( ) u t ( f , s e , u a , , d ) : c c head , managing CPU l a r a c e w t h . a . e ( f . f h e ) : # f h t head e d f . f h d ( e x , d + 1 ) # Update r head h l ( f . c e ) <= e l . c e . e ( [ ] ) f . u c . e ( [ ] ) : i e ( f . c e [ e x ] ) <= d : f . c e [ e l . u c [ e x ] . e ( None ) ] . e ( None ) # r on r i d c v = f . h d c ( e l . c e [ e l . u c [ e i ] [ d ] = s e . ( i ) ] [ d ] = u a . ( i ) , d ) 23 HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading # c e u head e d f . c a ( e i , d 1 ) u e a , u a d r t e ( f , e x , d ) : n o s moves t head GPU e d e d >= f . num heads : u a k = f . c e [ e c = f . u c [ e x ] [ d ] ] [ d ] c e n None and h . i ! = c . i ( a ) : h = h . a ( b k = e ) h = h . a ( b k = e ) e t d ( f , e Moves v s head CPU e d e d < 0 : , d ) : t w t h . a . e ( f . c r ) : h = f . c e [ e c = f . u c [ e x ] [ d ] ] [ d ] c e n None and h . i == c . i ( a ) : h = h . cpu ( ) h = h . cpu ( ) Our implementation shows that these modifications can be made with minimal changes to existing model architectures while maintaining full compatibility with standard transformer frameworks huggingface. The complete implementation would be available at https://github.com/wdlctc/headinfer."
        }
    ],
    "affiliations": [
        "California Institute of Technology",
        "Carnegie Mellon University",
        "Microsoft",
        "Rutgers University",
        "University of Wisconsin-Madison"
    ]
}