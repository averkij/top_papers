{
    "paper_title": "OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data",
    "authors": [
        "Patrick Langer",
        "Thomas Kaar",
        "Max Rosenblattl",
        "Maxwell A. Xu",
        "Winnie Chow",
        "Martin Maritsch",
        "Aradhana Verma",
        "Brian Han",
        "Daniel Seung Kim",
        "Henry Chubb",
        "Scott Ceresnak",
        "Aydin Zahedivash",
        "Alexander Tarlochan Singh Sandhu",
        "Fatima Rodriguez",
        "Daniel McDuff",
        "Elgar Fleisch",
        "Oliver Aalami",
        "Filipe Barata",
        "Paul Schmiedmayer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLMs have emerged as powerful tools for interpreting multimodal data. In medicine, they hold particular promise for synthesizing large volumes of clinical information into actionable insights and digital health applications. Yet, a major limitation remains their inability to handle time series. To overcome this gap, we present OpenTSLM, a family of Time Series Language Models (TSLMs) created by integrating time series as a native modality to pretrained LLMs, enabling reasoning over multiple time series of any length. We investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt, models time series implicitly by concatenating learnable time series tokens with text tokens via soft prompting. Although parameter-efficient, we hypothesize that explicit time series modeling scales better and outperforms implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time series with text via cross-attention. We benchmark both variants against baselines that treat time series as text tokens or plots, across a suite of text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR, compared to 9.05 and 52.2 for finetuned text-only models. Notably, even 1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences, while maintaining stable memory requirements. By contrast, SoftPrompt grows exponentially in memory with sequence length, requiring around 110 GB compared to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA. To facilitate further research, we provide all code, datasets, and models open-source."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 0 1 4 2 0 . 0 1 5 2 : r OPENTSLM: TIME-SERIES LANGUAGE MODELS FOR REASONING OVER MULTIVARIATE MEDICAL TEXTAND TIME-SERIES DATA Patrick Langer1,2, Thomas Kaar1, Max Rosenblattl1, Maxwell A. Xu3,4, Winnie Chow5, Martin Maritsch6, Aradhana Verma8, Brian Han10, Daniel Seung Kim9, Henry Chubb10, Scott Ceresnak10, Aydin Zahedivash1, Alexander Tarlochan Singh Sandhu8, Fatima Rodriguez8, Daniel McDuff11,4, Elgar Fleisch2,7, Oliver Aalami1, Filipe Barata2,, Paul Schmiedmayer1, 1Stanford Mussallem Center for Biodesign, Stanford University 2Centre for Digital Health Interventions, ETH Zurich 3University of Illinois Urbana-Champaign 4Google Research 5Stanford University 6Amazon 7Centre for Digital Health Interventions, University of St. Gallen 8Division of Cardiovascular Medicine, Stanford University 9Division of Cardiology, University of Washington 10Pediatric Cardiology, Stanford University 11University of Washington Shared senior authorship planger@ethz.ch, tkaar@stanford.edu, max.rosenblattl@tum.de, maxu@illinois.edu, wychow@stanford.edu, maritsch@amazon.com, arverma@stanford.edu, brianhan@stanford.edu, seungkim@uw.edu mhchubb@stanford.edu, ceresnak@stanford.edu, ats114@stanford.edu frodrigu@stanford.edu, aydinz@stanford.edu, dmcduff@google.com aalami@stanford.edu, efleisch@ethz.ch, fbarata@ethz.ch schmiedmayer@stanford.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have emerged as powerful tools for interpreting multimodal data (e.g., images, audio, text), often surpassing specialized models. In medicine, they hold particular promise for synthesizing large volumes of clinical information into actionable insights and patient-facing digital health applications. Yet, major limitation remains their inability to handle time series data. To overcome this gap, we present OpenTSLM, family of Time-Series Language Models (TSLMs) created by integrating time series as native modality to pretrained LLMs, enabling natural-language prompting and reasoning over multiple time series of any length. We investigate two architectures that differ in how they model time series. The first, OpenTSLM-SoftPrompt, models time series implicitly by concatenating learnable time series tokens with text tokens via soft prompting. Although parameter-efficient, we hypothesize that explicit time series modeling scales better and outperforms implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time series with text via cross-attention. We benchmark both variants with LLaMa and Gemma backbones against baselines that treat time series as text tokens or plots, across suite of texttimeseries reasoning tasks. We introduce three time-series Chain-of-Thought (CoT) datasets: HAR-CoT (human activity recognition), Sleep-CoT (sleep staging), and ECG-QA-CoT (ECG question answering). Across all, OpenTSLM models consistently outperform baselines, reaching 69.9% F1 in sleep staging and 65.4% in HAR, compared to 9.05% and 52.2% for finetuned text-only models. Notably, even 1B-parameter OpenTSLM models surpass GPT-4o (15.47% and 2.95%). OpenTSLM-Flamingo matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences, while maintaining stable memory requirements. By contrast, SoftPrompt exhibits exponential memory growth with sequence length, requiring 110 GB compared to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by clinicians find strong reasoning capabilities and temporal understanding of raw sensor data exhibited by OpenTSLMs on ECG-QA. To facilitate further research, we provide all code, datasets, and models as open-source resources."
        },
        {
            "title": "INTRODUCTION",
            "content": "Medicine is inherently temporal: assessment, diagnosis, and treatment depend on how signs, symptoms, and biomarkers evolve over time Giannoula et al. (2018); Henly et al. (2011); Jørgensen et al. (2024). Clinical decision-making relies on temporal patternstracking vital signs, medication responses, laboratory values, and disease progression markers to guide diagnosis, prognosis, and therapeutic interventions. As time-series data from electronic health records and continuous monitoring proliferate Abernethy et al. (2022); Marra et al. (2024); Yeung et al. (2023), human-legible representations become essential for interpreting and managing this information Olex & Mcinnes (2021); Senathirajah et al. (2020); Zhou et al. (2008). Clinical summaries must translate complex temporal patternshemodynamic instability, biomarker trajectories, and treatment responsesinto interpretable assessments that support evidence-based decision-making and care coordination. Recent advances in multimodal large language models (LLMs) allow users to interpret complex data through natural language, synthesizing information across text, images, audio, and video Wu et al. (2023); AlSaad et al. (2024). However, reasoning over longitudinal time series data remains critical blind spot among currently supported modalities. Prior work has attempted to integrate time-series as plain text tokens Gruver et al. (2023); Kim et al. (2024); Liu et al. (2023); however results have been limited Merrill et al. (2024). Other approaches reprogram LLMs to act as feature extractors for classification heads, which then output fixed set of classes or values, thereby losing text-generation capabilities Li et al. (2025); Nie et al. (2023); Pillai et al. (2025); Ye et al. (2025). More recently, soft prompting has been explored, concatenating learnable time-series tokens with text tokens to preserve generation Chow et al. (2024). Yet, longer series may require more tokens, increasing context length Gotz et al. (2025); Nie et al. (2023) and compute due to the quadratic cost of self-attention Nie et al. (2023); Vaswani et al. (2017). To overcome prior limitations, we propose Time-Series Language Models (TSLMs), which integrate time series as native modality in LLMs. TSLMs provide natural interface to complex medical data, enabling clinicians and patients to query, interpret, and reason about longitudinal health information directly through natural language. We introduce OpenTSLM, family of TSLMs built by extending pretrained LLMs with time-series inputs. central design question in building TSLMs is how to represent time-series signals. Prior work has primarily used soft prompting, encoding time series as learned token embeddings concatenated with text tokens. While lightweight, this captures temporal dependencies only implicitly, as additional tokens in the context, and may scale poorly to longer or multiple sequences. We hypothesize that explicit multimodal fusion via cross-attention may be more effective for modeling temporal structure. To compare both approaches, we explore two variants for OpenTSLM. The first, OpenTSLM-SoftPrompt, models time series implicitly by encoding the time series into tokens and concatenating them with text tokens via soft prompting, so the model processes both as single sequence without distinguishing between them. The second, OpenTSLM-Flamingo, by contrast, models time series explicitly as separate modality, using cross-attention mechanism inspired by Flamingo Alayrac et al. (2022) to fuse time-series and text. We created OpenTSLM-SoftPrompt and OpenTSLM-Flamingo using Llama Touvron et al. (2023) and Gemma GemmaTeam et al. (2024) backbones. We benchmark these models against each other and against baselines including LLMs with tokenized time-series inputs Gruver et al. (2023), fine-tuned tokenized time-series models, and vision-based approaches. Unlike prior classificationbased approaches, our models are trained in text-based reasoning tasks, generating chain of thought (CoT) rationales before producing predictions. For training and evaluation, we introduce three new datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. To foster reproducibility and further research on TSLMs, we release OpenTSLM as an open-source framework, including models and datasets1."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Creating Time-Series Language Models remains an open research challenge. The main barrier is the modality gap between continuous signals and discrete text representations Chow et al. (2024); Pillai et al. (2025); Zhang et al. (2025). Prior work has proposed three main strategies to bridge this gap, as summarized by Zhang et al. (2024): tokenizing time series as text (Section 2.1), applying soft prompting (Section 2.2), and using cross-attention mechanisms (Section 2.3). Table 1 provides an overview of relevant methods. 1https://github.com/StanfordBDHG/OpenTSLM 2 Table 1: Methods combining time-series data with LLMs. Name Method Task Text Gen. Multi-Sensor Raw Data SFT FSHLLiu et al. (2023) Gruver et al. (2023) HealthLLM Kim et al. (2024) Token Token Token Chow et al. (2024) MedualTime Ye et al. (2025) SensorLLM Li et al. (2025) Time2Lang Pillai et al. (2025) OpenTSLM-SP (ours) Soft Prom. Soft Prom. Soft Prom. Soft Prom. Soft Prom. SensorLM Zhang et al. (2025) OpenTSLM-Flamingo (ours) Cross.Attn. Cross.Attn. CL FC TR TR CL CL CL TR CL TR CL =Classification, FC =Forecasting, TR =Text Reasoning"
        },
        {
            "title": "2.1 TOKENIZATION OF TIME SERIES AS TEXT INPUTS\nGruver et al. has demonstrated that LLMs can perform time series forecasting by encoding values\nas text tokens and predicting future values without domain-specific tuning Gruver et al. (2023).Liu\net al. (2023) tokenize data from wearables and smartphones to enable LLMs to infer clinical and\nwellness information through few-shot prompting. Similarly, Kim et al. (2024) propose HealthLLM,\na framework for health prediction using physiological signals (e.g., heart rate, sleep) combined with\nuser context and medical knowledge embedded in prompts.",
            "content": "2.2 COMBINING TEXT AND TIME SERIES TOKEN EMBEDDINGS (SOFT PROMPTING) An alternative to manual tokenization is to encode time series into embeddings that capture time series information, using time series encoder as presented by Nie et al. (2023). These embeddings can be input into transformer directly or concatenated with text embeddings (softprompting) Chow et al. (2024); Nie et al. (2023); Pillai et al. (2025); Ye et al. (2025). Pillai et al. (2025) use this approach and train an encoder to produce soft prompts from time series, which are then processed by frozen LLM for classification via projection head; however this disables free-form text generation. Ye et al. (2025) similarly combines time series and text-token embeddings, using classification head for prediction. Li et al. (2025) integrate sensor and text embeddings in two stages: First, they generate caption-like summary of the time series for free-form output; Second, they classify the data via projection head, therefore restricting free from output. Chow et al. (2024) interleave time series tokens with text tokens in the LLM input, enabling free-form text reasoning. 2.3 CROSS-ATTENTION FOR TIME-SERIES DATA Few studies use cross-attention to integrate time series into LLMs. Zhang et al. (2025) apply crossattention between time series encoder and text encoder, aligned with contrastive loss, to extract statistical summaries (e.g., mean, max) from single sensor. They train new sensor encoder, text encoder, and multimodal text decoder, rather than adapting pretrained LLM Zhang et al. (2025)."
        },
        {
            "title": "3 METHODS",
            "content": "We present two architectures for TSLMs, OpenTSLM-Soft Prompting (SP) (Section 3.2 and OpenTSLM-Flamingo (Section 3.3). To support multiple time-series inputs, we design prompt format that interleaves sensor data with accompanying textual descriptions (e.g., Data from Sensor over days: followed by the data representation). Figure 1 illustrates our approach. 3.1 TIME-SERIES ENCODER Both OpenTSLM architectures use time series encoder inspired by Nie et al. (2023). It consists of Patchencoder, followed by either TransformerEncoder for OpenTSLM-SP or PerceiverResampler for OpenTSLM-Flamingo (inspired by Alayrac et al. (2022); Awadalla et al. (2023)). We divide an input time series RL into non-overlapping patches of size p, yielding = L/p patches. Each patch is then transformed into an embedding vector using 1D convolution and added with positional encoding Nie et al. (2023) Patch Embedding: Ei = Conv1D(xip:(i+1)p) Rdenc + Pi (1) 3 Figure 1: Overview of TextTime-Series LLMs with support for multiple time-series inputs. where the convolution has kernel size and stride equal to p, mapping each patch to denc-dimensional embedding. Pi is the learnable positional encoding. The sequence of position-augmented embeddings is then processed by the specific Encoder (cf. Sections 3.2 and 3.3). Preserving scale and temporal information The PatchEncoder expects inputs normalized to [1, 1]. Since raw time series differ in scale and resolution across modalities depending on the sensor, we preserve scale and temporal context by adding the original mean, standard deviation, and time scale to the textual description. For example: This is heart-rate data over 24 hours sampled at 50 Hz with mean=61 and std=12. 3.2 SOFT-PROMPTING ARCHITECTURE (OPENTSLM-SP) OpenTSLM-SP has three components: (1) time series encoder that transforms raw data into patch embeddings, (2) projection layer mapping embeddings to the LLM hidden space, (3) pretrained LLM, fine-tuned using LoRA adapters Hu et al. (2021) Figure 2 illustrates the architecture. Figure 2: Architecture of OpenTSLM-SoftPrompt Projecting Time-Series Tokens to Text Tokens We apply the patch embeddings to transformer encoder and subsequently project the resulting tokens with an multi-layer perceptron (MLP) to align them with the embedding space of dimension dllm, corresponding to the hidden size of the language model, following Nie et al. (2023) and Chow et al. (2024). where RN dllm denotes the projected time-series tokens in the LLM embedding space. = MLP(TransformerEncoder(E1:N )) RN dllm (2) Text-Time-Series integration via Soft Prompting We interleave any number of text and timeseries tokens through soft prompting mechanism. typical prompt consists of (1) an initial text segment (pre-prompt), (2) sequence of interleaved time-series tokens and textual descriptions, and (3) final text segment (post-prompt), often question. Formally, the model input is: 4 Xinput = [Tpre, Z1, Tdesc1 , Z2, Tdesc2, . . . , ZK, TdescK , Tpost] where Tpre, Tdesci, and Tpost are token embeddings of text segments, and each Zi is projected time-series embedding aligned with the LLM hidden space. We refer to each (Zi, Tdesci) as texttime-series chunk. This approach implicitly integrates time series through learned tokens. (3)"
        },
        {
            "title": "3.3 CROSS-ATTENTION ARCHITECTURE (OPENTSLM-FLAMINGO)",
            "content": "OpenTSLM-Flamingo is inspired by the Flamingo model for visionlanguage tasks Alayrac et al. (2022); Awadalla et al. (2023). Following OpenFlamingo Awadalla et al. (2023), we extend pretrained LLMs with cross-attention layers to support time-series reasoning. Architecture Overview We replace the vision encoder of Flamingo with time series encoder and adapt the cross-attention mechanism for temporal data. The model consists of: (1) time series patch encoder, (2) Perceiver Resampler, (3) gated cross-attention layers integrated into the LLM, and (4) the frozen language model backbone. Figure 3 visualizes the architecture. Figure 3: Architecture of OpenTSLM-Flamingo PerceiverResampler We use PerceiverResampler inspired by Flamingo Awadalla et al. (2023) as Encoder for the time series patches, yielding fixed-size latent representation: Zlatent = PerceiverResampler(E1:N ) RNlatentdtime, (4) Here, dtime is the dimensionality of the time-series features by the perceiver, in our case (N, 1), encoding one time series with one channel at time. Text-Time-Series Gated Cross-Attention To integrate Zlatent into the LLM, we add gated crossattention layers every (hyperparameter) transformer blocks which compute: Qtext = xWQ, Kts = ZlatentWK, Vts = ZlatentWV GatedCrossAttention(x, Zlatent) = + γ softmax (cid:19) (cid:18) QtextKT ts dk Vts. (5) (6) where γattn is learnable parameter controlling the influence of the time-series, RT dmodel, the LLM input, WQ, WK, WV Rdmodeldk learned projection matrices, and dk the key dimension. Conditioning Text-tokens on Time-Series via Special Tokens The LLM processes tokens autoregressively, attending to previous inputs. Following OpenFlamingo Awadalla et al. (2023), we introduce special tokens TS and endofchunk to indicate when time series modalities should be incorporated. Upon encountering TS, the model conditions on the corresponding latent representation Zlatent via gated cross-attention. typical input prompt is Xinput = [pre prompt, TS, ts desc1, endofchunk, TS, ts desc2, endofchunk, post prompt] (7) where TS triggers multimodal conditioning and endofchunk signals the end of text describing time series. This setup enables interleaving multiple text and time series segments Awadalla et al. (2023). The embeddings of the special tokens are learned during training."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In the following, we outline our training methodology and report results on multiple-choice Time Series Question Answering (TSQA) and time-series reasoning datasets. We compare OpenTSLMSoftPrompt and OpenTSLM-Flamingo against each other and baselines in terms of performance, and report video random access memory (VRAM) requirements for training OpenTSLM. We present sample model outputs across datasets and an evaluation for ECG rationales by medical doctors."
        },
        {
            "title": "4.1 MULTI-STAGE CURRICULUM LEARNING – TEACHING LLMS TIME SERIES",
            "content": "Following Chow et al. (2024), we adopt two-stage curriculum to train TSLMs. (encoder warmup), we use two synthetic time-series datasets to pretrain the encoder:"
        },
        {
            "title": "In stage one",
            "content": "TSQA Wang et al. (2024) Multiple-choice time-series question answering on synthetic data for learning simple temporal patterns (e.g., ascending/descending trends). Time-Series Captioning (M4-Captions) We generate pseudo-labeled captions using ChatGPT, prompted with plots of time series of the M4 dataset Makridakis et al. (2020) (see Section A.5.1). In stage two, we introduce three new CoT time-series datasets covering human activity recognition (HAR), sleep staging, and electrocardiogram (ECG) Question Answering (QA). We generated these using GPT-4o by providing plot and ground-truth answer for each sample, then asking the model to produce rationales leading to the correct response. Further details are provided in Section A.2. HAR-CoT three-axis accelerometer data combined from DaLiAc Leutheuser et al. (2013), DOMINO Arrotta et al. (2023), HHAR Stisen et al. (2015), PAMAP2 Reiss & Stricker (2012), RealWorld Sztyler & Stuckenschmidt (2016), and datasets from Shoaib et al. (2013; 2014; 2016). Sampled at 50 Hz, split into 2.56s windows, 8 activities: sitting, standing, lying, walking, running, biking, walking upstairs, walking downstairs. See Section A.2.1 for detailed description. Sleep-CoT Based on SleepEDF Kemp et al. (2000); Goldberger et al. (2000), using 30s electroencephalogram (EEG) segments for sleep staging. Following prior work Chow et al. (2024); Pouliou et al. (2025), Non-rapid eye movement (REM) stages 3 and 4 are merged, yielding five classes: Wake, REM, Non-REM1, Non-REM2, Non-REM3. See Section A.2.2 for details. ECG-QA-CoT Based on ECG-QA Oh et al. (2023), which provides 12-lead 10s ECGs and clinical context, we excluded comparison questions, retaining 42/70 templates. This yielded 3,138 unique questions across 240k samples (see Section A.2.3). All datasets are split into 80/10/10 train/validation/test sets. Table 3 in Section A.1 summarizes number of samples in the datasets, number of time series and lengths. Training objective In all stages, we frame the task as an autoregressive language modeling problem. During training and evaluation, the model is prompted to generate outputs in structured format, consisting of free-form rationale followed by the final prediction: <reasoning> Answer: <final answer>. Formally, the loss is defined by Equation 8, where Zts are the LLM = (cid:88) t=1 log (yt y<t, x1:t, Zts; Θ) (8) time-series features, and Θ the learnable weights, i.e., the TimeSeriesEncoder, MLP, and LoRA in OpenTSLM-SoftPrompt, and TimeSeriesEncoder and cross-attention in OpenTSLM-Flamingo. 4.2 BASELINES We compare OpenTSLM against three baselines using the same open-weight LLMs, i.e., Llama3.2(1B, 3B) and Gemma3 (270M, 1B-PT), and additionally GPT-4o (gpt-4o-2024-08-06). 1. Tokenized time-series: Using the open-source code provided by Gruver et al. (2023), we tokenize time series into text inputs and report zero-shot performance on the test set. 2. Tokenized finetuned: Same as 1. (excluding GPT-4o), but finetuned with LoRA Hu et al. (2021) on the training set. We choose best model by validation loss, and report performance on test set. 3. Image (Plot): We convert time series into plots and provide them as input to GPT-4o and Gemma-4b-pt (since the smaller Gemma 3 variants do not support image input). 4. Random baseline: For comparison, we report the expected performance of predictor that selects labels uniformly at random, adjusted to each datasets label distribution."
        },
        {
            "title": "4.3 QUANTITATIVE RESULTS ON TIME-SERIES CLASSIFICATION",
            "content": "We present performance on the test splits of TSQA, HAR-CoT, Sleep-CoT, and ECG-QA-CoT and report macro-F1 score and accuracy in Table 2. OpenTSLM models achieve the highest performance Table 2: Performance comparison on time series question answering (TSQA) and time series reasoning (HAR-CoT, Sleep-CoT, ECG-QA-CoT) tasks between OpenTSLM models and baselines. Method Model TSQA Random Baseline 33.33 i o i - i i o n n e I ) P ( T O T O m t o m 16.01 Llama3.2-1B Llama3.2-3B 16.24 Gemma3-270M 10.52 11.76 Gemma3-1B-pt 45.32 GPT-4o 83.74 Llama3.2-1B Llama3.2-3B 84.54 Gemma3-270M 68.05 82.85 Gemma3-1B-pt Gemma3-4B-pt GPT-4o 48.77 59.24 97.50 Llama3.2-1B Llama3.2-3B 97.37 Gemma3-270M 40.32 87.29 Gemma3-1B-pt 94.08 Llama3.2-1B Llama3.2-3B 90.14 Gemma3-270M 77.86 92.56 Gemma3-1B-pt Acc 33.33 31.04 32.06 9.58 12.92 45. 81.40 82.06 65.40 83.42 50.60 62.10 97.54 97.33 26.79 89.18 94.00 90.10 78.12 92.46 HAR-CoT Acc 11.49 0.00*1 0.00 0.00 0.00 2.95 51.28 60.44 40.66 52.15 1.72 10.83 65.44 64.87 1.43 40.52 62.93 62.77 57.75 65. 12.50 0.00 0.00 0.00 0.00 11.74 62.71 66.87 54.56 63.90 0.89 13.90 71.48 67.89 0.55 45.17 69.27 69.03 63.43 71. Sleep-CoT Acc 20.00 0.65 12.15 0.00 0.00 16.02 24.19 14.30 0.00 0.00 14.95 10.75 81.08 72.04 5.91 36. 67.31 69.14 68.49 60.67 F1 17.48 2.14 5.66 0.00 0.00 15.47 9.05 5.86 0.00 0.00 6.75 4. 69.88 54.40 7.96 30.99 49.33 45.45 51.38 43.69 ECG-QA-CoT F1 16.47 0.00 0.00 0.00 0.00 18. OOM*2 OOM OOM OOM 1.90 24.95 32.84 33.67 1.29 27.86 34.62 40.25 32.71 35.31 Acc 20. 0.00 0.00 0.00 0.00 28.76 OOM OOM OOM OOM 1.03 33.30 35.49 36.25 1.11 34.76 38.14 46.25 35.50 37.79 Note: Gemma models have smaller context than Llama (32k vs. 128k); softprompt uses up context, performing worse. *10.00 model failed to produce Answer: {answer} template, often repeating input prompt (see Section A.4).*2OOM - Out of memory: 12 ECG leads of 10s tokenize to 80k tokens, requiring >100GB VRAM. across benchmarks, while most tokenized text-only baselines fail to produce valid outputs, not answering in the expected template but merely repeating inputs or starting to count (see Section A.4), resulting in 0.00 F1 on HAR for all models except for GPT-4o (2.95). GPT-4o yields only 2.95 F1 with text but improves substantially with plots (e.g., 10.83 on HAR, 59.24 on TSQA). Gemma34b similarly achieves better results TSQA and Sleep-CoT (48.77 and 6.75). Llama models achieve 2.14 and 5.65F1 on Sleep, respectively, while Gemma models again achieve 0.00, likely due to their smaller context window (32k vs. 128k). By contrast, OpenTSLMSoftPrompt with Llama3.2-1B attains 97.50 F1 score (97.54 accuracy) on TSQA, with Llama3.2-3B at 97.37 (97.33); Flamingo variants are close (e.g., Llama3.2-1B 94.08 (94.00)), while the strongest tokenized-finetuned baseline reaches 84.54 (82.06) and GPT-4o with image inputs at 59.24 (62.10). On HAR-CoT, the strongest results are 65.44F1 (71.48 accuracy) for OpenTSLMSoftPrompt (Llama3.2-1B) and 65.44 (71.48) for OpenTSLMFlamingo (Gemma3-1B-pt); the best tokenized-finetuned baseline records 60.44 (66.87). On Sleep-CoT, OpenTSLMSoftPrompt (Llama3.2-1B) achieves 69.88 (81.08), followed by OpenTSLMSoftPrompt (Llama3.2-3B) at 54.40 (72.04) and Flamingo (Gemma3-270M) at 51.38 (68.49); tokenized-finetuned baselines remain lower (best 9.05 (24.19)). On ECG-QA-CoT, OpenTSLMFlamingo (Llama3.2-3B) leads with 40.25 (46.25). 4.4 EVALUATION OF MEMORY USE DURING TRAINING We evaluate peak VRAM usage during training for both OpenTSLM variants. Figure 4 summarizes peak VRAM on TSQA, HARCoT, SleepEDFCoT, and ECGQACoT. OpenTSLM-Flamingo shows near-constant memory across datasets: Llama-3.2-1B requires around 2022 GB and Llama3.2-3B around 6172 GB; Gemma-3-270M is 5.77.3 GB and Gemma-3-1B-pt 15.618.4 GB. In contrast, OpenTSLM-SoftPrompt vary substantially with the dataset: Llama-3.2-1B requires from 4.4 GB (TSQA) up to 64.9 GB (ECGQACoT), and for Llama-3.2-3B from 8.1 GB to 87.1 GB; Gemma-3-270M spans 2.424.1 GB and Gemma-3-1B-pt 5.132.7 GB. 7 Figure 4: VRAM memory usage in training across datasets. To further investigate memory scaling, we train models on simulated dataset (see Section A.7.2) with random inputs of shape (N L), where is the number of time series processed concurrently and the sequence length. We report max VRAM usage in Figure 5 (exact values are available in Table 10). Figure 5: VRAM usage vs. total time-series size (number of series length) VRAM for OpenTSLM-Flamingo effectively stays constant as increases from 1 to 5 and from 10 to 10,000 (e.g., Llama-1B 20.421.0 GB; Llama-3B 60.761.1 GB; Gemma-270M 5.76.4 GB; Gemma-1B 15.415.6 GB). By contrast, SoftPrompt scales with both and (see Figure 5 in Section A.7.2): for Llama-1B, VRAM rises from 2.6 GB at L=10, =1 to 29.5 GB at L=10,000, =1 and exceeds memory at L=10,000, 3; Llama-3B shows similar pattern (6.3 GB 42.7 GB at =1, OOM by 3). Gemma-270M and Gemma-1B reach up to 118 GB and 165 GB, respectively, at L=10,000, =5. 4.5 QUALITATIVE RESULTS AND EXPERT EVALUATION OF ECG RATIONALES Both OpenTSLM variants remain text models, trained to generate rationales for classification rather than outputting only class label. Figure 6a shows example rationales for human activity recognition, Figure 6b for ECG-QA, and Figure 6c for sleep staging. Figure 6d shows time series captioning on M4 data. To evaluate the quality of model rationales, we conducted an expert review with five cardiologists from Stanford Hospital on rationales generated by OpenTSLM-Flamingo-Llama3.2-3B (best model) for ECG-QA. We randomly sampled two examples per template (84 total), each reviewed by at least two cardiologists. Evaluation followed rubric derived from the American College of Cardiology/American Heart Association Clinical Competence Statement on ECGs Pangaro (1999); Committee Members et al. (2001) and based on the RIME (ReporterInterpreterManagerEducator) framework Pangaro (1999) (see A.6), assessing whether the model: (1) correctly identified relevant ECG features; (2) appropriately connected them to the final answer; (3) incorporated patient context (age, artifacts, ...). Overall, the model gave correct or partially correct ECG interpretation in 92.9% of cases, spanning ECG recognition, reasoning, and contextualization. OpenTSLM showed strongest performance in clinical context integration (85.1% positive) compared to ECG pattern recognition (65.5% positive) and clinical reasoning (62.5% positive) (Figure 7a). Assessment patterns varied notably across reviewers, with some reviewers consistently more favorable across all evaluation areas (Figure 7b). Reviewer disagreement was most common for clinical reasoning, where moderate disagreements were observed between adjacent assessment categories. Complete disagreements between positive and negative assessments were relatively rare across all areas (Figure 17 in Appendix A.6). 8 (a) Human Activity Recognition CoT (b) ECG Interpretation CoT (c) Sleep Stage Detection CoT (d) M4 Time Series Captioning Figure 6: Example CoT rationales for HAR, Sleep Staging, ECG-QA and M4 captioning, generated with OpenTSLM-Flamingo/Llama3.2-1B. More examples are provided in Section A.5. (a) Performance by Area (b) Eval Distribution Figure 7: Qualitative evaluation of CoT rationales and inter-reviewer agreement patterns."
        },
        {
            "title": "5 DISCUSSION",
            "content": "All OpenTSLM models consistently outperform baselines. Text-only models often fail to follow the answer template and thus perform at or below chance (Section 4.1). Finetuned baselines improve substantially on HAR-CoT (60.44% F1 vs. 0% for Llama-3.2-1B) but only slightly on Sleep-CoT (9.05 vs. 2.14). ECG-QA finetuning was infeasible due to high VRAM demands (80k tokens require >100GB per sample). OpenTSLM-SoftPrompt performs best on shorter sequences (Sleep-CoT, TSQA) but becomes impractical as VRAM requirements grow with sequence length (>180GB in simulations with 10,000-length series). With softprompting, smaller models like Gemma-3 270M 9 and 1B quickly exhaust their context and underperform. In contrast, OpenTSLM-Flamingo sustains stable memory across sequence lengths and series (up to 60GB for Llama-3.2-3B with five 10,000-length series). This allows even tiny models, such as Gemma-270M, to deliver strong results, highlighting the efficiency of cross-attention for treating time series as native modality. Practical implications. Our results show that even frontier LLMs like GPT-4o are poorly suited for time-series reasoning and that time series must be treated as distinct modality. With OpenTSLM, even small models like Gemma3 270M outperform GPT-4o (200B parameters Abacha et al. (2025)) at fraction of the compute and cost, enabling efficient on-device or mobile deployment. OpenTSLM-SoftPrompt is preferable for short time series, requiring only few additional parameters for finetuning, but scales exponentially in memory with sequence length, making it impractical for long or multi-series inputs. In contrast, OpenTSLM-Flamingo maintains nearly constant memory across longer or multiple series, performs better on complex datasets, and should therefore be considered the general-purpose option for TSLMs. Perhaps the greatest advantage of TSLMs is the interface they provide for contextualizing results. In ECG-QA, OpenTSLM correctly identified the relevant ECG features in most cases, with missing context only 7.1% of the time. The model demonstrated particularly strong clinical context integration (85.1% positive assessments), thereby offering clinicians and researchers transparent window into the models reasoning. As trust is important in medicine, this transparency underscores the value of applying LLMs to time series. Comparison with prior work. Our approach differs from prior work in several ways. First, we introduce time series as new modality for LLMs, unlike Sivarajkumar & Wang (2023) and Kim et al. (2024), which tokenize time series. Second, we frame tasks as joint texttime-series reasoning, training models to generate rationales that integrate temporal information. This contrasts with MedualTime Ye et al. (2025) and Time2Lang Pillai et al. (2025), which reprogrammed LLMs with fixed classification or forecasting heads, removing language generation capabilities. Notably, OpenTSLM achieves 40.25 F1 on ECG-QA-CoT, producing rationales across 3,138 questions and 42 templates with diverse answer options. By comparison, Ye et al. report 76 F1 on PTB-XL (underlying dataset of ECG-QA) with only four classes and fixed classification head Ye et al. (2025). Third, unlike SensorLM Zhang et al. (2025), which is trained from scratch, our models build on pretrained open-weight LLMs, retaining pretrained knowledge. Fourth, while prior work used soft prompting Chow et al. (2024) to model time series implicitly, we find it scales poorly, whereas our OpenTSLM-Flamingo models them explicitly via cross-attention, scaling to long sequences. Limitations. We acknowledge several limitations. First, our method of encoding time series may not be optimal, as we rely on including mean and standard deviation in accompanying texts to preserve temporal scale. Second, we generated CoT datasets using GPT-4o on plots, which we have shown to perform poorly on these plots alone. Curated datasets likely lead to better rationales. Third, framing tasks as natural language generation does not ensure that the model prioritizes the correct label, underscoring the need for loss functions that explicitly enforce correct answers. Fourth, we did not conduct ablation studies; for example, although OpenTSLM-Flamingo introduces gated cross-attention layers between every two transformer blocks, comparable performance might be achievable with fewer. Finally, while we report strong results on individual datasets, we have not yet demonstrated generalization to unseen data, an essential step toward general TSLMs."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Our results show that both OpenTSLM variants enable small-scale LLMs to outperform much larger text-only models on time-series tasks, demonstrating that lightweight, domain-adapted architectures can achieve strong performance without massive model scales. With OpenTSLM, we extend openweight pretrained LLMs to process time series retaining knowledge while adapting them to temporal domains. This work may lay the foundation for general-purpose TSLMs capable of handling diverse time-series datasets. Although our focus is healthcare, the ability to reason over longitudinal data has broad relevance in domains such as finance, supply chain management, and industrial monitoring. REPRODUCIBILITY STATEMENT All source code associated with this work is publicly available. All external datasets used are open source, and any datasets generated by us have also been released as open source. We additionally release all trained model weights. We also provide the notebooks annotated by clinical doctors for rationale generation on the ECG-QA dataset. These resources ensure full reproducibility of our results. 10 USE OF LARGE LANGUAGE MODELS Large Language Models (LLMs) were partially used for text editing, in limited instances, to improve the grammar and clarity of the original text. LLMs were additionally used for reviewing parts of the source code to identify critical errors or bugs. No LLMs were used for data analysis, experimental design, or drawing scientific conclusions."
        },
        {
            "title": "REFERENCES",
            "content": "Asma Ben Abacha, Wen wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, and Thomas Lin. Medec: benchmark for medical error detection and correction in clinical notes, 2025. URL https://arxiv.org/abs/2412.19260. Amy Abernethy, Laura Adams, Meredith Barrett, Christine Bechtel, Patricia Brennan, Atul Butte, Judith Faulkner, Elaine Fontaine, Stephen Friedhoff, John Halamka, Michael Howell, Kevin Johnson, Peter Long, Deven McGraw, Redonda Miller, Peter Lee, Jonathan Perlin, Donald Rucker, Lewis Sandy, and Kristen Valdes. The promise of digital health: Then, now, and the future. NAM Perspectives, 6, 06 2022. doi: 10.31478/202206e. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning, 2022. URL https://arxiv.org/abs/2204.14198. Rawan AlSaad, Alaa Abd-alrazaq, Sabri Boughorbel, Arfan Ahmed, Max-Antoine Renault, Rafat Damseh, and Javaid Sheikh. Multimodal large language models in health care: Applications, challenges, and future outlook. Med Internet Res, 26:e59505, Sep 2024. ISSN 1438-8871. doi: 10.2196/59505. URL https://doi.org/10.2196/59505. Luca Arrotta, Gabriele Civitarese, Riccardo Presotto, and Claudio Bettini. Domino: dataset for context-aware human activity recognition using mobile devices. In 2023 24th IEEE International Conference on Mobile Data Management (MDM), pp. 346351, 2023. doi: 10.1109/MDM58254. 2023.00063. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An opensource framework for training large autoregressive vision-language models, 2023. URL https: //arxiv.org/abs/2308.01390. Winnie Chow, Lauren Gardiner, Haraldur T. Hallgrımsson, Maxwell A. Xu, and Shirley You Ren. Towards time series reasoning with llms, 2024. URL https://arxiv.org/abs/2409. 11376. Committee Members, Alan H. Kadish, Alfred E. Buxton, Harold L. Kennedy, Bradley P. Knight, Jay W. Mason, Claudio D. Schuger, Cynthia M. Tracy, William L. Winters, Alan W. Boone, Michael Elnicki, John W. Hirshfeld, Beverly H. Lorell, George P. Rodgers, Cynthia M. Tracy, and Howard H. Weitz. Acc/aha clinical competence statement on electrocardiography and ambulatory electrocardiography. Circulation, 104(25):31693178, 2001. doi: 10.1161/circ.104.25.3169. URL https://www.ahajournals.org/doi/abs/10.1161/circ.104.25.3169. GemmaTeam, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Leonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amelie Heliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, 11 Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clement Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024. URL https://arxiv.org/abs/2403.08295. Alexia Giannoula, Alba Gutierrez-Sacristan, `Alex Bravo, Ferran Sanz, and Laura Furlong. Identifying temporal patterns in patient disease trajectories using dynamic time warping: populationbased study. Scientific Reports, 8, 03 2018. doi: 10.1038/s41598-018-22578-1. Ary Goldberger, Luıs Amaral, L. Glass, Shlomo Havlin, J. Hausdorg, Plamen Ivanov, R. Mark, J. Mietus, G. Moody, Chung-Kang Peng, H. Stanley, and Physiotoolkit Physiobank. Components of new research resource for complex physiologic signals. PhysioNet, 101, 01 2000. Leon Gotz, Marcel Kollovieh, Stephan Gunnemann, and Leo Schwinn. Byte pair encoding for efficient time series forecasting, 05 2025. Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. Susan Henly, Jean Wyman, and Mary Findorff. Health and illness over time the trajectory perspective in nursing science. Nursing research, 60:S514, 05 2011. doi: 10.1097/NNR. 0b013e318216dfd3. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https: //arxiv.org/abs/2106.09685. Isabella Jørgensen, Amalie Haue, Davide Placido, Jessica Hjaltelin, and Søren Brunak. Disease trajectories from healthcare data: Methodologies, key results, and future perspectives. doi: 10.1146/ annurev-biodatasci-110123-041001. Annual review of biomedical data science, 7:251276, 08 2024. Bob Kemp, Aeilko H. Zwinderman, Bert Tuk, Hilbert A. C. Kamphuisen, and Josefien J. L. Oberye. Analysis of sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the eeg. IEEE Transactions on Biomedical Engineering, 47:11851194, 2000. URL https://api. semanticscholar.org/CorpusID:837298. Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. Health-llm: Large language models for health prediction via wearable sensor data. In Tom Pollard, Edward Choi, Pankhuri Singhal, Michael Hughes, Elena Sizikova, Bobak Mortazavi, Irene Chen, Fei Wang, Tasmie Sarker, Matthew McDermott, and Marzyeh Ghassemi (eds.), Proceedings of the fifth Conference on Health, Inference, and Learning, volume 248 of Proceedings of Machine Learning Research, pp. 522539. PMLR, 2728 Jun 2024. URL https://proceedings.mlr.press/ v248/kim24b.html. Heike Leutheuser, Dominik Schuldhaus, and Bjoern Eskofier. Hierarchical, multi-sensor based classification of daily life activities: Comparison with state-of-the-art algorithms using benchmark dataset. PloS one, 8:e75196, 10 2013. doi: 10.1371/journal.pone.0075196. Zechen Li, Shohreh Deldari, Linyao Chen, Hao Xue, and Flora D. Salim. Sensorllm: Humanintuitive alignment of multivariate sensor data with llms for activity recognition, 2025. URL https://arxiv.org/abs/2410.10624. 12 Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, MingZher Poh, Shun Liao, Paolo Di Achille, and Shwetak Patel. Large language models are few-shot health learners, 2023. URL https://arxiv.org/abs/2305.15525. Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: 100,000 time series and 61 forecasting methods. International Journal of Forecasting, 36:54 74, 2020. URL https://api.semanticscholar.org/CorpusID:200091182. Caroline Marra, Tim Chico, April Alexandrow, Will Dixon, Norman Briffa, Erin Rainaldi, Max Little, Kristin Size, Athanasios Tsanas, Joseph Franklin, Ritu Kapur, Helen Grice, Anwar Gariban, Joy Ellery, Cathie Sudlow, Amy Abernethy, and Andrew Morris. Addressing the challenges of integrating digital health technologies to measure patient-centred outcomes in clinical registries. The Lancet Digital Health, 7, 12 2024. doi: 10.1016/S2589-7500(24)00223-1. Mike Merrill, Mingtian Tan, Vinayak Gupta, Thomas Hartvigsen, and Tim Althoff. Language models still struggle to zero-shot reason about time series. pp. 35123533, 01 2024. doi: 10.18653/v1/2024.findings-emnlp.201. Yuqi Nie, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. time series is worth In The Eleventh International Confer64 words: Long-term forecasting with transformers. ence on Learning Representations, 2023. URL https://openreview.net/forum?id= Jbdc0vTOcol. Jungwoo Oh, Gyubok Lee, Seongsu Bae, Joon-myoung Kwon, and Edward Choi. Ecg-qa: comprehensive question answering dataset combined with electrocardiogram. Advances in Neural Information Processing Systems, 36:6627766288, 2023. Amy Olex and Bridget Mcinnes. Review of temporal reasoning in the clinical domain for timeline extraction: Where we are and where we need to be. Journal of Biomedical Informatics, 118: 103784, 04 2021. doi: 10.1016/j.jbi.2021.103784. Louis Pangaro. new vocabulary and other innovations for improving descriptive in-training evaluations. Academic medicine : journal of the Association of American Medical Colleges, 74(11): 12037, 1999. doi: 10.1097/00001888-199911000-00012. Arvind Pillai, Dimitris Spathis, Subigya Nepal, Amanda Collins, Daniel Mackin, Michael Heinz, Tess Griffin, Nicholas Jacobson, and Andrew Campbell. Time2lang: Bridging timeseries foundation models and large language models for health sensing beyond prompting, 2025. URL https://arxiv.org/abs/2502.07608. Areti Pouliou, Vasileios Papageorgiou, Georgios Petmezas, Diogo Pessoa, Rui Pedro Paiva, N. Maglaveras, and George Tsaklidis. new approach for sleep stage identification combining hidden markov models and eeg signal processing. Journal of Medical and Biological Engineering, 45, 02 2025. doi: 10.1007/s40846-025-00928-5. Attila Reiss and Didier Stricker. Creating and benchmarking new dataset for physical activity monitoring. 06 2012. doi: 10.1145/2413097.2413148. Yalini Senathirajah, David Kaufman, Kenrick Cato, Elizabeth Borycki, Jaime Fawcett, and Andre Kushniruk. Characterizing and visualizing display and task fragmentation in the electronic health record: methodological approaches (preprint). JMIR Human Factors, 7, 05 2020. doi: 10.2196/ 18484. Muhammad Shoaib, Hans Scholten, and Paul Havinga. Towards physical activity recognition using smartphone sensors. pp. 8087, 12 2013. doi: 10.1109/UIC-ATC.2013.43. Muhammad Shoaib, Stephan Bosch, Ozlem Durmaz Incel, Hans Scholten, and Paul J. M. Havinga. Fusion of smartphone motion sensors for physical activity recognition. Sensors, 14(6):10146 10176, 2014. ISSN 1424-8220. doi: 10.3390/s140610146. URL https://www.mdpi.com/ 1424-8220/14/6/10146. Muhammad Shoaib, Stephan Bosch, Ozlem Durmaz Incel, Hans Scholten, and Paul J. M. Havinga. Complex human activity recognition using smartphone and wrist-worn motion sensors. Sensors, 16(4), 2016. ISSN 1424-8220. doi: 10.3390/s16040426. URL https://www.mdpi.com/ 1424-8220/16/4/426. Sonish Sivarajkumar and Yanshan Wang. Healthprompt: zero-shot learning paradigm for clinical natural language processing. AMIA ... Annual Symposium proceedings. AMIA Symposium, 2022: 972981, 04 2023. Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen. Smart devices are different: Assessing and In Proceedings of the 13th mitigatingmobile sensing heterogeneities for activity recognition. ACM Conference on Embedded Networked Sensor Systems, SenSys 15, pp. 127140, New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450336314. doi: 10.1145/ 2809695.2809718. URL https://doi.org/10.1145/2809695.2809718. Timo Sztyler and Heiner Stuckenschmidt. On-body localization of wearable devices: An investigation of position-aware activity recognition. In 2016 IEEE International Conference on Pervasive Computing and Communications (PerCom), pp. 19, 2016. doi: 10.1109/PERCOM.2016. 7456521. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 06 2017. doi: 10.48550/arXiv.1706.03762. Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima Lunze, Wojciech Samek, and Tobias Schaeffter. Ptb-xl, large publicly available electrocardiography dataset. Scientific Data, 7:154, 05 2020. doi: 10.1038/s41597-020-0495-6. Chengsen Wang, Qi Qi, Jingyu Wang, Haifeng Sun, Zirui Zhuang, Jinming Wu, Lei Zhang, and Jianxin Liao. Chattime: unified multimodal time series foundation model bridging numerical and textual data, 2024. URL https://arxiv.org/abs/2412.11376. Jiayang Wu, Wensheng Gan, Zefeng Chen, Wan Shicheng, and Philip Yu. Multimodal large language models: survey. 11 2023. doi: 10.1109/BigData59044.2023.10386743. Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Meng Zhao, and Fugee Tsung. Medualtime: dualadapter language model for medical time series-text multimodal learning, 2025. URL https: //arxiv.org/abs/2406.06620. Andy Wai Kan Yeung, Ali Torkamani, Atul Butte, Benjamin Glicksberg, Bjorn Schuller, Blanca Rodriguez, Daniel Ting, David Bates, Eva Schaden, Hanchuan Peng, Harald Willschke, Jeroen van der Laak, Josip Car, Kazem Rahimi, Leo Celi, Maciej Banach, M. Kletecka-Pulker, Oliver Kimberger, Roland Eils, and Atanas Atanasov. The promise of digital healthcare technologies. Frontiers in Public Health, 11, 09 2023. doi: 10.3389/fpubh.2023.1196596. Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, and Jingbo Shang. Large language models for time series: survey. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 24, 2024. ISBN 978-1-956792-04-1. doi: 10.24963/ijcai.2024/921. URL https://doi.org/10.24963/ijcai.2024/921. Yuwei Zhang, Kumar Ayush, Siyuan Qiao, A. Ali Heydari, Girish Narayanswamy, Maxwell A. Xu, Ahmed A. Metwally, Shawn Xu, Jake Garrison, Xuhai Xu, Tim Althoff, Yun Liu, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak Patel, Cecilia Mascolo, Xin Liu, Daniel McDuff, and Yuzhe Yang. Sensorlm: Learning the language of wearable sensors, 2025. URL https: //arxiv.org/abs/2506.09108. Li Zhou, Simon Parsons, and George Hripcsak. The evaluation of temporal reasoning system in processing clinical discharge summaries. Journal of the American Medical Informatics Association : JAMIA, 15:99106, 01 2008. doi: 10.1197/jamia.M2467."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 TRAINING DETAILS Table Table 3 provides an overview of the datasets used during training. All data was split into ratios Dataset #Samples (Train/Val/Test) Num series Length Frequency 1 TSQA* M4-Captions t 2 HAR-CoT Sleep-CoT ECG-QA-CoT 38,400 / 4,800 / 4,800 80,000 / 10,000 / 10,000 68,542 / 8,718 / 8,222 7,434 / 930 / 930 159,313 / 31,137 / 41,093 1 1 3 1 Hours to Years 64-512 points Not specified Not specified 2.56s 30s 10s 50Hz 100Hz 100Hz Table 3: *1TSQA Wang et al. (2024) Overview of datasets used in Stage 1 (pretraining tasks) and Stage 2 (task-specific CoT reasoning). Datasets are split in 80/10/10 ration. of 80/10/10 for train/val/test sets. A.1.1 TRAINING CONFIGURATION The models were trained with the following configuration: Optimizer: AdamW Learning Rates: OpenTSLM-SP: * Time series encoder: 2 104 * LoRA: 2 104 * Projector: 1 104 OpenTSLM-Flamingo: * Encoder: 2 104 * Cross-attention layers: 2 104 Scheduler: Linear learning rate schedule with warmup Warmup: 10% of total training steps Gradient Clipping: ℓ2-norm capped at 1.0 Weight Decay: 0.01 Training Length: Up to 200 epochs with early stopping (patience = 5 epochs) Learning rate choices were informed by Chow et al. (2024). A.2 GENERATION OF MULTIVARIATE TIME SERIES COT DATASETS This section provides detailed descriptions of the CoT datasets generated for our study: Human Activity Recognition (HAR-CoT), Sleep Stage Classification (SleepEDF-CoT), and Electrocardiogram Question Answering (ECG-QA-CoT). Our objective was to enable TSLMs not only to classify time series but also to generate explicit reasoning chains. Since few datasets include CoT text, we generated our own multivariate time series CoT datasets using widely adopted benchmarks in HAR, sleep staging, and ECG-QA, following similiar approach as proposed by Chow et al. (2024). For each dataset, we generated rationales with GPT-4o by providing plot of the data along with the correct label, and prompting the model to produce rationale leading to that label. The exact prompts are described in Sections A.2.1, A.2.2, and A.2.3. We carefully engineered the prompts and manually reviewed subset of samples to ensure the generated rationales were consistent and sensible. When plotting, original data was used without normalization. If multiple time series were present in sample (e.g., three in HAR or twelve in ECG), all were plotted as separate subplots but combined into single figure. GPT-4o snapshot: gpt-4o-2024-08-06 Temperature: 0.3 Seed: 42 The following subsections describe dataset-specific methodologies, data processing, prompts, answer selection, and final class distributions. 15 A.2.1 HUMAN ACTIVITY RECOGNITON (HAR) COT We merged multiple HAR datasets spanning DaLiAc Leutheuser et al. (2013), DOMINO Arrotta et al. (2023), HHAR Stisen et al. (2015), PAMAP2 Reiss & Stricker (2012), RealWorld Sztyler & Stuckenschmidt (2016), and datastes from Shoaib et al. (2013; 2014; 2016). We retain only those activity classes present in all datasets. The final dataset includes eight activity classes: sitting, walking, standing, running, walking up stairs, walking down stairs, lying, and biking. The data is split into 2.56 second windows. Data Processing The dataset was processed to create 2.56-second windows of triaxial accelerometer data (X, Y, axes). Each sample was visualized as multi-panel plot showing the acceleration signals across all three axes over the time window. Prompt for CoT generation We generated CoT rationales by prompting the model with correct and dissimilar label. The following prompt template was used for HAR-CoT generation: You are shown time-series plot of accelerometer over 2.56 second window. This data corresponds to one of two possible activities: [CORRECT_ACTIVITY] [DISSIMILAR_ACTIVITY] Your task is to classify the activity based on analysis of the data. Instructions: - Begin by analyzing the time series without assuming specific label. - Think step-by-step about what the observed patterns suggest regarding movement intensity and behavior. - Write your rationale as single, natural paragraph, do not use bullet points, numbered steps, or section headings. - Do not refer back to the plot or to the act of visual analysis in your rationale; the plot is only for reference but you should reason about the time-series data. - Do **not** assume any answer at the beginning, analyze as if you do not yet know which class is correct. - Do **not** mention either class label until the final sentence. - Make sure that your last word is the answer. You MUST end your response with \"Answer: [CORRECT_ACTIVITY]\": Answer Selection Strategy For each sample, we implemented dissimilarity-based answer selection strategy. Given correct activity label, we selected the most dissimilar activity from predefined mapping: Sitting: walking, running, biking, walking up, walking down Walking: sitting, lying, standing, biking, running Standing: walking, running, biking, walking up, walking down Running: sitting, lying, standing, biking, walking Walking up: sitting, lying, standing, biking, running Walking down: sitting, lying, standing, biking, running Lying: walking, running, biking, walking up, walking down Biking: sitting, lying, standing, walking, running This strategy ensured that the binary classification tasks were challenging and required genuine analysis of movement patterns rather than simple pattern recognition. Label distribution A.2.2 SLEEP STAGE CLASSIFICATION CHAIN-OF-THOUGHT (SLEEPEDF-COT) The SleepEDF-CoT dataset was generated from the Sleep-EDF database, which contains polysomnography recordings with expert-annotated sleep stage labels. The dataset includes five sleep stages: Wake (W), Non-REM stage 1 (N1), Non-REM stage 2 (N2), Non-REM stage 3 (N3), and REM sleep (REM). 16 Figure 8: Sample HAR signal input to GPT-4o for rationale generation Table 4: Per-class sample distribution for HAR-CoT train, validation, and test sets Class Biking Lying Running Sitting Standing Walking Walking Down Walking Up Train (n=68542) 4037 (5.9%) 4305 (6.3%) 8101 (11.8%) 18997 (27.7%) 11001 (16.1%) 12675 (18.5%) 4514 (6.6%) 4912 (7.2%) Val (n=8718) 435 (5.0%) 682 (7.8%) 948 (10.9%) 2315 (26.6%) 1449 (16.6%) 1611 (18.5%) 710 (8.1%) 568 (6.5%) Test (n=8222) 473 (5.8%) 444 (5.4%) 1057 (12.9%) 2342 (28.5%) 1264 (15.4%) 1508 (18.3%) 542 (6.6%) 592 (7.2%) Data Processing The dataset was processed to create 30-second windows of EEG data from the Fpz-Cz channel. Each sample was visualized as single-channel EEG plot showing brain activity patterns characteristic of different sleep stages. Prompt for CoT generation We generated CoT rationales by prompting the model with correct and dissimilar label. The following prompt template was used for SleepEDF-CoT generation: You are presented with time-series plot showing EEG data collected over 30-second interval. This signal corresponds to one of two possible sleep stages: - [SLEEP_STAGE_1] - [SLEEP_STAGE_2] Your task is to determine the correct sleep stage based solely on the observed patterns in the time series. Instructions: - Analyze the data objectively without presuming particular label. - Reason carefully and methodically about what the signal patterns suggest regarding sleep stage. 17 - Write your reasoning as single, coherent paragraph. Do not use bullet points, lists, or section headers. - Do not reference the plot, visuals, or the process of viewing the data in your explanation; focus only on the characteristics of the time series . - Do not mention or speculate about either class during the rationale, only reveal the correct class at the very end. - Never state that you are uncertain or unable to classify the data. You must always provide rationale and final answer. - Your final sentence must conclude with: \"Answer: [CORRECT_SLEEP_STAGE]\" Figure 9: Sample EEG signal input to GPT-4o for sleep stage rationale generation Answer Selection Strategy For sleep stage classification, we implemented dissimilarity-based strategy that pairs physiologically distinct sleep stages: Wake (W): N3, N4, REM N1: W, N3, N4 N2: W, REM N3: W, REM N4: W, REM REM: N2, N3, N4 This approach ensured that the binary classification tasks required understanding of fundamental differences in brain activity patterns between sleep stages. Label distribution SleepEDF dataset Table 5: Per-class sample distribution for train, validation, and test sets (Sleep stages) Label Non-REM 1 Non-REM 2 Non-REM 3 Non-REM 4 REM Wake Train (n=7434) 410 (5.5%) 2057 (27.7%) 357 (4.8%) 299 (4.0%) 944 (12.7%) 3367 (45.3%) Val (n=930) 52 (5.6%) 257 (27.6%) 45 (4.8%) 37 (4.0%) 118 (12.7%) 421 (45.3%) Test (n=930) 51 (5.5%) 257 (27.6%) 45 (4.8%) 38 (4.1%) 118 (12.7%) 421 (45.3%) A.2.3 ELECTROCARDIOGRAM QUESTION ANSWERING CHAIN-OF-THOUGHT (ECG-QA-COT) The ECG-QA-CoT dataset was generated from the PTB-XL Wagner et al. (2020) database combined with the ECG-QA Oh et al. (2023) question templates. This dataset contains 12-lead ECG recordings with clinical questions covering various aspects of cardiac analysis, including rhythm analysis, morphology assessment, and diagnostic classification. Data Processing The dataset was processed to create complete 12-lead ECG recordings (I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6) sampled at 100 Hz. Each ECG was visualized as multi-panel plot showing all 12 leads simultaneously, enabling comprehensive cardiac analysis. Prompt for CoT generation The following prompt template was used for ECG-QA-CoT generation: You are presented with complete 12-lead ECG recording showing all standard leads (I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6). Clinical Context: [CLINICAL_CONTEXT] Question: [QUESTION] This question has one of two possible answers: - [ANSWER_OPTION_1] - [ANSWER_OPTION_2] Your task is to analyze the ECG and determine the correct answer based on the observed cardiac patterns. You may include the clinical context in your analysis if it helps you determine the correct answer. Instructions: - Analyze the ECG systematically without presuming particular answer. - Consider rhythm, rate, morphology, intervals, and any abnormalities you observe across all 12 leads. - Think step-by-step about what the ECG patterns indicate regarding the clinical question above. - Write your reasoning as single, coherent paragraph. Do not use bullet points, lists, or section headers. - Do not reference the visual aspects of viewing the ECG plot; focus on the cardiac characteristics and clinical significance. - Do not mention or assume either answer option during your rationale, only reveal the correct answer at the very end. - NEVER state uncertainty or inability to determine the answer. You MUST always provide clinical reasoning and definitive answer. - Your final sentence must conclude with: \"Answer: [CORRECT_ANSWER]\" 19 Figure 10: Sample ECG signal input to GPT-4o for rationale generation 20 Table 6: Per-template sample distribution for ECG-QA CoT train, validation, and test sets Template ID Train (n=159,306) Val (n=31,137) Test (n=41,093) Template 1 Template 2 Template 3 Template 4 Template 5 Template 6 Template 7 Template 8 Template 9 Template 10 Template 11 Template 12 Template 13 Template 14 Template 15 Template 16 Template 17 Template 18 Template 19 Template 20 Template 21 Template 22 Template 23 Template 24 Template 25 Template 26 Template 27 Template 28 Template 29 Template 30 Template 31 Template 32 Template 33 Template 34 Template 35 Template 36 Template 37 Template 38 Template 39 Template 40 Template 41 Template 42 17,089 (10.7%) 300 (0.2%) 240 (0.2%) 20,861 (13.1%) 20,104 (12.6%) 5,356 (3.4%) 1,137 (0.7%) 4,371 (2.7%) 3,563 (2.2%) 894 (0.6%) 2,861 (1.8%) 300 (0.2%) 300 (0.2%) 300 (0.2%) 300 (0.2%) 300 (0.2%) 19,952 (12.5%) 9,580 (6.0%) 4,122 (2.6%) 1,200 (0.8%) 180 (0.1%) 400 (0.3%) 744 (0.5%) 90 (0.1%) 399 (0.3%) 10,585 (6.6%) 1,038 (0.7%) 3,600 (2.3%) 300 (0.2%) 224 (0.1%) 1,235 (0.8%) 697 (0.4%) 6,102 (3.8%) 2,411 (1.5%) 246 (0.2%) 900 (0.6%) 108 (0.1%) 523 (0.3%) 5,100 (3.2%) 480 (0.3%) 1,700 (1.1%) 9,114 (5.7%) 2,924 (9.4%) 60 (0.2%) 48 (0.2%) 3,782 (12.1%) 3,599 (11.6%) 1,022 (3.3%) 221 (0.7%) 747 (2.4%) 610 (2.0%) 311 (1.0%) 533 (1.7%) 60 (0.2%) 60 (0.2%) 60 (0.2%) 60 (0.2%) 60 (0.2%) 3,013 (9.7%) 2,178 (7.0%) 698 (2.2%) 228 (0.7%) 36 (0.1%) 131 (0.4%) 126 (0.4%) 18 (0.1%) 160 (0.5%) 1,894 (6.1%) 180 (0.6%) 720 (2.3%) 60 (0.2%) 36 (0.1%) 198 (0.6%) 246 (0.8%) 2,189 (7.0%) 494 (1.6%) 18 (0.1%) 176 (0.6%) 21 (0.1%) 192 (0.6%) 1,019 (3.3%) 104 (0.3%) 819 (2.6%) 2,026 (6.5%) 3,467 (8.4%) 60 (0.1%) 48 (0.1%) 4,096 (10.0%) 3,905 (9.5%) 1,085 (2.6%) 224 (0.5%) 1,466 (3.6%) 1,200 (2.9%) 377 (0.9%) 964 (2.3%) 60 (0.1%) 60 (0.1%) 60 (0.1%) 60 (0.1%) 60 (0.1%) 4,416 (10.7%) 3,806 (9.3%) 1,395 (3.4%) 237 (0.6%) 36 (0.1%) 167 (0.4%) 168 (0.4%) 18 (0.0%) 178 (0.4%) 2,193 (5.3%) 210 (0.5%) 720 (1.8%) 60 (0.1%) 43 (0.1%) 274 (0.7%) 313 (0.8%) 2,775 (6.8%) 872 (2.1%) 50 (0.1%) 180 (0.4%) 22 (0.1%) 241 (0.6%) 1,020 (2.5%) 104 (0.3%) 849 (2.1%) 3,554 (8.6%) Label distribution Per-Template Label Distribution Summary Template ID Train Labels Val Labels Test Labels Template 1 Template 2 no: 11360, yes: 4751, not sure: 978 no: 200, yes: no: 1995, yes: 796, not sure: 133 no: 40, yes: 20 no: 2215, yes: 991, not sure: 261 no: 40, yes: 20 21 Template 3 Template 4 Template Template 6 Template 7 Template 8 Template 9 st/t change: 60, myocardial infarction: 60, none: 60, hypertrophy: 60, conduction disturbance: 60 none: 6300, myocardial infarction in anteroseptal leads: 618, left anterior 593, fascicular block: myocardial infarction in inferior leads: 586, first degree av block: 585 none: 6300, myocardial infarction in anteroseptal leads: 578, left anterior fascicular block: 565, first degree av block: 558, non-specific intraventricular conduction disturbance (block): st/t change: 12, myocardial infarction: 12, none: 12, hypertrophy: 12, conduction disturbance: 12 none: 1258, left ventricular hypertrophy: 110, myocardial infarction in anteroseptal leads: 109, left anterior fascicular block: 107, first degree av block: 107 none: 1248, left anterior fascicular block: 105, first degree av block: 103, myocardial infarction in anteroseptal leads: 99, left ventricular hypertrophy: 95 1530, non-specific 295, nonnone: diagnostic abnormal306, ventricular ities: complex: premature 300, st nonchanges: specific st depression: 294 none: 360, bigeminal pattern (unknown origin, supraventricular, or ventricular): 105, atrial flutter: 99, sinus rhythm: 98, atrial fibrillation: 98 myocardial infarction in anteroseptal leads: 1050, myocardial infarction in inferior leads: 830, left ventricular hypertrophy: 791, left anterior fascicular block: 705, nonspecific ischemic: 512 myocardial infarction in anteroseptal leads: 635, left anterior fascicular block: 592, non-specific ischemic: left ventricular hypertrophy: 432, av block: first degree 459, none: 306, non-specific st depression: 57, nondiagnostic abnormalities: 56, ventricular premature complex: 55, voltage criteria (qrs) for left ventricular hypertrophy: 52 none: 72, sinus rhythm: patbigeminal 19, tern (unknown origin, supraventricular, or ventricular): 19, atrial flutter: 18, atrial fibrillation: 17 myocardial infarction in inferior leads: 130, left ventricular hypertrophy: 129, myocardial infarction in anteroseptal leads: 127, left anterior fascicular block: 114, none: 100 fascicular left anterior block: 111, none: 100, non-diagnostic abnormalities: 79, myocardial infarction in anterosepincomtal plete right bundle branch block: 70 leads: 74, 118, st/t change: 12, myocardial infarction: 12, none: 12, hypertrophy: 12, conduction disturbance: 12 none: 1260, myocardial infarction in anteroseptal leads: 122, myocarinfarction in infedial rior left leads: ventricular hypertrophy: 117, left anterior fascicular block: 117 none: 1260, myocardial infarction in anteroseptal leads: 117, left anterior 116, fascicular block: intravennon-specific conduction tricular (block): disturbance 112, av block: 109 none: 306, ventricular premature complex: 64, non-specific st depression: 63, non-diagnostic 60, abnormalities: comatrial premature plex: 60 first degree none: 72, bigeminal pattern (unknown origin, supraventricular, or ventricular): 21, sinus rhythm: 19, atrial fibrillation: 18, sinus tachycardia: 18 myocardial infarction in anteroseptal leads: 304, left ventricular hypertrophy: 282, myocardial infarction in inferior leads: 259, left anterior fascicular block: 236, nonspecific ischemic: 177 fascicular left anterior block: 206, myocardial infarction in anteroseptal 194, nonspecific ischemic: 155, left ventricular hypertrophy: 149, non-specific intraventricular conduction disturbance (block): 127 leads: 22 Template Template 11 Template 12 Template 13 Template 14 Template 15 Template 16 Template 17 Template 18 Template 19 Template 20 Template 21 Template 22 Template 23 Template Template 25 left criteria st 692, none: 200, sinus rhythm: 135, atrial fibrillation: 118, sinus tachycardia: 108, sinus bradycardia: 107 denon-specific pression: nondiagnostic abnormal570, ventricular ities: complex: premature low amplitude 414, 334, voltage t-wave: criteria left for (qrs) ventricular hypertrophy: 329 no: 200, yes: 100 no: 200, yes: 100 no: 200, yes: 100 no: 200, yes: 100 no: 200, yes: 100 no: 14455, yes: 5497 none: 2400, non-specific 1848, st depression: voltage (qrs) ventricular for 1510, hypertrophy: non-diagnostic abt low normalities: 1385, amplitude t-wave: 1138 none: 1695, lead I: 1509, lead V6: 1453, lead V5: 1322, lead aVL: 1242 no: 800, yes: 400 none: 60, left axis deviation: 30, right axis deviation: 30, extreme axis deviation: 30, normal heart axis: 30 left axis deviation: 100, right axis deviation: 100, extreme axis deviation: 100, normal heart axis: 100 no: 545, yes: 199 none: 30, early stage of myocardial infarction: 20, middle stage of myocardial infarction: 20, old stage of myocardial infarction: 20 none of myocardial infarction: 100, unknown stage of myocardial infarction: 100, middle stage of myocardial infarction: 100, early stage of myocardial infarction: 70, old stage of myocardial infarction: 29 sinus rhythm: 56, none: 56, atrial fibrillation: 51, sinus tachycardia: 51, sinus arrhythmia: sinus none: 100, sinus rhythm: 56, tachycardia: 52, atrial fibrillation: 52, sinus bradycardia: 51 100, none: nondiagnostic abnormalities: 99, non-specific st depression: 81, ventricular premature complex: 64, abnormal qrs: 64 no: 40, yes: 20 no: 40, yes: 20 no: 40, yes: 20 no: 40, yes: 20 no: 40, yes: 20 no: 2270, yes: 743 none: 1150, non-specific st depression: 378, voltage criteria (qrs) for left ventricular hypertrophy: 216, waves present: 114, non-diagnostic abnormalities: 107 none: 415, lead I: 165, lead V6: 154, lead V5: 153, lead aVL: 138 no: 160, yes: 68 none: 12, left axis deviation: 6, right axis deviation: 6, extreme axis deviation: 6, normal heart axis: 6 left axis deviation: 50, normal heart axis: 50, right axis deviation: 23, extreme axis deviation: no: 95, yes: 31 none: 6, early stage of myocardial infarction: 4, middle stage of myocardial infarction: 4, old stage of myocardial infarction: 4 none of myocardial infarction: 50, unknown stage of myocardial infarction: 50, middle stage of myocardial infarction: 49, early stage of myocardial infarction: 6, old stage of myocardial infarction: 5 23 st 194, denon-specific pression: nondiagnostic abnormal182, ventricular ities: complex: premature 142, criteria voltage (qrs) for left ventricular hypertrophy: waves present: 105 123, non-specific no: 40, yes: 20 no: 40, yes: 20 no: 40, yes: 20 no: 40, yes: 20 no: 40, yes: 20 no: 3150, yes: 1266 voltage 1200, none: left for (qrs) criteria ventricular hypertrophy: st 675, depression: 645, nondiagnostic abnormalities: 473, non-specific t-wave changes: 308 none: 655, lead I: 438, lead V6: 431, lead V5: 399, lead aVL: 392 no: 160, yes: 77 none: 12, left axis deviation: 6, right axis deviation: 6, extreme axis deviation: 6, normal heart axis: 6 left axis deviation: 50, right axis deviation: 50, normal heart axis: 50, extreme axis deviation: 17 no: 120, yes: 48 none: 6, early stage of myocardial infarction: 4, middle stage of myocardial infarction: 4, old stage of myocardial infarction: 4 none of myocardial infarction: 50, unknown stage of myocardial infarction: 50, middle stage of myocardial infarction: 50, early stage of myocardial infarction: 19, old stage of myocardial infarction: 9 Template 26 Template 27 Template 28 Template 29 Template 30 Template 31 Template 32 Template 33 Template Template 35 Template 36 Template 37 Template 38 Template 39 Template 40 Template 41 Template 42 no: 7335, yes: 3250 no: 715, yes: 323 no: 2400, yes: 1200 no: 200, yes: 100 none: 60, baseline drift: 58, 56, static noise: burst noise: 50, electrodes problems: 44 static noise: 448, none: 430, baseline drift: 333, burst noise: 309, electrodes problems: 17 baseline drift: 252, static noise: 241, none: 200, burst noise: 174, electrodes problems: 23 none: 2400, static noise: 1824, drift: baseline 1729, burst noise: 823, electrodes problems: 27 lead II: lead III: 972, 904, lead lead I: 864, aVR: 844, lead aVL: 779 no: 200, yes: 46 no: 600, yes: 300 supraventricular extrasystoles: 38, ventric30, ular extrasystoles: none: 30, extrasystoles: 28 none: 200, supraventricular extrasystoles: 125, ventricular extrasystoles: 115, extrasystoles: 108 no: 3400, yes: 1700 none: 160, within the 110, range: normal above the normal range: 110, below the normal range: 100 within the normal range: 600, above the normal range: 600, below the normal range: 500 qt interval: 4393, rr in4336, qt corterval: rected: 4262, duration: 4093, qrs duration: no: 1335, yes: 559 no: 120, yes: 60 no: 480, yes: 240 no: 40, yes: 20 none: 12, baseline drift: 10, static noise: 10, burst noise: 10 static noise: 95, none: 72, burst noise: 47, baseline drift: 45 none: 100, static noise: 83, baseline drift: 78, burst noise: 22 none: 1200, static noise: 675, baseline drift: 358, burst noise: 79 7, none: 215, lead III: 182, lead II: 175, lead I: 169, lead aVR: 165 no: 15, yes: 3 no: 120, yes: 56 exsupraventricular trasystoles: extrasystoles: 6, none: 6, ventricular extrasystoles: 5 none: 100, extrasystoles: 55, supraventricular extrasystoles: 27, ventricular extrasystoles: 16 no: 680, yes: 339 none: 36, within the normal range: 24, above the normal range: 24, below the normal range: no: 1470, yes: 723 no: 145, yes: 65 no: 480, yes: 240 no: 40, yes: 20 none: 12, static noise: 11, baseline drift: 10, burst noise: 10, electrodes problems: 7 static noise: 99, none: 88, burst noise: 80, baseline drift: 71, electrodes problems: 1 baseline drift: 112, static noise: 109, none: 100, burst noise: 58, electrodes problems: 5 none: 1200, static noise: 744, baseline drift: 712, burst noise: 283, electrodes problems: 6 lead II: lead III: 339, 327, lead lead I: 320, aVR: 305, lead aVL: 270 no: 40, yes: 10 no: 120, yes: 60 supraventricular extrasystoles: 8, extrasysventricular toles: extrasystoles: 6, none: 6 6, none: 100, supraventric57, ular extrasystoles: extrasystoles: 54, ventricular extrasystoles: 38 no: 680, yes: 340 none: 36, within the normal range: 24, above the normal range: 24, below the normal range: 20 within the normal range: 300, above the normal range: 300, below the normal range: 219 rr interval: 902, qt interval: 880, qt corrected: 879, duration: 872, qrs duration: 779 within the normal range: 300, above the normal range: 300, below the normal range: 249 rr interval: 1730, qt interval: 1672, duration: 1614, qt corrected: 1592, qrs duration: 1486 A.3 M4 CAPTION DATASET GENERATION We constructed the M4-Caption dataset by pairing time series from the M4 forecasting competition dataset Makridakis et al. (2020) with model-generated natural language captions. Data processing We removed trailing padding from each tensor by truncating after the last nonzero element. Prompt for caption generation We combine high-resolution plot, whose aspect ratio scales with sequence length to preserve visual fidelity and contextual detail, with the task to generate detailed caption. 24 Generate detailed caption for the following time-series data: Figure 11: Sample M4 signal input to GPT-4o for caption generation A.4 EXAMPLE OF BASELINES FAILING TO PRODUCE MEANINGFUL OUTPUT As shown in Table 2 in Appendix 4.3, some text-only models achieve 0% F1 score on the CoT datasets. This is because they fail to answer in the rationale Answer : answer template (see Appendix 4.1). We present some examples of such outputs in the following. A.4.1 LLAMA3.2-3B BASELINE OUTPUT ON HAR-COT INPUT PROMPT (TRUNCATED) You are given accelerometer data in all three dimensions. Your task is to classify the activity based on analysis of the data. Instructions: - Begin by analyzing the time series without assuming specific label. - Think step-by-step about what the observed patterns suggest regarding movement intensity and behavior. - Write your rationale as single, natural paragraph, do not use bullet points, numbered steps, or section headings. - Do **not** mention any class label until the final sentence. The following is the accelerometer data on the x-axis, it has mean -3.2434 and std 0.0474:n1 8 6 6 ,4 4 9 ,1 0 5 7 ,8 5 5 , -7 6 2 ,6 5 2 ,4 5 0 ,6 5 2 , -1 7 7 3 , -1 5 7 1 , -1 3 6 9 ,2 4 8 , -5 6 0 ,6 5 2 , -1 5 6 ,2 0 6 8 ,1 8 6 6 ,1 0 5 6 ,2 4 8 , -7 6 2 , -3 9 8 ,1 2 5 9 , -5 6 0 , -7 6 3 ,8 5 5 ,1 8 6 5 ,2 4 8 ,4 6 ,2 0 6 8 , -1 1 6 6 , -9 6 4 ,4 1 0 , -5 6 0 ,8 5 5 ... The following is the accelerometer data on the y-axis, it has mean 2.3132 and std 0.0550:n -3 7 5 , -1 2 4 , -1 3 7 5 ,1 4 8 2 ,2 3 2 , -4 8 1 , -4 8 2 , -3 0 3 , -1 2 4 , -4 8 1 , -6 6 0 ,2 3 2 ,5 3 , -1 7 3 2 , -8 3 9 , -3 0 3 ,9 4 6 , -3 0 3 , -1 0 1 7 ,7 6 7 ,3 3 9 ,5 8 9 , -3 0 3 , -1 1 9 6 , -1 7 3 2 ,5 8 9 , -3 0 3 , -4 8 1 , -1 7 3 2 , -1 1 9 6 ,5 8 9 , -5 1 7 , -4 8 1 , -1 2 4 , -1 9 1 1 , -1 5 5 3 , -6 6 0 , -1 2 4 , ... The following is the accelerometer data on the z-axis, it has mean 9.2017 and std 0.0639:n2 9 8 ,7 8 , -7 7 , -3 9 0 ,1 1 7 5 , -7 7 , -1 0 1 8 ,7 9 ,1 3 3 3 , -7 0 4 ,4 7 ,1 1 7 6 , -2 3 4 ,3 9 2 , -1 3 3 1 ,5 4 9 ,3 9 2 ,7 9 , -8 6 1 ,5 4 9 , -7 7 , -1 1 7 4 , -5 4 7 , -7 0 4 , -5 4 7 ,1 3 3 2 ,2 3 6 ,8 6 2 ,1 3 3 2 , -7 7 , ... Possible activity labels are: biking, lying, running, sitting, standing, walking, walking_down, walking_up. - Please now write your rationale. Make sure that your last word is the answer. You MUST end your response with \"Answer:\""
        },
        {
            "title": "GENERATED ANSWER",
            "content": "The accelerometer data on the x-axis has mean of -3.2434 and standard deviation of 0.0474. This indicates that the data is relatively stable, with small amount of variation. The data also has negative skew, which means that there are more negative values than positive values. This could indicate that the data is not evenly distributed, and that there may be bias towards negative values. The accelerometer data on the y-axis has mean of 2.3132 and standard deviation of 0.0550. This indicates that the data is relatively stable, with small amount of variation. The data also has negative skew, which means that there are more negative values than positive values. This could indicate that the data is not evenly distributed, and that there may be bias towards negative values. Based on the analysis of the accelerometer data, it is likely that the subject is performing repetitive motion, such as walking or running. The data is relatively stable, with small amount of variation, and there is negative skew, which could indicate that the data is not evenly distributed. This could suggest that the subject is performing repetitive motion, such as walking or running, which would result in consistent pattern of movement. In conclusion, based on the analysis of the accelerometer data, it is likely that the subject is performing repetitive motion, such as walking or running. The data is relatively stable, with small amount of variation, and there is negative skew, which could indicate that the data is not [... repeated multiple times] A.4.2 GEMMA3-1B-PT BASELINE OUTPUT ON SLEEP-COT INPUT PROMPT (TRUNCATED) You are given 30-second EEG time series segment. Your task is to classify the sleep stage based on analysis of the data. Instructions: - Analyze the data objectively without presuming particular label. - Reason carefully and methodically about what the signal patterns suggest regarding sleep stage. - Write your reasoning as single, coherent paragraph. - Only reveal the correct class at the very end. - Never state that you are uncertain or unable to classify the data. You must always provide rationale and final answer. The following is the EEG time series, it has mean 0.0000 and std 0.0000: -3 2 0 , -1 8 0 ,5 5 , -2 1 1 , -6 5 , -4 6 6 , -2 6 9 , -4 4 1 , -7 5 9 , -8 2 9 , -1 0 6 5 , -8 2 9 , -1 0 7 7 , -8 2 9 , -8 9 3 , -7 2 7 , -1 0 2 6 , -7 7 2 , -9 0 5 , -1 0 4 5 , -6 4 4 , -7 4 0 , -5 3 0 , -8 6 1 , -7 5 3 , -1 4 6 6 , -1 0 2 6 , -9 1 8 , -9 1 8 , -7 5 9 , -3 3 2 , -2 9 4 , -2 2 4 , -4 9 8 , -1 9 9 , -2 0 5 , -3 0 1 , -6 0 0 , -5 3 6 , -6 0 0 , -4 5 3 , -4 2 2 , -2 3 1 , -7 1 , -2 8 1 , -4 0 9 , -1 4 1 ,1 7 , -7 1 , -2 6 2 ,5 5 , -2 8 8 , -2 3 1 , -4 9 8 ,5 5 , -1 , -1 4 ,5 3 3 ,1 0 0 , -2 1 1 , -2 1 8 , -2 6 9 ,1 6 4 ,2 9 ,1 9 5 ,1 2 5 , -5 9 , -3 5 8 , -1 6 7 ... Possible sleep stages are: Wake, Non-REM stage 1, Non-REM stage 2, Non-REM stage 3, REM sleep, Movement - Please now write your rationale. Make sure that your last word is the answer. You MUST end your response with \"Answer:\" GENERATED ANSWER (TRUNCATED) - Please do not use bullet points, lists, or section headers. - Please do not use the word \"sleep\" in your response. 26 - Please do not use the word \"sleep\" in your response. - Please do not use the word \"sleep\" in your response. (repeated until out of new tokens) A.4.3 GEMMA3-270M BASELINE OUTPUT ON HAR-COT INPUT PROMPT (TRUNCATED) You are given accelerometer data in all three dimensions. Your task is to classify the activity based on analysis of the data. Instructions: - Begin by analyzing the time series without assuming specific label. - Think step-by-step about what the observed patterns suggest regarding movement intensity and behavior. - Write your rationale as single, natural paragraph, do not use bullet points, numbered steps, or section headings. - Do **not** mention any class label until the final sentence. The following is the accelerometer data on the x-axis, it has mean -1.9818 and std 1.8034:n1 2 7 7 ,9 8 5 ,1 2 1 3 ,1 2 5 1 ,1 3 5 1 ,1 8 7 2 ,1 6 1 2 ,6 9 8 ,4 4 3 ,6 2 9 ,4 3 8 ,6 1 3 ,9 3 2 ,9 2 7 ,1 0 3 2 ,9 2 1 ,9 3 7 ,6 7 7 ,5 4 4 ,6 5 6 ,5 3 9 ,9 2 7 ,8 9 5 ,9 6 4 ,1 0 7 5 ,1 0 4 9 ,8 5 2 ,9 3 2 ,1 5 9 6 ,1 9 5 2 ,1 8 8 3 ,1 4 1 0 ,3 7 4 , ... The following is the accelerometer data on the y-axis, it has mean 5.8203 and std 4.7959:n7 1 3 ,4 4 1 ,4 7 6 , -1 3 0 , -7 5 5 , -1 2 8 7 , -1 0 7 6 , -7 8 8 , -8 2 2 , -7 0 1 , -3 5 9 , -5 0 1 , -4 2 1 , -6 6 5 , -6 6 8 , -9 1 5 , -9 9 6 , -7 2 6 , -9 0 6 , -1 1 2 8 , -1 1 6 0 , -1 2 4 0 , -1 0 0 5 , -9 9 0 , -9 0 0 , -8 1 5 , -7 3 4 , -6 1 1 ,... The following is the accelerometer data on the z-axis, it has mean 6.3687 and std 3.9830:n -2 9 0 , -1 0 4 ,2 1 9 ,8 9 0 ,9 8 7 ,1 1 2 9 ,1 2 2 5 ,1 2 9 0 ,1 0 3 4 ,9 8 6 ,9 3 3 ,8 3 3 ,6 7 2 ,7 5 5 ,9 2 3 ,1 0 1 4 ,1 0 2 9 ,1 0 2 4 ,1 1 7 7 ,1 1 5 5 ,9 7 8 ,6 3 9 ,6 6 2 ,9 9 4 ,9 7 6 ,8 1 4 ,8 8 6 ,1 1 1 9 ,1 2 3 0 ,1 0 8 7 ,6 6 9 ,5 2 9 ,8 8 0 ,9 8 6 ,... Possible activity labels are: biking, lying, running, sitting, standing, walking, walking_down, walking_up. - Please now write your rationale. Make sure that your last word is the answer. You MUST end your response with \"Answer:\" GENERATED ANSWER (TEXT-ONLY BASELINE) The following is the accelerometer data on the x-axis, it has mean 1.0000 and std 1.0000: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 [... continues to count] 27 A.5 FURTHER EXAMPLES FOR RATIONALES GENERATED BY OPENTSLM MODELS A.5.1 M4 TIME-SERIES CAPTIONING (a) M4 Caption (Gemma3-1B) (b) M4 Caption (Llama3.2-1B) (c) M4 Caption (Llama3.2-1B) (d) M4 Caption (Llama3.2-1B) Figure 12: Examples of time series captioning on our generated M4 caption dataset. 28 A.5.2 HUMAN ACTIVITY RECOGNITION RATIONALES COT (a) Standing (Correct Prediction) (b) Sitting (Correct Prediction) (c) Standing (Correct Prediction) (d) Running (Correct Prediction) (e) Running (Correct Prediction) (f) Walking (Correct Prediction) Figure 13: Example chain of thought rationales for human activity recognition, generated with OpenTSLMFlamingo/Llama3.2-1B. A.5.3 SLEEP STAGE CLASSIFICATION RATIONALES COT (a) Stage Non-REM Stage 2 (b) Stage REM Sleep (c) Stage Wake (d) Stage Non-REM Stage 1 (e) Stage Non-REM Stage (f) Stage REM Sleep (g) Stage Wake (h) Stage Wake (i) Stage Wake Figure 14: Example rationales for sleep stage classification, generated with OpenTSLMFlamingo/Llama3.2-1B. A.5.4 ECG QA COT (a) none (Correct Prediction) (b) none (Correct Prediction) (c) pr interval (Correct Prediction) (d) Lead 1 (Correct Prediction) Figure 15: Example rationales for ECG QA, generated with OpenTSLM-Flamingo/Llama3.2-1B. 31 A.6 ECG EVALUATION RUBRIC These are the questions asked to clinicians during evaluation of ECG-QA rationales generated by OpenTSLMFlamingo/Llama3.2-3B. See Appendix 4.5 for details. Assessment Criteria 1. ECG Pattern Recognition Accuracy 2. Clinical Reasoning Quality 3. Clinical Context Integration Description Did the model correctly identify the relevant ECG features needed to answer the question? Did the model appropriately connect the identified ECG features to the final answer? Did the model appropriately incorporate patient clinical backrecording condiground (age, tions, artifacts) in its interpretation? Options Yes; Some but not all; None identified Yes; Some incorrect logic; Completely incorrect logic Yes; Used some key background; No did not use any relevant background Table 8: Assessment Criteria for ECG Interpretation Reasoning A.6.1 ECG REVIEW FORM Figure 16: ECG Review Form. This form was presented to clinicians to conduct the expert review of ECG-QA-CoT rationales generated by OpenTSLM-Flamingo/Llama3.2-3B (best model during evaluation, see Table 2). 32 A.6.2 REVIEWER DISAGREEMENT PATTERNS Figure 17 shows disagreement of reviewers on generated ECG-rationales (see Appendix 4.5). Figure 17: Disagreement Patterns A.7 EVALUATION OF MEMORY CONSUMPTION We complement the main results with detailed tables and plots. Figure 18 illustrates scaling trends, while the following subsections report detailed VRAM usage for both CoT datasets and synthetic simulation data. Figure 18: Simulation of memory scaling with total sequence length (N L). A.7.1 MEMORY USAGE ON COT DATASETS Table 9 reports VRAM for TSQA, HAR-CoT, Sleep-CoT, ECG-QA-CoT datasets. OpenTSLMFlamingo shows stable memory use mostly bound by the LLM backbone, whereas SoftPrompt varies substantially with datasets. 33 Table 9: VRAM Usage (GB) for Regular Datasets Method Model TSQA HAR-CoT SleepEDFCoT ECG-QACoT T O T O o f g a F"
        },
        {
            "title": "20.5\nLlama-3.2-1B\nLlama-3.2-3B\n61.1\nGemma-3-270M 5.7\n15.6\nGemma-3-1B-pt",
            "content": "9.6 14.3 8.6 6.1 22.0 63.5 6.4 16.3 15.9 20.3 20.1 14.7 21.6 63.4 6.3 15.7 64.9 87.1 24.1 32.7 20.9 71.6 7.3 18. A.7.2 MEMORY USAGE FOR SIMULATION DATA Table 10 shows results for simulated datasets, using permutations of = [1, 2, 3, 4, 5] and = [10, 100, 1000, 10000]. OpenTSLM-Flamingo requires almost constant memory with varying sequence length and number of concurrent series , while OpenTSLM-SoftPrompt grows with both until going out of memory (OOM) for larger time series. Simulation dataset generation. To generate the simulation dataset, we generate random data with combinations of = [1, 2, 3, 4, 5] and = [10, 100, 1000, 10000] according to the following pseudocode: num_series = series_length = simulation_dataset = [] for element_id in 1..200: time_series_texts = [] time_series_simulations = [] for in 1..num_series: series_i = random_normal(series_length) series_mean = mean(series_i) series_std = std(series_i) normalized_i = normalize(series_i) time_series_simualtions.append( normalized_i ) time_series_texts.append( \"This is time series with mean {series_mean} \" \"and std {series_std}.\" ) simulation_dataset.append([ { \"Series\": time_series_simualtions, \"Texts\": time_series_texts, \"PrePrompt\": \"You are given different time series. \" \"All have the same length\" \"of {length} data points.\", \"PostPrompt\": \"Predict the pattern \" \"of the time series. Answer:\", \"Answer\": \"This is random pattern.\" } ]) 34 Table 10: VRAM Usage (GB) for Simulation Datasets OpenTSLM-SoftPrompt OpenTSLM-Flamingo 1B LLaMA 3B Gemma 1B 270M LLaMA 3B 1B Gemma 1B 270M 10 10 10 10 10 100 100 100 100 100 1000 1000 1000 1000 1000 10000 10000 10000 10000 10000 2.6 1 2.6 2 2.7 3 2.7 4 2.8 5 2.7 1 2.8 2 2.9 3 3.0 4 3.2 5 3.6 1 5.0 2 6.9 3 9.2 4 12.0 5 29.5 1 93.3 2 3 OOM*1 4 OOM 5 OOM 5.0 5.0 4.9 5.0 5.0 4.9 5.0 5.0 5.0 5.0 5.0 4.9 7.4 8.7 10.2 19.2 43.6 76.0 116.4 164.5 Table 11: *1 OOM: Out of memory; OpenTSLM-SoftPrompt requires more tokens for longer time series, and separate tokens for separate time series. Introducing more or longer time series leads to more tokens, quickly scaling in memory use. 6.3 6.4 6.4 6.5 6.7 6.4 6.6 6.8 7.0 7.3 8.0 9.8 12.3 15.4 19.1 42.7 191.4 OOM OOM OOM 2.3 2.3 2.3 2.3 2.3 2.3 2.3 2.3 2.4 2.4 2.4 3.4 4.8 5.5 7.0 13.7 32.1 56.1 85.6 118.4 15.4 15.5 15.5 15.5 15.6 15.4 15.5 15.5 15.5 15.5 15.4 15.4 15.5 15.6 15.6 15.4 15.4 15.5 15.5 15.5 20.4 20.4 20.4 20.5 20.5 20.4 20.4 20.5 20.5 20.5 20.4 20.4 20.4 20.5 20.6 20.4 20.4 20.6 20.8 21.0 61.0 60.9 60.7 60.7 60.8 61.0 60.9 60.7 60.7 60.8 61.0 61.0 60.7 60.7 60.7 61.0 61.0 60.7 60.8 61.1 5.7 5.7 5.8 5.8 5.8 5.7 5.7 5.8 5.8 5.7 5.7 5.7 5.8 5.8 5.7 5.7 5.7 5.8 6.4 6."
        }
    ],
    "affiliations": [
        "Amazon",
        "Centre for Digital Health Interventions, ETH Zurich",
        "Centre for Digital Health Interventions, University of St. Gallen",
        "Division of Cardiology, University of Washington",
        "Division of Cardiovascular Medicine, Stanford University",
        "Google Research",
        "Pediatric Cardiology, Stanford University",
        "Stanford Mussallem Center for Biodesign, Stanford University",
        "Stanford University",
        "University of Illinois Urbana-Champaign",
        "University of Washington"
    ]
}