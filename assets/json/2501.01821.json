{
    "paper_title": "SDPO: Segment-Level Direct Preference Optimization for Social Agents",
    "authors": [
        "Aobo Kong",
        "Wentao Ma",
        "Shiwan Zhao",
        "Yongbin Li",
        "Yuchuan Wu",
        "Ke Wang",
        "Xiaoqian Liu",
        "Qicheng Li",
        "Yong Qin",
        "Fei Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex goal-oriented social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across a variety of agent tasks. Existing DPO-based approaches for multi-turn interactions are divided into turn-level and session-level methods. The turn-level method is overly fine-grained, focusing exclusively on individual turns, while session-level methods are too coarse-grained, often introducing training noise. To address these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which focuses on specific key segments within interactions to optimize multi-turn agent behavior while minimizing training noise. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPO's potential to advance the social intelligence of LLM-based agents. We release our code and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO."
        },
        {
            "title": "Start",
            "content": "SDPO: Segment-Level Direct Preference Optimization for Social Agents Aobo Kong1* Wentao Ma2* Shiwan Zhao1 Yongbin Li2 Yuchuan Wu2 Ke Wang2 Xiaoqian Liu2 Qicheng Li1 Yong Qin1 2Tongyi Lab 1TMCC, CS, Nankai University Fei Huang 1kongaobo@mail.nankai.edu.cn 1zhaosw@gmail.com 1{liqicheng, qinyong}@nankai.edu.cn 2{mawentao.mwt, shengxiu.wyc, shuide.lyb}@alibaba-inc.com"
        },
        {
            "title": "Abstract",
            "content": "Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex goaloriented social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across variety of agent tasks. Existing DPO-based approaches for multi-turn interactions are divided into turn-level and sessionlevel methods. The turn-level method is overly fine-grained, focusing exclusively on individual turns, while session-level methods are too coarse-grained, often introducing training noise. To address these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which focuses on specific key segments within interactions to optimize multiturn agent behavior while minimizing training noise. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPOs potential to advance the social intelligence of LLM-based agents. We release our code and data at this url."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs) have significantly enhanced their capabilities in language understanding and generation, particularly within the realm of human-machine interaction. By incorporating identity-specific information, LLM-based agents can simulate human social behaviors, demonstrating basic social intelligence in tasks such as role-playing casual conversations (Wang et al., 2024a; Lu et al., 2024) and navigate simulated social environments (Park et al., 2023). However, recent studies (Zhou et al., 2024) have shown that, in more complex, goal-oriented social scenarios, such as negotiation, competition, and * Equal contribution. Work done during internship. Corresponding authors. cooperation, LLMs still struggle to exhibit the nuanced decision-making abilities that are characteristic of human social interactions. In response to these challenges, several methods have been developed to better align LLM behavior with human preferences in multi-turn interactions. These approaches offer promising strategies for improving social decision-making in LLMs. Specifically, we focus on Direct Preference Optimization (DPO)-based methods, which can be divided into two primary categories: turn-level DPO (Rafailov et al., 2023), and session-level DPO approaches, such as ETO (Song et al., 2024) and DMPO (Shi et al., 2024). Turn-level DPO, for example, identifies single conversational turn and uses \"positive-negative\" pair of responses from that turn to optimize the model via preference loss function. While turn-level DPO has demonstrated some effectiveness, its focus on individual turns limits its ability to model goal completion in goal-oriented social dialogues, where success often relies on high-quality interactions spanning multiple conversational turns. To more effectively align agent behavior in multiturn interactions, session-level alignment methods such as ETO and DMPO have been proposed. These methods extend the sampling scope from individual turns to entire sessions, constructing pairs of good and bad sessions and applying an adapted DPO loss for training. However, these methods still exhibit certain limitations due to their relatively coarse alignment granularity: (i) Turns without errors in the negative sessions are still treated as bad outputs, introducing significant noise that can negatively affect the training process. (ii) Sampling from scratch provides the interlocutor with vast action space. higher score for positive session may result from changes in the interlocutors behavior, making it challenging for the model to learn the correct behavior pattern 5 2 0 2 3 ] . [ 1 1 2 8 1 0 . 1 0 5 2 : r Figure 1: An overview of three alignment algorithms with varying granularities, illustrated using SOTOPIA task as represents the agent to be tested. Positive sessions achieve higher scores on the goal and relationship an example. dimensions. For ease of presentation, we simplify the interaction content while preserving the original meaning. from the positive sample. To overcome the limitations of turn-level and session-level methods, we propose segment-level preference optimization, called SDPO, which aligns agent behavior in multi-turn interactions by optimizing turns within specific segments. Specifically, SDPO first identifies the erroneous turn in the negative session and then uses the interaction history preceding that turn to perform multiple samplings, thereby generating the positive session. Next, SDPO takes the first differing turn as the starting point, identifies the key segment from the positive session that contributes to higher score, and forms data pairs by taking the corresponding segment with the same length from the negative session. Finally, an adapted DPO loss is calculated for the turns within the segments. We present an overview of three alignment algorithms at different granularities for social dialogues in Figure 1. Compared to turn-level DPO, SDPO aligns multiple interaction turns, making it more suitable for goal-oriented social dialogues. Our method also addresses the drawbacks of session-level algorithms: (i) By calculating the loss only for the erroneous segments and their corresponding positive segments, training noise caused by non-erroneous turns is eliminated. (ii) Sampling from the erroneous turn narrows the action space of the interlocutor, making it more likely for the sampled positive sessions to contain the agents correct behavior patterns. We evaluate our approach on SOTOPIA (Zhou et al., 2024), an open-ended and interactive goaloriented benchmark for social intelligence, using both self-chat and interactions with other agents including GPT-4o and GPT-4o-mini, as interlocutors. Our results demonstrate that the SDPO-tuned agent consistently outperforms DPO, ETO, DMPO, and other robust baselines like GPT-4o, confirming the efficacy of segment-level alignment. Essentially, turn-level and session-level alignments are special cases of segment-level alignment. When the segment length is 1, SDPO simplifies to DPO. When the segment encompasses the entire interaction from start to finish, SDPO transforms into ETO. Compared to existing methods, our approach is more flexible and general, allowing for the selection of the optimal alignment granularity based on the specific data. Moreover, in this paper, we primarily apply SDPO to enhance agents social intelligence. However, we believe our approach can be applied to other scenarios, further enhancing the capabilities of agents across various domains. Our main contributions are three-fold: We propose SDPO, novel approach to align an agents behavior at the segment level in multi-turn interactions, supported by rigorous theoretical framework. We thoroughly evaluate our method on SOTOPIA, simulated and interactive social benchmark, substantiating the efficacy of segment-level alignment. We conduct an in-depth analysis of SDPO, validating its robustness across the variation in model output length, the efficiency of data utilization, the sources of data pairs, and the selection of segments."
        },
        {
            "title": "2.1 SOTOPIA Environment",
            "content": "Unlike previous social benchmarks that primarily test through static QA formats (Sap et al., 2019; Chen et al., 2024), SOTOPIA offers an interactive, open-ended, and realistic simulation environment, enabling more precise assessment of agents social intelligence. social task in SOTOPIA involves scenario, two role profiles, and their private social goals to be achieved through interaction. The diverse combinations of scenarios and social goals encompass broad spectrum of social interactions, such as negotiation, collaboration, and competition. SOTOPIA defines seven dimensions for evaluating social agents. We focus primarily on the \"goal\" (0-10, int) and \"relationship\" (-5 to 5, int), as GPT-4os ratings in these metrics closely align with human evaluations. SOTOPIA-π (Wang et al., 2024b) is follow-up work that leverages GPT-4 to automatically construct set of scenarios (completely non-overlapping with SOTOPIA), which serves as the training dataset for our study. Additionally, we restructure the prompt organization format of SOTOPIA to support multi-turn alignment, and the details are provided in Appendix A. interaction history hn faced by the agent in the n-th round is as follows: (cid:40) hn = b, y0, b, 0, . . . , yn1, n1, 0, y0, . . . , yn1, n, if speak first if speak later (1) Here, yi πθ(hi) represents the output generated by the LLM-based agent in round according to its policy πθ with parameter θ. On the other hand, represents the output of the interlocutor, which is drawn from an unknown distribution. Based on this formulation, we present the DPO and ETO loss functions (ETO directly extends turn-level DPO to the session level without rigorous proof) in Appendix B.1 and B.2. 2.3 DMPO DMPO introduces the state-action occupancy measure (SAOM) to help derive the loss of DPO in multi-turn interaction scenarios. The discounted SAOM dπ(s, a) of policy π is as follows: dπ(s = st, = at) = γt (s0) t1 (cid:89) π(aksk)P (sk+1sk, ak). (2) k=0 Similar to DPO, DMPO defines the following RL objective based on dπ: E(s,a)dπθ (s,a)[r(s, a)] max πθ βDKL[dπθ (s, a)dπref (s, a)], (3) where πref represents the reference policy. The reward function in DMPO takes the form: r(s, a) = β log dπ(s, a) dπref (s, a) + β log Z, (4) where is the partition function and remains constant for all (s, a) pairs. Then apply the BT model as follows: p(τ τ ls0) = (cid:32)Tw1 (cid:88) σ t=0 γtr(sw , aw ) (cid:33) γtr(sl t, al t) , (5) Tl1 (cid:88) t="
        },
        {
            "title": "2.2 Task Formulation",
            "content": "In SOTOPIA task, we denote the background information available to the agent as b, which includes the scenario, role profiles, and its goal. The where τ and τ represent the \"win\" and \"lose\" trajectories respectively, Tw, Tl denote the number of rounds in each. However, since = l, the partition function can not be canceled directly in Eq (5). To overcome this obstacle, DMPO introduces the length normalization and gets the final loss function (in the context of social dialogue): LDM = (cid:34)Tw1 (cid:88) (b,hw,hl)D log σ βϕ(t, Tw) log πθ(yw πref (yw hw ) hw ) t=0 Tl1 (cid:88) t= βϕ(t, Tl) log (cid:35) πθ(yl πref (yl thl t) thl t) , (6) where the discount function ϕ(t, ) = (1 γT t)/(1 γT ). closer examination of Eq (5) reveals that the summation over the (s, a) pairs should exclude γt, as it is already incorporated into dπ(s, a). Furthermore, the length normalization may introduce additional assumptions that do not always hold. detailed discussion of these issues are provided in Appendix B.3. Building on Eq. (5) without γt, we derive the SDPO loss in Section 3.3."
        },
        {
            "title": "3.1 Behavioral Cloning",
            "content": "Behavioral cloning (Pomerleau, 1991), as an effective method of imitation learning, is widely used in the construction of various LLM-based agents (Xu et al., 2024; Song et al., 2024). In this work, we utilize GPT-4-turbo as the expert to collect expert sessions on SOTOPIA-π through self-chat and interactions with GPT-4o. Based on this data, we fine-tune open-source LLMs like Llama-3.1, establishing the initial social agent for our experiments."
        },
        {
            "title": "3.2 Preference Data Construction",
            "content": "Building high-quality segment-level preference data pairs is the core of our approach. On SOTPIAπ, our social agent engages in self-chat and interactions with GPT-4o. We set threshold of 7 for the goal dimension, and all dialogues with goal completion level below this threshold are considered potential negative samples. Given negative session, the pipeline for generating positive data involves three steps, as illustrated in Figure 6. Error Locaton Unlike scenarios with clear error definitions such as math, errors in social dialogues are relatively ambiguous concept. In negative session, if our agents utterance in specific turn meets the following criteria: (1) the turn is critical for achieving the roles goal, (2) there is still room for improvement in the goal completion or their relationship, we identify that turn as erroneous. The error location is performed by GPT-4o. Positive Session Sampling After the error location, we sample 5 complete sessions based on the interaction history prior to that turn. Among these sessions, we select the one with the highest goal and relationship scores (with goal completion prioritized over relationship). If the goal or relationship score of the optimal session is higher than that of the negative sample, this session and the negative sample form data pair; otherwise, the negative sample is discarded. Segment Selection Once we obtain session-level data pairs, we provide both the positive and negative samples to GPT-4o, prompting it to select segment from the positive sample. This segment should correspond to the part that contributes to the positive sample achieving higher goal and relationship scores. Subsequently, we extract segment of the same length from the negative sample and pair it with the positive sample to form segmentlevel data pair. This process aims to exclude turns, such as pleasantries, that are not directly related to achieving the goal. The discussion of GPT-4os performance on error location and segment selection, along with the related prompts for these two steps, are provided in Appendix C.2 and C.3, respectively.."
        },
        {
            "title": "3.3 SDPO Loss",
            "content": "Session-level ETO and DMPO can not control the length of positive and negative sessions. To address this, DMPO utilizes the length normalization to eliminate in Eq (5). Different from them, SDPO selects segment from both the positive and negative sessions for optimization, allowing free control over their lengths. Specifically, after selecting segment from the positive session using GPT-4o, SDPO selects segment of equal length from the negative session. By ensuring the two segments are of equal length, we can directly eliminate Z, resulting in the following concise SDPO loss: (he,hw,hl)D log σ LSDP = (cid:18) (cid:34)e+k (cid:88) β log t=e πθ(yw πref (yw hw ) hw ) log πθ(yl πref (yl thl t) thl t) (cid:19)(cid:35) , (7) where denotes the round number of the erroneous turn, and represents the total number of rounds within the selected segments."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Datasets SOTPIA-π, used for training, includes total of 410 scenarios: 100 scenarios for BC, with 10 role pairs per scenario, and 310 scenarios for alignment, with 8 role pairs per scenario. SOTOPIA, used for testing, includes 90 scenarios, each with 5 role pairs, resulting in total of 450 tasks for self-chat and 900 tasks for non-self-chat. 4.2 Experimental Setup Training We primarily use Llama-3.1-8B-Chat (Dubey et al., 2024) as the base LLM to build the social agent. The maximum token limit is set to 4096, and AdamW optimization is employed for all training processes. During the SFT phase, the batch size is 32, the dropout rate is 0.2, and the learning rate is 1e5 with 5% warm-up ratio and cosine decay schedule. For the training phase of SDPO, the batch size remains 32, β in SDPO loss is 0.1, and the learning rate is 1e6 with no warmup but cosine decay schedule. The statistics of SDPO training data are detailed in Appendix C.1. SOTOPIA During the sampling of positive data, the temperature of the target agent is set to 1.0, while the other agents temperature is set to 0.7. For testing, we set the temperature of both interacting agents to 0.7. Though the temperature introduces randomness to the agents outputs, we find that the evaluation results remain numerically stable. Thus, we report the results based on single test."
        },
        {
            "title": "4.3 Baselines",
            "content": "We compare the proposed SDPO with several strong baselines. 1) OpenAI proprietary LLMs. We provide the specific model versions in Appendix D.1. 2) SFT Behavioral Cloning fine-tunes LLMs on expert interaction data, producing resulting model that serves as the base agent for SDPO and the following baselines. 3) DPO optimizes the agent policy based on turn-level data, specifically targeting the first differing turn in the positive and negative samples used by SDPO. 4) ETO optimizes the agent policy using session-level data. ETO utilizes the same negative sessions as SDPO while sampling five new sessions from scratch to form the data pairs. 5) DMPO leverages the same data as ETO and employs new loss function to update the policy. 6) Preferred-SFT fine-tunes the base gent on the positive sessions in SDPO. Figure 2: Variation in probability differences between positive and negative segments over training steps. DPOturn represents the probability difference in the first turn in segments."
        },
        {
            "title": "4.4 Results",
            "content": "We present the results of SDPO and all the baselines on SOTOPIA in Table 1. As shown, in both the goal and relationship dimensions, SDPO significantly outperforms turn-level DPO, sessionlevel ETO, and DMPO, even surpassing proprietary LLMs like GPT-4o by large margin, highlighting the effectiveness of segment-level alignment. By analyzing the interaction histories in SOTOPIA, we find that weaker agents often exhibit stubbornness and only express their demands repeatedly. This leads to lower goal and relationship levels, especially in self-chat scenarios. Behavioral cloning using expert data can effectively improve this situation, making the agent more communicative. The reason why Llama-8B+BCs goal rate drops in its interaction with GPT-4o is that the agent becomes persuadable. We also observe that aligned agents achieve simultaneous improvements in both goal and relationship. This indicates that alignment methods indeed enhance the social intelligence of models, rather than achieving goals through behaviors that violate social norms like threatening and deception. We also repeat the above experiments using Mistral-Instruct-v0.3 (Jiang et al., 2023), with the results presented in Table 2. The detailed experimental setup for Mistral is provided in Appendix D.2. SDPO consistently outperforms all baselines, demonstrating the generalization of our method. Self-Chat GPT-4o GPT-4o-mini GOAL REL GOAL REL GOAL REL Model GPT-4-turbo GPT-4o GPT-4o-mini GPT-3.5-turbo Llama-8B Llama-8B+BC Llama-8B+BC+DPO Llama-8B+BC+ETO Llama-8B+BC+DMPO Llama-8B+BC+Preferred-SFT Llama-8B+BC+SDPO 8.18 7.90 6.98 6.38 7.24 7.81 7.95 8.29 8.28 7. 8.56 2.96 2.67 2.11 1.36 1.94 3.05 3.28 3.39 3.37 3.05 3.69 7.92 7.90 7.44 7.19 7.70 7.53 7.80 8.02 8.00 7. 8.13 2.79 2.67 2.36 2.05 2.49 2.78 2.97 3.03 2.98 2.88 3.16 7.53 7.47 6.98 6.67 7.19 7.18 7.32 7.38 7.41 7. 7.53 AVG 5.32 5.17 4.66 4.25 4.78 5.16 5.34 5.45 5.43 5.17 2.54 2.40 2.11 1.84 2.13 2.59 2.70 2.56 2.54 2. 2.71 5.63 Table 1: The performance of various methods on SOTOPIA across the goal and relationship dimensions. Additionally, SOTOPIA designates the more challenging portion of the dataset as the Hard subset, where SDPO also achieves the best results. Detailed results and discussion are presented in Appendix E."
        },
        {
            "title": "BC\nDPO\nETO\nDMPO\nSDPO",
            "content": "Self-Chat With GPT-4o"
        },
        {
            "title": "GOAL REL GOAL REL",
            "content": "7.89 8.13 8.30 8.34 8.48 2.98 3.13 3.27 3.26 3.49 7.60 7.83 7.94 7.97 8.14 2.81 2.86 2.94 2.94 3.06 Table 2: The performance of different methods on SOTOPIA using Mistral."
        },
        {
            "title": "4.5 Analysis",
            "content": "Necessity of Multi-turn Alignment After DPO adjusts the first-turn output probabilities for positive and negative segments, will the probabilities of positive segments increase and those of negative segments decrease in subsequent turns? To explore this, we plot the probability differences between positive and negative segments for DPO and SDPO during training, as shown in Figure 2 (only SDPO can be directly compared with DPO; therefore, ETO and DMPO are not mentioned here.). The DPO-turn trajectory is nearly parallel to the DPO trajectory, indicating that DPO has almost no influence on the probability differences of subsequent turns. In contrast, the SDPO trajectory rises more steeply. These results demonstrate the necessity of explicitly modifying the probability distribution across turns within the entire segment, providing an explanation for the superiority of multi-turn Figure 3: The goal ratings and average words per session for various agents. The word count includes only the utterances of our agents and excludes those of GPT-4o. The square bracket denotes [average turns per session, average words per turn]. alignment over DPO. Variation in Model Output Length We present the output length of various agents during their interactions with GPT-4o in Figure 3. Compared to the BC agent, all alignment methods increase the output length of the models. This phenomenon is commonly observed when DPO is applied to AI chatbots. However, unlike the users potential bias toward longer responses, which might be misleading, effective social strategies in social scenarios often require more tokens for communication. Thus, Segment Length Self-Chat With GPT-4o GOAL REL GOAL REL BC - 7.81 3.05 7. 2.78 [1, 1] [3, 3] [5, 5] [Aw, Aw] [1, 3] [3, 1] [3, 5] [5, 3] [m, n] SDPO 3.28 3.64 3.60 3.69 3.08 - 3.16 - 3. 7.95 8.40 8.34 8.56 7.77 - 8.07 - 8.19 7.80 8.10 8.09 8.13 7.68 - 7.91 - 7.97 2.97 3.13 3.11 3.16 2.81 - 2.92 - 3. Table 3: Performance comparison of various segment selection methods. Aw denotes the length of the segment selected by GPT-4o from each positive sample, and its distribution is presented in Appendix C.1. m, represent the sums of all turns in negative and positive samples, respectively, after the erroneous rounds. ment. For symmetric segment lengths, methods with fixed lengths of 3 and 5 outperform the length of 1 (DPO), demonstrating the efficacy of multiturn alignment. The method with segment length of 5 is less effective than that with length of 3, indicating that longer segments are not always better. Building on this insight, we leverage GPT-4o to automatically identify key segments from each positive sample, achieving the best results. For asymmetric segment lengths, model training for segment lengths of [3,1] and [5,3] collapse and can not interact normally. Other asymmetric segments underperform compared to their symmetric counterparts, supporting the theoretical discussions in Sections 2.3 and 3.3. Furthermore, we observe that as the degree of asymmetry decreases, the models performance improves. This improvement could be attributed to the reduced effect caused by the uneliminated on the loss as asymmetry diminishes. This finding also helps explain the effectiveness of ETO, which does not impose constraints on the lengths of positive and negative sessions. Interlocutor for Sampling The alignment data for SDPO is collected separately using the BC agent itself and GPT-4o as interlocutors. We train models on each subset of data independently using SDPO, with the results summarized in Table Figure 4: Comparison of the quality of positive sessions sampled at the session level and segment level. the increase in output length is reasonable. Furthermore, we experiment with terminating the dialogue when the SDPO-tuned agent reaches 8 interaction turns. This approach achieves better performance using fewer tokens than LLama-3.1-8B, demonstrating that the SDPO-tuned agent utilizes tokens more efficiently. Efficiency of Positive Sample Utilization The quality of positive sessions sampled at the session level and segment level is illustrated in Figure 4. When the sampling count is consistent, sessionlevel positives outperform segment-level ones in both goal and relationship. Sampling from scratch provides larger sampling space compared to using partial interaction history, increasing the likelihood of generating high-quality sessions. However, some high-scoring sessions result from behavioral changes by the other agent, which prevents the model from learning correct behavioral patterns. In SDPO, the interaction history prior to erroneous turns introduces constraints to the action space of the interlocutor, effectively reducing such occurrences. Consequently, SDPO makes better use of positive samples and achieves superior performance compared to ETO and DMPO."
        },
        {
            "title": "4.6 Ablation Study",
            "content": "Segment Selection We explore different segment selection methods of SDPO, with the results presented in Table 3 (in the square brackets, the length of the negative segment is listed first, followed by the positive segment.). The segment length refers to the number of turns contained within the segSource Self-Chat With GPT-4o GOAL REL GOAL REL Self Both Self 4o Self Self 4-turbo Self 8.56 8.09 8.42 8.11 3.69 3.47 3.56 3.35 8.13 7.88 7. 7.90 3.16 3.05 3.09 3.01 Table 4: Performance comparison of models trained on data from different sources. The \"Source\" column indicates interaction participants: [test agent interlocutor]. 4. Models trained on single data source show improved performance in both self-chat and interactions with GPT-4o, further validating SDPOs generalization capabilities. Moreover, the model trained on the combined dataset outperforms those trained on individual datasets, highlighting that incorporating data from diverse interlocutors can further enhance the agents social intelligence. Out-of-Distribution Data The base BC agent learns from expert data generated by GPT-4-turbo. Would generating positive samples using GPT-4turbo result in better performance? We leverage GPT-4-turbo to interact with the BC agent and sample 5 times for SDPO. The resulting positive samples outperform self-sample ones in both goal and relationship scores. However, as shown in Table 4, the agent trained on this data underperforms compared to the self-sampling approach. This indicates that out-of-distribution positive samples are less effective than in-distribution ones. During training with out-of-distribution data, we observe that the probability of positive segments is markedly lower than that of negative segments. This significantly larger probability gap, compared to self-sampling, may account for the suboptimal performance."
        },
        {
            "title": "5 Related Work",
            "content": "Social Intelligence Social intelligence can be defined as an agents ability to understand, adapt to, and respond to the emotions, intentions, and behaviors of others in social interactions. Most research on social intelligence has centered around evaluation. For example, SOCIALIQA (Sap et al., 2019) emphasizes commonsense reasoning about social situations, while SocialIQ (Zadeh et al., 2019) extends evaluation modalities from plain text to video. Shapira et al. (2023) assess large language models (LLMs) using the Faux Pas Test, and SocialBench (Chen et al., 2024) evaluates the sociality of role-playing agents at both individual and group levels. Additionally, some studies (Le et al., 2019; Shapira et al., 2024) examine models social intelligence from theory-of-mind perspective. However, with the advancement of LLM, LLM-based social agents are now capable of interacting in real social scenarios. The traditional static QA-style benchmarks are no longer sufficient to evaluate the social intelligence of the agents. SOTOPIA (Zhou et al., 2024) is currently the only dynamic and interactive social benchmark, providing simulated testing environments for contemporary social agents. We hope this work will inspire further research aimed at enhancing the social intelligence of models through methodological innovation. Alignment Methods of Different Granularities Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) is an effective approach for aligning LLMs outputs with human preferences. However, it has notable drawbacks, including instability, complexity, and high resource consumption. To address these challenges, (Rafailov et al., 2023) introduce DPO, which simplifies training by using offline data with straightforward classification loss. We classify DPO as turn-level alignment method, and various alignment algorithms at different granularities have been developed based on DPO. Token-level DPO (TDPO) (Zeng et al., 2024) integrates forward KL divergence constraints at the token level, enhancing both alignment and diversity. Step-DPO (Lai et al., 2024) utilizes individual reasoning steps for preference optimization instead of holistic answerlevel evaluation. However, in multi-turn interaction scenarios such as goal-oriented social dialogues or web navigation, single-turn alignment is insufficient. To tackle this, ETO and DMPO extend DPO to multi-turn contexts by leveraging session-level data. We take step further by proposing SDPO, which introduces segment-level sample pair optimization framework to achieve finer-grained alignment in multi-turn interactions."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce Segment-Level Direct Preference Optimization (SDPO) to improve the performance of LLM-based agents in multiturn, goal-oriented social dialogues. Unlike existing alignment methods such as turn-level DPO and session-level approaches including ETO and DMPO, SDPO focuses on optimizing the agent policy by targeting specific key segments within session. Our extensive evaluation on the SOTOPIA benchmark shows that SDPO significantly outperforms existing methods, highlighting the superiority of segment-level alignment. Looking ahead, we plan to apply SDPO to other agent tasks to further explore its versatility and effectiveness."
        },
        {
            "title": "7 Limitations",
            "content": "Our proposed SDPO assumes equal lengths for positive and negative segments, achieving state-ofthe-art performance under this assumption. Specifically, after selecting segment from the positive sample, we choose segment of the same length from the negative sample to eliminate the partition function Z. However, this approach has certain limitations. Negative segments may include irrelevant or error-free turns, or fail to capture all erroneous turns, highlighting the need for more fine-grained control when selecting segments from negative samples. Currently, we have not identified theoretical framework that effectively supports the alignment of segments with unequal lengths. We hope our work will inspire further research and encourage diverse theoretical analyses for addressing this issue in multi-turn alignment. Additionally, as SOTOPIA is currently the only available interactive social benchmark, our experiments are conducted exclusively on this dataset. In the future, we plan to incorporate additional interactive agent tasks to further validate the generalizability of SDPO."
        },
        {
            "title": "References",
            "content": "Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Gao Xing, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, and Fei Huang. 2024. SocialBench: Sociality evaluation of role-playing conversational agents. In Findings of the Association for Computational Linguistics: ACL 2024, pages 21082126, Bangkok, Thailand. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. 2024. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629. Matthew Le, Y-Lan Boureau, and Maximilian Nickel. 2019. Revisiting the evaluation of theory of mind through question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 58725877, Hong Kong, China. Association for Computational Linguistics. Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024. Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 78287840, Bangkok, Thailand. Association for Computational Linguistics. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122. Dean A. Pomerleau. 1991. Efficient training of artificial neural networks for autonomous navigation. Neural Computation, page 8897. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463 4473, Hong Kong, China. Association for Computational Linguistics. Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. 2024. Clever hans or neural theory of mind? stress testing social reasoning in large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. 2024. SOTOPIA: Interactive evaluation for social intelligence in language agents. In The Twelfth International Conference on Learning Representations. Papers), pages 22572273, St. Julians, Malta. Association for Computational Linguistics. Natalie Shapira, Guy Zwirn, and Yoav Goldberg. 2023. How well do large language models perform on faux pas tests? In Findings of the Association for Computational Linguistics: ACL 2023, pages 1043810451, Toronto, Canada. Association for Computational Linguistics. Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, and Fuli Feng. 2024. Direct multi-turn preference optimization for language agents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 23122324, Miami, Florida, USA. Association for Computational Linguistics. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. 2024. Trial and error: Exploration-based trajectory optimization of LLM agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 75847600, Bangkok, Thailand. Association for Computational Linguistics. Noah Wang, Z.y. Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhao Huang, Jie Fu, and Junran Peng. 2024a. RoleLLM: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1474314777, Bangkok, Thailand. Association for Computational Linguistics. Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Yonatan Bisk, Graham Neubig, and Hao Zhu. 2024b. SOTOPIA-π: Interactive learning of socially intelligent language agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1291212940, Bangkok, Thailand. Association for Computational Linguistics. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations. Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency. 2019. Social-iq: question answering benchmark for artificial social intelligence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88078817. Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. 2024. Tokenlevel direct preference optimization. arXiv preprint arXiv:2404.11999."
        },
        {
            "title": "A Modifications to SOTOPIA",
            "content": "proof. The loss function is as follows: In SOTOPIA, each interaction is structured as single-turn format, which does not support multiturn alignment. To address this limitation, we modify the prompt organization format, as illustrated in Figure 5. These modifications are applied before invoking LLMs APIs, ensuring they remain invisible to SOTOPIA itself and do not impact the evaluation of GPT-4o. Further details can be found in our code repository. Figure 5: Prompt organization formats in original and modified SOTOPIA, respectively."
        },
        {
            "title": "B Supplementary Theoretical Analysis",
            "content": "B.1 DPO Rafailov et al. (2023) propose Direct Preference Optimization (DPO), method that leverages pairwise preference data to train policy models. In the context of social dialogue, we denote the number of the erroneous round as e. The DPO loss function is as follows: LDP = (cid:20) πθ(yw πref (yw (he,yw he) he) β log e)D log σ ,yl β log πθ(yl πref (yl ehe) ehe) (cid:21) , (8) where yw πθ(he) represent positive and negative output in the erroneous turn respectively. , yl B.2 ETO LET = (b,hw,hl)D log σ (cid:34) Tw1 (cid:88) t=0 β log πθ(yw πref (yw hw ) hw ) Tl1 (cid:88) t=0 β log (cid:35) πθ(yl πref (yl thl t) thl t) (9) where hw, hl represent complete positive and negative interaction histories respectively, Tw, Tl denote the number of rounds in each. B.3 Discussion on DMPO In Eq (5), the reward for the entire sequence should be calculated as the summation over all (s, a) pairs. Lets first discuss why it is valid to sum over time steps t. For LLMs, the state can be viewed as the input context, while represents the models output. In multi-turn interactions, all (s, a) pairs within the sequence are unique. Thus, summing over time steps is equivalent to summing over (s, a) pairs, making the process more straightforward. However, essentially, each (s, a) pair should be treated equally in the summation, without any inherent concept of time step t. Therefore, introducing discount factor γt is not appropriate. The correct Eq (5) is shown as follows: p(τ τ ls0) = (cid:32)Tw1 (cid:88) σ r(sw , aw ) t=0 (cid:33) r(sl t, al t) . (10) Tl1 (cid:88) t=0 Moreover, DMPO introduces regularization for rounds based on Eq (5) to eliminate Z. However, it does not discuss the impact of length normalization, and this transformation lacks rigorous theoretical justification."
        },
        {
            "title": "C Data Construction Details",
            "content": "C.1 Statistics and Analysis of SDPO Data SDPO dataset consists of 1019 pairs. The distribution of erroneous turns identified by GPT-4o is presented in Table 5. The distribution of segment lengths identified by GPT-4 is shown in Table 6. Additionally, the distribution of truncated turns is provided in Table 7. Song et al. (2024) propose Exploration-Based Trajectory Optimization (ETO), which extends turnlevel DPO to the session level without rigorous Combining Table 3 and 6, though segments of length 3 account for nearly 90% in the automatic segment length selection, the performance of the Figure 6: Data construction pipeline for SDPO. represents the agent to be tested. here denotes GPT-4o."
        },
        {
            "title": "Number",
            "content": "1 3 5 >7 358 104 160 Proportion (%) 30"
        },
        {
            "title": "Truncated Turns",
            "content": "0 2 4 >6 Table 5: Distribution of erroneous turns identified by GPT-4o. The index refers to the position of each erroneous turn."
        },
        {
            "title": "Number",
            "content": "174 471 248 126 Proportion (%) 46 24 23 Table 7: Distribution of the number of truncated turns."
        },
        {
            "title": "Segment Length",
            "content": "1 3 5 >"
        },
        {
            "title": "Number",
            "content": "41 909 60 Proportion (%) 4 6 9 1 Table 6: Distribution of segment lengths identified by GPT-4o. automatic selection method still clearly surpasses that of the fixed segment length-3 method, highlighting the effectiveness of the automatic selection approach."
        },
        {
            "title": "Location\nSelection",
            "content": "27.3 25.0 10.7 13.3 2.0 1.7 Table 8: Manual evaluation on error location and segment selection using GPT-4o. Figure 7: The prompt for error location with GPT-4o. \"History\" encompasses the background information, including the scenario, role profiles, goals, and interaction history within the negative session. processes. During the SFT phase, the batch size is 32, the dropout rate is 0.2, and the learning rate is 3e6 with 5% warm-up ratio and cosine decay schedule. For the training phase of SDPO, the batch size remains 32, β in SDPO loss is 0.1, and the learning rate is 5e7 with no warm-up but cosine decay schedule. The construction of SDPO data for Mistral follows the same process as that for Llama."
        },
        {
            "title": "E Additional Empirical Results",
            "content": "SOTOPIA designates the more challenging portion of the dataset as the Hard subset, with detailed results presented in Table 9. The ranking of various methods on the Hard subset is generally consistent with their performance on the full dataset. SDPO still achieves the best results, indicating that SDPO enhances the agents social intelligence across scenarios with different difficulty. C.2 GPT-4os Performance in Pipeline We randomly select 40 data pairs from SDPO data, and three authors independently evaluate GPT-4os performance in error localization and segment selection. In the context of social dialogues, the notions of correctness and error are inherently ambiguous. To address this, we define three evaluation categories: correct, ambiguous, and incorrect. The average evaluation results are presented in the Table 8. The evaluators all report that the primary reason for ambiguity is that they can determine GPT-4os choices are reasonable but find it difficult to judge whether they are optimal. Overall, we conclude that GPT-4o is capable of handling error localization and segment selection. C.3 Prompts in Data Construction The prompts for error localization and segment selection with GPT-4o are presented in Figures 7 and 8."
        },
        {
            "title": "D Supplementary Experimental Setup",
            "content": "D.1 Versions of OpenAI LLMs The OpenAI LLMs we used are as follows: GPT4o-2024-08-06, GPT-4-turbo-2024-04-09, GPT-4omini-2024-07-18, and GPT-3.5-turbo-0125. D.2 Mistral Training Details Consistent with the experimental settings of Llama, the maximum token limit is set to 4096, and AdamW optimization is employed for all training Figure 8: The prompt for segment selection with GPT-4o. s1 represents the complete positive session, s2 denotes the negative session, and indicates the error turn index, specifically the first differing turn between the positive and negative sessions."
        },
        {
            "title": "Model",
            "content": "Self-Chat GPT-4o GPT-4o-mini"
        },
        {
            "title": "GOAL REL GOAL REL GOAL REL",
            "content": "GPT-4-turbo GPT-4o GPT-4o-mini GPT-3.5-turbo Llama-8B Llama-8B+BC Llama-3.1-8B+BC+DPO Llama-8B+BC+ETO Llama-8B+BC+DMPO Llama-8B+BC+P-SFT Llama-8B+BC+SDPO 6.20 6.10 4.53 3.52 4.94 6.51 6.69 6.40 6.67 6.36 7.10 2.36 2.14 1.13 0.65 0.33 2.60 3.00 2.80 2.85 2.58 3.22 6.23 6.10 5.32 4.54 6.17 5.71 6.37 6.47 5.90 5.61 6.69 2.41 2.14 1.60 1.26 1.65 2.13 2.43 2.50 2.47 2.26 2.78 4.96 5.15 4.53 3.65 5.03 4.43 5.18 5.20 4.92 4.69 5. 1.76 1.59 1.13 0.89 1.26 1.76 1.68 1.81 1.86 1.63 1."
        },
        {
            "title": "AVG",
            "content": "3.99 3.87 3.04 2.42 3.23 3.86 4.23 4.20 4.11 3.86 4.44 Table 9: The performance of various methods on SOTOPIA-Hard-Subset across the goal and relationship dimensions."
        }
    ],
    "affiliations": [
        "TMCC, CS, Nankai University",
        "Tongyi Lab",
        "alibaba-inc.com"
    ]
}