{
    "paper_title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
    "authors": [
        "Lihuang Chen",
        "Xiangyu Luo",
        "Jun Meng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent."
        },
        {
            "title": "Start",
            "content": "PREPRINT VERSION. ACCEPTED TO NOTHING YET. PREPRINT VERSION. 1 LEO-RobotAgent: General-purpose Robotic Agent for Language-driven Embodied Operator Lihuang Chen, Xiangyu Luo, and Jun Meng 5 2 0 D 1 1 ] . [ 1 5 0 6 0 1 . 2 1 5 2 : r AbstractWe propose LEO-RobotAgent, general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEORobotAgent framework is designed with streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates human-robot interaction mechanism, enabling the algorithm to collaborate with humans like partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent. Index TermsAutonomous task planning, LLMs-based agent, intelligence, embodied intelligence, general-purpose human-robot collaboration. robotic I. INTRODUCTION ARGE language models (LLMs) and robotics are two highly prominent fields in current technology. In recent years, growing number of outstanding works have begun to focus on the integration of these two domains. Endowed with powerful natural language understanding and logical reasoning capabilities, LLMs hold considerable research value in the field of robot task planning. In the traditional robotics industry, developers need to manually design corresponding programs with specific logic for individual tasks. These programs suffer from poor flexibility and lack generalization. As task complexity and details increase, the program architecture inevitably becomes cumbersome and overcomplicated. The emergence of LLMs has made us realize that this entity with strong reasoning capabilities can effectively replace the diverse and complex logics that we previously embedded in programs Lihuang Chen, Xiangyu Luo, and Jun Meng are with Zhejiang lihuangchen@zju.edu.cn; University, Hangzhou 310000, China (e-mail: 22360566@zju.edu.cn; junmeng@zju.edu.cn). Corresponding author: Jun Meng. This work was supported by the Pioneer and Leading Goose R&D Program of Zhejiang Province (No. 2024C01170), the National Natural Science Foundation of China (No. 52475033) and the Robotics Institute of Zhejiang University under Grant K11801. manually. Indeed, works over the past two years have incorporated LLMs into robot planning to varying degrees. References [1] and [2] present and summarize numerous studies that have verified on different robot platforms, such as unmanned aerial vehicles (UAVs) and robotic arms, that LLMs can effectively enhance the planning performance of specific tasks, make program design more intuitive, and allow direct human-robot interaction. Meanwhile, intelligent agent architectures represented by the ReAct framework [3] have emerged and achieved remarkable development in the field of LLMs, aiming to address LLM task planning and execution problems in pure software applications. This is an exciting emerging subfield, indicating that LLMs also have the potential to be applied in robotics. This would allow LLMs, as the brain, to truly possess the body and tools to control robots performing various physical tasks. In this letter, we conduct in-depth research on the application of intelligent agent frameworks in the robotics domain. We propose self-designed, streamlined, and efficient generalpurpose robotic agent framework, enabling LLMs to independently perform behaviors such as planning, action execution, and reflective adjustment based on user requirements and task information, ultimately achieving self-iterative control of robots to complete tasks. Human users can optionally intervene during task execution to implement interactive behaviors through experiments, we of varying degrees. Furthermore, present the usage suggestions of different technologies for task deployment and human-robot communication from the perspective of ordinary users in this framework. Inspired by existing excellent works, we test and compare the strengths and weaknesses of agent frameworks with different core principles and structures in our designed task scenarios, hoping to provide valuable references for researchers. In summary, our main contributions are as follows: agent intelligent 1) We developed an elegantly structured general-purpose robotic framework called LEORobotAgent. The framework is equipped with humanrobot interaction capabilities. We applied several prompt engineering techniques from the LLMs domain to explore the boundary of the task planning capabilities of LLM within this framework and identified the current problems and challenges faced by LLMs in complex task planning. 2) We provide complete program system built around this framework. The system features simple and easily configurable environment with visual and monitorable operation processes. Users can register various tools in the frameworks toolset module to meet customized PREPRINT VERSION. ACCEPTED TO NOTHING YET. PREPRINT VERSION. 2 3) We verified that requirements. With high flexibility, the framework effectively lowers the threshold for human-robot interaction. this framework can be applied to different types of robots to accomplish tasks of varying complexity across diverse scenarios, exhibiting general applicability and high generalization. II. RELATED WORK 1) Large Models for Auxiliary Tasks: Large Models have been applied to varying degrees across diverse single scenarios or robot types. They can integrate semantic information from multiple sources, such as sensors, visual models, and task objectives [4], and provide critical information like positional relationships for visual image understanding to assist robotic arms in task execution [5]. Additionally, LLMs can be incorporated into the control loop of Unmanned Aerial Vehicles (UAVs) to generate dynamic and safe flight recommendations low frequencies [6], or serve as high-level planners to at develop action plans for disaster relief scenarios [7]. VisionLanguage Models (VLMs) can also act as downstream models of visual systems to further analyze urban patrol scenarios [8]. 2) LLMs in Task Planning and Execution: Representative works [9], [10], [11] have highlighted the significance of program generation for task execution, where executable code translates LLMs action plans into direct execution to accomplish short-duration, simple tasks. [12], [13] adopt straightforward approach by enabling LLMs to output robotic arm actions for completing basic task steps. In contrast, [14], [15], [16], [17] truly empower LLMs to act as agents for task planning and step-by-step execution: [15], [16] employ multiple LLMs with divided responsibilities (e.g., planning and supervision), while [17] designs complex upstreamdownstream framework that allows LLMs to perform multilevel planning and establish effective feedback channels for error correction. 3) Other Inspiring Works: [18] trains multimodal large language model for embodied reasoning tasks in robotics. [19] provides comprehensive task benchmarks for VisionLanguage-Action models(VLAs). The Qwen series models proposed in [20] and [21] offer excellent model options for our designed framework. [22] develops robust system for hardware-software co-design of LLMs on UAVs. [23] identifies remaining limitations of LLMs in task planning, understanding, and reasoning through benchmark testing and analysis."
        },
        {
            "title": "Current research endeavors face several",
            "content": "limitations and challenges. Most works focus solely on single robot type to execute single task category. In many studies, LLMs only participate in task planning to limited extent, resulting in poor algorithmic autonomy. Some program-generation-based frameworks often require rigorous validation, and the generated programs themselves cannot perform tasks that demand natural language understanding. Additionally, complex LLMcentric frameworks constructed in numerous works typically introduce issues such as difficult debugging and low stability. Furthermore, most algorithms do not support secondary human-robot interaction after task deployment. Fig. 1. Basic schematic of LEO-RobotAgent. The LLM is capable of planning, reasoning and evaluating tasks, while invoking tools to execute actions. After obtaining environmental observations, it proceeds to the next step until the task is completed, with users able to interact with the system at any time. III. METHODOLOGY The basic working principle of the LEO-RobotAgent frametask dework is illustrated in Fig. 1. After users input scriptions and relevant information, LLMs generate reasoning and planning content based on pre-defined prompts, while simultaneously invoking tools to execute actions. Each action yields corresponding observations, forming closed feedback loop. During the whole process, users can interact with the LLMs at any time according to the feedback, to participate in and alter the task trajectory. The framework features highly streamlined structure, functioning as self-cycling engine that endows LLMs with sufficient autonomy and flexibility. This section will elaborate on the specific technical details and detailed system architecture. The specific implementation details of this framework are illustrated in Fig. 2, while the complete system design is presented in Fig. 3. A. Agent for Robotics to output content 1) LLM Configuration: First, it should be clarified that the natural language text normally output by LLMs cannot be directly used for interaction; thus, we strictly constrain it in JSON format. The pre-configured system prompt for LLMs includes key information such as requirements for JSON output content, tool-related details, interpretation of historical data, and the current role definition of LLMs, ensuring that LLMs can take correct actions according to task requirements and progress. The JSON output by LLMs must contain at least three components: Message, Action, and Action Input. Among them, the Message optionally conveys LLMs task planning, assessment of the current situation, and reasoning processes, while the other two components specify the action to be executed by LLMs in the current step and its corresponding input parameters. It is worth noting that LLMs must be required to output the Message first to ensure that the subsequent action is guided by the current reasoning. 2) Toolset Module: The toolset encompasses all tools that can be invoked by LLMs. We can implement tools tailored to task requirements and register them in this module. Various core capabilities of robots, such as basic control, perception, and environmental interaction, can be realized in the form of tools; this module can even invoke other LLMs to construct PREPRINT VERSION. ACCEPTED TO NOTHING YET. PREPRINT VERSION. 3 Fig. 2. Detailed implementation diagram of LEO-RobotAgent. Based on pre-defined prompts and user tasks, LLMs output content containing information, actions, and action parameters. The toolset can cover various domains according to actual scenarios and is required to provide basic information such as activation status, tool names, corresponding functions, and tool descriptions. Observations will generate diverse feedback content depending on different tools. During the iterative process, historical records (History) are continuously accumulated to support subsequent operations of LLMs. multi-agent architecture. Each tool entry must include at least the tool name and its corresponding function name, tool description (specifying input and output formats), and availability status of the tool for the current task. This module endows the framework with flexibility in behavioral capabilities and provides simple yet convenient interface for users to configure tools according to practical demands. The output of tool, namely the Observation, varies depending on the tools nature. It can be description of target detection results, feedback on UAVs arrival at waypoint, confirmation of successful robotic arm motion planning, analysis of received tasks, and so on. 3) Cycle Structure and History Mechanism: With LLMs and the toolset in place, basic agent loop can be constructed, where LLMs continuously reason and invoke tools, then proceed to the next step upon receiving feedback. During this process, historical records are gradually accumulated, which include user tasks, LLMs outputs at each step, tool observations from each iteration, and interim inputs from users. When LLMs determine that the task has been completed or cannot proceed further during execution, the loop will terminate and generate final response. This response can be task completion report or description of the specific interruption cause. 4) Human-Robot Interaction Mechanism: Task instructions input by users must at least specify the task content, essential initial states, and scenario descriptions. When necessary, practical examples and precautions can be provided to standardize the action logic of LLMs. If the current task permits human intervention and collaboration, users can interrupt task execution while the agent framework is running to correct the frameworks existing errors and provide guidance, temporarily modify task content, issue instructions for the next task phase, and so on. Based on this mechanism, the framework, as an intelligent agent, is endowed with complete two-way humanrobot interaction capabilities. B. Complete Agent System As illustrated in Fig. 3, We have designed complete, interactive ROS-based system around the framework elaborated in the previous section, where LLMs and the toolset are deeply integrated into the ROS system as Agent nodes. The tool module can effectively interface with variety of tools; for instance, robot control nodes, visual perception nodes, RetrievalAugmented Generation (RAG), simulation environments, and others can be registered in the toolset and invoked by the Agent node after their functions are implemented. Dialogue messages, robot control, perception functions, and even tool feedback of the framework all rely on the topic mechanism of ROS for stable and long-term communication. For users, interaction with the ROS system is enabled via visual interface we built based on Web applications, making the dialog and communication with the Agent framework no different from interactions on conventional LLM platforms. Specifically, topics are transmitted via RosBridge, and video streams are transmitted and displayed through VideoServer. registration, node startup/shutdown, task preconfiguration, and other operations based on WebSocket, such that debugging and operation of the entire system can be conducted almost entirely within this application interface. This architecture provides paradigm for constructing LLM-agent applications in the robotics domain. In addition, we have refined tool PREPRINT VERSION. ACCEPTED TO NOTHING YET. PREPRINT VERSION. 4 success rates were high and close to each other, demonstrating good algorithm stability. Notably, partial failures in real-world experiments were attributed to limitations in the accuracy of flight control tools and object localization errors, which prevented the UAV from precisely hovering above the trash can. This experiment verifies that the framework can be effectively deployed on UAVs, and the algorithm enables straightforward sim-to-real transfer, demonstrating feasibility for engineering implementation. Since the reasoning logic of LLMs itself is not affected by whether the task is executed in virtual or physical environment, the sim-to-real gap of this framework largely depends on the sim-to-real gaps of the robot and the tools we have implemented. B. Prompt Experiment The design of prompts during task deployment largely determines the output quality of LLMs, which in turn affects task execution performance. This experiment verifies the impact of Chain-of-Thought (CoT) [24] and one-shot [25] on the task planning effectiveness and performance of the framework. CoT requires prompts to guide the agent in reasoning or provide reasoning processes, while one-shot entails providing an example to assist LLMs in comprehension. The task is divided into two subtasks: indoor small-scenario search and urban large-scenario search  (Fig. 5)  . The former requires the UAV to use target detection to locate all identifiable objects indoors as much as possible, while the latter only requires finding target building (pavilion) in large urban scene using VLM as the perception tool. Experimental results are presented in Table I, where each method was tested 10 times per scenario to obtain average outcomes. The time/item refers to the time required for the UAV to locate each unique object, while success time denotes the total duration of the task when it is completed successfully. It is evident that both methods effectively improve task planning performance, with the best results achieved when combining them. One-shot yields the fastest task execution in successful cases, as it enables relatively robust operations based on existing examples, ensuring high performance floor. In contrast, CoT incurs substantial token and time overhead, since it requires LLMs to conduct extensive, step-by-step reasoning and planning. As visualized in Fig. 6 for the UAVs field-of-view coverage during indoor searching tasks, without prompts, the UAV often performs ineffective searches (e.g., reaching boundaries and pointing the camera outward). One-shot leads to exploration patterns that closely mimic the provided example but fails to effectively cover corner details. CoT fosters more divergent reasoning in LLMs, generating feasible and innovative search paths. The combined approach produces search paths that are both thorough and efficient, and similar trends are observed in urban search scenarios. This sections experiments confirm that common prompting techniques in the LLM domain can be effectively applied to robotic agent frameworks. Therefore, when assigning tasks, we should provide as much necessary information as possible if Fig. 3. An application system designed around LEO-RobotAgent. We have built this complete system for the framework based on ROS and Web technologies. Users can directly operate the visual interface to configure existing tools, conduct conversations and interactions with the Agent, and monitor dialogue sessions. The system features high scalability and ease of use in terms of tool registration, node startup and shutdown, and other aspects. IV. EXPERIMENTS This section validates the feasibility, general value and efficiency of the LEO-RobotAgent framework through series of experiments. The experimental tasks cover multiple types of robots, tasks with varying complexity levels, and their corresponding scenarios. Additionally, comparative experiments are conducted to demonstrate how to interact with the agent using different prompt techniques, as well as to illustrate the characteristics scenarios of agent frameworks with different structures and underlying principles. A. Feasibility Verification and Real UAV Experiment This subsection conducts simulation experiments on UAV via moderately complex object-search task to initially verify that the framework can autonomously complete tasks and that its human-robot interaction function is practical. The same task is further executed on real UAV to validate the algorithms easy sim-to-real transfer. The UAV is chosen because it integrates control, perception, and interaction capabilities, representing the general competencies of robots. Equipped with depth camera, the UAV is required to search for specified nearby object in small scenario using target detection, and then fly above it after receiving secondary user instruction to complete the task. This task employs the UAVs position-hold flight control and target detection as core tools. In real experiments, an electromagnetically attached iron ball is mounted under the UAV, which must additionally drop the ball into target container. The experimental results are illustrated in Fig. 4. Ten trials were conducted for both simulation and real-world experiments, with 9 and 7 successful attempts respectively. The PREPRINT VERSION. ACCEPTED TO NOTHING YET. PREPRINT VERSION. 5 Fig. 4. Feasibility verification and real experiment The performance of object-search tasks. In the simulation environment, the UAV rotated sequentially to perform target detection. After completing identification, it reported the results to the user. Once the user specified target, the UAV flew to its overhead position, and the task was successfully completed. In real-world experiments, the UAV also accomplished the task correctly: it located the target (trash can) and ultimately dropped the ball into it smoothly. TABLE RESULTS OF PROMPT EXPERIMENT IN TWO SCENARIOS Method Room Scenario City Scenario score(20) token usage time(s) time/item(s) success rate(%) token usage time(s) success time(s) zero-shot one-shot CoT one-shot+CoT 7.13 17.33 16.50 17. 6714 18537 39937 36476 59.83 101.93 150.14 133.23 8.39 5.88 9.10 7.48 20.0 50.0 60.0 70.0 32656 32048 37791 44985 175.23 156.65 155.00 180. 183.83 123.38 126.93 172.81 (a) Room (b) City Fig. 5. UAV conducting indoor and urban searching tasks with prompt engineering techniques. conditions permit, such as pre-emptive avoidance of potential errors, demonstrations of feasible planning strategies, and logical analyses of reasoning processes. C. Agent Framework Architecture Comparison Experiment This subsection compares the performance of agent schemes with different principles and architectures across tasks of varying complexity. Based on the core ideas from the outstanding prior works mentioned earlier, we summarize four other highly distinctive schemes  (Fig. 7)  . Thanks to the simplicity and high scalability of our framework, these abstracted architectures can be easily adapted to the toolset, human-robot interaction mechanism, and application system of our framework. The schemes are described as follows: TABLE II OVERVIEW OF AGENT COMPARISON EXPERIMENT Task Description Delivery Searching Handover Given the coordinates of three bottles and three target spots, the robot must pick and place them in an arbitrary order and finally return to the origin. Command the robot to search for the nearest bottle in the scene, pick it up, and return to the origin. Two people are located by chair and lamp respectively. The robot is instructed to find the former, approach to receive sub-task, and return to the origin after completion. Sub-task: Retrieve bottle and place it near the other person. Perc. NLU 1) Direct Action Sequencing Agent (DAS): LLMs directly generate complete sequence of actions based on the task text and available toolset, and the system executes the sequence step-by-step in fully open-loop manner. 2) Code-generating Execution Agent (CGE): LLMs output executable Python code, where the action statements still do not exceed the scope of the toolset, and the code is directly executed after review. 3) Dual-LLM PlanEvaluate Agent (DLLMs): This archiPREPRINT VERSION. ACCEPTED TO NOTHING YET. PREPRINT VERSION. 6 (a) Room (b) City Fig. 6. Field of view coverage map for UAV searching tasks in indoor and urban scenarios. Thick black rectangles denote scenario boundaries. Stars denote the target to find. Deeper blue indicates higher number of times the UAVs field of view has covered the location. Different colored paths represent the search process of each task, marking all waypoints and yaw angles. Fig. 7. LEO-RobotAgent and four other agent schemes. (a) Robot (b) Cafe Fig. 8. The wheeled robot with robotic arm and the map of cafe for Agent Framework Architecture Comparison Experiment. tecture relies on the collaboration of two LLMs: the Planner is responsible for planning, reasoning, and executing actions; the Evaluator evaluates the planning content and execution feedback and then puts forward suggestions for the formers outputs. The process loops in this way and accumulates historical records. 4) Tri-LLM PlanActEvaluate Agent (TLLMs): This architecture decomposes agent functions into three LLMs: the Planner generates high-level reasoning and task plans; the Actor converts the plans into specific tool calls; the Evaluator analyzes execution performance based on observation results and provides new inputs. Each plan of the Planner operates in large loop, which can divide the task into single or multiple stages; within each plan, the Actor executes step-by-step and the Evaluator conducts gradual evaluation, which operates in small loop."
        },
        {
            "title": "Owing to the differing capability boundaries of",
            "content": "these schemes: The DAS lacks closed loop and cannot perform perception (Perc.); The CGE can achieve closed-loop functionality via program design but has no natural language understanding (NLU) capability. For this experiment, we designed three tasks of varying complexity to ensure that every scheme could participate in at least one task. The task platform was PREPRINT VERSION. ACCEPTED TO NOTHING YET. PREPRINT VERSION. 7 TABLE III RESULTS OF AGENT COMPARISON EXPERIMENT Task Agent Score (10) Token Usage Time (s) Success Time (s) Perfect Rate (%) Delivery Searching Handover DAS CGE DLLMs TLLMs LEO (ours) DAS CGE DLLMs TLLMs LEO (ours) DAS CGE DLLMs TLLMs LEO (ours) 9.156 9.344 7.938 8.375 9. 5.375 3.125 5.875 7.875 4.933 4.867 7.867 1054 968 24684 43278 19000 1447 54056 115848 14597 58757 188596 33577 232.18 208.89 233.35 277.37 264. 73.76 203.77 266.84 141.44 313.23 432.44 257.71 228.14 207.87 245.53 302.32 272.71 97.93 165.78 190.32 120.08 276.82 597.95 265.61 68.75 75.00 12.50 37.50 62. 43.75 18.75 43.75 56.25 13.33 13.33 46.67 wheeled mobile robot equipped with robotic arm  (Fig. 8)  ; this robot structure was fully custom-designed in simulation environment and could perform fixed-point movement, as well as simple grasping and releasing operations. The three tasks are detailed in Table II. Task 1 only requires completing object delivery in non-fixed sequential manner. Task 2 necessitates distinguishing different objects of the same category based on their relative distance relationships. In contrast, Task 3 involves fulfilling nested primary and subtasks, which requires understanding sub-tasks presented in natural language and conducting secondary planning; for this purpose, we introduce the summarization capability of LLMs as new tool. Given the lengthy workflow of this task, it also tests the agents ability to plan and memorize long-horizon tasks. We conducted 15 experiments for each scheme in every scenario, using the same large language model and identical task descriptions; the results are presented in Table III. The score was designed based on the degree of task completion, with full score of 10 points. Perfect Rate denotes the ratio of achieving full score, i.e., completing the task in its entirety. It can be observed that for Task 1, which is simple and well-defined, one-time generation methods such as DAS and CGE exhibit strong stability. Moreover, these methods incur extremely low token and time overhead in the tasks they can execute, indicating that they remain suitable for relatively simple and clearly specified tasks. In contrast, DLLMs and TLLMsframeworks with multiLLM division of laborunexpectedly yielded low scores. This is largely because multiple roles in the framework require tuning with corresponding preset prompts, and despite investing far more effort in this calibration than for other agent schemes, this was the best performance achieved. Coordination among multiple LLMs leads to rapid surge in token consumption and higher propensity for hallucinations, which hinders the execution of normal plans or results in the omission of key details. For instance, the Evaluator often incorrectly triggers replanning operations, thereby impeding the normal execution of plans; alternatively, misinterpretation of overly lengthy text may result in the omission of detailed steps such as returning to the origin, which are specified in the task requirements. Our framework not only achieves favorable performance in Task 1 (where DAS and CGE excel) but also generates fewer hallucinations in the relatively challenging Tasks 2 and 3, maintains stable long-term memory, and incurs reasonable time and token costs. This experiment indicates that LEO-RobotAgent, leveraging its more streamlined agent architecture, achieves lower debugging overhead and superior task planning performance with enhanced robustness, which validates the principle of less is more. Meanwhile, we successfully completed the experiment using this fully customized robot architecture, which also verified the universal value of the framework. V. CONCLUSION"
        },
        {
            "title": "This",
            "content": "letter proposes LEO-RobotAgent, streamlined, general-purpose agent framework tailored for robotic embodied operators. It exhibits remarkable versatility across multiple dimensions, including robot types, task scenarios, and task contents, with high degree of autonomy. complete application-level system built around LEO-RobotAgent significantly reduces the difficulty of human-robot interaction and mutual understanding. Moreover, the full solution features straightforward sim-to-real transfer and strong portability."
        },
        {
            "title": "The prompt experiment shows that",
            "content": "imparting sufficient reasoning guidance and task-related information such as reference example to the agent yields significant improvements in task planning performance. Through agent comparison experiments, we conclude that streamlined framework can better unleash the capabilities of LLMs and reduce the debugging burden for users. Our framework assigns multiple responsibilities to single LLM and allows it to sequentially complete the reasoning and action stages within each step, thereby endowing the agent with more coherent and unified thinking process and rational planning path. Additionally, the adoption of auxiliary LLMs or VLMs as tools for summarization and perception in our experiments demonstrates that multi-role architectures can be unified via our frameworks toolset module. The experiments also reveal the limitations of our method. Given the weak spatial common-sense understanding of current LLMs, it is necessary to guide them with processing PREPRINT VERSION. ACCEPTED TO NOTHING YET. PREPRINT VERSION. 8 [18] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang et al., Palm-e: An embodied multimodal language model, 2023. [19] S. Zhang, Z. Xu, P. Liu, X. Yu, Y. Li, Q. Gao, Z. Fei, Z. Yin, Z. Wu, Y.-G. Jiang et al., Vlabench: large-scale benchmark for languageconditioned robotics manipulation with long-horizon reasoning tasks, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025, pp. 11 14211 152. [20] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu et al., Qwen2. 5-coder technical report, arXiv preprint arXiv:2409.12186, 2024. [21] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv et al., Qwen3 technical report, arXiv preprint arXiv:2505.09388, 2025. [22] J. Zhao and X. Lin, General-purpose aerial intelligent agents empowered by large language models, arXiv preprint arXiv:2503.08302, 2025. [23] M. Zhang, Q. Dai, Y. Yang, J. Bao, D. Chen, K. Qiu, C. Luo, X. Geng, and B. Guo, Magebench: Bridging large multimodal models to agents, arXiv preprint arXiv:2412.04531, 2024. [24] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., Chain-of-thought prompting elicits reasoning in large language models, Advances in neural information processing systems, vol. 35, pp. 24 82424 837, 2022. [25] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan et al., Language models are few-shot learners, in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 33, 2020, pp. 18771901. strategies for three-dimensional (3D) space to enable the agent to execute correct control operations (e.g., adjusting angles to face objects directly). Thus, enhancing LLMs realworld spatial cognition is critical direction for advancing robotic agent frameworks, and also key topic for realizing artificial general intelligence (AGI) in the future. This work is preliminary exploration, and we expect more outstanding research to deliver further progress in this field."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Y. Tian, F. Lin, Y. Li, T. Zhang, Q. Zhang, X. Fu, J. Huang, X. Dai, Y. Wang, C. Tian et al., Uavs meet llms: Overviews and perspectives towards agentic low-altitude mobility, Information Fusion, vol. 122, p. 103158, 2025. [2] F. Zeng, W. Gan, Y. Wang, N. Liu, and P. S. Yu, Large language models for robotics: survey, arXiv preprint arXiv:2311.07226, 2023. [3] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao, React: Synergizing reasoning and acting in language models, in The eleventh international conference on learning representations, 2022. [4] X. Zhao, M. Li, C. Weber, M. B. Hafez, and S. Wermter, Chat with the environment: Interactive multimodal perception using large language models, in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 35903596. [5] Y. Ding, X. Zhang, C. Paxton, and S. Zhang, Task and motion planning with large language models for object rearrangement, in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 20862092. [6] A. Tagliabue, K. Kondo, T. Zhao, M. Peterson, C. T. Tewari, and J. P. How, Real: Resilience and adaptation using large language models on autonomous aerial robots, in 2024 IEEE 63rd Conference on Decision and Control (CDC). IEEE, 2024, pp. 15391546. [7] V. G. Goecks and N. R. Waytowich, Disasterresponsegpt: Large language models for accelerated plan of action development in disaster response scenarios, arXiv preprint arXiv:2306.17271, 2023. [8] Z. Yuan, F. Xie, and T. Ji, Patrol agent: An autonomous uav framework for urban patrol using on board vision language model and on cloud large language model, in 2024 6th International Conference on Robotics and Computer Vision (ICRCV). IEEE, 2024, pp. 237242. [9] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, Progprompt: Generating situated robot task plans using large language models, arXiv preprint arXiv:2209.11302, 2022. [10] G. Chen, X. Yu, N. Ling, and L. Zhong, Typefly: Flying drones with large language model, arXiv preprint arXiv:2312.14950, 2023. [11] Z. Hu, F. Lucchetti, C. Schlesinger, Y. Saxena, A. Freeman, S. Modak, A. Guha, and J. Biswas, Deploying and evaluating llms to program service mobile robots, IEEE Robotics and Automation Letters, vol. 9, no. 3, pp. 28532860, 2024. [12] Y. Yin, Z. Wang, Y. Sharma, D. Niu, T. Darrell, and R. Herzig, Incontext learning enables robot action prediction in llms, in 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2025, pp. 89728979. [13] R. Mon-Williams, G. Li, R. Long, W. Du, and C. G. Lucas, Embodied large language models enable robots to complete complex tasks in unpredictable environments, Nature Machine Intelligence, pp. 110, 2025. [14] H. Zhao, F. Pan, H. Ping, and Y. Zhou, Agent as cerebrum, controller as cerebellum: Implementing an embodied lmm-based agent on drones, arXiv preprint arXiv:2311.15033, 2023. [15] L. Sun, D. K. Jha, C. Hori, S. Jain, R. Corcodel, X. Zhu, M. Tomizuka, and D. Romeres, Interactive planning using large language models for partially observable robotic tasks, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 14 05414 061. [16] H. Singh, R. J. Das, M. Han, P. Nakov, and I. Laptev, Malmm: Multiagent large language models for zero-shot robotics manipulation, arXiv preprint arXiv:2411.17636, 2024. [17] F. Joublin, A. Ceravola, P. Smirnov, F. Ocker, J. Deigmoeller, A. Belardinelli, C. Wang, S. Hasler, D. Tanneberg, and M. Gienger, Copal: corrective planning of robot actions with large language models, in 2024 ieee international conference on robotics and automation (ICRA). IEEE, 2024, pp. 86648670."
        }
    ],
    "affiliations": [
        "Zhejiang University"
    ]
}