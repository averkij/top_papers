{
    "paper_title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation",
    "authors": [
        "Sauradip Nag",
        "Daniel Cohen-Or",
        "Hao Zhang",
        "Ali Mahdavi-Amiri"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from a minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize a video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ a hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through a deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as a user study, we show the effectiveness of our method and its components. The project page is available at https://in-2-4d.github.io/"
        },
        {
            "title": "Start",
            "content": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation Sauradip Nag1 Daniel Cohen-Or2 Hao (Richard) Zhang1 Ali Mahdavi-Amiri1 1Simon Fraser University, Canada 2Tel Aviv University, Israel https://in-2-4d.github.io/ 5 2 0 2 1 1 ] . [ 1 6 6 3 8 0 . 4 0 5 2 : r Figure 1. In-2-4D: 4D motion inbetweening from minimalistic input setting, i.e., 2 single-view images. Given two monocular RGB images of an object at two distinct motion states (start and end), our method generates smooth, natural, and seamless 4D (3D object + motion) interpolation between them. We make no assumptions on the object categories or motion types. Top: liquid motion with topology changes. Middle: man with wings is flying. Our method also supports challenging free-form motions, e.g., flower blooming, umbrella opening/closing, human-object interactions, and rotational motions. More results can be found in the Supplementary."
        },
        {
            "title": "Abstract",
            "content": "We propose new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as user study, we show the effectiveness of our method and its components. 1 1. Introduction Motion inbetweening is classic animation problem. When generating motions of 3D objects, the typical input consists of 3D object in two distinct motion states, as in point cloud interpolation [25, 43], for instance. With significant advances in 3D generative AI in recent years, many recent attempts have been made on video-to-4D [9, 28, 37, 41, 42], whose task is to lift an object captured in video into the 3D space so its motion from the video can be viewed from all angles. An intriguing question is whether these two problems can be fused to produce 4D contents (3D object with motion) from minimalistic input setting, one that can be easily and casually acquired, as shown in Fig. 1. In this paper, we seek solution to this novel task, whose goal is to generate 4D interpolative contents from merely two single-view images capturing an object in two distinct motion states, We call this task and our method both as In2-4D, for Inbetweening from two (2) single-view images to 4D generation. Aside from the sparse inputs, we aim to tackle additional challenges related to the diversity and complexity of the generated motions: a) no particular assumptions are made on the object or motion categories; b) arbitrary motions that might be freeform, i.e., without any assumptions on rigidity or volume/topology preservation, e.g., see Fig. 1 for floral motion that is non-rigid and quite intricate, and an avocado dropping into liquid container, causing splash and topology change; c) moderately complex and longer-range motions where the two motion states are not assumed to be close in time. Our goal is to synthesize smooth and believable 3D transition between them. At the high level, our method operates in two phases: 2D still images to video via interpolation, and then video-to-4D via lifting, as illustrated in Fig. 2. To handle arbitrary and diverse motions, we leverage video foundational models. However, most such models are built on video diffusion [3], which has been trained predominantly by short videos. As such, they can be ineffective for motion inbetweening when the input states span large geometry or structural changes, resulting in large motion jumps and absence of detailed and intricate object movements. To this end, we develop divide-and-conquer approach to adaptively and recursively generate set of overlapping fragments of video frames where each fragment covers shorter and simpler motion. To start, we employ foundational video interpolation model such as DynamiCrafter [38] to generate an initial set of intermediate frames between the two input states, with text prompt. Then we perform motion and appearance feature analyses over these frames to select one or more keyframes that are visually close to the input states and show significant motion jumps. Consecutive keyframes that incur large motion will anchor new video interpolation to generate more immediate frames, effectively magnifying the said motion. This process is carried out hierarchically until all motions between consecutive keyframes are sufficiently small. Notably, our video fragment generation does not require any pre-training or fine-tuning of video diffusion, offering an accessible and practical solution for the 4D generation task. For each video fragment, and in parallel with other fragments, we first learn distinct static 3D Gaussian splatting (3DGS) model to capture the object geometry. We then apply deformation field to convert this 3DGS into dynamic, i.e., 4D, model by utilizing multi-view diffusion priors to refine the warping, geometry, and textures over unseen areas. By construction, the fragment contains relatively simple motions, hence multi-view generation can effectively mitigate texture degradation and geometry misalignment. Finally, we merge the independently generated 4D fragments in bottom-up manner, where we first linearly interpolate and then optimize the deformation fields over an overlapping frame and regularize the geometry of novel views in cascading sliding window fashion to smooth the orientation of the dynamic 3DGS based on the neighboring frames. Fig. 2 overviews our pipeline with an example. Our main contributions are summarized below: To the best of our knowledge, In-2-4D is the first method for generative 4D inbetweening over two distant monocular frames spanning arbitrary motions. Our novel hierarchical approach breaks the complex inbetweening into series of simpler motion estimations via video, and then 4D (i.e., dynamic 3DGS) generation. To generate smooth 3D object and motion transitions, we further optimize the 3D trajectories using bottom-up merging strategy with smoothing regularization. We contribute new 4D interpolation benchmark I4D-15 on challenging object motions and real-world scenes. We conduct extensive experiments on I4D-15 for evaluation. Quantitative and qualitative comparisons are made to methods and baselines to demonstrate the effectiveness of our method in terms of the quality of generated results, generalizability, and handling of variety of motions; see Fig. 1. While achieving superior generation quality than other methods, our solution is far from artifact-free. As first attempt at tackling such complex problem, we hope it can serve as promising start to stimulate future work. 2. Related Work Video inbetweening. Recent methods have extended pretrained diffusion-based text-to-image models to generate motion from static images by adapting UNets to temporal data [13, 31, 36]. One notable model is AnimateDiff [7], which learns low-rank adapters for diverse motion patterns. More recent approaches condition pre-trained text-to-video models on input images. VideoCrafter1 [5] uses dual cross-attention layers to combine image features 2 Figure 2. Illustration of In-2-4D pipeline. Given two single view images as input, we first generate keyframes to avoid abrupt motions between consecutive frames and then interpolate between keyframes to generate multiple fragments. These keyframes are then utilized to learn the static 3D geometry per fragment which are then deformed using deformation field (e.g., Hexplane) to obtain 4D scene per fragment. To aggregate the deformations, we linearly interpolate the deformation field in cascading fashion and then apply smoothing constraints on 3D Gaussian splats to improve the novel views geometry. with text prompts, while DynamiCrafter [38] further refines this by concatenating the input image with noisy latent features. Our method builds on DynamiCrafter to enhance its outputs through recursive video magnification. While several video magnification techniques exist [14, 21], we leverage video inbetweening network (e.g., DynamiCrafter) to interpolate frames and amplify motion when large displacements are present. Decomposing large motions into smaller fragments with smoother transitions reduces geometric ambiguities between consecutive frames, producing 4D results with fewer artifacts and improved visual quality. 4D scene interpolation. Dynamic 3D scene interpolation (4D Interpolation) is recently becoming more popular in 4D literature. Earlier works [24] leverage neural radiance fields (NeRF) for temporally coherent 3D reconstructions, while NeuralPCI [43] employs neural fields for multi-frame, nonlinear 3D point cloud interpolation. PAPR [25] estimates motion via point-based rendering and local displacement optimization. Recent methods [9, 20, 29] use frame motions from Diffusion-based Video Interpolation models [2, 38] to infer 3D deformation. However, Video Diffusion models [3], trained on short clips (e.g., 16 frames), struggle with long sequences, causing artifacts from large per-frame motion jumps. To address this, we use generative Video Interpolation models (e.g., DynamiCrafter [38]) hierarchically for longer 3D trajectory estimation without extra training. 4D dynamic scene generation. Recent works [8, 18, 28, 35, 37, 40, 42] extend 3D Gaussian Splatting (3DGS)[11] to 4D using time-conditioned deformation networks with SDS and multi-view geometry. MAV3D [32] pioneered text-to-4D via NeRF and score distillation, followed by similar approaches [1]. Consistent4D [9] introduces videoto-4D with pre-trained image diffusion models, extending to image/video-conditional 4D generation [30, 34]. STAG4D [42] and 4DGen [41] refine diffusion with pseudolabels, while SC4D [37] employs sparse Gaussians and LBS for dynamic 3D. L4GM [29] proposed 4D foundation model effective for simple motions. Despite progress, video-to-4D methods struggle with high dynamics, accumulating errors over long videos due to reliance on single canonical model. We mitigate this by segmenting videos into shorter fragments with their own canonical model to 3 improving geometric consistency. 3. Methodology An overview of our method is shown in Fig. 2. Given two images representing the start and end states of an object in motion, we aim to generate and reconstruct the motion in 4D (3D+motion). To predict the motion, we use video interpolation model, but large motions between frames can lead to ambiguous interpretations and results with artifacts. To solve this, we employ hierarchical approach to identify keyframes that are visually close to the input states and exhibit significant motion, then generate smooth fragments between them. For each fragment, the 3D representation of the keyframe is first constructed using Gaussian Splatting. The temporal frames within the fragment serve as motion guidance, enabling their transformation into dynamic Gaussians through deformation field. To enhance temporal consistency and refine 3D motion of the fragment, we expand the self-attention of multi-view diffusion across time steps and introduce rigid transformation regularization. Finally, the independently generated 3D motion segments are merged by interpolating the boundary deformation fields and optimizing them to align with the guiding video. This ensures smooth and flicker-free transitions. 3.1. Problem Setup Task description. Given pair of start and end single view images Is and Ie RHW 3 representing dynamic scene possibly having complex and large motion, our task is to generate 4D interpolated scene that can be observed at any point of time or view. Our framework. Our objective is to generate smooth motion while minimizing 3D artifacts in novel views. To achieve this, we introduce gradual local displacements and insert frames in regions with complex motion to prevent abrupt transitions. First, keyframes are adaptively generated by analyzing motion differences in feature space, segmenting fragments with simple motion (Sec.3.2). These fragments are then individually lifted to 4D space using their respective motion (Sec.3.3). Finally, local deformations are integrated into globally smooth 4D motion with regularization (Sec. 3.4). 3.2. Temporal Fragment Hierarchy We propose method for identifying keyframes in fragments with significant deformations and adaptively expanding them. Large deformations between start and end states induce rapid intermediate motion changes, which hinder 3D deformation learning [19] as shown in Fig. 4. To mitigate this, we partition the motion trajectory into fragments with smoother quasi-static motions, selecting keyframes densely in dynamic regions and sparsely in static regions. This balFigure 3. Illustration of Hierarchal Fragment Generation. At each generation step, keyframe is selected by finding the largest motion from the DIFT heatmap and FID score. New frames are regenerated using the keyframe to minimize large motion changes. Selection of the keyframes and re-generation is done in hierarchial manner to generate fragments having simple motions ances training overhead, model size, and performance and enhances temporal consistency. Hierarchical key-frame generator. To generate keyframes for the intermediate motion between two initial states, we employ Video Interpolation Model (e.g., DynamiCrafter), denoted as ψ(.). Given input images Is and Ie along with motion prompt (extracted using BLIP [15]), we generate sequence of latent frames Z. The pairwise DIFT [33] features quantify frame-wise similarity, enabling rapid assessment of motion changes. As illustrated in Fig. 3, heatmap visualizes temporal variations, where significant object movements or new appearances are represented as bright regions. The heatmap between frames Ii and Ij is computed as: i,j = CS(f , ), where = arg max CS(f i , ) where CS(.) represents cosine similarity, and p, denote tokens of DIFT feature . frame is marked as keyframe if the mean heatmap value of Hi,j between frame pairs Ii, Ij falls below predefined threshold. To sample the best keyframe in terms of visual fidelity we further assess its consistency with the initial inputs using FID metric. The keyframe latent zm at timestep is selected based on the highest FID against the input states to remain faithful to inputs. For instance, in Fig. 3, the chosen keyframe exhibits the highest fidelity to the input states of the eagle. Once identified, the keyframe divides the motion trajectory into two segments: zs, zm and zm, ze. The interpolation model ψ(.) then utilizes these fragments iteratively in divideand-conquer fashion, identifying further keyframes until the full video is processed. This hierarchical approach en4 sures adaptive keyframe density, reducing redundant intermediate frames in low-motion areas while preserving complex motion details. Therefore, the hierarchical keyframe selection is performed recursively based on prior selections: = K(s)(1), K(1)(2), K(2)(3), ..., K(c)(e) (1) where K(i)(j) denotes the keyframe between states and j. Temporal fragment generation. Having keyframes K, we reuse the video interpolation module ψ(.) to perform inebetweening for consecutive keyframes K(i)(j) and K(j)(k). Since ψ(.) receives latents, we interpolate the latents and decode them using VAE decoder to insert new RGB frames. Since the consecutive keyframes represent simple quasi-static motions, this interpolation generates smooth fragments with fewer artifacts. We generate such fragments denoted by Vi each having fixed number of frames (e.g., 16) representing the motion between keyframes: = {Vs(1f ), V(1f )(2f ), ..., V((c1)f )e}, (2) where Vs(1f ) = D(ψ(z(s)(1), z(1)(2))); is VAE decoder. 3.3. Modelling Intra-Fragment Geometry We lift individual video fragments to 4D by generating multi-view videos of the object. Existing video-to-4D methods [28, 37] use multi-view diffusion models [22] to synthesize multi-view videos by independently processing each frame. However, this approach ensures cross-view consistency but leads to temporally inconsistent geometry Fig Fig. 7. Moreover, due to sole reliance on multi-view video supervision, Gaussian splatting often produces flickering and texture variations [23] due to its high degrees of freedom per point and lack of motion constraints. We address these issues by generating temporally consistent multi-view videos and regularizing motion with rigid constraints within each fragment. Learning canonical 3D. Similar to prior works, we first estimate canonical Gaussian representation and then add motion to it from the multi-view videos. For each temporal fragment Vi, we designate the keyframe K(i)(j) as the canonical reference and reconstruct its 3D structure via multi-view synthesis. Specifically, we employ the multiview diffusion model Era3D [16] to generate multi-view images from K(i)(j), followed by 3DGS [11] for coarse geometry reconstruction. As each fragment is processed independently, parallel execution reduces computation time. The resulting coarse geometry provides an effective initialization for learning texture and geometry across the remaining temporal frames. Dynamic 3D fragment generation. After learning 3D static Gaussians, we leverage motion priors from the video fragment to transform them into dynamic Gaussians. Since single-view videos cannot provide diverse observations of Figure 4. Effect of inbetweening on geometry. When the input states are significantly different, the 3D deformation module undergoes large movements (fast motion) leading to artifacts in novel views, whereas generating intermediate frames between the states (slow motion) enhances the geometry using smaller deformations. the scene from different viewpoints, we use multi-view videos. To promote temporal consistency, rather than generating multiple-view of the frames independently at each timestep, we propagate the self-attention features of the multi-view diffusion model [16] from the canonical frame across the entire frames of the fragment as follows: zt γ.zc + (1 γ).zt, = q.zt, = k.zt, = v.zt, Attention(Q, K, V) = Sof tmax( QKT dk .V). where zc is the multi-view latent of the canonical frame, is timestep of the video fragment, γ is the blending weight and dk is the key dimension. With quasi-static motions in each video fragment, the generated multi-view videos have minimal variation in viewpoints, making it easier for the model to capture accurate and consistent geometry  (Fig. 7)  . With the synthesized multi-view videos of the dynamic object, we optimize 3D deformation field (denoted by Φi) to enable free-viewpoint rendering. We chose Hexplanes [4] as our deformation field due to modeling efficiency. The deformation field predicts each Gaussians geometric offsets at given timestamp relative to the mean canonical state (keyframe). For each timestamp τ of video and 3D Gaussian p, Hexplanes predict displacement, rotation, and scaling for the 3D gaussian points. Optimization objective. To respect the driving video and optimize the deformation field, we fix the camera to view and minimize the Mean Squared Error (MSE) between the rendered image and each video frame: LRef = 1 (cid:88) τ =1 (ϕ(S, τ ), oRef) τ Ref2 2, (3) 5 optimizing only mergeΦij for few iterations (e.g., 1,000) at low learning rate using Eq.5 to ensure smooth interframe motion between fragments. Starting with Φ1, we progressively merge all deformation fields in bottom-up fashion, resulting in smooth and globally coherent 3D motionwithout abrupt transitions or flickers. Cascaded trajectory smoothing. Despite smooth transitions across fragments, minor 3D inconsistencies may persist (see Fig. 5), often due to disoriented Gaussians causing blurry or over-reconstructed artifacts [12]. Since the 3D Gaussian geometry is controlled by the covariance matrix (i.e., rotation and scaling s), we regularize and over fixed window with neighboring frame constraints. We adopt off-the-shelf tracking (e.g., CoTracker [10]) and depth models (e.g., DepthAnything [39]) in sliding window manner for post-processing. Given window w, we estimate depth and trajectory on the video (Eq. 2) and lift randomly selected trajectories to 3D using camera intrinsics. Points visible for at least 80% frames are smoothed with Exponential Moving Average (EMA) on rotation and scale: qt = sin(α.θ) sinθ qt + sin((1 α).θ) sinθ qt1, (8) st = αst + (1 α)st1, where θ is the angle between consecutive rotations, and α is the EMA decay factor. The process is repeated iteratively until all disoriented Gaussians are corrected, yielding stable and flicker-free 3D reconstructions. 4. Experiments and Results Dataset. We evaluate our method on 4D sequences with large object motions, defined when multiple object parts move. We introduce the I4D-15 benchmark, comprising 15 articulated objects across categories like Vehicles, Robots, Flowers, Humans, Animals, and Daily Life scenes. The dataset includes 64-frame sequences at 16 fps from Objaverse1.0 [6], rendered from 5 evenly spaced views at 0 elevation. We select the first and last frames of the front view as input states and reserve the remaining video for evaluation. The camera radius is 1.5, and the FOV is 49.1, consistent with [9]. Motion filtering [17] ensures large motion selection. Evaluation uses appearance metrics (LPIPS, FVD)[28] and geometry metrics (SI-CD, CD)[25]. More details are provided in the supplementary. Baselines. As we introduce novel task, no existing method can be directly applied to our setting. Thus, we establish the following baselines for quantitative comparison: For 4D baseline generation, we first perform 2D video interpolation without fragment generation and subsequently lift it to 4D using Video-to-4D approach. (a) BaselineI employs FILM [27] for 2D video interpolation and an adapted version of SC4D [37] for Video-to-4D conversion. Figure 5. Trajectory smoothing of fragments leads to correction of Gaussians and help render better novel views. where τ Ref is the τ -th frame, oRef is the reference viewpoint, and is the rendering function. Dynamic Gaussians tend to move freely across regions of similar color [23] without constraints, causing flickering and floating artifacts that degrade 4D motion realism. As motion within each fragment is minimal, we enforce rigid assumptions on point movements relative to the canonical state by regularization: Lrigid = d(µc , µc j) d(µτ , µτ )1 (4) where d(x, y) = y2 is the distance function, and µ denotes the Gaussian center of neighboring clusters . µc and µτ represent Gaussian centers in the canonical and arbitrary timestep frames, respectively. This regularization permits non-rigid deformations (like bending) while miniIn addition to this, we also mizing local rigid distortions. use random view at each timestep and apply foreground mask loss Lmask, resulting in total training objective: = λ1LRef + λ2Lmask + λ3Lrigid, (5) where λ1 and λ2 are weights. In training, for each fragment Vi, we first use to supervise the static 3D Gaussian, then train the dynamic 4D Gaussian with all reference frames. 3.4. Cascaded 3D Motion Aggregation Since we learn each fragments deformation independently, the entire video may lack consistency over the global geometry and motion. The overall 3D deformation field consists of mini-deformations per fragment: = [Φ1, Φ2, ..., ΦK ], (6) where each Φi is optimized separately. To achieve smooth, flicker-free motion in novel views, we need to merge these fragment deformations. Motion merging. With overlapping frames between adjacent fragments, we linearly interpolate the deformation fields for these overlaps. Specifically, we define the interpolated deformation field as: mergeΦij = λΦi + (1 λ)Φj, (7) where λ = 0.5, and only Φj is learnable. With intraframe motions already smooth, we freeze Φi and Φj, 6 Figure 6. Visual comparison between our method and the baselines. Our method produces less geometric and appearance artifacts in comparison with baselines II. More visual comparisons will be provided in the supplementary material. Table 1. Quantitative Analysis on proposed I4D-15 Dataset. Method Baseline-I Baseline-II Ours Appearance LPIPS 0.143 0.136 0.103 CLIP 0.81 0.84 0.91 FVD 992.23 729.32 679. Geometry SI-CD CD 0.76 0.73 0.59 33.58 31.79 22.67 uate single-image-to-4D task on our dataset, with further analysis provided in the supplementary. Baseline results are obtained using official GitHub implementations. Quantitative comparisons. We quantitatively evaluate our approach on our I4D-15 benchmark. Two images from one view are used as input and 4 videos (each 64 frames) from other viewpoints and their corresponding timsetep point clouds are used for evaluation. As shown in Tab. 1, our method outperforms the baseline across all metrics in appearance and geometry. This shows the effectiveness of our method in handling complex motions in 4D by dividing it into quasi-static temporal fragments. Qualitative comparisons. Fig. 6 provides some qualitative comparisons with the baseline. It is apparent that our method produces less artifacts. Additional visual results are shown in Fig. 8 for different motions and object categories. Ablation Study. This study evaluates the contribution of Figure 7. Effect of Inter-Fragment Consistency. Without using any consistency or regularization, blurriness and oversaturated artifacts are produced. Rigid consistency improves the structure and when combined with temporal-aware multi-view generation, better geometry and texture are obtained. (b) Baseline-II utilizes SVD [2] for 2D interpolation and recent Video-to-4D method [28]. Additionally, we eval7 Figure 8. Qualitative Results. Having only the first and last frames of motion, we are able to generate moving 3D objects that can be seen at different views. Objects seen from different view directions are still plausible although no direct supervision signal is available. Table 2. Ablation on the impact of temporal fragment generation. # Fragments LPIPS FVD SI-CD CD 0.74 0.70 0.60 0.59 w/o HSG 2 8 4 (Ours) 922.16 898.23 680.11 679. 32.56 29.68 22.59 22.67 0.137 0.124 0.101 0.103 Time 5 mins 8 mins 36 mins 17 mins Table 3. Ablation on the components of motion aggregation. Motion Merging Trajectory Smoothing Appearance LPIPS 0.103 0.116 0.137 FVD 679.23 783.28 922.16 Geometry SI-CD CD 0.59 0.71 0.74 22.67 25.40 32.56 key components in our method on the I4D-15 benchmark. As shown in Tab 2, using four segments enables our model to decompose complex motion into finer details, achieving the best cost-performance balance. Additionally, we visually analyze the effect of Intra-Fragment consistency (Sec. 3.3) in Fig. 7, revealing that mv-consistency significantly enhances novel view synthesis, while rigid consistency mitigates deformation artifacts. Tab. 3 further highlights the impact of 3D motion aggregation. The combination of merging and smoothing improves both appearance and geometry metrics, except for slight decline in FVD when trajectory smoothing is applied. Moreover, we benchmark runtime against all baselines, as shown in Tab. 4, demonstrating that our approach outperforms the fastest baseline (B-I) by 70% on an NVIDIA A100 GPU. Additional ablations are provided in the supplementary material. User study. user study was conducted, as human judg8 ment is most effective for assessing 3D generation and motion quality. We gathered 15 generated 4D motions and asked 20 participants to rank four methods (1 = best, 4 = worst) based on 3D geometry and motion consistency (reduced flicker). In case of ties in motion consistency, 3D generation quality was prioritized. As shown in Tab. 5, our method was preferred for overall 4D generation quality. Table 4. Ablation on runtime Table 5. User study Methods B-I B-II Ours FVD 992 729 679 Time 1.25 hr 4.25 hr 50 min Methods B-I B-II Ours Gen. Quality 2.93 2.44 1.29 Application: customized 4D motion. In contrast to most existing 4D generation methods [28, 42] that depend on SDS [26], our approach improves controllability and motion diversity. While BLIP [15] is used by default to extract motion prompts, users can input custom prompts to generate 4D motions for the same initial and final states. As shown in Fig. 9, both jumping and walking motions of dog are synthesized under identical start and end conditions. Despite motion complexity, our bottom-up 3D optimization ensures artifact-free novel view generation. 5. Conclusion, Limitations, Future Work We introduce the novel task of generative 4D inbetweening from two single view images at distinct motion states. To address this challenging task, we leverage the capabilities of foundational video diffusion models to extract motion in between the states. We identify complex and large motions and divide them into fragments with simpler and smoother motions through divide and conquer approach. 79968006, 2024. [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. [4] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. CVPR, 2023. [5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. [6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. [7] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toimage diffusion models without specific tuning, 2023. [8] Zhiyang Guo, Wen gang Zhou, Li Li, Min Wang, and Houqiang Li. Motion-aware 3d gaussian splatting for efficient dynamic scene reconstruction. ArXiv, abs/2403.11447, 2024. [9] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao. Consistent4d: Consistent 360 {deg} dynamic obarXiv preprint ject generation from monocular video. arXiv:2311.02848, 2023. [10] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. arXiv:2307.07635, 2023. [11] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4), 2023. [12] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (ToG), 42(4):114, 2023. [13] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-toimage diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. [14] Anh Cat Le Ngo and Raphael C.-W. Phan. Seeing the invisible: Survey of video motion magnification and small motion analysis. ACM Comput. Surv., 52(6), 2019. Figure 9. Controllable Motions. In-2-4D allows generation of diverse motions for the same start and end states Using multi-view priors, we lift the object at different states to 3D and merge these simple 3D motions in bottom-up fashion with smoothness constraints into flicker-free 4D motion. Although our work is able to outperform baselines but it is still strong baseline on this challenging task and paves the way for further exploration and advancement. Our method has some limitations. First, our method produces un-natural deformations when the in-between motion is extreme. Since the resulting videos are used to lift the object motion to the 3D space, the subtle movements may not look natural in 4D space. promising direction for future work would be to extend this approach to incorporate specific motion trajectories or other 2D or 3D conditional signals in 4D motion generation to provide more realistic dynamism. Additionally, the 3D and 2D components do not currently interact in way that allows mutual correction. Another valuable avenue for future research would be end-to-end training, enabling these two components to influence each other and produce more coherent results both in 2D and 3D. Acknowledgements. This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant. We sincerely thank labmates from GrUVi and Taiya Lab for their valuable suggestions, numerous brainstorming sessions and proofreading of the draft."
        },
        {
            "title": "References",
            "content": "[1] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score disIn Proceedings of the IEEE/CVF Contillation sampling. ference on Computer Vision and Pattern Recognition, pages 9 [15] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. [16] Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wei Xue, Wenhan Luo, et al. Era3d: high-resolution multiview diffusion using efficient row-wise attention. Advances in Neural Information Processing Systems, 37:5597556000, 2024. [17] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. Puppet-master: Scaling interactive video generation as motion prior for part-level dynamics. arXiv preprint arXiv:2408.04631, 2024. [18] Yiqing Liang, Numair Khan, Zhengqin Li, Thu NguyenPhuoc, Douglas Lanman, James Tompkin, and Lei Xiao. Gaufre: Gaussian deformation fields for real-time dynamic novel view synthesis. ArXiv, abs/2312.11458, 2023. [19] Yiqing Liang, Mikhail Okunev, Mikaela Angelina Uy, Runfeng Li, Leonidas Guibas, James Tompkin, and Adam Harley. Monocular dynamic gaussian splatting is fast arXiv preprint and brittle but smooth motion helps. arXiv:2412.04457, 2024. [20] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85768588, 2024. [21] Ce Liu, Antonio Torralba, William T. Freeman, Fredo Durand, and Edward H. Adelson. Motion magnification. TOG, 24(3), 2005. [22] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: In Proceedings of the Zero-shot one image to 3d object. IEEE/CVF International Conference on Computer Vision, pages 92989309, 2023. [23] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 3DV, 2024. [24] Sungheon Park, Minjung Son, Seokhwan Jang, Young Chun Ahn, Ji-Yeon Kim, and Nahyup Kang. Temporal interpolation is all you need for dynamic neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42124221, 2023. [25] Shichong Peng, Yanshu Zhang, and Ke Li. Papr in motion: Seamless point-level 3d scene interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2100721016, 2024. [26] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [27] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and Brian Curless. Film: Frame interpolation for large motion. In European Conference on Computer Vision, pages 250266. Springer, 2022. tive 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. [29] Jiawei Ren, Cheng Xie, Ashkan Mirzaei, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, Huan Ling, et al. L4gm: Large 4d gaussian reconstruction model. Advances in Neural Information Processing Systems, 37:5682856858, 2025. [30] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. [31] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data, 2022. [32] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation. arXiv preprint arXiv:2301.11280, 2023. [33] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36:13631389, 2023. [34] Peng Wang and Yichun Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation, 2023. [35] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023. [36] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation, 2023. [37] Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Sc4d: Sparse-controlled videoarXiv preprint Wang, and Xiang Bai. to-4d generation and motion transfer. arXiv:2404.03736, 2024. [38] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. [39] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. [40] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint arXiv:2309.13101, 2023. [41] Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, and Yunchao Wei. 4dgen: Grounded 4d content generation with spatial-temporal consistency. arXiv preprint arXiv:2312.17225, 2023. [28] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Genera- [42] Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao. Stag4d: 10 Spatial-temporal anchored generative 4d gaussians. arXiv preprint arXiv:2403.14939, 2024. [43] Zehan Zheng, Danni Wu, Ruisi Lu, Fan Lu, Guang Chen, and Changjun Jiang. Neuralpci: Spatio-temporal neural field for 3d point cloud multi-frame non-linear interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 909918, 2023."
        }
    ],
    "affiliations": [
        "Simon Fraser University, Canada",
        "Tel Aviv University, Israel"
    ]
}