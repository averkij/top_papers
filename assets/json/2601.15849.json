{
    "paper_title": "CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval",
    "authors": [
        "Tsung-Hsiang Chou",
        "Chen-Jui Yu",
        "Shui-Hsiang Hsu",
        "Yao-Chung Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 2 ] . [ 1 9 4 8 5 1 . 1 0 6 2 : r CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval Tsung-Hsiang Chou National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan yumeow0122@smail.nchu.edu.tw Chen-Jui Yu National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan rui0828@smail.nchu.edu.tw Shui-Hsiang Hsu National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan g113056055@smail.nchu.edu.tw Yao-Chung Fan National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan yfan@nchu.edu.tw"
        },
        {
            "title": "Abstract",
            "content": "General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and querytable mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54%. In unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT. CCS Concepts Information systems Content analysis and feature selection."
        },
        {
            "title": "Keywords",
            "content": "Table Retrieval, Synthetic Query Generation, Fine-tuning"
        },
        {
            "title": "1 Introduction",
            "content": "Tables are central medium for storing and communicating structured information across domains such as finance, science, logistics, and public knowledge bases. With the rapid growth of large webscale table corpora, effective table retrieval has become increasingly important for downstream tasks, e.g., table question answering. However, general-purpose embedding models trained on textual corpora exhibit strong generalization in text retrieval, yet face substantial challenges when extended to table retrieval. Encoding an entire table into single vector thus induces semantic compression, resulting in degraded retrieval performance. Enhancing representation learning [1, 5, 7] for table retrieval is therefore central objective. Beyond task-specific table embedding fine-tuning [1, 7], recent work has explored the use of large language models (LLMs) to provide additional supervision for table retrieval without modifying the embedding model directly. representative approach in this direction is QGpT [5], which selects the first ğ‘˜ rows of table to form partial table and prompts an LLM to generate synthetic queries (SQ). These synthetic queries serve as enhanced table representations and can improve retrieval accuracy. While effective, QGpT has two key limitations: (1) selecting only the first rows assumes they are representative, which does not hold when relevant information appears in later rows; and (2) the generated synthetic queries are seldom used as training supervision to improve the embedding model itself, leaving potential performance gains unrealized. To address these challenges, we propose CGPT, framework that enhances table retrieval through LLM-generated supervision. CGPT applies K-means clustering to group table rows into semantically coherent subsets and samples from each cluster to construct partial tables with broader coverage of table attributes and instances. Synthetic queries generated by an LLM are then leveraged in hard-negative contrastive fine-tuning, enabling the embedding model to better retrieve tables even when relevant information is sparsely located across rows. We evaluate CGPT on four public benchmarksMimoTable, OTTQA, FetaQA, and E2E-WTQand observe consistent improvements over retrieval augmentation baselines, with mean R@1 gain of 16.54%. Additional experiments on unified multi-domain corpus confirm cross-domain generalization, and experiments with Tsung-Hsiang Chou, Chen-Jui Yu, Shui-Hsiang Hsu, and Yao-Chung Fan Figure 1: Overall workflow of CGPT. The framework consists of four main stages: (1) clustering-based partial table generation; (2) synthetic query generation; (3) hard negative sampling; and (4) model fine-tuning. smaller LLMs show that CGPT remains cost-efficient without sacrificing retrieval effectiveness."
        },
        {
            "title": "2 Related Work\n2.1 Table Retrieval",
            "content": "Table retrieval has emerged as an important problem in natural language processing. TAPAS [2] extends the BERT architecture with table-aware positional encodings (e.g., row and column indices) and pre-trains on millions of Wikipedia tables, establishing strong foundation for table QA. Dense Table Retrieval (DTR) [1] adopts dense vector retrieval by encoding queries and tables separately with two TAPAS encoders and computing similarity via inner products. DTR further incorporates the Inverse Cloze Task [4] and hard negative sampling to improve retrieval quality. common limitation of these approaches is the use of single vector to represent an entire table. For large tables, this design can cause excessive semantic compression."
        },
        {
            "title": "2.2 Representation Augmentation",
            "content": "Recent work explores synthetic query generation to strengthen table representations. QGpT [5] adopts partial-table strategy by selecting the first 10 rows and generating synthetic queries using large language model; the partial table and generated queries jointly define the table representation. This reduces input length and yields more focused supervision, improving zero-shot performance of general-purpose embedding models. GenSearch [3] instead generates pseudo tables conditioned on queries, further illustrating the promise of generative retrieval. Building on these ideas, we aim to construct partial tables that more comprehensively cover the semantic space of the original table. Our method, CGPT, clusters table instances via K-means and samples across clusters to form semantically diverse partial tables with improved coverage."
        },
        {
            "title": "3 Method",
            "content": "As illustrated in Figure 1, CGPT follows four-stage training pipeline: clusteringbased partial table generation, synthetic query generation, hard negative sampling, and contrastive fine-tuning. Given table ğ‘‡ = {ğ», ğ¼1, . . . , ğ¼ğ‘š } with header ğ» and ğ‘š instances, we construct training data and optimize the embedding model through the steps below."
        },
        {
            "title": "3.1 Clustering-based Partial Table Generation",
            "content": "To capture broader portion of the tables semantic space, we generate multiple representative partial tables using K-means, called KPT (K-means Partial Table). This process includes instance embedding, clustering, and partial-table formation. Instance Embedding For each instance ğ¼ğ‘– in table ğ‘‡ , we encode it using pretrained embedding model, producing set of instance embedding vectors = {e1, e2, ..., eğ‘š }. K-means Clustering To ensure that the constructed partial tables capture the tables full semantic space, we apply K-means clustering over . The number of clusters ğ‘˜ is determined adaptively based on the table size: (cid:16)(cid:108)ğ‘š ğ‘Ÿ Forming K-means Partial Table From each cluster ğ¶ ğ‘— , we ğ‘˜ = min , ğ‘˜max (1) (cid:109) (cid:17) randomly sample ğ‘  instances to construct the ğ‘—-th partial table: ğ¾ğ‘ƒğ‘‡ğ‘— = {ğ» } {ğ¼ğ‘– ğ¼ğ‘– Sample(ğ¶ ğ‘—, ğ‘ )} (2) This procedure yields ğ‘˜ partial tables per original table, with each KPT reflecting the semantics of specific cluster."
        },
        {
            "title": "3.2 Synthetic Query Generation\nFor each partial table ğ¾ğ‘ƒğ‘‡ğ‘— , we prompt a LLM to generate ğ‘›ğ‘ syn-\nthetic queries. The generation process is model-agnostic and only\nrequires the ability to produce natural-language queries conditioned\non the content of the partial table. For reproducibility, the exact\nprompt template is provided in Appendix A.",
            "content": "CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval"
        },
        {
            "title": "3.3 Hard Negative Sampling and Fine-tuning",
            "content": "To strengthen the models ability to distinguish semantically similar but incorrect tables, we integrate hard negative sampling with contrastive fine-tuning."
        },
        {
            "title": "3.3.1 Hard Negative Sampling. For each synthetic query ğ‘, we\ncompute cosine similarity between ğ‘ and all KPTs from other ta-\nbles using a pretrained embedding model, selecting the top-â„ most\nsimilar but incorrect ones as hard negatives to form H N . These\nnegatives, which are easily confused with the true positive, provide\nstrong supervision for improving the modelâ€™s discrimination.",
            "content": "Fine-tuning. Given synthetic query ğ‘, its corresponding 3.3.2 positive KPT ğ‘+, and the hard negative set , we construct training triples (ğ‘, ğ‘+, ) and optimize the embedding model using the InfoNCE objective [6]: = log exp(sim(ğ‘, ğ‘+)/ğœ) exp(sim(ğ‘, ğ‘+)/ğœ) + (cid:205)ğ‘ exp(sim(ğ‘, ğ‘ )/ğœ) (3) The symbols are defined as follows: ğ‘ denotes the synthetic query. ğ‘+ is the corresponding positive KPT. ğ‘ are the selected hard-negative KPTs. sim(, ) denotes cosine similarity. ğœ is tunable temperature parameter."
        },
        {
            "title": "4 Experiment",
            "content": "We evaluate CGPT on four benchmarks: MimoTable (containing both Chinese and English tables), OTTQA, FetaQA, and E2E-WTQ."
        },
        {
            "title": "4.1 Experiment Setting",
            "content": "During partial table generation, three parameters control semantic coverage: the cluster granularity parameter ğ‘Ÿ is set to 10, the maximum number of clusters is ğ‘˜max = 5, and ğ‘  = 5 instances are sampled from each cluster to build the final KPTs. For synthetic query generation, we use Llama-3.1-8B-Instruct to produce queries, generating ğ‘›ğ‘ = 5 queries per partial table. The generation parameters include temperature of 0.4 and maximum output length of 1024 tokens. For hard negative sampling, we select â„ = 8 hard negatives per query. For model fine-tuning, we adopt BAAI/bge-m3 as the base embedding model. Training uses learning rate of 1e-5, two epochs, temperature ğœ = 0.01, and gradient accumulation with 32 steps. All experiments are conducted on single NVIDIA A6000 GPU with 48GB memory."
        },
        {
            "title": "4.2 Main Result",
            "content": "Table 1 presents the retrieval performance of CGPT and baseline systems across four benchmarks. We conduct systematic ablation study to isolate the contributions of K-means clustering and hard negative sampling. The comparison includes QGpT (using its original configuration and data), CGPT w/o FT (no fine-tuning), and CGPT w/o HNS (replacing hard negatives with random negatives). K-means clustering provides clear advantage even without finetuning, CGPT w/o FT achieves R@1 scores of 52.74% and 57.14% on the two compared subsets, improving over QGpT by 2.14 and 6.48 points, respectively. This demonstrates that semantically guided partial-table construction alone yields stronger table representations, underscoring the value of cluster-aware table generation. Regarding sampling strategies, CGPT w/o HNS attains the best R@5 and R@10 scores on MimoTable (CH), showing that the semantic diversity introduced by K-means is sufficient to support strong top-5/10 retrieval even with simplified negative sampling. For precision-focused evaluation, the full CGPT method shows the strongest R@1 performance. On MimoTable (EN), CGPT reaches 60.13% R@1, surpassing QGpT by 9.47 points. Similar improvements are observed on more complex datasets such as OTTQA and E2E-WTQ. Although hard negative sampling may slightly trade off top-5/10 performance, it improves top-1 score by providing more informative contrastive signals. These results indicate that while clustering-based partial table construction enhances semantic coverage, combining it with hard negative sampling is essential for maximizing precision in table retrieval tasks."
        },
        {
            "title": "4.3 Evaluation on the QGpT Dataset",
            "content": "Table 2 presents the performance of the CGPT-trained model on the original QGpT dataset, allowing us to assess cross-strategy transfer given the differing partial-table construction methods. CGPT shows clear improvements: MimoTable (EN) R@1 increases from 50.66% to 59.28%, and MimoTable (CH) from 50.6% to 53.54%. These results indicate that the semantic representations learned by CGPT transfer effectively across construction schemes, and that its clustering-based modeling offers robustness to semantically dispersed tables, supporting broad applicability in retrieval settings. Table 2: Retrieval performance of CGPT on the original QGpT dataset."
        },
        {
            "title": "Method",
            "content": "MimoTable (CH) MimoTable (EN) R@1 R@5 R@10 R@ R@5 R@10 QGpT QGpT w/ CGPT 50.6 53.54 75.0 77.27 81.45 82. 50.66 59.28 72.35 80.29 80.8 83."
        },
        {
            "title": "Dataset",
            "content": "To assess the cross-domain generalization capability of CGPT, we merge all datasets into single unified multi-domain corpus and conduct large-scale retrieval evaluation. The QGpT baseline is similarly tested on the merged corpus. Table 3 reports the performance of QGpT, KPT, the BGE-M3 model, and the CGPT-trained model. On the unfine-tuned BGE-M3 model, KPT yields notable gains. For example, on MimoTable (CH), it achieves 43.14% R@1, outperforming QGpT by 4.6 points. On E2E-WTQ, KPT reaches 53.94% R@1, surpassing QGpTs 35.27% by 18.67 points. These results indicate that KPT effectively preserves key table semantics and enhances retrieval accuracy even under multi-domain conditions. With CGPT, the improvements are further amplified. On MimoTable (EN), CGPT boosts R@1 to 57.79%, an 18.33-point increase over the BGE-M3 baseline; on MimoTable (CH), it reaches Table 1: Comparison of retrieval performance across four benchmark datasets. Tsung-Hsiang Chou, Chen-Jui Yu, Shui-Hsiang Hsu, and Yao-Chung Fan Method MimoTable (CH) MimoTable (EN) OTTQA FetaQA E2E-WTQ R@1 R@ R@10 R@1 R@5 R@10 R@1 R@ R@10 R@1 R@5 R@10 R@1 R@ R@10 QGpT 50.6 75.0 81.45 50. 72.35 80.8 51.45 78.14 86.68 33. 50.87 57.86 41.49 65.98 72.61 CGPT w/o FT CGPT w/o HNS CGPT 52.74 55.51 56.8 76.64 80.57 79.81 83.23 85.66 84.2 57.14 57.84 60.13 79.46 80.07 81.49 85.17 85.28 85. 84.24 84.69 86.86 97.24 97.02 97.07 98.46 98.55 98.33 34.85 34.85 34.9 52.82 52.32 51.52 60.91 61.11 59. 69.29 68.05 72.2 91.7 92.12 91.7 96.68 96.27 95.44 Table 3: Comparison of retrieval performance on the mixed dataset. Model Strategy BGE-m3 CGPT QGpT KPT QGpT KPT MimoTable (CH) MimoTable (EN) OTTQA FetaQA E2E-WTQ R@1 38.54 43.14 55.03 57.2 R@ 62.67 65.0 75.57 78.18 R@1 39.46 40.99 57.79 57.76 R@ 61.64 63.6 77.78 77.95 R@1 R@5 R@1 R@ R@1 R@5 82.02 84.64 95.58 96.21 78.36 80.22 92.59 94. 30.3 29.16 32.0 32.85 46.38 45.98 47.58 47.68 35.27 53.94 36.93 56. 61 74.27 58.09 73.44 Table 4: Comparison Across Different LLMs Table 5: Comparison of retrieval performance for different instance sampling strategies Method MimoTable (CH) MimoTable (EN) R@1 R@5 R@10 R@1 R@ R@10 CGPT (Llama-3.1-8B) w/ GPT-OSS-20B w/ Qwen3-4B 56.8 55.3 58.66 79.81 77.48 78.93 84.2 81.94 84.29 60.13 59.53 60. 81.49 80.58 80.75 85.65 85.92 84.8 55.03%, improving by 16.49 points. When combined with KPT, CGPT achieves 57.20% R@1 and 78.18% R@5 on MimoTable (CH). These results demonstrate that CGPT maintains strong and stable performance across diverse domains and languages, exhibiting robust cross-domain generalization suitable for large-scale table retrieval applications."
        },
        {
            "title": "4.5 Comparison Across Different LLMs",
            "content": "Table 4 reports the performance of different LLMs. We compare three LLMs of varying scales: Llama-3.1-8B-Instruct, GPT-OSS-20B, and Qwen3-4B to assess their impact on CGPTs retrieval performance on MimoTable (EN). Despite differences in model size and architecture, the results remain highly consistent, with R@1 varying by only 0.6 percentage points. This indicates that CGPT is robust to the choice of language model, allowing practitioners to adopt smaller, more cost-efficient models without sacrificing retrieval effectiveness."
        },
        {
            "title": "Method",
            "content": "MimoTable (CH) MimoTable (EN) R@1 R@5 R@10 R@ R@5 R@10 CGPT w/ CB Selection w/ Selection 56.8 51.62 58.13 79.81 70.55 80.11 84.2 76.58 84. 60.13 57.51 57.03 81.49 79.65 80.43 85.65 83.78 85.64 points below CGPT. Selection performs well on Chinese but drops to 57.03% on English, showing limited cross-lingual robustness. In contrast, CGPTs random intra-cluster sampling consistently yields strong results across languages, emphasizing the need to preserve semantic variation for reliable instance selection."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced CGPT, training framework that addresses the limited semantic coverage of zero-shot embedding models in table retrieval. By constructing semantically informed partial tables via K-means clustering and applying contrastive fine-tuning with hard negative sampling, CGPT substantially improves table representations. Across multiple benchmarks, it consistently surpasses QGpT, with an average R@1 gain of 16.54%."
        },
        {
            "title": "4.6 Ablation Study: Instance Sampling",
            "content": "Table 5 reports the performance of various instance sampling strategies. We evaluate two alternative sampling strategies: CB Selection, which replaces intra-cluster random sampling with centroid-based selection, and Selection, which retains only one centroid representative per cluster. Both strategies reduce semantic diversity and lead to clear performance drops. CB Selection reaches only 51.62% (CH) and 57.51% (EN) R@1 on MimoTable, falling 5.18 and 2.62 This research was supported (in part) by NSTC 114-2634-F-005002 - project Smart Sustainable New Agriculture Research Center (SMARTer). References [1] Jonathan Herzig, Thomas MÃ¼ller, Syrine Krichene, and Julian Eisenschlos. 2021. Open domain question answering over tables via dense retrieval. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 512519. CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval [2] Jonathan Herzig, Pawel Krzysztof Nowak, Thomas MÃ¼ller, Francesco Piccinno, and Julian Eisenschlos. 2020. TaPas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th annual meeting of the association for computational linguistics. 43204333. [3] Udayan Khurana, Sahil Suneja, and Horst Samulowitz. 2025. Table Retrieval using LLMs and Semantic Table Similarity. In Companion Proceedings of the ACM on Web Conference 2025. 10721076. [4] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval arXiv preprint for weakly supervised open domain question answering. arXiv:1906.00300 (2019). [5] Hsing-Ping Liang, Che-Wei Chang, and Yao-Chung Fan. 2025. Improving table retrieval with question generation from partial tables. In Proceedings of the 4th Table Representation Learning Workshop. 217228. [6] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [7] Feifei Pan, Mustafa Canim, Michael Glass, Alfio Gliozzo, and Peter Fox. 2021. Cltr: An end-to-end, transformer-based system for cell level table retrieval and table question answering. arXiv preprint arXiv:2106.04441 (2021)."
        },
        {
            "title": "A Prompts",
            "content": "This appendix provides the exact prompt template used for synthetic query generation in Section 3.2."
        },
        {
            "title": "Prompt",
            "content": "You are given table chunk with the following content: table_chunk Your Task: Generate questions_per_chunk diverse questions that would retrieve this specific table chunk. The questions should be based on the actual content shown in the table above. Question Types to Cover: 1. Entity-specific query 2. Temporal query 3. Comparison/Ranking query 4. Aggregation query 5. Complex reasoning query Important Requirements: - Use natural, conversational language - Make questions specific to the actual content shown in the table - Reference real values from the table when possible - Questions should be answerable by looking at this table chunk - Language: lang Output Format (JSON only): \"questions\": \"question3\", ...] Generate questions_per_chunk questions now: [\"question1\", \"question2\","
        }
    ],
    "affiliations": [
        "National Chung Hsing University",
        "Smart Sustainable New Agriculture Research Center (SMARTer)"
    ]
}