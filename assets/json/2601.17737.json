{
    "paper_title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation",
    "authors": [
        "Chenyu Mu",
        "Xin He",
        "Qu Yang",
        "Wanshun Chen",
        "Jiadi Yao",
        "Huang Liu",
        "Zihao Yi",
        "Bo Zhao",
        "Xingyu Chen",
        "Ruotian Ma",
        "Fanghua Ye",
        "Erkun Yang",
        "Cheng Deng",
        "Zhaopeng Tu",
        "Xiaolong Li",
        "Linus"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 2 ] . [ 1 7 3 7 7 1 . 1 0 6 2 : r An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Chenyu Mu ,1,2 , Xin He,1 , Qu Yang,1 , Wanshun Chen1 , Jiadi Yao1 , Huang Liu1 , Zihao Yi1 , Bo Zhao1 , Xingyu Chen1 , Ruotian Ma1 , Fanghua Ye1 , Erkun Yang2 , Cheng Deng2 , Zhaopeng Tu ,1 , Xiaolong Li1 , and Linus1 1Tencent Hunyuan Multimodal Department 2Xidian University https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent Figure 1: Our proposed pipeline consists of three key components: (1) ScripterAgent, trained with GRPO to align its outputs with professional directorial standards; (2) DirectorAgent, which ensures seamless visual continuity across scenes, thereby overcoming the temporal incoherence caused by the fixed-duration constraints of SOTA video generation models; and (3) CriticAgent, which evaluates the generated film from both technical and cinematic perspectives."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing semantic gap between creative idea and its cinematic execution. To bridge this gap, we introduce novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, model trained to translate coarse dialogue into fine-grained, executable cinematic script. To enable this, we construct ScriptBench, new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking. Equal Contribution. Correspondence to: Zhaopeng Tu <zptu@tencent.com>. 1 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation"
        },
        {
            "title": "Introduction",
            "content": "To make great film you need three things: the script, the script and the script. Alfred Hitchcock The emergence of powerful video generation models like Sora2-Pro (Brooks et al., 2024), Veo3.1, and Wan2.5 (Wan et al., 2025) has marked new era in artificial intelligence, demonstrating remarkable ability to synthesize high-fidelity, realistic video clips from simple text prompts. However, this paradigm reveals fundamental limitation when confronted with complex creative tasks. significant yet underexplored semantic gap exists between high-level narrative concept, such as dialogue-driven scene, and the detailed, executable plan required to produce cinematically coherent video. As the legendary director Alfred Hitchcock famously stated, To make great film you need three things: the script, the script and the script. Inspired by this wisdom, we argue that the missing piece in automated filmmaking is the script itself. This paper inverts the conventional video-language relationship from passive description (video-totext) to active generation. We tackle new, challenging task: given only coarse-grained dialogue, the model must anticipate and generate an executable filmmaking plan. This introduces three fundamental challenges: (1) fine-grained contextual understanding to resolve ambiguities in sparse dialogue; (2) domain knowledge of filmmaking to produce valid camera specifications; and (3) creative reasoning to bridge what is said with what must be shown. To address these challenges, we introduce complete, agentic framework for dialogue-to-cinematic-video generation, composed of three core components: ScripterAgent, DirectorAgent, and CriticAgent. To facilitate our research, we first construct ScriptBench, large-scale benchmark for this task. Each instance features rich, trimodal context (dialogue, audio, and character positions) and is annotated using novel, expert-guided pipeline that ensures cinematic plausibility through multi-round error correction. Building on this, we develop ScripterAgent, model trained to transform dialogue into structured cinematic script. We employ two-stage training paradigm: supervised fine-tuning (SFT) to learn the scripts structure, followed by Group Relative Policy Optimization (GRPO) (Shao et al., 2024) with hybrid reward function to align the models creative choices with expert aesthetics. The resulting script is then passed to DirectorAgent, which orchestrates SOTA video models using novel Cross-Scene Continuous Generation strategy with frame-anchoring to produce long-horizon, coherent videos that overcome the temporal limitations of current generators. Our comprehensive experiments demonstrate the effectiveness of this script-centric approach. The full ScripterAgent model significantly outperforms existing methods, with human experts rating its outputs higher in both Dramatic Tension (4.1 vs. 3.7) and Visual Imagery (4.3 vs. 3.8). Furthermore, using our generated scripts as input universally improves the performance of all tested video models (including Sora2-Pro and Veo3.1), boosting metrics like Script Faithfulness by up to +0.4 points. Our analysis also uncovers critical trade-off in these models between visual spectacle and script faithfulness. To quantify temporal fidelity, we introduce novel metric, Visual-Script Alignment (VSA), which confirms that our method enhances temporal-semantic coherence by over 7 points. This work provides the first end-to-end solution for dialogue-driven cinematic video generation and offers new paradigm for automated storytelling. Contributions. Our contributions are summarized as follows: 1. We propose novel agentic framework for the task of dialogue-to-cinematic-video generation, comprising three specialized agentsScripterAgent (for script generation), DirectorAgent (for long-horizon video execution), and CriticAgent (for evaluation)that together bridge the semantic gap between sparse dialogue and coherent cinematic output. 2. We introduce ScriptBench, large-scale, high-quality benchmark with rich multimodal context, curated via novel expert-guided pipeline. We also develop ScripterAgent, model trained with an innovative two-stage SFT and reinforcement learning paradigm to generate professionalquality cinematic scripts. 2 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Figure 2: The three-stage, expert-guided pipeline for creating the ScriptBench. 3. We conduct comprehensive evaluation of state-of-the-art video generation models, revealing fundamental trade-off between visual spectacle and script faithfulness. We validate our frameworks ability to improve temporal coherence with new metric, Visual-Script Alignment (VSA), demonstrating significant gains across all tested models."
        },
        {
            "title": "2 ScripterAgent: Dataset and Model",
            "content": "Prevailing video-language research has largely focused on the paradigm of passive description, where models learn to generate textual descriptions for existing video content (e.g., captioning or question answering). In contrast, our task inverts this relationship: given only coarse-grained dialogue, the model must anticipate and generate an executable filmmaking plan. This generative setting introduces three fundamental challenges absent from conventional benchmarks: 1. fine-grained contextual understanding to resolve ambiguities inherent in sparse dialogue; 2. domain knowledge of filmmaking to produce technically valid camera specifications and staging directions; 3. creative reasoning abilities to bridge the gap between what is said and what should be shown. To bridge this gap and foster research in automated cinematic planning, we introduce ScriptBench, new benchmark designed specifically for this task, along with ScripterAgent, dedicated model trained to transform dialogue into professional-quality cinematic scripts. 2.1 ScriptBench To facilitate our study, we constructed large-scale, high-quality dataset of cinematic scripts. We curated raw instances from high-fidelity cinematic cutscenes. These sources were selected for their rich dialogue, professional cinematography, and high visual consistency, which closely approximate real-world film production. central contribution of our work is scalable yet high-fidelity annotation pipeline that expands sparse dialogue into rich, shot-level cinematic scripts. Although we leverage the SOTA LLM (gemini-2.5-pro), the entire process is tightly governed by expert-defined templates, domain constraints, and validation rules to ensure both cinematic plausibility and physical consistency. The pipeline operates in three stages: Stage 1: Context Reconstruction and Dialogue Fusion. The model first parses the multimodal inputs to reconstruct comprehensive understanding of the scene. It jointly analyzes the textual script and dialogue audio to infer character relationships, scene settings, plot developments, emotional tendencies, and speaking intent. This process fuses the disparate signals into coherent narrative context that makes implicit causal relations explicit. 3 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Stage 2: Shot-Level Semantic Planning. Utilizing the reconstructed context, the model plans shots under four constraints to ensure visual and narrative continuity. Shot integrity enforces self-contained units, introducing cuts only upon clear camera or scene changes. Duration adaptation caps shots at 10 seconds to align with generation limits. Semantic coherence aligns boundaries with narrative transitions (e.g., emotional shifts), while Technical feasibility prevents segmentation during complex camera motions. These principles jointly ensure the shot units are narratively meaningful and technically viable for downstream generation. Stage 3: Multi-Round Adaptive Error Correction. In the final stage, the system executes multiround, adaptive error-correction loop (Figure 2, right) to ensure both structural validity and semantic fidelity of the generated scripts. We design four verification modules: (a) Dialogue Completeness, which ensures that all spoken content is either explicitly transcribed or marked as [No Dialogue]; (b) Character Appearance Consistency, which enforces strict adherence to predefined character descriptions; (c) Scene Coherence, which tracks environmental elements and validates narratively justified transitions; and (d) Positional and Physical Rationality, which verifies spatial relations against plausible blocking and camera geometry. An automated detector iteratively scans the scripts, feeds corrective signals back to the generator, and repeats the loop until all constraints are satisfied. To further validate practical reliability, professional script consultants conducted random audit on 60% of the generated instances, revealing that while the automated pass rate reached 94%, expert review exposed subtle semantic errors such as character teleportation, dialogueaction conflicts, and inconsistent prop states. These findings were incorporated into refined prompt constraints and verification logic, resulting in controlled refinement process that constitutes key novelty of our pipeline and yields cinematic scripts that are structured, internally consistent, and grounded in long-horizon narrative and physical continuity. Dataset Statistics and Usage This pipeline yielded 1,750 finalized script instances, each in oneto-one correspondence with raw multimodal input. The average duration of each video clip is approximately 15.4 seconds, providing sufficient temporal scope for multi-shot sequences while remaining tractable for current generative models. The dataset is partitioned into training set (1700 instances) and test set (50 instances). This partitioning scheme intentionally challenges the model to infer complete cinematic elements from conversational content alone, emulating the real-world process where directors visualize story from dialogue-driven script. 2.2 ScripterAgent Building upon ScriptBench, we develop ScripterAgent, generative model designed to automatically transform coarse-grained dialogue into fine-grained, structured cinematic script. While large-scale foundation models demonstrate strong general capabilities, we hypothesize that this specialized task that requires domain knowledge of shot composition, pacing, and visual continuity, can benefit from targeted training on curated data. To this end, we employ two-stage training paradigm: supervised fine-tuning (SFT) to learn the script format and narrative structure, followed by reinforcement learning (RL) to align the models outputs with professional directorial aesthetics. 2.2.1 Stage One: SFT for Structural Competence The initial stage focuses on teaching the model the fundamental syntax and structure of cinematic scripts. We formulate this as sequence-to-sequence task, where the input is multi-turn dialogue from our dataset, and the output is the target script in structured JSON format. We fine-tune Qwen-Omni-7B as our base model, πbase, chosen for its strong capabilities in long-context processing and instruction following. The training objective is to maximize the conditional log-likelihood of the ground-truth script: LSFT = (x,y)D (cid:35) log πθ(yty<t, x) (cid:34) t= 4 (1) An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation where θ denotes the model parameters. We train for 20 epochs using the AdamW optimizer with learning rate of η = 1 105, batch size of 4, and maximum sequence length of 8,192 tokens. This SFT stage equips the model to generate scripts that are structurally correct and content-complete, forming solid foundation for the subsequent creative refinement. 2.2.2 Stage Two: RL for Cinematic Alignment While SFT ensures structural validity, it is insufficient for capturing the subjective artistry of professional filmmaking. As suggested in our evaluation  (Table 1)  , effective scriptwriting transcends logical correctness, involving aesthetic judgments about shot composition, pacing, and emotional impact. To bridge this gap, we introduce reinforcement learning stage to align ScripterAgent with expert directorial preferences. We employ Group Relative Policy Optimization (GRPO) (Shao et al., 2024), an advanced preference alignment method whose group-based relative scoring is well-suited for creative tasks that have subjective, one-to-many nature of valid outputs. Hybrid Reward Function. key novelty of our RL stage is hybrid reward function, Rtotal, that balances objective correctness with subjective quality. It is weighted sum of two complementary signals, with α = 0.4: Rtotal(y) = α Rstructure(y) + (1 α) Rhuman(y) (2) Rule-Based Structural Reward (Rstructure): This component provides an objective signal for technical correctness, mirroring the verification modules from our data annotation pipeline. It evaluates and aggregates normalized scores from four automated checks: Format Compliance (correct JSON structure), Dialogue Completeness (all spoken lines accounted for), Scene and Character Consistency, and Physical Rationality (plausible character positions and camera geometry). Human Preference Reward (Rhuman): To capture cinematic aesthetics, we model expert human judgment. team of three senior art directors scored SFT model outputs on 15 scale across four creative dimensions: shot division rationality, character acting and emotion, visual aesthetics, and directorial intent. Using 500 such annotated samples (Dpref), we trained BERT-based regression model to predict normalized preference score in [0, 1], serving as scalable proxy for expert cinematic taste. GRPO Optimization. During optimization, for each input x, we generate = 8 candidate scripts k=1 from the current policy πθ(x) and calculate their rewards Rk = Rtotal(y(k)). These {y(k)}K rewards are then used to compute normalized advantage within the group: Ak = Rk σR + ϵ , where = 1 k= Rk (3) The policy is updated by maximizing the advantage-weighted log-likelihood, constrained by KL-divergence penalty to prevent large deviations from the SFT initialization: LGRPO = xD (cid:34) 1 k=1 Ak log πθ(y(k)x) β xD [KL (πθ(x) πSFT(x))] (4) (cid:35) where the KL coefficient β = 0.04. The model is trained for 5,000 steps using the Adam optimizer with learning rate of η = 106 and batch size of 4. This two-stage paradigm successfully elevates ScripterAgents capabilities from generating structurally correct scripts to producing cinematically compelling plans aligned with professional standards."
        },
        {
            "title": "3 DirectorAgent: Long-Horizon Script-to-Video Execution",
            "content": "While ScripterAgent provides structured, shot-by-shot blueprint, translating this plan into continuous video presents its own formidable challenge. The DirectorAgent is designed to 5 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Figure 3: Illustration of the Frame-Anchoring Mechanism. The DirectorAgent utilizes the last frame of the preceding scene (N 1) to visually condition the generation of the current scenes first frame (0). This frame-anchoring strategy preserves visual consistency in character appearance, attire, and scene layout across multiple generation cycles, creating seamless visual relay. bridge this execution gap, acting as an automated orchestrator that transforms the generated script into coherent, high-fidelity video sequence. Its primary function is to overcome fundamental limitation of current video generation models: restricted temporal capacity. State-of-the-art models are typically limited to generating short clips (e.g., 812 seconds), far short of the 13 minute duration of complete narrative scene. Naively segmenting script and generating clips independently leads to severe artifacts, such as identity drift, inconsistent styling, and loss of narrative continuity. The core novelty of the DirectorAgent is Cross-Scene Continuous Generation Strategy, which ensures both semantic coherence and visual consistency across multiple generated segments. This strategy combines (1) intelligent, shot-aware segmentation that respects cinematographic boundaries with (2) frame-anchoring mechanism that conditions each new segment on the final state of the previous one. Intelligent Shot-Based Segmentation. Instead of making arbitrary temporal cuts, the agent partitions the full script into sequence of generation tasks, or scenes that align with the natural cinematographic boundaries defined by ScripterAgent. This process adheres to four key principles: 1. Shot Integrity: Each scene must contain one or more complete shot units, preventing cuts in the middle of continuous camera take. 2. Duration Adaptation: The total duration of scene is constrained to fit within the target models generation window, with 10% safety buffer. 3. Semantic Coherence: Divisions are prioritized at natural narrative breakpoints, such as the end of characters line or shift in emotional tone. 4. Technical Feasibility: Segmentation is favored at fixed camera positions, avoiding cuts during complex camera movements which are harder to transition between seamlessly. Frame-Anchored Continuity. To ensure seamless visual transitions between scenes, the DirectorAgent employs First-Last Frame Connection Mechanism. As illustrated in Figure 3, the final frame of generated scene is extracted and used as visual anchor or conditioning image for 6 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation the generation of the subsequent scene + 1. This technique provides strong visual prior for the video model, explicitly instructing it to maintain consistency in character identity, clothing, facial details, and spatial layout. By forming visual relay from one segment to the next, this mechanism substantially reduces the identity drift and jarring scene changes that plague naive segmentation approaches. To further enhance transition quality, we also inject explicit text like Continuing from the previous scene into subsequent prompts. Effectiveness and Limitations. This strategy effectively extends the coherence window of any underlying video model. By conditioning each new segment on the visual state of its predecessor, it transforms long-horizon generation problem into sequence of locally solvable, continuitypreserving subproblems. While this approach significantly reduces identity drift and layout inconsistencies, challenges such as imperfect lip synchronization and residual misalignment of fine-grained actions remain."
        },
        {
            "title": "4 CriticAgent: The Evaluation Framework",
            "content": "To assess our system comprehensively, we introduce multifaceted evaluation framework that examines both stages of our pipeline: script generation (dialogue-to-script) and video generation (script-to-video). This framework is essential because cinematic quality is inherently multidimensional, encompassing technical correctness, narrative fidelity, and subjective artistic merit. The framework combines objective metrics, automated scoring via our AI-powered CriticAgent, and qualitative evaluations by human experts. All scores are assigned on 05 scale. Detailed metric definitions and evaluation prompts are provided in Appendix B. 4.1 Script Generation Evaluation For the dialogue-to-script stage, we evaluate the generated scripts on both their structural correctness, using our CriticAgent, and their artistic quality, via panel of human experts. AI Rating (CriticAgent) We employ our CriticAgent, powered by gemini-2.5-pro, to automatically assess generated scripts based on four criteria: 1. Format Compliance. Assesses strict adherence to the required JSON format, ensuring all key fields (e.g., Shot Type, Camera Movement, Description) are present and correctly structured. 2. Shot Division Rationality. Evaluates the logical segmentation of the script into shots, ensuring that breaks align with narrative beats and emotional shifts without being overly fragmented or lengthy. 3. Content Completeness. Measures whether the script provides rich, actionable details for filming and enriches the narrative with visual information absent from the source dialogue. 4. Narrative Coherence. Determines whether the sequence of shots is logically connected and if the visual storytelling flows smoothly to complement the dialogues context. Human Rating (Directors Panel) To complement the automated assessment, panel of professional directors and screenwriters evaluates the artistic quality of the scripts. They provide ratings on 05 scale for three key creative aspects, which collectively indicate the scripts potential for being filmed successfully: 1. Character Portrayal Consistency. Assesses whether each characters personality, speaking style, and behavior remain coherent and believable throughout the script. 2. Dramatic Tension & Rhythm. Measures the scripts effectiveness in building, sustaining, and releasing dramatic tension, as well as the naturalness and engagement of its pacing. 7 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation 3. Visual Imagery & Cinematic Expressiveness. Assesses how vividly the script conveys visual information and how effectively it employs cinematic language (e.g., shots, staging, atmosphere) to support the narrative. 4.2 Video Generation Evaluation We evaluate the script-to-video generation stage on two primary axes: script-video alignment and overall video quality. In addition, we conduct automatic evaluation that combines standard video quality metrics with novel measure of script alignment. AI Rating (CriticAgent) For the video generation stage, CriticAgentevaluates the cinematic quality and faithfulness of the generated video to the source script and reference audio across five dimensions: 1. Cinematic Camera Articulation. Measures the sophistication of camera work, including shot types, framing transitions, and dynamic movements that support the scripted narrative. 2. Kinetic Body Language & Blocking. Assesses whether character motions, physical interactions, and spatial arrangements are specific, expressive, and consistent with the scripted actions. 3. Visual Descriptive Fidelity. Evaluates how well the visual details (e.g., character appearance, clothing textures, scene layout, lighting) match the descriptive cues in the script. 4. Emotional Arc & Micro-Expressions. Examines whether the facial expressions, subtle gestures, and temporal evolution reflect the intended emotional progression in the script and audio delivery. 5. Narrative Pacing & Timing. Measures the alignment of shot timing, action beats, and pauses with the narrative structure and rhythm implied by the script and audio. Human Rating Human annotators also assess the final generated videos, providing ratings on five dimensions that collectively offer comprehensive view of video quality: 1. Visual Appeal. Evaluates the realism, aesthetic quality, and rendering stability of the video. 2. Script Faithfulness. Assesses how accurately the video adheres to the provided script in terms of scenes, actions, and plot progression. 3. Narrative Coherence. Measures whether the video forms logically consistent and easy-to-follow story, with reasonable scene transitions and pacing. 4. Character Consistency. Evaluates whether characters maintain stable identity and appearance throughout the video. 5. Physical Law Adherence. Assesses whether motions and interactions in the video plausibly adhere to real-world physical laws, contributing to natural-looking dynamics. Automated Metrics We complement our AI and human evaluations with automated metrics that measure video quality and script alignment. Standard Quality Metrics. We adopt several established metrics to assess general video quality, including the CLIP Score (Radford et al., 2021) for global semantic alignment and subset metrics (e.g., subject and background consistency, motion smoothness) of VBench (Huang et al., 2024). Visual-Script Alignment (VSA). key limitation of standard metrics is that they often measure whether content appears but not when it appears. To address this, we propose Visual-Script Alignment (VSA), novel metric designed to evaluate temporal-semantic fidelity. VSA measures whether visual events occur within their designated time intervals as specified by the script. Given script with shot units, each with an instruction Ik and time interval Tk, and video with frames vt, VSA is defined as: VSA = 1 k=1 Tk k=1 tTk 8 Sim (Evis(vt), Etxt(Ik)) , (5) An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Table 1: Script generation evaluation on the ScriptBench test set. AI Rating (0-5) Human Rating (0-5) Method Format Comp. Division Shot Content Narrative Character Dramatic Tension Comp. Consist. Coher. CHAE (Wang et al., 2022) MoPS (Ma et al., 2024a) SEED-Story (Yang et al., 2025) ScriptAgent (SFT only) ScriptAgent (Full) 3.3 3.2 3.6 3.9 4.0 3.2 3.1 3.5 3.6 3.9 3.4 3.3 3.7 3.8 4. 3.5 3.4 3.8 3.9 4.2 3.1 3.0 3.6 3.7 4.0 3.3 3.2 3.7 3.6 4. Visual Imagery 3.4 3.3 3.8 3.8 4.3 where Evis and Etxt are CLIP encoders. higher score indicates stronger alignment between the videos temporal structure and the scripts plan."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Results of Script Generation We compare against representative story visualization and screenplay generation methods: MoPS (Ma et al., 2024a) proposes modular framework for automated story premise synthesis by decomposing premises into theme, background, persona, and plot modules, then recombining them via nested dictionary and LLM-based integration to generate diverse, high-quality story foundations. CHAE (Wang et al., 2022) enables fine-grained controllable story generation by allowing users to specify characters, their actions, and emotions through structured input format, enhanced with copy mechanism and character-wise emotion loss for precise narrative control. SEED-Story (Yang et al., 2025) extends multimodal large language models to generate long, coherent narratives interleaved with images, utilizing multimodal attention sink mechanism to maintain consistency and enable generation beyond training sequence lengths. Our proposed ScriptAgent significantly outperforms all baseline methods in generating highquality cinematic scripts. As demonstrated in Table 1, the full ScripterAgent model achieves state-of-the-art results across all AI and human evaluation metrics. It shows marked superiority over the strongest baseline, MovieAgent, in structural quality, with improvements of +0.4 points in Format Compliance, Content Completeness, and Narrative Coherence. More importantly, human evaluators confirm its superior creative capabilities, awarding it substantially higher scores in Visual Imagery (4.3 vs. 3.8) and Dramatic Tension (4.1 vs. 3.7). These results confirm that ScripterAgent, trained on our ScriptBenchbenchmark, effectively bridges the semantic gap by transforming coarse dialogue into detailed, cinematically expressive, and director-level instructions. The preference-alignment RL stage is crucial for elevating creative and artistic quality beyond structural correctness. To isolate the impact of our two-stage training paradigm, we compare the full ScripterAgent (SFT+RL) with an SFT-only variant. The results show that the SFT-only model already learns the basic script structure, achieving strong scores in Format Compliance (3.9) and Narrative Coherence (3.9). However, the subsequent GRPO-based preference alignment stage provides significant boosts to more subjective, artistic dimensions. For instance, the score for Dramatic Tension improves from 3.6 to 4.1, and Visual Imagery rises from 3.8 to 4.3. This finding validates our hypothesis that while SFT is sufficient for structural competence, the RL stage with its hybrid reward function is essential for aligning the model with expert directorial aesthetics, refining its ability to handle nuanced creative elements like pacing and shot composition. 9 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Figure 4: Qualitative comparison: ScripterAgent vs. MovieAgent (Wu et al., 2025). Qualitative analysis further highlights ScripterAgents ability to generate detailed, executable filmmaking plan. direct comparison of generated outputs reveals the practical superiority of our approach, as shown in Figure 4. The script from MovieAgent provides simple plot summary, such as Camera slowly pans.... In stark contrast, the ScripterAgent output constitutes complete, fine-grained cinematic blueprint. It specifies precise technical details like camera settings (Shallow Depth of Field + panoramic shot), timestamps for synchronization ([00:00:00 00:00:08]), detailed character appearance (Approximately 180cm tall), atmospheric scene descriptions, and exact character positioning or blocking. This richness confirms that ScripterAgent successfully generates the professional-quality, executable script needed to guide automated video production, directly addressing the core challenge outlined in our introduction. 5.2 Results of Video Generation We evaluate SOTA text-to-video models supporting voice generation and long-text inputs (> 1000 tokens): Vidu2 (Bao et al., 2024) (Shengshu Technology, Tsinghua University): U-ViT-based model excelling in temporal consistency and generation speed. Seedance1.5-Pro (Chen et al., 2025b) (ByteDance Seed): high-fidelity diffusion model specialized in generating professional-grade videos with enhanced dynamic coherence and visual detail. Kling2.6 (Kuaishou Technology): Transformer-based generative model capable of synthesizing high-resolution videos with improved motion fluidity and scene understanding. Wan2.6 (Wan et al., 2025) (Alibaba): diffusion-based framework synthesizing realistic scenes with intricate details and smooth temporal dynamics. HYVideo1.5 (Wu et al., 2025) (Tencent): large-scale video generation framework featuring dual-stream diffusion transformer architecture that achieves state-of-the-art performance in prompt following and 4K-resolution synthesis. 10 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Table 2: Video generation evaluation on the ScriptBench test set. AI Rating (0-5) Human Rating (0-5) Overall Mean Model Cam. Body Visual Emotion Pace Visual Script Character Physical NarrativeAvg. Avg. Artic. Block. Fidelity Arc Timing Appeal Faith Consist. Coher. AI Human Law Vidu2 4.1 Seedance1.5-Pro 4.0 4.1 4.2 4.0 4.1 4.0 4.1 Kling2.6 Wan2.6 HYVideo1.5 Sora2-Pro Veo3.1 Average Score Vidu2 4.2 Seedance1.5-Pro 4.5 4.3 4.4 4.4 4.1 4.1 4. Kling2.6 Wan2.6 HYVideo1.5 Sora2-Pro Veo3.1 Average Score 4.1 4.0 4.1 4.2 4.0 4.0 3.9 4.0 4.4 4.6 4.5 4.6 4.5 4.4 4.4 4.5 Raw Dialogue (w/o ScripterAgent) 4.5 4.5 4.6 4.7 4.5 4.6 4.4 4.5 4.4 4.3 4.4 4.4 4.3 4.3 4.3 4.3 3.0 3.2 3.3 3.1 4.1 3.7 4.1 3. 4.1 3.7 3.5 3.2 4.2 3.6 4.0 3.8 4.4 4.2 4.4 4.4 4.3 4.3 4.4 4.3 3.7 3.5 3.6 3.5 4.0 4.2 3.9 3.8 w/ ScripterAgent 4.3 4.1 4.1 4.0 4.6 4.2 4.4 4.2 4.5 4.7 4.6 4.7 4.7 4.6 4.4 4.6 3.9 4.0 3.9 4.1 4.5 4.8 4.6 4. 4.5 4.6 4.5 4.6 4.5 4.5 4.6 4.5 3.7 4.1 4.0 3.8 4.4 4.3 4.3 4.1 4.7 4.7 4.6 4.7 4.8 4.7 4.5 4.7 3.3 3.1 3.4 3.7 3.8 4.1 3.9 3.6 3.9 3.9 4.2 4.0 4.2 4.5 4.4 4.2 3.1 3.5 3.7 3.4 4.1 3.9 4.0 3. 3.8 4.1 4.1 3.9 4.3 4.1 4.2 4.1 4.3 4.2 4.3 4.4 4.2 4.3 4.2 4.2 4.5 4.6 4.5 4.6 4.6 4.5 4.4 4.5 3.4 3.4 3.5 3.4 4.0 3.9 4.0 3.7 3.9 4.0 4.1 4.0 4.4 4.4 4.4 4.2 Sora2-Pro (Brooks et al., 2024) (OpenAI): diffusion transformer model generating high-fidelity, physically plausible videos with complex scenes. Veo3.1 (Google DeepMind): generative model creating high-resolution (e.g., 1080p) videos with strong cinematic quality and motion coherence. 5.2.1 Results of AI and Human Rating ScripterAgent enables faithful, temporally coherent video generation by translating dialogue into executable cinematic structure. Conditioning video models on structured scripts produced by ScripterAgent yields consistent improvements across all evaluated dimensions  (Table 2)  . Aggregated results reveal uniform uplift: the mean AI rating rises from 4.2 to 4.5, and the mean human rating increases from 3.7 to 4.2 . This efficacy is most pronounced in Script Faithfulness, with Wan2.6 improving from 3.2 to 4.0 and Sora2-Pro from 3.6 to 4.2, while HYVideo1.5 achieves the highest overall fidelity (4.6). Beyond semantic alignment, explicit shot-level blocking instructions enhance fine-grained execution, boosting AI-rated Pace Timing and Body Blocking by synchronizing motion with scene rhythm. Furthermore, gains in Character Consistency and Narrative Coherence validate DirectorAgents Cross-Scene strategy; by coupling boundary-aware segmentation with frame-anchoring, the framework mitigates identity drift and extends coherent generation beyond single-model limits. Collectively, these results confirm that the domain-informed plans from ScripterAgent provide essential guidance absent in raw dialogue. Our evaluation reveals fundamental trade-off in SOTA models between visual spectacle and strict script adherence. The results expose clear divergence in model capabilities that corroborates our third contribution. Sora2-Pro excels in visual impact, securing top scores in Visual Appeal (4.8) and Physical Law adherence (4.5), making it ideal for high-spectacle generation where realism is paramount. Conversely, HYVideo1.5 prioritizes narrative integrity, leading in Script Faithfulness (4.6), Character Consistency (4.4), and Narrative Coherence (4.3). This dichotomy suggests that current video models optimize along different axes: some prioritize perceptual realism, while others better 11 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation (a) Example1 (b) Example2 Figure 5: Examples of video generation using Sora2-Pro. maintain the semantic logic of storyline when guided by structured scripts. This insight provides valuable guidance for practitioners selecting models for specific filmmaking applications. 5.2.2 Results of Automatic Metrics Our VSA metric confirms ScripterAgent enhances temporal-semantic fidelity. As shown in Table 3, conditioning on ScripterAgent outputs yields consistent gains in our proposed VisualScript Alignment (VSA) metric. For instance, Veo3.1s VSA score increases from 51.4 to 53.8, and Sora2-Pros from 48.6 to 50.6. HYVideo1.5 achieves the highest performance with VSA score of 54.8. This quantitatively validates that the detailed instructions from ScripterAgent enable models to better adhere to script semantics and timing. This enhanced alignment is supported by consistent improvements in CLIP scores across all models (Average +1.7), although we note that strict adherence occasionally impacts pure aesthetic scores for some models, suggesting nuanced trade-off between fidelity and perceptual beauty. Automated metrics reinforce the trade-off between visual dynamism and script adherence. The quantitative data in Table 3 corroborates our qualitative findings. Sora2-Pro leads in visual spectacle, achieving the highest scores in Aesthetic quality (62.8), Dynamic Degree (79.5), and Motion Smoothness (98.2). In contrast, HYVideo1.5 excels at instruction following, posting the highest VSA score (54.8) for temporal alignment, while also leading in Subject Consistency (97.2) and Background Consistency (97.5), confirming its strength in maintaining narrative integrity over long horizons. 12 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Table 3: Automatic metrics of video generation on the ScriptBench test set. Alignment Fidelity VBench Metrics% Model CLIP VSA Subj. Cons. Bg Cons. Mot. Smth. Dyn. Deg. Aesthetic Vidu2 Seedance1.5-Pro Kling2.6 Wan2.5 HYVideo1.5 Sora2-Pro Veo3.1 Vidu2 Seedance1.5-Pro Kling2.6 Wan2.5 HYVideo1.5 Sora2-Pro Veo3.1 42.2 43.8 44.7 45.3 43.4 44.1 43.6 43.9 45.5 46.2 47.2 45.1 46.0 45.3 w/o ScripterAgent 92.8 93.5 92.5 91.5 95.8 90.6 94. 93.1 93.8 92.8 91.8 96.0 91.0 94.8 w/ ScripterAgent 94.1 94.8 93.8 92.8 97.2 91.8 95.8 94.3 95.0 94.0 93.1 97.5 92.5 96.1 48.2 50.4 51.3 52.1 52.7 48.6 51.4 50.0 52.6 53.5 54.1 54.8 50.6 53. 94.5 95.0 95.5 94.9 96.1 96.8 95.5 95.8 96.0 96.8 96.2 97.3 98.2 96.9 46.5 48.5 52.0 50.2 68.8 75.2 63.1 49.2 51.5 56.5 53.8 72.5 79.5 66.4 48.1 49.0 53.5 51.0 57.5 59.5 55.2 50.5 51.5 56.0 53.4 60.2 62.8 57. ScripterAgent elicits more dynamic and consistent video generation. Using ScripterAgent prompts an increase in the Dynamic Degree metric across all models. This is highlighted by Sora2-Pros increase from 75.2 to 79.5 and Kling2.6s rise from 52.0 to 56.5. This suggests that explicit action descriptions in the scripts guide models to create more visually complex scenes, moving beyond the static talking head outputs often produced from raw dialogue. Furthermore, widespread improvements in Subject and Background Consistency validate that our cross-scene generation strategy effectively mitigates identity drift."
        },
        {
            "title": "6 Related Work",
            "content": "Video Generation. Recent video generation relies on diffusion models (Blattmann et al., 2023; He et al., 2022; Ho et al., 2022; Khachatryan et al., 2023; Singer et al., 2022; Bao et al., 2024; Brooks et al., 2024; Wan et al., 2025; Wu et al., 2025) and language models (Hong et al., 2022; Chang et al., 2022; 2023; Kondratyuk et al., 2023; Villegas et al., 2022). However, existing video generation systems still face significant limitations in managing long-form narrative coherence, especially when dealing with complex film scripts (Chen et al., 2025a). Our work addresses these challenges with cross-scene generation strategy that mitigates fixed-duration constraints. LLMs for Film Production. LLMs are increasingly used to automate film production tasks like scene generation, character planning, and cinematography (Lin et al., 2023; Wei et al., 2022). Systems such as Anim-Director (Li et al., 2024) help generate storylines and refine scenes, while MovieAgent (Wu et al., 2025) uses multi-agent collaboration for automated film creation. key limitation of these models is their reliance on manual input for narrative and cinematographic planning. In contrast, our approach introduces comprehensive end-to-end pipeline (Huang et al., 2025) that automates scene structuring and planning, reducing manual intervention and ensuring adherence to professional filmmaking standards. Story Visualization. Story visualization maps scripts to visual sequences. Early methods (StoryGAN (Li et al., 2019)) produced static, temporally incoherent images. Recent diffusion models (StoryDiffusion (Zhou et al., 2024), Magic-Me (Ma et al., 2024b)) improve temporal consistency and motion but still lack automated high-level planning for cinematography, scene structure, and character interactions, thus requiring manual guidance. We introduce multi-agent, chain-of-thought 13 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation (CoT) framework that uses hierarchical reasoning to automate long-form movie generation, ensuring temporal consistency, narrative integrity, and visual appeal over extended durations."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we addressed the critical challenge of generating long-form, coherent cinematic videos from sparse dialogue. We proposed novel, script-centric agentic framework that successfully bridges the semantic gap between high-level narrative concepts and low-level video synthesis. Our system, featuring ScripterAgent, DirectorAgent, and CriticAgent, automates the pipeline from dialogue to final video by first generating detailed, executable cinematic script. The efficacy of our approach is rooted in the high-quality ScriptBench benchmark and the innovative two-stage (SFT+RL) training of ScripterAgent. Our experiments empirically validate that this script-driven approach universally enhances the narrative coherence, temporal fidelity, and script faithfulness of state-of-the-art video generation models. Our proposed Visual-Script Alignment (VSA) metric provides quantitative measure of this improvement. Moreover, this study offers the first systematic analysis of the trade-off between visual spectacle and narrative adherence in leading video models, key insight for the field. This work moves beyond simple prompt-based generation, laying the groundwork for sophisticated automated storytelling systems capable of reasoning about cinematography, pacing, and long-term consistency. Future research could focus on enhancing fine-grained control, such as precise lip synchronization, and developing adaptive models that can dynamically generate content in diverse cinematic styles. 14 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation"
        },
        {
            "title": "References",
            "content": "Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-tovideo generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2256322575, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1 (8):1, 2024. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025a. Siyan Chen, Yanfei Chen, Ying Chen, Zhuo Chen, Feng Cheng, Xuyan Chi, Jian Cong, Qinpeng Cui, Qide Dong, Junliang Fan, et al. Seedance 1.5 pro: native audio-visual joint generation foundation model. arXiv preprint arXiv:2512.13507, 2025b. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Kaiyi Huang, Yukun Huang, Xintao Wang, Zinan Lin, Xuefei Ning, Pengfei Wan, Di Zhang, Yu Wang, and Xihui Liu. Filmaster: Bridging cinematic principles and generative ai for automated film generation. arXiv preprint arXiv:2506.18899, 2025. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern generative models. Recognition, pp. 2180721818, 2024. Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1595415964, 2023. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 15 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, and Jianfeng Gao. Storygan: sequential conditional gan for story visualization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 63296338, 2019. Yunxin Li, Haoyuan Shi, Baotian Hu, Longyue Wang, Jiashun Zhu, Jinyi Xu, Zhen Zhao, and Min Zhang. Anim-director: large multimodal model powered agent for controllable animation video generation. In SIGGRAPH Asia 2024 Conference Papers, pp. 111, 2024. Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning. arXiv preprint arXiv:2309.15091, 2023. Yan Ma, Yu Qiao, and Pengfei Liu. Mops: Modular story premise synthesis for open-ended automatic story generation. arXiv preprint arXiv:2406.05690, 2024a. Ze Ma, Daquan Zhou, Xue-She Wang, Chun-Hsiao Yeh, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, and Jiashi Feng. Magic-me: Identity-specific video customized diffusion. In European Conference on Computer Vision, pp. 1937. Springer, 2024b. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Xinpeng Wang, Han Jiang, Zhihua Wei, and Shanlin Zhou. Chae: Fine-grained controllable story generation with characters, actions and emotions. arXiv preprint arXiv:2210.05221, 2022. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. Bing Wu, Chang Zou, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Jack Peng, et al. HunyuanVideo 1.5 Technical Report. arXiv:2511.18870, 2025. Weijia Wu, Zeyu Zhu, and Mike Zheng Shou. Automated movie generation via multi-agent cot planning. arXiv preprint arXiv:2503.07314, 2025. Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Ying-Cong Chen. Seedstory: Multimodal long story generation with large language model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 18501860, 2025. Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. Advances in Neural Information Processing Systems, 37:110315110340, 2024. 16 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation"
        },
        {
            "title": "A Supplementary Experiment",
            "content": "A.1 Additional experiments for Table 2 Table 4: Video generation evaluation on the ScriptBench test set by Qwen3-VL. Model Cam. Artic. Body Block. Visual Fidelity Emotion Arc Pace Timing Avg. AI Rating (0-5) Raw Dialogue (w/o ScripterAgent) Vidu2 Seedance1.5-Pro Kling2.6 Wan2.6 HYVideo1.5 Sora2-Pro Veo3.1 Average Score Vidu2 Seedance1.5-Pro Kling2.6 Wan2.6 HYVideo1.5 Sora2-Pro Veo3.1 Average Score 3.9 4.0 3.8 4.1 3.8 3.8 3.8 3.9 4.6 4.6 4.6 4.6 4.6 4.6 4.6 4.6 3.8 4.1 3.8 4.0 3.7 3.8 3.8 3.9 4.3 4.4 4.2 4.5 4.2 4.3 4.2 4.3 w/ ScripterAgent 4.6 4.5 4.6 4.6 4.6 4.5 4.5 4.6 4.4 4.4 4.4 4.4 4.4 4.3 4.4 4.4 4.3 4.4 4.2 4.5 4.2 4.3 4.3 4.3 4.7 4.7 4.7 4.7 4.7 4.6 4.6 4.7 4.1 4.2 4.0 4.3 4.0 4.1 4.0 4.1 4.7 4.7 4.8 4.7 4.7 4.7 4.7 4. 4.1 4.2 4.0 4.3 4.0 4.1 4.0 4.1 4.6 4.6 4.6 4.6 4.6 4.5 4.6 4.6 A.2 Ablation Study on Scripting and Agentic Components A.2.1 Experimental Configuration We evaluate the incremental impact of our framework across four stages: Baseline (Raw Dialogue): Generating videos directly from dialogue prompts without ScripterAgent. w/ Script Only: Conditioning on ScripterAgents detailed scripts, but generating as single long-horizon clip. w/ Script + Segment: Implementing shot-aware segmentation but without the frameanchoring consistency mechanism. Full Agent (Ours): The complete pipeline featuring script-conditioning, segmentation, and Frame-Anchoring for visual continuity. A.2.2 Quantitative Results Analysis We evaluate the generated videos across five cinematic dimensions using panel of three advanced LLMs as critics: Gemini 2.5 Pro, Qwen 3 VL, and GLM 4.6 V. As shown in Table 5, the Full Agent consistently outperforms baselines across all metrics and backbone models (Wan2.6 and HYVideo1.5). The ablation study reveals clear functional decoupling of our agents. Visual Descriptive Fidelity and Kinetic Body Language & Blocking show marked improvements with the introduction of w/ Script 17 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Setting Cam. Body Visual Emotion Artic. Block. Fidelity Arc Pace Cam. Body Visual Emotion Pace Cam. Body Visual Emotion Timing Artic. Block. Fidelity Arc Timing Artic. Block. Fidelity Arc Pace Timing Gemini 2.5 Pro Qwen 3 VL GLM 4.6 Baseline 4.2 w/ Script Only 4.2 4.3 w/ Script+Seg. 4.4 Full Agent Baseline 4.0 w/ Script Only 4.1 4.3 w/ Script+Seg. 4.4 Full Agent 4.2 4.3 4.4 4. 4.0 4.2 4.3 4.5 4.7 4.7 4.7 4.7 4.5 4.6 4.7 4.8 4.4 4.5 4.5 4.6 4.3 4.4 4.4 4.5 4.4 4.5 4.6 4. 4.3 4.4 4.6 4.7 Wan2.6 4.0 4.2 4.4 4.6 4.1 4.2 4.3 4.4 4.1 4.3 4.5 4.6 HYVideo1.5 4.0 4.3 4.5 4.6 4.1 4.2 4.3 4. 3.9 4.2 4.4 4.6 4.1 4.3 4.5 4.7 3.8 4.1 4.4 4.7 4.2 4.4 4.6 4.7 4.0 4.3 4.5 4.7 4.0 4.2 4.4 4. 4.1 4.3 4.5 4.6 4.1 4.4 4.6 4.8 3.8 4.0 4.1 4.2 4.2 4.3 4.4 4.5 4.2 4.5 4.7 4.9 4.0 4.2 4.4 4. 4.1 4.3 4.5 4.6 4.1 4.3 4.4 4.5 4.0 4.3 4.5 4.6 Table 5: Ablation results on Wan2.6 and HYVideo1.5 across three AI evaluators. All scores are on 0-5 scale. Only, attributing fine-grained visual control to the ScripterAgent. Conversely, the DirectorAgent is shown to be critical for temporal dimensions; the implementation of shot-aware segmentation and frame-anchoring drives scores in Narrative Pacing & Timing and Cinematic Camera Articulation to their highest levels. The high agreement among the three diverse critic models strongly corroborates the validity of these improvements."
        },
        {
            "title": "B Details on Evaluation Metrics",
            "content": "In this section, we detail the human evaluation criteria used in our experiments and all scores are assigned on 05 scale. B.1 Human Evaluation for Script Generation Annotators read the source dialogue and the generated script, then assign 05 scores for each metric according to the following criteria. Character Portrayal Consistency. This metric evaluates whether each characters personality, speaking style, and behavior remain coherent and believable throughout the script. Score 0 (Invalid or Off-Topic): The script is largely unrelated to the given dialogue, missing key characters or scenes, or is so fragmented that character portrayal cannot be meaningfully judged. Score 1 (Completely Inconsistent): Characters frequently change personality, tone, or goals without any narrative justification, making them feel arbitrary or incomprehensible. Score 2 (Severe Inconsistencies): Major contradictions in speech style or behavior appear, and characters often feel unstable or unconvincing. Score 3 (Noticeable Variations): Core traits are roughly maintained, but several noticeable shifts in dialogue or actions break immersion. Score 4 (Good Consistency): Characters are generally consistent in personality and voice, with only minor deviations that do not seriously affect believability. Score 5 (Perfect Consistency): Each character maintains stable and well-defined identity, with coherent speech and behavior in all scenes. Dramatic Tension & Rhythm. This metric measures how well the script builds, sustains, and releases dramatic tension, and whether the pacing of events feels natural and engaging. Score 0 (Invalid or No Story): The text does not form recognizable story (e.g., mostly noise, repetition, or unrelated fragments), so dramatic structure cannot be evaluated. 18 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Score 1 (No Dramatic Structure): The script lacks recognizable conflicts or turning points, and the pacing feels random or monotonous. Score 2 (Weak and Uneven Tension): Some conflicts exist but are poorly set up or resolved, with abrupt or dragging sections that hurt engagement. Score 3 (Basic but Uneven Rhythm): The overall arc is understandable, but there are noticeable pacing issues (e.g., rushed climaxes or overlong trivial scenes). Score 4 (Good Dramatic Arc): Conflicts, climaxes, and resolutions are well-structured, with generally appropriate pacing and emotional build-up. Score 5 (Strong and Compelling Tension): The script offers clear and engaging dramatic curve, with well-timed beats that sustain viewer interest throughout. Visual Imagery & Cinematic Expressiveness. This metric assesses how vividly the script conveys visual information and how well it uses cinematic language (shots, staging, atmosphere) to support filming. Score 0 (Not Filmable / Unreadable): The script is so incomplete, disorganized, or off-topic that it provides no meaningful basis for imagining shots or scenes. Score 1 (Vague and Non-Visual): Descriptions are extremely abstract, offering almost no cues for camera work, staging, or visual composition. Score 2 (Sparse Visual Guidance): Only few scattered visual hints are provided; most scenes are difficult to imagine concretely on screen. Score 3 (Basic Visual Clarity): Key scenes and actions are described clearly enough to imagine, but shot types or cinematic details remain generic. Score 4 (Good Cinematic Guidance): The script includes clear descriptions of scenes, actions, and rough shot intentions, enabling straightforward visualization. Score 5 (Highly Cinematic and Filmable): The script shows rich visual imagination and appropriate use of film language, making it easy to translate into professional storyboards. B.2 Human Evaluation for Video Generation Annotators watch each generated video with access to the corresponding script (or dialogue) as reference, and then give 05 scores according to the following guidelines. Visual Appeal. This metric evaluates the realism, aesthetic quality, and rendering stability of the video. Score 0 (Missing or Corrupted Video): The video cannot be properly viewed (e.g., file error, almost entirely static or black frames), so visual quality is not assessable. Score 1 (Severe Artifacts): Heavy distortions, glitches, or unrecognizable content dominate most frames, making the video difficult to watch. Score 2 (Poor Visual Quality): Frequent flickering, unstable textures, and obvious inconsistencies, though core content is still somewhat interpretable. Score 3 (Moderate Issues): Overall content is clear, but noticeable artifacts in textures, lighting, or motion transitions reduce visual quality. Score 4 (Good Visual Quality): Generally realistic and stable visuals with only minor distortions or occasional flickering that do not strongly affect viewing. Score 5 (High-Quality Rendering): Smooth, realistic, and aesthetically pleasing visuals, comparable to professionally produced footage, with minimal visible artifacts. Script Faithfulness. This metric assesses how accurately the video follows the intended script or ScriptAgent-generated plan in terms of scenes, actions, and plot progression. An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Score 0 (No Relation or No Reference): The video has almost no identifiable connection to the given script (or the script is missing/invalid), making faithfulness impossible to judge. Score 1 (Almost Unrelated): The video bears little or no resemblance to the script, missing key scenes, characters, or events. Score 2 (Major Deviations): Some script elements appear, but important settings, actions, or turning points are missing, misplaced, or heavily altered. Score 3 (Partial Alignment): The main storyline can be recognized, but there are noticeable inaccuracies or omissions in details and shot arrangement. Score 4 (Good Faithfulness): Most key moments and settings match the script, with only minor deviations that do not significantly affect overall story understanding. Score 5 (Almost Perfect Adherence): The video closely follows the script in both content and structure, accurately reflecting specified scenes, actions, and plot beats. Narrative Coherence. This metric measures whether the video forms logically consistent and easy-to-follow story, with reasonable scene transitions and pacing. Score 0 (No Discernible Narrative): The video is so fragmented, repetitive, or random that no coherent storyline or temporal order can be inferred. Score 1 (Completely Incoherent): Scenes appear random and disconnected, with no understandable storyline or causal relations. Score 2 (Frequent Confusion): Some story intention is visible, but abrupt cuts and illogical transitions make the plot hard to follow. Score 3 (Partially Coherent): basic story can be inferred, yet inconsistencies or awkward pacing often disrupt narrative flow. Score 4 (Well-Structured Story): The video presents mostly clear and coherent narrative, with only minor issues in pacing or transitions. Score 5 (Fully Coherent and Engaging): The plot develops smoothly and logically, with seamless scene transitions and clear, engaging storytelling arc. Character Consistency. This metric evaluates whether characters maintain stable identity and appearance across the entire video. Score 0 (Characters Not Identifiable): Human or main characters are almost entirely missing, severely deformed, or indistinguishable, so consistency cannot be meaningfully evaluated. Score 1 (Completely Inconsistent): Character faces, bodies, or outfits change drastically between shots, making them hard to recognize as the same person. Score 2 (Severe Inconsistencies): Frequent noticeable changes in facial features, clothing, or proportions, even if some continuity is preserved. Score 3 (Moderate Variations): Characters are generally recognizable, but variations in appearance or style appear multiple times and affect immersion. Score 4 (Good Consistency): Character identity and look are mostly stable, with only small visual fluctuations that do not seriously disrupt continuity. Score 5 (Perfect Consistency): Characters keep highly stable appearance and identity across all shots, making them visually coherent throughout the video. Physical Law Adherence. This metric assesses whether motions and interactions in the video roughly follow real-world physical laws, contributing to natural-looking dynamics. Score 0 (Motion Not Assessable): The video is nearly static, heavily corrupted, or too abstract, so physical plausibility of motion cannot be reasonably judged. 20 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Score 1 (Highly Unrealistic): Objects or characters frequently move in impossible ways (e.g., floating, severe limb distortions) without any narrative justification. Score 2 (Many Violations): Multiple obvious physical inconsistencies (e.g., unnatural collisions, gravity-defying motion), although not in every scene. Score 3 (Partially Plausible): Most movements are acceptable, but there are several noticeable errors in weight, balance, or continuity of motion. Score 4 (Mostly Realistic): Characters and objects move in generally natural way, with only minor physical anomalies that are easy to overlook. Score 5 (Highly Plausible Physics): Motion and interactions appear smooth and physically convincing, with no obvious violations of basic physical laws. B.3 AI-Based Rating Metrics To complement objective metrics, we use LLM-based evaluators to score both script generation and video generation on 05 scale aligned with film production standards. For scripts, Gemini-2.5-Pro focuses on structural and logical correctness; for videos, Gemini-2.5-Pro focuses on perceptual and technical audio-visual quality. Below we show the full prompts and the detailed scoring rubric for each dimension. Script Generation Evaluation Prompt You are professional film director and script supervisor. Your task is to evaluate the quality of generated shooting script based on the provided coarse-grained dialogue and context. Input Data: Source Dialogue: {Insert Origin Dialogue Here} Generated Script: {Insert Generated JSON Script Here} Evaluation Criteria: Please score the generated script on scale of 0 to 5 for each of the following dimensions. For each dimension, use the following general guideline: Score 0: Completely unusable or fails the requirement. Score 1: Very poor quality; severe issues in most parts. Score 2: Clearly below acceptable quality; many issues. Score 3: Acceptable but with noticeable issues. Score 4: Good quality with only minor issues. Score 5: Excellent quality; no meaningful issues. Then, judge each dimension more concretely as follows: 1. Format Compliance (05): Does the output strictly follow the required JSON format? Are all key fields (Shot Type, Camera Movement, Description, etc.) present and correctly structured? 0: Not valid JSON or completely ignores the requested schema. 1: Severe structural errors; many missing or malformed fields. 2: Multiple structural problems; only partially follows the schema. 3: Mostly follows the schema but with some missing fields or minor format issues. 4: Fully follows the schema with only very small formatting inconsistencies. 5: Perfectly formatted JSON with all fields correctly present and structured. 2. Shot Division Rationality (05): Is the script segmented into shots reasonably? Do the shot breaks align with narrative beats and emotional shifts without being too fragmented or too long? 0: No meaningful shot division; essentially single block or random splitting. 1: Very unreasonable segmentation; shots break the flow and ignore story structure. 2: Many inappropriate shot boundaries; frequent overor under-segmentation. 3: Basic correspondence to narrative beats, but with several awkward or suboptimal shot splits. 4: Mostly well-aligned with emotional and narrative shifts, with only minor segmentation issues. 21 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation 5: Shot division is highly reasonable, closely following narrative and emotional structure throughout. 3. Content Completeness (05): Does the script provide rich, actionable details for filming? Does it supplement necessary visual information that was missing in the source dialogue? 0: Almost no additional visual or staging information beyond the raw dialogue. 1: Very sparse detail; crucial information for filming is largely missing. 2: Some useful details, but important aspects (scene, actions, camera) remain underspecified. 3: Contains enough information to stage the scene, but important visual details are still missing. 4: Generally rich and specific, covering most necessary visual, spatial, and action details. 5: Highly complete and specific, providing clear and thorough guidance for key visual choices. 4. Narrative Coherence (05): Is the sequence of shots logically connected? Does the visual storytelling flow smoothly and match the context of the dialogue? 0: Completely incoherent sequence; shots appear random and unrelated to the dialogue. 1: Very confusing progression; frequent contradictions or abrupt jumps. 2: rough story is visible, but there are many logical gaps, contradictions, or unnatural transitions. 3: Overall story is understandable, but several transitions or details break the narrative flow. 4: Mostly coherent and smoothly flowing narrative with only minor inconsistencies. 5: Fully coherent, well-structured visual narrative that aligns closely with the dialogue context. Output Format: Return the result in the following JSON format: { } \"Format Compliance\": [Score], \"Shot Division Rationality\": [Score], \"Content Completeness\": [Score], \"Narrative Coherence\": [Score] Video Generation Evaluation Prompt You are an expert AI Film Critic and Cinematographer with deep expertise in visual storytelling, camera techniques, and cinematic language. Your task is to evaluate the videos cinematic quality and adherence to complex directorial instructions. Input Data: Reference Script: {Insert Reference Script Here} Generated Video: {Video File to be Evaluated} Evaluation Criteria: Please score the video on scale of 0.0 to 5.0 for each of the following cinematic dimensions. You can assign ANY decimal score (e.g., 2.3, 3.7, 4.2). The integer benchmarks (0, 1, 2, 3, 4, 5) serve as REFERENCE POINTS for quality boundaries. IMPORTANT: Simple dialogue videos with minimal movement should receive LOW scores (1-2). HIGH scores (4-5) are reserved ONLY for videos demonstrating sophisticated cinematic techniques. Score 0: Completely fails the requirement; no evidence of the evaluated quality. Score 1: Minimal/default quality; severe problems throughout. Score 2: Basic quality; many noticeable issues. Score 3: Competent/functional quality; acceptable but uninspired. Score 4: Advanced/dynamic quality; well-executed with minor issues. Score 5: Master-level quality; exceptional execution indistinguishable from professional cinema. Then, judge each dimension more concretely as follows: 1. Cinematic Camera Articulation (0.05.0): Evaluates the sophistication and intentionality of camera work, including movement, framing transitions, and visual storytelling techniques. 0: Completely static camera; single unchanging framing; feels like frozen screenshot. 22 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation 1: Predominantly static with occasional accidental shifts; simple linear zoom with no artistic purpose; AI default setting. 2: Simple panning/tilting with uniform speed; basic zoom uncorrelated with narrative; abrupt transitions. 3: Clear shot variety (Wide/Medium/Close-up); camera movements motivated by action; demonstrates basic cinematic grammar but mechanical execution. 4: Purposeful dynamic techniques (handheld shake, tracking shots, focal shifts); smooth transitions aligned with narrative peaks; camera positioning creates visual tension. 5: Exceptional sophisticated camera language (fluid handheld, crane shots, dolly moves); perfect composition; focal shifts precisely timed; every camera decision serves narrative purpose. 2. Kinetic Body Language & Blocking (0.05.0): Assesses physical performance quality, spatial relationships (blocking), and how bodies express narrative and emotion. 0: Characters completely static like mannequins; no gestures or facial movement; zero physicality. 1: Only basic lip movement; stiff/repetitive gestures; no spatial repositioning; AI-generated feel with no human quality. 2: Simple gestures lacking fluidity; mechanical walking; spatial relationships accidental; gestures dont match emotional context. 3: Characters move with basic purpose (A to B); contextually appropriate but unspecific gestures; basic blocking present; believable but not expressive. 4: Highly specific actions (running and stopping at precise point, leaning forward, catching breath); intentional spatial blocking creates tension; body language evolves through scene. 5: Every action precise and motivated; micro-movements (fidgeting, weight shifts); complex sequences; blocking choreographed to perfection reflecting power dynamics; culturally-specific gestures. 3. Visual Descriptive Fidelity (0.05.0): Measures how accurately visual output matches script descriptions, including character appearance, clothing textures, environmental details, and lighting. 0: Characters look random or change appearance; environment blank/incoherent; lighting broken. 1: Characters vaguely human but bear no resemblance to descriptions; generic clothing contradicts script; lighting ignores time-of-day cues. 2: Gender/age match but specific features wrong; clothing category correct but textures/colors incorrect; environment thematically correct but lacks details. 3: Major descriptors match (gender, age, clothing style); environment includes key elements but simplified; lighting matches time-of-day but lacks detailed effects. 4: Characters closely match detailed descriptions (hair style, clothing textures, body type); environment shows specific details (pavement texture, metal railings); sophisticated lighting with proper shadows. 5: Photorealistic precision with every descriptor present; micro-details (fabric wrinkles, button placement); environmental lighting interacts realistically; atmospheric depth; indistinguishable from high-end cinematography. 4. Emotional Arc & Micro-Expressions (0.05.0): Evaluates range and authenticity of emotional performance, including facial expressions, emotional transitions, and psychological subtext. 0: Faces blank/frozen/mask-like; no visible emotional state; characters appear lifeless. 1: Single unchanging expression; clearly looped frames; no emotional reaction to events; contradicts narrative context; robotic feel. 2: One or two basic emotions expressed simplistically/exaggerated; abrupt transitions; lacks nuance; doesnt align with dialogue tone; cartoonish. 3: Emotional states generally match dialogue; at least one clear shift; basic facial movements (eyebrow raises, mouth changes); recognizable but generic; lacks micro-expressions. 4: Multiple distinct emotional states with clear transitions (laughseriousquestioning); nuanced details (eyebrow furrows, eye contact changes); micro-expressions present; emotional intensity varies appropriately; feels acted not generated. 23 An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation 5: Rich layered emotional journey; complex arcs (playfulrealizationconcernresolve); exceptional micro-expressions (1-2 frame fleeting expressions); emotions blend naturally; characterconsistent and psychologically motivated; subtext visible; indistinguishable from professional actor performance. 5. Narrative Pacing & Timing (0.05.0): Assesses whether video executes clear narrative structure with appropriate timing, action sequencing, and rhythmic flow matching scripts story beats. 0: Video incoherent; no discernible beginning/middle/end; actions occur randomly; timing completely broken. 1: Duration wildly mismatches dialogue; actions in wrong order or omitted; no logical flow; feels like random clips stitched together. 2: Length approximately matches but internal pacing off; key actions happen at wrong times; some narrative beats present but sequencing confused; rhythm monotonous or chaotic. 3: Duration matches dialogue; basic narrative sequence present (setupeventconclusion); actions in correct order; pacing acceptable but lacks dynamic variation; competent but uninspired. 4: Dialogue and action timing precisely synchronized; (setupactionescalationresolution); pacing creates rhythm; sion appropriately; action sequences follow believable physics. clear purposeful structure timing builds/releases ten5: Perfect narrative timing with cinematic rhythm; three-act structure compressed into scene; actions timed with precision to the second; rhythmic variation creates emotional texture; timing builds and releases tension masterfully; pacing feels inevitable and organic; indistinguishable from professionally edited film. Output Format: Return ONLY JSON structure with decimal scores (0.0-5.0), detailed reasoning for each dimension (referencing which benchmarks the video falls between), Final Cinematic Grade (average of all 5 scores), and Overall Assessment. Scoring Reminders: Use decimal precision (e.g., 2.3, 3.7, 4.5) to distinguish quality levels Reference integer benchmarks but dont feel limited to them Explain in reasoning which benchmarks the video falls between and why Simple dialogue videos with minimal movement should score 1.0-2.5 Only sophisticated, cinema-quality videos should receive scores of 4.0-5."
        }
    ],
    "affiliations": [
        "Tencent Hunyuan Multimodal Department",
        "Xidian University"
    ]
}