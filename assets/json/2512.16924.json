{
    "paper_title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "authors": [
        "Hanlin Wang",
        "Hao Ouyang",
        "Qiuyu Wang",
        "Yue Yu",
        "Yihao Meng",
        "Wen Wang",
        "Ka Leong Cheng",
        "Shuailei Ma",
        "Qingyan Bai",
        "Yixuan Li",
        "Cheng Chen",
        "Yanhong Zeng",
        "Xing Zhu",
        "Yujun Shen",
        "Qifeng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/."
        },
        {
            "title": "Start",
            "content": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text Hanlin Wang1,2 Hao Ouyang2 Qiuyu Wang2 Yue Yu1,2 Yihao Meng1,2, Wen Wang3,2 Ka Leong Cheng2 Shuailei Ma4,2 Qingyan Bai1,2 Yixuan Li5,2, Cheng Chen6,2 Yanhong Zeng2 Xing Zhu2 Yujun Shen2 Qifeng Chen1 1HKUST 2Ant Group 3ZJU 4NEU 5CUHK 6 NTU 5 2 0 D 8 1 ] . [ 1 4 2 9 6 1 . 2 1 5 2 : r Figure 1. By combining text prompts, user-defined trajectories and reference images, our method effectively enables the controllable generation of promptable world events. We strongly recommend viewing our video results in our project page."
        },
        {
            "title": "Abstract",
            "content": "We present WorldCanvas, framework for promptable world events that enables rich, user-directed simuCorresponding author. lation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectorycontrolled image-to-video methods, our multimodal approach combines trajectoriesencoding motion, timing, language for semantic inand visibilitywith natural tent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/. 1. Introduction World models [3, 12, 15, 22, 38, 46] are unlocking their true potential, evolving from passive simulators into interactive canvases for creation. landmark step in this evolution is the introduction of promptable world events, concept pioneered by models like Genie 3 [3], which transforms the world model into an interactive canvas where text prompts can trigger significant environmental changes. We argue that realizing the full potential of promptable world events requires moving beyond text instructions. Text by itself is ambiguous or insufficient for specifying the complex spatiotemporal dynamics inherent in such events. To enable richer and more fine-grained control, we introduce WorldCanvas, framework designed to paint promptable world events with comprehensive control over their fundamental components: when, where, who, and what. We decompose the specification of promptable world event into three complementary modalities: Trajectories provide the when and where. They offer minimal yet powerful interface to encode motion dynamics: their positions define spatial paths, the spacing between points encodes speed, and visibility flags can signal occlusion or entry/exit. Reference images specify the who. It provides crucial visual grounding by defining the appearance and identity of the objects or agents involved in the event. Text describes the what. It supplies the high-level semantic intent, describing the interactions, goals, and causal structures that constitute the events narrative. Together, this multimodal triplet forms complete and unambiguous specification, empowering users to create controllable, promptable world events directly onto the canvas. basic solution for promptable events is to use imageto-video generation models conditioned on text or trajectory [4, 8, 21, 23, 24, 26, 29, 3335, 40, 42, 44, 45, 47, 48]. However, we identify three key shortcomings of existing local textual control. Existing methods: (i) Global vs. approaches typically rely on single, global text prompt to describe an entire video. This paradigm is insufficient for specifying distinct actions for multiple agents or complex motion dynamics, as it lacks mechanism to associate specific textual descriptions with individual trajectories. Our work addresses this by establishing direct correspondence between motion-focused text and its corresponding trajec- (ii) Lossy tory, enabling fine-grained, localized control. representation of trajectories. Current methods treat trajectories as mere sequence of spatial coordinates, discarding the wealth of spatiotemporal information they implicitly contain. Critical details like the timing of actions, speed variations encoded in point spacing, and visibility flags for occlusion or scene entry/exit are lost. (iii) Reference-based control is incomplete. Current I2V methods lack robust mechanism to intuitively support reference-based generation, making it difficult for users to directly integrate their reference images with the video to control the subject of the generated event. Consequently, these models fail to generate complex, compositional events. To bridge these gaps, we propose WorldCanvas, framework for promptable world event generation that redefines the synergy between trajectory, text and reference images. Specifically, we curate dataset of trajectoryvideotext triplets with action-centric captions tightly aligned to motion cues. We then employ data augmentation techniques like cropping to train our models capability in handling objects entry/exit and reference-based generation. Using this data, we adapt Wan 2.2 I2V [32] by injecting trajectory information and introducing spatial-aware crossattention mechanism to bind each textual phrase to its corresponding trajectory regioncritical for multi-agent scenarios. During inference, we design an interface that allows users to conveniently input trajectories and reference images, thereby fully leveraging their semantic, spatiotemporal, and appearance information. Built upon these capabilities, our method can be readily integrated into world models to effectively support user-controllable, semantically rich, and highly interactive promptable world events. Our experiments show that WorldCanvas enables users to generate complex, coherent events with finegrained control while maintaining consistencyeven when objects temporarily leave and re-enter the scene. These capabilities suggest that our approach captures structured world dynamics beyond surface-level generation. By providing practical interface for specifying and simulating user-defined events, WorldCanvas represents step toward interactive, controllable world modelsones that dont just predict the future, but let users shape it. 2. Related Work 2.1. Promptable World Events World models [1, 3, 1013, 15, 16, 22, 27, 28, 39, 46] aim to learn internal representations of environmental dynamics to support prediction and generation. However, existing approaches predominantly focus on low-level reconstruction, with user control limited to passive observation such as key-based navigation or camera pose control, rather than active changes in the environment. Crucially, these models largely overlook the generation of world events: semantically meaningful environmental changes that unfold over time. Recently, Genie3 [3] introduced the concept of promptable world events, enabling users to specify highlevel events via textual prompts and thereby simulate richer, more interactive scene dynamicsan essential step toward training agents in complex, open-world settings. Yet, textonly prompts often lack the spatial, temporal, and behavioral precision required to control complex, compositional events. To address this limitation, we extend the notion of promptable world events through multimodal prompting paradigm that integrates textual instructions with reference images and trajectories. This enables fine-grained specification of where, when, what happens, and who is involved, significantly enhancing controllability and paving the way toward truly interactive and generative world models. 2.2. Trajectory-Controlled Video Generation Trajectory-controlled I2V generation [6, 9, 19, 23, 25, 30, 3335, 4042, 47, 48] represents an instantiation of such multimodal prompting. However, existing approaches suffer from key limitations that hinder their ability to model complex world events. On the one hand, textual captions are usually global and fail to capture motion or localized semantics, while trajectories serve only as coarse positional cues, ignoring their visibility and spatiotemporal content. On the other hand, most methods offer limited customizability. Although Frame-In-Out [34] enables some customization, its architecture and lack of joint text-trajectoryimage modeling limit its capacity for event-level control. In contrast, we introduce pipeline for curating high-quality multimodal prompting data, coupled with Spatial-Aware Weighted Cross-Attention mechanism that effectively fuses reference images, trajectory sketches, and detailed textual instructions into the powerful pretrained Wan [32] model, enabling precise, compositional control over world events. 3. Method We build WorldCanvas based on Wan2.2 I2V 14B model [32]. In the following sections, we detail our data curation pipeline (Sec. 3.1), model architecture (Sec. 3.2), training objective and inference system (Sec. 3.3). The overall architecture of WorldCanvas is shown in Fig. 2. 3.1. Data Curation Pipeline Previous video annotations primarily consist of trajectory labels and global captions, which fail to meet our traIn contrast, jectoryreferencetext triplet requirements. our pipeline produces annotations that: (i) explicitly establish correspondence between individual trajectories and captions, ensuring accurate semanticspatiotemporal alignment in multi-agent scenes or complex motion dynamics; (ii) effectively extract reference images, enabling precise visual grounding of textual descriptions. Our data curation pipeline includes the following components: Keypoints tracking and filtering: We begin by collecting diverse set of videos from publicly available sources and segment them into shot-consistent clips with shot boundary detection algorithm [31]. For each clip, we detect objects in the first frame using YOLO [5] to obtain semantically meaningful bounding boxes corresponding to foreground objects. Leveraging these bounding boxes as prompts, we generate corresponding masks with SAM [18], and select 13 representative keypoints per mask via Kmeans. Trajectories of these keypoints will be the primary focus of our subsequent analysis. Additional keypoints are randomly sampled outside foreground object bounding boxes to capture background motion. Once the initial set of keypoints is established, we track them throughout the entire clip using CoTracker3 [17], yielding trajectories and visibility scores. After that, we apply random cropping to video frames, which can result in tracked foreground objects being absent in the first frame and reappearing later as they move back into view, effectively simulating objects entering the scene from outside. To prioritize clips containing semantically rich and visually dynamic motion, we further compute motion score as the average cumulative displacement of all tracked points and filter out clips below threshold, discarding near-static sequences and preserving high-quality, semantically meaningful tracking data. Trajectory-based motion captioning: To generate motion-focused captions rather than generic appearancebased global descriptions, we first visualize only the foreground object trajectories on the original video, with all trajectories from the same object rendered in consistent color to preserve identity. We then use Qwen2.5-VL 72B model [2] to caption these trajectory-visualized videos, prompting the model to identify each colored trajectorys subject and describe its motion in detail. To ensure the resulting captions remain focused on motion, we retain only minimal subject identifiers (e.g., man) and combine them with the detailed motion descriptions to form the final caption. By using color as an intermediary bridge between visual trajectories and language, this approach yields motion-rich captions while establishing clear correspondence between each subject and its associated trajectory. Reference images extraction: We treat the foreground objects detected by YOLO in the first frame of the video as motion agents and apply mild affine transformations (e.g., translation, scaling, rotation) to these objects to generate Figure 2. The architecture of our WorldCanvas. The data pipeline generates high-quality trajectoryreferencetext triplets (in the figure, gray boxes denote reference images extracted from the video, and hollow circles along trajectories indicate invisible points due to occlusion or rotation). The Spatial-Aware Weighted Cross-Attention mechanism explicitly aligns each caption with its associated trajectory. reference images. This approach removes constraints on the number and initial positions of reference objects, enabling support for an arbitrary number of reference images starting motion from any location. Moreover, using such transformed data compels the model to achieve modality alignment between the reference image and the video, allowing users to intuitively control the reference images initial position, scale, and pose through simple drag-and-drop interface to animate it. Combined with random cropping, this also enables simulation of scenarios where user-provided object (defined by reference image) enters the scene from off-screen, thereby supporting more flexible and interactive generation of promptable world events. 3.2. Video Generation with Multimodal Triplet Using our data curation pipeline, we can get our desired multimodal triplet for distinct objects in the video, represented as: = {(pi, bboxi, capi)}N i=1, (1) where denotes the number of foreground object trajectories, pi = {(xti, yti, vti)}T t=1 denotes the 2D coordinates and visibility score of the ith trajectory. bboxi is the bounding box of associated foreground object in the initial frame, and capi is the motion-focused textual description. For background points, only 2D coordinates and visibility are available. Note that for brevity, we omit explicit mention of reference images here and assume they have already been inserted into the first video frame as image condition (when reference images are provided). In this section, we present our core model design and its use of this multimodal triplet for controllable video generation. Our approach consists of two key components: Trajectory Injection for motion control via input trajectories, and Spatial-Aware Weighted Cross-Attention to align trajectories with text prompts in multi-agent scenarios. 3.2.1. Trajectory Injection We introduce additional conditional feature channels to inject trajectory information as control signal during training, enabling the model to generate motion guided by userspecified trajectories. Specifically, we represent all trajectories using Gaussian heatmap and propagate the image VAE latent feature at the first-frame keypoint location (x1i, y1i) of each trajectory to all subsequent points along that trajectoryforming what we call point VAE mapto enhance the models ability to follow and animate specific points over time. The Gaussian heatmap and point VAE map are then concatenated directly with the original DiT inputs of Wan2.2 I2V model (noise latent, conditional image latent and rearranged mask) along the channel dimension. Before being passed through the Wan DiT model, the combined features are processed by 3D convolutional layer to align their dimensionality with that of the DiT blocks, thereby seamlessly integrating the trajectory-based control signals into the generative process. Weights for the newly added Gaussian heatmap and point VAE map in the 3D convolutional layer are initialized with zero values. 3.2.2. Spatial-Aware Weighted Cross-Attention In multi-agent scenarios, especially when subjects look similar or lack distinct visual cues, matching multiple captions to their corresponding trajectories is challenging. To address this, we propose Spatial-Aware Weighted CrossAttention that explicitly aligns each caption with its associated trajectory. Instead of computing cross-attention between all video tokens and the full text prompt uniformly, our method encourages the model to focus more on visual tokens that spatially overlap with each trajectory. Specifically, for each data triplet (pi, bboxi, capi), we use the size of the bounding box bboxi as the spatial extent of trajectory i. Then, the coverage area of trajectory is the region centered at (xti, yti) with the same width and height as bboxi. Denote the visual query tokens that fall within this coverage area as Qi, the corresponding key-value pair KVi is derived from the text embedding of capi. During cross-attention, we assign higher weight bias to the attention scores between Qi and KVi. This operation can be expressed as: (cid:40) Wqk = log(w), 0, if vti = 1 and Qq Qi and Kk Ki, otherwise. (2) Here, is empirically set as 30. After computing the weight aggregation over all trajectories, we apply the resulting weight matrix to the cross-attention computation: Attention(Q, K, ) = Softmax( QK + )V. (3) Here, gives larger values to token pairs that belong to the same trajectory-caption pair. This design strengthens the correspondence between trajectories and their descriptive text, while still allowing the model to attend to other regions when needed. 3.3. Training and Inference We follow the flow matching framework [7, 20] to perform post-training using L1 reconstruction loss. Given random noise x0 (0, I), timestep [0, 1] and ground-truth video latent x1, we calculate the training input xt = tx1 + (1 t)x0, ground truth velocity vt = dxt dt = x1 x0, the training objective is formulated as: = Ex0,x1,t,C u(xt, t, C; θ) vt2(cid:105) (cid:104) , (4) where is the union of all conditions, θ is model parameters and u(xt, t, C; θ) denotes the output velocity predicted by the model. During inference, we provide an intuitive interface to specify desired inputs through the following designs: Trajectory timing: Users can define the start and end times of trajectory, controlling when an object begins and stops moving or when it enters or exits the frame. Trajectory shape via point sequences: Trajectories are defined by sequence of points, with equal time intervals between consecutive points. The spacing between points implicitly controls motion speed: sparser points for faster movement, while denser points for slower motion. Visibility control: Users can freely specify which segments of trajectory are visible or not, enabling realistic modeling of occlusions and complex semantic actions. Text-trajectory binding: Each trajectory is associated with motion caption, ensuring semantic alignment between the described action and the controlled motion. Reference image insertion: Users can place various reference images on the canvas and adjust their position and size to introduce specified subjects. These designs help users to create rich, controllable, and semantically meaningful events through intuitive trajectories, text inputs and reference images. 4. Experiments In this section, we demonstrate how WorldCanvas addresses key limitations of prior models in supporting promptable world events. Specifically, Sec. 4.1 details the implementation of our In Sec. 4.2, we compare WorldCanvas with model. state-of-the-art methods and demonstrate our superior performance over these baselines. Next, Sec. 4.3 demonstrates the advanced consistency maintenance capability of WorldCanvas. Finally, we validate the effectiveness of the Spatial-Aware Weighted Cross-Attention mechanism through an ablation study in Sec. 4.4. 4.1. Implementation Details We train our model on our curated dataset of 280k trajectoryvideotext triplets. All videos are processed at resolution of 480 832. Our model is trained for 9k steps with learning rate of 1 105 and linear warmup schedule. The entire training process is conducted on 64 NVIDIA H800 GPUs, with total batch size of 64. 4.2. Comparison We select the powerful base model Wan2.2 I2V 14B [32], ATI [33] (currently the strongest open-source I2V model), and Frame In-N-Out (an I2V model supporting reference-guided generation) as our baselines, to evaluate our models capabilities in semantic understanding (what), trajectory following (when and where), and reference-based generation (who), respectively. [34] Figure 3. Qualitative comparison on promptable world event modeling. Our model successfully generates results that align with given trajectories, text prompt and reference images, whereas the baselines fail to properly correspond to these inputs. 4.2.1. Qualitative Results Fig. 3 shows our leading results across what, when, where, and who, demonstrating its strong capability in generating promptable world events. In the tested event generation examples, both Wan and ATI fail to accurately understand the intended event. For instance, in example (a), Wan generates an elderly man chasing car, while ATI neither follows the trajectory of the man nor produces the expected interaction between the old man and the car. In example (b), neither Wan nor ATI succeeded in turning off the light. These results highlight that accurately generating complex events requires precise coordination between textual understanding and exact trajectory following. Without both, the output diverges significantly from user expectations. Our Table 1. Quantitative results. The best and runner-up are in bold and underlined. Method ObjMC WAN2.2 I2V ATI Frame In-N-Out Ours 139.59 127.21 142.70 91.06 Appearance Rate 70.65% 80.44% 64.74% 85.17% Subject Consistency 0.8947 0.8850 0.8411 0.9044 Background Consistency 0.9192 0.9225 0.8852 0. CLIP-T (Global) 0.1727 0.1617 0.1738 0.1742 CLIP-T (Local) 0.1678 0.1629 0.1656 0.1680 reference images with motion trajectories and text prompt, our approach accurately generates the desired output. We also conduct comparisons on Trajectory-Text Alignment capability, which is essential for generating coherent multi-subject events. This requires the model to correctly associate each caption with its intended agent, especially when multiple entities interact or appear dynamically. Fig. 4 evaluates this capability on two scenarios: controlling existing subjects in the scene and multiple objects entering from off-screen. In case (a), Wan partially captures subject distinctions from the text prompt (girl in front and girl behind) but still fails to fully respect the described actions. For instance, the front girl raises her hand despite the prompt specifying otherwise. ATI, limited by weaker text grounding, misinterprets both childrens motions. In the more challenging case (b), all baselines merely generate dog entering the frame, failing to capture the full semantic intent of the text prompt. In contrast, our model correctly binds each trajectory to its caption and generates the complete, intended multi-agent events. 4.2.2. Quantitative Results We evaluate our model from two complementary perspectrajectory following accuracy and semantic undertives: standing. Specifically, we collect 100 imagetrajectory pairs depicting semantic events and generate videos using our model. For each output, we apply CoTracker3 [17] to track the user-specified points and get the generated trajectories. We report the following metrics: ObjMC [37]: the mean Euclidean distance between generated and userdefined trajectories; Appearance Rate [33]: the proportion of frames in which the tracker accurately predicts point as visible whenever the input trajectory indicates it is visible; Subject & Background Consistency [14]: metrics reflect temporal consistency in video generation; CLIP-T [43]: score reflecting semantic alignment between text and generated video, validated at both global and local levels. Results in Tab. 1 show that our method consistently outperforms the baseline across three aspects: trajectory following, semantic alignment, and generated video quality, demonstrating its superiority in supporting promptable world events generation. More details on these metrics can be found in the supplementary material. Figure 4. Qualitative comparison of multi-subject trajectorytext alignment. Our method accurately aligns the textual descriptions with motions specified by trajectories, whereas the baselines fail to produce correct results in such cases. approach successfully achieves what previous text control methods and I2V models could not by correctly understanding both the text and the trajectory while ensuring their correspondence, enabling faithful and controllable generation of world events. In the reference-based generation example (c), Frame In-N-Out fails to adequately preserve the consistency of the reference images and also misinterprets the intended event. In contrast, our model maintains identity consistency with the reference images, treating them as the central subjects of the event. By seamlessly integrating the Figure 5. Consistency maintenance results. The shown examples correspond to object consistency preservation, scene consistency preservation, and character consistency preservation, respectively. 4.4. Ablation Study We conduct ablation experiments to validate the effectiveness of our Spatial-Aware Weighted Cross-Attention mechanism. Fig. 6 shows qualitative comparison on multisubject trajectorytext alignment: when using standard full cross-attention (without spatial weighting) or hard crossattention (only keep attention weights in the region of interest, and set all other positions to zero), the model fails to associate captions with their correct trajectories, leading to swapped or mismatched actions. In contrast, our spatialaware variant successfully preserves the intended correspondences. Quantitative results in the supplement further confirm this trend: our method achieves higher alignment accuracy and video quality, demonstrating that the spatial weighting is critical for correct trajectorytext binding in complex multi-agent scenarios. 5. Conclusion We introduce trajectory-text-reference-driven paradigm for generating promptable world eventsa crucial step toward controllable, semantically grounded video simulation. By enabling users to precisely specify what, when, where, and who through intuitive motion trajectories, natural language and ref images, our approach supports semantic actions, complex interactions, object entry/exit and referenceguided appearance. Critically, the model exhibits emergent object consistency and scene memory, suggesting it captures structured world dynamics beyond surface-level generation. These capabilities position our framework as practical and scalable pathway toward interactive world models that can simulate user-defined events with both realism and reliability. Acknowledgements: The authors thank Pixabay for providing some images used in this paper under the Pixabay License, which permits free use without attribution. Figure 6. Qualitative results for our ablation study. Compared to variants without Spatial-Aware Weighted Cross-Attention and Hard Cross-Attention, the former causes severe semantic-action In misalignment, while the latter yields incomplete semantics. contrast, our method effectively achieves accurate alignment between semantic content and trajectories. 4.3. Consistency Maintenance Capability We also observe that WorldCanvas exhibits strong ability to maintain scene and object consistency over time, even when subjects temporarily leave the frame and reappear later. This temporal coherence ensures that objects retain their appearance, identity, and spatial relationships across occlusions or off-screen intervals. Fig. 5 illustrates such cases, where characters or scenes exit the view and re-enter while preserving visual and semantic consistency. This capability is crucial for generating plausible world events and reflects an emergent form of visual memory in the model. It not only enhances realism but also highlights our models potential as building block for more advanced world models capable of coherent, persistent scene simulation. The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Details on Quantitative Metrics Table S1. Quantitative ablation results. The best is in bold. We evaluate our model from two complementary perspectrajectory following accuracy and semantic undertives: standing. Specifically, we construct benchmark of 100 imagetrajectory pairs that describe semantic events and generate corresponding videos using our model. For each generated video, we employ CoTracker3 [17] to trace the user-specified points and obtain the resulting motion trajectories. The evaluation includes the following metrics: ObjMC [37]: Measures trajectory following accuracy by computing the mean Euclidean distance between the generated and user-defined trajectories. Lower values indicate more precise motion control. Appearance Rate [33]: Quantifies visibility consistency by calculating the proportion of frames in which the tracker correctly predicts point as visible whenever the input trajectory marks it as visible. Subject & Background Consistency [14]: Assesses temporal coherence by evaluating feature-level stability for both the subject and the background across consecutive frames, reflecting smoothness and visual consistency. CLIP-T (Global) [43]: Measures the overall semantic consistency between the generated video and the input text prompt using ViCLIP [36]. Specifically, we compute the cosine similarity between ViCLIP features extracted from the entire video and those from the global text description, reflecting how well the generated scene aligns with the intended semantics. CLIP-T (Local) [43]: Evaluates fine-grained alignment between local text prompts and the corresponding visual regions. Following [43], we crop local video patches, and compute ViCLIP feature similarities between each cropped region and its associated local text. The final local CLIP-T score is averaged across multiple window sizes, providing more detailed measure of spatial semantic fidelity. B. More Quantitative Results In this section, we present additional quantitative experimental results, including quantitative ablation studies of the Spatial-Aware Weighted Cross-Attention mechanism and user study. B.1. Quantitative Ablation Study We conducted quantitative comparison among standard full cross-attention (without spatial weighting), hard crossMethod full attn. hard attn. Ours Subject Consistency 0.8948 0.9002 0.9044 Background Consistency 0.9285 0.9277 0.9326 CLIP-T (Global) 0.1694 0.1713 0.1742 CLIP-T (Local) 0.1595 0.1647 0.1680 attention (which retains attention weights only within the region of interest and sets all other positions to zero), and our proposed Spatial-Aware Weighted Cross-Attention on the evaluation dataset of 100 imagetrajectory pairs that we collected. As shown in Tab. S1, our method achieves the best results across all video quality metrics and semantic alignment metrics, demonstrating the effectiveness of the Spatial-Aware Weighted Cross-Attention mechanism in aligning text and trajectories in complex multi-agent interaction scenarios. B.2. Human Evaluation In the main text, we quantitatively compare our method against several baselines with metrics in Sec. A. However, these metrics have certain limitations. For instance, we observe that the model without trajectory control, Wan2.2 I2V [32], achieves better ObjMC and Appearance Rate scores than Frame In-N-Out [34], which does support trajectory control. This counterintuitive result arises because Frame In-N-Out produces outputs with unstable visual quality, leading to inaccurate tracking by CoTracker3 [17]. Additionally, the CLIP-T metric is not particularly accurate in evaluating fine-grained motion details, resulting in only marginal score differences between different models. To enable more precise comparison of the models capabilities in terms of following user-specified trajectories, generating semantic events, aligning trajectories with textual descriptions, and preserving reference image information, we conducted user study for validation. Specifically, we collected 20 challenging cases involving complex single-object motions, multi-object interactions, and object appearances or disappearances, as well as 10 reference-image-based generation cases (only used for Reference fidelity evaluation). These cases were used to generate videos using Wan 2.2 I2V 14B [32], ATI [33], Frame In-N-Out [34], and our WorldCanvas. We recruited 15 participantsincluding video generation researchers, artists, and non-expert usersto ensure comprehensive and objective evaluation. Each participant was asked to vote for the model they considered best on each of the following Table S2. Human evaluation results. We report the percentage of best votes received by each model from our user study across five criteria. The best and runner-up are in bold and underlined. Method Wan2.2 I2V ATI Frame In-N-Out WorldCanvas(Ours) Trajectory following 1.67% 19.00% 4.00% 75.33% Prompt Adherence 11.00% 9.67% 5.67% 73.67% Text-trajectory alignment 4.33% 3.33% 3.33% 89.00% Reference fidelity - - 7.33% 92.67% Overall video quality 19.67% 9.67% 1.33% 69.33% five criteria: Trajectory following: whether the generated video accurately follows the user-provided trajectories; Prompt Adherence: whether the result faithfully reflects the event described in the text prompt; Text-trajectory alignment: whether the content depicted by each trajectory correctly matches its corresponding textual description; Reference fidelity: whether reference-based generations accurately preserve the visual characteristics of the reference image; Overall video quality: whether the generated result is high-quality video, considering factors such as visual fidelity, motion smoothness, generation consistency, and aesthetics. The results of the user study are shown in Tab. S2, which clearly demonstrate that WorldCanvas achieves consistently superior performance across all evaluation metrics, confirming that our model genuinely supports the generation of semantic promptable world events. C. World-Model Related Capabilities In the main text, we highlighted WorldCanvas ability to maintain consistency in objects, scenes, and subjects, ensuring that the content associated with given trajectory remains temporally coherent throughout. This property is crucial for applying our method to world models. In this section, we explore additional world-model-related capabilities demonstrated by our model, including physical plausibility, causal reasoning, and predictive abilities for future events. We designed set of cases to evaluate the world-model related capabilities of WorldCanvas. In these examples, we only provide trajectories representing the cause, while ensuring that the input text prompt describes solely the content of the drawn trajectories and does not mention the resulting effects of such motions. By observing whether WorldCanvas can correctly generate the subsequent events that naturally follow from the given cause, we can assess its physical plausibility, causal reasoning, and future prediction abilities. The evaluation results shown in Fig. S1 demonstrate WorldCanvas physical plausibility, causal reasoning, and future prediction abilities. Specifically, Case (a) depicts domino chain reaction, where our model accurately generates the sequential falling of dominoeseach tile begins to fall only after being struck by the previous one, rather than all falling simultaneously or remaining upright. Case (b) shows burning torch approaching sheet of paper. Although we only control the motion of the torch, the model correctly generates the subsequent charring of the paper as it catches fire. Case (c) shows drink-filled bottle that is tipped over, and our model realistically renders the liquid spilling out, with the amount of liquid inside the bottle progressively decreasing. In case (d), we only specify the motion of book being pulled away; nevertheless, the generated video correctly reflects the presence of frictionthe glass cup resting on the book moves along with it, and the reflection on the table surface adheres to physical laws. These results collectively confirm that our model exhibits physical plausibility, causal reasoning, and future prediction capabilities, further underscoring WorldCanvas potential for promptable world event generation. D. Counterfactual generation We further evaluated WorldCanvas ability to generate counterfactual events. This capability provides clearer evidence of the models controllability and semantic alignment, as well as its grasp of commonsense knowledge. Fig. S2 presents two generated counterfactual events by WorldCanvas. In case (a), shark leaps out of the desert sand, dives back in, and leaps again. Although this scenario is physically impossible in reality, the model correctly simulates the shark submerging into the sand and the resulting plumes of dust upon each leap. In case (b), the dog flies upward into the sky, and the model generates the leash snapping while the owner stands on the ground in astonishment. These counterfactual results confirm that WorldCanvas indeed understands certain physical principles and causal relationships, and further validate its strong controllable generation capability and semantic fidelity. Figure S1. World-model related capabilities. In these examples, we provided only the trajectories and text describing the cause, and let the model generate the subsequent outcomes. The results demonstrate that our WorldCanvas exhibits physical plausibility, causal reasoning, and future prediction abilities. We strongly recommend viewing our video results in our project page. Figure S2. Counterfactual generation results. Our model is capable of correctly generating counterfactual events that adhere to physical laws and causal logic. We strongly recommend viewing our video results in our project page. E. Failure cases mations or logical reasoning. Fig. S3 illustrates such failure cases: Although our model demonstrates strong performance in generating promptable world events, we observe that it sometimes fails to produce correct results in particularly challenging scenarios involving complex spatial transforIn case (a), we instruct the camera to rotate 360 degrees and return to the original view. While the model largely fulfills this request, the books on the shelf exhibit noticeable blurriness and inconsistencies (highlighted in the red Figure S3. Failure cases. The red boxes highlight the blurring and distortion in case (a) and the insufficient water level rise in case (b), respectively. We strongly recommend viewing our video results in our project page. boxes). Case (b) presents more complex logical reasoning scenario: initially, as the cup is being filled with water, the water level should rise. When the camera moves upward and the cup is temporarily out of view, the filling process should continue, so the water level must continue to rise. Finally, when the camera refocuses on the cup, the water level should reflect the cumulative filling. The generated result correctly shows the water level rising while the cup is visible, but after the camera pans away and returns, the water level shows no significant increasecontradicting the expected physical behavior. These examples also highlight key challenges for future research in promptable world event generation: how to ensure consistency under drastic and complex motions, and how to enable the model to perform correct logical reasoning about content that is currently out of view."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 2 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3 [3] Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell, Aaron van den Oord, Inbar Mosseri, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 3: new frontier for world models. 2025. 2, 3 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [5] Alexey Bochkovskiy, Chien-Yao Wang, and HongYuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. 3 [6] Ruihang Chu, Yefei He, Zhekai Chen, Shiwei Zhang, Xiaogang Xu, Bin Xia, Dingdong Wang, Hongwei Yi, Xihui Liu, Hengshuang Zhao, et al. Wan-move: Motion-controllable video generation via latent trajectory guidance. In Adv. Neural Inform. Process. Syst., 2025. [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Int. Conf. Mach. Learn., 2024. 5 [8] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Yusuf Aytar, Michael Rubinstein, Chen Sun, et al. Motion prompting: Controlling video generation with motion trajectories. In IEEE Conf. Comput. Vis. Pattern Recog., 2025. 2 [9] Nate Gillman, Charles Herrmann, Michael Freeman, Daksh Aggarwal, Evan Luo, Deqing Sun, and Chen Sun. Force prompting: Video generation models can learn and genarXiv preprint eralize physics-based control signals. arXiv:2505.19386, 2025. 3 [10] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. 2 [11] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and Hongsheng Li. Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025. [12] Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, Baixin Xu, Hao-Xiang Guo, Kaixiong Gong, Cyrus Wu, Wei Li, Xuchen Song, Yang Liu, Eric Li, and Yahui Zhou. Matrix-game 2.0: An open-source, real-time, arXiv preprint and streaming interactive world model. arXiv:2508.13009, 2025. 2 [13] Siqiao Huang, Jialong Wu, Qixing Zhou, Shangchen Miao, and Mingsheng Long. Vid2world: Crafting video diffusion models to interactive world models. arXiv preprint arXiv:2505.14357, 2025. [14] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 21807 21818. IEEE, 2024. 7, 2 [15] Team HunyuanWorld. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels. arXiv preprint, 2025. 2 [16] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. 2 [17] Nikita Karaev, Yuri Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudolabelling real videos. In Int. Conf. Comput. Vis., 2025. 3, 7, 2 [18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross B. Girshick. Segment anything. In Int. Conf. Comput. Vis., 2023. 3 [19] Guojun Lei, Chi Wang, Rong Zhang, Yikai Wang, Hong Li, and Weiwei Xu. Animateanything: Consistent and controllable animation for video generation. In IEEE Conf. Comput. Vis. Pattern Recog., 2025. [20] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In Int. Conf. Learn. Represent., 2023. 5 [21] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded imageto-video generation. In Eur. Conf. Comput. Vis., 2024. 2 [22] Yifan Liu, Zhiyuan Min, Zhenwei Wang, Junta Wu, Tengfei Wang, Yixuan Yuan, Yawei Luo, and Chunchao Guo. Worldmirror: Universal 3d world reconstruction with any-prior prompting. arXiv preprint arXiv:2510.10726, 2025. 2 [23] Wan-Duo Kurt Ma, J. P. Lewis, and W. Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based video generation. arXiv preprint arXiv:2401.00896, 2023. 2, 3 [24] Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Jiangning Zhang, Hao Chen, Mingmin Chi, Yabiao Wang, and Wenhan Luo. Osv: One step is enough for high-quality image In IEEE Conf. Comput. Vis. Pattern to video generation. Recog., 2025. 2 [25] Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, and David Lindell. Sg-i2v: Self-guided trajectory control in image-to-video generation. In Int. Conf. Learn. Represent., 2025. 3 [26] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for tempoIn IEEE Conf. Comput. rally consistent video processing. Vis. Pattern Recog., 2024. 2 [27] Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, and Xun Huang. Long-context state-space video world models. In Int. Conf. Comput. Vis., 2025. 2 [28] Zhongwei Ren, Yunchao Wei, Xun Guo, Yao Zhao, Bingyi Kang, Jiashi Feng, and Xiaojie Jin. Videoworld: Exploring knowledge learning from unlabeled videos. In IEEE Conf. Comput. Vis. Pattern Recog., 2025. [29] Liao Shen, Wentao Jiang, Yiran Zhu, Tiezheng Ge, Zhiguo Identity-preserving image-to-video Cao, and Bo Zheng. generation via reward-guided optimization. arXiv preprint arXiv:2510.14255, 2025. 2 [30] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH Conference, 2024. 3 tories for locally controlled video generation. arXiv preprint arXiv:2510.15104, 2025. 7, 2 [44] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. 2 [45] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and I2vgen-xl: High-quality image-to-video Jingren Zhou. arXiv preprint synthesis via cascaded diffusion models. arXiv:2311.04145, 2023. 2 [46] Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Fei Kang, Biao Jiang, Zedong Gao, Eric Li, Yang Liu, and Yahui Zhou. Matrix-game: Interactive world foundation model. arXiv preprint arXiv:2506.18701, 2025. [47] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. In IEEE Conf. Comput. Vis. Pattern Recog., 2025. 2, 3 [48] Zhenghao Zhang, Junchao Liao, Xiangyu Meng, Long Qin, and Weizhi Wang. Tora2: Motion and appearance customized diffusion transformer for multi-entity video generation. arXiv preprint arXiv:2507.05963, 2025. 2, 3 [31] Tomas Soucek and Jakub Lokoc. Transnet V2: an effective deep network architecture for fast shot transition detection. In ACM Int. Conf. Multimedia, 2024. 3 [32] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 5 [33] Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, and Chongyang Ma. Ati: Any trajectory instruction for controllable video generation. arXiv preprint arXiv:2505.22944, 2025. 2, 3, 5, 7 [34] Boyang Wang, Xuweiyi Chen, Matheus Gadelha, and Frame in-n-out: Unbounded conarXiv preprint Zezhou Cheng. trollable image-to-video generation. arXiv:2505.21491, 2025. 3, 5, 2 [35] Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, and Limin Wang. Levitor: 3d trajectory oriented image-to-video synthesis. In IEEE Conf. Comput. Vis. Pattern Recog., 2025. 2, 3 [36] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: largescale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 2 [37] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for In ACM SIGGRAPH Conference, 2024. video generation. 7, 2 [38] Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. 2 [39] Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, and Gordon Wetzstein. Video world models with long-term spatial memory. arXiv preprint arXiv:2506.05284, 2025. 2 [40] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In Eur. Conf. Comput. Vis., 2024. 2, [41] Guy Yariv, Yuval Kirstain, Amit Zohar, Shelly Sheynin, Yaniv Taigman, Yossi Adi, Sagie Benaim, and Adam Polyak. Through-the-mask: Mask-based motion trajectories for image-to-video generation. In IEEE Conf. Comput. Vis. Pattern Recog., 2025. [42] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 2, 3 [43] Guofeng Zhang, Angtian Wang, Jacob Zhiyuan Fang, Liming Jiang, Haotian Yang, Bo Liu, Yiding Yang, Guang Chen, Longyin Wen, Alan Yuille, et al. Tgt: Text-grounded trajec-"
        }
    ],
    "affiliations": [
        "Ant Group",
        "CUHK",
        "HKUST",
        "NEU",
        "NTU",
        "ZJU"
    ]
}