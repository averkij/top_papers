{
    "paper_title": "ERNIE 5.0 Technical Report",
    "authors": [
        "Haifeng Wang",
        "Hua Wu",
        "Tian Wu",
        "Yu Sun",
        "Jing Liu",
        "Dianhai Yu",
        "Yanjun Ma",
        "Jingzhou He",
        "Zhongjun He",
        "Dou Hong",
        "Qiwen Liu",
        "Shuohuan Wang",
        "Junyuan Shang",
        "Zhenyu Zhang",
        "Yuchen Ding",
        "Jinle Zeng",
        "Jiabin Yang",
        "Liang Shen",
        "Ruibiao Chen",
        "Weichong Yin",
        "Siyu Ding",
        "Dai Dai",
        "Shikun Feng",
        "Siqi Bao",
        "Bolei He",
        "Yan Chen",
        "Zhenyu Jiao",
        "Ruiqing Zhang",
        "Zeyu Chen",
        "Qingqing Dang",
        "Kaipeng Deng",
        "Jiajun Jiang",
        "Enlei Gong",
        "Guoxia Wang",
        "Yanlin Sha",
        "Yi Liu",
        "Yehan Zheng",
        "Weijian Xu",
        "Jiaxiang Liu",
        "Zengfeng Zeng",
        "Yingqi Qu",
        "Zhongli Li",
        "Zhengkun Zhang",
        "Xiyang Wang",
        "Zixiang Xu",
        "Xinchao Xu",
        "Zhengjie Huang",
        "Dong Wang",
        "Bingjin Chen",
        "Yue Chang",
        "Xing Yuan",
        "Shiwei Huang",
        "Qiao Zhao",
        "Xinzhe Ding",
        "Shuangshuang Qiao",
        "Baoshan Yang",
        "Bihong Tang",
        "Bin Li",
        "Bingquan Wang",
        "Binhan Tang",
        "Binxiong Zheng",
        "Bo Cui",
        "Bo Ke",
        "Bo Zhang",
        "Bowen Zhang",
        "Boyan Zhang",
        "Boyang Liu",
        "Caiji Zhang",
        "Can Li",
        "Chang Xu",
        "Chao Pang",
        "Chao Zhang",
        "Chaoyi Yuan",
        "Chen Chen",
        "Cheng Cui",
        "Chenlin Yin",
        "Chun Gan",
        "Chunguang Chai",
        "Chuyu Fang",
        "Cuiyun Han",
        "Dan Zhang",
        "Danlei Feng",
        "Danxiang Zhu",
        "Dong Sun",
        "Dongbo Li",
        "Dongdong Li",
        "Dongdong Liu",
        "Dongxue Liu",
        "Fan Ding",
        "Fan Hu",
        "Fan Li",
        "Fan Mo",
        "Feisheng Wu",
        "Fengwei Liu",
        "Gangqiang Hu",
        "Gaofeng Lu",
        "Gaopeng Yong",
        "Gexiao Tian",
        "Guan Wang",
        "Guangchen Ni",
        "Guangshuo Wu",
        "Guanzhong Wang",
        "Guihua Liu",
        "Guishun Li",
        "Haibin Li",
        "Haijian Liang",
        "Haipeng Ming",
        "Haisu Wang",
        "Haiyang Lu",
        "Haiye Lin",
        "Han Zhou",
        "Hangting Lou",
        "Hanwen Du",
        "Hanzhi Zhang",
        "Hao Chen",
        "Hao Du",
        "Hao Liu",
        "Hao Zhou",
        "Haochen Jiang",
        "Haodong Tian",
        "Haoshuang Wang",
        "Haozhe Geng",
        "Heju Yin",
        "Hong Chen",
        "Hongchen Xue",
        "Hongen Liu",
        "Honggeng Zhang",
        "Hongji Xu",
        "Hongwei Chen",
        "Hongyang Zhang",
        "Hongyuan Zhang",
        "Hua Lu",
        "Huan Chen",
        "Huan Wang",
        "Huang He",
        "Hui Liu",
        "Hui Zhong",
        "Huibin Ruan",
        "Jiafeng Lu",
        "Jiage Liang",
        "Jiahao Hu",
        "Jiahao Hu",
        "Jiajie Yang",
        "Jialin Li",
        "Jian Chen",
        "Jian Wu",
        "Jianfeng Yang",
        "Jianguang Jiang",
        "Jianhua Wang",
        "Jianye Chen",
        "Jiaodi Liu",
        "Jiarui Zhou",
        "Jiawei Lv",
        "Jiaxin Zhou",
        "Jiaxuan Liu",
        "Jie Han",
        "Jie Sun",
        "Jiefan Fang",
        "Jihan Liu",
        "Jihua Liu",
        "Jing Hu",
        "Jing Qian",
        "Jing Yan",
        "Jingdong Du",
        "Jingdong Wang",
        "Jingjing Wu",
        "Jingyong Li",
        "Jinheng Wang",
        "Jinjin Li",
        "Jinliang Lu",
        "Jinlin Yu",
        "Jinnan Liu",
        "Jixiang Feng",
        "Jiyi Huang",
        "Jiyuan Zhang",
        "Jun Liang",
        "Jun Xia",
        "Jun Yu",
        "Junda Chen",
        "Junhao Feng",
        "Junhong Xiang",
        "Junliang Li",
        "Kai Liu",
        "Kailun Chen",
        "Kairan Su",
        "Kang Hu",
        "Kangkang Zhou",
        "Ke Chen",
        "Ke Wei",
        "Kui Huang",
        "Kun Wu",
        "Kunbin Chen",
        "Lei Han",
        "Lei Sun",
        "Lei Wen",
        "Linghui Meng",
        "Linhao Yu",
        "Liping Ouyang",
        "Liwen Zhang",
        "Longbin Ji",
        "Longzhi Wang",
        "Meng Sun",
        "Meng Tian",
        "Mengfei Li",
        "Mengqi Zeng",
        "Mengyu Zhang",
        "Ming Hong",
        "Mingcheng Zhou",
        "Mingming Huang",
        "Mingxin Chen",
        "Mingzhu Cai",
        "Naibin Gu",
        "Nemin Qiu",
        "Nian Wang",
        "Peng Qiu",
        "Peng Zhao",
        "Pengyu Zou",
        "Qi Wang",
        "Qi Xin",
        "Qian Wang",
        "Qiang Zhu",
        "Qianhui Luo",
        "Qianwei Yang",
        "Qianyue He",
        "Qifei Wu",
        "Qinrui Li",
        "Qiwen Bao",
        "Quan Zhang",
        "Quanxiang Liu",
        "Qunyi Xie",
        "Rongrui Zhan",
        "Rufeng Dai",
        "Rui Peng",
        "Ruian Liu",
        "Ruihao Xu",
        "Ruijie Wang",
        "Ruixi Zhang",
        "Ruixuan Liu",
        "Runsheng Shi",
        "Ruting Wang",
        "Senbo Kang",
        "Shan Lu",
        "Shaofei Yu",
        "Shaotian Gong",
        "Shenwei Hu",
        "Shifeng Zheng",
        "Shihao Guo",
        "Shilong Fan",
        "Shiqin Liu",
        "Shiwei Gu",
        "Shixi Zhang",
        "Shuai Yao",
        "Shuang Zhang",
        "Shuangqiao Liu",
        "Shuhao Liang",
        "Shuwei He",
        "Shuwen Yang",
        "Sijun He",
        "Siming Dai",
        "Siming Wu",
        "Siyi Long",
        "Songhe Deng",
        "Suhui Dong",
        "Suyin Liang",
        "Teng Hu",
        "Tianchan Xu",
        "Tianliang Lv",
        "Tianmeng Yang",
        "Tianyi Wei",
        "Tiezhu Gao",
        "Ting Sun",
        "Ting Zhang",
        "Tingdan Luo",
        "Wei He",
        "Wei Luan",
        "Wei Yin",
        "Wei Zhang",
        "Wei Zhou",
        "Weibao Gong",
        "Weibin Li",
        "Weicheng Huang",
        "Weichong Dang",
        "Weiguo Zhu",
        "Weilong Zhang",
        "Weiqi Tan",
        "Wen Huang",
        "Wenbin Chang",
        "Wenjing Du",
        "Wenlong Miao",
        "Wenpei Luo",
        "Wenquan Wu",
        "Xi Shi",
        "Xi Zhao",
        "Xiang Gao",
        "Xiangguo Zhang",
        "Xiangrui Yu",
        "Xiangsen Wang",
        "Xiangzhe Wang",
        "Xianlong Luo",
        "Xianying Ma",
        "Xiao Tan",
        "Xiaocong Lin",
        "Xiaofei Wang",
        "Xiaofeng Peng",
        "Xiaofeng Wu",
        "Xiaojian Xu",
        "Xiaolan Yuan",
        "Xiaopeng Cui",
        "Xiaotian Han",
        "Xiaoxiong Liu",
        "Xiaoxu Fei",
        "Xiaoxuan Wu",
        "Xiaoyu Wang",
        "Xiaoyu Zhang",
        "Xin Sun",
        "Xin Wang",
        "Xinhui Huang",
        "Xinming Zhu",
        "Xintong Yu",
        "Xinyi Xu",
        "Xinyu Wang",
        "Xiuxian Li",
        "XuanShi Zhu",
        "Xue Xu",
        "Xueying Lv",
        "Xuhong Li",
        "Xulong Wei",
        "Xuyi Chen",
        "Yabing Shi",
        "Yafeng Wang",
        "Yamei Li",
        "Yan Liu",
        "Yanfu Cheng",
        "Yang Gao",
        "Yang Liang",
        "Yang Wang",
        "Yang Wang",
        "Yang Yang",
        "Yanlong Liu",
        "Yannian Fu",
        "Yanpeng Wang",
        "Yanzheng Lin",
        "Yao Chen",
        "Yaozong Shen",
        "Yaqian Han",
        "Yehua Yang",
        "Yekun Chai",
        "Yesong Wang",
        "Yi Song",
        "Yichen Zhang",
        "Yifei Wang",
        "Yifeng Guo",
        "Yifeng Kou",
        "Yilong Chen",
        "Yilong Guo",
        "Yiming Wang",
        "Ying Chen",
        "Ying Wang",
        "Yingsheng Wu",
        "Yingzhan Lin",
        "Yinqi Yang",
        "Yiran Xing",
        "Yishu Lei",
        "Yixiang Tu",
        "Yiyan Chen",
        "Yong Zhang",
        "Yonghua Li",
        "Yongqiang Ma",
        "Yongxing Dai",
        "Yongyue Zhang",
        "Yu Ran",
        "Yu Sun",
        "Yu-Wen Michael Zhang",
        "Yuang Liu",
        "Yuanle Liu",
        "Yuanyuan Zhou",
        "Yubo Zhang",
        "Yuchen Han",
        "Yucheng Wang",
        "Yude Gao",
        "Yuedong Luo",
        "Yuehu Dong",
        "Yufeng Hu",
        "Yuhui Cao",
        "Yuhui Yun",
        "Yukun Chen",
        "Yukun Gao",
        "Yukun Li",
        "Yumeng Zhang",
        "Yun Fan",
        "Yun Ma",
        "Yunfei Zhang",
        "Yunshen Xie",
        "Yuping Xu",
        "Yuqin Zhang",
        "Yuqing Liu",
        "Yurui Li",
        "Yuwen Wang",
        "Yuxiang Lu",
        "Zefeng Cai",
        "Zelin Zhao",
        "Zelun Zhang",
        "Zenan Lin",
        "Zezhao Dong",
        "Zhaowu Pan",
        "Zhaoyu Liu",
        "Zhe Dong",
        "Zhe Zhang",
        "Zhen Zhang",
        "Zhengfan Wu",
        "Zhengrui Wei",
        "Zhengsheng Ning",
        "Zhenxing Li",
        "Zhenyu Li",
        "Zhenyu Qian",
        "Zhenyun Li",
        "Zhi Li",
        "Zhichao Chen",
        "Zhicheng Dong",
        "Zhida Feng",
        "Zhifan Feng",
        "Zhihao Deng",
        "Zhijin Yu",
        "Zhiyang Chen",
        "Zhonghui Zheng",
        "Zhuangzhuang Guo",
        "Zhujun Zhang",
        "Zhuo Sun",
        "Zichang Liu",
        "Zihan Lin",
        "Zihao Huang",
        "Zihe Zhu",
        "Ziheng Zhao",
        "Ziping Chen",
        "Zixuan Zhu",
        "Ziyang Xu",
        "Ziyi Liang",
        "Ziyuan Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community."
        },
        {
            "title": "Start",
            "content": "ERNIE 5.0 Technical Report ERNIE Team, Baidu ernie@baidu.com Abstract In this report, we introduce ERNIE 5.0, natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under unified next-groupof-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts novel elastic training paradigm. Within single pre-training run, the model learns family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memoryor time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community. Feb 4, 2026 6 2 0 F 4 ] . [ 1 5 0 7 4 0 . 2 0 6 2 : r 1 3 4 4 5 5 6 7 7 8 9 9 10 10 11 12 13 14 15 16 16 16 17 17 17 20 22 23 23 25"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Architecture . . . . . 2.1 Unified Autoregressive Backbone with Ultra-Sparse Mixture-of-Experts 2.2 Visual Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.1 Vision Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.2 Visual Understanding with Dual-Path Hybrid Representation . . . . . . . . . . . . 2.2.3 Visual Generation with Next-Frame-and-Scale Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.1 Audio Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.2 Audio Understanding and Generation with Next-Codec Prediction . . . . . . . . . 2.3 Audio Modeling . . . . . . . . . . . . . 3 Pre-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Pre-Training Data . 3.2 Training Recipe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Once-For-All with Elastic Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Post-Training 4.1 Enhancing Rollout Efficiency with Unbiased Replay Buffer . . . . . . . . . . . . . . . . . . 4.2 Stabilizing Training with Mitigated Entropy Collapse . . . . . . . . . . . . . . . . . . . . . 4.3 Boosting Sample Efficiency with Hint-based Learning . . . . . . . . . . . . . . . . . . . . . 5 Infrastructures 5.1 Hybrid Parallelism for Training at Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Disaggregation Architecture for Multimodal Training . . . . . . . . . . . . . . . . . . . . . 5.3 FlashMask for Flexible Multimodal Attention . . . . . . . . . . . . . . . . . . . . . . . . . . Scalable and Disaggregated RL Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 6 Evaluations 6.1 Evaluation on Language Benchmarks 6.2 Evaluation on Vision Benchmarks 6.3 Evaluation on Audio Benchmarks . 6.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.1 Modality-Agnostic Expert Routing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.2 Elastic Training . . . . . . . . . . . . . . . 7 Conclusion 8 Contributors 2 ERNIE 5.0 Technical Report Feb 4,"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language and vision-language models, including ERNIE (ERNIE Team, 2025), Gemini (DeepMind, 2025a;b), GPT (OpenAI, 2024; 2025), Claude (Anthropic, 2025), DeepSeek (Liu et al., 2024; 2025a), and Qwen (Yang et al., 2025; Bai et al., 2025), demonstrate that large-scale autoregressive sequence modeling provides powerful foundation for language and multimodal understanding. By modeling diverse inputs as sequences of tokens, these models exhibit strong reasoning and alignment capabilities across modalities. However, in most existing systems, autoregressive modeling serves multimodal understanding while the output is still text-centric, which restricts the models ability to engage in various multimodal interactions. To overcome this limitation, recent approaches augment pre-trained language models with modality-specific decoders or generators, which are connected to the language backbone through late-fusion designs (Xu et al., 2025a; Seedream et al., 2025). Although effective for individual modalities, these designs decouple multimodal generation from understanding and rely on modality-specific, non-autoregressive objectives, which hinder deep cross-modal integration and often force trade-off between multimodal integration and core language performance. Against this backdrop, designing unified autoregressive paradigm remains prominent open challenge. Such framework must natively support both multimodal understanding and generation, preserve strong unimodal capabilities, and scale effectively as model and data sizes continue to grow. In this report, we introduce ERNIE 5.0, next-generation foundation model natively designed to integrate text, image, audio, and video capabilities under unified autoregressive framework for both multimodal understanding and generation. Rather than augmenting pre-trained language model with modalityspecific components, ERNIE 5.0 trains all modalities simultaneously from scratch, which alleviates the ability seesaw problem observed in later-fusion approaches and ensures that all modalities evolve collectively without sacrificing performance. Specifically, heterogeneous inputs are mapped into shared token space, and modeling across all modalities is formulated under unified Next-Group-of-Tokens Prediction objective, which avoids explicit modality boundaries and inconsistent optimization trajectories. To support scalability, ERNIE 5.0 leverages an ultra-sparse Mixture-of-Experts (MoE) backbone with modality-agnostic expert routing. Routing decisions are conditioned on unified token representations rather than modality identifiers, allowing tokens from various modalities to be dispatched to shared pool of experts. This ultra-sparse, modality-agnostic architecture eliminates the need for heuristic modalityspecific expert allocation, offering sufficient capacity for both differentiation and collaboration among modality-specialized behaviors. During pre-training of ERNIE 5.0, we propose novel elastic training paradigm that enables single pre-training run to produce family of models with varying capacityefficiency trade-offs. Instead of optimizing static architecture, our elastic training approach dynamically samples sub-models with varying depth, width, and routing sparsity for each training instance, guided by pre-defined schedule. Both the sampled sub-models and the full-model are optimized in one backpropagation process under the same autoregressive objective. The sub-model sampling strategy improves the functional integrity of parameters and maintains competitive performance even when only subset of parameters is available. Elasticity in depth and width facilitates the production of models with smaller sizes, whereas the sparsity elasticity reduces the number of activated experts during inference, leading to higher throughput and improved computational efficiency. Meanwhile, elastic training enables sub-models to inherit knowledge from the full model and provides flexible instantiation of smaller models in subsequent post-training stages, thereby eliminating the need to pre-train multiple models of various sizes or rely on customized compression, and making ERNIE 5.0 well suited for deployment under diverse hardware, memory, and latency constraints. Following unified pre-training, we conduct multi-stage post-training pipeline that combines supervised fine-tuning (SFT) with unified multimodal reinforcement learning (UMRL). The coexistence of heterogeneous multimodal inputs and ultra-sparse MoE architecture introduces substantial optimization challenges, which increases the sensitivity of UMRL to sampling bias, sparse reward signals, and entropy collapse. To cope with these issues, we build unified verifier system and develop suite of scalable techniques to improve the stability and efficiency of RL training. An unbiased replay buffer is employed to improve rollout efficiency while preserving balanced data distribution. Multi-granularity importance sampling, together with positive sample masking, stabilizes policy optimization and effectively mitigates entropy collapse. For difficult tasks with sparse rewards, adaptive hint-based RL is introduced to provide auxiliary guidance when needed. By ensuring stable and efficient post-training, these designs support the excellent multimodal reasoning ability in ERNIE 5.0. For infrastructure, we utilize hybrid parallelism with fine-grained memory control to support effective training of trillion-parameter ultra-sparse MoE model. For unified multimodal training, tokenizers are decoupled from the MoE backbone and deployed on separate GPU nodes, so that each component can adopt its most suitable parallelization strategy. To accommodate local bidirectional attention in vision, we ERNIE 5.0 Technical Report Feb 4, 2026 Figure 1: ERNIE 5.0 architecture. It is trained from scratch under unified autoregressive paradigm that integrates multimodal understanding and generation. Text, vision, and audio are encoded and serialized, then processed by unified backbone. An ultra-sparse MoE architecture with modality-agnostic routing is employed, in which tokens from different modalities are dispatched to shared expert pool. employ FlashMask (Wang et al., 2024a) to efficiently handle per-sample heterogeneous attention masks. Finally, we design scalable and disaggregated RL infrastructure that coordinates training, rollout, and environment interaction, to ensure high-throughput and numerically consistent post-training. We evaluate ERNIE 5.0 on diverse set of text and multimodal benchmarks spanning perception, reasoning, understanding, and generation. Across these tasks, ERNIE 5.0 consistently matches or outperforms specialized baselines, indicating that unified training retains strong modality-specific performance without architectural fragmentation. Ablation results further highlight the effectiveness of modality-agnostic expert routing and elastic training. Despite employing single shared routing mechanism across modalities, experts exhibit clear specialization patterns that are primarily shaped by task requirements rather than modality boundaries. Reducing routing top-k to 25% during inference yields over 15% decoding speedup with only minor accuracy loss, while elastic training across depth, width, and sparsity preserves near-full performance using only 53.7% activated parameters and 35.8% total parameters, suggesting scalable and efficient foundation for next-generation unified multimodal models. In the following sections, we systematically present the design of the model architecture and its core technical components, followed by detailed description of the training and optimization pipeline. We then evaluate ERNIE 5.0 on comprehensive set of benchmarks to validate the effectiveness of the proposed unified framework. Finally, we share some key technical insights during the model training process, hoping to be helpful for future research on scalable and general-purpose foundation models."
        },
        {
            "title": "2 Architecture",
            "content": "As shown in Figure 1, ERNIE 5.0 adopts an ultra-sparse mixture-of-experts architecture that integrates language, image, video, and audio within single autoregressive framework for both multimodal understanding and generation. The model consists of shared backbone for unified sequence modeling, together with visual and audio tokenizers that convert multimodal inputs into unified token sequence. All modalities are trained under shared Next-Group-of-Tokens Prediction objective, enabling deep crossmodal interactions with end-to-end optimization. In this section, we first describe the autoregressive backbone in Sec. 2.1, which serves as the core of ERNIE 5.0, followed by the visual and audio processing pipelines in Secs. 2.2 and 2.3. 2.1 Unified Autoregressive Backbone with Ultra-Sparse Mixture-of-Experts Heterogeneous modalities differ substantially in token semantics, temporal structures, and optimization dynamics, making naive cross-modal parameter sharing prone to unstable optimization and performance degradation, especially when modeling understanding and generation in single model. To address these challenges, ERNIE 5.0 is designed to be trained from scratch under unified autoregressive framework. Multimodal inputs, including text, image, video, and audio, are projected into shared token space, serialized into unified sequence, and optimized under the Next-Group-of-Tokens Prediction objective. Specifically, text generation adheres to the standard Next-Token Prediction (NTP) paradigm, 4 ERNIE 5.0 Technical Report Feb 4, 2026 augmented by the Multi-Token Prediction (MTP) mechanism (Gloeckle et al., 2024; Liu et al., 2024) to enhance both output quality and inference efficiency. For vision and audio modalities, generation is formulated as group-of-tokens prediction task, so as to align their generative processes with the text autoregressive objective. Vision generation employs Next-Frame-and-Scale Prediction (NFSP) (Ji et al., 2026), and audio generation utilizes Next-Codec Prediction (NCP) to capture temporal and spectral structure. By unifying heterogeneous modalities under single optimization objective, all tokens are trained within consistent sequence prediction paradigm, enabling training from scratch and deep token-level multimodal interactions throughout the unified backbone. Although unifying the learning paradigm helps reduce modality discrepancies, it cannot fully eliminate the intrinsic differences across modalities, necessitating substantial model capacity to capture diverse multimodal knowledge. To this end, ERNIE 5.0 adopts sparse Mixture-of-Experts (MoE) architecture to scale model capacity efficiently while controlling both training and inference costs. At the core of this architecture is modality-agnostic expert routing, where routing decisions are conditioned on unified token representations rather than explicit modality identifiers. The router dispatches tokens from different modalities to shared pool of experts, enabling effective cross-modal parameter sharing. In contrast to modality-isolated routing strategies in our previous models (ERNIE Team, 2025), such unified routing mechanism promotes cross-modal knowledge generalization and improves single-modality performance through the emergent specialization of shared experts. Moreover, it obviates the requirement of heuristic modality-specific expert allocation, which is often non-trivial in practice, especially when more than two modalities are involved. By employing an ultra-sparse and fine-grained MoE architecture, ERNIE 5.0 achieves an activation rate below 3%, allowing the model to substantially expand its effective capacity without incurring proportional increase in computational overhead. The training is further stabalized by an auxiliary-loss-free load balancing (Wang et al., 2024c) , ensuring robust expert utilization at trillion-parameter scale. Based on the unified optimization objective and the shared parameter space, ERNIE 5.0 formally integrates multimodal understanding and generation within single autoregressive backbone. Despite such formal unification, some challenges remain in learning representations that can convincingly support both tasks. Typically, multimodal understanding focuses on abstract and semantic-level concepts, whereas generation requires accurate modeling of fine-grained perceptual details. ERNIE 5.0 is therefore designed to learn unified representation that captures high-level semantics while preserving fine-grained details, enabling both comprehension and synthesis in unified manner. In the unified framework, semanticlevel signals guide generative modeling toward global consistency, while generative training, in turn, strengthens fine-grained perception and detail-sensitive reasoning. This mutual reinforcement allows single backbone to robustly support perception, reasoning, and creative generation. Based on this design philosophy, we further introduce unified visual and audio inputoutput interfaces and their corresponding processing pipelines in the following sections, which constitute distinctive feature of ERNIE 5.0 compared to previous models. 2.2 Visual Modeling In ERNIE 5.0, image is treated as special case of video (e.g., single-frame video), sharing the unified design philosophy of visual understanding and generation. Visual understanding is built upon hybird representation that encodes both global semantic information and local perceptual details, allowing high-level reasoning while preserving fine-grained visual sensitivity. For image and video generation, visual autoregressive paradigm is proposed to support coherent modeling across both spatial and temporal dimensions in discrete token space, as shown in Figure 2. In the following, we first introduce vision tokenization, followed by our tailored designs for visual understanding and generation. 2.2.1 Vision Tokenization To support autoregressive visual modeling across both spatial and temporal dimensions, ERNIE 5.0 propose Next-Frame-and-Scale Prediction (NFSP), where image generation is formulated as Next-scale Prediction problem, and video generation further extends this formulation with Next-Frame Prediction (Ji et al., 2026). To this end, we first train causal 2D multi-scale tokenizer for images, which provides strong spatial representations through large-scale image pre-training. Building upon this image tokenizer, we inflate it into causal 3D convolutional tokenizer, thereby unifying image and video tokenization within single model. The progressive design preserves the spatial modeling capabilities learned from images while introducing temporal perception for videos, leading to faithful reconstruction of high-level visual elements such as scene text and human faces. During tokenizer training, we incorporate auxiliary supervision signals to enhance representation quality and training stability. Specifically, we utilize the adversarial loss (Karras et al., 2019) from GAN-based discriminators to improve distributional fidelity. Meanwhile, we incorporate semantic branch and apply 5 ERNIE 5.0 Technical Report Feb 4, Figure 2: Overview of the unified vision understanding and generation architecture. For understanding, visual features are extracted by hybrid CNNViT representation and then compressed via an AttentionBased Patch Merger. For generation, we introduce the Next-Frame-and-Scale Prediction (NFSP) paradigm, where image generation is formulated as Next-Scale Prediction, and video generation further extends this process with Next-Frame prediction along the temporal dimension. semantic regularization loss derived from large-scale vision foundation models to preserve high-level semantic consistency. These complementary objectives improve the learnability and stability of visual tokens, as well as facilitate effective autoregressive modeling in the unified backbone. Following the bit-wise quantization strategy, we quantize the unified visual latent representation into group of bit-codes, where the number of bits directly corresponding to the size of discrete vocabulary (Han et al., 2025). Based on this mechanism, we pre-train series of tokenizers with progressively increasing bit numbers. During the training of ERNIE 5.0, we adopt progressive tokenizer switching strategy, starting with low-bit tokenizer (i.e., small vocabulary) and gradually transitioning to higher-bit variants (i.e., larger vocabularies). By first learning coarse-grained, low-bit representations with small vocabularies and progressively introducing finer-grained, higher-bit tokenizers, the backbone follows smoother and stable optimization trajectory, effectively alleviating early-stage training instability and leading to improved visual generation quality. 2.2.2 Visual Understanding with Dual-Path Hybrid Representation While the above design mainly targets visual tokenization for unified autoregressive modeling and generation, we observe that, prior to quantization, visual features are typically compressed by downsampling module into low-dimensional representations, whose dimensionality is aligned with the tokenizer bitwidth. The compression inevitably leads to the loss of fine-grained semantic information, which has also been widely observed to limit the performance of visual understanding tasks (Ma et al., 2025a). To address this issue, ERNIE 5.0 directly leverages the dual-path visual features prior to quantization. We integrate perceptual features extracted by Convolutional Neural Networks (CNNs) with semantic features encoded by Vision Transformer (ViT). However, the representations produced by these two paths are misaligned in spatial structure, our empirical study shows that roughly fusing CNN and ViT features through MLP-based adapters often fails to fully exploit their complementary strengths and introduces representational interference, resulting in degraded understanding performance. This observation motivates the following Attention-based Patch Merger. Formally, given spatial token in an image or spatio-temporal token in video, we extract two sets of features, Fcnn RNKDcnn and Fvit RNKDvit , where is the number of visual understanding tokens and is the number of local patches grouped for each token, Dcnn and Dvit are the feature dimensions. In image understanding tasks, we group = 4 spatially adjacent patches, while in video understanding tasks, we group = 16 patches spanning 4 neighboring frames. Before feature fusion, we project CNN features to match the ViT feature space, and then concatenate the aligned CNN and ViT patch features along the patch dimension to obtain Fmrg RN2KDvit . Next, multi-head self-attention is applied to the concatenated patch tokens, = Attn(Fmrg), where the attention mechanism jointly models correlations between CNN and ViT features, as well as spatial and temporal dependencies among the group of patches. The output preserves the same shape, RN2KDvit . Finally, we perform mean pooling over 6 ERNIE 5.0 Technical Report Feb 4, 2026 the patch dimension to obtain compact representation Fout RNDvit , which is then projected to align with the embedding dimension of the unified backbone. The choice of feature fusion has significant impact on model performance, and the naive MLP-based fusion is proven inadequate to effectively integrate CNN and ViT features. In contrast, the proposed attention-based aggregation module consistently outperforms both CNN-only and ViT-only baselines on wide range of benchmarks without introducing noticeable computational overhead, with particularly pronounced gains in document and chart understanding as well as general visual understanding tasks. Importantly, ERNIE 5.0 is designed as unified framework for visual understanding and generation, where visual representations must support not only discriminative tasks but also fine-grained generative tasks such as pixel-level image and video editing. By utilizing attention, the module adaptively aggregates local patches together with high-level semantic information, capturing critical visual features while reducing the number of visual tokens. It results in compact yet expressive visual representation that provides strong and stable foundation for various vision-language tasks. 2.2.3 Visual Generation with Next-Frame-and-Scale Prediction Recall that the Next-Frame-and-Scale Prediction (NFSP) paradigm introduced in vision tokenization formulates visual generation in an autoregressive manner, where image generation is viewed as special case of single-frame video generation. Under this formulation, the model predicts visual tokens across multiple spatial scales within each image (or each frame) for image generation, while performing framewise prediction along the temporal dimension for video generation. When predicting tokens at certain scale, the model takes the previous generated scales as input, and scale-wise causal attention mask is applied, where tokens within the current scale are bidirectionally visible and are predicted in parallel, while tokens from all previous scales and historical frames are visible in causal (uni-directional) manner. The NFSP paradigm disentangles spatial and temporal modeling, that is, intra-frame prediction from low-resolution to high-resolution captures fine-grained spatial structures, whereas next-frame prediction models inter-frame temporal dependencies. To support positional modeling of heterogeneous tokens across spatial and temporal dimensions, we introduce Unified Spatiotemporal Rotary Positional Embedding (Uni-RoPE) and apply it to all tokens in ERNIE 5.0. For unified sequence of length N, the positional encoding of the i-th token is defined as Uni-RoPEi = (ti, hi, wi), {1, . . . , N}. For text and audio tokens, we set ti = hi = wi, where the shared value follows token index in the sequence. For visual tokens, ti is used for frame indexing, which increases monotonically to preserve temporal ordering, and (hi, wi) corresponds to spatial locations within each frame. To ensure spatial consistency across multi-scales, we adopt center-aligned coordinate strategy, where tokens at different scales are aligned based on the geometric centers of the scale. Empirically, auto-regressive visual generation is susceptible to error accumulation over extremely long token sequences. To mitigate this issue, we corrupt historical tokens during training by randomly flipping their bits, while supervising the model to self-correct toward the ground-truth tokens of the current scale. The corruption-based training strategy improves robustness against compounding errors in long-horizon generation. Meanwhile, we apply loss reweighting strategy to emphasize early-stage predictions and alleviate the token imbalance introduced by multi-scale tokenization. For video generation in particular, we further introduce windowed temporal attention and random historical frame masking to encourage the model to focus on relevant temporal context and improve robustness (Ji et al., 2026). Within the token-based modeling paradigm, training visual generation abilities under fixed modality token budget poses fundamental challenge for high-resolution images and videos. Increasing visual resolution enlarges the token sequence length, which in turn reduces the effective training batch size and degrades optimization stability. To address this challenge, we adopt cascaded diffusion refiner on top of the autoregressive backbone. The backbone generates low-resolution samples with precise semantics and structural layout, while the refiner focuses on enhancing fine-grained visual details at higher resolution. The diffusion refiner is trained separately from the backbone, using paired low-resolution samples with controlled degradation, together with their corresponding high-resolution images or videos. The decoupled training scheme enables high-fidelity refinement while preserving the complete semantic and structure produced by the autoregressive model, and avoids optimization conflicts caused by introducing autoregressive and diffusion losses within shared backbone. 2.3 Audio Modeling Similar to the vision modality, audio modeling in ERNIE 5.0 is also formulated under unified autoregressive, token-based framework that supports understanding and high-fidelity generation. Inspired by the success of neural audio codecs (Kumar et al., 2023; Zhang et al., 2024b), audio signals are represented as hierarchical discrete codec tokens that capture higher-level semantics and fine-grained acoustic details. To 7 ERNIE 5.0 Technical Report Feb 4, 2026 Figure 3: Overview of the depth-wise architecture for audio understanding and generation. For understanding, embeddings from multiple residual levels are additively combined to form audio token representation. For generation, ERNIE 5.0 introduces Next-Codec Prediction (NCP) to achieve hierarchical prediction across transformer layers, where the ground-truth code embedding (or the predicted one during inference) is fed back to condition subsequent predictions. avoid the prohibitive sequence length caused by flattening multi-codebook tokens into single sequence, we introduce depth-wise autoregressive architecture that performs structured prediction across codec dimensions, as illustrated in Figure 3. 2.3.1 Audio Tokenization Given an input waveform, we first map the continuous audio signal into sequence of discrete tokens using codec-style tokenizer with token rate of 12.5 Hz. The audio quantization module follows Residual Vector Quantization (RVQ) design, decomposing the signal into multiple tokens at different levels of granularity. In that case, the first token is explicitly assigned to encode high-level audio semantics, while the remaining tokens encode residual acoustic information with progressively finer details. To ensure that the first token captures rich semantic information, including linguistic and phonetic cues, required for audiotext modeling, we distill knowledge from pretrained Whisper model (Radford et al., 2023). Specifically, we align the representation of the first audio token with the encoder outputs of Whisper. Average pooling is applied to the Whisper representations to match the token rate of our audio tokenizer, which resolves the temporal mismatch between the teacher and student models. Complementary to the first semantic token, residual acoustic tokens preserve fine-grained characteristics of the audio signal, such as timbre and prosody. Together, the hierarchical tokenization process disentangles semantic content from acoustic realization and provides compact audio representation that is integrated naturally into ERNIE 5.0 unified autoregressive backbone. 2.3.2 Audio Understanding and Generation with Next-Codec Prediction Based on the audio tokenization described above, we utilize depth-wise autoregression architecture to model audio tokens for both understanding and generation, drawing inspiration from coarse-to-fine prediction paradigms developed in visual generation (Chen et al., 2024a). Instead of flattening all residual audio tokens into single long sequence, ERNIE 5.0 distributes the prediction of residual codes across transformer layers. Each layer models audio information at specific level of granularity, which allows multi-level audio representations to be efficiently handled within unified framework. For audio understanding tasks, text tokens are embedded using standard text embedding layers, while audio tokens are represented through depth-wise additive embedding mechanism. Each audio token consists of multiple discrete codes that correspond to different residual levels. At each level, the code is mapped to an embedding through level-specific embedding matrices, and embeddings from all levels are summed to form the final codec representation. The additive aggregation reflects the residual nature of audio representation, where each depth contributes complementary information at different granularities from coarse to fine. The resulting audio token representations are placed at the corresponding positions in the input sequence and processed uniformly with text tokens by the autoregressive backbone, enabling seamless multimodal understanding. ERNIE 5.0 Technical Report Feb 4, 2026 For audio generation tasks, ERNIE 5.0 introduces Next-Codec Prediction (NCP) to generate hierarchical audio tokens in coarse-to-fine manner. Multiple audio heads are inserted into the top transformer layers to support depth-wise prediction. Conditioned on the multimodal context, the model first predicts the first semantic code and then sequentially generates codes for subsequent residual levels. After each prediction, the generated code is mapped to its corresponding embedding and added back to the hidden state, which then conditions the prediction at the next level. During training, teacher forcing is applied, and the feedback embedding is derived from the ground-truth code. Such iterative process continues until all levels are predicted, allowing high-level semantic information to guide the synthesis of increasingly fine-grained acoustic details. Once the complete set of hierarchical audio codes is obtained, the audio decoder converts them into waveforms. For speech synthesis, speaker embedding is inserted as part of the conditioning context to enable controllable voice timbre, guiding acoustic realization without altering deep semantic content or depth-wise prediction structure. Overall, the NCP formulation is compatible with the residual design used during audio understanding, which ensures structural alignment between audio input and output."
        },
        {
            "title": "3 Pre-Training",
            "content": "The pre-training phase forms the foundation of ERNIE 5.0, where the model learns generalizable representations across multiple modalities. In this section, we first describe the composition of pre-training data (Sec. 3.1), followed by part of training recipe (Sec. 3.2). Finally, we focus on the Elastic Training technique (Sec. 3.3) introduced in ERNIE 5.0, which enables the production of multiple models of different sizes within single pre-training run, significantly reducing the computational costs associated with training series of models while maintaining both efficiency and performance. 3.1 Pre-Training Data ERNIE 5.0 is trained on an large, high-quality multimodal dataset that reflects its natively omni design and dual capabilities for both understanding and generation. Unlike conventional late-fusion approaches, ERNIE 5.0 is simultaneously exposed to text, images, videos, and audios from the very beginning of training. The unified training paradigm enables the model to learn representations that integrate semantic information across all modalities, while also requiring large amount of pre-training data to support learning from scratch for each modality. To manage such diverse data, we build standardized platform and organize all data according to their input and output modalities. Based on this organization, the pre-training data are broadly categorized into two groups: text data and multimodal data. Text Data The textual component spans vast collection of multilingual web crawls, curated corpora, books, scientific publications, code repositories, and structured knowledge sources selected for breadth, diversity, and linguistic richness. We retrain the text tokenizer to better support large-scale multilingual modeling. Specifically, we encode text in UTF-16BE to provide stable byte-level fallback and more compact representation for many non-Latin symbols, improving data throughput in multilingual training, and we use BPE dropout (Provilkov et al., 2020) to reduce overfitting to frequent patterns. It is worth noting that, for languages without explicit whitespace word boundaries (e.g., Chinese), we filter out long unspaced phrases that can be decomposed by standard word-segmentation tools, which helps reduce vocabulary sparsity, improve training efficiency, and enhance model generalization. Multimodal Data For visual and audio modalities, we curate dataset comprising paired imagetext, videotext, audio-text, as well as diverse interleaved multimodal sequences where text is integrated with images, videos, and audio, all accompanied by metadata and captions. Such data composition connects textual concepts with visual and audio contexts across both spatial and temporal dimensions. By explicitly modeling cross-modal alignment, the model is able to learn semantic relationships not only within individual modalities and across disparate ones, which supports wide range of tasks from multimodal understanding to creative multimodal generation. Rigorous preprocessing and quality controls are applied at scale to maintain both signal integrity and diversity. Heuristic and model-based filters remove low-quality and unsafe content, extensive deduplication prevents memorization artifacts, and decontamination safeguards keep benchmarks out of the training data. The finalized pre-training corpus comprises trillions of text tokens and multimodal instances that balance scale with high-fidelity semantic content. The large, diverse and well-filtered dataset is fundamental to ERNIE 5.0s strong performance on text and multimodal understanding, reasoning, and generation benchmarks. 9 ERNIE 5.0 Technical Report Feb 4, 2026 Figure 4: Overview of the elastic training framework in ERNIE 5.0. The framework supports elastic depth, width, and sparsity in unified MoE architecture. Elastic Depth randomly adapts the number of active layers, Elastic Width varies the total number of experts in each MoE layer, and Elastic Sparsity changes the top-k routing per token. These mechanisms collectively enable flexible deployment under different compute, memory, and latency constraints without retraining. 3.2 Training Recipe ERNIE 5.0 is trained with carefully designed recipe to ensure training stability, scalability, and efficient utilization of compute resources. The training follows multi-stage pre-training strategy to progressively extend context length while maintaining stable optimization dynamics. Stage 1: 8K Pre-Training The initial stage uses maximum context length of 8K tokens. We adopt Warmup-Stable-Decay (WSD) learning rate schedule (Hu et al., 2024) in this stage. The learning rate is linearly warmed up for 2,000 steps from zero to peak value of 1 104, and then kept constant for the remainder of the 8K training stage. To improve training efficiency and stability at scale, we employ batch size scheduling strategy, where the global batch size is gradually increased from 14M tokens to 56M tokens during early training. To facilitate seamless long-context extension, the RoPE base is set to 1,000,000 starting from the 8K stage. This design choice avoids the need for reparameterization or interpolation during subsequent context length expansion, ensuring lossless and stable long-context training. Stage 2: 32K&128K Mid-Training In the mid-training stage, we progressively extend the context length to 32K and 128K tokens while keeping the global batch size unchanged. During this phase, we switch to cosine learning rate schedule and anneal the learning rate from 1 104 to 1 105. For MoE-specific optimization, the bias update speed for auxiliary-loss-free load balancing (Wang et al., 2024c) is set to 1 104 in the 8K pre-training stage and reduced to 1 105 during mid-training, which effectively suppresses iteration-level oscillations observed in large-scale MoE training. The MTP loss weight (Liu et al., 2024) is decreased from 0.3 in the 8K stage to 0.1 during mid-training, ensuring stable adaptation as the model scales to longer contexts. Besides, we introduce posterior-based loss weighting strategy that rescales the autoregressive losses of different modalities to the same interval, thereby improving training stability and preventing imbalance across modalities. 3.3 Once-For-All with Elastic Training The scaling of modern models presents fundamental challenge. Although models with trillions of parameters achieve remarkable performance across wide range of tasks, their high computational and deployment costs limit the applicability in scenarios that demand flexibility and efficiency. Traditional approaches typically follow train-then-compress pipeline, employing techniques such as pruning (Sajjad et al., 2023; Xia et al., 2023; Men et al., 2025), knowledge distillation (Gu et al., 2023; Xu et al., 2024) or more efficient fusion-based variants (Wang et al., 2023; Chen et al., 2024d;e) to produce smaller models. However, this paradigm still suffers from notable limitations. Model compression requires dedicated pruning or distillation stage, which demands specialized infrastructure and incurs substantial computational overhead. Moreover, once model is compressed, its architecture becomes fixed. Consequently, creating models of other sizes necessitates repeating the full compression process, thereby constraining deployment flexibility. 10 ERNIE 5.0 Technical Report Feb 4, 2026 To address these issues, we propose novel elastic training strategy and apply it to ERNIE 5.0 for the first time. Rather than compressing pre-trained model post hoc, elastic training simultaneously optimizes family of sub-networks during pre-training, so that single large model to efficiently produce smaller, deployable variants on demand. It extends the design philosophy of Once-For-All (Devvrit et al., 2023; Cai et al., 2024; Gu et al., 2025) to pre-training, in which sub-network configurations of varying depth, width, and sparsity are trained together with the full-scale model. As result, ERNIE 5.0 can flexibly selects subsets of parameters to construct models at different scales, which reduces the computational and engineering overhead compared with traditional pruning or distillation methods. The elastic training is shown in Figure 4, which introduces structural flexibility along three orthogonal dimensions: Elastic Depth To support elastic depth, ERNIE 5.0 randomly varies the number of active transformer layers during training, enabling the extraction of sub-networks with different depths. Most of the time, the full-depth network is used to ensure all layers are well-optimized, while shallower sub-networks are occasionally sampled to foster resilience against layer removal. Specifically, the full model is trained with probability of 75%, while reduced-depth sub-network is activated with probability of 25%. Through this training scheme, intermediate representations are encouraged to remain informative even when some layers are bypassed, and the model supports flexible deployment across different depth configurations without requiring separate training. Elastic Width Complementary to elastic depth, ERNIE 5.0 also supports elastic width by varying the total number of experts in each Mixture-of-Experts (MoE) layer. Instead of always activating the full expert pool, the training process alternates between two modes. With probability of 80%, all experts participate in routing, preserving the full-width configuration. In the remaining 20% of cases, routing is restricted to randomly sampled subset of experts, leading to narrower effective model width. By exposing the model to both full and reduced expert configurations, the resulting model supports different capacity budgets with only partial experts, making it suitable for deployment in memory-constrained environments where hosting all experts is impractical. Elastic Sparsity To improve inference efficiency without changing the deployed model size, elastic sparsity is introduced by varying the number of activated experts per token. Similar to the elasticity of total number of experts, the default routing configuration is applied with probability of 80% during training. With probability of 20%, the routing top-k is randomly sampled from predefined range, where is smaller than the standard configuration. In other words, the number of activated experts for each token is decreased. Finally, the model is compatible with different compute budgets and exhibits improved robustness during latency-constrained inference. By training an elastic super-network, ERNIE 5.0 is able to produce smaller models of varying configurations by selecting subsets of parameters along the layer number, total expert number, and activated expert number. Elasticity along the representation dimension (i.e., hidden size), as explored in recent work (Chen et al., 2024c), is orthogonal to our design and can be naturally incorporated as future extension. These sub-networks can be instantiated on demand to meet different latency and memory constraints, serving as effective starting points for mid-training or fine-tuning. Compared with training separate models from scratch or relying on post-hoc compression techniques, our elastic training strategy substantially lowers overall computational overhead and engineering complexity. Detailed experimental results and ablation studies are provided in Sec 6.4.2."
        },
        {
            "title": "4 Post-Training",
            "content": "After unified pre-training, we follow the same post-training pipeline as ERNIE 4.5 (ERNIE Team, 2025) to obtain the final ERNIE 5.0, which includes two stages, supervised fine-tuning (SFT) and unified multimodal reinforcement learning (UM-RL). With curating comprehensive set of high-quality instruction pairs, SFT endows the model with fundamental instruction-following capability and strengthens its ability to think through long chains-of-thought. During UM-RL phase, we merge the training of various tasks such as reasoning, agent, and instruction following into multi-stage RL pipeline, enabling balanced performance across diverse tasks and modalities. We further extended the unified verifier system to generate accurate and consistent reward signals for model responses in wide range of multimodal scenarios, providing reliable supervision for unified multimodal RL training. In this section, we will discuss the key challenges of RL training and describe the solutions we propose. The RL training of ERNIE 5.0 faces several challenges. Firstly, RL training is computationally expensive, which is further amplified by the large scale of ERNIE 5.0. Secondly, the ultra-sparse MoE architecture exacerbates the traininginference discrepancy and undermines stability. Finally, compared to standalone 11 ERNIE 5.0 Technical Report Feb 4, 2026 Figure 5: Visualization of the Unbiased Replay Buffer (U-RB) in ERNIE 5.0, in comparison with existing methods, where each query is assigned unique index, and IB denotes the inference batch. In Sync RL, long-tail query (e.g., index 11) blocks the entire batch, leaving GPUs idle and poorly utilized. APRIL stops generation once the target number of responses (16) is reached, which leads to non-stationary data difficulty distribution. U-RB extends APRIL with data-ordering constraint that prepares future batches while waiting for long-tail queries, preserving query order and mitigating inefficiency. RLVR tasks such as mathematical reasoning or code generation, training model that simultaneously supports multiple scenarios and modalities introduces substantially higher complexity. To address these bottlenecks, we implement suite of synergistic engineering and algorithmic optimizations that enable stable RL training for large-scale, ultra-sparse MoE models. In the sections that follow, we delineate the formidable challenges encountered in this endeavor and present our corresponding solutions. 4.1 Enhancing Rollout Efficiency with Unbiased Replay Buffer Rollout generation accounts for more than 90% of the total training time in RL, and efficiency is often limited by the long-tail distribution of rollout response lengths. In that case, small number of unusually long responses stall entire batches, leaving GPUs idle and underutilized. Recent work, such as APRIL (Zhou et al., 2025), seeks to mitigate long-tail inefficiency by over-provisioning rollout requests. Generation is terminated once target number of responses is collected, and incomplete responses are recycled for continuation in subsequent steps. However, APRIL tends to update model parameters using trajectories with shorter reasoning steps, which usually correspond to easier queries. In contrast, longerhorizon samples are deferred, leading to non-stationary distribution of data difficulty. Consequently, periodic shifts in data difficulty may hinder convergence and ultimately degrade model performance. U-RB: Unbiased Replay Buffer Generation We introduce U-RB, an unbiased extension of APRIL that accelerates rollout generation in RL. As illustrated in Figure 5, U-RB introduces data-ordering constraint, under which only the data group assigned to the current iteration at initialization is allowed to participate in subsequent training process. Specifically, U-RB builds two modules. The first is high-throughput inference pool, Pin er, with capacity  BS is the training batch size and is the buffer size, The second component is training pool Ptrain with capacity  BS, which collects completed trajectories for RL training. At iteration t, the inference engine in er;t populates the inference pool by generating rollouts in parallel. Inference proceeds until the terminal state (i.e., [EOS]) is reached for the longest rollout belonging to the data group Dt assigned to iteration t. At this point, rollouts associated with Dt are moved from Pin er to the training pool Ptrain, enabling the training engine train;t to update model parameters. These rollouts may include trajectories resumed from earlier inference runs. By dynamically partitioning rollout generation, U-RB prevents computational idleness caused by individual long rollouts, while maintaining an unbiased data distribution. BS N, where  RBS =  12 ERNIE 5.0 Technical Report Feb 4, 2026 Figure 6: Training dynamics of applying JMixed IcePop (light-blue) to conduct RL training on ERNIE 5.0. By using Multi-granularity Importance Sampling Clipping (MISC), we avoid the entropy collapse during early stage and achieve stable RL training. IcePop (dark-blue) and JGSPO 4.2 Stabilizing Training with Mitigated Entropy Collapse The phenomenon of rapid entropy collapse in multimodal model is manifested as sharp increase or decrease in policy entropy during the early stages of RL. In multimodal decision-making tasks that integrate text, vision and audio information, such collapse gradually erodes the models ability to fuse information across modalities for flexible reasoning and reveals pronounced modality bias. Recent studies (Cui et al., 2025; Wang et al., 2025) attribute entropy collapse mainly to two factors. First, most contemporary RL frameworks rely on separate engines for training and inference, which introduces inconsistencies in numerical computation, and ultimately destabilizes policy optimization. The problem becomes more severe for MoE models, where dynamic routing further amplifies the numerical mismatch problem. Second, the policy model often overfits easy queries in the early stage of training. Such behavior accelerates entropy collapse and limits the models ability to discover alternative reasoning paths. To address these issues, we introduce Multi-granularity Importance Sampling Clipping (MISC) and Well-learned Positive Sample Mask (WPSM) to stabilize RL training at scale. MISC: Multi-granularity Importance Sampling Clipping IcePop (Ling-Team et al., 2025) suppresses training-inference mismatch through double-sided masking calibration on GRPO (Guo et al., 2025): IcePop() = JGRPO (cid:34) xD,{yi}G i= 1 i=1 1 yi in er(x;old) yi j=1 [M( train(yi,jx, yi,<j; old) in er(yi,jx, yi,<j; old) ri,j = train(yi,jx, yi,<j; ) train(yi,jx, yi,<j; old) M(k) = (cid:26)k 0 [, ], otherwise ; , ) min(ri,j Ai,j, clip(ri,j, 1 , 1 + ) Ai,j (cid:35) (1) where ,  controls the lower and upper limits. We apply this technology to GSPO (Zheng et al., 2025): IcePop() = JGSPO xD,{yi}G 1 i=1 [M( in er(x;old) i=1 (cid:32) train(yix;old) in er(yix;old) (cid:33) 1 yi ; , ) min(si() Ai, clip(si(), 1 , 1 + ) Ai si() = M(k) = (cid:18) train(yix; ) train(yix; old) (cid:26)k k [, ], otherwise (cid:19) 1 yi = exp (cid:32) 1 yi yi j= log train(yi,jx, yi,<j; ) train(yi,jx, yi,<j); old) (cid:33) (2) However, our experiments show that directly applying JGSPO IcePop to RL training on ERNIE 5.0 leads to rapid entropy collapse, as illustrated by the light-blue line in Figure 6. The phenomenon is caused by sequence-level truncated importance sampling, which prunes large number of low-entropy responses due to the traininginference mismatch. ERNIE 5.0 Technical Report Feb 4, 2026 Figure 7: Overview of the Adaptive Hint-based Reinforcement Learning (AHRL) in ERNIE 5.0, which introduces think skeletons to guide hard queries and mitigate sparse rewards. To address this issue, we revise JGSPO IcePop to JMixed IcePop: IcePop() = JMixed (cid:34) xD,{yi}G i=1 in er(x;old) 1 i=1 [Mj[1,yi]( si() = M(k) = (cid:18) train(yix; ) train(yix; old) (cid:26)k 0 [, ], otherwise train(yi,jx, yi,<j; old) in er(yi,jx, yi,<j; old) (cid:32) (cid:19) 1 yi = exp ; , ) min(si() Ai, clip(si(), 1 , 1 + ) Ai 1 yi yi j=1 log train(yi,jx, yi,<j; ) train(yi,jx, yi,<j); old) (cid:33) (cid:35) (3) By modulating the trust region according to the modality sensitivity, we achieve more balanced explorationexploitation trade-off. The mechanism avoids premature convergence to safe yet suboptimal strategy in complex multi-scenario settings, and preserves flexibility across various inputs. WPSM: Well-learned Positive Sample Mask We introduce sample mask strategy to prevent the model from over-optimizing on already mastered queries, in which the proficiency is tracked by maintaining success-rate for each query. For given query with rollout group = {yx G}, where is the group size, if the average accuracy accx in as well-learned response when its policy entropy Hyx () falls below stability bound . During training, the well-learned responses are masked as follows: in iteration exceeds threshold , we flag the rollout yx 2, ..., yx 1, yx J() = xD,{yi}G i=1 (cid:40) (cid:34) 1 i=1 old (x) [1 Mi mask]min(si()) Ai, clip(si(), 1 , 1 + ) Ai (cid:35) (4) Mi mask = () <  and accx >   Hyx 0 otherwise where si() denotes the importance ratio and  [0, 1] controls the degree of supplementary learning applied to well-learned responses. Under this design, the gradient budget is shifted toward harder samples, such as those with sparse rewards or diverse reasoning paths. By masking redundant positive signals, WPSM alleviates the entropy collapse problem caused by over-fitting to easy queries, and encourages the model to improve the performances of challenging, low-performing tasks. 4.3 Boosting Sample Efficiency with Hint-based Learning Recent studies (Yue et al., 2025b; Liu et al., 2025b; Zhao et al., 2025b) indicate that although state-of-the-art RL methods, such as GRPO (Guo et al., 2025) and DAPO (Yu et al., 2025), enhance the pass@1 metric by 14 ERNIE 5.0 Technical Report Feb 4, 2026 reinforcing high-reward completions, they exhibit clear limitations on challenging tasks where the base model performs poorly. Specifically, when all rollouts receive zero reward, the GRPO framework fails to provide effective gradient signals for policy optimization. In such cases, RL training on hard queries tends to progress much more slowly because sparse rewards and limited sample efficiency impede learning. To address this challenge, we propose Adaptive Hint-based Reinforcement Learning (AHRL), method that mitigates the issues of sparse rewords on hard queries. As can be seen in Figure 7, AHRL introduces partial hints that decompose the complex problem into intermediate steps and gradually increase the performance of trained models. The mechanism is described in detail below. AHRL: Adaptive Hint-based Reinforcement Learning Unlike approaches that modify reward functions or optimization algorithms, AHRL is designed to inject partial think sketches into queries during RL training. By decomposing problems into intermediate steps, AHRL increases the propensity of the base model to generate correct responses and improves sample efficiency. As result, the model is driven to master the hardest problems, which accelerates the RL training process. For given query with response consisting of thinking trajectory and final solution, denoted as = (think, solution), AHRL augments into x(p) by attaching the first phint tokens of the think to the original query. phint denotes the fraction of revealed thinking, allowing fine-grained control over query difficulty. Specifically, The probability phint follows an annealing schedule: phint(xt) = pinitial exp( passx initial) (5) where is the training iteration,  is the decay rate, and passx initial is the pass@k score of query evaluated on the SFT model. As training progresses and model performance improves, the fraction of revealed hints is gradually reduced, transitioning the model to full self-exploration. The mechanism provides necessary scaffolding to bridge the gap between initial exploration and successful task completion. It prevents training from stalling in complex tasks where valid reasoning paths are statistically rare, and ensures consistent performance improvements across modalities."
        },
        {
            "title": "Infrastructures",
            "content": "The training of ERNIE 5.0 is built upon PaddlePaddle (Ma et al., 2019). Based on the infrastructure of ERNIE 4.5 (ERNIE Team, 2025), we further address the unique challenges introduced by native multimodal training, ultra-sparse MoE models, and large-scale RL pipelines. Tremendous Memory Pressure and Communication Overhead The ultra-sparse MoE architecture of ERNIE 5.0 poses two major challenges for efficient training: ultra-sparse expert activation leads to heavy inter-node communication, and large-scale expert parameters impose significant memory pressure. To mitigate these challenges, we propose hybrid parallel strategy tailored for the MoE architecture, integrated with fine-grained memory control. Multimodal Training with Multiple Tokenizers As unified model supporting text, vision, and audio, ERNIE 5.0 relies on multiple modality-specific tokenizer models. Their computational characteristics differ significantly from those of the MoE backbone, making it challenging to apply traditional end-to-end optimization strategy across all parts. To resolve this mismatch, we adopt decoupled architecture that physically separates tokenizers from the MoE backbone and deploys them on different GPU nodes, allowing each component to use its most suitable parallelization strategy. Flexible Attention Patterns across Modalities ERNIE 5.0 processes inputs from multiple modalities with heterogeneous attention patterns. Visual inputs typically require bidirectional attention, whereas text and audio inputs rely on unidirectional attention. Although existing solutions such as FlexAttention (Dong et al., 2024) support flexible attention, they are less efficient when attention mask patterns vary across samples within the same batch. We therefore adopt the self-developed FlashMask (Wang et al., 2024a) to accelerate attention mask computation here. Scalable RL under Throughput and Consistent Constraints Reinforcement learning for trillionparameter model requires coordinated execution across training, inference, and environment interaction. Such heterogeneous workloads pose challenges in maintaining numerical consistency between training and rollout, mitigating data distribution bias in asynchronous pipelines, and maximizing the utilization of diverse hardware resources. We therefore design scalable and disaggregated RL infrastructure to resolve these bottlenecks while preserving strict computational determinism. ERNIE 5.0 Technical Report Feb 4, 2026 5.1 Hybrid Parallelism for Training at Scale To address the severe memory pressure and communication overhead introduced by the ultra-sparse MoE architecture, we develop distributed parallel strategy that adapts to varying training resources. The final configuration combines 4-way tensor parallelism (Shoeybi et al., 2019), 12-way pipeline parallelism (Huang et al., 2019) with virtual stages, 64-way expert parallelism (Lepikhin et al., 2020), ZeRO-1 data parallelism (Rajbhandari et al., 2020), and context parallelism (Liu et al., 2023a) for long context training. We also use DeepEP (Zhao et al., 2025a) to enable efficient inter-node communication, while virtual pipeline parallelism is employed to minimize pipeline bubbles. To ensure final performance, ERNIE 5.0 utilize no-token-dropping strategy throughout training, and out-of-memory (OOM) issues sometimes occur due to unbalanced expert routing, especially in the initial stage of training. To mitigate these issues and stabilize large-scale MoE training, we develop set of techniques as follows. To ensure memory sufficiency, we implement the following strategies: FP8 Mixed-Precision Training. Following the practice of ERNIE 4.5 (ERNIE Team, 2025), we adopt FP8 mixed-precision training and store activation tensors in FP8 format, which effectively reduces peak memory consumption during training. Dynamic Adaptive Offloading of Activation Memory. During forward propagation, all activation tensors retained for backward computation are tracked. We extend the memory allocator to enable adaptive offloading of selected activation tensors when an OOM event is encountered. No offloading is triggered when sufficient memory is available. This technology ensures sufficient total memory for training with minimal performance overhead. To reduce memory fragmentation, we implement the following strategies: Sub-batch Computations. We decompose large memory allocation requests into series of smaller requests with sub-batch computation, which reduces the probability of OOM caused by memory fragmentation. Automatic Memory Defragmentation. Based on CUDA virtual memory management (VMM), we develop memory allocation allocator capable of automatic memory defragmentation, ensuring successful memory allocation even under extreme conditions. Through the aforementioned techniques, we ensure the feasibility and reliability of pre-training ERNIE 5.0 in memory-constrained scenarios. 5.2 Disaggregation Architecture for Multimodal Training ERNIE 5.0 integrates heterogeneous multimodal inputs, where each modality is handled by dedicated tokenizer that converts raw signals into token sequences. Both sequence lengths and computational costs vary substantially across modalities and even among samples within the same modality. Deploying all tokenizers together with backbone on homogeneous hardware would induce significant load imbalance, which in turn degrades overall training efficiency. To solve this problem, we design tokenizer-backbone disaggregation architecture. We decouple the tokenizers from the backbone by deploying them as independent, horizontally scalable services on dedicated compute nodes, under data-parallel configuration. During training, the backbone interacts with these tokenizer services via remote calls to retrieve encoded representations. The architectural separation allows each component to adopt parallelization strategies suited to its own workload, improving scalability and efficiency in distributed multimodal training. 5.3 FlashMask for Flexible Multimodal Attention In ERNIE 5.0, attention patterns vary across modalities and may even differ across samples. Text modeling typically relies on causal attention, while visual features often employ bidirectional attention. That is, for visual samples, attention follows globally causal structure while allowing local bidirectional interactions to capture spatial dependencies. Efficiently supporting such heterogeneous masking patterns is therefore essential for unified multimodal training. To meet these requirements, we employ FlashMask (Wang et al., 2024a), which not only meets the flexible and diverse attention masking needs of ERNIE 5.0, but also significantly accelerates the computational efficiency of attention mask operations. In practice, FlashMask achieves up to 200% speedup over FlexAttention (Dong et al., 2024) at the operator-level, and delivers more than 20% end-to-end training acceleration. In addition, we integrate FlashMask with context parallelism (Liu et al., 2023a) at the kernel level, achieving an 80% performance improvement compared to the Megatron-LM solution. ERNIE 5.0 Technical Report Feb 4, 2026 5.4 Scalable and Disaggregated RL Infrastructure Scaling Reinforcement Learning (RL) to unified multimodal models with trillions of parameters presents unique challenges regarding computational consistency, data distribution bias, and heterogeneous resource utilization. To address these bottlenecks, we introduce the ERNIE 5.0 RL Infrastructure, disaggregated system designed to orchestrate large-scale asynchronous training. By prioritizing highthroughput execution and computation determinism, our system ensures stable and efficient RL training. The key architectural components are summarized as follows: Disaggregated Control Plane for Asynchronous RL. We introduce fully disaggregated control plane built around centralized RL controller to maximize system throughput, which coordinates training, inference, environment interaction, and reward evaluation in an asynchronous manner. Logical decoupling across these subsystems enables flexible scaling and efficient pipeline management, forming the foundation for large-scale asynchronous multimodal RL. Unified FP8 Stack for Consistent Training and Inference. Precision divergence is common issue in low-bit RL training. To alleviate this situation, we build unified FP8 execution engine. By employing identical high-performance operators across both training and inference (rollout) stages and integrating the Rollout Router Replay (Ma et al., 2025b) strategy, the engine minimizes numerical mismatch and ensures stable convergence under low-precision settings. Replay Buffer for Sequence-Length Bias Mitigation. Asynchronous rollout in RL may introduce sequence-length bias, where shorter responses enter training earlier and distort the data distribution. We design an unbiased replay buffer, in collaboration with the algorithm described in Section 4.1, that preserves the original data order, ensuring consistent data arrival and mitigating bias caused by asynchronous completion. Heterogeneous Resource Optimization with Elastic CPU Pooling. To address the under utilization of CPU resources commonly observed in GPU-dominated AI clusters, we implement an elastic CPU pooling strategy. The elastic mechanism isolates and virtualizes idle CPU capacity from the cluster to power logic-intensive tasks such as intensive RL environment interactions and result verification. It effectively amplifies the computational resources available for environment rollouts, enabling massive-scale parallel simulation. Consequently, it reduces the wall-clock time of training iterations while significantly improving the total cost of ownership (TCO) efficiency of the underlying hardware."
        },
        {
            "title": "6 Evaluations",
            "content": "We conduct systematic evaluations of ERNIE 5.0 against state-of-the-art models across wide range of text (Sec. 6.1), vision (Sec. 6.2), and audio benchmarks (Sec. 6.3) with internal evaluation framework, ERNIE-Eval1. Next, we further provide an in-depth analysis of our two key design choices, namely modality-agnostic expert routing (Sec. 6.4.1) and elastic training (Sec. 6.4.2). 6.1 Evaluation on Language Benchmarks To comprehensively assess the text-centric capabilities learned during large-scale pre-training and posttraining, we evaluate ERNIE 5.0 on diverse set of benchmarks covering factual knowledge, reasoning, mathematical problem solving, coding, multilingual understanding, instruction following and agentoriented tasks. These benchmarks are selected to encompass both core language modeling abilities and advanced reasoning and decision-making skills. This enables systematic examination of how the unified architecture and training strategies translate into downstream text performance. Benchmarks Knowledge: PreciseWikiQA (Bang et al., 2025), PopQA (Mallen et al., 2023), HotPotQA (Yang et al., 2018), ChineseSimpleQA (He et al., 2025), SimpleQA (Wei et al., 2024). General: MMLU-Pro (Wang et al., 2024d), MMCU (Zeng, 2023), AGIEval (Zhong et al., 2024), MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2025), C-Eval (Huang et al., 2023), CMMLU (Li et al., 2024), BBH (Suzgun et al., 2023), WinoGrande (Sakaguchi et al., 2021), Humanitys Last Exam (HLE) (Phan et al., 2025). 1For code-related tasks, evaluations are conducted with LiveCodeBench (Jain et al., 2024) and SandboxFusion (Cheng et al., 2024) to ensure reliable and execution-based assessment. 17 ERNIE 5.0 Technical Report Feb 4, Category Benchmark DS V3.2-Exp-Base Kimi K2-Base ERNIE 5.0-Base Knowledge General STEM Coding PreciseWikiQA10-shot PopQA10-shot HotPotQA10-shot ChineseSimpleQA10-shot MMLU-Pro5-shot MMCU5-shot AGIEval5-shot MMLU5-shot MMLU-Redux5-shot C-Eval5-shot CMMLU5-shot BBH5-shot, w.o.-cot (Direct) WinoGrande5-shot MATH (CoT)8-shot GPQA-Diamond5-shot LiveCodeBench v61-shot HumanEval+0-shot MBPP+0-shot CRUXEval-I1-shot CRUXEval-O1-shot Multilingual MMMLU5-shot INCLUDE5-shot 52.60 48.73 53.11 74.19 68.27 91.16 78.20 88.60 90.50 90.70 88.43 73.50 88.87 65.70 53.01 24.90 53.99 78.77 59.54 71. 70.99 77.45 61.66 51.74 57.07 78.29 67.19 90.63 76.43 88.40 89.88 92.49 90.61 70.07 87.61 65.90 48.10 26.30 70.73 79.36 73.64 81.61 61.50 72. 74.48 65.24 67.08 90.09 75.58 93.92 80.22 90.58 92.19 91.98 89.69 75.69 92.66 73.89 57.30 31.94 80.86 79.10 79.75 84.01 78.94 77.81 Table 1: Comparison of pre-trained models on various language benchmarks. The best performance in each row is highlighted in bold. STEM: MATH (CoT) (Hendrycks et al., 2021), GPQA-Diamond (Rein et al., 2024), AIME 2025 (AIME, 2025), HMMT 2025 (HMMT Organization, 2025). Coding: LiveCodeBench v6 (2408 to 2505) (Jain et al., 2024), HumanEval+ (Chen, 2021), MBPP+ (Austin et al., 2021), CRUXEval (Gu et al., 2024). Multilingual: MMMLU (Hendrycks et al., 2020), INCLUDE (Romanou et al., 2024). Reasoning: ZebraLogic (Lin et al., 2025), BBEH (Kazemi et al., 2025). Instruct Following: IFEval (Zhou et al., 2023), Multichallenge (Deshpande et al., 2025), MultiIF (He et al., 2024). Agent: TAU2-Bench (Barres et al., 2025), ACEBench (Chen et al., 2025), BFCL v4 (Patil et al., 2025), BrowseComp (Wei et al., 2025), SpreadSheetBench (Ma et al., 2024). Evaluation of Pre-trained Models. Table 1 summarizes the pre-training results of ERNIE 5.0 in comparison with strong open-source baselines on diverse set of text benchmarks. Across these benchmarks, ERNIE 5.0 exhibits consistently strong and well-balanced performance in knowledge, reasoning, mathematics, coding, and multilingual tasks: On knowledge-intensive benchmarks, ERNIE 5.0-Base demonstrates clear and substantial advantages over DeepSeek V3.2-Exp-Base (DS V3.2-Exp-Base) and Kimi K2-Base, particularly on both English and Chinese question answering tasks. The large margins observed on these datasets indicate that large-scale unified pre-training effectively consolidates factual knowledge and supports robust retrieval-style reasoning in multilingual settings. On general reasoning and exam-style benchmarks, ERNIE 5.0-Base achieves the best results on wide range of challenging evaluations, including MMLU-Pro, MMLU, MMCU, AGIEval, MMLU-Redux, and BBH. Notably, the pronounced gains on harder benchmarks such as MMLU-Pro suggest improved reasoning depth and robustness, which can be attributed to the shared backbone and modality-agnostic MoE routing that encourage effective expert specialization. On STEM tasks, ERNIE 5.0-Base consistently outperforms strong baselines on MATH (CoT) and GPQADiamond, demonstrating robust multi-step reasoning and solution consistency. These improvements reflect enhanced long-horizon dependency modeling enabled by the deep unified architecture, together with stable optimization under elastic pre-training. In the coding domain, ERNIE 5.0-Base attains state-of-the-art performance on LiveCodeBench v6 and CRUXEval, while remaining competitive on MBPP+, indicating strong generalization of algorithmic and 18 ERNIE 5.0 Technical Report Feb 4, 2026 Category Benchmark DS V3.2-Thinking Gemini 2.5-Pro GPT-5 (High) Gemini 3-Pro ERNIE 5.0 Knowledge SimpleQA ChineseSimpleQA General MMLU-Pro HLE STEM Coding GPQA-Diamond AIME 2025 HMMT 2025 LiveCodeBench v6 HumanEval+ MBPP+ Reasoning ZebraLogic BBEH Instruction Following IFEval MultiChallenge Multi-IF Agent TAU2-Bench ACEBench-en ACEBench-zh BFCL v4 BrowseComp-zh SpreadSheetBench 28.02 72.37 85.00 25.10 82.40 93.10 86. 81.06 90.80 81.48 97.60 67.04 91.87 42.43 71.17 80.30 81.40 83.40 61.18 65.00 35.29 54.00 76.50 86.20 21. 86.40 88.00 81.20 72.90 94.50 73.80 92.90 68.80 89.50 51.50 76.10 56.20 80.90 87.50 52.30 28.70 27.70 51.30 75. 87.10 24.80 85.70 94.60 93.30 81.70 92.70 83.10 98.80 69.00 94.10 58.30 70.00 80.10 79.30 83.60 61.60 61.90 34. 69.33 84.08 86.88 37.50 91.90 95.00 93.33 86.34 95.12 86.21 95.50 78.80 92.24 62.50 81. 85.40 80.90 85.00 68.14 63.67 55.36 74.01 86.03 83.80 25.81 86.36 89.06 79.58 76.21 94.48 82.54 96.50 66. 93.35 65.98 85.56 78.79 87.70 89.60 66.47 64.71 40.08 Table 2: Evaluation of post-trained models across wide range of language benchmarks. procedural reasoning learned during large-scale pre-training. On multilingual benchmarks, ERNIE 5.0-Base significantly surpasses baselines on MMMLU and INCLUDE, validating the effectiveness of unified tokenization and shared expert routing in learning robust multilingual representations. The results demonstrate that the proposed unified architecture and elastic pre-training strategy jointly yield strong generalization across diverse text-centric tasks, providing solid and versatile foundation for subsequent post-training and deployment. Evaluation of Post-trained Models. As shown in Table 2, the post-trained ERNIE 5.0 achieves competitive or leading performance across broad set of text-centric benchmarks, matching strong open-source and proprietary models on knowledge, instruction-following, coding, and agent-oriented tasks, while maintaining strong general reasoning ability, despite being natively unified omni model: On knowledge-intensive benchmarks, such as SimpleQA and ChineseSimpleQA, ERNIE 5.0 further improves upon the already strong pre-training results. This indicates that post-training effectively enhances factual recall and answer calibration, while preserving the underlying knowledge representations learned during large-scale pre-training. On general reasoning, mathematical, and coding benchmarks, ERNIE 5.0 demonstrates stable and competitive performance against strong post-trained baselines. Although Gemini 3-Pro achieves leading results on several particularly challenging benchmarks, including GPQA-Diamond, AIME 2025, HMMT 2025, and LiveCodeBench, ERNIE 5.0 consistently matches or outperforms DeepSeek-V3.2-Thinking (DS-V3.2Thinking), Gemini 2.5-Pro, and GPT-5 (High) on most of the benchmarks. This behavior aligns well with the pre-training observations, suggesting that ERNIE 5.0 emphasizes robust and balanced capability development rather than aggressive optimization toward extreme reasoning or competition-style tasks. On instruction-following benchmarks, ERNIE 5.0 shows clear advantages on multi-instruction evaluations, achieving the best performance on MultiChallenge and Multi-IF, as well as near-top results on IFEval. These results suggest that post-training effectively strengthens instruction compliance and compositional instruction understanding, complementing the strong pre-trained foundation. On agent-oriented benchmarks, ERNIE 5.0 demonstrates competitive and often leading performance, particularly on ACEBench (in both English and Chinese) and BrowseComp-zh. While Gemini 3-Pro excels on certain tool-intensive tasks, ERNIE 5.0 exhibits strong generalization across diverse agent scenarios, indicating practical usability in complex interactive environments. ERNIE 5.0 Technical Report Feb 4, 2026 Category Benchmark Qwen3-VL Thinking Gemini 2.5-Pro GPT-5 (High) Gemini 3-Pro ERNIE 5.0 STEM & Reasoning Document Understanding General VQA Video Understanding MMMU-Pro MathVista MathVerse MathVision VisualPuzzle VisuLogic VLMAreBlind ChartQA AI2D DocVQAval OCRBench ChartXiv-RQ ChartXiv-DQ SimpleVQA HallusionBench MMStar BLINK CV-Bench CountBench VideoMME(w sub) Video-MMMU MMVU 68.28 86.80 83.96 71.84 57.01 31.93 75.13 84.60 96.96 95.44 863 63.00 92.92 62.37 64.01 76.88 66.60 87.57 92.67 80.97 80.00 71.10 68.80 82.70 86.27 73.30 61.51 32.80 76. 84.08 97.09 91.43 866 67.80 93.38 68.19 63.70 77.50 70.60 84.87 91.00 86.90 83.60 76.10 78.40 82.10 84.19 78.06 57.75 29.80 69.60 78.24 95.63 94.16 804 81.10 91.17 55.84 66.58 82.10 70.39 84.99 88. 87.36 84.60 87.34 81.00 89.20 91.62 87.27 71.48 37.60 80.83 89.44 97.70 90.70 909 81.40 95.95 74.06 73.48 82.96 77.49 90.07 97.35 88.40 87.60 76.30 68.63 84.80 85.13 74.34 64.82 32.00 91. 87.80 96.89 95.45 878 67.10 89.05 67.64 63.87 75.54 70.02 87.19 96.54 81.35 81.11 72.24 Table 3: Evaluation of post-trained models across wide range of vision benchmarks. The comparison between pre-training and post-training results highlights consistent capability trajectory of ERNIE 5.0. Strengths in factual knowledge and general robustness established during pre-training are almost retained after post-training, while instruction following and agent capabilities are significantly enhanced. Although ERNIE 5.0 remains competitive across general reasoning, math, and coding tasks, moderate gap persists on the most challenging reasoning benchmarks compared to models such as Gemini 3-Pro. Our future wok will further leverage architectural design and advanced training strategies to better support complex, long-horizon reasoning. 6.2 Evaluation on Vision Benchmarks To evaluate the visual understanding and generation capabilities enabled by native multimodal training, we assess ERNIE 5.0 on various image-centric and video-centric benchmarks. These evaluations cover visual reasoning, document understanding, visual question answering, video understanding, as well as image and video generation, providing holistic assessment of how the unified multi-modal architecture manifests as performances across perception, reasoning, and generation tasks. Visual Understanding & Generation Benchmarks STEM and Reasoning: MMMU-Pro (Yue et al., 2025a), MathVista (Lu et al., 2023), MathVerse (Zhang et al., 2024a), MathVision (Wang et al., 2024b), VisualPuzzle (Song et al., 2025), VisuLogic (Xu et al., 2025b), VLMAreBlind (Rahmanzadehgervi et al., 2024), MMMU (Yue et al., 2024). Document Understanding: ChartQA (Masry et al., 2022), AI2D (Kembhavi et al., 2016), DocVQA(val) (Mathew et al., 2021), OCRBench (Liu et al., 2023b), ChartXiv-RQ, ChartXiv-DQ (Wang et al., 2024e). General VQA: SimpleVQA (Cheng et al., 2025), HallusionBench (Guan et al., 2024), MMStar (Chen et al., 2024b), BLINK (Fu et al., 2024), CV-Bench (Zhu et al., 2025), CountBench (Paiss et al., 2023). Video Understanding: VideoMME (Fu et al., 2025), Video-MMMU (Hu et al., 2025), MMVU (Zhao et al., 2025c). Image and Video Generation: GenEval (Ghosh et al., 2023), VBench (Huang et al., 2024). 20 ERNIE 5.0 Technical Report Feb 4, 2026 Model MathVista MathVerse MathVision VisualPuzzle VisuLogic ChartQA 84.40 81. 68.75 54.24 28.50 87.68 AI2D 96. ERNIE 5.0-Base OCRBench ChartXiv-RQ ChartXiv-DQ SimpleVQA MMStar BLINK CV-Bench 875 62. 90.35 61.54 74.07 64.12 86.96 Table 4: Evaluation of our pre-trained model across wide range of vision benchmarks, There are few publicly available visual base models and their results, here we only report the results of our model. Benchmark Nano Banana Pro Seedream 4.0 GPT-Image Qwen-Image ERNIE 5.0-Base ERNIE 5.0 GenEval 89. 85.4 84.0 91.0 88.4 90.1 Table 5: Comparison of the image generation ability on GenEval against specialized models. Benchmark Metric HunyuanVideo-1 Wan2.1-14B-0725 Veo3 ERNIE 5.0-Base ERNIE 5.0 VBench Quality Semantic Overall 85.07 76.88 83.43 85.59 76.11 83.69 85.70 82.49 85.06 84.14 82.31 83.78 84.40 83.40 84.20 Table 6: Comparison of the video generation ability on VBench against specialized models. Evaluation of Visual Understanding Table 3 and Table 4 illustrate the multimodal capabilities of ERNIE 5.0 under the instruction-tuned setting (ERNIE 5.0) and its native multimodal foundation (ERNIE 5.0Base), respectively. This comparison enables clearer understanding of how instruction tuning interacts with unified multimodal pre-training paradigm. ERNIE 5.0-Base exhibits strong and well-rounded performances across visual reasoning, document understanding, and general VQA benchmarks, despite the absence of instruction tuning. It suggests that core multimodal perception and cross-modal reasoning abilities are largely acquired during pre-training, benefiting from early fusion of visual and textual tokens and modality-agnostic representation learning. This behavior differs from modular or late-fusion designs, where comparable capabilities typically emerge only after extensive task-specific tuning. Based on this foundation, ERNIE 5.0 consistently improves performance on most benchmarks, particularly on tasks requiring explicit reasoning, compositional understanding, and robust visuallanguage alignment. Compared with other strong multimodal baselines, ERNIE 5.0 achieves competitive or superior results on wide range of reasoning, document understanding, and general VQA tasks, while maintaining balanced performance rather than optimizing for individual benchmarks. It indicates that instruction tuning in ERNIE 5.0 primarily refines reasoning and alignment behaviors, instead of compensating for perceptual capacity. In document understanding scenarios, the relatively strong results of ERNIE 5.0-Base already demonstrate effective layout-aware reasoning and text extraction, while ERNIE 5.0 further enhances question understanding and structured reasoning over complex visual documents. similar trend is observed in general VQA and video understanding benchmarks, where instruction tuning improves robustness and temporal reasoning without altering the underlying architecture. Evaluation of Visual Generation We evaluate ERNIE 5.0 on widely used benchmarks for both image and video generation, and compare it with leading commercial and open-source models. For image generation, as shown in Table 5, ERNIE 5.0 achieves competitive performance on the GenEval benchmark. In particular, ERNIE 5.0 performs on par with state-of-the-art commercial systems such as Nano-Banana Pro (DeepMind, 2025b) and Qwen-Image (Wu et al., 2025), and is comparable to GPTImage (OpenAI, 2024) and Seedream 4.0 (Seedream et al., 2025). It demonstrates that ERNIE 5.0 is capable of producing high-aesthetic images with strong semantic alignment and fine-grained visual details. For video generation, Table 6 summarizes the results on the VBench benchmark. ERNIE 5.0 achieves the best performance on VBench-Semantic, surpassing strong commercial models such as Veo3 (DeepMind, 2025c), which indicates its superior semantic alignment in video generation. This advantage aligns with the unified multimodal architecture, where high-level semantic representations are effectively transferred to generative tasks. Meanwhile, ERNIE 5.0 remains competitive on the overall and quality metrics, demonstrating solid visual fidelity and temporal consistency. In addition, ERNIE 5.0 performs on par with leading open-source models such as HunyuanVideo-1 (Kong et al., 2024) and Wan2.1-14B-0725 (Wan 21 ERNIE 5.0 Technical Report Feb 4, 2026 Benchmark Kimi Audio GPT-4o -Audio Qwen3-Omni -Instruct LongCat-Flash -Omni Gemini-3 -Pro ERNIE 5.0-Base ERNIE 5.0 AISHELL-1 AISHELL-2 WenetSpeech net meeting LibriSpeech clean other Fleurs-en Fleurs-zh AlpacaEval CommonEval SD-QA MMSU OpenBookQA IFEval AdvBench MMAU TUT2017 CochlScene ClothoAQA VocalSound Automatic Speech Recognition () 0.60 2.56 3.52 4.26 0.84 2. 0.63 2.78 3.04 4.98 0.75 2.90 0.31 2.64 5.37 6.28 15.30 32.27 4.69 5. 6.09 6.69 10.94 12.08 11.57 22.85 7.27 7.36 1.28 2.42 1.39 3. 1.22 2.48 1.57 4.01 2.74 4.40 1.47 3.73 1.16 2.61 4.44 2. 4.46 3.97 63.12 62.17 83.52 61.10 100.00 65.20 65.25 80.42 72.21 94.85 3.32 2.44 4.73 4.37 90.10 78.90 87.90 66.81 99.23 68.40 20.74 34.94 61.87 82.37 2.72 2. 4.74 4.54 76.90 69.00 89.70 77.80 99.30 5.02 3.99 VoiceBench () 4.94 4.32 82.46 81.95 93.41 77.99 100.00 Audio Understanding () 77.50 40.74 43.03 75.16 91. 75.90 65.43 70.02 72.83 92.76 3.63 4.98 4.80 4.68 94.39 92.16 96.26 87.45 98.46 80.80 61.42 74.60 74.41 92.01 4.39 1.58 4.65 4.39 86.44 76.61 88.35 71.17 98. 80.80 57.65 75.24 65.70 91.48 3.14 0.83 4.62 3.74 77.58 84.68 92.97 72.67 99.23 80.40 68.09 82.77 73.68 90.73 Table 7: Comparison of the automatic speech recognition, speech-to-text dialogue, and audio understanding abilities against specialized models. Automatic speech recognition uses word error rate (WER) as the metric (lower is better), while other tasks use accuracy or score (higher is better). et al., 2025), indicating that the proposed architecture provides strong video generation foundation even before post-training. Overall, these results validate the effectiveness of ERNIE 5.0 in producing semantically accurate and visually coherent videos. 6.3 Evaluation on Audio Benchmarks To evaluate the audio understanding and generation capabilities enabled by native multimodal training, we assess ERNIE 5.0 on diverse suite of speechand audio-centric benchmarks. These evaluations cover automatic speech recognition, speech-based dialogue, general audio understanding, as well as speech generation, providing comprehensive view of how the unified multimodal architecture translates into robust audio perception, semantic understanding, and generation performance across across diverse linguistic and acoustic environments. Audio Understanding & Generation Benchmarks Automatic Speech Recognition: AISHELL-1 (Bu et al., 2017), AISHELL-2 (Du et al., 2018), WenetSpeech (Zhang et al., 2022), LibriSpeech (Panayotov et al., 2015), Fleurs (Conneau et al., 2023). Voice Chatting: VoiceBench (Chen et al., 2024f). Audio Understanding: MMAU (Sakshi et al., 2024), CochlScene (Jeong & Park, 2022), TUT2017 (Mesaros et al., 2016), ClothoAQA (Lipping et al., 2022), VocalSound (Gong et al., 2022). Speech Generation: Seed-TTS (Anastassiou et al., 2024). Evaluation of Audio Understanding We compare ERNIE 5.0 with other leading specialist and generalist models on ASR, voice-chatting, and other audio understanding benchmarks. As shown in Table 7 and Table 8, ERNIE 5.0 exihibits state-of-the-art or competitive performance across speech recognition, speechto-text dialogue, audio understanding and speech generation benchmarks. On automatic speech recognition (ASR) tasks, ERNIE 5.0 achieves low word error rate (WER) on both Chinese and English benchmarks, including AISHELL and LibriSpeech, indicating robust performance 22 ERNIE 5.0 Technical Report Feb 4, Benchmark Model Performance SEED-TTS () test-zh test-en Seed-TTSICL Seed-TTSRL MaskGCT E2 TTS F5-TTS Spark TTS CosyVoice 2 CosyVoice 3 Qwen2.5-Omni Qwen3-Omni ERNIE 5.0-Base ERNIE 5.0 1.11 2.24 1.00 1.94 2.27 2.62 1.97 2.19 1.56 1.83 1.20 1.98 1.45 2.57 0.71 1.45 1.42 2.33 1.07 1.39 3.41 2.44 1.35 1. Table 8: Comparison of the text-to-speech ability on SEED-TTS against specialized models. Word Error Ratio (WER) is used to evaluate content consistency, the lower the better. across languages and acoustic conditions. While some models achieve stronger results on specific datasets such as WenetSpeech, ERNIE 5.0 maintains consistently stable performance across wide range of ASR benchmarks, reflecting good generalization. On VoiceBench, ERNIE 5.0 remains competitive on tasks such as MMSU and OpenBookQA, reflecting its ability to handle speech-based interaction and knowledge-grounded reasoning. For audio understanding tasks, ERNIE 5.0 performs favorably on acoustic scene and environmental sound benchmarks such as TUT2017 and CochlScene, while achieving comparable results on more diverse benchmarks including MMAU and ClothoAQA. Overall, the model shows consistent performance across wide range of non-speech and general audio understanding tasks, indicating its ability to capture audio semantics beyond speech content. Evaluation of Audio Generation On SEED-TTS, ERNIE 5.0 achieves competitive content consistency on both Chinese and English test sets (test-zh, test-en), performing comparably to recent audiolanguage models such as Qwen3-Omni. While specialist TTS systems (e.g., CosyVoice-3) obtain stronger results, ERNIE 5.0 demonstrates reliable content preservation without task-specific TTS optimization. Taken together, the results in Table 7 indicate that ERNIE-5.0 is competitive unified audiolanguage model that balances speech recognition accuracy, speech interaction, general audio understanding and speech generation within single framework. 6.4 Discussion To better understand the behavior of ERNIE 5.0 in large-scale multimodal training, we dissect two pivotal architectural designs: modality-agnostic expert routing and elastic training. The following subsections analyze these two components and their effects on efficiency and scalability. 6.4.1 Modality-Agnostic Expert Routing ERNIE 5.0 employs modality-agnostic expert routing mechanism, without introducing modality-specific parameters or routing rules. All inputs, including text, image, video, and audio, are processed by the same routing network and share common pool of experts. In this discussion, we examine the expert routing behavior of the MoE model, with particular attention to modality-specific utilization patterns, cross-modality expert overlap, and routing balance across layers. Perspective of Expert Utilization and Specialization Figure 8 reveals clearly non-uniform expert activation distribution, suggesting that different experts still play distinct functional roles under the modality-agnostic expert routing setting. subset of experts is repeatedly activated across text, image, video, and audio inputs, whereas the remaining experts exhibit strong modality-specific activation patterns. Expert activations for image, video, and audio inputs are noticeably more concentrated than those for text-only inputs. From task-level perspective, visual generation and audio-related tasks lead to more concentrated expert activations than text and visual understanding tasks. Perspective of Cross-Modality Expert Collaboration Figure 9 further analyzes cross-modality expert collaboration by reporting the Intersection over Union (IoU) of the top 25% of experts by activation frequency for each modality across different layers. From the overlap patterns, text shows higher degree 23 ERNIE 5.0 Technical Report Feb 4, 2026 Figure 8: Visualization of expert utilization patterns across modalities and tasks for the representative first, middle and last layers. The y-axis indicates the frequency of expert activation. Figure 9: Visualization of expert collaboration across modalities and tasks for the representative first, middle and last layers, using the Intersection over Union (IoU) of the top 25% most frequently activated experts for each modality. of co-activation with audio than with other modalities, and this overlap with image and video becomes increasingly pronounced in deeper layers. This trend suggests that multimodal representations gradually shift from low-level modality-specific features toward higher-level unified semantic features, enabling stronger collaboration with text at deeper layers. For visual modalities, image and video understanding tasks exhibit high expert overlap, and similar pattern is observed for image and video generation tasks, which aligns with the architectural design in which an image is regarded as single-frame video. In contrast, the overlap between visual understanding and generation remains relatively low, with no clear 24 ERNIE 5.0 Technical Report Feb 4, 2026 Figure 10: Visualization of the load balancing across layers and modalities, using the normalized entropy (NE) metric of expert routing. Training Configuration Inference Configuration (# of Layers) Validation Loss BaselineLayers=16 Elastic DepthLayers [1, 16] Layers=16 Layers=16 Layers=12 1.945 1.941 2. Table 9: Validation loss under elastic depth training and inference configurations for small-scale MoE model. Elastic depth slightly improves full-depth performance and yields reduced-depth sub-networks with predictable degradation, enabling flexible deployment without retraining. preference toward either direction. Perspective of Load Balancing Figure 10 reports the normalized entropy (NE) of expert routing across layers and modalities, defined as NE = , which provides quantitative measure of expert load balance. Here, is the number of experts and pi is the fraction of tokens routed to i-th expert. The value of NE lies in [0, 1], where larger values correspond to more uniform expert utilization. i=1 pi log(pi) log From layer-wise perspective, the text modality exhibits consistently high and stable normalized entropy across almost all layers, with only mild drop observed at the final layer. Notably, the first layer does not show severe imbalance, contradicting the common assumption that early MoE layers require dense designs to maintain routing stability (Liu et al., 2024). For visual understanding, expert utilization becomes less balanced at both the lowest and highest layers, while the middle layers maintain relatively uniform routing, suggesting that visual perception benefits from balanced expert sharing during intermediate feature abstraction stages. In contrast, visual generation and audio-related tasks display different trend: the first layer shows moderate level of balance, followed by decrease in entropy in lower layers, partial recovery in lower-to-mid layers, and fluctuating drop at the higher layers. This pattern indicates alternating phases of expert specialization and re-integration along depth for generative and audio-centric tasks. Taken together, these analyses reveal clear expert utilization patterns across layers, modalities, and tasks in ERNIE 5.0. Although the routing mechanism is modality-agnostic during training, pronounced expert specialization across modalities still emerges, which suggests that the router could capture modality structure and allocate expert capacity in self-driven manner. From system perspective, the unified routing mechanism simplifies the overall design and avoids manual expert partitioning across modalities. Beyond empirical observations, the routing behaviors and load-balancing characteristics also offer practical guidance for future model design. For example, layer-aware expert allocation, adaptive balancing strategies, and modality-shared expert configurations may have the potential to become effective principles for building the next generation of native multimodal architectures. 6.4.2 Elastic Training Elastic training plays central role in enabling ERNIE 5.0 to adapt to different compute, memory, and latency constraints. In this section, we analyze elastic training strategies through controlled ablation studies on small-scale MoE model and report key findings on ERNIE 5.0. Unless otherwise specified, all 25 ERNIE 5.0 Technical Report Feb 4, 2026 Training Configuration Inference Configuration (# of Experts) Validation Loss BaselineExperts=64 Elastic WidthExperts {64, 32} Experts= Experts=64 Experts=32 1.957 1.964 2.218 Table 10: Validation loss under elastic width configurations for the small-scale MoE model. Elastic width introduces only minor degradation at full capacity, while reduced-width models remain usable, supporting deployment under constrained parameter budgets. Training Configuration Inference Configuration (MoE Routing Top-k) Validation Loss BaselineTop-k = 8 Elastic SparsityTop-k [1, 8] Top-k = 8 Top-k = 8 Top-k = 4 Top-k = 2 Top-k = 1 1.945 1.969 1.971 2.003 2.175 Table 11: Validation loss under different routing sparsity levels for the small-scale MoE model. Elastic sparsity allows stable inference across varying routing budgets, with graceful performance degradation under aggressive sparsification. elastic configurations are derived from the same pretrained checkpoint. Controlled-Scale Experiments We conduct ablation studies using small-scale MoE model with 64 experts, 454M activated parameters, and 3.2B total parameters. The model is trained on 250B tokens with default routing configuration of top-k = 8. Validation loss on held-out data is reported throughout. Based on this experimental setup, we analyze elasticity along the three dimensions separately. Elastic Depth. We first study elastic depth by varying the number of active transformer layers during training. Consistent with the configuration of ERNIE 5.0, 80% of training samples retain the full-depth model, while the remaining 20% reduced-depth sub-networks. As reported in Table 9, elastic depth slightly improves full-depth performance, which indicates regularization effect introduced by occasional layer dropping. Moreover, reduced-depth sub-networks exhibit smooth and predictable performance degradation, suggesting that intermediate representations remain robust under layer removal. Overall, elastic depth offers low-risk and cost-efficient mechanism for deriving multiple deployable sub-models from single pretrained checkpoint. Elastic Width. We next evaluate elasticity over MoE width. During training, 80% of samples retain all 64 experts, while the remaining 20% samples configurations with 32 experts. Table 10 shows that elastic width introduces only slight degradation at full capacity. Although reduced-width configurations incur minor performance fluctuations of larger models, they remain functional without retraining, enabling deployment under strict memory constraints. Elastic Sparsity. We also analyze elastic sparsity by varying routing top-k at inference. During training, most instances are equipped with default configuration (k = 8), while the remaining instances are trained with reduced activated experts. As shown in Table 11, elastic top-k training incurs modest degradation under the full-activation configuration, while enabling stable and effective inference under substantially reduced routing budgets. Scaling to ERNIE 5.0 Beyond controlled-scale experiments, we further evaluate the effectiveness of elastic training on the pre-trained experimental model ERNIE 5.0-Exp-Base and its post-trained model ERNIE 5.0-Exp. Starting from ERNIE 5.0-Exp, we first analyze elastic sparsity by reducing the routing top-k to 25% during inference. As summarized in Table 12, ERNIE 5.0-Exp-ES25.0% retains comparable performance across wide range of text and visual benchmarks. The resulting accuracy drop remains minor, while the reduction in routing sparsity brings substantial efficiency gains, providing more than 15% improvement in decoding speed. Moreover, we jointly activate elasticity along depth, width, and sparsity, deriving compact pre-trained model from ERNIE 5.0-Exp-Base, which operates with only 53.7% of the activated parameters and 35.8% of the total parameters. Using the same data and training strategy for mid-training and post-training, we obtain the post-trained model, ERNIE 5.0-Exp-EA35.8%. Despite its substantially reduced computational footprint, the elastic variant achieves competitive performance across benchmarks, attaining an average score of 75.17 compared to 75.55 for the full ERNIE 5.0-Exp. Moreover, strong robustness on challenging reasoning and perception tasks, such as ZebraLogic and VisualPuzzle, indicates that elastic training effectively mitigates the performance degradation typically 26 ERNIE 5.0 Technical Report Feb 4, 2026 Model AVG. ZebraLogic LiveCodeBench v6 TAU2 MMMU MathVista VisualPuzzle SimpleVQA ERNIE 5.0-Exp 75.55 ERNIE 5.0-Exp-ES25.0% 74.43 ERNIE 5.0-Exp-EA35.8% 75.17 95.00 94.10 95.20 73.35 70.70 70. 79.35 77.34 77.23 74.11 73.78 75.11 83.70 84.90 84.50 59.93 57.98 60.39 63.40 62.19 62.86 Table 12: Performance comparison between the full ERNIE 5.0-Exp and its elastic variants under reduced routing sparsity (ERNIE 5.0-Exp-ES) and all elastic configurations (ERNIE 5.0-Exp-EA). Elastic sparsity preserves comparable accuracy with improved decoding efficiency, while the fully elastic model achieves competitive performance despite substantially reduced activated computation and parameter usage. associated with aggressive reductions in computation and parameters. Overall, these results highlight that elasticity is not merely as post-hoc compression technique, but principled training paradigm. By jointly optimizing depth, width, and sparsity during pre-training and, the elastic model learns to redistribute representational capacity across layers and modalities, leading to favorable performanceefficiency trade-offs."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce ERNIE 5.0, natively unified foundation model that integrates multimodal understanding and generation of text, image, video, and audio with shared next-group-of-tokens prediction objective. To the best of our knowledge, ERNIE 5.0 represents the first realization of multimodal understanding and generation within unified, trillion-level autoregressive framework. An ultra-sparse mixture-of-experts architecture with modality-agnostic expert routing enables scalable cross-modal modeling without relying on modality-specific designs. We explore novel elastic training paradigm that supports flexible deployment configurations within single pre-trained model, which is proved successful to maintain both training efficiency and performances. We also address key challenges in reinforcement learning for large-scale multimodal models and ensure pos-training stability and efficiency. Extensive experiments demonstrate competitive and balanced performance across modalities. Overall, the results indicate that autoregressive unified multimodal and elastic pre-training provides scalable pathway toward the next generation of foundational models."
        },
        {
            "title": "8 Contributors",
            "content": "Haifeng Wang, Hua Wu, Tian Wu, Yu Sun, Jing Liu, Dianhai Yu, Yanjun Ma, Jingzhou He, Zhongjun He, Dou Hong, Qiwen Liu, Shuohuan Wang, Junyuan Shang, Zhenyu Zhang, Yuchen Ding, Jinle Zeng, Jiabin Yang, Liang Shen, Ruibiao Chen, Weichong Yin, Siyu Ding, Dai Dai, Shikun Feng, Siqi Bao, Bolei He, Yan Chen, Zhenyu Jiao, Ruiqing Zhang, Zeyu Chen, Qingqing Dang, Kaipeng Deng, Jiajun Jiang, Enlei Gong, Guoxia Wang, Yanlin Sha, Yi Liu, Yehan Zheng, Weijian Xu, Jiaxiang Liu, Zengfeng Zeng, Yingqi Qu, Zhongli Li, Zhengkun Zhang, Xiyang Wang, Zixiang Xu, Xinchao Xu, Zhengjie Huang, Dong Wang, Bingjin Chen, Yue Chang, Xing Yuan, Shiwei Huang, Qiao Zhao, Xinzhe Ding, Shuangshuang Qiao, Baoshan Yang, Bihong Tang, Bin Li, Bingquan Wang, Binhan Tang, Binxiong Zheng, Bo Cui, Bo Ke, Bo Zhang, Bowen Zhang, Boyan Zhang, Boyang Liu, Caiji Zhang, Can Li, Chang Xu, Chao Pang, Chao Zhang, Chaoyi Yuan, Chen Chen, Cheng Cui, Chenlin Yin, Chun Gan, Chunguang Chai, Chuyu Fang, Cuiyun Han, Dan Zhang, Danlei Feng, Danxiang Zhu, Dong Sun, Dongbo Li, Dongdong Li, Dongdong Liu, Dongxue Liu, Fan Ding, Fan Hu, Fan Li, Fan Mo, Feisheng Wu, Fengwei Liu, Gangqiang Hu, Gaofeng Lu, Gaopeng Yong, Gexiao Tian, Guan Wang, Guangchen Ni, Guangshuo Wu, Guanzhong Wang, Guihua Liu, Guishun Li, Haibin Li, Haijian Liang, Haipeng Ming, Haisu Wang, Haiyang Lu, Haiye Lin, Han Zhou, Hangting Lou, Hanwen Du, Hanzhi Zhang, Hao Chen, Hao Du, Hao Liu, Hao Zhou, Haochen Jiang, Haodong Tian, Haoshuang Wang, Haozhe Geng, Heju Yin, Hong Chen, Hongchen Xue, Hongen Liu, Honggeng Zhang, Hongji Xu, Hongwei Chen, Hongyang Zhang, Hongyuan Zhang, Hua Lu, Huan Chen, Huan Wang, Huang He, Hui Liu, Hui Zhong, Huibin Ruan, Jiafeng Lu, Jiage Liang, Jiahao Hu, Jiahao Hu, Jiajie Yang, Jialin Li, Jian Chen, Jian Wu, Jianfeng Yang, Jianguang Jiang, Jianhua Wang, Jianye Chen, Jiaodi Liu, Jiarui Zhou, Jiawei Lv, Jiaxin Zhou, Jiaxuan Liu, Jie Han, Jie Sun, Jiefan Fang, Jihan Liu, Jihua Liu, Jing Hu, Jing Qian, Jing Yan, Jingdong Du, Jingdong Wang, Jingjing Wu, Jingyong Li, Jinheng Wang, Jinjin Li, Jinliang Lu, Jinlin Yu, Jinnan Liu, Jixiang Feng, Jiyi Huang, Jiyuan Zhang, Jun Liang, Jun Xia, Jun Yu, Junda Chen, Junhao Feng, Junhong Xiang, Junliang Li, Kai Liu, Kailun Chen, Kairan Su, Kang Hu, Kangkang Zhou, Ke Chen, Ke Wei, Kui Huang, Kun Wu, Kunbin Chen, Lei Han, Lei Sun, Lei Wen, Linghui Meng, Linhao Yu, Liping Ouyang, Liwen Zhang, Longbin Ji, Longzhi Wang, Meng Sun, Meng Tian, Mengfei Li, Mengqi Zeng, Mengyu Zhang, Ming Hong, Mingcheng Zhou, Mingming Huang, Mingxin Chen, Mingzhu Cai, Naibin Gu, Nemin Qiu, Nian Wang, Peng Qiu, Peng 27 ERNIE 5.0 Technical Report Feb 4, 2026 Zhao, Pengyu Zou, Qi Wang, Qi Xin, Qian Wang, Qiang Zhu, Qianhui Luo, Qianwei Yang, Qianyue He, Qifei Wu, Qinrui Li, Qiwen Bao, Quan Zhang, Quanxiang Liu, Qunyi Xie, Rongrui Zhan, Rufeng Dai, Rui Peng, Ruian Liu, Ruihao Xu, Ruijie Wang, Ruixi Zhang, Ruixuan Liu, Runsheng Shi, Ruting Wang, Senbo Kang, Shan Lu, Shaofei Yu, Shaotian Gong, Shenwei Hu, Shifeng Zheng, Shihao Guo, Shilong Fan, Shiqin Liu, Shiwei Gu, Shixi Zhang, Shuai Yao, Shuang Zhang, Shuangqiao Liu, Shuhao Liang, Shuwei He, Shuwen Yang, Sijun He, Siming Dai, Siming Wu, Siyi Long, Songhe Deng, Suhui Dong, Suyin Liang, Teng Hu, Tianchan Xu, Tianliang Lv, Tianmeng Yang, Tianyi Wei, Tiezhu Gao, Ting Sun, Ting Zhang, Tingdan Luo, Wei He, Wei Luan, Wei Yin, Wei Zhang, Wei Zhou, Weibao Gong, Weibin Li, Weicheng Huang, Weichong Dang, Weiguo Zhu, Weilong Zhang, Weiqi Tan, Wen Huang, Wenbin Chang, Wenjing Du, Wenlong Miao, Wenpei Luo, Wenquan Wu, Xi Shi, Xi Zhao, Xiang Gao, Xiangguo Zhang, Xiangrui Yu, Xiangsen Wang, Xiangzhe Wang, Xianlong Luo, Xianying Ma, Xiao Tan, Xiaocong Lin, Xiaofei Wang, Xiaofeng Peng, Xiaofeng Wu, Xiaojian Xu, Xiaolan Yuan, Xiaopeng Cui, Xiaotian Han, Xiaoxiong Liu, Xiaoxu Fei, Xiaoxuan Wu, Xiaoyu Wang, Xiaoyu Zhang, Xin Sun, Xin Wang, Xinhui Huang, Xinming Zhu, Xintong Yu, Xinyi Xu, Xinyu Wang, Xiuxian Li, XuanShi Zhu, Xue Xu, Xueying Lv, Xuhong Li, Xulong Wei, Xuyi Chen, Yabing Shi, Yafeng Wang, Yamei Li, Yan Liu, Yanfu Cheng, Yang Gao, Yang Liang, Yang Wang, Yang Wang, Yang Yang, Yanlong Liu, Yannian Fu, Yanpeng Wang, Yanzheng Lin, Yao Chen, Yaozong Shen, Yaqian Han, Yehua Yang, Yekun Chai, Yesong Wang, Yi Song, Yichen Zhang, Yifei Wang, Yifeng Guo, Yifeng Kou, Yilong Chen, Yilong Guo, Yiming Wang, Ying Chen, Ying Wang, Yingsheng Wu, Yingzhan Lin, Yinqi Yang, Yiran Xing, Yishu Lei, Yixiang Tu, Yiyan Chen, Yong Zhang, Yonghua Li, Yongqiang Ma, Yongxing Dai, Yongyue Zhang, Yu Ran, Yu Sun, Yu-Wen Michael Zhang, Yuang Liu, Yuanle Liu, Yuanyuan Zhou, Yubo Zhang, Yuchen Han, Yucheng Wang, Yude Gao, Yuedong Luo, Yuehu Dong, Yufeng Hu, Yuhui Cao, Yuhui Yun, Yukun Chen, Yukun Gao, Yukun Li, Yumeng Zhang, Yun Fan, Yun Ma, Yunfei Zhang, Yunshen Xie, Yuping Xu, Yuqin Zhang, Yuqing Liu, Yurui Li, Yuwen Wang, Yuxiang Lu, Zefeng Cai, Zelin Zhao, Zelun Zhang, Zenan Lin, Zezhao Dong, Zhaowu Pan, Zhaoyu Liu, Zhe Dong, Zhe Zhang, Zhen Zhang, Zhengfan Wu, Zhengrui Wei, Zhengsheng Ning, Zhenxing Li, Zhenyu Li, Zhenyu Qian, Zhenyun Li, Zhi Li, Zhichao Chen, Zhicheng Dong, Zhida Feng, Zhifan Feng, Zhihao Deng, Zhijin Yu, Zhiyang Chen, Zhonghui Zheng, Zhuangzhuang Guo, Zhujun Zhang, Zhuo Sun, Zichang Liu, Zihan Lin, Zihao Huang, Zihe Zhu, Ziheng Zhao, Ziping Chen, Zixuan Zhu, Ziyang Xu, Ziyi Liang, Ziyuan Gao ERNIE 5.0 Technical Report Feb 4,"
        },
        {
            "title": "References",
            "content": "AIME. Aime problems and solutions, 2025. URL https://artofproblemsolving.com/wiki/index.php /AIME Problems and Solutions. Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, et al. Seed-tts: family of high-quality versatile speech generation models. arXiv preprint arXiv:2406.02430, 2024. Anthropic. Claude Opus 4.5, 2025. URL https://www-cdn.anthropic.com/bf10f64990cfda0ba858290 be7b8cc6317685f47.pdf. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, et al. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, and Pascale Fung. Hallulens: Llm hallucination benchmark. arXiv preprint arXiv:2504.17550, 2025. Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. 2-Bench: Evaluating Conversational Agents in Dual-Control Environment. arXiv preprint arXiv:2506.07982, 2025. Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. Aishell-1: An open-source mandarin speech corpus and speech recognition baseline. In 2017 20th conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA), pp. 15. IEEE, 2017. Ruisi Cai, Saurav Muralidharan, Greg Heinrich, Hongxu Yin, Zhangyang Wang, Jan Kautz, and Pavlo Molchanov. Flextron: Many-in-one flexible large language model. arXiv preprint arXiv:2406.10260, 2024. Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, et al. Acebench: Who wins the match point in tool usage? arXiv preprint arXiv:2501.12851, 2025. Liang Chen, Sinan Tan, Zefan Cai, Weichu Xie, Haozhe Zhao, Yichi Zhang, Junyang Lin, Jinze Bai, Tianyu Liu, and Baobao Chang. spark of vision-language intelligence: 2-dimensional autoregressive transformer for efficient finegrained image generation. arXiv preprint arXiv:2410.01912, 2024a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024b. Mark Chen. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Yilong Chen, Junyuan Shang, Zhengyu Zhang, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, and Haifeng Wang. Mixture of hidden-dimensions transformer. arXiv preprint arXiv:2412.05644, 2024c. Yilong Chen, Junyuan Shang, Zhenyu Zhang, Shiyao Cui, Tingwen Liu, Shuohuan Wang, Yu Sun, and Hua Wu. Lemon: Reviving stronger and smaller lms from larger lms with linear parameter fusion. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 80058019, 2024d. Yilong Chen, Linhao Zhang, Junyuan Shang, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, and Yu Sun. Dha: Learning decoupled-head attention from transformer checkpoints via adaptive heads fusion. Advances in Neural Information Processing Systems, 37:4587945913, 2024e. Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby Tan, and Haizhou Li. Voicebench: Benchmarking llm-based voice assistants. arXiv preprint arXiv:2410.17196, 2024f. Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4637 4646, 2025. 29 ERNIE 5.0 Technical Report Feb 4, 2026 Yao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, Wentao Chen, Zhengyu Chen, Shijie Geng, arXiv preprint Aoyan Li, Bo Li, et al. Fullstack bench: Evaluating llms as full stack coders. arXiv:2412.00535, 2024. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pp. 798805. IEEE, 2023. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Google DeepMind. Gemini 2.5, 2025a. URL https://blog.google/technology/google-deepmind/ge mini-model-thinking-updates-march-2025/. Google DeepMind. Gemini 3.0, 2025b. URL https://blog.google/products-and-platforms/product s/gemini/gemini-3/. Google DeepMind. Introducing Veo 3, 2025c. URL https://deepmind.google/models/veo/. Kaustubh Deshpande, Ved Sirdeshmukh, Johannes Baptist Mols, Lifeng Jin, Ed-Yeremai HernandezCardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: realistic multi-turn conversation evaluation benchmark challenging to frontier llms. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 1863218702, 2025. Fnu Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, et al. Matformer: Nested transformer for elastic inference. In Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023), 2023. Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels. arXiv preprint arXiv:2412.05496, 2024. Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research into industrial scale. arXiv preprint arXiv:1808.10583, 2018. Baidu ERNIE Team. Ernie 4.5 technical report, 2025. URL https://yiyan.baidu.com/blog/publicati on/ERNIE Technical Report.pdf. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2410824118, 2025. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pp. 148166. Springer, 2024. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are we done with mmlu? In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 50695096, 2025. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi`ere, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024. Yuan Gong, Jin Yu, and James Glass. Vocalsound: dataset for improving human vocal sounds recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 151155. IEEE, 2022. Alex Gu, Baptiste Rozi`ere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024. 30 ERNIE 5.0 Technical Report Feb 4, 2026 Naibin Gu, Zhenyu Zhang, Yuchen Feng, Yilong Chen, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, et al. Elastic moe: Unlocking the inference-time scalability of mixture-of-experts. arXiv preprint arXiv:2509.21892, 2025. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled In Proceedings of the language hallucination and visual illusion in large vision-language models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1437514385, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1573315744, 2025. Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, et al. Chinese simpleqa: chinese factuality evaluation for large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1918219208, 2025. Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, et al. Multi-if: Benchmarking llms on multi-turn and multilingual instructions following. arXiv preprint arXiv:2410.15553, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. HMMT Organization. Hmmt, 2025. URL https://www.hmmt.org/. Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Videommmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36:6299163010, 2023. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Il-Young Jeong and Jeongsoo Park. Cochlscene: Acquisition of acoustic scene data using crowdsourcing. In 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), pp. 1721. IEEE, 2022. ERNIE 5.0 Technical Report Feb 4, 2026 Longbin Ji, Xiaoxiong Liu, Junyuan Shang, Shuohuan Wang, Yu Sun, Hua Wu, and Haifeng Wang. arXiv preprint Videoar: Autoregressive video generation via next-frame & scale prediction. arXiv:2601.05966, 2026. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. networks. 44014410, 2019. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Yuanzhu Peter Chen, et al. Big-bench extra hard. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2647326501, 2025. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European conference on computer vision, pp. 235251. Springer, 2016. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved rvqgan. Advances in Neural Information Processing Systems, 36: 2798027993, 2023. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 1126011285, 2024. Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning. arXiv preprint arXiv:2502.01100, 2025. Ling-Team, Anqi Shen, Baihui Li, Bin Hu, Bin Jing, Cai Chen, Chao Huang, Chao Zhang, Chaokun Yang, Cheng Lin, et al. Every step evolves: Scaling reinforcement learning for trillion-scale thinking model. arXiv preprint arXiv:2510.18855, 2025. Samuel Lipping, Parthasaarathy Sudarsanam, Konstantinos Drossos, and Tuomas Virtanen. Clothoaqa: crowdsourced dataset for audio question answering. In 2022 30th European Signal Processing Conference (EUSIPCO), pp. 11401144. IEEE, 2022. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, et al. Deepseek-v3.2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025a. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023a. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025b. Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2(5):6, 2023b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. arXiv preprint arXiv:2310.02255, 2023. 32 ERNIE 5.0 Technical Report Feb 4, 2026 Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025a. Wenhan Ma, Hailin Zhang, Liang Zhao, Yifan Song, Yudong Wang, Zhifang Sui, and Fuli Luo. Stabilizing moe reinforcement learning by aligning training and inference routers. arXiv preprint arXiv:2510.11370, 2025b. Yanjun Ma, Dianhai Yu, Tian Wu, and Haifeng Wang. Paddlepaddle: An open-source deep learning platform from industrial practice. Frontiers of Data and Domputing, 1(1):105115, 2019. Zeyao Ma, Bohan Zhang, Jing Zhang, Jifan Yu, Xiaokang Zhang, Xiaohan Zhang, Sijia Luo, Xi Wang, and Jie Tang. Spreadsheetbench: Towards challenging real world spreadsheet manipulation. Advances in Neural Information Processing Systems, 37:9487194908, 2024. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 98029822, 2023. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the association for computational linguistics: ACL 2022, pp. 22632279, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Xin Men, Mingyu Xu, Qingyu Zhang, Qianhao Yuan, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 2019220204, 2025. Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen. Tut database for acoustic scene classification and sound event detection. In 2016 24th European signal processing conference (EUSIPCO), pp. 11281132. IEEE, 2016. OpenAI. Hello GPT-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. OpenAI. Introducing gpt-5, 2025. URL https://openai.com/index/introducing-gpt-5/. Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count to ten. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 31703180, 2023. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 52065210. IEEE, 2015. Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. Bpe-dropout: Simple and effective subword regularization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 18821892, 2020. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pp. 2849228518. PMLR, 2023. Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind: Failing to translate detailed visual features into words. arXiv preprint arXiv:2407.06581, 2024. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. 33 ERNIE 5.0 Technical Report Feb 4, David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed Haggag, Alfonso Amayuelas, et al. Include: Evaluating multilingual language understanding with regional knowledge. arXiv preprint arXiv:2411.19799, 2024. Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. On the effect of dropping layers of pre-trained transformer models. Computer Speech & Language, 77:101429, 2023. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, and Dinesh Manocha. Mmau: massive multi-task audio understanding and reasoning benchmark. arXiv preprint arXiv:2410.19168, 2024. Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, and Xiang Yue. Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge. arXiv preprint arXiv:2504.10342, 2025. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, 2023. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Guoxia Wang, Jinle Zeng, Xiyuan Xiao, Siming Wu, Jiabin Yang, Lujing Zheng, Zeyu Chen, Jiang Bian, Dianhai Yu, and Haifeng Wang. Flashmask: Efficient and rich mask extension of flashattention. arXiv preprint arXiv:2410.01359, 2024a. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024b. Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. arXiv preprint arXiv:2408.15664, 2024c. Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. arXiv preprint arXiv:2303.00980, 2023. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024d. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024e. 34 ERNIE 5.0 Technical Report Feb 4, 2026 Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023. Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025a. Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025b. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 23692380, 2018. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1513415186, 2025a. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025b. Hui Zeng. Measuring massive multitask chinese understanding. arXiv preprint arXiv:2304.12986, 2023. Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, et al. Wenetspeech: 10000+ hours multi-domain mandarin corpus for speech recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 61826186. IEEE, 2022. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024a. Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech tokenizer for speech language models. In The Twelfth International Conference on Learning Representations, 2024b. Chenggang Zhao, Shangyan Zhou, Liyue Zhang, Chengqi Deng, Zhean Xu, Yuxuan Liu, Kuai Yu, Jiashi Li, and Liang Zhao. Deepep: an efficient expert-parallel communication library. https://github.com /deepseek-ai/DeepEP, 2025a. 35 ERNIE 5.0 Technical Report Feb 4, 2026 Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining. arXiv preprint arXiv:2504.07912, 2025b. Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multi-discipline video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 84758489, 2025c. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 22992314, 2024. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Yuzhen Zhou, Jiajun Li, Yusheng Su, Gowtham Ramesh, Zilin Zhu, Xiang Long, Chenyang Zhao, Jin Pan, Xiaodong Yu, Ze Wang, et al. April: Active partial rollouts in reinforcement learning to tame long-tail generation. arXiv preprint arXiv:2509.18521, 2025. Nannan Zhu, Yonghao Dong, Teng Wang, Xueqian Li, Shengjun Deng, Yijia Wang, Zheng Hong, Tiantian Geng, Guo Niu, Hanyan Huang, et al. Cvbench: Benchmarking cross-video synergies for complex multimodal reasoning. arXiv preprint arXiv:2508.19542, 2025."
        }
    ],
    "affiliations": [
        "Baidu"
    ]
}