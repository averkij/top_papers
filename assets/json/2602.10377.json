{
    "paper_title": "Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs",
    "authors": [
        "Luoyang Sun",
        "Jiwen Jiang",
        "Yifeng Ding",
        "Fengfa Li",
        "Yan Song",
        "Haifeng Zhang",
        "Jian Ying",
        "Lei Ren",
        "Kun Zhan",
        "Wei Chen",
        "Yan Xie",
        "Cheng Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available."
        },
        {
            "title": "Start",
            "content": "Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Luoyang Sun1,2,3,4, Jiwen Jiang1,2,4, Yifeng Ding4, Fengfa Li4, Yan Song5, Haifeng Zhang2,3, Jian Ying1, Lei Ren4,, Kun Zhan4, Wei Chen4,, Yan Xie4 and Cheng Deng6, 1AI Lab, The Yangtze River Delta, 2Institution of Automation, Chinese Academy of Sciences, 3University of Chinese Academy of Sciences, 4Li Auto, 5University College London, 6The University of Edinburgh Vision-Language-Action Models (VLAs) have emerged as key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design game-changing requirement for on-device LLM deployment, where each hardware platform demands tailored architectural solution. We propose hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available. 6 2 0 2 0 1 ] . [ 1 7 7 3 0 1 . 2 0 6 2 : r Figure 1 Hardware co-design scaling law for on-device LLMs. Architectural choices and hardware platforms jointly shape the loss-latency Pareto frontier, revealing Pareto-optimal configurations under system constraints. Version: v1 (major update on Feb 10, 2026). Project Leader. Correspondence to Cheng Deng. Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 2.1 Efficient LLM Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 On-Device LLM Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Hardware-Aware Architecture Optimization . . . . . . . . . . . . . . . . . . . . . . . 3 Formulating Hardware Co-Design Law for on-Device LLM 3.1 Implicit Optimization Objective of Hardware Co-Design Law . . . . . . . . . . . . . . 3.2 Precision Modeling via Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Performance Modeling via Latency . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.4 Pareto-optimal architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "4 Pareto-Optimal Architecture Search"
        },
        {
            "title": "4.1 Loss Prediction via Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "4 6 6 6 6 6 7 8 8"
        },
        {
            "title": "4.3 Pareto Frontier Analysis",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5 Theoretical Framework for Hardware-Aware Architecture Optimization"
        },
        {
            "title": "A Architecture Search Space",
            "content": "B Pre-training Details"
        },
        {
            "title": "D Latency Modeling Details",
            "content": "21 26 26 26 27 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Problem Formulation and Roofline Analysis Case D1: Decode, Latency-Constrained Case D2: Decode, Memory-Constrained Case D3: Decode, Dual-Constrained Case P1: Prefill, Latency-Constrained Case P2: Prefill, Memory-Constrained Case P3: Prefill, Dual-Constrained Summary 28 36 41 44 46 47 49 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs 1. Introduction Large language models are increasingly deployed in embodied AI systems such as autonomous vehicles and mobile robots, where they serve as high-level planners within VisionLanguage-Action frameworks [1, 2, 3]. However, on-device platforms face strict constraints on memory, bandwidth, power, and latency that fundamentally reshape model design [4, 5]. Architectures optimized for cloud GPUs often become infeasible on the edge: high-accuracy models may violate latency budgets, while latency-optimized pipelines can degrade accuracy. This tension motivates hardwaresoftware co-design, where architectural choices are explicitly guided by hardware capabilities and deployment constraints [6]. The fundamental challenge stems from the irregular computememory profile of transformers [7, 8]: attention is bandwidth-bound, feedforward layers are compute-bound, and KV-cache stresses on-chip memory. Despite high theoretical throughput from AI-SoCs, LLM inference rarely reaches peak utilization. Instead, performance is dictated by arithmetic intensity, on-chip locality, and workload patterns like KV-cache footprint and MoE routing. Architectural modification can shift operations across regimes in the hardware Roofline model [9, 10], shown in Figure 2(a). representative example is scaling model depth and width in Transformer-based LLMs. Increasing depth results in linear growth in both computation and memory traffic, as each additional layer introduces fixed amount of parameter reads and arithmetic operations. In contrast, increasing model width leads to quadratic growth in parameter size and memory I/O, since both attention and feed-forward layers scale with the square of the hidden dimension. Under batch-1 inference on edge devices, where weight reuse across tokens is limited and on-chip cache capacity is insufficient to hold model parameters, inference latency is dominated by repeated weight loads from off-chip memory. As consequence, arithmetic intensity remains low and does not scale proportionally with model width, pushing execution into memory bandwidthlimited regime in the roofline model [11, 12]. This mismatch between scaling behavior and hardware characteristics motivates hardware-aware architectural design beyond naive depth and width scaling. (a) Roofline Model (b) Neural Architecture Search (c) Pareto Frontier Figure 2 Hardware co-design strategy preliminaries. (a) Roofline model [9] comparing achieved performance against theoretical hardware limits. (b) One-shot NAS [13] framework jointly optimizing architecture and weights. (c) Pareto frontier visualizing accuracy-efficiency trade-offs [14]. In order to find an optimal model, Neural architecture search (NAS) [15] usually is an appropriate way, shown in Figure 2(b). NAS has traditionally focused on optimizing single objective, such as validation loss or accuracy, often under loosely defined computational budgets [13]. While effective in unconstrained settings, these approaches are ill-suited to edge and on-device platforms, where latencyaccuracy trade-offs are unavoidable. As shown in Table 1, LLM inference system design involves several fundamental trade-offs [16, 17, 18, 19, 20, 21, 8]. This paper focuses on the interplay between model loss and inference latency in hardware co-designed LLMs (bold in Table 1), which is commonly observed in practice but has not been systematically characterized. This trade-off motivates 4 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Pareto-based approach to architecture search [14], visualized as Figure 2(c). Instead of single optimal model, Pareto optimization identifies frontier of non-dominated architectures balancing accuracy and latency. This allows designers to directly select models fitting specific deployment constraints, avoiding exhaustive enumeration [7, 22]. Table 1 Trade-off Analysis for LLM Inference System Design Scenario Memory allocation optimization Trade-off Targets Throughput vs. Latency Pareto Trade-off Factors Single-Query Batch size selection and KV-cache reuse strategies Quantization strategy selection Memory Cost vs. Model Precision INT8 / INT4 quantization levels and accuracy verification methods Distributed inference system Network Communication Cost vs. Compute Parallelism Model parallelism granularity and pipeline depth Hardware co-designed LLM Model Loss vs. Latency Inference Equivalent parameter count and inference time over hardwares We propose hardware-aware modeling framework (shown in Figure 1) for on-device LLMs that jointly captures accuracy and inference performance. We model loss via architectural hyperparameters and predict latency using roofline analysis, providing unified view of the accuracylatency tradeoff. We validate the framework on NVIDIA Jetson Orin by benchmarking thousands of candidate architectures and training subset to fit parameterloss scaling law. Combining this law with hardware-level latency modeling yields the Pareto frontier for the target system. We extend this into theoretical joint optimization formulation over precision and performance, deriving feasible design regions under practical constraints. Our main contribution can be listed as follows: We develop hardware co-design law that combines loss scaling laws with roofline-based latency modeling, enabling an explicit Pareto characterization of accuracylatency trade-offs under fixed hardware constraints for on-device LLMs. To the best of our knowledge, this is the first practical and operational hardware co-design scaling law for on-device LLMs. We benchmark approximately 1,942 LLM architectures on NVIDIA Jetson Orin to identify hardwarealigned architectural patterns, select 170 representative models, and train each for 10B tokens to empirically fit the loss scaling law and validate the resulting accuracylatency Pareto structure. We extend the empirical framework into principled theoretical formulation that casts architecture search as joint optimization problem over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Since this approach reduces architecture selection time from months to only few days, we will release the full methodology, codebase, trained models, and detailed evaluation protocols to support reproducibility and support the development of hardware co-design community. The paper is organized as follows. Section 2 reviews related works. In Section 3, we formulate the hardware co-design law under on-device constraints. In Section 4, we present Pareto-optimal architecture discovery via roofline modeling, detailing how optimal model architectures are identified for given hardware platform. As for Section 5, we extend the empirical framework to theoretical formulation, casting architecture search as joint optimization problem over precision and performance. Finally, we discuss practical usage and application scenarios of the proposed hardware co-design law. 5 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs 2. Related Work 2.1. Efficient LLM Architectures Recent LLM designs prioritize inference efficiency beyond parameter scaling. Key directions include: (1) sparse activation via MoE routing [23, 24], (2) KV-cache reduction through multi-head latent attention (MLA) [8] or sliding-window attention [25], (3) sub-quadratic attention using linearcomplexity mechanisms [26, 27], and (4) hybrid architectures combining SSM with attention [28]. Gated attention variants [29, 30] further enable token-dependent compute modulation. These innovations fundamentally shift inference bottlenecks from compute-bound to memoryor routingsensitive regimes. 2.2. On-Device LLM Deployment Deploying LLMs on edge devices requires co-optimization across model, compression, and system layers. SLMs such as Qwen [31], MiniCPM [32], and SmolLM [33] achieve strong performance through data-efficient training. Quantization methods (AWQ [34], GPTQ [35]) and inference engines (vLLM [36], MLC-LLM [37], PowerInfer [38]) enable efficient execution on heterogeneous hardware. Architecturally, deeper designs with shared embeddings [7] and sparsity-aware mechanisms [8] demonstrate favorable accuracyefficiency trade-offs for resource-constrained deployment. 2.3. Hardware-Aware Architecture Optimization Neural Architecture Search (NAS) [39, 40] automates architecture design but faces challenges in LLM contexts: prohibitive search costs, difficulty incorporating latency/memory constraints, and limited interpretability. Profiling tools like LLM-Viewer [10] and LLMCompass [41] provide hardware-aware analysis but lack integration with architecture search. Studies on depthwidth trade-offs [42, 11] yield inconsistent conclusions, often neglecting deployment constraints. Positioning. This work bridges LLM architecture design with hardware-aware performance modeling. We extend roofline analysis to explicitly model how architectural choices, MoE sparsity, attention variants, KV-cache strategies, map to computeor bandwidth-limited regimes on edge devices, enabling architecture search guided by hardware co-design law rather than post-hoc benchmarking. 3. Formulating Hardware Co-Design Law for on-Device LLM Classical scaling laws [43, 44] characterize the relationship between model size, data size, and training compute under fixed training budgets. In contrast, this work focuses on the deployment regime, where the objective is to identify the optimal architecture ğœ½ under fixed inference latency budget ğ‘‡lat and precision constraints. In this section, we formalize this problem and introduce hardware co-design formulation for on-device LLMs. 3.1. Implicit Optimization Objective of Hardware Co-Design Law Optimization Objective. We seek to minimize validation loss subject to latency and memory constraints: min ğœ½Î˜ (ğœ½) s.t. ğ‘‡ (ğœ½; ğ», ğ‘Š) ğ‘‡lat, ğ‘€ (ğœ½; ğ‘Š) ğ‘€budget (1) where ğœ½ = (ğ‘™, ğ‘‘, ğ‘‘ğ‘š, ğ‘Ÿ, ğœŒ) denotes the model architecture. Specifically, ğ‘™ is the number of transformer layers (depth), ğ‘‘ is the model width (hidden dimension), and ğ‘‘ğ‘š = ğ‘‘â„ ğ‘›ğ‘˜ğ‘£ is the keyvalue cache 6 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs dimension, determined by the per-head dimension ğ‘‘â„ and the number of keyvalue heads ğ‘›ğ‘˜ğ‘£ in grouped-query attention (GQA), which directly governs KV-cache memory footprint and bandwidth consumption during autoregressive decoding. The FFN expansion ratio ğ‘Ÿ controls the size of the intermediate feed-forward layer, with intermediate dimension ğ‘Ÿ ğ‘‘, and therefore dominates per-token compute. For mixture-of-experts (MoE) models, ğœŒ = ğ¾/ğ¸ (0, 1] denotes the expert activation rate, where ğ¾ experts are selected from pool of ğ¸ experts per token. In this setting, ğ‘Ÿ represents the total expansion ratio across all activated experts: if each expert has per-expert expansion ğ‘Ÿsingle, then ğ‘Ÿ = ğ¾ ğ‘Ÿsingle, ensuring that FFN compute remains comparable across different sparsity levels. The latency surrogate ğ‘‡ (ğœ½; ğ», ğ‘Š) models the end-to-end latency under context encoding and autoregressive generation, depending on both architectural and hardware characteristics. Hardware parameters ğ» = (ğœ‹ğ», ğ›½ğ») include the peak compute throughput ğœ‹ğ» (FLOPS) and sustained memory bandwidth ğ›½ğ» (Bytes/s). Under roofline analysis, ğœ‹ğ» and ğ›½ğ» jointly determine whether inference is computeor bandwidth-bound. The workload configuration ğ‘Š = (ğµ, ğ‘†in, ğ‘†out) specifies the batch size ğµ, input sequence length ğ‘†in, and output sequence length ğ‘†out, which collectively affect attention complexity and KV-cache access patterns during decoding. The memory surrogate ğ‘€ (ğœ½; ğ‘Š) estimates memory consumption based on model architecture and workload configuration. Problem Tractability. Due to the high dimensionality of architectural hyperparameters in modern LLMs, the search space induced by Equation 1 is prohibitively large to enumerate. As result, directly solving this constrained optimization problem is computationally infeasible in realistic deployment settings. Rather than performing brute-force search, we approximate the optimization landscape through explicit surrogate models that capture stable trends in both learning dynamics and system behavior. Specifically, in the following two subsections, we model validation loss using parametric polynomial approximation fitted from empirical training runs, and characterize inference latency via roofline-based hardware performance model. This approach enables principled and scalable identification of Pareto-optimal architectures under fixed hardware constraints. 3.2. Precision Modeling via Loss Beyond reducing the cost of hyperparameter search, our goal is to capture systematic and generalizable relationships between architectural design choices and model quality. We therefore construct an explicit analytical surrogate for validation loss, approximating the true objective in Equation 1 using empirical scaling behavior observed during training. As described in Section 4, we train 170 architectures covering both dense and MoE models and fit an empirical scaling law based on Equation 2. This polynomial approximation enables direct prediction of validation loss from architectural parameters, facilitating efficient exploration of the design space. Consistent with prior unified scaling analyses [45, 46], we model loss as separable function of architecture components. Our empirical results reveal that sparsity-driven and base-capacity terms follow different width scaling exponents, Ë†L (ğœ½) = ğœ…ğ‘™ ğ‘™ğ›¼ğ‘™ ğœ…ğœŒ ğœŒğ›¼ğœŒ ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½1 + ğœ…ğ‘‘ ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½2 + + ğœ…ğ‘š ğ‘‘ğ›¼ğ‘š ğ‘š + L, (2) where ğœ‘ = {ğœ…ğ‘™, ğœ…ğœŒ, ğœ…ğ‘‘, ğœ…ğ‘š, ğ›¼ğ‘™, ğ›¼ğœŒ, ğ›¼ğ‘Ÿ, ğ›¼ğ‘š, ğ›½1, ğ›½2, L} are fitted parameters. Note that the KV-cache dimension satisfies ğ‘‘ğ‘š = ğ‘‘/gqa, where gqa = ğ‘›â„/ğ‘›ğ‘˜ğ‘£ is the GQA group ratio; this reparameterization is used in the theoretical analysis of Section 5. 7 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs 3.3. Performance Modeling via Latency For the latency term, we derive an approximate mathematical expression grounded in roofline analysis, as latency is determined by the interaction between model compute, memory access patterns, and hardware characteristics. The hardware parameters required for this modeling include peak compute throughput and sustained memory bandwidth. While our current formulation targets conventional memory-centric accelerator architectures, it naturally generalizes to other hardware systems, which we leave for future work. In this paper, we derive first-principles latency model grounded in the roofline framework [47]. The roofline model characterizes computational kernels by arithmetic intensity = /M (FLOPs per byte). Given hardware with peak compute ğœ‹ğ» (FLOPS) and memory bandwidth ğ›½ğ» (Bytes/s), ideal latency under the theoretical roofline model satisfies: = max (cid:18) ğœ‹ğ» , ğ›½ğ» (cid:19) . (3) Here we directly give out the total inference latency under the roofline modeling. For model with ğ‘™ layers, ğ‘†in input tokens, and ğ‘†out generated tokens, the end-to-end latency ğ‘‡total, including prefill and decode, is formulated as follows. Ë†ğ‘‡ğœ½ = ğ‘‡total(ğ‘†in, ğ‘†out) = ğ‘™ ğ‘‡ pre layer(ğ‘†in) + ğ‘†out ğ‘†=1 ğ‘™ ğ‘‡ dec layer(ğ‘† + ğ‘†ğ‘–ğ‘›), (4) All FLOP counts and memory-traffic analysis are derived in Appendix E. 3.4. Pareto-optimal architectures During the empirical experiments, we search for optimal LLM architectures via Pareto frontier analysis. An architecture ğœ½ is said to be Pareto-optimal if (cid:154) ğœ½ ğœ½ s.t. (ğœ½) (ğœ½ ) ğ‘‡ (ğœ½; ğ», ğ‘Š) ğ‘‡ (ğœ½ ; ğ», ğ‘Š), (5) with at least one inequality strict. The set of all such ğœ½ defines the Pareto frontier in the losslatency plane. In the following section, we present empirical experiments for Pareto-optimal architecture discovery under fixed hardware and context constraints. By combining the empirical loss model (Section 3.2) with the analytical latency model (Section 3.3), we obtain two jointly optimizable objective functions that capture accuracy and inference efficiency, respectively. Within the Pareto optimality framework, we perform both empirical evaluation and analytical reasoning to identify architectures that optimally trade off validation loss and end-to-end inference latency on given hardware platform. We refer to the resulting architecture selection principle, together with its associated parameter scaling behavior under hardware constraints, as the hardware co-design scaling law. This law serves as practical guideline for selecting and deploying on-device large language models under strict latency and resource budgets. 4. Pareto-Optimal Architecture Search This section presents PLAS (Pareto-optimal LLM Architecture Search), framework that jointly models training loss and inference latency to enable hardware-aware architecture selection. We first 8 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Figure 3 Overview of Pareto-optimal LLM Architecture Search framework (PLAS). The framework integrates (1) empirical loss modeling via scaling law fitting, (2) roofline-based latency estimation, and (3) Pareto frontier construction to enable hardware-aware architecture selection. construct an empirical loss model by fitting results from 170 trained architectures to approximate validation loss without exhaustive search. We then characterize inference latency through rooflinebased analytical modeling and practical measurements on edge platforms. Finally, we integrate both models to derive Pareto frontiers and demonstrate how they guide architecture selection under different application-specific latency budgets. Figure 3 illustrates the overall workflow. 4.1. Loss Prediction via Scaling Laws Obtaining high-fidelity parametric scaling law is non-trivial. Our fitting is grounded in 170 trained Transformer configurations spanning both sparse (MoE) and dense architectures, each trained for fixed 10B-token budget under tightly controlled settings. The architectural configurations are carefully selected to span the full design space, jointly varying depth, width, MoE sparsity, FFN expansion ratio, and KV-cache dimensions (detailed search space in Appendix A), while avoiding degenerate or ill-conditioned regimes. 4.1.1. Pre-training Protocol All models share the following training setup to ensure fair comparison: Training Data. Each configuration is trained on 10B tokens comprising mixture of general corpus, mathematical reasoning, and code data, sufficient to observe scaling behavior while remaining computationally tractable. The training corpus will be released upon publication. 9 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Optimization. All models are trained using the AdamW optimizer with ğ›½1 = 0.90, ğ›½2 = 0.95, and weight decay of 0.01. The learning rate follows cosine decay schedule from 1 104 to 1 106, with linear warmup over the first 0.2% of training steps. QK-Norm is employed to enhance training stability, particularly for MoE configurations. All experiments use global batch size of 256. Evaluation. Model performance is evaluated using upstream validation loss on held-out subset of approximately 1B tokens, averaged over the final 10 optimization steps to mitigate variance. We further assess generalization by reporting perplexity on the WikiText-2 test set. Further details on the pre-training protocol are provided in Appendix B. 4.1.2. Scaling Law Fitting Figure 4 Scaling law fit quality. Training ğ‘…2 = 0.975 (138 configurations); validation ğ‘…2 = 0.952 (32 held-out configurations). We fit parametric scaling law of the form in Equation 2 using nonlinear least squares on 120 training configurations, with 17 held-out configurations reserved for validation. This extensive and structured exploration enables stable fit with strong generalization: as shown in Figure 4, the resulting model achieves training ğ‘…2 of 0.975 and validation ğ‘…2 of 0.952. Such predictive accuracy is difficult to obtain in practice, as loss landscapes across architectural dimensions are highly non-convex and often confounded by parameter coupling effects. Despite operating over substantially more heterogeneous architectural space that includes both dense and sparse models, the fitted scaling law exhibits stable and consistent exponents across depth, width, sparsity, and FFN expansion. This level of consistency is comparable to prior empirical scaling analyses [48, 46], while achieving stronger generalization performance on held-out configurations, suggesting improved robustness beyond architecture-specific fitting. The fitted coefficients and complete functional form are provided in Appendix C. Crucially, the quality of the fit validates our central premise: architecture-level loss can be modeled explicitly and predictably when training compute, data budget, and optimization protocol are fixed, thereby enabling principled extrapolation and Pareto-optimal architecture selection under hardware constraints. 10 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs 4.2. Latency Modeling To enable efficient architecture search, we require fast and accurate latency estimation that can evaluate tens of thousands of configurations without exhaustive measurement. Our framework employs roofline-based analytical modeling as the primary evaluation backend, with empirical validation for top candidates. 4.2.1. Roofline-Based Prediction We estimate inference latency by classifying each operator as compute-bound or memory-bound based on its arithmetic intensity relative to hardware capabilities. For each operator, latency is estimated from FLOPs, memory access volume, and hardware peak throughput (compute capacity and memory bandwidth). This analytical approach enables evaluation of 50,000+ configurations in approximately 20 minutes, making it ideal for large-scale exploration. To ensure prediction fidelity, we validate top Pareto candidates via empirical measurement using the vLLM inference engine with subprocess isolation for accurate GPU memory accounting. The roofline predictions exhibit strong correlation with measured latencies (details in Appendix D), confirming the reliability of our analytical approach for architecture ranking. 4.2.2. Workload Configuration For on-device deployment targeting VLA workloads in autonomous driving, we focus on batch size ğµ = 1 with 1,024 input tokens and 16 output tokens. Under these settings: Prefill Latency scales with input sequence length due to attention computation (ğ‘‚(ğ‘†2) complexity), and is primarily compute-bound at moderate sequence lengths. Decode Latency is dominated by weight loading from memory, as each token generation requires accessing the full model weights while performing minimal computation per byte loaded. The appropriate optimization target depends on workload characteristics: decode latency for interactive or streaming applications where per-token throughput is critical, prefill latency for long-context processing with short outputs, and total end-to-end latency for balanced tasks. As we demonstrate in Section 4.3, different optimization targets yield markedly different optimal architectures, motivating our multi-objective Pareto analysis. Further details on latency scaling behavior across sequence lengths and batch sizes are provided in Appendix D. 4.3. Pareto Frontier Analysis Given explicit loss and latency models, we cast architecture selection as bi-objective optimization problem and identify Pareto-optimal designs that jointly minimize validation loss and inference latency. This formulation enables systematic exploration of the accuracyefficiency trade-off and supports principled, scenario-aware architecture selection under hardware constraints. 4.3.1. Frontier Construction Given loss predictions { Ë†L (ğœ½ğ‘–)} and latency estimates {Ë†ğ‘‡ (ğœ½ğ‘–)} for set of architectural configurations, we identify the practical Pareto frontier as: = {ğœ½ğ‘– : (cid:154) ğœ½ ğ‘— s.t. Ë†L (ğœ½ ğ‘—) < Ë†L (ğœ½ğ‘–) Ë†ğ‘‡ (ğœ½ ğ‘—) < Ë†ğ‘‡ (ğœ½ğ‘–)} (6) 11 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs We construct the Pareto frontier using an adaptive search strategy. Starting from an initial set of architectures generated via Latin hypercube sampling to ensure broad coverage of the design space, we identify the current Pareto-optimal set based on predicted loss and latency. We then iteratively refine the search by sampling new configurations in sparsely covered regions of the frontier and in local neighborhoods of Pareto-optimal points. This process repeats until the frontier stabilizes and no further improvements are observed. 4.3.2. PrecisionPerformance Trade-off Figure 5 presents Pareto frontiers under three latency objectives (prefill, decode, and total), each comparing FP16 and INT8 precision. Across all scenarios, INT8 quantization consistently shifts the frontier toward lower latency at equivalent loss, demonstrating clear efficiency gains from reducedprecision inference. Figure 5 Pareto frontiers under prefill (1,024 tokens), decode (16 tokens), and total latency optimization on NVIDIA Jetson Orin, comparing FP16 and INT8 precision. However, the observed speedup is notably less than the theoretical 2 improvement. This sublinear scaling arises from two primary factors: (1) INT8 acceleration applies only to linear operations (matrix multiplications), while non-linear componentsattention softmax, layer normalization, and activation functionsremain in higher precision; and (2) quantization and dequantization overhead at layer boundaries partially offsets the computational savings from reduced-precision arithmetic. These observations suggest that realizing the full potential of quantized inference requires co-designed architectures that minimize non-linear operation overhead and reduce precision-conversion frequencya promising direction for future work. 4.3.3. Architecture Selection Guidelines The Pareto frontier provides menu of optimal configurations for different latency budgets. We map representative latency targets to application domains in Table 2, providing practitioners with actionable guidance for architecture selection. Table 2 Latency requirements for representative edge deployment scenarios. Application Latency Target Rationale Embodied AI Smart Home Autonomous Driving Private Serving <20 ms (decode) Real-time interaction <500 ms (total) <100 ms (total) <2 (total) Conversational response Safety-critical decisions Quality-focused, on-device To select an architecture for target application, practitioners first identify the latency budget dictated by system requirements, then determine the relevant optimization objective based on 12 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs workload characteristics. The appropriate Pareto frontier is consulted to locate the configuration operating at the target latency, which by construction achieves the lowest attainable loss within that budget. The corresponding architectural parameters can then be directly read off and deployed, just like the different region in Figure 6. Figure 6 Different applications live in completely different regions of the Pareto frontier. 4.3.4. Architecture Parameter Evolution Figure 7 traces how Pareto-optimal architectures evolve as the latency budget increases across different optimization objectives. Several key patterns emerge from this analysis. MoE Dominance. Sparse MoE architectures constitute 100% of Pareto-optimal configurations across all latency regimes. Under the batch-one constraint typical of on-device deployment, MoE models achieve superior efficiency over dense counterparts: they provide greater model capacity (total parameters) while maintaining comparable activated parameters per token, yielding better loss-per-FLOP trade-offs. This finding strongly motivates the adoption of sparse architectures for edge deployment scenarios. In contrast to conventional LLM designs that favor deep, narrow Wide-and-Shallow Preference. architectures, the Pareto-optimal configurations exhibit distinctive wide-and-shallow pattern: depth remains relatively constrained (generally below 20 layers) while width is substantially larger than comparably-sized models. Both dimensions increase with the latency budget, but width saturates earlier at the search space upper bound, after which additional capacity is allocated to depth. This pattern suggests that under strict latency constraints, width provides more efficient loss reduction per unit latency than deptha finding with important implications for on-device model design. Phase-Dependent Expert Configuration. The optimal MoE configuration differs markedly between prefill and decode phases, driven by their distinct computational characteristics: Prefill phase: With relatively few input tokens per expert in on-device scenarios, increasing the number of experts requires loading more parameters without proportional compute utilization, shifting the bottleneck from compute-bound to memory-bound operation and degrading hardware Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Figure 7 Architecture parameter evolution along Pareto frontiers under prefill-optimized (top), decode-optimized (middle), and total latency-optimized (bottom) objectives, comparing FP16 and INT8 precision. As the latency budget increases, optimal configurations exhibit systematic shifts in depth, width, expert count, and FFN expansion ratio. efficiency. Consequently, prefill-optimized configurations favor fewer experts, with the expert count increasing gradually only as the latency budget relaxes. Decode phase: At batch size one, each token activates fixed subset of experts, so increasing the total number of experts incurs negligible additional latency while substantially expanding model capacity. Decode-optimized configurations therefore favor maximizing the expert count within the search space. Routing strategy: Both phases consistently prefer Top-ğ¾=1 routing, as activating multiple experts per token substantially increases memory bandwidth consumption during the memory-bound decode phase. Balanced Configuration under Total Latency. When optimizing for total end-to-end latency, the optimal expert count reflects trade-off between prefill and decode contributions. Prefill-dominated workloads (long input, short output) favor fewer experts; decode-dominated workloads (short input, long generation) favor more experts. Under balanced inputoutput ratios typical of many practical applications, the optimal configuration converges to moderate expert count (typically around 8), consistent with the design choices observed in recent production models [23, 49]. Compact FFN Expansion. Notably, the optimal FFN expansion ratio under on-device constraints is substantially smaller than the conventional 4 used in standard Transformer designs. In many Pareto-optimal configurations, ratios below 1 emerge as viable design choices, suggesting that reallocating parameters from FFN width to other dimensions (e.g., more experts or increased model width) yields better efficiency under memory-constrained inference. 4.3.5. Empirical Validation To validate the practical benefits of hardware-aware architecture selection, we conduct an empirical comparison against an existing production model. Using vLLM, we first measure the inference latency 14 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs (a) Pareto frontier (b) Training dynamics Figure 8 Empirical validation on NVIDIA Jetson Orin. (a) Pareto frontier with the co-designed model and Qwen2.5-0.5B marked. (b) Training loss curves showing faster convergence for the Pareto-optimal architecture. of Qwen2.5-0.5B on the target hardware (NVIDIA Jetson Orin), then identify Pareto-optimal architecture from our framework that matches this measured latency, as shown in Figure 8(a). Both models are trained using an identical data mixture and optimization protocol to ensure fair comparison. Figure 8(b) shows that the co-designed architecture achieves consistently lower training loss throughout optimization, indicating better utilization of model capacity under the same computational budget. For downstream evaluation, we measure perplexity on WikiText-2 after training: the codesigned architecture achieves 19.42% lower perplexity compared to Qwen2.5-0.5B (50.88 vs. 63.14). This substantial improvement at equivalent inference latency demonstrates that hardware-aware architecture selection yields measurable quality gains without sacrificing deployment efficiency, validating the practical utility of the PLAS framework. 4.3.6. Summary of Findings We summarize the key findings from our Pareto analysis: Sparse architectures dominate. MoE configurations constitute 100% of Pareto-optimal designs under on-device batch-one inference, providing superior capacity-efficiency trade-offs. Wide-and-shallow designs are preferred. Optimal architectures are wider and shallower than conventional designs at equivalent latency, with width providing more efficient loss reduction under tight constraints. Phase-specific expert configuration. Prefill and decode phases demand opposing expert configurations; total-latency optimization requires balancing both contributions. Compact FFN expansion. The optimal FFN expansion ratio is substantially smaller than the conventional 4, with ratios below 1 emerging as viable choices. Quantization helps but sub-linearly. INT8 quantization consistently improves the Pareto frontier, though gains are sub-linear due to non-linear operation and precision-conversion overhead. 15 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs No universal optimal architecture. Optimal designs are hardwareand workload-specific; architectures do not transfer across platforms or deployment scenarios. These observations provide actionable guidance for practitioners designing on-device LLMs, while the complete Pareto frontier enables precise architecture selection for specific deployment constraints. The PLAS framework and trained model checkpoints will be released to facilitate further research in hardware-aware neural architecture design. 5. Theoretical Framework for Hardware-Aware Architecture Optimization 5.1. From Empirical Search to Principled Optimization Section 4.3 empirically discovered Pareto frontiers through large-scale search over 1,942 architectures. While effective, this approach raises fundamental questions: Can we predict optimal architectures without exhaustive search? What structural principles govern the Pareto frontier? How do solutions generalize to new hardware platforms? This section addresses these questions by developing theoretical framework that derives closedform solutions for optimal architectures under different hardware constraint regimes. Rather than treating Pareto-optimal designs as empirical outcomes, we formalize architecture selection as an explicit constrained optimization problem. Key Insight. Different hardware constraint regimes induce qualitatively distinct optimal solutions, particularly in how sparsity (MoE activation rate ğœŒ) should be allocated. This explains why certain architectural patterns consistently emerge on the empirical Pareto frontier. 5.2. Problem Formulation and Constraint Types We formalize the hardware co-design problem as: min ğœ½ Ë†ğ¿(ğœ½) subject to Ë†ğ‘‡ (ğœ½; ğ», ğ‘Š) ğ‘‡lat, Ë†ğ‘€ (ğœ½) ğ‘€budget (7) where ğœ½ = (ğ‘™, ğ‘‘, ğ‘Ÿ, ğœŒ, gqa) denotes depth, width, FFN ratio, activation rate, and GQA ratio. Building on roofline analysis (Appendix E), we identify three constraint types. The prefill constraint (compute-bound) takes the form ğ‘™ ğœ‰ğ¹ ğ‘‘2 ğ¹ ğ‘ where ğœ‰ğ¹ = 4 + 4/gqa + 6ğ‘Ÿ. The decode constraint (bandwidth-bound) includes both weight loading and KV-cache access: ğ‘™ (ğœ‰dec ğ‘Š ğ‘‘2ğ‘ğ‘¤ + 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£/gqa) ğ‘€ğ‘‘ where ğœ‰dec ğ‘Š = 2 + 2/gqa + 3ğ‘Ÿ. The memory constraint (storage-bound) accounts for ğ‘Š = 2 + 2/gqa + 3ğ‘Ÿ/ğœŒ. Here ğ‘† = ğ‘†in + (ğ‘†out + 1)/2 all model parameters: ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ ğ‘€budget where ğœ‰all denotes the average context length during decoding. Key Observation. The activation rate ğœŒ appears only in ğœ‰all ğ‘Š , reflecting that sparsity affects storage but not per-token computation. This asymmetry drives our main theoretical results. 5.3. Optimal Activation Rate Across Constraint Regimes We characterize optimal activation rates ğœŒ for three canonical regimes: latency-only (inference speed-limited with ample memory), memory-only (storage-limited with sufficient compute), and dualconstrained (tightly coupled hardware limits). These regimes naturally arise in different deployment scenarios: edge devices are often memory-constrained, automotive platforms are typically latencyconstrained, and embedded systems are frequently dual-constrained. 16 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Theorem 5.1 (Latency-Constrained Regime). When only latency constraints are active (memory unconstrained): ğœŒ = ğœŒmin (8) Proof Sketch. Since ğœ‰ğ¹/ğœŒ = ğœ‰dec Thus ğœŒ occurs at the boundary. Full proof in Appendix and Appendix I. ğ‘Š /ğœŒ = 0, the Lagrangian gradient L/ğœŒ = Ë†ğ¿/ğœŒ > 0 everywhere. Interpretation. Under latency constraints, MoE sparsity provides free lunch: reducing ğœŒ (activating fewer experts) decreases loss without increasing per-token latency, since only ğ¾ experts are computed regardless of total pool size ğ¸. The optimal strategy is therefore to maximize sparsity (minimize ğœŒ) within the fixed latency budget. For latency-critical applications such as autonomous driving with sub-50ms requirements, this suggests preferring top-1 routing and increasing the total expert count as much as memory permits. Theorem 5.2 (Memory-Constrained Regime). When only memory constraint is active (latency unconstrained): ğ‘‘ ( ğ›½1 ğ›½2 )/ğ›¼ğœŒ (9) valid when ğ›¼ğœŒ > ğ›¼ğ‘Ÿ. ğœŒ = (cid:20) ğ›¼ğ‘Ÿğœ…ğ‘‘ (ğ›¼ğœŒ ğ›¼ğ‘Ÿ)ğœ…ğœŒ (cid:21) 1/ğ›¼ğœŒ Proof Sketch. Eliminating the Lagrange multiplier from KKT conditions for ğœŒ and ğ‘Ÿ yields an algebraic relation. Substituting the loss function and solving gives Equation 9. Complete derivation in Appendix and Appendix . Corollary 1 (Width-Sparsity Scaling Law). Under memory constraints: ğœŒ ğ‘‘ ( ğ›½1 ğ›½2 )/ğ›¼ğœŒ. With fitted exponents (ğ›½1 0.33, ğ›½2 0.97, ğ›¼ğœŒ 1.09), this implies wider models should use sparser MoE. Interpretation. Memory-constrained systems face fundamental trade-off: storing all ğ¸ experts costs proportional to 1/ğœŒ, but increased sparsity provides capacity gains that benefit wider models more than narrower ones. Equation 9 characterizes the optimal balance point. The width-sparsity coupling arises because the sparsity capacity term in the loss has negative width exponent ğ›½1 < 0 (making sparsity more valuable for wider models), while the base capacity term has positive exponent ğ›½2 > 0 (meaning dense capacity scales favorably with width). For practical deployment on memorylimited devices with 48 GB DRAM, 2B-parameter model with ğ‘‘ 2048 should use ğœŒ 0.15 (e.g., ğ¾ = 2, ğ¸ = 16), while 500M-parameter model with ğ‘‘ 1024 should use denser MoE at ğœŒ 0.25. Theorem 5.3 (Dual-Constrained Regimes). When both latency and memory constraints are active, the optimal activation rate depends on which latency phase is limiting. (a) Prefill + Memory: ğœŒ = 3ğœ‚ ğ‘ğ‘ğ‘¤ğ‘Ÿ ğ›¼attn(2 ğœ‚ ğ‘ğ‘ğ‘¤) + 6ğ‘Ÿ , ğœ‚ ğ‘ = ğ¹ ğ‘/ğ‘€budget, ğœ‚ ğ‘ğ‘ğ‘¤ < 2 (b) Decode + Memory: ğœŒ = 3ğ‘Ÿ ğœ‰ ğ‘€ ğ›¼attn , ğœ‰ ğ‘€ = (ğ›¼attn + 3ğ‘Ÿ) + (ğ›¼attn + 3ğ‘Ÿ)2 + 4ğœ‚ğ›¿ 2ğœ‚ (10) (11) where ğœ‚ = ğ‘€ğ‘‘/ğ‘€budget, ğ›¼attn = 2 + 2/gqa, and ğ›¿ = 2ğ‘†ğ‘ğ‘˜ğ‘£/(gqa ğ‘‘ ğ‘ğ‘¤) is the KV-cache correction. Here (cid:12) ğ‘€ ğœ‰all ğœ‰ (cid:12)ğœŒ=ğœŒ denotes the equilibrium value of the storage coefficient (defined in Appendix E) under the dual constraint, obtained by solving quadratic system. ğ‘Š 17 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Proof Sketch. Constraint compatibility requires ğœ‰ğ¹/(ğ‘ğ‘¤ğœ‰all ğ‘Š ) = ğœ‚ ğ‘ (prefill case) or the decode analog. Solving these algebraic equations yields the stated forms. The decode case involves quadratic due to the KV-cache term. Comparison. The prefill+memory case admits simple closed form, while decode+memory requires solving quadratic due to KV-cache coupling. The key difference is that the decode constraint includes term proportional to ğ‘†/gqa absent in the prefill case, creating stronger coupling between gqa and the latency budget. For systems with tight coupling between latency and memory such as automotive SoCs, practitioners should compute the constraint ratio ğœ‚ or ğœ‚ ğ‘ and apply the corresponding formula, verifying that ğœŒmin ğœŒ 1. Table 3 provides cross-reference between the theoretical results presented in this section and their complete derivations in the appendices. Table 3 Cross-reference of theoretical results and detailed proofs. Result Theorem 5.1 Theorem 5.2, Corollary 1 Equation 10 in Theorem 5.3 Equation 11 in Theorem 5.3 Equation 12Equation 13 Detailed Proof Main Result ğœŒ = ğœŒmin ğœŒ ğ‘‘ ( ğ›½1 ğ›½2 )/ğ›¼ğœŒ ğœŒ prefill+memory ğœŒ decode+memory ğ‘Ÿ, gqa closed forms Appendix Appendix (D1), Appendix (P1) Appendix (D2), Appendix (P2) Appendix (P3) Appendix (D3) 5.4. Optimal Depth, FFN Ratio, and GQA The optimal depth always saturates the active constraint, taking the form ğ‘™ = ğ¹ ğ‘/(ğœ‰ğ¹ğ‘‘2) for prefill, ğ‘™ = ğ‘€ğ‘‘/(ğœ‰eff ğ‘Š ğ‘‘2ğ‘ğ‘¤) for memory constraints. This implies fundamental depth-width trade-off: ğ‘™ ğ‘‘ 2 at fixed budget, explaining the inverse scaling behavior observed along empirical Pareto frontiers. ğ‘Š ğ‘‘2ğ‘ğ‘¤) for decode, or ğ‘™ = ğ‘€budget/(ğœ‰all The optimal FFN ratio and GQA ratio follow structured closed forms. We define the aggregate loss gradient ğ· ğœ…ğœŒğœŒğ›¼ğœŒ ğ‘‘ ğ›½2 ğ›½1 + ğœ…ğ‘‘, which combines the sparsity and base capacity contributions. Under prefill latency constraints, the solutions are: ğ‘Ÿ = gqa = (cid:34) ğ›¼ğ‘Ÿ ğ· 6ğ›¼ğ‘™ğœ…ğ‘™ (cid:34) 4ğ›¼ğ‘™ğœ…ğ‘™ ğ›¼ğ‘šğœ…ğ‘š ğœ‰ğ›¼ğ‘™ 1 ğ¹ ğœ‰ğ›¼ğ‘™ 1 ğ¹ ğ¹ğ›¼ğ‘™ ğ‘ ğ‘‘2ğ›¼ğ‘™+ğ›½2 ğ‘‘2ğ›¼ğ‘™+ğ›¼ğ‘š ğ¹ğ›¼ğ‘™ ğ‘ (cid:35) 1/(ğ›¼ğ‘Ÿ+1) (cid:35) 1/(ğ›¼ğ‘š+1) (12) (13) Other constraint regimes share the same structural form with modified coefficients and budget terms. Under decode latency constraints, the per-layer constraint coefficient generalizes to Î“ ğ‘Š ğ‘‘2ğ‘ğ‘¤ + 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£/gqa, which accounts for KV-cache bandwidth; the ğ‘Ÿ coefficient changes from 1/6 ğœ‰dec to 1/3 (since ğœ‰dec ğ‘Š /ğ‘Ÿ = 3 vs. ğœ‰ğ¹/ğ‘Ÿ = 6), and the gqa formula acquires an additional KV-cache coupling factor (ğ‘‘2ğ‘ğ‘¤ + ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£). Under memory constraints, an extra ğœŒ factor enters the ğ‘Ÿ numerator. Table 4 summarizes the key differences; complete formulas for all regimes are provided in Appendix L. The 2 difference in coefficients between prefill and decode (e.g., ğ¶ğ‘Ÿ = 6 vs. 3) arises from the fundamental relation ğœ‰ğ¹ = 2ğœ‰dec ğ‘Š , which reflects the FLOPs-to-memory-access ratio: each multiplyaccumulate operation counts as 2 FLOPs but requires loading each weight parameter only once. Note that the decode ğ¶ğ‘” = 2 in Table 4 reflects only the weight-loading contribution; the full decode 18 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Table 4 Coefficient comparison across constraint regimes. ğ¶ğ‘Ÿ and ğ¶ğ‘” denote the constraint-derivative coefficients for ğ‘Ÿ and gqa, respectively. Regime ğœŒ Prefill Latency Decode Latency Memory ğœŒmin ğœŒmin Equation 9 ğ¶ğ‘Ÿ 6 3 3/ğœŒ ğ¶ğ‘” Budget Constraint Coeff. 4 2 2 ğ‘€budget ğœ‰ğ¹ Î“ ğ‘Š ğ‘‘2ğ‘ğ‘¤ ğœ‰all ğ¹ ğ‘ ğ‘€ğ‘‘ Prefill + Mem Equation 10 Decode + Mem Equation 11 see Appendix see Appendix GQA derivative additionally includes KV-cache correction term 2ğ‘†ğ‘ğ‘˜ğ‘£/(ğ‘‘ ğ‘ğ‘¤) (see Appendix for the complete expression). This structural asymmetry has important practical implications: prefilloptimized models should use smaller FFN ratios and larger GQA values compared to decode-optimized models at equivalent performance levels. 5.5. Design Principles and Practical Guidelines 5.5.1. Key Structural Insights Four structural results emerge from the theoretical analysis. First, memory-constrained solutions exhibit scenario independence: the optimal parameters (ğœŒ, ğ‘™, ğ‘Ÿ, gqa) are identical for prefill and decode phases, since memory constraints concern model storage rather than inference dynamics. Second, prefill versus decode constraints induce coefficient asymmetry, with 2 differences in FFN and GQA coefficients stemming from ğœ‰ğ¹ = 2ğœ‰dec ğ‘Š . Third, decode constraints exhibit KV-cache coupling through term proportional to ğ‘†/gqa that creates sequence-length dependence absent in prefill constraints. Fourth, the width-sparsity scaling law ğœŒ ğ‘‘ 1.19 implies that doubling model width should reduce activation rate by approximately 2.3, providing principled basis for allocating sparsity in memory-limited deployments. 5.5.2. Actionable Design Guidelines Sparsity Allocation Strategy. The optimal sparsity allocation depends critically on the active constraint regime. For latency-bound systems, practitioners should maximize sparsity by setting ğœŒ = ğœŒmin (typically top-1 routing with ğ¾ = 1). For memory-bound systems, the width-sparsity scaling law (Equation 9) provides the principled allocation: wider models require sparser MoE configurations to balance capacity gains against storage costs. For dual-constrained systems, the regime-specific formulas (Equation 10 or Equation 11) should be applied after computing the constraint ratio ğœ‚ or ğœ‚ ğ‘ to determine which formula is appropriate. Depth-First Budget Allocation. The ğ‘™ ğ‘‘ 2 relationship suggests systematic allocation strategy. Practitioners should first select target width ğ‘‘ based on the parameter budget and width-sparsity law, then compute optimal depth ğ‘™ to saturate the active constraint using the appropriate formula from Section 5.4. If the computed ğ‘™ exceeds the architectural search space upper bound (e.g., ğ‘™ > 48 layers), the width ğ‘‘ should be reduced iteratively until feasible depth is obtained. This depth-first strategy aligns with the empirical observation that depth grows monotonically along Pareto frontiers until reaching architectural limits. Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Phase-Aware Parameter Tuning. The coefficient asymmetries in Table 4 translate directly into optimization strategies. For prefill-dominant workloads such as long-context question answering, models should employ smaller FFN ratios (exploiting the 1/6 coefficient versus 1/3 in decode) and larger GQA values (more KV heads) to amortize projection costs, while ignoring KV-cache overhead in the optimization. Conversely, for decode-dominant workloads such as chatbots and code generation, models should use larger FFN ratios and carefully balance GQA against KV-cache bandwidth, as the decode gqa formula includes sequence-length-dependent KV-cache correction (see Appendix L). For balanced workloads with mixed prefill and decode phases, practitioners should optimize for total end-to-end latency or defer to memory-constrained formulas if storage is the primary limitation. Generalization to New Hardware Platforms. One primary motivation for this theoretical framework is enabling efficient architecture search on new hardware platforms without repeating exhaustive empirical evaluation. For new platform with specifications (ğœ‹ğ», ğ›½ğ», ğ‘€budget), the deployment workflow proceeds as follows. First, measure the hardware parameters: peak compute ğœ‹ğ», sustained memory bandwidth ğ›½ğ», and available memory ğ‘€budget. Second, define application requirements including target latency budgets ğ‘‡ pre and workload configuration (ğµ, ğ‘†in, ğ‘†out). Third, compute lat normalized budgets ğ¹ ğ‘ = ğ‘‡ pre ğ›½ğ»/ğ‘†out. Fourth, determine the active lat constraint regime by computing ratios ğœ‚ ğ‘ = ğ¹ ğ‘/ğ‘€budget and ğœ‚ = ğ‘€ğ‘‘/ğ‘€budget: if either ratio is much less than 1, the system is memory-constrained; if much greater than 1, it is latency-constrained; otherwise, it is dual-constrained. Fifth, apply the corresponding theorem to predict optimal parameters ğœ½ and round to feasible discrete values. Finally, validate predictions with 35 small-scale training runs (12B tokens each) to measure actual latency and refine if systematic bias is observed. ğœ‹ğ»/(ğµğ‘†in) and ğ‘€ğ‘‘ = ğ‘‡ dec lat , ğ‘‡ dec lat This workflow reduces architecture selection time from months (full empirical search) to under one week (theoretical prediction plus small-scale validation), as demonstrated in our deployment case study. As concrete example, consider deploying on new edge device with 10 TOPS compute, 50 GB/s bandwidth, 4 GB memory, and target decode latency below 100ms for single-token generation. Computing ğ‘€ğ‘‘ = 0.1 50/10 = 0.5 GB and the ratio ğœ‚ = 0.5/4 = 0.125 < 1 identifies this as memory-constrained regime. Applying Theorem 5.2 for width ğ‘‘ = 1024 predicts ğœŒ 0.20. Training targeted 20-layer, 1024-width MoE model with ğ¾ = 2, ğ¸ = 10 validates these predictions without evaluating thousands of candidate architectures. 5.5.3. Limitations and Future Extensions The theoretical framework rests on three key assumptions that bound its applicability. First, the fitted loss scaling law (Equation 2) is based on 170 architectures trained for 10B tokens; extrapolation to significantly different training budgets or data distributions may reduce prediction accuracy and should be validated empirically. Second, the latency model assumes idealized roofline behavior (Equation 3), whereas real systems exhibit kernel launch overhead, cache effects, and operator fusion that may cause deviations of 1020% from theoretical predictions. Third, the framework assumes standard transformer components including attention, FFN, and MoE; extensions to hybrid architectures such as SSM-Transformer blends or linear attention mechanisms would require re-deriving constraint forms and may exhibit qualitatively different scaling behaviors. Future work could address these limitations by incorporating training dynamics (learning rate schedules, optimizer state) into the loss model, developing more refined latency models that account for operator fusion and system-level effects, extending the theory to hybrid architectures and emerging attention mechanisms, and validating the framework across broader range of hardware platforms including TPUs and specialized AI accelerators. Nonetheless, the current framework represents significant advance toward principled, hardware-aware LLM architecture design grounded in explicit 20 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs optimization theory rather than pure empirical search. 5.6. Summary This section developed comprehensive theoretical framework for hardware-aware architecture optimization. Theorem 5.1 through Theorem 5.3 characterize optimal activation rates ğœŒ across latency-only, memory-only, and dual-constrained regimes, revealing that different hardware constraints induce qualitatively different optimal solutions. The width-sparsity scaling law (Corollary 1) establishes that ğœŒ ğ‘‘ 1.19, providing principled basis for allocating sparsity in memory-constrained settings. Optimal depth, FFN ratio, and GQA configurations expose structural asymmetries between prefill-optimized and decode-optimized architectures, with 2 coefficient differences arising from the fundamental FLOPs-to-memory-access ratio. The derived design principles enable rapid architecture selection on new hardware platforms, reducing deployment time from months to under one week. The theoretical predictions align closely with empirical Pareto frontiers discovered in Section 4.3, validating the framework while providing deeper insight into why certain architectural patterns emerge as optimal. Complete proofs and detailed parameter formulas are provided in Appendix through Appendix L. 6. Conclusion We present hardware-aligned framework that connects transformer architecture to model quality and end-to-end inference efficiency via equivalent-parameter scaling and hardware-aware latency modeling. Evaluating 1,942 architectures, we identify clear structural principles and Pareto-optimal regimes that govern the trade-off between loss, latency, and roofline efficiency. Our results show that effective LLM deployment on edge and embedded accelerators requires explicit hardware-model co-design. Beyond heuristic selection, the proposed hardware co-design scaling law reveals stable and predictable relationship between architecture and hardware constraints, enabling principled extrapolation of optimal designs across deployment regimes. 21 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs"
        },
        {
            "title": "References",
            "content": "[1] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [2] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023. [3] Ranjan Sapkota, Yang Cao, Konstantinos Roumeliotis, and Manoj Karkee. Visionlanguage-action models: Concepts, progress, applications and challenges. arXiv preprint arXiv:2505.04769, 2025. [4] Yupeng Zhou, Can Cui, Juntong Peng, Zichong Yang, Juanwu Lu, Jitesh Panchal, Bin Yao, and Ziran Wang. hierarchical test platform for vision language model (vlm)-integrated real-world autonomous driving. ACM Transactions on Internet of Things, 2025. [5] Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. Llm4drive: survey of large language models for autonomous driving. arXiv preprint arXiv:2311.01043, 2023. [6] Cong Guo, Feng Cheng, Zhixu Du, James Kiessling, Jonathan Ku, Shiyu Li, Ziru Li, Mingyuan Ma, Tergel Molom-Ochir, Benjamin Morris, et al. survey: Collaborative hardware and software design in the era of large language models. IEEE Circuits and Systems Magazine, 25(1):3557, 2025. [7] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905, 2024. [8] Cheng Deng, Luoyang Sun, Jiwen Jiang, Yongcheng Zeng, Xinjian Wu, Wenxin Zhao, Qingfa Xiao, Jiachuan Wang, Haoyang Li, Lei Chen, et al. Plm: Efficient peripheral language models hardware-co-designed for ubiquitous computing. arXiv preprint arXiv:2503.12167, 2025. [9] Samuel Webb Williams, Andrew Waterman, and David Patterson. Roofline: An insightful visual performance model for floating-point programs and multicore architectures. Technical report, Technical Report UCB/EECS-2008-134, EECS Department, University of . . . , 2008. [10] Hahnyuan. Llm-viewer: Large language model visualization toolkit. https://github.com/ hahnyuan/LLM-Viewer, 2024. [11] Song Bian, Minghao Yan, and Shivaram Venkataraman. Scaling inference-efficient language models. arXiv preprint arXiv:2501.18107, 2025. [12] Song Bian, Tao Yu, Shivaram Venkataraman, and Youngsuk Park. Scaling laws meet model architecture: Toward inference-efficient llms. arXiv preprint arXiv:2510.18245, 2025. [13] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: survey. Journal of Machine Learning Research, 20(55):121, 2019. [14] An-Chieh Cheng, Jin-Dong Dong, Chi-Hung Hsu, Shu-Huan Chang, Min Sun, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, and Da-Cheng Juan. Searching toward pareto-optimal deviceaware neural architectures. In Proceedings of the international conference on computer-aided design, pages 17, 2018. 22 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs [15] Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016. [16] Yihua Cheng, Yuhan Liu, Jiayi Yao, Yuwei An, Xiaokun Chen, Shaoting Feng, Yuyang Huang, Samuel Shen, Kuntai Du, and Junchen Jiang. Lmcache: An efficient kv cache layer for enterprisescale llm inference. arXiv preprint arXiv:2510.09665, 2025. [17] Jingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, and Shiyu Chang. Kvlink: Accelerating large language models via efficient kv cache reuse. arXiv preprint arXiv:2502.16002, 2025. [18] Eldar Kurtic, Alexandre Noll Marques, Shubhra Pandit, Mark Kurtz, and Dan Alistarh. give me bf16 or give me death? accuracy-performance trade-offs in llm quantization. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2687226886, 2025. [19] Vage Egiazarian, Roberto Castro, Denis Kuznedelev, Andrei Panferov, Eldar Kurtic, Shubhra Pandit, Alexandre Marques, Mark Kurtz, Saleh Ashkboos, Torsten Hoefler, et al. Bridging the gap between promise and performance for microscaling fp4 quantization. arXiv preprint arXiv:2509.23202, 2025. [20] Haofei Yin, Mengbai Xiao, Tinghong Li, Xiao Zhang, Dongxiao Yu, and Guanghui Zhang. Specpipe: Accelerating pipeline parallelism-based llm inference with speculative decoding, 2025. [21] Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, and Dhabaleswar Panda. Characterizing communication patterns in distributed large language model inference. In 2025 IEEE Symposium on High-Performance Interconnects (HOTI), pages 111. IEEE, 2025. [22] Yehui Tang, Kai Han, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, and Yunhe Wang. Rethinking optimization and architecture for tiny language models. In Forty-first International Conference on Machine Learning, 2024. [23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [24] Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b and gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. [25] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane RiviÃ¨re, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [26] Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, et al. Kimi linear: An expressive, efficient attention architecture. arXiv preprint arXiv:2510.26692, 2025. [27] MiniMax-AI. MiniMax-M2, model built for Max coding and agentic workflows. https: //github.com/MiniMax-AI/MiniMax-M2, 2025. [28] Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, et al. Nvidia nemotron 3: Efficient and open intelligence. arXiv preprint arXiv:2512.20856, 2025. 23 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs [29] Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, et al. Gated attention for large language models: Non-linearity, sparsity, and attention-sink-free. arXiv preprint arXiv:2505.06708, 2025. [30] Luoyang Sun, Cheng Deng, Jiwen Jiang, Xinjian Wu, Haifeng Zhang, Lei Chen, Lionel Ni, and Jun Wang. Gta: Grouped-head latent attention. arXiv preprint arXiv:2506.17286, 2025. [31] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [32] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. [33] Elie Bakouch, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Lewis Tunstall, Carlos Miguel PatiÃ±o, Edward Beeching, Aymeric Roucher, Aksel Joonas Reedi, Quentin GallouÃ©dec, Kashif Rasul, Nathan Habib, ClÃ©mentine Fourrier, Hynek Kydlicek, Guilherme Penedo, Hugo Larcher, Mathieu Morlon, Vaibhav Srivastav, Joshua Lochner, Xuan-Son Nguyen, Colin Raffel, Leandro von Werra, and Thomas Wolf. SmolLM3: smol, multilingual, long-context reasoner. https: //huggingface.co/blog/smollm3, 2025. [34] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. [35] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [36] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. [37] MLC team. MLC-LLM, 2023-2025. [38] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer: Fast large language model serving with consumer-grade gpu. In Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles, pages 590606, 2024. [39] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 86978710, 2018. [40] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In International Conference on Learning Representations, 2019. [41] Princeton University. Llmcompass: Large language model performance profiling. https: //github.com/PrincetonUniversity/LLMCompass, 2024. [42] Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efficiently: Insights from pretraining and fine-tuning transformers. In International Conference on Learning Representations, 2022. 24 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs [43] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [44] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [45] Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International conference on machine learning, pages 40574086. PMLR, 2022. [46] Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej PiÃ³ro, MichaÅ‚ Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian KrÃ³l, Tomasz OdrzygÃ³ÅºdÅº, Piotr Sankowski, et al. Scaling laws for fine-grained mixture of experts. arXiv preprint arXiv:2402.07871, 2024. [47] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: An insightful visual performance model for multicore architectures. Communications of the ACM, 52(4):6576, 2009. [48] Samira Abnar, Harshay Shah, Dan Busbridge, Alaaeldin Mohamed Elnouby Ali, Josh Susskind, and Vimal Thilak. Parameters vs flops: Scaling laws for optimal sparsity for mixture-of-experts language models. arXiv preprint arXiv:2501.12370, 2025. [49] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [50] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico LebrÃ³n, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [51] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [52] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [53] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. 25 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs A. Architecture Search Space Table 5 specifies the architectural hyperparameters explored in our scaling law study. Table 5 Architecture search space for scaling law fitting. Parameter Values Depth ğ‘™ Width ğ‘‘ MoE (ğ¸, ğ¾) GQA ğ‘›ğ‘˜ğ‘£ {4, 8, 12, 16, 20, 24, 28, 32} {768, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 3072} {(1, 1), (8, 1), (8, 2), (16, 1), (16, 2)} {1, 2, 4, 8, ğ‘›â„} The search space jointly covers depth (432 layers), width (7683072 hidden dimensions), MoE configurations (dense to 16 experts with Top-1/Top-2 routing), and grouped-query attention settings. This design ensures coverage of both dense and sparse architectures while avoiding degenerate or ill-conditioned regimes. B. Pre-training Details All 170 model configurations are trained under identical conditions to ensure fair comparison: Training Data. Each model is trained on 10B tokens from mixture of general corpus, mathematics, and code data. This budget is sufficient to observe scaling behavior while remaining computationally tractable. The training corpus will be released upon publication. Optimization. We use the AdamW optimizer with ğ›½1 = 0.9, ğ›½2 = 0.95, and weight decay 0.01. The learning rate follows cosine decay schedule from 1 104 to 1 106, with linear warmup over the first 0.2% of training steps. QK-Norm is applied to stabilize MoE pre-training. All experiments use global batch size of 256. Evaluation. Model performance is measured by validation loss on held-out set of approximately 1B tokens, averaged over the final 1,000 optimization steps to reduce variance. We additionally report perplexity on WikiText-2 for downstream evaluation. Computational Cost. Each configuration is trained on 8 NVIDIA H200 GPUs for approximately 10 hours, totaling 170 8 10 = 13,600 GPU-hours for the full study. C. Scaling Law Coefficients The fitted parametric loss model takes the following form: Ë†L (ğœ½) = + ğœ…ğ‘™ ğ‘™ğ›¼ğ‘™ (cid:124)(cid:123)(cid:122)(cid:125) depth ğœ…ğœŒğœŒğ›¼ğœŒ ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½1 (cid:32) (cid:32) (cid:123)(cid:122) (cid:125) (cid:124) sparsity-width + + ğœ…ğ‘‘ ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½2 (cid:32) (cid:32) (cid:124) (cid:125) (cid:123)(cid:122) capacity ğœ…ğ‘š ğ‘‘ğ›¼ğ‘š ğ‘š (cid:124)(cid:123)(cid:122)(cid:125) KV-cache +L Table 6 reports the fitted coefficients. The resulting concrete form is: (14) 26 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Ë†L (ğœ½) = 9.96 ğ‘™1.63 + 0.031 ğœŒ1.09 ğ‘Ÿ0.17 ğ‘‘ 0.33 + 500 ğ‘Ÿ0.17 ğ‘‘0.97 + 0.20 ğ‘‘0.05 ğ‘š + 2.53 (15) Table 6 Fitted scaling law coefficients. Term Depth Sparsity Capacity FFN ratio KV-cache Irreducible Coefficient ğœ…ğ‘™ = 9.96 ğœ…ğœŒ = 0.031 ğœ…ğ‘‘ = 500 ğœ…ğ‘š = 0.20 Exponent ğ›¼ğ‘™ = 1.63 Interpretation Strong depth dependence ğ›¼ğœŒ = 1.09, ğ›½1 = 0.33 Width-sparsity coupling ğ›½2 = 0.97 ğ›¼ğ‘Ÿ = 0.17 ğ›¼ğ‘š = 0.05 Base capacity scaling Unified FFN scaling Cache efficiency = 2.53 The exponents reveal several insights: (1) depth exhibits the strongest scaling (ğ›¼ğ‘™ = 1.63), indicating high sensitivity to layer count; (2) width and sparsity are coupled (ğ›½1 = 0.33): the sparsity-related loss penalty grows with width, implying that wider models require greater sparsity (lower ğœŒ) to remain competitive; (3) FFN expansion ratio contributes modestly (ğ›¼ğ‘Ÿ = 0.17); and (4) KV-cache configuration has minimal impact on loss (ğ›¼ğ‘š = 0.05) but significantly affects inference efficiency. D. Latency Modeling Details Roofline-Based Prediction. We analytically model latency using hardware roofline analysis. Each operator is classified as compute-bound or memory-bound based on its arithmetic intensity ğ¼ = FLOPs/Bytes relative to the hardwares compute-to-bandwidth ratio. Latency is estimated as: ğ‘‡op = max (cid:18)"
        },
        {
            "title": "FLOPs\nPeak Compute",
            "content": ","
        },
        {
            "title": "Memory Access\nPeak Bandwidth",
            "content": "(cid:19) (16) This approach enables rapid evaluation of over 50,000 configurations in minutes, making it suitable for large-scale architecture exploration. Empirical Validation. We validate roofline predictions using vLLM with subprocess isolation to ensure accurate GPU memory accounting. Top Pareto candidates identified by analytical modeling are measured empirically to confirm their optimality. Latency Scaling Behavior. Inference latency varies with workload parameters: Sequence length: Prefill latency scales quadratically with input length due to attention (ğ‘‚(ğ‘†2)); decode latency scales linearly due to KV-cache access (ğ‘‚(ğ‘†)). Output length: Total latency is dominated by decode for long generations, making decode optimization critical for conversational applications. Batch size: Both phases benefit from batching with diminishing returns. Prefill becomes computebound at larger batches; decode remains memory-bound due to weight loading. For on-vehicle deployment, we focus on batch size 1. Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Scenario-Specific Targets. The appropriate latency optimization target depends on deployment scenario: Interactive/streaming applications (chatbots, speculative decoding): optimize decode latency. Long-context processing (document QA): optimize prefill latency. Balanced workloads (summarization): optimize total latency. E. Problem Formulation and Roofline Analysis This section presents comprehensive roofline analysis for decoder-only Transformers, deriving FLOPs, memory traffic, and latency models for both prefill and decode phases. E.1. Notation Table 7 Symbol definitions for roofline analysis. Symbol Definition Symbol Definition ğµ ğ‘™ ğ‘‘â„ ğ‘›ğ‘˜ğ‘£ ğ‘‘ğ‘š ğ¸ ğœŒ ğ‘ğ‘ ğœ‹ğ» ğ‘†in Batch size Number of layers Head dimension Number of KV heads KV dimension = ğ‘‘/gqa Total experts Activation rate = ğ¾/ğ¸ Bytes per activation Peak compute (FLOP/s) Input sequence length ğ‘† ğ‘‘ ğ‘›â„ gqa ğ‘Ÿ ğ¾ ğ‘ğ‘¤ ğ‘ğ‘˜ğ‘£ ğ›½ğ» ğ‘†out Sequence length Hidden dimension Number of query heads GQA ratio = ğ‘›â„/ğ‘›ğ‘˜ğ‘£ FFN expansion ratio Active experts per token Bytes per weight Bytes per KV element Memory bandwidth (Bytes/s) Output sequence length E.2. Transformer Architecture We consider decoder-only Transformer with the following components per layer: Multi-Head Attention. The attention mechanism uses ğ‘›â„ query heads and ğ‘›ğ‘˜ğ‘£ key-value heads (Grouped Query Attention). The hidden dimension is ğ‘‘, and the GQA ratio is gqa = ğ‘›â„/ğ‘›ğ‘˜ğ‘£. The projection matrices are: Query projection ğ‘Šğ‘„ â„ğ‘‘ğ‘‘ Key projection ğ‘Šğ¾ â„ğ‘‘ğ‘‘/gqa Value projection ğ‘Šğ‘‰ â„ğ‘‘ğ‘‘/gqa Output projection ğ‘Šğ‘‚ â„ğ‘‘ğ‘‘ Feed-Forward Network. We consider gated FFN (SwiGLU) with expansion ratio ğ‘Ÿ = ğ‘‘ffn/ğ‘‘: Up projection ğ‘Šup â„ğ‘‘ğ‘Ÿğ‘‘ Gate projection ğ‘Šgate â„ğ‘‘ğ‘Ÿğ‘‘ Down projection ğ‘Šdown â„ğ‘Ÿğ‘‘ğ‘‘ 28 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Mixture of Experts. per token. The activation rate is ğœŒ = ğ¾/ğ¸. In an MoE layer, the FFN is replicated into ğ¸ experts, of which ğ¾ are activated E.3. Per-Operator Analysis E.3.1. Attention Projection Layers Multi-head attention uses four linear projections: Query (Q), Key (K), Value (V), and Output (O). Modern architectures employ Grouped-Query Attention (GQA) [50] to reduce K/V dimensions by factor of gqa. Projection. Transform input ğ‘‹ â„ğµğ‘†ğ‘‘ to queries ğ‘„ â„ğµğ‘†ğ‘‘ via weight ğ‘Šğ‘„ â„ğ‘‘ğ‘‘. Table 8 projection costs in prefill and decode phases. Metric FLOPs Weight Load Activation Load Activation Store Prefill 2ğµğ‘†ğ‘‘2 ğ‘‘2 ğ‘ğ‘¤ ğµğ‘†ğ‘‘ ğ‘ğ‘ ğµğ‘†ğ‘‘ ğ‘ğ‘ Decode (ğ‘†ğ‘ = 1) 2ğµğ‘‘2 ğ‘‘2 ğ‘ğ‘¤ ğµğ‘‘ ğ‘ğ‘ ğµğ‘‘ ğ‘ğ‘ Arithmetic Intensity. From Table 8: Prefill: 2ğµğ‘† ğ‘ğ‘¤ Decode: 2ğµ ğ‘ğ‘¤ , typically compute-bound for large ğµğ‘†. , typically memory-bound for small ğµ. and Projections (GQA). Project to reduced dimension ğ‘‘ğ‘š = ğ‘‘/gqa: ğ‘‹ â„ğµğ‘†ğ‘‘ ğ¾, ğ‘‰ â„ğµğ‘†ğ‘‘ğ‘š via ğ‘Šğ¾, ğ‘Šğ‘‰ â„ğ‘‘ğ‘‘ğ‘š. Table 9 and projection costs with GQA. Each operates on dimension ğ‘‘ğ‘š = ğ‘‘/gqa. Metric (each) FLOPs Weight Load KV Cache Store Prefill 2ğµğ‘†ğ‘‘2/gqa ğ‘‘2ğ‘ğ‘¤/gqa ğµğ‘†ğ‘‘ ğ‘ğ‘˜ğ‘£/gqa Decode 2ğµğ‘‘2/gqa ğ‘‘2ğ‘ğ‘¤/gqa ğµğ‘‘ ğ‘ğ‘˜ğ‘£/gqa GQA reduces K/V computation, weight memory, and critically KV-cache storage by factor gqa compared to Q/O projections. Projection. Identical to projection (see Table 8). Summary. Total costs for Q, K, V, projections: FLOPs = 2ğµğ‘†ğ‘‘2 (cid:18) Weight Memory = ğ‘‘2ğ‘ğ‘¤ (cid:18) 2 + 2 + 2 gqa (cid:19) 2 gqa (cid:19) (17) (18) 29 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs For gqa = 1 (standard MHA): 8ğµğ‘†ğ‘‘2 FLOPs, 4ğ‘‘2ğ‘ğ‘¤ memory. For gqa = 8: 44% reduction. E.3.2. Attention Score Computation The attention mechanism computes scores through three main operations: query-key multiplication, softmax normalization, and score-value multiplication. We analyze each operation separately. QK Matmul (Query-Key Attention Scores). For each attention head, compute attention scores by multiplying query ğ‘„â„ â„ğ‘†ğ‘ ğ‘‘â„ with key transpose ğ¾ğ‘‡ â„ â„ğ‘‘â„ ğ‘†ğ‘˜ğ‘£, producing scores â„ğ‘†ğ‘ ğ‘†ğ‘˜ğ‘£. Table 10 Query-key attention score computation costs. Note ğ‘‘ğ‘š = ğ‘‘/gqa is the reduced dimension for cache with grouped-query attention. Metric Prefill (ğ‘†ğ‘ = ğ‘†ğ‘˜ğ‘£ = ğ‘†) Decode (ğ‘†ğ‘ = 1) FLOPs Load Load Cache Store Scores 2ğµğ‘†2ğ‘‘ ğµğ‘†ğ‘‘ ğ‘ğ‘ ğµğ‘†ğ‘‘ğ‘š ğ‘ğ‘˜ğ‘£ ğµğ‘›â„ğ‘†2 ğ‘ğ‘ 2ğµğ‘†ğ‘‘ ğµğ‘‘ ğ‘ğ‘ ğµğ‘†ğ‘‘ğ‘š ğ‘ğ‘˜ğ‘£ ğµğ‘›â„ğ‘† ğ‘ğ‘ Prefill computes full ğ‘† ğ‘† attention matrix per head (ğ‘‚(ğ‘†2) complexity), while decode only computes scores for one new token against ğ‘† cached keys (ğ‘‚(ğ‘†) complexity). Softmax Normalization. Attention scores are normalized using softmax, requiring approximately 5 operations per element: finding maximum (for numerical stability), subtraction, exponentiation, summation, and division. Table 11 Softmax normalization costs over ğµ batches, ğ‘›â„ heads, each processing ğ‘†ğ‘ ğ‘†ğ‘˜ğ‘£ scores. Metric Prefill (ğ‘†ğ‘ = ğ‘†) Decode (ğ‘†ğ‘ = 1) FLOPs Memory Load Memory Store 5ğµğ‘›â„ğ‘†2 ğµğ‘›â„ğ‘†2 ğ‘ğ‘ ğµğ‘›â„ğ‘†2 ğ‘ğ‘ 5ğµğ‘›â„ğ‘† ğµğ‘›â„ğ‘† ğ‘ğ‘ ğµğ‘›â„ğ‘† ğ‘ğ‘ Softmax FLOPs are typically negligible compared to matrix multiplications, contributing ğ‘‚(ğ‘›â„ğ‘†2) versus ğ‘‚(ğ‘†ğ‘‘2) for QK matmul when ğ‘‘ ğ‘›â„. Score-Value Matmul (Weighted Aggregation). Normalized attention scores â„ğ‘†ğ‘ ğ‘†ğ‘˜ğ‘£ multiply value matrix ğ‘‰â„ â„ğ‘†ğ‘˜ğ‘£ ğ‘‘â„ to produce attention output â„ğ‘†ğ‘ ğ‘‘â„ per head. Table 12 Score-value multiplication costs for weighted value aggregation. Metric Prefill (ğ‘†ğ‘ = ğ‘†) Decode (ğ‘†ğ‘ = 1) FLOPs Load Scores Load Cache Store Output 2ğµğ‘†2ğ‘‘ ğµğ‘›â„ğ‘†2 ğ‘ğ‘ ğµğ‘†ğ‘‘ğ‘š ğ‘ğ‘˜ğ‘£ ğµğ‘†ğ‘‘ ğ‘ğ‘ 2ğµğ‘†ğ‘‘ ğµğ‘›â„ğ‘† ğ‘ğ‘ ğµğ‘†ğ‘‘ğ‘š ğ‘ğ‘˜ğ‘£ ğµğ‘‘ ğ‘ğ‘ Like QK matmul, score-value multiplication exhibits ğ‘‚(ğ‘†2) complexity in prefill and ğ‘‚(ğ‘†) in decode. 30 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Summary. Combining all three operations (QK matmul, softmax, score-value matmul): Table 13 Total computational and memory costs for attention score computation. Metric Total FLOPs KV Cache Access Prefill (ğ‘†ğ‘ = ğ‘†) 4ğµğ‘†2ğ‘‘ + ğ‘‚(ğµğ‘›â„ğ‘†2) 2ğµğ‘†ğ‘‘ğ‘š ğ‘ğ‘˜ğ‘£ Decode (ğ‘†ğ‘ = 1) 4ğµğ‘†ğ‘‘ + ğ‘‚(ğµğ‘›â„ğ‘†) 2ğµğ‘†ğ‘‘ğ‘š ğ‘ğ‘˜ğ‘£ Key Observations. From Table 13: Quadratic vs. Linear Scaling: Prefill attention scales as ğ‘‚(ğ‘†2) due to full ğ‘† ğ‘† attention matrix, while decode scales linearly as ğ‘‚(ğ‘†) since only one new token attends to all previous tokens. Projection Dominance: For typical configurations where ğ‘‘ ğ‘† (e.g., ğ‘‘ = 4096, ğ‘† = 2048), projection FLOPs (ğ‘‚(ğµğ‘‘2)) dominate over attention score FLOPs (ğ‘‚(ğµğ‘†ğ‘‘)), especially in decode phase. KV Cache Bottleneck: KV cache access remains ğ‘‚(ğµğ‘†ğ‘‘ğ‘š) in both phases, becoming critical bottleneck in decode when sequence length ğ‘† is large. E.3.3. FFN Layers Modern transformers use either dense FFN layers or sparse Mixture-of-Experts (MoE) FFN layers. Dense FFN with SwiGLU Activation. SwiGLU [51] uses three linear projections with gated activation: ğ‘Šğ‘” ğº â„ğµğ‘†ğ‘Ÿğ‘‘ 1. Gate projection: ğ‘‹ â„ğµğ‘†ğ‘‘ 2. Up projection: ğ‘‹ â„ğµğ‘†ğ‘‘ ğ‘Šğ‘¢ ğ‘ˆ â„ğµğ‘†ğ‘Ÿğ‘‘ 3. Gated activation: ğ» = SiLU(ğº) ğ‘ˆ (element-wise) 4. Down projection: ğ» â„ğµğ‘†ğ‘Ÿğ‘‘ ğ‘Šğ‘‘ ğ‘Œ â„ğµğ‘†ğ‘‘ where ğ‘Ÿ is the expansion ratio (typically ğ‘Ÿ = 8 3 or ğ‘Ÿ = 4). Table 14 SwiGLU FFN costs. Element-wise ops contribute negligible ğ‘‚(ğµğ‘†ğ‘Ÿğ‘‘) FLOPs vs ğ‘‚(ğµğ‘†ğ‘Ÿğ‘‘2) for projections. Component Gate (ğ‘Šğ‘” â„ğ‘‘ğ‘Ÿğ‘‘) Up (ğ‘Šğ‘¢ â„ğ‘‘ğ‘Ÿğ‘‘) Down (ğ‘Šğ‘‘ â„ğ‘Ÿğ‘‘ğ‘‘) Total (3 projections) FLOPs (Prefill/Decode) Weight Memory 2ğµğ‘†ğ‘Ÿğ‘‘2 / 2ğµğ‘Ÿğ‘‘2 2ğµğ‘†ğ‘Ÿğ‘‘2 / 2ğµğ‘Ÿğ‘‘2 2ğµğ‘†ğ‘Ÿğ‘‘2 / 2ğµğ‘Ÿğ‘‘2 6ğµğ‘†ğ‘Ÿğ‘‘2 / 6ğµğ‘Ÿğ‘‘2 ğ‘Ÿğ‘‘2 ğ‘ğ‘¤ ğ‘Ÿğ‘‘2 ğ‘ğ‘¤ ğ‘Ÿğ‘‘2 ğ‘ğ‘¤ 3ğ‘Ÿğ‘‘2 ğ‘ğ‘¤ Mixture-of-Experts (MoE) FFN. MoE [52, 53] replaces dense FFN with ğ¸ parallel experts, activating only top-ğ¾ per token via learned routing, decoupling computation from capacity. Key parameters: ğ¸ = total experts; ğ¾ = active experts/token (typically ğ¾ = 2); ğœŒ = ğ¾/ğ¸ = activation rate; ğ‘Ÿsingle = per-expert expansion; ğ‘Ÿ = ğ¾ ğ‘Ÿsingle = effective expansion. 31 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Costs: FLOPs match dense FFN with same effective ğ‘Ÿ, but all ğ¸ experts must be stored: MoE FLOPs = 6ğµğ‘†ğ‘Ÿğ‘‘2 (prefill), 6ğµğ‘Ÿğ‘‘2 (decode) MoE Weight Memory = 3ğ‘Ÿğ‘‘2ğ‘ğ‘¤ ğœŒ = ğ¸ 3ğ‘Ÿsingleğ‘‘2ğ‘ğ‘¤ (19) (20) Table 15 Dense vs MoE FFN comparison. MoE achieves 1/ğœŒ more capacity at same FLOPs. Architecture Dense FFN MoE FFN FLOPs Prefill Decode 6ğµğ‘†ğ‘Ÿğ‘‘2 6ğµğ‘Ÿğ‘‘2 6ğµğ‘†ğ‘Ÿğ‘‘2 6ğµğ‘Ÿğ‘‘ Ratio (MoE/Dense) 1 1 Key Observations. Weight Memory Total 3ğ‘Ÿğ‘‘2ğ‘ğ‘¤ 3ğ‘Ÿğ‘‘2ğ‘ğ‘¤ ğœŒ ğ¸/ğ¾ Capacity-Computation Decoupling: MoE scales parameters without increasing FLOPs. Example: ğ¸ = 8, ğ¾ = 2 (ğœŒ = 0.25) yields 4 more parameters at same compute. Memory-Bound Regime: In decode with small ğµ, MoEs 1/ğœŒ larger weight memory exacerbates bottlenecks, requiring quantization. FFN Dominance: FFN accounts for 60-70% of total FLOPs when ğ‘Ÿ 4 (vs attention projection costs in Equation 17 and Equation 18). E.4. Per-Layer Coefficient Summary Combining attention projections and FFN, the per-layer costs are: FLOPs per layer = 2ğµğ‘†ğ‘‘2 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:19) (cid:18) 2 + 2 gqa (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) + 6ğµğ‘†ğ‘Ÿğ‘‘2 (cid:32)(cid:32) (cid:32)(cid:32) (cid:123)(cid:122) (cid:125) (cid:124) FFN + ğ‘‚(ğµğ‘†2ğ‘‘) (cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:125) (cid:123)(cid:122) Attn Score Weight (compute) = ğ‘‘2ğ‘ğ‘¤ Weight (storage) = ğ‘‘2ğ‘ğ‘¤ (cid:124) (cid:124) (cid:124) (cid:18) 2 + (cid:123)(cid:122) Attention Proj. 2 gqa (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) Attention Proj. 2 gqa (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) Attention Proj. 2 + (cid:18) (cid:19) (cid:125) (cid:125) + 3ğ‘Ÿğ‘‘2ğ‘ğ‘¤ (cid:32)(cid:32) (cid:32)(cid:32) (cid:125) (cid:123)(cid:122) (cid:124) FFN (active) (cid:19) + 3ğ‘Ÿğ‘‘2ğ‘ğ‘¤ ğœŒ (cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) (cid:124) FFN (all experts) For large ğ‘‘ where projection FLOPs dominate, we define the normalized coefficients: ğœ‰ğ¹ = 4 + ğœ‰dec ğ‘Š = 2 + ğœ‰all ğ‘Š = 2 + 4 gqa 2 gqa 2 gqa + 6ğ‘Ÿ + 3ğ‘Ÿ + 3ğ‘Ÿ ğœŒ (FLOPs coefficient) (Decode weight coefficient) (Storage coefficient) (21) (22) (23) (24) (25) (26) 32 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Key Relations: 1. ğœ‰ğ¹ = 2 ğœ‰dec ğ‘Š for any (ğ‘Ÿ, gqa). This factor of 2 arises because each multiply-accumulate operation counts as 2 FLOPs but requires loading each weight only once. 2. ğœŒ appears only in ğœ‰all ğ‘Š , not in ğœ‰ğ¹ or ğœ‰dec ğ‘Š . This reflects that sparsity affects storage but not per-token computation. 3. For dense models (ğœŒ = 1): ğœ‰all ğ‘Š = ğœ‰dec ğ‘Š . E.5. Inference Phases and Roofline Model Autoregressive generation consists of two phases: Prefill Phase. The model processes ğ‘†in input tokens in parallel. This phase is typically computebound. Decode Phase. The model generates ğ‘†out tokens autoregressively. At step ğ‘¡, the KV-cache contains ğ‘†in + ğ‘¡ entries. This phase is typically memory-bandwidth-bound. Roofline Model. The roofline model [47] characterizes kernel latency as: ğ‘‡ = max (cid:19) (cid:18) ğœ‹ğ» , ğ›½ğ» where is FLOPs, is memory traffic, ğœ‹ğ» is peak compute, and ğ›½ğ» is memory bandwidth. E.6. Latency Modeling E.6.1. Prefill Latency For batch size ğµ and input sequence length ğ‘†in, the total FLOPs per layer is: Flayer = ğµğ‘†inğ‘‘2 ğœ‰ğ¹ With large batch-sequence product ğµğ‘†in, prefill is typically compute-bound: ğ‘‡pre = ğ‘™ ğµğ‘†inğ‘‘2 ğœ‰ğ¹ ğœ‹ğ» E.6.2. Decode Latency: Single Step Analysis At decode step ğ‘¡, processing ğµ tokens with context length ğ‘†in + ğ‘¡: Memory Traffic per Step. (1) Weight loading: (2) KV-cache loading: Wweight = ğœ‰dec ğ‘Š ğ‘‘2 ğ‘ğ‘¤ WKV (ğ‘¡) = 2(ğ‘†in + ğ‘¡) ğ‘‘ ğ‘ğ‘˜ğ‘£ gqa (27) (28) (29) (30) (31) 33 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Total per layer: Wlayer(ğ‘¡) = ğœ‰dec ğ‘Š ğ‘‘2 ğ‘ğ‘¤ + 2(ğ‘†in + ğ‘¡) ğ‘‘ ğ‘ğ‘˜ğ‘£ gqa Single Step Latency. With small batch ğµ (often ğµ = 1), decode is typically memory-bound: ğ‘‡step(ğ‘¡) = (cid:20) ğ‘™ ğ›½ğ» ğ‘Š ğ‘‘2ğ‘ğ‘¤ + ğœ‰dec 2(ğ‘†in + ğ‘¡)ğ‘‘ğ‘ğ‘˜ğ‘£ gqa (cid:21) E.6.3. Decode Latency: Total Latency Summing over ğ‘¡ = 1, . . . , ğ‘†out: Evaluating the sum: ğ‘‡decode = (cid:34) ğ‘™ ğ›½ğ» ğ‘†out ğœ‰dec ğ‘Š ğ‘‘2ğ‘ğ‘¤ + 2ğ‘‘ğ‘ğ‘˜ğ‘£ gqa ğ‘†out ğ‘¡= (cid:35) (ğ‘†in + ğ‘¡) ğ‘†out ğ‘¡=1 (ğ‘†in + ğ‘¡) = ğ‘†out ğ‘†in + ğ‘†out(ğ‘†out + 1) = ğ‘†out ğ‘† where the average context length is: ğ‘† = ğ‘†in + ğ‘†out + 1 2 Complete Decode Latency. ğ‘‡decode = (cid:20) ğ‘™ ğ‘†out ğ›½ğ» ğ‘Š ğ‘‘2 ğ‘ğ‘¤ + ğœ‰dec 2ğ‘† ğ‘‘ ğ‘ğ‘˜ğ‘£ gqa (cid:21) E.6.4. Decode Latency: Unified Form Define the effective decode coefficient: Then: Expanding ğœ‰eff ğ‘Š : ğ‘Š (ğ‘‘, gqa) = ğœ‰dec ğœ‰eff ğ‘Š + 2ğ‘† ğ‘ğ‘˜ğ‘£ gqa ğ‘‘ ğ‘ğ‘¤ ğ‘‡decode = ğ‘Š ğ‘‘2 ğ‘ğ‘¤ ğ‘™ ğ‘†out ğœ‰eff ğ›½ğ» ğœ‰eff ğ‘Š = 2 + 2 gqa + 3ğ‘Ÿ + 2ğ‘† ğ‘ğ‘˜ğ‘£ gqa ğ‘‘ ğ‘ğ‘¤ (32) (33) (34) (35) (36) (37) (38) (39) (40) Note: ğœ‰eff ğ‘Š depends on ğ‘‘ and gqa, which introduces additional coupling in optimization problems. Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs E.7. Memory Footprint Per-layer storage (all ğ¸ experts): Total model memory: ğ‘€layer = ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ ğ‘€ = ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ E.8. Constraint Formulations E.8.1. Prefill Constraint Given target latency ğ‘‡ pre lat , define ğ¹ ğ‘ = ğ‘‡ pre lat ğœ‹ğ»/(ğµğ‘†in): ğ‘™ ğœ‰ğ¹ ğ‘‘2 ğ¹ ğ‘ E.8.2. Decode Constraint Given target latency ğ‘‡ dec lat , define ğ‘€ğ‘‘ = ğ‘‡ dec lat ğ›½ğ»/ğ‘†out: ğ‘™ ğœ‰eff ğ‘Š ğ‘‘2 ğ‘ğ‘¤ ğ‘€ğ‘‘ Expanding with the full KV-cache term: ğ‘™ ğœ‰dec ğ‘Š ğ‘‘2 ğ‘ğ‘¤ + 2ğ‘™ ğ‘† ğ‘‘ ğ‘ğ‘˜ğ‘£ gqa ğ‘€ğ‘‘ E.8.3. Memory Constraint ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ ğ‘€budget E.9. Summary Table 16 Roofline coefficients and their usage. Coefficient ğœ‰ğ¹ ğœ‰dec ğ‘Š ğœ‰eff ğ‘Š ğœ‰all ğ‘Š Formula 4 + 4 gqa + 6ğ‘Ÿ 2 + 2 gqa + 3ğ‘Ÿ ğ‘Š + 2ğ‘†ğ‘ğ‘˜ğ‘£ ğœ‰dec gqağ‘‘ğ‘ğ‘¤ gqa + 3ğ‘Ÿ 2 + 2 ğœŒ Usage Prefill FLOPs: = ğ‘™ ğœ‰ğ¹ ğµğ‘† ğ‘‘"
        },
        {
            "title": "Decode weight traffic",
            "content": "Decode total traffic (weight + KV) Storage: ğ‘€ = ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ (41) (42) (43) (44) (45) (46) Note: ğœ‰eff ğ‘Š depends on ğ‘‘, which introduces additional coupling in the optimization. For dense models (ğœŒ = 1), we have ğœ‰all ğ‘Š = ğœ‰dec ğ‘Š . Additionally, we define the aggregate loss gradient ğ· ğœ…ğœŒğœŒğ›¼ğœŒ ğ‘‘ ğ›½2 ğ›½1 + ğœ…ğ‘‘, which combines the sparsity and base capacity contributions from the loss function. This shorthand appears throughout the following case derivations (Cases D1D3, P1P3) in the stationarity conditions for ğ‘Ÿ. 35 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Table 17 Partial derivatives of coefficients. /ğ‘Ÿ 6 3 3 3 ğœŒ /gqa 4 gqa2 2 gqa2 gqa2 2ğ‘†ğ‘ğ‘˜ğ‘£ 2 gqa2ğ‘‘ğ‘ğ‘¤ 2 gqa2 /ğœŒ /ğ‘‘ 0 0 0 3ğ‘Ÿ ğœŒ2 0 0 2ğ‘†ğ‘ğ‘˜ğ‘£ gqağ‘‘2ğ‘ğ‘¤ ğœ‰ğ¹ ğœ‰dec ğ‘Š ğœ‰eff ğ‘Š ğœ‰all ğ‘Š F. Case D1: Decode, Latency-Constrained F.1. Problem Statement min ğ‘™,ğ‘‘,ğ‘Ÿ,gqa,ğœŒ Ë†L (ğœƒ) = ğœ…ğ‘™ ğ‘™ğ›¼ğ‘™ ğœ…ğœŒğœŒğ›¼ğœŒ ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½1 + ğœ…ğ‘‘ ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½2 + ğœ…ğ‘š gqağ›¼ğ‘š ğ‘‘ğ›¼ğ‘š + + Ë†L s.t. ğ‘™ ğœ‰dec ğ‘Š ğ‘‘2 ğ‘ğ‘¤ + 2ğ‘™ ğ‘† ğ‘‘ ğ‘ğ‘˜ğ‘£ gqa = ğ‘€ğ‘‘ ğœŒ ğœŒmin where ğœ‰dec ğ‘Š = 2 + 2/gqa + 3ğ‘Ÿ. Define the constraint function: ğ‘”ğ‘‡ (ğœƒ) = ğ‘™ ğœ‰dec ğ‘Š ğ‘‘2 ğ‘ğ‘¤ + 2ğ‘™ ğ‘† ğ‘‘ ğ‘ğ‘˜ğ‘£ gqa ğ‘€ğ‘‘ F.2. Lagrangian â„’ = Ë†L (ğœƒ) + ğœ‡ğ‘‡ ğ‘”ğ‘‡ (ğœƒ) F.3. Constraint Partial Derivatives = ğœ‰dec ğ‘Š ğ‘‘2 ğ‘ğ‘¤ + 2ğ‘† ğ‘‘ ğ‘ğ‘˜ğ‘£ gqa = 2ğ‘™ ğœ‰dec ğ‘Š ğ‘‘ ğ‘ğ‘¤ + 2ğ‘™ ğ‘† ğ‘ğ‘˜ğ‘£ gqa = 3ğ‘™ ğ‘‘2 ğ‘ğ‘¤ ğ‘”ğ‘‡ ğ‘™ ğ‘”ğ‘‡ ğ‘‘ ğ‘”ğ‘‡ ğ‘Ÿ ğ‘”ğ‘‡ gqa = 2ğ‘™ ğ‘‘2 ğ‘ğ‘¤ gqa 2ğ‘™ ğ‘† ğ‘‘ ğ‘ğ‘˜ğ‘£ gqa2 ğ‘”ğ‘‡ ğœŒ = 0 Note: ğ‘”ğ‘‡ /ğœŒ = 0 still holds because the KV-cache term is also independent of ğœŒ. (47) (48) (49) (50) (51) (52) (53) (54) 36 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs F.4. KKT Conditions Stationarity for ğ‘™: Solving for ğœ‡ğ‘‡ : Stationarity for ğœŒ: (cid:18) ğ›¼ğ‘™ğœ…ğ‘™ ğ‘™ğ›¼ğ‘™+1 + ğœ‡ğ‘‡ ğ‘Š ğ‘‘2ğ‘ğ‘¤ + ğœ‰dec (cid:19) 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ gqa = 0 ğœ‡ğ‘‡ = ğ›¼ğ‘™ğœ…ğ‘™ ğ‘™ğ›¼ğ‘™+1 (cid:16) ğ‘Š ğ‘‘2ğ‘ğ‘¤ + 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ ğœ‰dec gqa (cid:17) â„’ ğœŒ = ğ›¼ğœŒğœ…ğœŒğœŒğ›¼ğœŒ1 ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½1 + ğœ‡ğ‘‡ 0 = ğ›¼ğœŒğœ…ğœŒğœŒğ›¼ğœŒ1 ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½1 > 0 Since â„’/ğœŒ > 0 for all ğœŒ > 0: ğœŒ = ğœŒmin (55) (56) (57) (58) Physical interpretation. The activation rate ğœŒ does not appear in the decode latency constraint (ğ‘”ğ‘‡ /ğœŒ = 0), because only ğ¾ experts are activated per token regardless of the total pool size ğ¸thus per-token computation and bandwidth cost are invariant to ğœŒ. Meanwhile, the loss is monotonically increasing in ğœŒ: fewer activated experts relative to total experts means greater total model capacity at no additional per-token cost. Therefore, the optimal strategy under latency constraints is to maximize sparsity (minimize ğœŒ), i.e., increase the expert pool ğ¸ as far as memory permits while keeping ğ¾ fixed. Stationarity for ğ‘Ÿ: Stationarity for gqa: Simplifying: Stationarity for ğ‘‘: ğ›¼ğ‘Ÿ ğ· ğ‘Ÿğ›¼ğ‘Ÿ+1ğ‘‘ ğ›½2 + 3ğœ‡ğ‘‡ ğ‘™ğ‘‘2ğ‘ğ‘¤ = 0 ğ›¼ğ‘šğœ…ğ‘š gqağ›¼ğ‘š 1 ğ‘‘ğ›¼ğ‘š ğœ‡ğ‘‡ (cid:18) 2ğ‘™ğ‘‘2ğ‘ğ‘¤ gqa2 + (cid:19) 2ğ‘™ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ gqa2 = 0 ğ›¼ğ‘šğœ…ğ‘š gqağ›¼ğ‘š 1 ğ‘‘ğ›¼ğ‘š = (cid:16) 2ğœ‡ğ‘‡ ğ‘™ gqa2 ğ‘‘2ğ‘ğ‘¤ + ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ (cid:17) ğ›½1ğœ…ğœŒğœŒğ›¼ğœŒ ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½1+1 ğ›½2ğœ…ğ‘‘ ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½2+1 ğ›¼ğ‘šğœ…ğ‘šgqağ›¼ğ‘š ğ‘‘ğ›¼ğ‘š+1 (cid:18) + ğœ‡ğ‘‡ 2ğ‘™ğœ‰dec ğ‘Š ğ‘‘ğ‘ğ‘¤ + (cid:19) 2ğ‘™ğ‘†ğ‘ğ‘˜ğ‘£ gqa = 0 F.5. Solution Derivation Step 1: Depth. From the active constraint: ğ‘™ = ğ‘€ğ‘‘ ğ‘Š ğ‘‘2ğ‘ğ‘¤ + 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ ğœ‰dec gqa (59) (60) (61) (62) (63) 37 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Or equivalently using ğœ‰eff ğ‘Š : ğ‘™ = ğ‘€ğ‘‘ ğœ‰eff ğ‘Š ğ‘‘2 ğ‘ğ‘¤ Step 2: Multiplier. Define the effective constraint coefficient: Then: Î“ ğœ‰dec ğ‘Š ğ‘‘2ğ‘ğ‘¤ + 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ gqa = ğœ‰eff ğ‘Š ğ‘‘2 ğ‘ğ‘¤ ğœ‡ğ‘‡ ğ‘™ = ğ›¼ğ‘™ğœ…ğ‘™ ğ‘™ğ›¼ğ‘™ Î“ = ğ›¼ğ‘™ğœ…ğ‘™Î“ğ›¼ğ‘™ 1 ğ‘€ğ›¼ğ‘™ ğ‘‘ Step 3: FFN Ratio. From stationarity for ğ‘Ÿ: ğ›¼ğ‘Ÿ ğ· ğ‘Ÿğ›¼ğ‘Ÿ+1ğ‘‘ ğ›½2 = 3ğœ‡ğ‘‡ ğ‘™ğ‘‘2ğ‘ğ‘¤ ğ‘Ÿ = (cid:34) ğ›¼ğ‘Ÿ ğ· 3ğ›¼ğ‘™ğœ…ğ‘™ ğ‘€ğ›¼ğ‘™ ğ‘‘ Î“ğ›¼ğ‘™ 1ğ‘‘2+ğ›½2 ğ‘ğ‘¤ (cid:35) 1 ğ›¼ğ‘Ÿ +1 Step 4: GQA Ratio. From stationarity for gqa: F.6. Solution Summary gqağ›¼ğ‘š+1 = 2ğœ‡ğ‘‡ ğ‘™ğ‘‘ğ›¼ğ‘š ğ›¼ğ‘šğœ…ğ‘š (cid:16) ğ‘‘2ğ‘ğ‘¤ + ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ (cid:17) gqa = (cid:34) 2ğ›¼ğ‘™ğœ…ğ‘™ ğ›¼ğ‘šğœ…ğ‘š Î“ğ›¼ğ‘™ 1ğ‘‘ğ›¼ğ‘š ğ‘€ğ›¼ğ‘™ ğ‘‘ (cid:16) 1 ğ›¼ğ‘š+1 (cid:35) (cid:17) ğ‘‘2ğ‘ğ‘¤ + ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ ğœŒ = ğœŒmin ğ‘™ = ğ‘€ğ‘‘ ğ‘Š ğ‘‘2ğ‘ğ‘¤ + 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ ğœ‰dec gqa ğ‘Ÿ = gqa = (cid:34) ğ›¼ğ‘Ÿ ğ· 3ğ›¼ğ‘™ğœ…ğ‘™ (cid:34) 2ğ›¼ğ‘™ğœ…ğ‘™ ğ›¼ğ‘šğœ…ğ‘š (cid:35) 1 ğ›¼ğ‘Ÿ +1 ğ‘€ğ›¼ğ‘™ ğ‘‘ Î“ğ›¼ğ‘™ 1ğ‘‘2+ğ›½2 ğ‘ğ‘¤ 1 ğ›¼ğ‘š+1 (cid:35) (cid:17) ğ‘‘2ğ‘ğ‘¤ + ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ (cid:16) Î“ğ›¼ğ‘™ 1ğ‘‘ğ›¼ğ‘š ğ‘€ğ›¼ğ‘™ ğ‘‘ (64) (65) (66) (67) (68) (69) (70) (71) (72) (73) (74) where Î“ = ğœ‰dec ğ‘Š ğ‘‘2ğ‘ğ‘¤ + 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£/gqa and ğœ‰dec corresponds to Theorem 5.1 in the main text. ğ‘Š = 2 + 2/gqa + 3ğ‘Ÿ (implicit). The activation rate result 38 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs G. Case D2: Decode, Memory-Constrained G.1. Problem Statement min ğ‘™,ğ‘‘,ğ‘Ÿ,gqa,ğœŒ Ë†L (ğœƒ) s.t. ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ = ğ‘€budget (75) where ğœ‰all ğ‘Š = 2 + 2/gqa + 3ğ‘Ÿ/ğœŒ. Note: The memory constraint concerns model storage, which is independent of the KV-cache runtime overhead. G.2. Lagrangian â„’ = Ë†L (ğœƒ) + ğœ‡ ğ‘€ (cid:16) ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ ğ‘€budget (cid:17) G.3. Constraint Partial Derivatives ğ‘”ğ‘€ ğ‘™ ğ‘”ğ‘€ ğ‘‘ ğ‘”ğ‘€ ğ‘Ÿ = ğœ‰all ğ‘Š ğ‘‘2ğ‘ğ‘¤ = 2ğ‘™ğœ‰all ğ‘Š ğ‘‘ğ‘ğ‘¤ 3ğ‘™ğ‘‘2ğ‘ğ‘¤ ğœŒ = ğ‘”ğ‘€ gqa = 2ğ‘™ğ‘‘2ğ‘ğ‘¤ gqa2 ğ‘”ğ‘€ ğœŒ = 3ğ‘™ğ‘Ÿğ‘‘2ğ‘ğ‘¤ ğœŒ2 G.4. KKT Conditions Stationarity for ğ‘™: Stationarity for ğœŒ: Rearranging: ğ›¼ğ‘™ğœ…ğ‘™ ğ‘™ğ›¼ğ‘™+1 + ğœ‡ ğ‘€ğœ‰all ğ‘Š ğ‘‘2ğ‘ğ‘¤ = 0 ğœ‡ ğ‘€ = ğ›¼ğ‘™ğœ…ğ‘™ ğ‘™ğ›¼ğ‘™+1ğœ‰all ğ‘Š ğ‘‘2ğ‘ğ‘¤ ğ›¼ğœŒğœ…ğœŒğœŒğ›¼ğœŒ1 ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½1 3ğœ‡ ğ‘€ ğ‘™ğ‘Ÿğ‘‘2ğ‘ğ‘¤ ğœŒ2 = 0 ğœ‡ ğ‘€ ğ‘™ = ğ›¼ğœŒğœ…ğœŒğœŒğ›¼ğœŒ+1 3ğ‘Ÿğ›¼ğ‘Ÿ+1ğ‘‘ ğ›½1+2ğ‘ğ‘¤ (76) (77) (78) (79) (80) (81) (82) (83) (84) 39 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Stationarity for ğ‘Ÿ: Rearranging: Stationarity for gqa: ğ›¼ğ‘Ÿ ğ· ğ‘Ÿğ›¼ğ‘Ÿ+1ğ‘‘ ğ›½ 3ğœ‡ ğ‘€ ğ‘™ğ‘‘2ğ‘ğ‘¤ ğœŒ + = 0 ğœ‡ ğ‘€ ğ‘™ = ğ›¼ğ‘Ÿ ğ·ğœŒ 3ğ‘Ÿğ›¼ğ‘Ÿ+1ğ‘‘ ğ›½2+2ğ‘ğ‘¤ ğ›¼ğ‘šğœ…ğ‘š gqağ›¼ğ‘š 1 ğ‘‘ğ›¼ğ‘š 2ğœ‡ ğ‘€ ğ‘™ğ‘‘2ğ‘ğ‘¤ gqa2 = 0 G.5. Key Derivation: Activation Rate Equation 84 and Equation 86: Canceling common factors: ğ›¼ğœŒğœ…ğœŒğœŒğ›¼ğœŒ+1 3ğ‘Ÿğ›¼ğ‘Ÿ+1ğ‘‘ ğ›½1+2ğ‘ğ‘¤ = ğ›¼ğ‘Ÿ ğ·ğœŒ 3ğ‘Ÿğ›¼ğ‘Ÿ+1ğ‘‘ ğ›½2+2ğ‘ğ‘¤ ğ›¼ğœŒğœ…ğœŒğœŒğ›¼ğœŒ ğ‘‘ ğ›½1 ğ›¼ğ‘Ÿ ğ· ğ‘‘ ğ›½ = Substituting ğ· = ğœ…ğœŒğœŒğ›¼ğœŒ ğ‘‘ ğ›½2 ğ›½1 + ğœ…ğ‘‘: ğ›¼ğœŒğœ…ğœŒğœŒğ›¼ğœŒ ğ‘‘ ğ›½2 ğ›½1 = ğ›¼ğ‘Ÿğœ…ğœŒğœŒğ›¼ğœŒ ğ‘‘ ğ›½2 ğ›½1 + ğ›¼ğ‘Ÿğœ…ğ‘‘ Collecting terms: Solving: Validity requires ğ›¼ğœŒ > ğ›¼ğ‘Ÿ. G.6. Remaining Solutions Depth. (ğ›¼ğœŒ ğ›¼ğ‘Ÿ)ğœ…ğœŒğœŒğ›¼ğœŒ ğ‘‘ ğ›½2 ğ›½1 = ğ›¼ğ‘Ÿğœ…ğ‘‘ ğœŒ = (cid:20) ğ›¼ğ‘Ÿğœ…ğ‘‘ (ğ›¼ğœŒ ğ›¼ğ‘Ÿ)ğœ…ğœŒ (cid:21) 1/ğ›¼ğœŒ ğ‘‘ ( ğ›½1 ğ›½2 )/ğ›¼ğœŒ ğ‘™ = ğ‘€budget ğœ‰all ğ‘Š ğ‘‘2ğ‘ğ‘¤ Multiplier. FFN Ratio. ğœ‡ ğ‘€ ğ‘™ = ğ›¼ğ‘™ğœ…ğ‘™ (ğœ‰all ğ‘Š )ğ›¼ğ‘™ 1ğ‘‘2(ğ›¼ğ‘™ 1) ğ‘ğ›¼ğ‘™ 1 ğ‘¤ ğ‘€ğ›¼ğ‘™ budget ğ‘€ğ›¼ğ‘™ budget 1 ğ›¼ğ‘Ÿ +1 (cid:35) (ğœ‰all ğ‘Š )ğ›¼ğ‘™ 1ğ‘‘2ğ›¼ğ‘™+ğ›½2 ğ‘ğ›¼ğ‘™ ğ‘¤ ğ‘Ÿ = (cid:34) ğ›¼ğ‘Ÿ ğ·ğœŒ 3ğ›¼ğ‘™ğœ…ğ‘™ (85) (86) (87) (88) (89) (90) (91) (92) (93) (94) (95) 40 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs GQA Ratio. gqa = (cid:34) 2ğ›¼ğ‘™ğœ…ğ‘™ ğ›¼ğ‘šğœ…ğ‘š G.7. Solution Summary (ğœ‰all ğ‘Š )ğ›¼ğ‘™ 1ğ‘‘2ğ›¼ğ‘™+ğ›¼ğ‘š ğ‘ğ›¼ğ‘™ ğ‘¤ 1 ğ›¼ğ‘š+1 (cid:35) ğ‘€ğ›¼ğ‘™ budget ğœŒ = (cid:20) ğ›¼ğ‘Ÿğœ…ğ‘‘ (ğ›¼ğœŒ ğ›¼ğ‘Ÿ)ğœ…ğœŒ (cid:21) 1/ğ›¼ğœŒ ğ‘‘ ( ğ›½1 ğ›½2 )/ğ›¼ğœŒ ğ‘™ = ğ‘€budget ğœ‰all ğ‘Š ğ‘‘2ğ‘ğ‘¤ ğ‘Ÿ = (cid:34) ğ›¼ğ‘Ÿ ğ·ğœŒ 3ğ›¼ğ‘™ğœ…ğ‘™ gqa = (cid:34) 2ğ›¼ğ‘™ğœ…ğ‘™ ğ›¼ğ‘šğœ…ğ‘š ğ‘€ğ›¼ğ‘™ budget 1 ğ›¼ğ‘Ÿ +1 (cid:35) (ğœ‰all ğ‘Š )ğ›¼ğ‘™ 1ğ‘‘2ğ›¼ğ‘™+ğ›½2 ğ‘ğ›¼ğ‘™ ğ‘¤ (ğœ‰all ğ‘Š )ğ›¼ğ‘™ 1ğ‘‘2ğ›¼ğ‘™+ğ›¼ğ‘š ğ‘ğ›¼ğ‘™ ğ‘¤ 1 ğ›¼ğ‘š+1 (cid:35) ğ‘€ğ›¼ğ‘™ budget (96) (97) (98) (99) (100) where ğœ‰all in the main text. ğ‘Š = 2 + 2/gqa + 3ğ‘Ÿ/ğœŒ (implicit). The activation rate result corresponds to Theorem 5.2 H. Case D3: Decode, Dual-Constrained H.1. Problem Statement min ğ‘™,ğ‘‘,ğ‘Ÿ,gqa,ğœŒ Ë†L (ğœƒ) s.t. ğ‘™ ğœ‰dec ğ‘Š ğ‘‘2 ğ‘ğ‘¤ + 2ğ‘™ ğ‘† ğ‘‘ ğ‘ğ‘˜ğ‘£ gqa = ğ‘€ğ‘‘ ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ = ğ‘€budget H.2. Constraint Compatibility Define: From the two constraints: Dividing: Expanding: Î“ ğœ‰dec ğ‘Š ğ‘‘2 ğ‘ğ‘¤ + 2ğ‘† ğ‘‘ ğ‘ğ‘˜ğ‘£ gqa ğ‘™ Î“ = ğ‘€ğ‘‘, ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ = ğ‘€budget Î“ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ = ğ‘€ğ‘‘ ğ‘€budget ğœ‚ ğ‘Š ğ‘‘2 ğ‘ğ‘¤ + 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ ğœ‰dec ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ gqa = ğœ‚ (101) (102) (103) (104) (105) 41 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs ğœ‰dec ğ‘Š ğœ‰all ğ‘Š + 2ğ‘† ğ‘ğ‘˜ğ‘£ ğœ‰all ğ‘Š gqa ğ‘‘ ğ‘ğ‘¤ = ğœ‚ H.3. Derivation of Activation Rate Substituting ğœ‰dec ğ‘Š = ğ›¼attn + 3ğ‘Ÿ and ğœ‰all ğ‘Š = ğ›¼attn + 3ğ‘Ÿ/ğœŒ where ğ›¼attn = 2 + 2/gqa: ğ›¼attn + 3ğ‘Ÿ ğ›¼attn + 3ğ‘Ÿ ğœŒ + (cid:16) 2ğ‘† ğ‘ğ‘˜ğ‘£ ğ›¼attn + 3ğ‘Ÿ ğœŒ (cid:17) gqa ğ‘‘ ğ‘ğ‘¤ = ğœ‚ Define the KV-cache correction term: Then: This gives: Rearranging: ğ›¿ 2ğ‘† ğ‘ğ‘˜ğ‘£ gqa ğ‘‘ ğ‘ğ‘¤ ğ›¼attn + 3ğ‘Ÿ + ğ›¿ ğœ‰all ğ‘Š ğœ‰all ğ‘Š = ğœ‚ ğ›¼attn + 3ğ‘Ÿ + = ğœ‚ğœ‰all ğ‘Š = ğœ‚ (cid:18) ğ›¼attn + (cid:19) 3ğ‘Ÿ ğœŒ ğ›¿ ğœ‰all ğ‘Š ğ›¼attn(1 ğœ‚) + 3ğ‘Ÿ + ğ›¿ ğ›¼attn + 3ğ‘Ÿ ğœŒ 3ğœ‚ğ‘Ÿ ğœŒ = This is an implicit equation for ğœŒ due to the presence of ğœŒ in the ğ›¿ terms denominator. Simplified Form. Multiplying through by ğœ‰all ğ‘Š = ğ›¼attn + 3ğ‘Ÿ/ğœŒ: (ğ›¼attn + 3ğ‘Ÿ)ğœ‰all ğ‘Š + ğ›¿ = ğœ‚(ğœ‰all ğ‘Š )2 Let ğ‘¥ = ğœ‰all ğ‘Š : Solving the quadratic: (ğ›¼attn + 3ğ‘Ÿ)ğ‘¥ + ğ›¿ = ğœ‚ğ‘¥ ğœ‚ğ‘¥2 (ğ›¼attn + 3ğ‘Ÿ)ğ‘¥ ğ›¿ = 0 ğœ‰all ğ‘Š = (ğ›¼attn + 3ğ‘Ÿ) + (ğ›¼attn + 3ğ‘Ÿ)2 + 4ğœ‚ğ›¿ 2ğœ‚ From ğœ‰all ğ‘Š = ğ›¼attn + 3ğ‘Ÿ/ğœŒ: 3ğ‘Ÿ ğœŒ = ğœ‰all ğ‘Š ğ›¼attn (106) (107) (108) (109) (110) (111) (112) (113) (114) (115) (116) 42 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs ğœŒ = 3ğ‘Ÿ ğœ‰all ğ‘Š ğ›¼attn = 6ğœ‚ğ‘Ÿ (ğ›¼attn + 3ğ‘Ÿ) 2ğœ‚ğ›¼attn + (ğ›¼attn + 3ğ‘Ÿ)2 + 4ğœ‚ğ›¿ (117) where ğ›¿ = 2ğ‘†ğ‘ğ‘˜ğ‘£/(gqa ğ‘‘ ğ‘ğ‘¤) and ğ›¼attn = 2 + 2/gqa. H.4. Special Case: ğ›¿ 0 When the KV-cache term is negligible (ğ›¿ 0): ğœ‰all ğ‘Š ğ›¼attn + 3ğ‘Ÿ ğœ‚ ğœŒ 3ğœ‚ğ‘Ÿ ğ›¼attn + 3ğ‘Ÿ ğœ‚ğ›¼attn = 3ğœ‚ğ‘Ÿ ğ›¼attn(1 ğœ‚) + 3ğ‘Ÿ This recovers the simplified formula from the weight-dominated approximation. H.5. Remaining Solutions Depth. ğ‘™ = ğ‘€budget ğœ‰all ğ‘Š ğ‘‘2ğ‘ğ‘¤ (118) (119) (120) Other Variables. Unlike the single-constraint cases (D1, D2, P1), where single Lagrange multiplier allows sequential elimination, the dual-constrained case involves two active constraints with multipliers ğœ‡ğ‘‡ and ğœ‡ ğ‘€. The stationarity conditions for ğ‘Ÿ and gqa each depend on both multipliers, and the KVcache term in the decode constraint further couples gqa to the latency budget. As result, ğ‘Ÿ, gqa, and ğ‘‘ do not admit independent closed-form expressions and must be obtained by numerically solving the coupled KKT system. In practice, ğœŒ and ğ‘™ from the equations above are substituted first, reducing the system to three unknowns. H.6. Solution Summary ğœ‚ = ğ‘€ğ‘‘/ğ‘€budget ğ›¿ = ğœ‰all ğ‘Š = 2ğ‘†ğ‘ğ‘˜ğ‘£ gqa ğ‘‘ ğ‘ğ‘¤ (ğ›¼attn + 3ğ‘Ÿ) + (ğ›¼attn + 3ğ‘Ÿ)2 + 4ğœ‚ğ›¿ 2ğœ‚ ğœŒ = ğ‘™ = 3ğ‘Ÿ ğœ‰all ğ‘Š ğ›¼attn ğ‘€budget ğœ‰all ğ‘Š ğ‘‘2ğ‘ğ‘¤ (121) (122) (123) (124) (125) where ğ›¼attn = 2 + 2/gqa, and ğ‘Ÿ, gqa are coupled solutions. The activation rate result corresponds to Theorem 5.3(b) in the main text. Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs I. Case P1: Prefill, Latency-Constrained I.1. Problem Statement min ğ‘™,ğ‘‘,ğ‘Ÿ,gqa,ğœŒ Ë†L (ğœƒ) s.t. ğ‘™ ğœ‰ğ¹ ğ‘‘2 = ğ¹ ğ‘ ğœŒ ğœŒmin where ğœ‰ğ¹ = 4 + 4/gqa + 6ğ‘Ÿ. Note: The prefill constraint is compute-bound and does not involve KV-cache traffic. I.2. Lagrangian â„’ = Ë†L (ğœƒ) + ğœ‡ğ‘‡ (cid:16) ğ‘™ ğœ‰ğ¹ ğ‘‘2 ğ¹ ğ‘ (cid:17) I.3. Constraint Partial Derivatives ğ‘”ğ‘‡ ğ‘™ ğ‘”ğ‘‡ ğ‘‘ ğ‘”ğ‘‡ ğ‘Ÿ = ğœ‰ğ¹ğ‘‘2 = 2ğ‘™ğœ‰ğ¹ğ‘‘ = 6ğ‘™ğ‘‘2 ğ‘”ğ‘‡ gqa = 4ğ‘™ğ‘‘2 gqa2 ğ‘”ğ‘‡ ğœŒ = ğ›¼ğ‘™ğœ…ğ‘™ ğ‘™ğ›¼ğ‘™+1 + ğœ‡ğ‘‡ ğœ‰ğ¹ğ‘‘2 = 0 ğœ‡ğ‘‡ = ğ›¼ğ‘™ğœ…ğ‘™ ğ‘™ğ›¼ğ‘™+1ğœ‰ğ¹ğ‘‘2 I.4. KKT Conditions Stationarity for ğ‘™: Stationarity for ğœŒ: Since ğ‘”ğ‘‡ /ğœŒ = 0 and Ë†L/ğœŒ > 0: Stationarity for ğ‘Ÿ: â„’ ğœŒ = ğ›¼ğœŒğœ…ğœŒğœŒğ›¼ğœŒ1 ğ‘Ÿğ›¼ğ‘Ÿ ğ‘‘ ğ›½1 > ğœŒ = ğœŒmin ğ›¼ğ‘Ÿ ğ· ğ‘Ÿğ›¼ğ‘Ÿ+1ğ‘‘ ğ›½2 + 6ğœ‡ğ‘‡ ğ‘™ğ‘‘2 = 0 (126) (127) (128) (129) (130) (131) (132) (133) (134) (135) (136) 44 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Note: coefficient is 6 (vs 3 in D1) from ğœ‰ğ¹/ğ‘Ÿ = 6. The ğ‘ğ‘¤ factor is absent because the prefill constraint operates in FLOPs rather than bytes, yielding ğ‘”ğ‘‡ /ğ‘Ÿ = 6ğ‘™ğ‘‘2 without the byte-width scaling present in the decode case (ğ‘”ğ‘‡ /ğ‘Ÿ = 3ğ‘™ğ‘‘2ğ‘ğ‘¤). Stationarity for gqa: ğ›¼ğ‘šğœ…ğ‘š gqağ›¼ğ‘š 1 ğ‘‘ğ›¼ğ‘š 4ğœ‡ğ‘‡ ğ‘™ğ‘‘2 gqa2 = 0 Note: coefficient is 4 (vs 2 in D1) from ğœ‰ğ¹/gqa = 4/gqa2. I.5. Solution Derivation Depth. ğ‘™ = ğ¹ ğ‘ ğœ‰ğ¹ğ‘‘2 Multiplier. FFN Ratio. GQA Ratio. ğœ‡ğ‘‡ ğ‘™ = ğ‘‘2(ğ›¼ğ‘™ 1) ğ›¼ğ‘™ğœ…ğ‘™ğœ‰ğ›¼ğ‘™ 1 ğ¹ğ›¼ğ‘™ ğ‘ ğ¹ ğ‘Ÿ = (cid:34) ğ›¼ğ‘Ÿ ğ· 6ğ›¼ğ‘™ğœ…ğ‘™ ğ¹ğ›¼ğ‘™ ğ‘ ğ‘‘2ğ›¼ğ‘™+ğ›½2 ğœ‰ğ›¼ğ‘™ 1 ğ¹ (cid:35) 1 ğ›¼ğ‘Ÿ +1 gqa = (cid:34) 4ğ›¼ğ‘™ğœ…ğ‘™ ğ›¼ğ‘šğœ…ğ‘š ğœ‰ğ›¼ğ‘™ 1 ğ¹ ğ‘‘2ğ›¼ğ‘™+ğ›¼ğ‘š ğ¹ğ›¼ğ‘™ ğ‘ 1 ğ›¼ğ‘š+1 (cid:35) (137) (138) (139) (140) (141) I.6. Comparison with Case D1 Table 18 Comparison of prefill (P1) and decode (D1) phase optimal hyperparameters. Decode phase requires larger expansion ratio (ğ‘Ÿ coefficient doubles) and smaller GQA groups (gqa coefficient halves) due to memory-bound constraints. P1 (Prefill) D1 (Decode) ğ‘Ÿ coefficient gqa coefficient Constraint 1/6 4 ğœ‰ğ¹ 1/3 2 ğœ‰dec ğ‘Š + KV term Table 18 reveals how optimal architectural choices differ between prefill and decode phases. In prefill (P1), the compute-bound regime (ğœ‰ğ¹ constraint) favors smaller expansion ratios and larger GQA groups. In decode (D1), memory bandwidth constraints (ğœ‰dec ğ‘Š plus KV-cache access) necessitate doubling the expansion ratio coefficient (from 1/6 to 1/3) while halving the GQA coefficient (from 4 to 2), reflecting the need to balance computation against memory access costs. Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs I.7. Solution Summary ğœŒ = ğœŒmin ğ¹ ğ‘ ğœ‰ğ¹ğ‘‘2 ğ‘™ = ğ‘Ÿ = gqa = (cid:34) ğ›¼ğ‘Ÿ ğ· 6ğ›¼ğ‘™ğœ…ğ‘™ (cid:34) 4ğ›¼ğ‘™ğœ…ğ‘™ ğ›¼ğ‘šğœ…ğ‘š (cid:35) 1 ğ›¼ğ‘Ÿ +1 ğ¹ğ›¼ğ‘™ ğ‘ ğ‘‘2ğ›¼ğ‘™+ğ›½ ğœ‰ğ›¼ğ‘™ 1 ğ¹ 1 ğ›¼ğ‘š+1 (cid:35) ğœ‰ğ›¼ğ‘™ 1 ğ¹ ğ‘‘2ğ›¼ğ‘™+ğ›¼ğ‘š ğ¹ğ›¼ğ‘™ ğ‘ (142) (143) (144) (145) where ğœ‰ğ¹ = 4 + 4/gqa + 6ğ‘Ÿ (implicit). The activation rate result corresponds to Theorem 5.1 in the main text. J. Case P2: Prefill, Memory-Constrained J.1. Problem Statement min ğ‘™,ğ‘‘,ğ‘Ÿ,gqa,ğœŒ Ë†L (ğœƒ) s.t. ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ = ğ‘€budget (146) where ğœ‰all ğ‘Š = 2 + 2/gqa + 3ğ‘Ÿ/ğœŒ. J.2. Equivalence to Case D2 The Lagrangian is: â„’ = Ë†L (ğœƒ) + ğœ‡ ğ‘€ (cid:16) ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ ğ‘€budget (cid:17) (147) This is identical to Case D2 because: 1. The loss function Ë†L (ğœƒ) is independent of scenario 2. The memory constraint concerns model storage, not runtime behavior 3. ğœŒ appears only in ğœ‰all ğ‘Š All KKT conditions and solutions are identical to Case D2. 46 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs J.3. Solution Summary ğœŒ = (cid:20) ğ›¼ğ‘Ÿğœ…ğ‘‘ (ğ›¼ğœŒ ğ›¼ğ‘Ÿ)ğœ…ğœŒ (cid:21) 1/ğ›¼ğœŒ ğ‘‘ ( ğ›½1 ğ›½2 )/ğ›¼ğœŒ ğ‘™ = ğ‘€budget ğœ‰all ğ‘Š ğ‘‘2ğ‘ğ‘¤ ğ‘Ÿ = (cid:34) ğ›¼ğ‘Ÿ ğ·ğœŒ 3ğ›¼ğ‘™ğœ…ğ‘™ gqa = (cid:34) 2ğ›¼ğ‘™ğœ…ğ‘™ ğ›¼ğ‘šğœ…ğ‘š ğ‘€ğ›¼ğ‘™ budget 1 ğ›¼ğ‘Ÿ +1 (cid:35) (ğœ‰all ğ‘Š )ğ›¼ğ‘™ 1ğ‘‘2ğ›¼ğ‘™+ğ›½2 ğ‘ğ›¼ğ‘™ ğ‘¤ (ğœ‰all ğ‘Š )ğ›¼ğ‘™ 1ğ‘‘2ğ›¼ğ‘™+ğ›¼ğ‘š ğ‘ğ›¼ğ‘™ ğ‘¤ 1 ğ›¼ğ‘š+1 (cid:35) ğ‘€ğ›¼ğ‘™ budget (148) (149) (150) (151) All solutions identical to Case D2. The activation rate result corresponds to Theorem 5.2 in the main text. K. Case P3: Prefill, Dual-Constrained K.1. Problem Statement min ğ‘™,ğ‘‘,ğ‘Ÿ,gqa,ğœŒ Ë†L (ğœƒ) s.t. ğ‘™ ğœ‰ğ¹ ğ‘‘2 = ğ¹ ğ‘ ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ = ğ‘€budget Note: The prefill constraint is compute-bound and does not involve KV-cache. K.2. Constraint Compatibility Dividing the two constraints: ğœ‰ğ¹ ğ‘ğ‘¤ ğœ‰all ğ‘Š ğ¹ ğ‘ ğ‘€budget = ğœ‚ ğ‘ K.3. Derivation of Activation Rate Using ğœ‰ğ¹ = 2(ğ›¼attn + 3ğ‘Ÿ) and ğœ‰all ğ‘Š = ğ›¼attn + 3ğ‘Ÿ/ğœŒ: Cross-multiplying: Rearranging: 2(ğ›¼attn + 3ğ‘Ÿ) ğ‘ğ‘¤(ğ›¼attn + 3ğ‘Ÿ ğœŒ ) = ğœ‚ ğ‘ 2ğ›¼attn + 6ğ‘Ÿ = ğœ‚ ğ‘ğ‘ğ‘¤ğ›¼attn + 3ğœ‚ ğ‘ğ‘ğ‘¤ğ‘Ÿ ğœŒ ğ›¼attn(2 ğœ‚ ğ‘ğ‘ğ‘¤) + 6ğ‘Ÿ = 3ğœ‚ ğ‘ğ‘ğ‘¤ğ‘Ÿ ğœŒ (152) (153) (154) (155) (156) 47 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Solving: ğœŒ = 3ğœ‚ ğ‘ğ‘ğ‘¤ğ‘Ÿ ğ›¼attn(2 ğœ‚ ğ‘ğ‘ğ‘¤) + 6ğ‘Ÿ (157) where ğœ‚ ğ‘ = ğ¹ ğ‘/ğ‘€budget and ğ›¼attn = 2 + 2/gqa. Validity requires ğœ‚ ğ‘ğ‘ğ‘¤ < 2. This result corresponds to Theorem 5.3(a) in the main text. K.4. Comparison with Case D3 Table 19 Comparison of prefill (P3) and decode (D3) phase characteristics. Decode includes KV-cache memory access costs and requires solving quadratic equation for optimal depth-width ratio ğœŒ. ğœ‚ definition Latency constraint KV-cache term P3 (Prefill) ğ¹ ğ‘/ğ‘€budget ğ‘™ğœ‰ğ¹ğ‘‘2 absent D3 (Decode) ğ‘€ğ‘‘/ğ‘€budget ğ‘Š ğ‘‘2ğ‘ğ‘¤ + 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ gqa ) present ğ‘™(ğœ‰dec ğœŒ formula closed-form involves quadratic As shown in Table 19, the key difference between prefill and decode phases lies in the memory access patterns and their impact on the optimal architecture. In prefill (P3), the latency constraint depends only on compute (ğœ‰ğ¹ğ‘‘2) and admits closed-form solution for ğœŒ. In decode (D3), the latency constraint includes both weight loading and KV-cache access, with the KV-cache term 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ gqa becoming dominant for large sequence lengths. This additional complexity requires solving quadratic equation to find the optimal depth-width ratio. K.5. Remaining Solutions Depth. ğ‘™ = ğ¹ ğ‘ ğœ‰ğ¹ğ‘‘2 Other Variables. The solutions for ğ‘Ÿ, gqa, ğ‘‘ require solving the coupled KKT system. K.6. Solution Summary ğœ‚ ğ‘ = ğ¹ ğ‘/ğ‘€budget, ğœ‚ ğ‘ğ‘ğ‘¤ < 2 ğœŒ = ğ‘™ = 3ğœ‚ ğ‘ğ‘ğ‘¤ğ‘Ÿ ğ›¼attn(2 ğœ‚ ğ‘ğ‘ğ‘¤) + 6ğ‘Ÿ ğ¹ ğ‘ ğœ‰ğ¹ğ‘‘2 (158) (159) (160) (161) where ğ›¼attn = 2 + 2/gqa, and ğ‘Ÿ, gqa are coupled solutions. The activation rate result corresponds to Theorem 5.3(a) in the main text. 48 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs L. Summary L.1. Constraint Forms The constraint forms are shown in Table 20. Table 20 Constraint formulations. Constraint Prefill latency Decode latency Memory Formula ğ‘™ ğœ‰ğ¹ ğ‘‘2 ğ¹ ğ‘ ğ‘Š ğ‘‘2 ğ‘ğ‘¤ + 2ğ‘™ ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ ğ‘™ ğœ‰all ğ‘Š ğ‘‘2 ğ‘ğ‘¤ ğ‘€budget gqa ğ‘™ ğœ‰dec ğ‘€ğ‘‘ L.2. Solution Matrix: Activation Rate ğœŒ Table 21 Solution Matrix: Activation Rate ğœŒ. Latency (cid:104) Decode Prefill ğœŒmin ğœŒmin Memory (cid:105) 1/ğ›¼ğœŒ Dual ğ›½1 ğ›½2 ğ›¼ğœŒ quadratic form ğ‘‘ ğ›¼ğ‘Ÿğœ…ğ‘‘ (ğ›¼ğœŒğ›¼ğ‘Ÿ )ğœ…ğœŒ same as Decode 3ğœ‚ ğ‘ğ‘ğ‘¤ğ‘Ÿ ğ›¼attn (2ğœ‚ ğ‘ğ‘ğ‘¤ )+6ğ‘Ÿ The activation rate ğœŒ in solution matrix are shown in Table 21. L.3. Solution Matrix: Depth ğ‘™ The Depth ğ‘™ in solution matrix are shown in Table 22. Table 22 Solution Matrix: Depth ğ‘™. Latency / Dual Memory Decode"
        },
        {
            "title": "Prefill",
            "content": "ğ‘€ğ‘‘ ğœ‰dec ğ‘Š ğ‘‘2ğ‘ğ‘¤+ ğ¹ ğ‘ ğœ‰ğ¹ ğ‘‘2 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£ gqa ğ‘€budget ğœ‰all ğ‘Š ğ‘‘2ğ‘ğ‘¤ ğ‘€budget ğœ‰all ğ‘Š ğ‘‘2ğ‘ğ‘¤ L.4. Coefficient Comparison The coefficient comparison are shown in Table 23. Table 23 Coefficient Comparison. ğ‘Ÿ coefficient Prefill Decode gqa coefficient Prefill Decode"
        },
        {
            "title": "Latency\nMemory",
            "content": "1/6 1/3 1/3 1/3 4 2 2 2 49 Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs L.5. Key Results 1. Width-Sparsity Scaling. Under memory constraint: ğœŒ = (cid:20) ğ›¼ğ‘Ÿğœ…ğ‘‘ (ğ›¼ğœŒ ğ›¼ğ‘Ÿ)ğœ…ğœŒ (cid:21) 1/ğ›¼ğœŒ ğ‘‘ ( ğ›½1 ğ›½2 )/ğ›¼ğœŒ (162) 2. Scenario Independence. Memory-constrained ğœŒ is identical for Prefill and Decode. 3. Prefill-Decode Asymmetry. Latency-constrained coefficients differ since ğœ‰ğ¹/ğ‘Ÿ = 6, comparing to ğœ‰dec ğ‘Š /ğ‘Ÿ = 3. 4. KV-Cache Effect. The decode latency constraint includes the KV-cache term 2ğ‘™ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£/gqa, which: Increases effective constraint tightness Couples gqa more strongly to the constraint Leads to quadratic form in D3 dual-constrained case L.6. Notation Table 24 Notation Extension for Appendices. ğœ‰ğ¹ ğœ‰all ğ‘Š ğ›¼attn ğ¹ ğ‘ ğœ‚ Î“ 4 + 4/gqa + 6ğ‘Ÿ 2 + 2/gqa + 3ğ‘Ÿ/ğœŒ 2 + 2/gqa ğœ‰dec ğ‘Š ğ· ğ‘† ğ‘€ğ‘‘ ğ‘‡latğœ‹ğ»/(ğµğ‘†in) ğ‘€ğ‘‘/ğ‘€budget ğœ‚ ğ‘ ğ‘Š ğ‘‘2ğ‘ğ‘¤ + 2ğ‘†ğ‘‘ğ‘ğ‘˜ğ‘£/gqa ğœ‰dec 2 + 2/gqa + 3ğ‘Ÿ ğœ…ğœŒğœŒğ›¼ğœŒ ğ‘‘ ğ›½2 ğ›½1 + ğœ…ğ‘‘ average context length ğ‘‡lat ğ›½ğ»/ğ‘†out ğ¹ ğ‘/ğ‘€budget Here we present the notation Table 24 used in the Appendices, as an extension to Table 7."
        }
    ],
    "affiliations": [
        "AI Lab, The Yangtze River Delta",
        "Institution of Automation, Chinese Academy of Sciences",
        "Li Auto",
        "The University of Edinburgh",
        "University College London",
        "University of Chinese Academy of Sciences"
    ]
}