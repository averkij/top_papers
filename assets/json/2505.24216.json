{
    "paper_title": "Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation",
    "authors": [
        "Prasanna Reddy Pulakurthi",
        "Majid Rabbani",
        "Jamison Heard",
        "Sohail Dianat",
        "Celso M. de Melo",
        "Raghuveer Rao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work investigates Source-Free Domain Adaptation (SFDA), where a model adapts to a target domain without access to source data. A new augmentation technique, Shuffle PatchMix (SPM), and a novel reweighting strategy are introduced to enhance performance. SPM shuffles and blends image patches to generate diverse and challenging augmentations, while the reweighting strategy prioritizes reliable pseudo-labels to mitigate label noise. These techniques are particularly effective on smaller datasets like PACS, where overfitting and pseudo-label noise pose greater risks. State-of-the-art results are achieved on three major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS, improvements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target and multi-target settings, respectively, while gains of 2.8% and 0.7% are attained on DomainNet-126 and VisDA-C. This combination of advanced augmentation and robust pseudo-label reweighting establishes a new benchmark for SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 6 1 2 4 2 . 5 0 5 2 : r SHUFFLE PATCHMIX AUGMENTATION WITH CONFIDENCE-MARGIN WEIGHTED PSEUDO-LABELS FOR ENHANCED SOURCE-FREE DOMAIN ADAPTATION Prasanna Reddy Pulakurthi1, Majid Rabbani1, Jamison Heard1, Sohail Dianat1, Celso M. de Melo2, and Raghuveer Rao2 1Rochester Institute of Technology, Rochester, NY, USA 2DEVCOM Army Research Laboratory, Adelphi, MD, USA"
        },
        {
            "title": "ABSTRACT",
            "content": "This work investigates Source-Free Domain Adaptation (SFDA), where model adapts to target domain without access to source data. new augmentation technique, Shuffle PatchMix (SPM), and novel reweighting strategy are introduced to enhance performance. SPM shuffles and blends image patches to generate diverse and challenging augmentations, while the reweighting strategy prioritizes reliable pseudo-labels to mitigate label noise. These techniques are particularly effective on smaller datasets like PACS, where overfitting and pseudo-label noise pose greater risks. State-of-the-art results are achieved on three major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS, improvements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target and multi-target settings, respectively, while gains of 2.8% and 0.7% are attained on DomainNet-126 and VisDA-C. This combination of advanced augmentation and robust pseudo-label reweighting establishes new benchmark for SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM. Index Terms Source-Free Domain Adaptation, Classification, Contrastive Learning, Pseudo-Labels, Self-Training 1. INTRODUCTION Deep neural networks have achieved remarkable success in tasks where the training and test data share similar distributions. However, domain shifts can degrade network performance considerably [1]. To address this challenge, Domain Adaptation (DA) techniques have been developed to improve model performance in the target domain by leveraging knowledge from related source domain. The most studied DA paradigm, Unsupervised Domain Adaptation (UDA), adapts models from source to target domains where labeled target data is unavailable. key strategy in UDA involves aligning feature distributions between source and target domains to reduce domain discrepancies [2, 3, 4, 5]. This research was supported by DEVCOM Army Research Laboratory under contract W911QX-21-D-0001. In many real-world scenarios, access to source data during adaptation is restricted due to privacy, security, or logistical constraints. This has led to Source-Free Domain Adaptation (SFDA), subfield of UDA where pre-trained source model is adapted to target domain without source data. The lack of source data makes adaptation more challenging, requiring innovative techniques to align the model to the target domain while retaining knowledge learned from the source domain. Advancements, such as AdaContrast [6], have demonstrated the effectiveness of self-training by pseudo-labeling, jointly trained with contrastive learning on strongly augmented target data to exploit the pairwise relationships among target samples. Despite their effectiveness, these methods heavily depend on the quality of pseudo-labels. If noisy pseudo-labels are treated equally, errors can be amplified, negatively impacting adaptation performance. To address these limitations, this work proposes two key innovations: (1) Confidence-Margin Reweighting strategy that leverages both confidence (probability of the top-1 class prediction) and margin (difference between top-1 and top2 class probabilities) of the pseudo-labels for self-training, which prioritizes reliable pseudo-labels to reduce the impact of label noise, and (2) Shuffle PatchMix (SPM) that is novel augmentation strategy to enhance target domain representation by introducing diverse and challenging augmentations. Together, these contributions lead to significant performance gains on three benchmark datasets. 2. RELATED WORK Domain Adaptation: In UDA, several methods have been proposed to reduce domain discrepancies by aligning the feature distributions between source and target domains, MMD [7], MCC [8], MDD [9], and CMD [10]. Additionally, GAN-based methods use adversarial training to align the distributions in feature space [2, 4] and image space [11]. However, all these methods require access to the source data. In contrast, SFDA methods adapt to unlabeled target data without source samples. Several significant works have been proposed to address SFDA, including leveraging entropy minimization by TENT [12], class prototypes by BAIT [13], 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. latent-prototype generation USFDA [14], and self-training by pseudo-labeling by SHOT [15]. Significant improvements are achieved by AdaContrast [6] using self-training by pseudolabels along with contrastive learning. Learning with Noisy Pseudo-Labels: The limitation of previous methods such as [6] is that all the pseudo-labels equally contribute to the loss without considering the noise associated with the pseudo-labels. Various approaches were proposed to address noisy pseudo-labels. In semi-supervised learning, FixMatch [16], fixed confidence threshold was used to filter pseudo-labels, while FreeMatch [17] improved upon this by dynamically adapting the threshold based on prediction uncertainty. meta-learning-based method [18] reweighted the loss but relied on noise-free validation set, making it unsuitable for the SFDA setting. In SFDA, NEL [19] method employed pseudo-label refinement framework that relied on an ensemble of pseudo-label predictions generated from different augmentations. While this method improved the robustness of pseudo-labels, it incurred significant computational cost due to the need for multiple augmentations and predictions. UPA [20] filters low-confidence pseudo-labels via uncertainty awareness, achieving the best results on VISDA-C. In contrast, our reweighting strategy assigns continuous weights to pseudo-labels by leveraging both confidence and margin from aggregated neighboring predictions, ensuring more stable and reliable adaptation. Data Augmentation: Data augmentation techniques are extensively utilized in domain adaptation to enhance model generalization, while contrastive learning aids in learning robust features. Recently, mixing-based data augmentation techniques have gained popularity, such as Mixup [21], CutMix [22], Manifold-Mixup [23], Tokenmix [24], and TransMix [25]. In CNN-based UDA tasks, several methods [26, 27, 28] also use mixing-based strategies by linearly mixing the source and target domain data. In the SFDA setting, notable mixing-based methods such as ProxyMix [29] construct proxy source domain by selecting confident target samples. Most recently, Improved SFDA [30] introduced learnable data augmentation via teacher-student framework, whereas SF(DA)2 [31] generates an augmentation graph in feature space to achieve the best results on DomainNet-126. Our proposed SPM augmentation differs from prior patchshuffling methods [24, 25] by seamlessly blending overlapping patches, effectively reducing blocking artifacts. In contrast to [24, 25], which mix patches across different images and were designed for supervised training, SPM operates intra-image and is inherently suited for source-free domain adaptation. By generating challenging adaptations, SPM outperforms previous methods to set new state-of-the-art. 3. METHOD In the SFDA setting, the source model gs(.) is trained on labeled source data {xi represent the i=1, where xi s}ns s, yi Fig. 1. An overview of the adaptation method. source images and yi represent the source labels. Durt}nt ing adaptation, the target images {xi i=1 are available, while the underlying target labels {yi i=1 are accessed only for evaluation. SFDA aims to train model gt(.) = ht(ft(.)), comprising of an encoder network ft(.) and classification network ht(.), to classify the target data. During adaptation, the target models parameters θt are initialized with the source models parameters θs. t}nt Adaptation Method Overview: As illustrated in Figure 1, the proposed adaptation method builds on the framework introduced in [6], with two key modifications: the integration of the SPM augmentation into the strong augmentation phase, and reweighting the self-training loss using pseudolabel confidence and margin estimation. The process begins by generating two strong augmentations, ts(xt) and s(xt), using SPM, along with weak augmentation, tw(xt). The weakly augmented image is processed through the encoder ft(.) to extract target features, which are then refined through the pseudo-labeling process to produce pseudo-labels, ˆyt. The weight associated with each pseudo-label is determined using the proposed Confidence-Margin method. These refined pseudo-labels, ˆyt, along with their weights, wxt, are used to train the model gt(.) to classify the strongly augmented data, ts(xt). The training optimizes composite loss function that includes classification, diversity, and contrastive losses. This approach enables effective adaptation to the target domain by leveraging the diverse augmentations introduced by SPM and appropriately weighting the contribution of the pseudo-labels. Shuffle PatchMix (SPM): This work introduces SPM, novel augmentation strategy that creates strong augmentations for contrastive learning. As illustrated in Figure 2, SPM first divides the target image into patches, denoted as . These patches are randomly shuffled to form xsp xp . The original and shuffled patches are linearly combined using scalar parameter λ, which is randomly generated from Beta distribution. The Beta distribution has been successfully applied in the mixing-based augmentation method [21, 27]. Additional standard strong augmentations from [32] are applied. The strength of patch mixing can be controlled with hyperparameters such as the number of patches ν, and parame2 Although applying SPM to all strongly augmented images can maximize diversity, two potential risks are introduced: reduced exposure to standard strong augmentations [32], and the generation of overly unrealistic samples that may hinder model training. Therefore, SPM is applied to large fraction ρ of the standard strong augmentations but not the entire set. Pseudo-label Refinement: During adaptation, pseudolabels are generated for the weakly augmented unlabeled target data tw(xt) using the target model initialized with source weights, allowing knowledge transfer while gradually adapting to the target domain. This is performed on per-batch basis through nearest-neighbor soft voting strategy similar to [6] using memory queue Qw representing the target feature space. For each target image, its weakly augmented version is encoded into feature vector, which is then used to find the nearest neighbors in the target feature space. The pseudo-label is refined by averaging the probabilities of these neighbors, followed by an argmax operation. To enable the nearest-neighbor search, the memory queue Qw is maintained by storing features and probability predictions of the weakly augmented target samples, updated at each mini-batch. The feature space is stabilized using slowly changing momentum model t(.). The momentum models parameters θ are initialized with source weights θs, and are updated with momentum = 0.999 at each mini-batch step according to the update rule: θ + (1 m)θt. mθ Loss Reweighting by Pseudo-Label Confidence-Margin Estimation: The pseudo-labels are obtained by averaging predictions of nearest neighbors and serve as self-supervised signals to classify strongly augmented images. Uniformly weighting all pseudo-labels can hinder adaptation, as noisy labels negatively affect the training process. To address this, we introduce the Confidence-Margin reweighting strategy, where Confidence, defined as the probability of the top prediction (ptop1), and Margin (), defined as the difference between the top two prediction probabilities ( = ptop1 ptop2) for the pseudo-labels. If all the neighbors predominantly belong to the same class, the pseudo-label is considered highly reliable and should receive high weight. This reliability is reflected in both the confidence and margin. high margin corresponds to more reliable pseudo-label, as it indicates clear distinction between the most likely class and the second most likely class. Conversely, low margin signals greater uncertainty and reduced reliability. The weights are computed as follows: wxt = ptop1 (cid:124)(cid:123)(cid:122)(cid:125) confidence (cid:124)(cid:123)(cid:122)(cid:125) margin exp(). (2) All symbols in Equation 2 are scalars, evaluated independently for each target image xt. The weighting function multiplies the confidence ptop1 and the margin , then scales their product by the exponential of margin exp(). The term exp() supplies smooth, margin-aware gain that exponentially enlarges the contribution of samples with large mar3 Fig. 2. Overview of the proposed Shuffle PatchMix method. Fig. 3. (a) Different mixing strength SPM images (top). (b) SPM images with overlapping patch blending (bottom). ters of the Beta distribution and b. The mixing parameter λ follows Beta distribution, λ Beta(a, b), where a, > 0, with the probability density function (PDF) given by: λa1(1 λ)b1 (cid:82) 1 0 ua1(1 u)b1du for 0 λ 1. (λ; a, b) = (1) The effects of varying patch mixing strengths, determined by different values of and b, are illustrated in Figure 3(a). Adaptive Mixing Strength: Our simulations demonstrate that gradually decreasing the mean of the random variable λ during the training (i.e., increasing the mixing strength) allows the model to better adapt to the SPM augmentation distribution. To achieve this, the parameter in the Beta distribution Beta(a, b) is progressively reduced from start value as to an end value ae (ae as) during the training. Mitigating Blocking Artifacts: As shown in Figure 3(a), patch shuffling introduces blocking artifacts. To address this, an overlapping patch blending approach is proposed, where patches are extracted with linear dimension 30% larger than the intended size and blended linearly with neighboring patches. This effectively reduces blocking artifacts, resulting in smoother transitions, as illustrated in Figure 3(b). Additional visualizations are provided in the supplement (link). gins, whose pseudo-labels are statistically the most trustworthy. Samples with small margins are down-weighted, so that unreliable predictions exert little influence on the gradients. This continuous weighting eliminates the need for hand-tuned confidence thresholds and focuses learning on reliable targets, leading to faster and more stable adaptation. Since pseudolabels tend to be highly noisy in the early training stages, the reweighting strategy is gradually introduced as training progresses to ensure stability. Weighted Classification Loss: The refined pseudo-labels ˆyt generated from the weakly augmented data, along with their corresponding weights wxt, are used to supervise the models prediction for the strongly augmented version of the target data, as illustrated in Figure 1. The proposed weighted classification loss is formulated as: (cid:34) (cid:35) Lce = ExtX wxt ˆyt log pc , (3) (cid:88) c= where is the number of classes and pc ability of class for the strongly-augmented image ts(xt). is the predicted probOverall Loss: The proposed weighted classification is the jointly optimized with two additional losses from [6]: contrastive Lctr loss and the diversity Ldiv loss. The total loss function is defined as: Lt = Lce + Lctr + Ldiv. (4) The diversity loss (Ldiv) serves as regularization term to prevent model collapse by promoting diverse predictions. The contrastive loss (Lctr) utilizes the SPM module to generate two augmentations, which are processed by the target and momentum encoders. memory queue stores past keys, optimizing the model by pulling positive pairs closer while pushing negative pairs apart. Some of the images from the negative pairs may share the same pseudo-label; these same-class negative pairs are excluded. For more details refer to [6]. 4. EXPERIMENTAL SETUP Datasets: The PACS [33], VisDA-C [34], and DomainNet126 [35] datasets are used to evaluate the proposed method. For DomainNet-126, seven domain shifts were constructed from four domains (Real, Sketch, Clipart, Painting) following the protocol described in [6], with the top-1 accuracy (%) and the average of the seven shifts reported. For VisDA-C, the per-class top-1 accuracies (%) and their average are compared. For PACS, the evaluation is performed following the single-target and multi-target adaptation protocol outlined in [19]. Since [6] had not originally reported results for PACS, the publicly available version of the code (after being verified on the published results in [6]) was used to generate the baseline PACS results in Tables 1 and 2, as they outperformed the best-reported results in NEL [19]. Backbone: Following the standard DA protocols, all the experiments use ResNet-101 backbone for VisDA-C, ResNet50 for DomainNet-126, and ResNet-18 for PACS. 4 Table 1. Classification accuracy (%) on PACS for the singletarget setting with ResNet-18. Legend: P: Photo, A: ArtPainting, C: Cartoon, and S: Sketch. The highest accuracies are in bold. * indicates our reproduced results using [6]. Method NEL [19] AdaContrast [6]* SPM (Ours) PA PC PS AP AC AS Avg. 72.4 98.4 82.6 79.4 98.7 81.3 86.7 99.1 89.7 56.1 77.9 86. 32.3 66.7 74.5 80.5 72.2 82.3 84.3 79.7 87.9 Table 2. Classification accuracy (%) on PACS for the multitarget setting with ResNet-18 backbone. The highest value is bolded. * indicates our reproduced results using [6]. Multi-Target UDA SF Method 15.2 1-NN 24.3 ADDA [2] 28.4 DSN [36] 31.4 ITA [37] 24.6 KD [38] 80.1 NEL [19] AdaContrast [6]* 70.1 85.2 SPM (Ours) A, C, 18.1 20.1 21.1 23.0 32.2 76.1 77.9 89.2 25.6 22.4 25.6 28.2 33.8 25.9 62.9 66.4 P, C, 19.7 17.6 25.8 27.0 46.6 82.8 72.7 76.4 22.7 18.9 26.8 28.9 57.5 49.8 72.9 81.0 22.7 32.5 29.5 35.7 35.6 96.0 95.9 97. Avg. 20.7 22.6 25.8 29.0 46.6 68.4 75.4 82.6 Hyperparameters: All hyperparameters are similar to those in [6], except for the following deviations, which were found to yield the optimal performance. The learning rate was fixed at 2 104 for the SGD optimizer, the number of nearest neighbors was set to 3, and the model was trained for 100 epochs for PACS and 50 epochs for DomainNet126 and VisDA-C. The starting value as of the Beta distribution was set to 8 for DomainNet-126 and PACS, and 4 for VisDA-C, with ρ value of 0.8 used across all experiments. SPM randomly selected the number of patches ν from {22, 42, 82, 162} in each mini-batch to enhance diversity. 5. RESULTS PACS Results: Table 1 and Table 2 summarize the performance of our proposed method on the PACS dataset under single-target and multi-target settings. For the single-target setting, our method achieves an average accuracy of 86.7%, outperforming the baseline AdaContrast [6] by significant margin of 7.3%. The most notable improvements over baseline are observed in PC (82.3% vs. 72.2%) and PS (74.5% vs. 66.7%), demonstrating superior generalization across domain shifts. In the multi-target setting, our method achieves the highest average accuracy of 82.6%, surpassing AdaContrast [6] by 7.2%. Notable improvements are observed in challenging domain shifts, including PA (85.2% vs. 70.1%) and AS (81.0% vs. 72.9%), demonstrating the models ability to adapt effectively to multiple target domains. VisDA-C Results: Table 3 presents the classification accuracy of the VisDA-C dataset. Our method outperforms the best-performing method [20] by 0.7%, achieving the highest average accuracy of 89.4%. Our method achieves the best or Table 3. Classification accuracy (%) across various methods on VisDA-C (ResNet-101). SF stands for Source-Free. The highest values are shown in bold and the second highest are underlined. SF plane Method 81.9 DANN [3] 85.2 CDAN [4] 90.8 SWD [39] 88.7 MCC [8] 97.0 CAN [40] 96.1 FixBi [26] 57.2 - Source only 94.8 MA [41] 93.7 BAIT [13] 95.3 SHOT [15] 97.0 AdaContrast [6] SF(DA)2 [31] 96.8 Improved SFDA [30] 97.5 97.0 UPA [20] 98.1 SPM (Ours) bcycl 77.7 66.9 82.5 80.3 87.2 87.8 11.1 73.4 83.2 87.5 84.7 89.3 91.4 90.4 87.9 bus 82.8 83.0 81.7 80.5 82.5 90.5 42.4 68.8 84.5 78.7 84.0 82.9 87.9 82.6 86.7 car 44.3 50.8 70.5 71.5 74.3 90.3 66.9 74.8 65.0 55.6 77.3 81.4 79.4 65.0 86.2 horse 81.2 84.2 91.7 90.1 97.8 96.8 55.0 93.1 92.9 94.1 96.7 96.8 97.2 96.7 97.7 knife mcycl 65.1 29.5 88.1 74.9 86.3 69.5 85.0 93.2 90.8 96.2 92.8 95.3 81.1 4.4 88.6 95.4 88.1 95.4 81.4 94.2 91.9 93.8 90.4 95.7 97.2 92.2 91.0 96.7 93.3 94. person 28.6 74.5 77.5 71.6 80.7 88.7 27.3 84.7 80.8 80.0 84.8 81.3 83.0 87.0 85.6 plant 51.9 83.4 87.4 89.4 96.6 97.2 57.9 89.1 90.0 91.8 94.3 95.5 96.4 96.8 95.9 sktbrd 54.6 76.0 63.6 73.8 96.3 94.2 29.4 84.7 89.0 90.7 93.1 93.7 94.2 96.5 95.6 train 82.8 81.9 85.6 85.0 87.5 90.9 86.7 83.5 84.0 86.5 94.1 88.5 91.1 89.2 95.5 truck Avg. 57.4 73.9 76.4 78.8 87.2 87.2 43.8 81.6 82.7 83.0 86.8 88.1 88.4 88.7 89.4 7.8 38.0 29.2 36.9 59.9 25.7 5.8 48.1 45.3 59.8 49.7 64.7 53.0 75.0 55. Table 4. Classification accuracy (%) on 7 domain shifts of the DomainNet-126 dataset (ResNet-50). Legend: R: Real, C: Clipart, P: Painting, and S: Sketch. Table 5. Ablation studies of sub-components of the proposed method measured by classification accuracy (%) on DomainNet-126 and PACS. SF RC RP PC CS Method 44.8 34.9 65.7 MCC [8] 46.9 62.7 55.5 - Source only 48.5 65.7 58.5 TENT [12] 60.1 68.4 67.7 SHOT [15] AdaContrast [6] 58.0 69.8 70.2 60.9 69.5 68.6 UPA [20] SF(DA)2 [31] 83.5 59.6 67.7 71.9 74.2 62.4 SPM (Ours) 41.9 53.0 57.9 66.9 68.6 67.6 67.8 72.5 SP RS PR Avg. 48.9 47.3 55.6 50.1 57.7 52.4 67.1 66.1 67.8 65.9 68.0 66.8 68.3 60.2 71.1 68.1 72.4 75.0 67.0 80.8 80.5 80.9 70.5 81. 35.3 46.3 54.0 59.9 61.5 61.5 68.8 66.4 second-best performance in 8 out of 12 classes. DomainNet-126 Results: Table 4 summarizes classification accuracy on the DomainNet-126 dataset. Our method achieves the highest average accuracy of 71.1% and outperforms the SOTA [31] by 2.8%. In addition, our method achieves the best performance on 5 of 7 domain shifts. Ablation Studies: An ablation study was conducted on DomainNet-126 and PACS to assess the impact of SPM augmentation, patch overlap, and the reweighting strategy. The results, summarized in Table 5, demonstrate the effectiveness of each component in improving classification accuracy. The baseline AdaContrast achieves an accuracy of 67.8% on DomainNet-126 and 79.4% on PACS. When the reweighting strategy is applied, accuracy improves to 69.1% as unreliable pseudo-labels are down-weighted, while higher weights are assigned to more reliable ones during self-training. The SPM augmentation enhances performance by introducing challenging augmentations that encourage the network to learn class-invariant features, increasing accuracy to 70.2%. Additionally, patch overlapping mitigates blocking artifacts, resulting in slight but consistent improvement to 70.4%. Since the strength of SPM augmentation increases progressively during training, the model is trained for an extended period, as detailed in Section 4, allowing for better adaptation to varying augmentation distributions. When all components Baseline [6] SPM Patch Reweighting DomainNet-126 Augmentation Overlap Strategy Avg. 67.8 69.1 70.2 70.4 71.1 PACS Avg. 79.4 81.8 83.8 84.4 86.7 are combined, the highest accuracy of 71.1% on DomainNet126 and 86.7% on PACS is achieved, demonstrating their complementary benefits in improving SFDA performance."
        },
        {
            "title": "The largest",
            "content": "improvement is observed in the smaller dataset, PACS, due to the following reasons. First, its limited size makes the model more prone to overfitting, so SPM is particularly effective in enhancing data diversity through patch mixing. Second, the impact of noisy pseudo-labels is more pronounced in smaller datasets, making ConfidenceMargin reweighting essential for reducing the influence of uncertain labels while emphasizing confident predictions. 6. CONCLUSION This paper introduces the SPM augmentation technique and the Confidence-Margin reweighting strategy to advance SFDA. Experimental results demonstrate that the proposed approach outperforms state-of-the-art methods across three benchmark datasets. Notably, the method is especially effective on smaller datasets like PACS, where limited target samples increase the risk of overfitting and label noise. The SPM augmentation technique enhances generalization by generating diverse training samples, while the reweighting strategy prioritizes reliable pseudo-labels, mitigating the impact of noisy predictions. Future work could explore extending SPM to broader domain adaptation paradigms, including self-supervised and semi-supervised learning. 5 7. REFERENCES [1] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil Lawrence, Dataset Shift in Machine Learning, The MIT Press, 2009. [2] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell, Adversarial discriminative domain adaptation, in CVPR, July 2017. [3] Yaroslav Ganin and Victor Lempitsky, Unsupervised domain adaptation by backpropagation, in ICML. PMLR, 2015, pp. 11801189. [4] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael Jordan, Conditional adversarial domain adaptation, NeurIPS, vol. 31, 2018. [5] Prasanna Reddy Pulakurthi, Sohail Dianat, Majid Rabbani, Suya You, and Raghuveer Rao, Unsupervised domain adaptation using feature aligned maximum classifier discrepancy, in Applications of Machine Learning 2022. SPIE, 2022, vol. 12227, pp. 3745. [6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi, Contrastive test-time adaptation, in CVPR, 2022, pp. 295305. [7] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan, Learning transferable features with deep adaptation networks, in ICML. PMLR, 2015, pp. 97105. [8] Ying Jin, Ximei Wang, Mingsheng Long, and Jianmin Wang, Minin ECCV. imum class confusion for versatile domain adaptation, Springer, 2020, pp. 464480. [9] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan, in ICML. Bridging theory and algorithm for domain adaptation, PMLR, 2019, pp. 74047413. [10] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschlager, and Susanne Saminger-Platz, Central moment discrepancy for domain-invariant representation learning, in ICLR, 2017. [11] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell, CyCADA: Cycleconsistent adversarial domain adaptation, in ICML, 2018. [12] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell, Tent: Fully test-time adaptation by entropy minimization, in ICLR, 2021. [13] Shiqi Yang, Yaxing Wang, Luis Herranz, Shangling Jui, and Joost van de Weijer, Casting bait for offline and online source-free domain adaptation, Comput. Vis. Image Underst., vol. 234, no. C, 2023. [14] Jogendra Nath Kundu, Naveen Venkat, M. V. Rahul, and R. Venkatesh Babu, Universal source-free domain adaptation, in CVPR, 2020, pp. 45434552. [15] Jian Liang, Dapeng Hu, and Jiashi Feng, Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation, in ICML. PMLR, 2020, pp. 60286039. [16] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and ChunLiang Li, Fixmatch: Simplifying semi-supervised learning with consistency and confidence, NeurIPS, vol. 33, pp. 596608, 2020. [17] Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, and Xing Xie, Freematch: Self-adaptive thresholding for semi-supervised learning, in ICLR, 2023. [18] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun, Learning to reweight examples for robust deep learning, in ICML. PMLR, 2018, pp. 43344343. [19] Waqar Ahmed, Pietro Morerio, and Vittorio Murino, Cleaning noisy labels by negative ensemble learning for source-free unsupervised domain adaptation, in WACV, 2022, pp. 356365. [20] Xi Chen, Haosen Yang, Huicong Zhang, Hongxun Yao, and Xiatian Zhu, Uncertainty-aware pseudo-label filtering for source-free unsupervised domain adaptation, Neurocomputing, vol. 575, 2024. [21] Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David LopezPaz, mixup: Beyond empirical risk minimization, ICLR, 2018. [22] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo, Cutmix: Regularization strategy to train strong classifiers with localizable features, in ICCV, 2019. [23] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio, Manifold mixup: Better representations by interpolating hidden states, in ICML. PMLR, 2019, pp. 64386447. [24] Jihao Liu, Boxiao Liu, Hang Zhou, Hongsheng Li, and Yu Liu, Tokenmix: Rethinking image mixing for data augmentation in vision transformers, in ECCV. Springer, 2022, pp. 455471. [25] Jie-Neng Chen, Shuyang Sun, Ju He, Philip HS Torr, Alan Yuille, and Song Bai, Transmix: Attend to mix for vision transformers, in CVPR, 2022, pp. 1213512144. [26] Jaemin Na, Heechul Jung, Hyung Jin Chang, and Wonjun Hwang, Fixbi: Bridging domain spaces for unsupervised domain adaptation, in CVPR, 2021, pp. 10941103. [27] Jinjing Zhu, Haotian Bai, and Lin Wang, Patch-mix transformer for unsupervised domain adaptation: game perspective, in CVPR, 2023. [28] Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang, Adversarial domain adaptation with domain mixup, in AAAI, 2020, vol. 34, pp. 65026509. [29] Yuhe Ding, Lijun Sheng, Jian Liang, Aihua Zheng, and Ran He, Proxymix: Proxy-based mixup training with label refinery for source-free domain adaptation, Neural Networks, vol. 167, pp. 92103, 2023. [30] Yu Mitsuzumi, Akisato Kimura, and Hisashi Kashima, Understanding and improving source-free domain adaptation from theoretical perspective, in CVPR, 2024, pp. 2851528524. [31] Uiwon Hwang, Jonghyun Lee, Juhyeon Shin, and Sungroh Yoon, Sf(da)2: Source-free domain adaptation through the lens of data augmentation, in ICLR, 2024. [32] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He, Improved baselines with momentum contrastive learning, arXiv preprint arXiv:2003.04297, 2020. [33] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales, Deeper, broader and artier domain generalization, in ICCV, 2017. [34] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko, Visda: The visual domain adaptation challenge, arXiv preprint arXiv:1710.06924, 2017. [35] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang, Moment matching for multi-source domain adaptation, in CVPR, 2019, pp. 14061415. [36] Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip in Domain separation networks, Krishnan, and Dumitru Erhan, NeurIPS, 2016, vol. 29, p. 343351. [37] Behnam Gholami, Pritish Sahu, Ognjen Rudovic, Konstantinos Bousmalis, and Vladimir Pavlovic, Unsupervised multi-target domain adaptation: An information theoretic approach, IEEE Transactions on Image Processing, vol. 29, pp. 39934002, 2020. [38] Le Thanh Nguyen-Meidine, Atif Belal, Madhu Kiran, Jose Dolz, Louis-Antoine Blais-Morin, and Eric Granger, Knowledge distillation methods for efficient unsupervised adaptation across multiple domains, Image and Vision Computing, vol. 108, pp. 104096, 2021. [39] Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht, Sliced wasserstein discrepancy for unsupervised domain adaptation, in CVPR, 2019, pp. 1028510295. [40] Guoliang Kang, Lu Jiang, Yi Yang, and Alexander Hauptmann, Contrastive adaptation network for unsupervised domain adaptation, in CVPR, 2019, pp. 48934902. [41] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu, Model adaptation: Unsupervised domain adaptation without source data, in CVPR, 2020, pp. 96419650."
        }
    ],
    "affiliations": [
        "DEVCOM Army Research Laboratory, Adelphi, MD, USA",
        "Rochester Institute of Technology, Rochester, NY, USA"
    ]
}