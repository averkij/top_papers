{
    "paper_title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models",
    "authors": [
        "Jing Gu",
        "Xian Liu",
        "Yu Zeng",
        "Ashwin Nagarajan",
        "Fangrui Zhu",
        "Daniel Hong",
        "Yue Fan",
        "Qianqi Yan",
        "Kaiwen Zhou",
        "Ming-Yu Liu",
        "Xin Eric Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles."
        },
        {
            "title": "Start",
            "content": "PhyWorldBench: Comprehensive Evaluation of Physical Realism in Text-to-Video Models Jing Gu 1 Xian Liu 2 Yu Zeng 2 Ashwin Nagarajan 1 Fangrui Zhu 3 Daniel Hong 1 Yue Fan 1 Qianqi Yan 1 Kaiwen Zhou 1 MingYu Liu 2 Xin Eric Wang 1 5 2 0 2 7 1 ] . [ 1 8 2 4 3 1 . 7 0 5 2 : r Figure 1. Overview of PhyWorldBench. The benchmark follows structured design, starting with 10 main physics categories, derived from physics literature and expert consultations. Each category is divided into 5 subcategories, capturing different aspects. Under each subcategory, 7 scenarios are created, with 3 prompt variations per scenario to provide varying levels of detail and complexity. The figure presents the benchmark structure, showcasing the 10 main categories and their corresponding 5 subcategories."
        },
        {
            "title": "Abstract",
            "content": "Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains critical and unresolved challenge. This paper presents PhyWorldBench, comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Adco-advisor 1University of California, Santa Cruz 2NVIDIA Research 3Northeastern University. Correspondence to: Jing Gu <jgu110@ucsc.edu>. ditionally, we introduce novel Anti-Physics category, where prompts intentionally violate realworld physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design simple yet effective method that could utilize current MLLM to evaluate the physics realism in zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated promptsspanning fundamental, composite, and anti-physics scenarioswe identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical 1 and precise, allowing for more thorough assessment of video generation models capabilities. Furthermore, we present context-aware-prompt metric using MLLM (OpenAI Team, 2024; Gemini Team, 2024), which directly assesses if the video satisfies the physics standards or not. Such evaluation not only provided an unbiased metric but also significantly reduced the evaluation cost. To examine the current status of video generation models and provide detailed analysis, we selected five proprietary modelsSora-Turbo (OpenAI, 2024), Gen-3 (Runway Team, 2024), Kling 1.6 (KlingAI, 2024), Pika 2.0 (Pika Labs Team, 2024), and Luma (Luma AI Team, 2024)along with seven open-source models: Hunyuan 720p (Kong et al., 2024), Open-Sora 2.0 (Peng et al., 2025), Open-Sora-Plan 1.3 (Lin et al., 2024), CogVideo 14b (Hong et al., 2022), Step-video-T2V (Ma et al., 2025), Wanx-2.1 (WanTeam et al., 2025), and LTX-Video (HaCohen et al., 2024). We generated 12,600 videos to evaluate SOTA models and analyze their ability to simulate real-world physics. We identify challenges, difficult scenarios, and key physics categories while proposing structured approach to improve physical realism through prompt design. The insights obtained from this benchmark will contribute to the development of more robust and physically accurate video generation models, addressing both fundamental scientific questions and practical challenges in the field. Figure 2 presents the overall benchmark results on PhyWorldBench. Despite recent advancements, models continue to struggle with temporal consistency, realistic motion, and physical plausibility, emphasizing the need for further research to enhance their physics correctness for real-world simulation. Our work provides benchmark for video generation models with focus on physical realism. The key contributions are: We propose PhyWorldBench, large-scale, multidimensional physics benchmark for evaluating the physics ability of the video generation model. We conduct an extensive evaluation of twelve state-of-theart video generation models (five proprietary and seven open-source) with 12,600 generated videos and identified key challenges in simulating real-world physics. We study the effect of prompt variation on the performance of the video generation model and provided prompt guidelines for generating physics-following videos. 2. Related Work Benchmarks for Text-to-Video Generation. Proprietary video generators (OpenAI, 2024; KlingAI, 2024; Pika Labs Team, 2024; Runway Team, 2024; HailuoAI, 2024; Luma AI Team, 2024) achieve striking quality and temporal coherence but remain opaque. Open-source counterparts (Peng Figure 2. Success rates of video generation models on PhyWorldBench. Among open-source models, Wanx demonstrated the highest performance, while Pika achieved the best results among proprietary models with success rate of 0.262. Despite these advancements, substantial progress remains necessary to refine the capability of these models to accurately simulate the intricate dynamics of the real world. phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles. 1. Introduction The field of video generation has made remarkable progress, with models producing visually compelling and often photorealistic outputs. These advances have enabled transformative applications across industries such as entertainment, education, and scientific visualization. However, despite their visual fidelity, do video generation models truly understand the laws of physics in the real world? To answer this question, we introduce PhyWorldBench, rigorous benchmark designed to evaluate how well video generation models can simulate real-world physics. As illustrated in Figure 1, PhyWorldBench systematically tests models across multiple levels of physical phenomena, from fundamental concepts like object motion to complex dynamics, including rigid body interactions and human/animal motion. Additionally, we propose novel Anti-Physics category, where prompts deliberately violate real-world physics. On one hand, this design verifies whether models genuinely understand physical lawsrather than merely reproducing patterns from real-world training data. On the other hand, anti-physics content itself holds practical value in creative applications, where imaginative or otherwise impossible scenarios are beneficial. We meticulously designed and annotated 1,050 prompts and the standard set for each prompt individually to cover broad range of physical scenarios. This substantial annotation work ensures that our benchmark is both comprehensive 2 et al., 2025; Lin et al., 2024; Hong et al., 2022; Wang et al., 2023; HaCohen et al., 2024; Kong et al., 2024; Agarwal et al., 2025) enable reproducible research and flexible benchmarking. The development of Text-to-Video (T2V) Generation models has been accelerated by benchmarks that evaluate performance across diverse aspects like quality, realism, and compositionality. VBench (Huang et al., 2024) and EvalCrafter (Liu et al., 2024) provide comprehensive frameworks for assessing video generation models, focusing on metrics such as diversity, temporal coherence, and scalability. Besides, T2V-CompBench (Sun et al., 2024) emphasizes compositionality, ensuring generated videos align with intricate textual prompts, key challenge in real-world applications. Recent works emphasize the importance of physical plausibility in video generation. VideoPhy (Bansal et al., 2024) and PhyGenBench (Meng et al., 2024) assess models adherence to physical commonsense, while DEVIL (Liao et al., 2024) evaluates dynamic realism, including motion and temporal consistency. Physics-IQ (Motameda et al., 2025) further tests understanding of fundamental principles like fluid dynamics and thermodynamics, highlighting the demand for physics-aware video generation. We introduce novel and substantially expanded dataset for evaluating video generation models across broader range of physical laws. Unlike prior benchmarks, it includes diverse, carefully curated scenarios with both real and deliberately unphysical dynamics. Models perform significantly worse on our benchmark under identical settings, revealing its greater difficulty and realism. Evaluation Metrics for Text-to-Video Generation. Traditional text-to-video evaluation metrics like Frechet Video Distance (FVD) (Unterthiner et al., 2018) and Inception Score (IS) (Salimans et al., 2016) focus on visual quality but fail to assess motion plausibility and adherence to physical laws, especially without reference videos. Recent methods address this gap: VideoScore (He et al., 2024) integrates human feedback, T2V-CompBench (Sun et al., 2024) uses VLMs for spatial and temporal evaluation, and PhyGenBench (Meng et al., 2024) explicitly tests physical commonsense. However, these methods can be computationally intensive or subjective. We introduce Yes/No answering metric for efficient, scalable, and objective evaluation of physical commonsense in video generation. Additionally, we propose simple yet effective strategy leveraging MLLMs like GPT-o1 to assess video quality. 3. PhyWorldBench To evaluate the ability of text-to-video generation models to simulate physical reality, we introduce PhyWorldBench, comprehensive benchmark spanning wide range of physical principles and scenarios. Its design is rooted in fundamental physics concepts such as motion, energy conservaFigure 3. Creation Process of PhyWorldBench. The dataset is built through three-stage pipeline for clarity, consistency, and completeness. First, physics categories and prompts are defined using literature and expert input. Next, GPT-4o, Gemini-1.5-Pro together with human refine prompts for diversity and accuracy. Finally, curation phase standardizes all prompts, with human-inthe-loop reviews ensuring clarity and eliminating ambiguities. tion, and interaction dynamics, drawing from established literature, including Fundamentals of Physics (Halliday et al., 2013) and Classical Mechanics (Goldstein et al., 2002). Developed with input from experts in physics and video generation. 3.1. Dataset Creation Pipeline As shown in Figure 3, the creation pipeline consists of three steps, with human-in-the-loop reviews integrated at each step to ensure the benchmarks accuracy and diversity. Step 1: Initial Physics Categories Definition. collaborative process between physics experts and the authors to define the key (main) physics categories to be included in the benchmark. Our benchmark categorizes three levels: Fundamental Physics: Covers basic laws such as object motion, energy transfer, and optics. Composite Physics: Involves real-world phenomena that emerge from multiple interacting principles, such as human motion. Anti-Physics: Scenarios intentionally designed to violate real-world physics. The inclusion of anti-physics cases is crucial, as discrepancies in model performance between physically accurate and unphysical scenarios reveal whether model truly understands physics or merely replicates patterns from its training data, which predominantly follows real-world laws. We identify broad categoriessuch as kinematics, rigid body dynamics, fluid behavior, optics, and thermal dynamicsas depicted in the main physics categories in Figure 1. Each category is systematically divided into five subcategories to ensure comprehensive coverage of physics principles from multiple perspectives. Step 2: Prompt Creation Each subcategory is further expanded into 7 distinct scenarios, with each scenario incorporating three prompt variations: Event Prompt, Physicsenhanced Prompt, and Detailed Narrative Prompt, as 3 Figure 4. Example of Three Prompt Types for Scenario. This figure illustrates scenario from the subcategory Linear Motion under the main category Object Motion and Kinematics. The scenario is presented through three levels of prompts: (1) Event Prompt, providing concise and straightforward event description; (2) Physics-Enhanced Prompt, which builds on the general prompt by incorporating physics-related phenomena while avoiding explicit physical laws; and (3) Detailed Narrative Prompt, enriching the Evnet Prompt with vivid details and contextual elements. illustrated in Figure 4. Specifically, we define these three types of prompts as follows: Event Prompt: concise and straightforward description of an event. These prompts are designed to assess the models ability to generate based on minimal input, focusing on high-level scene comprehension. Physics-Enhanced Prompt: This type incorporates physics-related phenomena for Event Prompt to enrich the description naturally. Detailed Prompt: Inspired by how current T2V models generate videos, these prompts are enriched with vivid details and contextual elements, aiming to create more immersive and visually rich outputs. This level of prompt allows us to assess whether providing richer context leads to improved physical accuracy in video generation. We first use state-of-the-art large language models (LLMs), specifically GPT-4o (OpenAI Team, 2024) and Gemini-1.5Pro (Gemini Team, 2024) to generate 20 Event Prompts under the predefined category and subcategory. Then we ask humans to select 7 out of them based on diversity and physics representation. Then human experts meticulously review and validate the generated prompts, providing feedback to iteratively enhance their quality and relevance to the corresponding physics subcategory, ensuring balanced and representative assessment across various aspects of physical understanding. Furthermore, an ethical review is conducted to ensure that all prompts are fair, unbiased, and free from Table 1. Statistical Comparison with Existing PhysicsFocused T2V Benchmarks. Compared to prior benchmarks, PhyWorldBench offers significantly larger and more diverse testbed for evaluating the physical commonsense capabilities of T2V models. It provides broader coverage of physics categories and greater number of prompts, ensuring comprehensive assessment of T2V performance."
        },
        {
            "title": "PhyGenBench",
            "content": "Physics-IQ PhyWorldBench"
        },
        {
            "title": "Physics Categories\nPrompts",
            "content": "5 688 27 160 5 396 50 1050 potentially harmful content. Based on the Event Prompt, human annotators create Physics-Enhanced Prompt by enriching it with physical consequences. Detailed Narrative Prompt is generated by LLM based on Event Prompt using query in Table 11. This rigorous verification process results in refined set of 1,050 high-quality prompts, systematically organized into main and subcategories. Step 3: Standard Creation. We adopt Yes/No evaluation metric to assess whether model can generate videos that accurately align with given prompt. This metric is chosen for its simplicity and effectiveness and for allowing for an objective assessment while minimizing subjectivity in evaluation. As shown in Figure 5, we define two types of evaluation standards: Basic Standards and Key Standards. The Basic Standards specify the essential object(s) that should appear in the video and ensure that the main action or event is depicted. Meanwhile, the Key Standards describe the key 4 Figure 5. Illustration of Our Evaluation Metric and Human Annotations. We demonstrate our evaluation process for assessing the quality of generated videos based on two evaluation criteria: Basic Standards and Key Standards. For Basic Standards, we verify whether the generated video contains the correct number of objects and accurately represents the intended action or event. For Key Standards, we define specific physical phenomena as ground truth and measure if all of these phenomena the generated video satisfies. Both lead to either score of 0 or 1 for generated video. Red circles and yellow lines in the figure highlight instances where the generated videos fail to meet the Key Standards. physical phenomena that would occur if the event took place in the real world. The annotators are instructed to focus on the natural physical consequences of the action and to be explicit and concise in their annotations. By following these guidelines, we ensure that the evaluation standards remain clear and objective. Following (Bansal et al., 2024; Meng et al., 2024), we use Semantic Adherence (SA) and Physical Commonsense (PC) to evaluate video performance. SA checks if both objects and Action/Event align with video, while PC assesses adherence to real-world physics and it checks when all Key Standards are satisfied. Annotated as SA, PC 0, 1 (1 indicating proper grounding), we report the fraction of videos satisfying success when SA = 1, PC = 1, and both jointly. 3.2. Statistics of the dataset We compare our benchmark with existing physics-focused T2V benchmarks (Bansal et al., 2024; Meng et al., 2024), with detailed statistical comparison presented in Table 1. PhyWorldBench offers the most extensive coverage in terms of the number of physics categories, curated prompts, human annotations, and evaluated T2V videos. 3.3. MLLM as Zero Shot Physics Evaluator for"
        },
        {
            "title": "Generated Videos",
            "content": "Human evaluation, while flexible, is costly at scale. Existing metrics often lack generalizability and struggle with the diversity in PhyWorldBench. (See Appendix C.) MLLMs like GPT-o1 (OpenAI Team, 2024) and Gemini (Gemini Team, 2024) show promise for video evaluation but fall short in reliably assessing physical realism (Bansal et al., 2024; He et al., 2024; Meng et al., 2024). We find that these models tend to rationalize unrealistic content, but their accuracy improves when informed that the video is AI-generated, as illustrated in Figure 6. We hypothesize that this improvement stems from MLLMs being primarily trained on real-world videos, leading them to inherently justify observed phenomena unless prompted otherwise. In5 Figure 6. Context-aware prompt improves the assessment quality for GPT-4o. In the two examples, prompt (a) explicitly tells the MLLM that the video might contain potential issues, while prompt does not. We found that the quality assessment is usually more accurate when the context is used. The text is irrelevant to quality is ignored for readability. This phenomenon applied to all our tested models including GPT-4o, GPT-o1, Gemini-2.0-flash, and Qwen-VL-2.0. spired by this, we propose simple yet effective method, Context-Aware Prompt (CAP), which explicitly informs the MLLM that the video is generated rather than real. This approach enables the MLLM to provide more accurate assessment of the videos physical realism. Critically, CAP does not assume videos are incorrect by default but instead uses structured chain-of-thought promptfirst asking the model to describe the video and then to reason through any potential physics issuesand only labels video as incorrect if it clearly violates the category-specific standards. We validate CAP neutrality by measuring false positive rates in physics correct videos and observe only 0.001 absolute change in semantic adhesion (0.188 to 0.187) and 0.014 change in physical commonsense (0.172 to 0.186), showing that it does not significantly increase misclassification of correct videos. Besides, inspired by Chain-of-Thought (Wei et al., 2022), we design two-step prompting strategy, where we first ask the MLLM to give thorough description of the video including Object, Event, and Observations, then continue to prompt the MLLM for the final Yes/No answer for video quality. Please refer to Appendix for prompts used in the two steps. We found this leads to consistent improvement in various MLLMs. In Table 2, we present the ROC AUC calculations comparing CAP with human results on PhyWorldBench with videos generated by models in Figure 2, where random guess would achieve score of 50. We evenly sample 8 frames. In this evaluation, CAP is powered by GPT-o1. CAP significantly outperforms other methods, particularly in Physics Commonsense, where it achieves an absolute performance boost of 13.5 compared to the standard GPT-o1. We find CAP also achieves great performance when equipping other proprietary or open-sourced MLLM. Table 2 shows the ablation study of various components in CAP. without CoT means we directly ask the Table 2. ROC-AUC for automatic evaluation methods and ablation on CAP. Rows now list the two metricsSA (semantic adherence) and PC (physical commonsense)while columns compare all baselines and ablations. Metric Qwen-VL-2.0 Gemini-2.0-Flash GPT-4o GPT-o1 CAP (w/o CoT) CAP (w/o Context) CAP"
        },
        {
            "title": "SA\nPC",
            "content": "72.4 59.8 74.6 60.9 72.1 60.1 75.4 61.6 76.3 73.6 77.3 65. 80.3 75.1 MLLM to give the answer of the question, and without Context means we do not inform MLLM that the video could be of low quality. Please refer Appendix for performance of our method on other MLLM and analysis. 4. Video Generation Results and Analysis We evaluate both closed-source models including SoraTurbo (OpenAI, 2024), Gen-3 (Runway Team, 2024), Kling 1.6 (KlingAI, 2024), Pika 2.0 (Pika Labs Team, 2024), Luma (Luma AI Team, 2024), and open-source models including Hunyuan 720p (Kong et al., 2024), Open-Sora 2.0 (Peng et al., 2025), Open-Sora-Plan 1.3 (Lin et al., 2024), CogVideo 14b (Hong et al., 2022), LTX-Video (HaCohen et al., 2024), Step-video-T2V (Ma et al., 2025), Wanx2.1 (WanTeam et al., 2025). Please check Appendix for the implementation details. We generate 1050 videos for each model. We conduct human evaluation of generated videos using Amazon Mechanical Turk. We first introduce model performance and model-wise analysis and then break down the challenge for the video generation model to achieve real-world physics. 4.1. Model Performance Table 3 presents comprehensive evaluation of twelve video generation models. We report the percentage of videos that satisfy both semantic adherence and physical common6 Table 3. We present the human evaluation results of 10 video generation models across 3 physics types. SA denotes semantic adherence, while PC represents physical commonsense. Both indicates satisfaction of both criteria."
        },
        {
            "title": "Model",
            "content": "Proprietary Models Sora-Turbo (OpenAI, 2024) Gen-3 (Runway Team, 2024) Kling-1.6 (KlingAI, 2024) Pika 2.0 (Pika Labs Team, 2024) Luma (Luma AI Team, 2024) Open-sourced Models Hunyuan 720p (Kong et al., 2024) Open-Sora 2.0 (Peng et al., 2025) Open-Sora-Plan 1.3 (Lin et al., 2024) CogVideo-5B (Hong et al., 2022) Step-video-T2V (Ma et al., 2025) Wanx-2.1 (WanTeam et al., 2025) LTX-Video (HaCohen et al., 2024)"
        },
        {
            "title": "Composite Physics",
            "content": "Anti-Physics"
        },
        {
            "title": "Overall Performance",
            "content": "SA PC"
        },
        {
            "title": "Both",
            "content": "SA PC"
        },
        {
            "title": "Both",
            "content": "SA PC"
        },
        {
            "title": "Both",
            "content": "SA PC"
        },
        {
            "title": "Both",
            "content": "0.438 0.320 0.417 0.587 0.464 0.385 0.434 0.175 0.419 0.353 0.389 0.221 0.315 0.228 0.299 0.375 0.259 0.278 0.268 0.136 0.266 0.245 0.271 0.102 0.246 0.161 0.235 0.312 0.220 0.205 0.205 0.096 0.192 0.189 0.220 0. 0.361 0.258 0.312 0.479 0.328 0.342 0.279 0.159 0.308 0.333 0.323 0.177 0.215 0.142 0.175 0.274 0.199 0.264 0.202 0.057 0.207 0.251 0.235 0.071 0.188 0.103 0.139 0.239 0.160 0.199 0.154 0.044 0.154 0.191 0.187 0. 0.136 0.108 0.125 0.228 0.056 0.082 0.056 0.069 0.062 0.085 0.082 0.076 0.078 0.029 0.073 0.043 0.000 0.021 0.021 0.059 0.021 0.021 0.017 0.011 0.039 0.020 0.042 0.011 0.000 0.010 0.010 0.039 0.000 0.020 0.011 0. 0.384 0.280 0.357 0.521 0.385 0.344 0.348 0.158 0.351 0.320 0.339 0.194 0.261 0.183 0.241 0.314 0.218 0.250 0.223 0.104 0.225 0.224 0.235 0.085 0.208 0.130 0.188 0.262 0.184 0.185 0.170 0.075 0.163 0.173 0.189 0. sense criteria (Both). The Overall Performance column provides an aggregate score for each model, averaged across 10 physics categories. Pika 2.0 achieves the best overall performance across all models. In open-sourced models, Hunyuan 720p achieves the best performance on PC, Wanx2.1 achieves the best performance on Both, and CogVideo achieves the best SA performance. It is worth noting that they even achieve better performance compared with some of the proprietary. Please check Appendix for Leaderboard. 4.2. Model Specific Analysis Our evaluation reveals trade-off between realism, stylization, and consistency in text-to-video models. Wanx, Step-Video-T2V, Gen-3, Hunyuan, Kling, and Sora produce the most realistic videos, while Pika and Luma add stylized lighting and colors. Open-Sora and LTX-Video struggle with realism, often generating low-fidelity or distorted outputs. Object stability variesCogVideo, Open-Sora, and LTX-Video frequently produce warped shapes, while Gen-3, Hunyuan, and Kling maintain more stable forms. Motion realism also differs: Kling and Gen-3 generate smooth motion, Open-Sora-Plan and Luma introduce slow-motion effects, and CogVideo and Open-Sora exhibit jittery, erratic movement. For detailed analysis, see Appendix J. 4.3. Effect of Physics and Prompt Enhancement Understanding real-world physics is essential for text-tovideo models. Event-based prompts describe actions, while physics-enhanced prompts include natural consequences. models ability to generate realistic videos based on these prompts reflects its real-world understanding. As proprietary models conceal their generation processes, we cannot verify prompt refinement. Instead, we analyze prompt types using open-source models, where prompt integrity can be manually ensured. In Table 4, we compare model performance across different prompt types to gauge the effects of physics-aware prompting and refinement. While refinement improves clarity, object detail, and aesthetics, it does not notably enhance physics accuracy. The persistent inconsistencies suggest that these refinements primarily address surface-level quality, leaving deeper physics limitations unresolved. On the other hand, physics-enhanced prompts often yield better results, reflecting the models strong prompt-following abilityeven though they still struggle with true physical comprehension. Consequently, we propose straightforward receipt: explicitly integrate physical phenomena into the prompt to nudge the model toward more realistic outcomes. 4.4. Anti-Physics Analysis As shown in Table 3, all models show performance decline from Fundamental to Composite to Anti-Physics categories, reflecting the challenge of generating complex physical phenomena. While they capture basic physics, simulating intricate interactions and unphysical scenes remains difficult, highlighting gap in physics adherence. Our analysis found that failures often stem from models interpreting prompts in more reasonable way rather than attempting and failing. For example, when prompted with glass sits on the table, and the wine in the glass has reversed gravity, the model generates still image, adhering to real-world physics instead of the prompt. This suggests that models prioritize mimicking training data over true physics understanding. 4.5. Failure to Handle Erratic Visual Changes 4.7. Cinematic Effects vs. Physical Plausibility Certain physical events inherently result in erratic visual changes, such as collisions, breaking, or sudden state transitions. However, current video generation models struggle to accurately depict these scenarios. In the third example of Figure 10, glass cup falls from table, an event that should cause it to shatter upon impact, leading to sudden and complex visual transformation. Instead, the model rationalizes the action, generating video where the glass remains intact and in the same vertical position after falling, completely ignoring the expected breakage and fragmentation. This highlights fundamental limitation: when an event is supposed to cause an erratic visual change, current AI models fail to reflect the correct physical outcome. Rather than simulating realistic disruptions in structure, motion, or object state, the models tend to default to continuous, smoothed animations, avoiding abrupt transformations that are critical for representing real-world physics. 4.6. Breakdown in Physics at Higher Complexity While many models handle simple, isolated physics well, their performance deteriorates under complex interactions involving multiple forces or constraints. For instance, in billiard collisions, models often fail to preserve momentum and friction, leading to unnatural stopping, incorrect deflections, or interpenetrating objects. Similarly, fluid simulations frequently lack realistic turbulence and surface tension when water interacts with solid surfaces. As demonstrated in Appendix M, prompts involving multiple objects introduce significantly higher requirements for semantic adherence. For instance, when tested with the prompt feather and stone are dropped in the air, where both fall, and the stone drops much faster, all models fail to represent the phenomenon correctly. Beyond failing to adhere to semantics, these errors highlight broader issue: models struggle to depict the correct physical relationships between interacting objects, often treating them as independent entities rather than systems governed by unified physical laws. This suggests that while models have learned pattern-based approximations of physics, they lack robust understanding of how multiple physical forces interact dynamically in complex environments. Table 4. Physics-following percentage on different types of prompt. Explicitly adding physics usually increases the physicsfollowing ability of video generation model, while prompt refinement process does not necessary leads to improvement."
        },
        {
            "title": "Model",
            "content": "Event Prompt Physics-enhanced Prompt Detailed Prompt CogVideo-5B (Hong et al., 2022) Hunyuan 720p (Kong et al., 2024) LTX-Video (HaCohen et al., 2024) Open-Sora-Plan 1.3 (Lin et al., 2024) Open-Sora 2.0 (Peng et al., 2025) Wanx-2.1 (WanTeam et al., 2025) Step-Video-T2V (Ma et al., 2025) 0.123 0.159 0.056 0.062 0.167 0.175 0.158 0.177 0.198 0.065 0.067 0.177 0.202 0.182 0.168 0.155 0.066 0.063 0.173 0.190 0.179 major challenge in video generation is the tendency for models to prioritize cinematic aesthetics over strict physical realism. Well-performed models like Sora, Gen-3, and Pika often introduce exaggerated motion dynamics, such as objects moving with heightened fluidity or unnatural acceleration, making scenes appear choreographed rather than physically grounded. For example, an object that should fall naturally under gravity might instead descend too smoothly or float unrealistically, breaking expected motion physics. This issue is particularly evident in action sequences, where momentum conservation and force dynamics are distorted for dramatic effect, leading to visually striking but physically implausible results. The first example in Figure 11 presented an apple floating in the air, then it suddenly dropped as the camera moved; such stylized rendering, while beneficial for artistic storytelling, poses significant challenges for applications requiring strict physical fidelity. 5. Contributions and Future Work We propose PhyWorldBench, large-scale, thorough, and multi-dimensional benchmark for evaluating text-tovideo generation models across diverse physics categories. It provides insights into which physical phenomena are hardest to simulate and what capabilities current models lack. Additionally, we propose an automated evaluator to measure physical accuracy and straightforward receipt for designing prompt. Future work will involve continuously evaluating emerging video generation models, updating our benchmark to reflect advancements in the field, and refining our leaderboard and analysis to track progress in physical accuracy and model capabilities. Impact Statement PhyWorldBench establishes new standard for physicsfocused text-to-video generation, significantly elevating the fields expectations of realism and accuracy. By rigorously assessing models capabilities across comprehensive spectrum of physical phenomenaincluding motion, energy transfer, and complex interactionsthis benchmark not only highlights critical gaps in current systems but also guides researchers toward developing more robust and physicsaware solutions. Moreover, PhyWorldBench acknowledges the ethical implications of physics-driven video synthesis, emphasizing the need for transparency, fairness, and responsible deployment. By mitigating risks such as misinformation, biased simulations, and unintended real-world consequences, it ensures that advancements in this field are aligned with scientific integrity and societal well-being. Ultimately, PhyWorldBench paves the way for advanced applications in education, scientific visualization, and industry, demanding both photorealism and fidelity to real-world physics while upholding ethical standards."
        },
        {
            "title": "References",
            "content": "Agarwal, N., Ali, A., Bala, M., Balaji, Y., Barker, E., Cai, T., Chattopadhyay, P., Chen, Y., Cui, Y., Ding, Y., et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Bansal, H., Lin, Z., Xie, T., Zong, Z., Yarom, M., Bitton, Y., Jiang, C., Sun, Y., Chang, K.-W., and Grover, A. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. Gemini Team. Gemini: family of highly capable multimodal models, 2024. Goldstein, H., Poole, C., Safko, J., and Addison, S. R. Classical mechanics, 2002. Guo, X., Huo, J., Shi, Z., Song, Z., Zhang, J., and Zhao, J. T2vphysbench: first-principles benchmark for physical consistency in text-to-video generation. arXiv preprint arXiv:2505.00337, 2025. HaCohen, Y., Chiprut, N., Brazowski, B., Shalem, D., Moshe, D., Richardson, E., Levin, E., Shiran, G., Zabari, N., Gordon, O., Panet, P., Weissbuch, S., Kulikov, V., Bitterman, Y., Melumian, Z., and Bibi, O. Ltxvideo: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. HailuoAI. Hailuo. https://hailuoai.video/, 2024. Halliday, D., Resnick, R., and Walker, J. Fundamentals of physics. John Wiley & Sons, 2013. He, X., Jiang, D., Zhang, G., Ku, M., Soni, A., Siu, S., Chen, H., Chandra, A., Jiang, Z., Arulraj, A., et al. Videoscore: Building automatic metrics to simulate finegrained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. KlingAI. KLING. https://www.klingai.com/, 2024. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., Wu, K., Lin, Q., Wang, A., Wang, A., Li, C., Huang, D., Yang, F., Tan, H., Wang, H., Song, J., Bai, J., Wu, J., Xue, J., Wang, J., Yuan, J., Wang, K., Liu, M., Li, P., Li, S., Wang, W., Yu, W., Deng, X., Li, Y., Long, Y., Chen, Y., Cui, Y., Peng, Y., Yu, Z., He, Z., Xu, Z., Zhou, Z., Xu, Z., Tao, Y., Lu, Q., Liu, S., Zhou, D., Wang, H., Yang, Y., Wang, D., Liu, Y., Jiang, J., and Zhong, C. Hunyuanvideo: systematic framework for large video generative models, 2024. URL https://arxiv.org/abs/2412.03603. Liao, J., Tan, X., Shao, W., Zhang, K., Cheng, Y., Li, D., and Luo, P. Devil: comprehensive benchmark for dynamics evaluation in video generation. arXiv preprint arXiv:2407.01094, 2024. Lin, B., Ge, Y., Cheng, X., Li, Z., Zhu, B., Wang, S., He, X., Ye, Y., Yuan, S., Chen, L., et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. Liu, Y., Cun, X., Liu, X., Wang, X., Zhang, Y., Chen, H., Liu, Y., Zeng, T., Chan, R., and Shan, Y. Evalcrafter: Benchmarking and evaluating large video generation models. In CVPR, 2024. Luma AI Team. Luma AI. https://lumalabs.ai, 2024. Generative AI platform specializing in 3D content and photorealistic modeling. Ma, G., Huang, H., Yan, K., Chen, L., Duan, N., Yin, S., Wan, C., Ming, R., Song, X., Chen, X., Zhou, Y., Sun, D., Zhou, D., Zhou, J., Tan, K., An, K., Chen, M., Ji, W., Wu, Q., Sun, W., Han, X., Wei, Y., Ge, Z., Li, A., Wang, B., Huang, B., Wang, B., Li, B., Miao, C., Xu, C., Wu, C., Yu, C., Shi, D., Hu, D., Liu, E., Yu, G., Yang, G., Huang, G., Yan, G., Feng, H., Nie, H., Jia, H., Hu, H., Chen, H., Yan, H., Wang, H., Guo, H., Xiong, H., Xiong, H., Gong, J., Wu, J., Wu, J., Wu, J., Yang, J., Liu, J., Li, J., Zhang, J., Guo, J., Lin, J., Li, K., Liu, L., Xia, L., Zhao, L., Tan, L., Huang, L., Shi, L., Li, M., Li, M., Cheng, M., Wang, N., Chen, Q., He, Q., Liang, Q., Sun, Q., Sun, R., Wang, R., Pang, S., Yang, S., Liu, S., Liu, S., Gao, S., Cao, T., Wang, T., Ming, W., He, W., Zhao, X., Zhang, X., Zeng, X., Liu, X., Yang, X., Dai, Y., Yu, Y., Li, Y., Deng, Y., Wang, Y., Wang, Y., Lu, Y., Chen, Y., Luo, Y., Luo, Y., Yin, Y., Feng, Y., Yang, Y., Tang, Z., Zhang, Z., Yang, Z., Jiao, B., Chen, J., Li, J., Zhou, S., Zhang, X., Zhang, X., Zhu, Y., Shum, H.-Y., and Jiang, D. Step-video-t2v technical report: The practice, challenges, and future of video foundation model, 2025. URL https://arxiv.org/abs/2502.10248. Meng, F., Liao, J., Tan, X., Shao, W., Lu, Q., Zhang, K., Cheng, Y., Li, D., Qiao, Y., and Luo, P. Towards world simulator: Crafting physical commonsensebased benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. 9 Motameda, S., Culp, L., Swersky, K., Jain, P., and Geirhos, R. Physics-iq: Evaluating physical understanding in generative video models. arXiv preprint arXiv:2501.09038, 2025. Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J. Zhang, C., Cherniavskii, D., Zadaianchuk, A., Tragoudaras, A., Vozikis, A., Nijdam, T., Prinzhorn, D. W., Bodracska, M., Sebe, N., and Gavves, E. Morpheus: Benchmarking physical reasoning of video generative models with real physical experiments. arXiv preprint arXiv:2504.02918, 2025. OpenAI. Sora: OpenAIs Multimodal Agent. https:// openai.com/index/sora/, 2024. Accessed: 202411-24. OpenAI Team. Gpt-4o. https://openai.com/ index/hello-gpt-4o/, 2024. Peng, X., Zheng, Z., Shen, C., Young, T., Guo, X., Wang, B., Xu, H., Liu, H., Jiang, M., Li, W., Wang, Y., Ye, A., Ren, G., Ma, Q., Liang, W., Lian, X., Wu, X., Zhong, Y., Li, Z., Gong, C., Lei, G., Cheng, L., Zhang, L., Li, M., Zhang, R., Hu, S., Huang, S., Wang, X., Zhao, Y., Wang, Y., Wei, Z., and You, Y. Open-sora 2.0: Training commercial-level video generation model in $200k. arXiv preprint arXiv:2503.09642, 2025. Pika Labs Team. Pika Labs. https://pikalabs.com, 2024. Generative AI platform for creating video and visual content. Runway Team. Runway. https://runwayml.com, 2024. Platform for AI-powered video editing and generative media creation. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Sun, K., Huang, K., Liu, X., Wu, Y., Xu, Z., Li, Z., and Liu, X. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. Unterthiner, T., Van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., and Gelly, S. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang, Y., Yang, C., He, Y., Yu, J., Yang, P., et al. Lavie: Highquality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. WanTeam, Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, 10 A. Benchmark Details The benchmark revolves around 10 main physics categories, each divided into 5 subcategories that cover specific physics phenomena, such as object occlusion, center of mass, and circular motion. Each subcategory includes 7 distinct scenarios, with 3 variations of prompts: general prompt, physics-enhanced prompt, and detailed narrative prompt. These variations are designed to provide differing levels of complexity and context for the text-to-video models. The dataset was curated through three-stage process: first, defining key categories based on fundamental physics literature; second, refining prompts through iterative reviews with large language models and human experts to ensure clarity and alignment with physics principles; and third, conducting detailed curation to create final set of 1,050 high-quality prompts for diverse set of physics phenomena. The evaluation utilizes yes/no framework to assess model adherence to physical laws, categorized into basic standards and key standards, enabling comprehensive performance evaluation. This benchmark establishes foundation for evaluating models ability to generate videos that align with both physical principles and real-world scenarios. Following (Bansal et al., 2024) and (Meng et al., 2024), we use Semantic Adherence (SA) and Physical Commonsense (PC) as metric to evaluate the video performance. SA measures if caption aligns with video frames, while PC assesses if actions obey real-world physics. Annotated as SA, PC {0, 1}, where 1 indicates proper grounding. We report the fraction of videos satisfying SA = 1, PC = 1, and both jointly. We create SA and PC based on thorough review of all prompts and extensive feedback from human annotators, we establish set of evaluation criteria to systematically assess the generated videos, as illustrated in Figure 5. The evaluation is structured around two key standards, each encompassing specific rules: Semantic Adherence: These criteria evaluate fundamental aspects of video generation to ensure prompt fidelity. The assessment includes: 1. Whether the video contains the correct number of objects specified in the prompt. (Yes/No) 2. Whether the depicted event or action is accurately represented. (Yes/No) Physical Commonsense: This aspect evaluates the models capability to capture and represent underlying physical phenomena within the video. The assessment measures the number of distinct physical phenomena accurately showcased in the generated video. By integrating these two standards, which consist of three core evaluation rules, we define comprehensive evaluation metric that ensures balanced and thorough assessment of T2V model performance across multiple dimensions. B. Annotation Details Prompt Annotation After designing the physics categories, we further prepared prompts within each category. To achieve this, we gathered undergraduate and graduate students majoring in Computer Science for an annotation session. They were instructed to provide high-quality annotations by creating both an Event Prompt and corresponding Physics-enhanced Prompt that aligned with the primary physics category and sub-category. The Detailed Prompt was generated using GPT-4o with the MLLM prompt (as described in Table 11) and subsequently verified by human annotators. Finally, each prompt underwent final verification by the authors. Standard Annotation Undergraduate and graduate students in Computer Science were also tasked with annotating the objects and events described in the prompts. These annotations were used for SA (Semantic Adherence) evaluation. Additionally, they identified key physics phenomena that should be visually observable in the generated videos, which were used for PC (Physics Commonsense) evaluation. All annotations were reviewed and verified by the authors. Video Evaluation For large-scale video evaluation, we utilized Amazon Mechanical Turk. The interface is presented in Figure 7 Annotators were asked to assess videos based on three criteria: the existence of objects, the presence of the described event, and the visibility of key physics phenomena, selecting either Yes or No for each. Each video was evaluated by three independent annotators, with the final decision determined by majority vote. All annotators were compensated at rate of $18 USD per hour. C. Detailed Analysis on Evaluator While some existing physics metrics have been proposed, none fit PhyWorldBenchs needs due to domain or structural mismatches (Guo et al., 2025; Zhang et al., 2025). For example, PhyGenEval (Meng et al., 2024) assumes fixed event 11 Figure 7. Amazon Mechanical Turk interface. 12 Table 5. Precision and Recall for automatic evaluation methods and ablation on CAP, together with ROC-AUC from the submission."
        },
        {
            "title": "SA Precision of SA Recall of SA PC Precision of PC Recall of PC",
            "content": "Qwen-VL-2.0 Gemini-2.0-Flash GPT-4o GPT-o1 CAP (without CoT) CAP (without Context) CAP 72.4 74.6 72.1 75.4 76.3 77.3 80.3 66.9 68.0 69.3 71.1 71.8 72.1 75. 72.5 73.1 71.0 73.2 79.3 78.8 83.1 59.8 60.9 60.1 61.6 73.6 65.6 75.1 59.7 61.1 59.4 63.1 70.3 70.1 74. 62.5 61.8 63.1 62.4 74.6 71.6 79.3 order, which many of PhyWorldBenchs scenarios do not have, and VideoCon-Physics (Bansal et al., 2024) is trained specifically on its own VideoPhy benchmark, yielding sub-optimal performance on PhyWorldBenchso we exclude both to avoid unfair comparisons. In Table 6, we therefore evaluate our CAP across multiple MLLMs, including proprietary GPT-4o, GPT-o1, Gemini-Flash-2.0, and open-source Qwen-VL-2.0. Crucially, CAP uses two-step chain-of-thought promptfirst asking the model to describe the video, then to reason about any physics issuesso it does not assume videos are incorrect by default and yields only minimal false-positive increases (0.001 in Semantic Adherence, 0.014 in Physical Commonsense), confirming its neutrality. Moreover, because it operates solely on the final video and its physics-based standards, CAP is agnostic to the underlying generative pipelinewhether text-to-video, image-to-video, video-to-video, or hybrid. We do observe two common failure modes: 1) Aesthetic bias, where visually compelling videos receive high scores despite only moderate physical accuracy (visual appeal and physical plausibility should ideally be assessed independently); and 2) Overconfidence, with the MLLM giving firm judgments in ambiguous or edge-case scenarios where human annotators often express uncertainty or disagreement. To further validate our design, we also measure Precision and Recall for both Semantic Adherence (SA) and Physical Commonsense (PC), which is shown in Table 5. Table 6. ROC-AUC Improvement of various MLLM for with our method automatic evaluation methods. Our method consistently achieves great performance on both open-sourced and proprietary models."
        },
        {
            "title": "Method",
            "content": "SA PC Qwen-VL-2.0 Gemini-2.0-Flash GPT-4o GPT-o1 72.478.6 74.679.9 72.178.5 75.480.3 59.874.1 60.974.0 60.173.9 61.675.1 Moreover, even without any fine-tuning, our CAP evaluator achieves 80.3 Semantic Adherence and 75.1 Physical Commonsense on PhyWorldBench, and when this is compared to VideoCon-Physicss 82.0 SA and 73.0 PC on the VideoPhy benchmark after fine-tuning, it highlights CAPs strong out-of-domain performance. Additionally, we conducted fine-tuning on an 80/20 train/test split of our annotated videos and found that these numbers can be boosted substantially: Qwen-VL-2.0 improves from 77.5 82.3 SA and 74.5 79.1 PC, and GPT-4o from 78.5 84.7 SA and 73.9 80.7 PC. D. Model Application Details Open-source generators (code + weights). Hunyuan (Tencent Hunyuan Community License Agreement) : based on the official 720p repo https://github.com/Tencent/HunyuanVideo. LTX-Video (RAIL-M License) : Hugging Face Diffusers pipeline https://huggingface.co/docs/diffusers/api/pipelines/ltx_video. 13 Table 7. Influence of model scale on physics performance. SA denotes semantic adherence, PC denotes physical commonsense, and Both indicates satisfaction of both criteria. To isolate the effect of size, we compare variants of the same base architecture."
        },
        {
            "title": "Composite Physics",
            "content": "Anti-Physics"
        },
        {
            "title": "Overall Performance",
            "content": "SA PC"
        },
        {
            "title": "Both",
            "content": "SA PC"
        },
        {
            "title": "Both",
            "content": "SA PC"
        },
        {
            "title": "Both",
            "content": "SA PC"
        },
        {
            "title": "Both",
            "content": "Cosmos 14B (Agarwal et al., 2025) Cosmos 7B (Agarwal et al., 2025) CogVideo-5B (Hong et al., 2022) CogVideo-1.5B (Hong et al., 2022) 0.374 0.354 0.419 0.382 0.268 0.243 0.266 0. 0.212 0.201 0.192 0.175 0.331 0.339 0.308 0.246 0.243 0.221 0.207 0. 0.182 0.185 0.154 0.148 0.091 0.088 0.062 0.073 0.069 0.054 0.021 0. 0.020 0.020 0.000 0.010 0.333 0.323 0.351 0.310 0.241 0.218 0.225 0. 0.184 0.178 0.163 0.150 CogVideo v1.5 (Apache-2.0 License) : https://github.com/THUDM/CogVideo. Open-Sora v2.0 (Apache-2.0 License) : https://github.com/hpcaitech/Open-Sora. Wanx v2.1 (Apache-2.0 License) : https://github.com/Wan-Video/Wan2.1. Step-Video-T2V (MIT License) : https://github.com/stepfun-ai/Step-Video-T2V. Open-Sora-Plan v1.3 (MIT License) : https://github.com/PKU-YuanGroup/Open-Sora-Plan. Commercial / proprietary generators. Sora-Turbo, Pika 2.0, Gen-3, Kling 1.6: accessed via the vendors public web interfaces under their standard Terms of Service (no OSS license). Luma: accessed through the official API. All third-party code or data are used under the above licenses/ToS but are not redistributed in our release. Our own benchmark assets are released under the MIT License. E. Insights into Performance Between Models Our analysis reveals that both model size and sampling budget significantly impact physics performance. Larger models consistently outperform smaller ones on foundational physics tasks, as shown in Table 7. For instance, Cosmos 14B outperforms Cosmos 7B across all SA/PC metrics, and CogVideo-5B beats CogVideo-1.5B, confirming that scaling yields clear gains in semantic adherence and physical commonsense. We also observe that generation time correlates with physics accuracy: models with longer sampling schedules (e.g., Hunyuan at 40 min/video or Wanx-2.1 at 1 hr/video) achieve stronger performance, and simply increasing Open-Soras sampling steps from 50 to 200 yields significant boost. These findings suggest that, alongside scaling, dedicating more compute to finer-grained sampling can materially improve adherence to physical laws. F. Detailed Performance over All Physics Categories In the main paper, we presented results across different levels of physics. Here, we provide detailed breakdown of results for each specific physics category in Table 8. G. Limitations While PhyWorldBench spans broad set of physical scenarios with its 1,050 prompts across ten categories, it may not fully capture very specialized or highly intricate interactions that occur in some domains or advanced applications. Similarly, our CAP evaluator generally provides balanced feedback, yet it can display slight preference for videos with particularly polished visual presentation, such as smooth lighting or dynamic camera movement. Future work will focus on expanding the prompt collection to include additional niche phenomena and on enhancing the evaluation workflow to more clearly distinguish visual style from physical correctness. 14 Table 8. We present the evaluation results of 10 video generation models across 10 physics categories. SA denotes semantic adherence, while PC represents physical commonsense. SA, PC indicates satisfaction of both criteria. Model Proprietary Models Sora-Turbo Gen-3 Kling-1.6 Pika 2.0 Luma Open-sourced Models Hunyuan 720p Open-Sora 2.0 Open-Sora-Plan 1.3 CogVideo 5b Step-video-T2V Wanx-2.1 LTX-Video SA 0.552 0.448 0.510 0.731 0.533 0.505 0.534 0.221 0.476 0.401 0.422 0.363 C1 PC 0.429 0.371 0.471 0.587 0. 0.447 0.303 0.202 0.272 0.265 0.281 0.176 Both SA 0.362 0.276 0.356 0.519 0.333 0.350 0.220 0.144 0.223 0.204 0.235 0.167 0.279 0.223 0.272 0.446 0. 0.340 0.411 0.107 0.333 0.273 0.355 0.152 C2 PC 0.144 0.097 0.136 0.267 0.077 0.150 0.234 0.049 0.125 0.202 0.255 0.010 Both SA 0.135 0.097 0.136 0.228 0.067 0.130 0.189 0.039 0.115 0.166 0.205 0.010 0.337 0.250 0.352 0.544 0.423 0.333 0.403 0.086 0.438 0.361 0.374 0.191 Fundamental Both SA 0.115 0.077 0.143 0.243 0.173 0.137 0.179 0.019 0.156 0.145 0.195 0.021 0.524 0.366 0.515 0.660 0.544 0.462 0.454 0.252 0.470 0.371 0.413 0. C3 PC 0.163 0.125 0.171 0.291 0.183 0.167 0.225 0.019 0.198 0.239 0.255 0.032 C4 PC 0.476 0.436 0.455 0.456 0.359 0.413 0.303 0.252 0.500 0.264 0.298 0.202 Composite Physics Anti-Physics Both SA 0.371 0.267 0.347 0.350 0.320 0.279 0.214 0.194 0.350 0.227 0.214 0.121 0.578 0.444 0.571 0.642 0.628 0.350 0.383 0.265 0.495 0.384 0.386 0.268 C5 PC 0.500 0.212 0.418 0.347 0.395 0.272 0.310 0.235 0.326 0.273 0.264 0.144 Both SA 0.363 0.152 0.297 0.274 0.302 0.155 0.237 0.122 0.179 0.238 0.249 0. 0.356 0.188 0.283 0.500 0.395 0.323 0.419 0.120 0.304 0.328 0.384 0.138 C6 PC 0.178 0.129 0.141 0.302 0.185 0.219 0.233 0.060 0.174 0.227 0.273 0. Both SA 0.129 0.099 0.130 0.260 0.123 0.177 0.191 0.060 0.130 0.154 0.222 0.050 0.136 0.107 0.196 0.314 0.317 0.196 0.157 0.097 0.143 0.165 0.244 0. C7 PC 0.019 0.000 0.022 0.029 0.037 0.000 0.022 0.010 0.010 0.000 0.022 0.000 Both SA 0.019 0.000 0.022 0.020 0.037 0.000 0.022 0.010 0.000 0.000 0.022 0.000 0.408 0.275 0.369 0.469 0.317 0.436 0.307 0.115 0.367 0.388 0.310 0.135 C8 PC 0.306 0.235 0.243 0.396 0.297 0.416 0.265 0.077 0.306 0.385 0.313 0.104 Both SA 0.276 0.167 0.184 0.354 0.218 0.327 0.176 0.058 0.235 0.25107 0.266 0. 0.540 0.394 0.370 0.656 0.350 0.396 0.373 0.266 0.413 0.446 0.415 0.305 C9 PC 0.320 0.192 0.259 0.398 0.263 0.375 0.319 0.085 0.304 0.368 0.370 0. Both SA 0.270 0.141 0.210 0.344 0.225 0.271 0.264 0.064 0.228 0.31466 0.273 0.085 0.136 0.108 0.125 0.228 0.056 0.082 0.056 0.069 0.062 0.085 0.085 0. C10 PC 0.078 0.029 0.073 0.043 0.000 0.021 0.021 0.059 0.021 0.021 0.021 0.011 0.039 0.020 0.042 0.011 0. 0.010 0.010 0.039 0.000 0.020 0.020 0.000 H. Code and Data Availability To ensure full reproducibility, we provide both the generated video assets and all experiment code: Generated videos and prompts: All videos used in our experiments, along with the corresponding prompt JSON files, are hosted on HuggingFace at https://huggingface.co/datasets/phyworldbench/ phyworldbench. Note that these generated videos are not part of PhyWorldBench, they were instead utilized for our paper experiments. Also, the HuggingFace does not contain the evaluation standards, those are part of the GitHub listed below. Experimentation code and instructions: The complete implementation of the experiments code as well as detailed setup instructions are available at https://github.com/ashwin-333/phy-world-bench. Website: Additionally, this work is available on the NVIDIA Deep Imagination Research website at https:// research.nvidia.com/labs/dir/phyworldbench/. I. Query Used Here we list the three MLLM queries used in this work, corresponding to the first-stage description prompt  (Table 9)  , the second-stage structured-answer prompt  (Table 10)  , and the detailed-prompt refinement query  (Table 11)  . Note that for proprietary MLLMs such as GPT-o1, GPT-4o and Gemini-2.0-Flash, the model may refuse to evaluate small fraction of generated videos; the refusal rate is below 3 Table 9. First prompt for MLLM to generate detailed description. Suppose you are an expert in judging and evaluating the quality of AI-generated videos. These are frames evenly sampled from generated video from the beginning to the end. This is generated video from video model rather than captured from real world, so the video could be low quality, such as fuzzy, inconsistency, especially not following real world physics. Please tell me what is in this video, including what happened and any physics phenomena you observe. Please be sure to include objects in the video, the main event, and any physics phenomena you observe. J. Qualitative Analysis on Individual Model We present here qualitative examples from various models, as shown Figure 8, Figure 9, Figure 10, Figure 11, Figure 12, Figure 13, Figure 14, Figure 15, Figure 16, Figure 17, Figure 18, Figure 19 that demonstrate violations of fundamental physics laws, highlighting their limitations. Below, we summarize key observations for each model: CogVideo: Tends to produce distorted objects, such as non-spherical balls that appear dented. Movements are often excessively fast, blurred, erratic, or jittery, sometimes appearing unnaturally sped up. Gen-3: Generates videos with cinematic quality, featuring rich lighting and high level of realism, akin to footage captured with high-quality camera. Lighting is particularly smooth and well-balanced. Table 10. Second prompt for MLLM to generate the final answer Suppose you are an expert in summarization and finding answers. Here is the text description from another large language model about an AI generated video: {previous response}. Based on this description, compare the objects and quantities present in the video with the specified object(s): {object}. Answer Yes if the object(s) could be found in the video, otherwise answer No. Also, please check if {event} is visually depicted in the video, and answer Yes or No. Lastly, please check if video satisfies the standards in list: {physics phenomenon list}, and answer Yes or No for each standard in the list. Return your evaluation in the following JSON format: Objects: Yes/No, Event: Yes/No, Standard 1: Yes/No, Standard 2: Yes/No, ...: Yes/No Table 11. MLLM prompt used to get detailed video generation prompts. Refine the sentence: {prompt} to contain subject description, action, scene description. (Optional: camera language, light and shadow, atmosphere) and conceive some additional actions to make the sentence more dynamic. Make sure it is fluent sentence, not nonsense. Hunyuan: Produces highly realistic videos, with quality resembling footage taken by high-end camera. Kling: Generates realistic videos, often incorporating dynamic, moving shots that simulate footage captured with moving camera. LTX-Video: Produces grainy footage with distorted objects that are difficult to identify. Moving objects often shift shape and struggle to maintain form. Luma: Tends to generate realistic footage but with overly vibrant or bright colorization. Moving objects sometimes experience distortion or loss of form. Open-Sora-Plan: Generates relatively realistic videos but with low fidelity, often lacking detail and featuring still objects. Movement can appear overly smoothed or surreal, sometimes resembling slow-motion. The output often has VHS-like quality. Open-Sora: Generates videos where objects frequently distort and warp, failing to hold their shape. The outputs often appear less realistic, resembling animation or 3D graphics. Pika: Produces realistic yet highly stylized videos. Lighting and objects have an unrealistic softness, contributing to distinctive stylized aesthetic. Sora: Generates highly realistic videos with cinematic or stylized lighting. Movement is often exaggerated, enhancing the dramatic quality of the footage. Wanx: Produces visually crisp footage, yet physical interactions are frequently unreliable; rigid bodies may pass through one another, gravity-driven motions appear floaty, and impacts are often muted or entirely missing. Step-Video-T2V: Creates sharp, aesthetically pleasing clips, yet physical reasoning is routinely oversimplified; light fails to refract, flexible objects stay rigid, mass differences do not affect motion, and phenomena such as duplication or buoyancy are either absent or behave contrary to real-world expectations. K. Further Analysis on Generative Models Struggling with Certain Physical Scenarios We identify three key factors behind current models failures on physics-intensive scenarios: (1) Training Data Distribution: large web-scale corpora such as WebVid, Panda70M, or other private web-crawled datasets are dominated by everyday scenesour analysis shows this yields strong performance on C1 (Object Motion and Kinematics), C4 (Fluid and Particle Dynamics), C5 (Lighting and Shadows), C8 (Rigid Body Dynamics), and C9 (Human and Animal Motion) but underrepresents abstract or multi-object interactions; (2) Video Captioning Process: MLLM-based prompt extraction captures main 16 Figure 8. Physics violations observed in videos generated by the Sora-Turbo model. a. Ceiling fans blades phase into the ceiling during rotation. b. Birds supporting branch phases into nearby leaves, disconnected from the birds movement. c. Bottle enters the persons mouth without spatial adjustment as they react to pepper. d. Person pedals through bicycle instead of interacting correctly with the pedals. e. Childs face deforms unnaturally while sliding down slide. Figure 9. Physics Violations Observed in Videos Generated by the Gen-3 Model. a. Buoy remains unnaturally stationary despite visible water movement. b. Child morphs into different positions and exhibits abnormal motion. c. Cyclist pedaling forward moves backward while rounding curve on track. d. Rubber ball bounces off the air instead of the wall. e. Child displays abnormal motion, with knees bending unnaturally. 18 Figure 10. Physics Violations Observed in Videos Generated by the Kling-1.6 Model. a. Glass with wine remains unaffected by reversed gravity, showing no changes in the liquids behavior. b. Small toy airplane and full-sized airplane are depicted as the same size, violating realistic proportions. c. Glass cup glides down instead of falling naturally and slides abnormally after hitting the ground. d. Feather and stone fall at the same speed, inaccurately disregarding differences in air resistance. e. Aluminum can remains intact after being struck by hammer, failing to collapse as expected. 19 Figure 11. Physics Violations Observed in Videos Generated by the Pika 2.0 Model. a. Apple floats in the air, then suddenly falls unnaturally as the camera moves. b. Box moves up without any contact from hands. c. Dogs face deforms unnaturally along with water motion as it shakes to dry off. d. Plastic bottle resists fully crushing when stepped on. e. Tin foil folds automatically into persons hand without proper crumpling motion. Figure 12. Physics Violations Observed in Videos Generated by the Luma Model. a. Soccer ball rolls away from the goal in straight line, showing no natural curve or arc in its path after being kicked. b. Diver exhibits abnormal motion and phases into the water upon jumping from platform. c. Marble floats unnaturally upward, deviating from expected motion. d. Hands phase into lump of dough while stretching it out. e. Book exhibits abnormal phasing during its motion. 21 Figure 13. Physics Violations Observed in Videos Generated by the Hunyuan 720p Model. a. Glass cup rests on the table instead of phasing through it, failing to follow the anti-physics prompt. b. Two balloons with like charges remain stationary instead of repelling each other, disobeying electromagnetic interaction physics. c. Pendulum swinging between two peaks exhibits irregular and unrealistic motion. d. Steel ball dropping onto soft clay block does not deform the clay, defying expected material interaction physics. e. Scissors cutting through paper fail to visibly separate the paper, disobeying shear and cutting physics. 22 Figure 14. Physics Violations Observed in Videos Generated by the Open Sora Model. a. Train traveling upside down on suspended tracks appears right-side-up, failing to depict the intended anti-physics scenario. b. Flashlight moving across dark room does not illuminate its surroundings. c. Two billiard balls collide but awkwardly phase into each other instead of bouncing off. d. Backpack loaded off-center remains unnaturally straight, with no tipping or imbalance. e. Bowling ball rolling toward pins exhibits abnormal phasing and unnatural motion. Figure 15. Physics Violations Observed in Videos Generated by the Open-Sora-Plan 1.3 Model. a. Lighthouse beam fails to visibly rotate across the ocean, undermining its expected motion. b. Person walking in front of projector screen casts no shadow on the screen. c. Rolling tire phases into another tire while moving down the street. d. Rock dropped from cliff glides unnaturally and exhibits abnormal motion and phasing. e. Tightrope walker disobeys gravity with abnormal movement while crossing the wire. 24 Figure 16. Physics Violations Observed in Videos Generated by the CogVideo 5b Model. a. Raindrops do not return to the clouds after hitting the ground; instead, they exhibit normal rainfall, failing to demonstrate anti-physics. b. Toy car and real car rolling down ramps are depicted as the same size, violating proportional realism. c. Car driving on an icy road shows no slippage, unrealistically maintaining normal traction. d. Basketball dropped onto the ground displays unnatural bouncing motion. e. Person standing on one leg during yoga exhibits unrealistic human anatomy. 25 Figure 17. Physics Violations Observed in Videos Generated by the LTX-Video Model. a. Stationary bicycle appears with rider, failing to follow the anti-physics prompt of pedaling without one. b. Chair placed on an uneven floor remains stable, inaccurately disregarding the effect of the uneven surface. c. Stone thrown across river skips the parabolic motion and instead appears directly in the water. d. Rock sitting on the edge of hill does not fall, disobeying natural gravitational physics and the prompt. e. Wind blowing over sand dunes fails to pick up sand particles, violating expected particle behavior physics. Figure 18. Physics Violations Observed in Videos Generated by the Wanx-2.1 Model. a. Basketball dropped from shoulder height rebounds to only fraction of its release height, indicating excessive inelasticity. b. During the javelin throw, the athletes fore-arm visually interpenetrates the javelin shaft, breaking rigid-body constraints. c. Multiple ice cubes clip through one another instead of exhibiting realistic rigid-body collisions and buoyancy. d. The stone ricochets repeatedly on calm pond surface and never sinks, contrary to expected gravity-driven submersion. e. The aluminum can remains undeformed when struck by the hammer, ignoring momentum transfer and material plasticity. 27 Figure 19. Physics Violations Observed in Videos Generated by the Step-Video-T2V Model. a. An object strikes calm pond, but there is no visible beam of light or refraction as it enters the water. b. tree branch remains rigid in the wind, never bending or springing back. c. child and an adult jump on trampoline yet rebound to identical heights, ignoring mass-dependent dynamics. d. single coin was expected to duplicate continuously, but no new coins appear. e. balloon with hole should deflate downward; instead, it inexplicably rises, contradicting physics. 28 Figure 20. Sora failed examples on multi-object. We found that generation models tend to fail when the prompt is relatively complex, such as when containing multiple objects. objects and events but routinely omits fine-grained physical dynamics; and (3) Model Architecture Limitations: even with detailed prompts, current architectures emphasize short-term frame coherence without physics priors, leading to physically inconsistent outputsby contrast, models like TORA explicitly model trajectories for better motion fidelity. L. Leaderboard Table 12 presents the leaderboard performance on both human evaluation and our designed CAP. The performance shows highly consistency and only difference is that for proprietary models, Sora ranked number 2 while Kling ranked number 2 in CAP. Through further analysis, we found that Klings more cinematic style can bias CAP toward higher scores despite only moderate physical correctness, whereas our human annotatorswho were instructed to focus strictly on physics plausibilitypenalize these artifacts, causing Sora to rank higher under human evaluation. Table 12. Leaderboard of open-sourced models and proprietary models. We report leaderboard on both human evaluation and CAP."
        },
        {
            "title": "CAP\nWanx\nHunyuan",
            "content": "Step-Video-T2V Step-Video-T2V Open-Sora Cogvideo Open-Sora Cogvideo Open-Sora-Plan Open-Sora-Plan LTX-video LTX-video Proprietary models CAP Human Pika 2.0 Pika 2.0 Kling Sora-Turbo Sora-Turbo Kling Luma Luma Gen-3 Gen-3 M. Influence of Multiple Objects We noticed that generation models often fail when the prompt is more complex, such as when containing multiple objects. We show examples of Sora failing when given prompts with multiple objects in Figure 20. N. Dataset Structure The dataset is organized into three main categories of physics phenomena: Fundamental Physics, Composite Physics, and Anti-Physics. Each category is divided into subcategories, which are further broken down into five sub-subcategories, as summarized below:"
        },
        {
            "title": "Fundamental Physics",
            "content": "(a) Object Motion and Kinematics: Linear Motion, Parabolic Motion, Circular Motion, Rotational Motion, Combined"
        },
        {
            "title": "Translational and Rotational Motion",
            "content": "(b) Interaction Dynamics: Collisions, Friction and Sliding, Gravity and Free Fall, Electromagnetic Interactions, Tension and Compression (c) Energy Conservation: Potential to Kinetic Energy, Elastic Energy, Energy Loss (Damping), Conservation of Mechanical Energy, Heat Transfer and Phase Changes. (d) Fluid and Particle Dynamics: Water Motion, Smoke and Gas Dynamics, Particle Behavior, Buoyancy and Floating,"
        },
        {
            "title": "Viscosity and Flow",
            "content": "(e) Lighting and Shadows: Consistency of Illumination, Object Occlusion, Chromatic Aberration and Dispersion, Reflection and Refraction, Shadow Softness and Penumbra (f) Deformations and Elasticity: Material Flexibility, Elastic Rebound, Plastic Deformation, Brittle Fracture, Thermal"
        },
        {
            "title": "Composite Physics",
            "content": "(a) Scale and Proportions: Consistency of Forces Across Scales, Environmental Effects on Different Scales, Proportional Energy Transfer, Structural Integrity Across Scales, Scaling of Mechanical Properties (b) Rigid Body Dynamics: Rotation and Torque, Balance and Stability, Center of Mass, Impact and Deformation, Shear and Cutting (c) Human and Animal Motion: Anatomical Feasibility, Balance and Posture, Locomotion Across Terrains, Reflexive and Involuntary Motion, Coordination and Fine Motor Skills Anti-Physics (a) Anti-Physics: Defying Gravity, Energy Creation or Perpetual Motion, Object Phasing and Surreal Interactions, Time Reversal and Alteration, Infinite Duplication and Division This hierarchical structure ensures coverage of realistic and non-realistic physical scenarios for diverse analysis. Figure 21 visualises how these ten categories map onto the three physics levels and highlights the numbers of prompts in each branch."
        },
        {
            "title": "Impact Statement",
            "content": "PhyWorldBench establishes new standard for physics-focused text-to-video generation, significantly elevating the fields expectations of realism and accuracy. By rigorously assessing models capabilities across comprehensive spectrum of physical phenomenaincluding motion, energy transfer, and complex interactionsthis benchmark not only highlights critical gaps in current systems but also guides researchers toward developing more robust and physics-aware solutions. Moreover, PhyWorldBench acknowledges the ethical implications of physics-driven video synthesis, emphasizing the need for transparency, fairness, and responsible deployment. By mitigating risks such as misinformation, biased simulations, and unintended real-world consequences, it ensures that advancements in this field are aligned with scientific integrity and societal well-being. Ultimately, PhyWorldBench paves the way for advanced applications in education, scientific visualization, and industry, demanding both photorealism and fidelity to real-world physics while upholding ethical standards. O. Data Release We publicly released comprehensive dataset that includes the core assets of PhyWorldBench, the 1,050 JSON prompt files, the evaluation standards, and the physics categories and subcategories, used to assess physical realism in text-to-video 30 Figure 21. Dataset Overview. PhyWorldBench contains ten physics categories under three different physics levels. models. We have also attached over 10k of the generated videos that we used for experimentation. The licensing terms for the videos will follow those set by the respective model creators, as referenced in this work, while the curated captions and human annotations will be provided under the MIT License. Additionally, our release will include standardized evaluation protocols, and evaluation scripts to facilitate rigorous assessment. The entire project will be open-sourced, ensuring free access for research and academic purposes."
        }
    ],
    "affiliations": [
        "NVIDIA Research",
        "Northeastern University",
        "University of California, Santa Cruz"
    ]
}