{
    "paper_title": "Collaborative Multi-Modal Coding for High-Quality 3D Generation",
    "authors": [
        "Ziang Cao",
        "Zhaoxi Chen",
        "Liang Pan",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D content inherently encompasses multi-modal characteristics and can be projected into different modalities (e.g., RGB images, RGBD, and point clouds). Each modality exhibits distinct advantages in 3D asset modeling: RGB images contain vivid 3D textures, whereas point clouds define fine-grained 3D geometries. However, most existing 3D-native generative architectures either operate predominantly within single-modality paradigms-thus overlooking the complementary benefits of multi-modality data-or restrict themselves to 3D structures, thereby limiting the scope of available training datasets. To holistically harness multi-modalities for 3D modeling, we present TriMM, the first feed-forward 3D-native generative model that learns from basic multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM first introduces collaborative multi-modal coding, which integrates modality-specific features while preserving their unique representational strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to raise the robustness and performance of multi-modal coding. 3) Based on the embedded multi-modal code, TriMM employs a triplane latent diffusion model to generate 3D assets of superior quality, enhancing both the texture and the geometric detail. Extensive experiments on multiple well-known datasets demonstrate that TriMM, by effectively leveraging multi-modality, achieves competitive performance with models trained on large-scale datasets, despite utilizing a small amount of training data. Furthermore, we conduct additional experiments on recent RGB-D datasets, verifying the feasibility of incorporating other multi-modal datasets into 3D generation."
        },
        {
            "title": "Start",
            "content": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 1 Collaborative Multi-Modal Coding for High-Quality 3D Generation Ziang Cao, Zhaoxi Chen, Liang Pan(cid:0), Ziwei Liu(cid:0) 5 2 0 2 1 2 ] . [ 1 8 2 2 5 1 . 8 0 5 2 : r Abstract3D content inherently encompasses multi-modal characteristics and can be projected into different modalities (e.g., RGB images, RGBD, and point clouds). Each modality exhibits distinct advantages in 3D asset modeling: RGB images contain vivid 3D textures, whereas point clouds define fine-grained 3D geometries. However, most existing 3D-native generative architectures either operate predominantly within single-modality paradigmsthus overlooking the complementary benefits of multi-modality dataor restrict themselves to 3D structures, thereby limiting the scope of available training datasets. To holistically harness multi-modalities for 3D modeling, we present TriMM, the first feed-forward 3D-native generative model that learns from basic multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM first introduces collaborative multimodal coding, which integrates modality-specific features while preserving their unique representational strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to raise the robustness and performance of multi-modal coding. 3) Based on the embedded multi-modal code, TriMM employs triplane latent diffusion model to generate 3D assets of superior quality, enhancing both the texture and the geometric detail. Extensive experiments on multiple well-known datasets demonstrate that TriMM, by effectively leveraging multi-modality, achieves competitive performance with models trained on large-scale datasets, despite utilizing small amount of training data. Furthermore, we conduct additional experiments on recent RGBD datasets, verifying the feasibility of incorporating other multimodal datasets into 3D generation. Index TermsImage-to-3D Generation, Diffusion Model, Multi-modality I. INTRODUCTION He automatic generation of high-quality 3D assets from images or texts has wide range of applications, from virtual reality and robotics simulation to industrial design and animation. Recent advances have been witnessed in image [1] and video generation [2] given open-source billion-scale 2D datasets [3]. However, the largest public 3D dataset, Objaverse [4], [5] only includes millions of 3D objects. Therefore, to compensate for the data-hungry of 3D generative models, effectively leveraging heterogeneous 3D datasets through methodological designs is crucial for scalable 3D generative models. Prior arts on 3D generation could be further categorized into three: 1) optimization-based methods [6] using Score Distillation Sampling (SDS); 2) reconstruction-based feedforward models [7][9] which employ transformer to regress Ziang Cao, Zhaoxi Chen, Ziwei Liu are with S-Lab, Nanyang Technological University, Singapore. {ziang001, zhaoxi001}@e.ntu.edu.sg, {ziwei.liu }@ntu.edu.sg Liang Pan is with Shanghai Artificial Intelligence Laboratory panliang1@pjlab.org.cn (cid:0) Corresponding Author 3D representation given sparse-view images; 3) 3D-native feedforward models [10][14] with 3D diffusion or autoregressive framework. Current 3D generative approaches, despite architectural variations, predominantly adhere to monolithic modality paradigmstypically relying on colored renderings as main training sources. While RGB images provide dense texture priors of 3D assets (e.g., material reflectance, specular highlights), this photometric modality suffers from fundamental limitations: 1) geometric ambiguity in occluded regions, and 2) topological uncertainty due to projective viewpoint. These modality-specific constraints limit the scalability and quality of existing 3D generative models. To address this challenge, we propose TriMM, collaborative multi-modal coding for high-quality feed-forward 3D generation. At the core of our framework is unified latent coding that synergistically integrates photometric and geometric representations from multimodal data (RGB, RGBD, and point clouds), jointly benefiting from the high-frequency texture details encoded by RGB images and metric-accurate topologies encoded by point clouds and depth maps. Our framework employs dedicated modality-specific encoders coupled with shared decoder architecture to map heterogeneous inputs into unified triplane-structured latent representation. To effectively harness the complementary strengths of multimodal inputs, we introduce the reconstruction-based mechanism in the generative model that explicitly guides the model to discern and optimally utilize the distinctive advantages of each modality. To avoid distortion at the large elevation angle and raise the robustness and performance of multi-modal coding, we adopt auxiliary 2D (e.g., RGB, mask, depth loss) and 3D supervision (SDF loss). This collaborative multi-modal coding further facilitates efficient generative modeling which trains lightweight latent diffusion model conditioned on input images. The well-structured latent representation space established by our multimodal framework enables streamlined generative model training, requiring neither intricate hyperparameter adjustments nor extensive optimization cycles. Extensive evaluations on standard benchmarks [4], [15] demonstrate that, by leveraging multi-modality information, TriMM achieves competitive performance with recent state-of-the-art methods in generating high-quality 3D assets from images, even when trained on small-scale datasets illustrate in Fig. 1. Beyond exploring the potential of current 3D data, our multimodal encoding architecture inherently supports the tokenization of real-world multi-modal inputs through its extensible design. This framework provides novel and promising pathway to systematically address the challenge of 3D training data scarcity. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 2 Fig. 1. By leveraging a) Collaborative Multi-Modal Coding encoded from photometric (RGB, RGBD) and geometric (RGBD, Point Clouds) information, b) TriMM can create high-quality textured meshes within 4 seconds from single image. Our contributions could be summarized as: II. RELATED WORK We propose TriMM, Collaborative Multi-Modal Coding that marries geometric and photometric information in multi-modal data into unified space for high-quality 3D generation. We introduce hybrid 2D image-space and 3D geometric space loss for fast training and robust learning of multimodal coding. Based on multi-modal coding, we build generative framework that can leverage the potential and strength of multi-modal data while avoiding the weakness of specific modalities via special reconstruction loss for 3D generation. Extensive evaluations on well-known datasets [4], [15], [16] demonstrate the superiority of our method in imageto-3D tasks quantitatively and qualitatively. A. Optimization-based methods As one of the most representative works in 3D generation, DreamFusion [6] introduces SDS loss into optimization. It achieves impressive performance in 3D generation by utilizing pre-trained 2D diffusion priors. However, despite various improvements [17][20], SDS-based methods continue to face the multi-face Janus problem and low optimization efficiency, particularly when creating large number of 3D assets. B. Feed-forward methods 1) 3D-aware GANs:: Before the breakthrough of the diffusion model, GAN-based methods achieve competitive performance in 3D generation. As the pioneering approach, 3DGAN [21] tries to learn latent representation converted via voxel. To improve the performance, EG3D [22] proposes popular representation, Triplane, which keeps satisfactory balance between performance and efficiency. Furthermore, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 3 Fig. 2. Overview of our TriMM. To extract the unique attributes of multi-modal triplanes and avoid their specific weakness, we introduce the loss 2, i.e., reconstruction loss during training. It can guide our generative model to leverage the strength of multi-modalities coding, thereby achieving promising performance in 3D modeling. GET3D [23] adopts two-branch framework to enrich the details in geometry. 2) 3D-native diffusion models:: As one of the powerful generative models, diffusion model has demonstrated its impressive potential in the 3D generation task. Point-E [24] proposes diffusion model based on point cloud which is flexible but lacks the capability of representing watertight and solid surface of 3D content. To handle this, MeshDiffusion [25] and DiffRF [12] adopt mesh and voxel as the representation. Although explicit representation has superiority in querying and evaluating, those explicit 3D representation methods are memory-expensive when using high-resolution grids. Different from explicit representation, the neural fieldbased methods [26], [27] adopt an implicit way to encode the geometry and texture information. Despite low memory loading in high-resolution grids, it will cost more time to evaluate. Compared with pure explicit or implicit methods, the hybrid representation triplane achieves promising tradeoff between efficiency and performance [13], [14], [28], [29]. Recently, TRELLIS [30] proposes powerful 3D generation framework that uses large-scale high-quality 3D data and 2D&3D reference input for reconstruction. Despite introducing multi-modal data, the early fuse framework limits its scalability across different modalities and still suffers from the scarcity of 3D data. To avoid this problem, we integrate the strength of multi-modal data in the later module. In this way, our model not only utilizes the potential of current 3D data to raise the performance but also inherently supports the tokenization of heterogeneous multi-modal inputs through its extensible design. 3) Reconstruction-based methods:: Recently, LRM-based methods [31][35] have gained significant attention for their impressive performance and efficiency. Based on pure transformer-based network, LRM [7] can reconstruct the triplane from single input image. Building on LRM, Instant3D [36] extends LRM to the multi-view model. By inputting the multi-view images generated from the multiview diffusion model [37], [38], Instant3D achieves impressive performance from single image. To raise the performance further, LGM [8] replaces triplane by Gaussian splatting representation. Despite superior texture performance, Gaussians are limited in explicit geometry modeling and high-quality surface extraction. To deal with this, InstantMesh [39] adopts mesh-based representation, i.e., Flexicube [40]. However, most of those methods are based on single modality data which cannot provide sufficient information for high-quality 3D generation. To boost the performance effectively, we opt to introduce more comprehensive reference information. Therefore, we develop multi-modal encoding framework to capture and leverage the unique advantages of different modalities for enhancing 3D generative performance. Besides direct performance improvement, our model can also be applied to 3D data extension that converts other modality data into 3D triplanes to enrich the details and diversity of generated 3D assets. The overview of our TriMM is shown in Fig. 2. Our method starts from 1) Collaborative Multi-Modal Coding which integrates multi-modal data into unified space, followed by 2) triplane latent diffusion model that performs generative modeling on top of the superiority of multi-modal data and embeddings. C. Collaborative multi-modal coding As the key component of our TriMM, Collaborative MultiModal Coding consists of 1) modality-specific encoders and 2) collaborative multi-modal decoders. In specific, the encoder part includes three branches, i.e., RGB, RGBD, and point cloud branch. To leverage dense texture information of RGB and spatial details of depth and point cloud, we apply three different encoders to project multi-modal data to the same latent space. Besides, since our Collaborative Multi-Modal Coding is trained on the public 3D dataset, it can be utilized to extend the scarce 3D data for generation tasks. By converting different RGB images, RGBD images, and point cloud datasets into shared latent space spanned by triplane, we can introduce diverse training data to raise the capability of modeling complex structures. For clarity, we denote three different encoders as Ergb, Ergbd, and Epointcloud, respectively. The detailed structures of encoders are illustrated in Fig 3. Inspired by [41], we adopt pure transformer architecture to encode the RGB image into triplane. As for the RGBD images, we split and patchify the RGB image and depth image using different convolution IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 4 Fig. 3. Detailed structure of our Collaborative Multi-Modal Coding. The proposed Collaborative Multi-Modal Coding can be tokenized from each of the modalities (i.e. RGB, RGBD, Point Clouds) using different encoders, shown as the three branches above. By adopting share-weight triplane-flexicube decoder, the coding (i.e. corresponding triplanes) from different modalities collaboratively share joint latent space. layers. To maintain the robustness and performance of the encoder, we utilize and fine-tune the pre-train foundation model, i.e., DINOv2. By introducing the depth image via the cross-attention module and residual connection, we can obtain an RGBD tokenizer efficiently. Therefore, the triplane from the RGB and RGBD images can be formulated as: Trgb = Ergb(Rrgb), Trgbd = Ergbd(Irgb, Rd), (1) (2) where Rrgb R3512512 and Rd R1512512 represent the inputted RGB and depth images. the spatial"
        },
        {
            "title": "To explicitly represent",
            "content": "information of point clouds, we opt to transform the point cloud features into voxel-like representations. Let us denote the input point cloud as Rn6 where the first three channels represent the normalized XYZ coordinates of points while the last three channels represent the RGB value of corresponding points. After shuffling the point clouds, we adopt the most classic point cloud feature backbone, i.e., PointNet [42], to extract point cloud features as follows: ˆP = Epn(P), ˆP Rn64 , (3) where Epn represent the PointNet. To establish the relationship between point cloud features and the voxel grid, we partition the unit cube into grids (denoted as C). The features in each voxel grid can be queried from corresponding 3D cubes. Therefore, the voxel-like feature can be calculated by: Fpc[:, i, j, k] ="
        },
        {
            "title": "1\nL",
            "content": "N (cid:88) l=0 ( ˆP[l, :] C[i, j, k] ). (4) Then, we adopt three different convolution layers to project the voxel-like feature Fpc[i, j, k] to three planes by: ˆTpc = Cat{f yz Conv(F pc), xz Conv(F pc), xy Conv(F pc)}, (5) where pc, = X, Y, and fConv represent calculating mean value on the axis and convolution layer, respectively. Fig. 4. Training pipeline of our triplane latent diffusion model. It can harness and integrate the distinctive attributes of various modalities via reconstruction loss, thereby producing 3D assets enriched with rich texture and finely detailed structures. Besides, to effectively maintain the spatial information of the colored point clouds, we introduce 3D convolution layer and 3D-aware cross-attention layer in the transformer encoder. Thus, the triplanes from point clouds can be formulated as: Tpc = Etf ( ˆTpc, Fpc) , (6) where Etf denotes the transformer model for point clouds. To project multi-modal triplanes to the same latent space, we adopt share-weight decoder. To accelerate the training process, we first train the RGB branch including the RGB encoder and shared decoder. Then, we initialize other encoders with RGB triplanes and fix the parameters of the decoder to train the RGBD encoder and point cloud encoder. To improve the performance in terms of geometry and texture, we introduce hybrid 2D image-space loss and 3D geometric loss. Similar to prior work, we use reconstruction loss in the 2D image space of rendering, depth, and mask. Besides, we introduce 3D loss based on the Signed Distance Function (SDF) to optimize the geometry of 3D assets directly. To accelerate the convergence and avoid the negative influence caused by the unbalanced positive-negative ratio, we calculate the BCEloss on positive and negative areas, respectively. to avoid unnecessary triangle and highPlease note that frequency noise, the regularization function is also included IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 5 in our loss function following previous work [40]. Thus, the loss function can be formulated as: Lcode = Lrgb( ˆIi, gt ) + λdLd( ˆId, gt 2 ) + λregLreg 2 + λsdf Lsdf ( ˆS, Sgt), ˆMi gt (7) +λmask (cid:88) , ID, gt , ˆMi, and gt where Ii, gt the rendered RGB image, ground-truth image, rendered depth image, ground-truth depth image, rendered mask, ground-truth mask of i-th view. represent Lsdf ( ˆS, Sgt) = (LBCE( ˆSp, Sgt ) + LBCE( ˆSn, Sgt )), (8) where LBCE is the BCE loss, ˆSp, Sgt denote the positive SDF area, ground-truth positive SDF area, negative SDF area, and ground-truth negative area. , ˆSn, and Sgt D. Triplane latent diffusion model After obtaining the multi-modal triplanes from collaborative Multi-Modal Coding, we adopt typical diffusion model to learn the superiority of different modality data. Note that to reduce the difficulty of training the diffusion model, we introduce variational autoencoder (VAE) model for spatial compression of multi-modal triplanes. Besides, during the optimization of VAE, we use the KL loss to constrain the distribution of the compressed triplanes. Compared to directly integrating the VAE model with our collaborative Multi-Modal Coding, using two separate models allows for more parameters and larger batch sizes during training while also reducing the convergence difficulty of our Multi-Modal Coding. Using the RGB triplane as an example, the VAE loss function is defined as follows: Lvae = LMSE( ˆT rgb, rgb)) + λklDKL(N (µ, σ)N (0, 1)) , (9) where LMSE is the MSE loss, (µ, σ) is the distribution of the reconstructed compressed triplanes ( ˆT represent the compressed triplanes. ) and After obtaining the VAE model, we then train the diffusion model. The pipeline of our diffusion model is shown in Fig. 4. Considering the wide application of image-to-3D models, we use image embedding extracted by the CLIP model as the condition to optimize the diffusion model. standard UNet diffusion model with ResBlocks and downsampling and upsampling layers is utilized. We train the triplane diffusion model to predict the noise ϵ added to the compressed triplane features, applying an L2 loss to the prediction. Besides, to extract and leverage the attributes of different modalities, we add the reconstruction loss for corresponding modalities as follows: Ldiff = Et[1,T ], ϵN (0,1)[fθ(zt, t, c) ϵ] + λrecLrec , (10) where T, zt, and are the time steps of the diffusion model, noised triplane features in t-th step, and conditional image Fig. 5. Details of the decoder in our collaborative multi-modal coding. Leveraging the lightweight decoder, our model efficiently and effectively transforms the triplane into colored mesh. embedding while fθ is the triplane diffusion model. Lrec is corresponding to the modality of triplanes as follows: Lrec = Lrgb(I , gt), Lrgb(I , gt) + Ld(I Lsdf ( ˆS, Sgt), d, gt ), if is Trgb if is Trgbd if is Tpc (11) d, and ˆS are RGB image, depth image, and SDF where , value rendered from generated triplane Tgen. E. Implementation details Training data. Our proposed collaborative multi-modal coding is trained on high-quality subset of Objaverse [4] including around 80K 3D objects. We adopt the same rules as [8] to filter the low-quality 3D objects. Then, we render the RGB, mask, and depth images from 8 random views at the resolution of 512 512 for training. As for the input reference image, we adopt the rendered images of 512 512 in elevation angles between [-15,30] degrees. In the reconstruction evaluation, we randomly sample 2000 objects from Objaverse. Evaluation data. To evaluate the generalization of our model in unseen objects, we conduct qualitative and quantitative experiments on Google Scanned Objects [15] (GSO) and OmniObject3D [16]. The GSO dataset includes about 1K 3D objects while OmniObject3D [16] involves more than 6K diverse 3D objects. We used 400 objects in the GSO dataset and 2000 3D objects in OmniObject3D during the evaluation. Besides, we sample 1000 3D objects from Objaverse for reconstruction evaluations. More implementation details including the structure, training details, and evaluation metrics are released in the supplementary. Evaluation metrics. In our experiments, we evaluate the texture and geometry of generated 3D assets. For texture evaluation, we normalize the mesh and sample 16 random views from unit sphere to calculate the mean Peak Signalto-Noise Ratio (PSNR). Besides, we also introduce the CLIP score to measure the consistency between the generated results and the input prompts. To evaluate the quality of geometry, we calculate the standard shape metrics of Chamfer Distance (CD) and F-score (FS) with thresholds of 0.05 and 0.1. Details of TriMM The decoder consists of super-resolution module and 4 MLP networks shown in Fig. 5. We adopt traditional convolution network [43] to enrich the texture of IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 6 Fig. 6. Qualitative results. We compared our TriMM with other methods on image-to-3D. Thanks to our multi-modal coding the collaboratively marries photometric and geometric information in the unified triplane space, our TriMM achieves impressive generative performance, especially for fine-grained geometric details like wings and hairs. TABLE COMPARISON OF SEVERAL MODULES PARAMETERS, TRAINING, AND INFERENCE TIME. Model name #params. Training time Inference time 676.6M RGB branch coding RGBD branch coding 761.9M Point cloud branch coding 631.5M 585.6M VAE 985.9M Diffusion 90 hours <5 hours 12 hours 24 hours 24 hours 3.3s 4.1s 4.5s 0.8s 1.2s the generated objects. Besides, to maintain the robustness of unified triplane space, we only use two layers to project the triplane feature to SDF, deform, weight, and RGB. Training stages In this subsection, we report more details about our TriMM. We compare the parameters, training, and inference time of different modules in Table I. We use 24 NVIDIA A100 GPUs to train our model. Note that the inference time is tested on NVIDIA A100 GPUs. We split our model into two stages: 1) multi-modal coding training including RGB, RGBD, point cloud tokenizer, and shared decoder; 2) Triplane latent diffusion training including VAE and image-conditional diffusion model. For the 1st stage, we first train RGB reconstruction model similar to other LRMbased methods, e.g., InstantMesh [9]. Besides, to accelerate the convergence of our multi-modal coding, we use the pre-trained RGB triplane to initialize other triplanes. After obtaining all multi-modality triplanes, we train typical latent diffusion model (2nd stage). Considering the application, we adopt single image as the condition to train the generative model. Inference stages After training, we can use the diffusion model, decoder of VAE, and shared decoder of multi-modal coding to generate and render the results. Based on an inputted single RGB image, it can generate 3D object within 4 seconds. F. Representation learning results In this subsection, we compare our collaborative multimodal coding results with the reconstruction results of other LRM-based baseline methods, e.g., TripoSR [41], LGM [8], and InstantMesh [9]. Table II shows the impressive reconstruction performance of our proposed modules. The comparison among the last three lines validates the claims that different modalities have their unique superiority in 3D modeling. Relying on additional depth information, the RGBD branch achieves better geometry metrics results than the RGB branch, thereby having the ability to modeling small structures which brings the gain in texture. Besides, the point cloud branch shows the best performance in geometry learning. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 7 Fig. 7. Qualitative comparison among branches based on different modalities. It shows the strengths of different modality data for 3D modeling. The point cloud branch excels at capturing geometry, whereas the RGB and RGBD branches perform better at modeling texture. TABLE II QUANTATIVE RESULTS ON THE OBJAVERSE [4]. WE COMPARE THE RECONSTRUCTION RESULTS WITH OTHER METHODS. OUR METHOD OUTPERFORMS BOTH PHOTOMETRICALLY AND GEOMETRICALLY."
        },
        {
            "title": "Methods",
            "content": "PSNR CD FS@0.05 TripoSR [41] LGM [8] InstantMesh [9] RGB (Ours) RGBD (Ours) PointCloud (Ours) 24.67 26.23 26. 27.81 28.32 26.10 0.0147 0.0287 0.0124 0.0084 0.0041 0.0026 0.972 0.951 0.987 0.999 0.999 0.999 G. Triplane diffusion results 1) Qualitative evaluations: In this subsection, we compared our method with our baseline and the most recently existing works including TripoSR [41], CRM [44], LGM [8], InstantMesh [9], and TRELLIS [30]. Baseline represents the generative model trained only on RGB triplanes. We adopt the pre-trained model LGM (trained on 80k data), CRM (trained on 376k data), InstantMesh (trained on 270k data), and TRELLIS (trained on 500k data). We compared our TriMM with other SOTA methods qualitatively in Fig. 6. We can notice that the generated 3D meshes of our TriMM are more stable when facing objects with different appearances and structures. TripoSR can generate promising texture for 3D assets, especially in the frontal view. However, because triplane representation lacks the robust capability to express complex geometry structures, it lent to generate flattened 3D objects. Besides, since LGM, CRM, and InstantMesh are built based on multi-view diffusion model, the performance of those methods is constrained by the performance of the diffusion model which decreases the final performance of image-to-3D. Relying on high-quality large-scale training data, TRELLIS achieves superior robustness and performance in various 3D assets. Different from integrating 2D and 3D data directly, we introduce the multi-modal coding module based on multi-modal data, thereby raising the robustness when facing complex structures and maintaining the capability of extension. Despite using smaller data, by exploring the potential of current 3D assets, our model achieves competitive performance compared with TRELLIS. More qualitative results are shown in Fig. 8. 2) Quantitative evaluations: In this subsection, we report the quantitative results for image-to-3D on unseen samples from public datasets [15], [16]. As shown in Table III and Table IV, TriMM achieves competitive quality both geometrically and photometrically. It is worth noting that by utilizing and leveraging the multi-modal data, our TriMMcan achieve competitive performance of other methods trained on largescale datasets. We attribute this performance boost to our collaborative multimodal coding which explicitly incorporates both 2D and 3D information. H. Ablation studies In this subsection, we discuss the effectiveness of introducing multi-modal data, reconstruction loss, 2D&3D supervision, and VAE model. 1) Analysis about multi-modal data.: As shown in the first three lines of Table. V, we can notice that developing IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 8 TABLE IV QUANTATIVE RESULTS ON OMNIOBJECT3D [16]. WE COMPARE TRIMM TRAINED ON 80K OBJECTS WITH FOUR STATE-OF-THE-ART APPROACHES: LGM (80K), CRM (376K), INSTANTMESH (270K), AND TRELLIS (500K) ON 1700 UNSEEN SAMPLES FROM THE OMNIOBJECT3D DATASET. OUR METHOD OUTPERFORMS BOTH PHOTOMETRICALLY AND GEOMETRICALLY."
        },
        {
            "title": "Geometry",
            "content": "CLIP PSNR CD FS@0.05 FS@0."
        },
        {
            "title": "65.3\nBaseline\n66.4\nTripoSR [41]\n67.2\nCRM [44]\nLGM [8]\n66.3\nInstantMesh [9] 66.1\nTrellis [30]\n67.3",
            "content": "12.14 0.131 12.64 0.129 13.22 0.109 13.29 0.131 13.37 0.105 13.50 0.102 0.259 0.262 0.283 0.253 0.331 0.374 TriMM (Ours) 67.4 14.13 0.096 0. 0.427 0.439 0.477 0.421 0.512 0.556 0.561 TABLE ABLATION STUDIES ABOUT MULTI-MODAL CODING. BY INTRODUCING SPECIFIC MULTI-MODAL DATA, OUR TRIMM CAN IMPROVE THE PERFORMANCE IN ASPECTS CORRESPONDING TO THE ATTRIBUTES OF EACH MODALITY. TOP-3 ARE HIGHLIGHTED IN DIFFERENT COLORS. RGB RGBD Point Recons. Cloud Loss Appearance Geometry CLIP PSNR CD FS@0.05 (cid:33) (cid:37) (cid:33) (cid:37) (cid:37) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) 55.2 (cid:33) 58.3 (cid:33) 58.4 (cid:33) 56.1 (cid:33) 60.1 (cid:33) 59.3 (cid:33) 58.1 (cid:37) 57.5 (cid:33) 64. 13.6 14.5 14.5 13.6 15.4 14.6 14.2 14.3 16.6 0.116 0.084 0.046 0.023 0.047 0.021 0.015 0.048 0. 0.362 0.425 0.512 0.613 0.511 0.622 0.639 0.503 0.641 Fig. 8. More qualitative results of our proposed method. It shows that our method can generate high-quality 3D assets with photo-realistic textures and detailed geometry TABLE III QUANTATIVE RESULTS ON GOOGLE SCANNED OBJECTS (GSO) [15]. WE COMPARE TRIMM TRAINED ON 80K DATA WITH FOUR STATE-OF-THE-ART APPROACHES: LGM (80K), CRM (376K), INSTANTMESH (270K), AND TRELLIS (500K) ON THE GSO DATASET. OUR METHOD OUTPERFORMS BOTH PHOTOMETRICALLY AND GEOMETRICALLY."
        },
        {
            "title": "Geometry",
            "content": "CLIP PSNR CD FS@0.05 FS@0."
        },
        {
            "title": "48.2\nBaseline\n47.8\nTripoSR [41]\n49.7\nCRM [44]\nLGM [8]\n50.3\nInstantMesh [9] 50.5\n51.4\nTrellis [30]",
            "content": "13.37 0.061 13.05 0.096 13.83 0.043 14.32 0.072 14.17 0.058 14.27 0.035 0.427 0.409 0.562 0.327 0.457 0.612 TriMM (Ours) 52.5 14.34 0.034 0. 0.591 0.564 0.756 0.525 0.654 0.775 0.786 generative models based on the RGB, RGBD, or point cloud can lead to different performances of texture and geometry. Integrating two different modalities can mitigate the limitations of generative models based on single modality. For instance, training the generative model based on RGB and colored point cloud can bring improvement in geometry compared to the model based on RGB and enrich the texture compared to the model based on colored point cloud. Since the multimodality triplanes are mixed during training, the distribution of different modality triplanes will affect the final generative performance. Therefore, the model using RGBD and point cloud sources introduces more geometric information (depth and 3D coordinates). It causes the model to pay more attention to geometry, thereby impeding the improvement in texture quality. Finally, by leveraging the different attributes of all three multi-modal data, i.e., the dense texture information of RGB image, depth information of RGBD image, and abundant geometric information of colored point clouds, our TriMM achieves impressive overall performance. 2) Analysis about reconstruction loss.: We also conduct experiments on reconstruction loss in our model and we adopt the RGB, RGBD, and point clouds as our multimodal sources. To effectively harness the strength of multimodal data for 3D modeling, we try to integrate multimodal triplanes in the generative model by the reconstruction loss. It aims to optimize the generative model based on different superiority of different modalities. As shown in Table. V, the model with reconstruction loss achieves significant improvement. Also, we compare the model without reconstruction loss qualitatively in Fig. 9. The model without reconstruction loss can merely 3) Analysis of 2D/3D supervision: In this section, we validate the effectiveness of incorporating 2D/3D hybrid supervision. As shown in Fig. 10, introducing 3D supervision enables our model to better capture geometric details, particularly when handling 3D content with complex structures. Furthermore, it reduces training time and accelerates convergence. The quantitative comparison, presented in Table VI, shows that trained with both 2D and 3D supervision significantly outperforms the model using only 2D supervision in terms of texture and geometry quality. These qualitative and quantitative results strongly demonstrate the effectiveness of 3D supervision. the model 4) Discussion on VAE.: We conduct experiments on the introduction of the VAE model. By introducing the VAE model, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 9 Fig 7. By introducing more geometric information, the point cloud branch can avoid the artifacts effectively. However, since it uses sparse RGB information, it is more likely to ignore the details of the texture while the RGB and RGBD branch can handle this problem by introducing dense RGB information. 2) Introducing multi-modalities for 3D generation: In this paper, we propose novel collaborative multimodal coding module to project multimodal data into triplane latent space. It can not only extract and utilize the attributes of multimodal data but also provide new direction for extending trainable 3D data. To validate the feasibility of this application, we conduct the experiments on subset of Objaverse [4], WildRGB-D [45]. Note that since Objaverse focussed more on isolated objects, we utilize the RGBD and point cloud data of isolated objects rather than scenes. In this experiment, we adopt two-step training strategy: 1) adopting triplanes from all multimodal data (Objaverse& WildRGB-D) to pre-train the generative model; and 2) adopting high-quality triplanes from Objaverse to enhance the performance of the generative model. In this way, we can improve the quality and diversity of generated 3D assets. As shown in Fig. 12, the model introducing multimodal sources is more stable compared to the one without using multimodal sources when facing diverse 3D objects. Besides, we compare the generative performance of w/o and w/ additional datasets shown in Table VIII. It proves that introducing RGBD data can improve the quality and diversity of the generative model. Therefore, we think that by introducing existing large-scale multi-modalities data and high-quality 3D data, 3D generation capabilities can be effectively enhanced. We will continue on this problem in our future work. 3) User studies.: To evaluate our methods more comprehensively, we also conduct user studies on the quality of generated 3D assets. Specifically, we render 360-degree rotating videos for five generative methods [8], [9], [30], [41], [44] and our proposed method. There are 48 videos totally in our evaluation, including car, vegetable, food, house, and so on. Each volunteer will rate the generated results of those six methods anonymously from 1 to 6. We normalize and report the score in Fig. 13. It shows that our methods are more aligned with human preferences. 4) Limitation and future work: Despite promising results, our method still has some limitations. The inefficient information utilization inherent in triplane representation imposes memory-bound restrictions on resolution enhancement, consequently diminishing their capacity for detailed feature characterization. Developing new representation to improve Fig. 9. Ablation studies of reconstruction loss. By introducing the reconstruction loss, our model can avoid the weakness of specific modality, thereby enhancing the generative performance further. our TriMM can be built on the compressed latent features encoded by multi-modal triplanes. As shown in Fig. 11, we compared the diffusion results of the same epoch. It proves that introducing VAE model can accelerate the convergence speed of our TriMM Besides, the quantitative results in Table VII validate the effectiveness of the VAE model. I. Further analysis 1) Multi-modal data to 3D.: To validate our statement about the multi-modality, we visualize the reconstruction results of RGB-to-3D, RGBD-to-3D, and point cloud-to-3D in TABLE VI ABLATION STUDIES ABOUT 2D/3D SUPERVISION IN RECONSTRUCTION MODEL ON THE SUBSET OF OBJAVERSE. BY ADOPTING 3D SUPERVISION, OUR COLLABORATIVE MULTI-MODAL CODING GAINS PROMISING IMPROVEMENT. TABLE VII ABLATION STUDIES ABOUT VAE. WE COMPARE THE GENERATIVE RESULTS WITH AND WITHOUT THE VAE MODEL. BY INTRODUCING THE VAE MODULE, THE LEARNED FEATURES BECOME MORE COMPACT AND INFORMATIVE, THEREBY LEADING TO IMPROVED PERFORMANCE."
        },
        {
            "title": "Methods",
            "content": "PSNR CD FS@0.05 w/o 3D suerpvision 24.26 0.015 w/ 3D supervision (ours) 28. 0.0048 0.961 0.999 Methods Appearance Geometry CLIP PSNR CD FS@0.05 FS@0.1 w/o VAE 43.2 12.71 0.108 0.387 w/ VAE (ours) 52.1 14.83 0.034 0.612 0.521 0.761 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 10 Fig. 10. Comparison between model using 2D supervision and model using 2D&3D supervision. By leveraging 3D supervision, our method can directly learn geometric structures and effectively avoid artifacts. Fig. 11. Ablation studies on the VAE in Triplane latent diffusion model. We analyze the effectiveness of VAE. It shows that introducing VAE to compress the multi-modal triplanes can accelerate the convergence of the generative model. Fig. 12. Comparison between models with and without utilizing multimodal data. By Introducing multimodal data, the generative model is more stable in various conditions. the quality while maintaining reasonable distribution is our next work. Besides, considering the different configurations of existing multi-modality data, it is still many bottlenecks in introducing multi-modality data for 3D modeling. For example, most of the point cloud dataset has only geometric information and most of the RGBD dataset focuses on scenes. The implementation of object segmentation masks, while effective for the localization of target objects, cannot inherently compensate for the resolution degradation in RGBD sensing data. We will carry them in our future work. III. CONCLUSION Generally, RGB image contains dense texture information while point clouds and depth images include more geometrical details. Those special attributes can provide abundant reference information for robust 3D generation. However, most existing TABLE VIII FURTHER ANALYSIS ON ADDITIONAL MULTI-MODALITIES DATA. BY INTRODUCING ADDITIONAL DATA, OUR METHOD ACHIEVES PROMISING IMPROVEMENT IN TERMS OF GEOMETRY AND TEXTURE. Methods CLIP CD FS@0.05 w/o additional dataset 62.4 0.022 0.637 w/ additional dataset 64.9 0. 0.645 works are developed based on single-modality data, neglecting the prospective direction of improvement by introducing multimodal data. Therefore, In this work, we propose the novel collaborative multi-modal coding for 3D generation, i.e., TriMM. It consists of two stages: 1) collaborative multi-modal coding and 2) triplane latent diffusion model. The former can capture the special attributes of multi-modal data and integrate them IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 11 [9] J. Xu, W. Cheng, Y. Gao, X. Wang, S. Gao, and Y. Shan, Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models, arXiv preprint arXiv:2404.07191, 2024. [10] F. Hong, J. Tang, Z. Cao, M. Shi, T. Wu, Z. Chen, S. Yang, T. Wang, L. Pan, D. Lin et al., 3dtopia: Large text-to-3d generation model with hybrid diffusion priors, arXiv preprint arXiv:2403.02234, 2024. [11] Z. Chen, J. Tang, Y. Dong, Z. Cao, F. Hong, Y. Lan, T. Wang, H. Xie, T. Wu, S. Saito et al., 3dtopia-xl: Scaling high-quality 3d asset generation via primitive diffusion, arXiv preprint arXiv:2409.12957, 2024. [12] N. Muller, Y. Siddiqui, L. Porzi, S. R. Bulo, P. Kontschieder, and M. Nießner, Diffrf: Rendering-guided 3d radiance field diffusion, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 43284338. [13] Z. Cao, F. Hong, T. Wu, L. Pan, and Z. Liu, Large-vocabulary 3d diffusion model with transformer, arXiv preprint arXiv:2309.07920, 2023. [14] , Difftf++: 3d-aware diffusion transformer for large-vocabulary 3d generation, arXiv preprint arXiv:2405.08055, 2024. [15] L. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B. McHugh, and V. Vanhoucke, Google scanned objects: highquality dataset of 3d scanned household items, in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 25532560. [16] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian et al., Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation, arXiv preprint arXiv:2301.07525, 2023. [17] C.-H. Lin, J. Gao, L. Tang, T. Takikawa, X. Zeng, X. Huang, K. Kreis, S. Fidler, M.-Y. Liu, and T.-Y. Lin, Magic3d: High-resolution text-to3d content creation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 300309. [18] Y. Li, Y. Dou, Y. Shi, Y. Lei, X. Chen, Y. Zhang, P. Zhou, and B. Ni, Focaldreamer: Text-driven 3d editing via focal-fusion assembly, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 4, 2024, pp. 32793287. [19] Z. Wang, C. Lu, Y. Wang, F. Bao, C. Li, H. Su, and J. Zhu, Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation, Advances in Neural Information Processing Systems, vol. 36, 2024. [20] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, Dreamgaussian: Generative gaussian splatting for efficient 3d content creation, arXiv preprint arXiv:2309.16653, 2023. [21] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum, Learning probabilistic latent space of object shapes via 3d generative-adversarial modeling, Advances in neural information processing systems, vol. 29, 2016. [22] E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. De Mello, O. Gallo, L. J. Guibas, J. Tremblay, S. Khamis et al., Efficient geometry-aware 3d generative adversarial networks, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 12316 133. [23] J. Gao, T. Shen, Z. Wang, W. Chen, K. Yin, D. Li, O. Litany, Z. Gojcic, and S. Fidler, Get3d: generative model of high quality 3d textured shapes learned from images, Advances In Neural Information Processing Systems, vol. 35, pp. 31 84131 854, 2022. [24] A. Nichol, H. Jun, P. Dhariwal, P. Mishkin, and M. Chen, Point-e: system for generating 3d point clouds from complex prompts, arXiv preprint arXiv:2212.08751, 2022. [25] Z. Liu, Y. Feng, M. J. Black, D. Nowrouzezahrai, L. Paull, and W. Liu, Meshdiffusion: Score-based generative 3d mesh modeling, arXiv preprint arXiv:2303.08133, 2023. [26] G. Nam, M. Khlifi, A. Rodriguez, A. Tono, L. Zhou, and P. Guerrero, implicit 3d shape generation with latent diffusion 3d-ldm: Neural models, arXiv preprint arXiv:2212.00842, 2022. [27] H. Jun and A. Nichol, Shap-e: Generating conditional 3d implicit functions, arXiv preprint arXiv:2305.02463, 2023. [28] T. Wang, B. Zhang, T. Zhang, S. Gu, J. Bao, T. Baltrusaitis, J. Shen, D. Chen, F. Wen, Q. Chen et al., Rodin: generative model for sculpting 3d digital avatars using diffusion, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 45634573. [29] J. R. Shue, E. R. Chan, R. Po, Z. Ankner, J. Wu, and G. Wetzstein, 3d neural field generation using triplane diffusion, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 20 87520 886. Fig. 13. User study results on the overall quality of 3D generation from different methods. The normalized score proves the promising performance of our method. into unified latent space. Based on the multi-modal coding module, we use triplane latent diffusion model to generate high-quality 3D assets. By introducing special reconstruction, it can leverage the strength of specific modalities while avoiding their weakness. Comprehensive experiment results on public datasets validate the impressive performance of our TriMM. Furthermore, TriMM highlights promising direction for developing unified multimodal 3D generation model by extending multi-source data."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This study is supported by the National Key R&D Program of China (2022ZD0160201), and Shanghai Artificial Intelligence Laboratory. This study is also supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-T2EP202210012), NTU NAP, and under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s)."
        },
        {
            "title": "REFERENCES",
            "content": "[1] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, Highresolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10 68410 695. [2] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts et al., Stable video diffusion: Scaling latent video diffusion models to large datasets, arXiv preprint arXiv:2311.15127, 2023. [3] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al., Laion5b: An open large-scale dataset for training next generation image-text models, Advances in Neural Information Processing Systems, vol. 35, pp. 25 27825 294, 2022. [4] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi, Objaverse: universe of annotated 3d objects, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13 14213 153. [5] M. Deitke, R. Liu, M. Wallingford, H. Ngo, O. Michel, A. Kusupati, A. Fan, C. Laforte, V. Voleti, S. Y. Gadre et al., Objaverse-xl: universe of 10m+ 3d objects, Advances in Neural Information Processing Systems, vol. 36, 2024. [6] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall, Dreamfusion: Textto-3d using 2d diffusion, arXiv preprint arXiv:2209.14988, 2022. [7] Y. Hong, K. Zhang, J. Gu, S. Bi, Y. Zhou, D. Liu, F. Liu, K. Sunkavalli, T. Bui, and H. Tan, Lrm: Large reconstruction model for single image to 3d, arXiv preprint arXiv:2311.04400, 2023. [8] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu, Lgm: Large multi-view gaussian model for high-resolution 3d content creation, in European Conference on Computer Vision. Springer, 2025, pp. 118. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, MMMMMMM YYYY 12 Zhaoxi Chen received the bachelors degree from Tsinghua University, in 2021. He is currently working toward the PhD degree with MMLabatNTU, Nanyang Technological University, supervised by Prof. Ziwei Liu. He received the AISG PhD Fellowship in 2021. His research interests include inverse rendering and 3D generative models. He has published several papers in CVPR, ICCV, ECCV, ICLR, IEEE Transactions on Pattern Analysis and Machine Intelligence, and ACM Transactions on Graphics. He also served as reviewer for CVPR, ICCV, NeurIPS, ACM Transactions on Graphics, and International Journal of Computer Vision Liang Pan is presently Researcher at the Shanghai AI Laboratory. He earned his Ph.D. in Mechanical Engineering from the National University of Singapore (NUS) in 2019. He then served as Research Fellow at the S-Lab of Nanyang Technological University from 2020 to 2023. His research focuses on computer vision, 3D point clouds, and virtual humans. He has made top-tier publications in relevant conferences and journals. Furthermore, he actively contributes to the academic community by serving as reviewer for esteemed conferences and journals in computer vision, machine learning, and robotics. [30] J. Xiang, Z. Lv, S. Xu, Y. Deng, R. Wang, B. Zhang, D. Chen, X. Tong, and J. Yang, Structured 3d latents for scalable and versatile 3d generation, arXiv preprint arXiv:2412.01506, 2024. [31] Z. He and T. Wang, Openlrm: Open-source large reconstruction models, https://github.com/3DTopia/OpenLRM, 2023. [32] X. Wei, K. Zhang, S. Bi, H. Tan, F. Luan, V. Deschaintre, K. Sunkavalli, H. Su, and Z. Xu, Meshlrm: Large reconstruction model for highquality mesh, arXiv preprint arXiv:2404.12385, 2024. [33] M. Liu, C. Zeng, X. Wei, R. Shi, L. Chen, C. Xu, M. Zhang, Z. Wang, X. Zhang, I. Liu, H. Wu, and H. Su, Meshformer : High-quality mesh generation with 3d-guided reconstruction model, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [Online]. Available: https://openreview.net/forum?id=x7pjdDod6Z [34] C. Zhang, H. Song, Y. Wei, Y. Chen, J. Lu, and Y. Tang, Geolrm: Geometry-aware large reconstruction model for high-quality 3d gaussian generation, arXiv preprint arXiv:2406.15333, 2024. [35] Y. Xu, Z. Shi, W. Yifan, H. Chen, C. Yang, S. Peng, Y. Shen, and G. Wetzstein, Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation, arXiv preprint arXiv:2403.14621, 2024. [36] J. Li, H. Tan, K. Zhang, Z. Xu, F. Luan, Y. Xu, Y. Hong, K. Sunkavalli, G. Shakhnarovich, and S. Bi, Instant3d: Fast text-to-3d with sparseview generation and large reconstruction model, arXiv preprint arXiv:2311.06214, 2023. [37] Y. Shi, P. Wang, J. Ye, M. Long, K. Li, and X. Yang, Mvdream: Multiview diffusion for 3d generation, arXiv preprint arXiv:2308.16512, 2023. [38] P. Wang and Y. Shi, Imagedream: Image-prompt multi-view diffusion for 3d generation, arXiv preprint arXiv:2312.02201, 2023. [39] J. Xu, W. Cheng, Y. Gao, X. Wang, S. Gao, and Y. Shan, Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models, arXiv preprint arXiv:2404.07191, 2024. [40] T. Shen, J. Munkberg, J. Hasselgren, K. Yin, Z. Wang, W. Chen, Z. Gojcic, S. Fidler, N. Sharp, and J. Gao, Flexible isosurface extraction for gradient-based mesh optimization. ACM Trans. Graph., vol. 42, no. 4, pp. 371, 2023. [41] D. Tochilkin, D. Pankratz, Z. Liu, Z. Huang, A. Letts, Y. Li, D. Liang, C. Laforte, V. Jampani, and Y.-P. Cao, Triposr: Fast 3d object reconstruction from single image, arXiv preprint arXiv:2403.02151, 2024. [42] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, Pointnet++: Deep hierarchical feature learning on point sets in metric space, vol. 30, 2017. [43] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, Enhanced deep residual networks for single image super-resolution, in Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2017, pp. 136144. [44] Z. Wang, Y. Wang, Y. Chen, C. Xiang, S. Chen, D. Yu, C. Li, H. Su, and J. Zhu, Crm: Single image to 3d textured mesh with convolutional reconstruction model, in European Conference on Computer Vision. Springer, 2025, pp. 5774. [45] H. Xia, Y. Fu, S. Liu, and X. Wang, Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 22 37822 389. Ziwei Liu Ziwei Liu is currently an Assistant Professor at Nanyang Technological University (NTU). Previously, he was senior research fellow at the Chinese University of Hong Kong and postdoctoral researcher at the University of California, Berkeley. Ziwei received his Ph.D. from the Chinese University of Hong Kong in 2017. His research revolves around computer vision/graphics, machine learning, and robotics. He has published extensively on toptier conferences and journals in relevant fields, including CVPR, ICCV, ECCV, NeurIPS, IROS, SIGGRAPH, TOG, and TPAMI. He is the recipient of the Microsoft Young Fellowship, Hong Kong PhD Fellowship, ICCV Young Researcher Award, and HKSTP best paper award. Ziang Cao is currently pursuing Ph.D. in the College of Computing and Data Science at Nanyang Technological University, supervised by Prof. Ziwei Liu. His research interests lie on computer vision, deep learning, and 3D generation."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University, Singapore",
        "Shanghai Artificial Intelligence Laboratory"
    ]
}