{
    "paper_title": "R3: Robust Rubric-Agnostic Reward Models",
    "authors": [
        "David Anugraha",
        "Zilu Tang",
        "Lester James V. Miranda",
        "Hanyang Zhao",
        "Mohammad Rifqi Farhansyah",
        "Garry Kuwanto",
        "Derry Wijaya",
        "Genta Indra Winata"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce R3, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. R3 enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3"
        },
        {
            "title": "Start",
            "content": "R3: Robust Rubric-Agnostic Reward Models David Anugraha1, Zilu Tang2, Lester James V. Miranda3, Hanyang Zhao4, Mohammad Rifqi Farhansyah5, Garry Kuwanto2, Derry Wijaya2,6, Genta Indra Winata7 4Columbia University 3Allen Institute for AI 1Stanford University 2Boston University 5Institut Teknologi Bandung 6Monash Indonesia 7Capital One 5 2 0 2 9 1 ] . [ 1 8 8 3 3 1 . 5 0 5 2 : r david.anugraha@stanford.edu, genta.winata@capitalone.com"
        },
        {
            "title": "Abstract",
            "content": "Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce R3, novel reward modeling framework that is rubricagnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. R3 enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3."
        },
        {
            "title": "Introduction",
            "content": "Reward models play central role in aligning language model outputs with human preferences by assigning scalar scores to generated responses [30, 19]. However, current reward modeling approaches suffer from two significant limitations: limited controllability and poor interpretability. First, these models are often optimized for narrow objectivessuch as helpfulness or harmlessnessresulting in behavior that is overly tailored to specific metrics and not readily generalizable to broader range of downstream tasks [20, 36]. Second, the interpretability of reward scores remains unclear. For instance, scalar values like 1 or 2 on Likert scale are not inherently meaningful without an explicit explanation of what those scores represent in context. Aligning models with human preferences is crucial, but obtaining human judgments is often costly and time-consuming [38, 22, 47]. Leveraging existing human evaluations from prior research appears promising; however, it poses several challenges, including lack of standardization, varying evaluation criteria, insufficient documentation, data privacy issues, and proprietary restrictions [14]. As an alternative, using model-generated outputs for reward modeling or annotation offers greater efficiency and flexibility. This lack of generalizability and transparency presents challenges for reliably evaluating and guiding language model behavior across diverse use cases. To address these issues, we propose R3, novel reward modeling framework that is rubric-agnostic, generalizable to various evaluation dimensions, and grounded in interpretable, measurable scores. Our approach not only supports more flexible alignment with human values but also includes explicit reasoning for score assignments, enabling more transparent and trustworthy model evaluation. Corresponding authors. Preprint. Figure 1: ROBUST RUBRIC-AGNOSTIC REWARD (R3) models both the input and output of task. It takes prompt that includes an instruction, task description, input, response(s), and evaluation rubrics, and generates score along with the corresponding reasoning. Our contributions can be summarized as follows: We introduce R3, novel task-agnostic robust reward model training framework that leverages fine-grained rubrics to provide highly controllable and interpretable reward scores. These rubrics can be either hand-crafted by humans or generated by LLMs. We propose unified framework for training reward models by adapting various types of data into three standard formats: point-wise, pair-wise, and binary. We curate new reward modeling R3 dataset collected from 45 diverse sources that covers tasks such as classification, preference optimization, and question answering (Figure 2). Each example in the dataset contains an instruction and task description, input, response(s), evaluation rubrics, and score along with the corresponding reasoning (Figure 1). We demonstrate that our R3 models exhibit robust and superior performance, not only matching but often exceeding both established baselines and proprietary models across diverse suite of tasks, including reward modeling, knowledge recall, reasoning, and summarization. Importantly, our framework maintains this high level of effectiveness even under stringent resource constraintsutilizing no more than 14k training examples and limited computational capacityby leveraging efficient adaptation techniques such as low-rank adaptation (LoRA) [12]."
        },
        {
            "title": "2 Aren’t Existing Reward Models Robust Enough?",
            "content": "The challenge of building models that generalize across diverse tasks and domainsparticularly in evaluating quality from multiple aspects or human annotation metricsis well established. In this section, we present the motivation behind the need for developing new reward models. Controllability. Existing reward models, such as ArmoRM [42] and UniEval [56], offer limited support for evaluating models on fine-grained aspects. They typically require separate training for each aspect along with corresponding parameter weights, reducing flexibility during both training and evaluationespecially when dealing with unseen aspects. Similarly, models like Prometheus [13, 15] are restricted in the range of supported task types; for example, they do not accommodate binary classification. ArmoRM is further limited in that it only supports point-wise tasks, making it unsuitable for pair-wise comparisons. Interpretability. Scores generated by reward modelsparticularly those based on generative LLMs [34, 50] or some custom classifiers [45, 46, 54] can be difficult to interpret. For example, score of 0.6543 on 01 scale offers little clarity: Is it measuring helpfulness, correctness, coherence, or some 2 Table 1: comparison between existing models and R3 across various dimensions, including data types, task formats, and evaluation rubrics. The model is neither closed-source nor proprietary. Method Data ArmoRM [42] CLoud [3] GenRM [52] JudgeLRM [5] Prometheus1 [13] Prometheus2 [15] m-Prometheus [29] Self-Taught [44] SynRM [49] UniEval [56] G-Eval [24] FLAMe [38] RM-R1 [6] Size 974.4k 280k 157.2k 100K 100k 300k 480k ? 5k 185.5k ? 5M+ 100k R3 {4k, 14k} Point-wise - - ? - Pair-wise Binary - - - - - ? - - - - - - - - ? - Point-wise - - - - Tasks Pair-wise Binary Customizable Rubrics Access - - - - - - - - - - - - - - - - - - - - - opaque combination of all three? Without clearly defined rubric or accompanying explanation, such scores provide limited actionable insight, leaving users to guess what aspect of quality the number is intended to capture. Limited Compatibility on Various Tasks. Existing reward models often have limited compatibility with diverse range of tasks. For instance, models like RM-R1 [6] are primarily designed for pairwise comparisons, making them less suitable for point-wise or binary classification tasks, which limits their applicability. Similarly, Prometheus supports point-wise and pair-wise evaluations but lacks native support for binary classificationan approach that can be particularly effective for tasks like hallucination or toxicity detection, where simple binary judgments are often sufficient for assessing data quality."
        },
        {
            "title": "3 Tasks and Datasets",
            "content": "The goal of our open-ended evaluation model is to assess the quality of response according to humandefined criteria, producing both final score and natural language explanation for interpretability. Formally, given task instruction t, input instance i, one or more candidate responses a, and an evaluation rubric r, the model is tasked with generating an explanation e, that justifies the evaluation and score that reflects the response quality under the given rubric r. We define this evaluation process as function: (x) = y, where = (t, i, a, r) and = (e, s). (1) 3.1 Task Formats To support wide range of evaluation settings, we define three task formats within our unified framework: point-wise, pair-wise, and binary evaluation. Each format shares the same input structure = (t, i, a, r)and output structure = (e, s) but differs in how the candidate responses are structured and how the score is defined. Point-wise Evaluation. This format assesses the quality of single response a1 by assigning an integer score, typically on 15 scale [13]. It is suitable for open-ended generation tasks where scalar assessments of quality are needed, such as helpfulness, relevance, coherence, etc. Formally, = a1, fpointwise(t, i, a, r) = (e, s), {1, 2, 3, 4, 5}. (2) In this setting, the model compares two candidate responses a1 and a2 to Pair-wise Evaluation. the same input and selects the preferred one, along with an explanation. This format is commonly used in preference-based training. Formally, = (a1, a2), fpairwise(t, i, a, r) = (e, s), {a1, a2}. (3) 3 Figure 2: Dataset sources utilized in training the R3 model. Binary Evaluation. Binary task requires the model to make definitive judgment about the correctness or acceptability of response a1, given the input and rubric. These tasks span variety of use cases, including factual verification, binary classification (e.g., determining whether summary is faithful), and structured reasoning (e.g., assessing the validity of math or code solution). Formally, = a1, fbinary(t, i, a, r) = (e, s), {true, false}. (4) 3.2 R3 Datasets To support open-ended evaluation across diverse domains and task formats, we begin with large pool of publicly available datasets spanning over 1 million examples, which include general chat, reasoning, and classification tasks, as shown in Figure 2. However, most of these datasets lack consistent evaluation rubrics and explanation traces, which are key components to train our evaluation model to output both scores and natural language justifications. Generating such traces, particularly using strong reasoning model such as DeepSeek-R1 [9], is also computationally expensive and infeasible on large scale. To address this, we build our training dataset in multiple stages, drawing inspiration from Muennighoff et al. [26] to emphasize both quality and diversity of the training data while on limited budget. We first sample diverse subset from the raw pool, then enrich each example with on-the-fly rubric generation and explanation traces. Finally, we apply filtering and refinement to produce smaller, higher-quality datasets used in supervised training. The following sections describe each stage. 3.2.1 Initial Curation We begin by curating large collection of publicly available datasets, denoted by Dinit, which spans on three broad categories: general chat, reasoning, and classification or evaluation tasks. Each example x(j) Dinit is tuple x(j) = (cid:0)t(j), i(j), a(j), r(j) opt is optional rubric from the original dataset. (cid:1), where r(j) opt General Chat and Instruction-Following: This category includes open-domain instruction tuning and user preference data, drawn from resources such as the Tulu subset [18], UltraFeedback [7], and Skywork Reward Preference [23]. These datasets contain point-wise and pair-wise tasks. Reasoning Tasks: To support math and code reasoning evaluations, we include datasets like Math-Step-DPO-10K [17] and AceCodePair-300K [51], which contain preference annotations focused on correctness and reasoning quality on math and coding tasks. 4 Classification and Factual Evaluation: This category consists of binary and pair-wise tasks aimed at assessing factuality, consistency, and alignment with task rubrics. We include GLUE [39], SuperGLUE [40], SummEval [8], FeedbackCollection [13], PreferenceCollection [15], and EVOUNA [41]. These tasks span summarization, natural language inference, general rubric-based classification, and factual correctness. To construct binary-labeled data that includes false scores, we need to generate negative answers, as many datasets only provide the correct response (e.g., EVOUNA, GLUE, SuperGLUE). When possible, we sample negative answers from existing multiple-choice options. Otherwise, we generate negative answers using GPT-4o mini [2]. 3.2.2 Diversity Sampling To ensure feasibility for distilling reasoning traces while maintaining representative coverage across domains and reducing redundancy, we downsample Dinit to 20k-example subset D20k Dinit, manually allocating quotas per dataset to balance task types and formats. Dataset composition details can be found in Section B. For each dataset in Dinit, we perform three-stage sampling process to extract the most diverse examples: 1. Embedding and Preprocessing. We begin by embedding each instance using semantic representation that combines its task instruction and input text to capture the samples semantics across topics. Specifically, we use the gte-Qwen2-7B-instruct model [21] to compute embeddings over h(x(j)) = t(j) i(j), where denotes string concatenation. The resulting embedding Emb(h(x(j))) = q(j) is used to measure similarity and diversity in semantic space during clustering. 2. Cluster Determination and Assignment. To identify an appropriate number of groups {kmin, . . . , kmax}, we select the value of that maximizes the average Silhouette score [32]. Here we choose kmin = 3 and kmax = 10. If the dataset includes labeled subcategories (e.g., topics or task types), clustering is applied independently within each subcategory to preserve intra-category diversity. The Silhouette score for sample x(j) is defined as sj = vj wj max(vj ,wj ) where vj is the mean distance between xj and other points in the same cluster, and wj is the mean distance to the nearest cluster not containing x(j). We select the optimal number of clusters by = arg max k{kmin,...,kmax} 1 D (cid:88) j=1 s(k) , (5) where s(k) clusters. is the Silhouette score of sample x(j) under the clustering configuration with 3. Stratified Sampling with Maximal Marginal Relevance (MMR). We perform stratified sampling from each cluster with minimum of 10 samples per cluster. For each cluster with centroid qC: We retain the first 25% of samples based on the closest to the cluster centroid, to ensure topical relevance, i.e., Rclosest = op0.25C{x Emb(x) qC2}; The next 75% of the samples are selected via MMR, which balances relevance and diversity among the already selected samples. Let denote the set of already selected examples, in which initially = Rclosest. To sample the next candidate R, we compute the MMR score as: MMR(x) = λ sim(x, qC) (1 λ) max xrR sim(x, xr), (6) where sim(, ) denotes cosine similarity, and λ [0, 1] is tunable trade-off parameter, in which we set λ = 0.5 to balance relevance and diversity. The next selected example is = arg maxxCR MMR(x). For binary datasets, we retain only one instance per question, either the positive or the negative, to avoid redundancy from semantically similar content. 5 3.2.3 Rubric Generation Many datasets lack explicit evaluation rubrics, which are essential to our framework for generating structured supervision. To address this, we automatically generate rubrics based on task type at inference time. Although rubrics are produced dynamically to simulate realistic deployment, they are cached for reusability in our dataset. Formally, for each sample x(j) in D20k, we transform the optional rubric r(j) r(j), so the dataset becomes D20k = {(t(j), i(j), a(j), r(j))}20000 on task type using the following methodology: opt into required rubric j=1 . The rubrics are generated based Pair-wise and Binary Tasks. We use templated prompts to generate rubric variations tailored to each format. To encourage generalization and mitigate overfitting, we randomize the rubric phrasing across three prompt variants. Full templates are listed in Appendices C.3 and C.4. Point-wise Tasks. When original rubrics r(j) opt are available (e.g., in FeedbackCollection), we reuse them. Otherwise, we generate task-specific rubrics targeting relevant evaluation criteria (e.g., helpfulness in UltraFeedback) using few-shot prompting strategy with GPT-4o mini. Details on rubric prompting are available in Appendix C.1. 3.2.4 Explanation Trace Generation Given the curated dataset D20k = {(t(j), i(j), a(j), r(j))}20,000 tions using reasoning distillation model. Specifically, we define function: j= , we distill natural language explanaReasoningModelθ : (t(j), i(j), a(j), r(j)) (cid:0)reasoning_trace(j), ˆs(j), ˆe(j)(cid:1), where ReasoningModelθ is instantiated with DeepSeek-R1 [9]. This model generates natural language explanation (reasoning_trace(j)) along with short response of its predicted score ˆs(j) and short justification span ˆe(j), following methodologies from prior work on explanation-based distillation [35, 38]. Prompting templates are provided in Appendix C. We define the final target for each sample x(j) as: (7) y(j) = reasoning_trace(j) (ˆs(j), ˆe(j)) (8) where is string concatenation. Therefore, we define the dataset D20k as D20k = {(x(j), y(j))}20000 j=1 . Approximately 20% of the reasoning traces are either overly verbose or contain repetitive content. For any example where y(j) exceeds 4,096 tokens, we apply post-processing step using GPT-4.1 mini to summarize the reasoning trace. The summarization preserves the core explanation while removing redundant content and maintains stylistic coherence with the original output. Details and heuristics for this step are provided in Appendix E. As both the reasoning traces and their summaries are machine-generated, to verify the quality of the generated data, we conduct human evaluation in Section 4.2, where we assess the factual correctness and logical coherence of the original reasoning traces, as well as the faithfulness and style consistency of the trace summarizations. 3.2.5 Quality Filtering Finally, to improve the quality of our training dataset while preserving the diversity of responses, we apply two-stage filtering pipeline to the annotated dataset D20k. Step 1: Incorrect Prediction Filtering. We discard examples for which the predicted score differs from the ground truth. Formally, we construct filtered dataset D14k D20k such that for each retained example (x(j), y(j)) D14k, we have ˆs(j) = s(j), where s(j) is the true score for sample x(j). This ensures that all reasoning signals used for training are consistent with the gold labels. After this step, approximately 14,000 examples remain. Step 2: Triviality Filtering via Small Model Agreement. To remove overly easy examples that provide limited training signal, we evaluate each example in D14k using our smallest model, Qwen36 Table 2: Dataset size and composition of the top 7 source datasets at each stage of filtering. FC = Feedback Collection, PC = Preference Collection. Additional statistics are provided in Appendix B. ace code pair math step DPO PC ultrafeedback skywork Count tulu3 FC Raw R1 filter R1 + Qwen3-4b filter 20,000 13,772 3, 0.18 0.19 0.13 0.15 0.20 0.28 0.15 0.21 0.19 0.13 0.09 0.12 0.10 0.07 0.03 0.10 0.06 0. 0.10 0.11 0.05 4B [48]. For each example x(j), we compute predictions across five decoding runs without chainof-thought reasoning as {ˆs(j) [k] = s(j) for all {1, . . . , 5}, then we discard x(j) This results in the final dataset D4k D14k, containing approximately 4,000 challenging and diverse training examples. Brief dataset statistics are provided in Table 2. We train models with both D4k (-4K) and D14k (-14K) to assess the impact of data size. [5] } = Qwen3-4B(x(j)). If ˆs(j) [1] , . . . , ˆs(j) 3.3 Training Given our generated training data, we further use supervised fine-tuning (SFT) to enhance the base models reasoning capability as reward model by minimizing the negative log-likelihood of reference responses. Given our training dataset = {(x(i), y(i))}N i=1, where x(i) is prompt input previously introduced in eq. (1) and y(i) = (y(i) ) introduced in eq. (8) is the corresponding target sequence, the objective is the cross-entropy loss: 1 , . . . , y(i) Ti LSFT(θ) = 1 (cid:88) Ti(cid:88) i= t=1 log πθ (cid:0)y(i) y(i) <t, x(i)(cid:1) , (9) where πθ(yt y<t, x) denotes the models conditional probability of token yt given the history y<t and prompt x, parameterized by θ. By directly maximizing the log-likelihood of the ground-truth tokens, this loss encourages the base model to produce high-quality reasoning traces and the desired format for pair-wise comparisons or single-answer rewards. We further investigate lightweight fine-tuning via Low-rank Adaptation (LoRA) [12] on R3-4K data as part of our additional study to reduce training costs and data requirements while maintaining competitive performance. Given that our tasks are within multiple domains, including but not limited to verifiable tasks like math and coding, reinforcement learning (RL) techniques are not directly applicable here. Nevertheless, it is possible to explore the effectiveness of using our trained model as reward signal for enhancing other models capabilities via RL, and we leave it as future work. 3.4 Base Training Models and Evaluation For our R3 models, we perform SFT on the Qwen3 model family [48] on the 4B, 8B, and 14B scales, as well as on Phi-4-reasoning plus [1]. For evaluation, we primarily compare our models against the original reasoning teacher DeepSeek-R1 [9], Prometheus-7B-v2.0 [15], RM-R1 [6], GPT-4.1 mini, and GPT-o4 ministrong baselines known for their general and reasoning-specific capabilities. Additionally, we include models that have reported results in their respective publications. 3.5 Evaluation Datasets RewardBench [19] is popular reward model evaluation benchmark consists of 3K instances of preference pairs on four categories: Chat, Chat-Hard, Safety, Reasoning. We measure the accuracy on each category along with the overall average accuracy. FeedbackBench [13] is the test split of FeedbackCollection introduced with the Prometheus model for evaluating point-wise tasks. It contains 1K score rubrics, 200 instructions, and 1K responses that do not overlap with the train data. We measure the Kendall-Tau [31] correlation as previously done by Kim et al. [13]. 7 Table 3: Comparison of existing models with R3 on RM-Bench. Bolded numbers indicate the best-performing results between R3 models and baseline models. Proprietary models are bolded and compared independently. Model Domain Difficulty Chat Math Code Safety Easy Medium Hard Overall Avg. 46.0 Prometheus-7B-v2.0 59.9 JudgeLRM 66.6 RM-R1-Qwen-Instruct-7B 64.0 RM-R1-DeepSeek-Distilled-Qwen-7B 75.6 RM-R1-Qwen-Instruct-14B RM-R1-Qwen-Instruct-32B 75.3 RM-R1-DeepSeek-Distilled-Qwen-14B 71.8 RM-R1-DeepSeek-Distilled-Qwen-32B 74.2 R3 Models (Ours) R3-QWEN3-4B-LORA-4K R3-QWEN3-4B-4K R3-QWEN3-4B-14K R3-QWEN3-8B-LORA-4K R3-QWEN3-8B-4K R3-QWEN3-8B-14K R3-QWEN3-14B-LORA-4K R3-QWEN3-14B-4K R3-QWEN3-14B-14K Proprietary Models GPT-4.1 mini GPT-o4 mini DeepSeek-R1 71.9 71.4 69.3 72.1 73.2 72.5 76.5 76.9 74.3 67.6 77.6 78.6 52.6 59.9 67.0 83.9 75.4 80.2 90.5 91. 90.6 90.3 91.0 91.0 90.8 91.3 92.4 91.3 92.6 73.0 93.0 66.2 47.6 51.9 54.6 56.2 60.6 66.8 69.5 74.1 69.2 69.8 71.4 71.3 71.7 72.7 78.7 75.7 76.9 71.3 80.8 81.9 73.9 87.3 92.6 85.3 93.6 93.9 94.1 95. 88.5 88.6 90.5 90.0 90.8 91.8 91.9 91.3 91.2 90.7 93.4 88.7 68.8 73.2 79.2 75.9 82.6 86.3 86.2 89.5 87.7 87.2 88.8 88.2 87.8 89.4 91.4 90.9 91.2 87.0 92.0 86.9 54.9 76.6 71.7 73.1 77.5 80.5 83.6 85. 80.7 80.7 82.5 81.7 82.7 84.5 86.2 85.5 86.3 78.4 88.7 82.2 41.3 54.8 59.7 68.1 68.8 70.4 74.4 76.7 71.8 72.1 70.4 73.4 74.4 72.3 77.1 75.6 73.8 61.7 78.0 67.3 55.0 64.7 70.2 72.4 76.1 79.1 81.5 83. 80.0 80.0 80.6 81.1 81.6 82.1 84.9 84.0 83.8 75.7 86.2 78.8 MMLU-STEM [11] is STEM-subject related subset2 of the MMLU benchmark with multiplechoice questions from various branches of knowledge. Given four potential choices and one correct answer, we convert it to binary evaluation task. For each original question, we evaluate models response given the correct and separate randomly selected incorrect answer. We measure the overall accuracy, along with the accuracy on each subject. BBH [37] is collection of 27 non-trivial reasoning-like tasks sourced from BigBench [4] with total of 6.7K instances. The format of the tasks can be multiple choice or short string completion. Similar to MMLU-STEM, we include copy of the data with the correct response and copy with the incorrect response. Details of the dataset generation process is in Appendix G. We measure the overall accuracy. RM-Bench [25] is reward model evaluation benchmark consisting of 1.3K instances that cover four domains: Chat, Safety, Math, and Code. Each instance consists of three prompts categorized by difficulty level: easy, medium, and hard. We measure the accuracy on each domain and difficulty level, along with the overall average accuracy. XSUM [27] is news summarization dataset. For our evaluation, we use subset that has been annotated by human evaluators across three criteria: faithfulness (binary), coherence (Likert scale 15), and relevance (Likert scale 15), following the annotation protocol of Zhang et al. [53]. We measure the Kendall-Tau [31] correlation for coherence and relevance, while we measure accuracy for faithfulness."
        },
        {
            "title": "4 Results and Analysis",
            "content": "In this section, we present the overall performance and describe the human evaluation conducted to assess the quality of reasoning traces generated by the DeepSeek-R1 model. 2https://huggingface.co/datasets/TIGER-Lab/MMLU-STEM. 8 Table 4: Comparison of existing models with R3 on RewardBench using pair-wise scoring. Bolded numbers indicate the best-performing results between R3 models and baseline models. Proprietary models are bolded and compared independently. Models Chat Chat Hard Safety Reasoning Avg. 90.2 Prometheus-7B-v2.0 93.6 m-Prometheus-14B 92.9 JudgeLRM 38.0 SynRM 88.9 RM-R1-DeepSeek-Distilled-Qwen-7B 94.1 RM-R1-Qwen-Instruct-7B RM-R1-Qwen-Instruct-14B 93.6 RM-R1-DeepSeek-Distilled-Qwen-14B 91.3 R3 Models (Ours) R3-QWEN3-4B-LORA-4K R3-QWEN3-4B-4K R3-QWEN3-4B-14K R3-QWEN3-8B-LORA-4K R3-QWEN3-8B-4K R3-QWEN3-8B-14K R3-QWEN3-14B-LORA-4K R3-QWEN3-14B-4K R3-QWEN3-14B-14K R3-Phi4-R+ 14B (4k) Propretiary Models GPT-4.1 mini GPT-o4 mini DeepSeek-R 93.0 91.9 92.7 93.6 91.1 93.3 93.0 92.7 93.6 91.6 96.1 95.3 93.6 45.6 59.0 56.4 82.5 66.2 74.6 80.5 79.4 68.0 76.5 75.7 75.4 75.7 75.7 79.8 78.1 77.6 76.1 75.2 81.8 79.2 75.8 85.1 78.2 74.1 78.4 85.2 86.9 89. 81.9 85.5 87.2 85.9 85.5 85.7 87.4 85.4 85.3 82.9 87.0 91.6 86.9 74.6 84.8 73.6 87.1 87.0 86.7 92.0 95.5 92.9 94.0 94.5 94.9 93.8 95.3 97.1 96.2 96.3 91.0 89.6 98.4 97.4 71.6 80.6 75.2 70.4 80.1 85.2 88.2 88. 83.9 87.0 87.5 87.5 86.5 87.5 89.3 88.1 88.2 85.4 87.0 91.8 89.3 4.1 Overall Performance Table 3 and Table 4 highlight the strong performance of our R3 models on RM-Bench and RewardBench, showcasing the effectiveness of R3 models in pair-wise preference scoring under training budget. Particularly in RM-bench, our models deliver remarkable results where even our smallest model, R3-QWEN3-4B, including its LoRA variant, outperforms nearly all other reasoning models from RM-R1, with the exception of RM-R1-DeepSeek-Distilled-Qwen-14B and RM-R1-DeepSeekDistilled-Qwen-32B. It also surpasses Prometheus-7B-v2.0, GPT-4.1 mini, and even DeepSeek-R1 as well, demonstrating its competitiveness. Furthermore, both full SFT and LoRA SFT versions of R3-QWEN3-14B-4K outperforms RM-R1s best model, RM-R1-DeepSeek-Distilled-Qwen-32B, by up to 1.0 point. Overall, the exceptional performance of our R3-QWEN3-14B-LORA-4K model on key benchmarks highlights the robustness and effectiveness of our training approach and meticulous data curation strategies. In RewardBench, we achieve similarly impressive results. Our R3-QWEN3-4B models, despite being half the size of the RM-R1 7B models, outperform all RM-R1 7B models as well as Prometheus-7Bv2.0 by at least 1.8 points. Furthermore, the R3-QWEN3-4B-14K model surpasses GPT-4.1 mini by 0.5 points. When assessing our R3-QWEN3-14B models against the RM-R1 14B model families, the R3-QWEN3-14B-LORA-4K model surpasses RM-R1-DeepSeek-Distilled-Qwen-14B by 0.4 points, while matching the average performance of DeepSeek-R1. Notably, our models demonstrate competitive performance even compared with DeepSeek-R1 under training budget constraints in terms of data and memory. Table 5 presents the performance of our R3 models on point-wise assessment tasks, which are XSUM and FeedbackBench, along with binary tasks from BBH and MMLU-STEM. For XSUM, all R3 models consistently outperform DeepSeek-R1 and Prometheus-7B-v2.0 in terms of faithfulness, highlighting the effectiveness of binary assessment for R3 models. In terms of coherence and relevance, our models also outperform Prometheus-7B-v2.0 and GPT-o4 mini, while remaining competitive with DeepSeek-R1 in coherence. On the generic rubric-based assessment from FeedbackBench, R3 models perform competitively against all baselines. While Prometheus-7B-v2.0 achieves the highest 9 Table 5: Comparison of existing models with R3 on XSUM, Feedback Bench, BBH, and MMLUSTEM. Bolded numbers indicate the best-performing results between R3 models and baseline models. Proprietary models are bolded and compared independently. Models XSUM Acc. Kendall Tau Faithfulness Coherence Relevance Prometheus-7B-v2.0 R3 Models (Ours) R3-QWEN3-4B-LORA-4K R3-QWEN3-4B-4K R3-QWEN3-4B-14K R3-QWEN3-8B-LORA-4K R3-QWEN3-8B-4K R3-QWEN3-8B-14K R3-QWEN3-14B-LORA-4K R3-QWEN3-14B-4K R3-QWEN3-14B-14K R3-Phi4-R+ 14B (4k) Propretiary Models GPT-4.1 mini GPT-o4 mini DeepSeek-R 60.7 70.6 67.8 67.2 68.6 68.1 66.3 68.6 68.5 67.3 67.0 72.6 69.1 60.4 0.12 0.16 0.11 0.24 0.23 0.21 0.35 0.38 0.27 0.36 0.31 0. 0.07 0.16 0.35 0.23 0.27 0.30 0.32 0.31 0.34 0.33 0.35 0.35 0.46 0.38 0.30 0.38 Feedback Bench BBH MMLU-STEM Acc. Kendall Tau Acc. 0.79 0.62 0.65 0.70 0.66 0.64 0.75 0.66 0.69 0.71 0.65 0.69 0.65 0.72 54.0 56.5 88.6 89.0 89.2 91.1 90.2 90.7 91.7 91.8 91.8 90. 91.0 93.2 94.0 91.7 91.5 92.0 93.3 93.1 93.3 94.5 94.3 94.7 93.4 93.3 95.3 96.2 score on FeedbackBench, its relatively weaker performance on other benchmarks suggests that it may be better aligned with the specific rubric or distribution of FeedbackBench, rather than generalizing broadly across diverse tasks. For binary classification tasks such as BBH and MMLU-STEM, we observe that both larger model size and greater training data improve performance, reflecting stronger reasoning capabilities. All of our R3 models outperform Prometheus-7B-v2.0, while R3-QWEN3-14B models surpass GPT-4.1 minis performance. Overall, these results highlight the competitive and robust performance of R3 models across range of point-wise and binary evaluation tasks. 4.2 Human Evaluation of Reasoning Traces We annotate about 2% of the 4K dataset, stratified sampled from different dataset sources to verify both the reliability of reasoning traces and quality of the trace summarization. Details of the annotations setup, experiments, and results are in Appendix H. We find on average the reasoning traces score 2.9 0.2 (out of 3, higher better) in factual correctness, 2.8 0.2 in logical coherence (n=93). The faithfulness of the summary scores on average 2.8 0.5 and the style consistency scores 2.7 0.4 (n=84). These confirm the high quality reasoning traces used in our dataset. 4.3 Model Scaling and Efficiency We observe that our R3 models consistently improve with larger model sizes across all benchmarks, with some showing significant gains. For instance, on RM-Bench, our method achieves up to 4.9-point improvement. In contrast, baseline models such as RM-R1 and Prometheus-7B-v2.0 do not perform as well at similar model sizes. Notably, our models even outperform larger models like RM-R1-DeepSeek-Distilled-Qwen-32B. Furthermore, models trained using LoRA demonstrate performance on par with full fine-tuning, highlighting the effectiveness of our approach even in compute-efficient training regimes. These results suggest that both our methodology and dataset are highly effective for training reward models in resource-constrained settings. 4.4 Robustness Among proprietary models, GPT-4o-mini outperforms DeepSeek-R1 on reward benchmarks involving pair-wise scoring, while DeepSeek-R1 demonstrates stronger performance on tasks such as XSUM, Feedback Bench, BBH, and MMLU-STEM. For open-weight models, our R3 models consistently outperform existing reward models, such as Prometheus-7B-v2.0 and all RM-R1 variants, across most benchmarks. The only exception is Feedback Bench, where Prometheus-7B-v2.0 performs exceptionally well. However, this suggests that Prometheus-7B-v2.0 is highly specialized rather than robust across tasks. In contrast, RM-R1 is more robust than Prometheus-7B-v2.0 but lacks flexibility in supporting diverse evaluation formats such as point-wise and binary scoring; Prometheus, meanwhile, supports only point-wise and pair-wise formats. Our R3 models offer both robustness and versatility, making it more suitable for general-purpose reward modeling."
        },
        {
            "title": "5 Related Work",
            "content": "Rubric-Based Evaluation Models. Recent works leverage explicit rubrics to guide LLM evaluation. Kim et al. [13] created FeedbackCollection, fine-grained text evaluation finetuning dataset using detailed rubric for point-wise (direct accessment) evaluation. [15] followed-up by adding pair-wise evaluation to the training and found that weight merging performs better than training jointly trained model. Likewise, LLM-Rubric (Hashemi et al. [10]) prompts an LLM on human-authored multi-question rubric (e.g. dimensions like naturalness, conciseness, citation quality) and calibrates its outputs via small model to match human judges. These rubric-driven methods yield fine-grained, interpretable assessments, but their reliance on laboriously constructed rubrics and reference solutions limits scalability and generality ([10]; [13]). By contrast, R3 eliminates the need for external rubrics, learning reward signals directly in transparent form to enable broad, rubric-agnostic evaluation. Preference-Based Reward Models. Reward models learned from (implicit or explicit) human preferencestypically via RLHF or related methodshave become standard alignment approach [30]. In practice, however, learned RMs often exploit trivial cues: for instance, they tend to favor longer or more elaborate outputs (a well-known length bias) over brevity [33], and recent analyses show LLM evaluators even self-recognize and prefer their own generations over others of equal quality [28]. Zhu et al. [57] further demonstrate model preference bias in RMs, whereby certain models outputs are systematically overvalued. Such biases and spurious correlations undermine fairness and generalization. New techniques mitigate these issues: Direct Preference Optimization (DPO) recasts RLHF in simpler optimization framework [30], and Vu et al. [38] train FLAMe on 5M+ human judgements across 100+ tasks, achieving stronger OOD generalization and even outperforming GPT-4 on reward-modeling benchmarks. Despite these advances, preference-trained RMs remain large, opaque models tied to specific data, motivating R3s interpretable, rubric-free reward formulation as more transparent alternative. LLM-as-a-judge Framework. Using pretrained LLM itself as the evaluator has gained popularity due to its zero-shot flexibility [15]. However, numerous studies reveal reliability issues. For instance, Wang et al. [43] found that simply altering the order of candidate responses can drastically flip an LLM judges ranking, making one model appear vastly superior or inferior. More broadly, LLM evaluators suffer from hallucinations and entrenched biases; e.g., Panickssery et al. [28] show LLM judges systematically score their own outputs higher than others (self-preference bias), and Zhu et al. [57] observe strong model-specific scoring bias in LLM-based evaluation. These flaws undermine trust and consistency in LLM-as-judge systems. R3 addresses these gaps by offering fully interpretable reward model that does not rely on opaque LLM judgments or fixed rubrics."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce R3, novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and capable of producing interpretable, reasoning-based score assignments. Leveraging reasoning distillation, targeted dataset curation (4K and 14K examples), and two-stage quality filtering pipeline, R3 addresses key limitations of prior reward models in terms of interpretability, controllability, and generalizability. Despite using training datasets that are an order of magnitude smaller than those of many baselines, R3 models matches or surpasses state-of-the-art performance. Our experiments demonstrate the methods strong training efficiency and scalability, including effective use of compute-efficient techniques such as low-rank adaptation (LoRA). By enabling more transparent and adaptable evaluation, R3 advances robust alignment with diverse human values and real-world applicationspaving the way for more trustworthy and versatile reward models."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to acknowledge Shou-Yi Hung for his support and contributions to this project."
        },
        {
            "title": "References",
            "content": "[1] M. Abdin, S. Agarwal, A. Awadallah, V. Balachandran, H. Behl, L. Chen, G. de Rosa, S. Gunasekar, M. Javaheripi, N. Joshi, et al. Phi-4-reasoning technical report. arXiv preprint arXiv:2504.21318, 2025. [2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Z. Ankner, M. Paul, B. Cui, J. D. Chang, and P. Ammanabrolu. Critique-out-loud reward models. arXiv preprint arXiv:2408.11791, 2024. [4] B. bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj. [5] N. Chen, Z. Hu, Q. Zou, J. Wu, Q. Wang, B. Hooi, and B. He. Judgelrm: Large reasoning models as judge. arXiv preprint arXiv:2504.00050, 2025. [6] X. Chen, G. Li, Z. Wang, B. Jin, C. Qian, Y. Wang, H. Wang, Y. Zhang, D. Zhang, T. Zhang, et al. Rm-r1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387, 2025. [7] G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. [8] A. R. Fabbri, W. Kryscinski, B. McCann, C. Xiong, R. Socher, and D. Radev. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391409, 2021. [9] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [10] H. Hashemi, J. Eisner, C. Rosset, B. Van Durme, and C. Kedzie. Llm-rubric: multidimensional, calibrated approach to automated evaluation of natural language texts. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1380613834, 2024. [11] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [12] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [13] S. Kim, J. Shin, Y. Cho, J. Jang, S. Longpre, H. Lee, S. Yun, S. Shin, S. Kim, J. Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations, 2023. [14] S. Kim, J. Suk, J. Y. Cho, S. Longpre, C. Kim, D. Yoon, G. Son, Y. Cho, S. Shafayat, J. Baek, et al. The biggen bench: principled benchmark for fine-grained evaluation of language models with language models. arXiv preprint arXiv:2406.05761, 2024. [15] S. Kim, J. Suk, S. Longpre, B. Y. Lin, J. Shin, S. Welleck, G. Neubig, M. Lee, K. Lee, and M. Seo. Prometheus 2: An open source language model specialized in evaluating other language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 43344353, 2024. 12 [16] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [17] X. Lai, Z. Tian, Y. Chen, S. Yang, X. Peng, and J. Jia. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. [18] N. Lambert, J. Morrison, V. Pyatkin, S. Huang, H. Ivison, F. Brahman, L. J. V. Miranda, A. Liu, N. Dziri, S. Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [19] N. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin, K. Chandu, N. Dziri, S. Kumar, T. Zick, Y. Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. [20] M. Li, J. Weston, and S. Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087, 2019. [21] Z. Li, X. Zhang, Y. Zhang, D. Long, P. Xie, and M. Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. [22] Q. Lin, H. Lu, C. Yuan, X. Wang, H. Jiang, and W. Chen. Data with high and consistent preference difference are better for reward model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2748227490, 2025. [23] C. Y. Liu, L. Zeng, J. Liu, R. Yan, J. He, C. Wang, S. Yan, Y. Liu, and Y. Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024. [24] Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 25112522, 2023. [25] Y. Liu, Z. Yao, R. Min, Y. Cao, L. Hou, and J. Li. Rm-bench: Benchmarking reward models of language models with subtlety and style. arXiv preprint arXiv:2410.16184, 2024. [26] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Candès, and T. Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [27] S. Narayan, S. B. Cohen, and M. Lapata. Dont give me the details, just the summary! arXiv preprint topic-aware convolutional neural networks for extreme summarization. arXiv:1808.08745, 2018. [28] A. Panickssery, S. Bowman, and S. Feng. Llm evaluators recognize and favor their own generations. Advances in Neural Information Processing Systems, 37:6877268802, 2024. [29] J. Pombal, D. Yoon, P. Fernandes, I. Wu, S. Kim, R. Rei, G. Neubig, and A. F. Martins. M-prometheus: suite of open multilingual llm judges. arXiv preprint arXiv:2504.04953, 2025. [30] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [31] P. K. Sen. Estimates of the regression coefficient based on kendalls tau. Journal of the American statistical association, 63(324):13791389, 1968. [32] K. R. Shahapure and C. Nicholas. Cluster quality analysis using silhouette score. In 2020 IEEE 7th international conference on data science and advanced analytics (DSAA), pages 747748. IEEE, 2020. [33] W. Shen, R. Zheng, W. Zhan, J. Zhao, S. Dou, T. Gui, Q. Zhang, and X.-J. Huang. Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 28592873, 2023. 13 [34] T. Shiwen, Z. Liang, C. Y. Liu, L. Zeng, and Y. Liu. Skywork critic model series. https:// huggingface.co/Skywork, September 2024. URL https://huggingface.co/Skywork. [35] K. Shridhar, A. Stolfo, and M. Sachan. Distilling reasoning capabilities into smaller language In Findings of the Association for Computational Linguistics: ACL 2023, pages models. 70597073, 2023. [36] R. Stureborg, D. Alikaniotis, and Y. Suhara. Large language models are inconsistent and biased evaluators. arXiv preprint arXiv:2405.01724, 2024. [37] M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, , and J. Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [38] T. Vu, K. Krishna, S. Alzubi, C. Tar, M. Faruqui, and Y.-H. Sung. Foundational autoraters: Taming large language models for better automatic evaluation. arXiv preprint arXiv:2407.10817, 2024. [39] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: multitask benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. [40] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. Superglue: stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019. [41] C. Wang, S. Cheng, Q. Guo, Y. Yue, B. Ding, Z. Xu, Y. Wang, X. Hu, Z. Zhang, and Y. Zhang. Evaluating open-qa evaluation. Advances in Neural Information Processing Systems, 36: 7701377042, 2023. [42] H. Wang, W. Xiong, T. Xie, H. Zhao, and T. Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1058210592, 2024. [43] P. Wang, L. Li, L. Chen, Z. Cai, D. Zhu, B. Lin, Y. Cao, L. Kong, Q. Liu, T. Liu, et al. Large language models are not fair evaluators. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94409450, 2024. [44] T. Wang, I. Kulikov, O. Golovneva, P. Yu, W. Yuan, J. Dwivedi-Yu, R. Y. Pang, M. Fazel-Zarandi, J. Weston, and X. Li. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024. [45] Z. Wang, A. Bukharin, O. Delalleau, D. Egert, G. Shen, J. Zeng, O. Kuchaiev, and Y. Dong. Helpsteer2-preference: Complementing ratings with preferences. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=MnfHxPP5gs. [46] G. I. Winata, D. Anugraha, L. Susanto, G. Kuwanto, and D. T. Wijaya. Metametrics: Calibrating metrics for generation tasks using human preferences. arXiv preprint arXiv:2410.02381, 2024. [47] G. I. Winata, H. Zhao, A. Das, W. Tang, D. D. Yao, S.-X. Zhang, and S. Sahu. Preference tuning with human feedback on language, speech, and vision tasks: survey. Journal of Artificial Intelligence Research, 82:25952661, 2025. [48] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [49] Z. Ye, F. Greenlee-Scott, M. Bartolo, P. Blunsom, J. A. Campos, and M. Gallé. Improving reward models with synthetic critiques. arXiv preprint arXiv:2405.20850, 2024. [50] J. Yu, S. Sun, X. Hu, J. Yan, K. Yu, and X. Li. Improve llm-as-a-judge ability as general ability. arXiv preprint arXiv:2502.11689, 2025. [51] H. Zeng, D. Jiang, H. Wang, P. Nie, X. Chen, and W. Chen. Acecoder: Acing coder rl via automated test-case synthesis. arXiv preprint arXiv:2502.01718, 2025. 14 [52] L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar, and R. Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [53] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B. Hashimoto. Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics, 12:3957, 2024. [54] Y. Zhang, G. Zhang, Y. Wu, K. Xu, and Q. Gu. General preference modeling with preference representations for aligning language models. arXiv preprint arXiv:2410.02197, 2024. [55] Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, Z. Feng, and Y. Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/ 2403.13372. [56] M. Zhong, Y. Liu, D. Yin, Y. Mao, Y. Jiao, P. Liu, C. Zhu, H. Ji, and J. Han. Towards unified multi-dimensional evaluator for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 20232038, 2022. [57] X. Zhu, C. Tan, P. Chen, R. Sennrich, Y. Zhang, and H. Hu. Charm: Calibrating reward models with chatbot arena scores. arXiv preprint arXiv:2504.10045, 2025."
        },
        {
            "title": "A Limitations",
            "content": "In our experiments, we limit our exploration to models with up to 14B parameters due to resource constraints. We also include smaller models in our study, aiming to shed light on scaling behavior and its impact on performance. Larger models, such as those with 32B parameters or more, are left for future investigation."
        },
        {
            "title": "B Dataset Statistics",
            "content": "B.1 Prompt and Response Length In Table 6 we document the length distribution of our dataset. Table 6: Length (white-space separated word count) distribution of our dataset. Response length includes DeepSeek-R1 thinking tokens along with the short response, which contains an explanation and the score assigned. prompt length response length D20k D14k (Filter Step 1) D4k (Filter Step 2) 504 302 497 413 442 224 850 847 729 538 851 599 B.2 Label Distribution In Table B.2 we show the label distribution of our dataset across different filtering stages. Our raw dataset has balanced distribution within each evaluation type. In D14k (Filter Step 1), binary labels are slightly biased towards \"false\" and pair-wise labels are slightly biased towards \"Response 1\". In D4k (Filter Step 2), binary labels are slightly biased towards \"true\" and and pair-wise labels are slightly biased towards \"Response 1\". Point-wise scores are also biased towards middle values (i.e., \"3\"). Table 7: Dataset label distribution across the filtering process. binary pair-wise point-wise true false resp. 1 resp. 1 2 3 4 5 D20k D14k (Filter Step 1) D4k (Filter Step 2) 0.024 0.024 0.033 0.026 0.031 0.022 0.34 0.429 0.365 0.335 0.354 0.304 0.047 0.04 0.035 0.053 0.036 0. 0.055 0.021 0.7 0.058 0.038 0.061 0.062 0.027 0."
        },
        {
            "title": "C Prompt Template",
            "content": "C.1 Rubric Generation Template For point-wise tasks, we generate rubric with Likert score from 1 to 5 using the following template. Rubric generation template You are an expert evaluator. Given defined task, analyze the task and create rubric using Likert scale from 1 to 5 to that will help to perform the given task. Please follow these steps: 1. Explain the criteria for distinguishing between the scores (e.g., how score of 1 differs from score of 5). 2. Based on your analysis, generate rubric in JSON format with the Likert scale ranging from 1 to 5, including descriptions for each score. 3. Ensure that the rubric is clear, actionable, and covers key aspects of the task. 16 ### TASK {task_instruction} ### INPUT {input/question} ### EXAMPLE RUBRICS (Unrelated Tasks) {sample_rubrics} ### RUBRIC FOR CURRENT TASK C.2 Point-wise Evaluation For point-wise tasks where the judge model needs to assign score for response from 1-5, we use the following template. Pointwise evaluation prompt template Evaluate the response based on the given task, input, response, and evaluation rubric. Provide fair and detailed assessment following the rubric. ### TASK {task_instruction} ### INPUT {input/question} ### RESPONSE {response} ### EVALUATION RUBRIC 1: {score_of_1_description} 2: {score_of_2_description} 3: {score_of_3_description} 4: {score_of_4_description} 5: {score_of_5_description} ### OUTPUT FORMAT Return JSON response in the following format: { \"explanation\": \"Explanation of why the response received particular score\", \"score\": \"Score assigned to the response based on the rubric between 1 to 5\" } ### EVALUATION C.3 Pair-wise Evaluation For pair-wise tasks where the judge model needs to compare against two responses, we use the following template."
        },
        {
            "title": "Pairwise evaluation prompt template",
            "content": "Evaluate the response based on the given task, input, two responses, and evaluation rubric. Provide fair and detailed assessment following the rubric. ### TASK {task_instruction} ### INPUT {input/question} ### RESPONSE 1 {response_1} ### RESPONSE 2 {response_2} ### EVALUATION RUBRIC Response 1: Response 1 provided better response, rejecting Response 2. Response 2: Response 2 provided better response, rejecting Response 1. ### OUTPUT FORMAT Return JSON response in the following format: { \"explanation\": \"Explanation of why one response is preferred over the other\", \"score\": \"Final selection between Response 1 or Response 2\" } ### EVALUATION For rubrics, we include three variations and uniformly randomly sample from them when creating our dataset. Pairwise evaluation rubric variation 1 { \"response_1\": \"Response 1 is the preferred choice over Response 2.\", \"response_2\": \"Response 2 is the preferred choice over Response 1.\" } Pairwise evaluation rubric variation 2 { \"response_1\": \"Response 1 provided better response, rejecting Response 2.\", \"response_2\": \"Response 2 provided better response, rejecting Response 1.\" } Pairwise evaluation rubric variation 3 { \"response_1\": \"Response 1 is superior, meaning Response 2 is not chosen.\", \"response_2\": \"Response 2 is superior, meaning Response 1 is not chosen.\" } 18 C.4 Binary Evaluation For binary tasks where the judge model needs to classify true or false to the response, we use the following template."
        },
        {
            "title": "Binary evaluation prompt template",
            "content": "Evaluate the response based on the given task, input, response, and evaluation rubric. Provide fair and detailed assessment following the rubric. ### TASK {task_instruction} ### INPUT {input/question} ### RESPONSE {response} ### EVALUATION RUBRIC true: The response accurately reflects the correct answer based on the input. false: The response does not accurately reflect the correct answer based on the input. ### OUTPUT FORMAT Return JSON response in the following format: { \"explanation\": \"Explanation of why the answer is true or false\", \"score\": \"Final boolean answer between true or false\" } ### EVALUATION For rubrics, we include three variations and uniformly randomly sample from them when creating our dataset. Binary evaluation rubric variation 1 { \"true\": \"The answer correctly corresponds to the given input.\", \"false\": \"The answer does not correctly correspond to the given input.\" } Binary evaluation rubric variation { \"true\": \"The response accurately reflects the correct answer based on the input.\", \"false\": \"The response does not accurately reflect the correct answer based on the input.\" } Binary evaluation rubric variation 3 { \"true\": \"The answer is appropriate and correctly derived from the input.\", \"false\": \"The answer is inappropriate or incorrectly derived from the input.\" }"
        },
        {
            "title": "D Example Prompts and Responses",
            "content": "D.1 Point-wise Evaluation Prompt example for pointwise evaluation (sourced from FeedbackCollection) Evaluate the response based on the given task, input, response, and evaluation rubric. Provide fair and detailed assessment following the rubric. ### TASK Does the final conclusion drawn by the response hold up to logical scrutiny and provide correct solution for an instruction with definite answer? ### INPUT Imagine situation where there is debate going on regarding the total number of prime numbers between 1 and 100. Your task is to determine the correct count and also provide the list. The debate is currently revolving around three different answers - 25, 26 and 27 prime numbers. ### RESPONSE The total number of prime numbers between 1 and 100 is definitely 30. Prime numbers are those numbers that only have 1 and the number itself as factors. We dont need to list down each of them because its already known that there are 30 prime numbers between 1 and 100. Its simple mathematics, and there is no need for any debate on this matter. The discussion around the number being 25, 26, or 27 is irrelevant and baseless. So, the conclusion is there are 30 prime numbers between 1 and 100. ### EVALUATION RUBRIC 1: The concluding answer from the model is entirely false and devoid of logical foundation. 2: The concluding answer from the model has major flaws that seriously compromise its validity. 3: The concluding answer from the model bears considerable mistakes that demand significant rectification. 4: The concluding answer from the model has slight inaccuracies, but these are simple to fix and do not greatly affect its overall validity. 5: The concluding answer from the model is wholly correct and logically sound. ### OUTPUT FORMAT Return JSON response in the following format: { \"explanation\": \"Explanation of why the response received particular score\", \"score\": \"Score assigned to the response based on the rubric between 1 to 5\" } ### EVALUATION Example response from R1 (summarized) used for SFT <think> Okay, let me evaluate this response. The user asked about the number of primes between 1 and 100, with the debate being 25, 26, or 27. The response says there are 30 primes, which is definitely wrong. The actual count is 25. The response claims its \"simple mathematics\" and dismisses the debate, but thats incorrect. The answer provided is entirely false, and the reasoning is flawed because they didnt list or verify the primes. The rubric says score of 1 is for entirely false and no logical foundation. This fits because the answer is wrong and the logic is absent. So, score 1. 20 </think> { \"explanation\": \"The response concludes there are 30 prime numbers between 1 and 100, which is factually incorrect. The actual number is 25. The claim that its already known there are 30 primes is baseless, and the dismissal of the debate over 25, 26, or 27 as irrelevant ignores the correct answer (25). The response fails to provide logical reasoning, correct list, or acknowledgment of established mathematical facts. This demonstrates complete lack of validity and logical foundation.\", \"score\": \"1\" } D.2 Pair-wise Evaluation Prompt example for pairwise evaluation (sourced from Tulu-3-sft) Evaluate the response based on the given task, input, two responses, and evaluation rubric. Provide fair and detailed assessment following the rubric. ### TASK Evaluate the factual accuracy of the response. Consider whether the information provided is correct, up-to-date, and free from errors or misconceptions. ### INPUT Come up with question and reasoning that would justify this answer: Raphael Sbarge Your ENTIRE response should be in Chinese, no other language is allowed. Come up with question and reasoning that would justify this answer: Raphael Sbarge ### RESPONSE 1 Question: Blade这部吸血鬼题材的电影中扮演主角V的演员是谁 Reasoning: 电 影 Blade 是 一 部 著 名 的 吸 血 鬼 题 材 作 品 Raphael Sbarge在 这 部电影中饰演了主角V这是一个非常有影响力的角色因此这个问题和回答完 美契合Raphael Sbarge正是该角色的扮演者 ### RESPONSE 2 问题谁是演 Once Upon Time 中的 \"金Pinocchio/Archie Hopper\" 一角的演员 理 由 Raphael Sbarge 是 美 国 演 员 他 最 著 名 的 角 色 之 一 就 是 美 国 电 视 剧 Once Upon Time 中的金Pinocchio/Archie Hopper ### EVALUATION RUBRIC Response 1: Response 1 provided better response, rejecting Response 2. Response 2: Response 2 provided better response, rejecting Response 1. ### OUTPUT FORMAT Return JSON response in the following format: { \"explanation\": \"Explanation of why one response is preferred over the other\", \"score\": \"Final selection between Response 1 or Response 2\" } ### EVALUATION 21 Pairwise example response from R1 (summarized) used for SFT <think> 好的我现在需要评估两个回答看看哪个在事实准确性上更好任务是根据提供 的输入判断哪个回答更准确最新且没有错误输入是让用户想出一个问题和理 由来证明答案是Raphael Sbarge 首先看Response 1的问题Blade这部吸血鬼题材的电影中扮演主角V的演 员是谁理由里说Raphael Sbarge在这部电影中饰演了主角V但根据我的了解 Blade系列电影的主角Blade是由Wesley Snipes扮演的而V可能是指另一部电 影V字仇杀队里的主角V但那个是由Hugo Weaving饰演的Raphael Sbarge确实 参演过Blade吗我需要确认一下查资料发现Raphael Sbarge确实在1998年 的刀锋战士Blade中出演过一个角色但并不是主角V而是次要角色Dr. Curtis Webb所以Response 1的问题和理由存在错误把主角V的扮演者错误地归 为Sbarge这显然是错误的 接下来看Response 2的问题谁是演 Once Upon Time 中的 金Pinocchio/Archie Hopper 一角的演员理由指出Sbarge在该剧中扮演这个角色根据资料Raphael Sbarge确实在童话镇Once Upon Time中扮演了Archie Hopper也就是木偶 匹诺曹的人类形态这个信息是正确的所以Response 2的问题和理由都是事实正确 的 根 据 评 分 标 准 应 该 选 择 事 实 准 确 的 回 答 显 然Response 2正 确 而Response 1有错误因此Response 2更优 </think>json { \"explanation\": \"Response 1存在事实错误Blade系列电影的主角V并非由Raphael Sbarge饰演实际由Wesley Snipes饰演且该电影中Raphael Sbarge仅饰演次要角 色Dr. Curtis WebbResponse 2则正确指出Raphael Sbarge在Once Upon Time中饰 演Archie Hopper/金Pinocchio这一信息符合事实且无错误因此Response 2在事实 准确性上明显优于Response 1\", \"score\": \"Response 2\" } D.3 Binary Evaluation Prompt example for binary evaluation (sourced from evouna) Evaluate the response based on the given task, input, response, and evaluation rubric. Provide fair and detailed assessment following the rubric. ### TASK Your task is to determine whether the given answer response is correct based on the query input. ### INPUT who was the killer in the movie know what you did last summer ### RESPONSE Ben Willis ### EVALUATION RUBRIC true: The response accurately reflects the correct answer based on the input. false: The response does not accurately reflect the correct answer based on the input. ### OUTPUT FORMAT Return JSON response in the following format: { \"explanation\": \"Explanation of why the answer is true or false\", \"score\": \"Final boolean answer between true or false\" } ### EVALUATION Prompt example for binary evaluation (sourced from evouna) <think> Okay, lets tackle this evaluation. The user is asking whether the killer in the movie \"I Know What You Did Last Summer\" is Ben Willis, as stated in the response. First, need to recall the plot of the movie to verify this. From what remember, the movie revolves around group of friends who hit someone with their car and then cover it up. The killer is later revealed to be seeking revenge for that incident. The main antagonist is indeed Ben Willis, who is also known as the Fisherman. Hes the father of the person they hit, and he faked his own death to frame someone else. So, the response \"Ben Willis\" is correct. The answer should be true. </think> { \"explanation\": \"In the movie Know What You Did Last Summer, the killer is revealed to be Ben Willis, also known as the Fisherman. He is the father of the victim the group accidentally hit with their car and covers up. Ben Willis fakes his death and seeks revenge, making the response accurate.\", \"score\": \"true\" }"
        },
        {
            "title": "E Explanation Trace Summarization Details",
            "content": "First, we perform inference using the model to obtain the initial reasoning trace. This training trace is then passed through the model once more; conditioned on the prompt shown in \"Prompt for Summarization Tracing\" to generate concise version. The second inference produces shortened reasoning trace by removing redundant or unnecessary reasoning steps while preserving the original tone, style, and logical progression. Prompt for Reasoning Trace Summarization Shorten the following reasoning trace by removing redundant or unnecessary thinking loops while preserving the exact same tone, style, and progression of thought. Output only the shortened reasoning trace without any explanation. {DeepSeek-R1 Reasoning Trace} Training Hyper-parameters For all of our experiments, we use 4 A800 80GB GPUs. We use LLaMA-Factory [55] to perform SFT for all R3 models. We set the maximum sequence length to 8192, with learning rate of 1e5, trained for 5 epochs using cosine learning rate scheduler. The batch size per device is 16. For R3 LoRA models, we use LoRA rank of 64 and alpha of 128. For inference, we use vLLM [16]."
        },
        {
            "title": "G Evaluation Prompt",
            "content": "Since RewardBench and FeedbackBench are of pair-wise and point-wise evaluation format, they do not require extra processing to format into our prompt template. For both MMLU-STEM and BBH, since we are converting them to binary evaluation, we need to sample negative responses to augment the dataset. The original MMLU-STEM consists of multiple-choice questions. We simply randomly sample wrong answer as the negative. For subtasks of BBH that are also in the format of multiple-choice questions, we do the same. There are four tasks that require custom adaptation for negative label sampling: DyckLanguages is task where models are tasked to complete un-closed parentheses of different types. To sample negatives, with equal chance, we randomly delete, swap, or insert symbol that appears in the context. WordSorting is task where models are tasked to sort set of unordered words. We randomly swap pair of words from the target order to create the negative. MultistepArithmeticTwo is task where models are expected to perform arithmetic calculations involving 8 single-digit operands. We calculate the mean and standard deviation of the label distribution, and randomly sample number within the distribution. ObjectCounting is task where models are expected to count the number of objects (possibly subset of all mentioned objects) mentioned in sentence. We calculate the mean and standard deviation of the label count distribution, and randomly sample number within the distribution."
        },
        {
            "title": "H Human Annotation Details",
            "content": "We stratified-sample 100 instances of data, and have the authors of the paper annotate the quality of the reasoning and reasoning summarizations. In total we have 5 annotators, annotating total of around 2% of the 4K dataset. H.1 Reliability of Reasoning Trace To ensure reasoning trace is reliable, we define two metrics Factual Correctness and Logical Coherence to ensure consistent labeling: Factual Correctness (Scale: 13) assesses whether the statements in the reasoning trace are true and supported by external knowledge or evidence. When scoring, treat retrievable evidence or commonsense facts as acceptable grounding. 1. (Incorrect) Contains one or more clear factual errors or hallucinations that undermine the trace. May lead to incorrect conclusions or mislead the model. 2. (Partially Correct) Most statements are accurate, but minor factual errors or unverifiable claims exist. Does not change the final conclusion, but may reduce trace reliability. 3. (Fully Correct) All statements are factually accurate and supported by known facts, context, or ground truth. No hallucinations or inaccuracies. Logical Coherence measures whether the reasoning steps logically follow from each other and form coherent argument or thought process. Judge based on internal consistency, not factuality. trace can be factually wrong but still logically coherent. 1. (Incoherent) Trace is illogical, disjointed, or internally inconsistent. Steps may contradict, skip crucial logic, or appear arbitrary. 2. (Somewhat Coherent) Mostly logical, but has minor gaps, unclear transitions, or weak justifications. Still understandable, but less robust as supervision. 24 3. (Fully Coherent) All steps follow logically and consistently. No missing steps, contradictions, or unjustified jumps in reasoning. smooth, interpretable chain. In Table 8 we show detailed annotation results across annotators. Table 8: Human annotation results on reasoning trace factual correctness and logical coherence (out of 3, higher better). Annotator 1 Annotator 2 Annotator 3 Annotator 4 Average"
        },
        {
            "title": "Factual Correctness\nLogical Coherence\nCount",
            "content": "3 0.2 2.9 0.4 27 3 0 2.6 0.7 10 2.9 0.3 2.9 0.3 28 2.8 0.5 2.7 0.5 28 2.9 0.2 2.8 0.2 - H.2 Reasoning Trace Summary Quality During dataset curation, we use GPT-4.1 mini to summarize the reasoning traces that are too long. We want to measure faithfulness and style similarity Faithfulness measures how well the summary covers the ideas of the original reasoning trace 1. (Unfaithful) Omits key reasoning or introduces incorrect logic. Could mislead model or change the original meaning. 2. (Partially Faithful) Minor omissions or slightly altered emphasis, but preserves the general logic and outcome. Acceptable for training. 3. (Fully Faithful) Captures all core and necessary reasoning steps accurately. No hallucinations, distortions, or omissions of crucial logic. Style Similarity includes similar tone, level of formality, structured markers (\"first\", \"therefore\"), or domain-specific phrasing. 1. (Completely different) Omits all tone, level of formality, etc. from original trace 2. (Somewhat similar style) Somewhat similar in terms of tone, level of formality, etc. from original trace 3. (Same style) Same style with the original reasoning trace In Table 9 we show detailed annotation results across annotators. Table 9: Human annotation results on reasoning trace summary faithfulness and style similarity (out of 3, higher better). Annotator 1 Annotator 2 Annotator 3 Annotator 4 Annotator 5 Average Faithfulness Style similarity Count 2.6 0.8 2.5 0.6 2.8 0.5 3 0.2 26 2.8 0.4 2.7 0.5 25 2.7 0.8 2.8 0.4 6 3 0 2.7 0.5 7 2.8 0.5 2.7 0.4 -"
        },
        {
            "title": "I Other results",
            "content": "I.1 RM-Bench & Reward Bench Additional results presented in Table 10 and Table 11 are derived from the findings reported in [6]. 25 Table 10: Comparison of existing models with R3 on RM-Bench. Bolded numbers indicate the best-performing results within each group section independently. Model Scalar RMs Domain Difficulty Chat Math Code Safety Easy Medium Hard Overall Avg. steerlm-70b tulu-v2.5-70b-preference-mix-rm Mistral-7B-instruct-Unified-Feedback RM-Mistral-7B Eurus-RM-7b internlm2-7b-reward Skywork-Reward-Gemma-2-27B ArmoRM-Llama3-8B-v0.1 GRM-llama3-8B-sftreg internlm2-20b-reward Llama-3-OffsetBias-RM-8B Nemotron-340B-Reward URM-Llama-3.1-8B Skywork-Reward-Llama-3.1-8B infly/INF-ORM-Llama3.1-70B Generative RMs 56.4 58.2 56.5 57.4 59.9 61.7 69.5 67.8 62.7 63.1 71.3 71.2 71.2 69.5 66.3 64.9 tulu-v2.5-dpo-13b-chatbot-arena-2023 56.3 tulu-v2.5-dpo-13b-nectar-60k 67.2 stablelm-2-12b-chat 66.4 tulu-v2.5-dpo-13b-stackexchange-60k 58.8 Nous-Hermes-2-Mistral-7B-DPO 62.5 Claude-3-5-sonnet-20240620 68.4 tulu-v2.5-dpo-13b-hh-rlhf-60k 66.4 tulu-2-dpo-13b 78.6 SOLAR-10.7B-Instruct-v1.0 64.3 Llama3.1-70B-Instruct 71.4 Skywork-Critic-Llama-3.1-70B 67.2 GPT-4o-0806 71.6 Gemini-1.5-pro 46.0 Prometheus-7B-v2.0 59.9 JudgeLRM 66.6 RM-R1-Qwen-Instruct-7B 64.0 RM-R1-DeepSeek-Distilled-Qwen-7B 75.6 RM-R1-Qwen-Instruct-14B RM-R1-Qwen-Instruct-32B 75.3 RM-R1-DeepSeek-Distilled-Qwen-14B 71.8 RM-R1-DeepSeek-Distilled-Qwen-32B 74.2 R3 Models (Ours) R3-QWEN3-4B-LORA-4K R3-QWEN3-4B-4K R3-QWEN3-4B-14K R3-QWEN3-8B-LORA-4K R3-QWEN3-8B-4K R3-QWEN3-8B-14K R3-QWEN3-14B-LORA-4K R3-QWEN3-14B-4K R3-QWEN3-14B-14K Proprietary Models GPT-4.1 mini GPT-o4 mini DeepSeek-R1 71.9 71.4 69.3 72.1 73.2 72.5 76.5 76.9 74.3 67.6 77.6 78.6 49.3 55.5 51.7 52.7 56.9 49.7 53.2 53.1 57.8 56.7 53.2 59.4 54.1 54.5 56.8 50.5 52.6 51.6 54.2 51.3 54.5 52.3 51.8 49.6 47.5 56.8 63.6 63.7 47.6 51.9 54.6 56.2 60.6 66.8 69.5 74.1 69.2 69.8 71.4 71.3 71.7 72.7 78.7 75.7 76. 71.3 80.8 81.9 53.0 51.4 58.0 57.0 60.2 71.4 54.7 57.5 62.5 66.8 61.9 59.8 61.8 60.6 65.6 52.3 52.4 54.9 49.9 55.6 62.6 51.1 51.4 52.3 67.3 64.6 67.5 73.9 52.6 59.9 67.0 83.9 75.4 80.2 90.5 91.8 90.6 90.3 91.0 91.0 90.8 91.3 92.4 91.3 92.6 73.0 93.0 66.2 51.2 87.1 86.8 87.2 86.5 85.5 91.9 92.4 90.0 86.5 89.6 87.5 93.1 95.7 94.8 62.3 73.8 65.2 69.0 73.9 64.4 76.5 85.4 78.9 83.0 94.8 91.7 91.3 73.9 87.3 92.6 85.3 93.6 93.9 94.1 95.4 88.5 88.6 90.5 90.0 90.8 91.8 91.9 91.3 91.2 90.7 93.4 88.7 48.3 72.8 87.1 88.6 87.2 85.4 78.0 82.2 83.5 82.6 84.6 81.0 84.0 89.0 91.8 82.8 86.7 69.1 79.5 69.5 73.8 53.6 86.9 57.5 74.7 85.6 83.4 83.1 68.8 73.2 79.2 75.9 82.6 86.3 86.2 89. 87.7 87.2 88.8 88.2 87.8 89.4 91.4 90.9 91.2 87.0 92.0 86.9 54.9 65.6 67.3 67.1 70.2 70.7 69.2 71.0 72.7 71.6 72.2 71.4 73.2 74.7 76.1 60.2 64.3 63.5 63.0 61.1 63.4 63.0 66.7 67.6 67.8 73.7 75.6 77.6 54.9 76.6 71.7 73.1 77.5 80.5 83.6 85.4 80.7 80.7 82.5 81.7 82.7 84.5 86.2 85.5 86.3 78.4 88.7 82. 54.3 50.7 35.3 34.9 40.2 45.1 54.9 49.8 48.6 50.7 50.2 56.1 53.0 46.6 44.8 29.5 25.4 46.6 37.2 49.1 45.9 69.6 37.7 69.4 54.1 56.5 58.7 64.7 41.3 54.8 59.7 68.1 68.8 70.4 74.4 76.7 71.8 72.1 70.4 73.4 74.4 72.3 77.1 75.6 73.8 61.7 78.0 67.3 52.5 63.0 63.2 63.5 65.9 67.1 67.3 67.7 68.2 68.3 69.0 69.5 70.0 70.1 70.9 57.5 58.8 59.7 59.9 59.9 61.0 62.1 63.8 64.8 65.5 71.9 72.5 75.2 55.0 64.7 70.2 72.4 76.1 79.1 81.5 83. 80.0 80.0 80.6 81.1 81.6 82.1 84.9 84.0 83.8 75.7 86.2 78.8 Table 11: Comparison of existing models with R3 on RewardBench using pair-wise scoring. Bolded numbers indicate the best-performing results within each group section independently."
        },
        {
            "title": "Chat Chat Hard",
            "content": "Safety Reasoning Avg."
        },
        {
            "title": "Scalar RMs",
            "content": "Eurus-RM-7b Internlm2-7b-reward SteerLM-RM 70B Cohere-0514 Internlm2-20b-reward ArmoRM-Llama3-8B-v0.1 Nemotrom-4-340B-Reward Skywork-Reward-Llama-3.1-8B Skywork-Reward-Gemma-2-27B infly/INF-ORM-Llama3.1-70B"
        },
        {
            "title": "Generative RMs",
            "content": "98.0 99.2 91.3 96.4 98.9 96.9 95.8 95.8 95.8 96.6 Llama3.1-8B-Instruct 85.5 97.2 Llama3.1-70B-Intruct 97.2 Llama3.1-405B-Intruct Claude-3-5-sonnet-20240620 96.4 GPT-4o-0806 96.1 Gemini-1.5-pro 92.3 Self-taught-evaluator-llama3.1-70B 96.9 SFR-LLaMa-3.1-70B-Judge-r 96.9 Skywork-Critic-Llama-3.1-70B 96.6 Prometheus-7B-v2.0 90.2 m-Prometheus-14B 93.6 JudgeLRM 92.9 SynRM 38.0 RM-R1-DeepSeek-Distilled-Qwen-7B 88.9 RM-R1-Qwen-Instruct-7B 94.1 93.6 RM-R1-Qwen-Instruct-14B RM-R1-DeepSeek-Distilled-Qwen-14B 91.3 R3 Models (Ours) R3-QWEN3-4B-LORA-4K R3-QWEN3-4B-4K R3-QWEN3-4B-14K R3-QWEN3-8B-LORA-4K R3-QWEN3-8B-4K R3-QWEN3-8B-14K R3-QWEN3-14B-LORA-4K R3-QWEN3-14B-4K R3-QWEN3-14B-14K"
        },
        {
            "title": "Propretiary Models",
            "content": "GPT-4.1 mini GPT-o4 mini DeepSeek-R1 93.0 91.9 92.7 93.6 91.1 93.3 93.0 92.7 93.6 96.1 95.3 93.6 27 65.6 69.5 80.3 71.3 76.5 76.8 87.1 87.3 91.4 91.0 48.5 70.2 74.6 74.0 76.1 80.6 85.1 84.8 87.9 45.6 59.0 56.4 82.5 66.2 74.6 80.5 79. 68.0 76.5 75.7 75.4 75.7 75.7 79.8 78.1 77.6 75.2 81.8 79.2 81.4 87.2 92.8 92.3 89.5 90.5 91.5 90.8 91.9 93.6 75.6 82.8 77.6 81.6 86.6 87.9 89.6 91.6 93.1 75.8 85.1 78.2 74.1 78.4 85.2 86.9 89.3 81.9 85.5 87.2 85.9 85.5 85.7 87.4 85.4 85.3 87.0 91.6 86. 86.3 94.5 90.6 97.7 95.8 97.3 93.6 96.2 96.1 99.1 72.1 86.0 87.1 84.7 88.1 92.0 88.4 97.6 95.5 74.6 84.8 73.6 87.1 87.0 86.7 92.0 95.5 92.9 94.0 94.5 94.9 93.8 95.3 97.1 96.2 96.3 89.6 98.4 97.4 82.8 87.6 88.8 89.4 90.2 90.4 92.0 92.5 93.8 95.1 70.4 84.0 84.1 84.2 86.7 88.2 90.0 92.7 93.3 71.6 80.6 75.2 70.4 80.1 85.2 88.2 88. 83.9 87.0 87.5 87.5 86.5 87.5 89.3 88.1 88.2 87.0 91.8 89."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "Boston University",
        "Capital One",
        "Columbia University",
        "Institut Teknologi Bandung",
        "Monash Indonesia",
        "Stanford University"
    ]
}