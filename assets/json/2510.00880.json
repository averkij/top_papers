{
    "paper_title": "HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation",
    "authors": [
        "Loris Bergeron",
        "Ioana Buhnila",
        "Jérôme François",
        "Radu State"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) excel in many NLP tasks but remain prone to hallucinations, limiting trust in real-world applications. We present HalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating hallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies document-claim pairs as grounded or hallucinated and produces evidence-grounded justifications for transparency. Our approach combines (i) a domain-agnostic synthetic dataset derived from FineWeb and refined through multi-stage curation and data reformation, (ii) synthetic grounded and hallucinated claims, and (iii) preference-based fine-tuning with Odds Ratio Preference Optimization to distill large-model reasoning into a smaller backbone. On the RAGTruth subset of the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy (BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian 3.3 (8B; 82.2%) while using roughly half their parameters. Over the full benchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as GPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon acceptance."
        },
        {
            "title": "Start",
            "content": "HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation Loris Bergeron1,4 1Banque de Luxembourg Ioana Buhnila2,3 2Center for Data Science in Humanities, Chosun University Jérôme François4 Radu State4 3ATILF, University of LorraineCNRS 4SnT, University of Luxembourg Correspondence: loris.bergeron@blu.bank 5 2 0 2 1 ] . [ 1 0 8 8 0 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) excel in many NLP tasks but remain prone to hallucinations, limiting trust in real-world applications. We present HalluGuard, 4B-parameter Small Reasoning Model (SRM) for mitigating hallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies documentclaim pairs as grounded or hallucinated and produces evidence-grounded justifications for transparency. Our approach combines (i) domain-agnostic synthetic dataset derived from FineWeb and refined through multi-stage curation and data reformation, (ii) synthetic grounded and hallucinated claims, and (iii) preference-based fine-tuning with Odds Ratio Preference Optimization to distill largemodel reasoning into smaller backbone. On the RAGTruth subset of the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy (BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian 3.3 (8B; 82.2%) while using roughly half their parameters. Over the full benchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as GPT4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon acceptance."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have been used for variety of Natural Language Processing (NLP) tasks, achieving strong results in summarization, text classification, and question answering (Tan et al., 2023; Singhal et al., 2023). However, recent research shows that Small Language Models (SLMs) (Schick and Schütze, 2021) can achieve competitive results in specific tasks, especially when fine-tuned on domain-specific data. In addition to being cost and energy efficient, SLMs are practical in resource-constrained settings (Lepagnol et al., 2024) such as on-premise environments, often required in the financial sector and industries with strict compliance requirements. 1 Figure 1: HalluGuard Concept. Given document and claim c, the model first thinks before classifying their relationship as grounded or hallucinated, and then produces justification citing relevant parts of x. However, major remaining challenge is that both LLMs and SLMs are prone to hallucinations, outputs inconsistent with the input prompt or factual knowledge (Zhang et al., 2025a; Huang et al., 2023), and are problematic in Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) applications, increasingly deployed in companies due to their ability to deliver context-aware responses. Even when using documents, RAGs remain vulnerable to hallucinations (Niu et al., 2024), undermining trust and explainability (Ni et al., 2025). To address this, models must be able to detect hallucinations, justify their outputs with evidence, and integrate into RAG applications. Recent work emphasizes models designed for reasoning, the ability to perform multi-step inference, follow logical chains, and provide transparent reasoning traces (Wei et al., 2022). Small Reasoning Models (SRMs) are not merely SLMs run with Chain-of-Thought (CoT) prompts. Rather, they are trained to produce structured intermediate reasoning that decomposes complex tasks before generating output, often through distillation from stronger reasoners and reward-guided training (Wang et al., 2025). This makes SRMs particularly well suited for mitigating hallucinations in RAG applications. Moreover, most of the previous work on hallucination detection uses BERT-based classifiers (Devlin et al., 2019). Although effective, these models do not provide justifications, making them unsuitable when explainability is mandatory. This challenge is especially pressing in business environments, where regulations require decisiontraceable justifications. In fact, to improve efficiency, companies, especially in finance, are deploying custom RAG solutions with team-specific knowledge (e.g., compliance, legal). Similar deployments are spreading to other industries where specialized knowledge is critical. In these settings, trust and explainability are essential. Users must see which passages of the retrieved document support or contradict the claim. To address this gap, we propose HalluGuard, an SRM for the mitigation of hallucinations in RAG. As shown in Figure 1, given document and claim c, HalluGuard first thinks about their relationship before predicting whether the claim is grounded or hallucinated, while generating an evidence-grounded justification, fostering the transparent and reliable use of RAG in companies. Our contributions are threefold: We introduce HalluGuard1, Small Reasoning Model (SRM) for hallucination mitigation in Retrieval-Augmented Generation (RAG). HalluGuard detects hallucinations and generates evidence-grounded justifications, making it transparent for human oversight. We will publicly release HalluGuard and the datasets used for fine-tuning upon acceptance. We construct HalluClaim2, large-scale synthetic dataset derived from FineWeb (Penedo et al., 2024) using Llama3.3-70B (Dubey et al., 2024). HalluClaim provides controlled yet diverse benchmark for training and evaluating hallucination detection in RAGlike scenarios and will also be released. We show that HalluGuard improves the balanced accuracy of its backbone and achieves competitive performance compared to larger open-source and closed LLMs. Our ablation study highlights the role of reasoning traces, consensus filtering, and Odds Ratio Preference Optimization (ORPO) (Hong et al., 2024) fine-tuning in driving these gains. 1https://anonymous.website 2https://anonymous.website"
        },
        {
            "title": "2 Related Work",
            "content": "Mitigating hallucinations in LLMs has been approached through prompt engineering, RetrievalAugmented Generation, decoding strategies, supervised fine-tuning, and self-reflection (Ji et al. 2023; Song et al. 2024; Tonmoy et al. 2024; Zhang et al. 2025b). Despite the extensive study of hallucinations in LLMs, there is no consensus on general classification, as the boundary between hallucination and factuality is often blurred (Wei et al. 2024; Mallen et al. 2023). To address this, Bang et al. (2025) proposed three-type taxonomy. Linked to our work, LYNX (Ravi et al., 2024) is an open-source hallucination evaluation model that outperforms GPT-4o and Claude-3 Sonnet, with 8B and 70B-parameter variants. In addition, IBMs Granite Guardian 3.3 (Padhi et al., 2024), an 8B model, detects hallucinations in RAG settings and provides yes/no scores with optional reasoning traces through hybrid thinking modes. Fact-checking has been studied beyond hallucination detection. Tang et al. (2024a) introduced MiniCheck, model trained on synthetic data matching GPT-4 on multi-fact reasoning benchmarks, while remaining more cost-effective. This line of research highlights the importance of lightweight and scalable models for this specific task. More recently, Pandit et al. (2025) presented HaluCheck, hallucination detection model trained with curriculum-based Direct Preference Optimization (DPO) (Rafailov et al., 2023) framework. HaluCheck was not publicly available at that time."
        },
        {
            "title": "3 Problem Formulation",
            "content": "We define the task as determining the relationship between document and claim c. The relation t(x, c) can take one of three values: t(x, c) = if is supported by grounded, intrinsic_hallu, if contradicts extrinsic_hallu, if is not in A claim is grounded if it is fully supported by the information explicitly present in x. It is an intrinsic hallucination if it directly contradicts x, and an extrinsic hallucination if its truth requires external knowledge beyond x. Concrete examples of these relationships can be found in Figure 2. For the remainder of this work, we group intrinsic and extrinsic hallucinations under single hallucinated label, reducing the task to binary classification (grounded vs. hallucinated). 2 Figure 2: Examples of Relations. grounded claim, an intrinsic hallucination, and an extrinsic hallucination."
        },
        {
            "title": "4.1 HalluGuard Overview",
            "content": "HalluGuard is Small Reasoning Model (SRM) designed to mitigate hallucinations in RetrievalAugmented Generation (RAG). Given documentclaim pair, it predicts whether the claim is grounded or hallucinated, and provides justification citing the document, improving transparency and user trust. HalluGuard supports two inference modes: in the think mode, it generates intermediate reasoning traces before the final output, while in non-think mode, it skips these traces and outputs directly. The mode is controlled at inference time by adding /think or /no_think to the prompt. As shown in Figure 3, our method begins with large, high-quality, domain-agnostic corpus that has been curated for safety, quality, and diversity. The texts in this corpus are then linguistically reformed in tone and style by the Data Reformer (DR; Llama-3.3-70B) to improve cross-domain generalization. From these reformed texts, we generate grounded and hallucinated synthetic claims using the Claim Generator (CG; Llama-3.3-70B). To align the model towards high-quality reasoning and justifications, we construct synthetic preference dataset. For each documentclaim pair, we generate two candidate completions: one from the Preference Generator-Large (PG-L; Qwen3-32B) and one from the Preference Generator-Small (PGS; Qwen3-0.6B (Yang et al., 2025)). We designate the output of the PG-L as the chosen response and the output of the PG-S as the rejected response. This creates preference pairs that exploit the empirical quality gap between large and small models, enabling us to build training dataset without the need for additional human annotation. To further improve reliability, we apply two filtering steps: (i) model-agreement verification, in which the label deduced from the synthetic claim (from CG) is compared with the classification produced by PGL; and (ii) LLM-based consensus filtering. In this step, two Independent Evaluators (IE-1; Llama3.3-70B and IE-2; Mistral Large 2 (Mistral AI team, 2024)) judge both completions. Only pairs in which both evaluators select the chosen completion are retained. Finally, we fine-tune Qwen3-4B backbone using LoRA (Hu et al., 2022) for efficiency and Odds Ratio Preference Optimization (ORPO), which merges Supervised Fine-Tuning (SFT) and preference alignment into single stage. Qwen3-4B was selected to avoid the Learnability Gap observed in SLMs (Li et al., 2025). Thus, HalluGuard is Small Reasoning Model that delivers reliable hallucination mitigation and interpretable justifications, ready for seamless integration into enterprise RAG applications."
        },
        {
            "title": "4.2 Structured Claim Dataset Construction",
            "content": "Domain-Agnostic Corpus. The performance of LLMs depends on both the size and the quality of the dataset (Gunasekar et al., 2023). Larger and more diverse datasets improve generalization by exposing models to varied contexts. We therefore use FineWeb (Penedo et al., 2024), large-scale, open-source, domain-agnostic web corpus. From the 10TB FineWeb sample3, we retain only documents with high confidence of being in English (language_score 0.95) and remove exact duplicates. From the remaining pool, we randomly sample 250,000 documents to form the baseline dataset, denoted Dagnostic. 3https://hf.co/datasets/HuggingFaceFW/fineweb 3 Figure 3: HalluGuard Training Pipeline. domain-agnostic corpus is filtered, reformed, and used to generate three types of synthetic claims (grounded, intrinsic hallucinated, and extrinsic hallucinated). Preference data are built via cross-model generation (Qwen3-32B and Qwen3-0.6B), model-agreement verification and LLM-based consensus filtering are used to enhance quality and confidence. The Qwen3-4B backbone is then fine-tuned using LoRA and ORPO to mitigate hallucinations and produce evidence-grounded justifications in RAG applications. Multi-Stage Dataset Curation. We further filter Dagnostic to ensure safety, quality and diversity, following practices similar to C4 (Raffel et al., 2020). Without this step, models risk learning unsafe, lowquality, or repetitive patterns. Specifically, we remove documents containing unsafe terms4, discard those that do not comply with C4-style quality rules (e.g., pages with fewer than five sentences, lines missing terminal punctuation, boilerplate such as Lorem Ipsum or cookie notices, and malformed text such as single token over 1000 characters). Finally, we remove documents shorter than 50 words and near-duplicates of any three consecutive sentences from documents that have already been retained. This deduplication step is important for promoting diversity: by eliminating redundant content, it reduces the repeated boilerplate and ensures that wider range of topics and writing styles are represented. The resulting dataset, denoted Dclean, contains 86,024 documents. Prompt-Guided Data Reformation. Despite multi-stage curation, Dclean remains web-centric in style due to the nature of FineWeb. To increase linguistic diversity and improve generalization to non-web formats (e.g., reports, dialogues), we use DR to rewrite each document, producing wider range of styles that better reflect real-world variation (Veselovsky et al., 2023; Long et al., 2024). The reformed dataset is then: (cid:0)x; (x)(cid:1) (cid:12) Dreformed = (cid:8) sj(x) (cid:12) Dclean (cid:9) (1) where j(x) is random style from = {s1, s2, . . . , s18} defined in Appendix B, and (x) the temperature sampled uniformly from [0.2, 0.7]. Synthetic Claim Generation. We generate one synthetic claim per document in Dreformed. To balance the binary classification task, we generate half grounded and half hallucinated claims, with the hallucinated split evenly into intrinsic and extrinsic, but both labeled as hallucinated. For each document xi Dreformed, we ask CG to generate claim ci in structured JSON format (He et al., 2024) (see Appendix C), and assign it label ti {grounded, hallucinated}. This results in 86,024 balanced documentclaimlabel triplets: HalluClaim = (cid:91) tC {(xi, ci, ti) xi Dt} (2)"
        },
        {
            "title": "4.3 Preference Training Dataset Construction",
            "content": "Reasoning-Guided Preference Pairs. The balanced dataset HalluClaim contains document claimlabel triplets. However, our goal is not only to classify claims correctly, but also to train models to produce evidence-grounded justification. We convert HalluClaim into the preference dataset format5, where each instance comprises 4https://github.com/LDNOOBW 5https://hf.co/docs/trl/dataset_formats 4 prompt and two completions: chosen completion and rejected one (see Appendix J). For each triplet of documentsclaimlabel, we construct prompt Pi containing: (i) task instructions defining the grounded and hallucinated labels, (ii) the document xi and (iii) the claim ci. The prompt requires classification and justification (see Appendix D). Thus, we used PG-L and PG-S, with the same prompt Pi. Each model {PG-L, PG-S} produces the response as follows: (cid:1) , j(m) , r(m) = (cid:0)y(m) R(m) is the predicted label (grounded or is the justification and r(m) where y(m) hallucinated), j(m) is the model reasoning within the <think> tags. (3) Assuming that larger models perform better, we as apply size-based heuristic, marking R(PG-L) chosen and R(PG-S) as rejected. For each triplet (xi, ci, ti) in HalluClaim, we produce preference tuples of the form: zi = (cid:0)Pi, R(PG-L) (chosen), R(PG-S) (rejected) (cid:1) (4) Model-Agreement Verification. The size-based heuristic provides useful starting point, but some chosen completions may still misclassify the claim. To correct this, we require agreement between the synthetic label assigned by CG during claim generation and the classification predicted by PG-L. Any tuple where the chosen label disagrees with the synthetic label is removed. After this verification, HalluClaimpref contains 83,020 tuples. LLM-Based Consensus Filtering. To further improve reliability, each tuple is independently evaluated by IE-1 and IE-2 in few-shot setting (Brown et al., 2020) using dedicated prompt that asks for the selection of the best completion according to three criteria: (i) classification correctness, (ii) coherence of reasoning, and (iii) clarity of justification (see Appendix E). The models receive the full prompt Pi and completions, without being told which one is the chosen completion. tuple is retained only if IE-1 and IE-2 select the same completion that matches the chosen one. IE-1(Pi) = IE-2(Pi) = R(chosen) (5) This LLM-based consensus step reduces label noise and mitigates size-based heuristic bias, thereby yielding total of 75,360 high-quality preference tuples for fine-tuning."
        },
        {
            "title": "4.4 Preference-Based Fine-Tuning",
            "content": "Pre-trained Model Backbone. After creating high-quality preference dataset through modelagreement verification and consensus filtering, we use Qwen3-4B (Yang et al., 2025) as backbone for fine-tuning. It supports context window of up to 32,768 tokens, which is important for document-level reasoning. The 4B variant remains lightweight enough for enterprise on-prem deployment. Using Qwen3-4B, we address the Small Model Learnability Gap (Li et al., 2025) observed in models with at most 3B parameters. Parameter-Efficient Fine-Tuning. We fine-tune Qwen3-4B using ORPO, fine-tuning technique increases the gap between chosen and that rejected completions so that the model consistently favors the chosen one (see Appendix G). Unlike DPO, ORPO performs an SFT stage during preference alignment, without relying on reference model. This makes training more efficient and allows HalluGuard to accurately classify claims while generating justifications and reasoning distilled from stronger models. To apply ORPO in parameter-efficient manner, we use LoRA (Hu et al., 2022), which freezes most base weights and trains only small adapter layers. This reduces memory and compute costs while mitigating catastrophic forgetting when adapting pre-trained models to specific tasks (Bafghi et al., 2025). Given the 32k token context window, finetuning is memory intensive. We therefore use Unsloth6, which accelerates fine-tuning with custom kernels, and memory optimizations, to enable faster, stable training. Reproducibility and finetuning details are in Appendices and I."
        },
        {
            "title": "5 Experimental Setup",
            "content": "Benchmark Dataset. We evaluate on LLMAggreFact (Tang et al., 2024a), collection of human-annotated datasets designed to assess whether model-generated claims are supported by evidence documents. The benchmark spans diverse domains and incorporates real hallucinations from recent LLMs, directly aligning with our task of detecting if claim is grounded or hallucinated. Importantly, it also includes RAGTruth (Niu et al., 2024), which is particularly relevant to our focus on hallucination mitigation in RAG (see Appendix F). 6https://unsloth.ai 5 Model Qwen3-32B MiniCheck-7B Claude-3.5 Sonnet Granite Guardian 3.3 Mistral-Large 2 gpt-4o-2024-05-13 HalluGuard-4B Qwen2.5-72B-Instruct Llama-3.1-70B-Instruct Claude-3 Opus Llama-3.3-70B-Instruct Llama-3.1-405B-Instruct gpt-4o-mini-2024-07-18 Qwen3-4B Llama-3-70B-Instruct Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Qwen3-0.6B Size 32B 7B - 8B 123B - 4B 72B 70B - 70B 405B - 4B 70B 8B 1B 0.6B AGGREFACT CNN XSum MediaS MeetB TofuEval WiCE REVEAL Claim Verify Fact Check Expert QA LFQA RAG Truth BAcc Avg. 69.1 65.5 67.6 67.0 64.8 68.1 61.1 63.6 65.7 65.2 68.7 64.8 61.8 64.9 63.7 54.7 50.1 20.5 76.3 77.8 75.1 74.9 74.7 76.8 73.1 73.0 72.5 72.4 74.7 75.1 73.6 73.8 70.2 68.5 50.9 43. 72.0 76.0 73.4 74.0 69.6 71.4 71.7 71.9 72.9 74.1 69.5 68.6 71.3 70.9 71.5 71.1 50.0 15.9 82.2 78.3 84.6 78.6 84.2 79.8 77.0 80.4 81.0 82.4 78.4 81.2 79.7 77.4 80.6 75.5 50.2 26.2 80.6 83.0 77.7 76.6 80.3 78.5 80.1 80.2 73.9 75.0 76.6 71.8 76.3 68.9 74.4 72.0 49.7 26.5 90.0 88.0 89.1 89.6 87.7 86.5 89.3 88.9 86.4 83.8 85.5 86.4 85.8 89.5 85.9 83.5 50.4 81.1 73.3 75.3 71.4 75.9 71.8 69.0 73.6 70.0 70.3 69.3 67.4 67.5 69.8 64.8 67.8 66.5 50.5 23.6 77.9 77.7 77.8 76.1 74.5 77.5 77.8 77.0 78.6 78.8 78.5 79.4 76.0 78.7 76.2 72.3 50.2 69. 60.2 59.2 60.9 59.6 60.8 59.6 60.0 60.1 58.5 58.8 58.3 58.5 58.3 57.5 57.8 57.8 49.9 38.5 85.5 86.7 85.6 86.9 87.0 83.6 85.1 84.3 83.8 81.6 79.8 81.9 80.3 81.5 82.4 77.5 50.1 25.5 85.9 84.0 86.1 82.2 85.9 84.3 84.0 81.9 83.0 81.8 82.6 82.9 81.6 83.7 80.6 73.6 50.9 14.9 77.6 77.4 77.2 76.5 76.5 75.9 75.7 75.6 75.1 74.8 74.5 74.4 74.0 73.8 73.7 70.3 50.3 35.0 Table 1: Evaluation on LLM-AggreFact. Models are ordered by average balanced accuracy (BAcc Avg.; higher is better). HalluGuard-4B (ours), Qwen3-0.6B, 4B and 32B were evaluated using our specific prompt in think mode. All other results are taken from the public leaderboard. The higher score between HalluGuard-4B and Qwen3-4B is shaded in dark green. Alternating grey rows improve readability. Models used within our training pipeline. (cid:16) TP TP+FN + TN Evaluation Metric. Performance is measured using balanced accuracy (BAcc) (Brodersen et al., (cid:17) 2010), defined as BAcc = 1 2 where TP, TN, FP, and FN denote true positives, true negatives, false positives, and false negatives. We adopted BAcc to ensure comparability with prior work, as it was also used in the paper that introduced LLM-AggreFact. TN+FP"
        },
        {
            "title": "6 Results",
            "content": "Evaluation on Benchmark. As shown in Table 1, HalluGuard-4B achieves an average BAcc of 75.7%, improving upon its backbone Qwen3-4B (73.8) by +1.9 points, with strong gains on WiCE (+11.2) and ClaimVerify (+8.8). These scores are obtained in think mode using specific prompt and inference parameters (see Appendices and H). HalluGuard-4B is competitive with larger general-purpose LLMs (e.g., GPT-4o (75.9), Claude-3 Opus (74.8), Llama-3.3-70B (74.5), and Mistral-Large 2 (76.5). Compared to specialized models, HalluGuard-4B is behind Granite Guardian 3.3 (76.5) and MiniCheck-7B (77.4). However, these baselines are larger (8B and 7B parameters). HalluGuard-4B trails Granite Guardian by only 0.8 points and MiniCheck-7B by 1.7, while surpassing them on some benchmarks. These results show that our fine-tuning pipeline transforms lightweight backbone into model that rivals both closed and open models, including general-purpose and specialized models, making HalluGuard-4B well suited for enterprise RAG applications where hallucination detection is crucial. 6 RAGTruth Detailed Evaluation. This subset focuses on RAG settings, evaluating whether claims are supported by retrieved documents. HalluGuard-4B achieves an average BAcc of 84.0% like MiniCheck-7B (84.0) and surpasses Granite Guardian 3.3 (82.2) despite using roughly half their parameters. It correctly classifies 13,649 grounded claims and detects 984 hallucinations, missing only 282 (see Table 2). This corresponds to True Positive Rate (TPR) of 77.7% and True Negative Rate (TNR) of 90.7%, showing that HalluGuard captures most hallucinations while preserving grounded content. This is essential for user trust. By combining high recall with evidencegrounded justifications, HalluGuard provides transparent decisions that users can verify, reinforcing its suitability in enterprise RAG applications."
        },
        {
            "title": "Predicted\nHallucinated Grounded",
            "content": "l Hallucinated c A"
        },
        {
            "title": "Grounded",
            "content": "984 1396 282 13649 Table 2: Confusion Matrix on the RAGTruth Dataset. Rows denote actual labels, columns denote predictions. The hallucinated label is treated as the positive class. Justification Evaluation. common baseline to evaluate generated text against reference is metrics such as ROUGE (Lin, 2004). However, these metrics are inadequate for our task, because they cannot assess whether justification is factually grounded in the source document and have been shown to correlate poorly with human judg80 60 40 ) % ( A a A 20 AGGREFACT-CNN AGGREFACT-XSum TofuEval-MediaS TofuEval-MeetB"
        },
        {
            "title": "ClaimVerify",
            "content": "FactCheck-GPT"
        },
        {
            "title": "RAGTruth",
            "content": "BAccAvg. Figure 4: Ablation of HalluGuard-4B. Comparison of the full model and three variants on LLM-AggreFact. Full w/o filter w/o reasoning SFT only ments (Wang et al., 2023). Thus, we adopt the G-Eval framework (Liu et al., 2023b), using GPT4o (OpenAI et al., 2024) as the evaluator. Concretely, for each RAGTruth document, we evaluate the justification of PG-L, HalluGuard-4B, and PGS. G-Eval assesses four dimensions: Relevance, Consistency, and Coherence (each on 15 scale), and Fluency (on 13 scale). As shown in Table 3, the quality of the justification presents significant disparities. Qwen3-32B achieves scores higher than Qwen3-0.6B in all dimensions. Importantly, HalluGuard-4B, although almost an order of magnitude smaller than Qwen332B, achieves comparable quality, indicating that ORPO effectively transfers strong model behavior to smaller backbone. In addition, fluency remains uniformly high, suggesting that the observed gains stem primarily from improved factual grounding and reasoning, rather than surface-level language quality. Finally, these results show that HalluGuard4B can match the quality of justification of 32B parameter model."
        },
        {
            "title": "Flu",
            "content": "Qwen3-32B 4.41 HalluGuard-4B 4.36 3.72 Qwen3-0.6B 4.29 4.27 3.65 4.47 4.51 3.58 2.98 2.97 2.75 Human Alignment Evaluation. We evaluated the alignment of our preference construction based on heuristics (Section 4.3) with human judgments. We sampled 100 preference tuples zi, balancing grounded and hallucinated claims. Each tuple was assessed by two independent NLP expert annotators using the same criteria as those used to construct the preference dataset: correct classification, coherence of reasoning, and clarity of justification. Annotators were asked to indicate which completion they preferred between the two options. Importantly, they were blind to the labels and did not know which completion had been designated as chosen or rejected during dataset construction. To avoid bias, the completions were presented in random order and without any indications. At the item level (75 pairs with full annotator agreement), chosen was preferred in 71 cases (94.7%) vs. 4 for rejected (p = 3.41017, binomial test vs. 50%, 95% CI [0.89, 1.00]). At the annotation level (considering all 200 individual judgments), 83.5% favored chosen (p = 4.7 1023, 95% CI [0.79, 1.00]) (see Table 4)."
        },
        {
            "title": "Evaluation level",
            "content": "Pref. for chosen Item level (n = 75) Annotation level (n = 200) 94.7% 83.5% Table 3: G-Eval Results. Evaluation of justification on RAGTruth using four dimensions: Relevance (Rel), Coherence (Coh), Consistency (Con), and Fluency (Flu). Table 4: Human Alignment Results. Annotators preferred the chosen completions (94.7% of the 75 fully agreed items; 83.5% of the 200 individual judgments). These results show that our heuristic is closely aligned with human preferences. The annotators clearly favored PG-L over PG-S, both at the item level (94.7%) and across all judgments (83.5%), confirming that our heuristic provides an effective proxy for human preference."
        },
        {
            "title": "7 Ablation Study",
            "content": "Impact of Consensus Filtering. Applying LLMbased consensus filtering using independent evaluators (IE-1 and IE-2) to preference tuples provides small but decisive improvement. With filtering, HalluGuard reaches 75.7% BAcc, compared to 75.3% without it (0.4%). Although the gain is In fact, without this commodest, it is crucial. ponent, HalluGuard falls behind Qwen2.5-72BInstruct (75.6%). Contribution of Reasoning. Disabling reasoning by using /no_think in the prompt leads to In think mode, Haldecrease in performance. luGuard reaches BAcc of 75.7%, whereas in non-think mode the BAcc decreases to 67.6% (8.1%). This represents the second largest drop in our ablation study, highlighting the critical role of reasoning in mitigating hallucinations. This is even more marked on RAGTruth, where reasoning improves BAcc (+21.8%), with consistent gains across all other datasets (see Figure 5). Figure 5: Effect of Model Reasoning. Radar plot comparing HalluGuard in think mode (lighter blue) vs. in /no_think mode (darker blue). Ablation Results. Figure 4 compares the full HalluGuard-4B model with three ablated variants on the benchmark datasets. The complete model consistently outperforms all variants, indicating that its robustness arises from the interaction of components rather than from any single factor. Consensus filtering yields modest but consistent improvement of +0.4% in BAcc, suggesting that pruning noisy preference pairs improves alignment. The second largest drop occurs when the reasoning traces are disable via /no_think in the prompt, with BAcc decreasing by 8.1% overall and reasoning providing particularly large gain of +21.8% on RAGTruth. Replacing ORPO with SFT alone further reduces performance by 27.6%, confirming the importance of preference alignment. Together, these results support the retention of the entire pipeline to fine-tune HalluGuard-4B."
        },
        {
            "title": "8 Conclusion",
            "content": "We presented HalluGuard, 4B-parameter Small Reasoning Model designed to mitigate hallucinations in Retrieval-Augmented Generation while providing evidence-grounded justifications. Built on domain-agnostic synthetic dataset with multi-stage curation and preference-based fine-tuning via ORPO and LoRA, we transform compact backbone into model that rivals or surpasses much larger LLMs, as well as recent specialized hallucination-detection models. In fact, HalluGuard-4B achieves competitive performance on LLM-AggreFact while providing justifications that are relevant, consistent, and comparable in quality to those of 32B-parameter model. Ablation studies also highlight the importance of reasoning traces, consensus filtering, and preference alignment in driving these gains. Thus, our findings demonstrate that carefully aligned small reasoning models can deliver both reliability and deployability for enterprise RAG applications, closing much of the gap with frontier LLMs. To foster research, we will release HalluGuard and datasets under Apache 2.0 upon acceptance."
        },
        {
            "title": "9 Future Work",
            "content": "Effect of Preference Alignment. Replacing ORPO with SFT alone results in the largest drop, with BAcc decreasing from 75.7% to 48.1% (27.6%). This indicates that preference alignment, as embedded in ORPO, plays crucial role in enhancing the reliability and quality of reasoning. In future work, we will (i) distinguish intrinsic and extrinsic hallucinations, and (ii) investigate multimodal extensions to support charts frequently present in enterprise documents. We will also release larger Qwen3-based variants (8B and 14B) to balance performance with deployment constraints."
        },
        {
            "title": "Limitations",
            "content": "Synthetic Data. Although multiple filters are applied, synthetic claims may not fully capture the nuances of hallucinations encountered in real-world RAG applications, since the training is based on synthetic data. Output Formatting. To ensure deployment realism, we enforce strict output structure: the response from HalluGuard-4B must be JSON object containing CLASSIFICATION and JUSTIFICATION keys only. Any deviation from this is scored as incorrect and can underestimate performance. Hallucination Coverage. The current model merges intrinsic and extrinsic hallucinations under single hallucinated label, which reduces explainability in settings where the distinction between different types of hallucination is important. Language and Domain Generalization. HalluGuard has been trained and evaluated on English data. Its performance in other languages or specialized domains remains uncertain."
        },
        {
            "title": "Ethical Considerations",
            "content": "As with any hallucination detection model, HalluGuard must be used with caution. Overflagging grounded claims may reduce user trust, while failing to detect hallucinations can lead to harmful errors further down the line. For this reason, HalluGuard should be used as decision support tool rather than as fully autonomous system, and should always be paired with human oversight. We therefore encourage responsible deployment in sensitive domains when integrating HalluGuard into real-world RAG applications."
        },
        {
            "title": "References",
            "content": "Reza Akbarian Bafghi, Carden Bagwell, Avinash Ravichandran, Ashish Shrivastava, and Maziar Raissi. 2025. Fine tuning without catastrophic forgetting via selective low rank adaptation. arXiv preprint arXiv:2501.15377. Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, and Pascale Fung. 2025. HalluLens: LLM hallucination benchmark. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 24128 24156, Vienna, Austria. Association for Computational Linguistics. Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim Buhmann. 2010. The balanced accuracy and its posterior distribution. In 2010 20th international conference on pattern recognition, pages 31213124. IEEE. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Hung-Ting Chen, Fangyuan Xu, Shane Arora, and Eunsol Choi. 2023. Understanding retrieval augmentation for long-form question answering. Preprint, arXiv:2310.12150. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv e-prints, pages arXiv2407. Suriya Gunasekar, Yi Zhang, and Jyoti Aneja. Preprint, Textbooks are all you need. 2023. arXiv:2306.11644. Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin Wang, and Sadid Hasan. 2024. Does prompt formatting have any impact on llm performance? arXiv preprint arXiv:2411.10541. Jiwoo Hong, Noah Lee, and James Thorne. 2024. ORPO: Monolithic preference optimization without reference model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1117011189, Miami, Florida, USA. Association for Computational Linguistics. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and 1 others. 2023. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232. Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or Honovich, Michael Tseng, Michael Collins, Roee Aharoni, and Mor Geva. 2024. chain-of-thought is as strong as its weakest link: benchmark for verifiers of reasoning chains. Preprint, arXiv:2402.00559. 9 Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards mitigating In Findings llm hallucination via self reflection. of the Association for Computational Linguistics: EMNLP 2023, pages 18271843. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 30253045, Mexico City, Mexico. Association for Computational Linguistics. Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023. WiCE: Real-world entailment for claims in Wikipedia. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 75617583, Singapore. Association for Computational Linguistics. Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700. Pierre Lepagnol, Thomas Gerald, Sahar Ghannay, Christophe Servan, and Sophie Rosset. 2024. Small language models are good too: An empirical study In Proceedings of the of zero-shot classification. 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1492314936. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and 1 others. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459 9474. Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. 2025. Small models In Findstruggle to learn from strong reasoners. ings of the Association for Computational Linguistics: ACL 2025, pages 2536625394, Vienna, Austria. Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Nelson Liu, Tianyi Zhang, and Percy Liang. 2023a. Evaluating verifiability in generative search engines. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 70017025, Singapore. Association for Computational Linguistics. Yang Liu, Dan Iter, and Yichong Xu. 2023b. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. arXiv preprint. ArXiv:2303.16634 [cs]. Lin Long, Rui Wang, and Ruixuan Xiao. 2024. On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: Survey. arXiv preprint. ArXiv:2406.15126 [cs]. Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2024. ExpertQA: Expert-curated questions and attributed anIn Proceedings of the 2024 Conference of swers. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98029822."
        },
        {
            "title": "Mistral AI",
            "content": "team. 2024. large 2. tral mistral-large-2407. Accessed: 2025-08-09. Large Enough: Mishttps://mistral.ai/news/ Bo Ni, Zheyuan Liu, Leyao Wang, Yongjia Lei, Yuying Zhao, Xueqi Cheng, Qingkai Zeng, Luna Dong, Yinglong Xia, Krishnaram Kenthapadi, and 1 others. 2025. Towards trustworthy retrieval augmented generation for large language models: survey. arXiv preprint arXiv:2502.06872. Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong Zhang. 2024. RAGTruth: hallucination corpus for developing trustworthy retrieval-augmented language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10862 10878, Bangkok, Thailand. Association for Computational Linguistics. OpenAI, Josh Achiam, Steven Adler, and Sandhini Agarwal. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Inkit Padhi, Manish Nagireddy, Giandomenico Cornacchia, Subhajit Chaudhury, Tejaswini Pedapati, Pierre Dognin, Keerthiram Murugesan, Erik Miehling, Martín Santillán Cooper, Kieran Fraser, and al. 2024. Granite guardian. Preprint, arXiv:2412.07724. Shrey Pandit, Ashwin Vinod, Liu Leqi, and Ying Ding. 2025. Teaching with lies: Curriculum dpo on synthetic negatives for hallucination detection. arXiv preprint arXiv:2505.17558. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, and 1 others. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, 10 Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. Selvan Sunitha Ravi, Bartosz Mielczarek, Anand Kannappan, Douwe Kiela, and Rebecca Qian. 2024. Lynx: An open source hallucination evaluation model. arXiv preprint arXiv:2407.08488. Timo Schick and Hinrich Schütze. 2021. Its not just size that matters: Small language models are also fewshot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 23392352. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, and 1 others. 2023. Large language models encode clinical knowledge. Nature, 620(7972):172180. Juntong Song, Xingguang Wang, Juno Zhu, Yuanhao Wu, Xuxin Cheng, Randy Zhong, and Cheng Niu. 2024. Rag-hat: hallucination-aware tuning pipeline for llm in retrieval-augmented generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 15481558. Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. 2023. Can chatgpt replace traditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family. In International Semantic Web Conference, pages 348367. Springer. Liyan Tang, Tanya Goyal, Alex Fabbri, Philippe Laban, Jiacheng Xu, Semih Yavuz, Wojciech Kryscinski, Justin Rousseau, and Greg Durrett. 2023. Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1162611644, Toronto, Canada. Association for Computational Linguistics. Liyan Tang, Philippe Laban, and Greg Durrett. 2024a. MiniCheck: Efficient fact-checking of LLMs on grounding documents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 88188847, Miami, Florida, USA. Association for Computational Linguistics. Liyan Tang, Igor Shalyminov, Amy Wong, Jon Burnsky, Jake Vincent, Yuan Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia Sun, Yi Zhang, Saab Mansour, and Kathleen McKeown. 2024b. TofuEval: Evaluating hallucinations of LLMs on topic-focused dialogue summarization. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 44554480, Mexico City, Mexico. Association for Computational Linguistics. SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313. Veniamin Veselovsky, Manoel Horta Ribeiro, and Akhil Arora. 2023. Generating Faithful Synthetic Data with Large Language Models: Case Study in Computational Social Science. arXiv preprint. ArXiv:2305.15041 [cs]. Chengyu Wang, Taolin Zhang, Richang Hong, and Jun Huang. 2025. short survey on small reasoning models: Training, inference, applications and research directions. arXiv preprint arXiv:2504.09100. Jiaan Wang, Yunlong Liang, and Fandong Meng. 2023. Is ChatGPT Good NLG Evaluator? Preliminary Study. arXiv preprint. ArXiv:2303.04048 [cs]. Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, and Preslav Nakov. 2024. Factcheck-bench: Fine-grained evaluation benchmark for automatic fact-checkers. Preprint, arXiv:2311.09000. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, and 1 others. 2024. Longform factuality in large language models. Advances in Neural Information Processing Systems, 37:80756 80827. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, and 1 others. 2025a. Sirens song in the ai ocean: survey on hallucination in large language models. Computational Linguistics, pages 146. Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng. 2025b. Llm hallucinations in practical code generation: Phenomena, mechanism, and mitigation. Proceedings of the ACM on Software Engineering, 2(ISSTA):481503."
        },
        {
            "title": "A Technical Reproducibility",
            "content": "C Prompt Template: Claim Generation To facilitate reproducibility and transparency, we report the hardware and software environment used for all experiments. HalluGuard was fine-tuned on single NVIDIA H100 PCIe GPU (80GB memory, TDP 350W) for 16 hours. Training consumed approximately 7.35 kWh of energy as calculated by the Machine Learning Impact Calculator (MLIC) (Lacoste et al., 2019). The experiments were carried out on Linux server running CUDA 12.4.1 and PyTorch 2.4.0. We used the default random seeds and PyTorch settings. Prompt Template: Style Reformation grounded { } \"instructions\": [ \"Generate claim that is factually accurate and fully grounded in the provided context.\", \"Ensure that the claim is explicitly supported by the context - do not introduce information that is not directly verifiable from the context.\", \"Only return the claim as the answer. Do not include any additional text, explanation, or formatting.\" ], \"context\": <text>, \"answer\": \"\" Style paraphrase summarize expand news_article blog_post report story dialogue letter social_media_post script interview Instruction Paraphrase the following text while retaining its original meaning. Provide concise summary of the following text. Expand on the following text by adding more details and context. Rewrite the following information as news article. Transform the following text into an engaging blog post. Convert the following information into formal report. Rewrite the following text as narrative story. Transform the following text into dialogue between two characters. Rewrite the following text as formal letter. Transform the following text into social media post. Transform the following text into script for short video or play. Rewrite the following text as an interview between an interviewer and an expert. product_description Transform the following text into review news_summary formalize_news meeting_summary meeting_dialogue product description. Rewrite the following text as review of product or service. Summarize the following article into concise news brief. Rewrite the following content in formal journalistic style. Rewrite the following text as if it were summary of team meeting. Rewrite the following content as conversation between multiple meeting participants. Table 5: Each style is applied to reform FineWeb raw data and increase stylistic diversity. hallucinated_intrinsic { } \"instructions\": [ \"Generate claim that contradicts the provided context.\", \"The claim should remain fluent and grammatically correct but should be identifiable as incorrect upon quick read.\", \"Only return the claim as the answer. Do not include any additional text, explanation, or formatting.\" ], \"context\": <text>, \"answer\": \"\" hallucinated_extrinsic { } \"instructions\": [ \"Generate claim that includes information that cannot be verified within the provided context.\", \"Ensure the claim is plausible but requires external knowledge to verify its accuracy.\", \"Only return the claim as the answer. Do not include any additional text, explanation, or formatting.\" ], \"context\": <text>, \"answer\": \"\" Prompt Template: Synthetic Pairs { \"instructions\": [ \"You will be given document and claim. Determine whether the claim is 'GROUNDED' or 'HALLUCINATED' based on the document.\", \"A 'GROUNDED' claim is factually accurate and fully supported by the information provided in the document. It should be directly verifiable from the document.\", 12 \"A 'HALLUCINATED' claim is either:\", - Intrinsically incorrect: It \" contradicts the information provided in the document, or\", \" - Extrinsically incorrect: It includes information that cannot be verified within the document and requires external knowledge to assess its accuracy.\", \"Return the classification as the answer (i.e., GROUNDED or HALLUCINATED). Include justification.\" evidence documents; LFQA (Chen et al., 2023), where LLM long-form answers conditioned on retrieved or random documents are labeled; and RAGTruth (Niu et al., 2024), retrieval-augmented generation benchmark where outputs grounded in retrieved passages are annotated."
        },
        {
            "title": "G Reward Gap Across Training Epochs",
            "content": "], \"document\": <document>, \"claim\": <claim>, \"answer\": { \"CLASSIFICATION\": \"\", \"JUSTIFICATION\": \"\" } } Prompt Template: Consensus Filter { } \"instructions\": [ \"You will be given document and claim, along with two responses (RESPONSE_A and RESPONSE_B).\", \"Determine which response is better based on classification correctness, thinking coherence and clarity, and justification quality.\", \"Return your answer as either 'RESPONSE_A' or 'RESPONSE_B', without any justification.\" ], \"examples\": <examples>, \"document\": <document>, \"claim\": <claim>, \"RESPONSE_A\": <response_a>, \"RESPONSE_B\": <response_b>, \"best_response\": \"\" 102 2 6 8 a 0 0.2 0. 0.6 0.8 1 epoch rewards/chosen rewards/rejected Figure 6: The gap between chosen and rejected responses increases over training, showing that the model progressively learns to prefer chosen examples while assigning lower rewards to rejected ones."
        },
        {
            "title": "Parameter",
            "content": "Non-Thinking"
        },
        {
            "title": "Thinking",
            "content": "temperature min_p top_p top_k 0.7 0.0 0.8 20 0.6 0.0 0.95 20 Table 6: Inference parameters used in our experiments, following the recommended Qwen settings for nonthinking and thinking modes. LLM-AggreFact includes the following datasets: AGGREFACT (Tang et al., 2023), factual consistency benchmark for summarization; TofuEval (Tang et al., 2024b), dialogue summarization benchmark with LLM summaries annotated for factual consistency; WiCE (Kamoi et al., 2023), textual entailment dataset of Wikipedia claims and cited sources; REVEAL (Jacovi et al., 2024), which evaluates reasoning chains in open-domain QA with sentence-level attribution labels against retrieved Wikipedia passages; ClaimVerify (Liu et al., 2023a), which assesses generative search engine responses by verifying check-worthy sentences against cited documents with binary factuality labels; FactCheck-GPT (Wang et al., 2024), which decomposes LLM responses to search queries into atomic facts; ExpertQA (Malaviya et al., 2024), consisting of expert-curated queries across 32 domains where system responses are verified against { \"role\": \"assistant\", \"content\": \"<think>Okay, let's tackle this. The user wants to determine if the claim about St. (truncated) ... Therefore, the claim is GROUNDED</think> Blasius Church... { \"CLASSIFICATION\": \"GROUNDED\", \"JUSTIFICATION\": \"The claim that 'St. Blasius Church is over 850 years old and was built during the reign of King Stephen' is directly supported by the document... (truncated) ...\" }\" } ], \"rejected\": [ { \"role\": \"assistant\", \"content\": \"<think>Okay, let's tackle this. The user wants me to determine if the claim... (truncated) ... Since the document contradicts the claim, the claim is intrinsically hallucinated.</think> { \"CLASSIFICATION\": \"HALLUCINATED\", \"JUSTIFICATION\": \"The document explicitly states: 'The church itself was over 850 years old, built during the reign of King Stephen ... (truncated) ...'\" }\" } Fine-Tuning Configuration"
        },
        {
            "title": "Value",
            "content": "lora_layers_attn lora_layers_ffn lora_rank lora_alpha lora_dropout precision epochs batch_size grad_accumulation effective_batch_size optimizer learning_rate lr_schedule orpo_beta max_seq_len k_proj, q_proj, v_proj gate_proj, up_proj, down_proj 16 16 0 bfloat16 1 2 4 8 AdamW (8-bit) 1 106 linear 0.1 32768 Table 7: The setup trains 33M parameters (0.81% of the full model) using LoRA for 1 epoch. ORPO Preference Tuple: Full Example ] } { \"prompt\": \"{ \"instructions\": [ \"You will be given document and claim. Determine whether the claim is 'GROUNDED' or 'HALLUCINATED' based on the document.\", \"A 'GROUNDED' claim is factually accurate and fully supported by the information provided in the document. It should be directly verifiable from the document.\", \"A 'HALLUCINATED' claim is either:\", \" - Intrinsically incorrect: It contradicts the information provided in the document, or\", \" - Extrinsically incorrect: It includes information that cannot be verified within the document and requires external knowledge to assess its accuracy.\", \"Return the classification as the answer (i.e., GROUNDED or HALLUCINATED). Include justification.\" ], \"document\": \"'As stepped into the charming St. Blasius parish... (truncated) ...'\", \"claim\": \"'St. Blasius Church is over 850 years old and was built during... (truncated) ...'\", \"answer\": { \"CLASSIFICATION\": \"\", \"JUSTIFICATION\": \"\" } }\", \"chosen\": ["
        }
    ],
    "affiliations": [
        "ATILF, University of LorraineCNRS",
        "Banque de Luxembourg",
        "Center for Data Science in Humanities, Chosun University",
        "SnT, University of Luxembourg"
    ]
}