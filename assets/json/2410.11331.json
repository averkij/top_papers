{
    "paper_title": "SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments",
    "authors": [
        "Syed Abdul Gaffar Shakhadri",
        "Kruthika KR",
        "Rakshit Aralimatti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Shakti, a 2.5 billion parameter language model specifically optimized for resource-constrained environments such as edge devices, including smartphones, wearables, and IoT systems. Shakti combines high-performance NLP with optimized efficiency and precision, making it ideal for real-time AI applications where computational resources and memory are limited. With support for vernacular languages and domain-specific tasks, Shakti excels in industries such as healthcare, finance, and customer service. Benchmark evaluations demonstrate that Shakti performs competitively against larger models while maintaining low latency and on-device efficiency, positioning it as a leading solution for edge AI."
        },
        {
            "title": "Start",
            "content": "SHAKTI: 2.5 BILLION PARAMETER SMALL LANGUAGE MODEL OPTIMIZED FOR EDGE AI AND LOW-RESOURCE ENVIRONMENTS 4 2 0 2 5 1 ] . [ 1 1 3 3 1 1 . 0 1 4 2 : r Syed Abdul Gaffar Shakhadri Lead AI Developer SandLogic Technologies Pvt Ltd. syed.abdul@sandlogic.com Dr. Kruthika KR AI Researcher SandLogic Technologies Pvt Ltd kruthika.kr@sandlogic.com Rakshit Aralimatti AI Developer SandLogic Technologies Pvt Ltd rakshit.aralimatti@sandlogic.com"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Shakti, 2.5 billion parameter language model specifically optimized for resourceconstrained environments such as edge devices, including smartphones, wearables, and IoT systems. Shakti combines high-performance NLP with optimized efficiency and precision, making it ideal for real-time AI applications where computational resources and memory are limited. With support for vernacular languages and domain-specific tasks, Shakti excels in industries such as healthcare, finance, and customer service. Benchmark evaluations demonstrate that Shakti performs competitively against larger models while maintaining low latency and on-device efficiency, positioning it as leading solution for edge AI. Keywords Shakti Small Language Model Multilingual Support Domain Specific Task Performance Optimization"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs), such as GPT-3 [1] and LLaMA [2], have made substantial strides in the field of Natural Language Processing (NLP), delivering state-of-the-art performance across tasks like text summarization, machine translation, and question answering. However, their considerable computational and memory requirements render them impractical for deployment on edge devices such as smartphones, wearables, and Internet of Things (IoT) devices, where low-latency and energy efficiency are critical for real-time applications. The scaling laws that govern LLM performance suggest that increasing model size and dataset volume leads to better results [3]. However, the computational complexity and resource demands associated with larger models present significant challenge for real-time deployment on devices with limited hardware. Industries such as healthcare, finance, and customer service require domain-specific insights with minimal latency, which current LLM architectures struggle to provide due to their reliance on cloud infrastructure and specialized hardware. To address these challenges, Shakti was developed as solution that balances high performance, efficiency, and scalability, making it well-suited for resource-constrained environments. Shakti combines several technical innovations to enhance its efficiency and performance on edge devices. One of Shaktis core innovations is the introduction of Variable Grouped Query Attention (VGQA). VGQA groups multiple queries per key during attention computations, significantly reducing the memory footprint and accelerating inference times. Inspired by models like Mistral and Phi-3 [4, 5], this mechanism ensures that Shakti operates efficiently in low-latency, real-time environments, making it ideal for tasks where speed and resource efficiency are paramount. In addition to VGQA, Shakti incorporates pre-normalization and SwiGLU activations, which improve the training process by stabilizing gradient flows and preventing issues like vanishing or exploding gradients. Compared to traditional activation functions like ReLU, SwiGLU provides more consistent training results and ensures efficient gradient flow, particularly in resource-constrained environments [6]. To handle long text sequences without increasing computational overhead, Shakti integrates Rotary Positional Embeddings (RoPE) [7]. RoPE enhances the models ability to process longer sequences efficiently, making it suitable for tasks such as document summarization and complex queries, all while maintaining low memory usage. Moreover, Shaktis versatility extends to its ability to handle domain-specific tasks through fine-tuning on datasets enriched with vernacular languages. This fine-tuning enables the model to perform exceptionally well in multilingual environments, particularly in regions where low-resource languages such as Hindi, Kannada, and Telugu dominate. In contrast to global models that often struggle in these markets, Shakti offers robust performance across both multilingual and domain-specific contexts. These innovations position Shakti as highly efficient and scalable solution for on-device AI. By delivering high performance while optimizing for energy efficiency and low-latency applications, Shakti addresses the growing demand for real-time AI across industries that require localized AI solutions and low-resource deployments. Its ability to balance these demands ensures that it remains competitive against larger models in real-world AI applications, particularly in resource-constrained environments."
        },
        {
            "title": "2 Related Work: Transformer Architectures, Small Language Models, and On-Device AI",
            "content": "The Transformer architecture, introduced by Vaswani et al. [8], revolutionized Natural Language Processing (NLP) by leveraging the self-attention mechanism, which allowed for parallel computation and greater scalability. This innovation paved the way for Large Language Models (LLMs) to achieve state-of-the-art performance across tasks such as text generation, translation, and question answering. However, the computational and memory requirements of these models pose challenges for deployment on resource-constrained devices such as smartphones and IoT devices. 2.1 Evolution of Transformer Architectures The original Transformer model [8] replaced traditional sequence models like LSTMs and GRUs with multi-head self-attention mechanism, enabling faster and more accurate training on large datasets. Since then, models like BERT [9], GPT-3 [1], and T5 [10] have expanded the size and scale of these architectures by leveraging massive datasets and computational resources to achieve breakthroughs in language understanding and text generation. Scaling models to billions of parameters, however, makes them impractical for use in low-resource environments. For instance, LLaMA [2] introduced several optimizations such as pre-normalization and Rotary Positional Embeddings (RoPE) [7], significantly reducing memory usage while maintaining competitive performance. These innovations set new standard for balancing performance and efficiency in LLMs. Further advancements came with models like Mistral 7B [4] and Phi-3 Mini [5], which introduced techniques such as Grouped Query Attention (GQA) and sliding window attention, enhancing inference efficiency by reducing redundant computations. These models illustrate efforts to optimize Transformer architectures for real-time applications on devices with limited memory and processing power. 2. Small Language Models (SLMs) The rise of Small Language Models (SLMs) has made it possible to deploy AI efficiently on resource-constrained devices. DistilBERT [11], for example, uses knowledge distillation, which transfers knowledge from larger teacher model to smaller student model, retaining much of the performance while significantly reducing the number of parameters. Other models like TinyBERT [12] and MobileBERT [13] have adopted similar techniques, cutting computational costs by over 40 In addition to knowledge distillation, techniques like model pruning and quantization have also been applied to optimize models for on-device deployment. Model pruning removes parameters that minimally impact performance, while quantization reduces the precision of weights and activations, thereby lowering memory usage and computation time [14]. These techniques have been used in models like MobileBERT [13] and EdgeBERT [15], making them more suitable for mobile and IoT devices. 2 2.3 Advances in On-Device AI As the need for on-device AI grows, deploying models on edge devices such as smartphones and IoT systems has gained importance due to the demand for real-time inference and the need for improved data privacy. Models designed for edge devices must balance accuracy, speed, memory efficiency, and energy consumption to be effective in resourceconstrained environments. Models such as EdgeBERT [15] and Edge Transformers [16] have introduced lightweight attention mechanisms that reduce memory requirements while maintaining high performance. Techniques like block-wise memory management [14] and sliding window attention allow these models to process sequences efficiently without sacrificing accuracy. Additionally, quantization enables these models to run with 8-bit precision or lower, significantly reducing computational load and making them well-suited for low-power devices. In this context, Shaktis architecture builds on these advances by integrating key techniques from LLaMA and Mistral, while introducing innovations such as Variable Grouped Query Attention (VGQA) and SwiGLU activations, allowing it to deliver real-time performance on edge devices without extensive hardware. This makes Shakti an ideal solution for on-device AI applications, where low-latency and efficiency are critical."
        },
        {
            "title": "3 Architecture of Shakti-LLM",
            "content": "The architecture of Shakti-LLM refer Table 1 is optimized for resource-constrained environments such as edge devices, including smartphones, wearables, and IoT systems. With 2.5 billion parameters and context length of 4096 tokens, Shakti is designed for high-performance NLP with focus on real-time applications. One of the core innovations in Shakti-LLM is the use of Variable Grouped Query Attention (VGQA), inspired by models such as Mistral 7B [4] and Phi-3 Mini [5]. VGQA allows multiple queries to share single key during the attention process, significantly reducing the memory footprint while improving inference times. This makes Shakti highly suitable for low-latency applications, such as virtual assistants and smart home devices. Shakti also employs Pre-Normalization and SwiGLU activations to stabilize the training process. By normalizing the input before it is passed to the attention mechanism, Pre-Normalization prevents vanishing or exploding gradients, while SwiGLU activation functions enhance gradient flow, resulting in more efficient training [17]. These methods provide significant improvements over traditional activation functions like ReLU . To handle long sequences efficiently, Shakti incorporates Rotary Positional Embeddings (RoPE) [7]. RoPE enables Shakti to process long text contextssuch as document summarization and multi-turn dialogue processingwithout significantly increasing memory usage [7]. This makes Shakti particularly effective in tasks that require the handling of long sequences while maintaining small computational footprint. Lastly, Direct Preference Optimization (DPO) is used to fine-tune Shakti based on ranked human feedback [17]. Unlike Reinforcement Learning from Human Feedback (RLHF), which requires reward model, DPO simplifies the optimization process by directly learning from human preferences through log-sigmoid loss function [17, 18]. This ensures that Shakti generates user-aligned outputs efficiently, making it ideal for real-time AI applications like customer support and healthcare [19]. Shakti-LLM is designed to be scalable, efficient, and highly adaptable for real-world use cases in industries such as healthcare, finance, and customer service, where low-latency and real-time performance are essential. Shakti supports sliding window attention and Key-Value Caching, ensuring efficient processing of long sequences during inference. These optimizations make Shakti suitable for edge computing environments, where memory efficiency and real-time processing are essential."
        },
        {
            "title": "4 Training and Fine-Tuning Methodologies",
            "content": "Shakti-LLMs training and fine-tuning processes are designed to optimize its performance for both general NLP tasks and domain-specific applications. This section provides an in-depth look at the methodologies employed to enhance the models capabilities. 3 Features Shakti-LLM Specification Model Parameters Layers Model Dimension FFN Dimension Attention Heads Key/Value Heads Peak Learning Rate Activation Function Vocabulary Size Positional Embeddings GPU Consumption (Raw) GPU Consumption (Quantized)"
        },
        {
            "title": "2.5 Billion\n16\n4096\n4096\n32\n8\n3.6e-5\nSwiGLU\n128256\nRoPE (Î¸ = 500,000)\n9 GB\n4 GB",
            "content": "Table 1: Specifications of Shakti-LLM 4.1 Continued Pretraining (CPT) The first stage of Shakti-LLMs training involves Continued Pretraining (CPT), where the model is exposed to largescale datasets to capture general language structures and patterns. The model is initialized with random weights and tasked with predicting the next token in sequence, which is standard approach for language model pretraining. Shakti-LLMs training corpus includes approximately 2.8 trillion tokens, sourced from high-quality datasets, including: English Common Crawl: large corpus of web text processed using CCNet, which filters for high-quality content and removes duplicates [20]. C4: publicly available dataset, rigorously preprocessed, including language identification and the removal of low-quality pages [10]. Wikipedia: structured and reliable source of general knowledge, preprocessed to eliminate non-text elements [21]. Sangraha: custom dataset designed to support vernacular languages such as Hindi, Kannada, and Telugu, enhancing Shakti-LLMs performance in multilingual settings [22]. CulturaX: dataset that emphasizes cultural diversity, particularly useful for context-aware applications [23]. During this phase, Shakti-LLM is trained with learning rate of 2.0 104 and maximum sequence length of 4096 tokens. The gradient accumulation steps are set to 1, with warmup ratio of 0.1 to ensure smooth convergence. These hyperparameters are carefully selected to balance training speed with the models ability to capture complex linguistic patterns across the large-scale datasets [20]. 4.2 Supervised Fine-Tuning (SFT) After pretraining, Shakti-LLM undergoes Supervised Fine-Tuning (SFT) to adapt to specific, task-oriented datasets. This phase exposes the model to labeled examples from wide range of applications, improving its ability to handle domain-specific tasks and provide contextually relevant responses. The fine-tuning process employs learning rate of 2.0 105 with cosine decay learning rate scheduler, which adaptively reduces the learning rate as training progresses. The maximum sequence length remains at 4096 tokens, and the gradient accumulation steps are set to 1. These hyperparameters are optimized to allow for fine-grained adjustments, ensuring that Shakti-LLM excels in tasks requiring domain-specific knowledge while maintaining generalization capabilities. Key datasets such as Ultrachat 200K and Cosmedia V2 are leveraged during SFT to enhance Shakti-LLMs conversational abilities and its capacity to understand complex domains like healthcare and finance. The fine-tuning stage enables the model to follow specific instructions more effectively and handle real-world user prompts, similar to approaches seen in InstructGPT. 4.3 Direct Preference Optimization (DPO) In the final stage of training, Direct Preference Optimization (DPO) is employed to align Shakti-LLMs outputs with human preferences, ensuring that its responses are contextually and ethically aligned. Unlike Reinforcement Learning from Human Feedback (RLHF), which relies on reward model, DPO simplifies the optimization process by directly learning from ranked human feedback through log-sigmoid loss function [17]. The DPO process involves presenting human annotators with multiple model outputs for the same input. These outputs are ranked based on relevance, clarity, and appropriateness, and preference-based loss function is used to adjust the models weights accordingly. This ensures that Shakti-LLM generates outputs that are not only accurate but also aligned with ethical considerations and user expectations [19]. For DPO, the model is fine-tuned using learning rate of 5.0 107, with beta coefficient of 0.01 to manage optimization momentum. The maximum prompt length is set to 1024 tokens, and the model uses AdamW as the optimizer, with gradient accumulation steps set to 2. This combination of hyperparameters ensures the model is fine-tuned efficiently while retaining high-quality responses [17]. 4.4 Data Quality and Augmentation Throughout its training process, Shakti-LLM emphasizes the importance of data quality over sheer volume. While some models rely on synthetic data augmentation to expand training datasets, Shakti-LLM primarily focuses on human-labeled datasets and high-quality real-world data, reducing the introduction of noise into the training process [17]. This approach ensures that the model learns meaningful patterns while maintaining computational efficiency. Shakti-LLMs reliance on high-quality data, combined with its carefully designed training methodologies, allows the model to excel in real-world use cases, providing contextual relevance and high performance across general and domain-specific applications. By adhering to these rigorous principles in training and fine-tuning, Shakti-LLM achieves balance between performance and efficiency, making it ideal for deployment on edge devices and in low-resource environments."
        },
        {
            "title": "5 Benchmark Comparisons",
            "content": "To evaluate the performance of Shakti-LLM, we compared it against larger models, such as Mistral 7B [4], Phi-3 Mini-4k [5], and Llama 3 8B [2], using widely recognized NLP benchmarks. These benchmarks assess various tasks, including massive multitask language understanding, commonsense reasoning, and factual knowledge retrieval. Despite Shakti-LLMs smaller parameter size of 2.5 billion, it achieves competitive results, even outperforming larger models in specific categories. 5.1 Popular Benchmarks and Results The Table 2 summarizes the performance of Shakti-LLM compared to other models across key NLP benchmarks: 5.2 Key Observations Massive Multitask Language Understanding (MMLU): Shakti-LLM achieved score of 71.7 %, outperforming Phi-3 Mini-4k[5] and Gemma 7B[24]. This demonstrates Shakti-LLMs strong generalization ability across diverse domains, despite its smaller size. PIQA: Shakti-LLM scored 86.2% in the Physical Interaction QA (PIQA) task, surpassing Phi-3 Mini[5] and Mistral 7B[4]. This indicates the effectiveness of Shakti-LLMs attention mechanisms and fine-tuning strategies in handling commonsense reasoning tasks. BigBenchHard (BBH): Shakti-LLMs 58.2% score is competitive but lags behind Phi-3 Mini and Mistral 7B in this challenging benchmark, which tests more complex reasoning tasks. Further domain-specific fine-tuning could help close this performance gap [5]. Factual Knowledge Retrieval: Shakti-LLM shows room for improvement in factual knowledge tasks like Bool and Trivia QA, where larger models such as Mistral 7B[4] and Llama 3 8B[2] perform better. This suggests that while Shakti-LLM excels in reasoning tasks, it could benefit from additional pretraining or fine-tuning on factual datasets . 5 Category Benchmark Shakti-LLM (2.5B) Phi-3 Mini-4k [5] Gemma 7B [24] Mistral 7B [4] Mistral 8x7B[4] Llama 3 8B[2] Massive Multitask Language Understanding (MMLU) Commonsense Reasoning Language Understanding Reasoning Medical Knowledge Social Understanding Truthful QA Factual Knowledge Trivia QA MMLU (5-shot) BigBenchHard (0-shot) Hellaswag (5-shot) PIQA (5-shot) MedQA (2-shot) Social QA (5-shot) Truthful QA (10-shot) Bool (0-shot) Trivia QA (5-shot) 71.7% 58.2% 52.4% 86.2% 60.3% 79.2% 68.4% 61.1% 58.2% 68.8% 63.6% 61.7% 70.5% 66.5% 71.7% 76.7% 84.2% 53.8% 76.6% 65.0% 77.6% 64.0% 59.6% 49.8% 78.1% 49.6% 65.5% 52.1% 66.0% 72.3% 51.5% 69.7% 57.3% 71.1% 70.4% 58.5% 86.0% 75.7% 77.7% 62.2% 60.5% 50.0% 73.9% 75.9% 74.6% 63.1% 60.1% 53.0% 80.9% 72.2% 76.6% 75.2% 82.2% 67.7% Table 2: Benchmark Comparison of Various Models. Bolded values indicate the highest scores, and underlined values indicate the second highest. 5.3 Insights and Interpretations The benchmark results show that Shakti-LLM provides competitive performance across broad range of tasks, particularly in commonsense reasoning and multitask language understanding. Shakti-LLMs efficient architecture, particularly innovations like Variable Grouped Query Attention (VGQA) and Sliding Window Attention, allows it to handle diverse tasks without the large memory footprint of models like Mistral 7B[4] or Llama 3 8B [2]. However, the models relatively modest performance in factual knowledge retrieval (as shown in Bool and Trivia QA) highlights potential area for improvement. Future work could involve further fine-tuning on knowledge-heavy datasets or incorporating broader range of factual data during pretraining to enhance the models ability to recall and generate fact-based knowledge [17]. 5.4 Prompt-Based Comparative Evaluation In this section, we compare Shaktis performance to Phi-3[5] across several prompts. Table 3 showcases their respective responses to variety of tasks, such as question answering, creative writing, and travel suggestions. The comparative analysis of Shakti and Phi-3[5] demonstrates Shaktis ability to maintain contextual accuracy and provide detailed responses across various real-world scenarios, including travel itineraries and multilingual tasks. Shaktis creative outputs, such as poetry, exhibit linguistic richness and deeper engagement with the language. Furthermore, Shaktis fine-tuning for low-resource languages, such as Hindi, enhances its capability to generate culturally relevant and accurate content. These attributes reflect its ability to handle both creative and factual tasks effectively. 6 7 Phi-3.1-mini-4k-instruct Q5 KM [5] Phi-3.1-mini-4k-instruct Q4 KM [5] 5.5 Model Inference and Performance Efficiency Shakti-LLMs inference performance was evaluated alongside Phi-3.1-mini-4k[5] across multiple hardware configurations, focusing on generating 512 tokens per prompt refer Table 3. The hardware environments included VM configured with an AMD EPYC 7R13 processor, 30 GB RAM, NVIDIA L40s GPU, and 4 cores, as well as an Apple M3 Max with 36 GB RAM"
        },
        {
            "title": "Size",
            "content": "(tokens/sec) (tokens/sec) (tokens/sec) Shakti Q4 KM Shakti Q5 KM Q4 KM Q5 KM"
        },
        {
            "title": "1.5 GB",
            "content": "331."
        },
        {
            "title": "1.71 GB 305.89",
            "content": "18.93 15.90 Q5 KM"
        },
        {
            "title": "2.82 GB 163.17",
            "content": "8.44 128 110 74 Q4 KM"
        },
        {
            "title": "2.39 GB 180.4",
            "content": "10.72 88.21 Table 3: Performance comparison of different quantized language models across various hardware platforms. The table shows model names, quantization types, model sizes, and inference speeds (in tokens per second) on GPU, CPU, and Mac systems The Shakti Q4 KM model demonstrated higher token generation speeds across all hardware, particularly excelling on GPU and Mac configurations. Despite its smaller size, it outperformed Phi-3.1 models [5], underscoring Shaktis optimized efficiency for real-time tasks on edge devices."
        },
        {
            "title": "6 Applications and Future Directions",
            "content": "Shakti-LLM is designed with versatility and scalability in mind, making it suitable for wide range of real-world applications. Its lightweight architecture, optimized for on-device performance, positions it as an efficient solution in industries requiring low-latency, on-device AI such as healthcare, finance, and customer service. 6.1 On-Device AI for Mobile and IoT key advantage of Shakti-LLM is its ability to operate efficiently on small devices, including smartphones, wearables, and Internet of Things (IoT) devices. Its compact size and innovative attention mechanisms make it ideal for real-time applications where latency and power consumption are critical constraints. Smartphones and Wearables: Shakti-LLM can power applications such as real-time translation, virtual assistants, and language-based health monitoring. Its low memory footprint ensures minimal impact on device performance, making it ideal for consumer-grade hardware. IoT Devices: Shakti-LLM can be deployed in smart home systems, industrial automation, and environmental monitoring, where real-time language processing is required. Its low-latency operation ensures rapid decisionmaking and responses in resource-constrained environments. 6. Industry-Specific Use Cases Shakti-LLMs fine-tuning on vernacular and domain-specific datasets gives it competitive edge in industries requiring specialized knowledge. In particular, healthcare, finance, and customer service can benefit from its real-time interaction capabilities and ability to deliver accurate, contextually relevant insights. Healthcare: Shakti-LLM can be fine-tuned for personalized healthcare advice, diagnostic support, and realtime assistance for medical professionals and patients. It can be deployed in low-resource settings where vernacular language support is essential for patient communication. 8 Finance: In the financial sector, Shakti-LLM can assist in tasks like document analysis, regulatory compliance, and fraud detection, providing real-time decision-making support for professionals . Customer Service: Shakti-LLMs ability to support multiple languages and adapt to user-specific queries makes it powerful tool for automating customer interactions, improving customer satisfaction and reducing response times [19]. 6.3 Multilingual and Low-Resource Language Support defining feature of Shakti-LLM is its fine-tuning on vernacular languages, addressing the significant need for AI models to operate effectively in low-resource language environments. Large global models often underperform in regional contexts due to insufficient training on low-resource languages such as Hindi, Kannada, and Telugu. ShaktiLLM bridges this gap, positioning itself as an AI solution well-suited for multilingual environments where linguistic diversity is high [18]. 6.4 Future Directions Looking ahead, several key areas for further development could enhance Shakti-LLMs capabilities: 1. Multimodal Integration: Extending Shakti-LLM to process multiple modalitiessuch as text, images, and speechcan unlock new applications in fields such as real-time video captioning and image processing. Integrating multiple input modalities will broaden the scope of applications across industries such as education, entertainment, and marketing[10]. 2. Advanced Fine-Tuning for Specialized Domains: While Shakti-LLM already demonstrates strong performance in general and domain-specific applications, future work could involve fine-tuning on specialized corpora, particularly for knowledge-heavy domains such as legal, scientific research, and manufacturing. Enhancing the models capabilities in these areas could make it valuable tool for professionals requiring high-precision language models [20]. 3. Code Generation and Programming Tasks: Given that Shakti-LLM currently underperforms in code generation tasks such as HumanEval, future iterations could benefit from additional pretraining on programming datasets. This would enhance the models proficiency in tasks such as software development, automation, and code completion, making it useful for software engineers and developers. 4. Ethical AI and Safety: Shakti-LLMs use of Direct Preference Optimization (DPO) to align its outputs with human ethical standards is key strength. Future development could further refine this capability, ensuring that Shakti-LLM continues to generate safe, ethical outputs, particularly in industries where privacy and ethical considerations are paramount, such as healthcare and education [17]."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we presented Shakti-LLM, highly efficient Small Language Model (SLM) optimized for deployment in resource-constrained environments such as smartphones and IoT systems. Shakti-LLM builds on the foundations of transformer-based architectures such as LLaMA [2], while introducing several key innovations, such as Variable Grouped Query Attention (VGQA) and SwiGLU activations [6]. These innovations ensure high performance while maintaining minimal computational footprint, making Shakti-LLM ideal for edge-AI applications. Through Continued Pretraining (CPT), Supervised Fine-Tuning (SFT), and Direct Preference Optimization (DPO), Shakti-LLM adapts to real-world needs and excels in domain-specific tasks across industries such as healthcare, finance, and customer service. Its fine-tuning on vernacular languages allows it to perform exceptionally well in low-resource environments, making it unique solution for regions with linguistic diversity. As Shakti-LLM continues to evolve, the integration of multimodal capabilities, improvements in code generation, and further fine-tuning for specialized domains will unlock new possibilities, making it an invaluable tool across wide range of industries. Shakti-LLM represents step forward in making AI more accessible, efficient, and inclusive, driving real-world impact across global industries and communities."
        },
        {
            "title": "References",
            "content": "[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, 9 Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. [3] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. [4] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. [5] Marah Abdin, Jyoti Aneja, Hany Awadalla, and Ahmed Awadallah. Phi-3 technical report: highly capable language model locally on your phone, 2024. [6] Noam Shazeer. Glu variants improve transformer, 2020. [7] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. [8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. [10] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. [11] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter, 2020. [12] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding, 2020. [13] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: compact task-agnostic bert for resource-limited devices, 2020. [14] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2016. [15] Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu Jia, En-Yu Yang, Marco Donato, Victor Sanh, Paul N. Whatmough, Alexander M. Rush, David Brooks, and Gu-Yeon Wei. Edgebert: Sentence-level energy optimizations for latency-aware multi-task nlp inference, 2021. [16] Leon Bergen, Timothy J. ODonnell, and Dzmitry Bahdanau. Systematic generalization with edge transformers, 2021. [17] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. [18] Leandro von Werra Rasul, Younes Belkada. Fine-tune llama 2 with dpo. https://huggingface.co/blog/ dpo-trl, 2023. Accessed: 2024-09-26. [19] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. [20] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data, 2019. [21] Wikimedia Foundation. Wikimedia downloads. [22] Mohammed Safi Ur Rahman Khan, Priyam Mehta, Ananth Sankar, Umashankar Kumaravelan, Sumanth Doddapaneni, Suriyaprasaad G, Varun Balan G, Sparsh Jain, Anoop Kunchukuttan, Pratyush Kumar, Raj Dabre, and Mitesh M. Khapra. Indicllmsuite: blueprint for creating pre-training and fine-tuning datasets for indian languages, 2024. 10 [23] Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. Culturax: cleaned, enormous, and multilingual dataset for large language models in 167 languages, 2023. [24] Gemma Team. Gemma: Open models based on gemini research and technology, 2024."
        }
    ],
    "affiliations": [
        "SandLogic Technologies Pvt Ltd."
    ]
}