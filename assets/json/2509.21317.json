{
    "paper_title": "Interactive Recommendation Agent with Active User Commands",
    "authors": [
        "Jiakai Tang",
        "Yujie Luo",
        "Xunke Xi",
        "Fei Sun",
        "Xueyang Feng",
        "Sunhao Dai",
        "Chao Yi",
        "Dian Chen",
        "Zhujin Gao",
        "Yang Li",
        "Xu Chen",
        "Wen Chen",
        "Jian Wu",
        "Yuning Jiang",
        "Bo Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional recommender systems rely on passive feedback mechanisms that limit users to simple choices such as like and dislike. However, these coarse-grained signals fail to capture users' nuanced behavior motivations and intentions. In turn, current systems cannot also distinguish which specific item attributes drive user satisfaction or dissatisfaction, resulting in inaccurate preference modeling. These fundamental limitations create a persistent gap between user intentions and system interpretations, ultimately undermining user satisfaction and harming system effectiveness. To address these limitations, we introduce the Interactive Recommendation Feed (IRF), a pioneering paradigm that enables natural language commands within mainstream recommendation feeds. Unlike traditional systems that confine users to passive implicit behavioral influence, IRF empowers active explicit control over recommendation policies through real-time linguistic commands. To support this paradigm, we develop RecBot, a dual-agent architecture where a Parser Agent transforms linguistic expressions into structured preferences and a Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly policy adjustment. To enable practical deployment, we employ simulation-augmented knowledge distillation to achieve efficient performance while maintaining strong reasoning capabilities. Through extensive offline and long-term online experiments, RecBot shows significant improvements in both user satisfaction and business outcomes."
        },
        {
            "title": "Start",
            "content": "Jiakai Tang1, Yujie Luo3, Xunke Xi3, Fei Sun2, Xueyang Feng1, Sunhao Dai1, Chao Yi3, Dian Chen3, Zhujin Gao3, Yang Li3, Xu Chen1(cid:66), Wen Chen3(cid:66), Jian Wu3, Yuning Jiang3, Bo Zheng3 1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China 2University of Chinese Academy of Sciences, China 3Alibaba Group, Beijing, China tangjiakai5704@ruc.edu.cn 5 2 0 2 5 2 ] . [ 1 7 1 3 1 2 . 9 0 5 2 : r Abstract Traditional recommender systems rely on passive feedback mechanisms that limit users to simple choices such as like and dislike. However, these coarse-grained signals fail to capture users nuanced behavior motivations and intentions. In turn, current systems cannot also distinguish which specific item attributes drive user satisfaction or dissatisfaction, resulting in inaccurate preference modeling. These fundamental limitations create persistent gap between user intentions and system interpretations, ultimately undermining user satisfaction and harming system effectiveness. To address these limitations, we introduce the Interactive Recommendation Feed (IRF), pioneering paradigm that enables natural language commands within mainstream recommendation feeds. Unlike traditional systems that confine users to passive implicit behavioral influence, IRF empowers active explicit control over recommendation policies through real-time linguistic commands. To support this paradigm, we develop RecBot, dual-agent architecture where Parser Agent transforms linguistic expressions into structured preferences and Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly policy adjustment. To enable practical deployment, we employ simulation-augmented knowledge distillation to achieve efficient performance while maintaining strong reasoning capabilities. Through extensive offline and longterm online experiments, RecBot shows significant improvements in both user satisfaction and business outcomes. CCS Concepts Information systems Recommender systems. Keywords Interactive Recommendation, Large Language Models, Agent ACM Reference Format: Jiakai Tang, Yujie Luo, Xunke Xi, Fei Sun, Xueyang Feng, Sunhao Dai, Chao Yi, Dian Chen, Zhujin Gao, Yang Li, Xu Chen, Wen Chen, Jian Wu, Yuning Jiang, Bo Zheng. 2025. Interactive Recommendation Agent with Work done during internship at Alibaba Group. (cid:66) Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference17, Washington, DC, USA 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn Figure 1: Comparison between traditional and novel interactive recommendation feeds. (a) Traditional systems rely on constrained and implicit feedback signals (e.g., likes/dislikes), making it difficult to accurately infer users true intentions. (b) Our interactive paradigm enables free-form natural language commands, where RecBot responds and adjusts recommendation policy on-the-fly based on active user commands. Active User Commands. In . ACM, New York, NY, USA, 16 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "1 Introduction\nModern digital ecosystems are saturated with vast amounts of in-\nformation, making Recommender Systems (RecSys) indispensable\ncomponents for managing information overload [5, 26], from e-\ncommerce platforms (e.g., Taobao, Amazon) [42, 60] to social media\nfeeds (e.g., TikTok, YouTube) [6, 67]. By filtering and presenting\ncontent based on individual user preferences, these systems pro-\nfoundly shape how users interact with digital content daily. Existing\nrecommender systems typically operate through a passive feedback\nloop, as illustrated in Fig. 1(a). In this paradigm, users are presented\nwith curated recommendation lists and respond through implicit\nbehavioral signals such as clicks, likes, dislikes, or viewing duration.\nThe recommendation algorithms then analyze these signals to infer\nuser preferences and iteratively refine future recommendations,\nestablishing an iterative implicit feedback loop [29, 30, 51].",
            "content": "However, this passive feedback paradigm suffers from fundamental limitations that create persistent communication gap between user intentions and system interpretations, ultimately undermining recommendation effectiveness. We identify the root causes of this problem from three complementary perspectives: how users experience interaction constraints, how algorithms interpret ambiguous signals, and how their interplay creates systemic dysfunction. First, from the user perspective, current recommendation interfaces restrict users to rigid interaction mechanisms such as click or star ratings, which cannot adequately capture the nuanced reasoning Conference17, July 2017, Washington, DC, USA Jiakai Tang et al. behind user preferences or identify which specific item attributes drive their satisfaction or dissatisfaction. This design limitation forces users to express complex preferences through vague signals that lose critical information about their actual intentions. Second, from the algorithm perspective, faced with such ambiguous and incomplete user feedback, recommendation algorithms resort to indiscriminate preference attribution, treating all item characteristics as equally responsible for user responses. This approach inevitably leads to inaccurate preference modeling that misrepresent user intentions and contribute to filter bubbles that narrow content diversity. Third, from the ecosystem perspective, these user expression constraints and algorithmic interpretation flaws interact to create cascading dysfunction beyond their respective impacts. When systems misinterpret signals and users respond to irrelevant recommendations, this creates communication deadlock where neither party can effectively convey or understand the others intentions, necessitating fundamental paradigm change rather than incremental improvements. To overcome these limitations, we first propose novel Interactive Recommendation Feed (IRF) product paradigm from user-centric design philosophy, as illustrated in Fig. 1(b). Unlike traditional interactive recommendation that confine users to passive consumption [9, 41, 66], IRF introduces an active voice channel enabling users to directly interact with the recommender systems through free-form natural language commands. This transforms the conventional one-way delivery into bidirectional interaction where users can explicitly articulate their requirements and constraints to guide the systems policy adjustment in real-time. Moreover, IRF also differs intrinsically from Conversational Recommender Systems (CRS), which require independent dialogue windows for guided questioning. Instead, IRF seamlessly integrates into mainstream recommendation feeds (e.g., Youtube, Taobao) as lightweight interface, allowing direct commands without interrupting natural browsing flow. By empowering users with direct algorithmic control, IRF essentially rethinks the human-system relationship, enabling users to proactively shape their content consumption experience rather than being passive recipients of algorithmic decisions. Furthermore, accurately responding to textual user commands requires bridging the gap between open-ended user expressions and actionable recommendation strategies, yet existing interactive methods lack the natural language understanding and reasoning capabilities necessary to parse free-form commands and translate them into command-aware item scoring adjustments. To tackle this problem, we develop RecBot, large language model-powered multi-agent framework for interactive recommender systems. The framework comprises two core components: First, the User-IntentUnderstanding Agent (Parser) parses natural language descriptions into structured recommendation specifications, extracting both positive and negative user intents while filtering irrelevant information. Second, the Planning Agent (Planner) then receives these structured preferences and orchestrates customized tool invocation chains, transforming domain-specific requirements into concrete actions that modify item relevance scores. This tool-chaining approach enables the Planner to handle sophisticated user intentions (e.g., positive interest, negative feedback, and interest drift), facilitating on-the-fly policy adaptation that directly influences next recommendation feed. To better support production deployment, RecBot incorporates dynamic memory consolidation for efficient multi-turn interactions and employs knowledge distillation to transfer understanding and reasoning capabilities from powerful teacher models to lightweight student models, achieving scalable and costeffective deployment. To comprehensively evaluate the effectiveness of our proposed Interactive Recommendation Feed paradigm and the RecBot framework, we conduct extensive empirical validation through both offline and online experiments. Our offline evaluation spans three real-world recommendation datasets (Amazon, MovieLens, and Taobao), where RecBot demonstrates superior performance against numerous state-of-the-art baseline methods across multiple evaluation metrics. RecBot has been fully deployed in production environment within large-scale e-commerce platforms homepage, with three-month online A/B testing evaluation. The long-term online experimental results reveal substantial improvements across multiple key metrics, including enhanced user engagement (e.g., 0.71% reduction in Negative Feedback Frequency (NFF) and 1.44% increase in Click Item Category Diversity (CICD)) and significant business revenue gains (e.g., 1.28% increase in Add-to-Cart (ATC) and 1.40% increase in Gross Merchandise Volume (GMV)). Our contributions are summarized as follows: We introduce the novel Interactive Recommendation Feed (IRF) product paradigm that breaks the silence of recommender systems by allowing users to directly communicate with the system through natural language commands, achieving usercentric controllable recommendation. We propose RecBot, multi-agent framework comprising the Parser and Planner agents that achieve precise intent parsing and on-the-fly command-aware recommendation policy adaptation through automated tool chaining, effectively bridging the gap between free-form user commands and actionable adjustments. Extensive offline and online experiments are conducted to comprehensively validate RecBots effectiveness. Notably, our online deployment demonstrates significant improvements in user engagement, content diversity, and business benefits."
        },
        {
            "title": "2 Problem Setup and Formulation\nIn this section, we introduce the Interactive Recommendation\nFeed (IRF) paradigm, which formulates the recommendation pro-\ncess as a sequential decision-making problem where the system\niteratively refines recommendations based on user linguistic com-\nmands and behavioral signals. Let 𝐼 = {𝑖1, 𝑖2, . . . , 𝑖𝑛 } denote the\ncandidate item pool, and 𝑢 represent a target user. At each interac-\ntion round 𝑡 ∈ {1, 2, . . . ,𝑇 }, the system presents a recommendation\nfeed 𝑅𝑡 = {𝑟 (1)\n} ⊂ 𝐼 of 𝐾 items to the user. Upon\nreceiving 𝑅𝑡 , the user provides linguistic feedback 𝑐𝑡 . This feed-\nback captures the user’s assessment of current recommendations,\npreferences, or any specific constraints or requirements.",
            "content": ", . . . , 𝑟 (𝐾 ) , 𝑟 (2) 𝑡 The system maintains dual-component user model: Explicit Preferences 𝑃𝑡 and Implicit Preferences 𝐻𝑡 . Explicit preferences represent structured knowledge extracted from user linguistic feedback through natural language understanding, formally defined as 𝑃𝑡 = 𝜙 (𝑐1, 𝑐2, . . . , 𝑐𝑡 ), where 𝜙 maps sequence of linguistic 𝑡 𝑡 Interactive Recommendation Agent with Active User Commands Conference17, July 2017, Washington, DC, USA Figure 2: Overview of the RecBot framework for interactive recommendation. The framework comprises Parser Agent that transforms user natural language command 𝑐𝑡 into structured preferences 𝑃𝑡 +1, and Planner Agent that orchestrates tool chains to dynamically adjust recommendation policies and generate the next feed 𝑅𝑡 +1. feedbacks to structured preference representations. Implicit preferences are captured by the users historical interaction sequence 𝐻𝑡 = {𝑖1, 𝑖2, . . . , 𝑖𝑚 }, reflecting long-term behavioral patterns. At round 𝑡, the system state is defined as 𝑆𝑡 = {𝑅𝑡, 𝑐𝑡 , 𝑃𝑡, 𝐻𝑡 }. The systems action involves selecting the next recommendation feed: 𝑅𝑡 +1 = 𝜋 (𝑆𝑡 ). The IRF process operates iteratively: beginning with initial feed 𝑅0, the system collects feedback 𝑐𝑡 , updates explicit preferences 𝑃𝑡 while incorporating historical behavior 𝐻𝑡 , constructs state 𝑆𝑡 , and generates the next feed 𝑅𝑡 +1. This continues until user satisfaction is achieved satisfy(𝑢, 𝑅𝑡 +1) = 1 or the maximum interaction limit is reached 𝑡 𝑇max, where 𝑇max represents the threshold for user engagement fatigue beyond which users typically disengage from the system."
        },
        {
            "title": "3.1 Overall Architecture\nTo enable precise responsiveness to user-initiated active commands,\nwe introduce RecBot, a multi-agent framework that systematically\ndecomposes multi-turn user commands, dynamically adjusts rec-\nommendation strategies, and modifies subsequent recommenda-\ntion feeds to progressively satisfy user requirements. The RecBot\nframework comprises two core intelligent agents coupled with a\ncomprehensive and extensible tool suite, facilitating the transfor-\nmation from free-form natural language commands to actionable\nitem scoring computations.",
            "content": "As illustrated in Fig. 2, our approach operates through sequential two-stage process. First, the User-Intent-Understanding Agent (Parser) transforms user open-domain textual instructions into structured domain-specific recommendation language, parsing both positive and negative user intents while employing dynamic memory consolidation prompting strategy to maintain efficient multi-turn dialogue state management. Subsequently, the Planning Agent (Planner) receives the structured recommendation specifications and orchestrates appropriate tool chain invocations, ultimately curating and presenting the top-𝐾 items to users. This systematic decomposition enables RecBot to bridge the semantic gap between diverse user expressions and concrete recommendation adjustments, facilitating real-time policy adaptation through principled tool orchestration."
        },
        {
            "title": "3.2 Parser\nReal-world user utterances exhibit considerable variability and di-\nversity in both style and semantic content [46, 63], often containing\nredundant noise such as verbose complaints, irrelevant contextual\ninformation. The key challenge lies in transforming free-form user\ncommands into structured recommendation instructions, which\nis essential for accurately capturing users’ proactive intentions.\nInspired by the robust contextual reasoning capabilities of large\nlanguage models [28, 35, 43, 56], we design a User Intent Under-\nstanding Agent (Parser) that bridges the semantic gap between\ndiverse user expressions and actionable recommendation specifica-\ntions. The Parser exploits the inherent ability of large language mod-\nels to handle linguistic variability and extract meaningful signals\nfrom noisy textual input, enabling precise intent understanding.",
            "content": "3.2.1 Structured Command Parsing. Given the previous recommendation feed 𝑅𝑡 presented to the user, user linguistic feedback 𝑐𝑡 in response to 𝑅𝑡 , and historical preference memory 𝑃𝑡 , the Parser performs the transformation: : (𝑅𝑡, 𝑐𝑡, 𝑃𝑡 ) 𝑃𝑡 +1, where denotes the parsing function that maps the input tuple to updated preference specifications 𝑃𝑡 +1. The updated preference specification 𝑃𝑡 +1 decomposes into two orthogonal dimensions based on user sentiment orientation: (1) 𝑃𝑡 +1 = {𝑃 + 𝑡 +1 , 𝑃 𝑡 +1}. (2) where 𝑃 + 𝑡 +1 represents positive preferences (item attributes the user shows interest in) and 𝑃 𝑡 +1 represents negative preferences (item attributes the user wants to exclude). This bidirectional decomposition enables better modeling of user intentions by capturing both attraction and aversion signals from natural language feedback. Furthermore, each preference dimension further categorizes into constraint types based on enforcement strictness: , 𝐶+,soft 𝑡 +1 }, 𝑡 +1 = {𝐶 ,hard 𝑃 𝑡 +1 = {𝐶+,hard 𝑃 + , 𝐶 ,soft 𝑡 +1 }, (3) 𝑡 +1 𝑡 +1 where 𝐶hard represents strict rule-based constraints that can be deterministically verified through direct attribute matching (e.g., price thresholds: under $50), and 𝐶soft denotes flexible semantic-based inclinations that require nuanced understanding of users subjective interests (e.g., genre preference: prefer romantic movies). Remark. Unlike existing interactive recommendation methods that predominantly focus on positive user requirements [22, 53, 54], Parser explicitly models both positive and negative preference signals. This bidirectional approach is motivated by empirical observations in production environments, where negative feedback accounts for the majority of user interactions1, necessitating comprehensive preference representation that encompasses the full spectrum of user intentional expressions. 3.2.2 Dynamic Memory Consolidation. Multi-turn interactions are crucial for interactive recommender systems as they allow users to iteratively refine their preferences and guide the system toward more in-depth understanding of their evolving interests. 1In-house platform deployment analysis reveals negative feedback accounts for approximately 57% of user commands versus 43% positive feedback. Conference17, July 2017, Washington, DC, USA Jiakai Tang et al. However, multi-turn interactions introduce extra computational and semantic challenges in maintaining consistent user preference understanding across extended dialogue sequences. Naive approaches that accumulate extensive conversation histories lead to computational overhead and semantic drift, while oversimplified state management fails to capture evolving user intentions accurately. To this end, we design Dynamic Memory Consolidation Strategy that maintains preference coherence across interaction rounds while ensuring computational efficiency through principled state management rather than exhaustive history retention. In specific, the consolidation mechanism operates through three strategic update principles that guide the Parsers preference synthesis process: (1) Preservation Principle: When current feedback 𝑐𝑡 indicates satisfaction with existing recommendations or provides neutral commentary, historical preferences 𝑃𝑡 are preserved unchanged to maintain preference stability: 𝑃𝑡 +1 = 𝑃𝑡 . (2) Integration Principle: When feedback introduces compatible new preferences that complement existing memory, these preferences are integrated through structured merging operations: 𝑡 +1 = 𝑃 + 𝑃 + 𝑡 Extract+ (𝑐𝑡 ), 𝑡 +1 = 𝑃 𝑃 𝑡 Extract (𝑐𝑡 ), where denotes the structured integration operator that handles both hard constraints and soft preferences separately, and Extract (𝑐𝑡 ) extracts positive and negative preference signals from current user command 𝑐𝑡 respectively. (3) Resolution Principle: When feedback explicitly contradicts established preferences, the Parser employs linguistic cue analysis to identify change indicators (e.g., instead of, no longer interested in, different from before) and performs targeted preference updates while preserving non-conflicting knowledge: 𝑃𝑡 +1 = Resolve(𝑃𝑡 , 𝑐𝑡, 𝑅𝑡 ). The consolidation process is formalized through context-aware decision function that dynamically selects the appropriate update strategy based on the interplay between historical preferences 𝑃𝑡 , current feedback 𝑐𝑡 , and recommendation context 𝑅𝑡 : 𝑃𝑡 +1 = 𝑃𝑡 𝑃𝑡 Extract(𝑐𝑡 ) Resolve(𝑃𝑡 , 𝑐𝑡, 𝑅𝑡 ) if 𝜙sat (𝑐𝑡 , 𝑅𝑡 ), if 𝜙com (𝑃𝑡 , 𝑐𝑡, 𝑅𝑡 ), if 𝜙con (𝑃𝑡 , 𝑐𝑡, 𝑅𝑡 ), where 𝜙sat, 𝜙com, and 𝜙con represent context-aware analysis functions that respectively detect user satisfaction states, preference compatibility, and preference conflicts. Remark. The proposed dynamic memory consolidation approach concentrates large language model reasoning on synthesizing compressed historical preferences with current feedback, avoiding repetitive processing of extensive multi-turn conversation histories while maintaining information fidelity across extended interaction rounds with bounded computational complexity."
        },
        {
            "title": "3.3 Planner\nUpon receiving the structured recommendation specifications 𝑃𝑡 +1\nfrom the Parser, the next task is to translate these explicit user\npreferences into concrete recommendation policy adjustments. To\naddress this, we introduce the Planning Agent (Planner), which",
            "content": "Figure 3: Illustration of the Parser for user intent understanding. The Parser integrates history preference memory 𝑃𝑡 , current recommendation feed 𝑅𝑡 , and active user command 𝑐𝑡 to generate new preference representation 𝑃𝑡 +1 through structured parsing and dynamic memory consolidation. orchestrates adaptive tool chain invocations to dynamically modify item scoring mechanisms based on user commands. The Planner operates through principled reasoning over comprehensive recommendation domain toolset, enabling flexible adaptation to diverse user requirements. Given structured pref𝑡 +1} and user implicit preferences 𝐻𝑡 , the erences 𝑃𝑡 +1 = {𝑃 + 𝑡 +1 Planner performs the mapping: , 𝑃 (4) : (𝑃𝑡 +1, 𝐻𝑡 , 𝐼 ) 𝑆𝑡 +1, where denotes the planning function that transforms preference specifications and candidate item pool 𝐼 into updated item scoring 𝑆𝑡 +1 = {𝑠1, 𝑠2, . . . , 𝑠𝑛 }, enabling top-𝐾 item selection for the subsequent recommendation feed 𝑅𝑡 +1. 3.3.1 Recommendation Domain Toolset. The Planner uses modular and extensible toolset, each designed to handle specific aspects of user preference satisfaction through targeted item scoring. The core toolset comprises four complementary components: (i) Filter for hard constraint enforcement, (ii) Matcher for positive preference alignment, (iii) Attenuator for negative feedback incorporation, and (iv) Aggregator for comprehensive score integration. In the following, we detail the specific functionality of each tool. Filter Tool. The Filter enforces hard constraint satisfaction by both selecting items that meet positive requirements and eliminating items that violate negative constraints. Given hard constraints 𝐶+,hard from 𝑃𝑡 +1, the filter adopts binary selection over 𝑡 +1 the candidate item pool 𝐼 to produce refined subset 𝐼 : and 𝐶 ,hard 𝑡 +1 𝑡 +1 𝑡 +1 𝐼 = {𝑖 𝐼 : C+ (𝑖, 𝐶+,hard ) = 1 (𝑖, 𝐶 ,hard where C+ (𝑖, 𝐶+,hard ) evaluates whether item 𝑖 satisfies all positive hard constraints, and (𝑖, 𝐶 ,hard ) determines whether item 𝑖 violates any negative hard constraints. Items excluded from 𝐼 are effectively assigned score of , ensuring they are not recommended in the subsequent feed. ) = 0}, (5) 𝑡 +1 𝑡 +1 This filtering mechanism enables efficient candidate space pruning while enforcing strict user requirements. For instance, when users specify temporal constraints (e.g., movies released before 2020) or budget limitations (e.g., no luxury products above $500), the Filter precisely narrows the search space according to these explicit boundaries. The resulting filtered candidate set 𝐼 serves as Interactive Recommendation Agent with Active User Commands Conference17, July 2017, Washington, DC, USA the refined input for downstream scoring tools, namely the Matcher and Attenuator, ensuring computational efficiency while maintaining strict adherence to user-specified requirements. Matcher Tool. The Matcher tool computes positive relevance scores based on user positive intentions through two-path architecture that combines semantic understanding with personalized collaborative knowledge. (1) Semantic Relevance Path: Positive intentions 𝑃 + 𝑡 +1 are first transformed into structured natural language descriptions following the format: attribute1:[value, . . .], attribute2:[value, . . .]. The semantic component leverages pre-trained embedding model (e.g., BGE [55], Sentence-BERT [38]) to compute semantic affinity between item descriptions and positive intent representations: 𝑠sem (𝑖, 𝑃 + 𝑡 +1) = sim(eitem (𝑖), eintent (𝑃 + (6) where eitem (𝑖) and eintent (𝑃 + 𝑡 +1) denote the embedding representations of item 𝑖s description and formatted positive intentions, respectively, and sim(, ) indicates cosine similarity. 𝑡 +1)) (2) Active-Intent-Aware Collaborative Path: To incorporate personalized collaborative information, we design an Active-IntentAware (AIA) sequential recommender that treats user positive feedback as queries to extract intent-relevant patterns from historical multimodal item representations. Specifically, the AIA model first processes multimodal item features through dedicated encoders 𝑓𝑚 (), where 𝑚 denotes modality type (e.g., text and image), to obtain modality-specific item representations h𝑖 𝑚 R𝑑 . These are then fused into unified multimodal representations: 𝑖 𝑖 𝑖 image . . .]), text fused = Linear([h where denotes concatenation and Linear() represents linear network that performs dimension reduction from the concatenated multimodal representations to unified 𝑑-dimensional representations. Similarly, user positive intent 𝑃 + 𝑡 +1 is encoded into the textual modality representation hintent R𝑑 . Subsequently, modality-specific sequential encoders extract temporal patterns from user historical interactions 𝐻𝑡 : 𝑖𝐻𝑡 fused 𝑖1 Hfused = SeqEnc([h fused where SeqEnc employs Multi-Head Self-Attention (MHSA) mechanisms. To further capture intent-aware collaborative patterns, we utilize Multi-Head Cross-Attention (MHCA) between transformed user intent and fused sequence representations: 𝑖2 , fused , . . . , ]), 𝑠aia (𝑖, 𝑃 + 𝑡 + 𝑖 , 𝐻𝑡 ) = MHCA(hintent, Hfused, Hfused) fused , where MHCA(q, K, V) computes attention-weighted representations using query q, keys K, and values V. The final positive relevance score integrates both semantic and collaborative components through weighted summation: 𝑠match (𝑖) = 𝛼 𝑠sem (𝑖, 𝑃 + (7) where 𝛼 [0, 1] balances semantic and collaborative contributions. 𝑡 +1) + (1 𝛼) Saia (𝑖, 𝑃 + 𝑡 +1 , 𝐻𝑡 ), Attenuator Tool. The Attenuator computes negative relevance penalties based on user negative intentions. Similar to the semantic path in Matcher, negative intentions 𝑃 𝑡 +1 are converted to structured descriptions and semantic similarity scores are computed: 𝑠atten (𝑖) = 𝛽 sim(eitem (𝑖), eintent (𝑃 𝑡 +1)), (8) Figure 4: Illustration of the Planner for on-the-fly recommendation policy adaptation. The Planner dynamically constructs optimal tool invocation sequences based on parsed user preferences 𝑃𝑡 +1 to compute updated item scores 𝑠final for next recommendation feed 𝑅𝑡 +1. where 𝛽 > 0 controls the attenuation strength, and the negative sign ensures that items semantically similar to negative intentions receive penalty scores, thereby reducing their likelihood of being recommended in the next feed. Aggregator Tool. The Aggregator tool synthesizes scores from all preceding tools to produce final item rankings: 𝑠final (𝑖) = 𝑠match (𝑖) + 𝑠atten (𝑖), (9) Finally, the top-𝐾 items based on final scores 𝑠final (𝑖) constitute the next recommendation feed 𝑅𝑡 +1. Remark. The modular toolset design enables the Planner to flexibly orchestrate semantic understanding, collaborative filtering, and constraint satisfaction through standardized tool interfaces. This design philosophy aligns with emerging paradigms such as Model Context Protocol (MCP) [15, 37], which advocates for structured tool integration in AI systems. For example, Searcher tool for online information retrievalwhen user feedback contains trending topics or seasonal preferencescan be seamlessly integrated following MCP principles without requiring fundamental framework modifications. This comprehensive foundation facilitates responsive recommendation policy adaptation that effectively bridges explicit user commands with personalized implicit preferences. 3.3.2 Adaptive Tool Chain Orchestration. The Planner employs context-aware reasoning to dynamically construct optimal tool invocation sequences based on the parsed user preferences 𝑃𝑡 +1. This orchestration operates through two coordinated phases: strategic tool selection and execution coordination. Strategic Tool Selection The Planner systematically analyzes 𝑃𝑡 +1 to determine necessary tool activations and execution strategies. Hard constraints (𝐶+,hard ) trigger prioritized Filter activation to establish refined candidate space 𝐼 𝐼 . Positive intentions (𝐶+,soft ) activate the Matcher for relevance computa𝑡 +1 tion, while negative feedback (𝐶 ,soft ) necessitates Attenuator 𝑡 +1 deployment for penalty scoring. The Planner adaptively scales tool chain complexity: deploying selective tool subsets for focused feedback to minimize computational overhead, or activating full sequences (Filter Matcher & Attenuator Aggregator) for multi-dimensional user requirements containing both preferences and constraints. , 𝐶 ,hard 𝑡 +1 𝑡 +1 Conference17, July 2017, Washington, DC, USA Jiakai Tang et al. Execution Coordination The Planner manages seamless information flow between activated tools through structured dependency management. The filtered candidate set 𝐼 serves as input constraint for both Matcher and Attenuator, ensuring all scoring computations operate within the valid item space defined by hard constraints. When applicable, Matcher and Attenuator execute in parallel to optimize computational throughput, with the Aggregator synthesizing multi-tool outputs through weighted combination strategies to produce unified item scores reflecting comprehensive user reference satisfaction."
        },
        {
            "title": "3.4 Multi-Agent Optimization\nThe Parser and Planner agents face highly complex personalized\nreasoning and decision-making tasks involving ambiguous user\ncommand parsing, multi-turn preference state maintenance, and\nadaptive tool chain orchestration. Our empirical experiments re-\nveal that closed-source large language models (e.g., GPT-4.1 [1])\nsignificantly outperform open-source alternatives in these criti-\ncal tasks. However, these LLMs face deployment constraints (e.g.,\nprohibitive inference costs, data privacy concerns, and limited con-\ntrollability). To achieve lightweight online deployment while recon-\nciling this performance-deployment tension, we introduce a Multi-\nAgent Optimization framework that facilitates knowledge trans-\nfer from powerful closed-source teacher models to cost-effective\nopen-source student models through simulation-augmented knowl-\nedge distillation method.",
            "content": "3.4.1 Simulation-Augmented Knowledge Distillation Framework. Our optimization approach leverages the versatile role-playing capabilities [14, 33, 64] of large language models to construct simulation-based training environment where synthetic user agents interact with teacher RecBot systems across multiple interaction rounds. This paradigm enables diverse data generation that captures the varied user-system interactions while maintaining authentic dialogue patterns and preference dynamics. We establish simulation framework where User Simulation Agent Usim and Teacher RecBot Rteacher engage in multi-turn interactive processes. Given target item anchor 𝑖target 𝐼 representing the users latent objective and an initial recommendation feed 𝑅0, the simulation proceeds iteratively: the User Simulation Agent generates contextually appropriate linguistic feedback following natural user expression patterns: 𝑡 = Usim (𝑅𝑡, 𝑖target, Gpersona), 𝑐sim where Gpersona represents persona specifications that define user behavioral characteristics, including preference expression styles, constraint specification patterns, and interaction tendencies. (10) Then, Rteacher processes this feedback through both Parser and Planner components, generating structured preferences and corresponding tool chains for recommendation adaptation. This process continues until the user agent reaches satisfaction or meets termination criteria (e.g., maximum interaction rounds), yielding multi-turn interaction trajectories that capture varied user scenarios, preference evolution patterns, and system response strategies across different contexts. 3.4.2 Training Data Collection. From the generated simulation trajectories, we systematically extract training samples for both 𝑡 Parser and Planner agents. For the Parser, each training sample captures the complete preference parsing transformation: input tuple (𝑅𝑡 , 𝑐sim , 𝑃𝑡 ) and corresponding structured output 𝑃 sim 𝑡 +1. For the Planner, training instances demonstrate adaptive tool orchestration: structured preference specifications 𝑃 sim 𝑡 +1 paired with optimal tool invocation sequences sim determined by the teacher model. The training datasets are formally defined as: DParser = {((𝑅𝑡, 𝑐sim )}, where Ω represents available recommendation tool descriptions. Here, to achieve efficient online inference, we implement the Parser as an end-to-end function that jointly performs user command interpretation and dynamic memory consolidation in one pass. 𝑡 +1)}, DPlanner = {((𝑃 sim 𝑡 +1 , Ω), sim , 𝑃𝑡 ), 𝑃 sim 𝑡 𝑡 𝑡 3.4.3 Optimization Objective. We optimize both agents jointly through Supervised Fine-Tuning (SFT) on unified language model backbone using mixed training dataset that combines both Parser and Planner training samples. Specifically, the unified training dataset is constructed by merging both agent-specific samples: DMixed = DParser DPlanner. The optimization objective follows standard Next-Token Prediction (NTP) learning paradigm, that is, minimizing the Negative Log-Likelihood (NLL) of target sequences: 𝑦 (𝜃 ) = log 𝑃𝜃 (𝑦 𝑗 𝑥, 𝑦< 𝑗 ) (11) (𝑥,𝑦) DMixed 𝑗=1 where 𝜃 represents the shared model parameters, and 𝑦 denotes the target output sequence (structured preferences for Parser tasks or tool chains for Planner tasks). This unified training paradigm leverages the models capacity to handle diverse reasoning patterns through appropriate input formatting and task-specific prompting, enabling cost-effective deployment while maintaining functional modularity. In the future, we will explore independent agent training method to achieve enhanced task-specific performance and more advanced multi-agent learning approaches (e.g., agentic reinforced evolution [7, 8, 11]) to further improve collaborative capabilities."
        },
        {
            "title": "3.5 Discussion\nIn this section, we discuss RecBot’s distinctions from existing inter-\nactive recommendation agents and the differences between Interac-\ntive Recommendation Feed (IRF) and Conversational Recommender\nsystems (CRS) paradigms.",
            "content": "3.5.1 Comparison of RecBot and Existing Interactive Recommendation Agents. As summarized in Table 1, we compare RecBot against existing interactive recommendation agents across three dimensions: agent capabilities, interaction features, and practicality. RecBot demonstrates multi-aspect advancement as follows: Agent Capabilities: RecBot demonstrates five key advances: (1) Collaborative Knowledge: Integrates historical user behaviors with explicit commands through intent-aware collaborative filtering. (2) Tool Invocation: Enables flexible scenario adaptation and seamless functionality extension through modular interfaces. (3) Agent-Tuning: Achieves production-ready performance with cost-effective large language models via knowledge distillation. (4) Memory: Maintains preference coherence across extended interactions through dynamic consolidation. (5) Multi-Modal: Interactive Recommendation Agent with Active User Commands Conference17, July 2017, Washington, DC, USA Table 1: Comparison of Different Interactive Recommendation Agent Frameworks. The comparison criteria are categorized into three aspects: Agent Capabilities , Interaction Features , and Practicality . Capability InteRecAgent [23] RecBench+ [22] InstructAgent [57] GOMMIR [53] RecBot (Ours) Collaborative Knowledge Tool Invocation Agent-Tuning Memory Multi-Modal Multi-turn Interaction Complex User Command Industrial Deployment Processes heterogeneous item representations across multiple modalities through unified encoding frameworks. Interaction Features: RecBot can handle complex multi-turn dialogues and sophisticated user commands (e.g., mixed positive and negative feedback, and interest shifts), addressing limitations of prior approaches that are constrained to single-turn or simplistic interactions, thereby enhancing real-world applicability. Practical Impact: RecBot achieves successful industrial deployment with validated business impact, while existing approaches remain largely theoretical or confined to academic prototypes. 3.5.2 Interactive Recommendation Feed vs. Conversational Recommendation. The IRF paradigm differs fundamentally from conversational recommender systems across two dimensions: (1) Usage Scenario: CRS address goal-directed search tasks where users possess well-defined information needs and explicit purchase intentions. IRF, conversely, facilitates exploratory browsing scenarios where users engage in open-ended discovery, with preferences emerging and evolving through iterative system interactions. (2) Preference Elicitation: CRS employ structured interrogative approaches that strategically extract user preferences via predefined conversational schemas. IRF adopts reactive paradigm where users provide contextually-anchored feedback through natural language responses to specific recommendation outputs, enabling preference articulation directly grounded in presented recommendation feed."
        },
        {
            "title": "4.1 Experimental Setup\n4.1.1 Datasets. We conduct our experiments on the following\nthree recommendation datasets:\n• Amazon [16]: We select the “Books” category from the Amazon\ndataset. Following previous studies [17, 45, 65], we treat all user\nhistorical reviews as interactions and apply 20-core filtering to\nremove users and items with fewer than 20 interactions, from\nwhich we sample 1,000 users for our experiments. For this dataset,\nwe utilize price, language, and binding format as hard constraints,\nwhile book categories serve as soft conditions.",
            "content": "MovieLens 2: We consider user-item interactions with ratings greater than 3 as positive interactions. For MovieLens, we employ movie release date as hard constraints, while movie genres as soft conditions for recommendation guidance. Taobao: The experiments utilize interaction records from Taobao, leading e-commerce platform serving billions of users and items. The experimental scenario focuses on the Guess What You Like column displayed on the Taobao APPs homepage, where the core task is to predict the next items users will interact with based on their historical behaviors and profile information. We sample 3,000 users for this offline experiment. In the Taobao dataset, we set price, style, and material as hard constraints, while the platforms multi-level product categories serve as soft conditions. Additionally, we incorporate product cover images as visual features to enhance multimodal understanding. Consistent with existing works [23], we adopt the commonlyused Leave-One-Out (LOO) evaluation protocol, where user historical interactions are organized in chronological order, and the last item in each user sequence is held out for testing. 4.1.2 Offline Interactive Recommendation Experiments. To closely simulate realistic online user-system interaction environments, we employ GPT-4.13 as the backbone for our user simulation framework, leveraging its robust language generation capabilities to emulate authentic user behaviors. Based on prevalent user command patterns observed in production systems, we design three distinct interaction scenarios of increasing complexity: Single-Round Interaction (SR): Users possess clearly defined preferences and can express their requirements for target item characteristics comprehensively within single command. This scenario is suited for users who have clear intentions. Multi-Round Interaction (MR): Users initially exhibit ambiguous or exploratory preferences. Through iterative exposure to recommendation feeds, users progressively refine their requirements by providing both positive interest signals for desired attributes and negative feedback regarding unsatisfactory aspects of current recommendations. In our implementation, we employ pre-constructed prompting strategies to drive diverse user feedback styles, enabling both proactive new requirements and reactive negative feedback to exposed items. This iterative 2https://grouplens.org/datasets/movielens/ 3The specific snapshot version used is GPT-4.1-2025-04-14. Conference17, July 2017, Washington, DC, USA Jiakai Tang et al. refinement process expects the system to dynamically adjust its recommendation policy based on accumulated user guidance. Multi-Round Interaction with Interest Drift (MRID): Building upon the MR scenario, this setting introduces preference evolution where users may exhibit conflicting or shifting interests across interaction rounds. For instance, user initially seeking Windows-based computers may subsequently pivot toward Mac preferences during the browsing session. This scenario represents the most challenging yet realistic user behavior pattern commonly observed in exploratory contexts. In our implementation, we simulate this interest drift by initially pre-sampling random item as pseudo-target to guide the users early interactions, then strategically redirecting the users preference toward their actual ground-truth next interaction item at round 3, thereby creating realistic preference shift scenario. For multi-round interaction scenarios (i.e., MR and MRID), we establish experimental parameters with maximum interaction limit of 5 rounds and recommendation list size of 𝐾 = 5 items per round, reflecting typical user attention spans focused on top-ranked recommendations [24]. We assume that users will select the top-1 recommended item from each exposed list to issue explicit feedback commands that guide the systems strategy adjustment for subsequent rounds. The interaction terminates when either: (1) the users ground-truth target item appears within the top-5 recommendations, indicating successful preference satisfaction, or (2) the maximum round limit is exceeded, representing interaction failure or user abandonment. These different experimental designs enable systematic evaluation of RecBots adaptive capabilities across diverse user interaction patterns while maintaining computational feasibility and realistic user behavior modeling. 4.1.3 Evaluation Metrics. For offline experiments, we follow prior work [48, 50] and adopt standard ranking-based metrics including Recall@N and NDCG@N, where 𝑁 {10, 20, 50}. Additionally, we introduce three specialized metrics to comprehensively evaluate interactive recommendation performance: Condition Satisfaction Rate (CSR@N): This metric measures the attribute-level coverage ratio between recommended items in top-𝑁 positions and the target items characteristics. CSR@N serves as an attribute-oriented ranking accuracy measure that, compared to the strict precision requirements of Recall and NDCG, reflects the models capability to correctly infer user preferences regarding item category and attribute preferences. Pass Rate (PR): This binary metric calculates whether the target item is successfully delivered within the top-𝐾 recommendation feed during the limited interaction rounds 𝑇 (𝑇 =1 for SR scenarios, 𝑇 =5 for MR and MRID scenarios). Average Rounds (AR): This efficiency metric quantifies the average interaction rounds required for the system to successfully predict the target item to users at the top-𝐾 positions. For cases where the system fails to recommend the target item within the maximum 𝑇 rounds, we assign penalty score of 𝑇 +1 to reflect unsuccessful interactions. Note: Among all offline metrics, AR follows lower-is-better principle, indicating that the system achieves higher efficiency by identifying correct target items through fewer interaction rounds. All other metrics follow higher-is-better evaluation paradigm. For online experiments, we assess RecBots real-world performance through two complementary evaluation dimensions that capture both user satisfaction and business impact. In specific, (1) User Experience Metrics focus on measuring user satisfaction and engagement quality, including: Negative Feedback Frequency (NFF), measuring the frequency of user dissatisfaction signals toward recommendation feeds; Exposed Item Category Diversity (EICD), quantifying the average category diversity of items presented to users; Clicked Item Category Diversity (CICD), measuring the average category diversity of items users actively engage with through clicks. (2) Business Utility Metrics evaluate the commercial utility, including Page Views (PV), capturing overall user engagement with recommended content; Add-to-Cart (ATC), measuring conversion intent through shopping cart additions; Gross Merchandise Volume (GMV), quantifying the total transaction value generated from recommendations. Note. Among the above online metrics, NFF follows lower-isbetter evaluation criterion, as reduced negative feedback frequency indicates improved user satisfaction with recommendation quality. All remaining online metrics adhere to higher-is-better assessment protocol, where increased values demonstrate enhanced user engagement and business benefits. 4.1.4 Baselines. We compare RecBot against various traditional sequential recommendation algorithms, instruction-aware methods, and interactive recommendation agent approaches as follows: (1) Traditional Sequential Recommendation Methods: SASRec [25], self-attention based sequential recommendation model that adaptively weights user historical interactions to predict the next item. BERT4Rec [44], applies bidirectional modeling to user behavior sequences, using masked item prediction with self-attention to leverage both past and future context information. MoRec [61], modality-based recommendation approach that replaces ID embeddings with pre-trained encoders through endto-end training to leverage item content features. UniSRec [18], universal sequence representation learning approach that leverages item text descriptions with parametric whitening and mixture-of-experts enhanced adaptors to learn transferable representations across domains. (2) Command-Aware Recommendation Methods: BM25 [39], classic probabilistic ranking function for information retrieval that computes relevance scores between user queries and item descriptions using term frequency and inverse document frequency with length normalization parameters. BGE [55], ranking-based recommendation approach that computes item relevance scores using BGE embeddings to measure semantic similarity between user queries and candidate items for recommendation ranking. (3) Interactive Recommendation Agent Methods: GOMMIR [53], goal-oriented multi-modal interactive recommendation model that addresses the coupling issue between policy optimization and representation learning through joint supervised and reinforcement learning. Interactive Recommendation Agent with Active User Commands Conference17, July 2017, Washington, DC, USA Table 2: Performance comparison of different methods on Single-Round (SR) interaction scenarios across three datasets. We report results based on Recall@K (R@K), NDCG@K (N@K), Condition Satisfaction Rate@K (C@K), and Pass Rate (PR) metrics. Best results are highlighted in bold. Method R@10 R@20 R@50 N@10 N@20 N@50 C@10 C@20 C@ PR Amazon (SR) SASRec BERT4Rec MoRec UniSRec BM25 BGE GOMMIR InteRecAgent Intruct2Agent RecBot-Qwen (Orig.) RecBot-Qwen (Align.) RecBot-GPT SASRec BERT4Rec MoRec UniSRec BM25 BGE GOMMIR InteRecAgent Intruct2Agent RecBot-Qwen (Orig.) RecBot-Qwen (Align.) RecBot-GPT SASRec BERT4Rec MoRec UniSRec BM25 BGE GOMMIR InteRecAgent Intruct2Agent RecBot-Qwen (Orig.) RecBot-Qwen (Align.) RecBot-GPT 0.0098 0.0076 0.0316 0.0370 0.0283 0.0598 0.0011 0.0609 0.0033 0.2078 0.2198 0. 0.0236 0.0364 0.0642 0.0664 0.0257 0.1370 0.0021 0.1103 0.0214 0.3073 0.3383 0.4293 0.0267 0.0210 0.1421 0.1535 0.0000 0.2025 0.0003 0.2122 0.0230 0.3797 0.4531 0.4918 0.0163 0.0109 0.0457 0.0544 0.0370 0.1012 0.0022 0.1023 0.0054 0.2851 0.3101 0.3547 0.0428 0.0493 0.1103 0.1338 0.0535 0.2077 0.0043 0.1959 0.0460 0.4058 0.4208 0.5161 0.0400 0.0340 0.1922 0.2095 0.0000 0.2753 0.0003 0.2673 0.0377 0.4668 0.5402 0.5879 0.0326 0.0229 0.0827 0.1001 0.0816 0.1795 0.0054 0.1708 0.0131 0.4091 0.4614 0. 0.0942 0.0996 0.2366 0.2612 0.1381 0.3512 0.0150 0.3555 0.0835 0.5589 0.5675 0.6649 0.0744 0.0654 0.2639 0.2840 0.0000 0.3947 0.0007 0.3347 0.0704 0.5756 0.6483 0.6980 0.0061 0.0054 0.0184 0.0202 0.0128 0.0284 0.0004 0.0321 0.0011 0.1119 0.1207 0.1391 0.0077 0.0062 0.0220 0.0246 0.0150 0.0387 0.0007 0.0425 0.0016 0.1313 0.1434 0.1667 MovieLens (SR) 0.0130 0.0179 0.0302 0.0368 0.0123 0.0645 0.0007 0.0591 0.0087 0.1935 0.2101 0. 0.0177 0.0212 0.0418 0.0539 0.0192 0.0822 0.0013 0.0807 0.0148 0.2181 0.2309 0.2967 Taobao (SR) 0.0140 0.0101 0.0703 0.0734 0.0000 0.1174 0.0001 0.1184 0.0104 0.2432 0.2875 0.3174 0.0173 0.0134 0.0830 0.0875 0.0000 0.1358 0.0001 0.1323 0.0140 0.2650 0.3094 0.3417 0.0109 0.0085 0.0292 0.0336 0.0239 0.0543 0.0013 0.0560 0.0031 0.1555 0.1736 0.1994 0.0279 0.0311 0.0663 0.0791 0.0358 0.1101 0.0033 0.0591 0.0224 0.2484 0.2598 0. 0.0240 0.0196 0.0972 0.1023 0.0000 0.1594 0.0002 0.1457 0.0205 0.2866 0.3310 0.3636 0.76% 65.81% 0.65% 64.84% 2.29% 73.49% 2.18% 73.87% 1.31% 80.51% 3.16% 92.76% 0.00% 53.14% 3.81% 81.98% 0.00% 67.02% 14.04% 92.12% 14.58% 94.92% 97.63% 98.47% 99.39% 16.76% 73.88% 73.82% 82.46% 81.99% 91.29% 97.23% 71.65% 87.28% 78.53% 95.02% 97.34% 69.08% 69.40% 77.65% 77.65% 85.96% 95.19% 60.89% 84.21% 72.42% 93.70% 96.13% 1.61% 57.55% 1.71% 57.44% 3.00% 60.65% 4.07% 60.28% 1.28% 60.81% 7.49% 77.25% 0.11% 43.36% 6.64% 66.81% 1.28% 54.12% 23.13% 80.57% 25.70% 81.10% 88.81% 91.22% 93.36% 32.76% 70.45% 70.24% 76.61% 76.71% 77.19% 92.24% 61.03% 82.92% 68.58% 88.22% 88.38% 62.15% 61.99% 66.43% 66.60% 66.86% 85.06% 43.36% 73.45% 59.37% 83.99% 84.69% 37.75% 1.67% 36.56% 1.20% 45.31% 9.48% 45.16% 9.41% 28.10% 0.00% 82.60% 14.25% 17.24% 0.03% 57.18% 15.72% 34.15% 1.20% 81.18% 29.56% 35.37% 86.12% 89.40% 91.58% 93.83% 39.31% 44.71% 44.23% 54.67% 54.21% 41.38% 90.22% 23.49% 65.89% 50.10% 88.13% 91.02% 40.36% 39.74% 49.13% 49.14% 34.00% 86.35% 19.70% 60.74% 41.11% 84.59% 88.39% InteRecAgent [23], LLM-based interactive recommender framework that bridges traditional recommendation models with large language models using plan-first execution, dynamic demonstrations, and reflection mechanisms with shared candidate bus. Instruct2Agent [57], user-controllable recommender agent that serves as protective interface between users and recommender systems using instruction-aware parsing, external knowledge tools, and dynamic memory mechanisms. RecBot (Ours), to evaluate our proposed RecBot, we conduct experiments across three distinct configurations as follows: (1) RecBot-Qwen (Orig.), serves as our base version utilizing the Qwen3-14B [59] foundation model. (2) RecBot-Qwen (Align.) is an enhanced variant built upon the Qwen3-14B foundation model that integrates the multi-agent optimization strategy detailed in Sec. 3.4, where GPT-4.1 serves as the Teacher RecBot Agent to guide the simulation-augmented knowledge distillation. (3) Conference17, July 2017, Washington, DC, USA Jiakai Tang et al. Table 3: Performance comparison on Multi-Round (MR) interaction scenarios. We report results based on Recall (R@K), NDCG (N@K), Condition Satisfaction Rate (C@K), Pass Rate (PR), and Average Rounds (AR) metrics. Best results are highlighted in bold. indicates that higher values are better, while indicates that lower values are better. Amazon (MR) Method R@10 R@20 R@50 N@10 N@20 N@50 C@10 C@20 C@50 PR AR BM25 BGE GOMMIR InteRecAgent Intruct2Agent RecBot-Qwen (Orig.) RecBot-Qwen (Align.) RecBot-GPT BM25 BGE GOMMIR InteRecAgent Intruct2Agent RecBot-Qwen (Orig.) RecBot-Qwen (Align.) RecBot-GPT BM25 BGE GOMMIR InteRecAgent Intruct2Agent RecBot-Qwen (Orig.) RecBot-Qwen (Align.) RecBot-GPT 0.0381 0.0609 0.0000 0.0533 0.0250 0.1632 0.1893 0. 0.0621 0.1146 0.0043 0.1081 0.0557 0.3812 0.4315 0.4036 0.0941 0.1919 0.0000 0.2166 0.1061 0.3967 0.4238 0.4618 0.0479 0.0740 0.0011 0.0849 0.0283 0.1904 0.2416 0.3199 0.0931 0.1745 0.0064 0.1820 0.0749 0.4625 0.5021 0.4829 0.0941 0.2259 0.0000 0.2619 0.1091 0.4518 0.4735 0.5305 0.0664 0.1153 0.0033 0.1436 0.0326 0.2677 0.3177 0. 0.2034 0.3266 0.0171 0.3255 0.1221 0.5717 0.6221 0.6146 0.0941 0.2880 0.0003 0.3160 0.1205 0.5199 0.5252 0.6220 0.0253 0.0329 0.0000 0.0284 0.0170 0.0882 0.1044 0.1250 0.0277 0.0361 0.0003 0.0363 0.0177 0.0951 0.1177 0.1475 MovieLens (MR) 0.0381 0.0617 0.0018 0.0582 0.0351 0.2407 0.2742 0. 0.0458 0.0765 0.0024 0.0767 0.0398 0.2610 0.2921 0.2749 Taobao (MR) 0.0540 0.1059 0.0000 0.1137 0.0602 0.2415 0.2627 0.2795 0.0540 0.1143 0.0000 0.1252 0.0610 0.2553 0.2752 0.2967 0.0313 0.0444 0.0007 0.0479 0.0187 0.1104 0.1327 0.1708 0.0673 0.1063 0.0044 0.1047 0.0489 0.2828 0.3159 0. 0.0540 0.1267 0.0001 0.1360 0.0632 0.2689 0.2855 0.3149 77.37% 87.29% 52.20% 78.89% 66.69% 82.30% 84.21% 95.20% 63.28% 71.63% 43.47% 65.42% 55.09% 84.21% 87.53% 86.13% 35.01% 64.11% 15.04% 53.26% 33.85% 82.44% 76.94% 84.86% 83.44% 89.84% 64.01% 81.77% 71.98% 83.99% 86.20% 96.56% 68.47% 79.76% 43.47% 71.31% 60.55% 86.30% 89.61% 88.28% 40.33% 68.30% 19.07% 57.21% 38.67% 84.83% 79.35% 87.32% 88.26% 93.05% 71.21% 85.24% 77.35% 86.43% 88.71% 97.90% 78.80% 89.61% 61.03% 80.89% 67.56% 89.08% 92.51% 90.79% 47.01% 74.10% 23.11% 62.49% 46.05% 87.76% 81.98% 90.22% 5.8194 3.70% 5.7661 5.22% 6.0000 0.00% 5.8085 3.70% 5.8596 2.50% 5.6126 12.30% 5.5234 15.02% 17.19% 5.4244 5.7099 5.57% 5.5675 9.53% 5.9818 0.32% 5.6006 7.49% 5.7173 5.03% 4.9261 34.05% 38.12% 4.7837 4.8298 35.01% 5.4354 9.41% 5.1235 17.18% 6.0000 0.00% 5.0791 18.42% 5.3971 10.44% 4.5122 35.14% 38.47% 4.3827 41.14% 4.2809 RecBot-GPT, leverages the advanced GPT-4.1 [1] to demonstrate the performance potential of our framework when powered by state-of-the-art closed-source models. Implementation Details. For multi-agent optimization, 4.1.5 we employ Qwen3-14B-Instruct [59] as our backbone large language model, implemented using OpenRLHF [21] and PyTorch [34] frameworks on 8NVIDIA A100 GPUs. We adopt LoRA [20] for Parameter-Efficient Fine-Tuning (PEFT) with rank=32, alpha=32, dropout=0.1, training for 3 epochs with learning rate 5𝑒 6 and batch size 256. User historical interaction sequences are truncated to maximum length of 50 items, and BGE [55] serves as the pre-trained embedding model for all semantic computations. The simulationaugmented knowledge distillation leverages GPT-4.1 [1] as the teacher model to generate training trajectories for fine-tuning."
        },
        {
            "title": "4.2 Offline Experiments\n4.2.1 Overall Performance. The performance comparison results\nacross Single-Round (SR), Multi-Round (MR), and Multi-Round\nwith Interest Drift (MRID) interaction scenarios are presented in",
            "content": "Tables 2, 3, and 4, respectively. These results reveal several compelling findings as follows: (1) Compared to ID-based user interaction modeling, multimodal fusion approaches incorporating textual (and visual) information more effectively capture users fine-grained sequence evolving patterns. Across all datasets and metrics, MoRec and UniSRec significantly outperform SASRec and BERT4Rec. On the Amazon SR task, for example, UniSRec delivers Recall@10 of 0.0370, markedly surpassing SASRec (0.0098). This demonstrates the critical importance of leveraging rich content representations beyond simple ID embeddings for nuanced preference modeling. (2) While traditional recommendation models rely on implicit behavioral signals for preference inference, their inability to incorporate users explicit feedback fundamentally limits timely policy adjustment based on linguistic commands. Command-aware methods achieve superior performance by explicitly matching user instructions with semantically relevant items. Moreover, BGE consistently outperforms BM25 in most cases due to its pre-training on extensive text matching corpora, which facilitates deep semantic Interactive Recommendation Agent with Active User Commands Conference17, July 2017, Washington, DC, USA Table 4: Performance comparison on Multi-Round with Interest Drift (MRID) scenarios. We report results based on Recall (R@K), NDCG (N@K), Condition Satisfaction Rate (C@K), Pass Rate (PR), and Average Rounds (AR) metrics. Best results are highlighted in bold. indicates that higher values are better, while indicates that lower values are better. Amazon (MRID) Method R@10 R@20 R@50 N@10 N@20 N@50 C@10 C@20 C@50 PR AR BM25 BGE GOMMIR InteRecAgent Intruct2Agent RecBot-Qwen (Orig.) RecBot-Qwen (Align.) RecBot-GPT BM25 BGE GOMMIR InteRecAgent Intruct2Agent RecBot-Qwen (Orig.) RecBot-Qwen (Align.) RecBot-GPT BM25 BGE GOMMIR InteRecAgent Intruct2Agent RecBot-Qwen (Orig.) RecBot-Qwen (Align.) RecBot-GPT 0.0424 0.0316 0.0011 0.0511 0.0239 0.0958 0.1056 0.1164 0.0600 0.0974 0.0043 0.1039 0.0578 0.2955 0.3940 0.3158 0.0941 0.0998 0.0000 0.2089 0.0948 0.2396 0.2964 0.2826 0.0544 0.0392 0.0011 0.0783 0.0250 0.1360 0.1371 0. 0.0942 0.1360 0.0064 0.1788 0.0696 0.3683 0.4582 0.4111 0.0941 0.1024 0.0000 0.2526 0.0948 0.2736 0.3432 0.3377 0.0860 0.0555 0.0033 0.1273 0.0272 0.2133 0.1937 0.2688 0.2120 0.2463 0.0171 0.3191 0.1028 0.4743 0.5867 0.5675 0.0941 0.1091 0.0003 0.3103 0.0948 0.3203 0.3953 0.4141 0.0241 0.0198 0.0004 0.0263 0.0165 0.0507 0.0595 0. 0.0270 0.0216 0.0004 0.0333 0.0167 0.0607 0.0673 0.0740 0.0332 0.0248 0.0008 0.0429 0.0172 0.0758 0.0784 0.0972 MovieLens (MRID) 0.0365 0.0551 0.0018 0.0554 0.0375 0.1870 0.2506 0.1945 0.0449 0.0646 0.0024 0.0743 0.0404 0.2055 0.2668 0.2183 Taobao (MRID) 0.0540 0.0561 0.0000 0.1085 0.0543 0.1335 0.1793 0.1622 0.0540 0.0568 0.0000 0.1196 0.0543 0.1422 0.1911 0.1761 0.0679 0.0864 0.0044 0.1018 0.0468 0.2268 0.2922 0.2491 0.0540 0.0581 0.0001 0.1312 0.0543 0.1514 0.2016 0.1914 78.55% 80.65% 50.08% 76.48% 65.26% 77.54% 77.84% 89.13% 62.31% 66.33% 43.47% 65.90% 54.93% 81.96% 84.05% 81.48% 36.25% 37.41% 14.93% 51.94% 23.55% 56.50% 65.00% 72.18% 84.28% 85.14% 60.05% 79.67% 70.97% 81.01% 80.73% 91.68% 69.49% 73.77% 43.47% 71.73% 58.94% 84.53% 85.92% 84.85% 41.32% 42.32% 19.25% 55.52% 26.45% 59.14% 67.80% 75.34% 89.55% 89.59% 65.44% 83.44% 76.69% 84.70% 85.07% 93.51% 80.57% 82.66% 61.03% 80.94% 67.56% 88.06% 88.60% 89.19% 47.47% 49.16% 23.07% 60.41% 29.99% 62.68% 71.10% 79.28% 3.16% 2.72% 0.00% 3.26% 2.29% 6.86% 8.38% 8.49% 5.8564 5.8531 6.0000 5.8335 5.8640 5.8466 5.8292 5.8324 5.7195 5.03% 5.6895 6.64% 5.9818 0.32% 5.6702 7.17% 5.7099 5.35% 5.4111 24.52% 33.51% 5.2591 5.3951 26.02% 9.41% 5.4354 9.68% 5.4308 0.00% 6.0000 16.85% 5.2155 9.44% 5.4348 5.2456 20.42% 25.60% 5.1906 22.92% 5.1869 comprehension that transcends BM25s shallow token-level matching. This performance gap is particularly evident on MovieLens SR, where BGE obtains 0.1370 Recall@10 versus BM25s 0.0257. (3) Existing interactive recommendation agents reveal fundamental architectural limitations in complex scenarios. In specific, GOMMIR exhibits catastrophic failure with near-zero performance due to its overly naive encoding approach through shallow transformers. While InteRec2Agent incorporates most advanced agent design modules including tool invocation and memory mechanisms, it still lacks fine-grained decomposition and parsing capabilities for user commands, hindering its performance in complex interaction scenarios and lacking command-aware multimodal sequential modeling abilities. InstructAgent demonstrates limited capability due to heavy reliance on LLMs intrinsic abilities while lacking domain-specific collaborative knowledge. (4) Our RecBot achieves state-of-the-art performance across all datasets, tasks and metrics, with particularly remarkable efficiency in multi-round scenarios. For example, on Taobao MR, RecBot-GPT achieves 41.14% pass rate with 4.2809 average rounds compared to the InteRecAgent at 18.42% pass rate with 5.0791 rounds. Most intriguingly, our knowledge distillation-based mulit-agent optimization enables student models to surpass their teachers. Specifically, RecBot-Qwen (Align.) achieves 0.3940 Recall@10 and 33.51% pass rate on MovieLens MRID, exceeding its GPT-4.1 teachers 0.3158 Recall@10 and 26.02% pass rate. This phenomenon aligns with previous work [19, 58] demonstrating that focused knowledge transfer can unlock latent capabilities in smaller models, providing both practical deployment advantages and theoretical insights into the untapped potential of student model. 4.2.2 Ablation Study. To further evaluate the contribution of each component within RecBot, we conduct comprehensive ablation experiments by comparing several simplified variants against the complete framework. Specifically, we examine five configurations: (1) V1 employs only the Semantic Relevance Path of the Matcher Tool, relying exclusively on semantic similarity between structured positive user preferences and candidate items; (2) V2 utilizes only the Active-Intent-Aware (AIA) Collaborative Path, leveraging user command-guided interaction feature extraction; (3) V3 incorporates the complete Matcher Tool combined with the Attenuator Tool for comprehensive positive and negative preference modeling; Conference17, July 2017, Washington, DC, USA Jiakai Tang et al. Figure 5: Offline ablation study results on Amazon dataset. All numerical values on axes correspond to percentages (percentage notation is omitted for conciseness). (4) V4 implements simplified BGE-based semantic approach for both positive and negative preferences without personalized knowledge; and (5) Full represents the complete RecBot framework with the entire tool suite including the Filter Tool for hard constraint enforcement. All experiments are conducted using GPT-4.1, with results presented for Single-Round (SR) and Multi-Round (MR) scenarios. Due to space limitations, experimental results under the MRID scenario are omitted, but they yield consistent conclusions. As illustrated in Figure 5, Full RecBot consistently achieves superior performance across both SR and MR scenarios, particularly demonstrating efficiency in MR tasks where it attains higher pass rates while requiring fewer interaction rounds. The results reveal clear performance hierarchy, with V1 exhibiting the weakest performance in most evaluation metrics due to its reliance solely on positive preference semantic matching, which neglects negative feedback signals that constitute the majority of user expressions in real-world scenarios and completely ignores personalized collaborative information. In contrast, V2 and V3 demonstrate substantially improved performance by incorporating user command-aware sequential modeling, with the Active-Intent-Aware collaborative component effectively bridging explicit user commands with implicit behavioral preferences. The progression from V3 to Full RecBot illustrates the additive benefits of more effective tool integration, where the inclusion of the Filter Tool for hard constraint enforcement provides additional performance gains by ensuring strict adherence to user specifications while reducing computational overhead through candidate space pruning. These ablation results conclusively validate the necessity and effectiveness of each component within our proposed tool suite, demonstrating that optimal performance requires the coordinated deployment of all modules to better capture the multifaceted nature of user preferences in interactive contexts."
        },
        {
            "title": "4.3 Online Experiments\n4.3.1 Long-term Performance. To investigate the long-term effec-\ntiveness of the Interactive Recommendation Feed (IRF) paradigm\nand the RecBot framework in real-world deployment scenarios, we",
            "content": "Figure 6: Online performance curves during the three-month A/B testing period. The comparison shows RecBot vs. the base system with all metrics normalized using min-max scaling. Table 5: Online average performance improvement of RecBot over the baseline model during the three-month A/B testing. NFF EICD CICD PV ATC GMV -0.71% +0.88% +1.44% +0.56% +1.28% +1.40% conducted comprehensive online A/B testing over three-month period within leading Asian e-commerce platforms homepage recommendation feed using controlled traffic allocation. Fig. 6 presents the daily performance trajectory comparison between RecBot and the baseline system, while Table 5 summarizes the average metric improvements across the evaluation period. To protect proprietary business information, all metric values in Fig. 6 are applied min-max normalization while preserving the relative changing trends. The experimental results demonstrate substantial improvements across both user experience and business utility dimensions. Most notably, RecBot achieves notable 0.71% reduction in Negative Feedback Frequency (NFF), indicating that the framework effectively identifies user-preferred items through explicit linguistic guidance rather than relying solely on coarse-grained implicit behavioral signal interpretation characteristic of traditional recommendation paradigms. This expected improvement in user satisfaction validates the core hypothesis that enabling direct user-system communication through natural language commands substantially enhances recommendation accuracy and user experience quality. Interactive Recommendation Agent with Active User Commands Conference17, July 2017, Washington, DC, USA Furthermore, RecBot effectively mitigates the information cocoon effect commonly observed in homogeneous recommendation scenarios. The system demonstrates enhanced content diversity with 0.88% and 1.44% improvements in Exposed Item Category Diversity (EICD) and Clicked Item Category Diversity (CICD), respectively. This diversity enhancement reflects the frameworks capability to capture nuanced user preferences and deliver varied content that broadens user exploration beyond historical behavioral patterns. From business utility perspective, RecBot delivers consistent commercial benefits with 0.56%, 1.28%, and 1.40% improvements in Page Views (PV), Add-to-Cart (ATC), and Gross Merchandise Volume (GMV), respectively. These positive business outcomes demonstrate that user-centric controllable recommendations not only enhance user satisfaction but also generate tangible commercial value. The simultaneous improvement in user experience metrics and business performance validates the sustainable nature of the proposed approach. Figure 7: Online performance improvements across different user groups split by historical negative feedback frequency. 4.3.2 User Group Analysis. To further investigate the differential impacts of RecBot across distinct user segments, we conducted detailed user group analysis by partitioning users based on historical negative feedback frequency. As illustrated in Fig. 7, we established controlled experimental conditions where the baseline methods employed traditional option-based negative feedback mechanisms while our methods utilized novel language command interface. The population distributions between baseline and experimental groups remain statistically equivalent, effectively eliminating potential confounding factors arising from demographic biases. The results demonstrate RecBots robust effectiveness across different user groups, achieving consistent NFF reductions ranging from 2.4% to 3.3% for users spanning from low to moderately-high historical negative feedback frequencies. Particularly noteworthy is the [20,50) groups optimal 3.3% improvement, indicating that the natural language manner successfully enables users with varying complaint patterns to articulate preferences more effectively than traditional implicit feedback mechanisms. The consistent improvement validates RecBots broad applicability, though users with extremely high historical negative feedback [100,200) show Table 6: Performance Evaluation of Online RecBots User Command Fulfillment. The LLM-Judge accuracy is validated at 96.5% using human evaluation as ground truth. Evaluation Method Success Rate (%) Human Evaluation LLM-Judge Evaluation 88.9 87.5 1.6% increase in NFF, as these chronically dissatisfied users rarely receive positive reinforcement and represent inherently challenging cases requiring more comprehensive product and algorithmic design beyond recommendation accuracy improvements. 4.3.3 User-Command Fulfillment Analysis. To directly assess the real-world command fulfillment capabilities of the deployed RecBot system, we conducted detailed evaluation using both human expert annotation and an llm-as-a-Judge approach (Qwen3-14BInstruct fine-tuned on expert data). The evaluation focused on whether items in subsequent recommendation feeds strictly adhered to user-specified requirements following system policy adjustments, with each interaction classified as successful (complete requirement satisfaction) or failed (non-compliance). Our analysis encompassed approximately 180,000 online interaction instances. As presented in Table 6, the online RecBot achieved an 88.9% success rate according to human expert evaluation, while the automated LLM-Judge evaluation yielded closely aligned 87.5% success rate with 96.5% consistency compared to human assessments. These results demonstrate that RecBot accurately interprets and responds to natural language user commands in production environments, while our LLM-based evaluation methodology provides reliable and cost-effective alternative to manual assessment for large-scale commercial deployment monitoring. 4.3.4 Case Study. To illustrate RecBots practical effectiveness, we present representative multi-round interaction case from our production deployment as shown in Fig. 8. The user initially receives short skirts but requests long skirts for autumn weather, prompting RecBot to adapt with longer garments. Across subsequent rounds, the user progressively adds constraints: color preference (light blue), budget limitation (around 200), and negative feedback (Dont want floral dresses). RecBot successfully maintains all accumulated preferences while satisfying each new requirement, demonstrating effective command parsing, memory consolidation, and adaptive tool orchestration. The interaction achieves successful convergence in Round 4 with complete user satisfaction (Perfect, love this long dress!), exemplifying how natural language commands enable efficient preference refinement."
        },
        {
            "title": "5 Related Work\n5.1 Interactive Recommendation.\nInteractive recommender systems have emerged as a critical para-\ndigm that enables dynamic adaptation to user preferences through\nreal-time feedback mechanisms, in contrast to traditional static\napproaches that rely solely on historical behavioral patterns. These\nsystems are typically formulated as Markov Decision Processes\n(MDP), where recommendation policies are iteratively updated\nbased on current system states and user responses [3, 9, 49, 52,",
            "content": "Conference17, July 2017, Washington, DC, USA Jiakai Tang et al. combines LLMs as reasoning engines with traditional recommender models as execution tools through modular toolsets including information querying, item retrieval, and ranking functions. However, these approaches are constrained to dialogue-based assistant interfaces, limiting their applicability to mainstream recommendation feed scenarios (e.g., e-commerce homepages, social media feeds) where users lack direct communication channels with the system. Our work addresses this gap by introducing the Interactive Recommendation Feed paradigm, which enables natural language commands within traditional feed-based interfaces, bridging explicit user control with scalable recommendation delivery."
        },
        {
            "title": "6 Conclusion\nIn this paper, we identify fundamental limitations in traditional rec-\nommender systems that constrain user expression through passive\nfeedback paradigms, leading to systematic misalignment between\nuser intentions and system interpretations. To address these chal-\nlenges, we introduce the Interactive Recommendation Feed (IRF)\nparadigm and develop RecBot, a multi-agent framework that en-\nables user-controllable recommendation experiences through natu-\nral language commands. Comprehensive evaluation across three\ndatasets and long-term online deployment validate the effectiveness\nof our proposed approach, achieving substantial improvements in\nboth user satisfaction and business outcomes. In the future, we will\nfocus on developing online learning mechanisms for continuous\nagent evolution through online user feedback, enhancing personal-\nized reasoning capabilities, and extending toward more intelligent\ninteractive recommender systems with proactive anticipation and\nexplanatory capabilities.",
            "content": "Figure 8: Case study of RecBot on production platform. 54, 66]. Recent advances have explored diverse optimization objectives and methodologies: CIRS [10] addresses filter bubble effects through causal reasoning-enhanced offline reinforcement learning to separate intrinsic user interests from over-exposure biases, while BiLLP [41] leverages LLMs planning capabilities for longterm recommendation through bi-level learning mechanisms. With the advancement of large language model agents [47], conversational interactive recommendation has gained prominence through natural language interfaces. InstructAgent [57] introduces an LLMmediated framework that interprets free-text user instructions and performs knowledge-enhanced re-ranking, while InteRecAgent [23] Interactive Recommendation Agent with Active User Commands Conference17, July 2017, Washington, DC, USA References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang. 2020. Controllable multi-interest framework for recommendation. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 29422951. [3] Haokun Chen, Chenxu Zhu, Ruiming Tang, Weinan Zhang, Xiuqiang He, and Yong Yu. 2021. Large-scale interactive recommendation with tree-structured reinforcement learning. IEEE Transactions on Knowledge and Data Engineering 35, 4 (2021), 40184032. [4] Sirui Chen, Yuan Wang, Zijing Wen, Zhiyu Li, Changshuo Zhang, Xiao Zhang, Quan Lin, Cheng Zhu, and Jun Xu. 2023. Controllable multi-objective re-ranking with policy hypernetworks. In Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining. 38553864. [5] Hernani Costa and Luis Macedo. 2013. Emotion-based recommender system for overcoming the problem of information overload. In International conference on practical applications of agents and multi-agent systems. Springer, 178189. [6] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al. 2010. The YouTube video recommendation system. In Proceedings of the fourth ACM conference on Recommender systems. 293296. [7] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. 2025. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849 (2025). [8] Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, et al. 2025. comprehensive survey of self-evolving ai agents: new paradigm bridging foundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407 (2025). [9] Chongming Gao, Kexin Huang, Jiawei Chen, Yuan Zhang, Biao Li, Peng Jiang, Shiqi Wang, Zhong Zhang, and Xiangnan He. 2023. Alleviating matthew effect of offline reinforcement learning in interactive recommendation. In Proceedings of the 46th international ACM SIGIR conference on research and development in information retrieval. 238248. [10] Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang Lei, Biao Li, Yuan Zhang, and Peng Jiang. 2023. CIRS: Bursting filter bubbles by counterfactual interactive recommender system. ACM Transactions on Information Systems 42, 1 (2023), 127. [11] Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. 2025. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046 (2025). [12] Zhaolin Gao, Joyce Zhou, Yijia Dai, and Thorsten Joachims. 2024. End-to-end Training for Recommendation with Language-based User Profiles. arXiv preprint arXiv:2410.18870 (2024). [13] Yingqiang Ge, Xiaoting Zhao, Lucia Yu, Saurabh Paul, Diane Hu, Chu-Cheng Hsieh, and Yongfeng Zhang. 2022. Toward pareto efficient fairness-utility tradeoff in recommendation through reinforcement learning. In Proceedings of the fifteenth ACM international conference on web search and data mining. 316324. [14] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. 2024. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594 (2024). [15] Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. 2025. Model context protocol (mcp): Landscape, security threats, and future research directions. arXiv preprint arXiv:2503.23278 (2025). [16] Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley. 2024. Bridging Language and Items for Retrieval and Recommendation. arXiv preprint arXiv:2403.03952 (2024). [17] Yupeng Hou, Jiacheng Li, Ashley Shin, Jinsung Jeon, Abhishek Santhanam, Wei Shao, Kaveh Hassani, Ning Yao, and Julian McAuley. 2025. Generating long semantic IDs in parallel for recommendation. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2. 956966. [18] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards universal sequence representation learning for recommender systems. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining. 585593. [19] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301 (2023). [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. Lora: Low-rank adaptation of large language models. ICLR 1, 2 (2022), 3. [21] Jian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran Wang, Hao Chen, Bin Chen, et al. 2024. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143 (2024). [22] Jiani Huang, Shijie Wang, Liang-bo Ning, Wenqi Fan, Shuaiqiang Wang, Dawei Yin, and Qing Li. 2025. Towards Next-Generation Recommender Systems: Benchmark for Personalized Recommendation Assistant with LLMs. arXiv preprint arXiv:2503.09382 (2025). [23] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. 2025. Recommender ai agent: Integrating large language models for interactive recommendations. ACM Transactions on Information Systems 43, 4 (2025), 133. [24] Olivier Jeunen. 2023. probabilistic position bias model for short-video recommendation feeds. In Proceedings of the 17th ACM conference on recommender systems. 675681. [25] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM). IEEE, 197206. [26] Shah Khusro, Zafar Ali, and Irfan Ullah. 2016. Recommender systems: issues, challenges, and research opportunities. In Information science and applications (ICISA) 2016. Springer, 11791189. [27] Xiaopeng Li, Fan Yan, Xiangyu Zhao, Yichao Wang, Bo Chen, Huifeng Guo, and Ruiming Tang. 2023. Hamur: Hyper adapter for multi-domain recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 12681277. [28] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. 2025. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419 (2025). [29] Chia-Yu Lin, Li-Chun Wang, and Kun-Hung Tsai. 2018. Hybrid real-time matrix factorization for implicit feedback recommendation systems. Ieee Access 6 (2018), 2136921380. [30] Siyi Lin, Sheng Zhou, Jiawei Chen, Yan Feng, Qihao Shi, Chun Chen, Ying Li, and Can Wang. 2024. ReCRec: Reasoning the causes of implicit feedback for debiased recommendation. ACM Transactions on Information Systems 42, 6 (2024), 126. [31] Wensheng Lu, Jianxun Lian, Wei Zhang, Guanghua Li, Mingyang Zhou, Hao Liao, and Xing Xie. 2024. Aligning large language models for controllable recommendations. arXiv preprint arXiv:2403.05063 (2024). [32] Sheshera Mysore, Mahmood Jasim, Andrew McCallum, and Hamed Zamani. 2023. Editable user profiles for controllable text recommendations. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 9931003. [33] Qian Pan, Zahra Ashktorab, Michael Desmond, Martin Santillan Cooper, James Johnson, Rahul Nair, Elizabeth Daly, and Werner Geyer. 2024. Human-Centered Design Recommendations for LLM-as-a-judge. arXiv preprint arXiv:2407.03479 (2024). [34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019). [35] Avinash Patil and Aryan Jadon. 2025. Advancing reasoning in large language models: Promising methods and approaches. arXiv preprint arXiv:2502.03671 (2025). [36] Emiliano Penaloza, Olivier Gouvert, Haolun Wu, and Laurent Charlin. 2024. TEARS: Textual Representations for Scrutable Recommendations. arXiv preprint arXiv:2410.19302 (2024). [37] Partha Pratim Ray. 2025. survey on model context protocol: Architecture, state-of-the-art, challenges and future directions. Authorea Preprints (2025). [38] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019). [39] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval 3, 4 (2009), 333389. [40] Chenglei Shen, Xiao Zhang, Wei Wei, and Jun Xu. 2023. Hyperbandit: Contextual bandit with hypernewtork for time-varying user preferences in streaming recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 22392248. [41] Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, and Fuli Feng. 2024. Large language models are learnable planners for long-term recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 18931903. [42] Brent Smith and Greg Linden. 2017. Two decades of recommender systems at Amazon. com. Ieee internet computing 21, 3 (2017), 1218. [43] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. 2025. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419 (2025). [44] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management. 14411450. Conference17, July 2017, Washington, DC, USA Jiakai Tang et al. 18931902. [66] Sijin Zhou, Xinyi Dai, Haokun Chen, Weinan Zhang, Kan Ren, Ruiming Tang, Xiuqiang He, and Yong Yu. 2020. Interactive recommender system via knowledge graph-enhanced reinforcement learning. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 179 188. [67] Yongchun Zhu, Jingwu Chen, Ling Chen, Yitan Li, Feng Zhang, and Zuotao Liu. 2024. Interest clock: Time perception in real-time streaming recommendation system. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 29152919. Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009 [45] Jiakai Tang, Sunhao Dai, Teng Shi, Jun Xu, Xu Chen, Wen Chen, Jian Wu, and Yuning Jiang. 2025. Think before recommend: Unleashing the latent reasoning power for sequential recommendation. arXiv preprint arXiv:2503.22675 (2025). [46] Jiakai Tang, Shiqi Shen, ZhipengWang ZhipengWang, Gong Zhi, Xueyang Feng, Zexu Sun, Haoran Tan, and Xu Chen. 2025. KAPA: Deliberative Agent Framework with Tree-Structured Knowledge Base for Multi-Domain User Intent Understanding. In Findings of the Association for Computational Linguistics: ACL 2025. 61506166. [47] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. survey on large language model based autonomous agents. Frontiers of Computer Science 18, 6 (2024), 186345. [48] Wenjie Wang, Fuli Feng, Liqiang Nie, and Tat-Seng Chua. 2022. User-controllable recommendation against filter bubbles. In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval. 1251 1261. [49] Xinxi Wang, Yi Wang, David Hsu, and Ye Wang. 2014. Exploration in interactive personalized music recommendation: reinforcement learning approach. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 11, 1 (2014), 122. [50] Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, and Xiao Wang. 2024. Can small language models be good reasoners for sequential recommendation?. In Proceedings of the ACM Web Conference 2024. 38763887. [51] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models with graph augmentation for recommendation. In Proceedings of the 17th ACM international conference on web search and data mining. 806815. [52] Yaxiong Wu, Craig Macdonald, and Iadh Ounis. 2021. Partially observable reinforcement learning for dialog-based interactive recommendation. In Proceedings of the 15th ACM conference on recommender systems. 241251. [53] Yaxiong Wu, Craig Macdonald, and Iadh Ounis. 2023. Goal-oriented multi-modal interactive recommendation with verbal and non-verbal relevance feedback. In Proceedings of the 17th ACM Conference on Recommender Systems. 362373. [54] Yaxiong Wu, Craig Macdonald, and Iadh Ounis. 2024. Personalised multi-modal interactive recommendation with hierarchical state representations. ACM Transactions on Recommender Systems 2, 3 (2024), 125. [55] Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th international ACM SIGIR conference on research and development in information retrieval. 641649. [56] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. 2025. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686 (2025). [57] Wujiang Xu, Yunxiao Shi, Zujie Liang, Xuying Ning, Kai Mei, Kun Wang, Xi Zhu, Min Xu, and Yongfeng Zhang. 2025. Instructagent: Building user controllable recommender via llm agent. arXiv e-prints (2025), arXiv2502. [58] Wujiang Xu, Qitian Wu, Zujie Liang, Jiaojiao Han, Xuying Ning, Yunxiao Shi, Wenfang Lin, and Yongfeng Zhang. 2024. SLMRec: Distilling large language models into small for sequential recommendation. arXiv preprint arXiv:2405.17890 (2024). [59] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [60] Chao Yi, Dian Chen, Gaoyang Guo, Jiakai Tang, Jian Wu, Jing Yu, Sunhao Dai, Wen Chen, Wenjun Yang, Yuning Jiang, et al. 2025. RecGPT Technical Report. arXiv preprint arXiv:2507.22879 (2025). [61] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. 2023. Where to go next for recommender systems? idvs. modality-based recommender models revisited. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 26392649. [62] Tao Zhang, Luwei Yang, Zhibo Xiao, Wen Jiang, and Wei Ning. 2024. On practical diversified recommendation with controllable category diversity framework. In Companion Proceedings of the ACM Web Conference 2024. 255263. [63] Xiaoyu Zhang, Ruobing Xie, Yougang Lyu, Xin Xin, Pengjie Ren, Mingfei Liang, Bo Zhang, Zhanhui Kang, Maarten de Rijke, and Zhaochun Ren. 2024. Towards empathetic conversational recommender systems. In Proceedings of the 18th ACM Conference on Recommender Systems. 8493. [64] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems 36 (2023), 4659546623. [65] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM international conference on information & knowledge management."
        }
    ],
    "affiliations": [
        "Alibaba Group, Beijing, China",
        "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China",
        "University of Chinese Academy of Sciences, China"
    ]
}