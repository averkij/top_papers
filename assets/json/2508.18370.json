{
    "paper_title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo",
    "authors": [
        "Terry Yue Zhuo",
        "Dingmin Wang",
        "Hantian Ding",
        "Varun Kumar",
        "Zijian Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems."
        },
        {
            "title": "Start",
            "content": "TRAINING LANGUAGE MODEL AGENTS TO FIND VULNERABILITIES WITH CTF-DOJO Terry Yue Zhuo1,2 Dingmin Wang2 Hantian Ding2 Varun Kumar2 Zijian Wang2 1 Monash University"
        },
        {
            "title": "AWS AI Labs",
            "content": "terry.zhuo@monash.edu {wdimmy, dhantian, kuvrun, zijwan}@amazon.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-DOJO, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-FORGE, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-DOJO, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as benchmark for executableagent learning, CTF-DOJO demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems. 5 2 0 2 5 2 ] . [ 1 0 7 3 8 1 . 8 0 5 2 : r Figure 1: CTF-FORGE powers automated creation of configuration files from publicly sourced CTF artifacts for containerizing CTF challenges. Work done during an internship at Amazon."
        },
        {
            "title": "INTRODUCTION",
            "content": "Advanced cybersecurity necessitates the ongoing analysis of increasingly complex software systems. As globally connected infrastructures expand, their attack surfaces expand as well, making traditional manual security analysis insufficient for timely vulnerability identification and remediation. This urgency has spurred major research efforts, such as the DARPA Cyber Grand Challenge (Song & Alves-Foss, 2015) and DARPA AIxCC (DARPA, 2024), which focus on building autonomous systems capable of discovering and validating software flaws. In this context, Capture The Flag (CTF) competitions have emerged as the de facto benchmark for evaluating the cybersecurity reasoning abilities of machine learning models, demanding advanced, multi-step adversarial strategies to uncover system vulnerabilities and retrieve hidden flags (Anthropic, 2025a; xAI, 2025; OWASP GenAI Project (CTI Layer Team), 2025). Previous works have demonstrated promising results in applying large language model (LLM) agents to CTF challenges (Hurst et al., 2024; Jaech et al., 2024; Anthropic, 2025b; Abramovich et al., 2025), with systems like ENIGMA (Abramovich et al., 2025) achieving substantial progress on complex security tasks. While these approaches enable frontier proprietary models to achieve strong performance, they fail short when applied to open-source LLMs due to the lack of agentic training data. Recently, Zhuo et al. (2025) shows that training on thousands of synthetic agent trajectories can close the gap between proprietary and open-source LLMs. However, synthesizing large number of long-horizon trajectories from teacher models requires substantial computational resources, limiting generalization under budget constraints. Moreover, the validity of synthetic trajectories is hard to verify without runtime environments, limiting their reliability for training in high-stakes, safety-critical domains. To address these limitations, we present CTF-DOJO, the first execution environment that contains hundreds of fully functional CTF challenges in secure Docker containers. CTF-DOJO leverages CTF artifacts (e.g., challenge descriptions and files to reproduce each challenge) from pwn.college, public archive developed by Arizona State University for hands-on cybersecurity education, now used in 145 countries and actively maintained by team of professors and students. However, setting up the runtime environment for CTF challenges is extremely difficult for non-professionals and can take up to an hour per task even for experienced practitioners (documented Section 2). To eliminate this bottleneck, we propose CTF-FORGE (Figure 1), an automated pipeline that leverages LLMs to create hundreds of Docker images for CTF-DOJO within minutes, achieving over 98% success rate through manual validation. During trajectory collection from multiple LLMs within CTF-DOJO, we found that weaker models struggle to solve CTF challenges independently (detailed in Section 4.1). To improve yield rates, we collect diverse CTF writeups from CTFtime1 and incorporated them as inference-time hints. Although we notice that only 23% of the CTF-DOJO challenges matches at least one writuep, we empirically find that such writeup content, when available, can significantly boost the success rate of LLMs up to 64% relatively gains. Notably, while building these environments, CTF-DOJO uncovered four bugs from the existing pwn.college collection2. Models trained on CTF-DOJO trajectories achieve open-weight state-of-the-art performance on over 300 tasks across three established CTF benchmarks. Through the extensive analysis, we identify three key findings for building effective cybersecurity agents: (1) writeups are crucial for training, particularly when working with data generated by weak models, (2) augmenting the runtime environment (e.g., server domains and flags) helps models yield more solved more CTF challenges, and (3) employing diverse teacher LLMs in CTF-DOJO leads to better task diversity and stronger performance. We hope our insights from the proposed CTF-DOJO can shed light on the future development of cybersecurity agents. Our work provides following contributions: We introduce CTF-DOJO, the first large-scale, execution-ready environment for cybersecurity agent training, offering hundreds of verified CTF challenges in isolated Docker containers. We propose CTF-FORGE, scalable pipeline that leverages LLMs to automate the generation of Docker-based runtime environments, achieving over 98% success rate through manual validation. 1https://ctftime.org/ 2We have filed issues in their official repository. 2 Table 1: CTF-DOJO is the first cybersecurity executable environment deriving agent trajectories for training. Detection: whether the task requires vulnerability detection; exploitation: whether the task needs LLMs to verify the detected vulnerabilities; Agentic: whether each instance is repaired with an interactive environment for exploitation; Real Task: whether each instance is developed by human experts. Executable Environment Detection Exploitation Agentic Real Task # Total # Train SecRepoBench (Dilgren et al., 2025) CVE-Bench (Wang et al., 2025a) CVE-Bench (Zhu et al., 2025) SEC-bench (Lee et al., 2025) CyberGym (Wang et al., 2025b) CyberSecEval 3 (Wan et al., 2024) SecCodePLT (Yang et al., 2024b) InterCode-CTF (Yang et al., 2023) NYU CTF Bench (Shao et al., 2024) Cybench (Zhang et al., 2025b) BountyBench (Zhang et al., 2025a) CTF-DOJO (Ours) 318 509 1,507 1,507 6 1,345 100 200 40 40 658 0 0 0 0 0 0 0 0 0 0 658 We conduct thorough analysis through extensive ablation studies, identifying key factors that influence agent performance, including the presence of hint-guided trajectory collection, runtime environment augmentation, and teacher model diversity."
        },
        {
            "title": "AGENTS",
            "content": "CTF-DOJO is the first environment designed to synthesize verified agent trajectories for training LLMs on offensive cybersecurity tasks involving vulnerability detection and exploitation. As shown in Table 1, existing cybersecurity execution environments either lack agentic task instance or are not designed for training purposes, creating critical gap in the development of capable security agents. Inspired by the success of trajectory-based learning in software engineering agents (Jimenez et al., 2024; Yang et al., 2024a), CTF-DOJO adapts this paradigm to cybersecurity by sourcing publicly available CTF artifacts and transforming them into executable and interactive environments. Different from prior pipelines for software engineering tasks (Pan et al., 2024; Xie et al., 2025; Yang et al., 2025b), which often require human effort or complex multi-agent systems to construct Docker environments, our approach is lightweight and fully automated. Towards that end, we introduce CTF-FORGE, pipeline that automatically builds Docker containers for CTF-DOJO. While manual setup can take up to an hour per challenge even for experts3, CTF-FORGE completes each container in 0.5 seconds on average, reducing weeks of total setup time to just minutes. 2.1 SOURCE DATA COLLECTION We begin by surveying CTF collections that offer diverse challenges from CTF competitions. During our initial exploration, we determine few candidates: (1) Sajjadiums CTF Archives4, (2) r3kapigs Notion5, (3) CryptoHack CTF Archive6, (4) archive.ooo7, and (5) pwn.colleges CTF Archive8. However, most of these collections suffer from inconsistent maintenance, lack standardization across challenge formats, or are limited to specific categories (e.g., CryptoHack focuses solely on cryptography). We determine that pwn.colleges CTF Archive is not only free of these issues but 3This has been attempted by one of the authors. 4https://github.com/sajjadium/ctf-archives 5https://r3kapig-not1on.notion.site 6https://cryptohack.org/challenges/ctf-archive/ 7https://archive.ooo/ 8https://github.com/pwncollege/ctf-archive Table 2: Challenge distribution across CTF datasets. Benchmark Level # Competition # Crypto # Forensics # Pwn # Rev # Web # Misc # Total CTF-DOJO Multi-Level InterCode-CTF NYU CTF Bench Cybench High School University Professional 50 1 1 4 Training 228 Evaluation 16 53 16 38 13 15 4 123 2 38 2 27 51 6 21 2 19 8 31 24 4 658 91 192 40 additionally provides brief information about the steps to reproduce each CTF challenge. Table 2 shows the distribution of 658 CTF challenges (as of 2025/07) after decontaminating any tasks from evaluation benchmarks, demonstrating the diversity of CTF instances across different categories and competition events hosted between 2011 and 2025. CTF challenges employ two primary flag-handling mechanisms. The first type uses predefined flags, hashed with SHA-256 and verified through provided binary executable (e.g., flagCheck) that confirms submission correctness. Since these flags were manually captured and encoded, they are subject to occasional errors (see 4 identified bugs in Appendix E). The second type relies on dynamic flag generation, where the correct flag is generated at runtime and stored in system path such as /flag. In those challenges, participants must verify the system during execution to retrieve or compute the correct flag, rather than match against static value. 2.2 CTF-FORGE: AUTOMATIC ENVIRONMENT CREATION FOR CTF CHALLENGES Figure 1 illustrates CTF-FORGE, pipeline employing DeepSeek-V3-0324 to generate environments and metadata for CTF runtime. After we source the CTF artifacts from pwn.colleges CTF Archive, we design set of prompts to instruct LLMs to generate the compulsory files for Docker images in multiple stages. First, we determine whether the CTF challenge requires containerized server to interact with. Such servers are typically needed for web challenges, binary exploitation challenges, and cryptography challenges that provide interactive services. The pipeline automatically detects server requirements by analyzing the presence of flag verification files (SHA256 checksums or check scripts) and challenge descriptions. For existing CTF runtime, we can categorize them into several challenge types: 1) Web challenges that require web servers (Apache/Nginx) to serve PHP, Python, or Node.js applications; 2) Binary exploitation challenges that need socat to host binary services on port 1337 with appropriate library dependencies; 3) Cryptography challenges that may require Python runtime environments for cryptographic services; 4) Reverse engineering challenges providing downloadable binaries and potentially analysis services; and 5) Forensics challenges offering evidence files for offline analysis. The pipeline employs category-specific guidelines and adaptive Docker setup strategies to handle different architectures (32-bit vs 64-bit), library dependencies, and runtime environments. For each challenge type, CTF-FORGE generates appropriate Dockerfiles with proper base images, package installations, file copying, and service configurations, then produces docker-compose.yml files for orchestration and challenge.json metadata files that describe the challenge structure and provide flag verification mechanisms. 2.3 BUILDING SUSTAINABLE ENVIRONMENT FOR CYBERSECURITY AGENTS To ensure CTF-DOJO serves as robust foundation for long-term research on autonomous cybersecurity agents, we emphasize sustainability across two dimensions: reliability and scalability. Reliability To ensure the reliability of the CTF environments created via CTF-FORGE, we implement an automated validation script that performs two critical checks: (1) whether the Docker containers can be successfully built and executed without errors, and (2) whether the CTF services inside the containers respond correctly to network communication on the expected ports. We run CTF-FORGE three times independently on all 658 CTF challenges to evaluate consistency and determinism. Across these runs, 98% (650) of the challenges consistently pass all checks, demonstrating high reliability of the pipeline in producing stable, executable environments for cybersecurity agents. 4 Additionally, we sample 10% of the built CTF tasks and manually test the executables within each runtime to verify expected behavior. Scalability While CTF-DOJO currently contains fewer instances than existing software engineering environments that covers thousands of instances (Pan et al., 2024; Xie et al., 2025; Yang et al., 2025b), each CTF challenge environment is uniquely designed, mimicking diverse real-world software systems rather than variations of single codebase that is common in SWE tasks. To enhance scalability over time, CTF-DOJO builds on the actively growing CTF collections from the pwn.college community. As new challenges are added, CTF-FORGE can continuously and automatically convert them into interactive environments with minimal manual effort, enabling CTF-DOJO to scale organically alongside community-driven CTF development."
        },
        {
            "title": "2.4 TRAINING DATA CONSTRUCTION",
            "content": "We introduce data pipeline to produce large corpus of high-quality, multi-turn interaction traces from CTF-DOJO. This process supports the development of CTF-solving agents that require diverse, realistic demonstrations of iterative security problem-solving behavior. Agent Scaffold We build on ENIGMA+ (Zhuo et al., 2025), recently introduced agent scaffold designed for scalable and consistent evaluation of agents on cybersecurity tasks. ENIGMA+ extends the original ENIGMA framework to better support cybersecurity environments by incorporating interactive tools for debugging and remote server interaction. Notably, ENIGMA+ improves evaluation efficiency by executing tasks in parallel using isolated Docker containers, reducing runtime from days to hours for large-scale experiments. It also enables the control of agent interactions based on the number of interaction steps (e.g., 40 turns) rather than monetary cost, which aligns with best practices in agent evaluation. Additionally, it replaces ENIGMAs context-heavy summarization module with with lightweight alternative better suited for binary analysis outputs. Within this scaffold, we integrate the CTF-DOJO environment and collect agent trajectories through structured interactions. Trajectory Collection Within the ENIGMA+ scaffold, we deploy DeepSeek-V3-0324 to attempt solving CTF challenges in CTF-DOJO with temperature of 0.6, top-p of 0.95, and rollout count of 6. For each challenge instance, the agent is given the original task description and interactive access to the containerized environment, capped at 40 turns. We log every system command, intermediate output, and reasoning step until either the flag is captured or the turn budget is exhausted. Successful trajectories are stored in structured JSON format for downstream filtering and training. Our initial large-scale runs reveal that many trajectories stall due to brittle exploitation strategies or failure to discover the correct toolchain. While some challenges yield multiple successful runs, large fraction remain unsolved or are solved only rarely, leading to skewed dataset concentrated on limited tasks. Inference-Time Bag of Tricks To increase the yield rate of successful trajectories on CTF challenges, we introduce two inference-time techniques (analyzed in Section 4). First, we leverage publicly available CTF writeups to provide task-specific hints to LLMs. Specifically, we collect 8,361 writeups and apply fuzzy matching to align them with challenges in CTF-DOJO. This yields 252 matched writeups, covering 150 challenges with at least one relevant writeup. During preprocessing, we redact any potential flag values from the writeups and incorporate the cleaned content into the task prompt, as the direct answers may lead to the shortcut learning (Geirhos et al., 2020). We explicitly instruct the LLM to treat the writeup as source of inspiration, using its strategies and reasoning implicitly without direct referencing. To ensure the integrity of downstream evaluation, we remove all writeup content from collected trajectories after inference. Second, we augment the CTF runtime per agent rollout via CTF-FORGE by introducing randomized environment configurations. These augmentations include varying port numbers, modifying file system paths, injecting non-functional distractor code, and adjusting system-level metadata such as timestamps and installed packages. While preserving the core logic and solvability of each challenge, these perturbations reduce overfitting to static runtime cues and encourage agents to develop more generalizable exploitation strategies. They also help mitigate persistent misconfigurations introduced by LLMs. By resetting the runtime with diverse settings, the environment is more likely to land in valid configuration that enables flag discovery, even if previous runs failed due to deterministic setup errors. For challenges with dynamic 5 Figure 3: Number of turns in each successful trajectory (left) and number of successful trajectories for each challenge instance (right). flag generation, we re-seed the container environments at each rollout to ensure unique flag instances per interaction, further enriching training data diversity. Data Analysis We employ two models, Qwen3-Coder (Yang et al., 2025a) and DeepSeek-V3-0324 (Liu et al., 2024), to analyze the composition and characteristics of the raw 1,006 successful trajectories across multiple runs to better understand the coverage and difficulty distribution within CTF-DOJO. Figure 2 shows the category distribution across solved 274 challenges, where cryptography tasks constitute the largest portion, followed by reverse engineering, and miscellaneous categories. This distribution reflects the typical emphasis in modern CTFs on cryptographic reasoning and binary analysis. Figure 3 presents two key statistics of the collected data. The left panel visualizes the number of assistant turns per trajectory. The majority of trajectories fall between 5 to 15 turns, with heavy tail extending to 40 turns. This skew indicates that while many tasks can be solved efficiently, substantial portion demands prolonged, iterative explorations, highlighting the complex nature of real-world CTF problems. The right panel plots the number of successful trajectories obtained for each challenge, revealing that many challenges are solved only once within the total 12 rollouts, indicating that successful trajectories for certain instances are difficult to collect. Figure 2: solved CTF challenges. Breakdown of"
        },
        {
            "title": "3 TRAINING LLMS AS CYBERSECURITY AGENTS WITH CTF-DOJO",
            "content": "With CTF-DOJO, we train cybersecurity agents with various base models. Our primary objective is to establish strong baselines and demonstrate the effectiveness of training data derived from execution. We use Pass@k (Chen et al., 2021) as our main evaluation metric. Similar to Pan et al. (2024), we employ simple policy improvement algorithm: rejection sampling fine-tuning, where we fine-tune the model on trajectories successfully capturing flags inside CTF-DOJO. In addition, we apply sample capping of 2 per solved CTF challenges to avoid bias towards easy tasks, following Pan et al. (2024) and Yang et al. (2025b). We finally collect 486 trajectories from the 274 CTF challenges solved by Qwen3-Coder and DeepSeek-V3-0324 (see Table 7). 3.1 EXPERIMENT SETUP Training We fine-tuned Qwen3 models at three scales: 7B, 14B, and 32B (Yang et al., 2025a). All models undergo supervised fine-tuning via NVIDIA NeMo framework (Kuchaiev et al., 2019). Due to computational constraints, we only retain synthesized samples within 32,768 tokens, resulting in 486 trajectories. The hyperparameters are consistently set as the global batch size of 16, the learning rate of 5e-6, and the epoch of 2. 6 Table 3: Pass@1 performance on benchmark tasks. The improvements of CTF-DOJO are absolute in comparison with the Qwen3 model of corresponding sizes. Model Train Size InterCode-CTF NYU CTF Cybench Average Proprietary Models 86.8 85.7 81. Open Weight Models Claude-3.7-Sonnet (Anthropic, 2025a) Claude-3.5-Sonnet (Anthropic, 2024) Gemini-2.5-Flash (Comanici et al., 2025) DeepSeek-V3-0324 (Liu et al., 2024) Kimi-K2 (Team et al., 2025) Qwen3-Coder (Yang et al., 2025a) Qwen2.5-Coder-7B-Instruct (Hui et al., 2024) Qwen2.5-Coder-14B-Instruct (Hui et al., 2024) Qwen2.5-Coder-32B-Instruct (Hui et al., 2024) Qwen3-8B (Yang et al., 2025a) Qwen3-14B (Yang et al., 2025a) Qwen3-32B (Yang et al., 2025a) Cyber-Zero-8B (Zhuo et al., 2025) Cyber-Zero-14B (Zhuo et al., 2025) Cyber-Zero-32B (Zhuo et al., 2025) CTF-DOJO-8B (Ours) CTF-DOJO-14B (Ours) CTF-DOJO-32B (Ours) - - - - - - - - - - - - 9,464 9,464 9,464 486 486 486 82.5 72.5 70.3 34.1 44.0 68.1 46.5 55.0 60.0 64.8 73.6 82.4 18.2 16.7 14.1 6.2 4.7 5.7 2.0 3.1 4.7 0.8 2.6 4. 6.3 9.9 13.5 30.0 25.0 17.5 27.5 15.0 10.0 0.0 5.0 10.0 5.0 12.5 5.0 10.0 20.0 17.5 39.0 37.2 33.4 30.3 25.1 24.5 10.8 14.9 23.2 14.2 18.6 20. 23.2 29.1 33.4 53.8 (7.3% ) 71.4 (16.4% ) 83.5 (23.5% ) 4.2 (3.4% ) 5.7 (3.1% ) 10.4 (5.7% ) 10.0 (5.0% ) 17.5 (5.0% ) 17.5 (12.5% ) 18.9 (4.7% ) 25.7 (7.1% ) 31.9 (11.6% ) Evaluation Scaffolding We use ENIGMA+, an enhanced version of the ENIGMA scaffold with several key improvements for large-scale cybersecurity evaluation. ENIGMA+ executes evaluation tasks in parallel, significantly improving efficiency. Following Zhuo et al. (2025), we cap each rollout at 40 interaction turns, replacing ENIGMAs cost-based budget (Yang et al., 2024a) to ensure consistent evaluation across models. We also adopt the Simple Summarizer to prevent context overflows from verbose outputs like binary decompilation. Test Benchmarks We evaluate agents on three established CTF benchmarks detailed in Table 2: InterCode-CTF benchmark comprises 100 CTF challenges collected from picoCTF, an online educational platform for high-school rated CTF challenges. NYU CTF Benchmark contains 200 CTF challenges from CSAW competitions (2017-2023), representing university-level difficulty. Cybench benchmark includes 40 CTF challenges collected from four distinct professional competitions: HackTheBox, Sekai CTF, Glacier and HKCert (2022-2024). These benchmarks collectively span six challenge categories: Cryptography, Forensics, Binary exploitation, Reverse-Engineering, Miscellaneous, and Web. For evaluation, we deploy each LLM within the agent scaffold with access to the Linux Bash terminal. 3.2 RESULT ANALYSIS We evaluate all LLMs with the Pass@1 metric, where we sample one trajectory per task and validate whether the model captures the correct flag. Table 3 presents performance comparisons between zero-shot and fine-tuned models across all benchmarks. CTF-DOJO training enables efficient vulnerability exploitation. Our results show that CTFDOJO-fine-tuned models achieve performance comparable to Cyber-Zero while requiring 94.9% fewer training trajectories (486 vs. 9,464). Both approaches fine-tune on Qwen3 backbones, yet CTF-DOJO relies solely on compact set of successful CTF trajectories. For instance, CTFDOJO-32B reaches an average Pass@1 of 31.9%, approaching Cyber-Zero-32Bs 33.4%. Similarly, CTF-DOJO-14B achieves 25.7% versus 29.1% for Cyber-Zero-14B, and CTF-DOJO-8B attains 18.9% compared to Cyber-Zero-8Bs 23.2%. These results highlight that CTF-DOJO offers highly data-efficient alternative: competitive performance can be attained without massive-scale training. Notably, CTF-DOJO-trained models also begin to rival frontier systems such as Claude-3.5-Sonnet (37.2%), underscoring the practical feasibility of training capable cybersecurity agents at modest cost. 7 Scaling training data improves the performance linearly. Figure 4 shows the impact of increasing training trajectories on Pass@1 performance across different model sizes. All model variants (8B, 14B, 32B) demonstrate clear and consistent performance gains as training trajectories increase. Notably, the 32B model improves from 22.0% to 31.9% Pass@1 from 0 to 486 trajectories, demonstrating nearly linear performance scaling with data. This trend confirms that even modestly sized datasets can substantially enhance capability in cybersecurity tasks. Larger models not only start from higher baselines but also benefit more from additional supervision, highlighting the synergistic effect of scale and verified data in training paradigm."
        },
        {
            "title": "4 ABLATIONS ON CTF-DOJO DATA",
            "content": "Figure 4: Effect of data scaling. Models across sizes benefit from increased number of training trajectories. To better understand the components contributing to CTF-DOJOs effectiveness, we conduct ablation studies across three axes: external writeups as inference-time hints, runtime augmentation during data collection, and teacher model diversity. These experiments reveal the impact of key design choices and identify practical strategies for enhancing agent performance in cybersecurity environments. 4.1 WRITUPS AS HINTS Table 4: Solved rate (%) on CTF-DOJO tasks across categories, using ENIGMA+. indicates baseline without writeup hints; + includes writeups in the prompt. Models # Crypto # Forensics # Pwn # Rev # Web # Misc + + + + + + # Total + Proprietary Models Claude-3.7-Sonnet Claude-3.5-Sonnet 41.2 39.9 50.9 43. 42.1 39.5 50.0 47.4 14.7 8.0 20.9 13.5 41.5 39.8 49.6 41. 61.9 47.6 76.2 57.1 47.1 45.9 69.4 68.2 36.2 33.0 46.4 39. Open Weight Models DeepSeek-V3-0324 Qwen3-Coder Qwen3-32B Qwen3-14B 37.1 31.4 21.9 14.0 41.0 42.8 29.4 25.9 41.0 35.9 7.9 5.3 43.6 38.5 18.4 10. 12.0 7.9 1.8 1.8 13.5 9.1 6.7 4.9 34.1 26.8 22.8 20.3 36.6 39.8 28.5 25.2 33.3 23.8 9.5 9.5 52.4 28.6 23.5 14. 36.5 24.7 31.8 24.7 41.2 37.6 41.2 40.0 30.4 23.9 17.2 12.9 33.9 32.5 24.3 21.1 Setup To assess the value of incorporating external CTF writeups during data collection, we conduct controlled ablation on CTF-DOJO challenges. We compare two settings: (1) No-Hint (-), where models receive only the original challenge description, and (2) With-Hint (+), where one redacted matched writeups is randomly chosen to prepend to the prompt as non-referential hint for the corresponding challenge. All other settings remain constant with the main experiments. Analysis As shown in Table 4, writeup-based hints consistently improve the number of solved tasks across all models and challenge categories. On average, the number of solved challenges increases by 7.4%, from 168 (No-Hint) to 217 (With-Hint), underscoring the utility of public writeups for improving the yield rate of training trajectories. This effect is particularly pronounced in the Crypto, Reverse Engineering, and Miscellaneous categories where solution strategies often rely on reusable heuristics or canonical exploration workflows. This finding suggests that writeups can serve as rich reservoir of domain-specific knowledge, allowing models to bootstrap strategic reasoning and explore more promising solution paths. We believe the effectiveness of inference-time hints can generalize to various agent tasks like solving GitHub issues (Jimenez et al., 2024), where more diverse data can be distilled from LLMs to train stronger agentic models"
        },
        {
            "title": "4.2 AUGMENTING CTF RUNTIMES",
            "content": "Setup To evaluate the effect of runtime augmentation on agent performance, we compare two settings for environment construction: (1) Static, where each CTF instance uses fixed runtime parameters, and (2) Augmented, where we introduce perturbations such as randomized port numbers, file path shuffling, distractor code injection, and dynamic flag regeneration. We run both Qwen3Coder and DeepSeek-V3-0324 across 1 to 4 agent rollouts and count the number of unique CTF challenges successfully solved at least once under each setting. We keep all rollout and decoding hyperparameters identical across both variants to isolate the impact of augmentation. Analysis Figure 5 shows that augmented environments consistently yield more solved tasks across all rollout counts and both models. For example, Qwen3-Coder solves 211 challenges under augmentation at rollout 4, relative improvement of 24.9% compared to only 169 under static runtimes. Similarly, DeepSeek-V3-0324 improves from 156 to 217 solved tasks with augmentation at rollout 4. The performance gap widens with more rollouts, suggesting that augmentation amplifies agent exploration and generalization as more interactions are permitted. These results confirm that runtime diversity prevents brittle overfitting to environment artifacts and encourages the development of more robust, transferable strategies for flag capture. 4.3 DIVERSIFYING TEACHER MODELS Figure 5: Effect of runtime augmentation. Setup To assess the benefit of using multiple teacher models during trajectory collection, we compare the individual and combined contributions of Qwen3-Coder and DeepSeek-V30324. We first analyze how many unique challenges each model solves and their category-level overlaps. Then, we fine-tune Qwen3 models of sizes 8B, 14B, and 32B on three trajectory subsets: (1) Qwen3-Coder only, (2) DeepSeek-V3-0324 only, and (3) both combined. We report average Pass@1 across benchmarks to evaluate downstream agent performance. Decoding parameters and training setup match those in our main experiments. Crypto Forensics Pwn Rev Web Misc Category Qwen Both DeepSeek 31 1 2 6 0 4 84 13 15 37 6 26 26 3 3 9 2 6 Table 5: Solved challenge counts. Analysis In Table 5, Qwen3-Coder and DeepSeek-V3-0324 demonstrate complementary strengths. the models share 84 solves, but Qwen3-Coder For example, uniquely solves 31 while DeepSeek-V3-0324 adds another 26. Similar patterns emerge across other categories, with notable non-overlapping contributions in Reverse Engineering, Misc, and Forensics. Combining both models increases total coverage to 274 unique challenges, exceeding either model alone. This diversity translates into measurable downstream gains. in Crypto tasks, Table 6 reveals that training on combined trajectories improves Pass@1 performance across all model sizes. For example, the 32B model trained on combined data achieves 31.9%, outperforming both the Qwen3-Coderonly (29.4%) and DeepSeek-only (31.3%) variants. Similarly, the 8B and 14B models also benefit from the combined setting. These results confirm that teacher diversity enriches training data and yields more capable cybersecurity agents. Table 6: Pass@1 performance when varying teacher models. Teacher Model 8B 14B 32B Qwen3-Coder DeepSeek-V3-0324 Combined 17.3 17.6 18.9 23.8 24.8 25. 29.4 31.3 31."
        },
        {
            "title": "5 RELATED WORK",
            "content": "LLM Agents for Offensive Cybersecurity LLM agents are increasingly being applied to offensive cybersecurity, particularly in solving CTF challenges within dockerized environments (Yang et al., 2023; Shao et al., 2024; Zhang et al., 2025b; Mayoral-Vilches et al., 2025). These systems often build on Kali Linux due to its extensive suite of pre-installed security tools, serving as foundations for broader applications such as penetration testing, vulnerability exploitation, and cyberattack automation (Charan et al., 2023; Deng et al., 2024; Fang et al., 2024). To evaluate the risks and offensive potential of such systems, benchmarks like CyberSecEval (Bhatt et al., 2023; Wan et al., 2024) have been proposed, while others assess the dangerous capabilities of LLMs in tasks like CTFs and red-teaming (Phuong et al., 2024; Guo et al., 2024), though these models still show limited performance on more complex tasks. Recent efforts have advanced agent design. Project Naptime (Glazunov & Brand, 2024) and Big Sleep (Allamanis et al., 2024) demonstrated agents capable of discovering new SQLite vulnerabilities using integrated tools like debuggers and browsers. EnIGMA (Abramovich et al., 2025) further raises the bar by combining cybersecurity-specific tools and interactive environments tailored for LLMs, achieving state-of-the-art results. Recently, Zhuo et al. (2025) introduced Cyber-Zero, achieving the best performance among open-source LLMs. Unlike prior methods that primarily depend on inference-time scaffolds or unverified training data, we introduce runtime environment that efficiently enhances model performance via execution. Training LLM Agents to Code Previous training paradigms for software engineering have largely emphasized general-purpose coding capabilities (Li et al., 2023; Lozhkov et al., 2024; Muennighoff et al., 2024; Zhuo et al., 2024; Wei et al., 2024). While scaffolded approaches using proprietary models achieve strong results on real-world software engineering (SE) tasks, open-source models continue to lag behind, prompting shift toward domain-specific training strategies. Several recent efforts exemplify this trend. Lingma SWE-GPT (Ma et al., 2024) introduces 7B and 72B models trained with process-oriented development methodology. SWE-Gym (Pan et al., 2024) offers the first open training environment for SE agents, yielding notable gains on SWE-bench (Jimenez et al., 2024). More recent work includes SWE-smith (Yang et al., 2025b), which automatically scales training data for SE, and SWE-RL (Wei et al., 2025), which applies reinforcement learning (Grattafiori et al., 2024) to repair programs with reasoning. While these methods advance software engineering capabilities via execution-based environments, they do not address the distinct demands of cybersecurity (Zhuo et al., 2025). Our work fills this gap by introducing the first execution environment specifically tailored for security tasks, where traditional code-centric training fails to transfer effectively. Benchmarking Models Cybersecurity Capabilities Several benchmarks have been proposed to evaluate LLMs on cybersecurity tasks. Multiple-choice datasets (Li et al., 2024; Tihanyi et al., 2024; Liu, 2023) offer limited insight, as their results are often highly sensitive to prompt phrasing (Qi et al., 2024; Łucki et al., 2024) and lack alignment with real-world operational contexts. AutoAdvExBench (Carlini et al., 2025) assesses LLMs ability to autonomously break image-based adversarial defenses, while CyberSecEval (Bhatt et al., 2023) focuses on single-turn code exploitation, capturing only narrow slice of the interactive, multi-step nature of real-world attacks. In contrast, agent-based frameworks with integrated tool usage offer more realistic evaluations. As result, Capture-the-Flag (CTF) challenges have become popular proxy for measuring security capabilities. Recent systems (Abramovich et al., 2025; Mayoral-Vilches et al., 2025) further enhance realism by combining interactive environments with structured, chain-of-exploitation evaluations."
        },
        {
            "title": "6 CONCLUSION AND FUTURE WORK",
            "content": "Conclusion We present CTF-DOJO, the first large-scale execution environment for training cybersecurity LLM agents, addressing the long-standing challenge of limited runtime support in this domain. Powered by our automated pipeline CTF-FORGE, CTF-DOJO transforms public CTF artifacts into ready-to-use Docker containers in minutes, enabling scalable and reproducible trajectory collection. Training on just 486 high-quality agent trajectories synthesized through CTF-DOJO, our open-weight LLMs outperform strong baselines by up to 11.6% on three major CTF benchmarks. Our 32B model achieves state-of-the-art results among open models, approaching the performance of Claude3.5-Sonnet and DeepSeek-V3-0324. Our findings highlight the critical role of writeup-augmented training, runtime augmentations, and diverse agent behaviors in building effective cybersecurity mod10 els. Overall, CTF-DOJO provides scalable and democratized foundation for advancing LLM-based security systems. Future Work This work opens several promising avenues for future research. First, we envision the creation of live CTF benchmark, where models can be continuously evaluated on challenges collected from active CTF competitions in the wild. By leveraging CTF-FORGE to reconstruct challenge environments and containerize them dynamically, we can enable scalable, real-time benchmarking and trajectory collection without manual environment engineering. Second, while CTF-DOJO enables training with execution-verified data, it remains constrained by the static nature and finite scale of its current dataset (658 challenges). Exploring reinforcement learning for cybersecurity agents would be natural next step, where models interact with live environments and receive structured feedback, such as partial rewards or flag-based signals. This paradigm could unlock significantly higher data efficiency and adaptability, enabling agents to learn more generalizable strategies beyond imitation and better handle novel CTF problems."
        },
        {
            "title": "IMPACT STATEMENT",
            "content": "We recognize the dual-use implications of our work. While CTF-DOJO is intended to enhance cybersecurity by empowering developers and researchers to proactively identify and remediate vulnerabilities through automated penetration testing, the same techniques could also be misused for offensive purposes, such as discovering vulnerabilities in external systems or crafting malicious exploits. The nature of our approach further heightens this concern by lowering the technical barrier to training powerful cybersecurity agents. Our results show that models trained on CTF-DOJO-generated trajectories can reach performance levels comparable to leading proprietary systems, underscoring that the democratization of advanced cybersecurity capabilities is not only possible but imminent. As LLM-based security tools become more capable, we emphasize the need for sustained collaboration among researchers, developers, and safety organizations to guide their responsible development and use. We believe that open research, paired with thoughtful safeguards, remains essential for ensuring these technologies ultimately strengthen cybersecurity defenses."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We are deeply grateful to the ENIGMA team for open-sourcing the agent scaffold and reformatted benchmark data. We thank Yangruibo Ding for valuable early discussions and the pwn.college team (e.g., Yan Shoshitaishvili and Pratham Gupta) for initiating and maintaining one of the largest CTF archives that collects hundreds of verified CTF competitions. In addition, we thank Anoop Deoras and Stefano Soatto for their support. Lastly, we would like to address our appreciation to every CTF player taking the time to write detailed, informative writeups contributing to the collective knowledge that makes research like ours possible."
        },
        {
            "title": "REFERENCES",
            "content": "Talor Abramovich, Meet Udeshi, Minghao Shao, Kilian Lieret, Haoran Xi, Kimberly Milner, Sofija Jancheska, John Yang, Carlos Jimenez, Farshad Khorrami, et al. Enigma: Interactive tools substantially assist lm agents in finding security vulnerabilities. In Forty-second International Conference on Machine Learning, 2025. Miltiadis Allamanis, Martin Arjovsky, Charles Blundell, Lars Buesing, Maddie Brand, Sergei Glazunov, David Maier, Petros Maniatis, Guilherme Marinho, Henryk Michalewski, Koushik Sen, Charles Sutton, Varun Tulsyan, Matteo Vanotti, Thomas Weber, and Dawn Zheng. From naptime to big sleep: Using large language models to catch vulnerabilities in real-world code. https://googleprojectzero.blogspot.com/2024/10/ from-naptime-to-big-sleep.html, November 2024. Accessed July 2025. Anthropic. Claude 3.5 Model Card Addendum. https://www-cdn.anthropic.com/ fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_ Addendum.pdf, 2024. Accessed: 2025-07-03. 11 Anthropic. Claude 3.7 Sonnet System Card. https://assets.anthropic.com/m/ 785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf, 2025a. Accessed: 2025-07-03. Anthropic. System Card: Claude Opus 4 & Claude Sonnet 4. Technical report, Anthropic, May 2025b. Accessed: 2025-07-03. Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, et al. Purple llama cyberseceval: secure coding benchmark for language models. arXiv preprint arXiv:2312.04724, 2023. Nicholas Carlini, Javier Rando, Edoardo Debenedetti, Milad Nasr, and Florian Tramèr. Autoadvexbench: Benchmarking autonomous exploitation of adversarial example defenses. arXiv preprint arXiv:2503.01811, 2025. PV Charan, Hrushikesh Chunduri, Mohan Anand, and Sandeep Shukla. From text to mitre techniques: Exploring the malicious use of large language models for generating cyber attack payloads. arXiv preprint arXiv:2305.15336, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. DARPA. DARPA AIxCC, 2024. https://aicyberchallenge.com/about/, 2024. Accessed: 2025-07-03. Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, and Stefan Rass. {PentestGPT}: Evaluating and harnessing large language models for automated penetration testing. In 33rd USENIX Security Symposium (USENIX Security 24), pp. 847864, 2024. Connor Dilgren, Purva Chiniya, Luke Griffith, Yu Ding, and Yizheng Chen. Secrepobench: arXiv preprint Benchmarking llms for secure code generation in real-world repositories. arXiv:2504.21205, 2025. Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel Kang. Llm agents can autonomously hack websites. arXiv preprint arXiv:2402.06664, 2024. Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. Sergei Glazunov and Maddie Brand. Project naptime: Evaluating offensive security capabilities of large language models. https://googleprojectzero.blogspot.com/2024/06/ project-naptime.html, June 2024. Accessed July 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Chengquan Guo, Xun Liu, Chulin Xie, Andy Zhou, Yi Zeng, Zinan Lin, Dawn Song, and Bo Li. Redcode: Risky code execution and generation benchmark for code agents. Advances in Neural Information Processing Systems, 37:106190106236, 2024. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. 12 Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, et al. Nemo: toolkit for building ai applications using neural modules. arXiv preprint arXiv:1909.09577, 2019. Hwiwon Lee, Ziqi Zhang, Hanxiao Lu, and Lingming Zhang. Sec-bench: Automated benchmarking of llm agents on real-world software security tasks. arXiv preprint arXiv:2506.11791, 2025. Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin Li, Ann-Kathrin Dombrowski, Shashwat Goel, Gabriel Mukobi, et al. The wmdp benchmark: measuring and reducing malicious use with unlearning. In Proceedings of the 41st International Conference on Machine Learning, pp. 2852528550, 2024. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Zefang Liu. Secqa: concise question-answering dataset for evaluating large language models in computer security. arXiv preprint arXiv:2312.15838, 2023. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. Jakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tramèr, and Javier Rando. An adversarial perspective on machine unlearning for ai safety. arXiv preprint arXiv:2409.18025, 2024. Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, and Yongbin Li. Lingma swe-gpt: An open development-process-centric language model for automated software improvement. arXiv preprint arXiv:2411.00622, 2024. Víctor Mayoral-Vilches, Luis Javier Navarrete-Lozano, María Sanz-Gómez, Lidia Salas Espejo, Martiño Crespo-Álvarez, Francisco Oca-Gonzalez, Francesco Balassone, Alfonso Glera-Picón, Unai Ayucar-Carbajo, Jon Ander Ruiz-Alcalde, et al. Cai: An open, bug bounty-ready cybersecurity ai. arXiv preprint arXiv:2504.06017, 2025. Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: In The Twelfth International Conference on Instruction tuning code large language models. Learning Representations, 2024. OWASP GenAI Project (CTI Layer Team). OWASP LLM Exploit Generation Version 1.0. Technical report, OWASP GenAI Project, February 2025. Accessed: 3 July 2025. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. 13 Phuong, Aitchison, Catt, Cogan, Kaskasoli, Krakovna, Lindner, Rahtz, Assael, Hodkinson, et al. Evaluating frontier models for dangerous capabilities. arxiv. arXiv preprint arXiv:2403.13793, 2024. Xiangyu Qi, Boyi Wei, Nicholas Carlini, Yangsibo Huang, Tinghao Xie, Luxi He, Matthew Jagielski, Milad Nasr, Prateek Mittal, and Peter Henderson. On evaluating the durability of safeguards for open-weight llms. arXiv preprint arXiv:2412.07097, 2024. Minghao Shao, Sofija Jancheska, Meet Udeshi, Brendan Dolan-Gavitt, Kimberly Milner, Boyuan Chen, Max Yin, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, et al. Nyu ctf bench: scalable open-source benchmark dataset for evaluating llms in offensive security. Advances in Neural Information Processing Systems, 37:5747257498, 2024. Jia Song and Jim Alves-Foss. The darpa cyber grand challenge: competitors perspective. IEEE Security & Privacy, 13(6):7276, 2015. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Norbert Tihanyi, Mohamed Amine Ferrag, Ridhi Jain, Tamas Bisztray, and Merouane Debbah. Cybermetric: benchmark dataset based on retrieval-augmented generation for evaluating llms In 2024 IEEE International Conference on Cyber Security and in cybersecurity knowledge. Resilience (CSR), pp. 296302. IEEE, 2024. Shengye Wan, Cyrus Nikolaidis, Daniel Song, David Molnar, James Crnkovich, Jayson Grace, Manish Bhatt, Sahana Chennabasappa, Spencer Whitman, Stephanie Ding, et al. Cyberseceval 3: Advancing the evaluation of cybersecurity risks and capabilities in large language models. arXiv preprint arXiv:2408.01605, 2024. Peiran Wang, Xiaogeng Liu, and Chaowei Xiao. Cve-bench: Benchmarking llm-based software engineering agents ability to repair real-world cve vulnerabilities. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 42074224, 2025a. Zhun Wang, Tianneng Shi, Jingxuan He, Matthew Cai, Jialin Zhang, and Dawn Song. Cybergym: Evaluating ai agents cybersecurity capabilities with real-world vulnerabilities at scale. arXiv preprint arXiv:2506.02548, 2025b. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code generation with oss-instruct. In International Conference on Machine Learning, pp. 52632 52657. PMLR, 2024. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. xAI. xAI Risk Management Framework (Draft). Technical report, xAI, February 2025. Draft version accessed 3 July 2025. Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swe-fixer: Training open-source llms for effective and efficient github issue resolution. arXiv preprint arXiv:2501.05040, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. Advances in Neural Information Processing Systems, 36:2382623854, 2023. 14 John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024a. John Yang, Kilian Leret, Carlos Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents. arXiv preprint arXiv:2504.21798, 2025b. Yu Yang, Yuzhou Nie, Zhun Wang, Yuheng Tang, Wenbo Guo, Bo Li, and Dawn Song. Seccodeplt: unified platform for evaluating the security of code genai. arXiv preprint arXiv:2410.11096, 2024b. Andy Zhang, Joey Ji, Celeste Menders, Riya Dulepet, Thomas Qin, Ron Wang, Junrong Wu, Kyleen Liao, Jiliang Li, Jinghan Hu, et al. Bountybench: Dollar impact of ai agent attackers and defenders on real-world cybersecurity systems. arXiv preprint arXiv:2505.15216, 2025a. Andy Zhang, Neil Perry, Riya Dulepet, Joey Ji, Celeste Menders, Justin Lin, Eliot Jones, Gashon Hussein, Samantha Liu, Donovan Julian Jasper, Pura Peetathawatchai, Ari Glenn, Vikram Sivashankar, Daniel Zamoshchin, Leo Glikbarg, Derek Askaryar, Haoxiang Yang, Aolin Zhang, Rishi Alluri, Nathan Tran, Rinnara Sangpisit, Kenny Oseleononmen, Dan Boneh, Daniel E. Ho, and Percy Liang. Cybench: framework for evaluating cybersecurity capabilities and risks of language models. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=tc90LV0yRL. Yuxuan Zhu, Antony Kellermann, Dylan Bowman, Philip Li, Akul Gupta, Adarsh Danda, Richard Fang, Conner Jensen, Eric Ihli, Jason Benn, et al. Cve-bench: benchmark for ai agents ability to exploit real-world web application vulnerabilities. In Forty-second International Conference on Machine Learning, 2025. Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, and Niklas Muennighoff. Astraios: Parameter-efficient instruction tuning code large language models. arXiv preprint arXiv:2401.00788, 2024. Terry Yue Zhuo, Dingmin Wang, Hantian Ding, Varun Kumar, and Zijian Wang. Cyber-zero: Training cybersecurity agents without runtime. arXiv preprint, 2025."
        },
        {
            "title": "CONTENTS",
            "content": "A Statistics CTF-DOJO CTF Challenges Scaffolding Interface Prompt Design of CTF-FORGE D.1 Dockerfile Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Docker-Compose Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Challenge.json Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Finding Bugs in CTF-DOJO E.1 ECTF 2014 Lowkey (Filed Issue) . . . . . . . . . . . . . . . . . . . . . . . . . E.2 ångstromCTF 2019 Blank Paper (Filed Issue) . . . . . . . . . . . . . . . . . . E.3 HSCTF 2019 Hidden Flag (Filed Issue) . . . . . . . . . . . . . . . . . . . . . E.4 Access Denied CTF 2022 Binary (Filed Issue) . . . . . . . . . . . . . . . . . . 18 36 37 37 39 40 40 41 41"
        },
        {
            "title": "A STATISTICS",
            "content": "We provide summary of the important statistics mentioned in the paper. Table 7: Summary of data statistics."
        },
        {
            "title": "Item Description",
            "content": "CTF-DOJO Challenges Number of available CTF challenges Number of challenges with stable and reproducible environments, as confirmed by the original authors"
        },
        {
            "title": "Writeups for CTF Challenges",
            "content": "Total number of writeups collected from the CTFtime website Writeups successfully matched to CTF-DOJO challenges using competition and task metadata CTF-DOJO challenges for which at least one corresponding writeup is available"
        },
        {
            "title": "Successful Agent Samples",
            "content": "Raw agent trajectories collected before cleaning or filtering Unique trajectories remaining after removing duplicates and limiting the maximum number per challenge CTF-DOJO challenges that include at least one valid and successful trajectory"
        },
        {
            "title": "Count",
            "content": "658 650 8,361 252 150 1,006 274 17 CTF-DOJO CTF CHALLENGES"
        },
        {
            "title": "Competition",
            "content": "0CTF - 2017 0CTF - 2018 0CTF - 2019 0CTF Quals -"
        },
        {
            "title": "Challenge",
            "content": "babyheap diethard easiestprintf babyheap2018 blackhole freenote2018 heapstorm subtraction zerofs babyaegis babyheap babyrsa babysandbox elements flropyd plang sanitize scanner zerotask cloudpass future listbook vp zer0lfsr"
        },
        {
            "title": "Pwn\nPwn\nCrypto\nPwn\nRev\nPwn\nPwn\nMisc\nPwn\nPwn",
            "content": "Crypto Rev Pwn Rev Crypto Continued on next page 18 Table 8 Continued from previous page"
        },
        {
            "title": "Competition",
            "content": "0xCTF - 4141 29c3CTF - 2012 AccessdeniedCTF -"
        },
        {
            "title": "Challenge",
            "content": "client eazyrsa external factorize filereader hash moving-signals pyjail ret-of-the-rops shjail soul staple-aes the-pwn-inn wallet ware wrongdownload x-and-or findthekey maya memcached minesweeper proxy ru1337 updateserver babyc binary ecc enormous llvm merklegoodman mitm2 ret2system rsa1 rsa2 rsa3 smallkey"
        },
        {
            "title": "Rev\nCrypto\nPwn\nCrypto\nMisc\nRev\nPwn\nMisc\nPwn\nMisc\nCrypto\nCrypto\nPwn\nCrypto\nRev\nRev\nRev",
            "content": "Rev Rev Pwn Pwn Pwn Pwn Pwn Misc Rev Crypto Rev Rev Crypto Crypto Pwn Crypto Crypto Crypto Crypto Continued on next page 19 Table 8 Continued from previous page"
        },
        {
            "title": "Competition",
            "content": "AngstromCTF - 2016 AngstromCTF -"
        },
        {
            "title": "Challenge",
            "content": "amoebananas artifact asmtracing casino cipher ciphertwo client drag endian fender flaglock formatone hamlet headsup helpcenter hex imageencryptor javabest metasploit music oops recovery rsa spqr yankovic begin casino knockknock obligatory royalcasino substitutioncipher"
        },
        {
            "title": "Category Qwen DeepSeek",
            "content": "Web Crypto Rev Crypto Rev Rev Web Misc Pwn Forensics Misc Pwn Crypto Forensics Crypto Crypto Rev Rev Forensics Forensics Forensics Forensics Crypto Crypto Forensics Crypto Crypto Crypto Web Crypto Crypto Continued on next page 20 Table 8 Continued from previous page"
        },
        {
            "title": "Challenge",
            "content": "accumulator backtobasics bankroppery introtorsa productkey rev1 rev2 rev3 waldo2 warmup washington weirdmessage xor blankpaper chainofrope highqualitychecks icthyo like lithp onebite overmybrain paperbin reallysecurealgorithm runes"
        },
        {
            "title": "Competition",
            "content": "AngstromCTF - 2018 AngstromCTF - 2019 AngstromCTF -"
        },
        {
            "title": "Misc\nPwn\nRev\nRev\nRev\nMisc\nRev\nPwn\nMisc\nCrypto\nCrypto",
            "content": "Continued on next page Misc Crypto Rev Rev amongus caesaranddesister dyn numbergame randomlysampledalgorithm Crypto reallyobnoxiousproblem shark1 uninspired wah whatsmyname Pwn Misc Rev Pwn Pwn 21 Table 8 Continued from previous page"
        },
        {
            "title": "Competition",
            "content": "AngstromCTF - 2024 AsisCTF - 2013 AsisCTF - 2014 BackdoorCTF -"
        },
        {
            "title": "Challenge",
            "content": "awman bap exam heapify layers leftright og philosophy presidential simonsays snowman stacksort themectl tss1 tss2 dice encoding inaccessible licensekey memdump pcaps rsang serialnumber simpleofficer blocks randomimage babyheapbackdoorctf babytcache echo forgot matrix miscpwn rsanne team"
        },
        {
            "title": "Crypto\nPwn\nPwn\nPwn\nMisc\nPwn\nPwn\nCrypto\nPwn\nCrypto\nMisc\nPwn\nPwn\nCrypto\nCrypto",
            "content": "Rev Crypto Forensics Rev Forensics Crypto Crypto Rev Crypto Forensics Crypto Pwn Pwn Pwn Pwn Pwn Pwn Crypto Pwn Continued on next page 22 Table 8 Continued from previous page"
        },
        {
            "title": "Category Qwen DeepSeek",
            "content": "Crypto Rev Forensics Rev Rev Misc Forensics Crypto Forensics Crypto Crypto Forensics Misc Misc Misc Crypto Pwn Crypto Crypto Crypto Crypto Crypto Misc Crypto Crypto Crypto Misc Misc Crypto Misc Misc Misc Misc Forensics Misc Misc Misc Misc Misc Misc Continued on next page"
        },
        {
            "title": "Competition",
            "content": "ByuCTF - 2022 ByuCTF - 2023 ByuCTF - 2024 CactusconCTF -"
        },
        {
            "title": "Challenge",
            "content": "ballgame basicrev blue chicken funfact murdermystery qool shift stickykey truth xqr crcconfusion hexadecalingo misc006-1 misc006-2 poem pwn2038 rsa1 rsa2 rsa3 rsa4 rsa5 xkcd2637 aresa domath giveup gotmail meetgreg multiplied petrolhead typosquatting vacationboats wateryoudoing worstchallenge clueless frng numbersleuthv1 numbersleuthv2 numbersleuthv3 securerepititions 23 Table 8 Continued from previous page"
        },
        {
            "title": "Competition",
            "content": "CcscCTF - 2020 Codegate - 2011 CodegateCTF -"
        },
        {
            "title": "Challenge",
            "content": "basilisk64 echoes guy mouse routes spell binary100 binary200 binary300 binary400 binary500 crypto200 crypto300 crypto400 crypto500 forensics200 forensics300 forensics400 network100 bin100 bin200 bin300 bin400 bin500 forensics100 forensics200 forensics300 forensics400 vuln500 CodegateCTF - 2013 vuln100 Codegateprelims - 4stone angrydoraemon automata chronological crackme dodosandbox hypercat minibomb weirdsnus"
        },
        {
            "title": "Crypto\nMisc\nPwn\nCrypto\nCrypto\nPwn",
            "content": "Pwn Pwn Pwn Pwn Pwn Crypto Crypto Crypto Crypto Forensics Forensics Forensics Web Pwn Pwn Pwn Pwn Pwn Forensics Forensics Forensics Misc Pwn Pwn Pwn Pwn Rev Misc Rev Pwn Pwn Pwn Pwn Continued on next page 24 Table 8 Continued from previous page"
        },
        {
            "title": "Misc\nRev\nRev\nCrypto\nRev\nRev\nRev",
            "content": "Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Crypto Continued on next page"
        },
        {
            "title": "Competition",
            "content": "CorCTF - 2021 CorCTF - 2022 CryptoCTF - 2020 CryptoCTF -"
        },
        {
            "title": "Challenge",
            "content": "babyrand babyrev bank chainblock chance cshell fibinary fourninesix friedrice lcg vmquack babypad bogus edgelord exchanged msfrob turbocrab vmquacksrevenge amsterdam complextohell fatima onelinecrypto threeravens trailingbits dorsa ecchimera elegant farm frozen hamul hypernormal improved lower rima tinyecc triplet trunc wolf 25 Table 8 Continued from previous page"
        },
        {
            "title": "Competition",
            "content": "CryptoverseCTF - 2022 CryptoverseCTF - 2023 Csaw -"
        },
        {
            "title": "Challenge",
            "content": "bigrabin dlog rsa2 rsa3 tale worldcup acceptance babyaes backpack fractionalflag lsfr microassembly picochip1 picochip2 retschool simplecheckin standardvm almostxor auir babycrypt bananascript cvv grumpcheck minesweeper prophecy scv serial tablez twitchplayspwnable zone"
        },
        {
            "title": "Pwn\nCrypto\nCrypto\nCrypto\nCrypto\nRev\nCrypto\nCrypto\nPwn\nRev\nRev",
            "content": "Crypto Pwn Crypto Rev Pwn Rev Pwn Rev Pwn Misc Rev Misc Pwn Continued on next page 26 Table 8 Continued from previous page"
        },
        {
            "title": "Competition",
            "content": "CsawCTF - 2011 CsawCTF - 2012 CsawCTF -"
        },
        {
            "title": "Challenge",
            "content": "crypto1 crypto10 crypto2 crypto3 crypto4 crypto5 crypto6 crypto7 crypto8 crypto9 evilburritos2 hardware linux loveletter net1 net200 networking101 exploit200 exploit400 exploit500 networking100 networking200 networking300 networking400 rev400 aerosol bigdata bo cfbsum eggshells feal ish obscurity s3 saturn CsawCTF Quals - 2020 applicative CsawCTF Quals - alienmath contactus forgery sonicgraphy"
        },
        {
            "title": "Crypto\nCrypto\nCrypto\nCrypto\nCrypto\nCrypto\nCrypto\nCrypto\nCrypto\nCrypto\nWeb\nWeb\nRev\nWeb\nRev\nWeb\nWeb",
            "content": "Pwn Pwn Pwn Web Web Web Web Rev Rev Web Pwn Crypto Rev Crypto Pwn Forensics Pwn Pwn Pwn Pwn Forensics Crypto Forensics Continued on next page 27 Table 8 Continued from previous page"
        },
        {
            "title": "Competition",
            "content": "CsawCTF Quals - 2024 DownunderCTF - 2020 DownunderCTF - 2021 DownunderCTF -"
        },
        {
            "title": "Challenge",
            "content": "aes chinesefood covert diffusion golf nix rickshaw trapdoor 1337crypt babyrsa calcgame ceebc echos extracoolblockchaining formatting hexshiftcipher impeccable returnofwhat returnofwhatsrevenge roti shellthis vecc zombie babygame breakme flagchecker flagloader juniperus babyarx babypywn oracle rsaoracle1 rsaoracle2 rsaoracle3 rsaoracle4 timelocked"
        },
        {
            "title": "Category Qwen DeepSeek",
            "content": "Crypto Misc Forensics Crypto Pwn Pwn Misc Crypto"
        },
        {
            "title": "Crypto\nCrypto\nCrypto\nCrypto\nCrypto\nCrypto\nRev\nCrypto\nCrypto\nPwn\nPwn\nCrypto\nPwn\nPwn\nPwn",
            "content": "Pwn Crypto Rev Rev Rev Crypto Pwn Crypto Crypto Crypto Crypto Crypto Crypto Continued on next page"
        },
        {
            "title": "Category Qwen DeepSeek",
            "content": "Table 8 Continued from previous page"
        },
        {
            "title": "Forensics\nRev\nForensics\nPwn\nCrypto\nRev\nPwn\nPwn",
            "content": "Crypto Pwn Pwn Rev Misc Rev Crypto Crypto Crypto Crypto Misc Crypto Crypto Forensics Misc Rev Crypto Misc Pwn Pwn Pwn Pwn Pwn Continued on next page DownunderCTF - adorableencryptedanimal babysfirstforensics interceptedtransmission myarraygenerator shufflebox ternarybrained wackyreciepe ECTF - 2014 GitsCTF - 2012 GoogleCTF - 2020 Grehack - 2012 Greycattheflag - HackluCTF - 2011 HitconCTF - 2018 ectfhacked friendsofcrime hackermessage knotty lowkey python seddit sleepycoder crypto250 pwn200 pwn300 rev400 trivia25 beginner amanfromhell hackingfordummy baby block calculator catino dot challengetorrent mario pycrackme simplexor unknownplanet babytcache childrencache groot hitcon tftp 29 Table 8 Continued from previous page"
        },
        {
            "title": "Competition",
            "content": "Hitconquals - 2017 HkcertCTF - 2020 HkcertCTF - 2021 HkcertCTF -"
        },
        {
            "title": "Challenge",
            "content": "artifact babyfs easytosay luaky reeasy sakura seccomp sssp start veryluaky void angr calmdown rop signin easyheap freedom longstoryshort magicalpotion simplesignin base64 keyboard kingrps locate rogue sdcard zonn"
        },
        {
            "title": "Rev\nCrypto\nPwn\nCrypto",
            "content": "Pwn Crypto Crypto Crypto Crypto Crypto Misc Crypto Misc Crypto Forensics Misc Continued on next page"
        },
        {
            "title": "Category Qwen DeepSeek",
            "content": "Table 8 Continued from previous page a-lost-cause aria-writer broken-repl byte caesars-revenge caesars-revenge-wrapper combo-chain combo-chain-lite daheck fish forgotpassword hiddenflag keith-logger license slap the-quest the-real-reversal verbose virtualjava welcome-to-crypto-land apcs apenglish binaryword comments mountains pie primes unexpected xored aptenodytes canis multidimensional opisthocomus queen warmup"
        },
        {
            "title": "Crypto\nPwn\nMisc\nPwn\nPwn\nPwn\nPwn\nPwn\nRev\nForensics\nRev\nMisc\nWeb\nRev\nForensics\nWeb\nMisc\nMisc\nRev\nCrypto",
            "content": "Rev Rev Misc Forensics Forensics Misc Misc Crypto Crypto Crypto Crypto Rev Crypto Crypto Rev Continued on next page HsCTF - 2019 HsCTF - 2020 HsCTF - 31 Table 8 Continued from previous page"
        },
        {
            "title": "Competition",
            "content": "ImaginaryCTF - 2021 ImaginaryCTF - 2022 ImaginaryCTF - 2023 ImaginaryCTF - 2024 IrisCTF - 2025 IsitdtuCTF -"
        },
        {
            "title": "Challenge",
            "content": "foliage gottagofast inkaphobia linonophobia nothoughts notpwn cbc desrever emoji fmtfun hash livingwithoutexpectations otp poker secureencoding secureencodinghex smoll stream chaos crypto emoticons rsa scrambled sheepish signer signpost snailchecker base64 bf integrity vokram ayes dot sqlate winter mixer1 mixer2 random sign"
        },
        {
            "title": "Crypto\nRev\nCrypto\nPwn\nCrypto\nCrypto\nCrypto\nCrypto\nCrypto\nCrypto\nCrypto\nCrypto",
            "content": "Rev Forensics Crypto Crypto Rev Rev Crypto Misc Rev Crypto Rev Crypto Rev Crypto Misc Pwn Misc Crypto Crypto Crypto Crypto Continued on next page Table 8 Continued from previous page"
        },
        {
            "title": "Competition",
            "content": "JustCTF - 2019 M0leconteaserCTF - 2025 Neverlan - 2019 NoobzCTF -"
        },
        {
            "title": "Challenge",
            "content": "atm changevm exponent fsmir fsmir2 pandq phonebook safenotes shellcode bootme bootme2 ecsign ot ptmcasino quadratic talor telegram whispers wolfram alphabet bases binary1 feb14 keyz oink zerocool aes-1 asm ezrev maas mypin to-the-moon"
        },
        {
            "title": "Rev\nPwn\nCrypto\nCrypto\nWeb\nCrypto\nCrypto\nWeb\nRev\nWeb",
            "content": "Crypto Crypto Pwn Crypto Misc Crypto Crypto Crypto Pwn Rev Crypto Rev Misc Continued on next page"
        },
        {
            "title": "Category Qwen DeepSeek",
            "content": "Table 8 Continued from previous page"
        },
        {
            "title": "Crypto\nCrypto\nForensics\nCrypto\nRev\nCrypto\nForensics\nCrypto\nRev\nRev\nMisc\nMisc\nRev\nCrypto",
            "content": "Pwn Pwn Crypto Misc Pwn Pwn Pwn Rev Rev Rev Rev Rev Pwn Rev Rev Misc Pwn Rev Rev Rev Rev Rev Rev Pwn Continued on next page PatriotCTF - 2022 PatriotCTF - 2023 PicoCTF - barry base64times10 bezier cowsay crackme cryptogod exfil extremlycoolbook flowing goobf greek hike stringcheese twofifty bookshelf bookshelf2 breakfastclub flagfinder guessinggame printshop softshell asm1 asm2 asm3 asm4 johnpollard messymalloc needforspeed reversecipher seedspring sicecream vaultdoor3 vaultdoor4 vaultdoor5 vaultdoor6 vaultdoor7 vaultdoor8 zerotohero 34 Table 8 Continued from previous page"
        },
        {
            "title": "PlaidCTF",
            "content": "R3CTF - 2024 Ritsec - 2019 SekaiCTF - 2022 SekaiCTF - 2023 TamuCTF -"
        },
        {
            "title": "Challenge",
            "content": "emojidb liars-and-cheats potassium reee sandybox shop suffarring dao forbiddencontent hackcam scp simplestkernel sparrow tinseal bottles cleaners onion shiny game issues qr cosmic adminpanel confinement criminal Techcompfest - 2022 python UiuCTF - 2022 art asr ecc militarygradenc oddshell"
        },
        {
            "title": "Misc\nPwn\nPwn\nCrypto\nPwn\nCrypto\nMisc",
            "content": "Pwn Forensics Misc Crypto Web Misc Misc Pwn Pwn Pwn Crypto Web Rev Crypto Crypto Crypto Pwn Continued on next page Table 8 Continued from previous page"
        },
        {
            "title": "Competition",
            "content": "UiuCTF - 2023 UiuCTF - 2024 VsCTF - 2022 WtfCTF - 2021 Zh3r0CTF -"
        },
        {
            "title": "Challenge",
            "content": "athome chainmail explorer1 explorer2 explorer3 explorer4 explorer5 explorer6 fastcalc groupproject groupprojection morphing rattler threetime determined syscalls ezorange privatebank tuningtest k3y mom5m4g1c prison alicebobdave babyre bootleg chaos cheater estr injection mersenne numpymt optimiseme pyaz sabloom twist vault"
        },
        {
            "title": "Crypto\nPwn\nMisc\nMisc\nMisc\nMisc\nMisc\nMisc\nRev\nCrypto\nCrypto\nCrypto\nPwn\nCrypto",
            "content": "Crypto Pwn Pwn Misc Pwn Pwn Pwn Pwn Crypto Rev Crypto Misc Misc Rev Crypto Crypto Crypto Rev Rev Rev Crypto Misc"
        },
        {
            "title": "C SCAFFOLDING INTERFACE",
            "content": "We simulate the ENIGMA Scaffold interface in CTF-DOJO, and provide specialized tools inside Table 9 from the original ENIGMA paper (Abramovich et al., 2025). While we provide the interface to the models for data generation, there is no guarantees that they will utilize the customized tools regularly. 36 Table 9: In additional to the standard Linux Bash commands and the SWE-agent specialized tools, we provide ENIGMA with tools in the offensive cybersecurity domain, including binary decompilation and disassemble, and interactive agent tools for debugging and connecting to remote servers. Required arguments are enclosed in <> and optional arguments are in []. The last column shows the documentation presented to the LLMs. Command Documentation Category Static analysis decompile <binary_path> [-function_name <function_name>] disassemble <binary_path> [-function_name <function_name>] Dynamic analysis debug_start <binary> [<args>] debug_add_breakpoint <breakpoint> debug_continue debug_step [<number>] debug_exec <command> debug_stop Decompile binary and prints the decompilation of given function name, or main by default. Disassembles binary and prints the disassembly of given function name, or main by default. Starts debug session with the given binary and optional command-line arguments for the binary. Adds breakpoint in the debug session. Continues the program execution in the debug session. Steps number of instructions (default is 1) in the debug session. Executes arbitrary gdb command in debug session. Stops the current debug session. Network connect_start <server_address> <port> Starts new interactive connection to the server address and port. connect_sendline [<line>] connect_exec <command> connect_stop Task submit <flag> exit_forfeit Sends line with Unicode/hexadecimal values to the connection. Sending hexadecimal bytes should be done using x<hh> where hh stands for the specific byte you want to send. Executes arbitrary connect command in connect session. Stops the current connect session. Submits your current flag and terminates the session upon correct response, you should be aware to properly escape the flag as this is bash command and to put your flag in single quotes. Give up on the current challenge and terminate the session. PROMPT DESIGN OF CTF-FORGE D.1 DOCKERFILE GENERATION Based on the following CTF challenge information, generate Dockerfile that will properly set up server(cid:45) environment for CTF players to interact with. # Challenge Name: {task_name} # Challenge Category: {category} # Available Files: {available_files} # File Analysis: {file_analysis} # Challenge Description: {description} # REHOST.md Content: {rehost_content} # .init File Content: {init_content} **IMPORTANT**: If .init file content is provided above, this contains setup instructions or configuration (cid:45) specific to this challenge. The .init file may contain: - Environment setup commands that should be executed during Docker build - Configuration parameters or paths that should be used in the Dockerfile - Special instructions for handling this particular challenge - Library or dependency information - Runtime configuration that affects how the challenge should be containerized Use the .init content to inform your Dockerfile generation - execute any setup commands it specifies, copy(cid:45) any files it references, and follow any special instructions it provides. {flag_instruction} # CATEGORY-SPECIFIC GUIDELINES: {category_guidelines} # GENERAL DOCKER BEST PRACTICES: 1. Use ubuntu:20.04 as base image unless challenge specifically requires different environment 2. Install additional packages needed for the specific challenge (if any beyond the comprehensive set) 3. Create non-root user for running services when possible, but DO NOT use `useradd` 4. Use specific package versions when critical for challenge functionality 5. Properly set file permissions (especially for executables and sensitive files) 6. Use WORKDIR to organize files properly 7. Expose only the necessary port for the challenge 8. Use specific CMD/ENTRYPOINT that matches the challenge type 9. You MUST create `run.sh` wrapper script using printf or echo commands (DO NOT use heredoc syntax like (cid:45) << 'EOF') # CRITICAL SCRIPT CREATION SYNTAX: - CORRECT: Use printf to create shell scripts in Dockerfiles: ``` RUN printf '#!/bin/bashncd /challengenexec stdbuf -i0 -o0 -e0 ./your_executablen' > /challenge/run(cid:45) .sh ``` - CORRECT: Use echo with for newlines: ``` RUN echo -e '#!/bin/bashncd /challengenexec stdbuf -i0 -o0 -e0 ./your_executable' > /challenge/run.(cid:45) sh ``` - WRONG: Never use heredoc syntax in Dockerfiles (causes parsing errors): ``` RUN cat > /challenge/run.sh << 'EOF' #!/bin/bash cd /challenge exec stdbuf -i0 -o0 -e0 ./your_executable EOF ``` # This will FAIL # CRITICAL EXECUTABLE/SCRIPT WRAPPING REQUIREMENTS: {architecture_specific_wrapper} # SECURITY CONSIDERATIONS: - DO NOT copy flag.sha256 or flagcheck files to the Docker image - DO NOT copy any files not in the available files list - Set appropriate file permissions for challenge files - Use process isolation when running network services # SHEBANG HANDLING: - If any files have problematic shebangs (like #!/opt/pwn.college/python), they will be automatically (cid:45) fixed - You do not need to manually handle shebang issues - the system will detect and fix them - Focus on proper file copying and permissions # CTF-SPECIFIC REQUIREMENTS: - The server MUST be accessible to CTF players over the network - The flag should NEVER be directly accessed by the CTF players, and MUST always be stored in /flag with (cid:45) the permissions 444 - Choose appropriate port based on service type (1337 for general, 80/8080 for web, etc.) - Ensure the challenge service starts automatically and runs continuously - Handle connection multiplexing for multiple players if needed - Programs must respond to user input immediately without buffering delays (achieved through stdbuf) # CRITICAL CTF BINARY BEHAVIOR UNDERSTANDING: - CTF challenge binaries (especially pwn challenges) often exhibit specific behavior patterns: * When run directly from command line, they may exit immediately without output (THIS IS NORMAL) * They are designed to work through network services (socat) that provide stdin/stdout redirection * The binary may wait for specific input patterns or network connections to respond * Some binaries are designed to read from stdin and write to stdout in an interactive manner - Do NOT assume binary is broken if it runs without output when executed directly - The key is to properly wrap the binary with socat for network access - Test the service through network connection (nc localhost PORT) rather than direct execution # LIBRARY DEPENDENCY HANDLING: - Pay special attention to shared library dependencies (check with ldd if needed conceptually) - For 32-bit binaries on 64-bit systems, ensure 32-bit libraries are installed - If binary requires specific libraries (e.g., libpam.so.0), install the appropriate packages: * For libpam: install libpam0g:i386 for 32-bit or libpam0g for 64-bit * Use library path environment variables or LD_LIBRARY_PATH if needed * Consider using the system's dynamic linker directly for better compatibility 38 Generate complete, production-ready Dockerfile. Respond with ONLY the Dockerfile content, no (cid:45) explanations. IMPORTANT VALIDATION CHECKLIST: Base image specified (prefer ubuntu:20.04) NOTE: Comprehensive package installation will be automatically added Additional required packages installed (if needed beyond the comprehensive set) Challenge files copied correctly run.sh wrapper script created with stdbuf for proper interaction Appropriate port exposed Service command specified in CMD/ENTRYPOINT using socat with the run.sh wrapper File permissions set correctly for both executables and run.sh No sensitive files copied Service will accept network connections and respond immediately to user input CRITICAL: Scripts created using printf/echo commands, NOT heredoc syntax (<<) # CRITICAL DOCKERFILE SYNTAX WARNING: - NEVER use heredoc syntax like \"RUN cat > file << 'EOF'\" in Dockerfiles - This causes Docker parsing errors and build failures - ALWAYS use printf or echo commands instead - Example: RUN printf '#!/bin/bashncd /challengenexec ./binaryn' > /challenge/run.sh # PYTHON NETWORK SERVICES: - If the file analysis indicates Python script is network server listening on specific internal port(cid:45) (e.g., detected as listening on port XXXX): - The service MUST be run in the background (e.g., `python3 /challenge/server.py &`). - You MUST use `socat` to proxy connections from the public EXPOSED port (e.g., 1337) to the script's (cid:45) detected internal port. - **CORRECT WAY** to create `run.sh` for Python server on its detected internal port, exposed on 1337: ``` RUN printf '#!/bin/shncd /challengen# Start the server in the backgroundnpython3 /challenge/server(cid:45) .py &n# Wait moment for the server to startnsleep 1n# Use socat to forward connections from (cid:45) the public port to the internal portnexec socat TCP-LISTEN:1337,reuseaddr,fork TCP:localhost:XXXX(cid:45) n' > /challenge/run.sh && chmod +x /challenge/run.sh ``` - The `CMD` in the Dockerfile should then be `CMD [\"/challenge/run.sh\"]`. - DO NOT use `socat` with `EXEC` for these types of services, as it launches new process for every (cid:45) connection. D.2 DOCKER-COMPOSE GENERATION Based on the following CTF challenge information and generated Dockerfile, create docker-compose.yml (cid:45) file with an appropriate ctfnet alias. # Challenge Name: {task_name} # CTF Name: {ctf_name} # Available Files: {available_files} # Challenge Description: {description} Generated Dockerfile: {dockerfile_content} Requirements: 1. Use \"build: .\" to build from the local Dockerfile 2. Extract the exposed port from the Dockerfile and map it appropriately 3. Connect to the \"ctfnet\" external network 4. Generate meaningful, DNS-compliant alias for this specific challenge 5. The alias should be memorable and related to the challenge name/theme 6. Use format like: challengename.ctf.io or similar creative naming 7. Avoid generic names like \"web.chal.custom.io\" 8. Use the challenge information to create relevant alias Example good aliases: - showdown.csaw.io - cryptochallenge.picoctf.io - webshell.defcon.io - bufferoverflow.ctf.io Generate complete docker-compose.yml. Respond with ONLY the docker-compose.yml content, no explanations(cid:45) .' D.3 CHALLENGE.JSON GENERATION Generate challenge.json file for this CTF challenge following the exact schema. Challenge Information: # Name: {task_name} 39 # Category: {category} # Files: {task_files} # CTF Challenge RAW Description: {description} # REHOST.md Content: {rehost_content} {docker_compose_section} REQUIRED JSON SCHEMA: {{ \"name\": \"challenge name\", \"description\": \"human readable and self-contained challenge description for the CTF players, not for (cid:45) the CTF rehosters. flag checker is NOT available and you should NOT mention it in the description\", \"category\": \"will be set automatically from module.yml mapping\", \"files\": [\"list of files to upload for this challenge\"], \"box\": \"optional URL for external server challenge\", \"internal_port\": \"optional port for external server challenge\", \"compose\": true if hosting the ctf challenge requires server, false otherwise, \"flag\": \"flag for the challenge\" }} RULES: 1. Use the exact challenge name: \"{task_name}\" 2. Use the exact files list: {task_files} 3. DO NOT generate the category field - it will be set automatically from module.yml mapping 4. Always set \"box\": \"\" and \"internal_port\": \"\" 5. If server_needed is False: do NOT include \"server_description\" field 6. The flag should be valid flag for the challenge 7. You MUST NOT mention the flagCheck in the Challenge Description, as we will handle the validation in (cid:45) other way. OUTPUT: Return ONLY valid JSON matching the schema above. No explanations, no markdown, no extra text. FINDING BUGS IN CTF-DOJO Through our experiments on CTF-DOJO, we identify 4 bugs inside the current CTF-Archive maintain by pwn.college. Specifically, we observe consistent behavior pattern across these problematic challenges, where the LLMs make several attempts to submit the same flag mismatched with the one encoded inside the original repository. E.1 ECTF 2014 LOWKEY (FILED ISSUE) CTF Challenge Hey! just met you! And this is craazy! But heres product of prime numbers! Try me maybe! 2449 P.S. My mother told me that the 7th key fits in the lock ;) Hint: Hint is implied in the question. Solution To solve the lowkey cryptography challenge, the LLM began by extracting the contents of the provided archive low_key.tar.gz, which revealed file named whatami.txt containing sequence of integers. The challenge description hinted at product of prime numbers (2449) and 7th key, suggesting an RSA encryption scheme with public exponent = 7. The LLM factored 2449 into its prime components, = 31 and = 79, yielding = pq = 2449 and Eulers totient ϕ(n) = (p 1)(q 1) = 2340. Using the extended Euclidean algorithm, the LLM computed the modular inverse of 7 modulo 2340 to obtain the private key exponent = 1003. With this, the encrypted integers from whatami.txt were decrypted using the formula = cd mod n, and the resulting values were converted to ASCII characters. This will reveal the flag in the format of flag{...}. Verification The model attempts multiple times to submit the same flag but got rejected, as it did not align with the flag hashed by the pwn.college team. To verify the validity of captured flag, we have done some research on the possible ECTF2014 writeups online but could not find any of them. However, when searching for the flag content captured by the model, we notice there is blog in Chinese that describes the similar CTF challenge and confirms the flag correctness. E.2 ÅNGSTROMCTF 2019 BLANK PAPER (FILED ISSUE) CTF Challenge Someone scrubbed defunds paper too hard, and few of the bytes fell off. Solution To solve the blankpaper challenge, the PDF file blank_paper.pdf was corrupted due to missing header bytes. By inspecting the file with hex viewer, it discovered that the expected %PDF signature had been replaced with null bytes. It then created script to restore the missing header by replacing the first four bytes with %PDF. After regenerating the corrected PDF as fixed_paper.pdf, the model used pdftotext to extract its contents. Within the text, it found the flag in the format of actf{...}. the LLM identified that Verification As the flag format is not specified in the challenge description, the model has tried with the flag wrapper of flag{...} and actf{...}. Both of them fails the flagCheck and .flag.sha256. However, the submitted context inside the flag remains unchanged. We validate the flag using writeup shown on CTFtime. E.3 HSCTF 2019 HIDDEN FLAG (FILED ISSUE) CTF Challenge This image seems wrong.....did Keith lose the key again? Solution To solve the hiddenflag miscellaneous CTF challenge, the LLM was given file named chall.png, which, although named as PNG image, was identified by the file command as generic data. Upon inspecting the file using strings, the clue key is invisible was discovered. This led to the hypothesis that the file was XOR-encrypted using the key invisible. Python script was created to XOR-decrypt the file byte-by-byte using this key. The output, saved as decrypted.png, was confirmed to be valid PNG image. Optical character recognition (OCR) was then performed using Tesseract, which successfully extracted the flag embedded in the image. Verification The model made the same flag submission attempts for several times but all of them failed. We find writeup on the personal website that describes the similar solution and the flag value same as what the model captures. E.4 ACCESS DENIED CTF 2022 BINARY (FILED ISSUE) CTF Challenge Finally, you are in the binary stage. Solution To solve the hiddenflag CTF challenge, the LLM was provided with file named chall.png, which was not recognized as valid PNG file. Upon running strings on the file, we found the phrase key is invisible, suggesting XOR encryption with the key invisible. Python script was used to XOR each byte of the file with the repeating key, producing valid image saved as decrypted.png. After confirming the decrypted file was PNG, we ran OCR using Tesseract to extract any hidden text. The extracted text revealed the flag in the format of hsctf{...}. Verification The flag submitted by the model does not match with the officiallu provided hash in the repository. We confirm the correctness of the submission via writeup written in the personal blog."
        }
    ],
    "affiliations": [
        "Amazon",
        "Monash University"
    ]
}