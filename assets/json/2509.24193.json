{
    "paper_title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play",
    "authors": [
        "Ran Xu",
        "Yuchen Zhuang",
        "Zihan Dong",
        "Jonathan Wang",
        "Yue Yu",
        "Joyce C. Ho",
        "Linjun Zhang",
        "Haoyu Wang",
        "Wenqi Shi",
        "Carl Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at https://github.com/ritaranx/AceSearcher and https://huggingface.co/AceSearcher."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 3 9 1 4 2 . 9 0 5 2 : r AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play Ran Xu Yuchen Zhuang2 Zihan Dong3 Jonathan Wang1 Yue Yu2 Joyce C. Ho1 Linjun Zhang Haoyu Wang4 Wenqi Shi5 Carl Yang1 1Emory University 2Georgia Institute of Technology 3Rutgers University 4SUNY Albany 5UT Southwestern Medical Center Dataset/Model: https://huggingface.co/AceSearcher Code: https://github.com/ritaranx/AceSearcher/"
        },
        {
            "title": "Abstract",
            "content": "Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, cooperative self-play framework that trains single large language model (LLM) to alternate between two roles: decomposer that breaks down complex queries and solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the giant DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9 more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable performance in areas such as natural language generation [65, 80, 17] and complex reasoning [26, 20]. However, they often fall short when handling long-tailed or dynamically evolving knowledge [48]. To address these limitations, growing body of work has explored augmenting LLMs with external search tools that retrieve relevant information at inference time. Search-augmented LLMs not only improve factual accuracy [2, 58], but also facilitate efficient adaptation to new tasks and domains without costly parameter updates [78]. Despite notable advances in retrieval-augmented generation (RAG) [2, 39, 45, 86, 50], most existing approaches are restricted to relatively simple questions [33, 48] solvable through single-turn retrieval. However, real-world applications often demand more complex reasoning, requiring (i) multi-hop retrieval to gather relevant evidence from large corpora due to the low recall of direct single-step retrieval [83], and (ii) reasoning capability to integrate multiple pieces of information beyond span extraction for response generation [9]. To address these challenges, prior works propose multi-step search via iterative prompting [68, 31, 36, 42, 88], often relying on powerful, closed-source LLMs with strong reasoning abilities. Alternatively, tree-search algorithms have been explored to improve retrieval and reasoning at inference time [27, 72, 77], but at the expense of increased latency. Recent 39th Conference on Neural Information Processing Systems (NeurIPS 2025). efforts employing reinforcement learning (RL) frameworks allow LLMs to interact with search engines [30, 92, 62, 4, 63]. While promising, these methods are often memory-intensive and thus less practical for deployment in resource-constrained environments. Additionally, their exclusive reliance on QA datasets for supervision limits the broader potential of LLMs to integrate search with complex, multi-step reasoning across wider range of tasks. Motivated by these challenges, we aim to develop an efficient, data-centric training recipe to enhance the capabilities of LLMs for reasoning-intensive search scenarios. Inspired by human problemsolving strategies where complex tasks are decomposed into simpler subproblems [93, 31, 59], we propose AceSearcher that trains LLMs to act as two roles: decomposer and solver. The decomposer breaks down the original question into subquestions to guide retrieval, while the solver generates intermediate and final answers by integrating subquestions, their answers, and context. We then introduce two-stage fine-tuning framework to train both the decomposer and solver modules. In the first stage, we perform supervised fine-tuning (SFT) by extending existing open-domain QA datasets with open-source reasoning data. This covers task decomposition and problem-solving in both text and code. This simultaneously boosts the models ability to extract relevant information from context as well as strengthens its general reasoning capabilities. In the second stage, we apply reinforcement fine-tuning on targeted reasoning and QA tasks, using rewards derived solely from final outputs. To overcome the lack of intermediate annotations, we hypothesize that better decompositions lead to more accurate answers. The solver is reinforced to produce correct answers based on decompositions and context, while the decomposer is optimized to maximize the solvers accuracy. This framework promotes joint structured reasoning across both roles with one unified model, while eliminating dependence on supervision from proprietary frontier models. Notably, AceSearcher achieves strong performance using iterative preference optimization, without relying on memory-intensive online RL training or costly inference-time scaling. Our contributions can be summarized as follows: We introduce AceSearcher, cooperative self-play framework designed to jointly enhance LLMs capabilities in both search and reasoning. By introducing two roles, namely the decomposer and solver, AceSearcher equips single LLM with joint skills of task decomposition and task solving, providing an efficient and flexible solution for complex reasoning in search-augmented settings. We propose two-stage fine-tuning framework that first applies SFT on mixture of retrieval, reasoning, and decomposition datasets, followed by reinforcement fine-tuning using rewards solely from the final answer to train the decomposer and solver without intermediate supervision. This approach can be readily applied to LLMs with varying sizes (1.5B - 32B as shown in our study) to enhance the multi-step reasoning ability of search-augmented LLMs. We conduct extensive evaluations of AceSearcher covering three tasks across ten public datasets. Compared to strong baselines, including recent reasoning models and RL-enhanced search LLMs, AceSearcher demonstrates strong empirical performance with 7.6% gain on average. Moreover, AceSearcher demonstrates high parameter efficiency: the 1.5B variant matches the performance of models 10 larger on QA tasks, highlighting its suitability for low-resource settings."
        },
        {
            "title": "2 Related Works",
            "content": "Reasoning-intensive Search/Retrieval. Standard RAG pipelines often consider single-step retrieval only and cannot handle complex questions well [34, 58, 18]. To incorporate reasoning into RAG pipelines, earlier research [68, 70, 71, 36, 88] design multi-turn prompting techniques for complex QA. Besides, several works [2, 75] leverage SFT on high-quality chain-of-thoughts to improve the reasoning skills of LLMs, but without explicit task decomposition. Additionally, [27, 77] design reward-guided search during inference time, [22, 79] trains the query refinement model based on the feedback of generator LLMs, and [6, 37] leverage multi-agent fine-tuning to further enhance reasoning performance, but at the cost of serving multiple LLMs in deployment. Self-play Finetuning for LLMs. Self-play [60] is an effective technique that enables LLMs to learn through self-interaction, promoting diverse experience trajectories and prompt coverage. Recent studies have applied self-play to alignment [8, 76, 84], instruction following [13], theorem proving [15], and reasoning [10, 89]. Unlike these works, we consider collaborative self-play for complex problem solving, and tailor LLM self-play frameworks specifically for reasoning-intensive RAG applications. 2 Figure 1: Overview of AceSearcher. AceSearcher contains two-stage training process to teach LLM for joint precise question decomposition and answer generation with cooperative self-play. RL for Search-augmented LLMs. Very recently (concurrent to us), multiple studies [4, 62, 30, 92, 29] attempted to leverage the RL pipeline for RAG by viewing the search as an external tool and using open-domain QA datasets (e.g. NQ [33], HotpotQA [83]) to create verification rewards. In contrast to these approaches, we propose data-centric pipeline that enhances LLM retrieval and reasoning capabilities through unified self-play fine-tuning framework. Our method demonstrates strong generalization across broad range of reasoning-intensive RAG tasks beyond multi-hop QA."
        },
        {
            "title": "3 Overview of AceSearcher",
            "content": "In this section, we first define the problem setup and present an overview of AceSearcher. Then, we introduce the training and inference pipeline for AceSearcher. 3.1 Problem Formulation In our setting, let denote the space of questions and the space of all possible answers. Given question and corpus (e.g. Wikipedia) that provides background knowledge, the retriever (often embedding models) first find small set of relevant passages = {d1, ...dk}, then the LLM fθ generates the output conditioned on both and as pθ( q, D). Note that can be shortor long-form response, depending on the type of task. In reasoning-intensive scenarios, the question may require multi-step reasoning beyond simple retrieval to produce accurate answers. 3.2 AceSearcher: Cooperative Self-Play Framework Our AceSearcher model, shown in Figure 1, tightly couples reasoning and search by enabling single LLM to act as two roles (controlled by different input and prompt templates): decomposer ρ that converts the original question into sequence of subquestion templates1 = (z1, z2, . . . , zn), where the number of subquestions is and zi may depend on answers to earlier subquestions. These templates are sampled from pθ( q). solver π that generates intermediate answers = (w1, w2, . . . , wn) and final answer in stepwise manner: For each subquestion zi z, the solver produces the intermediate answer as wi pθ( zi, w<i, Di), where w<i denotes the answers to previous subquestions, Di = {di,1, . . . , di,k} is the set of retrieved passages for zi. After solving subquestions, the solver predicts the final answer pθ( q, z, w, D) based on the original question, intermediate answers and context passages. Joint Learning Objective. Given the question q, we train θ to maximize the probability of the LLM for generating the final answer a. In our framework, the learning objective can be written as (cid:32) (cid:33) pθ(a q) = pθ(z q) pθ(a q, z, w) pθ(w q, z) (3.1) (cid:88) (cid:88) In practice, marginalizing over all possible decompositions and intermediate answers is intractable. To approximate it during training, we sample small set of candidate (z, w) paths and 1We refer to them as templates since some subquestions are determined by answers to previous ones. An example template [19] is: Q1: What items did Aristotle use?; Q2: Is laptop in #1?. In practice, the template is text snippet with fixed format and will be parsed to multiple subquestions, detailed in Appendix E. 3 identify the most promising ones to encouraging the decomposer to help the solver generate the correct answer. At inference time, given question q, the decomposer ρ generates subquestion sequence z, and the solver π reasons over the intermediate answer to derive the final answer a."
        },
        {
            "title": "4 Two-Stage Finetuning for AceSearcher",
            "content": "To enable the LLM to perform both roles effectively, we first apply SFT on publicly available datasets to establish its foundational capabilities. Subsequently, we perform reinforcement fine-tuning to further improve LLMs capabilities, using only final answers as supervision. 4.1 Stage I: Supervised Finetuning (SFT) Although recent studies have introduced data mixing strategies for search-augmented LLMs [39, 45, 86, 37], they focus on enhancing the LLMs ability to extract answers from provided contexts. In contrast, our setting presents greater challenge requiring the LLM to automatically decompose and solve complex questions across diverse range of tasks that requires reasoning. Towards this goal, we extend the SFT data mixture Dsft for the following tasks: Context-rich QA Data. We follow [45, 39, 86, 37] to leverage multiple QA datasets to enhance the LLMs capability of using context for generation. Specifically, we consider the following datasets: NQ [33], SQuAD [56], DROP [16], NarrativeQA [32], Quoref [12], ROPES [38], FEVER [66], TAT-QA [94], which contains question, context passages, and an answer. Question Decomposition Data. To improve the LLMs ability to decompose complex questions into simpler subproblems, we incorporate GSM8K [11], ConvFinQA [9], and StrategyQA [19]. These datasets require generating sequence of subquestions for solving the original problem. Chain-of-thought Data. To enhance multi-step reasoning, we leverage chain-of-thought datasets including GSM8K [11], TabMWP [46], and IfQA [85]. Inspired by studies showing that combining Chain-of-Thought (CoT) [74] and Program-of-Thought (PoT) [5] rationales can boost reasoning capabilities, we incorporate MathInstruct [87], which contains CoT and PoT style prompts. Detailed descriptions of datasets, prompt formats, and the number of training examples are provided in Appendix D, E. In total, we curate 180K training examples in the SFT stage. The LLM is fine-tuned using the standard next-token prediction objective. 4.2 Stage II: Preference-based Reinforcement Finetuning (RFT) While SFT equips the LLM with basic capabilities for question decomposition and answer generation, it relies on richly annotated prompts with intermediate question decomposition and chain-of-thought annotations resources that are limited in practice. To overcome this scarcity, we further fine-tune the LLM on prompts = {(q, a)} covering RAG and context-reasoning scenarios that contain only the final answer given the question q. We frame this setting as an interactive environment, where the LLM learn to actively decompose the question and generate intermediate reasoning steps with external context. This motivates the use of reinforcement learning to optimize the reasoning trajectory in the absence of explicit intermediate supervision. Environment for RAG. We collect labeled pairs from multi-hop QA and fact verification datasets, including HotpotQA [83], 2WikiMHQA [21] and HOVER [28], which require the usage of retrieval to generate accurate answers. To formulate the RAG framework as an environment, the query decomposer ρ first generates sequence of candidate sub-questions = (z1, . . . , zn) Qn. For each sub-question qi, relevant documents are retrieved, denoted by Di. The solver pθ then produces intermediate solutions by generating wi pθ ( zi, w<i, Di), conditioned on the current subquestion, previously generated answers, and retrieved context. Finally, the solver predicts the final answer as pθ( (cid:83)n Environment for Context-Rich Reasoning. Beyond RAG-specific tasks, we also focus on improving the LLMs reasoning abilities. To this end, we incorporate three datasets from the SFT stage, including GSM8K [11], TabMWP [46], and ConvFinQA [9], which involve reasoning over contexts such as tables, passages, or problem conditions. Under this setting, ρ is used to generate subquestions = (z1, . . . , zn) Qn, and the solver pθ produces intermediate solutions by generating i=1 wi, (cid:83)n i=1 zi, (cid:83)n i=1 Di). 4 wi pθ ( zi, w<i, C), conditioned on the current subquestion, previous answers, and contexts. Finally, the solver predicts the final answer as pθ( (cid:83)n Reward Design. For both scenarios, the complete trajectory (q, z1, w1, . . . , zn, wn, a) is evaluated using reward signal derived from the final answer. Specifically, the reward function is defined as: r(q, a, a) = EM(a, a) I(f (q, a) = 1), (4.1) where EM denotes the exact match between the model-generated and ground-truth answer a. The function (q, a) represents format-based binary reward, verifying whether the model generates sub-questions, intermediate answers, and reasoning steps in the correct structure. i=1 zi, (cid:83)n i=1 wi, C). Optimization πθ and ρθ. During the RL phase, we use the reward function defined above as the feedback to update both πθ and ρθ. Denote uθ(a, z, wq) = pθ(zq)pθ(w, aq, z). Following existing works [52], the overall optimization objective is formulated as (cid:105) (cid:104) Ezρθ,(w,a)πθ [r(q, a, a)] βDKL [uθ(a, z, q)uref(a, z, q)] , Eq (4.2) max θ where β is the parameter for controlling deviation from the reference policy. We further decompose the KL divergence between uθ and uref as (cid:88) uθ(a, z, wq) (cid:2)log pθ(z) + log pθ(w, az, q) log pref(z) log pref(w, az, q)(cid:3) DKL(uθuref) = a,z,w (cid:88) = (cid:124) (cid:20) pθ(zq) log pθ(zq) pref(zq) (cid:123)(cid:122) DKL(ρθ ρref) (cid:21) (cid:125) (cid:88) + pθ(zq) (cid:88) w,a (cid:124) pθ(w, az, q) (cid:20) log pθ(w, az, q) pref(w, az, q) (cid:123)(cid:122) DKL(πθ πref) (cid:21) . (cid:125) Then, the optimization objective can be rewritten as Jθ = Eq max θ (cid:2)Ezρθ,(w,a)πθ [r(q, a, a)] β DKL(ρθρref) β Ezρθ [DKL(πθπref)](cid:3) . (4.3) The above optimization problem have the closed-form solution (details in Appendix A) [55] as p(z q) pref (z q) E(w,a)pref(q,z) exp (cid:20) (cid:19)(cid:21) r(q, a, a) , (cid:18) 1 β p(w, q, z) pref(w, q, z) exp (cid:19) r(q, a, a) . (cid:18) 1 β What does the form of π and ρ imply? The closed-form policies ρ (i.e. p(z q)) and π (i.e. p(w, q, z)) align with our intuitions: an effective decomposition policy ρ promotes higher overall expected reward by enabling better intermediate reasoning steps, while an improved solver π directly enhances the reward, regardless of the quality of the decomposition. Practical Implementation for Optimization. In practice, direct optimization under sparse reward signals from single-trajectory rollouts is often ineffective due to high variance and limited feedback. We employ rollout strategy to address this challenge and enrich the learning signal. For each question q, we first generate candidate decompositions by sampling from the decomposer policy, i.e., z(i) ρθ( q) for = 1, . . . , m. Then, for each decomposition z(i), we subsequently sample candidate solutions by drawing from the solver policy as aj πθ( q, z(i)) for = 1, . . . , m. To construct preference datasets for RFT, we first identify the best and worst decompositions for each question based on the expected reward over their corresponding solutions as r(q, z(i)) = (w,a)π(q,z(i))r(q, a, a). This results in the following preference pair dataset: Ddecompose = {(q, z(i+), z(i))(q, a) D}, where z(i+) = z(j), = arg max r(q, z(i)), z(i) = z(j), = arg min (4.4) r(q, z(i)). Constructing preference pairs to optimize the answer generation policy π (with fixed subquestions z) is more challenging due to the presence of multiple intermediate answers along the reasoning trajectory. Denote the trajectory (i) = (q, z1, w(i) , a(i)) with a(i) being the final prediction, we create preference pairs for intermediate Dsubq and final question answering Dfinal as 1 , . . . , zn, w(i) (cid:8)(zi, w+ Dsubq = Dfinal = (cid:8)(cid:0)[q, z1, w+ , ) w+ 1 , . . . , zn, w+ = , (q, a) D, (zi, w+ ], a+, a(cid:1) (q, a) D(cid:9) . i=1 ) +, (zi, ) (cid:9) , 5 where the best and worst trajectories are selected as: + = (q, z1, w+ = (q, z1, 1 , . . . , zn, w+ 1 , . . . , zn, , a+), where a+ = arg max , a), where = arg min r(q, a(i), a), r(q, a(i), a). To jointly optimize both the decomposer ρ and the solver π, we construct unified preference dataset by combining three sources of pairs: Dpref = Ddecompose Dsubq Dfinal. For notational consistency, we represent each example as (x, g+, g), where is the input, and g+, are the chosen and rejected responses. Following [55], we optimize the policy with the following preference loss: LDPO := E(x,g+,g)Dpref log σ (cid:18) (cid:20) β log pθ (g+ x) pref (g+ x) log pθ (g x) pref(g x) (cid:21)(cid:19) . Multi-turn DPO for Online Optimization. Motivated by the benefits of on-policy data sampling in RL, we adopt an iterative DPO framework for improved optimization. Specifically, in the t-th iteration, we use the LLM policy model2 (t) to act as πθ and ρθ to sample preference pairs to create the dataset D(t) pref to update the policy model for the next iteration (t+1) pref. Then, we use D(t) as θ LmDPO := (x,g+,g)D(t) pref (cid:32) (cid:34) log σ β log p(t+1) (g+ x) θ p(t) θ (g+ x) log p(t+1) (g x) θ p(t) θ (g x) θ (cid:35)(cid:33) . (4.5) Remark. To balance effectiveness and efficiency in practice, we adopt the following strategy: the model πθ directly generates answers for intermediate questions, while producing full rationale only for the final answer. To prevent overly long input contexts during final answer generation, we set the total number of documents to (N = 15 in this study), and allocate up to N/n top-ranked documents for each of subquestions produced by πθ. We discard preference pairs if the reward for the best and the worst response is the same. Theorem 4.1 (Informal). Under regularity conditions, with high probability, the minimizer of the loss (Eq. (4.5)) at step is close to the minimizer of the loss (Eq. (4.2)). Furthermore, as increases, the minimizer converges to the true parameter θ. The proof for the theorem is deferred to Appendix due to the space limit. This theorem implies that our optimization algorithm is equivalent to maximizing the reward in Eq. (4.2). Furthermore, it guarantees convergence of our algorithm, which we also empirically validate in Section 5.4."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we conduct experiments on various tasks to verify the effectiveness of AceSearcher. 5.1 Experiment Setups Tasks and Dataset Information. We consider the following 3 types of tasks: (i) Multi-hop QA, which includes 2WikiMHQA [21], HotpotQA [83], Bamboogle [53] and MusiQue [67]. (ii) Multi-hop Fact Verification, namely HOVER [28] and Exfever [47]. (iii) Document-level Reasoning, where we use the DocMath-Eval benchmark [91] with several financial reasoning datasets such as TAT-QA [94], FinQA [7], MultiHiertt [90], and TAT-HQA [35]. Note that some of datasets have very long contexts that make retrieval necessary. The detailed information for these datasets is in Appendix C. Baselines. For Multihop QA and Fact Verification tasks, we compare against the following categories of baselines: (i) Instruction-tuned LLMs with Single-turn RAG: we consider Llama-3.1-it [17], DeepSeek-R1-Distill [20], Qwen-3 [80]3, Llama-4-Maverick [1], GPT-4o [25], and GPT-4.1 [51]. (ii) Prompt-based Multi-step Retrieval: we include IRCOT [68], Plan-RAG [70], Search-o1 [36], IterDRAG [88]. (iii) Finetuned LLMs with Search: we compare with InstructRAG [75], RAGStar [27], ReARTeR [64], CORAG [72] and Iter-RetGen [57]. (iv) LLMs with Search Trained via Reinforcement Learning: Recent agentic search-augmented works such as Search-R1 [30], R1Searcher [62], DeepResearcher [92], MMOA-RAG [6], and ReSearch [4] are also included for comprehensive evaluation. For document-level reasoning, we follow DocMath-Eval [91] to compare 2We denote the model after the SFT stage described in Section 4.1 as (1) 3For Qwen-3, we evaluate both thinking and non-thinking prompting modes and report the better result. θ . 6 Table 1: Comparison of AceSearcher and baselines on Multi-hop QA and Fact Verification datasets. stands for results that are not publicly available. : This model often does not follow instructions and generates long answers. : Concurrent works (preprint appears online after 2025/03/01). Baselines 2WikiMHQA HotpotQA Bamboogle MusiQue Hover ExFever Avg. QA Avg. All Acc EM F1 Acc EM F1 Acc EM F1 Acc EM F1 EM EM Acc / EM EM Base Size: < 10B parameters Llama-3.1-it RAG 8B [17] R1-Distill RAG 8B [20] Qwen-3 RAG 8B [80] Plan-RAG 8B [70] Search-R1 7B [30] R1-Searcher 7B [62] DeepResearcher 7B [92] InstructRAG 8B [75] MMOA-RAG 8B [6] CORAG 8B (Greedy) [72] CORAG 8B (Inference Scaling) [72] AceSearcher 1.5B AceSearcher 8B Large Size: 10 - 30B parameters Qwen-2.5-it RAG 14B [81] R1-Distill RAG 14B [20] Qwen-3 RAG 14B [80] Plan-RAG 14B [70] Search-R1 14B [30] InstructRAG 14B [75] AceSearcher 14B XL Size: > 30B parameters 26.5 42.8 46.4 46.0 24.4 25.2 42.0 35.2 19.8 21.6 26.2 20.4 15.1 37.3 46.1 32.2 5.6 25.6 33.6 23. 24.8 39.2 50.4 31.0 46.2 45.2 56.0 47.6 16.0 30.0 35.4 36.6 34.5 36.2 55.1 45.1 12.5 43.0 21.4 50.8 23.8 54.2 47.8 21.2 38.2 43.3 43.2 19.6 63.6 65.4 52.8 28.2 72.8 71.0 29.3 27.1 66.6 59.7 64.3 52.8 54.4 44.0 54.6 49.5 43.2 58.6 22.8 14.8 21.2 35.5 24.8 35.2 48.3 46.4 42.8 36.2 39.2 41.4 29.3 63.2 37.6 62.3 50.1 56.5 69.8 54.4 77.3 56.3 72.5 42.4 37.0 33.6 38.4 59.8 50.4 60.6 68.5 60.6 69.8 47.7 55.2 58.8 68.2 80.6 60.8 69.2 76.7 66.0 51.4 18.6 68.3 30.9 26.8 37.2 41.7 35.4 46.8 63. 7.2 12.4 15.2 12.2 50.4 45.6 63.8 60.0 17.8 8.4 42.0 51.0 29.1 11.4 49.1 60.8 18.2 44.4 5.2 31.8 25.6 59.2 61.6 32.4 47.0 46.8 52.8 24.1 25.5 63.2 47.4 81.2 48.1 14.6 55.1 59. 35.4 12.1 46.7 53.2 22.4 8.0 36.8 41.4 35.6 11.2 44.6 48.2 9.8 1.8 15.0 23.4 40.8 34.8 50.4 51.2 21.8 20.8 32.4 34. 58.0 71.8 41.4 65.6 58.1 76.6 31.2 53.6 46.6 61.2 16.2 36. 50.4 65.6 58.2 70.8 24.6 48.6 37.6 60.0 19.8 51.4 31.6 Qwen-2.5-it RAG 32B 30.7 57.2 39.4 R1-Distill RAG 32B Qwen-3 RAG 32B [80] 30.7 39.8 61.0 28.2 58.0 Search-o1 32B [36] 62.0 35.4 52.4 Plan-RAG 32B [70] ReSearch 32B [4] 45.0 46.7 56.8 26.4 50.8 79.0 AceSearcher 32B 20.9 49.2 Qwen-2.5-it RAG 72B 36.1 R1-Distill RAG 70B 61.0 Llama-4 Maverick RAG 17B*128 [1] 63.0 26.2 Proprietary Retrieval-Augmented LMs (For reference) 65.2 22.2 10.4 58.9 30.4 18.6 19.0 32.6 53.6 67.8 16.6 25.4 37.2 62. 40.6 58.0 38.5 51.2 63.2 49.0 49.0 65.4 51.5 71.4 45.2 49.2 61.8 63.8 50.4 59.2 51.2 62.7 56.4 46.4 40.8 56.8 62.4 57.3 56.0 53.6 60.0 60.7 75.3 46.9 59.8 61.2 72.7 55.1 67.3 63.8 65.6 34.6 50.4 50.6 60.4 41.6 53.0 49. 57.6 46.4 48.8 48.8 40.2 11.2 22.8 16.0 73.8 58.6 67.6 63.6 52.2 24.0 36.6 23.8 66.6 60.7 61.1 66.3 61.6 56.8 60.8 64. GPT-4o RAG [25] GPT-4.1 RAG [51] IRCOT (zero shot, w/ GPT-4o) [68] IRCOT (few shot, w/ GPT-4o) [68] Iter-RetGen (w/ GPT-4o) [57] RAG-Star (w/ GPT-4o) [27] ReARTeR (w/ GPT-4o-mini) [64] IterDRAG (Gemini-1.5, 5M ctx) [88] 57.2 49.5 61.0 72.9 69.6 62.8 45.8 42.4 51.4 62.2 52.8 47.0 30.0 57.8 29.7 51.0 33.5 61.4 44.9 78.0 42.6 71.4 68.0 43.5 53.4 50.6 54.4 30.2 35.0 76.9 29.8 27.2 63.6 30.0 35.2 59.3 33.8 46.4 63.7 46.2 57.6 66.0 63.4 40.8 48.8 68.6 40.0 64.0 60.8 64.2 66.4 62.6 57. 47.2 44.0 48.0 52.8 48.4 48.0 17.4 18.4 22.4 30.4 26.6 29.0 35.2 40.8 60.8 66.4 62.4 37.2 44.3 56.9 70.2 67.7 64.4 67. 56.4 68.8 75.2 51.7 65.6 75. 30.5 22.5 66.3 63.0 65.7 57.9 65.3 60.7 68.3 65.7 68.3 67.5 52.5 67.5 69.3 70.3 72.3 70.5 68.3 66.7 67.0 69.3 67.7 74.0 61.7 67.5 63.7 70.2 68.3 45.0 48.2 68.8 62.5 58.0 64.2 73.8 42.9 62.5 70.5 63.6 65.3 75.0 69.6 67.0 65.3 74.4 66.4 73.2 57.2 65.2 73.9 64.8 66.4 64.8 70.5 69.6 33.5 / 13.3 39.2 / 23.3 46.7 / 31.6 36.7 / 26.9 / 36.1 52.5 / 42.4 / 31.7 / 40.7 / 53.5 51.5 / 42.9 64.1 / 53.9 39.4 / 21.4 33.3 / 7.4 51.5 / 34.6 51.8 / 41.0 / 42.7 45.9 / 36.1 65.2 / 54. 47.7 / 32.9 51.8 / 38.4 54.0 / 37.2 / 44.0 55.3 / 45.2 / 43.7 66.7 / 56.0 47.2 / 33.5 56.5 / 43.8 53.8 / 41.2 27.4 34.1 43.5 38.0 41.7 49.4 59.6 32.4 26.7 46.1 46.7 46.2 60.1 45.2 48.8 47.4 53.1 52.3 60.7 43.4 51.3 52.1 46.7 / 34.4 45.7 / 35.0 55.1 / 42.1 64.3 / 50.8 59.3 / 44.2 44.0 45.7 49.5 57.3 52.4 58.2 / 51.7 against general instruction-tuned LLMs [51, 25, 65, 40, 41, 17], reasoning LLMs [26, 20, 80], Code LLMs [96, 24], Math LLMs [43, 82] and specialized finance reasoning LLMs [44, 95]. Implementation Details. We consider four different backbones for AceSearcher with varying sizes including Qwen-2.5-Instruct-1.5B/14B/32B [81] and Llama-3.1-8B-Instruct [17]. For AceSearcher-32B, we apply LoRA fine-tuning [23] with = 8, α = 16, while other models use full fine-tuning. All models are trained with batch size of 64 and maximum token of 2048 for 1 epoch on both SFT and RFT stages, with RFT run for 2 total iterations. For HotpotQA, 2WikiMHQA, MusiQue, we use the corpora provided by their respective sources. For Bamboogle, Hover, ExFever, we use the Wikipedia from Dec. 2018 as the corpus. During inference, we set the temperature = 0.0, the number of retrieved passages to = 10. For QA and fact verification tasks, we adopt E5 [73] as the retriever, while for document-level reasoning, we follow [91] and use OpenAIs Embedding-3-Large as the retriever. Detailed implementation settings for AceSearcher and baselines are in Appendix F. Evaluation. For QA, we report Exact Match (EM), Accuracy, and F1 score. For fact verification, we use EM as the metric. For document-level reasoning, we use Accuracy computed via the official evaluation script, and report the better performance between CoT and PoT prompting [54]. 5.2 Evaluation on QA and Fact Verification The main results comparing AceSearcher with baseline methods are presented in Table 1. From the results, we have the following key observations: (i) AceSearcher achieves strong performance over baselines. Notably, AceSearcher-32B achieves the highest overall score (60.7), outperforming both proprietary and open-source baselines by up to 7.6%. (ii) Compared to reasoning models, AceSearcher better adapt to RAG tasks. Qwen-3 and Deepseek-R1-distill are trained with extensive knowledge distillation, we observe that their gains are limited. This suggests that long thinking does not fully address the inherent challenge of multi-hop retrieval, while AceSearcher tackles this more effectively. (iii) AceSearcher has strong parameter efficiency. AceSearcher-1.5B matches or exceeds 8B baselines, while AceSearcher-8B outperforms baseline models with 70B parameters. Table 3: Ablation results on QA and DocMath-Eval using Llama-3.1-8B. We report EM for QA and fact verification due to space constraints. For w/o ρ and w/o π, we replace the respective components with Llama-3.1-8B-Instruct. w/o Search excludes CQA, StrategyQA, and IfQA from SFT; w/o Reasoning removes GSM8K, TabMWP, ConvFinQA, and MathInstruct. w/ CQA follows [45] and finetune solely on context-aware QA tasks. Model Name 2WikiMHQA HotpotQA Bamboogle MusiQue Hover ExFever Avg. DMSS DMCS DMSL DMCL Avg. Ablation Study for Different Components of AceSearcher AceSearcher AceSearcher w/o RFT AceSearcher w/o SFT AceSearcher w/o ρ AceSearcher w/o π Ablation Study for SFT Data Mixture 66.0 61.8 40.0 57.8 41. AceSearcher w/o Search AceSearcher w/o Reasoning AceSearcher w/ CQA [45] Ablation Study for RL Algorithms RAFT [14] RESTEM [61] Offline DPO [55] (Iterative) SimPO [49] 52.6 62.4 35.8 63.6 65.4 64.6 67.2 58.8 53.8 37.2 52.4 32.0 53.0 55.8 40.0 55.6 57.6 57.8 57. 55.2 52.0 38.4 53.6 20.8 51.2 44.8 22.4 50.4 51.2 53.6 46.8 35.4 34.8 18.2 32.4 12.2 23.4 36.6 12.2 32.8 32.8 35.2 34. 68.3 64.1 74.7 65.0 63.7 56.5 57.7 61.6 66.7 67.5 64.6 69.3 73.8 71.4 76.8 70.5 75.0 58.9 71.4 45.7 69.6 68.7 73.2 70. 59.6 56.2 47.6 55.3 40.9 49.3 54.8 36.3 56.5 57.2 58.2 57.6 83.0 71.5 71.0 81.5 73.5 79.5 76.5 53.0 73.0 77.5 73.5 75. 80.5 73.0 51.5 78.0 72.0 83.0 74.0 52.0 69.5 81.0 83.5 78.0 48.0 49.0 46.0 45.0 45.0 42.0 38.0 38.0 43.0 48.0 49.0 44. 32.3 26.7 31.0 29.6 27.7 28.7 29.7 20.3 27.3 28.0 30.0 32.0 59.0 52.3 48.0 56.6 52.4 56.6 53.5 38.6 51.2 56.1 56.6 55. 5.3 Evaluation on Document-level Reasoning We evaluate AceSearcher on DocMath-Eval  (Table 2)  against large-scale LLMs, demonstrating notable improvements over similarly-sized baselines, including both reasoning and domain-specific models. For instance, AceSearcher-32B and AceSearcher-8B outperform size-comparable baselines by 6.2% and 9.0%, respectively. Furthermore, AceSearcher achieves performance comparable to significantly larger models: AceSearcher32B matches the accuracy of DeepSeek-V3 using less than 5% of its parameters, while AceSearcher14B exceeds baselines up to 72B (5) in size. These results highlight AceSearchers strong generalization capabilities beyond factual QA, particularly in complex reasoning scenarios involving long documents and tables. 5.4 Additional Studies Table 2: Results on DocMath-Eval [91], sorted by average performance. SS, CS, SL and CL stands for SimpShort, CompShort, SimpLong and CompLong, respectively. DMSS DMCS DMSL DMCL Avg. Datasets Proprietary Models GPT-o3-mini Gemini-1.5-Pro GPT-4.1 GPT-4o Claude-3.5-Sonnet Open-Sourced Models 89.5 DeepSeek-V3 685B 89.5 AceSearcher 32B 87.0 DeepSeek-V2 236B DeepSeek-R1 685B 89.0 DeepSeek-Coder-V2 236B 85.0 85.0 Mistral-Large 122B 84.0 AceSearcher 14B 77.5 AceMath 72B 78.0 Qwen-2.5-Math 72B DianJin-R1 32B 76.0 83.0 AceSearcher 8B 81.0 Qwen-2.5-Coder 32B 77.5 DeepSeek-R1-Distill 70B 81.5 Qwen-2.5 72B 74.0 DeepSeek-R1-Distill 32B 79.5 Llama-3.3 70B Qwen3 32B 80.0 Qwen3 14B 75.0 DianJin-R1 7B 67.0 65.5 AceMath 7B 66.5 AceSearcher 1.5B Qwen3 8B 76.0 Fin-R1 7B 66.5 67.5 DeepSeek-Coder-V2 16B 62.0 Llama-3.1 8B 52.0 Qwen-2.5-Math 7B 86.0 85.5 85.5 86.0 78.0 87.5 80.0 75.0 76.5 76. 59.0 58.0 62.0 64.0 54.0 53.0 53.0 61.0 53.0 56.0 56.0 49.0 59.0 58.0 46.0 48.0 57.0 53.0 64.0 50.0 54.0 44.0 41.0 41.0 47.0 39.0 32.0 40.0 30.0 32.0 36.0 35.0 40.3 39.3 36.7 44.0 42.3 43.0 43.0 38.7 41.0 41.0 39.3 39.7 41.0 42.3 32.3 30.0 34.7 24.7 40.3 31.7 25.3 26.7 29.3 26.7 18.0 11.7 21.3 20.3 19.0 16.7 63.9 63.7 62.6 62.4 61.8 66.4 66.1 64.4 64.3 63.1 62.8 62.4 60.9 60.4 59.9 59.0 58.4 58.0 57.9 57.6 57.1 54.5 53.5 50.0 47.8 47.6 46.5 42.5 41.6 37.6 36. 86.0 84.0 75.5 83.5 78.0 76.5 82.0 77.0 73.0 77.0 80.5 79.0 76.0 81.0 71.0 74.5 78.0 78.5 68.5 62.0 77.5 76.5 51.5 53.5 44.0 49.0 Ablation Study. Table 3 reports the results of AceSearcher. The top rows show that both SFT and RFT contribute to overall performance gains. Besides, AceSearcher improves both question decomposition (ρ) and answer generation (π), as replacing each component with the frozen Llama-8b-it hurts the performance. This verifies the complementary roles of these two components. Ablation Study For SFT Data. The middle rows in Table 3 show SFT performance under different data compositions. Removing either the Reasoning or Search data leads to performance drops across both knowledge-intensive tasks (QA and Fact Verification) and document-level reasoning, indicating that both components are jointly beneficial for building capable LLM with search. Ablation Study For RFT. In the bottom rows of Table 3, we compare our reinforcement finetuning algorithm with other alternatives and find AceSearcher achieves the best performance. This highlights the importance of using both positive and negative trajectories, and shows that online methods outperform their offline counterparts. Figure 2 shows results across RFT iterations of 8 Figure 2: Performance of AceSearcher over different stages. (a) SFT Data Efficiency (b) RFT Data Efficiency (c) Inference Efficiency Figure 3: Efficiency Studies of AceSearcher with Llama-3.1-8B-Instruct as the backbone. (a) (b) m, (a) Human Study (b) Passage Recall@10 Figure 4: Parameter Study Figure 5: Quality Analysis for AceSearcher AceSearcher-8B. We observe significant gains in the first two iterations, with diminishing returns in the third. We set the number of iterations to 2 to balance between performance and efficiency. 5.5 Efficiency Studies Data Efficiency. Figure 3(a) and 3(b) show the accuracy of AceSearcher under varying amounts of data. For SFT, we evaluate the performance with varying SFT subset sizes and its improvement after subsequent RFT. For RFT experiments, we fix the full SFT dataset to isolate the effect of RFT. With just 2K SFT examples (1%), AceSearcher matches strong baselines like Search-R1 and Search-O1 (with up to 4 more parameters), and surpasses them after RFT. In the RFT stage, the use of only 5K prompts leads to 1% gain in QA and fact verification and 2% gain on document-level reasoning, justifying the data efficiency of AceSearcher with diverse set of prompts. Inference Efficiency. Figure 3(c) shows the inference time of AceSearcher and baseline models on QA and fact verification tasks. Unless noted, all models are 8B or similar in size. While AceSearcher incurs higher latency than standard RAG due to question decomposition and multi-step reasoning, it achieves substantial performance gains even outperforming 32B models with comparable inference time. Besides, AceSearcher outperforms reasoning models and inference-time scaling methods costs 1.5 to 2.8 more time. These results justify AceSearcher balances between efficiency and efficacy. 5.6 Parameter Studies We study the effect of varying k, m, and on AceSearcher. As shown in Figure 4(a), performance improves with more retrieved contexts, with gains plateauing at = 10, which we adopt in our experiments. In Figure 4(b) shows that increasing the number of sampled decompositions (m) and final answers (m) generally improves performance as it will generate more valid preference pairs, but increases trajectory collection time. The study on the effect of β and retrievers is in Appendix G. 5.7 Quality Analysis of Question Decomposition Module As question decomposition is key component of AceSearcher, we analyze the quality of the generated subquestions. Figure 5(a) shows the average human evaluation scores (on 15 scale) for 40 randomly sampled subquestions per task. We observe that both SFT and RFT significantly enhance subquestion quality across different model sizes. To quantify the impact of decomposition on end-task performance, we evaluate passage-level answer recall on HotpotQA after applying question decomposition. As shown in Figure 5(b), AceSearcher achieves up to 25% improvement 9 in recall@10 over standard retrieval and surpasses strong passage reranking model4. The details for human studies as well as more cases studies are given in the Appendix H."
        },
        {
            "title": "6 Conclusion",
            "content": "We present AceSearcher, cooperative self-play framework specifically designed for RAG and document-level reasoning tasks. By training single LLM to act as both decomposer and solver, AceSearcher addresses complex multi-hop retrieval and reasoning effectively. Our two-stage finetuning framework combines SFT on diverse reasoning tasks with preference-based RFT guided by final answer accuracy, achieving strong performance without relying on expensive intermediate supervision. Evaluated on ten benchmarks, AceSearcher outperforms state-of-the-art models by 7.6% on multi-hop QA and fact verification, and matches Deepseek-V3 on document reasoning with under 5% of its parameters. Even with smaller models (1.5B, 8B), AceSearcher delivers competitive or superior performance, offering an efficient and generalizable solution for advanced reasoning under resource constraints."
        },
        {
            "title": "Acknowledgment",
            "content": "RX and CY were partially supported by the US National Science Foundation under Award Numbers 2319449, 2312502, and 2442172, as well as the US National Institute of Diabetes and Digestive and Kidney Diseases of the US National Institutes of Health under Award Number K25DK135913. JH was partially supported by the US National Science Foundation (NSF) grant IIS-2145411. WS was partially supported by the Texas Advanced Computing Center (TACC) and the NVIDIA Academic Grant Program. LZ was partially supported by the NSF CAREER DMS-2340241 and AI for Math Fund from Renaissance Philanthropy."
        },
        {
            "title": "Limitations and Impact Statement",
            "content": "Limitations. While AceSearcher demonstrates strong empirical performance across wide range of RAG and document-level reasoning benchmarks, several limitations remain. First, our framework is evaluated primarily on complex QA, fact verification, and document-level reasoning tasks; its applicability to other tasks such as open-ended generation, dialogue, or use of real-time tools remains to be explored, though our scope is comparable (or even broader) compared to concurrent works [62, 30, 92]. Second, AceSearcher relies on fixed retriever during training and inference. Joint optimization of retrieval and reasoning could offer further gains but is left for future work. Third, our decomposition-based pipeline introduces inference overhead, which may limit applicability in latency-sensitive settings. Nonetheless, as shown in Figure 3(c), AceSearcher achieves favorable tradeoffs, and many strong baselines [70, 36, 72] also adopt multi-turn retrieval. Finally, due to resource constraints, we adopt iterative preference optimization (Online DPO) as practical and efficient alternative to fully online reinforcement learning. While this approach achieves strong results in our setting, exploring more expressive RL formulations may offer further improvements. Impact Statement. This work advances the development of search-augmented LLMs capable of complex reasoning. By enabling smaller open-source LLMs to search and reason more effectively, AceSearcher reduces reliance on proprietary or extremely large models, which may have high computational or financial barriers. This can promote democratization of advanced AI capabilities in low-resource or domain-specific applications, such as finance, scientific discovery, and healthcare."
        },
        {
            "title": "References",
            "content": "[1] AI@Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation, 2025. [2] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In ICLR, 2024. 4Using https://huggingface.co/castorini/rankllama-v1-7b-lora-passage for reranking. [3] S. Balakrishnan, M. J. Wainwright, and B. Yu. Statistical guarantees for the em algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77, 2017. [4] M. Chen, T. Li, H. Sun, Y. Zhou, C. Zhu, F. Yang, Z. Zhou, W. Chen, H. Wang, J. Z. Pan, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025. [5] W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. TMLR, 2023. [6] Y. Chen, L. Yan, W. Sun, X. Ma, Y. Zhang, S. Wang, D. Yin, Y. Yang, and J. Mao. Improving retrieval-augmented generation through multi-agent reinforcement learning. arXiv preprint arXiv:2501.15228, 2025. [7] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. Moussa, M. Beane, T.-H. Huang, B. R. Routledge, et al. Finqa: dataset of numerical reasoning over financial data. In EMNLP, pages 36973711, 2021. [8] Z. Chen, Y. Deng, H. Yuan, K. Ji, and Q. Gu. Self-play fine-tuning converts weak language models to strong language models. In ICML, pages 66216642, 2024. [9] Z. Chen, S. Li, C. Smiley, Z. Ma, S. Shah, and W. Y. Wang. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. In EMNLP, 2022. [10] P. Cheng, T. Hu, H. Xu, Z. Zhang, Y. Dai, L. Han, N. Du, and X. Li. Self-playing adversarial language game enhances LLM reasoning. In NeurIPS, 2024. [11] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [12] P. Dasigi, N. F. Liu, A. Marasovic, N. A. Smith, and M. Gardner. Quoref: reading comprehension dataset with questions requiring coreferential reasoning. In EMNLP, 2019. [13] G. Dong, K. Lu, C. Li, T. Xia, B. Yu, C. Zhou, and J. Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. In ICLR, 2025. [14] H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research, 2023. [15] K. Dong and T. Ma. Beyond limited data: Self-play llm theorem provers with iterative conjecturing and proving. arXiv preprint arXiv:2502.00212, 2025. [16] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL, 2019. [17] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [18] Y. Gao, Y. Xiong, Y. Zhong, Y. Bi, M. Xue, and H. Wang. Synergizing rag and reasoning: systematic review. arXiv preprint arXiv:2504.15909, 2025. [19] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. TACL, 9:346361, 2021. [20] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [21] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. In COLING, 2020. [22] S. Hsu, O. Khattab, C. Finn, and A. Sharma. Grounding by trying: LLMs with reinforcement learning-enhanced retrieval. In ICLR, 2025. 11 [23] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. [24] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [25] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [26] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [27] J. Jiang, J. Chen, J. Li, R. Ren, S. Wang, W. X. Zhao, Y. Song, and T. Zhang. Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement. arXiv preprint arXiv:2412.12881, 2024. [28] Y. Jiang, S. Bordia, Z. Zhong, C. Dognin, M. Singh, and M. Bansal. Hover: dataset for many-hop fact extraction and claim verification. In Findings of EMNLP, pages 34413460, 2020. [29] B. Jin, J. Yoon, P. Kargupta, S. O. Arik, and J. Han. An empirical study on reinforcement learning for reasoning-search interleaved llm agents. arXiv preprint arXiv:2505.15117, 2025. [30] B. Jin, H. Zeng, Z. Yue, J. Yoon, S. Arik, D. Wang, H. Zamani, and J. Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [31] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts, and M. Zaharia. Demonstratesearch-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022. [32] T. Koˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis, and E. Grefenstette. The narrativeqa reading comprehension challenge. TACL, 2018. [33] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: benchmark for question answering research. TACL, 2019. [34] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. NeurIPS, 33, 2020. [35] M. Li, F. Feng, H. Zhang, X. He, F. Zhu, and T.-S. Chua. Learning to imagine: Integrating counterfactual thinking in neural discrete reasoning. In ACL, pages 5769, 2022. [36] X. Li, G. Dong, J. Jin, Y. Zhang, Y. Zhou, Y. Zhu, P. Zhang, and Z. Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025. [37] X. Li, S. Mei, Z. Liu, Y. Yan, S. Wang, S. Yu, Z. Zeng, H. Chen, G. Yu, Z. Liu, M. Sun, and C. Xiong. RAG-DDR: Optimizing retrieval-augmented generation using differentiable data rewards. In ICLR, 2025. [38] K. Lin, O. Tafjord, P. Clark, and M. Gardner. Reasoning over paragraph effects in situations. In Workshop on Machine Reading for Question Answering, 2019. [39] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Rodriguez, J. Kahn, G. Szilvasy, M. Lewis, L. Zettlemoyer, and W. tau Yih. RA-DIT: Retrieval-augmented dual instruction tuning. In ICLR, 2024. [40] A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. [41] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [42] T. Liu, H. Jiang, T. Wang, R. Xu, Y. Yu, L. Zhang, T. Zhao, and H. Wang. Roserag: Robust retrieval-augmented generation with small-scale llms via margin-aware preference optimization. arXiv preprint arXiv:2502.10993, 2025. [43] Z. Liu, Y. Chen, M. Shoeybi, B. Catanzaro, and W. Ping. Acemath: Advancing frontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084, 2024. [44] Z. Liu, X. Guo, F. Lou, L. Zeng, J. Niu, Z. Wang, J. Xu, W. Cai, Z. Yang, X. Zhao, et al. Fin-r1: large language model for financial reasoning through reinforcement learning. arXiv preprint arXiv:2503.16252, 2025. [45] Z. Liu, W. Ping, R. Roy, P. Xu, C. Lee, M. Shoeybi, and B. Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag. In NeurIPS, pages 1541615459, 2024. [46] P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In ICLR, 2023. [47] H. Ma, W. Xu, Y. Wei, L. Chen, L. Wang, Q. Liu, and S. Wu. Ex-fever: dataset for multi-hop explainable fact verification. In Findings of ACL, pages 93409353, 2024. [48] A. Mallen, A. Asai, V. Zhong, R. Das, D. Khashabi, and H. Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In ACL, 2023. [49] Y. Meng, M. Xia, and D. Chen. SimPO: Simple preference optimization with reference-free reward. In NeurIPS, 2024. [50] N. Muennighoff, H. Su, L. Wang, N. Yang, F. Wei, T. Yu, A. Singh, and D. Kiela. Generative representational instruction tuning. In ICLR, 2025. [51] OpenAI. Introducing gpt-4.1 in the api, 2025. [52] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 35, 2022. [53] O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith, and M. Lewis. Measuring and narrowing the compositionality gap in language models. In Findings of EMNLP, pages 56875711, 2023. [54] L. Qian, W. Zhou, Y. Wang, X. Peng, H. Yi, J. Huang, Q. Xie, and J. Nie. Fino1: On the transferability of reasoning enhanced llms to finance. arXiv preprint arXiv:2502.08127, 2025. [55] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. [56] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension of text. In EMNLP, 2016. [57] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. In Findings of EMNLP, 2023. [58] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W.-t. Yih. REPLUG: Retrieval-augmented black-box language models. In NAACL, 2024. [59] W. Shi, R. Xu, Y. Zhuang, Y. Yu, J. Zhang, H. Wu, Y. Zhu, J. Ho, C. Yang, and M. D. Wang. Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records. In EMNLP, pages 2231522339, 2024. [60] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. 13 [61] A. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, X. Garcia, P. J. Liu, J. Harrison, J. Lee, K. Xu, A. T. Parisi, A. Kumar, et al. Beyond human data: Scaling self-training for problem-solving with language models. TMLR, 2024. [62] H. Song, J. Jiang, Y. Min, J. Chen, Z. Chen, W. X. Zhao, L. Fang, and J.-R. Wen. R1searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. [63] H. Sun, Z. Qiao, J. Guo, X. Fan, Y. Hou, Y. Jiang, P. Xie, F. Huang, and Y. Zhang. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588, 2025. [64] Z. Sun, Q. Wang, W. Yu, X. Zang, K. Zheng, J. Xu, X. Zhang, S. Yang, and H. Li. Rearter: Retrieval-augmented reasoning with trustworthy process rewarding. arXiv preprint arXiv:2501.07861, 2025. [65] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [66] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal. Fever: large-scale dataset for fact extraction and verification. In NAACL, 2018. [67] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal. Musique: Multihop questions via single-hop question composition. TACL, 10:539554, 2022. [68] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal. Interleaving retrieval with chainof-thought reasoning for knowledge-intensive multi-step questions. In ACL, 2023. [69] A. W. Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000. [70] P. Verma, S. P. Midigeshi, G. Sinha, A. Solin, N. Natarajan, and A. Sharma. Plan-RAG: Planning-guided retrieval augmented generation, 2025. [71] H. Wang, R. Li, H. Jiang, J. Tian, Z. Wang, C. Luo, X. Tang, M. Cheng, T. Zhao, and J. Gao. Blendfilter: Advancing retrieval-augmented large language models via query generation blending and knowledge filtering. In EMNLP, pages 10091025, 2024. [72] L. Wang, H. Chen, N. Yang, X. Huang, Z. Dou, and F. Wei. Chain-of-retrieval augmented generation. arXiv preprint arXiv:2501.14342, 2025. [73] L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. [74] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. NeurIPS, 2022. [75] Z. Wei, W.-L. Chen, and Y. Meng. InstructRAG: Instructing retrieval-augmented generation via self-synthesized rationales. In ICLR, 2025. [76] Y. Wu, Z. Sun, H. Yuan, K. Ji, Y. Yang, and Q. Gu. Self-play preference optimization for language model alignment. In ICLR, 2025. [77] G. Xiong, Q. Jin, X. Wang, Y. Fang, H. Liu, Y. Yang, F. Chen, Z. Song, D. Wang, M. Zhang, et al. Rag-gym: Optimizing reasoning and search agents with process supervision. arXiv preprint arXiv:2502.13957, 2025. [78] R. Xu, H. Liu, S. Nag, Z. Dai, Y. Xie, X. Tang, C. Luo, Y. Li, J. C. Ho, C. Yang, and Q. He. SimRAG: Self-improving retrieval-augmented generation for adapting large language models to specialized domains. In NAACL, pages 1153411550, 2025. [79] R. Xu, W. Shi, Y. Zhuang, Y. Yu, J. C. Ho, H. Wang, and C. Yang. Collab-rag: Boosting retrieval-augmented generation for complex question answering via white-box and black-box llm collaboration. arXiv preprint arXiv:2504.04915, 2025. 14 [80] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [81] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [82] A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [83] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In EMNLP, 2018. [84] Z. Ye, R. Agarwal, T. Liu, R. Joshi, S. Velury, Q. V. Le, Q. Tan, and Y. Liu. Evolving alignment via asymmetric self-play. arXiv preprint arXiv:2411.00062, 2024. [85] W. Yu, M. Jiang, P. Clark, and A. Sabharwal. Ifqa: dataset for open-domain question answering under counterfactual presuppositions. In EMNLP, pages 82768288, 2023. [86] Y. Yu, W. Ping, Z. Liu, B. Wang, J. You, C. Zhang, M. Shoeybi, and B. Catanzaro. Rankrag: Unifying context ranking with retrieval-augmented generation in llms. In NeurIPS, pages 121156121184, 2024. [87] X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning. In ICLR, 2024. [88] Z. Yue, H. Zhuang, A. Bai, K. Hui, R. Jagerman, H. Zeng, Z. Qin, D. Wang, X. Wang, and M. Bendersky. Inference scaling for long-context retrieval augmented generation. In ICLR, 2025. [89] A. Zhao, Y. Wu, Y. Yue, T. Wu, Q. Xu, M. Lin, S. Wang, Q. Wu, Z. Zheng, and G. Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. [90] Y. Zhao, Y. Li, C. Li, and R. Zhang. Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data. In ACL, pages 65886600, 2022. [91] Y. Zhao, Y. Long, H. Liu, R. Kamoi, L. Nan, L. Chen, Y. Liu, X. Tang, R. Zhang, and A. Cohan. Docmath-eval: Evaluating math reasoning capabilities of llms in understanding long and specialized documents. In ACL, pages 1610316120, 2024. [92] Y. Zheng, D. Fu, X. Hu, X. Cai, L. Ye, P. Lu, and P. Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025. [93] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. V. Le, and E. H. Chi. Least-to-most prompting enables complex reasoning in large language models. In ICLR, 2023. [94] F. Zhu, W. Lei, Y. Huang, C. Wang, S. Zhang, J. Lv, F. Feng, and T.-S. Chua. Tat-qa: question answering benchmark on hybrid of tabular and textual content in finance. In ACL, 2021. [95] J. Zhu, Q. Chen, H. Dou, J. Li, L. Guo, F. Chen, and C. Zhang. Dianjin-r1: Evaluating and enhancing financial reasoning in large language models. arXiv preprint arXiv:2504.15716, 2025. [96] Q. Zhu, D. Guo, Z. Shao, D. Yang, P. Wang, R. Xu, Y. Wu, Y. Li, H. Gao, S. Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. 15 Derivation Step for Optimal Policy π and ρ We aim to maximize the following objective: (cid:104) Jθ = Eq Ezρθ (q), aπθ (q,z)[r(q, a, a)] β DKL (ρθρref ) β Ezρθ (q) [DKL (πθπref )] (cid:105) . Since ρ and π appear in separate terms, we can optimize them independently. 1. Optimal π for each z. For fixed z, consider the Lagrangian Lz(π, λz) = (cid:88) w,a π(w, q, z) r(q, a, a) β (cid:88) π(w, q, z) ln π(w, q, z) πref (w, q, z) w,a (cid:16)(cid:88) + λz w,a π(w, q, z) 1 (cid:17) . (A.1) (A.2) Taking the functional derivative with respect to π(w, q, z) and setting to zero gives r(q, a, a) β (cid:16) ln π(w, q, z) ln πref (w, q, z) + (cid:17) + λz = 0. Rearranging yields ln π(w, q, z) = ln πref (w, q, z) + 1 β r(q, a, a) + (cid:18) λz β (cid:19) 1 . (cid:124) (cid:125) (cid:123)(cid:122) constant in w,a Hence, the optimal policy is: π(w, q, z) = 1 Zπ(q, z) πref (w, q, z) exp (cid:19) r(q, a, a) (cid:18) 1 β where Now compute (π) so Therefore, Zπ(q, z) = (cid:88) w,a πref (w, q, z) exp (cid:19) r(q, a, a) (cid:18) 1 β log π(w, q, z) πref (w, q, z) = 1 β r(q, a, a) log Zπ(q, z) r(q, a) β log π(a q, z) πref (a q, z) = β log Zπ(q, z) (π) = Ew,aπ [β log Zπ(q, z)] = β log Zπ(q, z) 2. Optimal ρ. Substitute π back into Jq. Denote [ρ] = Ezρ (cid:20) β log Zπ(q, z) β log (cid:21) ρ(z q) ρref (z q) = βEzρ (cid:20) log ρref (z q)Zπ(q, z) ρ(z q) (cid:21) together with the constraint (cid:80) ρ(z q)β log Zπ(q, z) β ρ(z q) ln ρ(z q) = 1. Introduce multiplier µ and form (cid:32) (cid:88) ρ(z q) ρref (z q) + µ (cid:88) (cid:33) ρ(z q) . L[ρ, µ] = (cid:88) Taking ρ(z q) = β log Zπ(q, z) β (ln ρ(z q) ln ρref (z q) + 1) + µ = 0. 16 Rearranging: ln ρ(z q) = ln ρref (z q) + log Zπ(q, z) + (cid:19) . (cid:18) µ β The optimal policy is: where ρ(z q) ="
        },
        {
            "title": "1\nZρ(q)",
            "content": "ρref (z q)Zπ(q, z) Zρ(q) = (cid:88) ρref (z q)Zπ(q, z) Combining these two results yields exactly the stated closed-form solutions p(z q) pref (z q) E(w,a)pref(q,z) (cid:20) exp (cid:18) 1 β (cid:19) (cid:19)(cid:21) r(q, a, a) , (A.3) . (A.4) r(q, a, a) p(w, q, z) pref (w, q, z) exp (cid:18) 1 β"
        },
        {
            "title": "B Omitted Theorems and Proofs",
            "content": "B.1 Notion Let B(r, x) represent the l2-ball of radius centered at x. For two positive sequences {an} and {bn}, an bn if an Cbn. The l2 norm of vector Rd is defined as x2 := . 0, that is, Xn converges to 0 in probsequence of random variables Xn is said to be oP (1) if Xn ability as . The KullbackLeibler (KL) divergence from discrete distribution to discrete (cid:17) distribution (defined over common support ) is given by DKL [p q] := (cid:80) , under the assumption that whenever p(x) > 0, one also has q(x) > 0 for all . xX p(x) log (cid:16) p(x) q(x) i=1 x2 (cid:16)(cid:80)d (cid:17)1/2 B.2 Main theorem Recall the losses (4.2) and (4.5) are defined as follows: (cid:104) (cid:105) Ezρθ,(w,a)πθ [r(a, q, a)] βDKL [uθ(a, z, q)uref(a, z, q)] . Eq (B.1) LmDPO := (x,g+,g)D(t) pref (cid:32) (cid:34) log σ β log p(t+1) (g+ x) θ p(t) θ (g+ x) log (cid:35)(cid:33) p(t+1) (g x) θ p(t) θ (g x) . (B.2) To enable decomposition into decomposer and solver, we require the following assumption: assumption B.1 (Conditional Probability decomposition). We assume the following decomposition holds: pθ(a q) = (cid:88) pθ(z q) (cid:32) (cid:88) pθ(a q, z, w) pθ(w q, z) (cid:33) We present the informal version of our theorem below. Formal statements are given in Theorems B.2 and B.3. Theorem B.1 (Informal). Under regularity conditions, with high probability, the minimizer of the loss (B.2) at step is close to the minimizer of the loss (B.1). Furthermore, as increases, the minimizer converges to the true parameter θ. Remark B.1. The main theorem can be divided into two components. The first component establishes the equivalence between loss (B.1) and loss (B.2) are equivalent. The second component shows that, once the equivalence is established and the maximizer of loss (B.1) converges, the minimizer of loss (B.2) also converges. 17 The proof is organized as follows: In Appendices B.3 and B.4, we analyze the convergence properties of the maximizer of the population version loss (B.3) and sample version of loss (B.6) which corresponds exactly to loss (B.1). In Appendix B.5, we demonstrate the equivalence of loss (B.1) to loss (B.2). Finally, in Appendix B.6, building on these results, we prove that the minimizer of loss (B.2) converges as well. B.3 Population Version Based on the loss (B.1), define the population version loss as L(θ θt1) = E(q,a)pθ () (cid:2)Ezρθ(q) (cid:2)E(w,a)πθ(z,q)[r(a, q, a)](cid:3)(cid:3) βDKL (cid:0)uθ(a, z, q) uθt1 (a, z, q)(cid:1). (B.3) Define the operator : Θ Θ, (θ) = arg max θΘ L(θ θ), where Θ represents the parameter space. Notice that it is natural to assume that θ satisfy the self-consistency, i.e. θ = (θ). So the first assumption will be: assumption B.2 (Self-consistency). θ = (θ). assumption B.3 (λ-strong Concavity). There is some λ > 0 such that L(θ1 θ) L(θ2 θ) L(θ2 θ), θ1 θ2 θ1 θ22 for all θ1, θ2 B(r, θ). (B.4) Definition B.1 (First-order stability). The functions {L( θ), θ Θ} satisfy the First-order stability condition over B(r, θ) if λ 2 L(M (θ) θ) L(M (θ) θ)2 µθ θ2 for all θ B(r, θ). assumption B.4. Assume the functions {L( θ), θ Θ} satisfy the First-order stability condition (B.1). Proposition B.1 (Population Version). For some radius > 0 and pair (µ, λ) such that 0 µ < λ, suppose that the Assumption B.1-B.4 hold, then the population operator is contractive over B(r, θ), in particular with (θt1) θ2 µ λ θt1 θ2 for all θ B(r, θ). Proof. By the first order optimality condition, we have: L(θ θ), θ θ 0 θ L(θ θ), (θt1) θ 0 L(M (θt1) θt1), θ (θt1) 0 θ L(M (θt1) θt1), θ (θt1) 0. Combine the two terms, L(θ θ) L(M (θt1) θt1), (θt1) θ 0. Thus, L(θ θ) L(M (θt1) θ), (θt1) θ L(M (θt1) θ) L(M (θt1) θt1), (θt1) θ. For the right-hand side, by Cauchy-Schwarz inequality, L(M (θt1) θ) L(M (θt1) θt1), (θt1) θ L(M (θt1) θ) L(M (θt1) θt1)2M (θt1) θ2. 18 By Assumption B.4, L(M (θt1) θ) L(M (θt1) θt1)2M (θt1) θ2 µM (θt1) θ2 2. For the left-hand side, by Assumption B.3, (θt1) θ2 2 L(θ θ) L(M (θt1) θ) + L(θ θ), (θt1) θ, (θt1) θ2 2 L(M (θt1) θ) L(θ θ) + L(M (θt1) θ), θ (θt1). λ 2 λ 2 Hence, λM (θt1) θ2 2 L(θ θ) L(M (θt1) θ), (θt1) θ. (B.5) Combining all, λM (θt1) θ2 2 µM (θt1) θ2 2. Remark B.2. This theorem follows the idea in [3]. It suggests that, under self-play procedure, the algorithm progressively approaches the true underlying distribution. This behavior is characterized by contraction parameter µ λ , which ensures convergence toward the ground-truth parameter θ. The incorporation of an intermediate reasoning step smooths the local optimization landscape, rendering the loss approximately convex and thereby facilitating convergence to the global optimum. B.4 Sample Version We define the below sample version: assume we have the data Dq,a = {qi, ai}N i=1. The loss will be: LN (θ θt1) = E(q,a)(cid:101)pθ () (cid:2)Ezρθ(q) (cid:2)E(w,a)πθ(z,q)[r(a, q, a)](cid:3)(cid:3) βDKL (cid:0)uθ(a, z, q) uθt1 (a, z, q)(cid:1), (B.6) where (cid:101)p represents the empirical distribution defined as (cid:101)p(q, a) = 1 (cid:88) i=1 1{(q, a) = (qi, ai)}. We also have the similar convergence property. Similar to the population version, we define the sample-based operator MN : Θ Θ, MN (θ) = arg max θΘ LN (θ θ). For given sample size and tolerance parameter ϵ (0, 1), define ζ unif such that (N, ϵ) as the smallest scalar sup θB2(r;θ) MN (θ) (θ)2 ζ unif (N, ϵ) (B.7) with probability at least 1 ϵ. Proposition B.2 (Sample Version). Suppose that for all θ B(r, θ), the mapping satisfies (θt1) θ2 µ λ θt1 θ with probability at least 1 ϵ. Then we have MN (θt1) θ2 µ λ θt1 θ2 + ζ unif (N, ϵ), for all θ B(r, θ) with probability at least 1 ϵ. Proof. The result follows directly from the triangle inequality: MN (θt1) θ2 (θt1) θ2 + MN (θt1) (θt1)2 θt1 θ2 + ζ unif (N, ϵ). µ λ B.5 On the Equivalence with DPO setting - where or In the deterministic responses with the maximum and minimum rewards are selected - depending on the data tuple (cid:0)(amax, zmax, wmax), (amin, zmin, wmin), a, q(cid:1), we note that in practice the construction of positive and negative samples can vary, some containing or (z, w), and others including full triples such as (a, z, w). For simplicity, we unify the representation and consider the minimal component shared across all formats, namely the tuple (a, z, w). This process can thus be interpreted as observing finite dataset: is fixed and the = {((a+ , z+ , w+ ), (a , , ), ai, qi)}N i=1, Then the DPO loss will be: LmDPO(θ θt1) = E((a+,z+,w+),(a,z,w),a,q)D uθ(a+, z+, w+ q) uθt1(a+, z+, w+ q) βmDPO log σ log (cid:18) (cid:20) log uθ(a, z, q) uθt1 (a, z, q) (cid:21)(cid:19) (B.8) To demonstrate the closeness between the loss (B.6) and the loss (B.8), we first show that, with high probability, optimizing the loss (B.8) over the dataset is equivalent to maximizing the original reward up to scaling factor. Specifically, we can derive closed-form solution for Equation (B.6) at step t: (cid:19) uθ (a, z, q) uθt1(a, z, q) exp r(a, q, a) , (B.9) (cid:18) 1 β where θ be written as rγ consider the dataset D, which follows the following deterministic model: denotes the ground truth parameter at step t. Accordingly, the reward function r(a, q, a) can . Specifically, (a, q, a) to emphasize its dependence on the true reward parameter γ P((a+, z+, w+) (a, z, w) q) = 1 if rγ (a+ , q, a) > rγ (a , q, a), (B.10) indicating that we always select a+ we introduce the α-BT model: as the positive sample. To approximate this deterministic behavior, P((a+, z+, w+) (a, z, w) q) = eαrγ (a+,q,a) eαrγ (a+,q,a) + eαrγ (a,q,a) . (B.11) As α , the α-BT model becomes close to the deterministic model (B.10). Then given the above dataset D, we define the following data set Dα = {((a+ α,i, w+ α,i), (a α,i, α,i, α,i), ai, qi)}n i=1, α,i, z+ α,i, α,i)) is generated according to the α-BT model (B.11). To α,i, z+ α,i, α,i, w+ α,i), (a where ((a+ ensure the closeness between the dataset and Dα, we have the following lemma: assumption B.5 (Reward Seperation Condition). Assume that given (q, a), ((a+, z+, w+), (a, z, w)), there exists δ such that rγ Lemma B.1. Suppose the Assumption B.5 holds, given ϵ, there exists α0 log (a+, q, a) rγ (a, q, a) δ. 2ϵ , δ P(D = Dα0 ) 1 ϵ 2 . Proof. We start by bounding the probability of disagreement between two actions: for any (B.12) P((a+ α,i, z+ α,i, w+ α,i) = (a+ , z+ , w+ )) = (a+,q,a) + eαrγ The total probability that the datasets and Dα differ is bounded by eαrγ eαrγ (a,q,a) (a,q,a) 1 1 + eα0δ . P(D = Dα) = (cid:88) i=1 P(a+ = ) 1 + eα0δ . 20 Given α0 log 2ϵ δ , we conclude that P(D = Dα) = 1 P(D = Dα) 1 ϵ 2 . we can take our data generated according to the α0-BT model. In this case the new reward will be (a, q, a) = α0rγ (a, q, a). (B.13) (cid:101)rα0,γ Under this model, the minimizer of the loss (B.8) can be obtained via two-step optimization procedure [55]: Step 1: minimize the negative log-likelihood to obtain the reward: LN,NLL(γ α0) = E((a+,z+,w+),(a,z,w),a,q)D log σ(cid:0) (cid:101)rα0,γ(a+, q, a) (cid:101)rα0,γ(a, q, a)(cid:1), Denote the minimizer as (cid:101)rα0,(cid:98)γN,t(a, q, a). Step 2: maximize the reward (cid:101)rα0,(cid:98)γN,t(a, q, a): LREW(θ θt1) = E(a,q)(cid:101)pθ () The solution will be (cid:104) Ez,w(cid:101)pθt1 (a,q) βmDPODKL (cid:2)Eafθ(z,w,q,a)[(cid:101)rα0,(cid:98)γN,t(a, q, a)](cid:3)(cid:105) (cid:0)uθ(a z, w, q, a) uθt1(a z, w, q, a)(cid:1). (cid:98)θt (a z, w, q, a) uθt1(a z, w, q, a) exp (cid:18) βmDPO (cid:101)rα0,(cid:98)γN,t(a, q, a) (cid:19) , (B.14) (B.15) (B.16) This expression is identical to Equation (B.9), except that it uses different parameterization of the (a, q, a) is parameterized by the ground truth γ reward. Specifically, the reward function (cid:101)rα0,γ and hyperparameter α0. To ensure uniform consistency of the maximum likelihood estimator, we invoke the following lemma, which is modified from Theorem 5.7 in [69]. This result guarantees that (a, q, a). We need the following the minimizer in Step 1 converges to the true reward function (cid:101)rα0,γ assumption: assumption B.6. Suppose that there exists constant cα > 0, for every ϵ > 0, such that: sup α0[cα,) sup γΓ LN,NLL(γ α0) LNLL(γ α0) 0, (B.17) where Γ represents the parameter space and sup α0[cα,) sup γ:γγ 2ϵ (LNLL(γ α0) LNLL(γ α0)) < 0. (B.18) Lemma B.2 (Uniform MLE Consistency). Let LN,NLL(γ α0) be the negative log-likelihood function, and let LNLL(γ α0) denote its expected version. Let Assumption B.6 holds, then for the sequence of estimators (cid:98)γN,t obtained form minimizing the loss (B.14), we have: given ϵ > 0, there exists N1, when N1, for any α0 [cα, ), P(cid:0)(cid:98)γN,t γ 2 ϵ(cid:1) 1 (B.19) . ϵ 2 Proof. For given ϵ, according to the Equation (B.18), there exists cϵ,N LL, such that: sup α0[cα,) sup γ:γγ 2ϵ (LNLL(γ α0) LNLL(γ α0)) < cϵ,N LL. For cϵ,N LL, according to Equation (B.17), there exists N1, when N1, for any α0 [cα, ), (cid:16) (cid:16) LN,NLL((cid:98)γN,t α0) LNLL((cid:98)γN,t α0) α0) LNLL(γ LN,NLL(γ α0) cϵ,N LL 3 (cid:17) cϵ,N LL 3 (cid:17) 1 ϵ 4 , 1 ϵ 4 . Since (cid:98)γN,t is the minimizer of loss (B.14), for any α0 [cα, ), we have: LN,NLL((cid:98)γN,t α0) LN,NLL(γ α0) Consequently, (cid:18) (LNLL(γ α0) LNLL(γ0 α0)) (cid:19) 2cϵ,N LL 3 1 ϵ 2 . Thus for any α0 [cα, ), P(cid:0)(cid:98)γN,t γ (cid:18) 2 ϵ(cid:1) (LNLL(γ α0) LNLL(γ0 α0)) (cid:19) 2cϵ,N LL 1 ϵ 2 . Having established the necessary groundwork, we are now ready to present Theorem B.2, which establishes the equivalence between the minimizes of the two loss functions: Theorem B.2. Assume Assumptions B.5 and B.6 hold, given ϵ, there exists N1 and βmDPO log N1 the minimizer of loss (B.8) (cid:98)θt,mDPO will satisfy: δ β, 2ϵ (cid:16)(cid:13) (cid:13)(cid:98)θt,mDPO θ (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:17) ϵ < ϵ, (B.20) where θ is defined in Equation (B.9). Proof. First, according to the Lemma B.2, {(cid:98)γN1,t γ 2 ϵ}, , we have: P(Ω1) 1 ϵ 2 . there exists N1, if we define the event Ω1 = Secondly, just choose the sample size of as K, define the event Ω2 = {D = Dα0}, by Lemma B.1, when we take α0 , we have: cα (cid:18) (cid:19) log N1 2ϵ δ P(Ω2) 1 ϵ . Since cα is constant, we may, without loss of generality, take α0 log N1 . Henceforth, we restrict our analysis to the event Ω1 Ω2, which occurs with probability at least1 ϵ. Conditioned on this event, the data can be viewed as being generated from the α0-BT model (B.11). Consequently, the minimizer of the loss (B.8) coincides with that of Equation (B.16): 2ϵ δ (cid:98)θt,mDPO (a, z, q) uθt1(a, z, q) exp uθt1(a, z, q) exp (cid:18) 1 βmDPO (cid:18) α0 βmDPO (cid:19) (cid:101)rα0,(cid:98)γN1 ,t(a, q, a) (cid:19) . (cid:98)γN1,t(a, q, a) Compared to the solution in (B.9), when βmDPO = α0β log N1 tween (cid:98)θt, mDPO and θ Lemma B.2. Consequently, we obtain: reduces to controlling the distance between (cid:98)γN1, and γ δ β, controlling the distance bet , as established by 2ϵ This concludes the proof. (cid:16)(cid:13) (cid:13)(cid:98)θt,mDPO θ (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:17) ϵ < ϵ. 22 B.6 Convergence Property of DPO Finally, combining Proposition B.2, we conclude that the sequence (cid:98)θt,mDPO converges as increases. We formally state the following theorem: Theorem B.3. For given iteraton number , for some radius > 0 and pair (µ, λ) such that 0 µ < λ, suppose that the Assumption B.1-B.6 hold and assume (ϵ + ζ unif λ )r, then with probability at least 1 (T + 1)ϵ, we have: (N, ϵ)) < (1 µ (cid:13) (cid:13) (cid:13)(cid:98)θT,mDPO θ(cid:13) (cid:13) (cid:13)2 (cid:16) µ λ (cid:17)T θref θ2 + 1 1 µ λ ζ unif (n, ϵ) Proof. Notice that θ = MN ((cid:98)θt1,mDPO), apply Proposition B.2, we get: θ θ2 µ λ (cid:13) (cid:13) (cid:13)(cid:98)θt1,mDPO θ(cid:13) (cid:13) (cid:13) + ζ unif (N, ϵ) with probability at least 1 ϵ. Combining Theorem B.2, (cid:13) (cid:13) (cid:13)(cid:98)θt,mDPO θ(cid:13) (cid:13) (cid:13)2 µ λ (cid:13) (cid:13) (cid:13)(cid:98)θt1,mDPO θ(cid:13) (cid:13) (cid:13)2 + ϵ + ζ unif (N, ϵ), with probability at least 1 2ϵ. Notice that (ϵ + ζ unif Based on this, we can perform iteration: (N, ϵ)) (1 µ λ )r, then (cid:98)θt,mDPO B(r, θ). (cid:13) (cid:13) (cid:13)(cid:98)θT,mDPO θ(cid:13) (cid:13) (cid:13)2 µ λ µ λ (cid:13) (cid:13) (cid:13)(cid:98)θT 1,mDPO θ(cid:13) (cid:13) (cid:13)2 (cid:13)(cid:98)θT 2,mDPO θ(cid:13) (cid:13) (cid:13)2 1 (cid:88) (cid:16) µ λ (cid:17)T (cid:13) (cid:13) θref θ2 + (cid:16) µ λ (cid:17)T (cid:16) µ λ θref θ2 + s=0 1 1 µ λ + ϵ + ζ unif (N, ϵ) + ϵ + ζ unif (N, ϵ) (cid:17) (cid:17)s (ϵ + ζ unif (N, ϵ)) (cid:16) µ λ (ϵ + ζ unif (N, ϵ)) with probability at least 1 (T + 1)ϵ."
        },
        {
            "title": "C Information for Test Datasets",
            "content": "The information of the test datasets used in AceSearcher is listed in the following table. Note that We conduct evaluations on all questions from StrategyQA and Bamboogle, and the first 500 questions from the development sets of the other datasets following existing studies [68, 57, 36]. For dataset in DocMathEval, we use the testmini version as the evaluation set to compare the performance of AceSearcher and baselines. 23 Table 4: Descriptions of datasets used in AceSearcher. For SimpLong and CompLong, we use text-embedding-3 to retrieve top-10 relevant context before generate the answer. Dataset Description 2WikiMHQA [21] HotpotQA [83] Bamboogle [53] MusiQue [67] HOVER [28] ExFEVER [47] 2WikiMultiHopQA is multi-hop question answering dataset built from Wikipedia, It emphasizes where each question requires reasoning over two distinct articles. information synthesis across multiple documents for accurate answer retrieval. HotpotQA is crowd-sourced multi-hop QA dataset where each question demands reasoning over multiple Wikipedia passages. It also includes supporting fact annotations to promote explainability in QA systems. Bamboogle is multi-hop QA dataset constructed using Bing search engine snippets. It presents naturally occurring, challenging questions requiring reasoning over diverse web snippets rather than structured sources like Wikipedia. MusiQue is multi-hop QA dataset featuring real-world questions from community forums like Quora and Yahoo Answers. It targets complex questions requiring synthesis across multiple evidence passages, each carefully annotated. HOVER is multi-hop QA dataset with annotated supporting facts, built on entitylinked Wikipedia documents. It stresses explainable reasoning by providing intermediate evidence chains. ExFEVER extends the FEVER dataset by introducing multi-hop claims requiring evidence from multiple documents. It is designed to support research on fact verification and evidence-based reasoning. DMSS (DocMath SimpShort) dataset reannotated from TAT-QA [94] and FinQA [7], consisting of short financial documents with single table for simple numerical reasoning. DMCS (DocMath CompShort) dataset reannotated from TAT-HQA [35], consisting of short single-table documents for complex numerical reasoning, including hypotheticals. DMSL (DocMath SimpLong) dataset reannotated from MultiHiertt [90], consisting of long multi-table financial documents for simple reasoning in realistic contexts. DMCL (DocMath CompLong) dataset of long, structured financial documents requiring multi-step compositional numerical reasoning."
        },
        {
            "title": "D Details of Training Data",
            "content": "We provide the data composition for SFT and RFT, including their corresponding tasks, links to access the data, and the number we use in each stage in Table 5. To avoid data contamination, we follow the instructions in the MusiQue repository and remove the training data with overlapping IDs from NQ and Squad to avoid data leakage. Dataset Task Link Table 5: The data composition for SFT and RFT stages. Data composition for SFT NarrativeQA [32] Context-rich QA https://huggingface.co/datasets/deepmind/ narrativeqa SQuAD 1.1 [56] SQuAD 2.0 [56] TAT-QA [94] FEVER [66] DROP [16] Quoref [12] Context-rich QA https://rajpurkar.github.io/SQuAD-explorer/ Context-rich QA https://rajpurkar.github.io/SQuAD-explorer/ Context-rich QA https://github.com/NExTplusplus/TAT-QA/ tree/master/dataset_raw Context-rich QA https://fever.ai/dataset/fever.html Context-rich QA https://huggingface.co/datasets/ucinlp/drop Context-rich QA ROPES [38] Context-rich QA NQ [33] Context-rich QA GSM8K [11] Question Decomposition ConvFinQA [9] Question Decomposition https://github.com/czyssrs/ConvFinQA StrategyQA [19] Question Decomposition https://huggingface.co/datasets/allenai/ quoref https://huggingface.co/datasets/allenai/ ropes https://dl.fbaipublicfiles.com/dpr/data/ retriever/biencoder-nq-train.json.gz https://huggingface.co/datasets/openai/ gsm8k/viewer/socratic https://huggingface.co/datasets/ChilleD/ StrategyQA https://github.com/wyu97/IfQA/tree/main/ dataset https://promptpg.github.io/index.html# dataset https://huggingface.co/datasets/openai/ gsm8k/viewer/socratic https://huggingface.co/datasets/TIGER-Lab/ MathInstruct https://huggingface.co/datasets/TIGER-Lab/ MathInstruct https://github.com/hotpotqa/hotpot https://huggingface.co/datasets/xanhho/ 2WikiMultihopQA https://github.com/hover-nlp/hover IfQA [85] TabMWP [46] GSM8K [11] MathInstruct-COT [87] MathInstruct-POT [87] TOTAL Data composition for RFT HotpotQA [83] 2WikiMQA [21] HOVER [28] GSM8K [11] CoT CoT CoT CoT CoT RAG RAG RAG Context-rich Reasoning https://huggingface.co/datasets/openai/ gsm8k/viewer/socratic TabMWP [46] Context-rich Reasoning https://promptpg.github.io/index.html# dataset ConvFinQA [9] Context-rich Reasoning https://github.com/czyssrs/ConvFinQA Total 25 Count 20000 10000 10000 12000 10000 20000 10000 20000 7000 1000 2000 10000 7000 10000 10000 10000 10000 10000 7000"
        },
        {
            "title": "E Prompt Templates",
            "content": "E.1 Prompts for Direct RAG You have the following context passages: {context} Given the question: {question} as well as the context above, please answer the above question with one or list of entities with the given context as the reference. Your answer needs to be span with one or list of entities. Figure 6: Prompt for direct RAG on complex question answering tasks. Answer the following questions with SUPPORTED or NOT_SUPPORTED with the given context as the reference. Question: {question} Context: {context} Your answer should only be SUPPORTED or NOT_SUPPORTED. Figure 7: Prompt for direct RAG on fact verification tasks. You have the following passages and table: Passages: {passage} Tables: {table} For the question {question}, write Python program to solve the question. Store the final result in the variable ans. Figure 8: Prompt for direct RAG on document-level reasoning tasks with PoT. You have the following passages and table: Passages: {passage} For the question {question}, reason step by step to calculate the final answer. Please use boxed{} to wrap your final answer. Figure 9: Prompt for direct RAG on document-level reasoning tasks with CoT. E.2 Prompts for Decomposition Please break down the question {question} into multiple specific sub-questions that address individual components of the original question. Mark each sub-question with ### at the beginning. If you need to refer to answers from earlier sub-questions, use #1, #2, etc., to indicate the corresponding answers. Decomposed question: Figure 10: Prompt for question decomposition on complex question answering tasks. Please break down the claim {claim} into multiple smaller sub-claims that each focus on specific component of the original statement, making it easier for model to verify. Begin each sub-claim with ###. If needed, refer to answers from earlier sub-claims using #1, #2, etc. Decomposed claim: Figure 11: Prompt for question decomposition on fact verification tasks. You have the following passages and table: Passages: {passages} Tables: {tables} Please break down the question {question} into multiple specific sub-questions that address individual components of the original question, with the table and passages as the reference. Use ### to mark the start of each sub-question. Decomposed question: Figure 12: Prompt for question decomposition on document-level reasoning tasks. E.3 Prompts for subquestion answering You have the following context passages: {passages} Please answer the question {subquestion} with short span using the context as reference. If no answer is found in the context, use your own knowledge. Your answer needs to be as short as possible. Figure 13: Prompt for subquestion answering on complex question answering tasks. 27 You have the following context passages: {passages} Please verify whether the claim {subquestion} is correct using the context as reference. If no answer is found in the context, use your own knowledge. Please only output Yes or No and do not give any explanation. Figure 14: Prompt for subquestion answering on fact verification tasks. You have the following passages and tables: Passage: {passages} Table: {tables} For the question {subquestion}, write Python program to solve the question. Store the final result in the variable ans. Figure 15: Prompt for subquestion answering on document-level reasoning tasks with PoT. You have the following passages and tables: Passage: {passages} Table: {tables} For the question {subquestion}, reason step by step to calculate the final answer. Please use boxed{} to wrap your final answer. Figure 16: Prompt for subquestion answering on document-level reasoning tasks with CoT. E.4 Prompts for final answer generation You have the following passages: {passages} You are also given some subquestions and their answers: # subquestion #1: {subquestion_1} Answer: {answer_1} # subquestion #2: {subquestion_2} Answer: {answer_2} . . . Please answer the question {the_original_question} with short span using the documents and subquestions as reference. Make sure your response is grounded in documents and provides clear reasoning followed by concise conclusion. If no relevant information is found, use your own knowledge. Wrap your answer with <answer> and </answer> tags. Figure 17: Prompt for final answer generation on complex question answering tasks. 28 You are given some subquestions and their answers: # subquestion #1: {subquestion_1} Answer: {answer_1} # subquestion #2: {subquestion_2} Answer: {answer_2} . . . Please answer the question {the_original_question} with only Yes or No using the subquestions as reference. Provides clear reasoning followed by concise conclusion. If no relevant information is found, use your own knowledge. Wrap your answer with <answer> and </answer> tags. Figure 18: Prompt for final answer generation on fact verification tasks. You have the following passages and table: Passages: {passage} For the question {question}, here is referenced breakdown: {decomposition}. Write Python program to solve the question. Store the final result in the variable ans. Figure 19: Prompt for final answer generation on document-level reasoning tasks with PoT. You have the following passages and table: Passages: {passage} For the question {question}, here is referenced breakdown: {decomposition}. Reason step by step to calculate the final answer. Please use boxed{} to wrap your final answer. Figure 20: Prompt for final answer generation on document-level reasoning tasks with CoT. 29 E.5 Prompts for InstructRAG Read the following documents relevant to the given question: {question} Documents: {documents} ... Please identify documents that are useful to answer the given question: {question}. If none of the documents is aligned with the answer, in that case, you have to explain the answer only based on your own knowledge, without referring to the provided information. Note that the question may be compositional and require intermediate analysis to deduce the final answer. Make sure your response is grounded and provides clear reasoning details followed by concise conclusion. Your answer should be in short span with few keywords. Use <answer> and </answer> tag to mark your final answer. Figure 21: Prompt for InstructRAG on complex question answering tasks. Read the following documents relevant to the given question: {question} Documents: {documents} ... Please identify documents that are useful to answer the given question: {question}. If none of the documents is aligned with the answer, in that case, you have to explain the answer only based on your own knowledge, without referring to the provided information. Note that the question may be compositional and require intermediate analysis to deduce the final answer. Make sure your response is grounded and provides clear reasoning details followed by concise conclusion. Your answer should be yes or no only. Use <answer> and </answer> tag to mark your final answer. Figure 22: Prompt for InstructRAG on fact verification tasks."
        },
        {
            "title": "F Additional Implementation Details",
            "content": "F.1 Implementation Details for SFT For SFT, we set the batch size to 64 for every example, and set the learning rate as Table 7. With maximum number of tokens to 2560. Table 6: Results for different model sizes for SFT. Model Size Learning Rate Warmup Steps AceSearcher 1.5B AceSearcher 8B AceSearcher 14B AceSearcher 32B (LoRA) 5e 6 1e 6 1e 6 1e 5 5% 5% 5% 5% F.2 Implementation Details for RFT We set the hyperparameters to = 3, = 4, and = 1.0 when generating multiple rollouts. Examples with identical maximum and minimum rewards are discarded. For RFT, we use β = 0.1 Table 7: Results for different model sizes for RFT. Model Size Learning Rate Warmup Steps AceSearcher 1.5B AceSearcher 8B AceSearcher 14B AceSearcher 32B (LoRA) 1e 6 5e 7 5e 7 1e 6 5% 5% 5% 5% and run for the DPO for 2 iterations by default. All models are optimized using AdamW with β1 = 0.9 and β2 = 0.98, and experiments are conducted on 8 NVIDIA A100 GPUs. F.3 Implementation Details for Baselines We implement and evaluate variety of baselines using standardized decoding and prompting configurations to ensure fair comparison. For Qwen-3, we follow the official guidance5 to adopt distinct sampling strategies depending on the task setting. In thinking mode (enable_thinking=True), we use temperature = 0.6, top-p = 0.95, top-k = 20, and min-p = 0 to encourage diverse yet coherent generation. Greedy decoding is explicitly avoided to prevent performance degradation and repetitive outputs. In non-thinking mode (enable_thinking=False), we slightly increase the temperature to 0.7 and reduce top-p to 0.8 while keeping top-k and min-p unchanged. In practice, we find that using the thinking mode leads to slightly better performance despite being slower. For R1-distill models, we set the maximum generation length to 32,768 tokens and use temperature = 0.6, top-p = 0.95. In Plan-RAG, we incorporate 3-shot demonstrations in the prompt to guide the model toward producing outputs in the correct format. For InstructRAG, we use the same SFT training set as AceSearcher and generate CoT-style demonstrations tailored to context-rich QA datasets. For Llama-4, GPT-4.1, and GPT-4o, we use greedy decoding (temperature = 0) for consistency with their default inference behavior. For IRCOT and RAG-Star, we reproduce results by following the original repositories and hyperparameter settings. For these methods, we tune the number of retrieved passages from {5, 10, 20} and report the best performance. We refer to other baselines reported numbers in the corresponding paper."
        },
        {
            "title": "G Additional Experimental Results",
            "content": "Figure 23: Parameter Study on β Figure 24: Effect of different retrievers. (a) MusiQue (b) HotpotQA Effect of β. We study the effect of β in preference optimization with Llama-3.1-8B as the backbone, and find that AceSearcher is generally robust to this parameter, with β = 0.1 leads to slightly better performance. Effect of Different Retrievers. We evaluate AceSearcher and representative baselines (at the 8B scale) using two different retrievers: Dragon6 and Contriever7. Overall, the E5 retriever achieves 5https://huggingface.co/Qwen/Qwen3-32B#best-practices 6https://huggingface.co/facebook/dragon-plus-context-encoder 7https://huggingface.co/facebook/contriever-msmarco 31 Table 8: Performance comparison across models and prompting methods. Model Prompt Method DMSimpShort DMCompShort DMSimpLong DMCompLong Avg. 66.1 PoT AceSearcher-32B 62.4 PoT AceSearcher-14B 59.0 PoT AceSearcher-8B 47.6 AceSearcher-1.5B PoT 89.5 84.0 83.0 66. 53.0 49.0 48.0 39.0 43.0 39.3 32.3 18.0 84.0 82.0 80.5 77.5 CoT AceSearcher-32B CoT AceSearcher-14B AceSearcher-8B CoT AceSearcher-1.5B CoT 73.5 78.5 44.0 37.5 70.0 75.5 31.5 32. 50.0 44.0 30.0 18.0 33.0 34.7 15.7 9.7 54.5 57.0 28.5 23.2 the best performance, supporting our hypothesis that stronger retrieval models yield more relevant passages and thus enhance answer quality. Notably, AceSearcher consistently outperforms baselines across different retrievers, demonstrating its robustness to retrieval choices. Comparison of CoT and PoT for Document-level Reasoning. Table 8 presents comparison between Program of Thought (POT) and Chain of Thought (COT) prompting methods across four evaluation settings. POT consistently outperforms COT across all tasks, with notable improvements on both simple and complex reasoning benchmarks. For example, across models, POT yields higher average scores than COT on DMCompLong (e.g., 43.0 vs. 33.0 for AceSearcher-32b) and DMSimpShort (e.g., 89.5 vs. 73.5 for AceSearcher-32b), demonstrating its advantage in guiding structured reasoning. These results highlight the effectiveness of POT in enhancing model performance on decision-making tasks requiring multi-step reasoning, regardless of model scale."
        },
        {
            "title": "H Case Studies",
            "content": "Details of Human Study Our human study has received IRB approval from our institute. The below is the form used in human evaluation: You are provided with: The original complex question proposed decomposition into subquestions Please assess the overall quality of the decomposition using the criteria below. Relevance: Do the subquestions help solve the original question? Completeness: Are all important aspects covered? Coherence: Is the breakdown logically structured and easy to follow? Usefulness: Does the decomposition make the reasoning process easier or more interpretable? Please rate the overall quality of the subquestion decomposition. 1: Very Poor: Subquestions are irrelevant, incomplete, or incoherent. 2: Poor: Some relevance, but major gaps or unclear logic. 3: Fair: Moderately helpful with minor issues in coverage or clarity. 4: Good: Clear and mostly complete decomposition. 5: Excellent: Subquestions are highly relevant, complete, and well-structured, with proper reference to answers from previous subquestions. Details of Case Study We present two case studies from the MusiQue, 2WikiMHQA and DocMathEval datasets in Tables 9, 10 and 11. Despite its larger size, the Qwen3-32B-reasoning model struggles with handling complex 32 questions. In contrast, AceSearcher effectively decomposes each question into manageable subquestions, generates accurate answers for each sub-questions, and performs multi-step reasoning to arrive at the correct final answer. Table 9: case study from MusiQue dataset. sample from MusiQue Question: In which state is Vera Barbosas place of birth located? Ground Truth Answer: Lisbon District AceSearcher-8B Sub-question 1: Who is Vera Barbosa? Answer 1: Portuguese track and field athlete Sub-question 2: Where was Vera Barbosa born? Answer 2: Vila Franca de Xira Sub-question 3: In which state is #2 located? Answer 3: Lisbon District Final Answer: The documents state that Vera Barbosa is Portuguese track and field athlete and was born in Vila Franca de Xira. Additionally, Vila Franca de Xira is located in the Lisbon District. Therefore, Vera Barbosas place of birth, Vila Franca de Xira, is in the Lisbon District. <answer>Lisbon District</answer> Qwen3-32B Reasoning <think> Okay, lets see. The question is asking about the state where Vera Barbosas place of birth is located. First, need to figure out which Vera Barbosa theyre referring to here because there are several people named Vera Barbosa in the provided contexts. Looking at the first entry: Vera Barbosa, born 13 January 1989 in Vila Franca de Xira, Portugal. Thats Portuguese athlete. Then theres another Vera Krasova, Russian model, and Vera Begic, Croatian athlete. Theres also mention of place called Barbosa in Colombia and Brazil. But the key here is the name Vera Barbosa specifically. The user is asking about Vera Barbosas birthplace state. The first context says she was born in Vila Franca de Xira, which is in Portugal. However, the question is phrased as \"In which state is Vera Barbosas place of birth located?\" So, states are typically administrative divisions in countries like the US, Brazil, etc. Portugal is divided into districts, not states. But maybe the user is using \"state\" in general sense here. Wait, but looking at other entries, theres Carlos Barbosa in Brazils Rio Grande do Sul state, and Bárbara Barbosa is Brazilian footballer. But the specific Vera Barbosa mentioned in the first paragraph is the Portuguese athlete. Her birthplace is Vila Franca de Xira, Portugal. Since Portugal doesnt have states, but the question uses \"state,\" perhaps the answer is Portugal. Alternatively, if the user is confused and actually refers to different person, but based on the given data, the only Vera Barbosa with birthplace listed is the Portuguese one. Therefore, the answer should be Portugal. </think> <answer>Portugal</answer> 33 Table 10: case study from 2WikiMHQA dataset. sample from 2WikiMHQA Question: Which film has the director who was born later, The Silver Treasure or Taxi To Paradise? Ground Truth Answer: Taxi To Paradise AceSearcher-8B Sub-question 1: Who directed The Silver Treasure? Answer 1: Rowland V. Lee Sub-question 2: Who directed Taxi To Paradise? Answer 2: Adrian Brunel Sub-question 3: When was the director of #1 born? Answer 3: September 6, 1891 Sub-question 4: When was the director of #2 born? Answer 4: 4 September 1892 Sub-question 5: Is the year of #3 later than #4? Answer 5: no Final Answer: From the documents, The Silver Treasure was directed by Rowland V. Lee, born on September 6, 1891 (Q3), and \"Taxi To Paradise\" was directed by Adrian Brunel, born on September 4, 1892 (Q4). Comparing these dates, Adrian Brunel was born later than Rowland V. Lee (Q5 confirms this). Thus, the film with the director born later is Taxi To Paradise. <answer>Taxi To Paradise</answer> Qwen3-32B Reasoning <think> Okay, lets see. The user is asking which film has the director born later between The Silver Treasure and Taxi to Paradise. First, need to find the directors of both films. From the context: - The Silver Treasure (1926) is directed by Rowland V. Lee. - Taxi to Paradise (1933) is directed by Adrian Brunel. Now, need to check their birth dates. The context doesnt provide birth dates for these directors. So Ill have to rely on my own knowledge. Rowland V. Lee was born on April 22, 1894. Adrian Brunel was born on October 2, 1887. Wait, Brunel was born earlier than Lee? Wait, no: if Brunel was born in 1887 and Lee in 1894, then Brunel was born earlier. Therefore, Rowland V. Lee was born later. So The Silver Treasures director was born later than Taxi to Paradises director. Therefore, the answer is The Silver Treasure. </think> <answer>The Silver Treasure</answer> Table 11: case study from DMCS subset of the DocMathEval dataset. sample from DMCS subset of DocMathEval. Table: Year Segment TS 2019 2019 HPP 2019 Total 2019 % of Total 2018 TS 2018 HPP 2018 Total 2018 % of Total Americas Asia Europe (Amounts in thousands) $3,285 $771 $4,056 5% $9,059 $1,266 $10,325 14% $646 $1,837 $2,483 3% $1,344 $789 $2,133 3% $67,728 $5,294 $72,522 92% $52,034 $8,424 $60,458 83% Total % of Total $71,159 $7,902 $79,061 100% $62,437 $10,479 $72,916 100% 90% 10% 100% 86% 14% 100% Table 12: The following table details the Companys sales by operating segment for fiscal years ended September 30, 2019 and 2018. The Companys sales by geographic area based on the location of where the products were shipped or services rendered are as follows: Substantially all Americas amounts are United States. Question: What is the percentage change in Asia sales between 2018 and 2019 if the 2019 sales is doubled and increased by another 400 thousand? (in percent) Ground Truth Answer: 151.5705 AceSearcher-8B Sub-question 1: What was the value of Asia sales in 2019? Sub-question 2: What is the value of Asia sales in 2019 after doubling it? Sub-question 3: What is the value of Asia sales in 2019 after doubling it and then adding 400,000? Sub-question 4: What was the value of Asia sales in 2018? Sub-question 5: What is the net change in the value of Asia sales from 2018 to 2019, after adjusting the 2019 value? Sub-question 6: What is the percentage change in the value of Asia sales from 2018 to 2019, after adjusting the 2019 value? Final Answer: # Given data asia_sales_2019 = 2483 # in thousands (Q1) asia_sales_2018 = 2133 # in thousands (Q4) # Adjust 2019 sales: adjusted_2019_sales = (asia_sales_2019 * 2) + 400 double it and add 400 thousand (Q2, Q3) # Compute value change (Q5) change_in_sales = adjusted_2019_sales - asia_sales_2018 # Compute percentage change (Q6) ans = (change_in_sales / asia_sales_2018) * 151.57055789967183 Qwen3-32B Reasoning # Original 2019 Asia sales in thousands original_2019_asia = 646 # Adjusted 2019 sales after doubling and adding 400 thousand adjusted_2019_asia = original_2019_asia * 2 + 400 # Original 2018 Asia sales in thousands original_2018_asia = 1344 # Calculate percentage change percentage_change = ((adjusted_2019_asia - original_2018_asia) / original_2018_asia) * 100 ans = percentage_change 25."
        }
    ],
    "affiliations": [
        "Emory University",
        "Georgia Institute of Technology",
        "Rutgers University",
        "SUNY Albany",
        "UT Southwestern Medical Center"
    ]
}