{
    "paper_title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
    "authors": [
        "Ole Beisswenger",
        "Jan-Niklas Dihlmann",
        "Hendrik P. A. Lensch"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 0 7 6 6 1 . 2 1 5 2 : r FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering Ole Beisswenger ole.beisswenger@student.uni-tuebingen.de Jan-Niklas Dihlmann jan-niklas.dihlmann@uni-tuebingen.de Hendrik Lensch hendrik.lensch@uni-tuebingen.de Figure 1. G-buffer to Photorealistic Rendering. FrameDiffuser transforms geometric and material data from G-buffer into photorealistic rendered images with realistic global illumination (GI), shadows, and reflections. Our autoregressive approach maintains temporal consistency for long sequences, enabling neural rendering for interactive applications. Project page: https://framediffuser. jdihlmann.com/"
        },
        {
            "title": "Abstract",
            "content": "Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on frame-byframe basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After 1 an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches. 1. Introduction Complete AI-driven simulation of interactive environments represents one of the most challenging objectives in neural computing. Projects like Oasis AI [3] attempt to simulate entire games using single neural network, generating each frame based on user input without any traditional game engine or rendering pipeline. However, even for visually simple games like Minecraft [3], these approaches struggle with fundamental issues: environments change dramatically when players turn around, geometry is hallucinated inconsistently, and control remains imprecise and laggy. Modern games employ deferred rendering pipelines that generate geometry buffers (G-buffer) storing per-pixel surface properties [26, 27]. Recent research has leveraged Gbuffer for neural image and video synthesis [20, 32]. RGBX [32] demonstrated that diffusion models can effectively leverage G-buffer information for high-quality deterministic single-image generation. DiffusionRenderer [20] extended this concept to temporally consistent video sequences. These approaches leverage the precise geometric and material information in G-buffer as strong conditioning signals, reducing AI-hallucination or misinterpretations of the scene structure while still leaving room for neural models to generate the lighting and appearance in those bounds. However, interactive applications such as video games are overlooked by current research in this field. These applications require frame-by-frame generation where each new frame depends on player input and therefore cannot be predetermined. Single-image models like RGBX [32] generate each image independently, lacking temporal consistency between consecutive frames. Video models like DiffusionRenderer [20] need complete sequences at once, making them unsuitable for the sequential, frame-by-frame generation that interactive applications demand. Furthermore, existing G-buffer-conditioned models aim for broad generalization across diverse environments. While valuable for general-purpose applications, real-world deployment in games often operates within specific visual domains. game maintains its particular art style and lighting behavior throughout. Inspired by earlier work like ESTGAN [21], which used environment-specific training with G-buffer conditions, we explore whether specializing models for specific domains could yield superior consistency compared to broadly generalized approaches. In this work, we present FrameDiffuser, neural rendering framework that fills the void through autoregressive, temporally consistent frame generation conditioned on Gbuffer data. We train environment-specific models for six different Unreal Engine 5 [10] environments, demonstrating how specialization achieves superior consistency within specific domains. FrameDiffuser transforms G-buffer containing geometry and material properties into photorealistic rendered images with realistic lighting, shadows, and reflections. The model operates autoregressively using only incoming G-buffer data while conditioning on its own previously generated output, enabling stable generation over hundreds to thousands of frames. By combining precise geometric and material information from G-buffer with temporal conditioning on previous frames, our approach addresses the unique requirements of frame-by-frame generation for interactive applications. Our work makes the following contributions: Dual-Conditioning Architecture: We develop an architecture separating structural guidance (ControlNet with G-buffer + irradiance) from temporal coherence (ControlLoRA with previous frame latents). Three-Stage Training Strategy for Autoregressive Viability: We demonstrate that our progressive training approach starting with black irradiance, introducing temporal conditioning, then self-conditioning is critical for preventing error accumulation in autoregressive generation. Irradiance-Based Temporal Lighting Guidance: We introduce novel approximate irradiance computation from previous frame outputs that provides strong temporal lighting cues while enabling autoregressive generation without ground-truth lighting information. Environment-Specific Specialization: We demonstrate benefits of domain-specific training over generalization for interactive applications. Our approach explores path where AI augments and supports traditional rendering, preserving artist control over world building while demonstrating potential for individualized AI-based augmentation in interactive applications. By working within the constraints of frame-by-frame generation and embracing domain specialization, we investigate how neural rendering can complement existing pipelines rather than attempting to replace them entirely. 2. Related Work Image Generation. Neural image generation has evolved dramatically from early GAN-based approaches to modern diffusion models. Generative Adversarial Networks (GANs) [13] pioneered high-quality neural image synthesis through adversarial training between generator and discriminator networks. However, GANs suffer from training instability and mode collapse, limiting their practical deployment. The emergence of diffusion models [14] marked paradigm shift in generative modeling. These models learn to reverse gradual noising process, achieving superior image quality and training stability compared to GANs [5]. Stable Diffusion [25] further improved efficiency by operating in compressed latent space, enabling high-resolution generation with reduced computational requirements. 2 Figure 2. FrameDiffuser Architecture with dual conditioning: ControlNet processes 10-channel input comprising 9 G-buffer channels for structural guidance and 1 pred. irradiance channel for lighting guidance, computed from the previous frames model output and basecolor. ControlLoRA conditions on the previous frame encoded in VAE latent space for temporal coherence. The generated output at time is used to compute the irradiance input for the next frame at time + 1, enabling autoregressive frame generation. The encoder and decoder represent the VAE components operating in latent space. The training strategy on the right shows our three-stage approach: first, we train ControlNet on the G-buffer to image translation task without irradiance. Second, we add ControlLoRA and irradiance for temporal conditioning. Third, we train autoregressively using the models own generated frames as previous-frame inputs to make the model robust against its own generation errors. The success of diffusion models spurred research into fine-tuning techniques rather than training from scratch. ControlNet [34] introduced precise spatial control through additional conditioning networks, enabling structural guidance from depth maps, normal maps, and other inputs. LoRA [16] provided efficient adaptation through low-rank matrices, reducing trainable parameters while maintaining performance. Video Generation. While image generation models produce individual frames with high quality, video generation requires temporal consistency across sequences. Video diffusion models like Stable Video Diffusion [1] and earlier work by Ho et al. [15] generate entire sequences simultaneously, learning spatio-temporal patterns across complete videos. They produce complete sequences at once rather than frame-by-frame, making them unsuitable for interactive applications where future frames depend on user input. Autoregressive Generation for Interactive Applications. Interactive applications require user-driven frame-by-frame generation. Recent projects like Oasis AI [3] and DeepMinds work [29] explore complete game world simulation but struggle with geometric consistency and control, suggesting that combining traditional rendering with AI augmentation may be more feasible. key challenge in autoregressive generation is preventing drift, where errors accumulate over time. Without mechanisms to handle distribution shift between training on perfect inputs and inference on generated inputs, model output degrades rapidly [33]. Self-Conditioning and Robustness Techniques. The autoregressive drift problem has motivated research into robustness techniques. Self Forcing [17] and related work [2, 4, 19] demonstrate that training models on their own generated outputs bridges the train-test distribution gap, which is essential for stable autoregressive generation. Models without self-conditioning mechanisms fail at extended generation due to rapid error accumulation. G-Buffer-Based Neural Rendering. parallel line of research explores using intermediate render passes from traditional graphics pipelines to guide neural generation. Deferred rendering [26, 27], widely used in modern game engines [9, 28], produces geometry buffers (G-buffer) that store per-pixel surface properties including depth, normals, basecolor, roughness, and metallicity. EST-GAN [21] demonstrated that incorporating Gbuffer data including depth, normal, and basecolor maps significantly improved visual quality compared to semantic maps alone. They pioneered environment-specific training on particular game environments to achieve superior consistency within those domains. However, this GAN-based approach was limited not only by lack of temporal consistency but also by the inherent limitations of GANs compared to modern diffusion models. Recent work has advanced G-buffer-based rendering using diffusion models. RGBX [32] presents bidirectional framework handling both forward G-buffer-to-RGB and in3 verse RGB-to-G-buffer rendering. For the forward render task, they trained Stable Diffusion 2.1 with increased input channels to accommodate G-buffer data. Their intrinsic switch mechanism and channel dropout training enable generation with incomplete G-buffer sets, achieving highquality single-image synthesis. However, RGBX focuses on individual images without temporal coherence. Xue et al. [31] address these limitations through improved training strategies, notably demonstrating that using ControlNet for G-buffer conditioning achieves better results than direct channel concatenation. DiffusionRenderer [20] extends G-buffer-based neural rendering to video generation, processing normal, basecolor, depth, roughness, and metallic maps to generate temporally coherent sequences. Their lighting representation combines tone mapping, logarithmic intensity encoding, and directional information, achieving impressive visual quality for complete video sequences. However, like other video diffusion models, DiffusionRenderer does not operate autoregressively, making it unsuitable for interactive applications where future G-buffer depend on user input. Summarizing, image models like RGBX generate high-quality individual images but lack temporal consistency. Video models like DiffusionRenderer excel at temporal coherence but require generating entire sequences at once. Our work fills this gap by combining G-buffer conditioning with autoregressive generation. Inspired by ESTGANs environment-specific training [21], we train specialized models for individual environments rather than attempting broad generalization. 3. Method We present FrameDiffuser, neural rendering framework for autoregressive frame generation conditioned on Gbuffer data. Our dual-conditioning architecture separates structural guidance from temporal coherence, enabling stable frame-by-frame generation for interactive applications. Figure 2 illustrates the overall architecture. 3.1. Architecture FrameDiffuser builds on Stable Diffusion 1.5, which is pure text-to-image diffusion model, augmenting it with two complementary conditioning mechanisms that enable autoregressive frame generation. ControlNet for Structural Guidance. We employ ControlNet [34] to process 10-channel input comprising basecolor, Normals, Depth, Roughness, Metallic, and an additional irradiance channel. The first nine channels represent common G-buffer data: basecolor (3 channels), Normals (3 channels), Depth, Roughness, and Metallic. ControlNet creates trainable copy of the UNet encoder with zeroinitialized connections, ensuring that training starts from the pre-trained state without disruption. This pathway establishes the mapping from geometric and material properties to photorealistic appearance. Irradiance as Temporal Lighting Condition. We introduce an irradiance map It1 that encodes temporal lighting information. Unlike RGBX [32] which can use ground-truth irradiance channels making lighting approximation for the forward render task almost trivial, we derive irradiance from the models previous generated output and the basecolor channel, creating lighting intensity map that provides ControlNet with strong guidance about shadow and specular locations: It1(x, y) = L(Ft1(x, y)) L(Ct1(x, y)) + ϵ , (1) where L() converts RGB to grayscale, Ft1 is the previous generated output, Ct1 is the previous basecolor, and ϵ = 106. Values are clamped to [0, 2] and normalized for network input. This provides ControlNet with temporal lighting guidance while maintaining separation from ControlLoRAs role in temporal appearance consistency. ControlLoRA for Temporal Coherence. We use ControlLoRA [30] to maintain temporal consistency by conditioning on the previous frame. Low-rank adaptation matrices applied to convolutional and linear layers throughout the UNet enable parameter-efficient fine-tuning. The previous frame is encoded through the VAE and concatenated to the noisy latent input in latent space, providing temporal appearance information and light color consistency at every denoising step. ControlLoRA nudges the generation to remain temporally consistent, particularly regarding lighting color and overall appearance. Generation initiates from single starting frame and its corresponding irradiance, from which all subsequent frames are synthesized autoregressively. 3.2. Training Strategy We employ three-stage training methodology where irradiance conditioning plays central role in preventing temporal dependency and enabling stable autoregressive generation. Stage 1: Structural Learning with Black Irradiance. Only ControlNet trains while the base UNet remains frozen. ControlLoRA is not yet introduced. We provide ControlNet with the nine G-buffer channels and use all zeros for the irradiance channel, forcing ControlNet to learn the core translation from G-buffer geometry and material properties to photorealistic rendering. Starting with black irradiance ensures that ControlNet establishes strong reliance on the deterministic G-buffer guidance for the rendering task. This ensures the previous frame data serves only for temporal consistency while G-buffer remain the primary structural guidance. 4 Figure 3. Qualitative Results showing FrameDiffusers autoregressive generation across multiple frames, including long-term stability at frame 4000. From top to bottom: ground truth, our autoregressive output, and the G-buffer channels (basecolor, Normal, Depth, Roughness, Metallic) alongside computed Irradiance. The model maintains temporal consistency and accurate material properties across extended sequences. Stage 2: Temporal Coherence Introduction. Both ControlNet and ControlLoRA train together. We now provide irradiance computed from previous frames to ControlNets temporal channel. Thus, ControlNet receives the 9 G-buffer channels plus 1 irradiance channel. ControlLoRA receives the previous frame encoded in VAE latent space and concatenated to the noisy latent. By injecting noise to the previous frame, we force the model to still effectively utilize the G-buffer, as the irradiance and previous frame latent do not provide completely reliable data. Stage 3: Self-Conditioning for Robustness. Both ControlNet and ControlLoRA continue training together. We now introduce self-conditioning by periodically injecting generated frames into the training that are produced using the current model weights. The previous frame latent serves as ControlLoRA conditioning input. Irradiance maps are computed from these generated frames and fed to ControlNet. The generated frames are now used recursively as conditioning for subsequent generated training samples. Training on the models own imperfect outputs addresses the distribution mismatch between perfect training data and autoregressive deployment. The model becomes resilient to its own artifacts, preventing exponential quality degradation during extended autoregressive sequences. This training strategy ensures ControlNet first learns the translation from G-buffer to photorealistic rendering, while ControlLoRA nudges the generation toward temporal consistency with the previous frame. 3.3. Temporal Coherence Mechanisms Temporal Offset Sampling. We sample temporal offsets δ {2, ..., 2} with higher weights for adjacent frames to prevent overfitting to single-frame transitions. Sky Region Handling. Sky regions present challenge as deferred rendering provides no geometric data for skyboxes, resulting in empty G-buffer regions. Without geometric guidance, sky regions exhibit temporal inconsistencies. We address this through adaptive masking that injects sky information into the basecolor channel: Mij = (cid:40) 1 0 < τ and d(ij) < τ if c(ij) otherwise (2) where τ = 10/255 identifies black pixels in basecolor ct and Depth dt. The modified basecolor becomes: = (1 Mij) c(ij) c(ij) + Mij x(ij) (3) Figure 4. Qualitative Comparison with XRGB across Downtown West (urban) and Hillside Sample (indoor) environments. Our method achieves high-detail lighting while maintaining temporal consistency across frames over long sequences, while XRGB applies more uniform lighting. This provides ControlNet with complete scene information while maintaining clear differentiation through unchanged Depth, Normal, and material channels. Noise Injection. We apply noise to the previous RGB frame before irradiance computation (Equation 1): tδ = xtδ + σ ϵ, σ U(0.0, 0.2), ϵ (0, I) (4) Both conditioning paths use the same noisy frame, encouraging stronger G-buffer utilization. 4. Experiments and Results We evaluate FrameDiffuser on autoregressive frame generation, comparing against baseline methods and analyzing our three-stage training strategy on temporal stability and generation quality. 4.1. Experimental Setup Datasets. We train and evaluate separate models for each of our six Unreal Engine 5 environments: Electric Dreams [11], City Sample [8], Hillside Sample Project [12], Downtown West [23], City Park [22], and Derelict Corridor [24]. Each environment provides held-out validation splits covering diverse lighting conditions, material distributions, and geometric complexity. Metrics. We employ standard image quality metrics: SSIM (Structural Similarity Index) for structural preservation, PSNR (Peak Signal-to-Noise Ratio) for pixel-level accuracy, and LPIPS (Learned Perceptual Image Patch Similarity) for perceptual alignment. Baselines. We compare against XRGB from RGBX [32], recent neural rendering method using image diffusion models for G-buffer-based synthesis. with samples shuffled during training. We build upon the pretrained Stable Diffusion 1.5 [25] with ControlNet (10channel input) and ControlLoRA (rank-64). Training uses batch size 2 with gradient accumulation of 4, AdamW optimizer (weight decay 102), and cosine learning rate scheduling. Stage 1 trains ControlNet only with black irradiance (40k steps, LR 2 105). Stage 2 introduces temporal conditioning with real irradiance and noise injection (10k steps). Stage 3 adds self-conditioning with 50% generated frames (30k steps). Training utilized NVIDIA RTX 4090 and A100 GPUs. Inference employs 10 denoising steps with DPMSolver, achieving approximately 1 frame per second on an RTX 4090. 4.2. Quantitative Results Table 1. Quantitative Comparison on autoregressive frame generation. Results are averaged across validation sets from FrameDiffusers training distribution. Method XRGB [32] FrameDiffuser PSNR SSIM LPIPS 8.60 18.34 0.3566 0. 0.5150 0.2129 Table 1 shows quantitative comparison on held-out validation sets from each models training environment. FrameDiffuser achieves higher SSIM and lower LPIPS compared to XRGB, indicating better structural preservation and perceptual quality, demonstrating the effectiveness of our approach. 4.3. Ablation Studies 4.3.1. Self-Conditioning and Noise Injection Implementation. Training samples consist of previous frame and G-buffer pairs to generate the next frame, We evaluate the impact of self-conditioning and noise injection on autoregressive generation quality using the Elec6 Table 3. Irradiance Conditioning Ablation on City Park environment validation sequences with total of 900 frames. Metrics averaged over autoregressive generation. Method PSNR SSIM LPIPS Without Irradiance With Irradiance 19.443 19.998 0.435 0.440 0.255 0.246 Figure 5. Temporal Stability Analysis over 3000+ consecutive validation frames of pure autoregressive generation inside the Hillside Sample Project Environment [12] Figure 4. We compare our model output against pure VAE reconstruction to measure encoder degradation. Metrics show degradation between frames 8001700 when the camera enters very dark rooms; the models bias towards lit areas causes it to insufficiently capture extreme darkness. After frame 3000, all metrics degrade when camera movement reduces and G-buffer changes become minimal, causing error accumulation. tric Dreams environment. Table 2 compares models trained with and without these mechanisms on the MidRange validation sequence. Without self-conditioning and noise injection, autoregressive generation exhibits rapid quality degradation within 5 frames, as the model trained exclusively on ground-truth inputs cannot process its own imperfect outputs during inference. With self-conditioning and noise injection incorporated during training phases 2 and 3, quality remains stable over extended sequences (Figure 6). These mechanisms yield substantial improvements across all metrics: PSNR increases by 5.87 dB, SSIM improves by 40.6%, and LPIPS decreases by 41.3%, demonstrating that the model learns to handle imperfect inputs and maintain generation quality through temporal information propagation. Table 2. Self-conditioning and Noise Injection Ablation on MidRange sequence (Electric Dreams [11], 589 frames). Metrics averaged over autoregressive generation. Method PSNR SSIM LPIPS Without SC + NI With SC + NI 12.29 18.16 0.323 0.454 0.431 0.253 Improvement +47.8% +40.6% +41.3% Figure 6. Self-conditioning (SC) and Noise Injection (NI) Impact on autoregressive generation. Without these mechanisms, severe degradation occurs within 5 frames. With self-conditioning and noise injection, quality remains stable over extended sequences. 4.3.2. Irradiance Conditioning We evaluate the contribution of our irradiance mechanism by comparing models trained with and without the irradiance channel. Table 3 shows results on 900 validation frames from the City Park [22] environment, chosen for its combination of dense vegetation and urban structures. The irradiance channel provides small but consistent improvements across all metrics, which also proves the strong guidance G-buffer provide alone. 4.4. Qualitative Results Figure 3 shows FrameDiffuser outputs across multiple frames, including long-term generation at frame 4000. The model maintains temporal consistency and accurate geometric and material properties in extended sequences. Irradiance conditioning provides stable lighting cues, while Gbuffer inputs ensure geometric consistency. FrameDiffuser can convincingly approximate realistic shadows as can be seen with the overpass in frame 4000 and understands to effectively use material properties, as can be observed with the effect of different roughness scales on the road and sidewalk and the shiny reflection of the car. This becomes more apparent when one looks at the irradiance channel, which is computed by the previous model output and the previous base color input. Beyond the geometric and material information in Gbuffer, FrameDiffuser synthesizes photorealistic lighting including global illumination, shadows, reflections, and atmospheric effects that would traditionally require expensive ray tracing or pre-baked lightmaps. Figure 7. Scene Editing when objects are added to the scene through G-buffer modifications, FrameDiffuser automatically synthesizes appropriate lighting, shading, and cast shadows. Left: barrel insertion demonstrates correct lighting direction and shadow casting. Right: large concrete statue shows the models ability to infer complex shadow patterns and lighting intensities on geometric additions, maintaining photorealistic consistency. 4.5. Scene Editing Capabilities Figure 7 demonstrates FrameDiffusers ability to handle interactive scene modifications through G-buffer editing. When objects are added to the scene, such as the barrel insertion shown on the left, the model automatically infers lighting direction to generate appropriate shading and cast shadows. The cat statue example on the right shows the models capability to synthesize complex shadow patterns and lighting intensities on large geometric additions. This enables artists to maintain full control over scene composition while FrameDiffuser handles the computationally expensive lighting synthesis automatically. 4.6. Comparison with Baselines We compare FrameDiffuser with XRGB [32] in Figure 4. XRGB produces images that appear artificially flat, closely resembling the raw G-buffer inputs with uniform lighting, lacking the rich lighting variation, shadow depth, and atmospheric effects present in photorealistic rendering. XRGB also exhibits severe temporal inconsisin the Hillside indoor example, lighting changes tencies: drastically between Frame 1 and Frame 3, with door frame shadows appearing and disappearing, and floor lighting patterns shifting completely. In contrast, FrameDiffuser generates natural-looking scenes with realistic global illumination while maintaining temporal consistency across frames. 5. Conclusion FrameDiffuser enables frame-by-frame neural rendering for interactive applications through G-buffer conditioning and autoregressive generation. Our dual-conditioning architecture combines ControlNet for geometric guidance with ControlLoRA for temporal coherence, achieving stable generation where existing approaches fall short: single-image models like RGBX lack temporal consistency, while video models like DiffusionRenderer cannot accommodate user-driven frame generation. Our three-stage training strategy with self-conditioning proves essential for bridging the train-test distribution gap, enabling temporal consistency across extended sequences. We specialize models to individual environments rather than pursuing broad generalization, reflecting practical deployment needs where consistent rendering within specific visual domains matters more than moderate generalization across all environments. 5.1. Limitations and Future Work Our environment-specific approach prioritizes consistency over generalization, requiring separate models for different visual styles. While current inference speeds reach approximately 1 frame per second on consumer hardware, distillation techniques and architectural optimizations present clear paths toward real-time performance. Future work could combine video diffusion architectures like FramePack [33] with our G-buffer conditioning approach, or explore integration with recent 3D scene generation and reconstruction methods [6, 7], merging DiffusionRenderers temporal modeling strengths with interactive generation requirements. Further, experimenting if world knowledge capabilities like changing scene style via prompting are still accessible are interesting directions to explore in future research. 6. Acknowledgements Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy EXC number 2064/1 Project number 390727645. This work was supported by the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, TP 02, project number: 276693517. This work was supported by the Tubingen AI Center. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Jan-Niklas Dihlmann."
        },
        {
            "title": "References",
            "content": "[1] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [2] B. Chen, D. Zheng, R. Barzilay, and T. Jaakkola. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In Advances in Neural Information Processing Systems, 2024. 3 [3] Decart AI. Oasis: Ai-generated game. https://oasisai.org/, 2024. The first fully AI-generated game where every frame is generated in real-time at 20 frames per second. 2, 3 [4] E. Denton and R. Fergus. Stochastic video generation with In International Conference on Machine learned prior. Learning, pages 11741183, 2018. 3 [5] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:87808794, 2021. [6] Jan-Niklas Dihlmann, Andreas Engelhardt, and Hendrik P. A. Lensch. Signerf: Scene integrated generation for neural radiance fields. arXiv preprint arXiv:2401.01647, 2024. 8 [7] Andreas Engelhardt, Mark Boss, Vikram Voleti, Chun-Han Yao, Hendrik P. A. Lensch, and Varun Jampani. Svim3d: Stable video material diffusion for single image 3d generation. arXiv preprint arXiv:2510.08271, 2025. 8 City sample. https : / / www . fab . com / listings / 4898e707 - 7855 - 404b - af0e - a505ee690e68, 2022. free downloadable sample project demonstrating how the city scene from The Matrix Awakens: An Unreal Engine 5 Experience was built, featuring World Partition, Nanite, Lumen, Chaos, and other UE5 technologies. 6, 1, 4 [8] Epic Games. [9] Epic Games. https : / / Forward shading renderer. dev . epicgames . com / documentation / en - us / unrealengine/forwardshadingrendererinunreal- engine?application_version=5. 0, 2022. Documentation of Unreal Engines deferred and forward rendering pipelines. 3 [10] Epic Games. https : / / www . unrealengine.com/enUS/unrealengine5, 2022. Real-time 3D creation tool featuring Nanite virtualized Unreal engine 5. geometry, Lumen global illumination, and advanced rendering capabilities. 2 [11] Epic Games. Electric dreams environment sample project. https://www.unrealengine.com/enUS / electric - dreams - environment, 2023. demo showcasing the Procedural Content Generation framework (PCG) and Substrate material authoring system in Unreal Engine 5.2, featuring jungle environment constructed with Quixel assets. 6, 7, [12] Epic Games, Safdie Architects, and Neoscape. Hillside sample project. https://www.fab.com/listings/ 3277687b - a06f - 4ef7 - a285 - 63b981768c4a, 2024. Unreal Engine architectural sample project showcasing Moshe Safdies original unbuilt architectural design proposal and Habitat 67, demonstrating UE5 rendering features including Nanite, Lumen, Path Tracing, and Virtual Shadow Maps. 6, 7, 1, 3, 4 [13] I.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in Neural Information Processing Systems, 27, 2014. 2 [14] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 2 [15] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D.J. Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022. 3 [16] E.J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 3 [17] S. Huang, B. Chen, M. Liu, and K. Zhang. Self forcing: Bridging the train-test gap in autoregressive video diffusion. In International Conference on Machine Learning, 2025. 3 [18] Akio Kodaira, Chenfeng Xu, Toshiki Hazama, Takanori Yoshimoto, Kohei Ohno, Shogo Mitsuhori, Soichi Sugano, Hanying Cho, Zhijian Liu, and Kurt Keutzer. Streamdiffusion: pipeline-level solution for real-time interactive generation. 2023. 4 [19] Y. Li, H. Zhang, X. Wang, and L. Chen. Ar-diffusion: Asynchronous video generation with auto-regressive diffusion. In Proceedings of the International Conference on Computer Vision, 2025. [20] R. Liang, Z. Gojcic, H. Ling, J. Munkberg, J. Hasselgren, C.H. Lin, J. Gao, A. Keller, N. Vijaykumar, S. Fidler, and Z. Wang. Diffusion renderer: Neural inverse and forward rendering with video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2606926080, 2025. 2, 4, 1 [21] M. Mittermueller, Z. Ye, and H. Hlavacs. Est-gan: Enhancing style transfer gans with intermediate game render passes. In 2022 IEEE Conference on Games (CoG), pages 2532, 2022. 2, 3, 4 [22] Polyart Studio. City park environment collection. https: / / www . fab . com / listings / 11cc2abb - 126c - 4452-9fe4-6f2381d96544, 2020. comprehensive city park environment spanning over 200 acres, featuring park assets, vegetation, and urban structures. 6, 7, 1, 4 9 [23] PurePolygons. Downtown west modular pack. https: / / www . fab . com / listings / 0faf8b5d - 7a5f - 4fee-a297-7a8efaba8896, 2025. Modular urban environment pack with 350+ unique static meshes, 250+ materials, and 550+ 4K textures, featuring ray tracing support and comprehensive city-building assets. 6, 1, 4 [24] Quixel. Derelict corridor megascans sample. https: / / www . fab . com / listings / d825ff9c - 77da - 45a49619c989e6dfdda6, 2025. haunting environment featuring deteriorated corridors with distressed walls, realistic debris, and atmospheric lighting, showcasing Megascans photorealistic assets. 6, [25] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684 10695, 2022. 2, 6 [26] Takafumi Saito and Tokiichiro Takahashi. ComprehensiIn Proceedings of the 17th ble rendering of 3-d shapes. Annual Conference on Computer Graphics and Interactive Techniques, pages 197206. ACM, 1990. 2, 3 [27] Oles Shishkovtsov. Deferred shading in s.t.a.l.k.e.r. In GPU Gems 2: Programming Techniques for High-Performance Graphics and General-Purpose Computation, chapter 9, pages 143166. Addison-Wesley Professional, 2005. 2, 3 [28] Unity Technologies. Deferred rendering path. https: / / docs . unity3d . com / Packages / com . unity . renderpipelines.universal@12.0/manual/ rendering / deferred - rendering - path . html, 2023. Unity Documentation on the built-in deferred rendering path. 3 [29] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837v2, 2025. 3, 2 [30] H. Wu. Controllora version 3: Lora is all you need to control the spatial information of stable diffusion. Source code available at: https://github.com/HighCWu/ control-lora-v3, 2024. 4 [31] B. Xue, G.C. Guarnera, S. Zhao, and Z. Montazeri. Diffusion-based g-buffer generation and rendering. arXiv preprint arXiv:2503.15147, 2025. [32] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. RGBX: Image decomposition and synthesis using materialand lighting-aware diffusion models. SIGGRAPH Conference Papers 24, 2024. 2, 3, 4, 6, 8 [33] L. Zhang and M. Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. 3, 8 [34] L. Zhang, A. Rao, and M. Agrawala. Adding conditional In Proceedings control to text-to-image diffusion models. of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 3, 4 10 FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering"
        },
        {
            "title": "Supplementary Material",
            "content": "This appendix offers supplementary information on FrameDiffuser. It provides comparison with DiffusionRenderer [20] explaining our methodological differences (Sec. A), presents out-of-distribution generalization results (Sec. B), analyzes temporal flicker and its sources (Sec. C), presents GBufferDiffuser for inverse rendering (Sec. D), explores style transfer capabilities (Sec. E), and details our implementation specifications (Sec. F). Additional Material We wish to emphasize the inclusion of videos alongside this paper. Given the temporal nature of our results, these videos serve as the most effective medium for their evaluation. We strongly encourage viewing: FrameDiffuser hillside sequence.mp4 Pure autoregressive generation in the Hillside Sample Project environment, showing ground truth versus our model versus basecolor for reference. Towards the end, the camera flies through moderately dark room and then into completely dark room. While the model the predictions approximates reasonable illumination, are far from the ground truth which is completely dark in these areas. Such mismatches can be observed in the Temporal Stability Analysis figure. DiffusionRenderer comparison CityPark.mp4 Comparison between FrameDiffuser and DiffusionRenderer on City Park environment sequences, demonstrating FrameDiffusers superior lighting accuracy. DiffusionRenderer comparison Hillside.mp4 Comparison between FrameDiffuser and DiffusionRenderer on Hillside Sample Project sequences. vae flicker citysample.mp4 Video showing the temporal flicker artifacts in the City Sample The video compares our metropolitan environment. model output against pure VAE reconstruction, demonstrating that the VAE encoder-decoder cycle is the primary source of frame-to-frame inconsistencies, particularly in scenes with high spatial frequencies. A. DiffusionRenderer Comparison We did not include DiffusionRenderer [20] as baseline in the main paper due to fundamental differences in generation paradigm. DiffusionRenderer is designed for video generation, processing complete sequences at once with access to all frames during generation. In contrast, FrameDiffuser operates autoregressively, generating each frame based only on the current G-buffer and the previously generated frame. This autoregressive approach is essential for interactive applications such as video games, where future frames cannot be predetermined before user input. The users actions determine the next G-buffer state, making it impossible to provide future frame information during generation. Despite these methodological differences, we conducted quantitative comparison on 24-frame sequences, the maximum length DiffusionRenderer supports, even though FrameDiffuser can generate arbitrarily long sequences autoregressively. Results are averaged across validation sets from six environments. Table 4. DiffusionRenderer Comparison on 24-frame sequences. Results averaged across validation sets from all six training environments. Method PSNR SSIM LPIPS DiffusionRenderer [20] FrameDiffuser (Ours) 13.03 20.96 0.4683 0.6378 0.4583 0.2030 Figure 8 shows qualitative comparisons across two sequences from City Park [22] and Hillside Sample Project [12]. FrameDiffuser maintains closer alignment with ground truth lighting and atmospheric effects throughout the sequences, while DiffusionRenderer produces results with inconsistent lighting and reduced detail. B. Out-of-Distribution Generalization We evaluate out-of-distribution generalization using model trained on Downtown West [23] and tested on City Sample [8]. Downtown West features Pearl Street Mallinspired architecture with warm, sunny lighting, trees, and colorful storefronts, while City Sample presents grey, cloudy skies and concrete-dominated metropolitan architecture. Figure 9 shows the out-of-distribution generalization test. We use the prompt photorealistic metropolitan city with skyscrapers and streets under overcast grey skies for generation. The figure displays an example frame from the Downtown West training environment (top left) and the starting frame for generation (top right). Below, we show frames 1, 50, and 100 comparing ground truth, our model, and XRGB. At frame 1, the model output remains close to the ground truth as it can derive the appropriate style from the starting frame. However, as the autoregressive generation progresses, the model increasingly diverges from the cold, grey City Sample aesthetic towards more of the warm, vibrant style learned from Downtown West. By frames 50 1 Figure 8. DiffusionRenderer Comparison across two environments. GT: Ground Truth, Ours: FrameDiffuser, DR: DiffusionRenderer [20]. FrameDiffuser maintains closer alignment with ground truth lighting and atmospheric effects. See accompanying videos DiffusionRenderer comparison CityPark.mp4 and DiffusionRenderer comparison Hillside.mp4. and 100, we observe the model introducing warmer lights and more saturated colors, demonstrating how the learned environment-specific characteristics gradually dominate the output during extended autoregressive generation. Table 5 shows FrameDiffuser achieves superior performance despite training on single and very different environment. Table 5. Out-of-Distribution Performance. Downtown West model evaluated on City Sample (100 frames). Method PSNR SSIM LPIPS XRGB [32] FrameDiffuser (Ours) 7.52 14.17 0.255 0.454 0.442 0.363 C. Temporal Flicker Analysis provide vae flicker citysample.mp4 to We demonstrate temporal flicker in our generated sequences. As can be seen when comparing to the pure VAE reconstruction, the VAE of Stable Diffusion 1.5 [25] introduces flicker, especially in scenes with high spatial frequencies like the metropolitan city example in the video. We strongly believe that with different VAE this flicker could be substantially reduced or eliminated. However, training or fine-tuning specialized VAE requires significant computational resources and falls outside the scope of this work, which focuses on the frame generation pipeline itself. Potential solutions include finetuned VAE decoders, such as the approach by Valevski et al. [29] for GameNGen, or VAEs that incorporate previous frame context. D. GBufferDiffuser: Inverse Rendering While forward rendering (G-buffer to RGB) is the main focus of this work, we also developed GBufferDiffuser for 2 Figure 9. Out-of-Distribution Generalization. Top left: Downtown West training environment. Top right: Starting frame for generation. Bottom: Frames 1, 50, and 100 from 100-frame autoregressive sequence on City Sample, showing GT (top row), FrameDiffuser (middle), and XRGB (bottom). The warm Downtown West style is slightly transferred to the cold City Sample environment. inverse rendering (RGB to G-buffer), similar to the inverse capabilities demonstrated by RGBX [32]. This system employs five independent ControlLoRA models with rank128 matrices, each specialized for reconstructing specific G-buffer component: basecolor, Depth, Normals, Roughness, and Metallic. Each model conditions on the final rendered image to reconstruct its respective component. In contrast to generalist approaches that train single model across multiple G-buffer types and environments, we train smaller specialized adapters for each component on single environment. We trained each model for only 10k steps on the Hillside Sample Project [12] environment with the same batch size and gradient accumulation as FrameDiffuser. This specialization approach allows for faster training while achieving superior results within the target domain, at the cost of generalization to arbitrary environments. Table 6 shows quantitative results comparing GBufferDiffuser against RGBX from RGBX [32]. GBufferDiffuser substantially outperforms the generalist baseline across all G-buffer components. Depth reconstruction is not supported by RGBX. Table 6. GBufferDiffuser: Inverse Rendering on Hillside Sample Project validation sequence. RGBX does not support Depth reconstruction. Component Method PSNR SSIM LPIPS BaseColor Normals Roughness Metallic RGBX Ours RGBX Ours RGBX Ours RGBX Ours Depth Ours 12.60 19.39 5.01 10.77 13.32 22.30 6.42 14. 12.92 0.756 0.792 0.146 0.426 0.679 0.806 0.017 0.242 0. 0.398 0.172 0.700 0.387 0.527 0.202 0.917 0.272 0.496 Figure 10 shows qualitative comparisons for basecolor, Roughness and Metallicity. GBufferDiffuser produces more accurate reconstructions with better preservation of fine details, while RGBX struggles particularly with Metallic reconstruction. E. Style Transfer We explored artistic control through style transfer by applying first-frame augmentation. The process works as follows: given an original rendered frame, we apply Stable Diffusions image-to-image transformation with stylespecific text prompt (e.g., Forest fire, flames, smoke, burning trees) to create stylized version. This stylized frame then serves as the previous frame input for the first generFigure 10. GBufferDiffuser Qualitative Comparison. GBufferDiffuser produces more accurate reconstructions with better shape and color prediction compared to RGBX [32], due to specialization to the G-buffer type and environment. ated frame, and generation continues autoregressively. Additionally, fitting prompt for the new style is used. Figure 11 shows an example with burning style. The original frame is transformed into burning scene, which then conditions the subsequent autoregressive generation. Fire effects remain visible but are gradually diminishing as the model reverts to its training distribution. The experiments revealed limited style transfer capabilities due to the models specialization on training environments with fixed prompts. After training on single prompt-environment combination, the models ability to respond to alternative text conditioning decreased. Figure 11. Style Transfer: Burning Style. The original frame is stylized via image-to-image transformation (Edited), then used as previous frame input for autoregressive generation. Fire effects persist through temporal propagation but gradually diminish as the model reverts to its training distribution. F. Implementation Details Dataset Details Table 7 shows the training and validation set sizes for each environment. All sets consist of consecutive frame pairs with corresponding G-buffers. Valida3 tion sequences vary in length from 24 frames for short-term evaluation to over 3500 frames for long-term autoregressive stability analysis. Table 7. (rounded values). Dataset Statistics for each training environment Environment Train Validation Electric Dreams [11] City Sample [8] City Park [22] Derelict Corridor [24] Hillside Sample Project [12] Downtown West [23] 40,000 46,000 23,000 32,000 18,100 41, 600 11,500 5,800 8,100 3,500 10,000 Training Configuration The training configuration was optimized for NVIDIA RTX 4090 and A100 GPUs. We use batch size 2 with gradient accumulation of 4 for an effective batch size of 8. The AdamW optimizer is configured with weight decay 102 and cosine learning rate scheduling. Stage 1 trains ControlNet with learning rate 2 105 for 40k steps. Stages 2 and 3 continue with reduced learning rates for 10k and 30k steps respectively. The ControlLoRA configuration uses rank-64 matrices targeting all linear and convolutional layers. We employ gradient clipping with max norm = 1.0 and enable gradient checkpointing for memory optimization. Inference uses classifier-free guidance scale of 1.0, which achieved best results for in-distribution environments. Hardware and Performance Training requires approximately 23GB VRAM when training both ControlNet and ControlLoRA components. Inference achieves approximately 1 frame per second on an RTX 4090 with 10 denoising steps using the DPMSolver scheduler. Optimization Potential Significant optimization potential exists through methods not explored in this work, ranging from simple techniques like batching and mixed precision to more advanced approaches such as StreamDiffusion [18], model distillation, or recent NVIDIA DLSS advancements."
        }
    ],
    "affiliations": [
        "University of Tübingen"
    ]
}