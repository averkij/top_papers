{
    "paper_title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings",
    "authors": [
        "Shanghaoran Quan",
        "Jiaxi Yang",
        "Bowen Yu",
        "Bo Zheng",
        "Dayiheng Liu",
        "An Yang",
        "Xuancheng Ren",
        "Bofei Gao",
        "Yibo Miao",
        "Yunlong Feng",
        "Zekun Wang",
        "Jian Yang",
        "Zeyu Cui",
        "Yang Fan",
        "Yichang Zhang",
        "Binyuan Hui",
        "Junyang Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the increasing code reasoning capabilities of existing large language models (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3, there is a growing need to develop more challenging and comprehensive benchmarks that effectively test their sophisticated competition-level coding abilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to the unavailability of private test cases, lack of support for special judges, and misaligned execution environments. To bridge this gap, we introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. We compile the recent six months of contest problems on CodeForces with detailed information such as contest divisions, problem difficulty ratings, and problem algorithm tags. We introduce a unique judging method in which problems are submitted directly to the platform and develop a reliable Elo rating calculation system that aligns with the platform and is comparable with human participants but has lower variance. By testing on our CodeElo, we provide the Elo ratings of 30 existing popular open-source and 3 proprietary LLMs for the first time. The results show that o1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of 1578 and 1261, respectively, while other models struggle even with the easiest problems, placing in the lowest 20 percent among all human participants. Detailed analysis experiments are also conducted to provide insights into performance across algorithms and comparisons between using C++ and Python, which can suggest directions for future studies."
        },
        {
            "title": "Start",
            "content": "CODEELO: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings 2025-01-"
        },
        {
            "title": "Jiaxi Yang Bowen Yu Bo Zheng Dayiheng Liu An Yang\nJian Yang",
            "content": "Zeyu Cui Yang Fan Yichang Zhang Binyuan Hui(cid:66) Junyang Lin(cid:66) Qwen Team, Alibaba Group {quanshanghaoran,binyuan.hby,junyang.ljy}@alibaba-inc.com https://CodeElo-bench.github.io https://hf.co/Qwen/CodeElo"
        },
        {
            "title": "Abstract",
            "content": "With the increasing code reasoning capabilities of existing large language models (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3, there is growing need to develop more challenging and comprehensive benchmarks that effectively test their sophisticated competitionlevel coding abilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to the unavailability of private test cases, lack of support for special judges, and misaligned execution environments. To bridge this gap, we introduce CODEELO, standardized competitionlevel code generation benchmark that effectively addresses all these challenges for the first time. CODEELO benchmark is mainly based on the official CodeForces1 platform and tries to align with the platform as much as possible. We compile the recent six months of contest problems on CodeForces with detailed information such as contest divisions, problem difficulty ratings, and problem algorithm tags. We introduce unique judging method in which problems are submitted directly to the platform and develop reliable Elo rating calculation system that aligns with the platform and is comparable with human participants but has lower variance. By testing on our CODEELO, we provide the Elo ratings of 30 existing popular open-source and 3 proprietary LLMs for the first time. The results show that o1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of 1578 and 1261, respectively, while other models struggle even with the easiest problems, placing in the lowest 20 percent among all human participants. Detailed analysis experiments are also conducted to provide insights into performance across algorithms and comparisons between using C++ and Python, which can suggest directions for future studies. 5 2 0 2 2 ] . [ 1 7 5 2 1 0 . 1 0 5 2 : r Figure 1: The Elo rating leaderboard. The test results may be slightly lower than the actual performance, as we constrain the number of submissions for each problem to eight times. (cid:66)Corresponding authors 1https://codeforces.com"
        },
        {
            "title": "Introduction",
            "content": "With the increasing capabilities of existing LLMs and breakthroughs in reasoning models like OpenAI o1 and o3, there is growing need to develop benchmarks that effectively test their sophisticated reasoning abilities. Math and coding are two evaluation methods for this purpose, as they provide accurate and easily verifiable feedback. While math presents hard benchmarks, like AIME (MAA, 2024), Omni-MATH (Gao et al., 2024), and LiveAoPSBench (Anonymous, 2024), theres still lack of suitable benchmark in coding to appropriately measure LLMs reasoning skills. We find that the CodeForces platform is suitable, and in the reports of OpenAI o1 (OpenAI, 2024b), o3 (OpenAI, 2024c) and DeepSeek r1 (DeepSeek, 2024), they all test hard code on CodeForces. However, we still lack standardized CodeForces test suite, leading to repeated compiling and creating work on CodeForces and potential misaligned settings and results. These situations highlight the need for research community to standardize CodeForces evaluation benchmark to assess the competition-level coding capabilities of LLMs and reflect their corresponding sophisticated reasoning abilities. We analyze that existing competition-level code benchmarks like LiveCodeBench (Jain et al., 2024), USACO (Shi et al., 2024), and CodeContests (Li et al., 2022) cannot meet the above requirement due to certain limitations attributed to the following unique nature of competition code evaluation. (1) Firstly, unlike typical math and general coding problems, competition code problem often requires extensive, human-designed, robust test cases to validate solution correctness. Although it is easy to scrape any problems from the websites, the competition code platforms or so-called online judges often hide their test cases. Consequently, existing benchmarks can only rely on publicly available or self-generated test cases that are often small and weak, leading to high false-positive rates in judgment2. (2) Secondly, to truly evaluate LLM capabilities and provide human-comparable Elo ratings, its necessary to test on all problems in contests just as human participants do. However, not all problems can be judged by directly comparing the output with the correct answer, as about 30% of problems do not have unique correct outputs, which require specific judging codes called special judges3 to evaluate correctness. However, these special judges are often unavailable and difficult to write for some specific problems. (3) Thirdly, different from general code testing, execution time is significantly critical factor in competitive coding since problems often have strict constraints on running times and algorithm time complexity, but offline testing faces challenges due to varied efficiency across different machines, leading to potential misaligned evaluation results. Hence, there remains lack of comprehensive standardized benchmarks for competition-level coding. In this work, for the first time, we present the benchmark CODEELO and the corresponding evaluation method, which achieves zero false positives, supports special judges, and reaches an aligned execution environment to ensure absolutely correct judgments and provide human-comparable, standardized Elo ratings. We have compiled our test problems from CodeForces and categorized them by contest divisions, problem difficulty ratings, and problem algorithm tags for more comprehensive evaluation and analysis. To achieve absolutely correct judgments, we introduce simple and effective method: using bot to automatically submit model solutions to CodeForces and receive test results. Thanks to the judgment from the platform, this allows us to evaluate problems just like human participants, achieving zero false positives without needing access to the full test set, where test cases are often created adversarially and are robust. Similarly, given the favor of the platform judgments, we support special judges, which former benchmarks did not provide, and the execution environment is absolutely aligned since all use the same platform, even aligned with human participants. Based on the evaluation results and available user data on CodeForces, we also introduce an Elo rating calculation system that estimates the models expected Elo rating based on their performance. This system is aligned with the CodeForces platform but has much lower variance, as detailed in Section 3.3.2. We tested 30 existing popular open-source LLMs and 3 proprietary LLMs, with the leaderboard shown in Figure 1. From the results, we find the OpenAI o1-mini model stands out by achieving the best performance with an Elo rating of 1578, surpassing nearly 90 percent of human participants. The o1-like reasoning model, QwQ-32B-Preview, also achieves great performance and stands out among open-source models, with an Elo rating of 1261, placing it in about the 60th percentile. This suggests that increasing the length of the chain-of-thought (CoT) is promising way to improve the models reasoning ability. On the other hand, most models struggle to pass the easiest problems and fall within the lowest 10th percentile of Elo ratings among human participants, and several large and prominent open-source LLMs fall in the range of the 10th to 20th percentiles. Through more detailed experiments and analysis, we find that models are good at problems involving math and implementation, but struggle with dynamic programming (dp) and trees. We also find that for most models, the best-performing language is C++, instead of Python, which is most frequently used by LLMs and is also the main test language in previous benchmarks. We hope our CODEELO can pave the way for testing advanced LLMs code reasoning capabilities and provide insights to improve such abilities in LLMs. To summarize, our benchmark has the following main contributions, with more detailed discussion on them 2Passing all tests in CodeContests might still result in wrong answer (4%) or time limit error (42%), according to https://alphacode.deepmind.com/ 3Special judges are programs used to determine whether solutions are accepted for problems that do not have unique correct answer. detailed discussion is provided in Appendix F. 2 available in Section 6.1. We provide set of Codeforces problems with detailed information like contest divisions, problem ratings, and problem algorithm tags. We introduce unique evaluation method in which problems are submitted directly to the platform, achieving zero false positives, special judge support and fully aligned execution environment for the first time. We are the first to provide standardized human-comparable Elo ratings that fairly judge the models competitionlevel code generation for the existing popular open-source and proprietary LLMs. detailed analysis of experimental results provides insights that can suggest future studies."
        },
        {
            "title": "2 Related Work",
            "content": "Before our work, there were several competitive code benchmarks. Here, we list six representative ones that are most relevant to our work: APPS (Hendrycks et al., 2021): Proposed in 2021/05, APPS curated problems from Codewars, AtCoder, Kattis, and CodeForces. CodeContests (Li et al., 2022): Proposed in 2022/03, CodeContests includes problems, solutions, and test cases sourced from the CodeForces platform. xCodeEval (Khan et al., 2023): Proposed in 2023/03, xCodeEval is large-scale multilingual multitask code benchmark that also includes problems from CodeForces. TACO (Li et al., 2023): Proposed in 2023/12, TACO compiles problems from CodeContests and APPS, adding new problems gathered from several competitive coding websites. LiveCodeBench (Jain et al., 2024): Proposed in 2024/03, LiveCodeBench primarily contains scraped coding problems and test cases from LeetCode and AtCoder, with minimal content from CodeForces. It avoids contamination by re-scraping new problems every month and releasing online updates. USACO (Shi et al., 2024): Proposed in 2024/04, USACO features hundreds of problems and test cases from the USA Computing Olympiad. It updates its benchmark through new released versions. We include comparison against these benchmarks in Table 1. We find that these benchmarks all source problems from open-access coding competition websites and conduct offline evaluations. While scraping problems is simple, most online judges hide their test cases. To address this, existing benchmarks attempt to generate test cases; however, these are often not as robust as the original ones (which are often designed in an adversarial manner and require lot of labor from high-level participants), leading to many false-positive judgments. Additionally, these benchmarks do not support special judges. Another limitation is that they require execution on an individuals machine. Since runtime is critical factor in algorithm competitions, differing machine performance can affect results. Furthermore, none of these benchmarks offer human-comparable standardized Elo ratings. These characteristics highlight the uniqueness and significant advantages of our benchmark."
        },
        {
            "title": "Updates",
            "content": "Zero False Positive? Special Judge?"
        },
        {
            "title": "CODEELO",
            "content": "No updates No updates No updates No updates Offline Online"
        },
        {
            "title": "Online",
            "content": "Aligned Execution Environment? Standardized Elo Rating? Table 1: Comparison between CODEELO and other competition code benchmarks. Apart from competition code benchmarks, there are also some other more popular general code benchmarks, like HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and BigCodeBench (Zhuo et al., 2024). These benchmarks have three significant differences with competition code benchmarks: 1) While we need to analyze the problem and write complete code with focus on algorithm design, general benchmarks usually require writing only small function to test specific functionality. 2) The problems in our test set are much harder than these general benchmarks, often needing sophisticated reasoning while general benchmarks do not. 3) Execution time is significantly crucial constraint, as we not only judge the output for correctness but also demand on the algorithm complexity, whereas general benchmarks mainly focus on the former."
        },
        {
            "title": "3 CODEELO Benchmark",
            "content": "Our benchmark is primarily based on the well-known coding competition platform, CodeForces. In this section, we discuss the development of our benchmark, detailing the processes of problem collection, classification and categorization, and the evaluation method."
        },
        {
            "title": "3.1 Problem Collection",
            "content": "Our testing problems are sourced from the official CodeForces platform. We gathered all problems from the rated contests held since the platform inception (but we only tested on the recently held contests to avoid data contamination). An example problem can be found in Appendix Figure 4. The scraped problems were originally in raw HTML format. We parsed out different sections of each problem, including the problem description, input format, output format, examples, notes, and so on, to allow for more flexible restructuring of the prompts during testing. By default, however, the problems are displayed in their original HTML format to preserve critical information and structure with minimal format translation for better clarity of text and formulas format. While these HTML formats might be challenging for humans to process without rendering, they can be easily processed by LLMs (Gur et al., 2022)."
        },
        {
            "title": "3.2 Classification and Categories",
            "content": "The CODEELO includes several distinct classifications. These classifications are sourced and collected from the CodeForces platform, and we integrate them into our benchmark for more comprehensive analysis and detailed evaluations of various LLMs. Contest Difficulty Division CodeForces categorizes competitions into four divisions (Div. for short) based on their levels of difficulty: Div. 1, 2, 3, and 4, with Div. 1 being the most challenging and Div. 4 the least. Additionally, there are special divisions that combine Div. 1 and 2, as well as some global rounds whose difficulty averages between Div. 1 and 2. We classify these as Div. 1 + 2. Its important to note that each contest includes multiple problems, and the division just represents an overall average difficulty, so hard contests can also contain easier problems. Problem Difficulty Rating The problems also have rating attribute but please note that this is different concept with ratings for participants. The problem rating indicates the level of difficulty of problem. Specifically, problem rating of signifies that competitors with rating of have 50% probability of passing the problem the first time they see it. The problem ratings are derived from actual human performance in contests and represent statistically calculated value, making it relatively accurate. Problem Algorithm Tags Problems are categorized using algorithm tags, which identify the types of algorithms required to solve them. On average, problem is associated with 3.9 tags, as it may need multiple steps or belong to different algorithmic categories. We have total of 35 tags, and nearly 90% of the occurrences are covered by the top 16 tags. Note that these tags are not visible to humans during contests and also not visible to the models being tested, thus they cannot serve as hints for solutions."
        },
        {
            "title": "3.3.1 Solution Submitting and Judgment",
            "content": "Unlike other benchmarks that require testing on ones own machine, which may cause execution environment misalignment, we parse the solution code block and directly submit it to the CodeForces official platform after obtaining the models output to achieve absolutely accurate feedback. This is especially important in competitive coding problems since execution time is crucial metric in judging solution, and different environments may vary in this aspect. The proxy judging settings are also advantageous because they not only allow us to bypass the need for complete test cases but also support the evaluation of problems that require special judges, thereby enabling us to better assess the models capabilities and provide more accurate results. We use an automatic submission bot to help submit the problems and obtain judging results. Since the results come directly from the official platform, we can directly parse the status to see if it is an \"Accept.\" Unlike some other competitive coding benchmarks like APPS, we consider solution accepted only when it passes all the test cases; partially passing proportion of test cases does not count towards scoring, aligned with the criteria on the platform."
        },
        {
            "title": "3.3.2 Elo Rating Calculation System",
            "content": "We use an Elo rating calculation method similar to the official CodeForces platform Elo rating calculation system4 to obtain standardized Elo rating. This rating reflects an individuals competitive programming ability and is comparable between humans, models, and across both humans and models. Unlike CodeForces, which updates participants rating by considering changes after each contest to maintain it online, we treat each contest as independent for simplicity and accuracy. Thus, we calculate the models expected rating for each contest individually. Specifically, lets assume there are human participants in contest with ratings r(i) for = 1, 2, ..., n. For ease of mathematical representation, assume participants are ranked from best to worst in terms of performance in this contest. Position the models performance within this ranking, denoting the models rank as (where 1 + 1). Suppose the models expected rating is r. According to the definition of the Elo rating (Elo and Sloan, 1978), we have the following equation: = i=1 1 1 + 10(rr(i))/400 Since the expression on the right side of the equation is monotonically decreasing with respect to r, we can easily use binary search to determine the exact value of r. As demonstrated in Appendix C, we have analyzed that this calculation method maintains the same expected rating as the official platforms method but with significantly lower variance. Advantages of Elo Ratings Over Other Metrics While many existing benchmarks use pass@n (with typically being 1 or other values) as the evaluation metric, our benchmark adopts more advanced Elo rating system. This system accounts for multiple attempts, providing more comprehensive analysis than simply considering pass@1 and takes the sampling diversity into account. Moreover, it effectively balances pass attempts by penalizing failed attempts before the successful ones, which is superior to the traditional pass@n metric. In addition, passing more challenging problems will receive higher scores in rating calculations, encouraging models to tackle more difficult tasks, which is also feature not supported by most previous benchmarks."
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "Based on our observation that most models struggle with even the simplest problems in Div. 1 contests, we decided to discard these contests and focus solely on testing the remaining divisions in the most recent contests. We gathered contests held between May 4, 2024, and November 4, 2024, totaling 54 contests or 398 problems. We present basic statistics for these contests in Table 2. Div. Count Avg. Problems Avg. Ratings"
        },
        {
            "title": "Rating\nRequirement",
            "content": "1 1+2 2 3 4 3 8 33 10 3 7.0 9.1 6.5 7.5 8.3 2533 2106 1779 1436 1276 1900 All 2100 1600 1400 Table 2: Basic statics of different contest divisions. For each problem, we allow each tested model up to eight attempts. Given that the inference times for each model are significantly lower than those for humans and can be omitted, we assume that models will respond and submit solutions within the first minute, so no time penalties are applied. However, penalty points will still be counted for any failed attempts made before successful submission, which aligned with the platform. Each problem has specific score, and we calculate the final scores and penalties of the tested models to compare and rank them against human participants. All conditions not mentioned are kept as similar as possible to the official platform settings. We evaluated 30 popular open-source models using vLLM for inference and 4 proprietary models via API calls (detailed in the model cards found in Appendix A). All tested models followed the same chain-of-thought prompting: 4https://codeforces.com/blog/entry/20762 5 You are coding expert. Given competition-level coding problem, you need to write C++ program to solve it. You may start by outlining your thought process. In the end, please provide the complete code in code block enclosed with ``` ```. Here C++ was chosen as the test language because it generally elicits the best performance from models on competition code problems. detailed analysis of language use is provided in Section 5.2."
        },
        {
            "title": "4.2 Elo Ratings",
            "content": "We present the standardized Elo rating leaderboard in Figure 1. Our analysis reveals that o1-mini, with rating of 1578, and QwQ-32B-Preview, with rating of 1261, stand out significantly among proprietary and open-source models, respectively. This demonstrates the substantial advantage of long CoT o1-like reasoning models in tackling difficult competition code problems. Additionally, we observe clear trend that larger models tend to outperform smaller ones, and all open-source models that achieved rating above 500 are 30B+ models. However, their ratings remain below 700, which corresponds to approximately the lowest 20th percentile among all human participants in Table 6."
        },
        {
            "title": "4.3 Main Results",
            "content": "We present the performance details of all tested proprietary and open-source models in Table 3. For clearer comparison, we categorize open-source models by size and classify mixture-of-experts models by regarding their parameters as the square root of the product of their activation parameters and total parameters."
        },
        {
            "title": "Pass Rate for",
            "content": "Pass @"
        },
        {
            "title": "Overall",
            "content": "Div. 1 + 2 Div. 2 Div. 3 Div."
        },
        {
            "title": "Easy Medium Hard",
            "content": "1 2 4 8 ChatGPT-4o-latest-2024-11-20 Claude-3-5-Sonnet-2024-10-22 o1-mini (cid:181) 668 (22.2) (cid:181) 710 (24.1) (cid:181) 1578 (89.2) 586 430 1197 507 616 1541 1111 1092 1906 1149 1124 1792 36.54 46.47 74.52 14.0 11.0 42. 0.83 0.97 11.71 9.3 11.81 26.88 10.8 13.82 33.92 14.57 15.58 39.7 16.83 16.08 39."
        },
        {
            "title": "Proprietary LLMs",
            "content": "DS-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct 1.3B 1.5B 3B 21 (0.0) 93 (0.0) 160 (0.0) Mistral-7B-Instruct-v0.2 DS-V2-Lite-Chat OpenCoder-8B-Instruct DS-Coder-6.7B-Instruct Ministral-8B-Instruct-2410 Llama-3.1-8B-Instruct DS-V2-Lite-Instruct Yi-Coder-9B-Chat Qwen2.5-7B-Instruct Qwen2.5-Coder-7B-Instruct 7B 2.4/16B 8B 6.7B 8B 8B 2.4/16B 9B 7B 7B 49 (0.0) 60 (0.0) 152 (0.0) 155 (0.0) 219 (0.0) 223 (0.0) 254 (0.0) 296 (0.0) 315 (0.0) 397 (11.6) 0 0 0 0 0 0 0 0 0 187 108 123 143 1B+ Open-source LLMs 0 48 74 0 179 398 378 514 3.37 6.73 10.9 6B+ Open-source LLMs 0 16 70 97 118 207 155 228 242 334 146 151 372 319 548 325 446 560 581 647 378 378 667 606 745 585 851 606 676 842 13B+ Open-source Models Mixtral-8x7B-Instruct-v0.1 Starcoder2-15B-Instruct-v0.1 Codestral-22B-v0.1 Qwen2.5-14B-Instruct Qwen2.5-Coder-14B-Instruct 8/56B 15B 22B 14B 14B 98 (0.0) 129 (0.0) 385 (10.2) 414 (12.9) 424 (13.5) 0 0 57 497 123 30 48 345 277 292 226 333 586 752 644 641 926 606 1067 30B+ Open-source Models CodeLlama-70B-Instruct DS-Coder-33B-Instruct Mixtral-8x22B-Instruct-v0.1 DS-V2-Chat Llama-3.1-70B-Instruct Qwen2.5-32B-Instruct DS-Coder-V2-Instruct Qwen2.5-Coder-32B-Instruct DS-V2.5 Mistral-Large-Instruct-2411 Qwen2.5-72B-Instruct QwQ-32B-Preview 200 (0.0) 70B 207 (0.0) 33B 295 (0.0) 22/176B 318 (0.0) 21/236B 478 (15.0) 70B 513 (15.5) 32B 532 (15.7) 21/236B 575 (16.8) 32B 629 (20.4) 21/236B 631 (20.5) 123B 72B 634 (20.7) 32B 1261 (63.6) 0 0 75 59 255 350 420 206 594 632 439 1071 111 113 241 253 361 366 400 416 483 449 498 539 498 510 588 886 923 853 1166 958 1049 1033 1566 507 746 680 737 933 1146 1179 1222 1221 1226 1255 1700 0.0 0.0 0.5 0.0 0.0 0.5 0.25 0.5 0.25 3.5 1.75 1.5 3.0 0.25 0.0 2.25 1.5 5.75 0.75 1.5 0.5 2.25 3.0 6.5 7.5 7.75 10.0 9.5 12.0 21. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.05 0.0 0.05 0.09 0.09 0.14 0.05 0.0 0.14 0.32 0.32 0.05 0.0 0.05 0.0 0.46 0.46 0.37 0.46 0.65 0.65 0.97 4.54 0.75 1.26 2.26 1.26 1.01 1.01 1.76 2.51 2.26 3.02 2.76 4.27 4. 1.26 1.76 3.52 5.03 6.78 0.75 1.76 2.51 1.26 1.26 2.51 2.26 3.52 2.76 4.27 4.02 5.53 6.03 1.51 2.76 4.77 6.03 8.04 0.75 2.51 4.02 1.26 1.76 3.77 3.27 4.77 4.52 5.78 6.28 6.78 8. 2.26 3.27 6.78 7.79 9.55 1.76 3.02 3.27 3.77 5.03 5.53 6.53 6.03 8.79 8.29 9.3 18.59 2.76 4.27 4.27 4.77 7.29 8.29 8.79 8.54 11.56 11.81 11.06 23.12 5.03 4.52 5.78 6.53 10.55 10.8 11.81 12.06 13.32 13.07 13.32 29.4 0.75 2.51 4.77 1.26 1.76 4.52 4.52 6.28 6.53 6.78 7.29 7.79 10. 3.52 3.52 10.3 11.31 12.06 5.78 6.28 7.04 9.05 12.56 13.07 14.32 16.58 15.58 16.33 16.58 32.91 6.25 4.01 8.17 10.1 13.94 12.18 16.51 14.26 17.63 19.55 5.29 5.93 20.03 23.4 25.64 8.97 13.46 14.42 16.83 25.32 28.85 29.33 29.49 33.65 35.58 35.26 57.21 Table 3: Main results of different LLMs on CODEELO. The number in parentheses after the overall Elo rating shows the percentile rank among human participants. The underlined numbers represent the best scores within the same model size range. Elo Ratings Across Contest Divisions From contest perspective, we observe that contests in easier divisions often result in higher performance ratings. However, we note that this trend may not always hold true if the models capacity is larger. The best results are typically achieved when models are tested in contests that match their skill level (refer to rating requirements in Table 2 for approximate contest ratings based on Elo ratings). Consequently, 6 most models perform optimally in Div. 4. However, o1-mini demonstrates the best performance in Div. 3, consistent with its higher rating and the rating requirements for that division. From model perspective, we find that superior models consistently outperform inferior ones across different contest divisions. Pass Rate Across Problem Difficulty Levels We flatten the problems in contests and divide them by problem ratings: Easy ([800, 1000)), Medium ([1000, 1300)), and Hard ([1300, 3500]). We record the model pass rate at these difficulty levels and find that even the easy category is challenging for most models and demonstrates the best differentiation. The medium level effectively distinguishes several advanced models. Although the minimum problem rating in the hard level is only 1300, all models, except for o1-mini and QwQ-32B-Preview, are struggling to achieve pass. These results also indicate significant room for improvement in existing models and highlight the extensibility of our benchmark. Pass@n We also display the pass@n metrics for different values of n(n = 1, 2, 4, 8) for each model. Our findings indicate that most models increase their pass rate consistently as the number of samplings rises. Some models may have poor pass@1 results but improve significantly by pass@8, showcasing the models ability to produce diverse effective explorations. Moreover, we find that pass@n often correlates with Elo ratings, but they may not always align due to their differing calculation methods."
        },
        {
            "title": "5.1 Performance Across Algorithms",
            "content": "Each problem is associated with tags that suggest potential algorithms or methods for solving it. We have identified 16 tags that appear frequently, each corresponding to at least 30 problems tested. Note that single problem may be associated with multiple tags, as it may require multiple steps or fall under different algorithmic categories. The performance of different models on these tagged problems is summarized in Table 4. Model Gr. Ma. Im. BF. DP DS. CA. BS. So. Gr. DFS NT. Tr. Co. TP. Bi. ChatGPT-4o-latest-2024-11-20 Claude-3-5-Sonnet-2024-10-22 o1-mini (cid:181) 5.60 (cid:181) 9.40 (cid:181) 25.83 9.07 12.02 31.11 12.80 15.97 31.94 9.53 10.35 23.98 2.17 0.00 10. 1.59 2.50 14.77 6.39 7.07 22.15 4.17 5.25 22.38 14.58 17.50 34.58 1.82 0.78 13.54 0.00 0.80 11. 4.83 5.11 22.73 0.00 0.00 4.55 4.28 3.62 25.00 6.07 7.50 19.29 2.57 2.94 20.59 Proprietary LLMs DS-Coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct 1.3B 1.5B 3B Mistral-7B-Instruct-v0.2 DS-V2-Lite-Chat OpenCoder-8B-Instruct DS-Coder-6.7B-Instruct Ministral-8B-Instruct-2410 Llama-3.1-8B-Instruct DS-V2-Lite-Instruct Yi-Coder-9B-Chat Qwen2.5-7B-Instruct Qwen2.5-Coder-7B-Instruct Mixtral-8x7B-Instruct-v0.1 Starcoder2-15B-Instruct-v0.1 Codestral-22B-v0.1 Qwen2.5-14B-Instruct Qwen2.5-Coder-14B-Instruct 7B 2.4/16B 8B 6.7B 8B 8B 2.4/16B 9B 7B 7B 8/56B 15B 22B 14B 14B 0.00 0.06 0.54 0.00 0.06 0.54 0.60 1.01 0.65 1.73 1.67 1.49 2.14 0.06 0.36 2.32 3.21 3.33 0.55 1.10 2.06 1.03 0.69 1.24 1.79 2.40 2.61 3.78 2.82 3.78 3.98 1.17 0.96 3.71 5.43 5. 2.08 3.27 3.97 3.17 2.28 4.07 4.17 5.36 4.76 6.85 5.85 5.36 6.55 2.18 2.78 9.03 7.94 9.13 1B+ Open-source LLMs 0.00 0.51 1.74 0.00 0.00 0. 0.00 0.00 0.23 0.00 0.00 0.68 6B+ Open-source LLMs 0.72 0.20 0.51 1.23 1.84 1.13 3.48 2.15 2.97 3.38 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.43 0.00 0.11 0.00 0.00 0.11 0.00 0.34 0.00 0.68 0.23 0.80 1. 0.00 0.14 0.95 0.54 0.27 1.49 1.77 1.90 1.49 2.04 13B+ Open-source Models 0.51 0.61 2.77 3.38 5.74 0.00 0.00 0.00 0.65 1.20 0.00 0.00 0.23 1.14 1.36 0.27 0.68 2.45 2.31 2. 30B+ Open-source Models CodeLlama-70B-Instruct DS-Coder-33B-Instruct Mixtral-8x22B-Instruct-v0.1 DS-V2-Chat Llama-3.1-70B-Instruct Qwen2.5-32B-Instruct DS-Coder-V2-Instruct Qwen2.5-Coder-32B-Instruct DS-V2.5 Mistral-Large-Instruct-2411 Qwen2.5-72B-Instruct QwQ-32B-Preview 0.48 70B 1.37 33B 1.55 22/176B 1.61 21/236B 2.98 70B 3.75 32B 3.81 21/236B 4.05 32B 5.18 21/236B 6.01 123B 72B 6.79 32B 15.00 1.65 2.40 3.09 3.57 5.98 6.59 6.94 7.01 8.24 8.17 9.00 21.70 3.87 5.36 5.56 6.35 10.02 9.72 11.21 9.62 13.10 11.61 12.40 19.64 0.92 1.33 1.95 2.77 4.00 4.51 4.82 6.35 6.05 6.05 7.68 15. 0.00 0.33 0.00 0.11 0.33 0.87 1.09 1.52 1.30 1.63 1.41 3.37 0.34 0.11 0.11 0.68 0.80 1.59 1.14 1.59 1.70 2.16 1.48 8.18 0.41 0.82 1.90 1.90 2.72 3.67 3.94 4.76 4.89 4.48 7.34 15.35 0.00 0.62 0.77 0.00 0.00 0.15 0.62 1.08 0.62 1.08 1.23 1.54 1.85 0.93 0.00 0.77 2.01 2. 0.62 0.77 1.08 1.23 2.78 3.86 2.93 4.01 2.62 4.78 3.70 8.80 1.46 2.08 3.96 3.12 1.67 2.29 2.92 3.96 3.54 5.21 5.62 5.00 7.29 1.25 2.29 6.46 7.29 9.58 2.92 3.54 7.29 4.58 9.17 10.83 8.75 11.04 12.71 13.96 16.88 23.96 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.26 0.00 0.00 0.00 0.00 0.26 1.04 0.00 0.00 0.00 0.00 0.00 0.78 2.08 1.30 2.08 1.04 2.60 4.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.27 0.00 0.00 0.00 0.27 0.27 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.27 0.00 0.00 1.33 3.19 0.00 0.57 1.42 0.00 0.00 0.28 1.42 1.99 1.14 2.27 1.42 2.27 2.27 0.57 0.57 1.70 2.56 2.84 0.85 1.99 0.85 2.27 2.27 2.84 2.84 3.41 2.56 2.84 3.12 9.66 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.28 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.57 0.00 0.00 0.00 0.00 0.00 0.33 0.33 0.00 0.33 1.97 0.33 0.00 0.33 0.00 0.00 0.33 0.66 0. 0.66 0.00 0.00 0.00 0.66 0.66 2.63 1.32 3.62 0.99 1.32 14.14 0.00 0.00 1.07 0.00 0.00 0.36 0.36 0.71 0.36 1.43 0.00 2.14 1.07 0.00 0.00 0.36 1.43 2.86 0.71 0.71 0.71 0.00 2.86 1.79 5.00 5.00 3.57 7.50 5.71 6.43 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.37 0.00 0.37 0.00 0.00 0.37 0.37 0.00 0.00 0.37 0.37 0.00 1.10 1.47 1.10 0.74 1.84 2.94 1.84 8.09 Table 4: Pass rate (pass@1) on major problem categories that have at least 30 problems tested. The abbreviations \"Gr.\", \"Ma.\", \"Im.\", \"BF.\", \"DP\", \"DS.\", \"CA.\", \"BS.\", \"So.\", \"Gr.\", \"DFS\", \"NT.\", \"Tr.\", \"Co.\", \"TP.\", and \"Bi.\" stand for greedy, math, implementation, brute force, dp, data structures, constructive algorithms, binary search, sortings, graphs, dfs and similar, number theory, trees, combinatorics, two pointers, and bitmasks, respectively. We observe significant variation in model performance across different algorithms. Models demonstrate strong performance in areas such as math (Ma.), implementation (Im.), and sorting (So.), achieving the highest pass rates. However, they struggle with dp (DP), dfs and similar (DFS), and trees (Tr.), with many models failing to solve even single problem under these algorithms."
        },
        {
            "title": "5.2 Comparison between C++ and Python",
            "content": "An interesting observation is that while Python is the most commonly used programming language for most models, the best performance on CODEELO is achieved when models use C++ as the coding language. This performance surpasses even the scenarios where models select the language on their own. We conducted experiments where models were given prompts without specified programming language, allowing them to choose freely. We found that under our CODEELO, nearly all models defaulted to using Python more than 95% of the time, with only occasional use of C++, Java, and others. In contrast, human participants in coding competitions predominantly use C++, with usage rates close to 80%5. We further investigated the performance difference of models when they were constrained to use either C++ or Python. We instructed several popular LLMs by specifying the coding language in prompts, and the results are displayed in Figure 2. Our findings show that all models achieved higher ratings when using C++. This aligns with human experience, as competition-level code problems often impose constraints on algorithm running time, and C++ is more efficient at meeting these challenges. These findings disclose existing model training shortcomings and offer guidance on enhancing model language selection cognition: models should be trained to use C++ when facing problems where runtime efficiency is critical to get better performance. These findings also indicate that we had better test the models in C++ to unlock their best performance. This insight contrasts existing competition-level code benchmarks like APPS and LiveCodeBench, which predominantly assess performance using Python. Figure 2: The Elo ratings using C++ and Python as programming languages."
        },
        {
            "title": "5.3 Rating Variance",
            "content": "The variance in ratings is crucial aspect of the benchmark. In Figure 3, we present violin plots of several models across all tested contests. We observe that most models exhibit standard deviation between 300 and 500. This fluctuation in ratings can be attributed to the models limited ability; solving just one additional problem significantly boosts their ratings in one contest since they can only pass very few. To reduce this standard deviation, increasing the number of tested contests can be beneficial. In our experiments, by testing across 54 contests, the standard deviation in overall average ratings is reduced to around 50, which we is acceptable, and the violin plots can effectively demonstrate their performance comparisons."
        },
        {
            "title": "6.1 Contributions",
            "content": "Due to the unique nature of our work compared to other competition code benchmarks, it is important to clearly outline the contributions we make to the NLP community. 1. Similar to other benchmarks, we provide high-quality test problems. However, unlike others that also include CodeForces problems, our dataset offers full set of problems that can be updated online and includes detailed 5We randomly selected 250 human submissions and found 201 used C++, and this ratio will be even higher among proficient competitors. 8 Figure 3: Violin plots of ratings across tested contests. information such as contest divisions, problem difficulty ratings, and problem tags. This allows for more comprehensive evaluations compared to existing benchmarks. 2. Our approach stands out by using unique evaluation method in which problems are submitted directly to the platform. In our previous analysis, we found that this method is crucial for competition code problems, as many of them require special judges and have unaccessible hidden test cases. Unlike general code generation tasks, program efficiency is critical metric in judging competition code problems. Evaluating them independently can lead to misalignment of the environment and inaccurate results. Our method successfully provides an easy way to address these issues, providing more accurate evaluations that reflect the models true capabilities. 3. We are the first to provide human-comparable Elo ratings for existing models. This allows us to assess the progress of current AI models in relation to human performance. Our findings suggest that o1-like reasoning models show promise in improving coding capabilities, given their significant advantage over other advanced models on our benchmark. We also provide detailed results across different contest divisions, problem difficulties, and algorithm tags, which can help developers specifically target and improve corresponding abilities. 4. We offer insights that previous work has overlooked or failed to achieve. For example, we find that while Python is the most familiar language for existing LLMs, they often perform better when answering in C++ under CODEELO. This highlights limitation of previous competition code benchmarks that evaluate solely in Python, as they may not fully stimulate the models best performance. It also suggests that model developers should consider training their models to output C++ code when efficiency is critical factor."
        },
        {
            "title": "6.2 Limitations",
            "content": "We believe our benchmark has the following two limitations: One limitation is that we only allow eight submissions per problem. In practice, users can make additional submissions as long as the penalty scores remain lower than the passing scores. This constraint might result in the tested Elo ratings being slightly lower than the actual ratings, as models may successfully solve problems with more attempts. However, it is carefully considered value to balance the alignment of actual performance while avoiding excessive submissions that could contaminate the platform and impact other users access, since the margin of larger submission times, like 32 tries or more, will decrease quickly. Therefore, we adopt this setting and encourage others to remain the same for fair comparison. Another limitation is that we rely on interaction with the CodeForces platform to conduct the judging process, whereas previous benchmarks typically only required offline testing. This reliance is necessary due to the need for bypassing the access to hidden test cases and special judges to provide accurate feedback. In other words, if we had all the hidden test cases and special judges in our hands, we could conduct the evaluation independently. However, these resources are hidden from the platform and extremely difficult to access, and we cannot provide them either."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we propose CODEELO benchmark, collection of Codeforces problems with detailed problem information and unique judgment method that involves submitting solutions to the CodeForces platform and 9 receiving judgment status to achieve zero false positives, special judge support, and an aligned execution environment for the first time. Based on the judgment from the platform, we developed an Elo rating calculation system that aligns with the platform but has lower variance. Testing on 30 open-source and 3 proprietary LLMs, we find that o1-mini and QwQ-32B-Preview stand out significantly among all models, and most models struggle even with the easiest problems and fall in the lowest 20 percent among all human participants. We further conduct analysis experiments and find different performances of LLMs across algorithm tags, and the best performance is in C++ rather than Python, contradictory with former benchmarks. We have made our CODEELO benchmark publicly available and hope it can pave the way for the NLP community to test LLMs sophisticated reasoning abilities on code and provide insights for future studies."
        },
        {
            "title": "8 Ethical Statement",
            "content": "Our benchmark relies on the CodeForces platform to conduct judgments. We strictly adhere to the Codeforces Terms and Conditions6 throughout all experiments and emphasize that others should follow the same. This benchmark is for academic purposes only and should be used in limited way to avoid impacting user access to the platform. It is designed for virtual participation only and cannot be used for in-contest testing, in accordance with the CodeForces Rule Restricting the Use of AI7. Due to ethical considerations, we will conduct comprehensive risk assessment and seek permission from the CodeForces platform before open-sourcing the entire submission and evaluation scaffold, and we have not included it in this version of the paper. Before that, we recommend that others independently reproduce our proposed method to conduct evaluations. We would also like to express our great acknowledgment to Mike Mirzayanov for creating the remarkable CodeForces platform."
        },
        {
            "title": "References",
            "content": "01.AI. Meet yi-coder: small but mighty llm for code, September 2024. URL https://01-ai.github.io/ blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md. Anonymous. AoPS dataset: Leveraging online olympiad-level math problems for LLMs training and contaminationresistant evaluation. In Submitted to The Thirteenth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=Bgz3okeZ7H. under review. Anthropic. Anthropic: Introducing claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. DeepSeek. Deepseek-r1-lite-preview is now live: unleashing supercharged reasoning power!, 2024. URL https: //api-docs.deepseek.com/news/news1120. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Arpad Elo and Sam Sloan. The rating of chessplayers: Past and present. (No Title), 1978. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. Understanding html with large language models. arXiv preprint arXiv:2210.03945, 2022. 6https://codeforces.com/terms 7https://codeforces.com/blog/entry/133941 10 Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, Yang, JH Liu, Chenchen Zhang, Linzheng Chai, et al. Opencoder: The open cookbook for top-tier code large language models. arXiv preprint arXiv:2411.04905, 2024. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Mohammad Abdullah Matin Khan, Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty. xcodeeval: large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval. arXiv preprint arXiv:2303.03004, 2023. Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. MAA. American invitational mathematics examination - aime. Examination - AIME 2024, February 2024. american-invitational-mathematics-examination-aime. In American Invitational Mathematics URL https://maa.org/math-competitions/ OpenAI."
        },
        {
            "title": "Openai",
            "content": "o1-mini,"
        },
        {
            "title": "September",
            "content": "2024a."
        },
        {
            "title": "URL",
            "content": "https://openai.com/index/ openai-o1-mini-advancing-cost-efficient-reasoning/. OpenAI. Introducing openai o1-preview, September 2024b. URL https://openai.com/index/ introducing-openai-o1-preview/. OpenAI. nouncement, day-12-of-shipmas-new-frontier-models-o3-and-o3-mini-announcement/1061818."
        },
        {
            "title": "12\nDecember",
            "content": "of shipmas: 2024c."
        },
        {
            "title": "New frontier models",
            "content": "anand https://community.openai.com/t/ o3-mini o3 Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Quan Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. Can language models solve olympiad programming? arXiv preprint arXiv:2404.10952, 2024. Mistral AI Team. Codestral: Hello, world!, May 2024a. URL https://mistral.ai/news/codestral/. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024b. URL https://qwenlm. github.io/blog/qwq-32b-preview/. Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro Von Werra, Arjun Guha, and Lingming Zhang. Selfcodealign: Self-alignment for code generation. arXiv preprint arXiv:2410.24198, 2024. 11 An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        },
        {
            "title": "A Model Cards",
            "content": "We list and cite all the tested models in Table 5."
        },
        {
            "title": "HuggingFace Endpoint",
            "content": "Claude-3-5-Sonnet-2024-10-22 Anthropic (2024) ChatGPT-4o-latest-2024-11-20 Hurst et al. (2024) o1-mini OpenAI (2024a) - - - Qwen2.5-Coder-1.5B-Instruct Qwen2.5-Coder-3B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct QwQ-32B-Preview DS-Coder-1.3B-Instruct DS-Coder-6.7B-Instruct DS-Coder-33B-Instruct DS-Coder-V2-Lite-Instruct DS-Coder-V2-Instruct DS-V2-Lite-Chat DS-V2-Chat DS-V2.5 CodeLlama-70B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Codestral-22B-v0.1 Mistral-7B-Instruct-v0.2 Ministral-8B-Instruct-2410 Mistral-Large-Instruct-2411 Mixtral-8x7B-Instruct-v0.1 Mixtral-8x22B-Instruct-v0.1 Hui et al. (2024) Hui et al. (2024) Hui et al. (2024) Hui et al. (2024) Hui et al. (2024) Yang et al. (2024) Yang et al. (2024) Yang et al. (2024) Yang et al. (2024) Team (2024b) Guo et al. (2024) Guo et al. (2024) Guo et al. (2024) Zhu et al. (2024) Zhu et al. (2024) Liu et al. (2024) Liu et al. (2024) Liu et al. (2024) Qwen/Qwen2.5-Coder-1.5B-Instruct Qwen/Qwen2.5-Coder-3B-Instruct Qwen/Qwen2.5-Coder-7B-Instruct Qwen/Qwen2.5-Coder-14B-Instruct Qwen/Qwen2.5-Coder-32B-Instruct Qwen/Qwen2.5-7B-Instruct Qwen/Qwen2.5-14B-Instruct Qwen/Qwen2.5-32B-Instruct Qwen/Qwen2.5-72B-Instruct Qwen/QwQ-32B-Preview deepseek-ai/deepseek-coder-1.3b-instruct deepseek-ai/deepseek-coder-6.7b-instruct deepseek-ai/deepseek-coder-33b-instruct deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct deepseek-ai/DeepSeek-Coder-V2-Instruct deepseek-ai/DeepSeek-V2-Lite-Chat deepseek-ai/DeepSeek-V2-Chat deepseek-ai/DeepSeek-V2.5 Roziere et al. (2023) meta-llama/CodeLlama-70b-Instruct-hf Dubey et al. (2024) Dubey et al. (2024) meta-llama/Llama-3.1-8B-Instruct meta-llama/Llama-3.1-70B-Instruct Team (2024a) Jiang et al. (2023) Jiang et al. (2023) Jiang et al. (2023) Jiang et al. (2024) Jiang et al. (2024) mistralai/Codestral-22B-v0.1 mistralai/Mistral-7B-Instruct-v0.2 mistralai/Ministral-8B-Instruct-2410 mistralai/Mistral-Large-Instruct-2411 mistralai/Mixtral-8x7B-Instruct-v0.1 mistralai/Mixtral-8x22B-Instruct-v0.1 OpenCoder-8B-Instruct Yi-Coder-9B-Chat Starcoder2-15B-Instruct-v0.1 Huang et al. (2024) 01.AI (2024) Wei et al. (2024) infly/OpenCoder-8B-Instruct 01-ai/Yi-Coder-9B-Chat bigcode/starcoder2-15b-instruct-v0. Table 5: Model cards."
        },
        {
            "title": "B Decoding Hyperparameters",
            "content": "All proprietary models use API calls with default parameters. For open-source models, the inference settings are temperature=0.7, top_p=0.8, top_k=20, and repetition_penalty=1.1. The maximum number of output tokens is set to 4,096 for all models, except for QwQ-32B-Preview, which is set to 32,768 tokens."
        },
        {
            "title": "C Analysis of Our Elo Rating Calculation System",
            "content": "In Section 3.3.2, we present our method for calculating Elo ratings for each contest. Although there are slight differences between our system and the original CodeForces Elo rating calculation, we provide proof to demonstrate that our ratings are equivalent to the original. For simplicity, we do not consider the differences between divisions here. In fact, the following analysis will always hold under the same divisions, and since all LLMs attend the same set of contests, it will be fair. . It is evident that the calculated ratings will converge as approaches infinity. For any specific model, after each contest, we calculate the expected rating and then average them. We consider each contest to be independent, and since we acknowledge that the ratings are standardized, we assume the ratings for any specific model under different contests are independent and identically distributed (IID). Let this expected rating be r, and let the variance be Var(r). For total of contests, we calculate the average ratings. Since the ratings in all contests for specific model are IID, we can easily determine that the average will also have an expected value of and variance of Var(r) In the original calculation from the platform, for each individual, historical rating list is maintained after each contest, denoted as ri, with an initial value of r0 = 0. After calculating the expected rating E(ri) in the i-th contest based on performance, the rating is updated by moving halfway towards the expected rating from the current rating. Namely, the new rating can be calculated using the formula ri = ri1+E(ri) . Note that each contest is independent, and E(ri) shares the same distribution as r. Through simple mathematical transformations and deductions, we can determine that the expected value of ri will converge to r, and its variance will converge to Var(r) as the number of contests approaches infinity. These results indicate that we can achieve the same expected results as CodeForces while significantly reducing the variance by increasing the number of contests. 2 Human-Comparable Elo Rating One advantage of our benchmark is that we provide standardized Elo ratings that are comparable with those of human participants. We present each percentile of ratings among all human participants in Table 6, based on publicly available user ratings from the CodeForces platform."
        },
        {
            "title": "Percentile Rating",
            "content": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 348 351 353 356 359 362 366 371 376 383 391 401 415 437 478 559 577 591 605 621 639 662 687 707 724 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 740 754 767 781 794 806 819 832 845 858 872 886 900 913 927 942 956 971 985 1000 1014 1029 1044 1058 1073 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 1088 1103 1118 1133 1147 1162 1176 1191 1205 1218 1231 1243 1254 1265 1276 1288 1301 1313 1325 1338 1352 1365 1370 1375 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 1390 1398 1405 1411 1418 1427 1437 1448 1462 1478 1497 1518 1543 1571 1603 1624 1648 1678 1712 1751 1812 1916 2019 2157 4009 Table 6: Percentiles of ratings among all human participants, calculated based on publicly available user ratings from the CodeForces platform, collected in November, 2024."
        },
        {
            "title": "E Problem Demonstration",
            "content": "A problem demonstration in CODEELO is shown in Figure 4. Figure 4: An example of problem in CODEELO. Each problem contains: 1) title, 2) time limit, 3) memory limit, 4) problem description, 5) input format, 6) output format, 7) test case examples, and 8) note (optional). This problem can be found at https://codeforces.com/contest/2034/problem/E."
        },
        {
            "title": "F Special Judge",
            "content": "In some code competition problems, given input might have multiple valid outputs, all of which can be considered correct (Note that the input and outputs here refer to test cases, not the problem statement and model responses). In such situations, dedicated code is necessary to verify the validity of the outputs instead of simply comparing them against reference output; this is known as special judge. Its like logical unit test, but since the problems are more complex, creating special judge is also more challenging. Figure 5 showcases case demonstration. While most competition problems have single correct output for any given input and do not need special judge, 14 there are still proportion of problems that require one. We conducted an empirical study and found that 30 out of 100 randomly selected problems required special judges. Previous competition-level code benchmarks could not handle these situations and therefore did not accurately assess the full capabilities of models. Our evaluation method has the advantage of accommodating these types of problems. Similarly, we also support interactive problems8 that were not supported in earlier benchmarks. Supporting these kinds of problems is crucial for thoroughly evaluating models abilities and obtaining human-comparable Elo ratings. Figure 5: An example of problem (examples and note parts are omitted) that needs special judge since there can be multiple valid outputs for the same input (input and outputs refer to test cases but not problem and solutions). e.g., given the input \"abc\", acceptable outputs could include \"abb\", \"acc\", \"aac\", and any other string derived from \"abc\" except itself. So we cannot simply compare the output with predetermined correct solution for evaluation in this problem. CODEELO addresses this by evaluating the code submissions directly on their official platform, marking its first support for this kind of problem. The complete problem can be found at https://codeforces.com/contest/2047/problem/B. 8An example of interactive problems can be found at https://codeforces.com/contest/2036/problem/G"
        }
    ],
    "affiliations": [
        "Qwen Team, Alibaba Group"
    ]
}